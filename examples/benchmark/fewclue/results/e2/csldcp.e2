[33m[2022-08-26 14:27:35,040] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-26 14:27:35,040] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-26 14:27:35,041] [    INFO][0m - ============================================================[0m
[32m[2022-08-26 14:27:35,041] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-26 14:27:35,041] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-26 14:27:35,041] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-26 14:27:35,041] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-26 14:27:35,041] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-08-26 14:27:35,041] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-26 14:27:35,041] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-08-26 14:27:35,041] [    INFO][0m - [0m
[32m[2022-08-26 14:27:35,041] [    INFO][0m - ============================================================[0m
[32m[2022-08-26 14:27:35,041] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-26 14:27:35,042] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-26 14:27:35,042] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-26 14:27:35,042] [    INFO][0m - prompt                        :{'text':'text_a'}{'hard':'ËøôÁØáËÆ∫ÊñáÊèèËø∞‰∫Ü‰ªÄ‰πàÁü•ËØÜÔºü'}[0m
[32m[2022-08-26 14:27:35,042] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-26 14:27:35,042] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-26 14:27:35,042] [    INFO][0m - task_name                     :csldcp[0m
[32m[2022-08-26 14:27:35,042] [    INFO][0m - [0m
[32m[2022-08-26 14:27:35,042] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0826 14:27:35.043713  4872 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0826 14:27:35.048050  4872 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-26 14:27:37,889] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-08-26 14:27:37,913] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-08-26 14:27:37,914] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[33m[2022-08-26 14:27:37,920] [ WARNING][0m - Encoder has already set as lstm, change `prompt_encoder` will reset parameters.[0m
[33m[2022-08-26 14:27:37,925] [ WARNING][0m - No soft tokens in template. Use ManualTemplate for better performance.[0m
[33m[2022-08-26 14:27:37,925] [ WARNING][0m - No soft tokens in template. Use ManualTemplate for better performance.[0m
[32m[2022-08-26 14:27:37,925] [    INFO][0m - Using template: [{'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': 'ËøôÁØáËÆ∫ÊñáÊèèËø∞‰∫Ü‰ªÄ‰πàÁü•ËØÜÔºü'}][0m
2022-08-26 14:27:37,926 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-26 14:27:38,085] [    INFO][0m - ============================================================[0m
[32m[2022-08-26 14:27:38,085] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-26 14:27:38,085] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-26 14:27:38,085] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-26 14:27:38,086] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-26 14:27:38,086] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-26 14:27:38,086] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-26 14:27:38,086] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-26 14:27:38,086] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-26 14:27:38,086] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-26 14:27:38,086] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-26 14:27:38,086] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-26 14:27:38,086] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-26 14:27:38,086] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-26 14:27:38,086] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-26 14:27:38,086] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-26 14:27:38,087] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-26 14:27:38,087] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-26 14:27:38,087] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-26 14:27:38,087] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-26 14:27:38,087] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-26 14:27:38,087] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-26 14:27:38,087] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-26 14:27:38,087] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-26 14:27:38,087] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-26 14:27:38,087] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-26 14:27:38,087] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-26 14:27:38,087] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-26 14:27:38,087] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-26 14:27:38,088] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-26 14:27:38,088] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-26 14:27:38,088] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-26 14:27:38,088] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-26 14:27:38,088] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-26 14:27:38,088] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-26 14:27:38,088] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-26 14:27:38,088] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-26 14:27:38,088] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug26_14-27-35_instance-3bwob41y-01[0m
[32m[2022-08-26 14:27:38,088] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-26 14:27:38,088] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-26 14:27:38,088] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-26 14:27:38,088] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-26 14:27:38,089] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-26 14:27:38,089] [    INFO][0m - max_seq_length                :256[0m
[32m[2022-08-26 14:27:38,089] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-26 14:27:38,089] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-26 14:27:38,089] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-26 14:27:38,089] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-26 14:27:38,089] [    INFO][0m - num_train_epochs              :20.0[0m
[32m[2022-08-26 14:27:38,089] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-26 14:27:38,089] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-26 14:27:38,089] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-26 14:27:38,089] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-26 14:27:38,089] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-26 14:27:38,089] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-26 14:27:38,089] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-26 14:27:38,090] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-26 14:27:38,090] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-26 14:27:38,090] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-26 14:27:38,090] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-26 14:27:38,090] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-26 14:27:38,090] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-26 14:27:38,090] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-26 14:27:38,090] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-26 14:27:38,090] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-26 14:27:38,090] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-26 14:27:38,090] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-26 14:27:38,090] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-26 14:27:38,090] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-26 14:27:38,091] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-26 14:27:38,091] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-26 14:27:38,091] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-26 14:27:38,091] [    INFO][0m - seed                          :42[0m
[32m[2022-08-26 14:27:38,091] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-26 14:27:38,091] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-26 14:27:38,091] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-26 14:27:38,091] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-26 14:27:38,091] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-26 14:27:38,091] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-26 14:27:38,091] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-26 14:27:38,091] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-26 14:27:38,091] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-26 14:27:38,092] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-26 14:27:38,092] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-26 14:27:38,092] [    INFO][0m - [0m
[32m[2022-08-26 14:27:38,093] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-26 14:27:38,094] [    INFO][0m -   Num examples = 2036[0m
[32m[2022-08-26 14:27:38,094] [    INFO][0m -   Num Epochs = 20[0m
[32m[2022-08-26 14:27:38,094] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-26 14:27:38,094] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-26 14:27:38,094] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-26 14:27:38,094] [    INFO][0m -   Total optimization steps = 5100.0[0m
[32m[2022-08-26 14:27:38,094] [    INFO][0m -   Total num train samples = 40720[0m
[32m[2022-08-26 14:27:40,247] [    INFO][0m - loss: 4.31461105, learning_rate: 2.9941176470588237e-05, global_step: 10, interval_runtime: 2.1526, interval_samples_per_second: 3.716, interval_steps_per_second: 4.646, epoch: 0.0392[0m
[32m[2022-08-26 14:27:41,460] [    INFO][0m - loss: 4.2419754, learning_rate: 2.988235294117647e-05, global_step: 20, interval_runtime: 1.2127, interval_samples_per_second: 6.597, interval_steps_per_second: 8.246, epoch: 0.0784[0m
[32m[2022-08-26 14:27:42,701] [    INFO][0m - loss: 4.2082634, learning_rate: 2.9823529411764707e-05, global_step: 30, interval_runtime: 1.2413, interval_samples_per_second: 6.445, interval_steps_per_second: 8.056, epoch: 0.1176[0m
[32m[2022-08-26 14:27:43,955] [    INFO][0m - loss: 4.22238922, learning_rate: 2.9764705882352944e-05, global_step: 40, interval_runtime: 1.2538, interval_samples_per_second: 6.38, interval_steps_per_second: 7.976, epoch: 0.1569[0m
[32m[2022-08-26 14:27:45,230] [    INFO][0m - loss: 4.2244339, learning_rate: 2.9705882352941177e-05, global_step: 50, interval_runtime: 1.2745, interval_samples_per_second: 6.277, interval_steps_per_second: 7.846, epoch: 0.1961[0m
[32m[2022-08-26 14:27:46,527] [    INFO][0m - loss: 4.20516472, learning_rate: 2.9647058823529414e-05, global_step: 60, interval_runtime: 1.2974, interval_samples_per_second: 6.166, interval_steps_per_second: 7.708, epoch: 0.2353[0m
[32m[2022-08-26 14:27:47,851] [    INFO][0m - loss: 4.11944923, learning_rate: 2.958823529411765e-05, global_step: 70, interval_runtime: 1.3241, interval_samples_per_second: 6.042, interval_steps_per_second: 7.552, epoch: 0.2745[0m
[32m[2022-08-26 14:27:49,192] [    INFO][0m - loss: 4.0633873, learning_rate: 2.952941176470588e-05, global_step: 80, interval_runtime: 1.3409, interval_samples_per_second: 5.966, interval_steps_per_second: 7.458, epoch: 0.3137[0m
[32m[2022-08-26 14:27:50,559] [    INFO][0m - loss: 4.13185387, learning_rate: 2.9470588235294117e-05, global_step: 90, interval_runtime: 1.3668, interval_samples_per_second: 5.853, interval_steps_per_second: 7.316, epoch: 0.3529[0m
[32m[2022-08-26 14:27:51,941] [    INFO][0m - loss: 4.02761345, learning_rate: 2.9411764705882354e-05, global_step: 100, interval_runtime: 1.3823, interval_samples_per_second: 5.787, interval_steps_per_second: 7.234, epoch: 0.3922[0m
[32m[2022-08-26 14:27:51,942] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 14:27:51,942] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-26 14:27:51,942] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 14:27:51,942] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 14:27:51,942] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-26 14:28:10,006] [    INFO][0m - eval_loss: 3.9539401531219482, eval_accuracy: 0.0735009671179884, eval_runtime: 18.0634, eval_samples_per_second: 114.486, eval_steps_per_second: 3.598, epoch: 0.3922[0m
[32m[2022-08-26 14:28:10,007] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-26 14:28:10,007] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 14:28:12,728] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-26 14:28:12,728] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-26 14:28:18,050] [    INFO][0m - loss: 4.16712112, learning_rate: 2.9352941176470587e-05, global_step: 110, interval_runtime: 26.1085, interval_samples_per_second: 0.306, interval_steps_per_second: 0.383, epoch: 0.4314[0m
[32m[2022-08-26 14:28:19,619] [    INFO][0m - loss: 3.92820015, learning_rate: 2.9294117647058824e-05, global_step: 120, interval_runtime: 1.569, interval_samples_per_second: 5.099, interval_steps_per_second: 6.374, epoch: 0.4706[0m
[32m[2022-08-26 14:28:21,199] [    INFO][0m - loss: 3.95745316, learning_rate: 2.923529411764706e-05, global_step: 130, interval_runtime: 1.5798, interval_samples_per_second: 5.064, interval_steps_per_second: 6.33, epoch: 0.5098[0m
[32m[2022-08-26 14:28:22,796] [    INFO][0m - loss: 3.83192825, learning_rate: 2.9176470588235294e-05, global_step: 140, interval_runtime: 1.5966, interval_samples_per_second: 5.011, interval_steps_per_second: 6.263, epoch: 0.549[0m
[32m[2022-08-26 14:28:24,416] [    INFO][0m - loss: 3.74559822, learning_rate: 2.911764705882353e-05, global_step: 150, interval_runtime: 1.6198, interval_samples_per_second: 4.939, interval_steps_per_second: 6.174, epoch: 0.5882[0m
[32m[2022-08-26 14:28:26,052] [    INFO][0m - loss: 3.72149582, learning_rate: 2.9058823529411767e-05, global_step: 160, interval_runtime: 1.6373, interval_samples_per_second: 4.886, interval_steps_per_second: 6.108, epoch: 0.6275[0m
[32m[2022-08-26 14:28:27,707] [    INFO][0m - loss: 3.68628311, learning_rate: 2.9e-05, global_step: 170, interval_runtime: 1.654, interval_samples_per_second: 4.837, interval_steps_per_second: 6.046, epoch: 0.6667[0m
[32m[2022-08-26 14:28:29,379] [    INFO][0m - loss: 3.51801453, learning_rate: 2.8941176470588237e-05, global_step: 180, interval_runtime: 1.6726, interval_samples_per_second: 4.783, interval_steps_per_second: 5.979, epoch: 0.7059[0m
[32m[2022-08-26 14:28:31,074] [    INFO][0m - loss: 3.58124962, learning_rate: 2.8882352941176473e-05, global_step: 190, interval_runtime: 1.6952, interval_samples_per_second: 4.719, interval_steps_per_second: 5.899, epoch: 0.7451[0m
[32m[2022-08-26 14:28:32,793] [    INFO][0m - loss: 3.66942558, learning_rate: 2.8823529411764707e-05, global_step: 200, interval_runtime: 1.7184, interval_samples_per_second: 4.655, interval_steps_per_second: 5.819, epoch: 0.7843[0m
[32m[2022-08-26 14:28:32,793] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 14:28:32,794] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-26 14:28:32,794] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 14:28:32,794] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 14:28:32,794] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-26 14:28:57,627] [    INFO][0m - eval_loss: 3.412199020385742, eval_accuracy: 0.18617021276595744, eval_runtime: 24.8329, eval_samples_per_second: 83.276, eval_steps_per_second: 2.617, epoch: 0.7843[0m
[32m[2022-08-26 14:28:57,628] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-26 14:28:57,628] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 14:29:01,176] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-26 14:29:01,176] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-26 14:29:06,809] [    INFO][0m - loss: 3.53070679, learning_rate: 2.8764705882352943e-05, global_step: 210, interval_runtime: 34.016, interval_samples_per_second: 0.235, interval_steps_per_second: 0.294, epoch: 0.8235[0m
[32m[2022-08-26 14:29:08,694] [    INFO][0m - loss: 3.53921623, learning_rate: 2.870588235294118e-05, global_step: 220, interval_runtime: 1.885, interval_samples_per_second: 4.244, interval_steps_per_second: 5.305, epoch: 0.8627[0m
[32m[2022-08-26 14:29:10,602] [    INFO][0m - loss: 3.46786461, learning_rate: 2.8647058823529413e-05, global_step: 230, interval_runtime: 1.9083, interval_samples_per_second: 4.192, interval_steps_per_second: 5.24, epoch: 0.902[0m
[32m[2022-08-26 14:29:12,546] [    INFO][0m - loss: 3.35407104, learning_rate: 2.8588235294117647e-05, global_step: 240, interval_runtime: 1.944, interval_samples_per_second: 4.115, interval_steps_per_second: 5.144, epoch: 0.9412[0m
[32m[2022-08-26 14:29:14,496] [    INFO][0m - loss: 3.35954666, learning_rate: 2.8529411764705883e-05, global_step: 250, interval_runtime: 1.9496, interval_samples_per_second: 4.103, interval_steps_per_second: 5.129, epoch: 0.9804[0m
[32m[2022-08-26 14:29:16,484] [    INFO][0m - loss: 3.30236778, learning_rate: 2.8470588235294117e-05, global_step: 260, interval_runtime: 1.9888, interval_samples_per_second: 4.023, interval_steps_per_second: 5.028, epoch: 1.0196[0m
[32m[2022-08-26 14:29:18,494] [    INFO][0m - loss: 3.05094395, learning_rate: 2.8411764705882353e-05, global_step: 270, interval_runtime: 2.0099, interval_samples_per_second: 3.98, interval_steps_per_second: 4.975, epoch: 1.0588[0m
[32m[2022-08-26 14:29:20,539] [    INFO][0m - loss: 3.02793922, learning_rate: 2.835294117647059e-05, global_step: 280, interval_runtime: 2.0445, interval_samples_per_second: 3.913, interval_steps_per_second: 4.891, epoch: 1.098[0m
[32m[2022-08-26 14:29:22,591] [    INFO][0m - loss: 3.09260139, learning_rate: 2.8294117647058823e-05, global_step: 290, interval_runtime: 2.0525, interval_samples_per_second: 3.898, interval_steps_per_second: 4.872, epoch: 1.1373[0m
[32m[2022-08-26 14:29:24,660] [    INFO][0m - loss: 2.97169743, learning_rate: 2.823529411764706e-05, global_step: 300, interval_runtime: 2.0683, interval_samples_per_second: 3.868, interval_steps_per_second: 4.835, epoch: 1.1765[0m
[32m[2022-08-26 14:29:24,661] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 14:29:24,661] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-26 14:29:24,661] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 14:29:24,661] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 14:29:24,661] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-26 14:29:56,192] [    INFO][0m - eval_loss: 3.0085699558258057, eval_accuracy: 0.31382978723404253, eval_runtime: 31.5306, eval_samples_per_second: 65.587, eval_steps_per_second: 2.061, epoch: 1.1765[0m
[32m[2022-08-26 14:29:56,193] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-26 14:29:56,193] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 14:29:59,351] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-26 14:29:59,351] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-26 14:30:05,691] [    INFO][0m - loss: 2.86970825, learning_rate: 2.8176470588235293e-05, global_step: 310, interval_runtime: 41.0308, interval_samples_per_second: 0.195, interval_steps_per_second: 0.244, epoch: 1.2157[0m
[32m[2022-08-26 14:30:07,929] [    INFO][0m - loss: 3.03890839, learning_rate: 2.811764705882353e-05, global_step: 320, interval_runtime: 2.238, interval_samples_per_second: 3.575, interval_steps_per_second: 4.468, epoch: 1.2549[0m
[32m[2022-08-26 14:30:10,191] [    INFO][0m - loss: 2.96853752, learning_rate: 2.8058823529411766e-05, global_step: 330, interval_runtime: 2.2624, interval_samples_per_second: 3.536, interval_steps_per_second: 4.42, epoch: 1.2941[0m
[32m[2022-08-26 14:30:12,472] [    INFO][0m - loss: 2.94808102, learning_rate: 2.8e-05, global_step: 340, interval_runtime: 2.2811, interval_samples_per_second: 3.507, interval_steps_per_second: 4.384, epoch: 1.3333[0m
[32m[2022-08-26 14:30:14,769] [    INFO][0m - loss: 3.05263062, learning_rate: 2.7941176470588236e-05, global_step: 350, interval_runtime: 2.2975, interval_samples_per_second: 3.482, interval_steps_per_second: 4.352, epoch: 1.3725[0m
[32m[2022-08-26 14:30:17,088] [    INFO][0m - loss: 2.82633629, learning_rate: 2.7882352941176473e-05, global_step: 360, interval_runtime: 2.3183, interval_samples_per_second: 3.451, interval_steps_per_second: 4.314, epoch: 1.4118[0m
[32m[2022-08-26 14:30:19,425] [    INFO][0m - loss: 2.82403812, learning_rate: 2.7823529411764706e-05, global_step: 370, interval_runtime: 2.3368, interval_samples_per_second: 3.423, interval_steps_per_second: 4.279, epoch: 1.451[0m
[32m[2022-08-26 14:30:21,811] [    INFO][0m - loss: 2.82877808, learning_rate: 2.7764705882352943e-05, global_step: 380, interval_runtime: 2.3864, interval_samples_per_second: 3.352, interval_steps_per_second: 4.19, epoch: 1.4902[0m
[32m[2022-08-26 14:30:24,189] [    INFO][0m - loss: 2.91837502, learning_rate: 2.770588235294118e-05, global_step: 390, interval_runtime: 2.3776, interval_samples_per_second: 3.365, interval_steps_per_second: 4.206, epoch: 1.5294[0m
[32m[2022-08-26 14:30:26,592] [    INFO][0m - loss: 2.75047626, learning_rate: 2.764705882352941e-05, global_step: 400, interval_runtime: 2.4028, interval_samples_per_second: 3.329, interval_steps_per_second: 4.162, epoch: 1.5686[0m
[32m[2022-08-26 14:30:26,592] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 14:30:26,593] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-26 14:30:26,593] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 14:30:26,593] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 14:30:26,593] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-26 14:31:04,952] [    INFO][0m - eval_loss: 2.6552650928497314, eval_accuracy: 0.41102514506769827, eval_runtime: 38.3589, eval_samples_per_second: 53.912, eval_steps_per_second: 1.695, epoch: 1.5686[0m
[32m[2022-08-26 14:31:04,953] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-26 14:31:04,953] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 14:31:08,516] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-26 14:31:08,517] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-26 14:31:14,849] [    INFO][0m - loss: 2.74128075, learning_rate: 2.7588235294117646e-05, global_step: 410, interval_runtime: 48.2572, interval_samples_per_second: 0.166, interval_steps_per_second: 0.207, epoch: 1.6078[0m
[32m[2022-08-26 14:31:17,446] [    INFO][0m - loss: 2.72780609, learning_rate: 2.7529411764705883e-05, global_step: 420, interval_runtime: 2.5971, interval_samples_per_second: 3.08, interval_steps_per_second: 3.85, epoch: 1.6471[0m
[32m[2022-08-26 14:31:20,044] [    INFO][0m - loss: 2.73513031, learning_rate: 2.7470588235294116e-05, global_step: 430, interval_runtime: 2.5984, interval_samples_per_second: 3.079, interval_steps_per_second: 3.849, epoch: 1.6863[0m
[32m[2022-08-26 14:31:22,661] [    INFO][0m - loss: 2.69723759, learning_rate: 2.7411764705882353e-05, global_step: 440, interval_runtime: 2.6165, interval_samples_per_second: 3.057, interval_steps_per_second: 3.822, epoch: 1.7255[0m
[32m[2022-08-26 14:31:25,302] [    INFO][0m - loss: 2.48083534, learning_rate: 2.735294117647059e-05, global_step: 450, interval_runtime: 2.6414, interval_samples_per_second: 3.029, interval_steps_per_second: 3.786, epoch: 1.7647[0m
[32m[2022-08-26 14:31:27,944] [    INFO][0m - loss: 2.62584133, learning_rate: 2.7294117647058822e-05, global_step: 460, interval_runtime: 2.6417, interval_samples_per_second: 3.028, interval_steps_per_second: 3.785, epoch: 1.8039[0m
[32m[2022-08-26 14:31:30,604] [    INFO][0m - loss: 2.49835129, learning_rate: 2.723529411764706e-05, global_step: 470, interval_runtime: 2.6604, interval_samples_per_second: 3.007, interval_steps_per_second: 3.759, epoch: 1.8431[0m
[32m[2022-08-26 14:31:33,290] [    INFO][0m - loss: 2.65900822, learning_rate: 2.7176470588235296e-05, global_step: 480, interval_runtime: 2.6857, interval_samples_per_second: 2.979, interval_steps_per_second: 3.723, epoch: 1.8824[0m
[32m[2022-08-26 14:31:36,003] [    INFO][0m - loss: 2.45414257, learning_rate: 2.711764705882353e-05, global_step: 490, interval_runtime: 2.7129, interval_samples_per_second: 2.949, interval_steps_per_second: 3.686, epoch: 1.9216[0m
[32m[2022-08-26 14:31:38,722] [    INFO][0m - loss: 2.58174419, learning_rate: 2.7058823529411766e-05, global_step: 500, interval_runtime: 2.7196, interval_samples_per_second: 2.942, interval_steps_per_second: 3.677, epoch: 1.9608[0m
[32m[2022-08-26 14:31:38,723] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 14:31:38,723] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-26 14:31:38,723] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 14:31:38,723] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 14:31:38,723] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-26 14:32:23,502] [    INFO][0m - eval_loss: 2.3895888328552246, eval_accuracy: 0.4787234042553192, eval_runtime: 44.778, eval_samples_per_second: 46.183, eval_steps_per_second: 1.452, epoch: 1.9608[0m
[32m[2022-08-26 14:32:23,503] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-08-26 14:32:23,503] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 14:32:26,669] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-08-26 14:32:26,986] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-08-26 14:32:32,646] [    INFO][0m - loss: 2.29899731, learning_rate: 2.7000000000000002e-05, global_step: 510, interval_runtime: 53.9239, interval_samples_per_second: 0.148, interval_steps_per_second: 0.185, epoch: 2.0[0m
[32m[2022-08-26 14:32:36,507] [    INFO][0m - loss: 2.20007935, learning_rate: 2.6941176470588236e-05, global_step: 520, interval_runtime: 3.8606, interval_samples_per_second: 2.072, interval_steps_per_second: 2.59, epoch: 2.0392[0m
[32m[2022-08-26 14:32:39,444] [    INFO][0m - loss: 2.23319397, learning_rate: 2.6882352941176472e-05, global_step: 530, interval_runtime: 2.9368, interval_samples_per_second: 2.724, interval_steps_per_second: 3.405, epoch: 2.0784[0m
[32m[2022-08-26 14:32:42,406] [    INFO][0m - loss: 2.16820335, learning_rate: 2.682352941176471e-05, global_step: 540, interval_runtime: 2.9617, interval_samples_per_second: 2.701, interval_steps_per_second: 3.376, epoch: 2.1176[0m
[32m[2022-08-26 14:32:45,388] [    INFO][0m - loss: 2.21763763, learning_rate: 2.6764705882352942e-05, global_step: 550, interval_runtime: 2.9824, interval_samples_per_second: 2.682, interval_steps_per_second: 3.353, epoch: 2.1569[0m
[32m[2022-08-26 14:32:48,373] [    INFO][0m - loss: 2.03939114, learning_rate: 2.6705882352941175e-05, global_step: 560, interval_runtime: 2.9845, interval_samples_per_second: 2.681, interval_steps_per_second: 3.351, epoch: 2.1961[0m
[32m[2022-08-26 14:32:51,375] [    INFO][0m - loss: 2.04323997, learning_rate: 2.6647058823529412e-05, global_step: 570, interval_runtime: 3.0027, interval_samples_per_second: 2.664, interval_steps_per_second: 3.33, epoch: 2.2353[0m
[32m[2022-08-26 14:32:54,418] [    INFO][0m - loss: 2.09182396, learning_rate: 2.6588235294117645e-05, global_step: 580, interval_runtime: 3.0428, interval_samples_per_second: 2.629, interval_steps_per_second: 3.286, epoch: 2.2745[0m
[32m[2022-08-26 14:32:57,464] [    INFO][0m - loss: 2.20715485, learning_rate: 2.6529411764705882e-05, global_step: 590, interval_runtime: 3.0464, interval_samples_per_second: 2.626, interval_steps_per_second: 3.283, epoch: 2.3137[0m
[32m[2022-08-26 14:33:00,534] [    INFO][0m - loss: 1.95580711, learning_rate: 2.647058823529412e-05, global_step: 600, interval_runtime: 3.0692, interval_samples_per_second: 2.607, interval_steps_per_second: 3.258, epoch: 2.3529[0m
[32m[2022-08-26 14:33:00,534] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 14:33:00,535] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-26 14:33:00,535] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 14:33:00,535] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 14:33:00,535] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-26 14:33:52,235] [    INFO][0m - eval_loss: 2.178516387939453, eval_accuracy: 0.49806576402321084, eval_runtime: 51.6999, eval_samples_per_second: 40.0, eval_steps_per_second: 1.257, epoch: 2.3529[0m
[32m[2022-08-26 14:33:52,236] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-08-26 14:33:52,236] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 14:33:55,366] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-08-26 14:33:55,366] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-08-26 14:34:02,298] [    INFO][0m - loss: 2.08357277, learning_rate: 2.6411764705882352e-05, global_step: 610, interval_runtime: 61.7638, interval_samples_per_second: 0.13, interval_steps_per_second: 0.162, epoch: 2.3922[0m
[32m[2022-08-26 14:34:05,542] [    INFO][0m - loss: 1.94057388, learning_rate: 2.635294117647059e-05, global_step: 620, interval_runtime: 3.2446, interval_samples_per_second: 2.466, interval_steps_per_second: 3.082, epoch: 2.4314[0m
[32m[2022-08-26 14:34:08,802] [    INFO][0m - loss: 2.02431793, learning_rate: 2.6294117647058825e-05, global_step: 630, interval_runtime: 3.2598, interval_samples_per_second: 2.454, interval_steps_per_second: 3.068, epoch: 2.4706[0m
[32m[2022-08-26 14:34:12,083] [    INFO][0m - loss: 1.73263626, learning_rate: 2.623529411764706e-05, global_step: 640, interval_runtime: 3.2811, interval_samples_per_second: 2.438, interval_steps_per_second: 3.048, epoch: 2.5098[0m
[32m[2022-08-26 14:34:15,397] [    INFO][0m - loss: 1.97122097, learning_rate: 2.6176470588235295e-05, global_step: 650, interval_runtime: 3.3144, interval_samples_per_second: 2.414, interval_steps_per_second: 3.017, epoch: 2.549[0m
[32m[2022-08-26 14:34:18,727] [    INFO][0m - loss: 1.80770531, learning_rate: 2.6117647058823532e-05, global_step: 660, interval_runtime: 3.3302, interval_samples_per_second: 2.402, interval_steps_per_second: 3.003, epoch: 2.5882[0m
[32m[2022-08-26 14:34:22,128] [    INFO][0m - loss: 2.02301521, learning_rate: 2.6058823529411765e-05, global_step: 670, interval_runtime: 3.401, interval_samples_per_second: 2.352, interval_steps_per_second: 2.94, epoch: 2.6275[0m
[32m[2022-08-26 14:34:25,497] [    INFO][0m - loss: 2.02807407, learning_rate: 2.6000000000000002e-05, global_step: 680, interval_runtime: 3.3682, interval_samples_per_second: 2.375, interval_steps_per_second: 2.969, epoch: 2.6667[0m
[32m[2022-08-26 14:34:28,886] [    INFO][0m - loss: 1.91438637, learning_rate: 2.594117647058824e-05, global_step: 690, interval_runtime: 3.3898, interval_samples_per_second: 2.36, interval_steps_per_second: 2.95, epoch: 2.7059[0m
[32m[2022-08-26 14:34:32,319] [    INFO][0m - loss: 2.05596237, learning_rate: 2.5882352941176472e-05, global_step: 700, interval_runtime: 3.4329, interval_samples_per_second: 2.33, interval_steps_per_second: 2.913, epoch: 2.7451[0m
[32m[2022-08-26 14:34:32,320] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 14:34:32,320] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-26 14:34:32,320] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 14:34:32,320] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 14:34:32,320] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-26 14:35:30,385] [    INFO][0m - eval_loss: 1.9977798461914062, eval_accuracy: 0.5338491295938105, eval_runtime: 58.0648, eval_samples_per_second: 35.615, eval_steps_per_second: 1.119, epoch: 2.7451[0m
[32m[2022-08-26 14:35:30,386] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-700[0m
[32m[2022-08-26 14:35:30,386] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 14:35:33,882] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-700/tokenizer_config.json[0m
[32m[2022-08-26 14:35:33,883] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-700/special_tokens_map.json[0m
[32m[2022-08-26 14:35:41,254] [    INFO][0m - loss: 2.01397362, learning_rate: 2.582352941176471e-05, global_step: 710, interval_runtime: 68.9344, interval_samples_per_second: 0.116, interval_steps_per_second: 0.145, epoch: 2.7843[0m
[32m[2022-08-26 14:35:44,841] [    INFO][0m - loss: 1.80707054, learning_rate: 2.576470588235294e-05, global_step: 720, interval_runtime: 3.5869, interval_samples_per_second: 2.23, interval_steps_per_second: 2.788, epoch: 2.8235[0m
[32m[2022-08-26 14:35:48,432] [    INFO][0m - loss: 1.90059738, learning_rate: 2.5705882352941175e-05, global_step: 730, interval_runtime: 3.5913, interval_samples_per_second: 2.228, interval_steps_per_second: 2.785, epoch: 2.8627[0m
[32m[2022-08-26 14:35:52,046] [    INFO][0m - loss: 2.0956665, learning_rate: 2.564705882352941e-05, global_step: 740, interval_runtime: 3.6129, interval_samples_per_second: 2.214, interval_steps_per_second: 2.768, epoch: 2.902[0m
[32m[2022-08-26 14:35:55,686] [    INFO][0m - loss: 1.73693371, learning_rate: 2.5588235294117648e-05, global_step: 750, interval_runtime: 3.6412, interval_samples_per_second: 2.197, interval_steps_per_second: 2.746, epoch: 2.9412[0m
[32m[2022-08-26 14:35:59,296] [    INFO][0m - loss: 2.05185432, learning_rate: 2.552941176470588e-05, global_step: 760, interval_runtime: 3.6093, interval_samples_per_second: 2.216, interval_steps_per_second: 2.771, epoch: 2.9804[0m
[32m[2022-08-26 14:36:03,083] [    INFO][0m - loss: 1.84449139, learning_rate: 2.5470588235294118e-05, global_step: 770, interval_runtime: 3.7871, interval_samples_per_second: 2.112, interval_steps_per_second: 2.641, epoch: 3.0196[0m
[32m[2022-08-26 14:36:06,833] [    INFO][0m - loss: 1.58355398, learning_rate: 2.5411764705882355e-05, global_step: 780, interval_runtime: 3.7504, interval_samples_per_second: 2.133, interval_steps_per_second: 2.666, epoch: 3.0588[0m
[32m[2022-08-26 14:36:10,613] [    INFO][0m - loss: 1.69667854, learning_rate: 2.5352941176470588e-05, global_step: 790, interval_runtime: 3.7796, interval_samples_per_second: 2.117, interval_steps_per_second: 2.646, epoch: 3.098[0m
[32m[2022-08-26 14:36:14,365] [    INFO][0m - loss: 1.7042181, learning_rate: 2.5294117647058825e-05, global_step: 800, interval_runtime: 3.7518, interval_samples_per_second: 2.132, interval_steps_per_second: 2.665, epoch: 3.1373[0m
[32m[2022-08-26 14:36:14,365] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 14:36:14,365] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-26 14:36:14,365] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 14:36:14,366] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 14:36:14,366] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-26 14:37:19,838] [    INFO][0m - eval_loss: 1.8858968019485474, eval_accuracy: 0.559477756286267, eval_runtime: 65.4719, eval_samples_per_second: 31.586, eval_steps_per_second: 0.993, epoch: 3.1373[0m
[32m[2022-08-26 14:37:19,838] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-800[0m
[32m[2022-08-26 14:37:19,839] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 14:37:22,615] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-800/tokenizer_config.json[0m
[32m[2022-08-26 14:37:22,616] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-800/special_tokens_map.json[0m
[32m[2022-08-26 14:37:30,513] [    INFO][0m - loss: 1.59833345, learning_rate: 2.523529411764706e-05, global_step: 810, interval_runtime: 76.1482, interval_samples_per_second: 0.105, interval_steps_per_second: 0.131, epoch: 3.1765[0m
[32m[2022-08-26 14:37:34,431] [    INFO][0m - loss: 1.48679428, learning_rate: 2.5176470588235295e-05, global_step: 820, interval_runtime: 3.9185, interval_samples_per_second: 2.042, interval_steps_per_second: 2.552, epoch: 3.2157[0m
[32m[2022-08-26 14:37:38,367] [    INFO][0m - loss: 1.36323528, learning_rate: 2.511764705882353e-05, global_step: 830, interval_runtime: 3.9361, interval_samples_per_second: 2.032, interval_steps_per_second: 2.541, epoch: 3.2549[0m
[32m[2022-08-26 14:37:42,380] [    INFO][0m - loss: 1.57863445, learning_rate: 2.5058823529411768e-05, global_step: 840, interval_runtime: 4.0128, interval_samples_per_second: 1.994, interval_steps_per_second: 2.492, epoch: 3.2941[0m
[32m[2022-08-26 14:37:46,354] [    INFO][0m - loss: 1.41428413, learning_rate: 2.5e-05, global_step: 850, interval_runtime: 3.9743, interval_samples_per_second: 2.013, interval_steps_per_second: 2.516, epoch: 3.3333[0m
[32m[2022-08-26 14:37:50,352] [    INFO][0m - loss: 1.47335434, learning_rate: 2.4941176470588238e-05, global_step: 860, interval_runtime: 3.9975, interval_samples_per_second: 2.001, interval_steps_per_second: 2.502, epoch: 3.3725[0m
[32m[2022-08-26 14:37:54,363] [    INFO][0m - loss: 1.4479022, learning_rate: 2.488235294117647e-05, global_step: 870, interval_runtime: 4.0115, interval_samples_per_second: 1.994, interval_steps_per_second: 2.493, epoch: 3.4118[0m
[32m[2022-08-26 14:37:58,429] [    INFO][0m - loss: 1.60778065, learning_rate: 2.4823529411764704e-05, global_step: 880, interval_runtime: 4.0656, interval_samples_per_second: 1.968, interval_steps_per_second: 2.46, epoch: 3.451[0m
[32m[2022-08-26 14:38:02,527] [    INFO][0m - loss: 1.34254742, learning_rate: 2.476470588235294e-05, global_step: 890, interval_runtime: 4.0985, interval_samples_per_second: 1.952, interval_steps_per_second: 2.44, epoch: 3.4902[0m
[32m[2022-08-26 14:38:06,631] [    INFO][0m - loss: 1.38909912, learning_rate: 2.4705882352941174e-05, global_step: 900, interval_runtime: 4.1032, interval_samples_per_second: 1.95, interval_steps_per_second: 2.437, epoch: 3.5294[0m
[32m[2022-08-26 14:38:06,631] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 14:38:06,632] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-26 14:38:06,632] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 14:38:06,632] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 14:38:06,632] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-26 14:39:18,148] [    INFO][0m - eval_loss: 1.8057209253311157, eval_accuracy: 0.558027079303675, eval_runtime: 71.5157, eval_samples_per_second: 28.917, eval_steps_per_second: 0.909, epoch: 3.5294[0m
[32m[2022-08-26 14:39:18,149] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-900[0m
[32m[2022-08-26 14:39:18,149] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 14:39:20,653] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-900/tokenizer_config.json[0m
[32m[2022-08-26 14:39:20,653] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-900/special_tokens_map.json[0m
[32m[2022-08-26 14:39:28,964] [    INFO][0m - loss: 1.63372211, learning_rate: 2.464705882352941e-05, global_step: 910, interval_runtime: 82.3329, interval_samples_per_second: 0.097, interval_steps_per_second: 0.121, epoch: 3.5686[0m
[32m[2022-08-26 14:39:33,252] [    INFO][0m - loss: 1.64111748, learning_rate: 2.4588235294117648e-05, global_step: 920, interval_runtime: 4.288, interval_samples_per_second: 1.866, interval_steps_per_second: 2.332, epoch: 3.6078[0m
[32m[2022-08-26 14:39:37,565] [    INFO][0m - loss: 1.42782679, learning_rate: 2.452941176470588e-05, global_step: 930, interval_runtime: 4.3133, interval_samples_per_second: 1.855, interval_steps_per_second: 2.318, epoch: 3.6471[0m
[32m[2022-08-26 14:39:41,862] [    INFO][0m - loss: 1.29576273, learning_rate: 2.4470588235294118e-05, global_step: 940, interval_runtime: 4.2973, interval_samples_per_second: 1.862, interval_steps_per_second: 2.327, epoch: 3.6863[0m
[32m[2022-08-26 14:39:46,228] [    INFO][0m - loss: 1.5709549, learning_rate: 2.4411764705882354e-05, global_step: 950, interval_runtime: 4.3662, interval_samples_per_second: 1.832, interval_steps_per_second: 2.29, epoch: 3.7255[0m
[32m[2022-08-26 14:39:50,608] [    INFO][0m - loss: 1.5452117, learning_rate: 2.4352941176470587e-05, global_step: 960, interval_runtime: 4.3792, interval_samples_per_second: 1.827, interval_steps_per_second: 2.284, epoch: 3.7647[0m
[32m[2022-08-26 14:39:55,110] [    INFO][0m - loss: 1.47404375, learning_rate: 2.4294117647058824e-05, global_step: 970, interval_runtime: 4.5016, interval_samples_per_second: 1.777, interval_steps_per_second: 2.221, epoch: 3.8039[0m
[32m[2022-08-26 14:39:59,568] [    INFO][0m - loss: 1.37866402, learning_rate: 2.423529411764706e-05, global_step: 980, interval_runtime: 4.4581, interval_samples_per_second: 1.794, interval_steps_per_second: 2.243, epoch: 3.8431[0m
[32m[2022-08-26 14:40:04,063] [    INFO][0m - loss: 1.26121788, learning_rate: 2.4176470588235294e-05, global_step: 990, interval_runtime: 4.4954, interval_samples_per_second: 1.78, interval_steps_per_second: 2.224, epoch: 3.8824[0m
[32m[2022-08-26 14:40:08,569] [    INFO][0m - loss: 1.39907503, learning_rate: 2.411764705882353e-05, global_step: 1000, interval_runtime: 4.5063, interval_samples_per_second: 1.775, interval_steps_per_second: 2.219, epoch: 3.9216[0m
[32m[2022-08-26 14:40:08,570] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 14:40:08,570] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-26 14:40:08,570] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 14:40:08,570] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 14:40:08,570] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-26 14:41:27,543] [    INFO][0m - eval_loss: 1.720184087753296, eval_accuracy: 0.5739845261121856, eval_runtime: 78.9727, eval_samples_per_second: 26.186, eval_steps_per_second: 0.823, epoch: 3.9216[0m
[32m[2022-08-26 14:41:27,544] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1000[0m
[32m[2022-08-26 14:41:27,544] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 14:41:30,802] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1000/tokenizer_config.json[0m
[32m[2022-08-26 14:41:30,803] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1000/special_tokens_map.json[0m
[32m[2022-08-26 14:41:39,367] [    INFO][0m - loss: 1.44804382, learning_rate: 2.4058823529411767e-05, global_step: 1010, interval_runtime: 90.7979, interval_samples_per_second: 0.088, interval_steps_per_second: 0.11, epoch: 3.9608[0m
[32m[2022-08-26 14:41:42,122] [    INFO][0m - loss: 1.19415894, learning_rate: 2.4e-05, global_step: 1020, interval_runtime: 2.7555, interval_samples_per_second: 2.903, interval_steps_per_second: 3.629, epoch: 4.0[0m
[32m[2022-08-26 14:41:48,615] [    INFO][0m - loss: 1.08581209, learning_rate: 2.3941176470588237e-05, global_step: 1030, interval_runtime: 6.4924, interval_samples_per_second: 1.232, interval_steps_per_second: 1.54, epoch: 4.0392[0m
[32m[2022-08-26 14:41:53,320] [    INFO][0m - loss: 0.94838476, learning_rate: 2.388235294117647e-05, global_step: 1040, interval_runtime: 4.7044, interval_samples_per_second: 1.701, interval_steps_per_second: 2.126, epoch: 4.0784[0m
[32m[2022-08-26 14:41:58,109] [    INFO][0m - loss: 1.02291765, learning_rate: 2.3823529411764704e-05, global_step: 1050, interval_runtime: 4.7895, interval_samples_per_second: 1.67, interval_steps_per_second: 2.088, epoch: 4.1176[0m
[32m[2022-08-26 14:42:02,813] [    INFO][0m - loss: 1.1890048, learning_rate: 2.376470588235294e-05, global_step: 1060, interval_runtime: 4.7035, interval_samples_per_second: 1.701, interval_steps_per_second: 2.126, epoch: 4.1569[0m
[32m[2022-08-26 14:42:07,561] [    INFO][0m - loss: 0.96905479, learning_rate: 2.3705882352941177e-05, global_step: 1070, interval_runtime: 4.7482, interval_samples_per_second: 1.685, interval_steps_per_second: 2.106, epoch: 4.1961[0m
[32m[2022-08-26 14:42:12,313] [    INFO][0m - loss: 1.04789362, learning_rate: 2.364705882352941e-05, global_step: 1080, interval_runtime: 4.7518, interval_samples_per_second: 1.684, interval_steps_per_second: 2.104, epoch: 4.2353[0m
[32m[2022-08-26 14:42:17,133] [    INFO][0m - loss: 1.25430136, learning_rate: 2.3588235294117647e-05, global_step: 1090, interval_runtime: 4.8201, interval_samples_per_second: 1.66, interval_steps_per_second: 2.075, epoch: 4.2745[0m
[32m[2022-08-26 14:42:21,916] [    INFO][0m - loss: 1.12841825, learning_rate: 2.3529411764705884e-05, global_step: 1100, interval_runtime: 4.7836, interval_samples_per_second: 1.672, interval_steps_per_second: 2.09, epoch: 4.3137[0m
[32m[2022-08-26 14:42:21,917] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 14:42:21,917] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-26 14:42:21,917] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 14:42:21,918] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 14:42:21,918] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-26 14:43:47,696] [    INFO][0m - eval_loss: 1.6921948194503784, eval_accuracy: 0.5855899419729207, eval_runtime: 85.7775, eval_samples_per_second: 24.109, eval_steps_per_second: 0.758, epoch: 4.3137[0m
[32m[2022-08-26 14:43:47,696] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1100[0m
[32m[2022-08-26 14:43:47,697] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 14:43:50,478] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1100/tokenizer_config.json[0m
[32m[2022-08-26 14:43:50,478] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1100/special_tokens_map.json[0m
[32m[2022-08-26 14:43:59,066] [    INFO][0m - loss: 1.2533885, learning_rate: 2.3470588235294117e-05, global_step: 1110, interval_runtime: 97.1491, interval_samples_per_second: 0.082, interval_steps_per_second: 0.103, epoch: 4.3529[0m
[32m[2022-08-26 14:44:04,034] [    INFO][0m - loss: 1.10253811, learning_rate: 2.3411764705882354e-05, global_step: 1120, interval_runtime: 4.9683, interval_samples_per_second: 1.61, interval_steps_per_second: 2.013, epoch: 4.3922[0m
[32m[2022-08-26 14:44:09,059] [    INFO][0m - loss: 1.19913054, learning_rate: 2.335294117647059e-05, global_step: 1130, interval_runtime: 5.0248, interval_samples_per_second: 1.592, interval_steps_per_second: 1.99, epoch: 4.4314[0m
[32m[2022-08-26 14:44:14,132] [    INFO][0m - loss: 1.06911631, learning_rate: 2.3294117647058824e-05, global_step: 1140, interval_runtime: 5.0735, interval_samples_per_second: 1.577, interval_steps_per_second: 1.971, epoch: 4.4706[0m
[32m[2022-08-26 14:44:19,175] [    INFO][0m - loss: 1.21606531, learning_rate: 2.323529411764706e-05, global_step: 1150, interval_runtime: 5.0433, interval_samples_per_second: 1.586, interval_steps_per_second: 1.983, epoch: 4.5098[0m
[32m[2022-08-26 14:44:24,263] [    INFO][0m - loss: 1.05143051, learning_rate: 2.3176470588235297e-05, global_step: 1160, interval_runtime: 5.0875, interval_samples_per_second: 1.572, interval_steps_per_second: 1.966, epoch: 4.549[0m
[32m[2022-08-26 14:44:29,359] [    INFO][0m - loss: 1.08151398, learning_rate: 2.311764705882353e-05, global_step: 1170, interval_runtime: 5.0958, interval_samples_per_second: 1.57, interval_steps_per_second: 1.962, epoch: 4.5882[0m
[32m[2022-08-26 14:44:34,531] [    INFO][0m - loss: 1.06773005, learning_rate: 2.3058823529411767e-05, global_step: 1180, interval_runtime: 5.1722, interval_samples_per_second: 1.547, interval_steps_per_second: 1.933, epoch: 4.6275[0m
[32m[2022-08-26 14:44:39,635] [    INFO][0m - loss: 1.14786129, learning_rate: 2.3000000000000003e-05, global_step: 1190, interval_runtime: 5.1048, interval_samples_per_second: 1.567, interval_steps_per_second: 1.959, epoch: 4.6667[0m
[32m[2022-08-26 14:44:44,747] [    INFO][0m - loss: 0.97370548, learning_rate: 2.2941176470588233e-05, global_step: 1200, interval_runtime: 5.112, interval_samples_per_second: 1.565, interval_steps_per_second: 1.956, epoch: 4.7059[0m
[32m[2022-08-26 14:44:44,748] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 14:44:44,748] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-26 14:44:44,748] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 14:44:44,748] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 14:44:44,748] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-26 14:46:17,369] [    INFO][0m - eval_loss: 1.6456273794174194, eval_accuracy: 0.5812379110251451, eval_runtime: 92.6204, eval_samples_per_second: 22.328, eval_steps_per_second: 0.702, epoch: 4.7059[0m
[32m[2022-08-26 14:46:17,370] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1200[0m
[32m[2022-08-26 14:46:17,370] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 14:46:20,155] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1200/tokenizer_config.json[0m
[32m[2022-08-26 14:46:20,156] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1200/special_tokens_map.json[0m
[32m[2022-08-26 14:46:29,203] [    INFO][0m - loss: 0.99463291, learning_rate: 2.288235294117647e-05, global_step: 1210, interval_runtime: 104.4554, interval_samples_per_second: 0.077, interval_steps_per_second: 0.096, epoch: 4.7451[0m
[32m[2022-08-26 14:46:34,532] [    INFO][0m - loss: 0.97055626, learning_rate: 2.2823529411764707e-05, global_step: 1220, interval_runtime: 5.3286, interval_samples_per_second: 1.501, interval_steps_per_second: 1.877, epoch: 4.7843[0m
[32m[2022-08-26 14:46:39,817] [    INFO][0m - loss: 0.8400177, learning_rate: 2.276470588235294e-05, global_step: 1230, interval_runtime: 5.2859, interval_samples_per_second: 1.513, interval_steps_per_second: 1.892, epoch: 4.8235[0m
[32m[2022-08-26 14:46:45,194] [    INFO][0m - loss: 1.17993679, learning_rate: 2.2705882352941177e-05, global_step: 1240, interval_runtime: 5.3768, interval_samples_per_second: 1.488, interval_steps_per_second: 1.86, epoch: 4.8627[0m
[32m[2022-08-26 14:46:50,532] [    INFO][0m - loss: 0.94082069, learning_rate: 2.2647058823529413e-05, global_step: 1250, interval_runtime: 5.3382, interval_samples_per_second: 1.499, interval_steps_per_second: 1.873, epoch: 4.902[0m
[32m[2022-08-26 14:46:55,976] [    INFO][0m - loss: 0.96025286, learning_rate: 2.2588235294117646e-05, global_step: 1260, interval_runtime: 5.4435, interval_samples_per_second: 1.47, interval_steps_per_second: 1.837, epoch: 4.9412[0m
[32m[2022-08-26 14:47:01,335] [    INFO][0m - loss: 1.07809362, learning_rate: 2.2529411764705883e-05, global_step: 1270, interval_runtime: 5.3592, interval_samples_per_second: 1.493, interval_steps_per_second: 1.866, epoch: 4.9804[0m
[32m[2022-08-26 14:47:06,847] [    INFO][0m - loss: 1.08127718, learning_rate: 2.247058823529412e-05, global_step: 1280, interval_runtime: 5.5111, interval_samples_per_second: 1.452, interval_steps_per_second: 1.815, epoch: 5.0196[0m
[32m[2022-08-26 14:47:12,350] [    INFO][0m - loss: 0.91191921, learning_rate: 2.2411764705882353e-05, global_step: 1290, interval_runtime: 5.5039, interval_samples_per_second: 1.454, interval_steps_per_second: 1.817, epoch: 5.0588[0m
[32m[2022-08-26 14:47:17,885] [    INFO][0m - loss: 0.74967322, learning_rate: 2.235294117647059e-05, global_step: 1300, interval_runtime: 5.5348, interval_samples_per_second: 1.445, interval_steps_per_second: 1.807, epoch: 5.098[0m
[32m[2022-08-26 14:47:17,886] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 14:47:17,886] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-26 14:47:17,886] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 14:47:17,886] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 14:47:17,886] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-26 14:48:57,108] [    INFO][0m - eval_loss: 1.6287165880203247, eval_accuracy: 0.586073500967118, eval_runtime: 99.2209, eval_samples_per_second: 20.842, eval_steps_per_second: 0.655, epoch: 5.098[0m
[32m[2022-08-26 14:48:57,108] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1300[0m
[32m[2022-08-26 14:48:57,108] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 14:49:00,234] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1300/tokenizer_config.json[0m
[32m[2022-08-26 14:49:00,234] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1300/special_tokens_map.json[0m
[32m[2022-08-26 14:49:09,725] [    INFO][0m - loss: 0.85506907, learning_rate: 2.2294117647058826e-05, global_step: 1310, interval_runtime: 111.8391, interval_samples_per_second: 0.072, interval_steps_per_second: 0.089, epoch: 5.1373[0m
[32m[2022-08-26 14:49:15,362] [    INFO][0m - loss: 0.79011598, learning_rate: 2.223529411764706e-05, global_step: 1320, interval_runtime: 5.6379, interval_samples_per_second: 1.419, interval_steps_per_second: 1.774, epoch: 5.1765[0m
[32m[2022-08-26 14:49:21,030] [    INFO][0m - loss: 0.76358385, learning_rate: 2.2176470588235296e-05, global_step: 1330, interval_runtime: 5.6674, interval_samples_per_second: 1.412, interval_steps_per_second: 1.764, epoch: 5.2157[0m
[32m[2022-08-26 14:49:26,752] [    INFO][0m - loss: 0.83156462, learning_rate: 2.2117647058823533e-05, global_step: 1340, interval_runtime: 5.7224, interval_samples_per_second: 1.398, interval_steps_per_second: 1.748, epoch: 5.2549[0m
[32m[2022-08-26 14:49:32,437] [    INFO][0m - loss: 0.85255709, learning_rate: 2.2058823529411766e-05, global_step: 1350, interval_runtime: 5.6849, interval_samples_per_second: 1.407, interval_steps_per_second: 1.759, epoch: 5.2941[0m
[32m[2022-08-26 14:49:38,179] [    INFO][0m - loss: 0.84126673, learning_rate: 2.2e-05, global_step: 1360, interval_runtime: 5.7423, interval_samples_per_second: 1.393, interval_steps_per_second: 1.741, epoch: 5.3333[0m
[32m[2022-08-26 14:49:44,255] [    INFO][0m - loss: 0.89404812, learning_rate: 2.1941176470588236e-05, global_step: 1370, interval_runtime: 6.0761, interval_samples_per_second: 1.317, interval_steps_per_second: 1.646, epoch: 5.3725[0m
[32m[2022-08-26 14:49:50,174] [    INFO][0m - loss: 0.76631641, learning_rate: 2.188235294117647e-05, global_step: 1380, interval_runtime: 5.9193, interval_samples_per_second: 1.352, interval_steps_per_second: 1.689, epoch: 5.4118[0m
[32m[2022-08-26 14:49:55,996] [    INFO][0m - loss: 1.04210835, learning_rate: 2.1823529411764706e-05, global_step: 1390, interval_runtime: 5.821, interval_samples_per_second: 1.374, interval_steps_per_second: 1.718, epoch: 5.451[0m
[32m[2022-08-26 14:50:01,818] [    INFO][0m - loss: 0.83464279, learning_rate: 2.1764705882352943e-05, global_step: 1400, interval_runtime: 5.8223, interval_samples_per_second: 1.374, interval_steps_per_second: 1.718, epoch: 5.4902[0m
[32m[2022-08-26 14:50:01,819] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 14:50:01,819] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-26 14:50:01,819] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 14:50:01,819] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 14:50:01,819] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-26 14:51:48,515] [    INFO][0m - eval_loss: 1.6141022443771362, eval_accuracy: 0.5851063829787234, eval_runtime: 106.6954, eval_samples_per_second: 19.382, eval_steps_per_second: 0.609, epoch: 5.4902[0m
[32m[2022-08-26 14:51:48,516] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1400[0m
[32m[2022-08-26 14:51:48,516] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 14:51:50,704] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1400/tokenizer_config.json[0m
[32m[2022-08-26 14:51:50,704] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1400/special_tokens_map.json[0m
[32m[2022-08-26 14:52:00,348] [    INFO][0m - loss: 0.98636093, learning_rate: 2.1705882352941176e-05, global_step: 1410, interval_runtime: 118.5301, interval_samples_per_second: 0.067, interval_steps_per_second: 0.084, epoch: 5.5294[0m
[32m[2022-08-26 14:52:06,357] [    INFO][0m - loss: 0.72714205, learning_rate: 2.1647058823529413e-05, global_step: 1420, interval_runtime: 6.0086, interval_samples_per_second: 1.331, interval_steps_per_second: 1.664, epoch: 5.5686[0m
[32m[2022-08-26 14:52:12,459] [    INFO][0m - loss: 0.68175907, learning_rate: 2.1588235294117646e-05, global_step: 1430, interval_runtime: 6.102, interval_samples_per_second: 1.311, interval_steps_per_second: 1.639, epoch: 5.6078[0m
[32m[2022-08-26 14:52:18,617] [    INFO][0m - loss: 0.80510216, learning_rate: 2.1529411764705882e-05, global_step: 1440, interval_runtime: 6.1581, interval_samples_per_second: 1.299, interval_steps_per_second: 1.624, epoch: 5.6471[0m
[32m[2022-08-26 14:52:24,777] [    INFO][0m - loss: 0.71769514, learning_rate: 2.147058823529412e-05, global_step: 1450, interval_runtime: 6.1596, interval_samples_per_second: 1.299, interval_steps_per_second: 1.623, epoch: 5.6863[0m
[32m[2022-08-26 14:52:31,016] [    INFO][0m - loss: 0.70678105, learning_rate: 2.1411764705882352e-05, global_step: 1460, interval_runtime: 6.2398, interval_samples_per_second: 1.282, interval_steps_per_second: 1.603, epoch: 5.7255[0m
[32m[2022-08-26 14:52:37,111] [    INFO][0m - loss: 1.02152996, learning_rate: 2.135294117647059e-05, global_step: 1470, interval_runtime: 6.0949, interval_samples_per_second: 1.313, interval_steps_per_second: 1.641, epoch: 5.7647[0m
[32m[2022-08-26 14:52:43,252] [    INFO][0m - loss: 0.72021651, learning_rate: 2.1294117647058826e-05, global_step: 1480, interval_runtime: 6.1407, interval_samples_per_second: 1.303, interval_steps_per_second: 1.628, epoch: 5.8039[0m
[32m[2022-08-26 14:52:49,415] [    INFO][0m - loss: 0.70551991, learning_rate: 2.123529411764706e-05, global_step: 1490, interval_runtime: 6.1632, interval_samples_per_second: 1.298, interval_steps_per_second: 1.623, epoch: 5.8431[0m
[32m[2022-08-26 14:52:55,603] [    INFO][0m - loss: 0.66608858, learning_rate: 2.1176470588235296e-05, global_step: 1500, interval_runtime: 6.1872, interval_samples_per_second: 1.293, interval_steps_per_second: 1.616, epoch: 5.8824[0m
[32m[2022-08-26 14:52:55,603] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 14:52:55,603] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-26 14:52:55,603] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 14:52:55,604] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 14:52:55,604] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-26 14:54:48,915] [    INFO][0m - eval_loss: 1.5832490921020508, eval_accuracy: 0.5981624758220503, eval_runtime: 113.3109, eval_samples_per_second: 18.251, eval_steps_per_second: 0.574, epoch: 5.8824[0m
[32m[2022-08-26 14:54:48,916] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1500[0m
[32m[2022-08-26 14:54:48,916] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 14:54:51,435] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1500/tokenizer_config.json[0m
[32m[2022-08-26 14:54:51,436] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1500/special_tokens_map.json[0m
[32m[2022-08-26 14:55:01,300] [    INFO][0m - loss: 0.81537333, learning_rate: 2.1117647058823532e-05, global_step: 1510, interval_runtime: 125.6982, interval_samples_per_second: 0.064, interval_steps_per_second: 0.08, epoch: 5.9216[0m
[32m[2022-08-26 14:55:07,652] [    INFO][0m - loss: 0.40759177, learning_rate: 2.1058823529411762e-05, global_step: 1520, interval_runtime: 6.3514, interval_samples_per_second: 1.26, interval_steps_per_second: 1.574, epoch: 5.9608[0m
[32m[2022-08-26 14:55:11,192] [    INFO][0m - loss: 0.74581795, learning_rate: 2.1e-05, global_step: 1530, interval_runtime: 3.5407, interval_samples_per_second: 2.259, interval_steps_per_second: 2.824, epoch: 6.0[0m
[32m[2022-08-26 14:55:20,375] [    INFO][0m - loss: 0.38699336, learning_rate: 2.0941176470588235e-05, global_step: 1540, interval_runtime: 9.1828, interval_samples_per_second: 0.871, interval_steps_per_second: 1.089, epoch: 6.0392[0m
[32m[2022-08-26 14:55:26,839] [    INFO][0m - loss: 0.41436481, learning_rate: 2.088235294117647e-05, global_step: 1550, interval_runtime: 6.4638, interval_samples_per_second: 1.238, interval_steps_per_second: 1.547, epoch: 6.0784[0m
[32m[2022-08-26 14:55:33,446] [    INFO][0m - loss: 0.65246563, learning_rate: 2.0823529411764705e-05, global_step: 1560, interval_runtime: 6.6069, interval_samples_per_second: 1.211, interval_steps_per_second: 1.514, epoch: 6.1176[0m
[32m[2022-08-26 14:55:39,934] [    INFO][0m - loss: 0.56484241, learning_rate: 2.0764705882352942e-05, global_step: 1570, interval_runtime: 6.4882, interval_samples_per_second: 1.233, interval_steps_per_second: 1.541, epoch: 6.1569[0m
[32m[2022-08-26 14:55:46,397] [    INFO][0m - loss: 0.52045746, learning_rate: 2.0705882352941175e-05, global_step: 1580, interval_runtime: 6.4625, interval_samples_per_second: 1.238, interval_steps_per_second: 1.547, epoch: 6.1961[0m
[32m[2022-08-26 14:55:52,945] [    INFO][0m - loss: 0.81529264, learning_rate: 2.0647058823529412e-05, global_step: 1590, interval_runtime: 6.5484, interval_samples_per_second: 1.222, interval_steps_per_second: 1.527, epoch: 6.2353[0m
[32m[2022-08-26 14:55:59,496] [    INFO][0m - loss: 0.6307425, learning_rate: 2.058823529411765e-05, global_step: 1600, interval_runtime: 6.5503, interval_samples_per_second: 1.221, interval_steps_per_second: 1.527, epoch: 6.2745[0m
[32m[2022-08-26 14:55:59,496] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 14:55:59,496] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-26 14:55:59,496] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 14:55:59,497] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 14:55:59,497] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-26 14:57:59,364] [    INFO][0m - eval_loss: 1.5982937812805176, eval_accuracy: 0.589458413926499, eval_runtime: 119.8665, eval_samples_per_second: 17.253, eval_steps_per_second: 0.542, epoch: 6.2745[0m
[32m[2022-08-26 14:57:59,364] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1600[0m
[32m[2022-08-26 14:57:59,364] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 14:58:02,670] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1600/tokenizer_config.json[0m
[32m[2022-08-26 14:58:02,670] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1600/special_tokens_map.json[0m
[32m[2022-08-26 14:58:12,767] [    INFO][0m - loss: 0.58968325, learning_rate: 2.0529411764705882e-05, global_step: 1610, interval_runtime: 133.2715, interval_samples_per_second: 0.06, interval_steps_per_second: 0.075, epoch: 6.3137[0m
[32m[2022-08-26 14:58:19,493] [    INFO][0m - loss: 0.70507116, learning_rate: 2.047058823529412e-05, global_step: 1620, interval_runtime: 6.7264, interval_samples_per_second: 1.189, interval_steps_per_second: 1.487, epoch: 6.3529[0m
[32m[2022-08-26 14:58:26,273] [    INFO][0m - loss: 0.57399306, learning_rate: 2.0411764705882355e-05, global_step: 1630, interval_runtime: 6.7793, interval_samples_per_second: 1.18, interval_steps_per_second: 1.475, epoch: 6.3922[0m
[32m[2022-08-26 14:58:32,934] [    INFO][0m - loss: 0.47105184, learning_rate: 2.035294117647059e-05, global_step: 1640, interval_runtime: 6.6612, interval_samples_per_second: 1.201, interval_steps_per_second: 1.501, epoch: 6.4314[0m
[32m[2022-08-26 14:58:39,652] [    INFO][0m - loss: 0.42024102, learning_rate: 2.0294117647058825e-05, global_step: 1650, interval_runtime: 6.7182, interval_samples_per_second: 1.191, interval_steps_per_second: 1.489, epoch: 6.4706[0m
[32m[2022-08-26 14:58:46,358] [    INFO][0m - loss: 0.75178185, learning_rate: 2.0235294117647062e-05, global_step: 1660, interval_runtime: 6.7059, interval_samples_per_second: 1.193, interval_steps_per_second: 1.491, epoch: 6.5098[0m
[32m[2022-08-26 14:58:53,196] [    INFO][0m - loss: 0.58730316, learning_rate: 2.0176470588235295e-05, global_step: 1670, interval_runtime: 6.8376, interval_samples_per_second: 1.17, interval_steps_per_second: 1.463, epoch: 6.549[0m
[32m[2022-08-26 14:58:59,942] [    INFO][0m - loss: 0.63937082, learning_rate: 2.011764705882353e-05, global_step: 1680, interval_runtime: 6.7468, interval_samples_per_second: 1.186, interval_steps_per_second: 1.482, epoch: 6.5882[0m
[32m[2022-08-26 14:59:06,748] [    INFO][0m - loss: 0.60205836, learning_rate: 2.0058823529411765e-05, global_step: 1690, interval_runtime: 6.805, interval_samples_per_second: 1.176, interval_steps_per_second: 1.47, epoch: 6.6275[0m
[32m[2022-08-26 14:59:13,561] [    INFO][0m - loss: 0.54112782, learning_rate: 1.9999999999999998e-05, global_step: 1700, interval_runtime: 6.8139, interval_samples_per_second: 1.174, interval_steps_per_second: 1.468, epoch: 6.6667[0m
[32m[2022-08-26 14:59:13,562] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 14:59:13,562] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-26 14:59:13,562] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 14:59:13,562] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 14:59:13,563] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-26 15:01:19,411] [    INFO][0m - eval_loss: 1.6136294603347778, eval_accuracy: 0.5996131528046421, eval_runtime: 125.8476, eval_samples_per_second: 16.433, eval_steps_per_second: 0.516, epoch: 6.6667[0m
[32m[2022-08-26 15:01:19,411] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1700[0m
[32m[2022-08-26 15:01:19,411] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 15:01:22,167] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1700/tokenizer_config.json[0m
[32m[2022-08-26 15:01:22,167] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1700/special_tokens_map.json[0m
[32m[2022-08-26 15:01:32,426] [    INFO][0m - loss: 0.7308979, learning_rate: 1.9941176470588235e-05, global_step: 1710, interval_runtime: 138.8651, interval_samples_per_second: 0.058, interval_steps_per_second: 0.072, epoch: 6.7059[0m
[32m[2022-08-26 15:01:39,390] [    INFO][0m - loss: 0.56030393, learning_rate: 1.988235294117647e-05, global_step: 1720, interval_runtime: 6.9636, interval_samples_per_second: 1.149, interval_steps_per_second: 1.436, epoch: 6.7451[0m
[32m[2022-08-26 15:01:46,436] [    INFO][0m - loss: 0.70446873, learning_rate: 1.9823529411764705e-05, global_step: 1730, interval_runtime: 7.0456, interval_samples_per_second: 1.135, interval_steps_per_second: 1.419, epoch: 6.7843[0m
[32m[2022-08-26 15:01:53,427] [    INFO][0m - loss: 0.45888219, learning_rate: 1.976470588235294e-05, global_step: 1740, interval_runtime: 6.9918, interval_samples_per_second: 1.144, interval_steps_per_second: 1.43, epoch: 6.8235[0m
[32m[2022-08-26 15:02:00,440] [    INFO][0m - loss: 0.49929099, learning_rate: 1.9705882352941178e-05, global_step: 1750, interval_runtime: 7.0131, interval_samples_per_second: 1.141, interval_steps_per_second: 1.426, epoch: 6.8627[0m
[32m[2022-08-26 15:02:07,482] [    INFO][0m - loss: 0.79769549, learning_rate: 1.964705882352941e-05, global_step: 1760, interval_runtime: 7.0419, interval_samples_per_second: 1.136, interval_steps_per_second: 1.42, epoch: 6.902[0m
[32m[2022-08-26 15:02:14,634] [    INFO][0m - loss: 0.54895949, learning_rate: 1.9588235294117648e-05, global_step: 1770, interval_runtime: 7.1517, interval_samples_per_second: 1.119, interval_steps_per_second: 1.398, epoch: 6.9412[0m
[32m[2022-08-26 15:02:21,503] [    INFO][0m - loss: 0.40686545, learning_rate: 1.9529411764705885e-05, global_step: 1780, interval_runtime: 6.8692, interval_samples_per_second: 1.165, interval_steps_per_second: 1.456, epoch: 6.9804[0m
[32m[2022-08-26 15:02:28,890] [    INFO][0m - loss: 0.60373688, learning_rate: 1.9470588235294118e-05, global_step: 1790, interval_runtime: 7.3865, interval_samples_per_second: 1.083, interval_steps_per_second: 1.354, epoch: 7.0196[0m
[32m[2022-08-26 15:02:36,097] [    INFO][0m - loss: 0.50153537, learning_rate: 1.9411764705882355e-05, global_step: 1800, interval_runtime: 7.2073, interval_samples_per_second: 1.11, interval_steps_per_second: 1.387, epoch: 7.0588[0m
[32m[2022-08-26 15:02:36,098] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 15:02:36,098] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-26 15:02:36,098] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 15:02:36,098] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 15:02:36,098] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-26 15:04:47,745] [    INFO][0m - eval_loss: 1.6405324935913086, eval_accuracy: 0.5933268858800773, eval_runtime: 131.6461, eval_samples_per_second: 15.709, eval_steps_per_second: 0.494, epoch: 7.0588[0m
[32m[2022-08-26 15:04:47,745] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1800[0m
[32m[2022-08-26 15:04:47,745] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 15:04:52,880] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1800/tokenizer_config.json[0m
[32m[2022-08-26 15:04:52,881] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1800/special_tokens_map.json[0m
[32m[2022-08-26 15:05:04,501] [    INFO][0m - loss: 0.3708528, learning_rate: 1.935294117647059e-05, global_step: 1810, interval_runtime: 148.4039, interval_samples_per_second: 0.054, interval_steps_per_second: 0.067, epoch: 7.098[0m
[32m[2022-08-26 15:05:11,811] [    INFO][0m - loss: 0.37137918, learning_rate: 1.9294117647058825e-05, global_step: 1820, interval_runtime: 7.3095, interval_samples_per_second: 1.094, interval_steps_per_second: 1.368, epoch: 7.1373[0m
[32m[2022-08-26 15:05:19,171] [    INFO][0m - loss: 0.30344744, learning_rate: 1.923529411764706e-05, global_step: 1830, interval_runtime: 7.3602, interval_samples_per_second: 1.087, interval_steps_per_second: 1.359, epoch: 7.1765[0m
[32m[2022-08-26 15:05:26,622] [    INFO][0m - loss: 0.31849179, learning_rate: 1.9176470588235294e-05, global_step: 1840, interval_runtime: 7.4509, interval_samples_per_second: 1.074, interval_steps_per_second: 1.342, epoch: 7.2157[0m
[32m[2022-08-26 15:05:34,044] [    INFO][0m - loss: 0.44322567, learning_rate: 1.9117647058823528e-05, global_step: 1850, interval_runtime: 7.4221, interval_samples_per_second: 1.078, interval_steps_per_second: 1.347, epoch: 7.2549[0m
[32m[2022-08-26 15:05:41,530] [    INFO][0m - loss: 0.54116492, learning_rate: 1.9058823529411764e-05, global_step: 1860, interval_runtime: 7.4856, interval_samples_per_second: 1.069, interval_steps_per_second: 1.336, epoch: 7.2941[0m
[32m[2022-08-26 15:05:49,124] [    INFO][0m - loss: 0.50834446, learning_rate: 1.9e-05, global_step: 1870, interval_runtime: 7.5947, interval_samples_per_second: 1.053, interval_steps_per_second: 1.317, epoch: 7.3333[0m
[32m[2022-08-26 15:05:56,610] [    INFO][0m - loss: 0.28913336, learning_rate: 1.8941176470588234e-05, global_step: 1880, interval_runtime: 7.4856, interval_samples_per_second: 1.069, interval_steps_per_second: 1.336, epoch: 7.3725[0m
[32m[2022-08-26 15:06:05,257] [    INFO][0m - loss: 0.41024151, learning_rate: 1.888235294117647e-05, global_step: 1890, interval_runtime: 7.481, interval_samples_per_second: 1.069, interval_steps_per_second: 1.337, epoch: 7.4118[0m
[32m[2022-08-26 15:06:12,732] [    INFO][0m - loss: 0.43926616, learning_rate: 1.8823529411764708e-05, global_step: 1900, interval_runtime: 8.6414, interval_samples_per_second: 0.926, interval_steps_per_second: 1.157, epoch: 7.451[0m
[32m[2022-08-26 15:06:12,733] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 15:06:12,733] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-26 15:06:12,733] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 15:06:12,733] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 15:06:12,733] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-26 15:08:33,199] [    INFO][0m - eval_loss: 1.6419347524642944, eval_accuracy: 0.6005802707930368, eval_runtime: 140.4656, eval_samples_per_second: 14.722, eval_steps_per_second: 0.463, epoch: 7.451[0m
[32m[2022-08-26 15:08:33,200] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1900[0m
[32m[2022-08-26 15:08:33,200] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 15:08:36,034] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1900/tokenizer_config.json[0m
[32m[2022-08-26 15:08:36,034] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1900/special_tokens_map.json[0m
[32m[2022-08-26 15:08:47,215] [    INFO][0m - loss: 0.45482445, learning_rate: 1.876470588235294e-05, global_step: 1910, interval_runtime: 154.4828, interval_samples_per_second: 0.052, interval_steps_per_second: 0.065, epoch: 7.4902[0m
[32m[2022-08-26 15:08:54,904] [    INFO][0m - loss: 0.41062126, learning_rate: 1.8705882352941178e-05, global_step: 1920, interval_runtime: 7.689, interval_samples_per_second: 1.04, interval_steps_per_second: 1.301, epoch: 7.5294[0m
[32m[2022-08-26 15:09:02,736] [    INFO][0m - loss: 0.33258994, learning_rate: 1.8647058823529414e-05, global_step: 1930, interval_runtime: 7.8314, interval_samples_per_second: 1.022, interval_steps_per_second: 1.277, epoch: 7.5686[0m
[32m[2022-08-26 15:09:10,640] [    INFO][0m - loss: 0.38254347, learning_rate: 1.8588235294117647e-05, global_step: 1940, interval_runtime: 7.9044, interval_samples_per_second: 1.012, interval_steps_per_second: 1.265, epoch: 7.6078[0m
[32m[2022-08-26 15:09:18,530] [    INFO][0m - loss: 0.33288875, learning_rate: 1.8529411764705884e-05, global_step: 1950, interval_runtime: 7.8906, interval_samples_per_second: 1.014, interval_steps_per_second: 1.267, epoch: 7.6471[0m
[32m[2022-08-26 15:09:26,419] [    INFO][0m - loss: 0.40366402, learning_rate: 1.847058823529412e-05, global_step: 1960, interval_runtime: 7.8886, interval_samples_per_second: 1.014, interval_steps_per_second: 1.268, epoch: 7.6863[0m
[32m[2022-08-26 15:09:34,306] [    INFO][0m - loss: 0.26319187, learning_rate: 1.8411764705882354e-05, global_step: 1970, interval_runtime: 7.8873, interval_samples_per_second: 1.014, interval_steps_per_second: 1.268, epoch: 7.7255[0m
[32m[2022-08-26 15:09:42,241] [    INFO][0m - loss: 0.34146721, learning_rate: 1.835294117647059e-05, global_step: 1980, interval_runtime: 7.9346, interval_samples_per_second: 1.008, interval_steps_per_second: 1.26, epoch: 7.7647[0m
[32m[2022-08-26 15:09:50,206] [    INFO][0m - loss: 0.40418601, learning_rate: 1.8294117647058824e-05, global_step: 1990, interval_runtime: 7.9654, interval_samples_per_second: 1.004, interval_steps_per_second: 1.255, epoch: 7.8039[0m
[32m[2022-08-26 15:09:58,137] [    INFO][0m - loss: 0.36847103, learning_rate: 1.8235294117647057e-05, global_step: 2000, interval_runtime: 7.9303, interval_samples_per_second: 1.009, interval_steps_per_second: 1.261, epoch: 7.8431[0m
[32m[2022-08-26 15:09:58,137] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 15:09:58,138] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-26 15:09:58,138] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 15:09:58,138] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 15:09:58,138] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-26 15:12:25,139] [    INFO][0m - eval_loss: 1.6220664978027344, eval_accuracy: 0.6165377176015474, eval_runtime: 147.0004, eval_samples_per_second: 14.068, eval_steps_per_second: 0.442, epoch: 7.8431[0m
[32m[2022-08-26 15:12:25,139] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2000[0m
[32m[2022-08-26 15:12:25,140] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 15:12:28,314] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2000/tokenizer_config.json[0m
[32m[2022-08-26 15:12:28,315] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2000/special_tokens_map.json[0m
[32m[2022-08-26 15:12:40,009] [    INFO][0m - loss: 0.34377913, learning_rate: 1.8176470588235294e-05, global_step: 2010, interval_runtime: 161.872, interval_samples_per_second: 0.049, interval_steps_per_second: 0.062, epoch: 7.8824[0m
[32m[2022-08-26 15:12:48,124] [    INFO][0m - loss: 0.47573204, learning_rate: 1.8117647058823527e-05, global_step: 2020, interval_runtime: 8.1147, interval_samples_per_second: 0.986, interval_steps_per_second: 1.232, epoch: 7.9216[0m
[32m[2022-08-26 15:12:56,323] [    INFO][0m - loss: 0.44384999, learning_rate: 1.8058823529411764e-05, global_step: 2030, interval_runtime: 8.1994, interval_samples_per_second: 0.976, interval_steps_per_second: 1.22, epoch: 7.9608[0m
[32m[2022-08-26 15:13:00,678] [    INFO][0m - loss: 0.40477033, learning_rate: 1.8e-05, global_step: 2040, interval_runtime: 4.3548, interval_samples_per_second: 1.837, interval_steps_per_second: 2.296, epoch: 8.0[0m
[32m[2022-08-26 15:13:12,564] [    INFO][0m - loss: 0.28225632, learning_rate: 1.7941176470588234e-05, global_step: 2050, interval_runtime: 11.8859, interval_samples_per_second: 0.673, interval_steps_per_second: 0.841, epoch: 8.0392[0m
[32m[2022-08-26 15:13:20,780] [    INFO][0m - loss: 0.20353971, learning_rate: 1.788235294117647e-05, global_step: 2060, interval_runtime: 8.2167, interval_samples_per_second: 0.974, interval_steps_per_second: 1.217, epoch: 8.0784[0m
[32m[2022-08-26 15:13:28,916] [    INFO][0m - loss: 0.43724179, learning_rate: 1.7823529411764707e-05, global_step: 2070, interval_runtime: 8.1355, interval_samples_per_second: 0.983, interval_steps_per_second: 1.229, epoch: 8.1176[0m
[32m[2022-08-26 15:13:37,021] [    INFO][0m - loss: 0.24280391, learning_rate: 1.776470588235294e-05, global_step: 2080, interval_runtime: 8.1051, interval_samples_per_second: 0.987, interval_steps_per_second: 1.234, epoch: 8.1569[0m
[32m[2022-08-26 15:13:45,153] [    INFO][0m - loss: 0.37429957, learning_rate: 1.7705882352941177e-05, global_step: 2090, interval_runtime: 8.1321, interval_samples_per_second: 0.984, interval_steps_per_second: 1.23, epoch: 8.1961[0m
[32m[2022-08-26 15:13:53,374] [    INFO][0m - loss: 0.29452398, learning_rate: 1.7647058823529414e-05, global_step: 2100, interval_runtime: 8.2215, interval_samples_per_second: 0.973, interval_steps_per_second: 1.216, epoch: 8.2353[0m
[32m[2022-08-26 15:13:53,375] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 15:13:53,375] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-26 15:13:53,375] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 15:13:53,375] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 15:13:53,375] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-26 15:16:26,450] [    INFO][0m - eval_loss: 1.6642415523529053, eval_accuracy: 0.6073500967117988, eval_runtime: 153.0745, eval_samples_per_second: 13.51, eval_steps_per_second: 0.425, epoch: 8.2353[0m
[32m[2022-08-26 15:16:26,451] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2100[0m
[32m[2022-08-26 15:16:26,451] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 15:16:29,616] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2100/tokenizer_config.json[0m
[32m[2022-08-26 15:16:29,616] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2100/special_tokens_map.json[0m
[32m[2022-08-26 15:16:41,513] [    INFO][0m - loss: 0.27200425, learning_rate: 1.7588235294117647e-05, global_step: 2110, interval_runtime: 168.1389, interval_samples_per_second: 0.048, interval_steps_per_second: 0.059, epoch: 8.2745[0m
[32m[2022-08-26 15:16:49,843] [    INFO][0m - loss: 0.29287333, learning_rate: 1.7529411764705884e-05, global_step: 2120, interval_runtime: 8.3299, interval_samples_per_second: 0.96, interval_steps_per_second: 1.2, epoch: 8.3137[0m
[32m[2022-08-26 15:16:58,176] [    INFO][0m - loss: 0.36189442, learning_rate: 1.747058823529412e-05, global_step: 2130, interval_runtime: 8.333, interval_samples_per_second: 0.96, interval_steps_per_second: 1.2, epoch: 8.3529[0m
[32m[2022-08-26 15:17:06,521] [    INFO][0m - loss: 0.23554165, learning_rate: 1.7411764705882353e-05, global_step: 2140, interval_runtime: 8.3449, interval_samples_per_second: 0.959, interval_steps_per_second: 1.198, epoch: 8.3922[0m
[32m[2022-08-26 15:17:14,877] [    INFO][0m - loss: 0.34916656, learning_rate: 1.735294117647059e-05, global_step: 2150, interval_runtime: 8.3554, interval_samples_per_second: 0.957, interval_steps_per_second: 1.197, epoch: 8.4314[0m
[32m[2022-08-26 15:17:23,263] [    INFO][0m - loss: 0.22565541, learning_rate: 1.7294117647058823e-05, global_step: 2160, interval_runtime: 8.387, interval_samples_per_second: 0.954, interval_steps_per_second: 1.192, epoch: 8.4706[0m
[32m[2022-08-26 15:17:31,654] [    INFO][0m - loss: 0.33110085, learning_rate: 1.7235294117647057e-05, global_step: 2170, interval_runtime: 8.3908, interval_samples_per_second: 0.953, interval_steps_per_second: 1.192, epoch: 8.5098[0m
[32m[2022-08-26 15:17:40,072] [    INFO][0m - loss: 0.2979553, learning_rate: 1.7176470588235293e-05, global_step: 2180, interval_runtime: 8.418, interval_samples_per_second: 0.95, interval_steps_per_second: 1.188, epoch: 8.549[0m
[32m[2022-08-26 15:17:48,516] [    INFO][0m - loss: 0.22457182, learning_rate: 1.711764705882353e-05, global_step: 2190, interval_runtime: 8.4434, interval_samples_per_second: 0.947, interval_steps_per_second: 1.184, epoch: 8.5882[0m
[32m[2022-08-26 15:17:57,008] [    INFO][0m - loss: 0.20228057, learning_rate: 1.7058823529411763e-05, global_step: 2200, interval_runtime: 8.4918, interval_samples_per_second: 0.942, interval_steps_per_second: 1.178, epoch: 8.6275[0m
[32m[2022-08-26 15:17:57,008] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 15:17:57,009] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-26 15:17:57,009] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 15:17:57,009] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 15:17:57,009] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-26 15:20:36,079] [    INFO][0m - eval_loss: 1.6781022548675537, eval_accuracy: 0.6073500967117988, eval_runtime: 159.07, eval_samples_per_second: 13.001, eval_steps_per_second: 0.409, epoch: 8.6275[0m
[32m[2022-08-26 15:20:36,080] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2200[0m
[32m[2022-08-26 15:20:36,080] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 15:20:39,189] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2200/tokenizer_config.json[0m
[32m[2022-08-26 15:20:39,189] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2200/special_tokens_map.json[0m
[32m[2022-08-26 15:20:51,767] [    INFO][0m - loss: 0.16916484, learning_rate: 1.7e-05, global_step: 2210, interval_runtime: 174.7593, interval_samples_per_second: 0.046, interval_steps_per_second: 0.057, epoch: 8.6667[0m
[32m[2022-08-26 15:21:00,410] [    INFO][0m - loss: 0.33676262, learning_rate: 1.6941176470588237e-05, global_step: 2220, interval_runtime: 8.643, interval_samples_per_second: 0.926, interval_steps_per_second: 1.157, epoch: 8.7059[0m
[32m[2022-08-26 15:21:09,060] [    INFO][0m - loss: 0.22406561, learning_rate: 1.688235294117647e-05, global_step: 2230, interval_runtime: 8.6498, interval_samples_per_second: 0.925, interval_steps_per_second: 1.156, epoch: 8.7451[0m
[32m[2022-08-26 15:21:17,721] [    INFO][0m - loss: 0.2268455, learning_rate: 1.6823529411764706e-05, global_step: 2240, interval_runtime: 8.661, interval_samples_per_second: 0.924, interval_steps_per_second: 1.155, epoch: 8.7843[0m
[32m[2022-08-26 15:21:26,481] [    INFO][0m - loss: 0.32091224, learning_rate: 1.6764705882352943e-05, global_step: 2250, interval_runtime: 8.7603, interval_samples_per_second: 0.913, interval_steps_per_second: 1.142, epoch: 8.8235[0m
[32m[2022-08-26 15:21:35,232] [    INFO][0m - loss: 0.2707922, learning_rate: 1.6705882352941176e-05, global_step: 2260, interval_runtime: 8.7511, interval_samples_per_second: 0.914, interval_steps_per_second: 1.143, epoch: 8.8627[0m
[32m[2022-08-26 15:21:44,039] [    INFO][0m - loss: 0.24401996, learning_rate: 1.6647058823529413e-05, global_step: 2270, interval_runtime: 8.807, interval_samples_per_second: 0.908, interval_steps_per_second: 1.135, epoch: 8.902[0m
[32m[2022-08-26 15:21:52,823] [    INFO][0m - loss: 0.26013217, learning_rate: 1.658823529411765e-05, global_step: 2280, interval_runtime: 8.784, interval_samples_per_second: 0.911, interval_steps_per_second: 1.138, epoch: 8.9412[0m
[32m[2022-08-26 15:22:01,356] [    INFO][0m - loss: 0.19898645, learning_rate: 1.6529411764705883e-05, global_step: 2290, interval_runtime: 8.5333, interval_samples_per_second: 0.938, interval_steps_per_second: 1.172, epoch: 8.9804[0m
[32m[2022-08-26 15:22:10,444] [    INFO][0m - loss: 0.28227119, learning_rate: 1.647058823529412e-05, global_step: 2300, interval_runtime: 9.0875, interval_samples_per_second: 0.88, interval_steps_per_second: 1.1, epoch: 9.0196[0m
[32m[2022-08-26 15:22:10,445] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 15:22:10,445] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-26 15:22:10,445] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 15:22:10,445] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 15:22:10,445] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-26 15:24:55,767] [    INFO][0m - eval_loss: 1.743709921836853, eval_accuracy: 0.6063829787234043, eval_runtime: 165.3218, eval_samples_per_second: 12.509, eval_steps_per_second: 0.393, epoch: 9.0196[0m
[32m[2022-08-26 15:24:55,768] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2300[0m
[32m[2022-08-26 15:24:55,768] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 15:24:58,970] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2300/tokenizer_config.json[0m
[32m[2022-08-26 15:24:58,971] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2300/special_tokens_map.json[0m
[32m[2022-08-26 15:25:11,676] [    INFO][0m - loss: 0.26884105, learning_rate: 1.6411764705882356e-05, global_step: 2310, interval_runtime: 181.232, interval_samples_per_second: 0.044, interval_steps_per_second: 0.055, epoch: 9.0588[0m
[32m[2022-08-26 15:25:20,661] [    INFO][0m - loss: 0.25322289, learning_rate: 1.6352941176470586e-05, global_step: 2320, interval_runtime: 8.9852, interval_samples_per_second: 0.89, interval_steps_per_second: 1.113, epoch: 9.098[0m
[32m[2022-08-26 15:25:29,668] [    INFO][0m - loss: 0.10929091, learning_rate: 1.6294117647058823e-05, global_step: 2330, interval_runtime: 9.0064, interval_samples_per_second: 0.888, interval_steps_per_second: 1.11, epoch: 9.1373[0m
[32m[2022-08-26 15:25:38,692] [    INFO][0m - loss: 0.13481878, learning_rate: 1.623529411764706e-05, global_step: 2340, interval_runtime: 9.0247, interval_samples_per_second: 0.886, interval_steps_per_second: 1.108, epoch: 9.1765[0m
[32m[2022-08-26 15:25:47,708] [    INFO][0m - loss: 0.13007084, learning_rate: 1.6176470588235293e-05, global_step: 2350, interval_runtime: 9.0152, interval_samples_per_second: 0.887, interval_steps_per_second: 1.109, epoch: 9.2157[0m
[32m[2022-08-26 15:25:56,740] [    INFO][0m - loss: 0.13426089, learning_rate: 1.611764705882353e-05, global_step: 2360, interval_runtime: 9.033, interval_samples_per_second: 0.886, interval_steps_per_second: 1.107, epoch: 9.2549[0m
[32m[2022-08-26 15:26:05,804] [    INFO][0m - loss: 0.17673979, learning_rate: 1.6058823529411766e-05, global_step: 2370, interval_runtime: 9.0641, interval_samples_per_second: 0.883, interval_steps_per_second: 1.103, epoch: 9.2941[0m
[32m[2022-08-26 15:26:14,864] [    INFO][0m - loss: 0.13278174, learning_rate: 1.6e-05, global_step: 2380, interval_runtime: 9.0595, interval_samples_per_second: 0.883, interval_steps_per_second: 1.104, epoch: 9.3333[0m
[32m[2022-08-26 15:26:23,958] [    INFO][0m - loss: 0.24549961, learning_rate: 1.5941176470588236e-05, global_step: 2390, interval_runtime: 9.0939, interval_samples_per_second: 0.88, interval_steps_per_second: 1.1, epoch: 9.3725[0m
[32m[2022-08-26 15:26:33,232] [    INFO][0m - loss: 0.31243768, learning_rate: 1.5882352941176473e-05, global_step: 2400, interval_runtime: 9.2745, interval_samples_per_second: 0.863, interval_steps_per_second: 1.078, epoch: 9.4118[0m
[32m[2022-08-26 15:26:33,233] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 15:26:33,233] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-26 15:26:33,233] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 15:26:33,233] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 15:26:33,233] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-26 15:29:26,118] [    INFO][0m - eval_loss: 1.7866839170455933, eval_accuracy: 0.6068665377176016, eval_runtime: 172.8843, eval_samples_per_second: 11.962, eval_steps_per_second: 0.376, epoch: 9.4118[0m
[32m[2022-08-26 15:29:26,118] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2400[0m
[32m[2022-08-26 15:29:26,119] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 15:29:29,161] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2400/tokenizer_config.json[0m
[32m[2022-08-26 15:29:29,161] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2400/special_tokens_map.json[0m
[32m[2022-08-26 15:29:32,686] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-26 15:29:32,686] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-2000 (score: 0.6165377176015474).[0m
[32m[2022-08-26 15:29:33,807] [    INFO][0m - train_runtime: 3715.7122, train_samples_per_second: 10.959, train_steps_per_second: 1.373, train_loss: 1.4196868327260017, epoch: 9.4118[0m
[32m[2022-08-26 15:29:33,809] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-26 15:29:33,809] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 15:29:37,225] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-26 15:29:37,226] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-26 15:29:37,227] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-26 15:29:37,227] [    INFO][0m -   epoch                    =     9.4118[0m
[32m[2022-08-26 15:29:37,227] [    INFO][0m -   train_loss               =     1.4197[0m
[32m[2022-08-26 15:29:37,227] [    INFO][0m -   train_runtime            = 1:01:55.71[0m
[32m[2022-08-26 15:29:37,227] [    INFO][0m -   train_samples_per_second =     10.959[0m
[32m[2022-08-26 15:29:37,227] [    INFO][0m -   train_steps_per_second   =      1.373[0m
[32m[2022-08-26 15:29:37,237] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-26 15:29:37,238] [    INFO][0m -   Num examples = 1784[0m
[32m[2022-08-26 15:29:37,238] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 15:29:37,238] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 15:29:37,238] [    INFO][0m -   Total prediction steps = 56[0m
[32m[2022-08-26 15:32:08,100] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-26 15:32:08,101] [    INFO][0m -   test_accuracy           =     0.6188[0m
[32m[2022-08-26 15:32:08,101] [    INFO][0m -   test_loss               =     1.6691[0m
[32m[2022-08-26 15:32:08,101] [    INFO][0m -   test_runtime            = 0:02:30.86[0m
[32m[2022-08-26 15:32:08,101] [    INFO][0m -   test_samples_per_second =     11.825[0m
[32m[2022-08-26 15:32:08,101] [    INFO][0m -   test_steps_per_second   =      0.371[0m
[32m[2022-08-26 15:32:08,102] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-26 15:32:08,102] [    INFO][0m -   Num examples = 2999[0m
[32m[2022-08-26 15:32:08,102] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 15:32:08,102] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 15:32:08,102] [    INFO][0m -   Total prediction steps = 94[0m
[32m[2022-08-26 15:36:30,383] [    INFO][0m - Predictions for csldcp saved to ./fewclue_submit_examples.[0m
