[33m[2022-08-30 11:22:20,923] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-30 11:22:20,923] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-30 11:22:20,923] [    INFO][0m - ============================================================[0m
[32m[2022-08-30 11:22:20,923] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-30 11:22:20,923] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-30 11:22:20,923] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-30 11:22:20,923] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-30 11:22:20,924] [    INFO][0m - early_stop_patience           :6[0m
[32m[2022-08-30 11:22:20,924] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-30 11:22:20,924] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-08-30 11:22:20,924] [    INFO][0m - [0m
[32m[2022-08-30 11:22:20,924] [    INFO][0m - ============================================================[0m
[32m[2022-08-30 11:22:20,924] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-30 11:22:20,924] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-30 11:22:20,924] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-30 11:22:20,924] [    INFO][0m - prompt                        :{'text':'text_a'}[0m
[32m[2022-08-30 11:22:20,924] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-30 11:22:20,924] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-30 11:22:20,924] [    INFO][0m - task_name                     :cluewsc[0m
[32m[2022-08-30 11:22:20,924] [    INFO][0m - [0m
[32m[2022-08-30 11:22:20,925] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0830 11:22:20.926110 59910 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0830 11:22:20.930241 59910 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-30 11:22:23,713] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-08-30 11:22:23,739] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-08-30 11:22:23,739] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[33m[2022-08-30 11:22:23,746] [ WARNING][0m - Encoder has already set as lstm, change `prompt_encoder` will reset parameters.[0m
[33m[2022-08-30 11:22:23,751] [ WARNING][0m - No soft tokens in template. Use ManualTemplate for better performance.[0m
[33m[2022-08-30 11:22:23,752] [ WARNING][0m - No soft tokens in template. Use ManualTemplate for better performance.[0m
[32m[2022-08-30 11:22:23,752] [    INFO][0m - Using template: [{'add_prefix_space': '', 'text': 'text_a'}][0m
2022-08-30 11:22:23,753 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-30 11:22:23,858] [    INFO][0m - ============================================================[0m
[32m[2022-08-30 11:22:23,858] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-30 11:22:23,858] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-30 11:22:23,859] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-30 11:22:23,859] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-30 11:22:23,859] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-30 11:22:23,859] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-30 11:22:23,859] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-30 11:22:23,859] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-30 11:22:23,859] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-30 11:22:23,859] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-30 11:22:23,859] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-30 11:22:23,859] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-30 11:22:23,860] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-30 11:22:23,860] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-30 11:22:23,860] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-30 11:22:23,860] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-30 11:22:23,860] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-30 11:22:23,860] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-30 11:22:23,860] [    INFO][0m - eval_steps                    :50[0m
[32m[2022-08-30 11:22:23,860] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-30 11:22:23,860] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-30 11:22:23,860] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-30 11:22:23,860] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-30 11:22:23,860] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-30 11:22:23,861] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-30 11:22:23,861] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-30 11:22:23,861] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-30 11:22:23,861] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-30 11:22:23,861] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-30 11:22:23,861] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-30 11:22:23,861] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-30 11:22:23,861] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-30 11:22:23,861] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-30 11:22:23,861] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-30 11:22:23,861] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-30 11:22:23,861] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-30 11:22:23,861] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug30_11-22-20_instance-3bwob41y-01[0m
[32m[2022-08-30 11:22:23,862] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-30 11:22:23,862] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-30 11:22:23,862] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-30 11:22:23,862] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-30 11:22:23,862] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-30 11:22:23,862] [    INFO][0m - max_seq_length                :128[0m
[32m[2022-08-30 11:22:23,862] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-30 11:22:23,862] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-30 11:22:23,862] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-30 11:22:23,862] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-30 11:22:23,862] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-08-30 11:22:23,862] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-30 11:22:23,863] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-30 11:22:23,863] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-30 11:22:23,863] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-30 11:22:23,863] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-30 11:22:23,863] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-30 11:22:23,863] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-30 11:22:23,863] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-30 11:22:23,863] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-30 11:22:23,863] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-30 11:22:23,863] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-30 11:22:23,863] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-30 11:22:23,863] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-30 11:22:23,863] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-30 11:22:23,863] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-30 11:22:23,864] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-30 11:22:23,864] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-30 11:22:23,864] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-30 11:22:23,864] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-30 11:22:23,864] [    INFO][0m - save_steps                    :50[0m
[32m[2022-08-30 11:22:23,864] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-30 11:22:23,864] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-30 11:22:23,864] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-30 11:22:23,864] [    INFO][0m - seed                          :42[0m
[32m[2022-08-30 11:22:23,864] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-30 11:22:23,864] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-30 11:22:23,864] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-30 11:22:23,865] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-30 11:22:23,865] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-30 11:22:23,865] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-30 11:22:23,865] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-30 11:22:23,865] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-30 11:22:23,865] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-30 11:22:23,865] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-30 11:22:23,865] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-30 11:22:23,865] [    INFO][0m - [0m
[32m[2022-08-30 11:22:23,867] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-30 11:22:23,867] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 11:22:23,867] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-08-30 11:22:23,867] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-30 11:22:23,867] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-30 11:22:23,868] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-30 11:22:23,868] [    INFO][0m -   Total optimization steps = 1000.0[0m
[32m[2022-08-30 11:22:23,868] [    INFO][0m -   Total num train samples = 8000[0m
[32m[2022-08-30 11:22:25,653] [    INFO][0m - loss: 0.72942486, learning_rate: 2.97e-05, global_step: 10, interval_runtime: 1.7843, interval_samples_per_second: 4.483, interval_steps_per_second: 5.604, epoch: 0.5[0m
[32m[2022-08-30 11:22:26,333] [    INFO][0m - loss: 0.74466267, learning_rate: 2.94e-05, global_step: 20, interval_runtime: 0.6797, interval_samples_per_second: 11.769, interval_steps_per_second: 14.712, epoch: 1.0[0m
[32m[2022-08-30 11:22:27,128] [    INFO][0m - loss: 0.68678188, learning_rate: 2.91e-05, global_step: 30, interval_runtime: 0.7957, interval_samples_per_second: 10.054, interval_steps_per_second: 12.568, epoch: 1.5[0m
[32m[2022-08-30 11:22:27,820] [    INFO][0m - loss: 0.64663796, learning_rate: 2.88e-05, global_step: 40, interval_runtime: 0.6916, interval_samples_per_second: 11.567, interval_steps_per_second: 14.459, epoch: 2.0[0m
[32m[2022-08-30 11:22:28,623] [    INFO][0m - loss: 0.69970107, learning_rate: 2.8499999999999998e-05, global_step: 50, interval_runtime: 0.8031, interval_samples_per_second: 9.961, interval_steps_per_second: 12.451, epoch: 2.5[0m
[32m[2022-08-30 11:22:28,624] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 11:22:28,624] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-30 11:22:28,624] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 11:22:28,624] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 11:22:28,624] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 11:22:29,183] [    INFO][0m - eval_loss: 0.6882805824279785, eval_accuracy: 0.5345911949685535, eval_runtime: 0.5587, eval_samples_per_second: 284.612, eval_steps_per_second: 8.95, epoch: 2.5[0m
[32m[2022-08-30 11:22:29,184] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-50[0m
[32m[2022-08-30 11:22:29,184] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 11:22:32,371] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-50/tokenizer_config.json[0m
[32m[2022-08-30 11:22:32,371] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-50/special_tokens_map.json[0m
[32m[2022-08-30 11:22:36,836] [    INFO][0m - loss: 0.68914938, learning_rate: 2.8199999999999998e-05, global_step: 60, interval_runtime: 8.2124, interval_samples_per_second: 0.974, interval_steps_per_second: 1.218, epoch: 3.0[0m
[32m[2022-08-30 11:22:37,744] [    INFO][0m - loss: 0.50916452, learning_rate: 2.79e-05, global_step: 70, interval_runtime: 0.9087, interval_samples_per_second: 8.803, interval_steps_per_second: 11.004, epoch: 3.5[0m
[32m[2022-08-30 11:22:38,467] [    INFO][0m - loss: 0.64476738, learning_rate: 2.7600000000000003e-05, global_step: 80, interval_runtime: 0.7228, interval_samples_per_second: 11.069, interval_steps_per_second: 13.836, epoch: 4.0[0m
[32m[2022-08-30 11:22:39,367] [    INFO][0m - loss: 0.55211582, learning_rate: 2.7300000000000003e-05, global_step: 90, interval_runtime: 0.8997, interval_samples_per_second: 8.891, interval_steps_per_second: 11.114, epoch: 4.5[0m
[32m[2022-08-30 11:22:40,102] [    INFO][0m - loss: 0.4813313, learning_rate: 2.7000000000000002e-05, global_step: 100, interval_runtime: 0.7355, interval_samples_per_second: 10.877, interval_steps_per_second: 13.596, epoch: 5.0[0m
[32m[2022-08-30 11:22:40,103] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 11:22:40,103] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-30 11:22:40,103] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 11:22:40,103] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 11:22:40,103] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 11:22:40,758] [    INFO][0m - eval_loss: 0.7964811325073242, eval_accuracy: 0.5660377358490566, eval_runtime: 0.6544, eval_samples_per_second: 242.981, eval_steps_per_second: 7.641, epoch: 5.0[0m
[32m[2022-08-30 11:22:40,758] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-30 11:22:40,758] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 11:22:44,219] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-30 11:22:44,219] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-30 11:22:51,203] [    INFO][0m - loss: 0.45410414, learning_rate: 2.6700000000000002e-05, global_step: 110, interval_runtime: 11.1004, interval_samples_per_second: 0.721, interval_steps_per_second: 0.901, epoch: 5.5[0m
[32m[2022-08-30 11:22:51,956] [    INFO][0m - loss: 0.5572216, learning_rate: 2.64e-05, global_step: 120, interval_runtime: 0.7535, interval_samples_per_second: 10.618, interval_steps_per_second: 13.272, epoch: 6.0[0m
[32m[2022-08-30 11:22:52,978] [    INFO][0m - loss: 0.46398664, learning_rate: 2.61e-05, global_step: 130, interval_runtime: 1.0218, interval_samples_per_second: 7.829, interval_steps_per_second: 9.786, epoch: 6.5[0m
[32m[2022-08-30 11:22:53,749] [    INFO][0m - loss: 0.43778563, learning_rate: 2.58e-05, global_step: 140, interval_runtime: 0.7707, interval_samples_per_second: 10.38, interval_steps_per_second: 12.975, epoch: 7.0[0m
[32m[2022-08-30 11:22:54,781] [    INFO][0m - loss: 0.4544507, learning_rate: 2.55e-05, global_step: 150, interval_runtime: 1.032, interval_samples_per_second: 7.752, interval_steps_per_second: 9.689, epoch: 7.5[0m
[32m[2022-08-30 11:22:54,781] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 11:22:54,781] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-30 11:22:54,781] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 11:22:54,781] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 11:22:54,781] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 11:22:55,545] [    INFO][0m - eval_loss: 0.8492162823677063, eval_accuracy: 0.5660377358490566, eval_runtime: 0.7634, eval_samples_per_second: 208.291, eval_steps_per_second: 6.55, epoch: 7.5[0m
[32m[2022-08-30 11:22:55,545] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-150[0m
[32m[2022-08-30 11:22:55,546] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 11:22:59,011] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-150/tokenizer_config.json[0m
[32m[2022-08-30 11:22:59,012] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-150/special_tokens_map.json[0m
[32m[2022-08-30 11:23:03,895] [    INFO][0m - loss: 0.32772999, learning_rate: 2.52e-05, global_step: 160, interval_runtime: 9.1142, interval_samples_per_second: 0.878, interval_steps_per_second: 1.097, epoch: 8.0[0m
[32m[2022-08-30 11:23:04,977] [    INFO][0m - loss: 0.20366518, learning_rate: 2.49e-05, global_step: 170, interval_runtime: 1.0822, interval_samples_per_second: 7.392, interval_steps_per_second: 9.24, epoch: 8.5[0m
[32m[2022-08-30 11:23:05,774] [    INFO][0m - loss: 0.25934558, learning_rate: 2.4599999999999998e-05, global_step: 180, interval_runtime: 0.797, interval_samples_per_second: 10.038, interval_steps_per_second: 12.547, epoch: 9.0[0m
[32m[2022-08-30 11:23:06,898] [    INFO][0m - loss: 0.38961432, learning_rate: 2.43e-05, global_step: 190, interval_runtime: 1.1241, interval_samples_per_second: 7.117, interval_steps_per_second: 8.896, epoch: 9.5[0m
[32m[2022-08-30 11:23:07,713] [    INFO][0m - loss: 0.35360525, learning_rate: 2.4e-05, global_step: 200, interval_runtime: 0.8153, interval_samples_per_second: 9.812, interval_steps_per_second: 12.265, epoch: 10.0[0m
[32m[2022-08-30 11:23:07,714] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 11:23:07,714] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-30 11:23:07,714] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 11:23:07,714] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 11:23:07,714] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 11:23:08,561] [    INFO][0m - eval_loss: 1.4323129653930664, eval_accuracy: 0.5786163522012578, eval_runtime: 0.8464, eval_samples_per_second: 187.855, eval_steps_per_second: 5.907, epoch: 10.0[0m
[32m[2022-08-30 11:23:08,561] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-30 11:23:08,562] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 11:23:11,858] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-30 11:23:11,859] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-30 11:23:16,732] [    INFO][0m - loss: 0.29624183, learning_rate: 2.37e-05, global_step: 210, interval_runtime: 9.0186, interval_samples_per_second: 0.887, interval_steps_per_second: 1.109, epoch: 10.5[0m
[32m[2022-08-30 11:23:17,558] [    INFO][0m - loss: 0.14488473, learning_rate: 2.3400000000000003e-05, global_step: 220, interval_runtime: 0.8259, interval_samples_per_second: 9.686, interval_steps_per_second: 12.108, epoch: 11.0[0m
[32m[2022-08-30 11:23:18,776] [    INFO][0m - loss: 0.44061894, learning_rate: 2.3100000000000002e-05, global_step: 230, interval_runtime: 1.2174, interval_samples_per_second: 6.571, interval_steps_per_second: 8.214, epoch: 11.5[0m
[32m[2022-08-30 11:23:19,620] [    INFO][0m - loss: 0.25604539, learning_rate: 2.2800000000000002e-05, global_step: 240, interval_runtime: 0.845, interval_samples_per_second: 9.467, interval_steps_per_second: 11.834, epoch: 12.0[0m
[32m[2022-08-30 11:23:20,901] [    INFO][0m - loss: 0.14166321, learning_rate: 2.25e-05, global_step: 250, interval_runtime: 1.2799, interval_samples_per_second: 6.25, interval_steps_per_second: 7.813, epoch: 12.5[0m
[32m[2022-08-30 11:23:20,901] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 11:23:20,902] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-30 11:23:20,902] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 11:23:20,902] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 11:23:20,902] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 11:23:22,041] [    INFO][0m - eval_loss: 1.7668906450271606, eval_accuracy: 0.5660377358490566, eval_runtime: 0.9445, eval_samples_per_second: 168.347, eval_steps_per_second: 5.294, epoch: 12.5[0m
[32m[2022-08-30 11:23:22,042] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-250[0m
[32m[2022-08-30 11:23:22,042] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 11:23:25,203] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-250/tokenizer_config.json[0m
[32m[2022-08-30 11:23:25,203] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-250/special_tokens_map.json[0m
[32m[2022-08-30 11:23:29,933] [    INFO][0m - loss: 0.23335721, learning_rate: 2.22e-05, global_step: 260, interval_runtime: 9.0327, interval_samples_per_second: 0.886, interval_steps_per_second: 1.107, epoch: 13.0[0m
[32m[2022-08-30 11:23:31,284] [    INFO][0m - loss: 0.24531505, learning_rate: 2.19e-05, global_step: 270, interval_runtime: 1.3505, interval_samples_per_second: 5.924, interval_steps_per_second: 7.405, epoch: 13.5[0m
[32m[2022-08-30 11:23:32,168] [    INFO][0m - loss: 0.27492571, learning_rate: 2.16e-05, global_step: 280, interval_runtime: 0.8847, interval_samples_per_second: 9.043, interval_steps_per_second: 11.303, epoch: 14.0[0m
[32m[2022-08-30 11:23:33,519] [    INFO][0m - loss: 0.37018085, learning_rate: 2.13e-05, global_step: 290, interval_runtime: 1.3511, interval_samples_per_second: 5.921, interval_steps_per_second: 7.401, epoch: 14.5[0m
[32m[2022-08-30 11:23:34,412] [    INFO][0m - loss: 0.28224225, learning_rate: 2.1e-05, global_step: 300, interval_runtime: 0.8923, interval_samples_per_second: 8.966, interval_steps_per_second: 11.207, epoch: 15.0[0m
[32m[2022-08-30 11:23:34,412] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 11:23:34,412] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-30 11:23:34,412] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 11:23:34,412] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 11:23:34,413] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 11:23:35,459] [    INFO][0m - eval_loss: 2.0097038745880127, eval_accuracy: 0.5723270440251572, eval_runtime: 1.0465, eval_samples_per_second: 151.939, eval_steps_per_second: 4.778, epoch: 15.0[0m
[32m[2022-08-30 11:23:35,460] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-30 11:23:35,460] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 11:23:38,634] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-30 11:23:38,635] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-30 11:23:43,634] [    INFO][0m - loss: 0.22774765, learning_rate: 2.07e-05, global_step: 310, interval_runtime: 9.2219, interval_samples_per_second: 0.868, interval_steps_per_second: 1.084, epoch: 15.5[0m
[32m[2022-08-30 11:23:44,542] [    INFO][0m - loss: 0.16265125, learning_rate: 2.04e-05, global_step: 320, interval_runtime: 0.9083, interval_samples_per_second: 8.808, interval_steps_per_second: 11.009, epoch: 16.0[0m
[32m[2022-08-30 11:23:45,991] [    INFO][0m - loss: 0.20571117, learning_rate: 2.01e-05, global_step: 330, interval_runtime: 1.4493, interval_samples_per_second: 5.52, interval_steps_per_second: 6.9, epoch: 16.5[0m
[32m[2022-08-30 11:23:46,916] [    INFO][0m - loss: 0.08225512, learning_rate: 1.98e-05, global_step: 340, interval_runtime: 0.9247, interval_samples_per_second: 8.652, interval_steps_per_second: 10.814, epoch: 17.0[0m
[32m[2022-08-30 11:23:48,431] [    INFO][0m - loss: 0.17021403, learning_rate: 1.95e-05, global_step: 350, interval_runtime: 1.5152, interval_samples_per_second: 5.28, interval_steps_per_second: 6.6, epoch: 17.5[0m
[32m[2022-08-30 11:23:48,432] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 11:23:48,432] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-30 11:23:48,432] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 11:23:48,432] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 11:23:48,432] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 11:23:49,577] [    INFO][0m - eval_loss: 2.1665053367614746, eval_accuracy: 0.5723270440251572, eval_runtime: 1.1448, eval_samples_per_second: 138.883, eval_steps_per_second: 4.367, epoch: 17.5[0m
[32m[2022-08-30 11:23:49,577] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-350[0m
[32m[2022-08-30 11:23:49,578] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 11:23:55,048] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-350/tokenizer_config.json[0m
[32m[2022-08-30 11:23:55,048] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-350/special_tokens_map.json[0m
[32m[2022-08-30 11:24:00,230] [    INFO][0m - loss: 0.20265331, learning_rate: 1.9200000000000003e-05, global_step: 360, interval_runtime: 11.7991, interval_samples_per_second: 0.678, interval_steps_per_second: 0.848, epoch: 18.0[0m
[32m[2022-08-30 11:24:01,812] [    INFO][0m - loss: 0.19622691, learning_rate: 1.8900000000000002e-05, global_step: 370, interval_runtime: 1.5815, interval_samples_per_second: 5.059, interval_steps_per_second: 6.323, epoch: 18.5[0m
[32m[2022-08-30 11:24:02,762] [    INFO][0m - loss: 0.14549035, learning_rate: 1.86e-05, global_step: 380, interval_runtime: 0.95, interval_samples_per_second: 8.421, interval_steps_per_second: 10.527, epoch: 19.0[0m
[32m[2022-08-30 11:24:04,362] [    INFO][0m - loss: 0.25013015, learning_rate: 1.83e-05, global_step: 390, interval_runtime: 1.5996, interval_samples_per_second: 5.001, interval_steps_per_second: 6.251, epoch: 19.5[0m
[32m[2022-08-30 11:24:05,339] [    INFO][0m - loss: 0.08519334, learning_rate: 1.8e-05, global_step: 400, interval_runtime: 0.9777, interval_samples_per_second: 8.182, interval_steps_per_second: 10.228, epoch: 20.0[0m
[32m[2022-08-30 11:24:05,340] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 11:24:05,340] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-30 11:24:05,340] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 11:24:05,340] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 11:24:05,340] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 11:24:06,562] [    INFO][0m - eval_loss: 2.2630884647369385, eval_accuracy: 0.5408805031446541, eval_runtime: 1.222, eval_samples_per_second: 130.11, eval_steps_per_second: 4.092, epoch: 20.0[0m
[32m[2022-08-30 11:24:06,563] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-30 11:24:06,563] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 11:24:09,620] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-30 11:24:09,621] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-30 11:24:15,199] [    INFO][0m - loss: 0.05829728, learning_rate: 1.77e-05, global_step: 410, interval_runtime: 9.8593, interval_samples_per_second: 0.811, interval_steps_per_second: 1.014, epoch: 20.5[0m
[32m[2022-08-30 11:24:16,187] [    INFO][0m - loss: 0.12313074, learning_rate: 1.74e-05, global_step: 420, interval_runtime: 0.9888, interval_samples_per_second: 8.09, interval_steps_per_second: 10.113, epoch: 21.0[0m
[32m[2022-08-30 11:24:17,898] [    INFO][0m - loss: 0.07724927, learning_rate: 1.71e-05, global_step: 430, interval_runtime: 1.7109, interval_samples_per_second: 4.676, interval_steps_per_second: 5.845, epoch: 21.5[0m
[32m[2022-08-30 11:24:18,897] [    INFO][0m - loss: 0.09810249, learning_rate: 1.6800000000000002e-05, global_step: 440, interval_runtime: 0.9986, interval_samples_per_second: 8.011, interval_steps_per_second: 10.014, epoch: 22.0[0m
[32m[2022-08-30 11:24:20,655] [    INFO][0m - loss: 0.03012773, learning_rate: 1.65e-05, global_step: 450, interval_runtime: 1.7585, interval_samples_per_second: 4.549, interval_steps_per_second: 5.687, epoch: 22.5[0m
[32m[2022-08-30 11:24:20,656] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 11:24:20,656] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-30 11:24:20,656] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 11:24:20,656] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 11:24:20,656] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 11:24:22,003] [    INFO][0m - eval_loss: 2.243161678314209, eval_accuracy: 0.5911949685534591, eval_runtime: 1.3469, eval_samples_per_second: 118.049, eval_steps_per_second: 3.712, epoch: 22.5[0m
[32m[2022-08-30 11:24:22,004] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-450[0m
[32m[2022-08-30 11:24:22,004] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 11:24:25,351] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-450/tokenizer_config.json[0m
[32m[2022-08-30 11:24:25,352] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-450/special_tokens_map.json[0m
[32m[2022-08-30 11:24:30,732] [    INFO][0m - loss: 0.07337946, learning_rate: 1.62e-05, global_step: 460, interval_runtime: 10.0769, interval_samples_per_second: 0.794, interval_steps_per_second: 0.992, epoch: 23.0[0m
[32m[2022-08-30 11:24:32,514] [    INFO][0m - loss: 0.18075063, learning_rate: 1.59e-05, global_step: 470, interval_runtime: 1.7818, interval_samples_per_second: 4.49, interval_steps_per_second: 5.612, epoch: 23.5[0m
[32m[2022-08-30 11:24:33,548] [    INFO][0m - loss: 0.01347248, learning_rate: 1.56e-05, global_step: 480, interval_runtime: 1.0339, interval_samples_per_second: 7.738, interval_steps_per_second: 9.672, epoch: 24.0[0m
[32m[2022-08-30 11:24:35,435] [    INFO][0m - loss: 0.15868624, learning_rate: 1.53e-05, global_step: 490, interval_runtime: 1.8868, interval_samples_per_second: 4.24, interval_steps_per_second: 5.3, epoch: 24.5[0m
[32m[2022-08-30 11:24:36,482] [    INFO][0m - loss: 0.01294958, learning_rate: 1.5e-05, global_step: 500, interval_runtime: 1.0469, interval_samples_per_second: 7.641, interval_steps_per_second: 9.552, epoch: 25.0[0m
[32m[2022-08-30 11:24:36,482] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 11:24:36,482] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-30 11:24:36,483] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 11:24:36,483] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 11:24:36,483] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 11:24:37,912] [    INFO][0m - eval_loss: 2.4396121501922607, eval_accuracy: 0.5660377358490566, eval_runtime: 1.429, eval_samples_per_second: 111.27, eval_steps_per_second: 3.499, epoch: 25.0[0m
[32m[2022-08-30 11:24:37,912] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-08-30 11:24:37,912] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 11:24:41,159] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-08-30 11:24:41,159] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-08-30 11:24:46,726] [    INFO][0m - loss: 0.07002031, learning_rate: 1.47e-05, global_step: 510, interval_runtime: 10.2436, interval_samples_per_second: 0.781, interval_steps_per_second: 0.976, epoch: 25.5[0m
[32m[2022-08-30 11:24:47,778] [    INFO][0m - loss: 0.00089159, learning_rate: 1.44e-05, global_step: 520, interval_runtime: 1.0522, interval_samples_per_second: 7.603, interval_steps_per_second: 9.504, epoch: 26.0[0m
[32m[2022-08-30 11:24:49,683] [    INFO][0m - loss: 0.00196488, learning_rate: 1.4099999999999999e-05, global_step: 530, interval_runtime: 1.9052, interval_samples_per_second: 4.199, interval_steps_per_second: 5.249, epoch: 26.5[0m
[32m[2022-08-30 11:24:50,767] [    INFO][0m - loss: 0.05511813, learning_rate: 1.3800000000000002e-05, global_step: 540, interval_runtime: 1.0841, interval_samples_per_second: 7.379, interval_steps_per_second: 9.224, epoch: 27.0[0m
[32m[2022-08-30 11:24:52,718] [    INFO][0m - loss: 0.01512183, learning_rate: 1.3500000000000001e-05, global_step: 550, interval_runtime: 1.9505, interval_samples_per_second: 4.102, interval_steps_per_second: 5.127, epoch: 27.5[0m
[32m[2022-08-30 11:24:52,718] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 11:24:52,718] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-30 11:24:52,719] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 11:24:52,719] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 11:24:52,719] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 11:24:54,228] [    INFO][0m - eval_loss: 2.694026231765747, eval_accuracy: 0.5786163522012578, eval_runtime: 1.5094, eval_samples_per_second: 105.34, eval_steps_per_second: 3.313, epoch: 27.5[0m
[32m[2022-08-30 11:24:54,229] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-550[0m
[32m[2022-08-30 11:24:54,229] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 11:24:57,448] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-550/tokenizer_config.json[0m
[32m[2022-08-30 11:24:57,449] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-550/special_tokens_map.json[0m
[32m[2022-08-30 11:25:02,235] [    INFO][0m - loss: 0.05755316, learning_rate: 1.32e-05, global_step: 560, interval_runtime: 9.5174, interval_samples_per_second: 0.841, interval_steps_per_second: 1.051, epoch: 28.0[0m
[32m[2022-08-30 11:25:04,260] [    INFO][0m - loss: 0.02470258, learning_rate: 1.29e-05, global_step: 570, interval_runtime: 2.025, interval_samples_per_second: 3.951, interval_steps_per_second: 4.938, epoch: 28.5[0m
[32m[2022-08-30 11:25:05,364] [    INFO][0m - loss: 0.00103607, learning_rate: 1.26e-05, global_step: 580, interval_runtime: 1.1043, interval_samples_per_second: 7.245, interval_steps_per_second: 9.056, epoch: 29.0[0m
[32m[2022-08-30 11:25:07,397] [    INFO][0m - loss: 0.01624554, learning_rate: 1.2299999999999999e-05, global_step: 590, interval_runtime: 2.0325, interval_samples_per_second: 3.936, interval_steps_per_second: 4.92, epoch: 29.5[0m
[32m[2022-08-30 11:25:08,520] [    INFO][0m - loss: 0.00057588, learning_rate: 1.2e-05, global_step: 600, interval_runtime: 1.1235, interval_samples_per_second: 7.121, interval_steps_per_second: 8.901, epoch: 30.0[0m
[32m[2022-08-30 11:25:08,521] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 11:25:08,521] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-30 11:25:08,521] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 11:25:08,521] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 11:25:08,521] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 11:25:10,165] [    INFO][0m - eval_loss: 2.671680212020874, eval_accuracy: 0.559748427672956, eval_runtime: 1.6432, eval_samples_per_second: 96.763, eval_steps_per_second: 3.043, epoch: 30.0[0m
[32m[2022-08-30 11:25:10,165] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-08-30 11:25:10,166] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 11:25:13,634] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-08-30 11:25:13,634] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-08-30 11:25:19,339] [    INFO][0m - loss: 0.00144184, learning_rate: 1.1700000000000001e-05, global_step: 610, interval_runtime: 10.8191, interval_samples_per_second: 0.739, interval_steps_per_second: 0.924, epoch: 30.5[0m
[32m[2022-08-30 11:25:20,603] [    INFO][0m - loss: 0.00348016, learning_rate: 1.1400000000000001e-05, global_step: 620, interval_runtime: 1.2639, interval_samples_per_second: 6.33, interval_steps_per_second: 7.912, epoch: 31.0[0m
[32m[2022-08-30 11:25:22,733] [    INFO][0m - loss: 0.00179894, learning_rate: 1.11e-05, global_step: 630, interval_runtime: 2.1294, interval_samples_per_second: 3.757, interval_steps_per_second: 4.696, epoch: 31.5[0m
[32m[2022-08-30 11:25:23,879] [    INFO][0m - loss: 0.00050341, learning_rate: 1.08e-05, global_step: 640, interval_runtime: 1.146, interval_samples_per_second: 6.981, interval_steps_per_second: 8.726, epoch: 32.0[0m
[32m[2022-08-30 11:25:26,046] [    INFO][0m - loss: 0.01168031, learning_rate: 1.05e-05, global_step: 650, interval_runtime: 2.1673, interval_samples_per_second: 3.691, interval_steps_per_second: 4.614, epoch: 32.5[0m
[32m[2022-08-30 11:25:26,047] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 11:25:26,047] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-30 11:25:26,047] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 11:25:26,047] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 11:25:26,047] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 11:25:27,846] [    INFO][0m - eval_loss: 2.836138963699341, eval_accuracy: 0.5534591194968553, eval_runtime: 1.7989, eval_samples_per_second: 88.387, eval_steps_per_second: 2.779, epoch: 32.5[0m
[32m[2022-08-30 11:25:27,847] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-650[0m
[32m[2022-08-30 11:25:27,847] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 11:25:32,417] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-650/tokenizer_config.json[0m
[32m[2022-08-30 11:25:32,418] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-650/special_tokens_map.json[0m
[32m[2022-08-30 11:25:37,287] [    INFO][0m - loss: 0.00051323, learning_rate: 1.02e-05, global_step: 660, interval_runtime: 11.2413, interval_samples_per_second: 0.712, interval_steps_per_second: 0.89, epoch: 33.0[0m
[32m[2022-08-30 11:25:39,538] [    INFO][0m - loss: 0.00059986, learning_rate: 9.9e-06, global_step: 670, interval_runtime: 2.2508, interval_samples_per_second: 3.554, interval_steps_per_second: 4.443, epoch: 33.5[0m
[32m[2022-08-30 11:25:40,724] [    INFO][0m - loss: 0.00054443, learning_rate: 9.600000000000001e-06, global_step: 680, interval_runtime: 1.1854, interval_samples_per_second: 6.749, interval_steps_per_second: 8.436, epoch: 34.0[0m
[32m[2022-08-30 11:25:43,024] [    INFO][0m - loss: 0.00050997, learning_rate: 9.3e-06, global_step: 690, interval_runtime: 2.3004, interval_samples_per_second: 3.478, interval_steps_per_second: 4.347, epoch: 34.5[0m
[32m[2022-08-30 11:25:44,214] [    INFO][0m - loss: 0.00223572, learning_rate: 9e-06, global_step: 700, interval_runtime: 1.1898, interval_samples_per_second: 6.724, interval_steps_per_second: 8.405, epoch: 35.0[0m
[32m[2022-08-30 11:25:44,214] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 11:25:44,215] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-30 11:25:44,215] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 11:25:44,215] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 11:25:44,215] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 11:25:46,038] [    INFO][0m - eval_loss: 2.869097948074341, eval_accuracy: 0.559748427672956, eval_runtime: 1.8221, eval_samples_per_second: 87.26, eval_steps_per_second: 2.744, epoch: 35.0[0m
[32m[2022-08-30 11:25:46,038] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-700[0m
[32m[2022-08-30 11:25:46,038] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 11:25:49,442] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-700/tokenizer_config.json[0m
[32m[2022-08-30 11:25:49,443] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-700/special_tokens_map.json[0m
[32m[2022-08-30 11:25:55,932] [    INFO][0m - loss: 0.00038348, learning_rate: 8.7e-06, global_step: 710, interval_runtime: 11.7177, interval_samples_per_second: 0.683, interval_steps_per_second: 0.853, epoch: 35.5[0m
[32m[2022-08-30 11:25:57,165] [    INFO][0m - loss: 0.00038531, learning_rate: 8.400000000000001e-06, global_step: 720, interval_runtime: 1.2335, interval_samples_per_second: 6.486, interval_steps_per_second: 8.107, epoch: 36.0[0m
[32m[2022-08-30 11:25:59,572] [    INFO][0m - loss: 0.00043939, learning_rate: 8.1e-06, global_step: 730, interval_runtime: 2.4071, interval_samples_per_second: 3.324, interval_steps_per_second: 4.154, epoch: 36.5[0m
[32m[2022-08-30 11:26:00,817] [    INFO][0m - loss: 0.00047536, learning_rate: 7.8e-06, global_step: 740, interval_runtime: 1.2444, interval_samples_per_second: 6.429, interval_steps_per_second: 8.036, epoch: 37.0[0m
[32m[2022-08-30 11:26:04,547] [    INFO][0m - loss: 0.07801123, learning_rate: 7.5e-06, global_step: 750, interval_runtime: 2.4896, interval_samples_per_second: 3.213, interval_steps_per_second: 4.017, epoch: 37.5[0m
[32m[2022-08-30 11:26:04,548] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 11:26:04,548] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-30 11:26:04,548] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 11:26:04,548] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 11:26:04,548] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 11:26:06,468] [    INFO][0m - eval_loss: 2.6541595458984375, eval_accuracy: 0.6037735849056604, eval_runtime: 1.9197, eval_samples_per_second: 82.825, eval_steps_per_second: 2.605, epoch: 37.5[0m
[32m[2022-08-30 11:26:06,468] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-750[0m
[32m[2022-08-30 11:26:06,469] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 11:26:09,559] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-750/tokenizer_config.json[0m
[32m[2022-08-30 11:26:09,559] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-750/special_tokens_map.json[0m
[32m[2022-08-30 11:26:14,409] [    INFO][0m - loss: 0.00123948, learning_rate: 7.2e-06, global_step: 760, interval_runtime: 11.1022, interval_samples_per_second: 0.721, interval_steps_per_second: 0.901, epoch: 38.0[0m
[32m[2022-08-30 11:26:16,861] [    INFO][0m - loss: 0.00046962, learning_rate: 6.900000000000001e-06, global_step: 770, interval_runtime: 2.4527, interval_samples_per_second: 3.262, interval_steps_per_second: 4.077, epoch: 38.5[0m
[32m[2022-08-30 11:26:18,108] [    INFO][0m - loss: 0.000405, learning_rate: 6.6e-06, global_step: 780, interval_runtime: 1.2466, interval_samples_per_second: 6.418, interval_steps_per_second: 8.022, epoch: 39.0[0m
[32m[2022-08-30 11:26:20,596] [    INFO][0m - loss: 0.00034144, learning_rate: 6.3e-06, global_step: 790, interval_runtime: 2.4876, interval_samples_per_second: 3.216, interval_steps_per_second: 4.02, epoch: 39.5[0m
[32m[2022-08-30 11:26:21,870] [    INFO][0m - loss: 0.00037118, learning_rate: 6e-06, global_step: 800, interval_runtime: 1.2747, interval_samples_per_second: 6.276, interval_steps_per_second: 7.845, epoch: 40.0[0m
[32m[2022-08-30 11:26:21,871] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 11:26:21,871] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-30 11:26:21,871] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 11:26:21,871] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 11:26:21,871] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 11:26:23,907] [    INFO][0m - eval_loss: 2.740035057067871, eval_accuracy: 0.5911949685534591, eval_runtime: 2.0359, eval_samples_per_second: 78.1, eval_steps_per_second: 2.456, epoch: 40.0[0m
[32m[2022-08-30 11:26:23,908] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-800[0m
[32m[2022-08-30 11:26:23,908] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 11:26:25,347] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-800/tokenizer_config.json[0m
[32m[2022-08-30 11:26:25,347] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-800/special_tokens_map.json[0m
[32m[2022-08-30 11:26:29,633] [    INFO][0m - loss: 0.00033366, learning_rate: 5.7000000000000005e-06, global_step: 810, interval_runtime: 7.7622, interval_samples_per_second: 1.031, interval_steps_per_second: 1.288, epoch: 40.5[0m
[32m[2022-08-30 11:26:30,938] [    INFO][0m - loss: 0.00037337, learning_rate: 5.4e-06, global_step: 820, interval_runtime: 1.3059, interval_samples_per_second: 6.126, interval_steps_per_second: 7.657, epoch: 41.0[0m
[32m[2022-08-30 11:26:33,530] [    INFO][0m - loss: 0.00033914, learning_rate: 5.1e-06, global_step: 830, interval_runtime: 2.5916, interval_samples_per_second: 3.087, interval_steps_per_second: 3.859, epoch: 41.5[0m
[32m[2022-08-30 11:26:34,825] [    INFO][0m - loss: 0.06363298, learning_rate: 4.800000000000001e-06, global_step: 840, interval_runtime: 1.2953, interval_samples_per_second: 6.176, interval_steps_per_second: 7.72, epoch: 42.0[0m
[32m[2022-08-30 11:26:37,510] [    INFO][0m - loss: 0.00040234, learning_rate: 4.5e-06, global_step: 850, interval_runtime: 2.6852, interval_samples_per_second: 2.979, interval_steps_per_second: 3.724, epoch: 42.5[0m
[32m[2022-08-30 11:26:37,511] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 11:26:37,511] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-30 11:26:37,511] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 11:26:37,511] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 11:26:37,511] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 11:26:39,599] [    INFO][0m - eval_loss: 2.682901382446289, eval_accuracy: 0.5911949685534591, eval_runtime: 2.0872, eval_samples_per_second: 76.178, eval_steps_per_second: 2.396, epoch: 42.5[0m
[32m[2022-08-30 11:26:39,599] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-850[0m
[32m[2022-08-30 11:26:39,599] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 11:26:41,029] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-850/tokenizer_config.json[0m
[32m[2022-08-30 11:26:41,029] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-850/special_tokens_map.json[0m
[32m[2022-08-30 11:26:44,095] [    INFO][0m - loss: 0.00039091, learning_rate: 4.2000000000000004e-06, global_step: 860, interval_runtime: 6.5851, interval_samples_per_second: 1.215, interval_steps_per_second: 1.519, epoch: 43.0[0m
[32m[2022-08-30 11:26:46,767] [    INFO][0m - loss: 0.08694631, learning_rate: 3.9e-06, global_step: 870, interval_runtime: 2.6716, interval_samples_per_second: 2.994, interval_steps_per_second: 3.743, epoch: 43.5[0m
[32m[2022-08-30 11:26:48,099] [    INFO][0m - loss: 0.08901166, learning_rate: 3.6e-06, global_step: 880, interval_runtime: 1.3322, interval_samples_per_second: 6.005, interval_steps_per_second: 7.506, epoch: 44.0[0m
[32m[2022-08-30 11:26:50,848] [    INFO][0m - loss: 0.00031932, learning_rate: 3.3e-06, global_step: 890, interval_runtime: 2.7489, interval_samples_per_second: 2.91, interval_steps_per_second: 3.638, epoch: 44.5[0m
[32m[2022-08-30 11:26:52,202] [    INFO][0m - loss: 0.00035193, learning_rate: 3e-06, global_step: 900, interval_runtime: 1.3542, interval_samples_per_second: 5.908, interval_steps_per_second: 7.384, epoch: 45.0[0m
[32m[2022-08-30 11:26:52,203] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 11:26:52,203] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-30 11:26:52,203] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 11:26:52,203] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 11:26:52,203] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 11:26:54,437] [    INFO][0m - eval_loss: 2.7534191608428955, eval_accuracy: 0.6037735849056604, eval_runtime: 2.2336, eval_samples_per_second: 71.186, eval_steps_per_second: 2.239, epoch: 45.0[0m
[32m[2022-08-30 11:26:54,438] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-900[0m
[32m[2022-08-30 11:26:54,438] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 11:26:55,943] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-900/tokenizer_config.json[0m
[32m[2022-08-30 11:26:55,943] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-900/special_tokens_map.json[0m
[32m[2022-08-30 11:27:00,747] [    INFO][0m - loss: 0.00034846, learning_rate: 2.7e-06, global_step: 910, interval_runtime: 8.5444, interval_samples_per_second: 0.936, interval_steps_per_second: 1.17, epoch: 45.5[0m
[32m[2022-08-30 11:27:02,140] [    INFO][0m - loss: 0.00747918, learning_rate: 2.4000000000000003e-06, global_step: 920, interval_runtime: 1.3931, interval_samples_per_second: 5.743, interval_steps_per_second: 7.178, epoch: 46.0[0m
[32m[2022-08-30 11:27:04,957] [    INFO][0m - loss: 0.00031501, learning_rate: 2.1000000000000002e-06, global_step: 930, interval_runtime: 2.8171, interval_samples_per_second: 2.84, interval_steps_per_second: 3.55, epoch: 46.5[0m
[32m[2022-08-30 11:27:06,339] [    INFO][0m - loss: 0.00033834, learning_rate: 1.8e-06, global_step: 940, interval_runtime: 1.3821, interval_samples_per_second: 5.788, interval_steps_per_second: 7.235, epoch: 47.0[0m
[32m[2022-08-30 11:27:09,236] [    INFO][0m - loss: 0.00088977, learning_rate: 1.5e-06, global_step: 950, interval_runtime: 2.8963, interval_samples_per_second: 2.762, interval_steps_per_second: 3.453, epoch: 47.5[0m
[32m[2022-08-30 11:27:09,236] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 11:27:09,237] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-30 11:27:09,237] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 11:27:09,237] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 11:27:09,237] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 11:27:14,792] [    INFO][0m - eval_loss: 2.833073616027832, eval_accuracy: 0.5849056603773585, eval_runtime: 2.3125, eval_samples_per_second: 68.757, eval_steps_per_second: 2.162, epoch: 47.5[0m
[32m[2022-08-30 11:27:14,793] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-950[0m
[32m[2022-08-30 11:27:14,793] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 11:27:16,620] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-950/tokenizer_config.json[0m
[32m[2022-08-30 11:27:16,620] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-950/special_tokens_map.json[0m
[32m[2022-08-30 11:27:20,165] [    INFO][0m - loss: 0.00065832, learning_rate: 1.2000000000000002e-06, global_step: 960, interval_runtime: 10.9295, interval_samples_per_second: 0.732, interval_steps_per_second: 0.915, epoch: 48.0[0m
[32m[2022-08-30 11:27:23,140] [    INFO][0m - loss: 0.00031449, learning_rate: 9e-07, global_step: 970, interval_runtime: 2.9745, interval_samples_per_second: 2.689, interval_steps_per_second: 3.362, epoch: 48.5[0m
[32m[2022-08-30 11:27:24,562] [    INFO][0m - loss: 0.00032179, learning_rate: 6.000000000000001e-07, global_step: 980, interval_runtime: 1.4225, interval_samples_per_second: 5.624, interval_steps_per_second: 7.03, epoch: 49.0[0m
[32m[2022-08-30 11:27:27,533] [    INFO][0m - loss: 0.00031121, learning_rate: 3.0000000000000004e-07, global_step: 990, interval_runtime: 2.9707, interval_samples_per_second: 2.693, interval_steps_per_second: 3.366, epoch: 49.5[0m
[32m[2022-08-30 11:27:28,960] [    INFO][0m - loss: 0.00033303, learning_rate: 0.0, global_step: 1000, interval_runtime: 1.4275, interval_samples_per_second: 5.604, interval_steps_per_second: 7.005, epoch: 50.0[0m
[32m[2022-08-30 11:27:28,961] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 11:27:28,961] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-30 11:27:28,961] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 11:27:28,961] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 11:27:28,961] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 11:27:31,315] [    INFO][0m - eval_loss: 2.874966859817505, eval_accuracy: 0.5723270440251572, eval_runtime: 2.3531, eval_samples_per_second: 67.572, eval_steps_per_second: 2.125, epoch: 50.0[0m
[32m[2022-08-30 11:27:31,315] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1000[0m
[32m[2022-08-30 11:27:31,315] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 11:27:32,850] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1000/tokenizer_config.json[0m
[32m[2022-08-30 11:27:32,850] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1000/special_tokens_map.json[0m
[32m[2022-08-30 11:27:34,573] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-30 11:27:34,574] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-750 (score: 0.6037735849056604).[0m
[32m[2022-08-30 11:27:35,626] [    INFO][0m - train_runtime: 311.7578, train_samples_per_second: 25.661, train_steps_per_second: 3.208, train_loss: 0.16127610811800697, epoch: 50.0[0m
[32m[2022-08-30 11:27:35,627] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-30 11:27:35,628] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 11:27:38,875] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-30 11:27:38,876] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-30 11:27:38,876] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-30 11:27:38,877] [    INFO][0m -   epoch                    =       50.0[0m
[32m[2022-08-30 11:27:38,877] [    INFO][0m -   train_loss               =     0.1613[0m
[32m[2022-08-30 11:27:38,877] [    INFO][0m -   train_runtime            = 0:05:11.75[0m
[32m[2022-08-30 11:27:38,877] [    INFO][0m -   train_samples_per_second =     25.661[0m
[32m[2022-08-30 11:27:38,877] [    INFO][0m -   train_steps_per_second   =      3.208[0m
[32m[2022-08-30 11:27:38,900] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-30 11:27:38,900] [    INFO][0m -   Num examples = 976[0m
[32m[2022-08-30 11:27:38,900] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 11:27:38,900] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 11:27:38,900] [    INFO][0m -   Total prediction steps = 31[0m
[32m[2022-08-30 11:27:53,856] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-30 11:27:53,857] [    INFO][0m -   test_accuracy           =     0.5236[0m
[32m[2022-08-30 11:27:53,857] [    INFO][0m -   test_loss               =     3.4789[0m
[32m[2022-08-30 11:27:53,857] [    INFO][0m -   test_runtime            = 0:00:14.95[0m
[32m[2022-08-30 11:27:53,857] [    INFO][0m -   test_samples_per_second =     65.258[0m
[32m[2022-08-30 11:27:53,857] [    INFO][0m -   test_steps_per_second   =      2.073[0m
[32m[2022-08-30 11:27:53,858] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-30 11:27:53,858] [    INFO][0m -   Num examples = 290[0m
[32m[2022-08-30 11:27:53,858] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 11:27:53,858] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 11:27:53,858] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-08-30 11:27:58,563] [    INFO][0m - Predictions for cluewscf saved to ./fewclue_submit_examples.[0m
metric <function main.<locals>.compute_metrics at 0x7fcdf3b963a0>
