[33m[2022-08-30 11:39:51,368] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-30 11:39:51,368] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-30 11:39:51,368] [    INFO][0m - ============================================================[0m
[32m[2022-08-30 11:39:51,368] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-30 11:39:51,368] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-30 11:39:51,368] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-30 11:39:51,369] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-30 11:39:51,369] [    INFO][0m - early_stop_patience           :6[0m
[32m[2022-08-30 11:39:51,369] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-30 11:39:51,369] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-08-30 11:39:51,369] [    INFO][0m - [0m
[32m[2022-08-30 11:39:51,369] [    INFO][0m - ============================================================[0m
[32m[2022-08-30 11:39:51,369] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-30 11:39:51,369] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-30 11:39:51,369] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-30 11:39:51,369] [    INFO][0m - prompt                        :{'text':'text_a'}{'sep'}{'text':'text_b'}[0m
[32m[2022-08-30 11:39:51,369] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-30 11:39:51,369] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-30 11:39:51,369] [    INFO][0m - task_name                     :ocnli[0m
[32m[2022-08-30 11:39:51,370] [    INFO][0m - [0m
[32m[2022-08-30 11:39:51,370] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0830 11:39:51.371223 21650 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0830 11:39:51.375247 21650 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-30 11:39:54,392] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-08-30 11:39:54,418] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-08-30 11:39:54,419] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[33m[2022-08-30 11:39:54,426] [ WARNING][0m - Encoder has already set as lstm, change `prompt_encoder` will reset parameters.[0m
[33m[2022-08-30 11:39:54,431] [ WARNING][0m - No soft tokens in template. Use ManualTemplate for better performance.[0m
[33m[2022-08-30 11:39:54,431] [ WARNING][0m - No soft tokens in template. Use ManualTemplate for better performance.[0m
[32m[2022-08-30 11:39:54,431] [    INFO][0m - Using template: [{'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'sep': None}, {'add_prefix_space': '', 'text': 'text_b'}][0m
2022-08-30 11:39:54,433 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-30 11:39:54,566] [    INFO][0m - ============================================================[0m
[32m[2022-08-30 11:39:54,566] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-30 11:39:54,566] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-30 11:39:54,567] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-30 11:39:54,567] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-30 11:39:54,567] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-30 11:39:54,567] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-30 11:39:54,567] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-30 11:39:54,567] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-30 11:39:54,567] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-30 11:39:54,567] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-30 11:39:54,567] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-30 11:39:54,567] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-30 11:39:54,567] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-30 11:39:54,568] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-30 11:39:54,568] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-30 11:39:54,568] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-30 11:39:54,568] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-30 11:39:54,568] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-30 11:39:54,568] [    INFO][0m - eval_steps                    :50[0m
[32m[2022-08-30 11:39:54,568] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-30 11:39:54,568] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-30 11:39:54,568] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-30 11:39:54,568] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-30 11:39:54,568] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-30 11:39:54,568] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-30 11:39:54,568] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-30 11:39:54,569] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-30 11:39:54,569] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-30 11:39:54,569] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-30 11:39:54,569] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-30 11:39:54,569] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-30 11:39:54,569] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-30 11:39:54,569] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-30 11:39:54,569] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-30 11:39:54,569] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-30 11:39:54,569] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-30 11:39:54,569] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug30_11-39-51_instance-3bwob41y-01[0m
[32m[2022-08-30 11:39:54,569] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-30 11:39:54,569] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-30 11:39:54,570] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-30 11:39:54,570] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-30 11:39:54,570] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-30 11:39:54,570] [    INFO][0m - max_seq_length                :64[0m
[32m[2022-08-30 11:39:54,570] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-30 11:39:54,570] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-30 11:39:54,570] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-30 11:39:54,570] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-30 11:39:54,570] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-08-30 11:39:54,570] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-30 11:39:54,570] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-30 11:39:54,570] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-30 11:39:54,570] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-30 11:39:54,571] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-30 11:39:54,571] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-30 11:39:54,571] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-30 11:39:54,571] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-30 11:39:54,571] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-30 11:39:54,571] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-30 11:39:54,571] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-30 11:39:54,571] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-30 11:39:54,571] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-30 11:39:54,571] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-30 11:39:54,571] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-30 11:39:54,571] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-30 11:39:54,571] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-30 11:39:54,572] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-30 11:39:54,572] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-30 11:39:54,572] [    INFO][0m - save_steps                    :50[0m
[32m[2022-08-30 11:39:54,572] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-30 11:39:54,572] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-30 11:39:54,572] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-30 11:39:54,572] [    INFO][0m - seed                          :42[0m
[32m[2022-08-30 11:39:54,572] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-30 11:39:54,572] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-30 11:39:54,572] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-30 11:39:54,572] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-30 11:39:54,572] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-30 11:39:54,572] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-30 11:39:54,572] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-30 11:39:54,573] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-30 11:39:54,573] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-30 11:39:54,573] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-30 11:39:54,573] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-30 11:39:54,573] [    INFO][0m - [0m
[32m[2022-08-30 11:39:54,575] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-30 11:39:54,575] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 11:39:54,575] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-08-30 11:39:54,575] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-30 11:39:54,575] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-30 11:39:54,575] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-30 11:39:54,575] [    INFO][0m -   Total optimization steps = 1000.0[0m
[32m[2022-08-30 11:39:54,576] [    INFO][0m -   Total num train samples = 8000[0m
[32m[2022-08-30 11:39:56,261] [    INFO][0m - loss: 1.14555016, learning_rate: 2.97e-05, global_step: 10, interval_runtime: 1.6842, interval_samples_per_second: 4.75, interval_steps_per_second: 5.938, epoch: 0.5[0m
[32m[2022-08-30 11:39:56,782] [    INFO][0m - loss: 1.16299076, learning_rate: 2.94e-05, global_step: 20, interval_runtime: 0.5216, interval_samples_per_second: 15.336, interval_steps_per_second: 19.17, epoch: 1.0[0m
[32m[2022-08-30 11:39:57,388] [    INFO][0m - loss: 1.03014107, learning_rate: 2.91e-05, global_step: 30, interval_runtime: 0.6064, interval_samples_per_second: 13.194, interval_steps_per_second: 16.492, epoch: 1.5[0m
[32m[2022-08-30 11:39:57,918] [    INFO][0m - loss: 1.06009235, learning_rate: 2.88e-05, global_step: 40, interval_runtime: 0.5292, interval_samples_per_second: 15.117, interval_steps_per_second: 18.897, epoch: 2.0[0m
[32m[2022-08-30 11:39:58,560] [    INFO][0m - loss: 0.99096127, learning_rate: 2.8499999999999998e-05, global_step: 50, interval_runtime: 0.6423, interval_samples_per_second: 12.455, interval_steps_per_second: 15.569, epoch: 2.5[0m
[32m[2022-08-30 11:39:58,561] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 11:39:58,561] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 11:39:58,561] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 11:39:58,561] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 11:39:58,561] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 11:39:58,897] [    INFO][0m - eval_loss: 1.131166696548462, eval_accuracy: 0.31875, eval_runtime: 0.3354, eval_samples_per_second: 477.048, eval_steps_per_second: 14.908, epoch: 2.5[0m
[32m[2022-08-30 11:39:58,897] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-50[0m
[32m[2022-08-30 11:39:58,897] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 11:40:02,614] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-50/tokenizer_config.json[0m
[32m[2022-08-30 11:40:02,614] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-50/special_tokens_map.json[0m
[32m[2022-08-30 11:40:09,870] [    INFO][0m - loss: 0.97716465, learning_rate: 2.8199999999999998e-05, global_step: 60, interval_runtime: 11.3098, interval_samples_per_second: 0.707, interval_steps_per_second: 0.884, epoch: 3.0[0m
[32m[2022-08-30 11:40:10,554] [    INFO][0m - loss: 0.76377854, learning_rate: 2.79e-05, global_step: 70, interval_runtime: 0.6844, interval_samples_per_second: 11.689, interval_steps_per_second: 14.611, epoch: 3.5[0m
[32m[2022-08-30 11:40:11,110] [    INFO][0m - loss: 0.84395838, learning_rate: 2.7600000000000003e-05, global_step: 80, interval_runtime: 0.5561, interval_samples_per_second: 14.385, interval_steps_per_second: 17.981, epoch: 4.0[0m
[32m[2022-08-30 11:40:11,825] [    INFO][0m - loss: 0.63819799, learning_rate: 2.7300000000000003e-05, global_step: 90, interval_runtime: 0.7148, interval_samples_per_second: 11.191, interval_steps_per_second: 13.989, epoch: 4.5[0m
[32m[2022-08-30 11:40:12,396] [    INFO][0m - loss: 0.64413171, learning_rate: 2.7000000000000002e-05, global_step: 100, interval_runtime: 0.5703, interval_samples_per_second: 14.028, interval_steps_per_second: 17.534, epoch: 5.0[0m
[32m[2022-08-30 11:40:12,396] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 11:40:12,396] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 11:40:12,397] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 11:40:12,397] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 11:40:12,397] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 11:40:12,796] [    INFO][0m - eval_loss: 1.412204384803772, eval_accuracy: 0.30625, eval_runtime: 0.3995, eval_samples_per_second: 400.477, eval_steps_per_second: 12.515, epoch: 5.0[0m
[32m[2022-08-30 11:40:12,797] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-30 11:40:12,797] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 11:40:15,983] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-30 11:40:15,984] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-30 11:40:20,499] [    INFO][0m - loss: 0.38504472, learning_rate: 2.6700000000000002e-05, global_step: 110, interval_runtime: 8.1037, interval_samples_per_second: 0.987, interval_steps_per_second: 1.234, epoch: 5.5[0m
[32m[2022-08-30 11:40:21,087] [    INFO][0m - loss: 0.37914264, learning_rate: 2.64e-05, global_step: 120, interval_runtime: 0.5882, interval_samples_per_second: 13.6, interval_steps_per_second: 17.0, epoch: 6.0[0m
[32m[2022-08-30 11:40:21,881] [    INFO][0m - loss: 0.25134342, learning_rate: 2.61e-05, global_step: 130, interval_runtime: 0.7934, interval_samples_per_second: 10.083, interval_steps_per_second: 12.604, epoch: 6.5[0m
[32m[2022-08-30 11:40:22,482] [    INFO][0m - loss: 0.24708035, learning_rate: 2.58e-05, global_step: 140, interval_runtime: 0.6013, interval_samples_per_second: 13.304, interval_steps_per_second: 16.63, epoch: 7.0[0m
[32m[2022-08-30 11:40:23,303] [    INFO][0m - loss: 0.16751019, learning_rate: 2.55e-05, global_step: 150, interval_runtime: 0.821, interval_samples_per_second: 9.744, interval_steps_per_second: 12.18, epoch: 7.5[0m
[32m[2022-08-30 11:40:23,304] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 11:40:23,304] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 11:40:23,304] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 11:40:23,304] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 11:40:23,304] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 11:40:23,797] [    INFO][0m - eval_loss: 2.154376268386841, eval_accuracy: 0.36875, eval_runtime: 0.493, eval_samples_per_second: 324.546, eval_steps_per_second: 10.142, epoch: 7.5[0m
[32m[2022-08-30 11:40:23,798] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-150[0m
[32m[2022-08-30 11:40:23,798] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 11:40:27,378] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-150/tokenizer_config.json[0m
[32m[2022-08-30 11:40:27,378] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-150/special_tokens_map.json[0m
[32m[2022-08-30 11:40:31,642] [    INFO][0m - loss: 0.09290655, learning_rate: 2.52e-05, global_step: 160, interval_runtime: 8.3388, interval_samples_per_second: 0.959, interval_steps_per_second: 1.199, epoch: 8.0[0m
[32m[2022-08-30 11:40:32,560] [    INFO][0m - loss: 0.09409845, learning_rate: 2.49e-05, global_step: 170, interval_runtime: 0.9183, interval_samples_per_second: 8.712, interval_steps_per_second: 10.89, epoch: 8.5[0m
[32m[2022-08-30 11:40:33,256] [    INFO][0m - loss: 0.07230685, learning_rate: 2.4599999999999998e-05, global_step: 180, interval_runtime: 0.6958, interval_samples_per_second: 11.497, interval_steps_per_second: 14.371, epoch: 9.0[0m
[32m[2022-08-30 11:40:34,228] [    INFO][0m - loss: 0.04420442, learning_rate: 2.43e-05, global_step: 190, interval_runtime: 0.9715, interval_samples_per_second: 8.234, interval_steps_per_second: 10.293, epoch: 9.5[0m
[32m[2022-08-30 11:40:34,974] [    INFO][0m - loss: 0.02195474, learning_rate: 2.4e-05, global_step: 200, interval_runtime: 0.7461, interval_samples_per_second: 10.723, interval_steps_per_second: 13.403, epoch: 10.0[0m
[32m[2022-08-30 11:40:34,975] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 11:40:34,975] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 11:40:34,975] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 11:40:34,975] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 11:40:34,975] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 11:40:35,497] [    INFO][0m - eval_loss: 3.158432722091675, eval_accuracy: 0.3375, eval_runtime: 0.5217, eval_samples_per_second: 306.683, eval_steps_per_second: 9.584, epoch: 10.0[0m
[32m[2022-08-30 11:40:35,497] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-30 11:40:35,498] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 11:40:38,994] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-30 11:40:38,995] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-30 11:40:43,901] [    INFO][0m - loss: 0.01704589, learning_rate: 2.37e-05, global_step: 210, interval_runtime: 8.9267, interval_samples_per_second: 0.896, interval_steps_per_second: 1.12, epoch: 10.5[0m
[32m[2022-08-30 11:40:44,549] [    INFO][0m - loss: 0.01067727, learning_rate: 2.3400000000000003e-05, global_step: 220, interval_runtime: 0.6481, interval_samples_per_second: 12.344, interval_steps_per_second: 15.43, epoch: 11.0[0m
[32m[2022-08-30 11:40:45,515] [    INFO][0m - loss: 0.00310233, learning_rate: 2.3100000000000002e-05, global_step: 230, interval_runtime: 0.966, interval_samples_per_second: 8.281, interval_steps_per_second: 10.352, epoch: 11.5[0m
[32m[2022-08-30 11:40:46,175] [    INFO][0m - loss: 0.0030223, learning_rate: 2.2800000000000002e-05, global_step: 240, interval_runtime: 0.6607, interval_samples_per_second: 12.109, interval_steps_per_second: 15.137, epoch: 12.0[0m
[32m[2022-08-30 11:40:47,179] [    INFO][0m - loss: 0.00273959, learning_rate: 2.25e-05, global_step: 250, interval_runtime: 1.004, interval_samples_per_second: 7.968, interval_steps_per_second: 9.96, epoch: 12.5[0m
[32m[2022-08-30 11:40:47,180] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 11:40:47,180] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 11:40:47,180] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 11:40:47,180] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 11:40:47,180] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 11:40:47,761] [    INFO][0m - eval_loss: 3.594345808029175, eval_accuracy: 0.34375, eval_runtime: 0.5806, eval_samples_per_second: 275.581, eval_steps_per_second: 8.612, epoch: 12.5[0m
[32m[2022-08-30 11:40:47,761] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-250[0m
[32m[2022-08-30 11:40:47,762] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 11:40:50,924] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-250/tokenizer_config.json[0m
[32m[2022-08-30 11:40:50,924] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-250/special_tokens_map.json[0m
[32m[2022-08-30 11:40:55,305] [    INFO][0m - loss: 0.00204812, learning_rate: 2.22e-05, global_step: 260, interval_runtime: 8.1262, interval_samples_per_second: 0.984, interval_steps_per_second: 1.231, epoch: 13.0[0m
[32m[2022-08-30 11:40:56,327] [    INFO][0m - loss: 0.00193685, learning_rate: 2.19e-05, global_step: 270, interval_runtime: 1.0218, interval_samples_per_second: 7.83, interval_steps_per_second: 9.787, epoch: 13.5[0m
[32m[2022-08-30 11:40:57,013] [    INFO][0m - loss: 0.00163992, learning_rate: 2.16e-05, global_step: 280, interval_runtime: 0.6863, interval_samples_per_second: 11.657, interval_steps_per_second: 14.572, epoch: 14.0[0m
[32m[2022-08-30 11:40:58,093] [    INFO][0m - loss: 0.0063401, learning_rate: 2.13e-05, global_step: 290, interval_runtime: 1.0792, interval_samples_per_second: 7.413, interval_steps_per_second: 9.266, epoch: 14.5[0m
[32m[2022-08-30 11:40:58,805] [    INFO][0m - loss: 0.00152737, learning_rate: 2.1e-05, global_step: 300, interval_runtime: 0.7124, interval_samples_per_second: 11.229, interval_steps_per_second: 14.036, epoch: 15.0[0m
[32m[2022-08-30 11:40:58,806] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 11:40:58,806] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 11:40:58,806] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 11:40:58,806] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 11:40:58,806] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 11:40:59,447] [    INFO][0m - eval_loss: 3.8249804973602295, eval_accuracy: 0.325, eval_runtime: 0.6405, eval_samples_per_second: 249.801, eval_steps_per_second: 7.806, epoch: 15.0[0m
[32m[2022-08-30 11:40:59,447] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-30 11:40:59,447] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 11:41:02,936] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-30 11:41:02,936] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-30 11:41:07,989] [    INFO][0m - loss: 0.02041915, learning_rate: 2.07e-05, global_step: 310, interval_runtime: 9.1839, interval_samples_per_second: 0.871, interval_steps_per_second: 1.089, epoch: 15.5[0m
[32m[2022-08-30 11:41:08,697] [    INFO][0m - loss: 0.00131334, learning_rate: 2.04e-05, global_step: 320, interval_runtime: 0.7079, interval_samples_per_second: 11.301, interval_steps_per_second: 14.127, epoch: 16.0[0m
[32m[2022-08-30 11:41:09,931] [    INFO][0m - loss: 0.00263929, learning_rate: 2.01e-05, global_step: 330, interval_runtime: 1.2331, interval_samples_per_second: 6.487, interval_steps_per_second: 8.109, epoch: 16.5[0m
[32m[2022-08-30 11:41:10,665] [    INFO][0m - loss: 0.0012145, learning_rate: 1.98e-05, global_step: 340, interval_runtime: 0.7352, interval_samples_per_second: 10.881, interval_steps_per_second: 13.602, epoch: 17.0[0m
[32m[2022-08-30 11:41:11,846] [    INFO][0m - loss: 0.00137975, learning_rate: 1.95e-05, global_step: 350, interval_runtime: 1.1803, interval_samples_per_second: 6.778, interval_steps_per_second: 8.472, epoch: 17.5[0m
[32m[2022-08-30 11:41:11,846] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 11:41:11,846] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 11:41:11,846] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 11:41:11,846] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 11:41:11,846] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 11:41:12,559] [    INFO][0m - eval_loss: 3.9285378456115723, eval_accuracy: 0.39375, eval_runtime: 0.7122, eval_samples_per_second: 224.647, eval_steps_per_second: 7.02, epoch: 17.5[0m
[32m[2022-08-30 11:41:12,560] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-350[0m
[32m[2022-08-30 11:41:12,560] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 11:41:15,991] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-350/tokenizer_config.json[0m
[32m[2022-08-30 11:41:15,991] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-350/special_tokens_map.json[0m
[32m[2022-08-30 11:41:20,372] [    INFO][0m - loss: 0.00100774, learning_rate: 1.9200000000000003e-05, global_step: 360, interval_runtime: 8.5257, interval_samples_per_second: 0.938, interval_steps_per_second: 1.173, epoch: 18.0[0m
[32m[2022-08-30 11:41:21,606] [    INFO][0m - loss: 0.00096352, learning_rate: 1.8900000000000002e-05, global_step: 370, interval_runtime: 1.2347, interval_samples_per_second: 6.479, interval_steps_per_second: 8.099, epoch: 18.5[0m
[32m[2022-08-30 11:41:22,368] [    INFO][0m - loss: 0.00090288, learning_rate: 1.86e-05, global_step: 380, interval_runtime: 0.7614, interval_samples_per_second: 10.507, interval_steps_per_second: 13.134, epoch: 19.0[0m
[32m[2022-08-30 11:41:23,621] [    INFO][0m - loss: 0.00088847, learning_rate: 1.83e-05, global_step: 390, interval_runtime: 1.2535, interval_samples_per_second: 6.382, interval_steps_per_second: 7.978, epoch: 19.5[0m
[32m[2022-08-30 11:41:24,370] [    INFO][0m - loss: 0.00082797, learning_rate: 1.8e-05, global_step: 400, interval_runtime: 0.749, interval_samples_per_second: 10.68, interval_steps_per_second: 13.35, epoch: 20.0[0m
[32m[2022-08-30 11:41:24,370] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 11:41:24,371] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 11:41:24,371] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 11:41:24,371] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 11:41:24,371] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 11:41:25,115] [    INFO][0m - eval_loss: 4.02407693862915, eval_accuracy: 0.33125, eval_runtime: 0.7439, eval_samples_per_second: 215.091, eval_steps_per_second: 6.722, epoch: 20.0[0m
[32m[2022-08-30 11:41:25,115] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-30 11:41:25,115] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 11:41:28,217] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-30 11:41:28,217] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-30 11:41:33,466] [    INFO][0m - loss: 0.00237156, learning_rate: 1.77e-05, global_step: 410, interval_runtime: 9.0961, interval_samples_per_second: 0.879, interval_steps_per_second: 1.099, epoch: 20.5[0m
[32m[2022-08-30 11:41:34,235] [    INFO][0m - loss: 0.00086662, learning_rate: 1.74e-05, global_step: 420, interval_runtime: 0.7688, interval_samples_per_second: 10.406, interval_steps_per_second: 13.007, epoch: 21.0[0m
[32m[2022-08-30 11:41:35,550] [    INFO][0m - loss: 0.00082973, learning_rate: 1.71e-05, global_step: 430, interval_runtime: 1.3156, interval_samples_per_second: 6.081, interval_steps_per_second: 7.601, epoch: 21.5[0m
[32m[2022-08-30 11:41:36,338] [    INFO][0m - loss: 0.00068931, learning_rate: 1.6800000000000002e-05, global_step: 440, interval_runtime: 0.7875, interval_samples_per_second: 10.159, interval_steps_per_second: 12.698, epoch: 22.0[0m
[32m[2022-08-30 11:41:37,716] [    INFO][0m - loss: 0.00224238, learning_rate: 1.65e-05, global_step: 450, interval_runtime: 1.3779, interval_samples_per_second: 5.806, interval_steps_per_second: 7.258, epoch: 22.5[0m
[32m[2022-08-30 11:41:37,716] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 11:41:37,716] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 11:41:37,717] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 11:41:37,717] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 11:41:37,717] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 11:41:38,529] [    INFO][0m - eval_loss: 3.992410182952881, eval_accuracy: 0.35, eval_runtime: 0.8118, eval_samples_per_second: 197.096, eval_steps_per_second: 6.159, epoch: 22.5[0m
[32m[2022-08-30 11:41:38,529] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-450[0m
[32m[2022-08-30 11:41:38,529] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 11:41:43,200] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-450/tokenizer_config.json[0m
[32m[2022-08-30 11:41:43,201] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-450/special_tokens_map.json[0m
[32m[2022-08-30 11:41:47,940] [    INFO][0m - loss: 0.00247003, learning_rate: 1.62e-05, global_step: 460, interval_runtime: 10.2239, interval_samples_per_second: 0.782, interval_steps_per_second: 0.978, epoch: 23.0[0m
[32m[2022-08-30 11:41:49,343] [    INFO][0m - loss: 0.09602314, learning_rate: 1.59e-05, global_step: 470, interval_runtime: 1.4029, interval_samples_per_second: 5.702, interval_steps_per_second: 7.128, epoch: 23.5[0m
[32m[2022-08-30 11:41:50,159] [    INFO][0m - loss: 0.00073419, learning_rate: 1.56e-05, global_step: 480, interval_runtime: 0.8162, interval_samples_per_second: 9.802, interval_steps_per_second: 12.252, epoch: 24.0[0m
[32m[2022-08-30 11:41:51,587] [    INFO][0m - loss: 0.00088127, learning_rate: 1.53e-05, global_step: 490, interval_runtime: 1.4276, interval_samples_per_second: 5.604, interval_steps_per_second: 7.005, epoch: 24.5[0m
[32m[2022-08-30 11:41:52,419] [    INFO][0m - loss: 0.01701463, learning_rate: 1.5e-05, global_step: 500, interval_runtime: 0.8322, interval_samples_per_second: 9.613, interval_steps_per_second: 12.017, epoch: 25.0[0m
[32m[2022-08-30 11:41:52,420] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 11:41:52,420] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 11:41:52,420] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 11:41:52,420] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 11:41:52,420] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 11:41:53,320] [    INFO][0m - eval_loss: 4.237247943878174, eval_accuracy: 0.34375, eval_runtime: 0.8995, eval_samples_per_second: 177.874, eval_steps_per_second: 5.559, epoch: 25.0[0m
[32m[2022-08-30 11:41:53,320] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-08-30 11:41:53,320] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 11:41:56,577] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-08-30 11:41:56,578] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-08-30 11:42:01,849] [    INFO][0m - loss: 0.0007633, learning_rate: 1.47e-05, global_step: 510, interval_runtime: 9.4298, interval_samples_per_second: 0.848, interval_steps_per_second: 1.06, epoch: 25.5[0m
[32m[2022-08-30 11:42:02,706] [    INFO][0m - loss: 0.06844591, learning_rate: 1.44e-05, global_step: 520, interval_runtime: 0.857, interval_samples_per_second: 9.335, interval_steps_per_second: 11.669, epoch: 26.0[0m
[32m[2022-08-30 11:42:04,209] [    INFO][0m - loss: 0.09763434, learning_rate: 1.4099999999999999e-05, global_step: 530, interval_runtime: 1.5033, interval_samples_per_second: 5.322, interval_steps_per_second: 6.652, epoch: 26.5[0m
[32m[2022-08-30 11:42:05,039] [    INFO][0m - loss: 0.00082724, learning_rate: 1.3800000000000002e-05, global_step: 540, interval_runtime: 0.8307, interval_samples_per_second: 9.631, interval_steps_per_second: 12.039, epoch: 27.0[0m
[32m[2022-08-30 11:42:06,546] [    INFO][0m - loss: 0.00074585, learning_rate: 1.3500000000000001e-05, global_step: 550, interval_runtime: 1.507, interval_samples_per_second: 5.308, interval_steps_per_second: 6.636, epoch: 27.5[0m
[32m[2022-08-30 11:42:06,547] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 11:42:06,547] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 11:42:06,547] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 11:42:06,547] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 11:42:06,547] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 11:42:07,550] [    INFO][0m - eval_loss: 4.352628231048584, eval_accuracy: 0.35, eval_runtime: 1.0023, eval_samples_per_second: 159.634, eval_steps_per_second: 4.989, epoch: 27.5[0m
[32m[2022-08-30 11:42:07,550] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-550[0m
[32m[2022-08-30 11:42:07,550] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 11:42:10,753] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-550/tokenizer_config.json[0m
[32m[2022-08-30 11:42:10,754] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-550/special_tokens_map.json[0m
[32m[2022-08-30 11:42:16,966] [    INFO][0m - loss: 0.02185274, learning_rate: 1.32e-05, global_step: 560, interval_runtime: 10.4197, interval_samples_per_second: 0.768, interval_steps_per_second: 0.96, epoch: 28.0[0m
[32m[2022-08-30 11:42:18,500] [    INFO][0m - loss: 0.0006983, learning_rate: 1.29e-05, global_step: 570, interval_runtime: 1.5334, interval_samples_per_second: 5.217, interval_steps_per_second: 6.521, epoch: 28.5[0m
[32m[2022-08-30 11:42:19,393] [    INFO][0m - loss: 0.00252883, learning_rate: 1.26e-05, global_step: 580, interval_runtime: 0.8937, interval_samples_per_second: 8.951, interval_steps_per_second: 11.189, epoch: 29.0[0m
[32m[2022-08-30 11:42:21,010] [    INFO][0m - loss: 0.00053734, learning_rate: 1.2299999999999999e-05, global_step: 590, interval_runtime: 1.6168, interval_samples_per_second: 4.948, interval_steps_per_second: 6.185, epoch: 29.5[0m
[32m[2022-08-30 11:42:21,925] [    INFO][0m - loss: 0.00053697, learning_rate: 1.2e-05, global_step: 600, interval_runtime: 0.9145, interval_samples_per_second: 8.748, interval_steps_per_second: 10.935, epoch: 30.0[0m
[32m[2022-08-30 11:42:21,925] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 11:42:21,925] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 11:42:21,926] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 11:42:21,926] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 11:42:21,926] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 11:42:22,927] [    INFO][0m - eval_loss: 4.231139183044434, eval_accuracy: 0.3375, eval_runtime: 1.0006, eval_samples_per_second: 159.902, eval_steps_per_second: 4.997, epoch: 30.0[0m
[32m[2022-08-30 11:42:22,927] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-08-30 11:42:22,927] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 11:42:26,158] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-08-30 11:42:26,158] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-08-30 11:42:31,409] [    INFO][0m - loss: 0.02408724, learning_rate: 1.1700000000000001e-05, global_step: 610, interval_runtime: 9.4847, interval_samples_per_second: 0.843, interval_steps_per_second: 1.054, epoch: 30.5[0m
[32m[2022-08-30 11:42:32,305] [    INFO][0m - loss: 0.00058179, learning_rate: 1.1400000000000001e-05, global_step: 620, interval_runtime: 0.8961, interval_samples_per_second: 8.928, interval_steps_per_second: 11.16, epoch: 31.0[0m
[32m[2022-08-30 11:42:33,961] [    INFO][0m - loss: 0.0065103, learning_rate: 1.11e-05, global_step: 630, interval_runtime: 1.656, interval_samples_per_second: 4.831, interval_steps_per_second: 6.039, epoch: 31.5[0m
[32m[2022-08-30 11:42:34,874] [    INFO][0m - loss: 0.00046124, learning_rate: 1.08e-05, global_step: 640, interval_runtime: 0.913, interval_samples_per_second: 8.762, interval_steps_per_second: 10.953, epoch: 32.0[0m
[32m[2022-08-30 11:42:36,592] [    INFO][0m - loss: 0.00046264, learning_rate: 1.05e-05, global_step: 650, interval_runtime: 1.7171, interval_samples_per_second: 4.659, interval_steps_per_second: 5.824, epoch: 32.5[0m
[32m[2022-08-30 11:42:36,592] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 11:42:36,592] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 11:42:36,593] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 11:42:36,593] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 11:42:36,593] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 11:42:37,641] [    INFO][0m - eval_loss: 4.125781059265137, eval_accuracy: 0.3875, eval_runtime: 1.048, eval_samples_per_second: 152.67, eval_steps_per_second: 4.771, epoch: 32.5[0m
[32m[2022-08-30 11:42:37,641] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-650[0m
[32m[2022-08-30 11:42:37,641] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 11:42:40,777] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-650/tokenizer_config.json[0m
[32m[2022-08-30 11:42:40,777] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-650/special_tokens_map.json[0m
[32m[2022-08-30 11:42:44,465] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-30 11:42:44,466] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-350 (score: 0.39375).[0m
[32m[2022-08-30 11:42:45,749] [    INFO][0m - train_runtime: 171.1722, train_samples_per_second: 46.737, train_steps_per_second: 5.842, train_loss: 0.17607680744061677, epoch: 32.5[0m
[32m[2022-08-30 11:42:45,750] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-30 11:42:45,751] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 11:42:49,137] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-30 11:42:49,141] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-30 11:42:49,143] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-30 11:42:49,143] [    INFO][0m -   epoch                    =       32.5[0m
[32m[2022-08-30 11:42:49,143] [    INFO][0m -   train_loss               =     0.1761[0m
[32m[2022-08-30 11:42:49,144] [    INFO][0m -   train_runtime            = 0:02:51.17[0m
[32m[2022-08-30 11:42:49,144] [    INFO][0m -   train_samples_per_second =     46.737[0m
[32m[2022-08-30 11:42:49,144] [    INFO][0m -   train_steps_per_second   =      5.842[0m
[32m[2022-08-30 11:42:49,148] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-30 11:42:49,148] [    INFO][0m -   Num examples = 2520[0m
[32m[2022-08-30 11:42:49,148] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 11:42:49,148] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 11:42:49,148] [    INFO][0m -   Total prediction steps = 79[0m
[32m[2022-08-30 11:43:06,413] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-30 11:43:06,414] [    INFO][0m -   test_accuracy           =     0.3631[0m
[32m[2022-08-30 11:43:06,414] [    INFO][0m -   test_loss               =     4.0507[0m
[32m[2022-08-30 11:43:06,414] [    INFO][0m -   test_runtime            = 0:00:17.26[0m
[32m[2022-08-30 11:43:06,414] [    INFO][0m -   test_samples_per_second =    145.961[0m
[32m[2022-08-30 11:43:06,414] [    INFO][0m -   test_steps_per_second   =      4.576[0m
[32m[2022-08-30 11:43:06,414] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-30 11:43:06,414] [    INFO][0m -   Num examples = 3000[0m
[32m[2022-08-30 11:43:06,414] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 11:43:06,415] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 11:43:06,415] [    INFO][0m -   Total prediction steps = 94[0m
[32m[2022-08-30 11:43:30,929] [    INFO][0m - Predictions for ocnlif saved to ./fewclue_submit_examples.[0m
metric <function main.<locals>.compute_metrics at 0x7fdca7faf280>
