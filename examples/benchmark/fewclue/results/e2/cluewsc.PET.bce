[33m[2022-08-30 11:14:44,653] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-30 11:14:44,653] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-30 11:14:44,653] [    INFO][0m - ============================================================[0m
[32m[2022-08-30 11:14:44,653] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-30 11:14:44,654] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-30 11:14:44,654] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-30 11:14:44,654] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-30 11:14:44,654] [    INFO][0m - early_stop_patience           :6[0m
[32m[2022-08-30 11:14:44,654] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-30 11:14:44,654] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-08-30 11:14:44,654] [    INFO][0m - [0m
[32m[2022-08-30 11:14:44,654] [    INFO][0m - ============================================================[0m
[32m[2022-08-30 11:14:44,654] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-30 11:14:44,654] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-30 11:14:44,654] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-30 11:14:44,654] [    INFO][0m - prompt                        :{'text':'text_a'}[0m
[32m[2022-08-30 11:14:44,655] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-30 11:14:44,655] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-30 11:14:44,655] [    INFO][0m - task_name                     :cluewsc[0m
[32m[2022-08-30 11:14:44,655] [    INFO][0m - [0m
[32m[2022-08-30 11:14:44,655] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0830 11:14:44.656555 23842 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0830 11:14:44.660789 23842 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-30 11:14:47,617] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-08-30 11:14:47,642] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-08-30 11:14:47,642] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[33m[2022-08-30 11:14:47,650] [ WARNING][0m - Encoder has already set as lstm, change `prompt_encoder` will reset parameters.[0m
[33m[2022-08-30 11:14:47,654] [ WARNING][0m - No soft tokens in template. Use ManualTemplate for better performance.[0m
[33m[2022-08-30 11:14:47,655] [ WARNING][0m - No soft tokens in template. Use ManualTemplate for better performance.[0m
[32m[2022-08-30 11:14:47,655] [    INFO][0m - Using template: [{'add_prefix_space': '', 'text': 'text_a'}][0m
2022-08-30 11:14:47,656 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-30 11:14:47,760] [    INFO][0m - ============================================================[0m
[32m[2022-08-30 11:14:47,760] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-30 11:14:47,760] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-30 11:14:47,760] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-30 11:14:47,760] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-30 11:14:47,760] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-30 11:14:47,761] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-30 11:14:47,761] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-30 11:14:47,761] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-30 11:14:47,761] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-30 11:14:47,761] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-30 11:14:47,761] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-30 11:14:47,761] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-30 11:14:47,761] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-30 11:14:47,761] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-30 11:14:47,761] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-30 11:14:47,761] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-30 11:14:47,761] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-30 11:14:47,762] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-30 11:14:47,762] [    INFO][0m - eval_steps                    :50[0m
[32m[2022-08-30 11:14:47,762] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-30 11:14:47,762] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-30 11:14:47,762] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-30 11:14:47,762] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-30 11:14:47,762] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-30 11:14:47,762] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-30 11:14:47,762] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-30 11:14:47,762] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-30 11:14:47,762] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-30 11:14:47,762] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-30 11:14:47,763] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-30 11:14:47,763] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-30 11:14:47,763] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-30 11:14:47,763] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-30 11:14:47,763] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-30 11:14:47,763] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-30 11:14:47,763] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-30 11:14:47,763] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug30_11-14-44_instance-3bwob41y-01[0m
[32m[2022-08-30 11:14:47,763] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-30 11:14:47,763] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-30 11:14:47,763] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-30 11:14:47,763] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-30 11:14:47,763] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-30 11:14:47,763] [    INFO][0m - max_seq_length                :128[0m
[32m[2022-08-30 11:14:47,764] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-30 11:14:47,764] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-30 11:14:47,764] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-30 11:14:47,764] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-30 11:14:47,764] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-08-30 11:14:47,764] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-30 11:14:47,764] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-30 11:14:47,764] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-30 11:14:47,764] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-30 11:14:47,764] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-30 11:14:47,764] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-30 11:14:47,764] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-30 11:14:47,764] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-30 11:14:47,765] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-30 11:14:47,765] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-30 11:14:47,765] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-30 11:14:47,765] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-30 11:14:47,765] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-30 11:14:47,765] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-30 11:14:47,765] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-30 11:14:47,765] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-30 11:14:47,765] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-30 11:14:47,765] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-30 11:14:47,765] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-30 11:14:47,765] [    INFO][0m - save_steps                    :50[0m
[32m[2022-08-30 11:14:47,765] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-30 11:14:47,766] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-30 11:14:47,766] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-30 11:14:47,766] [    INFO][0m - seed                          :42[0m
[32m[2022-08-30 11:14:47,766] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-30 11:14:47,766] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-30 11:14:47,766] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-30 11:14:47,766] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-30 11:14:47,766] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-30 11:14:47,766] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-30 11:14:47,766] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-30 11:14:47,766] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-30 11:14:47,766] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-30 11:14:47,766] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-30 11:14:47,767] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-30 11:14:47,767] [    INFO][0m - [0m
[32m[2022-08-30 11:14:47,769] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-30 11:14:47,769] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 11:14:47,769] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-08-30 11:14:47,769] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-30 11:14:47,769] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-30 11:14:47,769] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-30 11:14:47,769] [    INFO][0m -   Total optimization steps = 1000.0[0m
[32m[2022-08-30 11:14:47,769] [    INFO][0m -   Total num train samples = 8000[0m
[32m[2022-08-30 11:14:49,612] [    INFO][0m - loss: 0.76466036, learning_rate: 2.97e-05, global_step: 10, interval_runtime: 1.8412, interval_samples_per_second: 4.345, interval_steps_per_second: 5.431, epoch: 0.5[0m
[32m[2022-08-30 11:14:50,307] [    INFO][0m - loss: 0.70291204, learning_rate: 2.94e-05, global_step: 20, interval_runtime: 0.6951, interval_samples_per_second: 11.509, interval_steps_per_second: 14.386, epoch: 1.0[0m
[32m[2022-08-30 11:14:51,074] [    INFO][0m - loss: 0.68841515, learning_rate: 2.91e-05, global_step: 30, interval_runtime: 0.7681, interval_samples_per_second: 10.415, interval_steps_per_second: 13.019, epoch: 1.5[0m
[32m[2022-08-30 11:14:51,770] [    INFO][0m - loss: 0.64754243, learning_rate: 2.88e-05, global_step: 40, interval_runtime: 0.6953, interval_samples_per_second: 11.507, interval_steps_per_second: 14.383, epoch: 2.0[0m
[32m[2022-08-30 11:14:52,590] [    INFO][0m - loss: 0.66613188, learning_rate: 2.8499999999999998e-05, global_step: 50, interval_runtime: 0.8202, interval_samples_per_second: 9.753, interval_steps_per_second: 12.192, epoch: 2.5[0m
[32m[2022-08-30 11:14:52,590] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 11:14:52,591] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-30 11:14:52,591] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 11:14:52,591] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 11:14:52,591] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 11:14:53,159] [    INFO][0m - eval_loss: 0.7370802164077759, eval_accuracy: 0.49056603773584906, eval_runtime: 0.5679, eval_samples_per_second: 279.968, eval_steps_per_second: 8.804, epoch: 2.5[0m
[32m[2022-08-30 11:14:53,159] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-50[0m
[32m[2022-08-30 11:14:53,159] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 11:14:56,651] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-50/tokenizer_config.json[0m
[32m[2022-08-30 11:14:56,652] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-50/special_tokens_map.json[0m
[32m[2022-08-30 11:15:01,680] [    INFO][0m - loss: 0.67329178, learning_rate: 2.8199999999999998e-05, global_step: 60, interval_runtime: 9.0893, interval_samples_per_second: 0.88, interval_steps_per_second: 1.1, epoch: 3.0[0m
[32m[2022-08-30 11:15:02,573] [    INFO][0m - loss: 0.52221336, learning_rate: 2.79e-05, global_step: 70, interval_runtime: 0.8933, interval_samples_per_second: 8.956, interval_steps_per_second: 11.195, epoch: 3.5[0m
[32m[2022-08-30 11:15:03,297] [    INFO][0m - loss: 0.56158452, learning_rate: 2.7600000000000003e-05, global_step: 80, interval_runtime: 0.7244, interval_samples_per_second: 11.043, interval_steps_per_second: 13.804, epoch: 4.0[0m
[32m[2022-08-30 11:15:04,200] [    INFO][0m - loss: 0.47280293, learning_rate: 2.7300000000000003e-05, global_step: 90, interval_runtime: 0.9033, interval_samples_per_second: 8.856, interval_steps_per_second: 11.07, epoch: 4.5[0m
[32m[2022-08-30 11:15:04,947] [    INFO][0m - loss: 0.4770823, learning_rate: 2.7000000000000002e-05, global_step: 100, interval_runtime: 0.7467, interval_samples_per_second: 10.714, interval_steps_per_second: 13.392, epoch: 5.0[0m
[32m[2022-08-30 11:15:04,948] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 11:15:04,948] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-30 11:15:04,948] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 11:15:04,948] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 11:15:04,948] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 11:15:05,669] [    INFO][0m - eval_loss: 1.1296864748001099, eval_accuracy: 0.559748427672956, eval_runtime: 0.7209, eval_samples_per_second: 220.567, eval_steps_per_second: 6.936, epoch: 5.0[0m
[32m[2022-08-30 11:15:05,670] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-30 11:15:05,670] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 11:15:08,820] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-30 11:15:08,820] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-30 11:15:13,905] [    INFO][0m - loss: 0.49734287, learning_rate: 2.6700000000000002e-05, global_step: 110, interval_runtime: 8.9573, interval_samples_per_second: 0.893, interval_steps_per_second: 1.116, epoch: 5.5[0m
[32m[2022-08-30 11:15:14,658] [    INFO][0m - loss: 0.46468887, learning_rate: 2.64e-05, global_step: 120, interval_runtime: 0.7536, interval_samples_per_second: 10.616, interval_steps_per_second: 13.27, epoch: 6.0[0m
[32m[2022-08-30 11:15:15,662] [    INFO][0m - loss: 0.53827319, learning_rate: 2.61e-05, global_step: 130, interval_runtime: 1.0039, interval_samples_per_second: 7.969, interval_steps_per_second: 9.961, epoch: 6.5[0m
[32m[2022-08-30 11:15:16,433] [    INFO][0m - loss: 0.35267498, learning_rate: 2.58e-05, global_step: 140, interval_runtime: 0.7711, interval_samples_per_second: 10.374, interval_steps_per_second: 12.968, epoch: 7.0[0m
[32m[2022-08-30 11:15:17,476] [    INFO][0m - loss: 0.41425414, learning_rate: 2.55e-05, global_step: 150, interval_runtime: 1.0426, interval_samples_per_second: 7.673, interval_steps_per_second: 9.591, epoch: 7.5[0m
[32m[2022-08-30 11:15:17,477] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 11:15:17,477] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-30 11:15:17,477] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 11:15:17,477] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 11:15:17,477] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 11:15:18,263] [    INFO][0m - eval_loss: 1.224571704864502, eval_accuracy: 0.5534591194968553, eval_runtime: 0.7855, eval_samples_per_second: 202.408, eval_steps_per_second: 6.365, epoch: 7.5[0m
[32m[2022-08-30 11:15:18,264] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-150[0m
[32m[2022-08-30 11:15:18,264] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 11:15:21,790] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-150/tokenizer_config.json[0m
[32m[2022-08-30 11:15:21,790] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-150/special_tokens_map.json[0m
[32m[2022-08-30 11:15:26,700] [    INFO][0m - loss: 0.30763335, learning_rate: 2.52e-05, global_step: 160, interval_runtime: 9.2245, interval_samples_per_second: 0.867, interval_steps_per_second: 1.084, epoch: 8.0[0m
[32m[2022-08-30 11:15:27,798] [    INFO][0m - loss: 0.37226222, learning_rate: 2.49e-05, global_step: 170, interval_runtime: 1.0976, interval_samples_per_second: 7.288, interval_steps_per_second: 9.11, epoch: 8.5[0m
[32m[2022-08-30 11:15:28,600] [    INFO][0m - loss: 0.41311517, learning_rate: 2.4599999999999998e-05, global_step: 180, interval_runtime: 0.8023, interval_samples_per_second: 9.971, interval_steps_per_second: 12.464, epoch: 9.0[0m
[32m[2022-08-30 11:15:29,746] [    INFO][0m - loss: 0.31015618, learning_rate: 2.43e-05, global_step: 190, interval_runtime: 1.1458, interval_samples_per_second: 6.982, interval_steps_per_second: 8.727, epoch: 9.5[0m
[32m[2022-08-30 11:15:30,564] [    INFO][0m - loss: 0.46139455, learning_rate: 2.4e-05, global_step: 200, interval_runtime: 0.8177, interval_samples_per_second: 9.784, interval_steps_per_second: 12.229, epoch: 10.0[0m
[32m[2022-08-30 11:15:30,565] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 11:15:30,565] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-30 11:15:30,565] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 11:15:30,565] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 11:15:30,565] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 11:15:31,435] [    INFO][0m - eval_loss: 1.6742559671401978, eval_accuracy: 0.5471698113207547, eval_runtime: 0.8699, eval_samples_per_second: 182.785, eval_steps_per_second: 5.748, epoch: 10.0[0m
[32m[2022-08-30 11:15:31,435] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-30 11:15:31,436] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 11:15:34,804] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-30 11:15:34,804] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-30 11:15:39,858] [    INFO][0m - loss: 0.32065418, learning_rate: 2.37e-05, global_step: 210, interval_runtime: 9.2938, interval_samples_per_second: 0.861, interval_steps_per_second: 1.076, epoch: 10.5[0m
[32m[2022-08-30 11:15:40,688] [    INFO][0m - loss: 0.22232804, learning_rate: 2.3400000000000003e-05, global_step: 220, interval_runtime: 0.8308, interval_samples_per_second: 9.629, interval_steps_per_second: 12.037, epoch: 11.0[0m
[32m[2022-08-30 11:15:41,942] [    INFO][0m - loss: 0.20194573, learning_rate: 2.3100000000000002e-05, global_step: 230, interval_runtime: 1.2542, interval_samples_per_second: 6.379, interval_steps_per_second: 7.973, epoch: 11.5[0m
[32m[2022-08-30 11:15:42,841] [    INFO][0m - loss: 0.2172497, learning_rate: 2.2800000000000002e-05, global_step: 240, interval_runtime: 0.8983, interval_samples_per_second: 8.906, interval_steps_per_second: 11.132, epoch: 12.0[0m
[32m[2022-08-30 11:15:44,190] [    INFO][0m - loss: 0.21397867, learning_rate: 2.25e-05, global_step: 250, interval_runtime: 1.3491, interval_samples_per_second: 5.93, interval_steps_per_second: 7.412, epoch: 12.5[0m
[32m[2022-08-30 11:15:44,191] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 11:15:44,191] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-30 11:15:44,191] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 11:15:44,191] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 11:15:44,191] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 11:15:45,148] [    INFO][0m - eval_loss: 2.29482364654541, eval_accuracy: 0.5283018867924528, eval_runtime: 0.9571, eval_samples_per_second: 166.129, eval_steps_per_second: 5.224, epoch: 12.5[0m
[32m[2022-08-30 11:15:45,149] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-250[0m
[32m[2022-08-30 11:15:45,149] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 11:15:48,811] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-250/tokenizer_config.json[0m
[32m[2022-08-30 11:15:48,812] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-250/special_tokens_map.json[0m
[32m[2022-08-30 11:15:54,122] [    INFO][0m - loss: 0.48771996, learning_rate: 2.22e-05, global_step: 260, interval_runtime: 9.9315, interval_samples_per_second: 0.806, interval_steps_per_second: 1.007, epoch: 13.0[0m
[32m[2022-08-30 11:15:55,450] [    INFO][0m - loss: 0.23162332, learning_rate: 2.19e-05, global_step: 270, interval_runtime: 1.3282, interval_samples_per_second: 6.023, interval_steps_per_second: 7.529, epoch: 13.5[0m
[32m[2022-08-30 11:15:56,353] [    INFO][0m - loss: 0.25555475, learning_rate: 2.16e-05, global_step: 280, interval_runtime: 0.9036, interval_samples_per_second: 8.854, interval_steps_per_second: 11.067, epoch: 14.0[0m
[32m[2022-08-30 11:15:57,717] [    INFO][0m - loss: 0.18838812, learning_rate: 2.13e-05, global_step: 290, interval_runtime: 1.3638, interval_samples_per_second: 5.866, interval_steps_per_second: 7.333, epoch: 14.5[0m
[32m[2022-08-30 11:15:58,621] [    INFO][0m - loss: 0.2092128, learning_rate: 2.1e-05, global_step: 300, interval_runtime: 0.9035, interval_samples_per_second: 8.855, interval_steps_per_second: 11.068, epoch: 15.0[0m
[32m[2022-08-30 11:15:58,621] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 11:15:58,622] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-30 11:15:58,622] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 11:15:58,622] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 11:15:58,622] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 11:15:59,712] [    INFO][0m - eval_loss: 2.2078957557678223, eval_accuracy: 0.5345911949685535, eval_runtime: 1.09, eval_samples_per_second: 145.871, eval_steps_per_second: 4.587, epoch: 15.0[0m
[32m[2022-08-30 11:15:59,712] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-30 11:15:59,713] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 11:16:03,483] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-30 11:16:03,484] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-30 11:16:09,124] [    INFO][0m - loss: 0.19005848, learning_rate: 2.07e-05, global_step: 310, interval_runtime: 10.5036, interval_samples_per_second: 0.762, interval_steps_per_second: 0.952, epoch: 15.5[0m
[32m[2022-08-30 11:16:10,073] [    INFO][0m - loss: 0.07529848, learning_rate: 2.04e-05, global_step: 320, interval_runtime: 0.949, interval_samples_per_second: 8.43, interval_steps_per_second: 10.538, epoch: 16.0[0m
[32m[2022-08-30 11:16:11,640] [    INFO][0m - loss: 0.24421186, learning_rate: 2.01e-05, global_step: 330, interval_runtime: 1.5667, interval_samples_per_second: 5.106, interval_steps_per_second: 6.383, epoch: 16.5[0m
[32m[2022-08-30 11:16:12,626] [    INFO][0m - loss: 0.11871983, learning_rate: 1.98e-05, global_step: 340, interval_runtime: 0.986, interval_samples_per_second: 8.114, interval_steps_per_second: 10.142, epoch: 17.0[0m
[32m[2022-08-30 11:16:14,168] [    INFO][0m - loss: 0.18428797, learning_rate: 1.95e-05, global_step: 350, interval_runtime: 1.542, interval_samples_per_second: 5.188, interval_steps_per_second: 6.485, epoch: 17.5[0m
[32m[2022-08-30 11:16:14,169] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 11:16:14,169] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-30 11:16:14,169] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 11:16:14,169] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 11:16:14,169] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 11:16:15,335] [    INFO][0m - eval_loss: 2.4937455654144287, eval_accuracy: 0.559748427672956, eval_runtime: 1.1657, eval_samples_per_second: 136.401, eval_steps_per_second: 4.289, epoch: 17.5[0m
[32m[2022-08-30 11:16:15,335] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-350[0m
[32m[2022-08-30 11:16:15,335] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 11:16:18,834] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-350/tokenizer_config.json[0m
[32m[2022-08-30 11:16:18,835] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-350/special_tokens_map.json[0m
[32m[2022-08-30 11:16:23,564] [    INFO][0m - loss: 0.35111413, learning_rate: 1.9200000000000003e-05, global_step: 360, interval_runtime: 9.3958, interval_samples_per_second: 0.851, interval_steps_per_second: 1.064, epoch: 18.0[0m
[32m[2022-08-30 11:16:25,117] [    INFO][0m - loss: 0.35807576, learning_rate: 1.8900000000000002e-05, global_step: 370, interval_runtime: 1.5533, interval_samples_per_second: 5.15, interval_steps_per_second: 6.438, epoch: 18.5[0m
[32m[2022-08-30 11:16:26,083] [    INFO][0m - loss: 0.13659768, learning_rate: 1.86e-05, global_step: 380, interval_runtime: 0.9658, interval_samples_per_second: 8.284, interval_steps_per_second: 10.355, epoch: 19.0[0m
[32m[2022-08-30 11:16:27,691] [    INFO][0m - loss: 0.20077784, learning_rate: 1.83e-05, global_step: 390, interval_runtime: 1.6081, interval_samples_per_second: 4.975, interval_steps_per_second: 6.218, epoch: 19.5[0m
[32m[2022-08-30 11:16:28,676] [    INFO][0m - loss: 0.20816, learning_rate: 1.8e-05, global_step: 400, interval_runtime: 0.9858, interval_samples_per_second: 8.116, interval_steps_per_second: 10.144, epoch: 20.0[0m
[32m[2022-08-30 11:16:28,677] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 11:16:28,677] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-30 11:16:28,677] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 11:16:28,677] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 11:16:28,677] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 11:16:29,920] [    INFO][0m - eval_loss: 2.121845245361328, eval_accuracy: 0.5911949685534591, eval_runtime: 1.2427, eval_samples_per_second: 127.95, eval_steps_per_second: 4.024, epoch: 20.0[0m
[32m[2022-08-30 11:16:29,920] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-30 11:16:29,921] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 11:16:34,059] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-30 11:16:34,059] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-30 11:16:39,774] [    INFO][0m - loss: 0.12214301, learning_rate: 1.77e-05, global_step: 410, interval_runtime: 11.0977, interval_samples_per_second: 0.721, interval_steps_per_second: 0.901, epoch: 20.5[0m
[32m[2022-08-30 11:16:40,765] [    INFO][0m - loss: 0.13947083, learning_rate: 1.74e-05, global_step: 420, interval_runtime: 0.9912, interval_samples_per_second: 8.071, interval_steps_per_second: 10.089, epoch: 21.0[0m
[32m[2022-08-30 11:16:42,469] [    INFO][0m - loss: 0.09549934, learning_rate: 1.71e-05, global_step: 430, interval_runtime: 1.7034, interval_samples_per_second: 4.697, interval_steps_per_second: 5.871, epoch: 21.5[0m
[32m[2022-08-30 11:16:43,468] [    INFO][0m - loss: 0.06833513, learning_rate: 1.6800000000000002e-05, global_step: 440, interval_runtime: 0.9996, interval_samples_per_second: 8.003, interval_steps_per_second: 10.004, epoch: 22.0[0m
[32m[2022-08-30 11:16:45,208] [    INFO][0m - loss: 0.06819022, learning_rate: 1.65e-05, global_step: 450, interval_runtime: 1.7391, interval_samples_per_second: 4.6, interval_steps_per_second: 5.75, epoch: 22.5[0m
[32m[2022-08-30 11:16:45,208] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 11:16:45,208] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-30 11:16:45,208] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 11:16:45,208] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 11:16:45,208] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 11:16:46,578] [    INFO][0m - eval_loss: 2.283695697784424, eval_accuracy: 0.6226415094339622, eval_runtime: 1.3694, eval_samples_per_second: 116.11, eval_steps_per_second: 3.651, epoch: 22.5[0m
[32m[2022-08-30 11:16:46,578] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-450[0m
[32m[2022-08-30 11:16:46,579] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 11:16:50,090] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-450/tokenizer_config.json[0m
[32m[2022-08-30 11:16:50,091] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-450/special_tokens_map.json[0m
[32m[2022-08-30 11:16:55,183] [    INFO][0m - loss: 0.1355468, learning_rate: 1.62e-05, global_step: 460, interval_runtime: 9.9756, interval_samples_per_second: 0.802, interval_steps_per_second: 1.002, epoch: 23.0[0m
[32m[2022-08-30 11:16:57,046] [    INFO][0m - loss: 0.14982381, learning_rate: 1.59e-05, global_step: 470, interval_runtime: 1.8633, interval_samples_per_second: 4.293, interval_steps_per_second: 5.367, epoch: 23.5[0m
[32m[2022-08-30 11:16:58,161] [    INFO][0m - loss: 0.11486384, learning_rate: 1.56e-05, global_step: 480, interval_runtime: 1.1146, interval_samples_per_second: 7.177, interval_steps_per_second: 8.972, epoch: 24.0[0m
[32m[2022-08-30 11:17:00,091] [    INFO][0m - loss: 0.12938893, learning_rate: 1.53e-05, global_step: 490, interval_runtime: 1.9301, interval_samples_per_second: 4.145, interval_steps_per_second: 5.181, epoch: 24.5[0m
[32m[2022-08-30 11:17:01,210] [    INFO][0m - loss: 0.0626459, learning_rate: 1.5e-05, global_step: 500, interval_runtime: 1.1187, interval_samples_per_second: 7.151, interval_steps_per_second: 8.939, epoch: 25.0[0m
[32m[2022-08-30 11:17:01,211] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 11:17:01,211] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-30 11:17:01,211] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 11:17:01,211] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 11:17:01,211] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 11:17:02,714] [    INFO][0m - eval_loss: 2.9321439266204834, eval_accuracy: 0.559748427672956, eval_runtime: 1.5026, eval_samples_per_second: 105.813, eval_steps_per_second: 3.327, epoch: 25.0[0m
[32m[2022-08-30 11:17:02,715] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-08-30 11:17:02,715] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 11:17:06,009] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-08-30 11:17:06,009] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-08-30 11:17:12,008] [    INFO][0m - loss: 0.09108973, learning_rate: 1.47e-05, global_step: 510, interval_runtime: 10.7975, interval_samples_per_second: 0.741, interval_steps_per_second: 0.926, epoch: 25.5[0m
[32m[2022-08-30 11:17:13,084] [    INFO][0m - loss: 0.09968621, learning_rate: 1.44e-05, global_step: 520, interval_runtime: 1.0763, interval_samples_per_second: 7.433, interval_steps_per_second: 9.291, epoch: 26.0[0m
[32m[2022-08-30 11:17:15,036] [    INFO][0m - loss: 0.11771301, learning_rate: 1.4099999999999999e-05, global_step: 530, interval_runtime: 1.9523, interval_samples_per_second: 4.098, interval_steps_per_second: 5.122, epoch: 26.5[0m
[32m[2022-08-30 11:17:16,140] [    INFO][0m - loss: 0.11102716, learning_rate: 1.3800000000000002e-05, global_step: 540, interval_runtime: 1.1039, interval_samples_per_second: 7.247, interval_steps_per_second: 9.059, epoch: 27.0[0m
[32m[2022-08-30 11:17:18,179] [    INFO][0m - loss: 0.05107598, learning_rate: 1.3500000000000001e-05, global_step: 550, interval_runtime: 2.039, interval_samples_per_second: 3.924, interval_steps_per_second: 4.904, epoch: 27.5[0m
[32m[2022-08-30 11:17:18,180] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 11:17:18,180] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-30 11:17:18,180] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 11:17:18,180] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 11:17:18,180] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 11:17:19,730] [    INFO][0m - eval_loss: 2.7322678565979004, eval_accuracy: 0.5849056603773585, eval_runtime: 1.5497, eval_samples_per_second: 102.603, eval_steps_per_second: 3.227, epoch: 27.5[0m
[32m[2022-08-30 11:17:19,731] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-550[0m
[32m[2022-08-30 11:17:19,731] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 11:17:23,028] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-550/tokenizer_config.json[0m
[32m[2022-08-30 11:17:23,029] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-550/special_tokens_map.json[0m
[32m[2022-08-30 11:17:27,983] [    INFO][0m - loss: 0.16620463, learning_rate: 1.32e-05, global_step: 560, interval_runtime: 9.8039, interval_samples_per_second: 0.816, interval_steps_per_second: 1.02, epoch: 28.0[0m
[32m[2022-08-30 11:17:30,097] [    INFO][0m - loss: 0.07324688, learning_rate: 1.29e-05, global_step: 570, interval_runtime: 2.1139, interval_samples_per_second: 3.784, interval_steps_per_second: 4.731, epoch: 28.5[0m
[32m[2022-08-30 11:17:31,250] [    INFO][0m - loss: 0.13155495, learning_rate: 1.26e-05, global_step: 580, interval_runtime: 1.1536, interval_samples_per_second: 6.935, interval_steps_per_second: 8.669, epoch: 29.0[0m
[32m[2022-08-30 11:17:33,392] [    INFO][0m - loss: 0.04296011, learning_rate: 1.2299999999999999e-05, global_step: 590, interval_runtime: 2.1412, interval_samples_per_second: 3.736, interval_steps_per_second: 4.67, epoch: 29.5[0m
[32m[2022-08-30 11:17:34,601] [    INFO][0m - loss: 0.0893358, learning_rate: 1.2e-05, global_step: 600, interval_runtime: 1.2092, interval_samples_per_second: 6.616, interval_steps_per_second: 8.27, epoch: 30.0[0m
[32m[2022-08-30 11:17:34,602] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 11:17:34,602] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-30 11:17:34,602] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 11:17:34,602] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 11:17:34,602] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 11:17:36,341] [    INFO][0m - eval_loss: 2.7107386589050293, eval_accuracy: 0.610062893081761, eval_runtime: 1.7387, eval_samples_per_second: 91.45, eval_steps_per_second: 2.876, epoch: 30.0[0m
[32m[2022-08-30 11:17:36,344] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-08-30 11:17:36,344] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 11:17:39,452] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-08-30 11:17:39,453] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-08-30 11:17:45,821] [    INFO][0m - loss: 0.12395369, learning_rate: 1.1700000000000001e-05, global_step: 610, interval_runtime: 11.2202, interval_samples_per_second: 0.713, interval_steps_per_second: 0.891, epoch: 30.5[0m
[32m[2022-08-30 11:17:46,973] [    INFO][0m - loss: 0.05067703, learning_rate: 1.1400000000000001e-05, global_step: 620, interval_runtime: 1.1518, interval_samples_per_second: 6.946, interval_steps_per_second: 8.682, epoch: 31.0[0m
[32m[2022-08-30 11:17:49,157] [    INFO][0m - loss: 0.05912519, learning_rate: 1.11e-05, global_step: 630, interval_runtime: 2.1837, interval_samples_per_second: 3.664, interval_steps_per_second: 4.579, epoch: 31.5[0m
[32m[2022-08-30 11:17:50,323] [    INFO][0m - loss: 0.13447837, learning_rate: 1.08e-05, global_step: 640, interval_runtime: 1.1668, interval_samples_per_second: 6.856, interval_steps_per_second: 8.57, epoch: 32.0[0m
[32m[2022-08-30 11:17:52,528] [    INFO][0m - loss: 0.14892823, learning_rate: 1.05e-05, global_step: 650, interval_runtime: 2.2045, interval_samples_per_second: 3.629, interval_steps_per_second: 4.536, epoch: 32.5[0m
[32m[2022-08-30 11:17:52,529] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 11:17:52,529] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-30 11:17:52,529] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 11:17:52,529] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 11:17:52,529] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 11:17:54,317] [    INFO][0m - eval_loss: 2.7212038040161133, eval_accuracy: 0.5974842767295597, eval_runtime: 1.7875, eval_samples_per_second: 88.949, eval_steps_per_second: 2.797, epoch: 32.5[0m
[32m[2022-08-30 11:17:54,317] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-650[0m
[32m[2022-08-30 11:17:54,317] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 11:17:57,408] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-650/tokenizer_config.json[0m
[32m[2022-08-30 11:17:57,408] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-650/special_tokens_map.json[0m
[32m[2022-08-30 11:18:02,315] [    INFO][0m - loss: 0.04358989, learning_rate: 1.02e-05, global_step: 660, interval_runtime: 9.7869, interval_samples_per_second: 0.817, interval_steps_per_second: 1.022, epoch: 33.0[0m
[32m[2022-08-30 11:18:04,649] [    INFO][0m - loss: 0.08953621, learning_rate: 9.9e-06, global_step: 670, interval_runtime: 2.3341, interval_samples_per_second: 3.427, interval_steps_per_second: 4.284, epoch: 33.5[0m
[32m[2022-08-30 11:18:05,915] [    INFO][0m - loss: 0.05072556, learning_rate: 9.600000000000001e-06, global_step: 680, interval_runtime: 1.2661, interval_samples_per_second: 6.319, interval_steps_per_second: 7.898, epoch: 34.0[0m
[32m[2022-08-30 11:18:08,311] [    INFO][0m - loss: 0.06468012, learning_rate: 9.3e-06, global_step: 690, interval_runtime: 2.3959, interval_samples_per_second: 3.339, interval_steps_per_second: 4.174, epoch: 34.5[0m
[32m[2022-08-30 11:18:09,513] [    INFO][0m - loss: 0.10661309, learning_rate: 9e-06, global_step: 700, interval_runtime: 1.2026, interval_samples_per_second: 6.652, interval_steps_per_second: 8.315, epoch: 35.0[0m
[32m[2022-08-30 11:18:09,514] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 11:18:09,514] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-30 11:18:09,514] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 11:18:09,514] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 11:18:09,514] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 11:18:11,425] [    INFO][0m - eval_loss: 2.8311069011688232, eval_accuracy: 0.5723270440251572, eval_runtime: 1.9109, eval_samples_per_second: 83.209, eval_steps_per_second: 2.617, epoch: 35.0[0m
[32m[2022-08-30 11:18:11,426] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-700[0m
[32m[2022-08-30 11:18:11,426] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 11:18:14,835] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-700/tokenizer_config.json[0m
[32m[2022-08-30 11:18:14,835] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-700/special_tokens_map.json[0m
[32m[2022-08-30 11:18:20,927] [    INFO][0m - loss: 0.0546376, learning_rate: 8.7e-06, global_step: 710, interval_runtime: 11.4129, interval_samples_per_second: 0.701, interval_steps_per_second: 0.876, epoch: 35.5[0m
[32m[2022-08-30 11:18:22,169] [    INFO][0m - loss: 0.09708092, learning_rate: 8.400000000000001e-06, global_step: 720, interval_runtime: 1.2421, interval_samples_per_second: 6.441, interval_steps_per_second: 8.051, epoch: 36.0[0m
[32m[2022-08-30 11:18:24,583] [    INFO][0m - loss: 0.08791097, learning_rate: 8.1e-06, global_step: 730, interval_runtime: 2.4146, interval_samples_per_second: 3.313, interval_steps_per_second: 4.141, epoch: 36.5[0m
[32m[2022-08-30 11:18:25,824] [    INFO][0m - loss: 0.14419641, learning_rate: 7.8e-06, global_step: 740, interval_runtime: 1.2402, interval_samples_per_second: 6.45, interval_steps_per_second: 8.063, epoch: 37.0[0m
[32m[2022-08-30 11:18:28,329] [    INFO][0m - loss: 0.08204914, learning_rate: 7.5e-06, global_step: 750, interval_runtime: 2.5056, interval_samples_per_second: 3.193, interval_steps_per_second: 3.991, epoch: 37.5[0m
[32m[2022-08-30 11:18:28,330] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 11:18:28,330] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-30 11:18:28,330] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 11:18:28,330] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 11:18:28,330] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 11:18:30,319] [    INFO][0m - eval_loss: 3.167372703552246, eval_accuracy: 0.5534591194968553, eval_runtime: 1.9887, eval_samples_per_second: 79.953, eval_steps_per_second: 2.514, epoch: 37.5[0m
[32m[2022-08-30 11:18:30,320] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-750[0m
[32m[2022-08-30 11:18:30,320] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 11:18:31,838] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-750/tokenizer_config.json[0m
[32m[2022-08-30 11:18:31,838] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-750/special_tokens_map.json[0m
[32m[2022-08-30 11:18:33,748] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-30 11:18:33,748] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-450 (score: 0.6226415094339622).[0m
[32m[2022-08-30 11:18:34,901] [    INFO][0m - train_runtime: 227.1312, train_samples_per_second: 35.222, train_steps_per_second: 4.403, train_loss: 0.24429832327365875, epoch: 37.5[0m
[32m[2022-08-30 11:18:34,903] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-30 11:18:34,903] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 11:18:38,137] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-30 11:18:38,138] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-30 11:18:38,140] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-30 11:18:38,140] [    INFO][0m -   epoch                    =       37.5[0m
[32m[2022-08-30 11:18:38,140] [    INFO][0m -   train_loss               =     0.2443[0m
[32m[2022-08-30 11:18:38,140] [    INFO][0m -   train_runtime            = 0:03:47.13[0m
[32m[2022-08-30 11:18:38,140] [    INFO][0m -   train_samples_per_second =     35.222[0m
[32m[2022-08-30 11:18:38,140] [    INFO][0m -   train_steps_per_second   =      4.403[0m
[32m[2022-08-30 11:18:38,144] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-30 11:18:38,145] [    INFO][0m -   Num examples = 976[0m
[32m[2022-08-30 11:18:38,145] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 11:18:38,145] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 11:18:38,145] [    INFO][0m -   Total prediction steps = 31[0m
[32m[2022-08-30 11:18:50,343] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-30 11:18:50,343] [    INFO][0m -   test_accuracy           =      0.501[0m
[32m[2022-08-30 11:18:50,344] [    INFO][0m -   test_loss               =     2.9961[0m
[32m[2022-08-30 11:18:50,344] [    INFO][0m -   test_runtime            = 0:00:12.19[0m
[32m[2022-08-30 11:18:50,344] [    INFO][0m -   test_samples_per_second =     80.012[0m
[32m[2022-08-30 11:18:50,344] [    INFO][0m -   test_steps_per_second   =      2.541[0m
[32m[2022-08-30 11:18:50,344] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-30 11:18:50,344] [    INFO][0m -   Num examples = 290[0m
[32m[2022-08-30 11:18:50,344] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 11:18:50,344] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 11:18:50,345] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-08-30 11:18:54,296] [    INFO][0m - Predictions for cluewscf saved to ./fewclue_submit_examples.[0m
metric <function main.<locals>.compute_metrics at 0x7f47fe7c03a0>
