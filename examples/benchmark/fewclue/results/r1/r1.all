 
==========
eprstmt
==========
 
[33m[2022-08-31 17:34:09,889] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-31 17:34:09,889] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-31 17:34:09,889] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 17:34:09,889] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-31 17:34:09,889] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 17:34:09,890] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-31 17:34:09,890] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-31 17:34:09,890] [    INFO][0m - early_stop_patience           :20[0m
[32m[2022-08-31 17:34:09,890] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-31 17:34:09,890] [    INFO][0m - model_name_or_path            :roformer_v2_chinese_char_base[0m
[32m[2022-08-31 17:34:09,890] [    INFO][0m - [0m
[32m[2022-08-31 17:34:09,890] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 17:34:09,890] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-31 17:34:09,890] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 17:34:09,890] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-31 17:34:09,890] [    INFO][0m - prompt                        :{'text':'text_a'}{'hard':'æˆ‘æ„Ÿè§‰'}{'mask'}{'hard':'å–œæ¬¢ã€‚'}[0m
[32m[2022-08-31 17:34:09,890] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-31 17:34:09,891] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-31 17:34:09,891] [    INFO][0m - task_name                     :eprstmt[0m
[32m[2022-08-31 17:34:09,891] [    INFO][0m - [0m
[32m[2022-08-31 17:34:09,891] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/roformer_v2_chinese_char_base/model_state.pdparams[0m
W0831 17:34:09.892349 10922 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0831 17:34:09.896382 10922 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-31 17:34:12,854] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/roformer_v2_chinese_char_base/vocab.txt[0m
[32m[2022-08-31 17:34:12,887] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/roformer_v2_chinese_char_base/tokenizer_config.json[0m
[32m[2022-08-31 17:34:12,887] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/roformer_v2_chinese_char_base/special_tokens_map.json[0m
[32m[2022-08-31 17:34:12,888] [    INFO][0m - Using template: [{'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': 'æˆ‘æ„Ÿè§‰'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'å–œæ¬¢ã€‚'}][0m
2022-08-31 17:34:12,895 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-31 17:34:13,001] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 17:34:13,002] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-31 17:34:13,002] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 17:34:13,002] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-31 17:34:13,002] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-31 17:34:13,002] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-31 17:34:13,002] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-31 17:34:13,002] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-31 17:34:13,002] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-31 17:34:13,002] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-31 17:34:13,003] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-31 17:34:13,003] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-31 17:34:13,003] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-31 17:34:13,003] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-31 17:34:13,003] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-31 17:34:13,003] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-31 17:34:13,003] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-31 17:34:13,003] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-31 17:34:13,003] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-31 17:34:13,003] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-31 17:34:13,003] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-31 17:34:13,003] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-31 17:34:13,003] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-31 17:34:13,004] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-31 17:34:13,004] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-31 17:34:13,004] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-31 17:34:13,004] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-31 17:34:13,004] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-31 17:34:13,004] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-31 17:34:13,004] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-31 17:34:13,004] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-31 17:34:13,004] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-31 17:34:13,004] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-31 17:34:13,004] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-31 17:34:13,004] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-31 17:34:13,004] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-31 17:34:13,005] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-31 17:34:13,005] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug31_17-34-09_instance-3bwob41y-01[0m
[32m[2022-08-31 17:34:13,005] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-31 17:34:13,005] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-31 17:34:13,005] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-31 17:34:13,005] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-31 17:34:13,005] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-31 17:34:13,005] [    INFO][0m - max_seq_length                :128[0m
[32m[2022-08-31 17:34:13,005] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-31 17:34:13,005] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-31 17:34:13,005] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-31 17:34:13,005] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-31 17:34:13,005] [    INFO][0m - num_train_epochs              :100.0[0m
[32m[2022-08-31 17:34:13,006] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-31 17:34:13,006] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-31 17:34:13,006] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-31 17:34:13,006] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-31 17:34:13,006] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-31 17:34:13,006] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-31 17:34:13,006] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-31 17:34:13,006] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-31 17:34:13,006] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-31 17:34:13,006] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-31 17:34:13,006] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-31 17:34:13,006] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-31 17:34:13,006] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-31 17:34:13,007] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-31 17:34:13,007] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-31 17:34:13,007] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-31 17:34:13,007] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-31 17:34:13,007] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-31 17:34:13,007] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-31 17:34:13,007] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-31 17:34:13,007] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-31 17:34:13,007] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-31 17:34:13,007] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-31 17:34:13,007] [    INFO][0m - seed                          :42[0m
[32m[2022-08-31 17:34:13,007] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-31 17:34:13,007] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-31 17:34:13,008] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-31 17:34:13,008] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-31 17:34:13,008] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-31 17:34:13,008] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-31 17:34:13,008] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-31 17:34:13,008] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-31 17:34:13,008] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-31 17:34:13,008] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-31 17:34:13,008] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-31 17:34:13,008] [    INFO][0m - [0m
[32m[2022-08-31 17:34:13,010] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-31 17:34:13,010] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 17:34:13,010] [    INFO][0m -   Num Epochs = 100[0m
[32m[2022-08-31 17:34:13,010] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-31 17:34:13,010] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-31 17:34:13,010] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-31 17:34:13,010] [    INFO][0m -   Total optimization steps = 2000.0[0m
[32m[2022-08-31 17:34:13,011] [    INFO][0m -   Total num train samples = 16000[0m
[32m[2022-08-31 17:34:14,568] [    INFO][0m - loss: 0.61864104, learning_rate: 2.985e-05, global_step: 10, interval_runtime: 1.5566, interval_samples_per_second: 5.139, interval_steps_per_second: 6.424, epoch: 0.5[0m
[32m[2022-08-31 17:34:15,310] [    INFO][0m - loss: 0.53821731, learning_rate: 2.97e-05, global_step: 20, interval_runtime: 0.7411, interval_samples_per_second: 10.794, interval_steps_per_second: 13.493, epoch: 1.0[0m
[32m[2022-08-31 17:34:16,190] [    INFO][0m - loss: 0.11235952, learning_rate: 2.955e-05, global_step: 30, interval_runtime: 0.8808, interval_samples_per_second: 9.083, interval_steps_per_second: 11.354, epoch: 1.5[0m
[32m[2022-08-31 17:34:16,948] [    INFO][0m - loss: 0.25030825, learning_rate: 2.94e-05, global_step: 40, interval_runtime: 0.7579, interval_samples_per_second: 10.556, interval_steps_per_second: 13.195, epoch: 2.0[0m
[32m[2022-08-31 17:34:17,812] [    INFO][0m - loss: 0.02101694, learning_rate: 2.925e-05, global_step: 50, interval_runtime: 0.8641, interval_samples_per_second: 9.258, interval_steps_per_second: 11.572, epoch: 2.5[0m
[32m[2022-08-31 17:34:18,649] [    INFO][0m - loss: 0.02502496, learning_rate: 2.91e-05, global_step: 60, interval_runtime: 0.8372, interval_samples_per_second: 9.555, interval_steps_per_second: 11.944, epoch: 3.0[0m
[32m[2022-08-31 17:34:19,570] [    INFO][0m - loss: 0.00065584, learning_rate: 2.895e-05, global_step: 70, interval_runtime: 0.9208, interval_samples_per_second: 8.688, interval_steps_per_second: 10.86, epoch: 3.5[0m
[32m[2022-08-31 17:34:20,381] [    INFO][0m - loss: 0.00676514, learning_rate: 2.88e-05, global_step: 80, interval_runtime: 0.8111, interval_samples_per_second: 9.863, interval_steps_per_second: 12.329, epoch: 4.0[0m
[32m[2022-08-31 17:34:21,225] [    INFO][0m - loss: 0.00012908, learning_rate: 2.865e-05, global_step: 90, interval_runtime: 0.8444, interval_samples_per_second: 9.475, interval_steps_per_second: 11.843, epoch: 4.5[0m
[32m[2022-08-31 17:34:21,990] [    INFO][0m - loss: 8.08e-06, learning_rate: 2.8499999999999998e-05, global_step: 100, interval_runtime: 0.7648, interval_samples_per_second: 10.46, interval_steps_per_second: 13.075, epoch: 5.0[0m
[32m[2022-08-31 17:34:21,991] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 17:34:21,991] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 17:34:21,991] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:34:21,991] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:34:21,991] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 17:34:22,501] [    INFO][0m - eval_loss: 1.3332326412200928, eval_accuracy: 0.88125, eval_runtime: 0.5088, eval_samples_per_second: 314.478, eval_steps_per_second: 9.827, epoch: 5.0[0m
[32m[2022-08-31 17:34:22,502] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-31 17:34:22,502] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 17:34:24,803] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-31 17:34:24,804] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-31 17:34:30,184] [    INFO][0m - loss: 2.494e-05, learning_rate: 2.8349999999999998e-05, global_step: 110, interval_runtime: 8.1932, interval_samples_per_second: 0.976, interval_steps_per_second: 1.221, epoch: 5.5[0m
[32m[2022-08-31 17:34:30,926] [    INFO][0m - loss: 3.47e-06, learning_rate: 2.8199999999999998e-05, global_step: 120, interval_runtime: 0.743, interval_samples_per_second: 10.767, interval_steps_per_second: 13.459, epoch: 6.0[0m
[32m[2022-08-31 17:34:31,790] [    INFO][0m - loss: 2.51e-06, learning_rate: 2.805e-05, global_step: 130, interval_runtime: 0.864, interval_samples_per_second: 9.259, interval_steps_per_second: 11.574, epoch: 6.5[0m
[32m[2022-08-31 17:34:32,557] [    INFO][0m - loss: 3.3e-07, learning_rate: 2.79e-05, global_step: 140, interval_runtime: 0.7663, interval_samples_per_second: 10.439, interval_steps_per_second: 13.049, epoch: 7.0[0m
[32m[2022-08-31 17:34:33,412] [    INFO][0m - loss: 4.9e-07, learning_rate: 2.7750000000000004e-05, global_step: 150, interval_runtime: 0.8553, interval_samples_per_second: 9.354, interval_steps_per_second: 11.692, epoch: 7.5[0m
[32m[2022-08-31 17:34:34,148] [    INFO][0m - loss: 4e-07, learning_rate: 2.7600000000000003e-05, global_step: 160, interval_runtime: 0.7358, interval_samples_per_second: 10.873, interval_steps_per_second: 13.591, epoch: 8.0[0m
[32m[2022-08-31 17:34:34,982] [    INFO][0m - loss: 2.3e-07, learning_rate: 2.7450000000000003e-05, global_step: 170, interval_runtime: 0.8337, interval_samples_per_second: 9.596, interval_steps_per_second: 11.994, epoch: 8.5[0m
[32m[2022-08-31 17:34:35,729] [    INFO][0m - loss: 4.4e-07, learning_rate: 2.7300000000000003e-05, global_step: 180, interval_runtime: 0.7478, interval_samples_per_second: 10.698, interval_steps_per_second: 13.373, epoch: 9.0[0m
[32m[2022-08-31 17:34:36,568] [    INFO][0m - loss: 3.3e-07, learning_rate: 2.7150000000000003e-05, global_step: 190, interval_runtime: 0.8389, interval_samples_per_second: 9.537, interval_steps_per_second: 11.921, epoch: 9.5[0m
[32m[2022-08-31 17:34:37,314] [    INFO][0m - loss: 5.6e-07, learning_rate: 2.7000000000000002e-05, global_step: 200, interval_runtime: 0.7459, interval_samples_per_second: 10.726, interval_steps_per_second: 13.407, epoch: 10.0[0m
[32m[2022-08-31 17:34:37,315] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 17:34:37,315] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 17:34:37,315] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:34:37,315] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:34:37,315] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 17:34:37,820] [    INFO][0m - eval_loss: 1.3535065650939941, eval_accuracy: 0.89375, eval_runtime: 0.5042, eval_samples_per_second: 317.344, eval_steps_per_second: 9.917, epoch: 10.0[0m
[32m[2022-08-31 17:34:37,820] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-31 17:34:37,820] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 17:34:39,701] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-31 17:34:39,701] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-31 17:34:44,758] [    INFO][0m - loss: 2.9e-07, learning_rate: 2.6850000000000002e-05, global_step: 210, interval_runtime: 7.4431, interval_samples_per_second: 1.075, interval_steps_per_second: 1.344, epoch: 10.5[0m
[32m[2022-08-31 17:34:45,504] [    INFO][0m - loss: 5.3e-07, learning_rate: 2.6700000000000002e-05, global_step: 220, interval_runtime: 0.7464, interval_samples_per_second: 10.719, interval_steps_per_second: 13.398, epoch: 11.0[0m
[32m[2022-08-31 17:34:46,367] [    INFO][0m - loss: 2.5e-07, learning_rate: 2.655e-05, global_step: 230, interval_runtime: 0.8631, interval_samples_per_second: 9.269, interval_steps_per_second: 11.586, epoch: 11.5[0m
[32m[2022-08-31 17:34:47,117] [    INFO][0m - loss: 2.2e-07, learning_rate: 2.64e-05, global_step: 240, interval_runtime: 0.7503, interval_samples_per_second: 10.663, interval_steps_per_second: 13.328, epoch: 12.0[0m
[32m[2022-08-31 17:34:47,972] [    INFO][0m - loss: 2.1e-07, learning_rate: 2.625e-05, global_step: 250, interval_runtime: 0.8546, interval_samples_per_second: 9.361, interval_steps_per_second: 11.701, epoch: 12.5[0m
[32m[2022-08-31 17:34:48,727] [    INFO][0m - loss: 1.8e-07, learning_rate: 2.61e-05, global_step: 260, interval_runtime: 0.7552, interval_samples_per_second: 10.594, interval_steps_per_second: 13.242, epoch: 13.0[0m
[32m[2022-08-31 17:34:49,623] [    INFO][0m - loss: 7.4e-07, learning_rate: 2.595e-05, global_step: 270, interval_runtime: 0.8966, interval_samples_per_second: 8.923, interval_steps_per_second: 11.153, epoch: 13.5[0m
[32m[2022-08-31 17:34:50,397] [    INFO][0m - loss: 7.7e-07, learning_rate: 2.58e-05, global_step: 280, interval_runtime: 0.7737, interval_samples_per_second: 10.34, interval_steps_per_second: 12.925, epoch: 14.0[0m
[32m[2022-08-31 17:34:51,263] [    INFO][0m - loss: 1e-07, learning_rate: 2.565e-05, global_step: 290, interval_runtime: 0.8649, interval_samples_per_second: 9.25, interval_steps_per_second: 11.562, epoch: 14.5[0m
[32m[2022-08-31 17:34:52,008] [    INFO][0m - loss: 1.11e-06, learning_rate: 2.55e-05, global_step: 300, interval_runtime: 0.7461, interval_samples_per_second: 10.722, interval_steps_per_second: 13.403, epoch: 15.0[0m
[32m[2022-08-31 17:34:52,009] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 17:34:52,009] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 17:34:52,009] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:34:52,009] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:34:52,009] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 17:34:52,507] [    INFO][0m - eval_loss: 1.362931251525879, eval_accuracy: 0.89375, eval_runtime: 0.4975, eval_samples_per_second: 321.631, eval_steps_per_second: 10.051, epoch: 15.0[0m
[32m[2022-08-31 17:34:52,508] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-31 17:34:52,508] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 17:34:54,417] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-31 17:34:54,417] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-31 17:34:59,569] [    INFO][0m - loss: 9.6e-07, learning_rate: 2.535e-05, global_step: 310, interval_runtime: 7.5611, interval_samples_per_second: 1.058, interval_steps_per_second: 1.323, epoch: 15.5[0m
[32m[2022-08-31 17:35:00,314] [    INFO][0m - loss: 3.7e-07, learning_rate: 2.52e-05, global_step: 320, interval_runtime: 0.7444, interval_samples_per_second: 10.747, interval_steps_per_second: 13.434, epoch: 16.0[0m
[32m[2022-08-31 17:35:01,164] [    INFO][0m - loss: 9.3e-07, learning_rate: 2.505e-05, global_step: 330, interval_runtime: 0.8507, interval_samples_per_second: 9.404, interval_steps_per_second: 11.756, epoch: 16.5[0m
[32m[2022-08-31 17:35:01,994] [    INFO][0m - loss: 1.6e-07, learning_rate: 2.49e-05, global_step: 340, interval_runtime: 0.8301, interval_samples_per_second: 9.638, interval_steps_per_second: 12.047, epoch: 17.0[0m
[32m[2022-08-31 17:35:02,838] [    INFO][0m - loss: 3e-07, learning_rate: 2.475e-05, global_step: 350, interval_runtime: 0.8441, interval_samples_per_second: 9.477, interval_steps_per_second: 11.847, epoch: 17.5[0m
[32m[2022-08-31 17:35:03,584] [    INFO][0m - loss: 8e-08, learning_rate: 2.4599999999999998e-05, global_step: 360, interval_runtime: 0.7455, interval_samples_per_second: 10.73, interval_steps_per_second: 13.413, epoch: 18.0[0m
[32m[2022-08-31 17:35:04,466] [    INFO][0m - loss: 2.7e-07, learning_rate: 2.4449999999999998e-05, global_step: 370, interval_runtime: 0.8817, interval_samples_per_second: 9.074, interval_steps_per_second: 11.342, epoch: 18.5[0m
[32m[2022-08-31 17:35:05,235] [    INFO][0m - loss: 2.7e-07, learning_rate: 2.43e-05, global_step: 380, interval_runtime: 0.7691, interval_samples_per_second: 10.402, interval_steps_per_second: 13.003, epoch: 19.0[0m
[32m[2022-08-31 17:35:06,093] [    INFO][0m - loss: 2.9e-07, learning_rate: 2.415e-05, global_step: 390, interval_runtime: 0.8578, interval_samples_per_second: 9.326, interval_steps_per_second: 11.657, epoch: 19.5[0m
[32m[2022-08-31 17:35:06,847] [    INFO][0m - loss: 2e-07, learning_rate: 2.4e-05, global_step: 400, interval_runtime: 0.7548, interval_samples_per_second: 10.599, interval_steps_per_second: 13.248, epoch: 20.0[0m
[32m[2022-08-31 17:35:06,848] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 17:35:06,848] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 17:35:06,848] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:35:06,848] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:35:06,848] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 17:35:07,355] [    INFO][0m - eval_loss: 1.3757998943328857, eval_accuracy: 0.89375, eval_runtime: 0.5063, eval_samples_per_second: 316.002, eval_steps_per_second: 9.875, epoch: 20.0[0m
[32m[2022-08-31 17:35:07,355] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-31 17:35:07,355] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 17:35:09,392] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-31 17:35:09,393] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-31 17:35:14,424] [    INFO][0m - loss: 4.6e-07, learning_rate: 2.385e-05, global_step: 410, interval_runtime: 7.5767, interval_samples_per_second: 1.056, interval_steps_per_second: 1.32, epoch: 20.5[0m
[32m[2022-08-31 17:35:15,164] [    INFO][0m - loss: 1.4e-07, learning_rate: 2.37e-05, global_step: 420, interval_runtime: 0.7404, interval_samples_per_second: 10.806, interval_steps_per_second: 13.507, epoch: 21.0[0m
[32m[2022-08-31 17:35:15,993] [    INFO][0m - loss: 3.6e-07, learning_rate: 2.3550000000000003e-05, global_step: 430, interval_runtime: 0.8283, interval_samples_per_second: 9.658, interval_steps_per_second: 12.072, epoch: 21.5[0m
[32m[2022-08-31 17:35:16,733] [    INFO][0m - loss: 2e-07, learning_rate: 2.3400000000000003e-05, global_step: 440, interval_runtime: 0.7395, interval_samples_per_second: 10.818, interval_steps_per_second: 13.522, epoch: 22.0[0m
[32m[2022-08-31 17:35:17,571] [    INFO][0m - loss: 5.1e-07, learning_rate: 2.3250000000000003e-05, global_step: 450, interval_runtime: 0.839, interval_samples_per_second: 9.535, interval_steps_per_second: 11.919, epoch: 22.5[0m
[32m[2022-08-31 17:35:18,309] [    INFO][0m - loss: 2.2e-07, learning_rate: 2.3100000000000002e-05, global_step: 460, interval_runtime: 0.7375, interval_samples_per_second: 10.847, interval_steps_per_second: 13.559, epoch: 23.0[0m
[32m[2022-08-31 17:35:19,145] [    INFO][0m - loss: 2.9e-07, learning_rate: 2.2950000000000002e-05, global_step: 470, interval_runtime: 0.8365, interval_samples_per_second: 9.564, interval_steps_per_second: 11.954, epoch: 23.5[0m
[32m[2022-08-31 17:35:19,882] [    INFO][0m - loss: 2.7e-07, learning_rate: 2.2800000000000002e-05, global_step: 480, interval_runtime: 0.7363, interval_samples_per_second: 10.865, interval_steps_per_second: 13.581, epoch: 24.0[0m
[32m[2022-08-31 17:35:20,730] [    INFO][0m - loss: 6e-08, learning_rate: 2.265e-05, global_step: 490, interval_runtime: 0.8488, interval_samples_per_second: 9.425, interval_steps_per_second: 11.782, epoch: 24.5[0m
[32m[2022-08-31 17:35:21,557] [    INFO][0m - loss: 2.7e-07, learning_rate: 2.25e-05, global_step: 500, interval_runtime: 0.8257, interval_samples_per_second: 9.689, interval_steps_per_second: 12.111, epoch: 25.0[0m
[32m[2022-08-31 17:35:21,558] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 17:35:21,558] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 17:35:21,558] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:35:21,559] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:35:21,559] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 17:35:22,069] [    INFO][0m - eval_loss: 1.3862534761428833, eval_accuracy: 0.89375, eval_runtime: 0.51, eval_samples_per_second: 313.714, eval_steps_per_second: 9.804, epoch: 25.0[0m
[32m[2022-08-31 17:35:22,069] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-08-31 17:35:22,069] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 17:35:23,990] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-08-31 17:35:23,991] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-08-31 17:35:29,003] [    INFO][0m - loss: 1.6e-07, learning_rate: 2.235e-05, global_step: 510, interval_runtime: 7.446, interval_samples_per_second: 1.074, interval_steps_per_second: 1.343, epoch: 25.5[0m
[32m[2022-08-31 17:35:29,747] [    INFO][0m - loss: 5.9e-07, learning_rate: 2.22e-05, global_step: 520, interval_runtime: 0.7438, interval_samples_per_second: 10.755, interval_steps_per_second: 13.444, epoch: 26.0[0m
[32m[2022-08-31 17:35:30,588] [    INFO][0m - loss: 8.3e-07, learning_rate: 2.205e-05, global_step: 530, interval_runtime: 0.8423, interval_samples_per_second: 9.498, interval_steps_per_second: 11.873, epoch: 26.5[0m
[32m[2022-08-31 17:35:31,326] [    INFO][0m - loss: 6.1e-07, learning_rate: 2.19e-05, global_step: 540, interval_runtime: 0.7371, interval_samples_per_second: 10.853, interval_steps_per_second: 13.567, epoch: 27.0[0m
[32m[2022-08-31 17:35:32,152] [    INFO][0m - loss: 7e-08, learning_rate: 2.175e-05, global_step: 550, interval_runtime: 0.8265, interval_samples_per_second: 9.68, interval_steps_per_second: 12.1, epoch: 27.5[0m
[32m[2022-08-31 17:35:32,934] [    INFO][0m - loss: 4.9e-07, learning_rate: 2.16e-05, global_step: 560, interval_runtime: 0.7814, interval_samples_per_second: 10.238, interval_steps_per_second: 12.798, epoch: 28.0[0m
[32m[2022-08-31 17:35:33,800] [    INFO][0m - loss: 1.4e-07, learning_rate: 2.145e-05, global_step: 570, interval_runtime: 0.8667, interval_samples_per_second: 9.23, interval_steps_per_second: 11.538, epoch: 28.5[0m
[32m[2022-08-31 17:35:34,534] [    INFO][0m - loss: 3.9e-07, learning_rate: 2.13e-05, global_step: 580, interval_runtime: 0.7342, interval_samples_per_second: 10.896, interval_steps_per_second: 13.62, epoch: 29.0[0m
[32m[2022-08-31 17:35:35,372] [    INFO][0m - loss: 1.3e-07, learning_rate: 2.115e-05, global_step: 590, interval_runtime: 0.8376, interval_samples_per_second: 9.551, interval_steps_per_second: 11.939, epoch: 29.5[0m
[32m[2022-08-31 17:35:36,134] [    INFO][0m - loss: 2e-07, learning_rate: 2.1e-05, global_step: 600, interval_runtime: 0.7619, interval_samples_per_second: 10.499, interval_steps_per_second: 13.124, epoch: 30.0[0m
[32m[2022-08-31 17:35:36,135] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 17:35:36,135] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 17:35:36,135] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:35:36,135] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:35:36,135] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 17:35:36,645] [    INFO][0m - eval_loss: 1.4009170532226562, eval_accuracy: 0.89375, eval_runtime: 0.5094, eval_samples_per_second: 314.105, eval_steps_per_second: 9.816, epoch: 30.0[0m
[32m[2022-08-31 17:35:36,645] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-08-31 17:35:36,645] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 17:35:38,535] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-08-31 17:35:38,536] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-08-31 17:35:43,853] [    INFO][0m - loss: 8e-08, learning_rate: 2.085e-05, global_step: 610, interval_runtime: 7.7183, interval_samples_per_second: 1.036, interval_steps_per_second: 1.296, epoch: 30.5[0m
[32m[2022-08-31 17:35:44,668] [    INFO][0m - loss: 1.7e-07, learning_rate: 2.07e-05, global_step: 620, interval_runtime: 0.8154, interval_samples_per_second: 9.812, interval_steps_per_second: 12.264, epoch: 31.0[0m
[32m[2022-08-31 17:35:45,519] [    INFO][0m - loss: 1.2e-07, learning_rate: 2.055e-05, global_step: 630, interval_runtime: 0.8511, interval_samples_per_second: 9.399, interval_steps_per_second: 11.749, epoch: 31.5[0m
[32m[2022-08-31 17:35:46,261] [    INFO][0m - loss: 1.7e-07, learning_rate: 2.04e-05, global_step: 640, interval_runtime: 0.742, interval_samples_per_second: 10.781, interval_steps_per_second: 13.476, epoch: 32.0[0m
[32m[2022-08-31 17:35:47,101] [    INFO][0m - loss: 3e-07, learning_rate: 2.025e-05, global_step: 650, interval_runtime: 0.8399, interval_samples_per_second: 9.525, interval_steps_per_second: 11.907, epoch: 32.5[0m
[32m[2022-08-31 17:35:47,853] [    INFO][0m - loss: 8e-08, learning_rate: 2.01e-05, global_step: 660, interval_runtime: 0.7521, interval_samples_per_second: 10.637, interval_steps_per_second: 13.297, epoch: 33.0[0m
[32m[2022-08-31 17:35:48,695] [    INFO][0m - loss: 9e-07, learning_rate: 1.995e-05, global_step: 670, interval_runtime: 0.8419, interval_samples_per_second: 9.502, interval_steps_per_second: 11.877, epoch: 33.5[0m
[32m[2022-08-31 17:35:49,438] [    INFO][0m - loss: 2.3e-07, learning_rate: 1.98e-05, global_step: 680, interval_runtime: 0.7428, interval_samples_per_second: 10.769, interval_steps_per_second: 13.462, epoch: 34.0[0m
[32m[2022-08-31 17:35:50,280] [    INFO][0m - loss: 1.1e-07, learning_rate: 1.965e-05, global_step: 690, interval_runtime: 0.8421, interval_samples_per_second: 9.501, interval_steps_per_second: 11.876, epoch: 34.5[0m
[32m[2022-08-31 17:35:51,026] [    INFO][0m - loss: 1.7e-07, learning_rate: 1.95e-05, global_step: 700, interval_runtime: 0.7464, interval_samples_per_second: 10.718, interval_steps_per_second: 13.397, epoch: 35.0[0m
[32m[2022-08-31 17:35:51,026] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 17:35:51,027] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 17:35:51,027] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:35:51,027] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:35:51,027] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 17:35:51,544] [    INFO][0m - eval_loss: 1.4119850397109985, eval_accuracy: 0.89375, eval_runtime: 0.5172, eval_samples_per_second: 309.356, eval_steps_per_second: 9.667, epoch: 35.0[0m
[32m[2022-08-31 17:35:51,545] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-700[0m
[32m[2022-08-31 17:35:51,545] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 17:35:53,384] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-700/tokenizer_config.json[0m
[32m[2022-08-31 17:35:53,384] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-700/special_tokens_map.json[0m
[32m[2022-08-31 17:35:58,660] [    INFO][0m - loss: 1e-07, learning_rate: 1.935e-05, global_step: 710, interval_runtime: 7.6346, interval_samples_per_second: 1.048, interval_steps_per_second: 1.31, epoch: 35.5[0m
[32m[2022-08-31 17:35:59,423] [    INFO][0m - loss: 1.2e-07, learning_rate: 1.9200000000000003e-05, global_step: 720, interval_runtime: 0.7626, interval_samples_per_second: 10.491, interval_steps_per_second: 13.114, epoch: 36.0[0m
[32m[2022-08-31 17:36:00,256] [    INFO][0m - loss: 3.5e-07, learning_rate: 1.9050000000000002e-05, global_step: 730, interval_runtime: 0.8331, interval_samples_per_second: 9.603, interval_steps_per_second: 12.004, epoch: 36.5[0m
[32m[2022-08-31 17:36:01,044] [    INFO][0m - loss: 1.6e-07, learning_rate: 1.8900000000000002e-05, global_step: 740, interval_runtime: 0.7874, interval_samples_per_second: 10.16, interval_steps_per_second: 12.701, epoch: 37.0[0m
[32m[2022-08-31 17:36:01,891] [    INFO][0m - loss: 9e-08, learning_rate: 1.8750000000000002e-05, global_step: 750, interval_runtime: 0.8473, interval_samples_per_second: 9.442, interval_steps_per_second: 11.802, epoch: 37.5[0m
[32m[2022-08-31 17:36:02,639] [    INFO][0m - loss: 1.3e-07, learning_rate: 1.86e-05, global_step: 760, interval_runtime: 0.7474, interval_samples_per_second: 10.703, interval_steps_per_second: 13.379, epoch: 38.0[0m
[32m[2022-08-31 17:36:03,487] [    INFO][0m - loss: 1.6e-07, learning_rate: 1.845e-05, global_step: 770, interval_runtime: 0.8483, interval_samples_per_second: 9.43, interval_steps_per_second: 11.788, epoch: 38.5[0m
[32m[2022-08-31 17:36:04,247] [    INFO][0m - loss: 3.9e-07, learning_rate: 1.83e-05, global_step: 780, interval_runtime: 0.7598, interval_samples_per_second: 10.529, interval_steps_per_second: 13.161, epoch: 39.0[0m
[32m[2022-08-31 17:36:05,087] [    INFO][0m - loss: 1.1e-07, learning_rate: 1.815e-05, global_step: 790, interval_runtime: 0.8408, interval_samples_per_second: 9.515, interval_steps_per_second: 11.894, epoch: 39.5[0m
[32m[2022-08-31 17:36:05,833] [    INFO][0m - loss: 1.1e-07, learning_rate: 1.8e-05, global_step: 800, interval_runtime: 0.7461, interval_samples_per_second: 10.722, interval_steps_per_second: 13.403, epoch: 40.0[0m
[32m[2022-08-31 17:36:05,834] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 17:36:05,834] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 17:36:05,834] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:36:05,834] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:36:05,834] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 17:36:06,324] [    INFO][0m - eval_loss: 1.4185789823532104, eval_accuracy: 0.89375, eval_runtime: 0.4894, eval_samples_per_second: 326.921, eval_steps_per_second: 10.216, epoch: 40.0[0m
[32m[2022-08-31 17:36:06,324] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-800[0m
[32m[2022-08-31 17:36:06,324] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 17:36:08,146] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-800/tokenizer_config.json[0m
[32m[2022-08-31 17:36:08,146] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-800/special_tokens_map.json[0m
[32m[2022-08-31 17:36:13,242] [    INFO][0m - loss: 1.2e-07, learning_rate: 1.785e-05, global_step: 810, interval_runtime: 7.4079, interval_samples_per_second: 1.08, interval_steps_per_second: 1.35, epoch: 40.5[0m
[32m[2022-08-31 17:36:14,062] [    INFO][0m - loss: 1.4e-07, learning_rate: 1.77e-05, global_step: 820, interval_runtime: 0.8209, interval_samples_per_second: 9.746, interval_steps_per_second: 12.182, epoch: 41.0[0m
[32m[2022-08-31 17:36:14,937] [    INFO][0m - loss: 8e-08, learning_rate: 1.755e-05, global_step: 830, interval_runtime: 0.874, interval_samples_per_second: 9.154, interval_steps_per_second: 11.442, epoch: 41.5[0m
[32m[2022-08-31 17:36:15,679] [    INFO][0m - loss: 8e-08, learning_rate: 1.74e-05, global_step: 840, interval_runtime: 0.742, interval_samples_per_second: 10.781, interval_steps_per_second: 13.476, epoch: 42.0[0m
[32m[2022-08-31 17:36:16,530] [    INFO][0m - loss: 1e-07, learning_rate: 1.725e-05, global_step: 850, interval_runtime: 0.8514, interval_samples_per_second: 9.397, interval_steps_per_second: 11.746, epoch: 42.5[0m
[32m[2022-08-31 17:36:17,278] [    INFO][0m - loss: 1.3e-07, learning_rate: 1.71e-05, global_step: 860, interval_runtime: 0.7475, interval_samples_per_second: 10.703, interval_steps_per_second: 13.379, epoch: 43.0[0m
[32m[2022-08-31 17:36:18,141] [    INFO][0m - loss: 1.6e-07, learning_rate: 1.695e-05, global_step: 870, interval_runtime: 0.8642, interval_samples_per_second: 9.257, interval_steps_per_second: 11.572, epoch: 43.5[0m
[32m[2022-08-31 17:36:18,888] [    INFO][0m - loss: 6e-08, learning_rate: 1.6800000000000002e-05, global_step: 880, interval_runtime: 0.747, interval_samples_per_second: 10.709, interval_steps_per_second: 13.387, epoch: 44.0[0m
[32m[2022-08-31 17:36:19,737] [    INFO][0m - loss: 1.3e-07, learning_rate: 1.665e-05, global_step: 890, interval_runtime: 0.8483, interval_samples_per_second: 9.431, interval_steps_per_second: 11.789, epoch: 44.5[0m
[32m[2022-08-31 17:36:20,578] [    INFO][0m - loss: 1e-07, learning_rate: 1.65e-05, global_step: 900, interval_runtime: 0.8408, interval_samples_per_second: 9.515, interval_steps_per_second: 11.894, epoch: 45.0[0m
[32m[2022-08-31 17:36:20,578] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 17:36:20,579] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 17:36:20,579] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:36:20,579] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:36:20,579] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 17:36:21,086] [    INFO][0m - eval_loss: 1.4235141277313232, eval_accuracy: 0.89375, eval_runtime: 0.5069, eval_samples_per_second: 315.646, eval_steps_per_second: 9.864, epoch: 45.0[0m
[32m[2022-08-31 17:36:21,087] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-900[0m
[32m[2022-08-31 17:36:21,087] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 17:36:22,988] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-900/tokenizer_config.json[0m
[32m[2022-08-31 17:36:22,989] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-900/special_tokens_map.json[0m
[32m[2022-08-31 17:36:28,307] [    INFO][0m - loss: 2.7e-07, learning_rate: 1.635e-05, global_step: 910, interval_runtime: 7.7295, interval_samples_per_second: 1.035, interval_steps_per_second: 1.294, epoch: 45.5[0m
[32m[2022-08-31 17:36:29,067] [    INFO][0m - loss: 3e-08, learning_rate: 1.62e-05, global_step: 920, interval_runtime: 0.7596, interval_samples_per_second: 10.532, interval_steps_per_second: 13.165, epoch: 46.0[0m
[32m[2022-08-31 17:36:29,904] [    INFO][0m - loss: 6e-08, learning_rate: 1.605e-05, global_step: 930, interval_runtime: 0.8371, interval_samples_per_second: 9.557, interval_steps_per_second: 11.946, epoch: 46.5[0m
[32m[2022-08-31 17:36:30,661] [    INFO][0m - loss: 9e-08, learning_rate: 1.59e-05, global_step: 940, interval_runtime: 0.7572, interval_samples_per_second: 10.566, interval_steps_per_second: 13.207, epoch: 47.0[0m
[32m[2022-08-31 17:36:31,530] [    INFO][0m - loss: 1.1e-07, learning_rate: 1.575e-05, global_step: 950, interval_runtime: 0.8687, interval_samples_per_second: 9.209, interval_steps_per_second: 11.511, epoch: 47.5[0m
[32m[2022-08-31 17:36:32,287] [    INFO][0m - loss: 9e-08, learning_rate: 1.56e-05, global_step: 960, interval_runtime: 0.7573, interval_samples_per_second: 10.563, interval_steps_per_second: 13.204, epoch: 48.0[0m
[32m[2022-08-31 17:36:33,183] [    INFO][0m - loss: 8e-08, learning_rate: 1.545e-05, global_step: 970, interval_runtime: 0.8967, interval_samples_per_second: 8.922, interval_steps_per_second: 11.153, epoch: 48.5[0m
[32m[2022-08-31 17:36:33,941] [    INFO][0m - loss: 1e-07, learning_rate: 1.53e-05, global_step: 980, interval_runtime: 0.7571, interval_samples_per_second: 10.566, interval_steps_per_second: 13.208, epoch: 49.0[0m
[32m[2022-08-31 17:36:34,781] [    INFO][0m - loss: 2.3e-07, learning_rate: 1.515e-05, global_step: 990, interval_runtime: 0.8399, interval_samples_per_second: 9.525, interval_steps_per_second: 11.906, epoch: 49.5[0m
[32m[2022-08-31 17:36:35,531] [    INFO][0m - loss: 4e-08, learning_rate: 1.5e-05, global_step: 1000, interval_runtime: 0.7503, interval_samples_per_second: 10.662, interval_steps_per_second: 13.327, epoch: 50.0[0m
[32m[2022-08-31 17:36:35,531] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 17:36:35,532] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 17:36:35,532] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:36:35,532] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:36:35,532] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 17:36:36,026] [    INFO][0m - eval_loss: 1.4291818141937256, eval_accuracy: 0.89375, eval_runtime: 0.4937, eval_samples_per_second: 324.066, eval_steps_per_second: 10.127, epoch: 50.0[0m
[32m[2022-08-31 17:36:36,026] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1000[0m
[32m[2022-08-31 17:36:36,026] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 17:36:37,899] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1000/tokenizer_config.json[0m
[32m[2022-08-31 17:36:37,899] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1000/special_tokens_map.json[0m
[32m[2022-08-31 17:36:42,891] [    INFO][0m - loss: 1.5e-07, learning_rate: 1.485e-05, global_step: 1010, interval_runtime: 7.3598, interval_samples_per_second: 1.087, interval_steps_per_second: 1.359, epoch: 50.5[0m
[32m[2022-08-31 17:36:43,657] [    INFO][0m - loss: 1.5e-07, learning_rate: 1.47e-05, global_step: 1020, interval_runtime: 0.7667, interval_samples_per_second: 10.434, interval_steps_per_second: 13.043, epoch: 51.0[0m
[32m[2022-08-31 17:36:44,527] [    INFO][0m - loss: 5e-08, learning_rate: 1.455e-05, global_step: 1030, interval_runtime: 0.8688, interval_samples_per_second: 9.208, interval_steps_per_second: 11.51, epoch: 51.5[0m
[32m[2022-08-31 17:36:45,291] [    INFO][0m - loss: 5e-08, learning_rate: 1.44e-05, global_step: 1040, interval_runtime: 0.7643, interval_samples_per_second: 10.467, interval_steps_per_second: 13.084, epoch: 52.0[0m
[32m[2022-08-31 17:36:46,146] [    INFO][0m - loss: 1.5e-07, learning_rate: 1.4249999999999999e-05, global_step: 1050, interval_runtime: 0.8554, interval_samples_per_second: 9.353, interval_steps_per_second: 11.691, epoch: 52.5[0m
[32m[2022-08-31 17:36:46,897] [    INFO][0m - loss: 5e-08, learning_rate: 1.4099999999999999e-05, global_step: 1060, interval_runtime: 0.7509, interval_samples_per_second: 10.653, interval_steps_per_second: 13.317, epoch: 53.0[0m
[32m[2022-08-31 17:36:47,756] [    INFO][0m - loss: 8e-08, learning_rate: 1.395e-05, global_step: 1070, interval_runtime: 0.8593, interval_samples_per_second: 9.31, interval_steps_per_second: 11.637, epoch: 53.5[0m
[32m[2022-08-31 17:36:48,500] [    INFO][0m - loss: 3.1e-07, learning_rate: 1.3800000000000002e-05, global_step: 1080, interval_runtime: 0.7438, interval_samples_per_second: 10.756, interval_steps_per_second: 13.444, epoch: 54.0[0m
[32m[2022-08-31 17:36:49,334] [    INFO][0m - loss: 4e-08, learning_rate: 1.3650000000000001e-05, global_step: 1090, interval_runtime: 0.8331, interval_samples_per_second: 9.603, interval_steps_per_second: 12.003, epoch: 54.5[0m
[32m[2022-08-31 17:36:50,081] [    INFO][0m - loss: 9e-08, learning_rate: 1.3500000000000001e-05, global_step: 1100, interval_runtime: 0.7476, interval_samples_per_second: 10.701, interval_steps_per_second: 13.376, epoch: 55.0[0m
[32m[2022-08-31 17:36:50,081] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 17:36:50,081] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 17:36:50,082] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:36:50,082] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:36:50,082] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 17:36:50,576] [    INFO][0m - eval_loss: 1.4342026710510254, eval_accuracy: 0.89375, eval_runtime: 0.4946, eval_samples_per_second: 323.514, eval_steps_per_second: 10.11, epoch: 55.0[0m
[32m[2022-08-31 17:36:50,577] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1100[0m
[32m[2022-08-31 17:36:50,577] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 17:36:52,404] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1100/tokenizer_config.json[0m
[32m[2022-08-31 17:36:52,405] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1100/special_tokens_map.json[0m
[32m[2022-08-31 17:36:57,271] [    INFO][0m - loss: 2.4e-07, learning_rate: 1.3350000000000001e-05, global_step: 1110, interval_runtime: 7.1894, interval_samples_per_second: 1.113, interval_steps_per_second: 1.391, epoch: 55.5[0m
[32m[2022-08-31 17:36:58,011] [    INFO][0m - loss: 1e-07, learning_rate: 1.32e-05, global_step: 1120, interval_runtime: 0.7414, interval_samples_per_second: 10.791, interval_steps_per_second: 13.489, epoch: 56.0[0m
[32m[2022-08-31 17:36:58,853] [    INFO][0m - loss: 9e-08, learning_rate: 1.305e-05, global_step: 1130, interval_runtime: 0.8413, interval_samples_per_second: 9.51, interval_steps_per_second: 11.887, epoch: 56.5[0m
[32m[2022-08-31 17:36:59,603] [    INFO][0m - loss: 1.9e-07, learning_rate: 1.29e-05, global_step: 1140, interval_runtime: 0.7501, interval_samples_per_second: 10.665, interval_steps_per_second: 13.331, epoch: 57.0[0m
[32m[2022-08-31 17:37:00,428] [    INFO][0m - loss: 1.4e-07, learning_rate: 1.275e-05, global_step: 1150, interval_runtime: 0.8248, interval_samples_per_second: 9.699, interval_steps_per_second: 12.123, epoch: 57.5[0m
[32m[2022-08-31 17:37:01,172] [    INFO][0m - loss: 5e-08, learning_rate: 1.26e-05, global_step: 1160, interval_runtime: 0.7436, interval_samples_per_second: 10.758, interval_steps_per_second: 13.448, epoch: 58.0[0m
[32m[2022-08-31 17:37:02,013] [    INFO][0m - loss: 8e-08, learning_rate: 1.245e-05, global_step: 1170, interval_runtime: 0.8413, interval_samples_per_second: 9.509, interval_steps_per_second: 11.886, epoch: 58.5[0m
[32m[2022-08-31 17:37:02,771] [    INFO][0m - loss: 1.1e-07, learning_rate: 1.2299999999999999e-05, global_step: 1180, interval_runtime: 0.7583, interval_samples_per_second: 10.55, interval_steps_per_second: 13.187, epoch: 59.0[0m
[32m[2022-08-31 17:37:03,635] [    INFO][0m - loss: 2e-07, learning_rate: 1.215e-05, global_step: 1190, interval_runtime: 0.8643, interval_samples_per_second: 9.256, interval_steps_per_second: 11.57, epoch: 59.5[0m
[32m[2022-08-31 17:37:04,411] [    INFO][0m - loss: 1.3e-07, learning_rate: 1.2e-05, global_step: 1200, interval_runtime: 0.7754, interval_samples_per_second: 10.318, interval_steps_per_second: 12.897, epoch: 60.0[0m
[32m[2022-08-31 17:37:04,412] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 17:37:04,412] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 17:37:04,412] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:37:04,412] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:37:04,412] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 17:37:04,927] [    INFO][0m - eval_loss: 1.4382350444793701, eval_accuracy: 0.89375, eval_runtime: 0.5146, eval_samples_per_second: 310.93, eval_steps_per_second: 9.717, epoch: 60.0[0m
[32m[2022-08-31 17:37:04,927] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1200[0m
[32m[2022-08-31 17:37:04,928] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 17:37:06,721] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1200/tokenizer_config.json[0m
[32m[2022-08-31 17:37:06,721] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1200/special_tokens_map.json[0m
[32m[2022-08-31 17:37:11,584] [    INFO][0m - loss: 1.3e-07, learning_rate: 1.185e-05, global_step: 1210, interval_runtime: 7.1732, interval_samples_per_second: 1.115, interval_steps_per_second: 1.394, epoch: 60.5[0m
[32m[2022-08-31 17:37:12,331] [    INFO][0m - loss: 1.1e-07, learning_rate: 1.1700000000000001e-05, global_step: 1220, interval_runtime: 0.7472, interval_samples_per_second: 10.707, interval_steps_per_second: 13.383, epoch: 61.0[0m
[32m[2022-08-31 17:37:13,164] [    INFO][0m - loss: 1.2e-07, learning_rate: 1.1550000000000001e-05, global_step: 1230, interval_runtime: 0.833, interval_samples_per_second: 9.603, interval_steps_per_second: 12.004, epoch: 61.5[0m
[32m[2022-08-31 17:37:13,917] [    INFO][0m - loss: 3e-07, learning_rate: 1.1400000000000001e-05, global_step: 1240, interval_runtime: 0.7531, interval_samples_per_second: 10.622, interval_steps_per_second: 13.278, epoch: 62.0[0m
[32m[2022-08-31 17:37:14,756] [    INFO][0m - loss: 5e-08, learning_rate: 1.125e-05, global_step: 1250, interval_runtime: 0.8384, interval_samples_per_second: 9.542, interval_steps_per_second: 11.927, epoch: 62.5[0m
[32m[2022-08-31 17:37:15,501] [    INFO][0m - loss: 9e-08, learning_rate: 1.11e-05, global_step: 1260, interval_runtime: 0.7452, interval_samples_per_second: 10.735, interval_steps_per_second: 13.419, epoch: 63.0[0m
[32m[2022-08-31 17:37:16,354] [    INFO][0m - loss: 7e-08, learning_rate: 1.095e-05, global_step: 1270, interval_runtime: 0.8477, interval_samples_per_second: 9.437, interval_steps_per_second: 11.796, epoch: 63.5[0m
[32m[2022-08-31 17:37:17,123] [    INFO][0m - loss: 1.1e-07, learning_rate: 1.08e-05, global_step: 1280, interval_runtime: 0.7746, interval_samples_per_second: 10.328, interval_steps_per_second: 12.91, epoch: 64.0[0m
[32m[2022-08-31 17:37:17,971] [    INFO][0m - loss: 1e-07, learning_rate: 1.065e-05, global_step: 1290, interval_runtime: 0.8479, interval_samples_per_second: 9.435, interval_steps_per_second: 11.793, epoch: 64.5[0m
[32m[2022-08-31 17:37:18,711] [    INFO][0m - loss: 5e-08, learning_rate: 1.05e-05, global_step: 1300, interval_runtime: 0.7389, interval_samples_per_second: 10.827, interval_steps_per_second: 13.534, epoch: 65.0[0m
[32m[2022-08-31 17:37:18,711] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 17:37:18,711] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 17:37:18,711] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:37:18,712] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:37:18,712] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 17:37:19,209] [    INFO][0m - eval_loss: 1.4434881210327148, eval_accuracy: 0.89375, eval_runtime: 0.4973, eval_samples_per_second: 321.76, eval_steps_per_second: 10.055, epoch: 65.0[0m
[32m[2022-08-31 17:37:19,210] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1300[0m
[32m[2022-08-31 17:37:19,210] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 17:37:21,009] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1300/tokenizer_config.json[0m
[32m[2022-08-31 17:37:21,009] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1300/special_tokens_map.json[0m
[32m[2022-08-31 17:37:26,024] [    INFO][0m - loss: 1.7e-07, learning_rate: 1.035e-05, global_step: 1310, interval_runtime: 7.3138, interval_samples_per_second: 1.094, interval_steps_per_second: 1.367, epoch: 65.5[0m
[32m[2022-08-31 17:37:26,767] [    INFO][0m - loss: 1.5e-07, learning_rate: 1.02e-05, global_step: 1320, interval_runtime: 0.7431, interval_samples_per_second: 10.766, interval_steps_per_second: 13.458, epoch: 66.0[0m
[32m[2022-08-31 17:37:27,619] [    INFO][0m - loss: 1.1e-07, learning_rate: 1.005e-05, global_step: 1330, interval_runtime: 0.8518, interval_samples_per_second: 9.392, interval_steps_per_second: 11.74, epoch: 66.5[0m
[32m[2022-08-31 17:37:28,362] [    INFO][0m - loss: 1.1e-07, learning_rate: 9.9e-06, global_step: 1340, interval_runtime: 0.7431, interval_samples_per_second: 10.766, interval_steps_per_second: 13.457, epoch: 67.0[0m
[32m[2022-08-31 17:37:29,180] [    INFO][0m - loss: 9e-08, learning_rate: 9.75e-06, global_step: 1350, interval_runtime: 0.8176, interval_samples_per_second: 9.784, interval_steps_per_second: 12.23, epoch: 67.5[0m
[32m[2022-08-31 17:37:29,970] [    INFO][0m - loss: 1e-07, learning_rate: 9.600000000000001e-06, global_step: 1360, interval_runtime: 0.7901, interval_samples_per_second: 10.125, interval_steps_per_second: 12.656, epoch: 68.0[0m
[32m[2022-08-31 17:37:30,795] [    INFO][0m - loss: 1.1e-07, learning_rate: 9.450000000000001e-06, global_step: 1370, interval_runtime: 0.8248, interval_samples_per_second: 9.7, interval_steps_per_second: 12.125, epoch: 68.5[0m
[32m[2022-08-31 17:37:31,523] [    INFO][0m - loss: 7e-08, learning_rate: 9.3e-06, global_step: 1380, interval_runtime: 0.7277, interval_samples_per_second: 10.993, interval_steps_per_second: 13.742, epoch: 69.0[0m
[32m[2022-08-31 17:37:32,360] [    INFO][0m - loss: 1.7e-07, learning_rate: 9.15e-06, global_step: 1390, interval_runtime: 0.8382, interval_samples_per_second: 9.544, interval_steps_per_second: 11.931, epoch: 69.5[0m
[32m[2022-08-31 17:37:33,104] [    INFO][0m - loss: 5.4e-07, learning_rate: 9e-06, global_step: 1400, interval_runtime: 0.7434, interval_samples_per_second: 10.761, interval_steps_per_second: 13.451, epoch: 70.0[0m
[32m[2022-08-31 17:37:33,105] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 17:37:33,105] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 17:37:33,105] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:37:33,105] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:37:33,105] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 17:37:33,602] [    INFO][0m - eval_loss: 1.4474289417266846, eval_accuracy: 0.89375, eval_runtime: 0.4968, eval_samples_per_second: 322.054, eval_steps_per_second: 10.064, epoch: 70.0[0m
[32m[2022-08-31 17:37:33,602] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1400[0m
[32m[2022-08-31 17:37:33,602] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 17:37:35,373] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1400/tokenizer_config.json[0m
[32m[2022-08-31 17:37:35,373] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1400/special_tokens_map.json[0m
[32m[2022-08-31 17:37:40,298] [    INFO][0m - loss: 5e-08, learning_rate: 8.85e-06, global_step: 1410, interval_runtime: 7.1941, interval_samples_per_second: 1.112, interval_steps_per_second: 1.39, epoch: 70.5[0m
[32m[2022-08-31 17:37:41,036] [    INFO][0m - loss: 1.8e-07, learning_rate: 8.7e-06, global_step: 1420, interval_runtime: 0.7383, interval_samples_per_second: 10.836, interval_steps_per_second: 13.545, epoch: 71.0[0m
[32m[2022-08-31 17:37:41,935] [    INFO][0m - loss: 4e-08, learning_rate: 8.55e-06, global_step: 1430, interval_runtime: 0.8982, interval_samples_per_second: 8.906, interval_steps_per_second: 11.133, epoch: 71.5[0m
[32m[2022-08-31 17:37:42,684] [    INFO][0m - loss: 9e-08, learning_rate: 8.400000000000001e-06, global_step: 1440, interval_runtime: 0.7497, interval_samples_per_second: 10.67, interval_steps_per_second: 13.338, epoch: 72.0[0m
[32m[2022-08-31 17:37:43,537] [    INFO][0m - loss: 1.4e-07, learning_rate: 8.25e-06, global_step: 1450, interval_runtime: 0.8526, interval_samples_per_second: 9.383, interval_steps_per_second: 11.728, epoch: 72.5[0m
[32m[2022-08-31 17:37:44,279] [    INFO][0m - loss: 6e-08, learning_rate: 8.1e-06, global_step: 1460, interval_runtime: 0.742, interval_samples_per_second: 10.781, interval_steps_per_second: 13.476, epoch: 73.0[0m
[32m[2022-08-31 17:37:45,113] [    INFO][0m - loss: 8e-08, learning_rate: 7.95e-06, global_step: 1470, interval_runtime: 0.8335, interval_samples_per_second: 9.598, interval_steps_per_second: 11.997, epoch: 73.5[0m
[32m[2022-08-31 17:37:45,867] [    INFO][0m - loss: 1.8e-07, learning_rate: 7.8e-06, global_step: 1480, interval_runtime: 0.7542, interval_samples_per_second: 10.607, interval_steps_per_second: 13.258, epoch: 74.0[0m
[32m[2022-08-31 17:37:46,700] [    INFO][0m - loss: 6e-08, learning_rate: 7.65e-06, global_step: 1490, interval_runtime: 0.8327, interval_samples_per_second: 9.607, interval_steps_per_second: 12.009, epoch: 74.5[0m
[32m[2022-08-31 17:37:47,456] [    INFO][0m - loss: 6e-08, learning_rate: 7.5e-06, global_step: 1500, interval_runtime: 0.756, interval_samples_per_second: 10.582, interval_steps_per_second: 13.228, epoch: 75.0[0m
[32m[2022-08-31 17:37:47,456] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 17:37:47,456] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 17:37:47,456] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:37:47,457] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:37:47,457] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 17:37:47,972] [    INFO][0m - eval_loss: 1.4505980014801025, eval_accuracy: 0.89375, eval_runtime: 0.5157, eval_samples_per_second: 310.266, eval_steps_per_second: 9.696, epoch: 75.0[0m
[32m[2022-08-31 17:37:47,973] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1500[0m
[32m[2022-08-31 17:37:47,973] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 17:37:49,801] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1500/tokenizer_config.json[0m
[32m[2022-08-31 17:37:49,801] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1500/special_tokens_map.json[0m
[32m[2022-08-31 17:37:54,678] [    INFO][0m - loss: 1.3e-07, learning_rate: 7.35e-06, global_step: 1510, interval_runtime: 7.2223, interval_samples_per_second: 1.108, interval_steps_per_second: 1.385, epoch: 75.5[0m
[32m[2022-08-31 17:37:55,418] [    INFO][0m - loss: 9e-08, learning_rate: 7.2e-06, global_step: 1520, interval_runtime: 0.74, interval_samples_per_second: 10.811, interval_steps_per_second: 13.513, epoch: 76.0[0m
[32m[2022-08-31 17:37:56,273] [    INFO][0m - loss: 7e-08, learning_rate: 7.049999999999999e-06, global_step: 1530, interval_runtime: 0.8556, interval_samples_per_second: 9.35, interval_steps_per_second: 11.687, epoch: 76.5[0m
[32m[2022-08-31 17:37:57,011] [    INFO][0m - loss: 1.1e-07, learning_rate: 6.900000000000001e-06, global_step: 1540, interval_runtime: 0.7371, interval_samples_per_second: 10.853, interval_steps_per_second: 13.566, epoch: 77.0[0m
[32m[2022-08-31 17:37:57,855] [    INFO][0m - loss: 3.3e-07, learning_rate: 6.750000000000001e-06, global_step: 1550, interval_runtime: 0.844, interval_samples_per_second: 9.478, interval_steps_per_second: 11.848, epoch: 77.5[0m
[32m[2022-08-31 17:37:58,594] [    INFO][0m - loss: 1e-07, learning_rate: 6.6e-06, global_step: 1560, interval_runtime: 0.7397, interval_samples_per_second: 10.815, interval_steps_per_second: 13.519, epoch: 78.0[0m
[32m[2022-08-31 17:37:59,421] [    INFO][0m - loss: 3e-08, learning_rate: 6.45e-06, global_step: 1570, interval_runtime: 0.8267, interval_samples_per_second: 9.677, interval_steps_per_second: 12.097, epoch: 78.5[0m
[32m[2022-08-31 17:38:00,154] [    INFO][0m - loss: 9e-08, learning_rate: 6.3e-06, global_step: 1580, interval_runtime: 0.7331, interval_samples_per_second: 10.913, interval_steps_per_second: 13.641, epoch: 79.0[0m
[32m[2022-08-31 17:38:00,997] [    INFO][0m - loss: 1.8e-07, learning_rate: 6.1499999999999996e-06, global_step: 1590, interval_runtime: 0.8427, interval_samples_per_second: 9.494, interval_steps_per_second: 11.867, epoch: 79.5[0m
[32m[2022-08-31 17:38:01,736] [    INFO][0m - loss: 6e-08, learning_rate: 6e-06, global_step: 1600, interval_runtime: 0.7388, interval_samples_per_second: 10.828, interval_steps_per_second: 13.535, epoch: 80.0[0m
[32m[2022-08-31 17:38:01,736] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 17:38:01,736] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 17:38:01,736] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:38:01,736] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:38:01,737] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 17:38:02,237] [    INFO][0m - eval_loss: 1.453368902206421, eval_accuracy: 0.89375, eval_runtime: 0.4996, eval_samples_per_second: 320.245, eval_steps_per_second: 10.008, epoch: 80.0[0m
[32m[2022-08-31 17:38:02,237] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1600[0m
[32m[2022-08-31 17:38:02,237] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 17:38:04,053] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1600/tokenizer_config.json[0m
[32m[2022-08-31 17:38:04,053] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1600/special_tokens_map.json[0m
[32m[2022-08-31 17:38:09,131] [    INFO][0m - loss: 6e-08, learning_rate: 5.850000000000001e-06, global_step: 1610, interval_runtime: 7.3953, interval_samples_per_second: 1.082, interval_steps_per_second: 1.352, epoch: 80.5[0m
[32m[2022-08-31 17:38:09,877] [    INFO][0m - loss: 8e-08, learning_rate: 5.7000000000000005e-06, global_step: 1620, interval_runtime: 0.7456, interval_samples_per_second: 10.729, interval_steps_per_second: 13.412, epoch: 81.0[0m
[32m[2022-08-31 17:38:10,744] [    INFO][0m - loss: 4.2e-07, learning_rate: 5.55e-06, global_step: 1630, interval_runtime: 0.8676, interval_samples_per_second: 9.221, interval_steps_per_second: 11.526, epoch: 81.5[0m
[32m[2022-08-31 17:38:11,523] [    INFO][0m - loss: 7e-08, learning_rate: 5.4e-06, global_step: 1640, interval_runtime: 0.7786, interval_samples_per_second: 10.274, interval_steps_per_second: 12.843, epoch: 82.0[0m
[32m[2022-08-31 17:38:12,402] [    INFO][0m - loss: 1.3e-07, learning_rate: 5.25e-06, global_step: 1650, interval_runtime: 0.8793, interval_samples_per_second: 9.098, interval_steps_per_second: 11.373, epoch: 82.5[0m
[32m[2022-08-31 17:38:13,163] [    INFO][0m - loss: 5e-08, learning_rate: 5.1e-06, global_step: 1660, interval_runtime: 0.76, interval_samples_per_second: 10.526, interval_steps_per_second: 13.157, epoch: 83.0[0m
[32m[2022-08-31 17:38:14,002] [    INFO][0m - loss: 7e-08, learning_rate: 4.95e-06, global_step: 1670, interval_runtime: 0.84, interval_samples_per_second: 9.524, interval_steps_per_second: 11.905, epoch: 83.5[0m
[32m[2022-08-31 17:38:14,752] [    INFO][0m - loss: 1.3e-07, learning_rate: 4.800000000000001e-06, global_step: 1680, interval_runtime: 0.7498, interval_samples_per_second: 10.67, interval_steps_per_second: 13.337, epoch: 84.0[0m
[32m[2022-08-31 17:38:15,607] [    INFO][0m - loss: 6e-08, learning_rate: 4.65e-06, global_step: 1690, interval_runtime: 0.8543, interval_samples_per_second: 9.364, interval_steps_per_second: 11.705, epoch: 84.5[0m
[32m[2022-08-31 17:38:16,357] [    INFO][0m - loss: 3.4e-07, learning_rate: 4.5e-06, global_step: 1700, interval_runtime: 0.7503, interval_samples_per_second: 10.662, interval_steps_per_second: 13.327, epoch: 85.0[0m
[32m[2022-08-31 17:38:16,358] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 17:38:16,358] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 17:38:16,358] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:38:16,358] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:38:16,358] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 17:38:16,858] [    INFO][0m - eval_loss: 1.452101469039917, eval_accuracy: 0.89375, eval_runtime: 0.4998, eval_samples_per_second: 320.158, eval_steps_per_second: 10.005, epoch: 85.0[0m
[32m[2022-08-31 17:38:16,858] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1700[0m
[32m[2022-08-31 17:38:16,859] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 17:38:18,666] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1700/tokenizer_config.json[0m
[32m[2022-08-31 17:38:18,667] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1700/special_tokens_map.json[0m
[32m[2022-08-31 17:38:23,865] [    INFO][0m - loss: 2.5e-07, learning_rate: 4.35e-06, global_step: 1710, interval_runtime: 7.508, interval_samples_per_second: 1.066, interval_steps_per_second: 1.332, epoch: 85.5[0m
[32m[2022-08-31 17:38:24,615] [    INFO][0m - loss: 5e-08, learning_rate: 4.2000000000000004e-06, global_step: 1720, interval_runtime: 0.7505, interval_samples_per_second: 10.66, interval_steps_per_second: 13.325, epoch: 86.0[0m
[32m[2022-08-31 17:38:25,445] [    INFO][0m - loss: 1.1e-07, learning_rate: 4.05e-06, global_step: 1730, interval_runtime: 0.8302, interval_samples_per_second: 9.637, interval_steps_per_second: 12.046, epoch: 86.5[0m
[32m[2022-08-31 17:38:26,180] [    INFO][0m - loss: 5e-08, learning_rate: 3.9e-06, global_step: 1740, interval_runtime: 0.7342, interval_samples_per_second: 10.896, interval_steps_per_second: 13.62, epoch: 87.0[0m
[32m[2022-08-31 17:38:27,035] [    INFO][0m - loss: 2.3e-07, learning_rate: 3.75e-06, global_step: 1750, interval_runtime: 0.855, interval_samples_per_second: 9.357, interval_steps_per_second: 11.696, epoch: 87.5[0m
[32m[2022-08-31 17:38:27,791] [    INFO][0m - loss: 3e-08, learning_rate: 3.6e-06, global_step: 1760, interval_runtime: 0.7564, interval_samples_per_second: 10.576, interval_steps_per_second: 13.22, epoch: 88.0[0m
[32m[2022-08-31 17:38:28,639] [    INFO][0m - loss: 7e-08, learning_rate: 3.4500000000000004e-06, global_step: 1770, interval_runtime: 0.8483, interval_samples_per_second: 9.431, interval_steps_per_second: 11.788, epoch: 88.5[0m
[32m[2022-08-31 17:38:29,393] [    INFO][0m - loss: 1.1e-07, learning_rate: 3.3e-06, global_step: 1780, interval_runtime: 0.7538, interval_samples_per_second: 10.613, interval_steps_per_second: 13.267, epoch: 89.0[0m
[32m[2022-08-31 17:38:30,236] [    INFO][0m - loss: 5e-08, learning_rate: 3.15e-06, global_step: 1790, interval_runtime: 0.8425, interval_samples_per_second: 9.496, interval_steps_per_second: 11.87, epoch: 89.5[0m
[32m[2022-08-31 17:38:30,972] [    INFO][0m - loss: 8e-08, learning_rate: 3e-06, global_step: 1800, interval_runtime: 0.7362, interval_samples_per_second: 10.866, interval_steps_per_second: 13.583, epoch: 90.0[0m
[32m[2022-08-31 17:38:30,973] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 17:38:30,973] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 17:38:30,973] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:38:30,973] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:38:30,973] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 17:38:31,480] [    INFO][0m - eval_loss: 1.4533674716949463, eval_accuracy: 0.89375, eval_runtime: 0.5059, eval_samples_per_second: 316.268, eval_steps_per_second: 9.883, epoch: 90.0[0m
[32m[2022-08-31 17:38:31,480] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1800[0m
[32m[2022-08-31 17:38:31,481] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 17:38:33,298] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1800/tokenizer_config.json[0m
[32m[2022-08-31 17:38:33,299] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1800/special_tokens_map.json[0m
[32m[2022-08-31 17:38:38,298] [    INFO][0m - loss: 5e-08, learning_rate: 2.8500000000000002e-06, global_step: 1810, interval_runtime: 7.3254, interval_samples_per_second: 1.092, interval_steps_per_second: 1.365, epoch: 90.5[0m
[32m[2022-08-31 17:38:39,044] [    INFO][0m - loss: 9e-08, learning_rate: 2.7e-06, global_step: 1820, interval_runtime: 0.7473, interval_samples_per_second: 10.705, interval_steps_per_second: 13.381, epoch: 91.0[0m
[32m[2022-08-31 17:38:39,906] [    INFO][0m - loss: 3.4e-07, learning_rate: 2.55e-06, global_step: 1830, interval_runtime: 0.8609, interval_samples_per_second: 9.292, interval_steps_per_second: 11.615, epoch: 91.5[0m
[32m[2022-08-31 17:38:40,678] [    INFO][0m - loss: 1.4e-07, learning_rate: 2.4000000000000003e-06, global_step: 1840, interval_runtime: 0.7715, interval_samples_per_second: 10.369, interval_steps_per_second: 12.961, epoch: 92.0[0m
[32m[2022-08-31 17:38:41,522] [    INFO][0m - loss: 5e-08, learning_rate: 2.25e-06, global_step: 1850, interval_runtime: 0.8445, interval_samples_per_second: 9.473, interval_steps_per_second: 11.841, epoch: 92.5[0m
[32m[2022-08-31 17:38:42,306] [    INFO][0m - loss: 1.3e-07, learning_rate: 2.1000000000000002e-06, global_step: 1860, interval_runtime: 0.7834, interval_samples_per_second: 10.212, interval_steps_per_second: 12.766, epoch: 93.0[0m
[32m[2022-08-31 17:38:43,147] [    INFO][0m - loss: 3e-08, learning_rate: 1.95e-06, global_step: 1870, interval_runtime: 0.842, interval_samples_per_second: 9.501, interval_steps_per_second: 11.876, epoch: 93.5[0m
[32m[2022-08-31 17:38:43,922] [    INFO][0m - loss: 8e-08, learning_rate: 1.8e-06, global_step: 1880, interval_runtime: 0.7755, interval_samples_per_second: 10.316, interval_steps_per_second: 12.895, epoch: 94.0[0m
[32m[2022-08-31 17:38:44,775] [    INFO][0m - loss: 1e-07, learning_rate: 1.65e-06, global_step: 1890, interval_runtime: 0.8529, interval_samples_per_second: 9.38, interval_steps_per_second: 11.725, epoch: 94.5[0m
[32m[2022-08-31 17:38:45,547] [    INFO][0m - loss: 7e-08, learning_rate: 1.5e-06, global_step: 1900, interval_runtime: 0.7708, interval_samples_per_second: 10.379, interval_steps_per_second: 12.973, epoch: 95.0[0m
[32m[2022-08-31 17:38:45,547] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 17:38:45,547] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 17:38:45,547] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:38:45,547] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:38:45,548] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 17:38:46,050] [    INFO][0m - eval_loss: 1.4547327756881714, eval_accuracy: 0.89375, eval_runtime: 0.5025, eval_samples_per_second: 318.419, eval_steps_per_second: 9.951, epoch: 95.0[0m
[32m[2022-08-31 17:38:46,051] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1900[0m
[32m[2022-08-31 17:38:46,051] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 17:38:47,991] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1900/tokenizer_config.json[0m
[32m[2022-08-31 17:38:47,992] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1900/special_tokens_map.json[0m
[32m[2022-08-31 17:38:53,005] [    INFO][0m - loss: 2.3e-07, learning_rate: 1.35e-06, global_step: 1910, interval_runtime: 7.4593, interval_samples_per_second: 1.072, interval_steps_per_second: 1.341, epoch: 95.5[0m
[32m[2022-08-31 17:38:53,755] [    INFO][0m - loss: 4e-08, learning_rate: 1.2000000000000002e-06, global_step: 1920, interval_runtime: 0.7497, interval_samples_per_second: 10.671, interval_steps_per_second: 13.339, epoch: 96.0[0m
[32m[2022-08-31 17:38:54,635] [    INFO][0m - loss: 5e-08, learning_rate: 1.0500000000000001e-06, global_step: 1930, interval_runtime: 0.8794, interval_samples_per_second: 9.097, interval_steps_per_second: 11.371, epoch: 96.5[0m
[32m[2022-08-31 17:38:55,413] [    INFO][0m - loss: 9e-08, learning_rate: 9e-07, global_step: 1940, interval_runtime: 0.7781, interval_samples_per_second: 10.282, interval_steps_per_second: 12.853, epoch: 97.0[0m
[32m[2022-08-31 17:38:56,270] [    INFO][0m - loss: 1.1e-07, learning_rate: 7.5e-07, global_step: 1950, interval_runtime: 0.8569, interval_samples_per_second: 9.336, interval_steps_per_second: 11.67, epoch: 97.5[0m
[32m[2022-08-31 17:38:57,036] [    INFO][0m - loss: 6e-08, learning_rate: 6.000000000000001e-07, global_step: 1960, interval_runtime: 0.767, interval_samples_per_second: 10.43, interval_steps_per_second: 13.038, epoch: 98.0[0m
[32m[2022-08-31 17:38:57,870] [    INFO][0m - loss: 4e-08, learning_rate: 4.5e-07, global_step: 1970, interval_runtime: 0.8336, interval_samples_per_second: 9.597, interval_steps_per_second: 11.997, epoch: 98.5[0m
[32m[2022-08-31 17:38:58,618] [    INFO][0m - loss: 5e-08, learning_rate: 3.0000000000000004e-07, global_step: 1980, interval_runtime: 0.7484, interval_samples_per_second: 10.689, interval_steps_per_second: 13.361, epoch: 99.0[0m
[32m[2022-08-31 17:38:59,470] [    INFO][0m - loss: 9e-08, learning_rate: 1.5000000000000002e-07, global_step: 1990, interval_runtime: 0.8512, interval_samples_per_second: 9.398, interval_steps_per_second: 11.748, epoch: 99.5[0m
[32m[2022-08-31 17:39:00,223] [    INFO][0m - loss: 6e-08, learning_rate: 0.0, global_step: 2000, interval_runtime: 0.7533, interval_samples_per_second: 10.619, interval_steps_per_second: 13.274, epoch: 100.0[0m
[32m[2022-08-31 17:39:00,224] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 17:39:00,224] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 17:39:00,224] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:39:00,224] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:39:00,224] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 17:39:00,733] [    INFO][0m - eval_loss: 1.4550288915634155, eval_accuracy: 0.89375, eval_runtime: 0.5083, eval_samples_per_second: 314.765, eval_steps_per_second: 9.836, epoch: 100.0[0m
[32m[2022-08-31 17:39:00,733] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2000[0m
[32m[2022-08-31 17:39:00,733] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 17:39:02,556] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2000/tokenizer_config.json[0m
[32m[2022-08-31 17:39:02,557] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2000/special_tokens_map.json[0m
[32m[2022-08-31 17:39:06,635] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-31 17:39:06,636] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-200 (score: 0.89375).[0m
[32m[2022-08-31 17:39:07,316] [    INFO][0m - train_runtime: 294.3048, train_samples_per_second: 54.365, train_steps_per_second: 6.796, train_loss: 0.007865962062192238, epoch: 100.0[0m
[32m[2022-08-31 17:39:07,317] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-31 17:39:07,318] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 17:39:09,200] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-31 17:39:09,200] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-31 17:39:09,201] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-31 17:39:09,201] [    INFO][0m -   epoch                    =      100.0[0m
[32m[2022-08-31 17:39:09,201] [    INFO][0m -   train_loss               =     0.0079[0m
[32m[2022-08-31 17:39:09,202] [    INFO][0m -   train_runtime            = 0:04:54.30[0m
[32m[2022-08-31 17:39:09,202] [    INFO][0m -   train_samples_per_second =     54.365[0m
[32m[2022-08-31 17:39:09,202] [    INFO][0m -   train_steps_per_second   =      6.796[0m
[32m[2022-08-31 17:39:09,211] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-31 17:39:09,211] [    INFO][0m -   Num examples = 610[0m
[32m[2022-08-31 17:39:09,211] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:39:09,211] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:39:09,211] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-08-31 17:39:11,170] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-31 17:39:11,170] [    INFO][0m -   test_accuracy           =     0.8869[0m
[32m[2022-08-31 17:39:11,170] [    INFO][0m -   test_loss               =     1.2035[0m
[32m[2022-08-31 17:39:11,170] [    INFO][0m -   test_runtime            = 0:00:01.95[0m
[32m[2022-08-31 17:39:11,171] [    INFO][0m -   test_samples_per_second =    311.392[0m
[32m[2022-08-31 17:39:11,171] [    INFO][0m -   test_steps_per_second   =      10.21[0m
[32m[2022-08-31 17:39:11,171] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-31 17:39:11,171] [    INFO][0m -   Num examples = 753[0m
[32m[2022-08-31 17:39:11,171] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:39:11,171] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:39:11,171] [    INFO][0m -   Total prediction steps = 24[0m
[32m[2022-08-31 17:39:14,113] [    INFO][0m - Predictions for eprstmt saved to ./fewclue_submit_examples.[0m
run.sh: line 65: --model_name_or_path: command not found
 
==========
csldcp
==========
 
[33m[2022-08-31 17:39:18,834] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-31 17:39:18,834] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-31 17:39:18,834] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 17:39:18,835] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-31 17:39:18,835] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 17:39:18,835] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-31 17:39:18,835] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-31 17:39:18,835] [    INFO][0m - early_stop_patience           :20[0m
[32m[2022-08-31 17:39:18,835] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-31 17:39:18,835] [    INFO][0m - model_name_or_path            :roformer_v2_chinese_char_base[0m
[32m[2022-08-31 17:39:18,835] [    INFO][0m - [0m
[32m[2022-08-31 17:39:18,836] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 17:39:18,836] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-31 17:39:18,836] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 17:39:18,836] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-31 17:39:18,836] [    INFO][0m - prompt                        :{'hard':'é˜…è¯»ä¸‹è¾¹'}{'mask'}{'mask'}{'hard':'ç›¸å…³çš„ææ–™'}{'text':'text_a'}[0m
[32m[2022-08-31 17:39:18,836] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-31 17:39:18,836] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-31 17:39:18,836] [    INFO][0m - task_name                     :csldcp[0m
[32m[2022-08-31 17:39:18,836] [    INFO][0m - [0m
[32m[2022-08-31 17:39:18,837] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/roformer_v2_chinese_char_base/model_state.pdparams[0m
W0831 17:39:18.839509 42921 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0831 17:39:18.844022 42921 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-31 17:39:21,753] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/roformer_v2_chinese_char_base/vocab.txt[0m
[32m[2022-08-31 17:39:21,761] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/roformer_v2_chinese_char_base/tokenizer_config.json[0m
[32m[2022-08-31 17:39:21,761] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/roformer_v2_chinese_char_base/special_tokens_map.json[0m
[32m[2022-08-31 17:39:21,762] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': 'é˜…è¯»ä¸‹è¾¹'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'ç›¸å…³çš„ææ–™'}, {'add_prefix_space': '', 'text': 'text_a'}][0m
2022-08-31 17:39:21,780 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-31 17:39:21,960] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 17:39:21,960] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-31 17:39:21,960] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 17:39:21,961] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-31 17:39:21,961] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-31 17:39:21,961] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-31 17:39:21,961] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-31 17:39:21,961] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-31 17:39:21,961] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-31 17:39:21,961] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-31 17:39:21,961] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-31 17:39:21,961] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-31 17:39:21,961] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-31 17:39:21,961] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-31 17:39:21,961] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-31 17:39:21,962] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-31 17:39:21,962] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-31 17:39:21,962] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-31 17:39:21,962] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-31 17:39:21,962] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-31 17:39:21,962] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-31 17:39:21,962] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-31 17:39:21,962] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-31 17:39:21,962] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-31 17:39:21,962] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-31 17:39:21,962] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-31 17:39:21,962] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-31 17:39:21,962] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-31 17:39:21,962] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-31 17:39:21,963] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-31 17:39:21,963] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-31 17:39:21,963] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-31 17:39:21,963] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-31 17:39:21,963] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-31 17:39:21,963] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-31 17:39:21,963] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-31 17:39:21,963] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-31 17:39:21,963] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug31_17-39-18_instance-3bwob41y-01[0m
[32m[2022-08-31 17:39:21,963] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-31 17:39:21,963] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-31 17:39:21,963] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-31 17:39:21,963] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-31 17:39:21,964] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-31 17:39:21,964] [    INFO][0m - max_seq_length                :256[0m
[32m[2022-08-31 17:39:21,964] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-31 17:39:21,964] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-31 17:39:21,964] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-31 17:39:21,964] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-31 17:39:21,964] [    INFO][0m - num_train_epochs              :100.0[0m
[32m[2022-08-31 17:39:21,964] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-31 17:39:21,964] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-31 17:39:21,964] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-31 17:39:21,964] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-31 17:39:21,964] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-31 17:39:21,964] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-31 17:39:21,964] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-31 17:39:21,965] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-31 17:39:21,965] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-31 17:39:21,965] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-31 17:39:21,965] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-31 17:39:21,965] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-31 17:39:21,965] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-31 17:39:21,965] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-31 17:39:21,965] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-31 17:39:21,965] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-31 17:39:21,965] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-31 17:39:21,965] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-31 17:39:21,965] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-31 17:39:21,965] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-31 17:39:21,965] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-31 17:39:21,966] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-31 17:39:21,966] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-31 17:39:21,966] [    INFO][0m - seed                          :42[0m
[32m[2022-08-31 17:39:21,966] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-31 17:39:21,966] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-31 17:39:21,966] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-31 17:39:21,966] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-31 17:39:21,966] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-31 17:39:21,966] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-31 17:39:21,966] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-31 17:39:21,967] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-31 17:39:21,967] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-31 17:39:21,967] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-31 17:39:21,967] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-31 17:39:21,967] [    INFO][0m - [0m
[32m[2022-08-31 17:39:21,970] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-31 17:39:21,970] [    INFO][0m -   Num examples = 2036[0m
[32m[2022-08-31 17:39:21,970] [    INFO][0m -   Num Epochs = 100[0m
[32m[2022-08-31 17:39:21,971] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-31 17:39:21,971] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-31 17:39:21,971] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-31 17:39:21,971] [    INFO][0m -   Total optimization steps = 25500.0[0m
[32m[2022-08-31 17:39:21,971] [    INFO][0m -   Total num train samples = 203600[0m
[32m[2022-08-31 17:39:23,932] [    INFO][0m - loss: 3.02689667, learning_rate: 2.998823529411765e-05, global_step: 10, interval_runtime: 1.9601, interval_samples_per_second: 4.081, interval_steps_per_second: 5.102, epoch: 0.0392[0m
[32m[2022-08-31 17:39:25,209] [    INFO][0m - loss: 2.20284958, learning_rate: 2.9976470588235296e-05, global_step: 20, interval_runtime: 1.2761, interval_samples_per_second: 6.269, interval_steps_per_second: 7.836, epoch: 0.0784[0m
[32m[2022-08-31 17:39:26,487] [    INFO][0m - loss: 2.61825733, learning_rate: 2.9964705882352942e-05, global_step: 30, interval_runtime: 1.2786, interval_samples_per_second: 6.257, interval_steps_per_second: 7.821, epoch: 0.1176[0m
[32m[2022-08-31 17:39:27,768] [    INFO][0m - loss: 2.19811058, learning_rate: 2.9952941176470588e-05, global_step: 40, interval_runtime: 1.2811, interval_samples_per_second: 6.244, interval_steps_per_second: 7.806, epoch: 0.1569[0m
[32m[2022-08-31 17:39:29,061] [    INFO][0m - loss: 1.89765205, learning_rate: 2.9941176470588237e-05, global_step: 50, interval_runtime: 1.2926, interval_samples_per_second: 6.189, interval_steps_per_second: 7.736, epoch: 0.1961[0m
[32m[2022-08-31 17:39:30,340] [    INFO][0m - loss: 2.08390064, learning_rate: 2.9929411764705883e-05, global_step: 60, interval_runtime: 1.2793, interval_samples_per_second: 6.253, interval_steps_per_second: 7.817, epoch: 0.2353[0m
[32m[2022-08-31 17:39:31,630] [    INFO][0m - loss: 2.09446144, learning_rate: 2.9917647058823533e-05, global_step: 70, interval_runtime: 1.2898, interval_samples_per_second: 6.203, interval_steps_per_second: 7.753, epoch: 0.2745[0m
[32m[2022-08-31 17:39:32,908] [    INFO][0m - loss: 1.8612463, learning_rate: 2.9905882352941175e-05, global_step: 80, interval_runtime: 1.2783, interval_samples_per_second: 6.259, interval_steps_per_second: 7.823, epoch: 0.3137[0m
[32m[2022-08-31 17:39:34,200] [    INFO][0m - loss: 2.04902515, learning_rate: 2.9894117647058825e-05, global_step: 90, interval_runtime: 1.2916, interval_samples_per_second: 6.194, interval_steps_per_second: 7.742, epoch: 0.3529[0m
[32m[2022-08-31 17:39:35,492] [    INFO][0m - loss: 1.87127285, learning_rate: 2.988235294117647e-05, global_step: 100, interval_runtime: 1.292, interval_samples_per_second: 6.192, interval_steps_per_second: 7.74, epoch: 0.3922[0m
[32m[2022-08-31 17:39:35,492] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 17:39:35,492] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 17:39:35,493] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:39:35,493] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:39:35,493] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 17:39:49,410] [    INFO][0m - eval_loss: 1.798076868057251, eval_accuracy: 0.5338491295938105, eval_runtime: 13.917, eval_samples_per_second: 148.595, eval_steps_per_second: 4.671, epoch: 0.3922[0m
[32m[2022-08-31 17:39:49,411] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-31 17:39:49,411] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 17:39:51,295] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-31 17:39:51,296] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-31 17:39:56,657] [    INFO][0m - loss: 2.46427479, learning_rate: 2.987058823529412e-05, global_step: 110, interval_runtime: 21.165, interval_samples_per_second: 0.378, interval_steps_per_second: 0.472, epoch: 0.4314[0m
[32m[2022-08-31 17:39:57,934] [    INFO][0m - loss: 1.65817947, learning_rate: 2.9858823529411763e-05, global_step: 120, interval_runtime: 1.2775, interval_samples_per_second: 6.262, interval_steps_per_second: 7.828, epoch: 0.4706[0m
[32m[2022-08-31 17:39:59,201] [    INFO][0m - loss: 2.07369156, learning_rate: 2.9847058823529412e-05, global_step: 130, interval_runtime: 1.2675, interval_samples_per_second: 6.312, interval_steps_per_second: 7.89, epoch: 0.5098[0m
[32m[2022-08-31 17:40:00,490] [    INFO][0m - loss: 1.84974613, learning_rate: 2.9835294117647058e-05, global_step: 140, interval_runtime: 1.288, interval_samples_per_second: 6.211, interval_steps_per_second: 7.764, epoch: 0.549[0m
[32m[2022-08-31 17:40:01,765] [    INFO][0m - loss: 1.58351479, learning_rate: 2.9823529411764707e-05, global_step: 150, interval_runtime: 1.2749, interval_samples_per_second: 6.275, interval_steps_per_second: 7.844, epoch: 0.5882[0m
[32m[2022-08-31 17:40:03,046] [    INFO][0m - loss: 1.94239101, learning_rate: 2.9811764705882357e-05, global_step: 160, interval_runtime: 1.2809, interval_samples_per_second: 6.245, interval_steps_per_second: 7.807, epoch: 0.6275[0m
[32m[2022-08-31 17:40:04,339] [    INFO][0m - loss: 1.66200581, learning_rate: 2.98e-05, global_step: 170, interval_runtime: 1.2934, interval_samples_per_second: 6.185, interval_steps_per_second: 7.731, epoch: 0.6667[0m
[32m[2022-08-31 17:40:05,608] [    INFO][0m - loss: 1.45647469, learning_rate: 2.978823529411765e-05, global_step: 180, interval_runtime: 1.269, interval_samples_per_second: 6.304, interval_steps_per_second: 7.88, epoch: 0.7059[0m
[32m[2022-08-31 17:40:06,878] [    INFO][0m - loss: 1.72315979, learning_rate: 2.9776470588235295e-05, global_step: 190, interval_runtime: 1.2702, interval_samples_per_second: 6.298, interval_steps_per_second: 7.873, epoch: 0.7451[0m
[32m[2022-08-31 17:40:08,164] [    INFO][0m - loss: 2.21999435, learning_rate: 2.9764705882352944e-05, global_step: 200, interval_runtime: 1.2856, interval_samples_per_second: 6.223, interval_steps_per_second: 7.778, epoch: 0.7843[0m
[32m[2022-08-31 17:40:08,164] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 17:40:08,164] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 17:40:08,165] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:40:08,165] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:40:08,165] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 17:40:22,225] [    INFO][0m - eval_loss: 1.6121026277542114, eval_accuracy: 0.5517408123791102, eval_runtime: 14.0594, eval_samples_per_second: 147.09, eval_steps_per_second: 4.623, epoch: 0.7843[0m
[32m[2022-08-31 17:40:22,225] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-31 17:40:22,225] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 17:40:24,539] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-31 17:40:24,539] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-31 17:40:30,137] [    INFO][0m - loss: 1.84839859, learning_rate: 2.975294117647059e-05, global_step: 210, interval_runtime: 21.9731, interval_samples_per_second: 0.364, interval_steps_per_second: 0.455, epoch: 0.8235[0m
[32m[2022-08-31 17:40:31,423] [    INFO][0m - loss: 1.4430583, learning_rate: 2.9741176470588236e-05, global_step: 220, interval_runtime: 1.2864, interval_samples_per_second: 6.219, interval_steps_per_second: 7.773, epoch: 0.8627[0m
[32m[2022-08-31 17:40:32,700] [    INFO][0m - loss: 1.40683632, learning_rate: 2.9729411764705882e-05, global_step: 230, interval_runtime: 1.2767, interval_samples_per_second: 6.266, interval_steps_per_second: 7.833, epoch: 0.902[0m
[32m[2022-08-31 17:40:33,987] [    INFO][0m - loss: 2.41105614, learning_rate: 2.971764705882353e-05, global_step: 240, interval_runtime: 1.2863, interval_samples_per_second: 6.219, interval_steps_per_second: 7.774, epoch: 0.9412[0m
[32m[2022-08-31 17:40:35,293] [    INFO][0m - loss: 1.79745884, learning_rate: 2.9705882352941177e-05, global_step: 250, interval_runtime: 1.3065, interval_samples_per_second: 6.123, interval_steps_per_second: 7.654, epoch: 0.9804[0m
[32m[2022-08-31 17:40:36,573] [    INFO][0m - loss: 1.49919443, learning_rate: 2.9694117647058823e-05, global_step: 260, interval_runtime: 1.2805, interval_samples_per_second: 6.247, interval_steps_per_second: 7.809, epoch: 1.0196[0m
[32m[2022-08-31 17:40:37,859] [    INFO][0m - loss: 0.87051344, learning_rate: 2.968235294117647e-05, global_step: 270, interval_runtime: 1.2858, interval_samples_per_second: 6.222, interval_steps_per_second: 7.777, epoch: 1.0588[0m
[32m[2022-08-31 17:40:39,148] [    INFO][0m - loss: 0.79891949, learning_rate: 2.967058823529412e-05, global_step: 280, interval_runtime: 1.2884, interval_samples_per_second: 6.209, interval_steps_per_second: 7.761, epoch: 1.098[0m
[32m[2022-08-31 17:40:40,427] [    INFO][0m - loss: 1.10728874, learning_rate: 2.9658823529411765e-05, global_step: 290, interval_runtime: 1.2785, interval_samples_per_second: 6.257, interval_steps_per_second: 7.822, epoch: 1.1373[0m
[32m[2022-08-31 17:40:41,716] [    INFO][0m - loss: 1.12900429, learning_rate: 2.9647058823529414e-05, global_step: 300, interval_runtime: 1.2904, interval_samples_per_second: 6.2, interval_steps_per_second: 7.75, epoch: 1.1765[0m
[32m[2022-08-31 17:40:41,717] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 17:40:41,717] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 17:40:41,718] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:40:41,718] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:40:41,718] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 17:40:55,693] [    INFO][0m - eval_loss: 1.657153606414795, eval_accuracy: 0.5585106382978723, eval_runtime: 13.9747, eval_samples_per_second: 147.982, eval_steps_per_second: 4.651, epoch: 1.1765[0m
[32m[2022-08-31 17:40:55,693] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-31 17:40:55,693] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 17:40:57,616] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-31 17:40:57,616] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-31 17:41:03,395] [    INFO][0m - loss: 0.78480463, learning_rate: 2.963529411764706e-05, global_step: 310, interval_runtime: 21.6785, interval_samples_per_second: 0.369, interval_steps_per_second: 0.461, epoch: 1.2157[0m
[32m[2022-08-31 17:41:04,689] [    INFO][0m - loss: 1.09033518, learning_rate: 2.9623529411764706e-05, global_step: 320, interval_runtime: 1.2895, interval_samples_per_second: 6.204, interval_steps_per_second: 7.755, epoch: 1.2549[0m
[32m[2022-08-31 17:41:05,982] [    INFO][0m - loss: 0.95611353, learning_rate: 2.9611764705882355e-05, global_step: 330, interval_runtime: 1.2976, interval_samples_per_second: 6.165, interval_steps_per_second: 7.706, epoch: 1.2941[0m
[32m[2022-08-31 17:41:07,260] [    INFO][0m - loss: 1.41934443, learning_rate: 2.96e-05, global_step: 340, interval_runtime: 1.2783, interval_samples_per_second: 6.258, interval_steps_per_second: 7.823, epoch: 1.3333[0m
[32m[2022-08-31 17:41:08,532] [    INFO][0m - loss: 1.32561169, learning_rate: 2.958823529411765e-05, global_step: 350, interval_runtime: 1.2713, interval_samples_per_second: 6.293, interval_steps_per_second: 7.866, epoch: 1.3725[0m
[32m[2022-08-31 17:41:09,826] [    INFO][0m - loss: 1.14970798, learning_rate: 2.9576470588235293e-05, global_step: 360, interval_runtime: 1.2939, interval_samples_per_second: 6.183, interval_steps_per_second: 7.728, epoch: 1.4118[0m
[32m[2022-08-31 17:41:11,133] [    INFO][0m - loss: 1.14655342, learning_rate: 2.9564705882352943e-05, global_step: 370, interval_runtime: 1.3073, interval_samples_per_second: 6.12, interval_steps_per_second: 7.65, epoch: 1.451[0m
[32m[2022-08-31 17:41:12,411] [    INFO][0m - loss: 1.00790062, learning_rate: 2.955294117647059e-05, global_step: 380, interval_runtime: 1.2777, interval_samples_per_second: 6.261, interval_steps_per_second: 7.827, epoch: 1.4902[0m
[32m[2022-08-31 17:41:13,719] [    INFO][0m - loss: 1.23189383, learning_rate: 2.9541176470588238e-05, global_step: 390, interval_runtime: 1.3081, interval_samples_per_second: 6.116, interval_steps_per_second: 7.645, epoch: 1.5294[0m
[32m[2022-08-31 17:41:15,002] [    INFO][0m - loss: 0.80825615, learning_rate: 2.952941176470588e-05, global_step: 400, interval_runtime: 1.2835, interval_samples_per_second: 6.233, interval_steps_per_second: 7.791, epoch: 1.5686[0m
[32m[2022-08-31 17:41:15,003] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 17:41:15,003] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 17:41:15,003] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:41:15,003] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:41:15,003] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 17:41:29,013] [    INFO][0m - eval_loss: 1.6810061931610107, eval_accuracy: 0.5672147001934236, eval_runtime: 14.0093, eval_samples_per_second: 147.616, eval_steps_per_second: 4.64, epoch: 1.5686[0m
[32m[2022-08-31 17:41:29,014] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-31 17:41:29,014] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 17:41:30,896] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-31 17:41:30,896] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-31 17:41:36,347] [    INFO][0m - loss: 0.99751854, learning_rate: 2.951764705882353e-05, global_step: 410, interval_runtime: 21.3446, interval_samples_per_second: 0.375, interval_steps_per_second: 0.469, epoch: 1.6078[0m
[32m[2022-08-31 17:41:37,621] [    INFO][0m - loss: 1.21742496, learning_rate: 2.9505882352941176e-05, global_step: 420, interval_runtime: 1.274, interval_samples_per_second: 6.279, interval_steps_per_second: 7.849, epoch: 1.6471[0m
[32m[2022-08-31 17:41:38,930] [    INFO][0m - loss: 0.74978757, learning_rate: 2.9494117647058825e-05, global_step: 430, interval_runtime: 1.3094, interval_samples_per_second: 6.11, interval_steps_per_second: 7.637, epoch: 1.6863[0m
[32m[2022-08-31 17:41:40,222] [    INFO][0m - loss: 1.19227953, learning_rate: 2.948235294117647e-05, global_step: 440, interval_runtime: 1.2915, interval_samples_per_second: 6.194, interval_steps_per_second: 7.743, epoch: 1.7255[0m
[32m[2022-08-31 17:41:41,514] [    INFO][0m - loss: 0.86714382, learning_rate: 2.9470588235294117e-05, global_step: 450, interval_runtime: 1.2921, interval_samples_per_second: 6.192, interval_steps_per_second: 7.74, epoch: 1.7647[0m
[32m[2022-08-31 17:41:42,793] [    INFO][0m - loss: 1.05836325, learning_rate: 2.9458823529411763e-05, global_step: 460, interval_runtime: 1.2793, interval_samples_per_second: 6.253, interval_steps_per_second: 7.817, epoch: 1.8039[0m
[32m[2022-08-31 17:41:44,074] [    INFO][0m - loss: 1.2774539, learning_rate: 2.9447058823529412e-05, global_step: 470, interval_runtime: 1.2803, interval_samples_per_second: 6.249, interval_steps_per_second: 7.811, epoch: 1.8431[0m
[32m[2022-08-31 17:41:45,353] [    INFO][0m - loss: 1.11085606, learning_rate: 2.9435294117647062e-05, global_step: 480, interval_runtime: 1.2792, interval_samples_per_second: 6.254, interval_steps_per_second: 7.818, epoch: 1.8824[0m
[32m[2022-08-31 17:41:46,626] [    INFO][0m - loss: 1.0234067, learning_rate: 2.9423529411764708e-05, global_step: 490, interval_runtime: 1.2729, interval_samples_per_second: 6.285, interval_steps_per_second: 7.856, epoch: 1.9216[0m
[32m[2022-08-31 17:41:47,929] [    INFO][0m - loss: 1.35202284, learning_rate: 2.9411764705882354e-05, global_step: 500, interval_runtime: 1.3037, interval_samples_per_second: 6.136, interval_steps_per_second: 7.67, epoch: 1.9608[0m
[32m[2022-08-31 17:41:47,930] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 17:41:47,930] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 17:41:47,930] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:41:47,930] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:41:47,930] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 17:42:01,995] [    INFO][0m - eval_loss: 1.6101635694503784, eval_accuracy: 0.5657640232108317, eval_runtime: 14.0645, eval_samples_per_second: 147.037, eval_steps_per_second: 4.622, epoch: 1.9608[0m
[32m[2022-08-31 17:42:01,996] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-08-31 17:42:01,996] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 17:42:03,873] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-08-31 17:42:03,873] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-08-31 17:42:09,552] [    INFO][0m - loss: 0.87601261, learning_rate: 2.94e-05, global_step: 510, interval_runtime: 21.6229, interval_samples_per_second: 0.37, interval_steps_per_second: 0.462, epoch: 2.0[0m
[32m[2022-08-31 17:42:10,937] [    INFO][0m - loss: 0.59270487, learning_rate: 2.938823529411765e-05, global_step: 520, interval_runtime: 1.3848, interval_samples_per_second: 5.777, interval_steps_per_second: 7.221, epoch: 2.0392[0m
[32m[2022-08-31 17:42:12,213] [    INFO][0m - loss: 0.45691814, learning_rate: 2.9376470588235295e-05, global_step: 530, interval_runtime: 1.2757, interval_samples_per_second: 6.271, interval_steps_per_second: 7.839, epoch: 2.0784[0m
[32m[2022-08-31 17:42:13,496] [    INFO][0m - loss: 0.48712125, learning_rate: 2.9364705882352944e-05, global_step: 540, interval_runtime: 1.2827, interval_samples_per_second: 6.237, interval_steps_per_second: 7.796, epoch: 2.1176[0m
[32m[2022-08-31 17:42:14,782] [    INFO][0m - loss: 0.41249743, learning_rate: 2.9352941176470587e-05, global_step: 550, interval_runtime: 1.2869, interval_samples_per_second: 6.216, interval_steps_per_second: 7.771, epoch: 2.1569[0m
[32m[2022-08-31 17:42:16,060] [    INFO][0m - loss: 0.44399209, learning_rate: 2.9341176470588236e-05, global_step: 560, interval_runtime: 1.2774, interval_samples_per_second: 6.263, interval_steps_per_second: 7.829, epoch: 2.1961[0m
[32m[2022-08-31 17:42:17,348] [    INFO][0m - loss: 0.39395838, learning_rate: 2.9329411764705882e-05, global_step: 570, interval_runtime: 1.2887, interval_samples_per_second: 6.208, interval_steps_per_second: 7.76, epoch: 2.2353[0m
[32m[2022-08-31 17:42:18,624] [    INFO][0m - loss: 0.67340617, learning_rate: 2.9317647058823532e-05, global_step: 580, interval_runtime: 1.2754, interval_samples_per_second: 6.273, interval_steps_per_second: 7.841, epoch: 2.2745[0m
[32m[2022-08-31 17:42:19,908] [    INFO][0m - loss: 0.79665399, learning_rate: 2.9305882352941174e-05, global_step: 590, interval_runtime: 1.284, interval_samples_per_second: 6.231, interval_steps_per_second: 7.788, epoch: 2.3137[0m
[32m[2022-08-31 17:42:21,192] [    INFO][0m - loss: 0.53721437, learning_rate: 2.9294117647058824e-05, global_step: 600, interval_runtime: 1.2841, interval_samples_per_second: 6.23, interval_steps_per_second: 7.788, epoch: 2.3529[0m
[32m[2022-08-31 17:42:21,192] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 17:42:21,192] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 17:42:21,193] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:42:21,193] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:42:21,193] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 17:42:35,162] [    INFO][0m - eval_loss: 1.7405182123184204, eval_accuracy: 0.5807543520309478, eval_runtime: 13.9684, eval_samples_per_second: 148.049, eval_steps_per_second: 4.653, epoch: 2.3529[0m
[32m[2022-08-31 17:42:35,162] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-08-31 17:42:35,162] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 17:42:37,101] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-08-31 17:42:37,101] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-08-31 17:42:42,545] [    INFO][0m - loss: 0.66320739, learning_rate: 2.928235294117647e-05, global_step: 610, interval_runtime: 21.3535, interval_samples_per_second: 0.375, interval_steps_per_second: 0.468, epoch: 2.3922[0m
[32m[2022-08-31 17:42:43,817] [    INFO][0m - loss: 0.68900766, learning_rate: 2.927058823529412e-05, global_step: 620, interval_runtime: 1.2713, interval_samples_per_second: 6.293, interval_steps_per_second: 7.866, epoch: 2.4314[0m
[32m[2022-08-31 17:42:45,093] [    INFO][0m - loss: 0.63774152, learning_rate: 2.925882352941177e-05, global_step: 630, interval_runtime: 1.2761, interval_samples_per_second: 6.269, interval_steps_per_second: 7.836, epoch: 2.4706[0m
[32m[2022-08-31 17:42:46,366] [    INFO][0m - loss: 0.51467562, learning_rate: 2.924705882352941e-05, global_step: 640, interval_runtime: 1.2731, interval_samples_per_second: 6.284, interval_steps_per_second: 7.855, epoch: 2.5098[0m
[32m[2022-08-31 17:42:47,648] [    INFO][0m - loss: 0.54430799, learning_rate: 2.923529411764706e-05, global_step: 650, interval_runtime: 1.2824, interval_samples_per_second: 6.238, interval_steps_per_second: 7.798, epoch: 2.549[0m
[32m[2022-08-31 17:42:48,930] [    INFO][0m - loss: 0.40509801, learning_rate: 2.9223529411764706e-05, global_step: 660, interval_runtime: 1.2803, interval_samples_per_second: 6.248, interval_steps_per_second: 7.811, epoch: 2.5882[0m
[32m[2022-08-31 17:42:50,207] [    INFO][0m - loss: 0.85465345, learning_rate: 2.9211764705882356e-05, global_step: 670, interval_runtime: 1.2783, interval_samples_per_second: 6.258, interval_steps_per_second: 7.823, epoch: 2.6275[0m
[32m[2022-08-31 17:42:51,484] [    INFO][0m - loss: 0.61148448, learning_rate: 2.92e-05, global_step: 680, interval_runtime: 1.2767, interval_samples_per_second: 6.266, interval_steps_per_second: 7.833, epoch: 2.6667[0m
[32m[2022-08-31 17:42:52,773] [    INFO][0m - loss: 0.52145371, learning_rate: 2.9188235294117648e-05, global_step: 690, interval_runtime: 1.289, interval_samples_per_second: 6.206, interval_steps_per_second: 7.758, epoch: 2.7059[0m
[32m[2022-08-31 17:42:54,057] [    INFO][0m - loss: 0.4385828, learning_rate: 2.9176470588235294e-05, global_step: 700, interval_runtime: 1.2841, interval_samples_per_second: 6.23, interval_steps_per_second: 7.787, epoch: 2.7451[0m
[32m[2022-08-31 17:42:54,058] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 17:42:54,058] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 17:42:54,058] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:42:54,058] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:42:54,058] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 17:43:08,073] [    INFO][0m - eval_loss: 1.7757508754730225, eval_accuracy: 0.5923597678916828, eval_runtime: 14.0138, eval_samples_per_second: 147.569, eval_steps_per_second: 4.638, epoch: 2.7451[0m
[32m[2022-08-31 17:43:08,073] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-700[0m
[32m[2022-08-31 17:43:08,073] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 17:43:09,954] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-700/tokenizer_config.json[0m
[32m[2022-08-31 17:43:09,954] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-700/special_tokens_map.json[0m
[32m[2022-08-31 17:43:15,304] [    INFO][0m - loss: 0.59680519, learning_rate: 2.9164705882352943e-05, global_step: 710, interval_runtime: 21.2469, interval_samples_per_second: 0.377, interval_steps_per_second: 0.471, epoch: 2.7843[0m
[32m[2022-08-31 17:43:16,583] [    INFO][0m - loss: 0.71742516, learning_rate: 2.915294117647059e-05, global_step: 720, interval_runtime: 1.2795, interval_samples_per_second: 6.252, interval_steps_per_second: 7.815, epoch: 2.8235[0m
[32m[2022-08-31 17:43:17,856] [    INFO][0m - loss: 0.7823349, learning_rate: 2.9141176470588235e-05, global_step: 730, interval_runtime: 1.2731, interval_samples_per_second: 6.284, interval_steps_per_second: 7.855, epoch: 2.8627[0m
[32m[2022-08-31 17:43:19,127] [    INFO][0m - loss: 0.69486475, learning_rate: 2.912941176470588e-05, global_step: 740, interval_runtime: 1.2706, interval_samples_per_second: 6.296, interval_steps_per_second: 7.87, epoch: 2.902[0m
[32m[2022-08-31 17:43:20,414] [    INFO][0m - loss: 0.71013393, learning_rate: 2.911764705882353e-05, global_step: 750, interval_runtime: 1.2864, interval_samples_per_second: 6.219, interval_steps_per_second: 7.774, epoch: 2.9412[0m
[32m[2022-08-31 17:43:21,695] [    INFO][0m - loss: 0.46218762, learning_rate: 2.9105882352941176e-05, global_step: 760, interval_runtime: 1.2811, interval_samples_per_second: 6.245, interval_steps_per_second: 7.806, epoch: 2.9804[0m
[32m[2022-08-31 17:43:22,989] [    INFO][0m - loss: 0.39050915, learning_rate: 2.9094117647058826e-05, global_step: 770, interval_runtime: 1.2941, interval_samples_per_second: 6.182, interval_steps_per_second: 7.727, epoch: 3.0196[0m
[32m[2022-08-31 17:43:24,270] [    INFO][0m - loss: 0.1724632, learning_rate: 2.908235294117647e-05, global_step: 780, interval_runtime: 1.2815, interval_samples_per_second: 6.243, interval_steps_per_second: 7.803, epoch: 3.0588[0m
[32m[2022-08-31 17:43:25,568] [    INFO][0m - loss: 0.30786409, learning_rate: 2.9070588235294118e-05, global_step: 790, interval_runtime: 1.2984, interval_samples_per_second: 6.162, interval_steps_per_second: 7.702, epoch: 3.098[0m
[32m[2022-08-31 17:43:26,837] [    INFO][0m - loss: 0.27491827, learning_rate: 2.9058823529411767e-05, global_step: 800, interval_runtime: 1.2688, interval_samples_per_second: 6.305, interval_steps_per_second: 7.882, epoch: 3.1373[0m
[32m[2022-08-31 17:43:26,838] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 17:43:26,838] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 17:43:26,838] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:43:26,838] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:43:26,839] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 17:43:40,734] [    INFO][0m - eval_loss: 1.8405728340148926, eval_accuracy: 0.586073500967118, eval_runtime: 13.8948, eval_samples_per_second: 148.832, eval_steps_per_second: 4.678, epoch: 3.1373[0m
[32m[2022-08-31 17:43:40,734] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-800[0m
[32m[2022-08-31 17:43:40,735] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 17:43:42,596] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-800/tokenizer_config.json[0m
[32m[2022-08-31 17:43:42,597] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-800/special_tokens_map.json[0m
[32m[2022-08-31 17:43:48,311] [    INFO][0m - loss: 0.28595233, learning_rate: 2.9047058823529413e-05, global_step: 810, interval_runtime: 21.4723, interval_samples_per_second: 0.373, interval_steps_per_second: 0.466, epoch: 3.1765[0m
[32m[2022-08-31 17:43:49,604] [    INFO][0m - loss: 0.24750733, learning_rate: 2.9035294117647062e-05, global_step: 820, interval_runtime: 1.2939, interval_samples_per_second: 6.183, interval_steps_per_second: 7.728, epoch: 3.2157[0m
[32m[2022-08-31 17:43:50,881] [    INFO][0m - loss: 0.25947976, learning_rate: 2.9023529411764705e-05, global_step: 830, interval_runtime: 1.2779, interval_samples_per_second: 6.26, interval_steps_per_second: 7.825, epoch: 3.2549[0m
[32m[2022-08-31 17:43:52,155] [    INFO][0m - loss: 0.19069952, learning_rate: 2.9011764705882354e-05, global_step: 840, interval_runtime: 1.2736, interval_samples_per_second: 6.281, interval_steps_per_second: 7.852, epoch: 3.2941[0m
[32m[2022-08-31 17:43:53,454] [    INFO][0m - loss: 0.29808218, learning_rate: 2.9e-05, global_step: 850, interval_runtime: 1.2981, interval_samples_per_second: 6.163, interval_steps_per_second: 7.704, epoch: 3.3333[0m
[32m[2022-08-31 17:43:54,742] [    INFO][0m - loss: 0.31304681, learning_rate: 2.898823529411765e-05, global_step: 860, interval_runtime: 1.2887, interval_samples_per_second: 6.208, interval_steps_per_second: 7.76, epoch: 3.3725[0m
[32m[2022-08-31 17:43:56,017] [    INFO][0m - loss: 0.30324306, learning_rate: 2.8976470588235296e-05, global_step: 870, interval_runtime: 1.2748, interval_samples_per_second: 6.276, interval_steps_per_second: 7.844, epoch: 3.4118[0m
[32m[2022-08-31 17:43:57,301] [    INFO][0m - loss: 0.41310797, learning_rate: 2.896470588235294e-05, global_step: 880, interval_runtime: 1.2841, interval_samples_per_second: 6.23, interval_steps_per_second: 7.788, epoch: 3.451[0m
[32m[2022-08-31 17:43:58,578] [    INFO][0m - loss: 0.2322597, learning_rate: 2.8952941176470587e-05, global_step: 890, interval_runtime: 1.2771, interval_samples_per_second: 6.264, interval_steps_per_second: 7.83, epoch: 3.4902[0m
[32m[2022-08-31 17:43:59,855] [    INFO][0m - loss: 0.13395481, learning_rate: 2.8941176470588237e-05, global_step: 900, interval_runtime: 1.2773, interval_samples_per_second: 6.263, interval_steps_per_second: 7.829, epoch: 3.5294[0m
[32m[2022-08-31 17:43:59,856] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 17:43:59,856] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 17:43:59,856] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:43:59,856] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:43:59,856] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 17:44:13,788] [    INFO][0m - eval_loss: 1.9230667352676392, eval_accuracy: 0.5889748549323017, eval_runtime: 13.9313, eval_samples_per_second: 148.442, eval_steps_per_second: 4.666, epoch: 3.5294[0m
[32m[2022-08-31 17:44:13,789] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-900[0m
[32m[2022-08-31 17:44:13,789] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 17:44:16,013] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-900/tokenizer_config.json[0m
[32m[2022-08-31 17:44:16,013] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-900/special_tokens_map.json[0m
[32m[2022-08-31 17:44:21,403] [    INFO][0m - loss: 0.21777694, learning_rate: 2.8929411764705883e-05, global_step: 910, interval_runtime: 21.5477, interval_samples_per_second: 0.371, interval_steps_per_second: 0.464, epoch: 3.5686[0m
[32m[2022-08-31 17:44:22,676] [    INFO][0m - loss: 0.22481403, learning_rate: 2.891764705882353e-05, global_step: 920, interval_runtime: 1.273, interval_samples_per_second: 6.284, interval_steps_per_second: 7.855, epoch: 3.6078[0m
[32m[2022-08-31 17:44:23,962] [    INFO][0m - loss: 0.28979089, learning_rate: 2.8905882352941175e-05, global_step: 930, interval_runtime: 1.2863, interval_samples_per_second: 6.219, interval_steps_per_second: 7.774, epoch: 3.6471[0m
[32m[2022-08-31 17:44:25,244] [    INFO][0m - loss: 0.14891013, learning_rate: 2.8894117647058824e-05, global_step: 940, interval_runtime: 1.2814, interval_samples_per_second: 6.243, interval_steps_per_second: 7.804, epoch: 3.6863[0m
[32m[2022-08-31 17:44:26,516] [    INFO][0m - loss: 0.5394599, learning_rate: 2.8882352941176473e-05, global_step: 950, interval_runtime: 1.2723, interval_samples_per_second: 6.288, interval_steps_per_second: 7.86, epoch: 3.7255[0m
[32m[2022-08-31 17:44:27,792] [    INFO][0m - loss: 0.29398854, learning_rate: 2.887058823529412e-05, global_step: 960, interval_runtime: 1.2762, interval_samples_per_second: 6.269, interval_steps_per_second: 7.836, epoch: 3.7647[0m
[32m[2022-08-31 17:44:29,069] [    INFO][0m - loss: 0.27387526, learning_rate: 2.8858823529411765e-05, global_step: 970, interval_runtime: 1.2772, interval_samples_per_second: 6.264, interval_steps_per_second: 7.83, epoch: 3.8039[0m
[32m[2022-08-31 17:44:30,342] [    INFO][0m - loss: 0.25269666, learning_rate: 2.884705882352941e-05, global_step: 980, interval_runtime: 1.2722, interval_samples_per_second: 6.288, interval_steps_per_second: 7.86, epoch: 3.8431[0m
[32m[2022-08-31 17:44:31,617] [    INFO][0m - loss: 0.46614203, learning_rate: 2.883529411764706e-05, global_step: 990, interval_runtime: 1.2758, interval_samples_per_second: 6.27, interval_steps_per_second: 7.838, epoch: 3.8824[0m
[32m[2022-08-31 17:44:32,918] [    INFO][0m - loss: 0.23965111, learning_rate: 2.8823529411764707e-05, global_step: 1000, interval_runtime: 1.3007, interval_samples_per_second: 6.151, interval_steps_per_second: 7.688, epoch: 3.9216[0m
[32m[2022-08-31 17:44:32,919] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 17:44:32,919] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 17:44:32,919] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:44:32,919] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:44:32,919] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 17:44:46,963] [    INFO][0m - eval_loss: 2.011629104614258, eval_accuracy: 0.5846228239845261, eval_runtime: 14.0435, eval_samples_per_second: 147.256, eval_steps_per_second: 4.628, epoch: 3.9216[0m
[32m[2022-08-31 17:44:46,963] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1000[0m
[32m[2022-08-31 17:44:46,964] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 17:44:48,790] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1000/tokenizer_config.json[0m
[32m[2022-08-31 17:44:48,790] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1000/special_tokens_map.json[0m
[32m[2022-08-31 17:44:54,309] [    INFO][0m - loss: 0.34245014, learning_rate: 2.8811764705882356e-05, global_step: 1010, interval_runtime: 21.3909, interval_samples_per_second: 0.374, interval_steps_per_second: 0.467, epoch: 3.9608[0m
[32m[2022-08-31 17:44:55,529] [    INFO][0m - loss: 0.19794612, learning_rate: 2.88e-05, global_step: 1020, interval_runtime: 1.2202, interval_samples_per_second: 6.556, interval_steps_per_second: 8.195, epoch: 4.0[0m
[32m[2022-08-31 17:44:56,923] [    INFO][0m - loss: 0.16473266, learning_rate: 2.8788235294117648e-05, global_step: 1030, interval_runtime: 1.3936, interval_samples_per_second: 5.74, interval_steps_per_second: 7.176, epoch: 4.0392[0m
[32m[2022-08-31 17:44:58,210] [    INFO][0m - loss: 0.1260841, learning_rate: 2.8776470588235294e-05, global_step: 1040, interval_runtime: 1.2866, interval_samples_per_second: 6.218, interval_steps_per_second: 7.773, epoch: 4.0784[0m
[32m[2022-08-31 17:44:59,488] [    INFO][0m - loss: 0.13984252, learning_rate: 2.8764705882352943e-05, global_step: 1050, interval_runtime: 1.2788, interval_samples_per_second: 6.256, interval_steps_per_second: 7.82, epoch: 4.1176[0m
[32m[2022-08-31 17:45:00,755] [    INFO][0m - loss: 0.07854595, learning_rate: 2.8752941176470586e-05, global_step: 1060, interval_runtime: 1.2673, interval_samples_per_second: 6.313, interval_steps_per_second: 7.891, epoch: 4.1569[0m
[32m[2022-08-31 17:45:02,037] [    INFO][0m - loss: 0.06032242, learning_rate: 2.8741176470588235e-05, global_step: 1070, interval_runtime: 1.2808, interval_samples_per_second: 6.246, interval_steps_per_second: 7.807, epoch: 4.1961[0m
[32m[2022-08-31 17:45:03,314] [    INFO][0m - loss: 0.04239088, learning_rate: 2.872941176470588e-05, global_step: 1080, interval_runtime: 1.2779, interval_samples_per_second: 6.26, interval_steps_per_second: 7.826, epoch: 4.2353[0m
[32m[2022-08-31 17:45:04,587] [    INFO][0m - loss: 0.08531326, learning_rate: 2.871764705882353e-05, global_step: 1090, interval_runtime: 1.2728, interval_samples_per_second: 6.285, interval_steps_per_second: 7.856, epoch: 4.2745[0m
[32m[2022-08-31 17:45:05,866] [    INFO][0m - loss: 0.18135282, learning_rate: 2.870588235294118e-05, global_step: 1100, interval_runtime: 1.2789, interval_samples_per_second: 6.255, interval_steps_per_second: 7.819, epoch: 4.3137[0m
[32m[2022-08-31 17:45:05,867] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 17:45:05,867] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 17:45:05,867] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:45:05,867] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:45:05,868] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 17:45:19,790] [    INFO][0m - eval_loss: 2.1481692790985107, eval_accuracy: 0.5802707930367504, eval_runtime: 13.9219, eval_samples_per_second: 148.543, eval_steps_per_second: 4.669, epoch: 4.3137[0m
[32m[2022-08-31 17:45:19,790] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1100[0m
[32m[2022-08-31 17:45:19,790] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 17:45:22,105] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1100/tokenizer_config.json[0m
[32m[2022-08-31 17:45:22,105] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1100/special_tokens_map.json[0m
[32m[2022-08-31 17:45:27,425] [    INFO][0m - loss: 0.06767999, learning_rate: 2.8694117647058823e-05, global_step: 1110, interval_runtime: 21.5584, interval_samples_per_second: 0.371, interval_steps_per_second: 0.464, epoch: 4.3529[0m
[32m[2022-08-31 17:45:28,697] [    INFO][0m - loss: 0.09794487, learning_rate: 2.8682352941176472e-05, global_step: 1120, interval_runtime: 1.2723, interval_samples_per_second: 6.288, interval_steps_per_second: 7.86, epoch: 4.3922[0m
[32m[2022-08-31 17:45:29,965] [    INFO][0m - loss: 0.18550267, learning_rate: 2.8670588235294118e-05, global_step: 1130, interval_runtime: 1.2679, interval_samples_per_second: 6.31, interval_steps_per_second: 7.887, epoch: 4.4314[0m
[32m[2022-08-31 17:45:31,236] [    INFO][0m - loss: 0.16112223, learning_rate: 2.8658823529411767e-05, global_step: 1140, interval_runtime: 1.2714, interval_samples_per_second: 6.292, interval_steps_per_second: 7.866, epoch: 4.4706[0m
[32m[2022-08-31 17:45:32,521] [    INFO][0m - loss: 0.22535038, learning_rate: 2.8647058823529413e-05, global_step: 1150, interval_runtime: 1.2853, interval_samples_per_second: 6.224, interval_steps_per_second: 7.78, epoch: 4.5098[0m
[32m[2022-08-31 17:45:33,807] [    INFO][0m - loss: 0.12070979, learning_rate: 2.863529411764706e-05, global_step: 1160, interval_runtime: 1.2861, interval_samples_per_second: 6.22, interval_steps_per_second: 7.775, epoch: 4.549[0m
[32m[2022-08-31 17:45:35,079] [    INFO][0m - loss: 0.07566244, learning_rate: 2.8623529411764705e-05, global_step: 1170, interval_runtime: 1.2716, interval_samples_per_second: 6.291, interval_steps_per_second: 7.864, epoch: 4.5882[0m
[32m[2022-08-31 17:45:36,360] [    INFO][0m - loss: 0.05166828, learning_rate: 2.8611764705882355e-05, global_step: 1180, interval_runtime: 1.2811, interval_samples_per_second: 6.245, interval_steps_per_second: 7.806, epoch: 4.6275[0m
[32m[2022-08-31 17:45:37,636] [    INFO][0m - loss: 0.10037801, learning_rate: 2.86e-05, global_step: 1190, interval_runtime: 1.2759, interval_samples_per_second: 6.27, interval_steps_per_second: 7.838, epoch: 4.6667[0m
[32m[2022-08-31 17:45:38,907] [    INFO][0m - loss: 0.13770159, learning_rate: 2.8588235294117647e-05, global_step: 1200, interval_runtime: 1.2709, interval_samples_per_second: 6.295, interval_steps_per_second: 7.868, epoch: 4.7059[0m
[32m[2022-08-31 17:45:38,908] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 17:45:38,908] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 17:45:38,908] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:45:38,908] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:45:38,908] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 17:45:52,858] [    INFO][0m - eval_loss: 2.2404417991638184, eval_accuracy: 0.6020309477756286, eval_runtime: 13.9491, eval_samples_per_second: 148.253, eval_steps_per_second: 4.66, epoch: 4.7059[0m
[32m[2022-08-31 17:45:52,858] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1200[0m
[32m[2022-08-31 17:45:52,858] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 17:45:54,713] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1200/tokenizer_config.json[0m
[32m[2022-08-31 17:45:54,713] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1200/special_tokens_map.json[0m
[32m[2022-08-31 17:46:00,370] [    INFO][0m - loss: 0.0513823, learning_rate: 2.8576470588235293e-05, global_step: 1210, interval_runtime: 21.463, interval_samples_per_second: 0.373, interval_steps_per_second: 0.466, epoch: 4.7451[0m
[32m[2022-08-31 17:46:01,652] [    INFO][0m - loss: 0.0665922, learning_rate: 2.8564705882352942e-05, global_step: 1220, interval_runtime: 1.2819, interval_samples_per_second: 6.241, interval_steps_per_second: 7.801, epoch: 4.7843[0m
[32m[2022-08-31 17:46:02,936] [    INFO][0m - loss: 0.08852326, learning_rate: 2.8552941176470588e-05, global_step: 1230, interval_runtime: 1.2843, interval_samples_per_second: 6.229, interval_steps_per_second: 7.787, epoch: 4.8235[0m
[32m[2022-08-31 17:46:04,235] [    INFO][0m - loss: 0.13123696, learning_rate: 2.8541176470588237e-05, global_step: 1240, interval_runtime: 1.2992, interval_samples_per_second: 6.158, interval_steps_per_second: 7.697, epoch: 4.8627[0m
[32m[2022-08-31 17:46:05,516] [    INFO][0m - loss: 0.1337597, learning_rate: 2.8529411764705883e-05, global_step: 1250, interval_runtime: 1.2802, interval_samples_per_second: 6.249, interval_steps_per_second: 7.811, epoch: 4.902[0m
[32m[2022-08-31 17:46:06,790] [    INFO][0m - loss: 0.09266401, learning_rate: 2.851764705882353e-05, global_step: 1260, interval_runtime: 1.2739, interval_samples_per_second: 6.28, interval_steps_per_second: 7.85, epoch: 4.9412[0m
[32m[2022-08-31 17:46:08,092] [    INFO][0m - loss: 0.17649721, learning_rate: 2.850588235294118e-05, global_step: 1270, interval_runtime: 1.3027, interval_samples_per_second: 6.141, interval_steps_per_second: 7.676, epoch: 4.9804[0m
[32m[2022-08-31 17:46:09,383] [    INFO][0m - loss: 0.06476989, learning_rate: 2.8494117647058825e-05, global_step: 1280, interval_runtime: 1.2906, interval_samples_per_second: 6.199, interval_steps_per_second: 7.748, epoch: 5.0196[0m
[32m[2022-08-31 17:46:10,679] [    INFO][0m - loss: 0.03281946, learning_rate: 2.8482352941176474e-05, global_step: 1290, interval_runtime: 1.296, interval_samples_per_second: 6.173, interval_steps_per_second: 7.716, epoch: 5.0588[0m
[32m[2022-08-31 17:46:11,960] [    INFO][0m - loss: 0.03768865, learning_rate: 2.8470588235294117e-05, global_step: 1300, interval_runtime: 1.2805, interval_samples_per_second: 6.248, interval_steps_per_second: 7.81, epoch: 5.098[0m
[32m[2022-08-31 17:46:11,961] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 17:46:11,961] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 17:46:11,961] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:46:11,961] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:46:11,961] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 17:46:26,232] [    INFO][0m - eval_loss: 2.216707706451416, eval_accuracy: 0.5957446808510638, eval_runtime: 14.2709, eval_samples_per_second: 144.911, eval_steps_per_second: 4.555, epoch: 5.098[0m
[32m[2022-08-31 17:46:26,233] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1300[0m
[32m[2022-08-31 17:46:26,233] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 17:46:28,058] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1300/tokenizer_config.json[0m
[32m[2022-08-31 17:46:28,058] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1300/special_tokens_map.json[0m
[32m[2022-08-31 17:46:33,392] [    INFO][0m - loss: 0.05674251, learning_rate: 2.8458823529411766e-05, global_step: 1310, interval_runtime: 21.4332, interval_samples_per_second: 0.373, interval_steps_per_second: 0.467, epoch: 5.1373[0m
[32m[2022-08-31 17:46:34,677] [    INFO][0m - loss: 0.03578096, learning_rate: 2.8447058823529412e-05, global_step: 1320, interval_runtime: 1.2846, interval_samples_per_second: 6.228, interval_steps_per_second: 7.785, epoch: 5.1765[0m
[32m[2022-08-31 17:46:35,960] [    INFO][0m - loss: 0.07720438, learning_rate: 2.843529411764706e-05, global_step: 1330, interval_runtime: 1.2829, interval_samples_per_second: 6.236, interval_steps_per_second: 7.795, epoch: 5.2157[0m
[32m[2022-08-31 17:46:37,243] [    INFO][0m - loss: 0.01792491, learning_rate: 2.8423529411764707e-05, global_step: 1340, interval_runtime: 1.2833, interval_samples_per_second: 6.234, interval_steps_per_second: 7.792, epoch: 5.2549[0m
[32m[2022-08-31 17:46:38,533] [    INFO][0m - loss: 0.04571821, learning_rate: 2.8411764705882353e-05, global_step: 1350, interval_runtime: 1.29, interval_samples_per_second: 6.202, interval_steps_per_second: 7.752, epoch: 5.2941[0m
[32m[2022-08-31 17:46:39,809] [    INFO][0m - loss: 0.01547789, learning_rate: 2.84e-05, global_step: 1360, interval_runtime: 1.276, interval_samples_per_second: 6.27, interval_steps_per_second: 7.837, epoch: 5.3333[0m
[32m[2022-08-31 17:46:41,091] [    INFO][0m - loss: 0.02088698, learning_rate: 2.838823529411765e-05, global_step: 1370, interval_runtime: 1.2823, interval_samples_per_second: 6.239, interval_steps_per_second: 7.799, epoch: 5.3725[0m
[32m[2022-08-31 17:46:42,380] [    INFO][0m - loss: 0.0140249, learning_rate: 2.8376470588235294e-05, global_step: 1380, interval_runtime: 1.2884, interval_samples_per_second: 6.209, interval_steps_per_second: 7.761, epoch: 5.4118[0m
[32m[2022-08-31 17:46:43,692] [    INFO][0m - loss: 0.07238513, learning_rate: 2.836470588235294e-05, global_step: 1390, interval_runtime: 1.3118, interval_samples_per_second: 6.098, interval_steps_per_second: 7.623, epoch: 5.451[0m
[32m[2022-08-31 17:46:44,969] [    INFO][0m - loss: 0.03808879, learning_rate: 2.835294117647059e-05, global_step: 1400, interval_runtime: 1.2773, interval_samples_per_second: 6.263, interval_steps_per_second: 7.829, epoch: 5.4902[0m
[32m[2022-08-31 17:46:44,970] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 17:46:44,970] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 17:46:44,970] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:46:44,970] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:46:44,970] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 17:46:58,968] [    INFO][0m - eval_loss: 2.365356683731079, eval_accuracy: 0.5947775628626693, eval_runtime: 13.9968, eval_samples_per_second: 147.748, eval_steps_per_second: 4.644, epoch: 5.4902[0m
[32m[2022-08-31 17:46:58,968] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1400[0m
[32m[2022-08-31 17:46:58,968] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 17:47:00,804] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1400/tokenizer_config.json[0m
[32m[2022-08-31 17:47:00,806] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1400/special_tokens_map.json[0m
[32m[2022-08-31 17:47:13,741] [    INFO][0m - loss: 0.10555036, learning_rate: 2.8341176470588236e-05, global_step: 1410, interval_runtime: 23.9494, interval_samples_per_second: 0.334, interval_steps_per_second: 0.418, epoch: 5.5294[0m
[32m[2022-08-31 17:47:15,034] [    INFO][0m - loss: 0.03393604, learning_rate: 2.8329411764705885e-05, global_step: 1420, interval_runtime: 6.1149, interval_samples_per_second: 1.308, interval_steps_per_second: 1.635, epoch: 5.5686[0m
[32m[2022-08-31 17:47:16,306] [    INFO][0m - loss: 0.02233957, learning_rate: 2.831764705882353e-05, global_step: 1430, interval_runtime: 1.2726, interval_samples_per_second: 6.286, interval_steps_per_second: 7.858, epoch: 5.6078[0m
[32m[2022-08-31 17:47:17,587] [    INFO][0m - loss: 0.04533936, learning_rate: 2.8305882352941177e-05, global_step: 1440, interval_runtime: 1.2801, interval_samples_per_second: 6.25, interval_steps_per_second: 7.812, epoch: 5.6471[0m
[32m[2022-08-31 17:47:18,871] [    INFO][0m - loss: 0.12151242, learning_rate: 2.8294117647058823e-05, global_step: 1450, interval_runtime: 1.2848, interval_samples_per_second: 6.227, interval_steps_per_second: 7.783, epoch: 5.6863[0m
[32m[2022-08-31 17:47:20,152] [    INFO][0m - loss: 0.0629802, learning_rate: 2.8282352941176472e-05, global_step: 1460, interval_runtime: 1.2807, interval_samples_per_second: 6.247, interval_steps_per_second: 7.808, epoch: 5.7255[0m
[32m[2022-08-31 17:47:21,435] [    INFO][0m - loss: 0.09476192, learning_rate: 2.827058823529412e-05, global_step: 1470, interval_runtime: 1.2834, interval_samples_per_second: 6.234, interval_steps_per_second: 7.792, epoch: 5.7647[0m
[32m[2022-08-31 17:47:22,709] [    INFO][0m - loss: 0.01102269, learning_rate: 2.8258823529411768e-05, global_step: 1480, interval_runtime: 1.2737, interval_samples_per_second: 6.281, interval_steps_per_second: 7.851, epoch: 5.8039[0m
[32m[2022-08-31 17:47:23,988] [    INFO][0m - loss: 0.00695753, learning_rate: 2.824705882352941e-05, global_step: 1490, interval_runtime: 1.2785, interval_samples_per_second: 6.257, interval_steps_per_second: 7.821, epoch: 5.8431[0m
[32m[2022-08-31 17:47:25,266] [    INFO][0m - loss: 0.03885506, learning_rate: 2.823529411764706e-05, global_step: 1500, interval_runtime: 1.2782, interval_samples_per_second: 6.259, interval_steps_per_second: 7.824, epoch: 5.8824[0m
[32m[2022-08-31 17:47:25,266] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 17:47:25,266] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 17:47:25,266] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:47:25,267] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:47:25,267] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 17:47:39,445] [    INFO][0m - eval_loss: 2.3244431018829346, eval_accuracy: 0.5952611218568665, eval_runtime: 14.1773, eval_samples_per_second: 145.867, eval_steps_per_second: 4.585, epoch: 5.8824[0m
[32m[2022-08-31 17:47:39,446] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1500[0m
[32m[2022-08-31 17:47:39,446] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 17:47:41,268] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1500/tokenizer_config.json[0m
[32m[2022-08-31 17:47:41,268] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1500/special_tokens_map.json[0m
[32m[2022-08-31 17:47:46,630] [    INFO][0m - loss: 0.01344386, learning_rate: 2.8223529411764706e-05, global_step: 1510, interval_runtime: 21.3641, interval_samples_per_second: 0.374, interval_steps_per_second: 0.468, epoch: 5.9216[0m
[32m[2022-08-31 17:47:47,913] [    INFO][0m - loss: 0.03911612, learning_rate: 2.8211764705882355e-05, global_step: 1520, interval_runtime: 1.2827, interval_samples_per_second: 6.237, interval_steps_per_second: 7.796, epoch: 5.9608[0m
[32m[2022-08-31 17:47:49,125] [    INFO][0m - loss: 0.01112697, learning_rate: 2.8199999999999998e-05, global_step: 1530, interval_runtime: 1.213, interval_samples_per_second: 6.595, interval_steps_per_second: 8.244, epoch: 6.0[0m
[32m[2022-08-31 17:47:50,485] [    INFO][0m - loss: 0.01993534, learning_rate: 2.8188235294117647e-05, global_step: 1540, interval_runtime: 1.3598, interval_samples_per_second: 5.883, interval_steps_per_second: 7.354, epoch: 6.0392[0m
[32m[2022-08-31 17:47:51,763] [    INFO][0m - loss: 0.01594153, learning_rate: 2.8176470588235293e-05, global_step: 1550, interval_runtime: 1.2783, interval_samples_per_second: 6.258, interval_steps_per_second: 7.823, epoch: 6.0784[0m
[32m[2022-08-31 17:47:53,045] [    INFO][0m - loss: 0.00942477, learning_rate: 2.8164705882352942e-05, global_step: 1560, interval_runtime: 1.281, interval_samples_per_second: 6.245, interval_steps_per_second: 7.807, epoch: 6.1176[0m
[32m[2022-08-31 17:47:54,345] [    INFO][0m - loss: 0.02090979, learning_rate: 2.8152941176470592e-05, global_step: 1570, interval_runtime: 1.3008, interval_samples_per_second: 6.15, interval_steps_per_second: 7.688, epoch: 6.1569[0m
[32m[2022-08-31 17:47:55,622] [    INFO][0m - loss: 0.00600292, learning_rate: 2.8141176470588234e-05, global_step: 1580, interval_runtime: 1.2769, interval_samples_per_second: 6.265, interval_steps_per_second: 7.831, epoch: 6.1961[0m
[32m[2022-08-31 17:47:56,903] [    INFO][0m - loss: 0.03391543, learning_rate: 2.8129411764705884e-05, global_step: 1590, interval_runtime: 1.2806, interval_samples_per_second: 6.247, interval_steps_per_second: 7.809, epoch: 6.2353[0m
[32m[2022-08-31 17:47:58,185] [    INFO][0m - loss: 0.01441633, learning_rate: 2.811764705882353e-05, global_step: 1600, interval_runtime: 1.2823, interval_samples_per_second: 6.239, interval_steps_per_second: 7.798, epoch: 6.2745[0m
[32m[2022-08-31 17:47:58,186] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 17:47:58,186] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 17:47:58,186] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:47:58,186] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:47:58,186] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 17:48:12,225] [    INFO][0m - eval_loss: 2.5228919982910156, eval_accuracy: 0.6034816247582205, eval_runtime: 14.0379, eval_samples_per_second: 147.315, eval_steps_per_second: 4.63, epoch: 6.2745[0m
[32m[2022-08-31 17:48:12,225] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1600[0m
[32m[2022-08-31 17:48:12,226] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 17:48:14,070] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1600/tokenizer_config.json[0m
[32m[2022-08-31 17:48:14,070] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1600/special_tokens_map.json[0m
[32m[2022-08-31 17:48:19,741] [    INFO][0m - loss: 0.13122025, learning_rate: 2.810588235294118e-05, global_step: 1610, interval_runtime: 21.5558, interval_samples_per_second: 0.371, interval_steps_per_second: 0.464, epoch: 6.3137[0m
[32m[2022-08-31 17:48:21,019] [    INFO][0m - loss: 0.03286683, learning_rate: 2.8094117647058825e-05, global_step: 1620, interval_runtime: 1.2778, interval_samples_per_second: 6.261, interval_steps_per_second: 7.826, epoch: 6.3529[0m
[32m[2022-08-31 17:48:22,306] [    INFO][0m - loss: 0.05704862, learning_rate: 2.808235294117647e-05, global_step: 1630, interval_runtime: 1.2868, interval_samples_per_second: 6.217, interval_steps_per_second: 7.771, epoch: 6.3922[0m
[32m[2022-08-31 17:48:23,592] [    INFO][0m - loss: 0.00335831, learning_rate: 2.8070588235294117e-05, global_step: 1640, interval_runtime: 1.2867, interval_samples_per_second: 6.217, interval_steps_per_second: 7.772, epoch: 6.4314[0m
[32m[2022-08-31 17:48:24,869] [    INFO][0m - loss: 0.00625007, learning_rate: 2.8058823529411766e-05, global_step: 1650, interval_runtime: 1.2754, interval_samples_per_second: 6.272, interval_steps_per_second: 7.841, epoch: 6.4706[0m
[32m[2022-08-31 17:48:26,148] [    INFO][0m - loss: 0.0140681, learning_rate: 2.8047058823529412e-05, global_step: 1660, interval_runtime: 1.2801, interval_samples_per_second: 6.249, interval_steps_per_second: 7.812, epoch: 6.5098[0m
[32m[2022-08-31 17:48:27,423] [    INFO][0m - loss: 0.01452555, learning_rate: 2.803529411764706e-05, global_step: 1670, interval_runtime: 1.275, interval_samples_per_second: 6.274, interval_steps_per_second: 7.843, epoch: 6.549[0m
[32m[2022-08-31 17:48:28,704] [    INFO][0m - loss: 0.03930223, learning_rate: 2.8023529411764704e-05, global_step: 1680, interval_runtime: 1.2806, interval_samples_per_second: 6.247, interval_steps_per_second: 7.809, epoch: 6.5882[0m
[32m[2022-08-31 17:48:30,000] [    INFO][0m - loss: 0.00449129, learning_rate: 2.8011764705882354e-05, global_step: 1690, interval_runtime: 1.2968, interval_samples_per_second: 6.169, interval_steps_per_second: 7.711, epoch: 6.6275[0m
[32m[2022-08-31 17:48:31,274] [    INFO][0m - loss: 0.01115494, learning_rate: 2.8e-05, global_step: 1700, interval_runtime: 1.2735, interval_samples_per_second: 6.282, interval_steps_per_second: 7.852, epoch: 6.6667[0m
[32m[2022-08-31 17:48:31,275] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 17:48:31,275] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 17:48:31,275] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:48:31,275] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:48:31,275] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 17:48:45,286] [    INFO][0m - eval_loss: 2.3995628356933594, eval_accuracy: 0.6126692456479691, eval_runtime: 14.0095, eval_samples_per_second: 147.614, eval_steps_per_second: 4.64, epoch: 6.6667[0m
[32m[2022-08-31 17:48:45,287] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1700[0m
[32m[2022-08-31 17:48:45,287] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 17:48:47,460] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1700/tokenizer_config.json[0m
[32m[2022-08-31 17:48:47,460] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1700/special_tokens_map.json[0m
[32m[2022-08-31 17:48:53,089] [    INFO][0m - loss: 0.00389364, learning_rate: 2.798823529411765e-05, global_step: 1710, interval_runtime: 21.8154, interval_samples_per_second: 0.367, interval_steps_per_second: 0.458, epoch: 6.7059[0m
[32m[2022-08-31 17:48:54,373] [    INFO][0m - loss: 0.00463689, learning_rate: 2.7976470588235295e-05, global_step: 1720, interval_runtime: 1.2832, interval_samples_per_second: 6.234, interval_steps_per_second: 7.793, epoch: 6.7451[0m
[32m[2022-08-31 17:48:55,656] [    INFO][0m - loss: 0.00579783, learning_rate: 2.796470588235294e-05, global_step: 1730, interval_runtime: 1.283, interval_samples_per_second: 6.236, interval_steps_per_second: 7.794, epoch: 6.7843[0m
[32m[2022-08-31 17:48:56,975] [    INFO][0m - loss: 0.00487575, learning_rate: 2.795294117647059e-05, global_step: 1740, interval_runtime: 1.3197, interval_samples_per_second: 6.062, interval_steps_per_second: 7.577, epoch: 6.8235[0m
[32m[2022-08-31 17:48:58,253] [    INFO][0m - loss: 0.04667433, learning_rate: 2.7941176470588236e-05, global_step: 1750, interval_runtime: 1.278, interval_samples_per_second: 6.26, interval_steps_per_second: 7.825, epoch: 6.8627[0m
[32m[2022-08-31 17:48:59,530] [    INFO][0m - loss: 0.01244864, learning_rate: 2.7929411764705886e-05, global_step: 1760, interval_runtime: 1.2769, interval_samples_per_second: 6.265, interval_steps_per_second: 7.831, epoch: 6.902[0m
[32m[2022-08-31 17:49:00,815] [    INFO][0m - loss: 0.01330289, learning_rate: 2.7917647058823528e-05, global_step: 1770, interval_runtime: 1.2851, interval_samples_per_second: 6.225, interval_steps_per_second: 7.781, epoch: 6.9412[0m
[32m[2022-08-31 17:49:02,097] [    INFO][0m - loss: 0.00578776, learning_rate: 2.7905882352941178e-05, global_step: 1780, interval_runtime: 1.2812, interval_samples_per_second: 6.244, interval_steps_per_second: 7.805, epoch: 6.9804[0m
[32m[2022-08-31 17:49:03,401] [    INFO][0m - loss: 0.00265903, learning_rate: 2.7894117647058824e-05, global_step: 1790, interval_runtime: 1.3043, interval_samples_per_second: 6.134, interval_steps_per_second: 7.667, epoch: 7.0196[0m
[32m[2022-08-31 17:49:04,681] [    INFO][0m - loss: 0.00842206, learning_rate: 2.7882352941176473e-05, global_step: 1800, interval_runtime: 1.2801, interval_samples_per_second: 6.25, interval_steps_per_second: 7.812, epoch: 7.0588[0m
[32m[2022-08-31 17:49:04,681] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 17:49:04,682] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 17:49:04,682] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:49:04,682] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:49:04,682] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 17:49:18,591] [    INFO][0m - eval_loss: 2.5905213356018066, eval_accuracy: 0.6015473887814313, eval_runtime: 13.9088, eval_samples_per_second: 148.683, eval_steps_per_second: 4.673, epoch: 7.0588[0m
[32m[2022-08-31 17:49:18,592] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1800[0m
[32m[2022-08-31 17:49:18,592] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 17:49:20,244] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1800/tokenizer_config.json[0m
[32m[2022-08-31 17:49:20,244] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1800/special_tokens_map.json[0m
[32m[2022-08-31 17:49:25,824] [    INFO][0m - loss: 0.00094859, learning_rate: 2.787058823529412e-05, global_step: 1810, interval_runtime: 21.1435, interval_samples_per_second: 0.378, interval_steps_per_second: 0.473, epoch: 7.098[0m
[32m[2022-08-31 17:49:27,103] [    INFO][0m - loss: 0.00214472, learning_rate: 2.7858823529411765e-05, global_step: 1820, interval_runtime: 1.2782, interval_samples_per_second: 6.259, interval_steps_per_second: 7.823, epoch: 7.1373[0m
[32m[2022-08-31 17:49:28,394] [    INFO][0m - loss: 0.00247297, learning_rate: 2.784705882352941e-05, global_step: 1830, interval_runtime: 1.2911, interval_samples_per_second: 6.196, interval_steps_per_second: 7.745, epoch: 7.1765[0m
[32m[2022-08-31 17:49:29,677] [    INFO][0m - loss: 0.00915222, learning_rate: 2.783529411764706e-05, global_step: 1840, interval_runtime: 1.2833, interval_samples_per_second: 6.234, interval_steps_per_second: 7.793, epoch: 7.2157[0m
[32m[2022-08-31 17:49:30,960] [    INFO][0m - loss: 0.02284099, learning_rate: 2.7823529411764706e-05, global_step: 1850, interval_runtime: 1.2832, interval_samples_per_second: 6.234, interval_steps_per_second: 7.793, epoch: 7.2549[0m
[32m[2022-08-31 17:49:32,254] [    INFO][0m - loss: 0.00506573, learning_rate: 2.7811764705882352e-05, global_step: 1860, interval_runtime: 1.2887, interval_samples_per_second: 6.208, interval_steps_per_second: 7.76, epoch: 7.2941[0m
[32m[2022-08-31 17:49:33,542] [    INFO][0m - loss: 0.00723492, learning_rate: 2.78e-05, global_step: 1870, interval_runtime: 1.2932, interval_samples_per_second: 6.186, interval_steps_per_second: 7.733, epoch: 7.3333[0m
[32m[2022-08-31 17:49:34,830] [    INFO][0m - loss: 0.00186207, learning_rate: 2.7788235294117647e-05, global_step: 1880, interval_runtime: 1.2876, interval_samples_per_second: 6.213, interval_steps_per_second: 7.766, epoch: 7.3725[0m
[32m[2022-08-31 17:49:36,119] [    INFO][0m - loss: 0.00306093, learning_rate: 2.7776470588235297e-05, global_step: 1890, interval_runtime: 1.289, interval_samples_per_second: 6.206, interval_steps_per_second: 7.758, epoch: 7.4118[0m
[32m[2022-08-31 17:49:37,405] [    INFO][0m - loss: 0.00756836, learning_rate: 2.7764705882352943e-05, global_step: 1900, interval_runtime: 1.2867, interval_samples_per_second: 6.218, interval_steps_per_second: 7.772, epoch: 7.451[0m
[32m[2022-08-31 17:49:37,406] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 17:49:37,406] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 17:49:37,406] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:49:37,407] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:49:37,407] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 17:49:51,437] [    INFO][0m - eval_loss: 2.6319382190704346, eval_accuracy: 0.6068665377176016, eval_runtime: 14.0295, eval_samples_per_second: 147.404, eval_steps_per_second: 4.633, epoch: 7.451[0m
[32m[2022-08-31 17:49:51,437] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1900[0m
[32m[2022-08-31 17:49:51,437] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 17:49:53,286] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1900/tokenizer_config.json[0m
[32m[2022-08-31 17:49:53,286] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1900/special_tokens_map.json[0m
[32m[2022-08-31 17:49:58,870] [    INFO][0m - loss: 0.00685616, learning_rate: 2.775294117647059e-05, global_step: 1910, interval_runtime: 21.4641, interval_samples_per_second: 0.373, interval_steps_per_second: 0.466, epoch: 7.4902[0m
[32m[2022-08-31 17:50:00,153] [    INFO][0m - loss: 0.0019126, learning_rate: 2.7741176470588235e-05, global_step: 1920, interval_runtime: 1.2836, interval_samples_per_second: 6.233, interval_steps_per_second: 7.791, epoch: 7.5294[0m
[32m[2022-08-31 17:50:01,426] [    INFO][0m - loss: 0.02007715, learning_rate: 2.7729411764705884e-05, global_step: 1930, interval_runtime: 1.2728, interval_samples_per_second: 6.286, interval_steps_per_second: 7.857, epoch: 7.5686[0m
[32m[2022-08-31 17:50:02,705] [    INFO][0m - loss: 0.00222422, learning_rate: 2.771764705882353e-05, global_step: 1940, interval_runtime: 1.2788, interval_samples_per_second: 6.256, interval_steps_per_second: 7.82, epoch: 7.6078[0m
[32m[2022-08-31 17:50:03,999] [    INFO][0m - loss: 0.00159899, learning_rate: 2.770588235294118e-05, global_step: 1950, interval_runtime: 1.2942, interval_samples_per_second: 6.181, interval_steps_per_second: 7.727, epoch: 7.6471[0m
[32m[2022-08-31 17:50:05,275] [    INFO][0m - loss: 0.06883934, learning_rate: 2.7694117647058822e-05, global_step: 1960, interval_runtime: 1.2756, interval_samples_per_second: 6.272, interval_steps_per_second: 7.839, epoch: 7.6863[0m
[32m[2022-08-31 17:50:06,550] [    INFO][0m - loss: 0.00132625, learning_rate: 2.768235294117647e-05, global_step: 1970, interval_runtime: 1.2752, interval_samples_per_second: 6.273, interval_steps_per_second: 7.842, epoch: 7.7255[0m
[32m[2022-08-31 17:50:07,833] [    INFO][0m - loss: 0.00101586, learning_rate: 2.7670588235294117e-05, global_step: 1980, interval_runtime: 1.2834, interval_samples_per_second: 6.233, interval_steps_per_second: 7.792, epoch: 7.7647[0m
[32m[2022-08-31 17:50:09,111] [    INFO][0m - loss: 0.00142538, learning_rate: 2.7658823529411767e-05, global_step: 1990, interval_runtime: 1.2782, interval_samples_per_second: 6.259, interval_steps_per_second: 7.824, epoch: 7.8039[0m
[32m[2022-08-31 17:50:10,389] [    INFO][0m - loss: 0.00292494, learning_rate: 2.764705882352941e-05, global_step: 2000, interval_runtime: 1.2769, interval_samples_per_second: 6.265, interval_steps_per_second: 7.831, epoch: 7.8431[0m
[32m[2022-08-31 17:50:10,389] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 17:50:10,390] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 17:50:10,390] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:50:10,390] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:50:10,390] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 17:50:24,317] [    INFO][0m - eval_loss: 2.51422381401062, eval_accuracy: 0.6126692456479691, eval_runtime: 13.9272, eval_samples_per_second: 148.487, eval_steps_per_second: 4.667, epoch: 7.8431[0m
[32m[2022-08-31 17:50:24,318] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2000[0m
[32m[2022-08-31 17:50:24,318] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 17:50:25,791] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2000/tokenizer_config.json[0m
[32m[2022-08-31 17:50:25,792] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2000/special_tokens_map.json[0m
[32m[2022-08-31 17:50:31,151] [    INFO][0m - loss: 0.00208683, learning_rate: 2.763529411764706e-05, global_step: 2010, interval_runtime: 20.7623, interval_samples_per_second: 0.385, interval_steps_per_second: 0.482, epoch: 7.8824[0m
[32m[2022-08-31 17:50:32,429] [    INFO][0m - loss: 0.01482409, learning_rate: 2.7623529411764705e-05, global_step: 2020, interval_runtime: 1.2777, interval_samples_per_second: 6.261, interval_steps_per_second: 7.827, epoch: 7.9216[0m
[32m[2022-08-31 17:50:33,704] [    INFO][0m - loss: 0.01960071, learning_rate: 2.7611764705882354e-05, global_step: 2030, interval_runtime: 1.276, interval_samples_per_second: 6.269, interval_steps_per_second: 7.837, epoch: 7.9608[0m
[32m[2022-08-31 17:50:34,915] [    INFO][0m - loss: 0.00392634, learning_rate: 2.7600000000000003e-05, global_step: 2040, interval_runtime: 1.2109, interval_samples_per_second: 6.607, interval_steps_per_second: 8.258, epoch: 8.0[0m
[32m[2022-08-31 17:50:36,275] [    INFO][0m - loss: 0.00131055, learning_rate: 2.7588235294117646e-05, global_step: 2050, interval_runtime: 1.3599, interval_samples_per_second: 5.883, interval_steps_per_second: 7.353, epoch: 8.0392[0m
[32m[2022-08-31 17:50:37,553] [    INFO][0m - loss: 0.00707092, learning_rate: 2.7576470588235295e-05, global_step: 2060, interval_runtime: 1.278, interval_samples_per_second: 6.26, interval_steps_per_second: 7.825, epoch: 8.0784[0m
[32m[2022-08-31 17:50:38,826] [    INFO][0m - loss: 0.00288913, learning_rate: 2.756470588235294e-05, global_step: 2070, interval_runtime: 1.2726, interval_samples_per_second: 6.287, interval_steps_per_second: 7.858, epoch: 8.1176[0m
[32m[2022-08-31 17:50:40,102] [    INFO][0m - loss: 0.0006859, learning_rate: 2.755294117647059e-05, global_step: 2080, interval_runtime: 1.2764, interval_samples_per_second: 6.268, interval_steps_per_second: 7.835, epoch: 8.1569[0m
[32m[2022-08-31 17:50:41,378] [    INFO][0m - loss: 0.0019785, learning_rate: 2.7541176470588237e-05, global_step: 2090, interval_runtime: 1.276, interval_samples_per_second: 6.269, interval_steps_per_second: 7.837, epoch: 8.1961[0m
[32m[2022-08-31 17:50:42,661] [    INFO][0m - loss: 0.00046382, learning_rate: 2.7529411764705883e-05, global_step: 2100, interval_runtime: 1.2824, interval_samples_per_second: 6.238, interval_steps_per_second: 7.798, epoch: 8.2353[0m
[32m[2022-08-31 17:50:42,661] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 17:50:42,661] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 17:50:42,661] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:50:42,662] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:50:42,662] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 17:50:56,563] [    INFO][0m - eval_loss: 2.5116238594055176, eval_accuracy: 0.6000967117988395, eval_runtime: 13.9006, eval_samples_per_second: 148.77, eval_steps_per_second: 4.676, epoch: 8.2353[0m
[32m[2022-08-31 17:50:56,563] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2100[0m
[32m[2022-08-31 17:50:56,563] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 17:50:58,383] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2100/tokenizer_config.json[0m
[32m[2022-08-31 17:50:58,384] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2100/special_tokens_map.json[0m
[32m[2022-08-31 17:51:03,990] [    INFO][0m - loss: 0.00255492, learning_rate: 2.751764705882353e-05, global_step: 2110, interval_runtime: 21.3294, interval_samples_per_second: 0.375, interval_steps_per_second: 0.469, epoch: 8.2745[0m
[32m[2022-08-31 17:51:05,273] [    INFO][0m - loss: 0.00101432, learning_rate: 2.7505882352941178e-05, global_step: 2120, interval_runtime: 1.2821, interval_samples_per_second: 6.24, interval_steps_per_second: 7.8, epoch: 8.3137[0m
[32m[2022-08-31 17:51:06,568] [    INFO][0m - loss: 0.00100006, learning_rate: 2.7494117647058824e-05, global_step: 2130, interval_runtime: 1.2957, interval_samples_per_second: 6.175, interval_steps_per_second: 7.718, epoch: 8.3529[0m
[32m[2022-08-31 17:51:07,873] [    INFO][0m - loss: 0.00310052, learning_rate: 2.7482352941176473e-05, global_step: 2140, interval_runtime: 1.3048, interval_samples_per_second: 6.131, interval_steps_per_second: 7.664, epoch: 8.3922[0m
[32m[2022-08-31 17:51:09,150] [    INFO][0m - loss: 0.00172914, learning_rate: 2.7470588235294116e-05, global_step: 2150, interval_runtime: 1.2774, interval_samples_per_second: 6.263, interval_steps_per_second: 7.828, epoch: 8.4314[0m
[32m[2022-08-31 17:51:10,425] [    INFO][0m - loss: 0.00093007, learning_rate: 2.7458823529411765e-05, global_step: 2160, interval_runtime: 1.2748, interval_samples_per_second: 6.275, interval_steps_per_second: 7.844, epoch: 8.4706[0m
[32m[2022-08-31 17:51:11,716] [    INFO][0m - loss: 0.04895796, learning_rate: 2.744705882352941e-05, global_step: 2170, interval_runtime: 1.2911, interval_samples_per_second: 6.196, interval_steps_per_second: 7.746, epoch: 8.5098[0m
[32m[2022-08-31 17:51:12,992] [    INFO][0m - loss: 0.01137862, learning_rate: 2.743529411764706e-05, global_step: 2180, interval_runtime: 1.2756, interval_samples_per_second: 6.272, interval_steps_per_second: 7.84, epoch: 8.549[0m
[32m[2022-08-31 17:51:14,275] [    INFO][0m - loss: 0.00099432, learning_rate: 2.7423529411764707e-05, global_step: 2190, interval_runtime: 1.2838, interval_samples_per_second: 6.231, interval_steps_per_second: 7.789, epoch: 8.5882[0m
[32m[2022-08-31 17:51:15,547] [    INFO][0m - loss: 0.00291462, learning_rate: 2.7411764705882353e-05, global_step: 2200, interval_runtime: 1.2716, interval_samples_per_second: 6.291, interval_steps_per_second: 7.864, epoch: 8.6275[0m
[32m[2022-08-31 17:51:15,547] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 17:51:15,548] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 17:51:15,548] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:51:15,548] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:51:15,548] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 17:51:29,472] [    INFO][0m - eval_loss: 2.5935044288635254, eval_accuracy: 0.6063829787234043, eval_runtime: 13.9232, eval_samples_per_second: 148.529, eval_steps_per_second: 4.668, epoch: 8.6275[0m
[32m[2022-08-31 17:51:29,472] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2200[0m
[32m[2022-08-31 17:51:29,472] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 17:51:31,125] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2200/tokenizer_config.json[0m
[32m[2022-08-31 17:51:31,125] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2200/special_tokens_map.json[0m
[32m[2022-08-31 17:51:36,428] [    INFO][0m - loss: 0.00232638, learning_rate: 2.7400000000000002e-05, global_step: 2210, interval_runtime: 20.8809, interval_samples_per_second: 0.383, interval_steps_per_second: 0.479, epoch: 8.6667[0m
[32m[2022-08-31 17:51:37,703] [    INFO][0m - loss: 0.00070633, learning_rate: 2.7388235294117648e-05, global_step: 2220, interval_runtime: 1.275, interval_samples_per_second: 6.275, interval_steps_per_second: 7.843, epoch: 8.7059[0m
[32m[2022-08-31 17:51:38,983] [    INFO][0m - loss: 0.00092874, learning_rate: 2.7376470588235297e-05, global_step: 2230, interval_runtime: 1.2805, interval_samples_per_second: 6.247, interval_steps_per_second: 7.809, epoch: 8.7451[0m
[32m[2022-08-31 17:51:40,262] [    INFO][0m - loss: 0.0010567, learning_rate: 2.736470588235294e-05, global_step: 2240, interval_runtime: 1.279, interval_samples_per_second: 6.255, interval_steps_per_second: 7.819, epoch: 8.7843[0m
[32m[2022-08-31 17:51:41,556] [    INFO][0m - loss: 0.0005141, learning_rate: 2.735294117647059e-05, global_step: 2250, interval_runtime: 1.2931, interval_samples_per_second: 6.187, interval_steps_per_second: 7.733, epoch: 8.8235[0m
[32m[2022-08-31 17:51:42,842] [    INFO][0m - loss: 0.00269909, learning_rate: 2.7341176470588235e-05, global_step: 2260, interval_runtime: 1.286, interval_samples_per_second: 6.221, interval_steps_per_second: 7.776, epoch: 8.8627[0m
[32m[2022-08-31 17:51:44,129] [    INFO][0m - loss: 0.00102776, learning_rate: 2.7329411764705885e-05, global_step: 2270, interval_runtime: 1.2869, interval_samples_per_second: 6.216, interval_steps_per_second: 7.771, epoch: 8.902[0m
[32m[2022-08-31 17:51:45,403] [    INFO][0m - loss: 0.00090263, learning_rate: 2.731764705882353e-05, global_step: 2280, interval_runtime: 1.2743, interval_samples_per_second: 6.278, interval_steps_per_second: 7.847, epoch: 8.9412[0m
[32m[2022-08-31 17:51:46,678] [    INFO][0m - loss: 0.00105979, learning_rate: 2.7305882352941177e-05, global_step: 2290, interval_runtime: 1.2746, interval_samples_per_second: 6.276, interval_steps_per_second: 7.845, epoch: 8.9804[0m
[32m[2022-08-31 17:51:47,967] [    INFO][0m - loss: 0.00068322, learning_rate: 2.7294117647058822e-05, global_step: 2300, interval_runtime: 1.2887, interval_samples_per_second: 6.208, interval_steps_per_second: 7.76, epoch: 9.0196[0m
[32m[2022-08-31 17:51:47,967] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 17:51:47,967] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 17:51:47,968] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:51:47,968] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:51:47,968] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 17:52:01,901] [    INFO][0m - eval_loss: 2.6485679149627686, eval_accuracy: 0.6107350096711799, eval_runtime: 13.9329, eval_samples_per_second: 148.426, eval_steps_per_second: 4.665, epoch: 9.0196[0m
[32m[2022-08-31 17:52:01,902] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2300[0m
[32m[2022-08-31 17:52:01,902] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 17:52:04,063] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2300/tokenizer_config.json[0m
[32m[2022-08-31 17:52:04,064] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2300/special_tokens_map.json[0m
[32m[2022-08-31 17:52:09,838] [    INFO][0m - loss: 0.00015332, learning_rate: 2.7282352941176472e-05, global_step: 2310, interval_runtime: 21.8714, interval_samples_per_second: 0.366, interval_steps_per_second: 0.457, epoch: 9.0588[0m
[32m[2022-08-31 17:52:11,124] [    INFO][0m - loss: 0.00069974, learning_rate: 2.7270588235294118e-05, global_step: 2320, interval_runtime: 1.286, interval_samples_per_second: 6.221, interval_steps_per_second: 7.776, epoch: 9.098[0m
[32m[2022-08-31 17:52:12,407] [    INFO][0m - loss: 0.00020686, learning_rate: 2.7258823529411764e-05, global_step: 2330, interval_runtime: 1.2834, interval_samples_per_second: 6.233, interval_steps_per_second: 7.792, epoch: 9.1373[0m
[32m[2022-08-31 17:52:13,684] [    INFO][0m - loss: 0.00058225, learning_rate: 2.7247058823529413e-05, global_step: 2340, interval_runtime: 1.2773, interval_samples_per_second: 6.263, interval_steps_per_second: 7.829, epoch: 9.1765[0m
[32m[2022-08-31 17:52:14,983] [    INFO][0m - loss: 0.00029449, learning_rate: 2.723529411764706e-05, global_step: 2350, interval_runtime: 1.2985, interval_samples_per_second: 6.161, interval_steps_per_second: 7.701, epoch: 9.2157[0m
[32m[2022-08-31 17:52:16,269] [    INFO][0m - loss: 0.00026754, learning_rate: 2.722352941176471e-05, global_step: 2360, interval_runtime: 1.2856, interval_samples_per_second: 6.223, interval_steps_per_second: 7.779, epoch: 9.2549[0m
[32m[2022-08-31 17:52:17,545] [    INFO][0m - loss: 0.00050401, learning_rate: 2.7211764705882354e-05, global_step: 2370, interval_runtime: 1.277, interval_samples_per_second: 6.265, interval_steps_per_second: 7.831, epoch: 9.2941[0m
[32m[2022-08-31 17:52:18,828] [    INFO][0m - loss: 0.00026261, learning_rate: 2.72e-05, global_step: 2380, interval_runtime: 1.2817, interval_samples_per_second: 6.242, interval_steps_per_second: 7.802, epoch: 9.3333[0m
[32m[2022-08-31 17:52:20,106] [    INFO][0m - loss: 0.00068233, learning_rate: 2.7188235294117646e-05, global_step: 2390, interval_runtime: 1.2783, interval_samples_per_second: 6.258, interval_steps_per_second: 7.823, epoch: 9.3725[0m
[32m[2022-08-31 17:52:21,379] [    INFO][0m - loss: 0.00057359, learning_rate: 2.7176470588235296e-05, global_step: 2400, interval_runtime: 1.2737, interval_samples_per_second: 6.281, interval_steps_per_second: 7.851, epoch: 9.4118[0m
[32m[2022-08-31 17:52:21,380] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 17:52:21,380] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 17:52:21,380] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:52:21,381] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:52:21,381] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 17:52:35,369] [    INFO][0m - eval_loss: 2.767237663269043, eval_accuracy: 0.6112185686653772, eval_runtime: 13.9874, eval_samples_per_second: 147.847, eval_steps_per_second: 4.647, epoch: 9.4118[0m
[32m[2022-08-31 17:52:35,370] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2400[0m
[32m[2022-08-31 17:52:35,370] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 17:52:37,030] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2400/tokenizer_config.json[0m
[32m[2022-08-31 17:52:37,031] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2400/special_tokens_map.json[0m
[32m[2022-08-31 17:52:42,314] [    INFO][0m - loss: 0.00086178, learning_rate: 2.7164705882352942e-05, global_step: 2410, interval_runtime: 20.9344, interval_samples_per_second: 0.382, interval_steps_per_second: 0.478, epoch: 9.451[0m
[32m[2022-08-31 17:52:43,589] [    INFO][0m - loss: 0.00123665, learning_rate: 2.715294117647059e-05, global_step: 2420, interval_runtime: 1.2756, interval_samples_per_second: 6.272, interval_steps_per_second: 7.84, epoch: 9.4902[0m
[32m[2022-08-31 17:52:44,855] [    INFO][0m - loss: 0.00028873, learning_rate: 2.7141176470588234e-05, global_step: 2430, interval_runtime: 1.2657, interval_samples_per_second: 6.321, interval_steps_per_second: 7.901, epoch: 9.5294[0m
[32m[2022-08-31 17:52:46,127] [    INFO][0m - loss: 0.00673955, learning_rate: 2.7129411764705883e-05, global_step: 2440, interval_runtime: 1.2722, interval_samples_per_second: 6.289, interval_steps_per_second: 7.861, epoch: 9.5686[0m
[32m[2022-08-31 17:52:47,413] [    INFO][0m - loss: 0.00071605, learning_rate: 2.711764705882353e-05, global_step: 2450, interval_runtime: 1.2855, interval_samples_per_second: 6.223, interval_steps_per_second: 7.779, epoch: 9.6078[0m
[32m[2022-08-31 17:52:48,694] [    INFO][0m - loss: 0.00329266, learning_rate: 2.710588235294118e-05, global_step: 2460, interval_runtime: 1.2816, interval_samples_per_second: 6.242, interval_steps_per_second: 7.803, epoch: 9.6471[0m
[32m[2022-08-31 17:52:49,964] [    INFO][0m - loss: 0.00429033, learning_rate: 2.7094117647058824e-05, global_step: 2470, interval_runtime: 1.2699, interval_samples_per_second: 6.3, interval_steps_per_second: 7.875, epoch: 9.6863[0m
[32m[2022-08-31 17:52:51,235] [    INFO][0m - loss: 0.00866356, learning_rate: 2.708235294117647e-05, global_step: 2480, interval_runtime: 1.2713, interval_samples_per_second: 6.293, interval_steps_per_second: 7.866, epoch: 9.7255[0m
[32m[2022-08-31 17:52:52,507] [    INFO][0m - loss: 0.00032649, learning_rate: 2.707058823529412e-05, global_step: 2490, interval_runtime: 1.2717, interval_samples_per_second: 6.291, interval_steps_per_second: 7.864, epoch: 9.7647[0m
[32m[2022-08-31 17:52:53,783] [    INFO][0m - loss: 0.00051371, learning_rate: 2.7058823529411766e-05, global_step: 2500, interval_runtime: 1.2763, interval_samples_per_second: 6.268, interval_steps_per_second: 7.835, epoch: 9.8039[0m
[32m[2022-08-31 17:52:53,784] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 17:52:53,784] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 17:52:53,784] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:52:53,784] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:52:53,784] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 17:53:07,700] [    INFO][0m - eval_loss: 2.6210994720458984, eval_accuracy: 0.6136363636363636, eval_runtime: 13.9155, eval_samples_per_second: 148.611, eval_steps_per_second: 4.671, epoch: 9.8039[0m
[32m[2022-08-31 17:53:07,701] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2500[0m
[32m[2022-08-31 17:53:07,701] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 17:53:09,447] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2500/tokenizer_config.json[0m
[32m[2022-08-31 17:53:09,447] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2500/special_tokens_map.json[0m
[32m[2022-08-31 17:53:14,699] [    INFO][0m - loss: 0.00074941, learning_rate: 2.7047058823529415e-05, global_step: 2510, interval_runtime: 20.9158, interval_samples_per_second: 0.382, interval_steps_per_second: 0.478, epoch: 9.8431[0m
[32m[2022-08-31 17:53:15,992] [    INFO][0m - loss: 0.00030569, learning_rate: 2.7035294117647058e-05, global_step: 2520, interval_runtime: 1.2922, interval_samples_per_second: 6.191, interval_steps_per_second: 7.739, epoch: 9.8824[0m
[32m[2022-08-31 17:53:17,268] [    INFO][0m - loss: 0.00050145, learning_rate: 2.7023529411764707e-05, global_step: 2530, interval_runtime: 1.2767, interval_samples_per_second: 6.266, interval_steps_per_second: 7.833, epoch: 9.9216[0m
[32m[2022-08-31 17:53:18,542] [    INFO][0m - loss: 0.00071953, learning_rate: 2.7011764705882353e-05, global_step: 2540, interval_runtime: 1.2739, interval_samples_per_second: 6.28, interval_steps_per_second: 7.85, epoch: 9.9608[0m
[32m[2022-08-31 17:53:19,756] [    INFO][0m - loss: 0.00244914, learning_rate: 2.7000000000000002e-05, global_step: 2550, interval_runtime: 1.2134, interval_samples_per_second: 6.593, interval_steps_per_second: 8.241, epoch: 10.0[0m
[32m[2022-08-31 17:53:21,116] [    INFO][0m - loss: 0.00046884, learning_rate: 2.698823529411765e-05, global_step: 2560, interval_runtime: 1.3606, interval_samples_per_second: 5.88, interval_steps_per_second: 7.35, epoch: 10.0392[0m
[32m[2022-08-31 17:53:22,417] [    INFO][0m - loss: 0.00189053, learning_rate: 2.6976470588235294e-05, global_step: 2570, interval_runtime: 1.3012, interval_samples_per_second: 6.148, interval_steps_per_second: 7.685, epoch: 10.0784[0m
[32m[2022-08-31 17:53:23,699] [    INFO][0m - loss: 0.00023125, learning_rate: 2.696470588235294e-05, global_step: 2580, interval_runtime: 1.2817, interval_samples_per_second: 6.242, interval_steps_per_second: 7.802, epoch: 10.1176[0m
[32m[2022-08-31 17:53:24,978] [    INFO][0m - loss: 0.00083423, learning_rate: 2.695294117647059e-05, global_step: 2590, interval_runtime: 1.2793, interval_samples_per_second: 6.254, interval_steps_per_second: 7.817, epoch: 10.1569[0m
[32m[2022-08-31 17:53:26,273] [    INFO][0m - loss: 0.0199442, learning_rate: 2.6941176470588236e-05, global_step: 2600, interval_runtime: 1.2945, interval_samples_per_second: 6.18, interval_steps_per_second: 7.725, epoch: 10.1961[0m
[32m[2022-08-31 17:53:26,274] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 17:53:26,275] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 17:53:26,275] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:53:26,275] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:53:26,275] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 17:53:40,229] [    INFO][0m - eval_loss: 2.7623236179351807, eval_accuracy: 0.6088007736943907, eval_runtime: 13.9541, eval_samples_per_second: 148.2, eval_steps_per_second: 4.658, epoch: 10.1961[0m
[32m[2022-08-31 17:53:40,230] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2600[0m
[32m[2022-08-31 17:53:40,230] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 17:53:43,592] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2600/tokenizer_config.json[0m
[32m[2022-08-31 17:53:43,592] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2600/special_tokens_map.json[0m
[32m[2022-08-31 17:53:54,981] [    INFO][0m - loss: 0.03566601, learning_rate: 2.6929411764705885e-05, global_step: 2610, interval_runtime: 27.8743, interval_samples_per_second: 0.287, interval_steps_per_second: 0.359, epoch: 10.2353[0m
[32m[2022-08-31 17:53:56,259] [    INFO][0m - loss: 0.00048662, learning_rate: 2.6917647058823528e-05, global_step: 2620, interval_runtime: 2.1111, interval_samples_per_second: 3.79, interval_steps_per_second: 4.737, epoch: 10.2745[0m
[32m[2022-08-31 17:53:57,540] [    INFO][0m - loss: 0.00018967, learning_rate: 2.6905882352941177e-05, global_step: 2630, interval_runtime: 1.2816, interval_samples_per_second: 6.242, interval_steps_per_second: 7.803, epoch: 10.3137[0m
[32m[2022-08-31 17:53:58,815] [    INFO][0m - loss: 0.00046491, learning_rate: 2.6894117647058823e-05, global_step: 2640, interval_runtime: 1.2751, interval_samples_per_second: 6.274, interval_steps_per_second: 7.843, epoch: 10.3529[0m
[32m[2022-08-31 17:54:00,095] [    INFO][0m - loss: 0.0001072, learning_rate: 2.6882352941176472e-05, global_step: 2650, interval_runtime: 1.2797, interval_samples_per_second: 6.251, interval_steps_per_second: 7.814, epoch: 10.3922[0m
[32m[2022-08-31 17:54:01,383] [    INFO][0m - loss: 0.0017345, learning_rate: 2.6870588235294118e-05, global_step: 2660, interval_runtime: 1.2882, interval_samples_per_second: 6.21, interval_steps_per_second: 7.763, epoch: 10.4314[0m
[32m[2022-08-31 17:54:02,665] [    INFO][0m - loss: 0.00064026, learning_rate: 2.6858823529411764e-05, global_step: 2670, interval_runtime: 1.2817, interval_samples_per_second: 6.242, interval_steps_per_second: 7.802, epoch: 10.4706[0m
[32m[2022-08-31 17:54:03,940] [    INFO][0m - loss: 0.00099398, learning_rate: 2.6847058823529414e-05, global_step: 2680, interval_runtime: 1.2754, interval_samples_per_second: 6.273, interval_steps_per_second: 7.841, epoch: 10.5098[0m
[32m[2022-08-31 17:54:05,231] [    INFO][0m - loss: 0.00031948, learning_rate: 2.683529411764706e-05, global_step: 2690, interval_runtime: 1.2908, interval_samples_per_second: 6.198, interval_steps_per_second: 7.747, epoch: 10.549[0m
[32m[2022-08-31 17:54:06,510] [    INFO][0m - loss: 0.00047515, learning_rate: 2.682352941176471e-05, global_step: 2700, interval_runtime: 1.2796, interval_samples_per_second: 6.252, interval_steps_per_second: 7.815, epoch: 10.5882[0m
[32m[2022-08-31 17:54:06,511] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 17:54:06,511] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 17:54:06,511] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:54:06,511] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:54:06,511] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 17:54:20,466] [    INFO][0m - eval_loss: 2.7530152797698975, eval_accuracy: 0.6107350096711799, eval_runtime: 13.9539, eval_samples_per_second: 148.202, eval_steps_per_second: 4.658, epoch: 10.5882[0m
[32m[2022-08-31 17:54:20,466] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2700[0m
[32m[2022-08-31 17:54:20,467] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 17:54:21,425] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2700/tokenizer_config.json[0m
[32m[2022-08-31 17:54:21,425] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2700/special_tokens_map.json[0m
[32m[2022-08-31 17:54:24,544] [    INFO][0m - loss: 0.05745364, learning_rate: 2.681176470588235e-05, global_step: 2710, interval_runtime: 18.0338, interval_samples_per_second: 0.444, interval_steps_per_second: 0.555, epoch: 10.6275[0m
[32m[2022-08-31 17:54:25,816] [    INFO][0m - loss: 0.00183045, learning_rate: 2.68e-05, global_step: 2720, interval_runtime: 1.2719, interval_samples_per_second: 6.29, interval_steps_per_second: 7.862, epoch: 10.6667[0m
[32m[2022-08-31 17:54:27,092] [    INFO][0m - loss: 0.00199404, learning_rate: 2.6788235294117647e-05, global_step: 2730, interval_runtime: 1.2755, interval_samples_per_second: 6.272, interval_steps_per_second: 7.84, epoch: 10.7059[0m
[32m[2022-08-31 17:54:28,376] [    INFO][0m - loss: 0.00034664, learning_rate: 2.6776470588235296e-05, global_step: 2740, interval_runtime: 1.2842, interval_samples_per_second: 6.229, interval_steps_per_second: 7.787, epoch: 10.7451[0m
[32m[2022-08-31 17:54:29,666] [    INFO][0m - loss: 0.00052609, learning_rate: 2.6764705882352942e-05, global_step: 2750, interval_runtime: 1.2899, interval_samples_per_second: 6.202, interval_steps_per_second: 7.753, epoch: 10.7843[0m
[32m[2022-08-31 17:54:30,945] [    INFO][0m - loss: 0.00051266, learning_rate: 2.6752941176470588e-05, global_step: 2760, interval_runtime: 1.279, interval_samples_per_second: 6.255, interval_steps_per_second: 7.819, epoch: 10.8235[0m
[32m[2022-08-31 17:54:32,229] [    INFO][0m - loss: 0.00022518, learning_rate: 2.6741176470588234e-05, global_step: 2770, interval_runtime: 1.2838, interval_samples_per_second: 6.232, interval_steps_per_second: 7.789, epoch: 10.8627[0m
[32m[2022-08-31 17:54:33,511] [    INFO][0m - loss: 0.00018325, learning_rate: 2.6729411764705884e-05, global_step: 2780, interval_runtime: 1.2827, interval_samples_per_second: 6.237, interval_steps_per_second: 7.796, epoch: 10.902[0m
[32m[2022-08-31 17:54:34,787] [    INFO][0m - loss: 0.00181895, learning_rate: 2.671764705882353e-05, global_step: 2790, interval_runtime: 1.2751, interval_samples_per_second: 6.274, interval_steps_per_second: 7.842, epoch: 10.9412[0m
[32m[2022-08-31 17:54:36,088] [    INFO][0m - loss: 0.00104498, learning_rate: 2.6705882352941175e-05, global_step: 2800, interval_runtime: 1.301, interval_samples_per_second: 6.149, interval_steps_per_second: 7.686, epoch: 10.9804[0m
[32m[2022-08-31 17:54:36,088] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 17:54:36,088] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 17:54:36,089] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:54:36,089] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:54:36,089] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 17:54:50,027] [    INFO][0m - eval_loss: 2.7968335151672363, eval_accuracy: 0.6073500967117988, eval_runtime: 13.9376, eval_samples_per_second: 148.376, eval_steps_per_second: 4.664, epoch: 10.9804[0m
[32m[2022-08-31 17:54:50,027] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2800[0m
[32m[2022-08-31 17:54:50,027] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 17:54:50,953] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2800/tokenizer_config.json[0m
[32m[2022-08-31 17:54:50,953] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2800/special_tokens_map.json[0m
[32m[2022-08-31 17:54:54,119] [    INFO][0m - loss: 0.00058302, learning_rate: 2.6694117647058825e-05, global_step: 2810, interval_runtime: 18.0311, interval_samples_per_second: 0.444, interval_steps_per_second: 0.555, epoch: 11.0196[0m
[32m[2022-08-31 17:54:55,399] [    INFO][0m - loss: 0.00033759, learning_rate: 2.668235294117647e-05, global_step: 2820, interval_runtime: 1.2801, interval_samples_per_second: 6.249, interval_steps_per_second: 7.812, epoch: 11.0588[0m
[32m[2022-08-31 17:54:56,668] [    INFO][0m - loss: 0.00085837, learning_rate: 2.667058823529412e-05, global_step: 2830, interval_runtime: 1.2692, interval_samples_per_second: 6.303, interval_steps_per_second: 7.879, epoch: 11.098[0m
[32m[2022-08-31 17:54:57,977] [    INFO][0m - loss: 0.00022756, learning_rate: 2.6658823529411766e-05, global_step: 2840, interval_runtime: 1.3091, interval_samples_per_second: 6.111, interval_steps_per_second: 7.639, epoch: 11.1373[0m
[32m[2022-08-31 17:54:59,259] [    INFO][0m - loss: 0.00054033, learning_rate: 2.6647058823529412e-05, global_step: 2850, interval_runtime: 1.2822, interval_samples_per_second: 6.239, interval_steps_per_second: 7.799, epoch: 11.1765[0m
[32m[2022-08-31 17:55:00,538] [    INFO][0m - loss: 0.00033534, learning_rate: 2.6635294117647058e-05, global_step: 2860, interval_runtime: 1.279, interval_samples_per_second: 6.255, interval_steps_per_second: 7.819, epoch: 11.2157[0m
[32m[2022-08-31 17:55:01,816] [    INFO][0m - loss: 0.00035606, learning_rate: 2.6623529411764707e-05, global_step: 2870, interval_runtime: 1.2779, interval_samples_per_second: 6.26, interval_steps_per_second: 7.825, epoch: 11.2549[0m
[32m[2022-08-31 17:55:03,099] [    INFO][0m - loss: 0.00022343, learning_rate: 2.6611764705882353e-05, global_step: 2880, interval_runtime: 1.2831, interval_samples_per_second: 6.235, interval_steps_per_second: 7.794, epoch: 11.2941[0m
[32m[2022-08-31 17:55:04,377] [    INFO][0m - loss: 0.00080014, learning_rate: 2.6600000000000003e-05, global_step: 2890, interval_runtime: 1.278, interval_samples_per_second: 6.26, interval_steps_per_second: 7.825, epoch: 11.3333[0m
[32m[2022-08-31 17:55:05,669] [    INFO][0m - loss: 0.00031109, learning_rate: 2.6588235294117645e-05, global_step: 2900, interval_runtime: 1.2912, interval_samples_per_second: 6.196, interval_steps_per_second: 7.745, epoch: 11.3725[0m
[32m[2022-08-31 17:55:05,669] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 17:55:05,670] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 17:55:05,670] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:55:05,670] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:55:05,670] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 17:55:19,704] [    INFO][0m - eval_loss: 2.6964046955108643, eval_accuracy: 0.6179883945841392, eval_runtime: 14.0333, eval_samples_per_second: 147.364, eval_steps_per_second: 4.632, epoch: 11.3725[0m
[32m[2022-08-31 17:55:19,704] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2900[0m
[32m[2022-08-31 17:55:19,704] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 17:55:20,652] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2900/tokenizer_config.json[0m
[32m[2022-08-31 17:55:20,652] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2900/special_tokens_map.json[0m
[32m[2022-08-31 17:55:23,779] [    INFO][0m - loss: 0.00026011, learning_rate: 2.6576470588235295e-05, global_step: 2910, interval_runtime: 18.1107, interval_samples_per_second: 0.442, interval_steps_per_second: 0.552, epoch: 11.4118[0m
[32m[2022-08-31 17:55:25,058] [    INFO][0m - loss: 0.00038946, learning_rate: 2.656470588235294e-05, global_step: 2920, interval_runtime: 1.2788, interval_samples_per_second: 6.256, interval_steps_per_second: 7.82, epoch: 11.451[0m
[32m[2022-08-31 17:55:26,337] [    INFO][0m - loss: 0.0002352, learning_rate: 2.655294117647059e-05, global_step: 2930, interval_runtime: 1.2789, interval_samples_per_second: 6.255, interval_steps_per_second: 7.819, epoch: 11.4902[0m
[32m[2022-08-31 17:55:27,615] [    INFO][0m - loss: 0.00054712, learning_rate: 2.6541176470588236e-05, global_step: 2940, interval_runtime: 1.2779, interval_samples_per_second: 6.26, interval_steps_per_second: 7.826, epoch: 11.5294[0m
[32m[2022-08-31 17:55:28,893] [    INFO][0m - loss: 0.00019965, learning_rate: 2.6529411764705882e-05, global_step: 2950, interval_runtime: 1.2786, interval_samples_per_second: 6.257, interval_steps_per_second: 7.821, epoch: 11.5686[0m
[32m[2022-08-31 17:55:30,177] [    INFO][0m - loss: 0.02437303, learning_rate: 2.651764705882353e-05, global_step: 2960, interval_runtime: 1.2839, interval_samples_per_second: 6.231, interval_steps_per_second: 7.789, epoch: 11.6078[0m
[32m[2022-08-31 17:55:31,464] [    INFO][0m - loss: 0.00015759, learning_rate: 2.6505882352941177e-05, global_step: 2970, interval_runtime: 1.2866, interval_samples_per_second: 6.218, interval_steps_per_second: 7.773, epoch: 11.6471[0m
[32m[2022-08-31 17:55:32,747] [    INFO][0m - loss: 0.00012509, learning_rate: 2.6494117647058827e-05, global_step: 2980, interval_runtime: 1.2833, interval_samples_per_second: 6.234, interval_steps_per_second: 7.792, epoch: 11.6863[0m
[32m[2022-08-31 17:55:34,028] [    INFO][0m - loss: 0.00020069, learning_rate: 2.648235294117647e-05, global_step: 2990, interval_runtime: 1.2803, interval_samples_per_second: 6.249, interval_steps_per_second: 7.811, epoch: 11.7255[0m
[32m[2022-08-31 17:55:35,311] [    INFO][0m - loss: 0.00025372, learning_rate: 2.647058823529412e-05, global_step: 3000, interval_runtime: 1.2834, interval_samples_per_second: 6.234, interval_steps_per_second: 7.792, epoch: 11.7647[0m
[32m[2022-08-31 17:55:35,312] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 17:55:35,312] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 17:55:35,312] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:55:35,312] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:55:35,312] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 17:55:49,322] [    INFO][0m - eval_loss: 2.7314929962158203, eval_accuracy: 0.6165377176015474, eval_runtime: 14.0101, eval_samples_per_second: 147.608, eval_steps_per_second: 4.64, epoch: 11.7647[0m
[32m[2022-08-31 17:55:49,323] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-3000[0m
[32m[2022-08-31 17:55:49,323] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 17:55:50,235] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-3000/tokenizer_config.json[0m
[32m[2022-08-31 17:55:50,235] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-3000/special_tokens_map.json[0m
[32m[2022-08-31 17:55:53,777] [    INFO][0m - loss: 0.00027322, learning_rate: 2.6458823529411765e-05, global_step: 3010, interval_runtime: 18.4657, interval_samples_per_second: 0.433, interval_steps_per_second: 0.542, epoch: 11.8039[0m
[32m[2022-08-31 17:55:55,053] [    INFO][0m - loss: 8.119e-05, learning_rate: 2.6447058823529414e-05, global_step: 3020, interval_runtime: 1.2762, interval_samples_per_second: 6.268, interval_steps_per_second: 7.836, epoch: 11.8431[0m
[32m[2022-08-31 17:55:56,333] [    INFO][0m - loss: 0.00023591, learning_rate: 2.643529411764706e-05, global_step: 3030, interval_runtime: 1.2802, interval_samples_per_second: 6.249, interval_steps_per_second: 7.811, epoch: 11.8824[0m
[32m[2022-08-31 17:55:57,610] [    INFO][0m - loss: 0.00023199, learning_rate: 2.6423529411764706e-05, global_step: 3040, interval_runtime: 1.2764, interval_samples_per_second: 6.268, interval_steps_per_second: 7.835, epoch: 11.9216[0m
[32m[2022-08-31 17:55:58,887] [    INFO][0m - loss: 0.00017689, learning_rate: 2.6411764705882352e-05, global_step: 3050, interval_runtime: 1.2777, interval_samples_per_second: 6.261, interval_steps_per_second: 7.827, epoch: 11.9608[0m
[32m[2022-08-31 17:56:00,124] [    INFO][0m - loss: 0.00028587, learning_rate: 2.64e-05, global_step: 3060, interval_runtime: 1.2369, interval_samples_per_second: 6.468, interval_steps_per_second: 8.085, epoch: 12.0[0m
[32m[2022-08-31 17:56:01,497] [    INFO][0m - loss: 0.00011842, learning_rate: 2.6388235294117647e-05, global_step: 3070, interval_runtime: 1.3728, interval_samples_per_second: 5.828, interval_steps_per_second: 7.285, epoch: 12.0392[0m
[32m[2022-08-31 17:56:02,793] [    INFO][0m - loss: 8.253e-05, learning_rate: 2.6376470588235297e-05, global_step: 3080, interval_runtime: 1.2963, interval_samples_per_second: 6.171, interval_steps_per_second: 7.714, epoch: 12.0784[0m
[32m[2022-08-31 17:56:04,082] [    INFO][0m - loss: 9.646e-05, learning_rate: 2.636470588235294e-05, global_step: 3090, interval_runtime: 1.2885, interval_samples_per_second: 6.209, interval_steps_per_second: 7.761, epoch: 12.1176[0m
[32m[2022-08-31 17:56:05,389] [    INFO][0m - loss: 0.00013539, learning_rate: 2.635294117647059e-05, global_step: 3100, interval_runtime: 1.3074, interval_samples_per_second: 6.119, interval_steps_per_second: 7.649, epoch: 12.1569[0m
[32m[2022-08-31 17:56:05,390] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 17:56:05,390] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 17:56:05,391] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:56:05,391] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:56:05,391] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 17:56:19,400] [    INFO][0m - eval_loss: 2.6996095180511475, eval_accuracy: 0.6165377176015474, eval_runtime: 14.0093, eval_samples_per_second: 147.616, eval_steps_per_second: 4.64, epoch: 12.1569[0m
[32m[2022-08-31 17:56:19,401] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-3100[0m
[32m[2022-08-31 17:56:19,401] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 17:56:20,369] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-3100/tokenizer_config.json[0m
[32m[2022-08-31 17:56:20,370] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-3100/special_tokens_map.json[0m
[32m[2022-08-31 17:56:24,203] [    INFO][0m - loss: 0.00012394, learning_rate: 2.6341176470588235e-05, global_step: 3110, interval_runtime: 18.8137, interval_samples_per_second: 0.425, interval_steps_per_second: 0.532, epoch: 12.1961[0m
[32m[2022-08-31 17:56:25,490] [    INFO][0m - loss: 0.00020807, learning_rate: 2.6329411764705884e-05, global_step: 3120, interval_runtime: 1.2874, interval_samples_per_second: 6.214, interval_steps_per_second: 7.768, epoch: 12.2353[0m
[32m[2022-08-31 17:56:26,768] [    INFO][0m - loss: 7.522e-05, learning_rate: 2.631764705882353e-05, global_step: 3130, interval_runtime: 1.2778, interval_samples_per_second: 6.261, interval_steps_per_second: 7.826, epoch: 12.2745[0m
[32m[2022-08-31 17:56:28,049] [    INFO][0m - loss: 0.00010751, learning_rate: 2.6305882352941176e-05, global_step: 3140, interval_runtime: 1.2809, interval_samples_per_second: 6.246, interval_steps_per_second: 7.807, epoch: 12.3137[0m
[32m[2022-08-31 17:56:29,334] [    INFO][0m - loss: 0.00011041, learning_rate: 2.6294117647058825e-05, global_step: 3150, interval_runtime: 1.2851, interval_samples_per_second: 6.225, interval_steps_per_second: 7.782, epoch: 12.3529[0m
[32m[2022-08-31 17:56:30,613] [    INFO][0m - loss: 0.0001559, learning_rate: 2.628235294117647e-05, global_step: 3160, interval_runtime: 1.2793, interval_samples_per_second: 6.253, interval_steps_per_second: 7.817, epoch: 12.3922[0m
[32m[2022-08-31 17:56:31,900] [    INFO][0m - loss: 9.224e-05, learning_rate: 2.627058823529412e-05, global_step: 3170, interval_runtime: 1.2799, interval_samples_per_second: 6.25, interval_steps_per_second: 7.813, epoch: 12.4314[0m
[32m[2022-08-31 17:56:33,187] [    INFO][0m - loss: 7.21e-05, learning_rate: 2.6258823529411763e-05, global_step: 3180, interval_runtime: 1.2934, interval_samples_per_second: 6.185, interval_steps_per_second: 7.731, epoch: 12.4706[0m
[32m[2022-08-31 17:56:34,475] [    INFO][0m - loss: 6.627e-05, learning_rate: 2.6247058823529413e-05, global_step: 3190, interval_runtime: 1.2876, interval_samples_per_second: 6.213, interval_steps_per_second: 7.766, epoch: 12.5098[0m
[32m[2022-08-31 17:56:35,759] [    INFO][0m - loss: 4.569e-05, learning_rate: 2.623529411764706e-05, global_step: 3200, interval_runtime: 1.2844, interval_samples_per_second: 6.229, interval_steps_per_second: 7.786, epoch: 12.549[0m
[32m[2022-08-31 17:56:35,759] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 17:56:35,759] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 17:56:35,759] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:56:35,760] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:56:35,760] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 17:56:49,829] [    INFO][0m - eval_loss: 2.680410861968994, eval_accuracy: 0.6204061895551257, eval_runtime: 14.0688, eval_samples_per_second: 146.992, eval_steps_per_second: 4.62, epoch: 12.549[0m
[32m[2022-08-31 17:56:49,829] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-3200[0m
[32m[2022-08-31 17:56:49,830] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 17:56:50,965] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-3200/tokenizer_config.json[0m
[32m[2022-08-31 17:56:50,965] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-3200/special_tokens_map.json[0m
[32m[2022-08-31 17:56:54,953] [    INFO][0m - loss: 9.833e-05, learning_rate: 2.6223529411764708e-05, global_step: 3210, interval_runtime: 19.1943, interval_samples_per_second: 0.417, interval_steps_per_second: 0.521, epoch: 12.5882[0m
[32m[2022-08-31 17:56:56,234] [    INFO][0m - loss: 6.034e-05, learning_rate: 2.6211764705882354e-05, global_step: 3220, interval_runtime: 1.2805, interval_samples_per_second: 6.248, interval_steps_per_second: 7.809, epoch: 12.6275[0m
[32m[2022-08-31 17:56:57,511] [    INFO][0m - loss: 6.04e-05, learning_rate: 2.62e-05, global_step: 3230, interval_runtime: 1.2776, interval_samples_per_second: 6.262, interval_steps_per_second: 7.827, epoch: 12.6667[0m
[32m[2022-08-31 17:56:58,804] [    INFO][0m - loss: 0.00015737, learning_rate: 2.6188235294117646e-05, global_step: 3240, interval_runtime: 1.293, interval_samples_per_second: 6.187, interval_steps_per_second: 7.734, epoch: 12.7059[0m
[32m[2022-08-31 17:57:00,101] [    INFO][0m - loss: 0.0001267, learning_rate: 2.6176470588235295e-05, global_step: 3250, interval_runtime: 1.2969, interval_samples_per_second: 6.168, interval_steps_per_second: 7.711, epoch: 12.7451[0m
[32m[2022-08-31 17:57:01,377] [    INFO][0m - loss: 9.087e-05, learning_rate: 2.616470588235294e-05, global_step: 3260, interval_runtime: 1.2762, interval_samples_per_second: 6.269, interval_steps_per_second: 7.836, epoch: 12.7843[0m
[32m[2022-08-31 17:57:02,670] [    INFO][0m - loss: 9.423e-05, learning_rate: 2.6152941176470587e-05, global_step: 3270, interval_runtime: 1.2923, interval_samples_per_second: 6.191, interval_steps_per_second: 7.738, epoch: 12.8235[0m
[32m[2022-08-31 17:57:03,946] [    INFO][0m - loss: 5.248e-05, learning_rate: 2.6141176470588237e-05, global_step: 3280, interval_runtime: 1.2762, interval_samples_per_second: 6.269, interval_steps_per_second: 7.836, epoch: 12.8627[0m
[32m[2022-08-31 17:57:05,226] [    INFO][0m - loss: 0.00013484, learning_rate: 2.6129411764705883e-05, global_step: 3290, interval_runtime: 1.2804, interval_samples_per_second: 6.248, interval_steps_per_second: 7.81, epoch: 12.902[0m
[32m[2022-08-31 17:57:06,523] [    INFO][0m - loss: 6.598e-05, learning_rate: 2.6117647058823532e-05, global_step: 3300, interval_runtime: 1.2964, interval_samples_per_second: 6.171, interval_steps_per_second: 7.714, epoch: 12.9412[0m
[32m[2022-08-31 17:57:06,524] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 17:57:06,524] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 17:57:06,524] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:57:06,524] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:57:06,524] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 17:57:20,663] [    INFO][0m - eval_loss: 2.687851905822754, eval_accuracy: 0.6194390715667312, eval_runtime: 14.1381, eval_samples_per_second: 146.271, eval_steps_per_second: 4.597, epoch: 12.9412[0m
[32m[2022-08-31 17:57:20,663] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-3300[0m
[32m[2022-08-31 17:57:20,663] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 17:57:21,936] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-3300/tokenizer_config.json[0m
[32m[2022-08-31 17:57:21,936] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-3300/special_tokens_map.json[0m
[32m[2022-08-31 17:57:25,925] [    INFO][0m - loss: 6.529e-05, learning_rate: 2.6105882352941178e-05, global_step: 3310, interval_runtime: 19.4023, interval_samples_per_second: 0.412, interval_steps_per_second: 0.515, epoch: 12.9804[0m
[32m[2022-08-31 17:57:27,229] [    INFO][0m - loss: 5.702e-05, learning_rate: 2.6094117647058824e-05, global_step: 3320, interval_runtime: 1.3045, interval_samples_per_second: 6.133, interval_steps_per_second: 7.666, epoch: 13.0196[0m
[32m[2022-08-31 17:57:28,512] [    INFO][0m - loss: 7.131e-05, learning_rate: 2.608235294117647e-05, global_step: 3330, interval_runtime: 1.2819, interval_samples_per_second: 6.241, interval_steps_per_second: 7.801, epoch: 13.0588[0m
[32m[2022-08-31 17:57:29,792] [    INFO][0m - loss: 3.935e-05, learning_rate: 2.607058823529412e-05, global_step: 3340, interval_runtime: 1.2809, interval_samples_per_second: 6.245, interval_steps_per_second: 7.807, epoch: 13.098[0m
[32m[2022-08-31 17:57:31,070] [    INFO][0m - loss: 0.00011101, learning_rate: 2.6058823529411765e-05, global_step: 3350, interval_runtime: 1.2773, interval_samples_per_second: 6.263, interval_steps_per_second: 7.829, epoch: 13.1373[0m
[32m[2022-08-31 17:57:32,362] [    INFO][0m - loss: 4.532e-05, learning_rate: 2.6047058823529414e-05, global_step: 3360, interval_runtime: 1.2928, interval_samples_per_second: 6.188, interval_steps_per_second: 7.735, epoch: 13.1765[0m
[32m[2022-08-31 17:57:33,640] [    INFO][0m - loss: 7.179e-05, learning_rate: 2.6035294117647057e-05, global_step: 3370, interval_runtime: 1.2773, interval_samples_per_second: 6.263, interval_steps_per_second: 7.829, epoch: 13.2157[0m
[32m[2022-08-31 17:57:34,936] [    INFO][0m - loss: 0.0001842, learning_rate: 2.6023529411764706e-05, global_step: 3380, interval_runtime: 1.2962, interval_samples_per_second: 6.172, interval_steps_per_second: 7.715, epoch: 13.2549[0m
[32m[2022-08-31 17:57:36,223] [    INFO][0m - loss: 0.00011067, learning_rate: 2.6011764705882352e-05, global_step: 3390, interval_runtime: 1.2865, interval_samples_per_second: 6.219, interval_steps_per_second: 7.773, epoch: 13.2941[0m
[32m[2022-08-31 17:57:37,500] [    INFO][0m - loss: 6.427e-05, learning_rate: 2.6000000000000002e-05, global_step: 3400, interval_runtime: 1.2776, interval_samples_per_second: 6.262, interval_steps_per_second: 7.827, epoch: 13.3333[0m
[32m[2022-08-31 17:57:37,500] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 17:57:37,501] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 17:57:37,501] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:57:37,501] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:57:37,501] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 17:57:51,540] [    INFO][0m - eval_loss: 2.7246079444885254, eval_accuracy: 0.6170212765957447, eval_runtime: 14.0389, eval_samples_per_second: 147.305, eval_steps_per_second: 4.63, epoch: 13.3333[0m
[32m[2022-08-31 17:57:51,541] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-3400[0m
[32m[2022-08-31 17:57:51,541] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 17:57:52,780] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-3400/tokenizer_config.json[0m
[32m[2022-08-31 17:57:52,780] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-3400/special_tokens_map.json[0m
[32m[2022-08-31 17:57:56,775] [    INFO][0m - loss: 6.107e-05, learning_rate: 2.5988235294117648e-05, global_step: 3410, interval_runtime: 19.2751, interval_samples_per_second: 0.415, interval_steps_per_second: 0.519, epoch: 13.3725[0m
[32m[2022-08-31 17:57:58,055] [    INFO][0m - loss: 5.803e-05, learning_rate: 2.5976470588235294e-05, global_step: 3420, interval_runtime: 1.28, interval_samples_per_second: 6.25, interval_steps_per_second: 7.813, epoch: 13.4118[0m
[32m[2022-08-31 17:57:59,329] [    INFO][0m - loss: 7.476e-05, learning_rate: 2.5964705882352943e-05, global_step: 3430, interval_runtime: 1.2736, interval_samples_per_second: 6.281, interval_steps_per_second: 7.852, epoch: 13.451[0m
[32m[2022-08-31 17:58:00,613] [    INFO][0m - loss: 3.475e-05, learning_rate: 2.595294117647059e-05, global_step: 3440, interval_runtime: 1.2844, interval_samples_per_second: 6.229, interval_steps_per_second: 7.786, epoch: 13.4902[0m
[32m[2022-08-31 17:58:01,889] [    INFO][0m - loss: 4.993e-05, learning_rate: 2.594117647058824e-05, global_step: 3450, interval_runtime: 1.2763, interval_samples_per_second: 6.268, interval_steps_per_second: 7.835, epoch: 13.5294[0m
[32m[2022-08-31 17:58:03,165] [    INFO][0m - loss: 8.62e-05, learning_rate: 2.592941176470588e-05, global_step: 3460, interval_runtime: 1.2753, interval_samples_per_second: 6.273, interval_steps_per_second: 7.841, epoch: 13.5686[0m
[32m[2022-08-31 17:58:04,437] [    INFO][0m - loss: 7.676e-05, learning_rate: 2.591764705882353e-05, global_step: 3470, interval_runtime: 1.2723, interval_samples_per_second: 6.288, interval_steps_per_second: 7.86, epoch: 13.6078[0m
[32m[2022-08-31 17:58:05,715] [    INFO][0m - loss: 6.427e-05, learning_rate: 2.5905882352941176e-05, global_step: 3480, interval_runtime: 1.2781, interval_samples_per_second: 6.259, interval_steps_per_second: 7.824, epoch: 13.6471[0m
[32m[2022-08-31 17:58:06,993] [    INFO][0m - loss: 4.499e-05, learning_rate: 2.5894117647058826e-05, global_step: 3490, interval_runtime: 1.2776, interval_samples_per_second: 6.262, interval_steps_per_second: 7.827, epoch: 13.6863[0m
[32m[2022-08-31 17:58:08,270] [    INFO][0m - loss: 5.794e-05, learning_rate: 2.5882352941176472e-05, global_step: 3500, interval_runtime: 1.277, interval_samples_per_second: 6.265, interval_steps_per_second: 7.831, epoch: 13.7255[0m
[32m[2022-08-31 17:58:08,270] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 17:58:08,271] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 17:58:08,271] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:58:08,271] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:58:08,271] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 17:58:22,223] [    INFO][0m - eval_loss: 2.732248067855835, eval_accuracy: 0.6165377176015474, eval_runtime: 13.9521, eval_samples_per_second: 148.221, eval_steps_per_second: 4.659, epoch: 13.7255[0m
[32m[2022-08-31 17:58:22,224] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-3500[0m
[32m[2022-08-31 17:58:22,224] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 17:58:23,405] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-3500/tokenizer_config.json[0m
[32m[2022-08-31 17:58:23,405] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-3500/special_tokens_map.json[0m
[32m[2022-08-31 17:58:28,057] [    INFO][0m - loss: 5.261e-05, learning_rate: 2.5870588235294118e-05, global_step: 3510, interval_runtime: 19.0457, interval_samples_per_second: 0.42, interval_steps_per_second: 0.525, epoch: 13.7647[0m
[32m[2022-08-31 17:58:29,330] [    INFO][0m - loss: 7.091e-05, learning_rate: 2.5858823529411764e-05, global_step: 3520, interval_runtime: 2.0149, interval_samples_per_second: 3.971, interval_steps_per_second: 4.963, epoch: 13.8039[0m
[32m[2022-08-31 17:58:30,634] [    INFO][0m - loss: 5.679e-05, learning_rate: 2.5847058823529413e-05, global_step: 3530, interval_runtime: 1.3034, interval_samples_per_second: 6.138, interval_steps_per_second: 7.672, epoch: 13.8431[0m
[32m[2022-08-31 17:58:31,930] [    INFO][0m - loss: 6.623e-05, learning_rate: 2.583529411764706e-05, global_step: 3540, interval_runtime: 1.2966, interval_samples_per_second: 6.17, interval_steps_per_second: 7.713, epoch: 13.8824[0m
[32m[2022-08-31 17:58:33,212] [    INFO][0m - loss: 3.832e-05, learning_rate: 2.582352941176471e-05, global_step: 3550, interval_runtime: 1.2809, interval_samples_per_second: 6.245, interval_steps_per_second: 7.807, epoch: 13.9216[0m
[32m[2022-08-31 17:58:34,497] [    INFO][0m - loss: 6.598e-05, learning_rate: 2.581176470588235e-05, global_step: 3560, interval_runtime: 1.2852, interval_samples_per_second: 6.225, interval_steps_per_second: 7.781, epoch: 13.9608[0m
[32m[2022-08-31 17:58:35,708] [    INFO][0m - loss: 5.74e-05, learning_rate: 2.58e-05, global_step: 3570, interval_runtime: 1.2115, interval_samples_per_second: 6.603, interval_steps_per_second: 8.254, epoch: 14.0[0m
[32m[2022-08-31 17:58:37,073] [    INFO][0m - loss: 3.521e-05, learning_rate: 2.578823529411765e-05, global_step: 3580, interval_runtime: 1.3654, interval_samples_per_second: 5.859, interval_steps_per_second: 7.324, epoch: 14.0392[0m
[32m[2022-08-31 17:58:38,358] [    INFO][0m - loss: 4.108e-05, learning_rate: 2.5776470588235296e-05, global_step: 3590, interval_runtime: 1.285, interval_samples_per_second: 6.226, interval_steps_per_second: 7.782, epoch: 14.0784[0m
[32m[2022-08-31 17:58:39,636] [    INFO][0m - loss: 4.639e-05, learning_rate: 2.576470588235294e-05, global_step: 3600, interval_runtime: 1.2776, interval_samples_per_second: 6.262, interval_steps_per_second: 7.827, epoch: 14.1176[0m
[32m[2022-08-31 17:58:39,637] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 17:58:39,637] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 17:58:39,637] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:58:39,637] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:58:39,637] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 17:58:53,790] [    INFO][0m - eval_loss: 2.734079360961914, eval_accuracy: 0.6194390715667312, eval_runtime: 14.1522, eval_samples_per_second: 146.126, eval_steps_per_second: 4.593, epoch: 14.1176[0m
[32m[2022-08-31 17:58:53,790] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-3600[0m
[32m[2022-08-31 17:58:53,791] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 17:58:55,011] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-3600/tokenizer_config.json[0m
[32m[2022-08-31 17:58:55,012] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-3600/special_tokens_map.json[0m
[32m[2022-08-31 17:58:58,983] [    INFO][0m - loss: 7.572e-05, learning_rate: 2.5752941176470588e-05, global_step: 3610, interval_runtime: 19.3461, interval_samples_per_second: 0.414, interval_steps_per_second: 0.517, epoch: 14.1569[0m
[32m[2022-08-31 17:59:00,278] [    INFO][0m - loss: 5.841e-05, learning_rate: 2.5741176470588237e-05, global_step: 3620, interval_runtime: 1.2962, interval_samples_per_second: 6.172, interval_steps_per_second: 7.715, epoch: 14.1961[0m
[32m[2022-08-31 17:59:01,560] [    INFO][0m - loss: 5.681e-05, learning_rate: 2.5729411764705883e-05, global_step: 3630, interval_runtime: 1.2818, interval_samples_per_second: 6.241, interval_steps_per_second: 7.802, epoch: 14.2353[0m
[32m[2022-08-31 17:59:02,837] [    INFO][0m - loss: 6.621e-05, learning_rate: 2.5717647058823532e-05, global_step: 3640, interval_runtime: 1.2771, interval_samples_per_second: 6.264, interval_steps_per_second: 7.83, epoch: 14.2745[0m
[32m[2022-08-31 17:59:04,122] [    INFO][0m - loss: 4.79e-05, learning_rate: 2.5705882352941175e-05, global_step: 3650, interval_runtime: 1.2845, interval_samples_per_second: 6.228, interval_steps_per_second: 7.785, epoch: 14.3137[0m
[32m[2022-08-31 17:59:05,405] [    INFO][0m - loss: 7.613e-05, learning_rate: 2.5694117647058824e-05, global_step: 3660, interval_runtime: 1.2827, interval_samples_per_second: 6.237, interval_steps_per_second: 7.796, epoch: 14.3529[0m
[32m[2022-08-31 17:59:06,686] [    INFO][0m - loss: 4.406e-05, learning_rate: 2.568235294117647e-05, global_step: 3670, interval_runtime: 1.2814, interval_samples_per_second: 6.243, interval_steps_per_second: 7.804, epoch: 14.3922[0m
[32m[2022-08-31 17:59:07,965] [    INFO][0m - loss: 4.367e-05, learning_rate: 2.567058823529412e-05, global_step: 3680, interval_runtime: 1.2791, interval_samples_per_second: 6.254, interval_steps_per_second: 7.818, epoch: 14.4314[0m
[32m[2022-08-31 17:59:09,255] [    INFO][0m - loss: 4.572e-05, learning_rate: 2.5658823529411766e-05, global_step: 3690, interval_runtime: 1.2901, interval_samples_per_second: 6.201, interval_steps_per_second: 7.751, epoch: 14.4706[0m
[32m[2022-08-31 17:59:10,538] [    INFO][0m - loss: 3.818e-05, learning_rate: 2.564705882352941e-05, global_step: 3700, interval_runtime: 1.2829, interval_samples_per_second: 6.236, interval_steps_per_second: 7.795, epoch: 14.5098[0m
[32m[2022-08-31 17:59:10,539] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 17:59:10,539] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 17:59:10,539] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:59:10,539] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:59:10,539] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 17:59:24,510] [    INFO][0m - eval_loss: 2.734714984893799, eval_accuracy: 0.6213733075435203, eval_runtime: 13.9704, eval_samples_per_second: 148.027, eval_steps_per_second: 4.653, epoch: 14.5098[0m
[32m[2022-08-31 17:59:24,511] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-3700[0m
[32m[2022-08-31 17:59:24,511] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 17:59:25,972] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-3700/tokenizer_config.json[0m
[32m[2022-08-31 17:59:25,972] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-3700/special_tokens_map.json[0m
[32m[2022-08-31 17:59:30,261] [    INFO][0m - loss: 4.486e-05, learning_rate: 2.5635294117647058e-05, global_step: 3710, interval_runtime: 19.7222, interval_samples_per_second: 0.406, interval_steps_per_second: 0.507, epoch: 14.549[0m
[32m[2022-08-31 17:59:31,542] [    INFO][0m - loss: 6.585e-05, learning_rate: 2.5623529411764707e-05, global_step: 3720, interval_runtime: 1.2821, interval_samples_per_second: 6.24, interval_steps_per_second: 7.799, epoch: 14.5882[0m
[32m[2022-08-31 17:59:32,847] [    INFO][0m - loss: 4.339e-05, learning_rate: 2.5611764705882353e-05, global_step: 3730, interval_runtime: 1.3045, interval_samples_per_second: 6.133, interval_steps_per_second: 7.666, epoch: 14.6275[0m
[32m[2022-08-31 17:59:34,132] [    INFO][0m - loss: 3.077e-05, learning_rate: 2.5600000000000002e-05, global_step: 3740, interval_runtime: 1.2848, interval_samples_per_second: 6.227, interval_steps_per_second: 7.784, epoch: 14.6667[0m
[32m[2022-08-31 17:59:35,416] [    INFO][0m - loss: 7.024e-05, learning_rate: 2.5588235294117648e-05, global_step: 3750, interval_runtime: 1.2839, interval_samples_per_second: 6.231, interval_steps_per_second: 7.789, epoch: 14.7059[0m
[32m[2022-08-31 17:59:36,713] [    INFO][0m - loss: 6.717e-05, learning_rate: 2.5576470588235294e-05, global_step: 3760, interval_runtime: 1.2973, interval_samples_per_second: 6.167, interval_steps_per_second: 7.708, epoch: 14.7451[0m
[32m[2022-08-31 17:59:37,989] [    INFO][0m - loss: 4.544e-05, learning_rate: 2.5564705882352944e-05, global_step: 3770, interval_runtime: 1.2765, interval_samples_per_second: 6.267, interval_steps_per_second: 7.834, epoch: 14.7843[0m
[32m[2022-08-31 17:59:39,274] [    INFO][0m - loss: 5.86e-05, learning_rate: 2.555294117647059e-05, global_step: 3780, interval_runtime: 1.2843, interval_samples_per_second: 6.229, interval_steps_per_second: 7.786, epoch: 14.8235[0m
[32m[2022-08-31 17:59:40,566] [    INFO][0m - loss: 7.18e-05, learning_rate: 2.5541176470588235e-05, global_step: 3790, interval_runtime: 1.2924, interval_samples_per_second: 6.19, interval_steps_per_second: 7.737, epoch: 14.8627[0m
[32m[2022-08-31 17:59:41,844] [    INFO][0m - loss: 4.434e-05, learning_rate: 2.552941176470588e-05, global_step: 3800, interval_runtime: 1.2783, interval_samples_per_second: 6.258, interval_steps_per_second: 7.823, epoch: 14.902[0m
[32m[2022-08-31 17:59:41,845] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 17:59:41,845] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 17:59:41,845] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 17:59:41,845] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 17:59:41,845] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 17:59:55,748] [    INFO][0m - eval_loss: 2.7408061027526855, eval_accuracy: 0.6204061895551257, eval_runtime: 13.9023, eval_samples_per_second: 148.753, eval_steps_per_second: 4.675, epoch: 14.902[0m
[32m[2022-08-31 17:59:55,749] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-3800[0m
[32m[2022-08-31 17:59:55,749] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 17:59:56,632] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-3800/tokenizer_config.json[0m
[32m[2022-08-31 17:59:56,632] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-3800/special_tokens_map.json[0m
[32m[2022-08-31 18:00:00,006] [    INFO][0m - loss: 5.28e-05, learning_rate: 2.551764705882353e-05, global_step: 3810, interval_runtime: 18.161, interval_samples_per_second: 0.441, interval_steps_per_second: 0.551, epoch: 14.9412[0m
[32m[2022-08-31 18:00:01,281] [    INFO][0m - loss: 5.678e-05, learning_rate: 2.5505882352941177e-05, global_step: 3820, interval_runtime: 1.275, interval_samples_per_second: 6.274, interval_steps_per_second: 7.843, epoch: 14.9804[0m
[32m[2022-08-31 18:00:02,575] [    INFO][0m - loss: 4.373e-05, learning_rate: 2.5494117647058826e-05, global_step: 3830, interval_runtime: 1.2947, interval_samples_per_second: 6.179, interval_steps_per_second: 7.724, epoch: 15.0196[0m
[32m[2022-08-31 18:00:03,859] [    INFO][0m - loss: 7.465e-05, learning_rate: 2.548235294117647e-05, global_step: 3840, interval_runtime: 1.2836, interval_samples_per_second: 6.232, interval_steps_per_second: 7.79, epoch: 15.0588[0m
[32m[2022-08-31 18:00:05,149] [    INFO][0m - loss: 3.851e-05, learning_rate: 2.5470588235294118e-05, global_step: 3850, interval_runtime: 1.2899, interval_samples_per_second: 6.202, interval_steps_per_second: 7.753, epoch: 15.098[0m
[32m[2022-08-31 18:00:06,438] [    INFO][0m - loss: 3.367e-05, learning_rate: 2.5458823529411764e-05, global_step: 3860, interval_runtime: 1.2888, interval_samples_per_second: 6.207, interval_steps_per_second: 7.759, epoch: 15.1373[0m
[32m[2022-08-31 18:00:07,725] [    INFO][0m - loss: 4.063e-05, learning_rate: 2.5447058823529413e-05, global_step: 3870, interval_runtime: 1.2874, interval_samples_per_second: 6.214, interval_steps_per_second: 7.768, epoch: 15.1765[0m
[32m[2022-08-31 18:00:09,013] [    INFO][0m - loss: 3.81e-05, learning_rate: 2.543529411764706e-05, global_step: 3880, interval_runtime: 1.2883, interval_samples_per_second: 6.21, interval_steps_per_second: 7.762, epoch: 15.2157[0m
[32m[2022-08-31 18:00:10,303] [    INFO][0m - loss: 3.014e-05, learning_rate: 2.5423529411764705e-05, global_step: 3890, interval_runtime: 1.2894, interval_samples_per_second: 6.204, interval_steps_per_second: 7.756, epoch: 15.2549[0m
[32m[2022-08-31 18:00:11,863] [    INFO][0m - loss: 7.467e-05, learning_rate: 2.5411764705882355e-05, global_step: 3900, interval_runtime: 1.283, interval_samples_per_second: 6.236, interval_steps_per_second: 7.794, epoch: 15.2941[0m
[32m[2022-08-31 18:00:11,864] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:00:11,864] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 18:00:11,864] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:00:11,864] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:00:11,864] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 18:00:25,773] [    INFO][0m - eval_loss: 2.7468247413635254, eval_accuracy: 0.6199226305609284, eval_runtime: 13.9087, eval_samples_per_second: 148.684, eval_steps_per_second: 4.673, epoch: 15.2941[0m
[32m[2022-08-31 18:00:25,774] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-3900[0m
[32m[2022-08-31 18:00:25,774] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:00:26,709] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-3900/tokenizer_config.json[0m
[32m[2022-08-31 18:00:26,709] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-3900/special_tokens_map.json[0m
[32m[2022-08-31 18:00:30,378] [    INFO][0m - loss: 5.862e-05, learning_rate: 2.54e-05, global_step: 3910, interval_runtime: 18.7924, interval_samples_per_second: 0.426, interval_steps_per_second: 0.532, epoch: 15.3333[0m
[32m[2022-08-31 18:00:31,736] [    INFO][0m - loss: 6.447e-05, learning_rate: 2.538823529411765e-05, global_step: 3920, interval_runtime: 1.2798, interval_samples_per_second: 6.251, interval_steps_per_second: 7.814, epoch: 15.3725[0m
[32m[2022-08-31 18:00:33,019] [    INFO][0m - loss: 4.04e-05, learning_rate: 2.5376470588235293e-05, global_step: 3930, interval_runtime: 1.3613, interval_samples_per_second: 5.877, interval_steps_per_second: 7.346, epoch: 15.4118[0m
[32m[2022-08-31 18:00:34,302] [    INFO][0m - loss: 3.4e-05, learning_rate: 2.5364705882352942e-05, global_step: 3940, interval_runtime: 1.2826, interval_samples_per_second: 6.237, interval_steps_per_second: 7.797, epoch: 15.451[0m
[32m[2022-08-31 18:00:35,581] [    INFO][0m - loss: 6.444e-05, learning_rate: 2.5352941176470588e-05, global_step: 3950, interval_runtime: 1.2786, interval_samples_per_second: 6.257, interval_steps_per_second: 7.821, epoch: 15.4902[0m
[32m[2022-08-31 18:00:38,841] [    INFO][0m - loss: 5.369e-05, learning_rate: 2.5341176470588237e-05, global_step: 3960, interval_runtime: 1.2924, interval_samples_per_second: 6.19, interval_steps_per_second: 7.738, epoch: 15.5294[0m
[32m[2022-08-31 18:00:40,112] [    INFO][0m - loss: 3.811e-05, learning_rate: 2.5329411764705883e-05, global_step: 3970, interval_runtime: 3.2396, interval_samples_per_second: 2.469, interval_steps_per_second: 3.087, epoch: 15.5686[0m
[32m[2022-08-31 18:00:41,392] [    INFO][0m - loss: 4.863e-05, learning_rate: 2.531764705882353e-05, global_step: 3980, interval_runtime: 1.2799, interval_samples_per_second: 6.25, interval_steps_per_second: 7.813, epoch: 15.6078[0m
[32m[2022-08-31 18:00:42,674] [    INFO][0m - loss: 4.827e-05, learning_rate: 2.5305882352941175e-05, global_step: 3990, interval_runtime: 1.2821, interval_samples_per_second: 6.24, interval_steps_per_second: 7.8, epoch: 15.6471[0m
[32m[2022-08-31 18:00:43,958] [    INFO][0m - loss: 6.596e-05, learning_rate: 2.5294117647058825e-05, global_step: 4000, interval_runtime: 1.2835, interval_samples_per_second: 6.233, interval_steps_per_second: 7.791, epoch: 15.6863[0m
[32m[2022-08-31 18:00:43,958] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:00:43,958] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 18:00:43,959] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:00:43,959] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:00:43,959] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 18:00:57,919] [    INFO][0m - eval_loss: 2.7666146755218506, eval_accuracy: 0.6189555125725339, eval_runtime: 13.9599, eval_samples_per_second: 148.139, eval_steps_per_second: 4.656, epoch: 15.6863[0m
[32m[2022-08-31 18:00:57,920] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-4000[0m
[32m[2022-08-31 18:00:57,920] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:00:59,084] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-4000/tokenizer_config.json[0m
[32m[2022-08-31 18:00:59,085] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-4000/special_tokens_map.json[0m
[32m[2022-08-31 18:01:02,651] [    INFO][0m - loss: 9.078e-05, learning_rate: 2.528235294117647e-05, global_step: 4010, interval_runtime: 18.6921, interval_samples_per_second: 0.428, interval_steps_per_second: 0.535, epoch: 15.7255[0m
[32m[2022-08-31 18:01:03,946] [    INFO][0m - loss: 4.286e-05, learning_rate: 2.527058823529412e-05, global_step: 4020, interval_runtime: 1.2957, interval_samples_per_second: 6.174, interval_steps_per_second: 7.718, epoch: 15.7647[0m
[32m[2022-08-31 18:01:05,234] [    INFO][0m - loss: 4.046e-05, learning_rate: 2.5258823529411763e-05, global_step: 4030, interval_runtime: 1.2884, interval_samples_per_second: 6.209, interval_steps_per_second: 7.761, epoch: 15.8039[0m
[32m[2022-08-31 18:01:06,519] [    INFO][0m - loss: 3.865e-05, learning_rate: 2.5247058823529412e-05, global_step: 4040, interval_runtime: 1.2846, interval_samples_per_second: 6.227, interval_steps_per_second: 7.784, epoch: 15.8431[0m
[32m[2022-08-31 18:01:07,813] [    INFO][0m - loss: 4.555e-05, learning_rate: 2.523529411764706e-05, global_step: 4050, interval_runtime: 1.2943, interval_samples_per_second: 6.181, interval_steps_per_second: 7.726, epoch: 15.8824[0m
[32m[2022-08-31 18:01:09,092] [    INFO][0m - loss: 4.16e-05, learning_rate: 2.5223529411764707e-05, global_step: 4060, interval_runtime: 1.2791, interval_samples_per_second: 6.254, interval_steps_per_second: 7.818, epoch: 15.9216[0m
[32m[2022-08-31 18:01:10,383] [    INFO][0m - loss: 3.557e-05, learning_rate: 2.5211764705882353e-05, global_step: 4070, interval_runtime: 1.2909, interval_samples_per_second: 6.197, interval_steps_per_second: 7.747, epoch: 15.9608[0m
[32m[2022-08-31 18:01:11,605] [    INFO][0m - loss: 4.251e-05, learning_rate: 2.52e-05, global_step: 4080, interval_runtime: 1.2217, interval_samples_per_second: 6.548, interval_steps_per_second: 8.185, epoch: 16.0[0m
[32m[2022-08-31 18:01:12,987] [    INFO][0m - loss: 3.89e-05, learning_rate: 2.518823529411765e-05, global_step: 4090, interval_runtime: 1.382, interval_samples_per_second: 5.789, interval_steps_per_second: 7.236, epoch: 16.0392[0m
[32m[2022-08-31 18:01:14,274] [    INFO][0m - loss: 4.846e-05, learning_rate: 2.5176470588235295e-05, global_step: 4100, interval_runtime: 1.2874, interval_samples_per_second: 6.214, interval_steps_per_second: 7.768, epoch: 16.0784[0m
[32m[2022-08-31 18:01:14,275] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:01:14,275] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 18:01:14,275] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:01:14,276] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:01:14,276] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 18:01:28,187] [    INFO][0m - eval_loss: 2.7978737354278564, eval_accuracy: 0.6179883945841392, eval_runtime: 13.9108, eval_samples_per_second: 148.661, eval_steps_per_second: 4.673, epoch: 16.0784[0m
[32m[2022-08-31 18:01:28,187] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-4100[0m
[32m[2022-08-31 18:01:28,188] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:01:29,237] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-4100/tokenizer_config.json[0m
[32m[2022-08-31 18:01:29,237] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-4100/special_tokens_map.json[0m
[32m[2022-08-31 18:01:32,647] [    INFO][0m - loss: 4.389e-05, learning_rate: 2.5164705882352944e-05, global_step: 4110, interval_runtime: 18.373, interval_samples_per_second: 0.435, interval_steps_per_second: 0.544, epoch: 16.1176[0m
[32m[2022-08-31 18:01:33,924] [    INFO][0m - loss: 3.929e-05, learning_rate: 2.5152941176470587e-05, global_step: 4120, interval_runtime: 1.2763, interval_samples_per_second: 6.268, interval_steps_per_second: 7.835, epoch: 16.1569[0m
[32m[2022-08-31 18:01:35,207] [    INFO][0m - loss: 3.274e-05, learning_rate: 2.5141176470588236e-05, global_step: 4130, interval_runtime: 1.283, interval_samples_per_second: 6.236, interval_steps_per_second: 7.794, epoch: 16.1961[0m
[32m[2022-08-31 18:01:36,479] [    INFO][0m - loss: 6.58e-05, learning_rate: 2.5129411764705882e-05, global_step: 4140, interval_runtime: 1.2726, interval_samples_per_second: 6.286, interval_steps_per_second: 7.858, epoch: 16.2353[0m
[32m[2022-08-31 18:01:37,771] [    INFO][0m - loss: 4.935e-05, learning_rate: 2.511764705882353e-05, global_step: 4150, interval_runtime: 1.2916, interval_samples_per_second: 6.194, interval_steps_per_second: 7.742, epoch: 16.2745[0m
[32m[2022-08-31 18:01:39,045] [    INFO][0m - loss: 4.526e-05, learning_rate: 2.5105882352941177e-05, global_step: 4160, interval_runtime: 1.2746, interval_samples_per_second: 6.276, interval_steps_per_second: 7.845, epoch: 16.3137[0m
[32m[2022-08-31 18:01:40,327] [    INFO][0m - loss: 3.692e-05, learning_rate: 2.5094117647058823e-05, global_step: 4170, interval_runtime: 1.2817, interval_samples_per_second: 6.242, interval_steps_per_second: 7.802, epoch: 16.3529[0m
[32m[2022-08-31 18:01:41,610] [    INFO][0m - loss: 3.693e-05, learning_rate: 2.508235294117647e-05, global_step: 4180, interval_runtime: 1.2829, interval_samples_per_second: 6.236, interval_steps_per_second: 7.795, epoch: 16.3922[0m
[32m[2022-08-31 18:01:42,900] [    INFO][0m - loss: 3.624e-05, learning_rate: 2.507058823529412e-05, global_step: 4190, interval_runtime: 1.29, interval_samples_per_second: 6.202, interval_steps_per_second: 7.752, epoch: 16.4314[0m
[32m[2022-08-31 18:01:44,182] [    INFO][0m - loss: 2.921e-05, learning_rate: 2.5058823529411768e-05, global_step: 4200, interval_runtime: 1.2819, interval_samples_per_second: 6.241, interval_steps_per_second: 7.801, epoch: 16.4706[0m
[32m[2022-08-31 18:01:44,183] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:01:44,183] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 18:01:44,183] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:01:44,183] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:01:44,183] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 18:01:58,147] [    INFO][0m - eval_loss: 2.806328535079956, eval_accuracy: 0.6179883945841392, eval_runtime: 13.9633, eval_samples_per_second: 148.102, eval_steps_per_second: 4.655, epoch: 16.4706[0m
[32m[2022-08-31 18:01:58,148] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-4200[0m
[32m[2022-08-31 18:01:58,148] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:01:59,169] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-4200/tokenizer_config.json[0m
[32m[2022-08-31 18:01:59,170] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-4200/special_tokens_map.json[0m
[32m[2022-08-31 18:02:02,549] [    INFO][0m - loss: 4.534e-05, learning_rate: 2.5047058823529414e-05, global_step: 4210, interval_runtime: 18.3664, interval_samples_per_second: 0.436, interval_steps_per_second: 0.544, epoch: 16.5098[0m
[32m[2022-08-31 18:02:03,830] [    INFO][0m - loss: 4.385e-05, learning_rate: 2.503529411764706e-05, global_step: 4220, interval_runtime: 1.2818, interval_samples_per_second: 6.241, interval_steps_per_second: 7.801, epoch: 16.549[0m
[32m[2022-08-31 18:02:05,108] [    INFO][0m - loss: 4.834e-05, learning_rate: 2.5023529411764706e-05, global_step: 4230, interval_runtime: 1.2773, interval_samples_per_second: 6.263, interval_steps_per_second: 7.829, epoch: 16.5882[0m
[32m[2022-08-31 18:02:06,385] [    INFO][0m - loss: 4.024e-05, learning_rate: 2.5011764705882355e-05, global_step: 4240, interval_runtime: 1.2778, interval_samples_per_second: 6.261, interval_steps_per_second: 7.826, epoch: 16.6275[0m
[32m[2022-08-31 18:02:07,665] [    INFO][0m - loss: 2.507e-05, learning_rate: 2.5e-05, global_step: 4250, interval_runtime: 1.2791, interval_samples_per_second: 6.254, interval_steps_per_second: 7.818, epoch: 16.6667[0m
[32m[2022-08-31 18:02:08,953] [    INFO][0m - loss: 4.675e-05, learning_rate: 2.4988235294117647e-05, global_step: 4260, interval_runtime: 1.2885, interval_samples_per_second: 6.209, interval_steps_per_second: 7.761, epoch: 16.7059[0m
[32m[2022-08-31 18:02:10,238] [    INFO][0m - loss: 4.198e-05, learning_rate: 2.4976470588235293e-05, global_step: 4270, interval_runtime: 1.2851, interval_samples_per_second: 6.225, interval_steps_per_second: 7.782, epoch: 16.7451[0m
[32m[2022-08-31 18:02:11,523] [    INFO][0m - loss: 5.123e-05, learning_rate: 2.4964705882352943e-05, global_step: 4280, interval_runtime: 1.2844, interval_samples_per_second: 6.228, interval_steps_per_second: 7.785, epoch: 16.7843[0m
[32m[2022-08-31 18:02:12,817] [    INFO][0m - loss: 3.486e-05, learning_rate: 2.495294117647059e-05, global_step: 4290, interval_runtime: 1.2939, interval_samples_per_second: 6.183, interval_steps_per_second: 7.729, epoch: 16.8235[0m
[32m[2022-08-31 18:02:14,100] [    INFO][0m - loss: 7.603e-05, learning_rate: 2.4941176470588238e-05, global_step: 4300, interval_runtime: 1.2834, interval_samples_per_second: 6.234, interval_steps_per_second: 7.792, epoch: 16.8627[0m
[32m[2022-08-31 18:02:14,101] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:02:14,101] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 18:02:14,101] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:02:14,101] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:02:14,101] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 18:02:28,117] [    INFO][0m - eval_loss: 2.812574625015259, eval_accuracy: 0.6179883945841392, eval_runtime: 14.0151, eval_samples_per_second: 147.555, eval_steps_per_second: 4.638, epoch: 16.8627[0m
[32m[2022-08-31 18:02:28,117] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-4300[0m
[32m[2022-08-31 18:02:28,117] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:02:29,270] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-4300/tokenizer_config.json[0m
[32m[2022-08-31 18:02:29,271] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-4300/special_tokens_map.json[0m
[32m[2022-08-31 18:02:32,820] [    INFO][0m - loss: 5.765e-05, learning_rate: 2.492941176470588e-05, global_step: 4310, interval_runtime: 18.7201, interval_samples_per_second: 0.427, interval_steps_per_second: 0.534, epoch: 16.902[0m
[32m[2022-08-31 18:02:34,103] [    INFO][0m - loss: 3.669e-05, learning_rate: 2.491764705882353e-05, global_step: 4320, interval_runtime: 1.2826, interval_samples_per_second: 6.237, interval_steps_per_second: 7.796, epoch: 16.9412[0m
[32m[2022-08-31 18:02:35,381] [    INFO][0m - loss: 4.481e-05, learning_rate: 2.4905882352941176e-05, global_step: 4330, interval_runtime: 1.2776, interval_samples_per_second: 6.262, interval_steps_per_second: 7.827, epoch: 16.9804[0m
[32m[2022-08-31 18:02:36,681] [    INFO][0m - loss: 3.392e-05, learning_rate: 2.4894117647058825e-05, global_step: 4340, interval_runtime: 1.3003, interval_samples_per_second: 6.152, interval_steps_per_second: 7.69, epoch: 17.0196[0m
[32m[2022-08-31 18:02:37,966] [    INFO][0m - loss: 3.613e-05, learning_rate: 2.488235294117647e-05, global_step: 4350, interval_runtime: 1.285, interval_samples_per_second: 6.226, interval_steps_per_second: 7.782, epoch: 17.0588[0m
[32m[2022-08-31 18:02:39,246] [    INFO][0m - loss: 3.997e-05, learning_rate: 2.4870588235294117e-05, global_step: 4360, interval_runtime: 1.2804, interval_samples_per_second: 6.248, interval_steps_per_second: 7.81, epoch: 17.098[0m
[32m[2022-08-31 18:02:40,545] [    INFO][0m - loss: 3.465e-05, learning_rate: 2.4858823529411766e-05, global_step: 4370, interval_runtime: 1.2988, interval_samples_per_second: 6.16, interval_steps_per_second: 7.699, epoch: 17.1373[0m
[32m[2022-08-31 18:02:41,833] [    INFO][0m - loss: 3.04e-05, learning_rate: 2.4847058823529412e-05, global_step: 4380, interval_runtime: 1.2877, interval_samples_per_second: 6.213, interval_steps_per_second: 7.766, epoch: 17.1765[0m
[32m[2022-08-31 18:02:43,125] [    INFO][0m - loss: 6.243e-05, learning_rate: 2.4835294117647062e-05, global_step: 4390, interval_runtime: 1.2923, interval_samples_per_second: 6.19, interval_steps_per_second: 7.738, epoch: 17.2157[0m
[32m[2022-08-31 18:02:44,423] [    INFO][0m - loss: 3.262e-05, learning_rate: 2.4823529411764704e-05, global_step: 4400, interval_runtime: 1.298, interval_samples_per_second: 6.163, interval_steps_per_second: 7.704, epoch: 17.2549[0m
[32m[2022-08-31 18:02:44,424] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:02:44,424] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 18:02:44,424] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:02:44,424] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:02:44,424] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 18:02:58,363] [    INFO][0m - eval_loss: 2.845311403274536, eval_accuracy: 0.6184719535783365, eval_runtime: 13.9383, eval_samples_per_second: 148.368, eval_steps_per_second: 4.663, epoch: 17.2549[0m
[32m[2022-08-31 18:02:58,364] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-4400[0m
[32m[2022-08-31 18:02:58,364] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:02:59,395] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-4400/tokenizer_config.json[0m
[32m[2022-08-31 18:02:59,395] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-4400/special_tokens_map.json[0m
[32m[2022-08-31 18:03:03,322] [    INFO][0m - loss: 5.225e-05, learning_rate: 2.4811764705882354e-05, global_step: 4410, interval_runtime: 18.8987, interval_samples_per_second: 0.423, interval_steps_per_second: 0.529, epoch: 17.2941[0m
[32m[2022-08-31 18:03:04,605] [    INFO][0m - loss: 5.226e-05, learning_rate: 2.48e-05, global_step: 4420, interval_runtime: 1.2834, interval_samples_per_second: 6.233, interval_steps_per_second: 7.792, epoch: 17.3333[0m
[32m[2022-08-31 18:03:05,892] [    INFO][0m - loss: 2.947e-05, learning_rate: 2.478823529411765e-05, global_step: 4430, interval_runtime: 1.2864, interval_samples_per_second: 6.219, interval_steps_per_second: 7.774, epoch: 17.3725[0m
[32m[2022-08-31 18:03:07,176] [    INFO][0m - loss: 4.513e-05, learning_rate: 2.4776470588235295e-05, global_step: 4440, interval_runtime: 1.2841, interval_samples_per_second: 6.23, interval_steps_per_second: 7.787, epoch: 17.4118[0m
[32m[2022-08-31 18:03:08,459] [    INFO][0m - loss: 2.738e-05, learning_rate: 2.476470588235294e-05, global_step: 4450, interval_runtime: 1.2837, interval_samples_per_second: 6.232, interval_steps_per_second: 7.79, epoch: 17.451[0m
[32m[2022-08-31 18:03:09,742] [    INFO][0m - loss: 5.233e-05, learning_rate: 2.4752941176470587e-05, global_step: 4460, interval_runtime: 1.2824, interval_samples_per_second: 6.238, interval_steps_per_second: 7.798, epoch: 17.4902[0m
[32m[2022-08-31 18:03:11,030] [    INFO][0m - loss: 3.416e-05, learning_rate: 2.4741176470588236e-05, global_step: 4470, interval_runtime: 1.2882, interval_samples_per_second: 6.21, interval_steps_per_second: 7.763, epoch: 17.5294[0m
[32m[2022-08-31 18:03:12,323] [    INFO][0m - loss: 3.006e-05, learning_rate: 2.4729411764705882e-05, global_step: 4480, interval_runtime: 1.2896, interval_samples_per_second: 6.204, interval_steps_per_second: 7.754, epoch: 17.5686[0m
[32m[2022-08-31 18:03:13,615] [    INFO][0m - loss: 3.054e-05, learning_rate: 2.4717647058823532e-05, global_step: 4490, interval_runtime: 1.2955, interval_samples_per_second: 6.175, interval_steps_per_second: 7.719, epoch: 17.6078[0m
[32m[2022-08-31 18:03:14,901] [    INFO][0m - loss: 5.932e-05, learning_rate: 2.4705882352941174e-05, global_step: 4500, interval_runtime: 1.2856, interval_samples_per_second: 6.223, interval_steps_per_second: 7.779, epoch: 17.6471[0m
[32m[2022-08-31 18:03:14,901] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:03:14,901] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 18:03:14,902] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:03:14,902] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:03:14,902] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 18:03:28,889] [    INFO][0m - eval_loss: 2.8343417644500732, eval_accuracy: 0.6199226305609284, eval_runtime: 13.9874, eval_samples_per_second: 147.847, eval_steps_per_second: 4.647, epoch: 17.6471[0m
[32m[2022-08-31 18:03:28,890] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-4500[0m
[32m[2022-08-31 18:03:28,890] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:03:30,011] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-4500/tokenizer_config.json[0m
[32m[2022-08-31 18:03:30,012] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-4500/special_tokens_map.json[0m
[32m[2022-08-31 18:03:33,916] [    INFO][0m - loss: 2.683e-05, learning_rate: 2.4694117647058824e-05, global_step: 4510, interval_runtime: 19.0149, interval_samples_per_second: 0.421, interval_steps_per_second: 0.526, epoch: 17.6863[0m
[32m[2022-08-31 18:03:35,203] [    INFO][0m - loss: 4.382e-05, learning_rate: 2.4682352941176473e-05, global_step: 4520, interval_runtime: 1.2877, interval_samples_per_second: 6.213, interval_steps_per_second: 7.766, epoch: 17.7255[0m
[32m[2022-08-31 18:03:36,492] [    INFO][0m - loss: 5.52e-05, learning_rate: 2.467058823529412e-05, global_step: 4530, interval_runtime: 1.2891, interval_samples_per_second: 6.206, interval_steps_per_second: 7.757, epoch: 17.7647[0m
[32m[2022-08-31 18:03:37,776] [    INFO][0m - loss: 2.977e-05, learning_rate: 2.465882352941177e-05, global_step: 4540, interval_runtime: 1.2834, interval_samples_per_second: 6.234, interval_steps_per_second: 7.792, epoch: 17.8039[0m
[32m[2022-08-31 18:03:39,073] [    INFO][0m - loss: 2.691e-05, learning_rate: 2.464705882352941e-05, global_step: 4550, interval_runtime: 1.2966, interval_samples_per_second: 6.17, interval_steps_per_second: 7.712, epoch: 17.8431[0m
[32m[2022-08-31 18:03:40,362] [    INFO][0m - loss: 3.215e-05, learning_rate: 2.463529411764706e-05, global_step: 4560, interval_runtime: 1.2895, interval_samples_per_second: 6.204, interval_steps_per_second: 7.755, epoch: 17.8824[0m
[32m[2022-08-31 18:03:41,661] [    INFO][0m - loss: 3.04e-05, learning_rate: 2.4623529411764706e-05, global_step: 4570, interval_runtime: 1.2994, interval_samples_per_second: 6.157, interval_steps_per_second: 7.696, epoch: 17.9216[0m
[32m[2022-08-31 18:03:42,950] [    INFO][0m - loss: 2.695e-05, learning_rate: 2.4611764705882356e-05, global_step: 4580, interval_runtime: 1.2885, interval_samples_per_second: 6.209, interval_steps_per_second: 7.761, epoch: 17.9608[0m
[32m[2022-08-31 18:03:44,176] [    INFO][0m - loss: 2.929e-05, learning_rate: 2.4599999999999998e-05, global_step: 4590, interval_runtime: 1.2258, interval_samples_per_second: 6.526, interval_steps_per_second: 8.158, epoch: 18.0[0m
[32m[2022-08-31 18:03:45,544] [    INFO][0m - loss: 3.624e-05, learning_rate: 2.4588235294117648e-05, global_step: 4600, interval_runtime: 1.3687, interval_samples_per_second: 5.845, interval_steps_per_second: 7.306, epoch: 18.0392[0m
[32m[2022-08-31 18:03:45,545] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:03:45,545] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 18:03:45,545] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:03:45,545] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:03:45,545] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 18:03:59,640] [    INFO][0m - eval_loss: 2.8158886432647705, eval_accuracy: 0.6189555125725339, eval_runtime: 14.0946, eval_samples_per_second: 146.723, eval_steps_per_second: 4.612, epoch: 18.0392[0m
[32m[2022-08-31 18:03:59,641] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-4600[0m
[32m[2022-08-31 18:03:59,641] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:04:00,892] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-4600/tokenizer_config.json[0m
[32m[2022-08-31 18:04:00,892] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-4600/special_tokens_map.json[0m
[32m[2022-08-31 18:04:04,832] [    INFO][0m - loss: 3.049e-05, learning_rate: 2.4576470588235294e-05, global_step: 4610, interval_runtime: 19.288, interval_samples_per_second: 0.415, interval_steps_per_second: 0.518, epoch: 18.0784[0m
[32m[2022-08-31 18:04:06,116] [    INFO][0m - loss: 2.139e-05, learning_rate: 2.4564705882352943e-05, global_step: 4620, interval_runtime: 1.2832, interval_samples_per_second: 6.234, interval_steps_per_second: 7.793, epoch: 18.1176[0m
[32m[2022-08-31 18:04:07,408] [    INFO][0m - loss: 1.75e-05, learning_rate: 2.455294117647059e-05, global_step: 4630, interval_runtime: 1.2915, interval_samples_per_second: 6.194, interval_steps_per_second: 7.743, epoch: 18.1569[0m
[32m[2022-08-31 18:04:08,721] [    INFO][0m - loss: 5.104e-05, learning_rate: 2.4541176470588235e-05, global_step: 4640, interval_runtime: 1.3143, interval_samples_per_second: 6.087, interval_steps_per_second: 7.609, epoch: 18.1961[0m
[32m[2022-08-31 18:04:10,008] [    INFO][0m - loss: 1.872e-05, learning_rate: 2.452941176470588e-05, global_step: 4650, interval_runtime: 1.2867, interval_samples_per_second: 6.217, interval_steps_per_second: 7.772, epoch: 18.2353[0m
[32m[2022-08-31 18:04:11,295] [    INFO][0m - loss: 4.786e-05, learning_rate: 2.451764705882353e-05, global_step: 4660, interval_runtime: 1.287, interval_samples_per_second: 6.216, interval_steps_per_second: 7.77, epoch: 18.2745[0m
[32m[2022-08-31 18:04:12,584] [    INFO][0m - loss: 5.332e-05, learning_rate: 2.450588235294118e-05, global_step: 4670, interval_runtime: 1.2888, interval_samples_per_second: 6.207, interval_steps_per_second: 7.759, epoch: 18.3137[0m
[32m[2022-08-31 18:04:13,867] [    INFO][0m - loss: 3.141e-05, learning_rate: 2.4494117647058826e-05, global_step: 4680, interval_runtime: 1.2828, interval_samples_per_second: 6.237, interval_steps_per_second: 7.796, epoch: 18.3529[0m
[32m[2022-08-31 18:04:15,162] [    INFO][0m - loss: 2.584e-05, learning_rate: 2.448235294117647e-05, global_step: 4690, interval_runtime: 1.2957, interval_samples_per_second: 6.174, interval_steps_per_second: 7.718, epoch: 18.3922[0m
[32m[2022-08-31 18:04:16,461] [    INFO][0m - loss: 2.27e-05, learning_rate: 2.4470588235294118e-05, global_step: 4700, interval_runtime: 1.2984, interval_samples_per_second: 6.161, interval_steps_per_second: 7.702, epoch: 18.4314[0m
[32m[2022-08-31 18:04:16,462] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:04:16,462] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 18:04:16,462] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:04:16,462] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:04:16,462] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 18:04:30,622] [    INFO][0m - eval_loss: 2.8294472694396973, eval_accuracy: 0.6179883945841392, eval_runtime: 14.1592, eval_samples_per_second: 146.053, eval_steps_per_second: 4.591, epoch: 18.4314[0m
[32m[2022-08-31 18:04:30,623] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-4700[0m
[32m[2022-08-31 18:04:30,623] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:04:35,688] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-4700/tokenizer_config.json[0m
[32m[2022-08-31 18:04:35,688] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-4700/special_tokens_map.json[0m
[32m[2022-08-31 18:04:38,978] [    INFO][0m - loss: 3.453e-05, learning_rate: 2.4458823529411767e-05, global_step: 4710, interval_runtime: 22.5166, interval_samples_per_second: 0.355, interval_steps_per_second: 0.444, epoch: 18.4706[0m
[32m[2022-08-31 18:04:40,268] [    INFO][0m - loss: 3.592e-05, learning_rate: 2.4447058823529413e-05, global_step: 4720, interval_runtime: 1.2911, interval_samples_per_second: 6.196, interval_steps_per_second: 7.746, epoch: 18.5098[0m
[32m[2022-08-31 18:04:41,578] [    INFO][0m - loss: 2.495e-05, learning_rate: 2.443529411764706e-05, global_step: 4730, interval_runtime: 1.3095, interval_samples_per_second: 6.109, interval_steps_per_second: 7.636, epoch: 18.549[0m
[32m[2022-08-31 18:04:43,266] [    INFO][0m - loss: 2.189e-05, learning_rate: 2.4423529411764705e-05, global_step: 4740, interval_runtime: 1.306, interval_samples_per_second: 6.126, interval_steps_per_second: 7.657, epoch: 18.5882[0m
[32m[2022-08-31 18:04:44,563] [    INFO][0m - loss: 1.914e-05, learning_rate: 2.4411764705882354e-05, global_step: 4750, interval_runtime: 1.6789, interval_samples_per_second: 4.765, interval_steps_per_second: 5.956, epoch: 18.6275[0m
[32m[2022-08-31 18:04:45,854] [    INFO][0m - loss: 2.383e-05, learning_rate: 2.44e-05, global_step: 4760, interval_runtime: 1.2914, interval_samples_per_second: 6.195, interval_steps_per_second: 7.744, epoch: 18.6667[0m
[32m[2022-08-31 18:04:47,154] [    INFO][0m - loss: 3.412e-05, learning_rate: 2.438823529411765e-05, global_step: 4770, interval_runtime: 1.2996, interval_samples_per_second: 6.156, interval_steps_per_second: 7.695, epoch: 18.7059[0m
[32m[2022-08-31 18:04:48,446] [    INFO][0m - loss: 2.014e-05, learning_rate: 2.4376470588235292e-05, global_step: 4780, interval_runtime: 1.2928, interval_samples_per_second: 6.188, interval_steps_per_second: 7.735, epoch: 18.7451[0m
[32m[2022-08-31 18:04:49,732] [    INFO][0m - loss: 2.206e-05, learning_rate: 2.436470588235294e-05, global_step: 4790, interval_runtime: 1.2852, interval_samples_per_second: 6.225, interval_steps_per_second: 7.781, epoch: 18.7843[0m
[32m[2022-08-31 18:04:51,024] [    INFO][0m - loss: 2.834e-05, learning_rate: 2.4352941176470587e-05, global_step: 4800, interval_runtime: 1.2918, interval_samples_per_second: 6.193, interval_steps_per_second: 7.741, epoch: 18.8235[0m
[32m[2022-08-31 18:04:51,025] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:04:51,025] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 18:04:51,025] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:04:51,025] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:04:51,025] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 18:05:05,114] [    INFO][0m - eval_loss: 2.8218634128570557, eval_accuracy: 0.6179883945841392, eval_runtime: 14.0882, eval_samples_per_second: 146.79, eval_steps_per_second: 4.614, epoch: 18.8235[0m
[32m[2022-08-31 18:05:05,115] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-4800[0m
[32m[2022-08-31 18:05:05,115] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:05:06,009] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-4800/tokenizer_config.json[0m
[32m[2022-08-31 18:05:06,009] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-4800/special_tokens_map.json[0m
[32m[2022-08-31 18:05:09,261] [    INFO][0m - loss: 3.111e-05, learning_rate: 2.4341176470588237e-05, global_step: 4810, interval_runtime: 18.2372, interval_samples_per_second: 0.439, interval_steps_per_second: 0.548, epoch: 18.8627[0m
[32m[2022-08-31 18:05:10,550] [    INFO][0m - loss: 2.092e-05, learning_rate: 2.4329411764705883e-05, global_step: 4820, interval_runtime: 1.289, interval_samples_per_second: 6.207, interval_steps_per_second: 7.758, epoch: 18.902[0m
[32m[2022-08-31 18:05:11,835] [    INFO][0m - loss: 3.087e-05, learning_rate: 2.431764705882353e-05, global_step: 4830, interval_runtime: 1.2856, interval_samples_per_second: 6.223, interval_steps_per_second: 7.778, epoch: 18.9412[0m
[32m[2022-08-31 18:05:13,120] [    INFO][0m - loss: 2.578e-05, learning_rate: 2.4305882352941178e-05, global_step: 4840, interval_runtime: 1.2845, interval_samples_per_second: 6.228, interval_steps_per_second: 7.785, epoch: 18.9804[0m
[32m[2022-08-31 18:05:14,412] [    INFO][0m - loss: 4.719e-05, learning_rate: 2.4294117647058824e-05, global_step: 4850, interval_runtime: 1.292, interval_samples_per_second: 6.192, interval_steps_per_second: 7.74, epoch: 19.0196[0m
[32m[2022-08-31 18:05:15,691] [    INFO][0m - loss: 1.504e-05, learning_rate: 2.4282352941176473e-05, global_step: 4860, interval_runtime: 1.279, interval_samples_per_second: 6.255, interval_steps_per_second: 7.819, epoch: 19.0588[0m
[32m[2022-08-31 18:05:16,976] [    INFO][0m - loss: 1.864e-05, learning_rate: 2.4270588235294116e-05, global_step: 4870, interval_runtime: 1.2851, interval_samples_per_second: 6.225, interval_steps_per_second: 7.781, epoch: 19.098[0m
[32m[2022-08-31 18:05:18,269] [    INFO][0m - loss: 2.386e-05, learning_rate: 2.4258823529411765e-05, global_step: 4880, interval_runtime: 1.2931, interval_samples_per_second: 6.187, interval_steps_per_second: 7.733, epoch: 19.1373[0m
[32m[2022-08-31 18:05:19,555] [    INFO][0m - loss: 3.137e-05, learning_rate: 2.424705882352941e-05, global_step: 4890, interval_runtime: 1.2859, interval_samples_per_second: 6.221, interval_steps_per_second: 7.777, epoch: 19.1765[0m
[32m[2022-08-31 18:05:20,865] [    INFO][0m - loss: 2.508e-05, learning_rate: 2.423529411764706e-05, global_step: 4900, interval_runtime: 1.3099, interval_samples_per_second: 6.107, interval_steps_per_second: 7.634, epoch: 19.2157[0m
[32m[2022-08-31 18:05:20,866] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:05:20,866] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 18:05:20,866] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:05:20,867] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:05:20,867] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 18:05:35,052] [    INFO][0m - eval_loss: 2.830559730529785, eval_accuracy: 0.6184719535783365, eval_runtime: 14.1843, eval_samples_per_second: 145.795, eval_steps_per_second: 4.583, epoch: 19.2157[0m
[32m[2022-08-31 18:05:35,052] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-4900[0m
[32m[2022-08-31 18:05:35,052] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:05:35,977] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-4900/tokenizer_config.json[0m
[32m[2022-08-31 18:05:35,977] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-4900/special_tokens_map.json[0m
[32m[2022-08-31 18:05:39,879] [    INFO][0m - loss: 2.663e-05, learning_rate: 2.4223529411764707e-05, global_step: 4910, interval_runtime: 18.3868, interval_samples_per_second: 0.435, interval_steps_per_second: 0.544, epoch: 19.2549[0m
[32m[2022-08-31 18:05:41,173] [    INFO][0m - loss: 2.069e-05, learning_rate: 2.4211764705882353e-05, global_step: 4920, interval_runtime: 1.9206, interval_samples_per_second: 4.165, interval_steps_per_second: 5.207, epoch: 19.2941[0m
[32m[2022-08-31 18:05:42,473] [    INFO][0m - loss: 1.911e-05, learning_rate: 2.42e-05, global_step: 4930, interval_runtime: 1.2966, interval_samples_per_second: 6.17, interval_steps_per_second: 7.712, epoch: 19.3333[0m
[32m[2022-08-31 18:05:43,811] [    INFO][0m - loss: 2.371e-05, learning_rate: 2.4188235294117648e-05, global_step: 4940, interval_runtime: 1.341, interval_samples_per_second: 5.966, interval_steps_per_second: 7.457, epoch: 19.3725[0m
[32m[2022-08-31 18:05:45,108] [    INFO][0m - loss: 3.48e-05, learning_rate: 2.4176470588235294e-05, global_step: 4950, interval_runtime: 1.2979, interval_samples_per_second: 6.164, interval_steps_per_second: 7.705, epoch: 19.4118[0m
[32m[2022-08-31 18:05:46,428] [    INFO][0m - loss: 1.732e-05, learning_rate: 2.4164705882352943e-05, global_step: 4960, interval_runtime: 1.3203, interval_samples_per_second: 6.059, interval_steps_per_second: 7.574, epoch: 19.451[0m
[32m[2022-08-31 18:05:47,718] [    INFO][0m - loss: 3.454e-05, learning_rate: 2.4152941176470586e-05, global_step: 4970, interval_runtime: 1.289, interval_samples_per_second: 6.207, interval_steps_per_second: 7.758, epoch: 19.4902[0m
[32m[2022-08-31 18:05:49,024] [    INFO][0m - loss: 1.697e-05, learning_rate: 2.4141176470588235e-05, global_step: 4980, interval_runtime: 1.3071, interval_samples_per_second: 6.12, interval_steps_per_second: 7.651, epoch: 19.5294[0m
[32m[2022-08-31 18:05:50,311] [    INFO][0m - loss: 2.046e-05, learning_rate: 2.4129411764705885e-05, global_step: 4990, interval_runtime: 1.2865, interval_samples_per_second: 6.218, interval_steps_per_second: 7.773, epoch: 19.5686[0m
[32m[2022-08-31 18:05:51,595] [    INFO][0m - loss: 2.56e-05, learning_rate: 2.411764705882353e-05, global_step: 5000, interval_runtime: 1.284, interval_samples_per_second: 6.231, interval_steps_per_second: 7.788, epoch: 19.6078[0m
[32m[2022-08-31 18:05:51,596] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:05:51,596] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 18:05:51,596] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:05:51,596] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:05:51,596] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 18:06:05,757] [    INFO][0m - eval_loss: 2.834465980529785, eval_accuracy: 0.6233075435203095, eval_runtime: 14.1595, eval_samples_per_second: 146.05, eval_steps_per_second: 4.591, epoch: 19.6078[0m
[32m[2022-08-31 18:06:05,757] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-5000[0m
[32m[2022-08-31 18:06:05,758] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:06:06,700] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-5000/tokenizer_config.json[0m
[32m[2022-08-31 18:06:06,700] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-5000/special_tokens_map.json[0m
[32m[2022-08-31 18:06:10,052] [    INFO][0m - loss: 2.093e-05, learning_rate: 2.410588235294118e-05, global_step: 5010, interval_runtime: 18.4565, interval_samples_per_second: 0.433, interval_steps_per_second: 0.542, epoch: 19.6471[0m
[32m[2022-08-31 18:06:11,360] [    INFO][0m - loss: 2.764e-05, learning_rate: 2.4094117647058823e-05, global_step: 5020, interval_runtime: 1.3087, interval_samples_per_second: 6.113, interval_steps_per_second: 7.641, epoch: 19.6863[0m
[32m[2022-08-31 18:06:14,176] [    INFO][0m - loss: 4.557e-05, learning_rate: 2.4082352941176472e-05, global_step: 5030, interval_runtime: 2.8157, interval_samples_per_second: 2.841, interval_steps_per_second: 3.551, epoch: 19.7255[0m
[32m[2022-08-31 18:06:15,462] [    INFO][0m - loss: 2.031e-05, learning_rate: 2.4070588235294118e-05, global_step: 5040, interval_runtime: 1.2868, interval_samples_per_second: 6.217, interval_steps_per_second: 7.771, epoch: 19.7647[0m
[32m[2022-08-31 18:06:16,760] [    INFO][0m - loss: 2.024e-05, learning_rate: 2.4058823529411767e-05, global_step: 5050, interval_runtime: 1.2977, interval_samples_per_second: 6.165, interval_steps_per_second: 7.706, epoch: 19.8039[0m
[32m[2022-08-31 18:06:18,067] [    INFO][0m - loss: 3.109e-05, learning_rate: 2.404705882352941e-05, global_step: 5060, interval_runtime: 1.3044, interval_samples_per_second: 6.133, interval_steps_per_second: 7.667, epoch: 19.8431[0m
[32m[2022-08-31 18:06:19,380] [    INFO][0m - loss: 1.997e-05, learning_rate: 2.403529411764706e-05, global_step: 5070, interval_runtime: 1.3152, interval_samples_per_second: 6.083, interval_steps_per_second: 7.603, epoch: 19.8824[0m
[32m[2022-08-31 18:06:20,685] [    INFO][0m - loss: 2.859e-05, learning_rate: 2.4023529411764705e-05, global_step: 5080, interval_runtime: 1.3047, interval_samples_per_second: 6.131, interval_steps_per_second: 7.664, epoch: 19.9216[0m
[32m[2022-08-31 18:06:21,971] [    INFO][0m - loss: 5.25e-05, learning_rate: 2.4011764705882355e-05, global_step: 5090, interval_runtime: 1.2858, interval_samples_per_second: 6.222, interval_steps_per_second: 7.777, epoch: 19.9608[0m
[32m[2022-08-31 18:06:23,193] [    INFO][0m - loss: 2.165e-05, learning_rate: 2.4e-05, global_step: 5100, interval_runtime: 1.2222, interval_samples_per_second: 6.546, interval_steps_per_second: 8.182, epoch: 20.0[0m
[32m[2022-08-31 18:06:23,193] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:06:23,194] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 18:06:23,194] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:06:23,194] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:06:23,194] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 18:06:37,237] [    INFO][0m - eval_loss: 2.8295469284057617, eval_accuracy: 0.620889748549323, eval_runtime: 14.0424, eval_samples_per_second: 147.268, eval_steps_per_second: 4.629, epoch: 20.0[0m
[32m[2022-08-31 18:06:37,237] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-5100[0m
[32m[2022-08-31 18:06:37,237] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:06:38,144] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-5100/tokenizer_config.json[0m
[32m[2022-08-31 18:06:38,145] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-5100/special_tokens_map.json[0m
[32m[2022-08-31 18:06:41,423] [    INFO][0m - loss: 2.709e-05, learning_rate: 2.3988235294117647e-05, global_step: 5110, interval_runtime: 18.2303, interval_samples_per_second: 0.439, interval_steps_per_second: 0.549, epoch: 20.0392[0m
[32m[2022-08-31 18:06:42,713] [    INFO][0m - loss: 2.368e-05, learning_rate: 2.3976470588235293e-05, global_step: 5120, interval_runtime: 1.2904, interval_samples_per_second: 6.2, interval_steps_per_second: 7.75, epoch: 20.0784[0m
[32m[2022-08-31 18:06:44,021] [    INFO][0m - loss: 2.775e-05, learning_rate: 2.3964705882352942e-05, global_step: 5130, interval_runtime: 1.3076, interval_samples_per_second: 6.118, interval_steps_per_second: 7.648, epoch: 20.1176[0m
[32m[2022-08-31 18:06:45,318] [    INFO][0m - loss: 2.119e-05, learning_rate: 2.395294117647059e-05, global_step: 5140, interval_runtime: 1.2966, interval_samples_per_second: 6.17, interval_steps_per_second: 7.712, epoch: 20.1569[0m
[32m[2022-08-31 18:06:46,622] [    INFO][0m - loss: 4.166e-05, learning_rate: 2.3941176470588237e-05, global_step: 5150, interval_runtime: 1.3047, interval_samples_per_second: 6.132, interval_steps_per_second: 7.665, epoch: 20.1961[0m
[32m[2022-08-31 18:06:47,908] [    INFO][0m - loss: 3.081e-05, learning_rate: 2.3929411764705883e-05, global_step: 5160, interval_runtime: 1.2858, interval_samples_per_second: 6.222, interval_steps_per_second: 7.777, epoch: 20.2353[0m
[32m[2022-08-31 18:06:49,232] [    INFO][0m - loss: 2.38e-05, learning_rate: 2.391764705882353e-05, global_step: 5170, interval_runtime: 1.3237, interval_samples_per_second: 6.044, interval_steps_per_second: 7.554, epoch: 20.2745[0m
[32m[2022-08-31 18:06:50,511] [    INFO][0m - loss: 2.186e-05, learning_rate: 2.390588235294118e-05, global_step: 5180, interval_runtime: 1.2788, interval_samples_per_second: 6.256, interval_steps_per_second: 7.82, epoch: 20.3137[0m
[32m[2022-08-31 18:06:51,800] [    INFO][0m - loss: 2.705e-05, learning_rate: 2.3894117647058825e-05, global_step: 5190, interval_runtime: 1.2893, interval_samples_per_second: 6.205, interval_steps_per_second: 7.756, epoch: 20.3529[0m
[32m[2022-08-31 18:06:53,090] [    INFO][0m - loss: 1.735e-05, learning_rate: 2.388235294117647e-05, global_step: 5200, interval_runtime: 1.2896, interval_samples_per_second: 6.204, interval_steps_per_second: 7.754, epoch: 20.3922[0m
[32m[2022-08-31 18:06:53,090] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:06:53,091] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 18:06:53,091] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:06:53,091] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:06:53,091] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 18:07:07,241] [    INFO][0m - eval_loss: 2.8419158458709717, eval_accuracy: 0.6184719535783365, eval_runtime: 14.1495, eval_samples_per_second: 146.153, eval_steps_per_second: 4.594, epoch: 20.3922[0m
[32m[2022-08-31 18:07:07,241] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-5200[0m
[32m[2022-08-31 18:07:07,242] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:07:08,230] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-5200/tokenizer_config.json[0m
[32m[2022-08-31 18:07:08,230] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-5200/special_tokens_map.json[0m
[32m[2022-08-31 18:07:11,879] [    INFO][0m - loss: 2.609e-05, learning_rate: 2.3870588235294117e-05, global_step: 5210, interval_runtime: 18.789, interval_samples_per_second: 0.426, interval_steps_per_second: 0.532, epoch: 20.4314[0m
[32m[2022-08-31 18:07:13,169] [    INFO][0m - loss: 4.981e-05, learning_rate: 2.3858823529411766e-05, global_step: 5220, interval_runtime: 1.2902, interval_samples_per_second: 6.201, interval_steps_per_second: 7.751, epoch: 20.4706[0m
[32m[2022-08-31 18:07:14,459] [    INFO][0m - loss: 3.905e-05, learning_rate: 2.3847058823529412e-05, global_step: 5230, interval_runtime: 1.2903, interval_samples_per_second: 6.2, interval_steps_per_second: 7.75, epoch: 20.5098[0m
[32m[2022-08-31 18:07:15,751] [    INFO][0m - loss: 1.539e-05, learning_rate: 2.383529411764706e-05, global_step: 5240, interval_runtime: 1.2915, interval_samples_per_second: 6.194, interval_steps_per_second: 7.743, epoch: 20.549[0m
[32m[2022-08-31 18:07:17,038] [    INFO][0m - loss: 1.835e-05, learning_rate: 2.3823529411764704e-05, global_step: 5250, interval_runtime: 1.2871, interval_samples_per_second: 6.216, interval_steps_per_second: 7.77, epoch: 20.5882[0m
[32m[2022-08-31 18:07:18,350] [    INFO][0m - loss: 2.296e-05, learning_rate: 2.3811764705882353e-05, global_step: 5260, interval_runtime: 1.3119, interval_samples_per_second: 6.098, interval_steps_per_second: 7.622, epoch: 20.6275[0m
[32m[2022-08-31 18:07:19,636] [    INFO][0m - loss: 2.083e-05, learning_rate: 2.38e-05, global_step: 5270, interval_runtime: 1.2864, interval_samples_per_second: 6.219, interval_steps_per_second: 7.774, epoch: 20.6667[0m
[32m[2022-08-31 18:07:20,936] [    INFO][0m - loss: 3.523e-05, learning_rate: 2.378823529411765e-05, global_step: 5280, interval_runtime: 1.2994, interval_samples_per_second: 6.157, interval_steps_per_second: 7.696, epoch: 20.7059[0m
[32m[2022-08-31 18:07:22,219] [    INFO][0m - loss: 1.715e-05, learning_rate: 2.3776470588235298e-05, global_step: 5290, interval_runtime: 1.283, interval_samples_per_second: 6.235, interval_steps_per_second: 7.794, epoch: 20.7451[0m
[32m[2022-08-31 18:07:23,502] [    INFO][0m - loss: 1.796e-05, learning_rate: 2.376470588235294e-05, global_step: 5300, interval_runtime: 1.2833, interval_samples_per_second: 6.234, interval_steps_per_second: 7.792, epoch: 20.7843[0m
[32m[2022-08-31 18:07:23,503] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:07:23,503] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 18:07:23,503] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:07:23,503] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:07:23,503] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 18:07:37,531] [    INFO][0m - eval_loss: 2.875288724899292, eval_accuracy: 0.6204061895551257, eval_runtime: 14.0276, eval_samples_per_second: 147.423, eval_steps_per_second: 4.634, epoch: 20.7843[0m
[32m[2022-08-31 18:07:37,532] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-5300[0m
[32m[2022-08-31 18:07:37,532] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:07:38,475] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-5300/tokenizer_config.json[0m
[32m[2022-08-31 18:07:38,476] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-5300/special_tokens_map.json[0m
[32m[2022-08-31 18:07:41,642] [    INFO][0m - loss: 2.139e-05, learning_rate: 2.375294117647059e-05, global_step: 5310, interval_runtime: 18.1403, interval_samples_per_second: 0.441, interval_steps_per_second: 0.551, epoch: 20.8235[0m
[32m[2022-08-31 18:07:42,919] [    INFO][0m - loss: 2.941e-05, learning_rate: 2.3741176470588236e-05, global_step: 5320, interval_runtime: 1.2772, interval_samples_per_second: 6.264, interval_steps_per_second: 7.83, epoch: 20.8627[0m
[32m[2022-08-31 18:07:44,205] [    INFO][0m - loss: 1.615e-05, learning_rate: 2.3729411764705885e-05, global_step: 5330, interval_runtime: 1.2853, interval_samples_per_second: 6.224, interval_steps_per_second: 7.78, epoch: 20.902[0m
[32m[2022-08-31 18:07:45,493] [    INFO][0m - loss: 2.734e-05, learning_rate: 2.3717647058823528e-05, global_step: 5340, interval_runtime: 1.2883, interval_samples_per_second: 6.21, interval_steps_per_second: 7.762, epoch: 20.9412[0m
[32m[2022-08-31 18:07:46,785] [    INFO][0m - loss: 2.439e-05, learning_rate: 2.3705882352941177e-05, global_step: 5350, interval_runtime: 1.2915, interval_samples_per_second: 6.195, interval_steps_per_second: 7.743, epoch: 20.9804[0m
[32m[2022-08-31 18:07:48,087] [    INFO][0m - loss: 1.475e-05, learning_rate: 2.3694117647058823e-05, global_step: 5360, interval_runtime: 1.3019, interval_samples_per_second: 6.145, interval_steps_per_second: 7.681, epoch: 21.0196[0m
[32m[2022-08-31 18:07:49,381] [    INFO][0m - loss: 1.216e-05, learning_rate: 2.3682352941176472e-05, global_step: 5370, interval_runtime: 1.2941, interval_samples_per_second: 6.182, interval_steps_per_second: 7.728, epoch: 21.0588[0m
[32m[2022-08-31 18:07:50,676] [    INFO][0m - loss: 2.157e-05, learning_rate: 2.367058823529412e-05, global_step: 5380, interval_runtime: 1.295, interval_samples_per_second: 6.178, interval_steps_per_second: 7.722, epoch: 21.098[0m
[32m[2022-08-31 18:07:51,969] [    INFO][0m - loss: 2.302e-05, learning_rate: 2.3658823529411764e-05, global_step: 5390, interval_runtime: 1.2928, interval_samples_per_second: 6.188, interval_steps_per_second: 7.735, epoch: 21.1373[0m
[32m[2022-08-31 18:07:53,267] [    INFO][0m - loss: 1.976e-05, learning_rate: 2.364705882352941e-05, global_step: 5400, interval_runtime: 1.2987, interval_samples_per_second: 6.16, interval_steps_per_second: 7.7, epoch: 21.1765[0m
[32m[2022-08-31 18:07:53,268] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:07:53,268] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 18:07:53,268] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:07:53,268] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:07:53,268] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 18:08:07,485] [    INFO][0m - eval_loss: 2.867133378982544, eval_accuracy: 0.6218568665377177, eval_runtime: 14.2163, eval_samples_per_second: 145.467, eval_steps_per_second: 4.572, epoch: 21.1765[0m
[32m[2022-08-31 18:08:07,486] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-5400[0m
[32m[2022-08-31 18:08:07,486] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:08:08,413] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-5400/tokenizer_config.json[0m
[32m[2022-08-31 18:08:08,413] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-5400/special_tokens_map.json[0m
[32m[2022-08-31 18:08:11,674] [    INFO][0m - loss: 1.028e-05, learning_rate: 2.363529411764706e-05, global_step: 5410, interval_runtime: 18.4073, interval_samples_per_second: 0.435, interval_steps_per_second: 0.543, epoch: 21.2157[0m
[32m[2022-08-31 18:08:12,960] [    INFO][0m - loss: 2.181e-05, learning_rate: 2.3623529411764706e-05, global_step: 5420, interval_runtime: 1.2861, interval_samples_per_second: 6.22, interval_steps_per_second: 7.775, epoch: 21.2549[0m
[32m[2022-08-31 18:08:14,265] [    INFO][0m - loss: 1.728e-05, learning_rate: 2.3611764705882355e-05, global_step: 5430, interval_runtime: 1.3046, interval_samples_per_second: 6.132, interval_steps_per_second: 7.665, epoch: 21.2941[0m
[32m[2022-08-31 18:08:15,558] [    INFO][0m - loss: 2.082e-05, learning_rate: 2.3599999999999998e-05, global_step: 5440, interval_runtime: 1.2926, interval_samples_per_second: 6.189, interval_steps_per_second: 7.736, epoch: 21.3333[0m
[32m[2022-08-31 18:08:16,846] [    INFO][0m - loss: 1.968e-05, learning_rate: 2.3588235294117647e-05, global_step: 5450, interval_runtime: 1.2876, interval_samples_per_second: 6.213, interval_steps_per_second: 7.766, epoch: 21.3725[0m
[32m[2022-08-31 18:08:18,137] [    INFO][0m - loss: 2.289e-05, learning_rate: 2.3576470588235296e-05, global_step: 5460, interval_runtime: 1.2916, interval_samples_per_second: 6.194, interval_steps_per_second: 7.743, epoch: 21.4118[0m
[32m[2022-08-31 18:08:19,431] [    INFO][0m - loss: 2.163e-05, learning_rate: 2.3564705882352942e-05, global_step: 5470, interval_runtime: 1.2937, interval_samples_per_second: 6.184, interval_steps_per_second: 7.73, epoch: 21.451[0m
[32m[2022-08-31 18:08:20,723] [    INFO][0m - loss: 2.97e-05, learning_rate: 2.3552941176470592e-05, global_step: 5480, interval_runtime: 1.292, interval_samples_per_second: 6.192, interval_steps_per_second: 7.74, epoch: 21.4902[0m
[32m[2022-08-31 18:08:22,016] [    INFO][0m - loss: 1.45e-05, learning_rate: 2.3541176470588234e-05, global_step: 5490, interval_runtime: 1.2929, interval_samples_per_second: 6.188, interval_steps_per_second: 7.735, epoch: 21.5294[0m
[32m[2022-08-31 18:08:23,313] [    INFO][0m - loss: 2.84e-05, learning_rate: 2.3529411764705884e-05, global_step: 5500, interval_runtime: 1.2969, interval_samples_per_second: 6.168, interval_steps_per_second: 7.71, epoch: 21.5686[0m
[32m[2022-08-31 18:08:23,314] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:08:23,314] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 18:08:23,314] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:08:23,314] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:08:23,314] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 18:08:37,495] [    INFO][0m - eval_loss: 2.867501974105835, eval_accuracy: 0.6194390715667312, eval_runtime: 14.1805, eval_samples_per_second: 145.834, eval_steps_per_second: 4.584, epoch: 21.5686[0m
[32m[2022-08-31 18:08:37,496] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-5500[0m
[32m[2022-08-31 18:08:37,496] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:08:38,426] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-5500/tokenizer_config.json[0m
[32m[2022-08-31 18:08:38,426] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-5500/special_tokens_map.json[0m
[32m[2022-08-31 18:08:42,229] [    INFO][0m - loss: 1.814e-05, learning_rate: 2.351764705882353e-05, global_step: 5510, interval_runtime: 18.5653, interval_samples_per_second: 0.431, interval_steps_per_second: 0.539, epoch: 21.6078[0m
[32m[2022-08-31 18:08:43,525] [    INFO][0m - loss: 1.855e-05, learning_rate: 2.350588235294118e-05, global_step: 5520, interval_runtime: 1.6463, interval_samples_per_second: 4.859, interval_steps_per_second: 6.074, epoch: 21.6471[0m
[32m[2022-08-31 18:08:44,814] [    INFO][0m - loss: 2.702e-05, learning_rate: 2.349411764705882e-05, global_step: 5530, interval_runtime: 1.2893, interval_samples_per_second: 6.205, interval_steps_per_second: 7.756, epoch: 21.6863[0m
[32m[2022-08-31 18:08:46,142] [    INFO][0m - loss: 1.906e-05, learning_rate: 2.348235294117647e-05, global_step: 5540, interval_runtime: 1.3288, interval_samples_per_second: 6.02, interval_steps_per_second: 7.526, epoch: 21.7255[0m
[32m[2022-08-31 18:08:47,439] [    INFO][0m - loss: 2.23e-05, learning_rate: 2.3470588235294117e-05, global_step: 5550, interval_runtime: 1.2962, interval_samples_per_second: 6.172, interval_steps_per_second: 7.715, epoch: 21.7647[0m
[32m[2022-08-31 18:08:48,731] [    INFO][0m - loss: 1.681e-05, learning_rate: 2.3458823529411766e-05, global_step: 5560, interval_runtime: 1.2927, interval_samples_per_second: 6.188, interval_steps_per_second: 7.735, epoch: 21.8039[0m
[32m[2022-08-31 18:08:50,028] [    INFO][0m - loss: 2.396e-05, learning_rate: 2.3447058823529412e-05, global_step: 5570, interval_runtime: 1.2968, interval_samples_per_second: 6.169, interval_steps_per_second: 7.711, epoch: 21.8431[0m
[32m[2022-08-31 18:08:51,335] [    INFO][0m - loss: 1.475e-05, learning_rate: 2.3435294117647058e-05, global_step: 5580, interval_runtime: 1.3061, interval_samples_per_second: 6.125, interval_steps_per_second: 7.656, epoch: 21.8824[0m
[32m[2022-08-31 18:08:52,619] [    INFO][0m - loss: 1.619e-05, learning_rate: 2.3423529411764704e-05, global_step: 5590, interval_runtime: 1.2848, interval_samples_per_second: 6.226, interval_steps_per_second: 7.783, epoch: 21.9216[0m
[32m[2022-08-31 18:08:53,922] [    INFO][0m - loss: 2.223e-05, learning_rate: 2.3411764705882354e-05, global_step: 5600, interval_runtime: 1.3034, interval_samples_per_second: 6.138, interval_steps_per_second: 7.672, epoch: 21.9608[0m
[32m[2022-08-31 18:08:53,923] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:08:53,923] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 18:08:53,923] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:08:53,923] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:08:53,924] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 18:09:08,158] [    INFO][0m - eval_loss: 2.869645118713379, eval_accuracy: 0.6179883945841392, eval_runtime: 14.2345, eval_samples_per_second: 145.281, eval_steps_per_second: 4.566, epoch: 21.9608[0m
[32m[2022-08-31 18:09:08,159] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-5600[0m
[32m[2022-08-31 18:09:08,159] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:09:09,232] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-5600/tokenizer_config.json[0m
[32m[2022-08-31 18:09:09,232] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-5600/special_tokens_map.json[0m
[32m[2022-08-31 18:09:12,897] [    INFO][0m - loss: 2.613e-05, learning_rate: 2.3400000000000003e-05, global_step: 5610, interval_runtime: 18.9742, interval_samples_per_second: 0.422, interval_steps_per_second: 0.527, epoch: 22.0[0m
[32m[2022-08-31 18:09:14,323] [    INFO][0m - loss: 2.619e-05, learning_rate: 2.338823529411765e-05, global_step: 5620, interval_runtime: 1.4265, interval_samples_per_second: 5.608, interval_steps_per_second: 7.01, epoch: 22.0392[0m
[32m[2022-08-31 18:09:15,610] [    INFO][0m - loss: 1.378e-05, learning_rate: 2.3376470588235295e-05, global_step: 5630, interval_runtime: 1.2872, interval_samples_per_second: 6.215, interval_steps_per_second: 7.769, epoch: 22.0784[0m
[32m[2022-08-31 18:09:16,904] [    INFO][0m - loss: 1.156e-05, learning_rate: 2.336470588235294e-05, global_step: 5640, interval_runtime: 1.2936, interval_samples_per_second: 6.184, interval_steps_per_second: 7.73, epoch: 22.1176[0m
[32m[2022-08-31 18:09:18,207] [    INFO][0m - loss: 1.524e-05, learning_rate: 2.335294117647059e-05, global_step: 5650, interval_runtime: 1.3032, interval_samples_per_second: 6.139, interval_steps_per_second: 7.673, epoch: 22.1569[0m
[32m[2022-08-31 18:09:19,497] [    INFO][0m - loss: 1.815e-05, learning_rate: 2.3341176470588236e-05, global_step: 5660, interval_runtime: 1.2901, interval_samples_per_second: 6.201, interval_steps_per_second: 7.751, epoch: 22.1961[0m
[32m[2022-08-31 18:09:20,795] [    INFO][0m - loss: 1.575e-05, learning_rate: 2.3329411764705882e-05, global_step: 5670, interval_runtime: 1.2976, interval_samples_per_second: 6.165, interval_steps_per_second: 7.706, epoch: 22.2353[0m
[32m[2022-08-31 18:09:22,085] [    INFO][0m - loss: 1.441e-05, learning_rate: 2.3317647058823528e-05, global_step: 5680, interval_runtime: 1.2903, interval_samples_per_second: 6.2, interval_steps_per_second: 7.75, epoch: 22.2745[0m
[32m[2022-08-31 18:09:23,381] [    INFO][0m - loss: 1.976e-05, learning_rate: 2.3305882352941178e-05, global_step: 5690, interval_runtime: 1.2955, interval_samples_per_second: 6.175, interval_steps_per_second: 7.719, epoch: 22.3137[0m
[32m[2022-08-31 18:09:24,673] [    INFO][0m - loss: 1.738e-05, learning_rate: 2.3294117647058824e-05, global_step: 5700, interval_runtime: 1.2916, interval_samples_per_second: 6.194, interval_steps_per_second: 7.742, epoch: 22.3529[0m
[32m[2022-08-31 18:09:24,673] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:09:24,673] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 18:09:24,673] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:09:24,674] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:09:24,674] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 18:09:38,796] [    INFO][0m - eval_loss: 2.881276845932007, eval_accuracy: 0.617504835589942, eval_runtime: 14.1213, eval_samples_per_second: 146.445, eval_steps_per_second: 4.603, epoch: 22.3529[0m
[32m[2022-08-31 18:09:38,796] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-5700[0m
[32m[2022-08-31 18:09:38,797] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:09:39,787] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-5700/tokenizer_config.json[0m
[32m[2022-08-31 18:09:39,788] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-5700/special_tokens_map.json[0m
[32m[2022-08-31 18:09:43,728] [    INFO][0m - loss: 2.317e-05, learning_rate: 2.3282352941176473e-05, global_step: 5710, interval_runtime: 19.0547, interval_samples_per_second: 0.42, interval_steps_per_second: 0.525, epoch: 22.3922[0m
[32m[2022-08-31 18:09:45,028] [    INFO][0m - loss: 1.56e-05, learning_rate: 2.3270588235294115e-05, global_step: 5720, interval_runtime: 1.3005, interval_samples_per_second: 6.152, interval_steps_per_second: 7.69, epoch: 22.4314[0m
[32m[2022-08-31 18:09:46,322] [    INFO][0m - loss: 3.568e-05, learning_rate: 2.3258823529411765e-05, global_step: 5730, interval_runtime: 1.2941, interval_samples_per_second: 6.182, interval_steps_per_second: 7.727, epoch: 22.4706[0m
[32m[2022-08-31 18:09:47,609] [    INFO][0m - loss: 1.794e-05, learning_rate: 2.324705882352941e-05, global_step: 5740, interval_runtime: 1.2865, interval_samples_per_second: 6.218, interval_steps_per_second: 7.773, epoch: 22.5098[0m
[32m[2022-08-31 18:09:48,913] [    INFO][0m - loss: 1.975e-05, learning_rate: 2.323529411764706e-05, global_step: 5750, interval_runtime: 1.3051, interval_samples_per_second: 6.13, interval_steps_per_second: 7.662, epoch: 22.549[0m
[32m[2022-08-31 18:09:50,225] [    INFO][0m - loss: 1.738e-05, learning_rate: 2.322352941176471e-05, global_step: 5760, interval_runtime: 1.3112, interval_samples_per_second: 6.101, interval_steps_per_second: 7.627, epoch: 22.5882[0m
[32m[2022-08-31 18:09:51,514] [    INFO][0m - loss: 3.028e-05, learning_rate: 2.3211764705882352e-05, global_step: 5770, interval_runtime: 1.289, interval_samples_per_second: 6.207, interval_steps_per_second: 7.758, epoch: 22.6275[0m
[32m[2022-08-31 18:09:52,813] [    INFO][0m - loss: 1.874e-05, learning_rate: 2.32e-05, global_step: 5780, interval_runtime: 1.299, interval_samples_per_second: 6.159, interval_steps_per_second: 7.698, epoch: 22.6667[0m
[32m[2022-08-31 18:09:54,123] [    INFO][0m - loss: 1.233e-05, learning_rate: 2.3188235294117647e-05, global_step: 5790, interval_runtime: 1.3107, interval_samples_per_second: 6.104, interval_steps_per_second: 7.63, epoch: 22.7059[0m
[32m[2022-08-31 18:09:55,413] [    INFO][0m - loss: 2.002e-05, learning_rate: 2.3176470588235297e-05, global_step: 5800, interval_runtime: 1.2892, interval_samples_per_second: 6.206, interval_steps_per_second: 7.757, epoch: 22.7451[0m
[32m[2022-08-31 18:09:55,414] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:09:55,414] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 18:09:55,414] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:09:55,414] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:09:55,414] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 18:10:09,480] [    INFO][0m - eval_loss: 2.868619680404663, eval_accuracy: 0.6237911025145068, eval_runtime: 14.0654, eval_samples_per_second: 147.027, eval_steps_per_second: 4.621, epoch: 22.7451[0m
[32m[2022-08-31 18:10:09,481] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-5800[0m
[32m[2022-08-31 18:10:09,481] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:10:10,544] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-5800/tokenizer_config.json[0m
[32m[2022-08-31 18:10:10,545] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-5800/special_tokens_map.json[0m
[32m[2022-08-31 18:10:14,161] [    INFO][0m - loss: 2.758e-05, learning_rate: 2.3164705882352943e-05, global_step: 5810, interval_runtime: 18.749, interval_samples_per_second: 0.427, interval_steps_per_second: 0.533, epoch: 22.7843[0m
[32m[2022-08-31 18:10:15,449] [    INFO][0m - loss: 3.159e-05, learning_rate: 2.315294117647059e-05, global_step: 5820, interval_runtime: 1.2879, interval_samples_per_second: 6.211, interval_steps_per_second: 7.764, epoch: 22.8235[0m
[32m[2022-08-31 18:10:16,743] [    INFO][0m - loss: 1.629e-05, learning_rate: 2.3141176470588235e-05, global_step: 5830, interval_runtime: 1.2939, interval_samples_per_second: 6.183, interval_steps_per_second: 7.728, epoch: 22.8627[0m
[32m[2022-08-31 18:10:18,022] [    INFO][0m - loss: 1.413e-05, learning_rate: 2.3129411764705884e-05, global_step: 5840, interval_runtime: 1.2789, interval_samples_per_second: 6.255, interval_steps_per_second: 7.819, epoch: 22.902[0m
[32m[2022-08-31 18:10:19,314] [    INFO][0m - loss: 2.599e-05, learning_rate: 2.311764705882353e-05, global_step: 5850, interval_runtime: 1.2917, interval_samples_per_second: 6.193, interval_steps_per_second: 7.742, epoch: 22.9412[0m
[32m[2022-08-31 18:10:20,596] [    INFO][0m - loss: 1.142e-05, learning_rate: 2.3105882352941176e-05, global_step: 5860, interval_runtime: 1.2818, interval_samples_per_second: 6.241, interval_steps_per_second: 7.801, epoch: 22.9804[0m
[32m[2022-08-31 18:10:21,893] [    INFO][0m - loss: 1.401e-05, learning_rate: 2.3094117647058822e-05, global_step: 5870, interval_runtime: 1.2973, interval_samples_per_second: 6.166, interval_steps_per_second: 7.708, epoch: 23.0196[0m
[32m[2022-08-31 18:10:23,191] [    INFO][0m - loss: 1.944e-05, learning_rate: 2.308235294117647e-05, global_step: 5880, interval_runtime: 1.2974, interval_samples_per_second: 6.166, interval_steps_per_second: 7.708, epoch: 23.0588[0m
[32m[2022-08-31 18:10:24,483] [    INFO][0m - loss: 2.23e-05, learning_rate: 2.3070588235294117e-05, global_step: 5890, interval_runtime: 1.2929, interval_samples_per_second: 6.188, interval_steps_per_second: 7.735, epoch: 23.098[0m
[32m[2022-08-31 18:10:25,779] [    INFO][0m - loss: 1.568e-05, learning_rate: 2.3058823529411767e-05, global_step: 5900, interval_runtime: 1.2954, interval_samples_per_second: 6.176, interval_steps_per_second: 7.72, epoch: 23.1373[0m
[32m[2022-08-31 18:10:25,780] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:10:25,780] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 18:10:25,780] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:10:25,780] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:10:25,780] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 18:10:39,843] [    INFO][0m - eval_loss: 2.8791890144348145, eval_accuracy: 0.6213733075435203, eval_runtime: 14.0624, eval_samples_per_second: 147.059, eval_steps_per_second: 4.622, epoch: 23.1373[0m
[32m[2022-08-31 18:10:39,843] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-5900[0m
[32m[2022-08-31 18:10:39,843] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:10:40,818] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-5900/tokenizer_config.json[0m
[32m[2022-08-31 18:10:40,818] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-5900/special_tokens_map.json[0m
[32m[2022-08-31 18:10:44,396] [    INFO][0m - loss: 1.545e-05, learning_rate: 2.3047058823529413e-05, global_step: 5910, interval_runtime: 18.6172, interval_samples_per_second: 0.43, interval_steps_per_second: 0.537, epoch: 23.1765[0m
[32m[2022-08-31 18:10:45,680] [    INFO][0m - loss: 1.529e-05, learning_rate: 2.303529411764706e-05, global_step: 5920, interval_runtime: 1.2841, interval_samples_per_second: 6.23, interval_steps_per_second: 7.787, epoch: 23.2157[0m
[32m[2022-08-31 18:10:46,966] [    INFO][0m - loss: 1.375e-05, learning_rate: 2.3023529411764708e-05, global_step: 5930, interval_runtime: 1.2853, interval_samples_per_second: 6.224, interval_steps_per_second: 7.781, epoch: 23.2549[0m
[32m[2022-08-31 18:10:48,265] [    INFO][0m - loss: 1.681e-05, learning_rate: 2.3011764705882354e-05, global_step: 5940, interval_runtime: 1.2991, interval_samples_per_second: 6.158, interval_steps_per_second: 7.698, epoch: 23.2941[0m
[32m[2022-08-31 18:10:49,579] [    INFO][0m - loss: 1.972e-05, learning_rate: 2.3000000000000003e-05, global_step: 5950, interval_runtime: 1.3112, interval_samples_per_second: 6.101, interval_steps_per_second: 7.626, epoch: 23.3333[0m
[32m[2022-08-31 18:10:50,867] [    INFO][0m - loss: 1.853e-05, learning_rate: 2.2988235294117646e-05, global_step: 5960, interval_runtime: 1.2908, interval_samples_per_second: 6.198, interval_steps_per_second: 7.747, epoch: 23.3725[0m
[32m[2022-08-31 18:10:52,151] [    INFO][0m - loss: 1.511e-05, learning_rate: 2.2976470588235295e-05, global_step: 5970, interval_runtime: 1.2845, interval_samples_per_second: 6.228, interval_steps_per_second: 7.785, epoch: 23.4118[0m
[32m[2022-08-31 18:10:53,440] [    INFO][0m - loss: 1.243e-05, learning_rate: 2.296470588235294e-05, global_step: 5980, interval_runtime: 1.2888, interval_samples_per_second: 6.207, interval_steps_per_second: 7.759, epoch: 23.451[0m
[32m[2022-08-31 18:10:54,729] [    INFO][0m - loss: 1.688e-05, learning_rate: 2.295294117647059e-05, global_step: 5990, interval_runtime: 1.2891, interval_samples_per_second: 6.206, interval_steps_per_second: 7.758, epoch: 23.4902[0m
[32m[2022-08-31 18:10:56,015] [    INFO][0m - loss: 1.085e-05, learning_rate: 2.2941176470588233e-05, global_step: 6000, interval_runtime: 1.2854, interval_samples_per_second: 6.224, interval_steps_per_second: 7.779, epoch: 23.5294[0m
[32m[2022-08-31 18:10:56,015] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:10:56,015] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 18:10:56,015] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:10:56,016] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:10:56,016] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 18:11:10,229] [    INFO][0m - eval_loss: 2.879037857055664, eval_accuracy: 0.6223404255319149, eval_runtime: 14.2132, eval_samples_per_second: 145.499, eval_steps_per_second: 4.573, epoch: 23.5294[0m
[32m[2022-08-31 18:11:10,230] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-6000[0m
[32m[2022-08-31 18:11:10,230] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:11:11,230] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-6000/tokenizer_config.json[0m
[32m[2022-08-31 18:11:11,230] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-6000/special_tokens_map.json[0m
[32m[2022-08-31 18:11:14,939] [    INFO][0m - loss: 1.25e-05, learning_rate: 2.2929411764705883e-05, global_step: 6010, interval_runtime: 18.9244, interval_samples_per_second: 0.423, interval_steps_per_second: 0.528, epoch: 23.5686[0m
[32m[2022-08-31 18:11:16,243] [    INFO][0m - loss: 2.07e-05, learning_rate: 2.291764705882353e-05, global_step: 6020, interval_runtime: 1.3037, interval_samples_per_second: 6.136, interval_steps_per_second: 7.67, epoch: 23.6078[0m
[32m[2022-08-31 18:11:17,538] [    INFO][0m - loss: 1.521e-05, learning_rate: 2.2905882352941178e-05, global_step: 6030, interval_runtime: 1.2951, interval_samples_per_second: 6.177, interval_steps_per_second: 7.721, epoch: 23.6471[0m
[32m[2022-08-31 18:11:18,872] [    INFO][0m - loss: 1.693e-05, learning_rate: 2.2894117647058824e-05, global_step: 6040, interval_runtime: 1.3328, interval_samples_per_second: 6.003, interval_steps_per_second: 7.503, epoch: 23.6863[0m
[32m[2022-08-31 18:11:20,188] [    INFO][0m - loss: 1.807e-05, learning_rate: 2.288235294117647e-05, global_step: 6050, interval_runtime: 1.3177, interval_samples_per_second: 6.071, interval_steps_per_second: 7.589, epoch: 23.7255[0m
[32m[2022-08-31 18:11:21,514] [    INFO][0m - loss: 2.132e-05, learning_rate: 2.2870588235294116e-05, global_step: 6060, interval_runtime: 1.3255, interval_samples_per_second: 6.035, interval_steps_per_second: 7.544, epoch: 23.7647[0m
[32m[2022-08-31 18:11:22,815] [    INFO][0m - loss: 1.865e-05, learning_rate: 2.2858823529411765e-05, global_step: 6070, interval_runtime: 1.3, interval_samples_per_second: 6.154, interval_steps_per_second: 7.692, epoch: 23.8039[0m
[32m[2022-08-31 18:11:24,113] [    INFO][0m - loss: 1.943e-05, learning_rate: 2.2847058823529415e-05, global_step: 6080, interval_runtime: 1.2994, interval_samples_per_second: 6.157, interval_steps_per_second: 7.696, epoch: 23.8431[0m
[32m[2022-08-31 18:11:25,420] [    INFO][0m - loss: 1.201e-05, learning_rate: 2.283529411764706e-05, global_step: 6090, interval_runtime: 1.3069, interval_samples_per_second: 6.121, interval_steps_per_second: 7.651, epoch: 23.8824[0m
[32m[2022-08-31 18:11:26,712] [    INFO][0m - loss: 1.381e-05, learning_rate: 2.2823529411764707e-05, global_step: 6100, interval_runtime: 1.292, interval_samples_per_second: 6.192, interval_steps_per_second: 7.74, epoch: 23.9216[0m
[32m[2022-08-31 18:11:26,713] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:11:26,713] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 18:11:26,713] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:11:26,713] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:11:26,713] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 18:11:40,883] [    INFO][0m - eval_loss: 2.8812103271484375, eval_accuracy: 0.6237911025145068, eval_runtime: 14.1683, eval_samples_per_second: 145.959, eval_steps_per_second: 4.588, epoch: 23.9216[0m
[32m[2022-08-31 18:11:40,883] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-6100[0m
[32m[2022-08-31 18:11:40,884] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:11:42,020] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-6100/tokenizer_config.json[0m
[32m[2022-08-31 18:11:42,021] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-6100/special_tokens_map.json[0m
[32m[2022-08-31 18:11:45,824] [    INFO][0m - loss: 4.273e-05, learning_rate: 2.2811764705882353e-05, global_step: 6110, interval_runtime: 19.1116, interval_samples_per_second: 0.419, interval_steps_per_second: 0.523, epoch: 23.9608[0m
[32m[2022-08-31 18:11:47,055] [    INFO][0m - loss: 1.241e-05, learning_rate: 2.2800000000000002e-05, global_step: 6120, interval_runtime: 1.2308, interval_samples_per_second: 6.5, interval_steps_per_second: 8.125, epoch: 24.0[0m
[32m[2022-08-31 18:11:48,454] [    INFO][0m - loss: 1.117e-05, learning_rate: 2.2788235294117648e-05, global_step: 6130, interval_runtime: 1.3987, interval_samples_per_second: 5.72, interval_steps_per_second: 7.15, epoch: 24.0392[0m
[32m[2022-08-31 18:11:49,757] [    INFO][0m - loss: 1.388e-05, learning_rate: 2.2776470588235294e-05, global_step: 6140, interval_runtime: 1.3041, interval_samples_per_second: 6.134, interval_steps_per_second: 7.668, epoch: 24.0784[0m
[32m[2022-08-31 18:11:51,056] [    INFO][0m - loss: 2.01e-05, learning_rate: 2.276470588235294e-05, global_step: 6150, interval_runtime: 1.2981, interval_samples_per_second: 6.163, interval_steps_per_second: 7.704, epoch: 24.1176[0m
[32m[2022-08-31 18:11:52,368] [    INFO][0m - loss: 5.473e-05, learning_rate: 2.275294117647059e-05, global_step: 6160, interval_runtime: 1.3121, interval_samples_per_second: 6.097, interval_steps_per_second: 7.621, epoch: 24.1569[0m
[32m[2022-08-31 18:11:53,666] [    INFO][0m - loss: 2.662e-05, learning_rate: 2.2741176470588235e-05, global_step: 6170, interval_runtime: 1.2984, interval_samples_per_second: 6.162, interval_steps_per_second: 7.702, epoch: 24.1961[0m
[32m[2022-08-31 18:11:54,965] [    INFO][0m - loss: 2.693e-05, learning_rate: 2.2729411764705885e-05, global_step: 6180, interval_runtime: 1.2988, interval_samples_per_second: 6.159, interval_steps_per_second: 7.699, epoch: 24.2353[0m
[32m[2022-08-31 18:11:56,261] [    INFO][0m - loss: 1.36e-05, learning_rate: 2.2717647058823527e-05, global_step: 6190, interval_runtime: 1.2957, interval_samples_per_second: 6.174, interval_steps_per_second: 7.718, epoch: 24.2745[0m
[32m[2022-08-31 18:11:57,562] [    INFO][0m - loss: 1.483e-05, learning_rate: 2.2705882352941177e-05, global_step: 6200, interval_runtime: 1.3013, interval_samples_per_second: 6.148, interval_steps_per_second: 7.685, epoch: 24.3137[0m
[32m[2022-08-31 18:11:57,563] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:11:57,563] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 18:11:57,563] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:11:57,563] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:11:57,563] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 18:12:11,720] [    INFO][0m - eval_loss: 2.8634607791900635, eval_accuracy: 0.6223404255319149, eval_runtime: 14.1564, eval_samples_per_second: 146.083, eval_steps_per_second: 4.592, epoch: 24.3137[0m
[32m[2022-08-31 18:12:11,721] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-6200[0m
[32m[2022-08-31 18:12:11,721] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:12:12,838] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-6200/tokenizer_config.json[0m
[32m[2022-08-31 18:12:12,839] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-6200/special_tokens_map.json[0m
[32m[2022-08-31 18:12:16,381] [    INFO][0m - loss: 3.979e-05, learning_rate: 2.2694117647058822e-05, global_step: 6210, interval_runtime: 18.8187, interval_samples_per_second: 0.425, interval_steps_per_second: 0.531, epoch: 24.3529[0m
[32m[2022-08-31 18:12:17,673] [    INFO][0m - loss: 2.158e-05, learning_rate: 2.2682352941176472e-05, global_step: 6220, interval_runtime: 1.2919, interval_samples_per_second: 6.193, interval_steps_per_second: 7.741, epoch: 24.3922[0m
[32m[2022-08-31 18:12:18,971] [    INFO][0m - loss: 1.883e-05, learning_rate: 2.267058823529412e-05, global_step: 6230, interval_runtime: 1.2978, interval_samples_per_second: 6.164, interval_steps_per_second: 7.705, epoch: 24.4314[0m
[32m[2022-08-31 18:12:20,260] [    INFO][0m - loss: 1.916e-05, learning_rate: 2.2658823529411764e-05, global_step: 6240, interval_runtime: 1.2892, interval_samples_per_second: 6.205, interval_steps_per_second: 7.757, epoch: 24.4706[0m
[32m[2022-08-31 18:12:21,551] [    INFO][0m - loss: 3.45e-05, learning_rate: 2.2647058823529413e-05, global_step: 6250, interval_runtime: 1.2915, interval_samples_per_second: 6.194, interval_steps_per_second: 7.743, epoch: 24.5098[0m
[32m[2022-08-31 18:12:22,836] [    INFO][0m - loss: 1.452e-05, learning_rate: 2.263529411764706e-05, global_step: 6260, interval_runtime: 1.2844, interval_samples_per_second: 6.229, interval_steps_per_second: 7.786, epoch: 24.549[0m
[32m[2022-08-31 18:12:24,128] [    INFO][0m - loss: 1.911e-05, learning_rate: 2.262352941176471e-05, global_step: 6270, interval_runtime: 1.2929, interval_samples_per_second: 6.188, interval_steps_per_second: 7.734, epoch: 24.5882[0m
[32m[2022-08-31 18:12:25,413] [    INFO][0m - loss: 2.08e-05, learning_rate: 2.2611764705882354e-05, global_step: 6280, interval_runtime: 1.2846, interval_samples_per_second: 6.227, interval_steps_per_second: 7.784, epoch: 24.6275[0m
[32m[2022-08-31 18:12:26,701] [    INFO][0m - loss: 1.561e-05, learning_rate: 2.26e-05, global_step: 6290, interval_runtime: 1.2885, interval_samples_per_second: 6.209, interval_steps_per_second: 7.761, epoch: 24.6667[0m
[32m[2022-08-31 18:12:27,995] [    INFO][0m - loss: 2.437e-05, learning_rate: 2.2588235294117646e-05, global_step: 6300, interval_runtime: 1.2931, interval_samples_per_second: 6.187, interval_steps_per_second: 7.734, epoch: 24.7059[0m
[32m[2022-08-31 18:12:27,995] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:12:27,996] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 18:12:27,996] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:12:27,996] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:12:27,996] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 18:12:41,980] [    INFO][0m - eval_loss: 2.9033613204956055, eval_accuracy: 0.6179883945841392, eval_runtime: 13.9841, eval_samples_per_second: 147.883, eval_steps_per_second: 4.648, epoch: 24.7059[0m
[32m[2022-08-31 18:12:41,981] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-6300[0m
[32m[2022-08-31 18:12:41,981] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:12:43,099] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-6300/tokenizer_config.json[0m
[32m[2022-08-31 18:12:43,099] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-6300/special_tokens_map.json[0m
[32m[2022-08-31 18:12:46,815] [    INFO][0m - loss: 1.489e-05, learning_rate: 2.2576470588235296e-05, global_step: 6310, interval_runtime: 18.8204, interval_samples_per_second: 0.425, interval_steps_per_second: 0.531, epoch: 24.7451[0m
[32m[2022-08-31 18:12:48,097] [    INFO][0m - loss: 1.694e-05, learning_rate: 2.2564705882352942e-05, global_step: 6320, interval_runtime: 1.2816, interval_samples_per_second: 6.242, interval_steps_per_second: 7.802, epoch: 24.7843[0m
[32m[2022-08-31 18:12:49,382] [    INFO][0m - loss: 1.708e-05, learning_rate: 2.2552941176470588e-05, global_step: 6330, interval_runtime: 1.2851, interval_samples_per_second: 6.225, interval_steps_per_second: 7.782, epoch: 24.8235[0m
[32m[2022-08-31 18:12:50,672] [    INFO][0m - loss: 1.468e-05, learning_rate: 2.2541176470588234e-05, global_step: 6340, interval_runtime: 1.2894, interval_samples_per_second: 6.205, interval_steps_per_second: 7.756, epoch: 24.8627[0m
[32m[2022-08-31 18:12:51,954] [    INFO][0m - loss: 1.781e-05, learning_rate: 2.2529411764705883e-05, global_step: 6350, interval_runtime: 1.2831, interval_samples_per_second: 6.235, interval_steps_per_second: 7.794, epoch: 24.902[0m
[32m[2022-08-31 18:12:53,243] [    INFO][0m - loss: 1.23e-05, learning_rate: 2.251764705882353e-05, global_step: 6360, interval_runtime: 1.2888, interval_samples_per_second: 6.207, interval_steps_per_second: 7.759, epoch: 24.9412[0m
[32m[2022-08-31 18:12:54,528] [    INFO][0m - loss: 1.485e-05, learning_rate: 2.250588235294118e-05, global_step: 6370, interval_runtime: 1.2847, interval_samples_per_second: 6.227, interval_steps_per_second: 7.784, epoch: 24.9804[0m
[32m[2022-08-31 18:12:55,872] [    INFO][0m - loss: 1.269e-05, learning_rate: 2.2494117647058824e-05, global_step: 6380, interval_runtime: 1.3436, interval_samples_per_second: 5.954, interval_steps_per_second: 7.442, epoch: 25.0196[0m
[32m[2022-08-31 18:12:57,164] [    INFO][0m - loss: 1.454e-05, learning_rate: 2.248235294117647e-05, global_step: 6390, interval_runtime: 1.2929, interval_samples_per_second: 6.188, interval_steps_per_second: 7.735, epoch: 25.0588[0m
[32m[2022-08-31 18:12:58,457] [    INFO][0m - loss: 9.17e-06, learning_rate: 2.247058823529412e-05, global_step: 6400, interval_runtime: 1.2913, interval_samples_per_second: 6.195, interval_steps_per_second: 7.744, epoch: 25.098[0m
[32m[2022-08-31 18:12:58,457] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:12:58,458] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 18:12:58,458] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:12:58,458] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:12:58,458] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 18:13:12,482] [    INFO][0m - eval_loss: 2.9041948318481445, eval_accuracy: 0.6213733075435203, eval_runtime: 14.0234, eval_samples_per_second: 147.468, eval_steps_per_second: 4.635, epoch: 25.098[0m
[32m[2022-08-31 18:13:12,482] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-6400[0m
[32m[2022-08-31 18:13:12,482] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:13:13,677] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-6400/tokenizer_config.json[0m
[32m[2022-08-31 18:13:13,677] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-6400/special_tokens_map.json[0m
[32m[2022-08-31 18:13:19,327] [    INFO][0m - loss: 1.523e-05, learning_rate: 2.2458823529411766e-05, global_step: 6410, interval_runtime: 19.0409, interval_samples_per_second: 0.42, interval_steps_per_second: 0.525, epoch: 25.1373[0m
[32m[2022-08-31 18:13:20,612] [    INFO][0m - loss: 1.692e-05, learning_rate: 2.2447058823529415e-05, global_step: 6420, interval_runtime: 3.1154, interval_samples_per_second: 2.568, interval_steps_per_second: 3.21, epoch: 25.1765[0m
[32m[2022-08-31 18:13:21,909] [    INFO][0m - loss: 1.081e-05, learning_rate: 2.2435294117647058e-05, global_step: 6430, interval_runtime: 1.2967, interval_samples_per_second: 6.17, interval_steps_per_second: 7.712, epoch: 25.2157[0m
[32m[2022-08-31 18:13:23,208] [    INFO][0m - loss: 1.124e-05, learning_rate: 2.2423529411764707e-05, global_step: 6440, interval_runtime: 1.2989, interval_samples_per_second: 6.159, interval_steps_per_second: 7.699, epoch: 25.2549[0m
[32m[2022-08-31 18:13:24,495] [    INFO][0m - loss: 1.038e-05, learning_rate: 2.2411764705882353e-05, global_step: 6450, interval_runtime: 1.2878, interval_samples_per_second: 6.212, interval_steps_per_second: 7.765, epoch: 25.2941[0m
[32m[2022-08-31 18:13:25,786] [    INFO][0m - loss: 1.204e-05, learning_rate: 2.2400000000000002e-05, global_step: 6460, interval_runtime: 1.2908, interval_samples_per_second: 6.198, interval_steps_per_second: 7.747, epoch: 25.3333[0m
[32m[2022-08-31 18:13:27,095] [    INFO][0m - loss: 2.658e-05, learning_rate: 2.2388235294117645e-05, global_step: 6470, interval_runtime: 1.3083, interval_samples_per_second: 6.115, interval_steps_per_second: 7.644, epoch: 25.3725[0m
[32m[2022-08-31 18:13:28,384] [    INFO][0m - loss: 2.233e-05, learning_rate: 2.2376470588235294e-05, global_step: 6480, interval_runtime: 1.2892, interval_samples_per_second: 6.205, interval_steps_per_second: 7.757, epoch: 25.4118[0m
[32m[2022-08-31 18:13:29,672] [    INFO][0m - loss: 1.518e-05, learning_rate: 2.236470588235294e-05, global_step: 6490, interval_runtime: 1.2882, interval_samples_per_second: 6.21, interval_steps_per_second: 7.763, epoch: 25.451[0m
[32m[2022-08-31 18:13:30,960] [    INFO][0m - loss: 1.769e-05, learning_rate: 2.235294117647059e-05, global_step: 6500, interval_runtime: 1.2883, interval_samples_per_second: 6.21, interval_steps_per_second: 7.762, epoch: 25.4902[0m
[32m[2022-08-31 18:13:30,961] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:13:30,961] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 18:13:30,961] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:13:30,961] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:13:30,961] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 18:13:44,981] [    INFO][0m - eval_loss: 2.8957202434539795, eval_accuracy: 0.620889748549323, eval_runtime: 14.019, eval_samples_per_second: 147.514, eval_steps_per_second: 4.637, epoch: 25.4902[0m
[32m[2022-08-31 18:13:44,982] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-6500[0m
[32m[2022-08-31 18:13:44,982] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:13:46,168] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-6500/tokenizer_config.json[0m
[32m[2022-08-31 18:13:46,168] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-6500/special_tokens_map.json[0m
[32m[2022-08-31 18:13:49,597] [    INFO][0m - loss: 2.012e-05, learning_rate: 2.2341176470588236e-05, global_step: 6510, interval_runtime: 18.6358, interval_samples_per_second: 0.429, interval_steps_per_second: 0.537, epoch: 25.5294[0m
[32m[2022-08-31 18:13:50,897] [    INFO][0m - loss: 2.517e-05, learning_rate: 2.232941176470588e-05, global_step: 6520, interval_runtime: 1.3006, interval_samples_per_second: 6.151, interval_steps_per_second: 7.689, epoch: 25.5686[0m
[32m[2022-08-31 18:13:52,179] [    INFO][0m - loss: 1.049e-05, learning_rate: 2.231764705882353e-05, global_step: 6530, interval_runtime: 1.2825, interval_samples_per_second: 6.238, interval_steps_per_second: 7.797, epoch: 25.6078[0m
[32m[2022-08-31 18:13:53,462] [    INFO][0m - loss: 1.175e-05, learning_rate: 2.2305882352941177e-05, global_step: 6540, interval_runtime: 1.2825, interval_samples_per_second: 6.238, interval_steps_per_second: 7.797, epoch: 25.6471[0m
[32m[2022-08-31 18:13:54,761] [    INFO][0m - loss: 1.025e-05, learning_rate: 2.2294117647058826e-05, global_step: 6550, interval_runtime: 1.2993, interval_samples_per_second: 6.157, interval_steps_per_second: 7.697, epoch: 25.6863[0m
[32m[2022-08-31 18:13:56,047] [    INFO][0m - loss: 3.323e-05, learning_rate: 2.2282352941176472e-05, global_step: 6560, interval_runtime: 1.2859, interval_samples_per_second: 6.222, interval_steps_per_second: 7.777, epoch: 25.7255[0m
[32m[2022-08-31 18:13:57,348] [    INFO][0m - loss: 1.818e-05, learning_rate: 2.2270588235294118e-05, global_step: 6570, interval_runtime: 1.3003, interval_samples_per_second: 6.152, interval_steps_per_second: 7.691, epoch: 25.7647[0m
[32m[2022-08-31 18:13:58,634] [    INFO][0m - loss: 1.125e-05, learning_rate: 2.2258823529411764e-05, global_step: 6580, interval_runtime: 1.2867, interval_samples_per_second: 6.218, interval_steps_per_second: 7.772, epoch: 25.8039[0m
[32m[2022-08-31 18:13:59,920] [    INFO][0m - loss: 1.718e-05, learning_rate: 2.2247058823529414e-05, global_step: 6590, interval_runtime: 1.2857, interval_samples_per_second: 6.222, interval_steps_per_second: 7.778, epoch: 25.8431[0m
[32m[2022-08-31 18:14:01,213] [    INFO][0m - loss: 1.347e-05, learning_rate: 2.223529411764706e-05, global_step: 6600, interval_runtime: 1.2935, interval_samples_per_second: 6.185, interval_steps_per_second: 7.731, epoch: 25.8824[0m
[32m[2022-08-31 18:14:01,214] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:14:01,214] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 18:14:01,214] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:14:01,214] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:14:01,214] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 18:14:15,241] [    INFO][0m - eval_loss: 2.947314977645874, eval_accuracy: 0.6179883945841392, eval_runtime: 14.0258, eval_samples_per_second: 147.443, eval_steps_per_second: 4.634, epoch: 25.8824[0m
[32m[2022-08-31 18:14:15,241] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-6600[0m
[32m[2022-08-31 18:14:15,241] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:14:16,184] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-6600/tokenizer_config.json[0m
[32m[2022-08-31 18:14:16,185] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-6600/special_tokens_map.json[0m
[32m[2022-08-31 18:14:21,309] [    INFO][0m - loss: 1.208e-05, learning_rate: 2.222352941176471e-05, global_step: 6610, interval_runtime: 20.0958, interval_samples_per_second: 0.398, interval_steps_per_second: 0.498, epoch: 25.9216[0m
[32m[2022-08-31 18:14:22,598] [    INFO][0m - loss: 1.164e-05, learning_rate: 2.221176470588235e-05, global_step: 6620, interval_runtime: 1.2893, interval_samples_per_second: 6.205, interval_steps_per_second: 7.756, epoch: 25.9608[0m
[32m[2022-08-31 18:14:23,822] [    INFO][0m - loss: 3.057e-05, learning_rate: 2.22e-05, global_step: 6630, interval_runtime: 1.2237, interval_samples_per_second: 6.538, interval_steps_per_second: 8.172, epoch: 26.0[0m
[32m[2022-08-31 18:14:25,211] [    INFO][0m - loss: 1.183e-05, learning_rate: 2.2188235294117647e-05, global_step: 6640, interval_runtime: 1.3895, interval_samples_per_second: 5.757, interval_steps_per_second: 7.197, epoch: 26.0392[0m
[32m[2022-08-31 18:14:26,495] [    INFO][0m - loss: 2.255e-05, learning_rate: 2.2176470588235296e-05, global_step: 6650, interval_runtime: 1.2836, interval_samples_per_second: 6.232, interval_steps_per_second: 7.791, epoch: 26.0784[0m
[32m[2022-08-31 18:14:27,773] [    INFO][0m - loss: 1.812e-05, learning_rate: 2.216470588235294e-05, global_step: 6660, interval_runtime: 1.2785, interval_samples_per_second: 6.257, interval_steps_per_second: 7.822, epoch: 26.1176[0m
[32m[2022-08-31 18:14:29,060] [    INFO][0m - loss: 1.123e-05, learning_rate: 2.2152941176470588e-05, global_step: 6670, interval_runtime: 1.2858, interval_samples_per_second: 6.222, interval_steps_per_second: 7.777, epoch: 26.1569[0m
[32m[2022-08-31 18:14:30,346] [    INFO][0m - loss: 1.616e-05, learning_rate: 2.2141176470588234e-05, global_step: 6680, interval_runtime: 1.2867, interval_samples_per_second: 6.217, interval_steps_per_second: 7.772, epoch: 26.1961[0m
[32m[2022-08-31 18:14:31,627] [    INFO][0m - loss: 1.013e-05, learning_rate: 2.2129411764705884e-05, global_step: 6690, interval_runtime: 1.2806, interval_samples_per_second: 6.247, interval_steps_per_second: 7.809, epoch: 26.2353[0m
[32m[2022-08-31 18:14:32,925] [    INFO][0m - loss: 9.73e-06, learning_rate: 2.2117647058823533e-05, global_step: 6700, interval_runtime: 1.2983, interval_samples_per_second: 6.162, interval_steps_per_second: 7.702, epoch: 26.2745[0m
[32m[2022-08-31 18:14:32,926] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:14:32,926] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 18:14:32,926] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:14:32,926] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:14:32,926] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 18:14:46,899] [    INFO][0m - eval_loss: 2.9278979301452637, eval_accuracy: 0.6218568665377177, eval_runtime: 13.9716, eval_samples_per_second: 148.015, eval_steps_per_second: 4.652, epoch: 26.2745[0m
[32m[2022-08-31 18:14:46,899] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-6700[0m
[32m[2022-08-31 18:14:46,899] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:14:48,009] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-6700/tokenizer_config.json[0m
[32m[2022-08-31 18:14:48,009] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-6700/special_tokens_map.json[0m
[32m[2022-08-31 18:14:53,079] [    INFO][0m - loss: 1.093e-05, learning_rate: 2.2105882352941175e-05, global_step: 6710, interval_runtime: 18.6329, interval_samples_per_second: 0.429, interval_steps_per_second: 0.537, epoch: 26.3137[0m
[32m[2022-08-31 18:14:54,374] [    INFO][0m - loss: 1.412e-05, learning_rate: 2.2094117647058825e-05, global_step: 6720, interval_runtime: 2.8161, interval_samples_per_second: 2.841, interval_steps_per_second: 3.551, epoch: 26.3529[0m
[32m[2022-08-31 18:14:55,668] [    INFO][0m - loss: 1.035e-05, learning_rate: 2.208235294117647e-05, global_step: 6730, interval_runtime: 1.294, interval_samples_per_second: 6.182, interval_steps_per_second: 7.728, epoch: 26.3922[0m
[32m[2022-08-31 18:14:56,949] [    INFO][0m - loss: 9.44e-06, learning_rate: 2.207058823529412e-05, global_step: 6740, interval_runtime: 1.2809, interval_samples_per_second: 6.246, interval_steps_per_second: 7.807, epoch: 26.4314[0m
[32m[2022-08-31 18:14:58,245] [    INFO][0m - loss: 2.04e-05, learning_rate: 2.2058823529411766e-05, global_step: 6750, interval_runtime: 1.2961, interval_samples_per_second: 6.172, interval_steps_per_second: 7.715, epoch: 26.4706[0m
[32m[2022-08-31 18:14:59,525] [    INFO][0m - loss: 1.249e-05, learning_rate: 2.2047058823529412e-05, global_step: 6760, interval_runtime: 1.2798, interval_samples_per_second: 6.251, interval_steps_per_second: 7.814, epoch: 26.5098[0m
[32m[2022-08-31 18:15:00,803] [    INFO][0m - loss: 1.134e-05, learning_rate: 2.2035294117647058e-05, global_step: 6770, interval_runtime: 1.2778, interval_samples_per_second: 6.261, interval_steps_per_second: 7.826, epoch: 26.549[0m
[32m[2022-08-31 18:15:02,085] [    INFO][0m - loss: 1.578e-05, learning_rate: 2.2023529411764707e-05, global_step: 6780, interval_runtime: 1.2821, interval_samples_per_second: 6.24, interval_steps_per_second: 7.8, epoch: 26.5882[0m
[32m[2022-08-31 18:15:03,373] [    INFO][0m - loss: 1.259e-05, learning_rate: 2.2011764705882353e-05, global_step: 6790, interval_runtime: 1.2882, interval_samples_per_second: 6.21, interval_steps_per_second: 7.763, epoch: 26.6275[0m
[32m[2022-08-31 18:15:04,671] [    INFO][0m - loss: 1.938e-05, learning_rate: 2.2e-05, global_step: 6800, interval_runtime: 1.2978, interval_samples_per_second: 6.164, interval_steps_per_second: 7.705, epoch: 26.6667[0m
[32m[2022-08-31 18:15:04,671] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:15:04,672] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 18:15:04,672] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:15:04,672] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:15:04,672] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 18:15:18,656] [    INFO][0m - eval_loss: 2.9402811527252197, eval_accuracy: 0.6184719535783365, eval_runtime: 13.9833, eval_samples_per_second: 147.89, eval_steps_per_second: 4.648, epoch: 26.6667[0m
[32m[2022-08-31 18:15:18,656] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-6800[0m
[32m[2022-08-31 18:15:18,657] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:15:19,768] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-6800/tokenizer_config.json[0m
[32m[2022-08-31 18:15:19,769] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-6800/special_tokens_map.json[0m
[32m[2022-08-31 18:15:23,443] [    INFO][0m - loss: 1.077e-05, learning_rate: 2.1988235294117645e-05, global_step: 6810, interval_runtime: 18.772, interval_samples_per_second: 0.426, interval_steps_per_second: 0.533, epoch: 26.7059[0m
[32m[2022-08-31 18:15:24,718] [    INFO][0m - loss: 9.11e-06, learning_rate: 2.1976470588235295e-05, global_step: 6820, interval_runtime: 1.2748, interval_samples_per_second: 6.276, interval_steps_per_second: 7.845, epoch: 26.7451[0m
[32m[2022-08-31 18:15:26,002] [    INFO][0m - loss: 1.159e-05, learning_rate: 2.196470588235294e-05, global_step: 6830, interval_runtime: 1.2839, interval_samples_per_second: 6.231, interval_steps_per_second: 7.788, epoch: 26.7843[0m
[32m[2022-08-31 18:15:27,283] [    INFO][0m - loss: 1.234e-05, learning_rate: 2.195294117647059e-05, global_step: 6840, interval_runtime: 1.2817, interval_samples_per_second: 6.242, interval_steps_per_second: 7.802, epoch: 26.8235[0m
[32m[2022-08-31 18:15:28,566] [    INFO][0m - loss: 2.291e-05, learning_rate: 2.1941176470588236e-05, global_step: 6850, interval_runtime: 1.2828, interval_samples_per_second: 6.236, interval_steps_per_second: 7.796, epoch: 26.8627[0m
[32m[2022-08-31 18:15:29,844] [    INFO][0m - loss: 1.924e-05, learning_rate: 2.1929411764705882e-05, global_step: 6860, interval_runtime: 1.2784, interval_samples_per_second: 6.258, interval_steps_per_second: 7.822, epoch: 26.902[0m
[32m[2022-08-31 18:15:31,146] [    INFO][0m - loss: 9.38e-06, learning_rate: 2.191764705882353e-05, global_step: 6870, interval_runtime: 1.3013, interval_samples_per_second: 6.148, interval_steps_per_second: 7.685, epoch: 26.9412[0m
[32m[2022-08-31 18:15:32,439] [    INFO][0m - loss: 9.8e-06, learning_rate: 2.1905882352941177e-05, global_step: 6880, interval_runtime: 1.2932, interval_samples_per_second: 6.186, interval_steps_per_second: 7.732, epoch: 26.9804[0m
[32m[2022-08-31 18:15:33,735] [    INFO][0m - loss: 1.337e-05, learning_rate: 2.1894117647058827e-05, global_step: 6890, interval_runtime: 1.2963, interval_samples_per_second: 6.171, interval_steps_per_second: 7.714, epoch: 27.0196[0m
[32m[2022-08-31 18:15:35,015] [    INFO][0m - loss: 8.79e-06, learning_rate: 2.188235294117647e-05, global_step: 6900, interval_runtime: 1.2801, interval_samples_per_second: 6.249, interval_steps_per_second: 7.812, epoch: 27.0588[0m
[32m[2022-08-31 18:15:35,016] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:15:35,016] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 18:15:35,016] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:15:35,016] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:15:35,016] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 18:15:49,005] [    INFO][0m - eval_loss: 2.9530088901519775, eval_accuracy: 0.6199226305609284, eval_runtime: 13.9879, eval_samples_per_second: 147.842, eval_steps_per_second: 4.647, epoch: 27.0588[0m
[32m[2022-08-31 18:15:49,005] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-6900[0m
[32m[2022-08-31 18:15:49,005] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:15:50,094] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-6900/tokenizer_config.json[0m
[32m[2022-08-31 18:15:50,095] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-6900/special_tokens_map.json[0m
[32m[2022-08-31 18:15:53,924] [    INFO][0m - loss: 1.166e-05, learning_rate: 2.187058823529412e-05, global_step: 6910, interval_runtime: 18.9087, interval_samples_per_second: 0.423, interval_steps_per_second: 0.529, epoch: 27.098[0m
[32m[2022-08-31 18:15:55,217] [    INFO][0m - loss: 1.225e-05, learning_rate: 2.1858823529411765e-05, global_step: 6920, interval_runtime: 1.2927, interval_samples_per_second: 6.189, interval_steps_per_second: 7.736, epoch: 27.1373[0m
[32m[2022-08-31 18:15:56,508] [    INFO][0m - loss: 1.035e-05, learning_rate: 2.1847058823529414e-05, global_step: 6930, interval_runtime: 1.2905, interval_samples_per_second: 6.199, interval_steps_per_second: 7.749, epoch: 27.1765[0m
[32m[2022-08-31 18:15:57,809] [    INFO][0m - loss: 1.597e-05, learning_rate: 2.1835294117647057e-05, global_step: 6940, interval_runtime: 1.3018, interval_samples_per_second: 6.145, interval_steps_per_second: 7.681, epoch: 27.2157[0m
[32m[2022-08-31 18:15:59,099] [    INFO][0m - loss: 8.82e-06, learning_rate: 2.1823529411764706e-05, global_step: 6950, interval_runtime: 1.2896, interval_samples_per_second: 6.203, interval_steps_per_second: 7.754, epoch: 27.2549[0m
[32m[2022-08-31 18:16:00,391] [    INFO][0m - loss: 1.412e-05, learning_rate: 2.1811764705882352e-05, global_step: 6960, interval_runtime: 1.2922, interval_samples_per_second: 6.191, interval_steps_per_second: 7.738, epoch: 27.2941[0m
[32m[2022-08-31 18:16:01,678] [    INFO][0m - loss: 1.198e-05, learning_rate: 2.18e-05, global_step: 6970, interval_runtime: 1.2869, interval_samples_per_second: 6.216, interval_steps_per_second: 7.771, epoch: 27.3333[0m
[32m[2022-08-31 18:16:02,969] [    INFO][0m - loss: 1.133e-05, learning_rate: 2.1788235294117647e-05, global_step: 6980, interval_runtime: 1.2913, interval_samples_per_second: 6.195, interval_steps_per_second: 7.744, epoch: 27.3725[0m
[32m[2022-08-31 18:16:04,257] [    INFO][0m - loss: 9.29e-06, learning_rate: 2.1776470588235293e-05, global_step: 6990, interval_runtime: 1.2874, interval_samples_per_second: 6.214, interval_steps_per_second: 7.768, epoch: 27.4118[0m
[32m[2022-08-31 18:16:05,536] [    INFO][0m - loss: 2.39e-05, learning_rate: 2.1764705882352943e-05, global_step: 7000, interval_runtime: 1.2787, interval_samples_per_second: 6.256, interval_steps_per_second: 7.82, epoch: 27.451[0m
[32m[2022-08-31 18:16:05,536] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:16:05,536] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 18:16:05,536] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:16:05,536] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:16:05,536] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 18:16:19,510] [    INFO][0m - eval_loss: 2.9763638973236084, eval_accuracy: 0.6179883945841392, eval_runtime: 13.973, eval_samples_per_second: 147.999, eval_steps_per_second: 4.652, epoch: 27.451[0m
[32m[2022-08-31 18:16:19,510] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-7000[0m
[32m[2022-08-31 18:16:19,511] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:16:20,779] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-7000/tokenizer_config.json[0m
[32m[2022-08-31 18:16:20,779] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-7000/special_tokens_map.json[0m
[32m[2022-08-31 18:16:24,503] [    INFO][0m - loss: 1.726e-05, learning_rate: 2.175294117647059e-05, global_step: 7010, interval_runtime: 18.9675, interval_samples_per_second: 0.422, interval_steps_per_second: 0.527, epoch: 27.4902[0m
[32m[2022-08-31 18:16:25,792] [    INFO][0m - loss: 9.25e-06, learning_rate: 2.1741176470588238e-05, global_step: 7020, interval_runtime: 1.2889, interval_samples_per_second: 6.207, interval_steps_per_second: 7.758, epoch: 27.5294[0m
[32m[2022-08-31 18:16:27,075] [    INFO][0m - loss: 1.084e-05, learning_rate: 2.1729411764705884e-05, global_step: 7030, interval_runtime: 1.2827, interval_samples_per_second: 6.237, interval_steps_per_second: 7.796, epoch: 27.5686[0m
[32m[2022-08-31 18:16:28,368] [    INFO][0m - loss: 1.085e-05, learning_rate: 2.171764705882353e-05, global_step: 7040, interval_runtime: 1.2931, interval_samples_per_second: 6.187, interval_steps_per_second: 7.733, epoch: 27.6078[0m
[32m[2022-08-31 18:16:29,657] [    INFO][0m - loss: 1.035e-05, learning_rate: 2.1705882352941176e-05, global_step: 7050, interval_runtime: 1.289, interval_samples_per_second: 6.206, interval_steps_per_second: 7.758, epoch: 27.6471[0m
[32m[2022-08-31 18:16:30,940] [    INFO][0m - loss: 1.119e-05, learning_rate: 2.1694117647058825e-05, global_step: 7060, interval_runtime: 1.2827, interval_samples_per_second: 6.237, interval_steps_per_second: 7.796, epoch: 27.6863[0m
[32m[2022-08-31 18:16:32,223] [    INFO][0m - loss: 8.71e-06, learning_rate: 2.168235294117647e-05, global_step: 7070, interval_runtime: 1.2831, interval_samples_per_second: 6.235, interval_steps_per_second: 7.793, epoch: 27.7255[0m
[32m[2022-08-31 18:16:33,504] [    INFO][0m - loss: 2.393e-05, learning_rate: 2.167058823529412e-05, global_step: 7080, interval_runtime: 1.2811, interval_samples_per_second: 6.245, interval_steps_per_second: 7.806, epoch: 27.7647[0m
[32m[2022-08-31 18:16:34,802] [    INFO][0m - loss: 1.079e-05, learning_rate: 2.1658823529411763e-05, global_step: 7090, interval_runtime: 1.2981, interval_samples_per_second: 6.163, interval_steps_per_second: 7.704, epoch: 27.8039[0m
[32m[2022-08-31 18:16:36,090] [    INFO][0m - loss: 1.673e-05, learning_rate: 2.1647058823529413e-05, global_step: 7100, interval_runtime: 1.2883, interval_samples_per_second: 6.21, interval_steps_per_second: 7.762, epoch: 27.8431[0m
[32m[2022-08-31 18:16:36,091] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:16:36,091] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 18:16:36,091] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:16:36,091] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:16:36,091] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 18:16:50,114] [    INFO][0m - eval_loss: 2.967109441757202, eval_accuracy: 0.6184719535783365, eval_runtime: 14.0225, eval_samples_per_second: 147.477, eval_steps_per_second: 4.635, epoch: 27.8431[0m
[32m[2022-08-31 18:16:50,115] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-7100[0m
[32m[2022-08-31 18:16:50,115] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:16:51,244] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-7100/tokenizer_config.json[0m
[32m[2022-08-31 18:16:51,244] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-7100/special_tokens_map.json[0m
[32m[2022-08-31 18:16:54,968] [    INFO][0m - loss: 9.64e-06, learning_rate: 2.163529411764706e-05, global_step: 7110, interval_runtime: 18.8777, interval_samples_per_second: 0.424, interval_steps_per_second: 0.53, epoch: 27.8824[0m
[32m[2022-08-31 18:16:56,265] [    INFO][0m - loss: 9.32e-06, learning_rate: 2.1623529411764708e-05, global_step: 7120, interval_runtime: 1.2974, interval_samples_per_second: 6.166, interval_steps_per_second: 7.708, epoch: 27.9216[0m
[32m[2022-08-31 18:16:57,558] [    INFO][0m - loss: 1.13e-05, learning_rate: 2.161176470588235e-05, global_step: 7130, interval_runtime: 1.2928, interval_samples_per_second: 6.188, interval_steps_per_second: 7.735, epoch: 27.9608[0m
[32m[2022-08-31 18:16:58,787] [    INFO][0m - loss: 7.97e-06, learning_rate: 2.16e-05, global_step: 7140, interval_runtime: 1.2284, interval_samples_per_second: 6.512, interval_steps_per_second: 8.141, epoch: 28.0[0m
[32m[2022-08-31 18:17:00,201] [    INFO][0m - loss: 1.049e-05, learning_rate: 2.1588235294117646e-05, global_step: 7150, interval_runtime: 1.4144, interval_samples_per_second: 5.656, interval_steps_per_second: 7.07, epoch: 28.0392[0m
[32m[2022-08-31 18:17:01,500] [    INFO][0m - loss: 1.089e-05, learning_rate: 2.1576470588235295e-05, global_step: 7160, interval_runtime: 1.2989, interval_samples_per_second: 6.159, interval_steps_per_second: 7.699, epoch: 28.0784[0m
[32m[2022-08-31 18:17:02,794] [    INFO][0m - loss: 1.988e-05, learning_rate: 2.1564705882352945e-05, global_step: 7170, interval_runtime: 1.2944, interval_samples_per_second: 6.18, interval_steps_per_second: 7.726, epoch: 28.1176[0m
[32m[2022-08-31 18:17:04,083] [    INFO][0m - loss: 1.212e-05, learning_rate: 2.1552941176470587e-05, global_step: 7180, interval_runtime: 1.2884, interval_samples_per_second: 6.209, interval_steps_per_second: 7.762, epoch: 28.1569[0m
[32m[2022-08-31 18:17:05,386] [    INFO][0m - loss: 7.71e-06, learning_rate: 2.1541176470588237e-05, global_step: 7190, interval_runtime: 1.3031, interval_samples_per_second: 6.139, interval_steps_per_second: 7.674, epoch: 28.1961[0m
[32m[2022-08-31 18:17:06,687] [    INFO][0m - loss: 8.93e-06, learning_rate: 2.1529411764705882e-05, global_step: 7200, interval_runtime: 1.3012, interval_samples_per_second: 6.148, interval_steps_per_second: 7.685, epoch: 28.2353[0m
[32m[2022-08-31 18:17:06,688] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:17:06,688] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 18:17:06,688] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:17:06,688] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:17:06,688] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 18:17:20,655] [    INFO][0m - eval_loss: 2.9600772857666016, eval_accuracy: 0.6189555125725339, eval_runtime: 13.9669, eval_samples_per_second: 148.064, eval_steps_per_second: 4.654, epoch: 28.2353[0m
[32m[2022-08-31 18:17:20,656] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-7200[0m
[32m[2022-08-31 18:17:20,656] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:17:21,778] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-7200/tokenizer_config.json[0m
[32m[2022-08-31 18:17:21,778] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-7200/special_tokens_map.json[0m
[32m[2022-08-31 18:17:25,653] [    INFO][0m - loss: 1.175e-05, learning_rate: 2.1517647058823532e-05, global_step: 7210, interval_runtime: 18.9664, interval_samples_per_second: 0.422, interval_steps_per_second: 0.527, epoch: 28.2745[0m
[32m[2022-08-31 18:17:26,935] [    INFO][0m - loss: 1.108e-05, learning_rate: 2.1505882352941178e-05, global_step: 7220, interval_runtime: 1.2813, interval_samples_per_second: 6.244, interval_steps_per_second: 7.805, epoch: 28.3137[0m
[32m[2022-08-31 18:17:28,223] [    INFO][0m - loss: 1.427e-05, learning_rate: 2.1494117647058824e-05, global_step: 7230, interval_runtime: 1.2884, interval_samples_per_second: 6.209, interval_steps_per_second: 7.762, epoch: 28.3529[0m
[32m[2022-08-31 18:17:29,510] [    INFO][0m - loss: 7.1e-06, learning_rate: 2.148235294117647e-05, global_step: 7240, interval_runtime: 1.2868, interval_samples_per_second: 6.217, interval_steps_per_second: 7.771, epoch: 28.3922[0m
[32m[2022-08-31 18:17:30,800] [    INFO][0m - loss: 8.94e-06, learning_rate: 2.147058823529412e-05, global_step: 7250, interval_runtime: 1.2902, interval_samples_per_second: 6.201, interval_steps_per_second: 7.751, epoch: 28.4314[0m
[32m[2022-08-31 18:17:32,098] [    INFO][0m - loss: 7.72e-06, learning_rate: 2.1458823529411765e-05, global_step: 7260, interval_runtime: 1.2973, interval_samples_per_second: 6.166, interval_steps_per_second: 7.708, epoch: 28.4706[0m
[32m[2022-08-31 18:17:33,406] [    INFO][0m - loss: 7.15e-06, learning_rate: 2.144705882352941e-05, global_step: 7270, interval_runtime: 1.3083, interval_samples_per_second: 6.115, interval_steps_per_second: 7.643, epoch: 28.5098[0m
[32m[2022-08-31 18:17:34,706] [    INFO][0m - loss: 9.18e-06, learning_rate: 2.1435294117647057e-05, global_step: 7280, interval_runtime: 1.2995, interval_samples_per_second: 6.156, interval_steps_per_second: 7.696, epoch: 28.549[0m
[32m[2022-08-31 18:17:36,026] [    INFO][0m - loss: 1.069e-05, learning_rate: 2.1423529411764706e-05, global_step: 7290, interval_runtime: 1.3205, interval_samples_per_second: 6.058, interval_steps_per_second: 7.573, epoch: 28.5882[0m
[32m[2022-08-31 18:17:37,322] [    INFO][0m - loss: 1.02e-05, learning_rate: 2.1411764705882352e-05, global_step: 7300, interval_runtime: 1.2959, interval_samples_per_second: 6.173, interval_steps_per_second: 7.717, epoch: 28.6275[0m
[32m[2022-08-31 18:17:37,322] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:17:37,323] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 18:17:37,323] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:17:37,323] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:17:37,323] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 18:17:51,386] [    INFO][0m - eval_loss: 2.958662509918213, eval_accuracy: 0.617504835589942, eval_runtime: 14.0622, eval_samples_per_second: 147.061, eval_steps_per_second: 4.622, epoch: 28.6275[0m
[32m[2022-08-31 18:17:51,386] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-7300[0m
[32m[2022-08-31 18:17:51,386] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:17:52,393] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-7300/tokenizer_config.json[0m
[32m[2022-08-31 18:17:52,393] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-7300/special_tokens_map.json[0m
[32m[2022-08-31 18:17:56,207] [    INFO][0m - loss: 9.86e-06, learning_rate: 2.1400000000000002e-05, global_step: 7310, interval_runtime: 18.8848, interval_samples_per_second: 0.424, interval_steps_per_second: 0.53, epoch: 28.6667[0m
[32m[2022-08-31 18:17:57,498] [    INFO][0m - loss: 7.67e-06, learning_rate: 2.1388235294117648e-05, global_step: 7320, interval_runtime: 1.2918, interval_samples_per_second: 6.193, interval_steps_per_second: 7.741, epoch: 28.7059[0m
[32m[2022-08-31 18:17:58,788] [    INFO][0m - loss: 5.85e-06, learning_rate: 2.1376470588235294e-05, global_step: 7330, interval_runtime: 1.2893, interval_samples_per_second: 6.205, interval_steps_per_second: 7.756, epoch: 28.7451[0m
[32m[2022-08-31 18:18:00,096] [    INFO][0m - loss: 7.48e-06, learning_rate: 2.1364705882352943e-05, global_step: 7340, interval_runtime: 1.3081, interval_samples_per_second: 6.116, interval_steps_per_second: 7.645, epoch: 28.7843[0m
[32m[2022-08-31 18:18:01,393] [    INFO][0m - loss: 6.66e-06, learning_rate: 2.135294117647059e-05, global_step: 7350, interval_runtime: 1.2971, interval_samples_per_second: 6.168, interval_steps_per_second: 7.71, epoch: 28.8235[0m
[32m[2022-08-31 18:18:02,684] [    INFO][0m - loss: 6.58e-06, learning_rate: 2.134117647058824e-05, global_step: 7360, interval_runtime: 1.2915, interval_samples_per_second: 6.195, interval_steps_per_second: 7.743, epoch: 28.8627[0m
[32m[2022-08-31 18:18:03,992] [    INFO][0m - loss: 1.168e-05, learning_rate: 2.132941176470588e-05, global_step: 7370, interval_runtime: 1.3074, interval_samples_per_second: 6.119, interval_steps_per_second: 7.649, epoch: 28.902[0m
[32m[2022-08-31 18:18:05,275] [    INFO][0m - loss: 8.58e-06, learning_rate: 2.131764705882353e-05, global_step: 7380, interval_runtime: 1.2835, interval_samples_per_second: 6.233, interval_steps_per_second: 7.791, epoch: 28.9412[0m
[32m[2022-08-31 18:18:06,605] [    INFO][0m - loss: 9.01e-06, learning_rate: 2.1305882352941176e-05, global_step: 7390, interval_runtime: 1.3292, interval_samples_per_second: 6.019, interval_steps_per_second: 7.523, epoch: 28.9804[0m
[32m[2022-08-31 18:18:07,903] [    INFO][0m - loss: 9.85e-06, learning_rate: 2.1294117647058826e-05, global_step: 7400, interval_runtime: 1.2983, interval_samples_per_second: 6.162, interval_steps_per_second: 7.703, epoch: 29.0196[0m
[32m[2022-08-31 18:18:07,903] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:18:07,903] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 18:18:07,904] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:18:07,904] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:18:07,904] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 18:18:21,955] [    INFO][0m - eval_loss: 2.9628090858459473, eval_accuracy: 0.6170212765957447, eval_runtime: 14.0509, eval_samples_per_second: 147.18, eval_steps_per_second: 4.626, epoch: 29.0196[0m
[32m[2022-08-31 18:18:21,956] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-7400[0m
[32m[2022-08-31 18:18:21,956] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:18:22,880] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-7400/tokenizer_config.json[0m
[32m[2022-08-31 18:18:22,880] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-7400/special_tokens_map.json[0m
[32m[2022-08-31 18:18:26,454] [    INFO][0m - loss: 8.3e-06, learning_rate: 2.1282352941176472e-05, global_step: 7410, interval_runtime: 18.551, interval_samples_per_second: 0.431, interval_steps_per_second: 0.539, epoch: 29.0588[0m
[32m[2022-08-31 18:18:27,744] [    INFO][0m - loss: 7.47e-06, learning_rate: 2.1270588235294118e-05, global_step: 7420, interval_runtime: 1.2904, interval_samples_per_second: 6.2, interval_steps_per_second: 7.749, epoch: 29.098[0m
[32m[2022-08-31 18:18:29,038] [    INFO][0m - loss: 8.29e-06, learning_rate: 2.1258823529411764e-05, global_step: 7430, interval_runtime: 1.2939, interval_samples_per_second: 6.183, interval_steps_per_second: 7.729, epoch: 29.1373[0m
[32m[2022-08-31 18:18:30,338] [    INFO][0m - loss: 6.91e-06, learning_rate: 2.1247058823529413e-05, global_step: 7440, interval_runtime: 1.2995, interval_samples_per_second: 6.156, interval_steps_per_second: 7.695, epoch: 29.1765[0m
[32m[2022-08-31 18:18:31,630] [    INFO][0m - loss: 8.43e-06, learning_rate: 2.123529411764706e-05, global_step: 7450, interval_runtime: 1.2919, interval_samples_per_second: 6.193, interval_steps_per_second: 7.741, epoch: 29.2157[0m
[32m[2022-08-31 18:18:32,920] [    INFO][0m - loss: 6.42e-06, learning_rate: 2.1223529411764705e-05, global_step: 7460, interval_runtime: 1.2898, interval_samples_per_second: 6.203, interval_steps_per_second: 7.753, epoch: 29.2549[0m
[32m[2022-08-31 18:18:34,211] [    INFO][0m - loss: 7.19e-06, learning_rate: 2.1211764705882354e-05, global_step: 7470, interval_runtime: 1.2919, interval_samples_per_second: 6.192, interval_steps_per_second: 7.74, epoch: 29.2941[0m
[32m[2022-08-31 18:18:35,506] [    INFO][0m - loss: 5.83e-06, learning_rate: 2.12e-05, global_step: 7480, interval_runtime: 1.2946, interval_samples_per_second: 6.179, interval_steps_per_second: 7.724, epoch: 29.3333[0m
[32m[2022-08-31 18:18:36,792] [    INFO][0m - loss: 1.82e-05, learning_rate: 2.118823529411765e-05, global_step: 7490, interval_runtime: 1.2864, interval_samples_per_second: 6.219, interval_steps_per_second: 7.774, epoch: 29.3725[0m
[32m[2022-08-31 18:18:38,080] [    INFO][0m - loss: 7.2e-06, learning_rate: 2.1176470588235296e-05, global_step: 7500, interval_runtime: 1.2876, interval_samples_per_second: 6.213, interval_steps_per_second: 7.766, epoch: 29.4118[0m
[32m[2022-08-31 18:18:38,081] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:18:38,081] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 18:18:38,081] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:18:38,081] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:18:38,081] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 18:18:52,131] [    INFO][0m - eval_loss: 2.9746196269989014, eval_accuracy: 0.6194390715667312, eval_runtime: 14.0493, eval_samples_per_second: 147.196, eval_steps_per_second: 4.627, epoch: 29.4118[0m
[32m[2022-08-31 18:18:52,131] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-7500[0m
[32m[2022-08-31 18:18:52,131] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:18:53,143] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-7500/tokenizer_config.json[0m
[32m[2022-08-31 18:18:53,508] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-7500/special_tokens_map.json[0m
[32m[2022-08-31 18:18:57,127] [    INFO][0m - loss: 7.94e-06, learning_rate: 2.116470588235294e-05, global_step: 7510, interval_runtime: 19.0465, interval_samples_per_second: 0.42, interval_steps_per_second: 0.525, epoch: 29.451[0m
[32m[2022-08-31 18:18:58,425] [    INFO][0m - loss: 1.033e-05, learning_rate: 2.1152941176470588e-05, global_step: 7520, interval_runtime: 1.2985, interval_samples_per_second: 6.161, interval_steps_per_second: 7.701, epoch: 29.4902[0m
[32m[2022-08-31 18:18:59,720] [    INFO][0m - loss: 6.01e-06, learning_rate: 2.1141176470588237e-05, global_step: 7530, interval_runtime: 1.2955, interval_samples_per_second: 6.175, interval_steps_per_second: 7.719, epoch: 29.5294[0m
[32m[2022-08-31 18:19:01,011] [    INFO][0m - loss: 7.71e-06, learning_rate: 2.1129411764705883e-05, global_step: 7540, interval_runtime: 1.2903, interval_samples_per_second: 6.2, interval_steps_per_second: 7.75, epoch: 29.5686[0m
[32m[2022-08-31 18:19:02,305] [    INFO][0m - loss: 8e-06, learning_rate: 2.1117647058823532e-05, global_step: 7550, interval_runtime: 1.2939, interval_samples_per_second: 6.183, interval_steps_per_second: 7.729, epoch: 29.6078[0m
[32m[2022-08-31 18:19:03,591] [    INFO][0m - loss: 7.63e-06, learning_rate: 2.1105882352941175e-05, global_step: 7560, interval_runtime: 1.2863, interval_samples_per_second: 6.219, interval_steps_per_second: 7.774, epoch: 29.6471[0m
[32m[2022-08-31 18:19:04,887] [    INFO][0m - loss: 6.03e-06, learning_rate: 2.1094117647058824e-05, global_step: 7570, interval_runtime: 1.2962, interval_samples_per_second: 6.172, interval_steps_per_second: 7.715, epoch: 29.6863[0m
[32m[2022-08-31 18:19:06,202] [    INFO][0m - loss: 1.407e-05, learning_rate: 2.108235294117647e-05, global_step: 7580, interval_runtime: 1.3148, interval_samples_per_second: 6.084, interval_steps_per_second: 7.606, epoch: 29.7255[0m
[32m[2022-08-31 18:19:07,504] [    INFO][0m - loss: 7.33e-06, learning_rate: 2.107058823529412e-05, global_step: 7590, interval_runtime: 1.3024, interval_samples_per_second: 6.142, interval_steps_per_second: 7.678, epoch: 29.7647[0m
[32m[2022-08-31 18:19:08,796] [    INFO][0m - loss: 9.03e-06, learning_rate: 2.1058823529411762e-05, global_step: 7600, interval_runtime: 1.2918, interval_samples_per_second: 6.193, interval_steps_per_second: 7.741, epoch: 29.8039[0m
[32m[2022-08-31 18:19:08,797] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:19:08,797] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 18:19:08,798] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:19:08,798] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:19:08,798] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 18:19:22,955] [    INFO][0m - eval_loss: 2.985055446624756, eval_accuracy: 0.620889748549323, eval_runtime: 14.1564, eval_samples_per_second: 146.082, eval_steps_per_second: 4.592, epoch: 29.8039[0m
[32m[2022-08-31 18:19:22,955] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-7600[0m
[32m[2022-08-31 18:19:22,955] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:19:24,038] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-7600/tokenizer_config.json[0m
[32m[2022-08-31 18:19:24,038] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-7600/special_tokens_map.json[0m
[32m[2022-08-31 18:19:27,934] [    INFO][0m - loss: 7.46e-06, learning_rate: 2.104705882352941e-05, global_step: 7610, interval_runtime: 19.1373, interval_samples_per_second: 0.418, interval_steps_per_second: 0.523, epoch: 29.8431[0m
[32m[2022-08-31 18:19:29,214] [    INFO][0m - loss: 8.92e-06, learning_rate: 2.103529411764706e-05, global_step: 7620, interval_runtime: 1.28, interval_samples_per_second: 6.25, interval_steps_per_second: 7.812, epoch: 29.8824[0m
[32m[2022-08-31 18:19:30,500] [    INFO][0m - loss: 7.45e-06, learning_rate: 2.1023529411764707e-05, global_step: 7630, interval_runtime: 1.2867, interval_samples_per_second: 6.218, interval_steps_per_second: 7.772, epoch: 29.9216[0m
[32m[2022-08-31 18:19:31,786] [    INFO][0m - loss: 9.56e-06, learning_rate: 2.1011764705882356e-05, global_step: 7640, interval_runtime: 1.2856, interval_samples_per_second: 6.223, interval_steps_per_second: 7.779, epoch: 29.9608[0m
[32m[2022-08-31 18:19:33,010] [    INFO][0m - loss: 8.38e-06, learning_rate: 2.1e-05, global_step: 7650, interval_runtime: 1.2244, interval_samples_per_second: 6.534, interval_steps_per_second: 8.167, epoch: 30.0[0m
[32m[2022-08-31 18:19:34,437] [    INFO][0m - loss: 5.17e-06, learning_rate: 2.0988235294117648e-05, global_step: 7660, interval_runtime: 1.4263, interval_samples_per_second: 5.609, interval_steps_per_second: 7.011, epoch: 30.0392[0m
[32m[2022-08-31 18:19:35,727] [    INFO][0m - loss: 7.82e-06, learning_rate: 2.0976470588235294e-05, global_step: 7670, interval_runtime: 1.2908, interval_samples_per_second: 6.198, interval_steps_per_second: 7.747, epoch: 30.0784[0m
[32m[2022-08-31 18:19:37,019] [    INFO][0m - loss: 7.23e-06, learning_rate: 2.0964705882352944e-05, global_step: 7680, interval_runtime: 1.292, interval_samples_per_second: 6.192, interval_steps_per_second: 7.74, epoch: 30.1176[0m
[32m[2022-08-31 18:19:38,307] [    INFO][0m - loss: 7.37e-06, learning_rate: 2.095294117647059e-05, global_step: 7690, interval_runtime: 1.2879, interval_samples_per_second: 6.212, interval_steps_per_second: 7.764, epoch: 30.1569[0m
[32m[2022-08-31 18:19:39,610] [    INFO][0m - loss: 5.39e-06, learning_rate: 2.0941176470588235e-05, global_step: 7700, interval_runtime: 1.302, interval_samples_per_second: 6.144, interval_steps_per_second: 7.68, epoch: 30.1961[0m
[32m[2022-08-31 18:19:39,610] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:19:39,611] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 18:19:39,611] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:19:39,611] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:19:39,611] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 18:19:53,678] [    INFO][0m - eval_loss: 2.9899938106536865, eval_accuracy: 0.6223404255319149, eval_runtime: 14.0672, eval_samples_per_second: 147.009, eval_steps_per_second: 4.621, epoch: 30.1961[0m
[32m[2022-08-31 18:19:53,679] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-7700[0m
[32m[2022-08-31 18:19:53,679] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:19:54,774] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-7700/tokenizer_config.json[0m
[32m[2022-08-31 18:19:54,774] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-7700/special_tokens_map.json[0m
[32m[2022-08-31 18:19:58,488] [    INFO][0m - loss: 9.74e-06, learning_rate: 2.092941176470588e-05, global_step: 7710, interval_runtime: 18.8785, interval_samples_per_second: 0.424, interval_steps_per_second: 0.53, epoch: 30.2353[0m
[32m[2022-08-31 18:20:01,707] [    INFO][0m - loss: 1.305e-05, learning_rate: 2.091764705882353e-05, global_step: 7720, interval_runtime: 1.2888, interval_samples_per_second: 6.207, interval_steps_per_second: 7.759, epoch: 30.2745[0m
[32m[2022-08-31 18:20:02,991] [    INFO][0m - loss: 6.23e-06, learning_rate: 2.0905882352941177e-05, global_step: 7730, interval_runtime: 3.2146, interval_samples_per_second: 2.489, interval_steps_per_second: 3.111, epoch: 30.3137[0m
[32m[2022-08-31 18:20:04,283] [    INFO][0m - loss: 8.78e-06, learning_rate: 2.0894117647058823e-05, global_step: 7740, interval_runtime: 1.2914, interval_samples_per_second: 6.195, interval_steps_per_second: 7.744, epoch: 30.3529[0m
[32m[2022-08-31 18:20:05,570] [    INFO][0m - loss: 8.45e-06, learning_rate: 2.088235294117647e-05, global_step: 7750, interval_runtime: 1.2876, interval_samples_per_second: 6.213, interval_steps_per_second: 7.767, epoch: 30.3922[0m
[32m[2022-08-31 18:20:06,856] [    INFO][0m - loss: 7.07e-06, learning_rate: 2.0870588235294118e-05, global_step: 7760, interval_runtime: 1.2849, interval_samples_per_second: 6.226, interval_steps_per_second: 7.783, epoch: 30.4314[0m
[32m[2022-08-31 18:20:08,150] [    INFO][0m - loss: 6.3e-06, learning_rate: 2.0858823529411764e-05, global_step: 7770, interval_runtime: 1.2949, interval_samples_per_second: 6.178, interval_steps_per_second: 7.723, epoch: 30.4706[0m
[32m[2022-08-31 18:20:09,467] [    INFO][0m - loss: 8.07e-06, learning_rate: 2.0847058823529413e-05, global_step: 7780, interval_runtime: 1.3166, interval_samples_per_second: 6.076, interval_steps_per_second: 7.595, epoch: 30.5098[0m
[32m[2022-08-31 18:20:10,761] [    INFO][0m - loss: 1.056e-05, learning_rate: 2.083529411764706e-05, global_step: 7790, interval_runtime: 1.2944, interval_samples_per_second: 6.18, interval_steps_per_second: 7.725, epoch: 30.549[0m
[32m[2022-08-31 18:20:12,051] [    INFO][0m - loss: 6.44e-06, learning_rate: 2.0823529411764705e-05, global_step: 7800, interval_runtime: 1.29, interval_samples_per_second: 6.202, interval_steps_per_second: 7.752, epoch: 30.5882[0m
[32m[2022-08-31 18:20:12,052] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:20:12,052] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 18:20:12,052] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:20:12,052] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:20:12,052] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 18:20:26,184] [    INFO][0m - eval_loss: 2.999523878097534, eval_accuracy: 0.6223404255319149, eval_runtime: 14.1315, eval_samples_per_second: 146.34, eval_steps_per_second: 4.6, epoch: 30.5882[0m
[32m[2022-08-31 18:20:26,185] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-7800[0m
[32m[2022-08-31 18:20:26,185] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:20:27,428] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-7800/tokenizer_config.json[0m
[32m[2022-08-31 18:20:27,429] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-7800/special_tokens_map.json[0m
[32m[2022-08-31 18:20:29,859] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-31 18:20:29,859] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-5800 (score: 0.6237911025145068).[0m
[32m[2022-08-31 18:20:30,467] [    INFO][0m - train_runtime: 2468.4948, train_samples_per_second: 82.479, train_steps_per_second: 10.33, train_loss: 0.1339599692580566, epoch: 30.5882[0m
[32m[2022-08-31 18:20:30,467] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-31 18:20:30,468] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:20:33,417] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-31 18:20:33,418] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-31 18:20:33,420] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-31 18:20:33,420] [    INFO][0m -   epoch                    =    30.5882[0m
[32m[2022-08-31 18:20:33,420] [    INFO][0m -   train_loss               =      0.134[0m
[32m[2022-08-31 18:20:33,420] [    INFO][0m -   train_runtime            = 0:41:08.49[0m
[32m[2022-08-31 18:20:33,420] [    INFO][0m -   train_samples_per_second =     82.479[0m
[32m[2022-08-31 18:20:33,421] [    INFO][0m -   train_steps_per_second   =      10.33[0m
[32m[2022-08-31 18:20:33,455] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-31 18:20:33,456] [    INFO][0m -   Num examples = 1784[0m
[32m[2022-08-31 18:20:33,456] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:20:33,456] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:20:33,456] [    INFO][0m -   Total prediction steps = 56[0m
[32m[2022-08-31 18:20:45,589] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-31 18:20:45,589] [    INFO][0m -   test_accuracy           =     0.6401[0m
[32m[2022-08-31 18:20:45,590] [    INFO][0m -   test_loss               =      2.774[0m
[32m[2022-08-31 18:20:45,590] [    INFO][0m -   test_runtime            = 0:00:12.13[0m
[32m[2022-08-31 18:20:45,590] [    INFO][0m -   test_samples_per_second =    147.033[0m
[32m[2022-08-31 18:20:45,590] [    INFO][0m -   test_steps_per_second   =      4.615[0m
[32m[2022-08-31 18:20:45,590] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-31 18:20:45,590] [    INFO][0m -   Num examples = 2999[0m
[32m[2022-08-31 18:20:45,590] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:20:45,590] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:20:45,591] [    INFO][0m -   Total prediction steps = 94[0m
[32m[2022-08-31 18:21:11,941] [    INFO][0m - Predictions for csldcp saved to ./fewclue_submit_examples.[0m
run.sh: line 65: --model_name_or_path: command not found
 
==========
tnews
==========
 
[33m[2022-08-31 18:21:16,394] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-31 18:21:16,394] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-31 18:21:16,394] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 18:21:16,394] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-31 18:21:16,394] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 18:21:16,394] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-31 18:21:16,395] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-31 18:21:16,395] [    INFO][0m - early_stop_patience           :20[0m
[32m[2022-08-31 18:21:16,395] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-31 18:21:16,395] [    INFO][0m - model_name_or_path            :roformer_v2_chinese_char_base[0m
[32m[2022-08-31 18:21:16,395] [    INFO][0m - [0m
[32m[2022-08-31 18:21:16,395] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 18:21:16,395] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-31 18:21:16,395] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 18:21:16,395] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-31 18:21:16,395] [    INFO][0m - prompt                        :{'hard':'ä¸‹è¾¹æ’­æŠ¥ä¸€åˆ™'}{'mask'}{'mask'}{'hard':'æ–°é—»ï¼š'}{'text':'text_a'}[0m
[32m[2022-08-31 18:21:16,395] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-31 18:21:16,395] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-31 18:21:16,395] [    INFO][0m - task_name                     :tnews[0m
[32m[2022-08-31 18:21:16,396] [    INFO][0m - [0m
[32m[2022-08-31 18:21:16,396] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/roformer_v2_chinese_char_base/model_state.pdparams[0m
W0831 18:21:16.397935 62729 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0831 18:21:16.402375 62729 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-31 18:21:19,322] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/roformer_v2_chinese_char_base/vocab.txt[0m
[32m[2022-08-31 18:21:19,333] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/roformer_v2_chinese_char_base/tokenizer_config.json[0m
[32m[2022-08-31 18:21:19,334] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/roformer_v2_chinese_char_base/special_tokens_map.json[0m
[32m[2022-08-31 18:21:19,335] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': 'ä¸‹è¾¹æ’­æŠ¥ä¸€åˆ™'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'æ–°é—»ï¼š'}, {'add_prefix_space': '', 'text': 'text_a'}][0m
2022-08-31 18:21:19,344 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-31 18:21:19,476] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 18:21:19,476] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-31 18:21:19,476] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 18:21:19,476] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-31 18:21:19,477] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-31 18:21:19,477] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-31 18:21:19,477] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-31 18:21:19,477] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-31 18:21:19,477] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-31 18:21:19,477] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-31 18:21:19,477] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-31 18:21:19,477] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-31 18:21:19,477] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-31 18:21:19,477] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-31 18:21:19,477] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-31 18:21:19,477] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-31 18:21:19,478] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-31 18:21:19,478] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-31 18:21:19,478] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-31 18:21:19,478] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-31 18:21:19,478] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-31 18:21:19,478] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-31 18:21:19,478] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-31 18:21:19,478] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-31 18:21:19,478] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-31 18:21:19,478] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-31 18:21:19,478] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-31 18:21:19,478] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-31 18:21:19,478] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-31 18:21:19,479] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-31 18:21:19,479] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-31 18:21:19,479] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-31 18:21:19,479] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-31 18:21:19,479] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-31 18:21:19,479] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-31 18:21:19,479] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-31 18:21:19,479] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-31 18:21:19,479] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug31_18-21-16_instance-3bwob41y-01[0m
[32m[2022-08-31 18:21:19,479] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-31 18:21:19,479] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-31 18:21:19,479] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-31 18:21:19,479] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-31 18:21:19,479] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-31 18:21:19,480] [    INFO][0m - max_seq_length                :64[0m
[32m[2022-08-31 18:21:19,480] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-31 18:21:19,480] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-31 18:21:19,480] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-31 18:21:19,480] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-31 18:21:19,480] [    INFO][0m - num_train_epochs              :100.0[0m
[32m[2022-08-31 18:21:19,480] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-31 18:21:19,480] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-31 18:21:19,480] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-31 18:21:19,480] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-31 18:21:19,480] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-31 18:21:19,480] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-31 18:21:19,480] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-31 18:21:19,481] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-31 18:21:19,481] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-31 18:21:19,481] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-31 18:21:19,481] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-31 18:21:19,481] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-31 18:21:19,481] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-31 18:21:19,481] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-31 18:21:19,481] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-31 18:21:19,481] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-31 18:21:19,481] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-31 18:21:19,481] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-31 18:21:19,481] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-31 18:21:19,481] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-31 18:21:19,481] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-31 18:21:19,482] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-31 18:21:19,482] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-31 18:21:19,482] [    INFO][0m - seed                          :42[0m
[32m[2022-08-31 18:21:19,482] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-31 18:21:19,482] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-31 18:21:19,482] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-31 18:21:19,482] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-31 18:21:19,482] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-31 18:21:19,482] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-31 18:21:19,482] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-31 18:21:19,482] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-31 18:21:19,482] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-31 18:21:19,482] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-31 18:21:19,482] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-31 18:21:19,483] [    INFO][0m - [0m
[32m[2022-08-31 18:21:19,484] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-31 18:21:19,484] [    INFO][0m -   Num examples = 1185[0m
[32m[2022-08-31 18:21:19,484] [    INFO][0m -   Num Epochs = 100[0m
[32m[2022-08-31 18:21:19,484] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-31 18:21:19,485] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-31 18:21:19,485] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-31 18:21:19,485] [    INFO][0m -   Total optimization steps = 14900.0[0m
[32m[2022-08-31 18:21:19,485] [    INFO][0m -   Total num train samples = 118500[0m
[32m[2022-08-31 18:21:20,895] [    INFO][0m - loss: 2.12392769, learning_rate: 2.9979865771812083e-05, global_step: 10, interval_runtime: 1.4089, interval_samples_per_second: 5.678, interval_steps_per_second: 7.098, epoch: 0.0671[0m
[32m[2022-08-31 18:21:21,552] [    INFO][0m - loss: 1.82101898, learning_rate: 2.9959731543624162e-05, global_step: 20, interval_runtime: 0.6577, interval_samples_per_second: 12.164, interval_steps_per_second: 15.205, epoch: 0.1342[0m
[32m[2022-08-31 18:21:22,214] [    INFO][0m - loss: 1.51781988, learning_rate: 2.9939597315436244e-05, global_step: 30, interval_runtime: 0.6618, interval_samples_per_second: 12.088, interval_steps_per_second: 15.11, epoch: 0.2013[0m
[32m[2022-08-31 18:21:22,895] [    INFO][0m - loss: 1.67303314, learning_rate: 2.9919463087248323e-05, global_step: 40, interval_runtime: 0.6814, interval_samples_per_second: 11.741, interval_steps_per_second: 14.676, epoch: 0.2685[0m
[32m[2022-08-31 18:21:23,606] [    INFO][0m - loss: 1.49647789, learning_rate: 2.9899328859060402e-05, global_step: 50, interval_runtime: 0.7104, interval_samples_per_second: 11.261, interval_steps_per_second: 14.076, epoch: 0.3356[0m
[32m[2022-08-31 18:21:24,281] [    INFO][0m - loss: 1.33681307, learning_rate: 2.9879194630872484e-05, global_step: 60, interval_runtime: 0.6756, interval_samples_per_second: 11.841, interval_steps_per_second: 14.801, epoch: 0.4027[0m
[32m[2022-08-31 18:21:24,940] [    INFO][0m - loss: 1.31850605, learning_rate: 2.9859060402684563e-05, global_step: 70, interval_runtime: 0.6587, interval_samples_per_second: 12.145, interval_steps_per_second: 15.181, epoch: 0.4698[0m
[32m[2022-08-31 18:21:25,596] [    INFO][0m - loss: 1.50947142, learning_rate: 2.9838926174496645e-05, global_step: 80, interval_runtime: 0.656, interval_samples_per_second: 12.195, interval_steps_per_second: 15.244, epoch: 0.5369[0m
[32m[2022-08-31 18:21:26,257] [    INFO][0m - loss: 1.74253712, learning_rate: 2.9818791946308724e-05, global_step: 90, interval_runtime: 0.6604, interval_samples_per_second: 12.114, interval_steps_per_second: 15.143, epoch: 0.604[0m
[32m[2022-08-31 18:21:26,909] [    INFO][0m - loss: 1.44411964, learning_rate: 2.9798657718120806e-05, global_step: 100, interval_runtime: 0.6524, interval_samples_per_second: 12.263, interval_steps_per_second: 15.329, epoch: 0.6711[0m
[32m[2022-08-31 18:21:26,909] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:21:26,909] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-08-31 18:21:26,909] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:21:26,909] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:21:26,909] [    INFO][0m -   Total prediction steps = 35[0m
[32m[2022-08-31 18:21:29,083] [    INFO][0m - eval_loss: 1.3779585361480713, eval_accuracy: 0.5573770491803278, eval_runtime: 2.1734, eval_samples_per_second: 505.208, eval_steps_per_second: 16.104, epoch: 0.6711[0m
[32m[2022-08-31 18:21:29,084] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-31 18:21:29,084] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:21:30,996] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-31 18:21:30,996] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-31 18:21:35,770] [    INFO][0m - loss: 1.65509033, learning_rate: 2.977852348993289e-05, global_step: 110, interval_runtime: 8.8614, interval_samples_per_second: 0.903, interval_steps_per_second: 1.128, epoch: 0.7383[0m
[32m[2022-08-31 18:21:36,439] [    INFO][0m - loss: 1.36741848, learning_rate: 2.9758389261744967e-05, global_step: 120, interval_runtime: 0.6685, interval_samples_per_second: 11.967, interval_steps_per_second: 14.958, epoch: 0.8054[0m
[32m[2022-08-31 18:21:37,085] [    INFO][0m - loss: 1.49742184, learning_rate: 2.973825503355705e-05, global_step: 130, interval_runtime: 0.6464, interval_samples_per_second: 12.377, interval_steps_per_second: 15.471, epoch: 0.8725[0m
[32m[2022-08-31 18:21:37,726] [    INFO][0m - loss: 1.77965622, learning_rate: 2.9718120805369125e-05, global_step: 140, interval_runtime: 0.6407, interval_samples_per_second: 12.486, interval_steps_per_second: 15.607, epoch: 0.9396[0m
[32m[2022-08-31 18:21:38,368] [    INFO][0m - loss: 1.3912055, learning_rate: 2.9697986577181207e-05, global_step: 150, interval_runtime: 0.6419, interval_samples_per_second: 12.464, interval_steps_per_second: 15.58, epoch: 1.0067[0m
[32m[2022-08-31 18:21:39,011] [    INFO][0m - loss: 0.97048235, learning_rate: 2.967785234899329e-05, global_step: 160, interval_runtime: 0.6434, interval_samples_per_second: 12.433, interval_steps_per_second: 15.542, epoch: 1.0738[0m
[32m[2022-08-31 18:21:39,687] [    INFO][0m - loss: 0.84629898, learning_rate: 2.965771812080537e-05, global_step: 170, interval_runtime: 0.6757, interval_samples_per_second: 11.839, interval_steps_per_second: 14.799, epoch: 1.1409[0m
[32m[2022-08-31 18:21:40,369] [    INFO][0m - loss: 0.78782172, learning_rate: 2.963758389261745e-05, global_step: 180, interval_runtime: 0.6824, interval_samples_per_second: 11.723, interval_steps_per_second: 14.654, epoch: 1.2081[0m
[32m[2022-08-31 18:21:41,008] [    INFO][0m - loss: 1.051443, learning_rate: 2.961744966442953e-05, global_step: 190, interval_runtime: 0.6381, interval_samples_per_second: 12.538, interval_steps_per_second: 15.673, epoch: 1.2752[0m
[32m[2022-08-31 18:21:41,660] [    INFO][0m - loss: 0.67804275, learning_rate: 2.9597315436241612e-05, global_step: 200, interval_runtime: 0.6525, interval_samples_per_second: 12.261, interval_steps_per_second: 15.326, epoch: 1.3423[0m
[32m[2022-08-31 18:21:41,661] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:21:41,661] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-08-31 18:21:41,661] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:21:41,662] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:21:41,662] [    INFO][0m -   Total prediction steps = 35[0m
[32m[2022-08-31 18:21:43,813] [    INFO][0m - eval_loss: 1.582367181777954, eval_accuracy: 0.5391621129326047, eval_runtime: 2.1517, eval_samples_per_second: 510.285, eval_steps_per_second: 16.266, epoch: 1.3423[0m
[32m[2022-08-31 18:21:43,814] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-31 18:21:43,814] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:21:45,875] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-31 18:21:45,875] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-31 18:21:51,155] [    INFO][0m - loss: 1.09818182, learning_rate: 2.9577181208053694e-05, global_step: 210, interval_runtime: 9.4952, interval_samples_per_second: 0.843, interval_steps_per_second: 1.053, epoch: 1.4094[0m
[32m[2022-08-31 18:21:51,845] [    INFO][0m - loss: 0.94761486, learning_rate: 2.9557046979865773e-05, global_step: 220, interval_runtime: 0.6894, interval_samples_per_second: 11.604, interval_steps_per_second: 14.505, epoch: 1.4765[0m
[32m[2022-08-31 18:21:52,557] [    INFO][0m - loss: 0.90131903, learning_rate: 2.9536912751677852e-05, global_step: 230, interval_runtime: 0.7124, interval_samples_per_second: 11.23, interval_steps_per_second: 14.038, epoch: 1.5436[0m
[32m[2022-08-31 18:21:53,217] [    INFO][0m - loss: 1.24175396, learning_rate: 2.951677852348993e-05, global_step: 240, interval_runtime: 0.6597, interval_samples_per_second: 12.127, interval_steps_per_second: 15.159, epoch: 1.6107[0m
[32m[2022-08-31 18:21:53,877] [    INFO][0m - loss: 0.93334093, learning_rate: 2.9496644295302013e-05, global_step: 250, interval_runtime: 0.6606, interval_samples_per_second: 12.111, interval_steps_per_second: 15.139, epoch: 1.6779[0m
[32m[2022-08-31 18:21:54,536] [    INFO][0m - loss: 0.95690203, learning_rate: 2.9476510067114095e-05, global_step: 260, interval_runtime: 0.6588, interval_samples_per_second: 12.143, interval_steps_per_second: 15.178, epoch: 1.745[0m
[32m[2022-08-31 18:21:55,195] [    INFO][0m - loss: 1.06992369, learning_rate: 2.9456375838926174e-05, global_step: 270, interval_runtime: 0.6581, interval_samples_per_second: 12.156, interval_steps_per_second: 15.195, epoch: 1.8121[0m
[32m[2022-08-31 18:21:55,874] [    INFO][0m - loss: 0.82628241, learning_rate: 2.9436241610738256e-05, global_step: 280, interval_runtime: 0.6798, interval_samples_per_second: 11.768, interval_steps_per_second: 14.71, epoch: 1.8792[0m
[32m[2022-08-31 18:21:56,573] [    INFO][0m - loss: 0.90340014, learning_rate: 2.9416107382550335e-05, global_step: 290, interval_runtime: 0.6989, interval_samples_per_second: 11.446, interval_steps_per_second: 14.308, epoch: 1.9463[0m
[32m[2022-08-31 18:21:57,235] [    INFO][0m - loss: 0.97760115, learning_rate: 2.9395973154362418e-05, global_step: 300, interval_runtime: 0.6621, interval_samples_per_second: 12.083, interval_steps_per_second: 15.104, epoch: 2.0134[0m
[32m[2022-08-31 18:21:57,236] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:21:57,236] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-08-31 18:21:57,236] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:21:57,236] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:21:57,236] [    INFO][0m -   Total prediction steps = 35[0m
[32m[2022-08-31 18:21:59,423] [    INFO][0m - eval_loss: 1.6869841814041138, eval_accuracy: 0.5428051001821493, eval_runtime: 2.1868, eval_samples_per_second: 502.11, eval_steps_per_second: 16.005, epoch: 2.0134[0m
[32m[2022-08-31 18:21:59,424] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-31 18:21:59,424] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:22:01,860] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-31 18:22:01,861] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-31 18:22:07,967] [    INFO][0m - loss: 0.5141531, learning_rate: 2.93758389261745e-05, global_step: 310, interval_runtime: 10.7313, interval_samples_per_second: 0.745, interval_steps_per_second: 0.932, epoch: 2.0805[0m
[32m[2022-08-31 18:22:08,662] [    INFO][0m - loss: 0.37290831, learning_rate: 2.935570469798658e-05, global_step: 320, interval_runtime: 0.6955, interval_samples_per_second: 11.503, interval_steps_per_second: 14.379, epoch: 2.1477[0m
[32m[2022-08-31 18:22:09,475] [    INFO][0m - loss: 0.26741242, learning_rate: 2.9335570469798658e-05, global_step: 330, interval_runtime: 0.8127, interval_samples_per_second: 9.843, interval_steps_per_second: 12.304, epoch: 2.2148[0m
[32m[2022-08-31 18:22:10,206] [    INFO][0m - loss: 0.58780174, learning_rate: 2.9315436241610736e-05, global_step: 340, interval_runtime: 0.7317, interval_samples_per_second: 10.934, interval_steps_per_second: 13.668, epoch: 2.2819[0m
[32m[2022-08-31 18:22:10,869] [    INFO][0m - loss: 0.29140923, learning_rate: 2.929530201342282e-05, global_step: 350, interval_runtime: 0.6627, interval_samples_per_second: 12.072, interval_steps_per_second: 15.09, epoch: 2.349[0m
[32m[2022-08-31 18:22:11,546] [    INFO][0m - loss: 0.41773381, learning_rate: 2.92751677852349e-05, global_step: 360, interval_runtime: 0.6767, interval_samples_per_second: 11.823, interval_steps_per_second: 14.778, epoch: 2.4161[0m
[32m[2022-08-31 18:22:12,198] [    INFO][0m - loss: 0.38934009, learning_rate: 2.925503355704698e-05, global_step: 370, interval_runtime: 0.6522, interval_samples_per_second: 12.266, interval_steps_per_second: 15.333, epoch: 2.4832[0m
[32m[2022-08-31 18:22:12,862] [    INFO][0m - loss: 0.57274623, learning_rate: 2.9234899328859062e-05, global_step: 380, interval_runtime: 0.6639, interval_samples_per_second: 12.051, interval_steps_per_second: 15.064, epoch: 2.5503[0m
[32m[2022-08-31 18:22:13,533] [    INFO][0m - loss: 0.42556319, learning_rate: 2.921476510067114e-05, global_step: 390, interval_runtime: 0.6714, interval_samples_per_second: 11.916, interval_steps_per_second: 14.894, epoch: 2.6174[0m
[32m[2022-08-31 18:22:14,196] [    INFO][0m - loss: 0.62055626, learning_rate: 2.9194630872483223e-05, global_step: 400, interval_runtime: 0.6628, interval_samples_per_second: 12.071, interval_steps_per_second: 15.088, epoch: 2.6846[0m
[32m[2022-08-31 18:22:14,197] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:22:14,197] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-08-31 18:22:14,197] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:22:14,197] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:22:14,197] [    INFO][0m -   Total prediction steps = 35[0m
[32m[2022-08-31 18:22:16,324] [    INFO][0m - eval_loss: 2.1891679763793945, eval_accuracy: 0.5109289617486339, eval_runtime: 2.1269, eval_samples_per_second: 516.253, eval_steps_per_second: 16.456, epoch: 2.6846[0m
[32m[2022-08-31 18:22:16,325] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-31 18:22:16,325] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:22:18,268] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-31 18:22:18,268] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-31 18:22:23,742] [    INFO][0m - loss: 0.44321623, learning_rate: 2.9174496644295305e-05, global_step: 410, interval_runtime: 9.5455, interval_samples_per_second: 0.838, interval_steps_per_second: 1.048, epoch: 2.7517[0m
[32m[2022-08-31 18:22:24,542] [    INFO][0m - loss: 0.50994554, learning_rate: 2.915436241610738e-05, global_step: 420, interval_runtime: 0.8002, interval_samples_per_second: 9.998, interval_steps_per_second: 12.497, epoch: 2.8188[0m
[32m[2022-08-31 18:22:25,320] [    INFO][0m - loss: 0.60218449, learning_rate: 2.9134228187919463e-05, global_step: 430, interval_runtime: 0.7779, interval_samples_per_second: 10.284, interval_steps_per_second: 12.855, epoch: 2.8859[0m
[32m[2022-08-31 18:22:26,033] [    INFO][0m - loss: 0.40835948, learning_rate: 2.9114093959731542e-05, global_step: 440, interval_runtime: 0.7132, interval_samples_per_second: 11.217, interval_steps_per_second: 14.021, epoch: 2.953[0m
[32m[2022-08-31 18:22:26,674] [    INFO][0m - loss: 0.31531746, learning_rate: 2.9093959731543624e-05, global_step: 450, interval_runtime: 0.6413, interval_samples_per_second: 12.474, interval_steps_per_second: 15.592, epoch: 3.0201[0m
[32m[2022-08-31 18:22:27,357] [    INFO][0m - loss: 0.13316337, learning_rate: 2.9073825503355706e-05, global_step: 460, interval_runtime: 0.6829, interval_samples_per_second: 11.714, interval_steps_per_second: 14.643, epoch: 3.0872[0m
[32m[2022-08-31 18:22:28,031] [    INFO][0m - loss: 0.08414372, learning_rate: 2.9053691275167785e-05, global_step: 470, interval_runtime: 0.6745, interval_samples_per_second: 11.861, interval_steps_per_second: 14.826, epoch: 3.1544[0m
[32m[2022-08-31 18:22:28,693] [    INFO][0m - loss: 0.15625497, learning_rate: 2.9033557046979868e-05, global_step: 480, interval_runtime: 0.661, interval_samples_per_second: 12.103, interval_steps_per_second: 15.128, epoch: 3.2215[0m
[32m[2022-08-31 18:22:29,341] [    INFO][0m - loss: 0.16771775, learning_rate: 2.9013422818791946e-05, global_step: 490, interval_runtime: 0.6478, interval_samples_per_second: 12.349, interval_steps_per_second: 15.437, epoch: 3.2886[0m
[32m[2022-08-31 18:22:29,958] [    INFO][0m - loss: 0.12039812, learning_rate: 2.899328859060403e-05, global_step: 500, interval_runtime: 0.618, interval_samples_per_second: 12.945, interval_steps_per_second: 16.181, epoch: 3.3557[0m
[32m[2022-08-31 18:22:29,959] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:22:29,959] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-08-31 18:22:29,959] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:22:29,959] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:22:29,959] [    INFO][0m -   Total prediction steps = 35[0m
[32m[2022-08-31 18:22:32,105] [    INFO][0m - eval_loss: 2.7938225269317627, eval_accuracy: 0.5400728597449909, eval_runtime: 2.1454, eval_samples_per_second: 511.786, eval_steps_per_second: 16.314, epoch: 3.3557[0m
[32m[2022-08-31 18:22:32,106] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-08-31 18:22:32,106] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:22:33,988] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-08-31 18:22:33,988] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-08-31 18:22:39,437] [    INFO][0m - loss: 0.24597821, learning_rate: 2.8973154362416108e-05, global_step: 510, interval_runtime: 9.4786, interval_samples_per_second: 0.844, interval_steps_per_second: 1.055, epoch: 3.4228[0m
[32m[2022-08-31 18:22:40,134] [    INFO][0m - loss: 0.2156518, learning_rate: 2.8953020134228186e-05, global_step: 520, interval_runtime: 0.6968, interval_samples_per_second: 11.481, interval_steps_per_second: 14.351, epoch: 3.4899[0m
[32m[2022-08-31 18:22:40,781] [    INFO][0m - loss: 0.13918233, learning_rate: 2.893288590604027e-05, global_step: 530, interval_runtime: 0.6473, interval_samples_per_second: 12.36, interval_steps_per_second: 15.45, epoch: 3.557[0m
[32m[2022-08-31 18:22:41,434] [    INFO][0m - loss: 0.37046058, learning_rate: 2.891275167785235e-05, global_step: 540, interval_runtime: 0.6525, interval_samples_per_second: 12.26, interval_steps_per_second: 15.325, epoch: 3.6242[0m
[32m[2022-08-31 18:22:42,105] [    INFO][0m - loss: 0.53612814, learning_rate: 2.889261744966443e-05, global_step: 550, interval_runtime: 0.6709, interval_samples_per_second: 11.924, interval_steps_per_second: 14.906, epoch: 3.6913[0m
[32m[2022-08-31 18:22:43,018] [    INFO][0m - loss: 0.19051154, learning_rate: 2.8872483221476512e-05, global_step: 560, interval_runtime: 0.6507, interval_samples_per_second: 12.294, interval_steps_per_second: 15.368, epoch: 3.7584[0m
[32m[2022-08-31 18:22:43,682] [    INFO][0m - loss: 0.17117051, learning_rate: 2.885234899328859e-05, global_step: 570, interval_runtime: 0.927, interval_samples_per_second: 8.63, interval_steps_per_second: 10.787, epoch: 3.8255[0m
[32m[2022-08-31 18:22:44,316] [    INFO][0m - loss: 0.20921867, learning_rate: 2.8832214765100673e-05, global_step: 580, interval_runtime: 0.6342, interval_samples_per_second: 12.614, interval_steps_per_second: 15.767, epoch: 3.8926[0m
[32m[2022-08-31 18:22:44,957] [    INFO][0m - loss: 0.16193924, learning_rate: 2.8812080536912755e-05, global_step: 590, interval_runtime: 0.6406, interval_samples_per_second: 12.488, interval_steps_per_second: 15.61, epoch: 3.9597[0m
[32m[2022-08-31 18:22:45,602] [    INFO][0m - loss: 0.04331178, learning_rate: 2.879194630872483e-05, global_step: 600, interval_runtime: 0.6443, interval_samples_per_second: 12.416, interval_steps_per_second: 15.52, epoch: 4.0268[0m
[32m[2022-08-31 18:22:45,602] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:22:45,603] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-08-31 18:22:45,603] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:22:45,603] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:22:45,603] [    INFO][0m -   Total prediction steps = 35[0m
[32m[2022-08-31 18:22:47,769] [    INFO][0m - eval_loss: 2.7862303256988525, eval_accuracy: 0.5163934426229508, eval_runtime: 2.1655, eval_samples_per_second: 507.033, eval_steps_per_second: 16.162, epoch: 4.0268[0m
[32m[2022-08-31 18:22:47,769] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-08-31 18:22:47,769] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:22:49,647] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-08-31 18:22:49,647] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-08-31 18:22:55,189] [    INFO][0m - loss: 0.05727203, learning_rate: 2.8771812080536913e-05, global_step: 610, interval_runtime: 9.5876, interval_samples_per_second: 0.834, interval_steps_per_second: 1.043, epoch: 4.094[0m
[32m[2022-08-31 18:22:55,875] [    INFO][0m - loss: 0.06144159, learning_rate: 2.8751677852348992e-05, global_step: 620, interval_runtime: 0.6863, interval_samples_per_second: 11.657, interval_steps_per_second: 14.571, epoch: 4.1611[0m
[32m[2022-08-31 18:22:56,505] [    INFO][0m - loss: 0.05339316, learning_rate: 2.8731543624161074e-05, global_step: 630, interval_runtime: 0.6298, interval_samples_per_second: 12.702, interval_steps_per_second: 15.878, epoch: 4.2282[0m
[32m[2022-08-31 18:22:57,146] [    INFO][0m - loss: 0.05444726, learning_rate: 2.8711409395973157e-05, global_step: 640, interval_runtime: 0.6408, interval_samples_per_second: 12.484, interval_steps_per_second: 15.605, epoch: 4.2953[0m
[32m[2022-08-31 18:22:57,791] [    INFO][0m - loss: 0.15724708, learning_rate: 2.8691275167785235e-05, global_step: 650, interval_runtime: 0.6449, interval_samples_per_second: 12.405, interval_steps_per_second: 15.506, epoch: 4.3624[0m
[32m[2022-08-31 18:22:58,435] [    INFO][0m - loss: 0.00973243, learning_rate: 2.8671140939597318e-05, global_step: 660, interval_runtime: 0.6437, interval_samples_per_second: 12.429, interval_steps_per_second: 15.536, epoch: 4.4295[0m
[32m[2022-08-31 18:22:59,079] [    INFO][0m - loss: 0.1449569, learning_rate: 2.8651006711409397e-05, global_step: 670, interval_runtime: 0.6447, interval_samples_per_second: 12.41, interval_steps_per_second: 15.512, epoch: 4.4966[0m
[32m[2022-08-31 18:22:59,729] [    INFO][0m - loss: 0.04703061, learning_rate: 2.863087248322148e-05, global_step: 680, interval_runtime: 0.6494, interval_samples_per_second: 12.32, interval_steps_per_second: 15.399, epoch: 4.5638[0m
[32m[2022-08-31 18:23:00,395] [    INFO][0m - loss: 0.1073073, learning_rate: 2.8610738255033558e-05, global_step: 690, interval_runtime: 0.6662, interval_samples_per_second: 12.009, interval_steps_per_second: 15.011, epoch: 4.6309[0m
[32m[2022-08-31 18:23:01,038] [    INFO][0m - loss: 0.07367688, learning_rate: 2.8590604026845637e-05, global_step: 700, interval_runtime: 0.6429, interval_samples_per_second: 12.443, interval_steps_per_second: 15.554, epoch: 4.698[0m
[32m[2022-08-31 18:23:01,039] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:23:01,039] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-08-31 18:23:01,039] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:23:01,039] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:23:01,039] [    INFO][0m -   Total prediction steps = 35[0m
[32m[2022-08-31 18:23:03,153] [    INFO][0m - eval_loss: 3.1666343212127686, eval_accuracy: 0.5391621129326047, eval_runtime: 2.1136, eval_samples_per_second: 519.5, eval_steps_per_second: 16.56, epoch: 4.698[0m
[32m[2022-08-31 18:23:03,153] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-700[0m
[32m[2022-08-31 18:23:03,153] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:23:05,236] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-700/tokenizer_config.json[0m
[32m[2022-08-31 18:23:05,237] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-700/special_tokens_map.json[0m
[32m[2022-08-31 18:23:10,444] [    INFO][0m - loss: 0.0553297, learning_rate: 2.857046979865772e-05, global_step: 710, interval_runtime: 9.4061, interval_samples_per_second: 0.851, interval_steps_per_second: 1.063, epoch: 4.7651[0m
[32m[2022-08-31 18:23:11,103] [    INFO][0m - loss: 0.11405799, learning_rate: 2.8550335570469798e-05, global_step: 720, interval_runtime: 0.6578, interval_samples_per_second: 12.161, interval_steps_per_second: 15.202, epoch: 4.8322[0m
[32m[2022-08-31 18:23:11,802] [    INFO][0m - loss: 0.23677714, learning_rate: 2.853020134228188e-05, global_step: 730, interval_runtime: 0.7007, interval_samples_per_second: 11.418, interval_steps_per_second: 14.272, epoch: 4.8993[0m
[32m[2022-08-31 18:23:12,467] [    INFO][0m - loss: 0.11281221, learning_rate: 2.8510067114093962e-05, global_step: 740, interval_runtime: 0.6648, interval_samples_per_second: 12.034, interval_steps_per_second: 15.043, epoch: 4.9664[0m
[32m[2022-08-31 18:23:13,122] [    INFO][0m - loss: 0.01806642, learning_rate: 2.848993288590604e-05, global_step: 750, interval_runtime: 0.6546, interval_samples_per_second: 12.221, interval_steps_per_second: 15.276, epoch: 5.0336[0m
[32m[2022-08-31 18:23:13,784] [    INFO][0m - loss: 0.0144379, learning_rate: 2.8469798657718123e-05, global_step: 760, interval_runtime: 0.6621, interval_samples_per_second: 12.083, interval_steps_per_second: 15.103, epoch: 5.1007[0m
[32m[2022-08-31 18:23:14,463] [    INFO][0m - loss: 0.01820529, learning_rate: 2.8449664429530202e-05, global_step: 770, interval_runtime: 0.6791, interval_samples_per_second: 11.78, interval_steps_per_second: 14.725, epoch: 5.1678[0m
[32m[2022-08-31 18:23:15,097] [    INFO][0m - loss: 0.009727, learning_rate: 2.8429530201342284e-05, global_step: 780, interval_runtime: 0.6338, interval_samples_per_second: 12.622, interval_steps_per_second: 15.777, epoch: 5.2349[0m
[32m[2022-08-31 18:23:15,757] [    INFO][0m - loss: 0.12950186, learning_rate: 2.8409395973154363e-05, global_step: 790, interval_runtime: 0.6601, interval_samples_per_second: 12.119, interval_steps_per_second: 15.149, epoch: 5.302[0m
[32m[2022-08-31 18:23:16,406] [    INFO][0m - loss: 0.03752338, learning_rate: 2.8389261744966442e-05, global_step: 800, interval_runtime: 0.6491, interval_samples_per_second: 12.326, interval_steps_per_second: 15.407, epoch: 5.3691[0m
[32m[2022-08-31 18:23:16,407] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:23:16,407] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-08-31 18:23:16,407] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:23:16,407] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:23:16,407] [    INFO][0m -   Total prediction steps = 35[0m
[32m[2022-08-31 18:23:18,491] [    INFO][0m - eval_loss: 3.5912303924560547, eval_accuracy: 0.5300546448087432, eval_runtime: 2.0827, eval_samples_per_second: 527.199, eval_steps_per_second: 16.805, epoch: 5.3691[0m
[32m[2022-08-31 18:23:18,491] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-800[0m
[32m[2022-08-31 18:23:18,492] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:23:20,299] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-800/tokenizer_config.json[0m
[32m[2022-08-31 18:23:20,300] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-800/special_tokens_map.json[0m
[32m[2022-08-31 18:23:25,256] [    INFO][0m - loss: 0.05496397, learning_rate: 2.8369127516778524e-05, global_step: 810, interval_runtime: 8.8495, interval_samples_per_second: 0.904, interval_steps_per_second: 1.13, epoch: 5.4362[0m
[32m[2022-08-31 18:23:25,940] [    INFO][0m - loss: 0.06552861, learning_rate: 2.8348993288590603e-05, global_step: 820, interval_runtime: 0.6843, interval_samples_per_second: 11.69, interval_steps_per_second: 14.613, epoch: 5.5034[0m
[32m[2022-08-31 18:23:26,597] [    INFO][0m - loss: 0.0351124, learning_rate: 2.8328859060402685e-05, global_step: 830, interval_runtime: 0.6561, interval_samples_per_second: 12.194, interval_steps_per_second: 15.242, epoch: 5.5705[0m
[32m[2022-08-31 18:23:27,269] [    INFO][0m - loss: 0.08320307, learning_rate: 2.8308724832214768e-05, global_step: 840, interval_runtime: 0.6673, interval_samples_per_second: 11.989, interval_steps_per_second: 14.986, epoch: 5.6376[0m
[32m[2022-08-31 18:23:27,939] [    INFO][0m - loss: 0.03192447, learning_rate: 2.8288590604026847e-05, global_step: 850, interval_runtime: 0.6752, interval_samples_per_second: 11.849, interval_steps_per_second: 14.811, epoch: 5.7047[0m
[32m[2022-08-31 18:23:28,633] [    INFO][0m - loss: 0.026418, learning_rate: 2.826845637583893e-05, global_step: 860, interval_runtime: 0.6941, interval_samples_per_second: 11.526, interval_steps_per_second: 14.407, epoch: 5.7718[0m
[32m[2022-08-31 18:23:29,310] [    INFO][0m - loss: 0.05827815, learning_rate: 2.8248322147651008e-05, global_step: 870, interval_runtime: 0.6775, interval_samples_per_second: 11.809, interval_steps_per_second: 14.761, epoch: 5.8389[0m
[32m[2022-08-31 18:23:29,957] [    INFO][0m - loss: 0.019985, learning_rate: 2.8228187919463087e-05, global_step: 880, interval_runtime: 0.6472, interval_samples_per_second: 12.361, interval_steps_per_second: 15.451, epoch: 5.906[0m
[32m[2022-08-31 18:23:30,627] [    INFO][0m - loss: 0.02794539, learning_rate: 2.820805369127517e-05, global_step: 890, interval_runtime: 0.6696, interval_samples_per_second: 11.947, interval_steps_per_second: 14.933, epoch: 5.9732[0m
[32m[2022-08-31 18:23:31,294] [    INFO][0m - loss: 0.2363863, learning_rate: 2.8187919463087248e-05, global_step: 900, interval_runtime: 0.6668, interval_samples_per_second: 11.997, interval_steps_per_second: 14.997, epoch: 6.0403[0m
[32m[2022-08-31 18:23:31,294] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:23:31,295] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-08-31 18:23:31,295] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:23:31,295] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:23:31,295] [    INFO][0m -   Total prediction steps = 35[0m
[32m[2022-08-31 18:23:33,483] [    INFO][0m - eval_loss: 3.3701257705688477, eval_accuracy: 0.5519125683060109, eval_runtime: 2.1884, eval_samples_per_second: 501.742, eval_steps_per_second: 15.994, epoch: 6.0403[0m
[32m[2022-08-31 18:23:33,484] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-900[0m
[32m[2022-08-31 18:23:33,484] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:23:35,355] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-900/tokenizer_config.json[0m
[32m[2022-08-31 18:23:35,356] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-900/special_tokens_map.json[0m
[32m[2022-08-31 18:23:40,630] [    INFO][0m - loss: 0.0050471, learning_rate: 2.816778523489933e-05, global_step: 910, interval_runtime: 9.3359, interval_samples_per_second: 0.857, interval_steps_per_second: 1.071, epoch: 6.1074[0m
[32m[2022-08-31 18:23:41,335] [    INFO][0m - loss: 0.00219947, learning_rate: 2.814765100671141e-05, global_step: 920, interval_runtime: 0.7057, interval_samples_per_second: 11.336, interval_steps_per_second: 14.17, epoch: 6.1745[0m
[32m[2022-08-31 18:23:41,970] [    INFO][0m - loss: 0.00088811, learning_rate: 2.812751677852349e-05, global_step: 930, interval_runtime: 0.635, interval_samples_per_second: 12.598, interval_steps_per_second: 15.748, epoch: 6.2416[0m
[32m[2022-08-31 18:23:42,650] [    INFO][0m - loss: 0.00344221, learning_rate: 2.8107382550335573e-05, global_step: 940, interval_runtime: 0.6791, interval_samples_per_second: 11.78, interval_steps_per_second: 14.725, epoch: 6.3087[0m
[32m[2022-08-31 18:23:43,321] [    INFO][0m - loss: 0.00055568, learning_rate: 2.8087248322147652e-05, global_step: 950, interval_runtime: 0.6715, interval_samples_per_second: 11.914, interval_steps_per_second: 14.893, epoch: 6.3758[0m
[32m[2022-08-31 18:23:43,990] [    INFO][0m - loss: 0.01835595, learning_rate: 2.8067114093959734e-05, global_step: 960, interval_runtime: 0.6681, interval_samples_per_second: 11.975, interval_steps_per_second: 14.969, epoch: 6.443[0m
[32m[2022-08-31 18:23:44,619] [    INFO][0m - loss: 0.00426311, learning_rate: 2.804697986577181e-05, global_step: 970, interval_runtime: 0.6303, interval_samples_per_second: 12.693, interval_steps_per_second: 15.866, epoch: 6.5101[0m
[32m[2022-08-31 18:23:45,323] [    INFO][0m - loss: 0.01331602, learning_rate: 2.8026845637583892e-05, global_step: 980, interval_runtime: 0.7034, interval_samples_per_second: 11.374, interval_steps_per_second: 14.218, epoch: 6.5772[0m
[32m[2022-08-31 18:23:46,063] [    INFO][0m - loss: 0.00477832, learning_rate: 2.8006711409395974e-05, global_step: 990, interval_runtime: 0.7398, interval_samples_per_second: 10.814, interval_steps_per_second: 13.517, epoch: 6.6443[0m
[32m[2022-08-31 18:23:46,738] [    INFO][0m - loss: 0.00825982, learning_rate: 2.7986577181208053e-05, global_step: 1000, interval_runtime: 0.6756, interval_samples_per_second: 11.842, interval_steps_per_second: 14.802, epoch: 6.7114[0m
[32m[2022-08-31 18:23:46,739] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:23:46,739] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-08-31 18:23:46,739] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:23:46,739] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:23:46,739] [    INFO][0m -   Total prediction steps = 35[0m
[32m[2022-08-31 18:23:48,900] [    INFO][0m - eval_loss: 3.749342679977417, eval_accuracy: 0.5528233151183971, eval_runtime: 2.159, eval_samples_per_second: 508.575, eval_steps_per_second: 16.211, epoch: 6.7114[0m
[32m[2022-08-31 18:23:48,901] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1000[0m
[32m[2022-08-31 18:23:48,902] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:23:50,886] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1000/tokenizer_config.json[0m
[32m[2022-08-31 18:23:50,887] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1000/special_tokens_map.json[0m
[32m[2022-08-31 18:23:56,088] [    INFO][0m - loss: 0.03586548, learning_rate: 2.7966442953020136e-05, global_step: 1010, interval_runtime: 9.3487, interval_samples_per_second: 0.856, interval_steps_per_second: 1.07, epoch: 6.7785[0m
[32m[2022-08-31 18:23:56,752] [    INFO][0m - loss: 0.02253547, learning_rate: 2.7946308724832214e-05, global_step: 1020, interval_runtime: 0.6649, interval_samples_per_second: 12.032, interval_steps_per_second: 15.039, epoch: 6.8456[0m
[32m[2022-08-31 18:23:57,430] [    INFO][0m - loss: 0.03238667, learning_rate: 2.7926174496644297e-05, global_step: 1030, interval_runtime: 0.6782, interval_samples_per_second: 11.796, interval_steps_per_second: 14.745, epoch: 6.9128[0m
[32m[2022-08-31 18:23:58,071] [    INFO][0m - loss: 0.00389844, learning_rate: 2.790604026845638e-05, global_step: 1040, interval_runtime: 0.6407, interval_samples_per_second: 12.487, interval_steps_per_second: 15.609, epoch: 6.9799[0m
[32m[2022-08-31 18:23:58,865] [    INFO][0m - loss: 0.02796979, learning_rate: 2.7885906040268458e-05, global_step: 1050, interval_runtime: 0.7927, interval_samples_per_second: 10.092, interval_steps_per_second: 12.616, epoch: 7.047[0m
[32m[2022-08-31 18:23:59,580] [    INFO][0m - loss: 0.05642084, learning_rate: 2.7865771812080537e-05, global_step: 1060, interval_runtime: 0.7163, interval_samples_per_second: 11.169, interval_steps_per_second: 13.961, epoch: 7.1141[0m
[32m[2022-08-31 18:24:00,218] [    INFO][0m - loss: 0.00344092, learning_rate: 2.7845637583892616e-05, global_step: 1070, interval_runtime: 0.6378, interval_samples_per_second: 12.544, interval_steps_per_second: 15.68, epoch: 7.1812[0m
[32m[2022-08-31 18:24:00,922] [    INFO][0m - loss: 0.0048782, learning_rate: 2.7825503355704698e-05, global_step: 1080, interval_runtime: 0.7048, interval_samples_per_second: 11.35, interval_steps_per_second: 14.188, epoch: 7.2483[0m
[32m[2022-08-31 18:24:01,607] [    INFO][0m - loss: 0.0144702, learning_rate: 2.780536912751678e-05, global_step: 1090, interval_runtime: 0.6848, interval_samples_per_second: 11.683, interval_steps_per_second: 14.603, epoch: 7.3154[0m
[32m[2022-08-31 18:24:02,290] [    INFO][0m - loss: 0.00684416, learning_rate: 2.778523489932886e-05, global_step: 1100, interval_runtime: 0.6833, interval_samples_per_second: 11.708, interval_steps_per_second: 14.634, epoch: 7.3826[0m
[32m[2022-08-31 18:24:02,291] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:24:02,291] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-08-31 18:24:02,291] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:24:02,291] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:24:02,291] [    INFO][0m -   Total prediction steps = 35[0m
[32m[2022-08-31 18:24:04,417] [    INFO][0m - eval_loss: 3.7385764122009277, eval_accuracy: 0.5555555555555556, eval_runtime: 2.1252, eval_samples_per_second: 516.65, eval_steps_per_second: 16.469, epoch: 7.3826[0m
[32m[2022-08-31 18:24:04,417] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1100[0m
[32m[2022-08-31 18:24:04,417] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:24:06,317] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1100/tokenizer_config.json[0m
[32m[2022-08-31 18:24:06,318] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1100/special_tokens_map.json[0m
[32m[2022-08-31 18:24:13,330] [    INFO][0m - loss: 0.00091475, learning_rate: 2.776510067114094e-05, global_step: 1110, interval_runtime: 11.0387, interval_samples_per_second: 0.725, interval_steps_per_second: 0.906, epoch: 7.4497[0m
[32m[2022-08-31 18:24:13,999] [    INFO][0m - loss: 0.00415838, learning_rate: 2.774496644295302e-05, global_step: 1120, interval_runtime: 0.6704, interval_samples_per_second: 11.933, interval_steps_per_second: 14.917, epoch: 7.5168[0m
[32m[2022-08-31 18:24:14,685] [    INFO][0m - loss: 0.0053869, learning_rate: 2.7724832214765102e-05, global_step: 1130, interval_runtime: 0.6854, interval_samples_per_second: 11.672, interval_steps_per_second: 14.59, epoch: 7.5839[0m
[32m[2022-08-31 18:24:15,367] [    INFO][0m - loss: 0.00088493, learning_rate: 2.7704697986577185e-05, global_step: 1140, interval_runtime: 0.6817, interval_samples_per_second: 11.735, interval_steps_per_second: 14.669, epoch: 7.651[0m
[32m[2022-08-31 18:24:16,010] [    INFO][0m - loss: 0.00082786, learning_rate: 2.768456375838926e-05, global_step: 1150, interval_runtime: 0.6435, interval_samples_per_second: 12.432, interval_steps_per_second: 15.539, epoch: 7.7181[0m
[32m[2022-08-31 18:24:16,682] [    INFO][0m - loss: 0.00700008, learning_rate: 2.7664429530201342e-05, global_step: 1160, interval_runtime: 0.6718, interval_samples_per_second: 11.908, interval_steps_per_second: 14.885, epoch: 7.7852[0m
[32m[2022-08-31 18:24:17,373] [    INFO][0m - loss: 0.00035475, learning_rate: 2.764429530201342e-05, global_step: 1170, interval_runtime: 0.6913, interval_samples_per_second: 11.573, interval_steps_per_second: 14.466, epoch: 7.8523[0m
[32m[2022-08-31 18:24:18,020] [    INFO][0m - loss: 0.00048702, learning_rate: 2.7624161073825503e-05, global_step: 1180, interval_runtime: 0.6471, interval_samples_per_second: 12.363, interval_steps_per_second: 15.454, epoch: 7.9195[0m
[32m[2022-08-31 18:24:18,639] [    INFO][0m - loss: 0.02541392, learning_rate: 2.7604026845637586e-05, global_step: 1190, interval_runtime: 0.6187, interval_samples_per_second: 12.93, interval_steps_per_second: 16.162, epoch: 7.9866[0m
[32m[2022-08-31 18:24:19,326] [    INFO][0m - loss: 0.00213465, learning_rate: 2.7583892617449664e-05, global_step: 1200, interval_runtime: 0.6866, interval_samples_per_second: 11.651, interval_steps_per_second: 14.564, epoch: 8.0537[0m
[32m[2022-08-31 18:24:19,326] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:24:19,326] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-08-31 18:24:19,326] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:24:19,326] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:24:19,327] [    INFO][0m -   Total prediction steps = 35[0m
[32m[2022-08-31 18:24:21,456] [    INFO][0m - eval_loss: 4.007510185241699, eval_accuracy: 0.5373406193078324, eval_runtime: 2.1281, eval_samples_per_second: 515.956, eval_steps_per_second: 16.447, epoch: 8.0537[0m
[32m[2022-08-31 18:24:21,456] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1200[0m
[32m[2022-08-31 18:24:21,456] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:24:23,298] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1200/tokenizer_config.json[0m
[32m[2022-08-31 18:24:23,299] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1200/special_tokens_map.json[0m
[32m[2022-08-31 18:24:28,135] [    INFO][0m - loss: 0.0012596, learning_rate: 2.7563758389261747e-05, global_step: 1210, interval_runtime: 8.8093, interval_samples_per_second: 0.908, interval_steps_per_second: 1.135, epoch: 8.1208[0m
[32m[2022-08-31 18:24:28,781] [    INFO][0m - loss: 0.00660193, learning_rate: 2.7543624161073826e-05, global_step: 1220, interval_runtime: 0.6457, interval_samples_per_second: 12.39, interval_steps_per_second: 15.488, epoch: 8.1879[0m
[32m[2022-08-31 18:24:29,454] [    INFO][0m - loss: 0.00200641, learning_rate: 2.7523489932885908e-05, global_step: 1230, interval_runtime: 0.6734, interval_samples_per_second: 11.88, interval_steps_per_second: 14.85, epoch: 8.255[0m
[32m[2022-08-31 18:24:30,119] [    INFO][0m - loss: 0.0013626, learning_rate: 2.750335570469799e-05, global_step: 1240, interval_runtime: 0.6648, interval_samples_per_second: 12.034, interval_steps_per_second: 15.043, epoch: 8.3221[0m
[32m[2022-08-31 18:24:30,764] [    INFO][0m - loss: 0.00699018, learning_rate: 2.7483221476510066e-05, global_step: 1250, interval_runtime: 0.6444, interval_samples_per_second: 12.414, interval_steps_per_second: 15.518, epoch: 8.3893[0m
[32m[2022-08-31 18:24:31,517] [    INFO][0m - loss: 0.00031109, learning_rate: 2.7463087248322148e-05, global_step: 1260, interval_runtime: 0.754, interval_samples_per_second: 10.61, interval_steps_per_second: 13.262, epoch: 8.4564[0m
[32m[2022-08-31 18:24:32,212] [    INFO][0m - loss: 0.03945932, learning_rate: 2.7442953020134227e-05, global_step: 1270, interval_runtime: 0.694, interval_samples_per_second: 11.527, interval_steps_per_second: 14.408, epoch: 8.5235[0m
[32m[2022-08-31 18:24:32,870] [    INFO][0m - loss: 0.00107452, learning_rate: 2.742281879194631e-05, global_step: 1280, interval_runtime: 0.6584, interval_samples_per_second: 12.151, interval_steps_per_second: 15.189, epoch: 8.5906[0m
[32m[2022-08-31 18:24:33,542] [    INFO][0m - loss: 7.249e-05, learning_rate: 2.740268456375839e-05, global_step: 1290, interval_runtime: 0.6721, interval_samples_per_second: 11.904, interval_steps_per_second: 14.88, epoch: 8.6577[0m
[32m[2022-08-31 18:24:34,178] [    INFO][0m - loss: 0.10280269, learning_rate: 2.738255033557047e-05, global_step: 1300, interval_runtime: 0.6361, interval_samples_per_second: 12.577, interval_steps_per_second: 15.721, epoch: 8.7248[0m
[32m[2022-08-31 18:24:34,178] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:24:34,179] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-08-31 18:24:34,179] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:24:34,179] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:24:34,179] [    INFO][0m -   Total prediction steps = 35[0m
[32m[2022-08-31 18:24:36,374] [    INFO][0m - eval_loss: 4.121700763702393, eval_accuracy: 0.5491803278688525, eval_runtime: 2.1948, eval_samples_per_second: 500.279, eval_steps_per_second: 15.947, epoch: 8.7248[0m
[32m[2022-08-31 18:24:36,375] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1300[0m
[32m[2022-08-31 18:24:36,375] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:24:38,255] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1300/tokenizer_config.json[0m
[32m[2022-08-31 18:24:38,255] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1300/special_tokens_map.json[0m
[32m[2022-08-31 18:24:43,047] [    INFO][0m - loss: 0.00950822, learning_rate: 2.7362416107382552e-05, global_step: 1310, interval_runtime: 8.8685, interval_samples_per_second: 0.902, interval_steps_per_second: 1.128, epoch: 8.7919[0m
[32m[2022-08-31 18:24:43,720] [    INFO][0m - loss: 0.03916811, learning_rate: 2.734228187919463e-05, global_step: 1320, interval_runtime: 0.6738, interval_samples_per_second: 11.872, interval_steps_per_second: 14.84, epoch: 8.8591[0m
[32m[2022-08-31 18:24:44,391] [    INFO][0m - loss: 0.00543504, learning_rate: 2.7322147651006713e-05, global_step: 1330, interval_runtime: 0.6706, interval_samples_per_second: 11.93, interval_steps_per_second: 14.912, epoch: 8.9262[0m
[32m[2022-08-31 18:24:45,007] [    INFO][0m - loss: 0.00098943, learning_rate: 2.7302013422818792e-05, global_step: 1340, interval_runtime: 0.6148, interval_samples_per_second: 13.012, interval_steps_per_second: 16.265, epoch: 8.9933[0m
[32m[2022-08-31 18:24:45,704] [    INFO][0m - loss: 0.00563839, learning_rate: 2.728187919463087e-05, global_step: 1350, interval_runtime: 0.6981, interval_samples_per_second: 11.46, interval_steps_per_second: 14.325, epoch: 9.0604[0m
[32m[2022-08-31 18:24:46,349] [    INFO][0m - loss: 0.04438734, learning_rate: 2.7261744966442953e-05, global_step: 1360, interval_runtime: 0.645, interval_samples_per_second: 12.404, interval_steps_per_second: 15.505, epoch: 9.1275[0m
[32m[2022-08-31 18:24:46,997] [    INFO][0m - loss: 0.00122059, learning_rate: 2.7241610738255032e-05, global_step: 1370, interval_runtime: 0.6479, interval_samples_per_second: 12.348, interval_steps_per_second: 15.436, epoch: 9.1946[0m
[32m[2022-08-31 18:24:47,644] [    INFO][0m - loss: 0.00297232, learning_rate: 2.7221476510067115e-05, global_step: 1380, interval_runtime: 0.6479, interval_samples_per_second: 12.348, interval_steps_per_second: 15.436, epoch: 9.2617[0m
[32m[2022-08-31 18:24:48,342] [    INFO][0m - loss: 0.00105535, learning_rate: 2.7201342281879197e-05, global_step: 1390, interval_runtime: 0.6974, interval_samples_per_second: 11.471, interval_steps_per_second: 14.339, epoch: 9.3289[0m
[32m[2022-08-31 18:24:49,034] [    INFO][0m - loss: 0.00154317, learning_rate: 2.7181208053691276e-05, global_step: 1400, interval_runtime: 0.6919, interval_samples_per_second: 11.562, interval_steps_per_second: 14.453, epoch: 9.396[0m
[32m[2022-08-31 18:24:49,034] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:24:49,034] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-08-31 18:24:49,035] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:24:49,035] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:24:49,035] [    INFO][0m -   Total prediction steps = 35[0m
[32m[2022-08-31 18:24:51,159] [    INFO][0m - eval_loss: 4.330391883850098, eval_accuracy: 0.5482695810564663, eval_runtime: 2.1241, eval_samples_per_second: 516.92, eval_steps_per_second: 16.477, epoch: 9.396[0m
[32m[2022-08-31 18:24:51,160] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1400[0m
[32m[2022-08-31 18:24:51,160] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:24:52,998] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1400/tokenizer_config.json[0m
[32m[2022-08-31 18:24:52,999] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1400/special_tokens_map.json[0m
[32m[2022-08-31 18:24:59,452] [    INFO][0m - loss: 6.96e-05, learning_rate: 2.7161073825503358e-05, global_step: 1410, interval_runtime: 10.4179, interval_samples_per_second: 0.768, interval_steps_per_second: 0.96, epoch: 9.4631[0m
[32m[2022-08-31 18:25:00,115] [    INFO][0m - loss: 0.00050177, learning_rate: 2.7140939597315437e-05, global_step: 1420, interval_runtime: 0.6634, interval_samples_per_second: 12.059, interval_steps_per_second: 15.073, epoch: 9.5302[0m
[32m[2022-08-31 18:25:00,771] [    INFO][0m - loss: 0.00045029, learning_rate: 2.7120805369127516e-05, global_step: 1430, interval_runtime: 0.6555, interval_samples_per_second: 12.205, interval_steps_per_second: 15.256, epoch: 9.5973[0m
[32m[2022-08-31 18:25:01,443] [    INFO][0m - loss: 0.02391391, learning_rate: 2.7100671140939598e-05, global_step: 1440, interval_runtime: 0.672, interval_samples_per_second: 11.905, interval_steps_per_second: 14.881, epoch: 9.6644[0m
[32m[2022-08-31 18:25:02,271] [    INFO][0m - loss: 0.00409336, learning_rate: 2.7080536912751677e-05, global_step: 1450, interval_runtime: 0.8282, interval_samples_per_second: 9.66, interval_steps_per_second: 12.075, epoch: 9.7315[0m
[32m[2022-08-31 18:25:03,111] [    INFO][0m - loss: 0.00013261, learning_rate: 2.706040268456376e-05, global_step: 1460, interval_runtime: 0.8403, interval_samples_per_second: 9.521, interval_steps_per_second: 11.901, epoch: 9.7987[0m
[32m[2022-08-31 18:25:03,797] [    INFO][0m - loss: 0.00013145, learning_rate: 2.7040268456375838e-05, global_step: 1470, interval_runtime: 0.6859, interval_samples_per_second: 11.663, interval_steps_per_second: 14.579, epoch: 9.8658[0m
[32m[2022-08-31 18:25:04,479] [    INFO][0m - loss: 0.00074376, learning_rate: 2.702013422818792e-05, global_step: 1480, interval_runtime: 0.6785, interval_samples_per_second: 11.791, interval_steps_per_second: 14.739, epoch: 9.9329[0m
[32m[2022-08-31 18:25:05,128] [    INFO][0m - loss: 7.426e-05, learning_rate: 2.7000000000000002e-05, global_step: 1490, interval_runtime: 0.6523, interval_samples_per_second: 12.265, interval_steps_per_second: 15.331, epoch: 10.0[0m
[32m[2022-08-31 18:25:05,829] [    INFO][0m - loss: 0.00039823, learning_rate: 2.697986577181208e-05, global_step: 1500, interval_runtime: 0.7009, interval_samples_per_second: 11.413, interval_steps_per_second: 14.267, epoch: 10.0671[0m
[32m[2022-08-31 18:25:05,829] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:25:05,829] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-08-31 18:25:05,830] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:25:05,830] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:25:05,830] [    INFO][0m -   Total prediction steps = 35[0m
[32m[2022-08-31 18:25:07,934] [    INFO][0m - eval_loss: 4.170134544372559, eval_accuracy: 0.5537340619307832, eval_runtime: 2.1034, eval_samples_per_second: 522.014, eval_steps_per_second: 16.64, epoch: 10.0671[0m
[32m[2022-08-31 18:25:07,934] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1500[0m
[32m[2022-08-31 18:25:07,934] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:25:10,106] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1500/tokenizer_config.json[0m
[32m[2022-08-31 18:25:10,106] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1500/special_tokens_map.json[0m
[32m[2022-08-31 18:25:14,769] [    INFO][0m - loss: 0.00123847, learning_rate: 2.6959731543624163e-05, global_step: 1510, interval_runtime: 8.9399, interval_samples_per_second: 0.895, interval_steps_per_second: 1.119, epoch: 10.1342[0m
[32m[2022-08-31 18:25:15,549] [    INFO][0m - loss: 7.238e-05, learning_rate: 2.6939597315436242e-05, global_step: 1520, interval_runtime: 0.7806, interval_samples_per_second: 10.249, interval_steps_per_second: 12.811, epoch: 10.2013[0m
[32m[2022-08-31 18:25:16,210] [    INFO][0m - loss: 0.00233275, learning_rate: 2.691946308724832e-05, global_step: 1530, interval_runtime: 0.6609, interval_samples_per_second: 12.106, interval_steps_per_second: 15.132, epoch: 10.2685[0m
[32m[2022-08-31 18:25:16,850] [    INFO][0m - loss: 0.00210205, learning_rate: 2.6899328859060403e-05, global_step: 1540, interval_runtime: 0.6391, interval_samples_per_second: 12.517, interval_steps_per_second: 15.647, epoch: 10.3356[0m
[32m[2022-08-31 18:25:17,623] [    INFO][0m - loss: 0.00089762, learning_rate: 2.6879194630872482e-05, global_step: 1550, interval_runtime: 0.7734, interval_samples_per_second: 10.344, interval_steps_per_second: 12.929, epoch: 10.4027[0m
[32m[2022-08-31 18:25:18,270] [    INFO][0m - loss: 0.00010982, learning_rate: 2.6859060402684565e-05, global_step: 1560, interval_runtime: 0.6472, interval_samples_per_second: 12.362, interval_steps_per_second: 15.452, epoch: 10.4698[0m
[32m[2022-08-31 18:25:18,921] [    INFO][0m - loss: 0.00313635, learning_rate: 2.6838926174496647e-05, global_step: 1570, interval_runtime: 0.6506, interval_samples_per_second: 12.297, interval_steps_per_second: 15.372, epoch: 10.5369[0m
[32m[2022-08-31 18:25:19,574] [    INFO][0m - loss: 0.00391023, learning_rate: 2.6818791946308726e-05, global_step: 1580, interval_runtime: 0.6538, interval_samples_per_second: 12.237, interval_steps_per_second: 15.296, epoch: 10.604[0m
[32m[2022-08-31 18:25:20,225] [    INFO][0m - loss: 0.00288463, learning_rate: 2.6798657718120808e-05, global_step: 1590, interval_runtime: 0.6507, interval_samples_per_second: 12.294, interval_steps_per_second: 15.367, epoch: 10.6711[0m
[32m[2022-08-31 18:25:20,991] [    INFO][0m - loss: 0.00022739, learning_rate: 2.6778523489932887e-05, global_step: 1600, interval_runtime: 0.7657, interval_samples_per_second: 10.447, interval_steps_per_second: 13.059, epoch: 10.7383[0m
[32m[2022-08-31 18:25:20,991] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:25:20,991] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-08-31 18:25:20,991] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:25:20,991] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:25:20,992] [    INFO][0m -   Total prediction steps = 35[0m
[32m[2022-08-31 18:25:23,096] [    INFO][0m - eval_loss: 4.339382171630859, eval_accuracy: 0.5391621129326047, eval_runtime: 2.104, eval_samples_per_second: 521.862, eval_steps_per_second: 16.635, epoch: 10.7383[0m
[32m[2022-08-31 18:25:23,096] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1600[0m
[32m[2022-08-31 18:25:23,096] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:25:24,921] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1600/tokenizer_config.json[0m
[32m[2022-08-31 18:25:24,922] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1600/special_tokens_map.json[0m
[32m[2022-08-31 18:25:29,952] [    INFO][0m - loss: 0.0196545, learning_rate: 2.6758389261744966e-05, global_step: 1610, interval_runtime: 8.961, interval_samples_per_second: 0.893, interval_steps_per_second: 1.116, epoch: 10.8054[0m
[32m[2022-08-31 18:25:30,614] [    INFO][0m - loss: 0.03990698, learning_rate: 2.6738255033557048e-05, global_step: 1620, interval_runtime: 0.6621, interval_samples_per_second: 12.082, interval_steps_per_second: 15.103, epoch: 10.8725[0m
[32m[2022-08-31 18:25:31,251] [    INFO][0m - loss: 7.182e-05, learning_rate: 2.6718120805369127e-05, global_step: 1630, interval_runtime: 0.6366, interval_samples_per_second: 12.566, interval_steps_per_second: 15.708, epoch: 10.9396[0m
[32m[2022-08-31 18:25:31,917] [    INFO][0m - loss: 0.01672855, learning_rate: 2.669798657718121e-05, global_step: 1640, interval_runtime: 0.6665, interval_samples_per_second: 12.002, interval_steps_per_second: 15.003, epoch: 11.0067[0m
[32m[2022-08-31 18:25:32,578] [    INFO][0m - loss: 9.96e-05, learning_rate: 2.6677852348993288e-05, global_step: 1650, interval_runtime: 0.6606, interval_samples_per_second: 12.11, interval_steps_per_second: 15.138, epoch: 11.0738[0m
[32m[2022-08-31 18:25:33,254] [    INFO][0m - loss: 0.08674601, learning_rate: 2.665771812080537e-05, global_step: 1660, interval_runtime: 0.6759, interval_samples_per_second: 11.836, interval_steps_per_second: 14.795, epoch: 11.1409[0m
[32m[2022-08-31 18:25:33,909] [    INFO][0m - loss: 0.00115709, learning_rate: 2.6637583892617452e-05, global_step: 1670, interval_runtime: 0.6557, interval_samples_per_second: 12.2, interval_steps_per_second: 15.251, epoch: 11.2081[0m
[32m[2022-08-31 18:25:34,566] [    INFO][0m - loss: 8.454e-05, learning_rate: 2.661744966442953e-05, global_step: 1680, interval_runtime: 0.6565, interval_samples_per_second: 12.186, interval_steps_per_second: 15.233, epoch: 11.2752[0m
[32m[2022-08-31 18:25:35,220] [    INFO][0m - loss: 0.00227542, learning_rate: 2.6597315436241614e-05, global_step: 1690, interval_runtime: 0.6546, interval_samples_per_second: 12.22, interval_steps_per_second: 15.275, epoch: 11.3423[0m
[32m[2022-08-31 18:25:35,872] [    INFO][0m - loss: 0.00932327, learning_rate: 2.6577181208053692e-05, global_step: 1700, interval_runtime: 0.651, interval_samples_per_second: 12.288, interval_steps_per_second: 15.36, epoch: 11.4094[0m
[32m[2022-08-31 18:25:35,872] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:25:35,873] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-08-31 18:25:35,873] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:25:35,873] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:25:35,873] [    INFO][0m -   Total prediction steps = 35[0m
[32m[2022-08-31 18:25:38,027] [    INFO][0m - eval_loss: 4.232057571411133, eval_accuracy: 0.5446265938069217, eval_runtime: 2.1534, eval_samples_per_second: 509.891, eval_steps_per_second: 16.253, epoch: 11.4094[0m
[32m[2022-08-31 18:25:38,027] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1700[0m
[32m[2022-08-31 18:25:38,027] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:25:40,461] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1700/tokenizer_config.json[0m
[32m[2022-08-31 18:25:40,461] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1700/special_tokens_map.json[0m
[32m[2022-08-31 18:25:45,403] [    INFO][0m - loss: 0.00716495, learning_rate: 2.655704697986577e-05, global_step: 1710, interval_runtime: 9.5312, interval_samples_per_second: 0.839, interval_steps_per_second: 1.049, epoch: 11.4765[0m
[32m[2022-08-31 18:25:46,121] [    INFO][0m - loss: 0.05102572, learning_rate: 2.6536912751677854e-05, global_step: 1720, interval_runtime: 0.7186, interval_samples_per_second: 11.132, interval_steps_per_second: 13.915, epoch: 11.5436[0m
[32m[2022-08-31 18:25:46,808] [    INFO][0m - loss: 0.06392888, learning_rate: 2.6516778523489932e-05, global_step: 1730, interval_runtime: 0.6864, interval_samples_per_second: 11.655, interval_steps_per_second: 14.569, epoch: 11.6107[0m
[32m[2022-08-31 18:25:47,491] [    INFO][0m - loss: 0.00023658, learning_rate: 2.6496644295302015e-05, global_step: 1740, interval_runtime: 0.683, interval_samples_per_second: 11.712, interval_steps_per_second: 14.641, epoch: 11.6779[0m
[32m[2022-08-31 18:25:48,174] [    INFO][0m - loss: 0.00121993, learning_rate: 2.6476510067114094e-05, global_step: 1750, interval_runtime: 0.6831, interval_samples_per_second: 11.711, interval_steps_per_second: 14.639, epoch: 11.745[0m
[32m[2022-08-31 18:25:48,853] [    INFO][0m - loss: 0.00357023, learning_rate: 2.6456375838926176e-05, global_step: 1760, interval_runtime: 0.6792, interval_samples_per_second: 11.779, interval_steps_per_second: 14.723, epoch: 11.8121[0m
[32m[2022-08-31 18:25:49,555] [    INFO][0m - loss: 0.00254831, learning_rate: 2.6436241610738258e-05, global_step: 1770, interval_runtime: 0.7021, interval_samples_per_second: 11.395, interval_steps_per_second: 14.244, epoch: 11.8792[0m
[32m[2022-08-31 18:25:50,198] [    INFO][0m - loss: 0.00119725, learning_rate: 2.6416107382550337e-05, global_step: 1780, interval_runtime: 0.6427, interval_samples_per_second: 12.447, interval_steps_per_second: 15.559, epoch: 11.9463[0m
[32m[2022-08-31 18:25:50,865] [    INFO][0m - loss: 0.0006102, learning_rate: 2.639597315436242e-05, global_step: 1790, interval_runtime: 0.6671, interval_samples_per_second: 11.992, interval_steps_per_second: 14.99, epoch: 12.0134[0m
[32m[2022-08-31 18:25:51,532] [    INFO][0m - loss: 0.04439104, learning_rate: 2.6375838926174495e-05, global_step: 1800, interval_runtime: 0.6667, interval_samples_per_second: 11.999, interval_steps_per_second: 14.999, epoch: 12.0805[0m
[32m[2022-08-31 18:25:51,532] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:25:51,532] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-08-31 18:25:51,533] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:25:51,533] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:25:51,533] [    INFO][0m -   Total prediction steps = 35[0m
[32m[2022-08-31 18:25:53,689] [    INFO][0m - eval_loss: 4.296806335449219, eval_accuracy: 0.5500910746812386, eval_runtime: 2.1558, eval_samples_per_second: 509.318, eval_steps_per_second: 16.235, epoch: 12.0805[0m
[32m[2022-08-31 18:25:53,690] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1800[0m
[32m[2022-08-31 18:25:53,690] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:25:55,537] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1800/tokenizer_config.json[0m
[32m[2022-08-31 18:25:55,537] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1800/special_tokens_map.json[0m
[32m[2022-08-31 18:26:00,237] [    INFO][0m - loss: 0.00385832, learning_rate: 2.6355704697986577e-05, global_step: 1810, interval_runtime: 8.7047, interval_samples_per_second: 0.919, interval_steps_per_second: 1.149, epoch: 12.1477[0m
[32m[2022-08-31 18:26:00,906] [    INFO][0m - loss: 0.04291326, learning_rate: 2.633557046979866e-05, global_step: 1820, interval_runtime: 0.6699, interval_samples_per_second: 11.941, interval_steps_per_second: 14.927, epoch: 12.2148[0m
[32m[2022-08-31 18:26:01,560] [    INFO][0m - loss: 0.0113576, learning_rate: 2.6315436241610738e-05, global_step: 1830, interval_runtime: 0.6531, interval_samples_per_second: 12.249, interval_steps_per_second: 15.311, epoch: 12.2819[0m
[32m[2022-08-31 18:26:02,211] [    INFO][0m - loss: 0.0752848, learning_rate: 2.629530201342282e-05, global_step: 1840, interval_runtime: 0.6513, interval_samples_per_second: 12.283, interval_steps_per_second: 15.354, epoch: 12.349[0m
[32m[2022-08-31 18:26:02,878] [    INFO][0m - loss: 0.00771721, learning_rate: 2.62751677852349e-05, global_step: 1850, interval_runtime: 0.6673, interval_samples_per_second: 11.989, interval_steps_per_second: 14.987, epoch: 12.4161[0m
[32m[2022-08-31 18:26:03,540] [    INFO][0m - loss: 7.702e-05, learning_rate: 2.625503355704698e-05, global_step: 1860, interval_runtime: 0.6614, interval_samples_per_second: 12.096, interval_steps_per_second: 15.12, epoch: 12.4832[0m
[32m[2022-08-31 18:26:04,160] [    INFO][0m - loss: 0.0005806, learning_rate: 2.6234899328859064e-05, global_step: 1870, interval_runtime: 0.6206, interval_samples_per_second: 12.891, interval_steps_per_second: 16.114, epoch: 12.5503[0m
[32m[2022-08-31 18:26:04,805] [    INFO][0m - loss: 0.03292278, learning_rate: 2.6214765100671142e-05, global_step: 1880, interval_runtime: 0.6454, interval_samples_per_second: 12.396, interval_steps_per_second: 15.494, epoch: 12.6174[0m
[32m[2022-08-31 18:26:05,433] [    INFO][0m - loss: 0.00066512, learning_rate: 2.619463087248322e-05, global_step: 1890, interval_runtime: 0.6278, interval_samples_per_second: 12.744, interval_steps_per_second: 15.93, epoch: 12.6846[0m
[32m[2022-08-31 18:26:06,052] [    INFO][0m - loss: 0.00012307, learning_rate: 2.61744966442953e-05, global_step: 1900, interval_runtime: 0.6186, interval_samples_per_second: 12.933, interval_steps_per_second: 16.166, epoch: 12.7517[0m
[32m[2022-08-31 18:26:06,052] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:26:06,052] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-08-31 18:26:06,053] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:26:06,053] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:26:06,053] [    INFO][0m -   Total prediction steps = 35[0m
[32m[2022-08-31 18:26:08,232] [    INFO][0m - eval_loss: 4.607343673706055, eval_accuracy: 0.5364298724954463, eval_runtime: 2.1794, eval_samples_per_second: 503.802, eval_steps_per_second: 16.059, epoch: 12.7517[0m
[32m[2022-08-31 18:26:08,233] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1900[0m
[32m[2022-08-31 18:26:08,233] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:26:10,068] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1900/tokenizer_config.json[0m
[32m[2022-08-31 18:26:10,068] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1900/special_tokens_map.json[0m
[32m[2022-08-31 18:26:14,798] [    INFO][0m - loss: 0.02401067, learning_rate: 2.6154362416107382e-05, global_step: 1910, interval_runtime: 8.7457, interval_samples_per_second: 0.915, interval_steps_per_second: 1.143, epoch: 12.8188[0m
[32m[2022-08-31 18:26:15,427] [    INFO][0m - loss: 0.00033175, learning_rate: 2.6134228187919465e-05, global_step: 1920, interval_runtime: 0.6292, interval_samples_per_second: 12.715, interval_steps_per_second: 15.894, epoch: 12.8859[0m
[32m[2022-08-31 18:26:16,072] [    INFO][0m - loss: 6.03e-05, learning_rate: 2.6114093959731544e-05, global_step: 1930, interval_runtime: 0.6457, interval_samples_per_second: 12.389, interval_steps_per_second: 15.486, epoch: 12.953[0m
[32m[2022-08-31 18:26:16,717] [    INFO][0m - loss: 0.00177566, learning_rate: 2.6093959731543626e-05, global_step: 1940, interval_runtime: 0.6441, interval_samples_per_second: 12.42, interval_steps_per_second: 15.526, epoch: 13.0201[0m
[32m[2022-08-31 18:26:17,352] [    INFO][0m - loss: 1.153e-05, learning_rate: 2.6073825503355705e-05, global_step: 1950, interval_runtime: 0.6356, interval_samples_per_second: 12.586, interval_steps_per_second: 15.732, epoch: 13.0872[0m
[32m[2022-08-31 18:26:18,028] [    INFO][0m - loss: 0.00091683, learning_rate: 2.6053691275167787e-05, global_step: 1960, interval_runtime: 0.6755, interval_samples_per_second: 11.843, interval_steps_per_second: 14.804, epoch: 13.1544[0m
[32m[2022-08-31 18:26:18,672] [    INFO][0m - loss: 0.00023102, learning_rate: 2.603355704697987e-05, global_step: 1970, interval_runtime: 0.6442, interval_samples_per_second: 12.419, interval_steps_per_second: 15.523, epoch: 13.2215[0m
[32m[2022-08-31 18:26:19,294] [    INFO][0m - loss: 0.06207703, learning_rate: 2.6013422818791945e-05, global_step: 1980, interval_runtime: 0.6219, interval_samples_per_second: 12.864, interval_steps_per_second: 16.08, epoch: 13.2886[0m
[32m[2022-08-31 18:26:19,940] [    INFO][0m - loss: 0.00059229, learning_rate: 2.5993288590604027e-05, global_step: 1990, interval_runtime: 0.6467, interval_samples_per_second: 12.371, interval_steps_per_second: 15.463, epoch: 13.3557[0m
[32m[2022-08-31 18:26:20,617] [    INFO][0m - loss: 0.00029101, learning_rate: 2.5973154362416106e-05, global_step: 2000, interval_runtime: 0.6762, interval_samples_per_second: 11.831, interval_steps_per_second: 14.789, epoch: 13.4228[0m
[32m[2022-08-31 18:26:20,617] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:26:20,617] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-08-31 18:26:20,617] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:26:20,617] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:26:20,617] [    INFO][0m -   Total prediction steps = 35[0m
[32m[2022-08-31 18:26:22,752] [    INFO][0m - eval_loss: 4.5313191413879395, eval_accuracy: 0.5482695810564663, eval_runtime: 2.1336, eval_samples_per_second: 514.621, eval_steps_per_second: 16.404, epoch: 13.4228[0m
[32m[2022-08-31 18:26:22,752] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2000[0m
[32m[2022-08-31 18:26:22,752] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:26:24,572] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2000/tokenizer_config.json[0m
[32m[2022-08-31 18:26:24,572] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2000/special_tokens_map.json[0m
[32m[2022-08-31 18:26:29,917] [    INFO][0m - loss: 0.02924466, learning_rate: 2.5953020134228188e-05, global_step: 2010, interval_runtime: 9.3001, interval_samples_per_second: 0.86, interval_steps_per_second: 1.075, epoch: 13.4899[0m
[32m[2022-08-31 18:26:30,586] [    INFO][0m - loss: 0.00030597, learning_rate: 2.593288590604027e-05, global_step: 2020, interval_runtime: 0.6692, interval_samples_per_second: 11.955, interval_steps_per_second: 14.943, epoch: 13.557[0m
[32m[2022-08-31 18:26:31,238] [    INFO][0m - loss: 0.00056624, learning_rate: 2.591275167785235e-05, global_step: 2030, interval_runtime: 0.6517, interval_samples_per_second: 12.276, interval_steps_per_second: 15.345, epoch: 13.6242[0m
[32m[2022-08-31 18:26:32,267] [    INFO][0m - loss: 0.00029467, learning_rate: 2.589261744966443e-05, global_step: 2040, interval_runtime: 0.6758, interval_samples_per_second: 11.839, interval_steps_per_second: 14.798, epoch: 13.6913[0m
[32m[2022-08-31 18:26:32,942] [    INFO][0m - loss: 0.05184419, learning_rate: 2.587248322147651e-05, global_step: 2050, interval_runtime: 1.0283, interval_samples_per_second: 7.78, interval_steps_per_second: 9.725, epoch: 13.7584[0m
[32m[2022-08-31 18:26:33,603] [    INFO][0m - loss: 0.00836881, learning_rate: 2.5852348993288593e-05, global_step: 2060, interval_runtime: 0.6611, interval_samples_per_second: 12.1, interval_steps_per_second: 15.125, epoch: 13.8255[0m
[32m[2022-08-31 18:26:34,227] [    INFO][0m - loss: 0.01312734, learning_rate: 2.583221476510067e-05, global_step: 2070, interval_runtime: 0.6246, interval_samples_per_second: 12.808, interval_steps_per_second: 16.01, epoch: 13.8926[0m
[32m[2022-08-31 18:26:34,865] [    INFO][0m - loss: 0.00019784, learning_rate: 2.581208053691275e-05, global_step: 2080, interval_runtime: 0.6378, interval_samples_per_second: 12.543, interval_steps_per_second: 15.679, epoch: 13.9597[0m
[32m[2022-08-31 18:26:35,524] [    INFO][0m - loss: 0.00183207, learning_rate: 2.5791946308724833e-05, global_step: 2090, interval_runtime: 0.6582, interval_samples_per_second: 12.155, interval_steps_per_second: 15.193, epoch: 14.0268[0m
[32m[2022-08-31 18:26:36,152] [    INFO][0m - loss: 0.00619507, learning_rate: 2.577181208053691e-05, global_step: 2100, interval_runtime: 0.6284, interval_samples_per_second: 12.731, interval_steps_per_second: 15.914, epoch: 14.094[0m
[32m[2022-08-31 18:26:36,153] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:26:36,153] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-08-31 18:26:36,153] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:26:36,153] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:26:36,153] [    INFO][0m -   Total prediction steps = 35[0m
[32m[2022-08-31 18:26:38,273] [    INFO][0m - eval_loss: 4.6280837059021, eval_accuracy: 0.546448087431694, eval_runtime: 2.1199, eval_samples_per_second: 517.955, eval_steps_per_second: 16.51, epoch: 14.094[0m
[32m[2022-08-31 18:26:38,273] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2100[0m
[32m[2022-08-31 18:26:38,273] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:26:40,108] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2100/tokenizer_config.json[0m
[32m[2022-08-31 18:26:40,108] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2100/special_tokens_map.json[0m
[32m[2022-08-31 18:26:44,346] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-31 18:26:44,347] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-100 (score: 0.5573770491803278).[0m
[32m[2022-08-31 18:26:45,002] [    INFO][0m - train_runtime: 325.5157, train_samples_per_second: 364.038, train_steps_per_second: 45.774, train_loss: 0.24318984933498583, epoch: 14.094[0m
[32m[2022-08-31 18:26:45,003] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-31 18:26:45,004] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:26:46,820] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-31 18:26:46,820] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-31 18:26:46,822] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-31 18:26:46,822] [    INFO][0m -   epoch                    =     14.094[0m
[32m[2022-08-31 18:26:46,822] [    INFO][0m -   train_loss               =     0.2432[0m
[32m[2022-08-31 18:26:46,822] [    INFO][0m -   train_runtime            = 0:05:25.51[0m
[32m[2022-08-31 18:26:46,822] [    INFO][0m -   train_samples_per_second =    364.038[0m
[32m[2022-08-31 18:26:46,822] [    INFO][0m -   train_steps_per_second   =     45.774[0m
[32m[2022-08-31 18:26:46,832] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-31 18:26:46,832] [    INFO][0m -   Num examples = 2010[0m
[32m[2022-08-31 18:26:46,832] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:26:46,832] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:26:46,832] [    INFO][0m -   Total prediction steps = 63[0m
[32m[2022-08-31 18:26:50,706] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-31 18:26:50,707] [    INFO][0m -   test_accuracy           =     0.5781[0m
[32m[2022-08-31 18:26:50,707] [    INFO][0m -   test_loss               =     1.3659[0m
[32m[2022-08-31 18:26:50,707] [    INFO][0m -   test_runtime            = 0:00:03.87[0m
[32m[2022-08-31 18:26:50,707] [    INFO][0m -   test_samples_per_second =    518.774[0m
[32m[2022-08-31 18:26:50,707] [    INFO][0m -   test_steps_per_second   =      16.26[0m
[32m[2022-08-31 18:26:50,707] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-31 18:26:50,708] [    INFO][0m -   Num examples = 1500[0m
[32m[2022-08-31 18:26:50,708] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:26:50,708] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:26:50,708] [    INFO][0m -   Total prediction steps = 47[0m
[32m[2022-08-31 18:26:54,564] [    INFO][0m - Predictions for tnewsf saved to ./fewclue_submit_examples.[0m
run.sh: line 65: --model_name_or_path: command not found
 
==========
iflytek
==========
 
[33m[2022-08-31 18:26:59,027] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-31 18:26:59,027] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-31 18:26:59,027] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 18:26:59,027] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-31 18:26:59,027] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 18:26:59,027] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-31 18:26:59,028] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-31 18:26:59,028] [    INFO][0m - early_stop_patience           :20[0m
[32m[2022-08-31 18:26:59,028] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-31 18:26:59,028] [    INFO][0m - model_name_or_path            :roformer_v2_chinese_char_base[0m
[32m[2022-08-31 18:26:59,028] [    INFO][0m - [0m
[32m[2022-08-31 18:26:59,028] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 18:26:59,028] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-31 18:26:59,028] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 18:26:59,028] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-31 18:26:59,028] [    INFO][0m - prompt                        :â€œ{'text':'text_a'}â€è¿™å¥è¯çš„ä¸»é¢˜æ˜¯{'mask'}{'mask'}ã€‚[0m
[32m[2022-08-31 18:26:59,028] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-31 18:26:59,028] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-31 18:26:59,028] [    INFO][0m - task_name                     :iflytek[0m
[32m[2022-08-31 18:26:59,029] [    INFO][0m - [0m
[32m[2022-08-31 18:26:59,029] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/roformer_v2_chinese_char_base/model_state.pdparams[0m
W0831 18:26:59.030625 13338 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0831 18:26:59.034813 13338 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-31 18:27:01,993] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/roformer_v2_chinese_char_base/vocab.txt[0m
[32m[2022-08-31 18:27:02,003] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/roformer_v2_chinese_char_base/tokenizer_config.json[0m
[32m[2022-08-31 18:27:02,003] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/roformer_v2_chinese_char_base/special_tokens_map.json[0m
[32m[2022-08-31 18:27:02,004] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': 'â€œ'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': 'â€è¿™å¥è¯çš„ä¸»é¢˜æ˜¯'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'ã€‚'}][0m
2022-08-31 18:27:02,029 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-31 18:27:02,199] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 18:27:02,199] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-31 18:27:02,199] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 18:27:02,199] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-31 18:27:02,199] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-31 18:27:02,199] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-31 18:27:02,200] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-31 18:27:02,200] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-31 18:27:02,200] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-31 18:27:02,200] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-31 18:27:02,200] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-31 18:27:02,200] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-31 18:27:02,200] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-31 18:27:02,200] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-31 18:27:02,200] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-31 18:27:02,200] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-31 18:27:02,200] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-31 18:27:02,200] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-31 18:27:02,200] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-31 18:27:02,201] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-31 18:27:02,201] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-31 18:27:02,201] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-31 18:27:02,201] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-31 18:27:02,201] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-31 18:27:02,201] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-31 18:27:02,201] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-31 18:27:02,201] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-31 18:27:02,201] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-31 18:27:02,201] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-31 18:27:02,201] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-31 18:27:02,201] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-31 18:27:02,201] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-31 18:27:02,202] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-31 18:27:02,202] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-31 18:27:02,202] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-31 18:27:02,202] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-31 18:27:02,202] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-31 18:27:02,202] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug31_18-26-59_instance-3bwob41y-01[0m
[32m[2022-08-31 18:27:02,202] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-31 18:27:02,202] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-31 18:27:02,202] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-31 18:27:02,202] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-31 18:27:02,202] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-31 18:27:02,202] [    INFO][0m - max_seq_length                :320[0m
[32m[2022-08-31 18:27:02,202] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-31 18:27:02,202] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-31 18:27:02,203] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-31 18:27:02,203] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-31 18:27:02,203] [    INFO][0m - num_train_epochs              :100.0[0m
[32m[2022-08-31 18:27:02,203] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-31 18:27:02,203] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-31 18:27:02,203] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-31 18:27:02,203] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-31 18:27:02,203] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-31 18:27:02,203] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-31 18:27:02,203] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-31 18:27:02,203] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-31 18:27:02,203] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-31 18:27:02,203] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-31 18:27:02,203] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-31 18:27:02,204] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-31 18:27:02,204] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-31 18:27:02,204] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-31 18:27:02,204] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-31 18:27:02,204] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-31 18:27:02,204] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-31 18:27:02,204] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-31 18:27:02,204] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-31 18:27:02,204] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-31 18:27:02,204] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-31 18:27:02,204] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-31 18:27:02,204] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-31 18:27:02,204] [    INFO][0m - seed                          :42[0m
[32m[2022-08-31 18:27:02,205] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-31 18:27:02,205] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-31 18:27:02,205] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-31 18:27:02,205] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-31 18:27:02,205] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-31 18:27:02,205] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-31 18:27:02,205] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-31 18:27:02,205] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-31 18:27:02,205] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-31 18:27:02,205] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-31 18:27:02,205] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-31 18:27:02,205] [    INFO][0m - [0m
[32m[2022-08-31 18:27:02,207] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-31 18:27:02,207] [    INFO][0m -   Num examples = 3024[0m
[32m[2022-08-31 18:27:02,207] [    INFO][0m -   Num Epochs = 100[0m
[32m[2022-08-31 18:27:02,207] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-31 18:27:02,207] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-31 18:27:02,207] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-31 18:27:02,207] [    INFO][0m -   Total optimization steps = 37800.0[0m
[32m[2022-08-31 18:27:02,207] [    INFO][0m -   Total num train samples = 302400[0m
[32m[2022-08-31 18:27:04,475] [    INFO][0m - loss: 3.25984955, learning_rate: 2.999206349206349e-05, global_step: 10, interval_runtime: 2.2671, interval_samples_per_second: 3.529, interval_steps_per_second: 4.411, epoch: 0.0265[0m
[32m[2022-08-31 18:27:06,027] [    INFO][0m - loss: 2.78352985, learning_rate: 2.9984126984126986e-05, global_step: 20, interval_runtime: 1.551, interval_samples_per_second: 5.158, interval_steps_per_second: 6.447, epoch: 0.0529[0m
[32m[2022-08-31 18:27:07,583] [    INFO][0m - loss: 2.89117947, learning_rate: 2.9976190476190477e-05, global_step: 30, interval_runtime: 1.5567, interval_samples_per_second: 5.139, interval_steps_per_second: 6.424, epoch: 0.0794[0m
[32m[2022-08-31 18:27:09,141] [    INFO][0m - loss: 2.48419552, learning_rate: 2.9968253968253967e-05, global_step: 40, interval_runtime: 1.5578, interval_samples_per_second: 5.135, interval_steps_per_second: 6.419, epoch: 0.1058[0m
[32m[2022-08-31 18:27:10,697] [    INFO][0m - loss: 2.85063744, learning_rate: 2.996031746031746e-05, global_step: 50, interval_runtime: 1.5559, interval_samples_per_second: 5.142, interval_steps_per_second: 6.427, epoch: 0.1323[0m
[32m[2022-08-31 18:27:12,245] [    INFO][0m - loss: 2.90624695, learning_rate: 2.9952380952380952e-05, global_step: 60, interval_runtime: 1.5487, interval_samples_per_second: 5.166, interval_steps_per_second: 6.457, epoch: 0.1587[0m
[32m[2022-08-31 18:27:13,788] [    INFO][0m - loss: 2.80386391, learning_rate: 2.9944444444444443e-05, global_step: 70, interval_runtime: 1.5429, interval_samples_per_second: 5.185, interval_steps_per_second: 6.481, epoch: 0.1852[0m
[32m[2022-08-31 18:27:15,332] [    INFO][0m - loss: 2.06956787, learning_rate: 2.9936507936507937e-05, global_step: 80, interval_runtime: 1.5439, interval_samples_per_second: 5.182, interval_steps_per_second: 6.477, epoch: 0.2116[0m
[32m[2022-08-31 18:27:16,895] [    INFO][0m - loss: 2.28889122, learning_rate: 2.9928571428571428e-05, global_step: 90, interval_runtime: 1.5621, interval_samples_per_second: 5.121, interval_steps_per_second: 6.402, epoch: 0.2381[0m
[32m[2022-08-31 18:27:18,462] [    INFO][0m - loss: 2.35642662, learning_rate: 2.992063492063492e-05, global_step: 100, interval_runtime: 1.5671, interval_samples_per_second: 5.105, interval_steps_per_second: 6.381, epoch: 0.2646[0m
[32m[2022-08-31 18:27:18,463] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:27:18,463] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 18:27:18,463] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:27:18,464] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:27:18,464] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 18:27:30,368] [    INFO][0m - eval_loss: 2.1133363246917725, eval_accuracy: 0.5010924981791697, eval_runtime: 11.9041, eval_samples_per_second: 115.338, eval_steps_per_second: 3.612, epoch: 0.2646[0m
[32m[2022-08-31 18:27:30,369] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-31 18:27:30,369] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:27:32,244] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-31 18:27:32,246] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-31 18:27:38,192] [    INFO][0m - loss: 2.06172695, learning_rate: 2.9912698412698416e-05, global_step: 110, interval_runtime: 19.7305, interval_samples_per_second: 0.405, interval_steps_per_second: 0.507, epoch: 0.291[0m
[32m[2022-08-31 18:27:39,743] [    INFO][0m - loss: 2.32363701, learning_rate: 2.9904761904761907e-05, global_step: 120, interval_runtime: 1.5507, interval_samples_per_second: 5.159, interval_steps_per_second: 6.449, epoch: 0.3175[0m
[32m[2022-08-31 18:27:41,291] [    INFO][0m - loss: 2.10136395, learning_rate: 2.9896825396825398e-05, global_step: 130, interval_runtime: 1.5471, interval_samples_per_second: 5.171, interval_steps_per_second: 6.464, epoch: 0.3439[0m
[32m[2022-08-31 18:27:42,867] [    INFO][0m - loss: 2.16799355, learning_rate: 2.9888888888888892e-05, global_step: 140, interval_runtime: 1.5771, interval_samples_per_second: 5.073, interval_steps_per_second: 6.341, epoch: 0.3704[0m
[32m[2022-08-31 18:27:44,423] [    INFO][0m - loss: 2.32503796, learning_rate: 2.9880952380952383e-05, global_step: 150, interval_runtime: 1.5549, interval_samples_per_second: 5.145, interval_steps_per_second: 6.431, epoch: 0.3968[0m
[32m[2022-08-31 18:27:45,981] [    INFO][0m - loss: 2.49752274, learning_rate: 2.9873015873015874e-05, global_step: 160, interval_runtime: 1.5594, interval_samples_per_second: 5.13, interval_steps_per_second: 6.413, epoch: 0.4233[0m
[32m[2022-08-31 18:27:47,540] [    INFO][0m - loss: 1.87412186, learning_rate: 2.9865079365079368e-05, global_step: 170, interval_runtime: 1.5573, interval_samples_per_second: 5.137, interval_steps_per_second: 6.421, epoch: 0.4497[0m
[32m[2022-08-31 18:27:49,090] [    INFO][0m - loss: 2.44811077, learning_rate: 2.985714285714286e-05, global_step: 180, interval_runtime: 1.5515, interval_samples_per_second: 5.156, interval_steps_per_second: 6.445, epoch: 0.4762[0m
[32m[2022-08-31 18:27:50,673] [    INFO][0m - loss: 1.85510521, learning_rate: 2.984920634920635e-05, global_step: 190, interval_runtime: 1.5826, interval_samples_per_second: 5.055, interval_steps_per_second: 6.319, epoch: 0.5026[0m
[32m[2022-08-31 18:27:52,232] [    INFO][0m - loss: 2.10655441, learning_rate: 2.984126984126984e-05, global_step: 200, interval_runtime: 1.5595, interval_samples_per_second: 5.13, interval_steps_per_second: 6.412, epoch: 0.5291[0m
[32m[2022-08-31 18:27:52,233] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:27:52,233] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 18:27:52,234] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:27:52,234] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:27:52,234] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 18:28:04,218] [    INFO][0m - eval_loss: 1.9835325479507446, eval_accuracy: 0.5032774945375091, eval_runtime: 11.9833, eval_samples_per_second: 114.576, eval_steps_per_second: 3.588, epoch: 0.5291[0m
[32m[2022-08-31 18:28:04,219] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-31 18:28:04,219] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:28:06,144] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-31 18:28:06,145] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-31 18:28:12,147] [    INFO][0m - loss: 1.88159561, learning_rate: 2.9833333333333335e-05, global_step: 210, interval_runtime: 19.9137, interval_samples_per_second: 0.402, interval_steps_per_second: 0.502, epoch: 0.5556[0m
[32m[2022-08-31 18:28:13,711] [    INFO][0m - loss: 2.17753601, learning_rate: 2.9825396825396825e-05, global_step: 220, interval_runtime: 1.5639, interval_samples_per_second: 5.116, interval_steps_per_second: 6.394, epoch: 0.582[0m
[32m[2022-08-31 18:28:15,262] [    INFO][0m - loss: 1.96569061, learning_rate: 2.9817460317460316e-05, global_step: 230, interval_runtime: 1.5516, interval_samples_per_second: 5.156, interval_steps_per_second: 6.445, epoch: 0.6085[0m
[32m[2022-08-31 18:28:16,811] [    INFO][0m - loss: 1.85714073, learning_rate: 2.980952380952381e-05, global_step: 240, interval_runtime: 1.5492, interval_samples_per_second: 5.164, interval_steps_per_second: 6.455, epoch: 0.6349[0m
[32m[2022-08-31 18:28:18,371] [    INFO][0m - loss: 1.75128002, learning_rate: 2.98015873015873e-05, global_step: 250, interval_runtime: 1.5594, interval_samples_per_second: 5.13, interval_steps_per_second: 6.413, epoch: 0.6614[0m
[32m[2022-08-31 18:28:19,927] [    INFO][0m - loss: 1.86551476, learning_rate: 2.9793650793650792e-05, global_step: 260, interval_runtime: 1.557, interval_samples_per_second: 5.138, interval_steps_per_second: 6.423, epoch: 0.6878[0m
[32m[2022-08-31 18:28:21,485] [    INFO][0m - loss: 1.65109024, learning_rate: 2.9785714285714286e-05, global_step: 270, interval_runtime: 1.5563, interval_samples_per_second: 5.14, interval_steps_per_second: 6.425, epoch: 0.7143[0m
[32m[2022-08-31 18:28:23,047] [    INFO][0m - loss: 2.13199863, learning_rate: 2.9777777777777777e-05, global_step: 280, interval_runtime: 1.5635, interval_samples_per_second: 5.117, interval_steps_per_second: 6.396, epoch: 0.7407[0m
[32m[2022-08-31 18:28:24,613] [    INFO][0m - loss: 1.88800373, learning_rate: 2.9769841269841268e-05, global_step: 290, interval_runtime: 1.566, interval_samples_per_second: 5.109, interval_steps_per_second: 6.386, epoch: 0.7672[0m
[32m[2022-08-31 18:28:26,167] [    INFO][0m - loss: 1.784338, learning_rate: 2.9761904761904762e-05, global_step: 300, interval_runtime: 1.5539, interval_samples_per_second: 5.148, interval_steps_per_second: 6.436, epoch: 0.7937[0m
[32m[2022-08-31 18:28:26,169] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:28:26,169] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 18:28:26,169] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:28:26,169] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:28:26,169] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 18:28:37,995] [    INFO][0m - eval_loss: 1.8826019763946533, eval_accuracy: 0.5375091041514931, eval_runtime: 11.8253, eval_samples_per_second: 116.107, eval_steps_per_second: 3.636, epoch: 0.7937[0m
[32m[2022-08-31 18:28:37,996] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-31 18:28:37,996] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:28:40,236] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-31 18:28:40,236] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-31 18:28:46,163] [    INFO][0m - loss: 1.94394226, learning_rate: 2.9753968253968256e-05, global_step: 310, interval_runtime: 19.9962, interval_samples_per_second: 0.4, interval_steps_per_second: 0.5, epoch: 0.8201[0m
[32m[2022-08-31 18:28:47,712] [    INFO][0m - loss: 1.64593582, learning_rate: 2.9746031746031747e-05, global_step: 320, interval_runtime: 1.5489, interval_samples_per_second: 5.165, interval_steps_per_second: 6.456, epoch: 0.8466[0m
[32m[2022-08-31 18:28:49,265] [    INFO][0m - loss: 1.89079571, learning_rate: 2.973809523809524e-05, global_step: 330, interval_runtime: 1.553, interval_samples_per_second: 5.151, interval_steps_per_second: 6.439, epoch: 0.873[0m
[32m[2022-08-31 18:28:50,816] [    INFO][0m - loss: 1.77127972, learning_rate: 2.9730158730158732e-05, global_step: 340, interval_runtime: 1.5512, interval_samples_per_second: 5.157, interval_steps_per_second: 6.447, epoch: 0.8995[0m
[32m[2022-08-31 18:28:52,363] [    INFO][0m - loss: 2.12223568, learning_rate: 2.9722222222222223e-05, global_step: 350, interval_runtime: 1.5467, interval_samples_per_second: 5.172, interval_steps_per_second: 6.465, epoch: 0.9259[0m
[32m[2022-08-31 18:28:53,911] [    INFO][0m - loss: 2.2722517, learning_rate: 2.9714285714285717e-05, global_step: 360, interval_runtime: 1.548, interval_samples_per_second: 5.168, interval_steps_per_second: 6.46, epoch: 0.9524[0m
[32m[2022-08-31 18:28:55,467] [    INFO][0m - loss: 1.82720356, learning_rate: 2.9706349206349208e-05, global_step: 370, interval_runtime: 1.5564, interval_samples_per_second: 5.14, interval_steps_per_second: 6.425, epoch: 0.9788[0m
[32m[2022-08-31 18:28:57,153] [    INFO][0m - loss: 1.45459242, learning_rate: 2.96984126984127e-05, global_step: 380, interval_runtime: 1.6858, interval_samples_per_second: 4.745, interval_steps_per_second: 5.932, epoch: 1.0053[0m
[32m[2022-08-31 18:28:58,709] [    INFO][0m - loss: 1.34402475, learning_rate: 2.9690476190476193e-05, global_step: 390, interval_runtime: 1.5489, interval_samples_per_second: 5.165, interval_steps_per_second: 6.456, epoch: 1.0317[0m
[32m[2022-08-31 18:29:00,258] [    INFO][0m - loss: 0.91415739, learning_rate: 2.9682539682539683e-05, global_step: 400, interval_runtime: 1.556, interval_samples_per_second: 5.142, interval_steps_per_second: 6.427, epoch: 1.0582[0m
[32m[2022-08-31 18:29:00,259] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:29:00,259] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 18:29:00,259] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:29:00,259] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:29:00,259] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 18:29:12,031] [    INFO][0m - eval_loss: 1.9631365537643433, eval_accuracy: 0.538965768390386, eval_runtime: 11.7713, eval_samples_per_second: 116.64, eval_steps_per_second: 3.653, epoch: 1.0582[0m
[32m[2022-08-31 18:29:12,032] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-31 18:29:12,032] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:29:14,744] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-31 18:29:14,745] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-31 18:29:20,604] [    INFO][0m - loss: 1.08392076, learning_rate: 2.9674603174603174e-05, global_step: 410, interval_runtime: 20.3457, interval_samples_per_second: 0.393, interval_steps_per_second: 0.492, epoch: 1.0847[0m
[32m[2022-08-31 18:29:22,151] [    INFO][0m - loss: 1.21842098, learning_rate: 2.966666666666667e-05, global_step: 420, interval_runtime: 1.5466, interval_samples_per_second: 5.173, interval_steps_per_second: 6.466, epoch: 1.1111[0m
[32m[2022-08-31 18:29:23,700] [    INFO][0m - loss: 1.06395588, learning_rate: 2.965873015873016e-05, global_step: 430, interval_runtime: 1.5501, interval_samples_per_second: 5.161, interval_steps_per_second: 6.451, epoch: 1.1376[0m
[32m[2022-08-31 18:29:25,252] [    INFO][0m - loss: 1.16349211, learning_rate: 2.965079365079365e-05, global_step: 440, interval_runtime: 1.5516, interval_samples_per_second: 5.156, interval_steps_per_second: 6.445, epoch: 1.164[0m
[32m[2022-08-31 18:29:26,793] [    INFO][0m - loss: 1.21272936, learning_rate: 2.9642857142857144e-05, global_step: 450, interval_runtime: 1.5405, interval_samples_per_second: 5.193, interval_steps_per_second: 6.491, epoch: 1.1905[0m
[32m[2022-08-31 18:29:28,335] [    INFO][0m - loss: 1.33678207, learning_rate: 2.9634920634920635e-05, global_step: 460, interval_runtime: 1.5421, interval_samples_per_second: 5.188, interval_steps_per_second: 6.485, epoch: 1.2169[0m
[32m[2022-08-31 18:29:29,878] [    INFO][0m - loss: 1.30266066, learning_rate: 2.9626984126984126e-05, global_step: 470, interval_runtime: 1.5429, interval_samples_per_second: 5.185, interval_steps_per_second: 6.481, epoch: 1.2434[0m
[32m[2022-08-31 18:29:31,426] [    INFO][0m - loss: 1.15071955, learning_rate: 2.961904761904762e-05, global_step: 480, interval_runtime: 1.5482, interval_samples_per_second: 5.167, interval_steps_per_second: 6.459, epoch: 1.2698[0m
[32m[2022-08-31 18:29:32,972] [    INFO][0m - loss: 1.28328648, learning_rate: 2.961111111111111e-05, global_step: 490, interval_runtime: 1.5462, interval_samples_per_second: 5.174, interval_steps_per_second: 6.467, epoch: 1.2963[0m
[32m[2022-08-31 18:29:34,520] [    INFO][0m - loss: 1.59028177, learning_rate: 2.96031746031746e-05, global_step: 500, interval_runtime: 1.5477, interval_samples_per_second: 5.169, interval_steps_per_second: 6.461, epoch: 1.3228[0m
[32m[2022-08-31 18:29:34,521] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:29:34,521] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 18:29:34,521] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:29:34,521] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:29:34,521] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 18:29:46,351] [    INFO][0m - eval_loss: 1.8973033428192139, eval_accuracy: 0.549890750182083, eval_runtime: 11.8295, eval_samples_per_second: 116.066, eval_steps_per_second: 3.635, epoch: 1.3228[0m
[32m[2022-08-31 18:29:46,352] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-08-31 18:29:46,352] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:29:48,251] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-08-31 18:29:48,251] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-08-31 18:29:56,108] [    INFO][0m - loss: 1.58281622, learning_rate: 2.95952380952381e-05, global_step: 510, interval_runtime: 21.588, interval_samples_per_second: 0.371, interval_steps_per_second: 0.463, epoch: 1.3492[0m
[32m[2022-08-31 18:29:57,662] [    INFO][0m - loss: 1.19444389, learning_rate: 2.958730158730159e-05, global_step: 520, interval_runtime: 1.5539, interval_samples_per_second: 5.148, interval_steps_per_second: 6.435, epoch: 1.3757[0m
[32m[2022-08-31 18:29:59,206] [    INFO][0m - loss: 1.09399662, learning_rate: 2.957936507936508e-05, global_step: 530, interval_runtime: 1.5432, interval_samples_per_second: 5.184, interval_steps_per_second: 6.48, epoch: 1.4021[0m
[32m[2022-08-31 18:30:00,772] [    INFO][0m - loss: 1.28885441, learning_rate: 2.9571428571428575e-05, global_step: 540, interval_runtime: 1.5659, interval_samples_per_second: 5.109, interval_steps_per_second: 6.386, epoch: 1.4286[0m
[32m[2022-08-31 18:30:02,319] [    INFO][0m - loss: 1.17276001, learning_rate: 2.9563492063492066e-05, global_step: 550, interval_runtime: 1.548, interval_samples_per_second: 5.168, interval_steps_per_second: 6.46, epoch: 1.455[0m
[32m[2022-08-31 18:30:03,867] [    INFO][0m - loss: 1.33787651, learning_rate: 2.9555555555555556e-05, global_step: 560, interval_runtime: 1.5482, interval_samples_per_second: 5.167, interval_steps_per_second: 6.459, epoch: 1.4815[0m
[32m[2022-08-31 18:30:05,404] [    INFO][0m - loss: 1.43700209, learning_rate: 2.954761904761905e-05, global_step: 570, interval_runtime: 1.5373, interval_samples_per_second: 5.204, interval_steps_per_second: 6.505, epoch: 1.5079[0m
[32m[2022-08-31 18:30:06,950] [    INFO][0m - loss: 1.32765532, learning_rate: 2.953968253968254e-05, global_step: 580, interval_runtime: 1.5451, interval_samples_per_second: 5.178, interval_steps_per_second: 6.472, epoch: 1.5344[0m
[32m[2022-08-31 18:30:08,496] [    INFO][0m - loss: 1.7135498, learning_rate: 2.9531746031746032e-05, global_step: 590, interval_runtime: 1.5465, interval_samples_per_second: 5.173, interval_steps_per_second: 6.466, epoch: 1.5608[0m
[32m[2022-08-31 18:30:10,043] [    INFO][0m - loss: 1.34066429, learning_rate: 2.9523809523809523e-05, global_step: 600, interval_runtime: 1.5473, interval_samples_per_second: 5.17, interval_steps_per_second: 6.463, epoch: 1.5873[0m
[32m[2022-08-31 18:30:10,044] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:30:10,044] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 18:30:10,044] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:30:10,044] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:30:10,044] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 18:30:21,780] [    INFO][0m - eval_loss: 1.867600917816162, eval_accuracy: 0.5579024034959942, eval_runtime: 11.735, eval_samples_per_second: 117.0, eval_steps_per_second: 3.664, epoch: 1.5873[0m
[32m[2022-08-31 18:30:21,780] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-08-31 18:30:21,781] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:30:23,593] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-08-31 18:30:23,593] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-08-31 18:30:30,729] [    INFO][0m - loss: 1.2647625, learning_rate: 2.9515873015873017e-05, global_step: 610, interval_runtime: 20.6858, interval_samples_per_second: 0.387, interval_steps_per_second: 0.483, epoch: 1.6138[0m
[32m[2022-08-31 18:30:32,277] [    INFO][0m - loss: 1.40981016, learning_rate: 2.9507936507936508e-05, global_step: 620, interval_runtime: 1.5478, interval_samples_per_second: 5.169, interval_steps_per_second: 6.461, epoch: 1.6402[0m
[32m[2022-08-31 18:30:33,820] [    INFO][0m - loss: 1.08438225, learning_rate: 2.95e-05, global_step: 630, interval_runtime: 1.5438, interval_samples_per_second: 5.182, interval_steps_per_second: 6.477, epoch: 1.6667[0m
[32m[2022-08-31 18:30:35,377] [    INFO][0m - loss: 1.32440081, learning_rate: 2.9492063492063493e-05, global_step: 640, interval_runtime: 1.5562, interval_samples_per_second: 5.141, interval_steps_per_second: 6.426, epoch: 1.6931[0m
[32m[2022-08-31 18:30:36,924] [    INFO][0m - loss: 1.39744616, learning_rate: 2.9484126984126984e-05, global_step: 650, interval_runtime: 1.5472, interval_samples_per_second: 5.171, interval_steps_per_second: 6.463, epoch: 1.7196[0m
[32m[2022-08-31 18:30:38,477] [    INFO][0m - loss: 1.60478649, learning_rate: 2.9476190476190475e-05, global_step: 660, interval_runtime: 1.5529, interval_samples_per_second: 5.152, interval_steps_per_second: 6.439, epoch: 1.746[0m
[32m[2022-08-31 18:30:40,019] [    INFO][0m - loss: 1.35073662, learning_rate: 2.946825396825397e-05, global_step: 670, interval_runtime: 1.5421, interval_samples_per_second: 5.188, interval_steps_per_second: 6.485, epoch: 1.7725[0m
[32m[2022-08-31 18:30:41,564] [    INFO][0m - loss: 1.59602852, learning_rate: 2.946031746031746e-05, global_step: 680, interval_runtime: 1.5446, interval_samples_per_second: 5.179, interval_steps_per_second: 6.474, epoch: 1.7989[0m
[32m[2022-08-31 18:30:43,116] [    INFO][0m - loss: 1.339746, learning_rate: 2.945238095238095e-05, global_step: 690, interval_runtime: 1.5525, interval_samples_per_second: 5.153, interval_steps_per_second: 6.441, epoch: 1.8254[0m
[32m[2022-08-31 18:30:44,666] [    INFO][0m - loss: 1.31678839, learning_rate: 2.9444444444444445e-05, global_step: 700, interval_runtime: 1.55, interval_samples_per_second: 5.161, interval_steps_per_second: 6.452, epoch: 1.8519[0m
[32m[2022-08-31 18:30:44,667] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:30:44,667] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 18:30:44,667] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:30:44,667] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:30:44,668] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 18:30:56,567] [    INFO][0m - eval_loss: 1.9585535526275635, eval_accuracy: 0.538965768390386, eval_runtime: 11.8996, eval_samples_per_second: 115.382, eval_steps_per_second: 3.614, epoch: 1.8519[0m
[32m[2022-08-31 18:30:56,568] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-700[0m
[32m[2022-08-31 18:30:56,568] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:30:58,436] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-700/tokenizer_config.json[0m
[32m[2022-08-31 18:30:58,436] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-700/special_tokens_map.json[0m
[32m[2022-08-31 18:31:04,355] [    INFO][0m - loss: 1.34911566, learning_rate: 2.943650793650794e-05, global_step: 710, interval_runtime: 19.6888, interval_samples_per_second: 0.406, interval_steps_per_second: 0.508, epoch: 1.8783[0m
[32m[2022-08-31 18:31:05,897] [    INFO][0m - loss: 1.21072168, learning_rate: 2.942857142857143e-05, global_step: 720, interval_runtime: 1.5417, interval_samples_per_second: 5.189, interval_steps_per_second: 6.487, epoch: 1.9048[0m
[32m[2022-08-31 18:31:07,457] [    INFO][0m - loss: 1.45830917, learning_rate: 2.9420634920634924e-05, global_step: 730, interval_runtime: 1.5602, interval_samples_per_second: 5.128, interval_steps_per_second: 6.41, epoch: 1.9312[0m
[32m[2022-08-31 18:31:09,005] [    INFO][0m - loss: 1.30328369, learning_rate: 2.9412698412698414e-05, global_step: 740, interval_runtime: 1.5476, interval_samples_per_second: 5.169, interval_steps_per_second: 6.462, epoch: 1.9577[0m
[32m[2022-08-31 18:31:10,566] [    INFO][0m - loss: 1.10049229, learning_rate: 2.9404761904761905e-05, global_step: 750, interval_runtime: 1.5615, interval_samples_per_second: 5.123, interval_steps_per_second: 6.404, epoch: 1.9841[0m
[32m[2022-08-31 18:31:12,233] [    INFO][0m - loss: 0.9038455, learning_rate: 2.93968253968254e-05, global_step: 760, interval_runtime: 1.6669, interval_samples_per_second: 4.799, interval_steps_per_second: 5.999, epoch: 2.0106[0m
[32m[2022-08-31 18:31:13,836] [    INFO][0m - loss: 0.58070064, learning_rate: 2.938888888888889e-05, global_step: 770, interval_runtime: 1.6037, interval_samples_per_second: 4.989, interval_steps_per_second: 6.236, epoch: 2.037[0m
[32m[2022-08-31 18:31:15,403] [    INFO][0m - loss: 0.73944707, learning_rate: 2.938095238095238e-05, global_step: 780, interval_runtime: 1.5664, interval_samples_per_second: 5.107, interval_steps_per_second: 6.384, epoch: 2.0635[0m
[32m[2022-08-31 18:31:16,953] [    INFO][0m - loss: 0.77606335, learning_rate: 2.9373015873015875e-05, global_step: 790, interval_runtime: 1.5506, interval_samples_per_second: 5.159, interval_steps_per_second: 6.449, epoch: 2.0899[0m
[32m[2022-08-31 18:31:18,499] [    INFO][0m - loss: 0.55463529, learning_rate: 2.9365079365079366e-05, global_step: 800, interval_runtime: 1.5454, interval_samples_per_second: 5.177, interval_steps_per_second: 6.471, epoch: 2.1164[0m
[32m[2022-08-31 18:31:18,500] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:31:18,500] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 18:31:18,500] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:31:18,500] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:31:18,500] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 18:31:30,308] [    INFO][0m - eval_loss: 2.086012601852417, eval_accuracy: 0.5469774217042972, eval_runtime: 11.8076, eval_samples_per_second: 116.281, eval_steps_per_second: 3.642, epoch: 2.1164[0m
[32m[2022-08-31 18:31:30,308] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-800[0m
[32m[2022-08-31 18:31:30,309] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:31:33,724] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-800/tokenizer_config.json[0m
[32m[2022-08-31 18:31:33,724] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-800/special_tokens_map.json[0m
[32m[2022-08-31 18:31:39,888] [    INFO][0m - loss: 0.74549694, learning_rate: 2.9357142857142857e-05, global_step: 810, interval_runtime: 21.3892, interval_samples_per_second: 0.374, interval_steps_per_second: 0.468, epoch: 2.1429[0m
[32m[2022-08-31 18:31:41,752] [    INFO][0m - loss: 0.81727161, learning_rate: 2.934920634920635e-05, global_step: 820, interval_runtime: 1.8637, interval_samples_per_second: 4.293, interval_steps_per_second: 5.366, epoch: 2.1693[0m
[32m[2022-08-31 18:31:43,321] [    INFO][0m - loss: 0.69965515, learning_rate: 2.9341269841269842e-05, global_step: 830, interval_runtime: 1.5688, interval_samples_per_second: 5.099, interval_steps_per_second: 6.374, epoch: 2.1958[0m
[32m[2022-08-31 18:31:44,875] [    INFO][0m - loss: 0.80851669, learning_rate: 2.9333333333333333e-05, global_step: 840, interval_runtime: 1.5544, interval_samples_per_second: 5.147, interval_steps_per_second: 6.433, epoch: 2.2222[0m
[32m[2022-08-31 18:31:46,428] [    INFO][0m - loss: 0.87148333, learning_rate: 2.9325396825396827e-05, global_step: 850, interval_runtime: 1.553, interval_samples_per_second: 5.151, interval_steps_per_second: 6.439, epoch: 2.2487[0m
[32m[2022-08-31 18:31:47,979] [    INFO][0m - loss: 0.76437149, learning_rate: 2.9317460317460318e-05, global_step: 860, interval_runtime: 1.5504, interval_samples_per_second: 5.16, interval_steps_per_second: 6.45, epoch: 2.2751[0m
[32m[2022-08-31 18:31:49,531] [    INFO][0m - loss: 0.80510817, learning_rate: 2.930952380952381e-05, global_step: 870, interval_runtime: 1.5524, interval_samples_per_second: 5.153, interval_steps_per_second: 6.442, epoch: 2.3016[0m
[32m[2022-08-31 18:31:51,075] [    INFO][0m - loss: 0.65722847, learning_rate: 2.9301587301587303e-05, global_step: 880, interval_runtime: 1.5436, interval_samples_per_second: 5.183, interval_steps_per_second: 6.478, epoch: 2.328[0m
[32m[2022-08-31 18:31:52,637] [    INFO][0m - loss: 0.7315001, learning_rate: 2.9293650793650793e-05, global_step: 890, interval_runtime: 1.5624, interval_samples_per_second: 5.12, interval_steps_per_second: 6.4, epoch: 2.3545[0m
[32m[2022-08-31 18:31:54,181] [    INFO][0m - loss: 0.57484293, learning_rate: 2.9285714285714284e-05, global_step: 900, interval_runtime: 1.5437, interval_samples_per_second: 5.183, interval_steps_per_second: 6.478, epoch: 2.381[0m
[32m[2022-08-31 18:31:54,181] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:31:54,181] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 18:31:54,182] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:31:54,182] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:31:54,182] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 18:32:05,986] [    INFO][0m - eval_loss: 2.3694334030151367, eval_accuracy: 0.5309541150764748, eval_runtime: 11.804, eval_samples_per_second: 116.317, eval_steps_per_second: 3.643, epoch: 2.381[0m
[32m[2022-08-31 18:32:05,987] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-900[0m
[32m[2022-08-31 18:32:05,987] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:32:07,840] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-900/tokenizer_config.json[0m
[32m[2022-08-31 18:32:07,840] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-900/special_tokens_map.json[0m
[32m[2022-08-31 18:32:13,458] [    INFO][0m - loss: 0.79570017, learning_rate: 2.9277777777777778e-05, global_step: 910, interval_runtime: 19.277, interval_samples_per_second: 0.415, interval_steps_per_second: 0.519, epoch: 2.4074[0m
[32m[2022-08-31 18:32:15,022] [    INFO][0m - loss: 0.8095377, learning_rate: 2.9269841269841272e-05, global_step: 920, interval_runtime: 1.5645, interval_samples_per_second: 5.114, interval_steps_per_second: 6.392, epoch: 2.4339[0m
[32m[2022-08-31 18:32:16,580] [    INFO][0m - loss: 0.88136635, learning_rate: 2.9261904761904763e-05, global_step: 930, interval_runtime: 1.5577, interval_samples_per_second: 5.136, interval_steps_per_second: 6.42, epoch: 2.4603[0m
[32m[2022-08-31 18:32:18,137] [    INFO][0m - loss: 0.62914972, learning_rate: 2.9253968253968257e-05, global_step: 940, interval_runtime: 1.5568, interval_samples_per_second: 5.139, interval_steps_per_second: 6.423, epoch: 2.4868[0m
[32m[2022-08-31 18:32:19,690] [    INFO][0m - loss: 0.97197056, learning_rate: 2.9246031746031748e-05, global_step: 950, interval_runtime: 1.5531, interval_samples_per_second: 5.151, interval_steps_per_second: 6.439, epoch: 2.5132[0m
[32m[2022-08-31 18:32:21,247] [    INFO][0m - loss: 0.73704166, learning_rate: 2.923809523809524e-05, global_step: 960, interval_runtime: 1.5578, interval_samples_per_second: 5.135, interval_steps_per_second: 6.419, epoch: 2.5397[0m
[32m[2022-08-31 18:32:22,801] [    INFO][0m - loss: 0.86959791, learning_rate: 2.9230158730158733e-05, global_step: 970, interval_runtime: 1.5538, interval_samples_per_second: 5.149, interval_steps_per_second: 6.436, epoch: 2.5661[0m
[32m[2022-08-31 18:32:24,356] [    INFO][0m - loss: 0.76145916, learning_rate: 2.9222222222222224e-05, global_step: 980, interval_runtime: 1.5547, interval_samples_per_second: 5.146, interval_steps_per_second: 6.432, epoch: 2.5926[0m
[32m[2022-08-31 18:32:25,904] [    INFO][0m - loss: 0.67834148, learning_rate: 2.9214285714285715e-05, global_step: 990, interval_runtime: 1.5483, interval_samples_per_second: 5.167, interval_steps_per_second: 6.459, epoch: 2.619[0m
[32m[2022-08-31 18:32:27,452] [    INFO][0m - loss: 0.91147232, learning_rate: 2.9206349206349206e-05, global_step: 1000, interval_runtime: 1.5476, interval_samples_per_second: 5.169, interval_steps_per_second: 6.461, epoch: 2.6455[0m
[32m[2022-08-31 18:32:27,453] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:32:27,453] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 18:32:27,453] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:32:27,453] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:32:27,453] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 18:32:39,420] [    INFO][0m - eval_loss: 2.33744740486145, eval_accuracy: 0.5120174799708667, eval_runtime: 11.9661, eval_samples_per_second: 114.741, eval_steps_per_second: 3.593, epoch: 2.6455[0m
[32m[2022-08-31 18:32:39,420] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1000[0m
[32m[2022-08-31 18:32:39,420] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:32:41,670] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1000/tokenizer_config.json[0m
[32m[2022-08-31 18:32:41,671] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1000/special_tokens_map.json[0m
[32m[2022-08-31 18:32:49,121] [    INFO][0m - loss: 0.81215124, learning_rate: 2.91984126984127e-05, global_step: 1010, interval_runtime: 21.6689, interval_samples_per_second: 0.369, interval_steps_per_second: 0.461, epoch: 2.672[0m
[32m[2022-08-31 18:32:50,678] [    INFO][0m - loss: 0.96405344, learning_rate: 2.919047619047619e-05, global_step: 1020, interval_runtime: 1.5569, interval_samples_per_second: 5.139, interval_steps_per_second: 6.423, epoch: 2.6984[0m
[32m[2022-08-31 18:32:52,218] [    INFO][0m - loss: 0.9233675, learning_rate: 2.918253968253968e-05, global_step: 1030, interval_runtime: 1.5406, interval_samples_per_second: 5.193, interval_steps_per_second: 6.491, epoch: 2.7249[0m
[32m[2022-08-31 18:32:53,768] [    INFO][0m - loss: 0.94828358, learning_rate: 2.9174603174603176e-05, global_step: 1040, interval_runtime: 1.55, interval_samples_per_second: 5.161, interval_steps_per_second: 6.452, epoch: 2.7513[0m
[32m[2022-08-31 18:32:55,317] [    INFO][0m - loss: 1.04895096, learning_rate: 2.9166666666666666e-05, global_step: 1050, interval_runtime: 1.5491, interval_samples_per_second: 5.164, interval_steps_per_second: 6.455, epoch: 2.7778[0m
[32m[2022-08-31 18:32:56,868] [    INFO][0m - loss: 0.77250929, learning_rate: 2.9158730158730157e-05, global_step: 1060, interval_runtime: 1.5509, interval_samples_per_second: 5.158, interval_steps_per_second: 6.448, epoch: 2.8042[0m
[32m[2022-08-31 18:32:58,421] [    INFO][0m - loss: 0.85314274, learning_rate: 2.915079365079365e-05, global_step: 1070, interval_runtime: 1.5529, interval_samples_per_second: 5.152, interval_steps_per_second: 6.44, epoch: 2.8307[0m
[32m[2022-08-31 18:32:59,979] [    INFO][0m - loss: 0.91186161, learning_rate: 2.9142857142857142e-05, global_step: 1080, interval_runtime: 1.5574, interval_samples_per_second: 5.137, interval_steps_per_second: 6.421, epoch: 2.8571[0m
[32m[2022-08-31 18:33:01,519] [    INFO][0m - loss: 0.79186692, learning_rate: 2.9134920634920633e-05, global_step: 1090, interval_runtime: 1.541, interval_samples_per_second: 5.192, interval_steps_per_second: 6.489, epoch: 2.8836[0m
[32m[2022-08-31 18:33:03,086] [    INFO][0m - loss: 0.82997236, learning_rate: 2.9126984126984127e-05, global_step: 1100, interval_runtime: 1.5664, interval_samples_per_second: 5.107, interval_steps_per_second: 6.384, epoch: 2.9101[0m
[32m[2022-08-31 18:33:03,087] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:33:03,087] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 18:33:03,087] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:33:03,087] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:33:03,087] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 18:33:14,966] [    INFO][0m - eval_loss: 2.114851236343384, eval_accuracy: 0.5542607428987618, eval_runtime: 11.8785, eval_samples_per_second: 115.587, eval_steps_per_second: 3.62, epoch: 2.9101[0m
[32m[2022-08-31 18:33:14,967] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1100[0m
[32m[2022-08-31 18:33:14,967] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:33:16,941] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1100/tokenizer_config.json[0m
[32m[2022-08-31 18:33:16,941] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1100/special_tokens_map.json[0m
[32m[2022-08-31 18:33:23,892] [    INFO][0m - loss: 0.89276457, learning_rate: 2.9119047619047618e-05, global_step: 1110, interval_runtime: 20.8059, interval_samples_per_second: 0.385, interval_steps_per_second: 0.481, epoch: 2.9365[0m
[32m[2022-08-31 18:33:25,439] [    INFO][0m - loss: 0.8625104, learning_rate: 2.9111111111111112e-05, global_step: 1120, interval_runtime: 1.5472, interval_samples_per_second: 5.171, interval_steps_per_second: 6.463, epoch: 2.963[0m
[32m[2022-08-31 18:33:26,986] [    INFO][0m - loss: 0.79780722, learning_rate: 2.9103174603174606e-05, global_step: 1130, interval_runtime: 1.5469, interval_samples_per_second: 5.172, interval_steps_per_second: 6.464, epoch: 2.9894[0m
[32m[2022-08-31 18:33:28,637] [    INFO][0m - loss: 0.48519707, learning_rate: 2.9095238095238097e-05, global_step: 1140, interval_runtime: 1.6506, interval_samples_per_second: 4.847, interval_steps_per_second: 6.059, epoch: 3.0159[0m
[32m[2022-08-31 18:33:30,190] [    INFO][0m - loss: 0.49478612, learning_rate: 2.9087301587301588e-05, global_step: 1150, interval_runtime: 1.5531, interval_samples_per_second: 5.151, interval_steps_per_second: 6.439, epoch: 3.0423[0m
[32m[2022-08-31 18:33:31,737] [    INFO][0m - loss: 0.4400816, learning_rate: 2.9079365079365082e-05, global_step: 1160, interval_runtime: 1.547, interval_samples_per_second: 5.171, interval_steps_per_second: 6.464, epoch: 3.0688[0m
[32m[2022-08-31 18:33:33,290] [    INFO][0m - loss: 0.43864846, learning_rate: 2.9071428571428573e-05, global_step: 1170, interval_runtime: 1.5534, interval_samples_per_second: 5.15, interval_steps_per_second: 6.437, epoch: 3.0952[0m
[32m[2022-08-31 18:33:34,865] [    INFO][0m - loss: 0.38982, learning_rate: 2.9063492063492064e-05, global_step: 1180, interval_runtime: 1.5744, interval_samples_per_second: 5.081, interval_steps_per_second: 6.352, epoch: 3.1217[0m
[32m[2022-08-31 18:33:36,949] [    INFO][0m - loss: 0.30069509, learning_rate: 2.9055555555555558e-05, global_step: 1190, interval_runtime: 1.556, interval_samples_per_second: 5.141, interval_steps_per_second: 6.427, epoch: 3.1481[0m
[32m[2022-08-31 18:33:38,502] [    INFO][0m - loss: 0.48243942, learning_rate: 2.904761904761905e-05, global_step: 1200, interval_runtime: 2.0811, interval_samples_per_second: 3.844, interval_steps_per_second: 4.805, epoch: 3.1746[0m
[32m[2022-08-31 18:33:38,503] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:33:38,503] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 18:33:38,503] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:33:38,503] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:33:38,503] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 18:33:50,313] [    INFO][0m - eval_loss: 2.4614360332489014, eval_accuracy: 0.5331391114348143, eval_runtime: 11.8095, eval_samples_per_second: 116.262, eval_steps_per_second: 3.641, epoch: 3.1746[0m
[32m[2022-08-31 18:33:50,314] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1200[0m
[32m[2022-08-31 18:33:50,314] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:33:52,226] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1200/tokenizer_config.json[0m
[32m[2022-08-31 18:33:52,226] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1200/special_tokens_map.json[0m
[32m[2022-08-31 18:33:58,286] [    INFO][0m - loss: 0.58790474, learning_rate: 2.903968253968254e-05, global_step: 1210, interval_runtime: 19.7845, interval_samples_per_second: 0.404, interval_steps_per_second: 0.505, epoch: 3.2011[0m
[32m[2022-08-31 18:33:59,830] [    INFO][0m - loss: 0.43430166, learning_rate: 2.9031746031746034e-05, global_step: 1220, interval_runtime: 1.5439, interval_samples_per_second: 5.182, interval_steps_per_second: 6.477, epoch: 3.2275[0m
[32m[2022-08-31 18:34:01,375] [    INFO][0m - loss: 0.4662199, learning_rate: 2.9023809523809524e-05, global_step: 1230, interval_runtime: 1.5455, interval_samples_per_second: 5.176, interval_steps_per_second: 6.47, epoch: 3.254[0m
[32m[2022-08-31 18:34:02,921] [    INFO][0m - loss: 0.33564479, learning_rate: 2.9015873015873015e-05, global_step: 1240, interval_runtime: 1.5454, interval_samples_per_second: 5.177, interval_steps_per_second: 6.471, epoch: 3.2804[0m
[32m[2022-08-31 18:34:04,471] [    INFO][0m - loss: 0.51607637, learning_rate: 2.900793650793651e-05, global_step: 1250, interval_runtime: 1.5497, interval_samples_per_second: 5.162, interval_steps_per_second: 6.453, epoch: 3.3069[0m
[32m[2022-08-31 18:34:06,024] [    INFO][0m - loss: 0.41908216, learning_rate: 2.9e-05, global_step: 1260, interval_runtime: 1.5534, interval_samples_per_second: 5.15, interval_steps_per_second: 6.437, epoch: 3.3333[0m
[32m[2022-08-31 18:34:07,572] [    INFO][0m - loss: 0.34583795, learning_rate: 2.899206349206349e-05, global_step: 1270, interval_runtime: 1.5481, interval_samples_per_second: 5.168, interval_steps_per_second: 6.459, epoch: 3.3598[0m
[32m[2022-08-31 18:34:09,123] [    INFO][0m - loss: 0.44191737, learning_rate: 2.8984126984126985e-05, global_step: 1280, interval_runtime: 1.5511, interval_samples_per_second: 5.158, interval_steps_per_second: 6.447, epoch: 3.3862[0m
[32m[2022-08-31 18:34:10,666] [    INFO][0m - loss: 0.37847533, learning_rate: 2.8976190476190476e-05, global_step: 1290, interval_runtime: 1.5433, interval_samples_per_second: 5.184, interval_steps_per_second: 6.48, epoch: 3.4127[0m
[32m[2022-08-31 18:34:12,214] [    INFO][0m - loss: 0.21058807, learning_rate: 2.8968253968253967e-05, global_step: 1300, interval_runtime: 1.5471, interval_samples_per_second: 5.171, interval_steps_per_second: 6.464, epoch: 3.4392[0m
[32m[2022-08-31 18:34:12,214] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:34:12,214] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 18:34:12,214] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:34:12,215] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:34:12,215] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 18:34:24,034] [    INFO][0m - eval_loss: 2.5378191471099854, eval_accuracy: 0.5455207574654042, eval_runtime: 11.8192, eval_samples_per_second: 116.167, eval_steps_per_second: 3.638, epoch: 3.4392[0m
[32m[2022-08-31 18:34:24,035] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1300[0m
[32m[2022-08-31 18:34:24,035] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:34:25,856] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1300/tokenizer_config.json[0m
[32m[2022-08-31 18:34:25,856] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1300/special_tokens_map.json[0m
[32m[2022-08-31 18:34:31,559] [    INFO][0m - loss: 0.36103051, learning_rate: 2.896031746031746e-05, global_step: 1310, interval_runtime: 19.3454, interval_samples_per_second: 0.414, interval_steps_per_second: 0.517, epoch: 3.4656[0m
[32m[2022-08-31 18:34:33,096] [    INFO][0m - loss: 0.4841599, learning_rate: 2.8952380952380955e-05, global_step: 1320, interval_runtime: 1.537, interval_samples_per_second: 5.205, interval_steps_per_second: 6.506, epoch: 3.4921[0m
[32m[2022-08-31 18:34:34,639] [    INFO][0m - loss: 0.25887568, learning_rate: 2.8944444444444446e-05, global_step: 1330, interval_runtime: 1.5426, interval_samples_per_second: 5.186, interval_steps_per_second: 6.482, epoch: 3.5185[0m
[32m[2022-08-31 18:34:36,180] [    INFO][0m - loss: 0.46868086, learning_rate: 2.893650793650794e-05, global_step: 1340, interval_runtime: 1.5418, interval_samples_per_second: 5.189, interval_steps_per_second: 6.486, epoch: 3.545[0m
[32m[2022-08-31 18:34:37,738] [    INFO][0m - loss: 0.51610918, learning_rate: 2.892857142857143e-05, global_step: 1350, interval_runtime: 1.5579, interval_samples_per_second: 5.135, interval_steps_per_second: 6.419, epoch: 3.5714[0m
[32m[2022-08-31 18:34:39,284] [    INFO][0m - loss: 0.44727092, learning_rate: 2.892063492063492e-05, global_step: 1360, interval_runtime: 1.5454, interval_samples_per_second: 5.177, interval_steps_per_second: 6.471, epoch: 3.5979[0m
[32m[2022-08-31 18:34:40,828] [    INFO][0m - loss: 0.57799454, learning_rate: 2.8912698412698416e-05, global_step: 1370, interval_runtime: 1.5446, interval_samples_per_second: 5.179, interval_steps_per_second: 6.474, epoch: 3.6243[0m
[32m[2022-08-31 18:34:42,382] [    INFO][0m - loss: 0.53450665, learning_rate: 2.8904761904761907e-05, global_step: 1380, interval_runtime: 1.5532, interval_samples_per_second: 5.151, interval_steps_per_second: 6.438, epoch: 3.6508[0m
[32m[2022-08-31 18:34:43,949] [    INFO][0m - loss: 0.40040827, learning_rate: 2.8896825396825397e-05, global_step: 1390, interval_runtime: 1.567, interval_samples_per_second: 5.105, interval_steps_per_second: 6.382, epoch: 3.6772[0m
[32m[2022-08-31 18:34:45,511] [    INFO][0m - loss: 0.56880741, learning_rate: 2.8888888888888888e-05, global_step: 1400, interval_runtime: 1.5621, interval_samples_per_second: 5.121, interval_steps_per_second: 6.402, epoch: 3.7037[0m
[32m[2022-08-31 18:34:45,511] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:34:45,512] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 18:34:45,512] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:34:45,512] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:34:45,512] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 18:34:57,489] [    INFO][0m - eval_loss: 2.602810859680176, eval_accuracy: 0.5324107793153678, eval_runtime: 11.9763, eval_samples_per_second: 114.643, eval_steps_per_second: 3.59, epoch: 3.7037[0m
[32m[2022-08-31 18:34:57,489] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1400[0m
[32m[2022-08-31 18:34:57,489] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:34:59,393] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1400/tokenizer_config.json[0m
[32m[2022-08-31 18:34:59,393] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1400/special_tokens_map.json[0m
[32m[2022-08-31 18:35:05,562] [    INFO][0m - loss: 0.29905145, learning_rate: 2.8880952380952382e-05, global_step: 1410, interval_runtime: 20.0512, interval_samples_per_second: 0.399, interval_steps_per_second: 0.499, epoch: 3.7302[0m
[32m[2022-08-31 18:35:07,112] [    INFO][0m - loss: 0.36860406, learning_rate: 2.8873015873015873e-05, global_step: 1420, interval_runtime: 1.5496, interval_samples_per_second: 5.163, interval_steps_per_second: 6.453, epoch: 3.7566[0m
[32m[2022-08-31 18:35:08,665] [    INFO][0m - loss: 0.46566305, learning_rate: 2.8865079365079364e-05, global_step: 1430, interval_runtime: 1.5529, interval_samples_per_second: 5.152, interval_steps_per_second: 6.44, epoch: 3.7831[0m
[32m[2022-08-31 18:35:10,216] [    INFO][0m - loss: 0.37631381, learning_rate: 2.8857142857142858e-05, global_step: 1440, interval_runtime: 1.5506, interval_samples_per_second: 5.159, interval_steps_per_second: 6.449, epoch: 3.8095[0m
[32m[2022-08-31 18:35:11,764] [    INFO][0m - loss: 0.80638123, learning_rate: 2.884920634920635e-05, global_step: 1450, interval_runtime: 1.5484, interval_samples_per_second: 5.167, interval_steps_per_second: 6.458, epoch: 3.836[0m
[32m[2022-08-31 18:35:13,314] [    INFO][0m - loss: 0.45980043, learning_rate: 2.884126984126984e-05, global_step: 1460, interval_runtime: 1.5505, interval_samples_per_second: 5.16, interval_steps_per_second: 6.45, epoch: 3.8624[0m
[32m[2022-08-31 18:35:14,877] [    INFO][0m - loss: 0.59534483, learning_rate: 2.8833333333333334e-05, global_step: 1470, interval_runtime: 1.5625, interval_samples_per_second: 5.12, interval_steps_per_second: 6.4, epoch: 3.8889[0m
[32m[2022-08-31 18:35:16,423] [    INFO][0m - loss: 0.65263071, learning_rate: 2.8825396825396825e-05, global_step: 1480, interval_runtime: 1.5464, interval_samples_per_second: 5.173, interval_steps_per_second: 6.467, epoch: 3.9153[0m
[32m[2022-08-31 18:35:17,977] [    INFO][0m - loss: 0.59611917, learning_rate: 2.8817460317460316e-05, global_step: 1490, interval_runtime: 1.5538, interval_samples_per_second: 5.149, interval_steps_per_second: 6.436, epoch: 3.9418[0m
[32m[2022-08-31 18:35:19,536] [    INFO][0m - loss: 0.56091828, learning_rate: 2.880952380952381e-05, global_step: 1500, interval_runtime: 1.5588, interval_samples_per_second: 5.132, interval_steps_per_second: 6.415, epoch: 3.9683[0m
[32m[2022-08-31 18:35:19,536] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:35:19,537] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 18:35:19,537] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:35:19,537] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:35:19,537] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 18:35:31,501] [    INFO][0m - eval_loss: 2.5059475898742676, eval_accuracy: 0.5345957756737072, eval_runtime: 11.9642, eval_samples_per_second: 114.759, eval_steps_per_second: 3.594, epoch: 3.9683[0m
[32m[2022-08-31 18:35:31,502] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1500[0m
[32m[2022-08-31 18:35:31,502] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:35:33,345] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1500/tokenizer_config.json[0m
[32m[2022-08-31 18:35:33,346] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1500/special_tokens_map.json[0m
[32m[2022-08-31 18:35:39,329] [    INFO][0m - loss: 0.56829128, learning_rate: 2.88015873015873e-05, global_step: 1510, interval_runtime: 19.7932, interval_samples_per_second: 0.404, interval_steps_per_second: 0.505, epoch: 3.9947[0m
[32m[2022-08-31 18:35:40,971] [    INFO][0m - loss: 0.33454158, learning_rate: 2.8793650793650795e-05, global_step: 1520, interval_runtime: 1.6419, interval_samples_per_second: 4.872, interval_steps_per_second: 6.091, epoch: 4.0212[0m
[32m[2022-08-31 18:35:42,517] [    INFO][0m - loss: 0.24227479, learning_rate: 2.878571428571429e-05, global_step: 1530, interval_runtime: 1.5467, interval_samples_per_second: 5.172, interval_steps_per_second: 6.465, epoch: 4.0476[0m
[32m[2022-08-31 18:35:44,072] [    INFO][0m - loss: 0.20233943, learning_rate: 2.877777777777778e-05, global_step: 1540, interval_runtime: 1.5539, interval_samples_per_second: 5.148, interval_steps_per_second: 6.435, epoch: 4.0741[0m
[32m[2022-08-31 18:35:45,616] [    INFO][0m - loss: 0.27587166, learning_rate: 2.876984126984127e-05, global_step: 1550, interval_runtime: 1.545, interval_samples_per_second: 5.178, interval_steps_per_second: 6.472, epoch: 4.1005[0m
[32m[2022-08-31 18:35:47,156] [    INFO][0m - loss: 0.15411265, learning_rate: 2.8761904761904765e-05, global_step: 1560, interval_runtime: 1.5401, interval_samples_per_second: 5.194, interval_steps_per_second: 6.493, epoch: 4.127[0m
[32m[2022-08-31 18:35:48,712] [    INFO][0m - loss: 0.25313144, learning_rate: 2.8753968253968255e-05, global_step: 1570, interval_runtime: 1.5557, interval_samples_per_second: 5.142, interval_steps_per_second: 6.428, epoch: 4.1534[0m
[32m[2022-08-31 18:35:50,264] [    INFO][0m - loss: 0.13057209, learning_rate: 2.8746031746031746e-05, global_step: 1580, interval_runtime: 1.5519, interval_samples_per_second: 5.155, interval_steps_per_second: 6.444, epoch: 4.1799[0m
[32m[2022-08-31 18:35:51,824] [    INFO][0m - loss: 0.09267333, learning_rate: 2.873809523809524e-05, global_step: 1590, interval_runtime: 1.5596, interval_samples_per_second: 5.13, interval_steps_per_second: 6.412, epoch: 4.2063[0m
[32m[2022-08-31 18:35:53,387] [    INFO][0m - loss: 0.11723876, learning_rate: 2.873015873015873e-05, global_step: 1600, interval_runtime: 1.5631, interval_samples_per_second: 5.118, interval_steps_per_second: 6.398, epoch: 4.2328[0m
[32m[2022-08-31 18:35:53,388] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:35:53,388] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 18:35:53,388] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:35:53,388] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:35:53,388] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 18:36:05,285] [    INFO][0m - eval_loss: 2.7319815158843994, eval_accuracy: 0.5396941005098325, eval_runtime: 11.8965, eval_samples_per_second: 115.412, eval_steps_per_second: 3.615, epoch: 4.2328[0m
[32m[2022-08-31 18:36:05,285] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1600[0m
[32m[2022-08-31 18:36:05,286] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:36:07,180] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1600/tokenizer_config.json[0m
[32m[2022-08-31 18:36:07,180] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1600/special_tokens_map.json[0m
[32m[2022-08-31 18:36:13,007] [    INFO][0m - loss: 0.32581651, learning_rate: 2.8722222222222222e-05, global_step: 1610, interval_runtime: 19.6203, interval_samples_per_second: 0.408, interval_steps_per_second: 0.51, epoch: 4.2593[0m
[32m[2022-08-31 18:36:14,563] [    INFO][0m - loss: 0.2385731, learning_rate: 2.8714285714285716e-05, global_step: 1620, interval_runtime: 1.5552, interval_samples_per_second: 5.144, interval_steps_per_second: 6.43, epoch: 4.2857[0m
[32m[2022-08-31 18:36:16,130] [    INFO][0m - loss: 0.19714457, learning_rate: 2.8706349206349207e-05, global_step: 1630, interval_runtime: 1.567, interval_samples_per_second: 5.105, interval_steps_per_second: 6.381, epoch: 4.3122[0m
[32m[2022-08-31 18:36:17,681] [    INFO][0m - loss: 0.23689606, learning_rate: 2.8698412698412698e-05, global_step: 1640, interval_runtime: 1.5513, interval_samples_per_second: 5.157, interval_steps_per_second: 6.446, epoch: 4.3386[0m
[32m[2022-08-31 18:36:19,232] [    INFO][0m - loss: 0.32509646, learning_rate: 2.8690476190476192e-05, global_step: 1650, interval_runtime: 1.5513, interval_samples_per_second: 5.157, interval_steps_per_second: 6.446, epoch: 4.3651[0m
[32m[2022-08-31 18:36:20,786] [    INFO][0m - loss: 0.18326938, learning_rate: 2.8682539682539683e-05, global_step: 1660, interval_runtime: 1.5535, interval_samples_per_second: 5.15, interval_steps_per_second: 6.437, epoch: 4.3915[0m
[32m[2022-08-31 18:36:22,325] [    INFO][0m - loss: 0.20824547, learning_rate: 2.8674603174603174e-05, global_step: 1670, interval_runtime: 1.5393, interval_samples_per_second: 5.197, interval_steps_per_second: 6.496, epoch: 4.418[0m
[32m[2022-08-31 18:36:23,892] [    INFO][0m - loss: 0.21416254, learning_rate: 2.8666666666666668e-05, global_step: 1680, interval_runtime: 1.5669, interval_samples_per_second: 5.106, interval_steps_per_second: 6.382, epoch: 4.4444[0m
[32m[2022-08-31 18:36:25,439] [    INFO][0m - loss: 0.15175488, learning_rate: 2.865873015873016e-05, global_step: 1690, interval_runtime: 1.5472, interval_samples_per_second: 5.171, interval_steps_per_second: 6.463, epoch: 4.4709[0m
[32m[2022-08-31 18:36:26,982] [    INFO][0m - loss: 0.15972246, learning_rate: 2.865079365079365e-05, global_step: 1700, interval_runtime: 1.5427, interval_samples_per_second: 5.186, interval_steps_per_second: 6.482, epoch: 4.4974[0m
[32m[2022-08-31 18:36:26,983] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:36:26,983] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 18:36:26,983] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:36:26,983] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:36:26,983] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 18:36:38,875] [    INFO][0m - eval_loss: 2.8106815814971924, eval_accuracy: 0.5200291332847778, eval_runtime: 11.8918, eval_samples_per_second: 115.458, eval_steps_per_second: 3.616, epoch: 4.4974[0m
[32m[2022-08-31 18:36:38,876] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1700[0m
[32m[2022-08-31 18:36:38,876] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:36:40,742] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1700/tokenizer_config.json[0m
[32m[2022-08-31 18:36:40,742] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1700/special_tokens_map.json[0m
[32m[2022-08-31 18:36:46,757] [    INFO][0m - loss: 0.29074717, learning_rate: 2.8642857142857144e-05, global_step: 1710, interval_runtime: 19.7748, interval_samples_per_second: 0.405, interval_steps_per_second: 0.506, epoch: 4.5238[0m
[32m[2022-08-31 18:36:48,323] [    INFO][0m - loss: 0.38710451, learning_rate: 2.8634920634920638e-05, global_step: 1720, interval_runtime: 1.5658, interval_samples_per_second: 5.109, interval_steps_per_second: 6.387, epoch: 4.5503[0m
[32m[2022-08-31 18:36:49,888] [    INFO][0m - loss: 0.23553956, learning_rate: 2.862698412698413e-05, global_step: 1730, interval_runtime: 1.5654, interval_samples_per_second: 5.111, interval_steps_per_second: 6.388, epoch: 4.5767[0m
[32m[2022-08-31 18:36:51,449] [    INFO][0m - loss: 0.17164233, learning_rate: 2.8619047619047623e-05, global_step: 1740, interval_runtime: 1.5611, interval_samples_per_second: 5.125, interval_steps_per_second: 6.406, epoch: 4.6032[0m
[32m[2022-08-31 18:36:53,019] [    INFO][0m - loss: 0.24544935, learning_rate: 2.8611111111111113e-05, global_step: 1750, interval_runtime: 1.5699, interval_samples_per_second: 5.096, interval_steps_per_second: 6.37, epoch: 4.6296[0m
[32m[2022-08-31 18:36:54,577] [    INFO][0m - loss: 0.30133173, learning_rate: 2.8603174603174604e-05, global_step: 1760, interval_runtime: 1.5578, interval_samples_per_second: 5.136, interval_steps_per_second: 6.42, epoch: 4.6561[0m
[32m[2022-08-31 18:36:56,123] [    INFO][0m - loss: 0.2692605, learning_rate: 2.85952380952381e-05, global_step: 1770, interval_runtime: 1.5461, interval_samples_per_second: 5.174, interval_steps_per_second: 6.468, epoch: 4.6825[0m
[32m[2022-08-31 18:36:57,675] [    INFO][0m - loss: 0.20153036, learning_rate: 2.858730158730159e-05, global_step: 1780, interval_runtime: 1.5524, interval_samples_per_second: 5.153, interval_steps_per_second: 6.442, epoch: 4.709[0m
[32m[2022-08-31 18:36:59,227] [    INFO][0m - loss: 0.34556141, learning_rate: 2.857936507936508e-05, global_step: 1790, interval_runtime: 1.552, interval_samples_per_second: 5.155, interval_steps_per_second: 6.443, epoch: 4.7354[0m
[32m[2022-08-31 18:37:00,775] [    INFO][0m - loss: 0.35704093, learning_rate: 2.857142857142857e-05, global_step: 1800, interval_runtime: 1.5479, interval_samples_per_second: 5.168, interval_steps_per_second: 6.46, epoch: 4.7619[0m
[32m[2022-08-31 18:37:00,776] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:37:00,776] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 18:37:00,776] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:37:00,776] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:37:00,776] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 18:37:12,706] [    INFO][0m - eval_loss: 2.8372366428375244, eval_accuracy: 0.5200291332847778, eval_runtime: 11.8279, eval_samples_per_second: 116.081, eval_steps_per_second: 3.635, epoch: 4.7619[0m
[32m[2022-08-31 18:37:12,707] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1800[0m
[32m[2022-08-31 18:37:12,707] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:37:14,868] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1800/tokenizer_config.json[0m
[32m[2022-08-31 18:37:14,868] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1800/special_tokens_map.json[0m
[32m[2022-08-31 18:37:20,749] [    INFO][0m - loss: 0.21460261, learning_rate: 2.8563492063492065e-05, global_step: 1810, interval_runtime: 19.9736, interval_samples_per_second: 0.401, interval_steps_per_second: 0.501, epoch: 4.7884[0m
[32m[2022-08-31 18:37:22,306] [    INFO][0m - loss: 0.35301018, learning_rate: 2.8555555555555556e-05, global_step: 1820, interval_runtime: 1.5568, interval_samples_per_second: 5.139, interval_steps_per_second: 6.424, epoch: 4.8148[0m
[32m[2022-08-31 18:37:23,859] [    INFO][0m - loss: 0.22463408, learning_rate: 2.8547619047619047e-05, global_step: 1830, interval_runtime: 1.5536, interval_samples_per_second: 5.149, interval_steps_per_second: 6.437, epoch: 4.8413[0m
[32m[2022-08-31 18:37:25,408] [    INFO][0m - loss: 0.28255944, learning_rate: 2.853968253968254e-05, global_step: 1840, interval_runtime: 1.5492, interval_samples_per_second: 5.164, interval_steps_per_second: 6.455, epoch: 4.8677[0m
[32m[2022-08-31 18:37:26,971] [    INFO][0m - loss: 0.22927265, learning_rate: 2.853174603174603e-05, global_step: 1850, interval_runtime: 1.5626, interval_samples_per_second: 5.12, interval_steps_per_second: 6.399, epoch: 4.8942[0m
[32m[2022-08-31 18:37:28,534] [    INFO][0m - loss: 0.34163957, learning_rate: 2.8523809523809522e-05, global_step: 1860, interval_runtime: 1.5627, interval_samples_per_second: 5.119, interval_steps_per_second: 6.399, epoch: 4.9206[0m
[32m[2022-08-31 18:37:30,087] [    INFO][0m - loss: 0.41577926, learning_rate: 2.8515873015873017e-05, global_step: 1870, interval_runtime: 1.5534, interval_samples_per_second: 5.15, interval_steps_per_second: 6.437, epoch: 4.9471[0m
[32m[2022-08-31 18:37:31,637] [    INFO][0m - loss: 0.25997028, learning_rate: 2.8507936507936507e-05, global_step: 1880, interval_runtime: 1.5494, interval_samples_per_second: 5.163, interval_steps_per_second: 6.454, epoch: 4.9735[0m
[32m[2022-08-31 18:37:33,180] [    INFO][0m - loss: 0.23839631, learning_rate: 2.8499999999999998e-05, global_step: 1890, interval_runtime: 1.5433, interval_samples_per_second: 5.184, interval_steps_per_second: 6.48, epoch: 5.0[0m
[32m[2022-08-31 18:37:34,844] [    INFO][0m - loss: 0.2046041, learning_rate: 2.8492063492063492e-05, global_step: 1900, interval_runtime: 1.6642, interval_samples_per_second: 4.807, interval_steps_per_second: 6.009, epoch: 5.0265[0m
[32m[2022-08-31 18:37:34,845] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:37:34,845] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 18:37:34,845] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:37:34,845] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:37:34,845] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 18:37:46,648] [    INFO][0m - eval_loss: 2.8851985931396484, eval_accuracy: 0.517115804806992, eval_runtime: 11.8029, eval_samples_per_second: 116.327, eval_steps_per_second: 3.643, epoch: 5.0265[0m
[32m[2022-08-31 18:37:46,649] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1900[0m
[32m[2022-08-31 18:37:46,649] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:37:50,836] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1900/tokenizer_config.json[0m
[32m[2022-08-31 18:37:50,836] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1900/special_tokens_map.json[0m
[32m[2022-08-31 18:37:56,729] [    INFO][0m - loss: 0.12184944, learning_rate: 2.8484126984126983e-05, global_step: 1910, interval_runtime: 21.8852, interval_samples_per_second: 0.366, interval_steps_per_second: 0.457, epoch: 5.0529[0m
[32m[2022-08-31 18:37:58,285] [    INFO][0m - loss: 0.13220686, learning_rate: 2.8476190476190477e-05, global_step: 1920, interval_runtime: 1.5562, interval_samples_per_second: 5.141, interval_steps_per_second: 6.426, epoch: 5.0794[0m
[32m[2022-08-31 18:37:59,840] [    INFO][0m - loss: 0.04518225, learning_rate: 2.846825396825397e-05, global_step: 1930, interval_runtime: 1.5546, interval_samples_per_second: 5.146, interval_steps_per_second: 6.433, epoch: 5.1058[0m
[32m[2022-08-31 18:38:01,401] [    INFO][0m - loss: 0.18622574, learning_rate: 2.8460317460317462e-05, global_step: 1940, interval_runtime: 1.5604, interval_samples_per_second: 5.127, interval_steps_per_second: 6.409, epoch: 5.1323[0m
[32m[2022-08-31 18:38:02,948] [    INFO][0m - loss: 0.10293398, learning_rate: 2.8452380952380953e-05, global_step: 1950, interval_runtime: 1.5471, interval_samples_per_second: 5.171, interval_steps_per_second: 6.464, epoch: 5.1587[0m
[32m[2022-08-31 18:38:04,500] [    INFO][0m - loss: 0.0789582, learning_rate: 2.8444444444444447e-05, global_step: 1960, interval_runtime: 1.5519, interval_samples_per_second: 5.155, interval_steps_per_second: 6.444, epoch: 5.1852[0m
[32m[2022-08-31 18:38:06,051] [    INFO][0m - loss: 0.08193932, learning_rate: 2.8436507936507938e-05, global_step: 1970, interval_runtime: 1.5515, interval_samples_per_second: 5.156, interval_steps_per_second: 6.445, epoch: 5.2116[0m
[32m[2022-08-31 18:38:07,603] [    INFO][0m - loss: 0.23924968, learning_rate: 2.842857142857143e-05, global_step: 1980, interval_runtime: 1.5524, interval_samples_per_second: 5.153, interval_steps_per_second: 6.442, epoch: 5.2381[0m
[32m[2022-08-31 18:38:09,169] [    INFO][0m - loss: 0.11055006, learning_rate: 2.8420634920634923e-05, global_step: 1990, interval_runtime: 1.565, interval_samples_per_second: 5.112, interval_steps_per_second: 6.39, epoch: 5.2646[0m
[32m[2022-08-31 18:38:10,723] [    INFO][0m - loss: 0.10572278, learning_rate: 2.8412698412698414e-05, global_step: 2000, interval_runtime: 1.5546, interval_samples_per_second: 5.146, interval_steps_per_second: 6.433, epoch: 5.291[0m
[32m[2022-08-31 18:38:10,724] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:38:10,724] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 18:38:10,724] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:38:10,724] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:38:10,724] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 18:38:22,625] [    INFO][0m - eval_loss: 3.000675678253174, eval_accuracy: 0.538965768390386, eval_runtime: 11.9007, eval_samples_per_second: 115.371, eval_steps_per_second: 3.613, epoch: 5.291[0m
[32m[2022-08-31 18:38:22,626] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2000[0m
[32m[2022-08-31 18:38:22,626] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:38:24,497] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2000/tokenizer_config.json[0m
[32m[2022-08-31 18:38:24,498] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2000/special_tokens_map.json[0m
[32m[2022-08-31 18:38:31,913] [    INFO][0m - loss: 0.08182008, learning_rate: 2.8404761904761905e-05, global_step: 2010, interval_runtime: 21.1901, interval_samples_per_second: 0.378, interval_steps_per_second: 0.472, epoch: 5.3175[0m
[32m[2022-08-31 18:38:33,469] [    INFO][0m - loss: 0.11347334, learning_rate: 2.83968253968254e-05, global_step: 2020, interval_runtime: 1.5561, interval_samples_per_second: 5.141, interval_steps_per_second: 6.426, epoch: 5.3439[0m
[32m[2022-08-31 18:38:35,024] [    INFO][0m - loss: 0.0680668, learning_rate: 2.838888888888889e-05, global_step: 2030, interval_runtime: 1.555, interval_samples_per_second: 5.145, interval_steps_per_second: 6.431, epoch: 5.3704[0m
[32m[2022-08-31 18:38:36,594] [    INFO][0m - loss: 0.05987589, learning_rate: 2.838095238095238e-05, global_step: 2040, interval_runtime: 1.5695, interval_samples_per_second: 5.097, interval_steps_per_second: 6.371, epoch: 5.3968[0m
[32m[2022-08-31 18:38:38,152] [    INFO][0m - loss: 0.27421675, learning_rate: 2.8373015873015875e-05, global_step: 2050, interval_runtime: 1.5582, interval_samples_per_second: 5.134, interval_steps_per_second: 6.418, epoch: 5.4233[0m
[32m[2022-08-31 18:38:39,705] [    INFO][0m - loss: 0.09689992, learning_rate: 2.8365079365079365e-05, global_step: 2060, interval_runtime: 1.5528, interval_samples_per_second: 5.152, interval_steps_per_second: 6.44, epoch: 5.4497[0m
[32m[2022-08-31 18:38:41,261] [    INFO][0m - loss: 0.08585841, learning_rate: 2.8357142857142856e-05, global_step: 2070, interval_runtime: 1.5565, interval_samples_per_second: 5.14, interval_steps_per_second: 6.425, epoch: 5.4762[0m
[32m[2022-08-31 18:38:42,817] [    INFO][0m - loss: 0.12443231, learning_rate: 2.834920634920635e-05, global_step: 2080, interval_runtime: 1.5562, interval_samples_per_second: 5.141, interval_steps_per_second: 6.426, epoch: 5.5026[0m
[32m[2022-08-31 18:38:44,369] [    INFO][0m - loss: 0.32728901, learning_rate: 2.834126984126984e-05, global_step: 2090, interval_runtime: 1.5514, interval_samples_per_second: 5.157, interval_steps_per_second: 6.446, epoch: 5.5291[0m
[32m[2022-08-31 18:38:45,948] [    INFO][0m - loss: 0.23808858, learning_rate: 2.8333333333333332e-05, global_step: 2100, interval_runtime: 1.5792, interval_samples_per_second: 5.066, interval_steps_per_second: 6.332, epoch: 5.5556[0m
[32m[2022-08-31 18:38:45,948] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:38:45,949] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 18:38:45,949] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:38:45,949] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:38:45,949] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 18:38:57,834] [    INFO][0m - eval_loss: 3.0683491230010986, eval_accuracy: 0.5243991260014567, eval_runtime: 11.885, eval_samples_per_second: 115.524, eval_steps_per_second: 3.618, epoch: 5.5556[0m
[32m[2022-08-31 18:38:57,835] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2100[0m
[32m[2022-08-31 18:38:57,835] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:38:59,734] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2100/tokenizer_config.json[0m
[32m[2022-08-31 18:38:59,734] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2100/special_tokens_map.json[0m
[32m[2022-08-31 18:39:05,564] [    INFO][0m - loss: 0.20246844, learning_rate: 2.8325396825396826e-05, global_step: 2110, interval_runtime: 19.6154, interval_samples_per_second: 0.408, interval_steps_per_second: 0.51, epoch: 5.582[0m
[32m[2022-08-31 18:39:07,106] [    INFO][0m - loss: 0.10946842, learning_rate: 2.8317460317460317e-05, global_step: 2120, interval_runtime: 1.5427, interval_samples_per_second: 5.186, interval_steps_per_second: 6.482, epoch: 5.6085[0m
[32m[2022-08-31 18:39:08,682] [    INFO][0m - loss: 0.06404734, learning_rate: 2.830952380952381e-05, global_step: 2130, interval_runtime: 1.5758, interval_samples_per_second: 5.077, interval_steps_per_second: 6.346, epoch: 5.6349[0m
[32m[2022-08-31 18:39:10,234] [    INFO][0m - loss: 0.0657865, learning_rate: 2.8301587301587305e-05, global_step: 2140, interval_runtime: 1.5519, interval_samples_per_second: 5.155, interval_steps_per_second: 6.444, epoch: 5.6614[0m
[32m[2022-08-31 18:39:11,784] [    INFO][0m - loss: 0.10591034, learning_rate: 2.8293650793650796e-05, global_step: 2150, interval_runtime: 1.5499, interval_samples_per_second: 5.162, interval_steps_per_second: 6.452, epoch: 5.6878[0m
[32m[2022-08-31 18:39:13,348] [    INFO][0m - loss: 0.10769151, learning_rate: 2.8285714285714287e-05, global_step: 2160, interval_runtime: 1.5639, interval_samples_per_second: 5.116, interval_steps_per_second: 6.394, epoch: 5.7143[0m
[32m[2022-08-31 18:39:14,901] [    INFO][0m - loss: 0.15124812, learning_rate: 2.8277777777777778e-05, global_step: 2170, interval_runtime: 1.5533, interval_samples_per_second: 5.15, interval_steps_per_second: 6.438, epoch: 5.7407[0m
[32m[2022-08-31 18:39:16,464] [    INFO][0m - loss: 0.10860012, learning_rate: 2.8269841269841272e-05, global_step: 2180, interval_runtime: 1.5635, interval_samples_per_second: 5.117, interval_steps_per_second: 6.396, epoch: 5.7672[0m
[32m[2022-08-31 18:39:18,022] [    INFO][0m - loss: 0.34694803, learning_rate: 2.8261904761904763e-05, global_step: 2190, interval_runtime: 1.5578, interval_samples_per_second: 5.136, interval_steps_per_second: 6.419, epoch: 5.7937[0m
[32m[2022-08-31 18:39:19,575] [    INFO][0m - loss: 0.25439382, learning_rate: 2.8253968253968253e-05, global_step: 2200, interval_runtime: 1.5532, interval_samples_per_second: 5.151, interval_steps_per_second: 6.438, epoch: 5.8201[0m
[32m[2022-08-31 18:39:19,576] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:39:19,576] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 18:39:19,576] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:39:19,576] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:39:19,577] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 18:39:31,455] [    INFO][0m - eval_loss: 3.244900941848755, eval_accuracy: 0.5258557902403496, eval_runtime: 11.8781, eval_samples_per_second: 115.591, eval_steps_per_second: 3.62, epoch: 5.8201[0m
[32m[2022-08-31 18:39:31,456] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2200[0m
[32m[2022-08-31 18:39:31,456] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:39:33,635] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2200/tokenizer_config.json[0m
[32m[2022-08-31 18:39:33,635] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2200/special_tokens_map.json[0m
[32m[2022-08-31 18:39:39,896] [    INFO][0m - loss: 0.08677312, learning_rate: 2.8246031746031748e-05, global_step: 2210, interval_runtime: 20.3205, interval_samples_per_second: 0.394, interval_steps_per_second: 0.492, epoch: 5.8466[0m
[32m[2022-08-31 18:39:41,458] [    INFO][0m - loss: 0.21208758, learning_rate: 2.823809523809524e-05, global_step: 2220, interval_runtime: 1.5624, interval_samples_per_second: 5.12, interval_steps_per_second: 6.4, epoch: 5.873[0m
[32m[2022-08-31 18:39:43,014] [    INFO][0m - loss: 0.25008185, learning_rate: 2.823015873015873e-05, global_step: 2230, interval_runtime: 1.5559, interval_samples_per_second: 5.142, interval_steps_per_second: 6.427, epoch: 5.8995[0m
[32m[2022-08-31 18:39:44,576] [    INFO][0m - loss: 0.08303615, learning_rate: 2.8222222222222223e-05, global_step: 2240, interval_runtime: 1.5621, interval_samples_per_second: 5.121, interval_steps_per_second: 6.402, epoch: 5.9259[0m
[32m[2022-08-31 18:39:46,160] [    INFO][0m - loss: 0.11344255, learning_rate: 2.8214285714285714e-05, global_step: 2250, interval_runtime: 1.5838, interval_samples_per_second: 5.051, interval_steps_per_second: 6.314, epoch: 5.9524[0m
[32m[2022-08-31 18:39:47,715] [    INFO][0m - loss: 0.21000404, learning_rate: 2.8206349206349205e-05, global_step: 2260, interval_runtime: 1.5545, interval_samples_per_second: 5.146, interval_steps_per_second: 6.433, epoch: 5.9788[0m
[32m[2022-08-31 18:39:49,396] [    INFO][0m - loss: 0.08360673, learning_rate: 2.81984126984127e-05, global_step: 2270, interval_runtime: 1.6814, interval_samples_per_second: 4.758, interval_steps_per_second: 5.947, epoch: 6.0053[0m
[32m[2022-08-31 18:39:50,959] [    INFO][0m - loss: 0.09626123, learning_rate: 2.819047619047619e-05, global_step: 2280, interval_runtime: 1.5623, interval_samples_per_second: 5.121, interval_steps_per_second: 6.401, epoch: 6.0317[0m
[32m[2022-08-31 18:39:52,517] [    INFO][0m - loss: 0.08348876, learning_rate: 2.818253968253968e-05, global_step: 2290, interval_runtime: 1.5582, interval_samples_per_second: 5.134, interval_steps_per_second: 6.418, epoch: 6.0582[0m
[32m[2022-08-31 18:39:54,083] [    INFO][0m - loss: 0.09865212, learning_rate: 2.8174603174603175e-05, global_step: 2300, interval_runtime: 1.5664, interval_samples_per_second: 5.107, interval_steps_per_second: 6.384, epoch: 6.0847[0m
[32m[2022-08-31 18:39:54,084] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:39:54,084] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 18:39:54,084] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:39:54,085] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:39:54,085] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 18:40:05,861] [    INFO][0m - eval_loss: 3.119755268096924, eval_accuracy: 0.5353241077931536, eval_runtime: 11.7759, eval_samples_per_second: 116.594, eval_steps_per_second: 3.652, epoch: 6.0847[0m
[32m[2022-08-31 18:40:05,861] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2300[0m
[32m[2022-08-31 18:40:05,862] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:40:07,982] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2300/tokenizer_config.json[0m
[32m[2022-08-31 18:40:07,982] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2300/special_tokens_map.json[0m
[32m[2022-08-31 18:40:13,799] [    INFO][0m - loss: 0.04050412, learning_rate: 2.8166666666666666e-05, global_step: 2310, interval_runtime: 19.7156, interval_samples_per_second: 0.406, interval_steps_per_second: 0.507, epoch: 6.1111[0m
[32m[2022-08-31 18:40:15,356] [    INFO][0m - loss: 0.03628874, learning_rate: 2.8158730158730157e-05, global_step: 2320, interval_runtime: 1.556, interval_samples_per_second: 5.141, interval_steps_per_second: 6.427, epoch: 6.1376[0m
[32m[2022-08-31 18:40:16,899] [    INFO][0m - loss: 0.13979101, learning_rate: 2.8150793650793654e-05, global_step: 2330, interval_runtime: 1.544, interval_samples_per_second: 5.181, interval_steps_per_second: 6.477, epoch: 6.164[0m
[32m[2022-08-31 18:40:18,449] [    INFO][0m - loss: 0.07669064, learning_rate: 2.8142857142857145e-05, global_step: 2340, interval_runtime: 1.5509, interval_samples_per_second: 5.158, interval_steps_per_second: 6.448, epoch: 6.1905[0m
[32m[2022-08-31 18:40:19,996] [    INFO][0m - loss: 0.05466138, learning_rate: 2.8134920634920636e-05, global_step: 2350, interval_runtime: 1.5465, interval_samples_per_second: 5.173, interval_steps_per_second: 6.466, epoch: 6.2169[0m
[32m[2022-08-31 18:40:21,556] [    INFO][0m - loss: 0.02408805, learning_rate: 2.812698412698413e-05, global_step: 2360, interval_runtime: 1.5599, interval_samples_per_second: 5.129, interval_steps_per_second: 6.411, epoch: 6.2434[0m
[32m[2022-08-31 18:40:23,109] [    INFO][0m - loss: 0.03263099, learning_rate: 2.811904761904762e-05, global_step: 2370, interval_runtime: 1.553, interval_samples_per_second: 5.151, interval_steps_per_second: 6.439, epoch: 6.2698[0m
[32m[2022-08-31 18:40:24,660] [    INFO][0m - loss: 0.04000376, learning_rate: 2.811111111111111e-05, global_step: 2380, interval_runtime: 1.5512, interval_samples_per_second: 5.157, interval_steps_per_second: 6.447, epoch: 6.2963[0m
[32m[2022-08-31 18:40:26,207] [    INFO][0m - loss: 0.03148208, learning_rate: 2.8103174603174606e-05, global_step: 2390, interval_runtime: 1.5469, interval_samples_per_second: 5.172, interval_steps_per_second: 6.465, epoch: 6.3228[0m
[32m[2022-08-31 18:40:27,758] [    INFO][0m - loss: 0.16975214, learning_rate: 2.8095238095238096e-05, global_step: 2400, interval_runtime: 1.5511, interval_samples_per_second: 5.158, interval_steps_per_second: 6.447, epoch: 6.3492[0m
[32m[2022-08-31 18:40:27,759] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:40:27,759] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 18:40:27,759] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:40:27,759] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:40:27,759] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 18:40:39,639] [    INFO][0m - eval_loss: 3.162802219390869, eval_accuracy: 0.5469774217042972, eval_runtime: 11.8793, eval_samples_per_second: 115.579, eval_steps_per_second: 3.62, epoch: 6.3492[0m
[32m[2022-08-31 18:40:39,640] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2400[0m
[32m[2022-08-31 18:40:39,640] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:40:41,447] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2400/tokenizer_config.json[0m
[32m[2022-08-31 18:40:41,447] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2400/special_tokens_map.json[0m
[32m[2022-08-31 18:40:47,387] [    INFO][0m - loss: 0.16531878, learning_rate: 2.8087301587301587e-05, global_step: 2410, interval_runtime: 19.6293, interval_samples_per_second: 0.408, interval_steps_per_second: 0.509, epoch: 6.3757[0m
[32m[2022-08-31 18:40:48,928] [    INFO][0m - loss: 0.14803129, learning_rate: 2.807936507936508e-05, global_step: 2420, interval_runtime: 1.5408, interval_samples_per_second: 5.192, interval_steps_per_second: 6.49, epoch: 6.4021[0m
[32m[2022-08-31 18:40:50,476] [    INFO][0m - loss: 0.09731022, learning_rate: 2.8071428571428572e-05, global_step: 2430, interval_runtime: 1.5476, interval_samples_per_second: 5.169, interval_steps_per_second: 6.462, epoch: 6.4286[0m
[32m[2022-08-31 18:40:52,025] [    INFO][0m - loss: 0.12192656, learning_rate: 2.8063492063492063e-05, global_step: 2440, interval_runtime: 1.5494, interval_samples_per_second: 5.163, interval_steps_per_second: 6.454, epoch: 6.455[0m
[32m[2022-08-31 18:40:53,575] [    INFO][0m - loss: 0.0974247, learning_rate: 2.8055555555555557e-05, global_step: 2450, interval_runtime: 1.5502, interval_samples_per_second: 5.16, interval_steps_per_second: 6.451, epoch: 6.4815[0m
[32m[2022-08-31 18:40:55,122] [    INFO][0m - loss: 0.03988686, learning_rate: 2.8047619047619048e-05, global_step: 2460, interval_runtime: 1.5463, interval_samples_per_second: 5.174, interval_steps_per_second: 6.467, epoch: 6.5079[0m
[32m[2022-08-31 18:40:56,668] [    INFO][0m - loss: 0.02409132, learning_rate: 2.803968253968254e-05, global_step: 2470, interval_runtime: 1.5464, interval_samples_per_second: 5.173, interval_steps_per_second: 6.466, epoch: 6.5344[0m
[32m[2022-08-31 18:40:58,233] [    INFO][0m - loss: 0.05929786, learning_rate: 2.8031746031746033e-05, global_step: 2480, interval_runtime: 1.5646, interval_samples_per_second: 5.113, interval_steps_per_second: 6.391, epoch: 6.5608[0m
[32m[2022-08-31 18:40:59,786] [    INFO][0m - loss: 0.05664588, learning_rate: 2.8023809523809524e-05, global_step: 2490, interval_runtime: 1.5533, interval_samples_per_second: 5.15, interval_steps_per_second: 6.438, epoch: 6.5873[0m
[32m[2022-08-31 18:41:01,332] [    INFO][0m - loss: 0.06427577, learning_rate: 2.8015873015873015e-05, global_step: 2500, interval_runtime: 1.5463, interval_samples_per_second: 5.174, interval_steps_per_second: 6.467, epoch: 6.6138[0m
[32m[2022-08-31 18:41:01,333] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:41:01,333] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 18:41:01,334] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:41:01,334] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:41:01,334] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 18:41:13,218] [    INFO][0m - eval_loss: 3.232463836669922, eval_accuracy: 0.538965768390386, eval_runtime: 11.8838, eval_samples_per_second: 115.535, eval_steps_per_second: 3.618, epoch: 6.6138[0m
[32m[2022-08-31 18:41:13,219] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2500[0m
[32m[2022-08-31 18:41:13,219] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:41:15,362] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2500/tokenizer_config.json[0m
[32m[2022-08-31 18:41:15,362] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2500/special_tokens_map.json[0m
[32m[2022-08-31 18:41:21,582] [    INFO][0m - loss: 0.01802103, learning_rate: 2.800793650793651e-05, global_step: 2510, interval_runtime: 20.2495, interval_samples_per_second: 0.395, interval_steps_per_second: 0.494, epoch: 6.6402[0m
[32m[2022-08-31 18:41:23,133] [    INFO][0m - loss: 0.01864737, learning_rate: 2.8e-05, global_step: 2520, interval_runtime: 1.5515, interval_samples_per_second: 5.156, interval_steps_per_second: 6.445, epoch: 6.6667[0m
[32m[2022-08-31 18:41:24,679] [    INFO][0m - loss: 0.04290175, learning_rate: 2.7992063492063494e-05, global_step: 2530, interval_runtime: 1.5456, interval_samples_per_second: 5.176, interval_steps_per_second: 6.47, epoch: 6.6931[0m
[32m[2022-08-31 18:41:26,220] [    INFO][0m - loss: 0.05571234, learning_rate: 2.7984126984126988e-05, global_step: 2540, interval_runtime: 1.5411, interval_samples_per_second: 5.191, interval_steps_per_second: 6.489, epoch: 6.7196[0m
[32m[2022-08-31 18:41:27,776] [    INFO][0m - loss: 0.14224555, learning_rate: 2.797619047619048e-05, global_step: 2550, interval_runtime: 1.5555, interval_samples_per_second: 5.143, interval_steps_per_second: 6.429, epoch: 6.746[0m
[32m[2022-08-31 18:41:29,349] [    INFO][0m - loss: 0.03094892, learning_rate: 2.796825396825397e-05, global_step: 2560, interval_runtime: 1.5729, interval_samples_per_second: 5.086, interval_steps_per_second: 6.358, epoch: 6.7725[0m
[32m[2022-08-31 18:41:30,929] [    INFO][0m - loss: 0.00854604, learning_rate: 2.796031746031746e-05, global_step: 2570, interval_runtime: 1.5806, interval_samples_per_second: 5.061, interval_steps_per_second: 6.327, epoch: 6.7989[0m
[32m[2022-08-31 18:41:32,483] [    INFO][0m - loss: 0.08841516, learning_rate: 2.7952380952380955e-05, global_step: 2580, interval_runtime: 1.5534, interval_samples_per_second: 5.15, interval_steps_per_second: 6.437, epoch: 6.8254[0m
[32m[2022-08-31 18:41:34,037] [    INFO][0m - loss: 0.19972862, learning_rate: 2.7944444444444445e-05, global_step: 2590, interval_runtime: 1.5542, interval_samples_per_second: 5.147, interval_steps_per_second: 6.434, epoch: 6.8519[0m
[32m[2022-08-31 18:41:35,587] [    INFO][0m - loss: 0.05599366, learning_rate: 2.7936507936507936e-05, global_step: 2600, interval_runtime: 1.5494, interval_samples_per_second: 5.163, interval_steps_per_second: 6.454, epoch: 6.8783[0m
[32m[2022-08-31 18:41:35,587] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:41:35,587] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 18:41:35,587] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:41:35,588] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:41:35,588] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 18:41:47,411] [    INFO][0m - eval_loss: 3.421567678451538, eval_accuracy: 0.5375091041514931, eval_runtime: 11.8225, eval_samples_per_second: 116.135, eval_steps_per_second: 3.637, epoch: 6.8783[0m
[32m[2022-08-31 18:41:47,411] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2600[0m
[32m[2022-08-31 18:41:47,412] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:41:49,265] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2600/tokenizer_config.json[0m
[32m[2022-08-31 18:41:49,265] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2600/special_tokens_map.json[0m
[32m[2022-08-31 18:41:53,261] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-31 18:41:53,261] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-600 (score: 0.5579024034959942).[0m
[32m[2022-08-31 18:41:53,867] [    INFO][0m - train_runtime: 891.6586, train_samples_per_second: 339.143, train_steps_per_second: 42.393, train_loss: 0.7535607133633815, epoch: 6.8783[0m
[32m[2022-08-31 18:41:53,868] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-31 18:41:53,868] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:41:55,675] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-31 18:41:55,679] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-31 18:41:55,682] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-31 18:41:55,682] [    INFO][0m -   epoch                    =     6.8783[0m
[32m[2022-08-31 18:41:55,682] [    INFO][0m -   train_loss               =     0.7536[0m
[32m[2022-08-31 18:41:55,682] [    INFO][0m -   train_runtime            = 0:14:51.65[0m
[32m[2022-08-31 18:41:55,682] [    INFO][0m -   train_samples_per_second =    339.143[0m
[32m[2022-08-31 18:41:55,682] [    INFO][0m -   train_steps_per_second   =     42.393[0m
[32m[2022-08-31 18:41:55,693] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-31 18:41:55,694] [    INFO][0m -   Num examples = 1749[0m
[32m[2022-08-31 18:41:55,694] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:41:55,694] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:41:55,694] [    INFO][0m -   Total prediction steps = 55[0m
[32m[2022-08-31 18:42:11,032] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-31 18:42:11,033] [    INFO][0m -   test_accuracy           =      0.562[0m
[32m[2022-08-31 18:42:11,033] [    INFO][0m -   test_loss               =     1.8539[0m
[32m[2022-08-31 18:42:11,033] [    INFO][0m -   test_runtime            = 0:00:15.33[0m
[32m[2022-08-31 18:42:11,033] [    INFO][0m -   test_samples_per_second =    114.026[0m
[32m[2022-08-31 18:42:11,033] [    INFO][0m -   test_steps_per_second   =      3.586[0m
[32m[2022-08-31 18:42:11,033] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-31 18:42:11,034] [    INFO][0m -   Num examples = 2600[0m
[32m[2022-08-31 18:42:11,034] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:42:11,034] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:42:11,034] [    INFO][0m -   Total prediction steps = 82[0m
[32m[2022-08-31 18:42:40,741] [    INFO][0m - Predictions for iflytekf saved to ./fewclue_submit_examples.[0m
run.sh: line 65: --model_name_or_path: command not found
 
==========
ocnli
==========
 
[33m[2022-08-31 18:42:45,131] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-31 18:42:45,131] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-31 18:42:45,131] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 18:42:45,131] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-31 18:42:45,131] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 18:42:45,132] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-31 18:42:45,132] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-31 18:42:45,132] [    INFO][0m - early_stop_patience           :20[0m
[32m[2022-08-31 18:42:45,132] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-31 18:42:45,132] [    INFO][0m - model_name_or_path            :roformer_v2_chinese_char_base[0m
[32m[2022-08-31 18:42:45,132] [    INFO][0m - [0m
[32m[2022-08-31 18:42:45,133] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 18:42:45,133] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-31 18:42:45,133] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 18:42:45,133] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-31 18:42:45,133] [    INFO][0m - prompt                        :{'hard':'è¯·ç”¨æ­£ç¡®çš„è¿žæŽ¥è¯å¡«ç©ºï¼š'}{'text':'text_a'}{'mask'}{'mask'}{'text':'text_b'}[0m
[32m[2022-08-31 18:42:45,133] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-31 18:42:45,134] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-31 18:42:45,134] [    INFO][0m - task_name                     :ocnli[0m
[32m[2022-08-31 18:42:45,134] [    INFO][0m - [0m
[32m[2022-08-31 18:42:45,134] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/roformer_v2_chinese_char_base/model_state.pdparams[0m
W0831 18:42:45.135879 26807 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0831 18:42:45.140197 26807 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-31 18:42:48,072] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/roformer_v2_chinese_char_base/vocab.txt[0m
[32m[2022-08-31 18:42:48,085] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/roformer_v2_chinese_char_base/tokenizer_config.json[0m
[32m[2022-08-31 18:42:48,086] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/roformer_v2_chinese_char_base/special_tokens_map.json[0m
[32m[2022-08-31 18:42:48,087] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': 'è¯·ç”¨æ­£ç¡®çš„è¿žæŽ¥è¯å¡«ç©ºï¼š'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'text': 'text_b'}][0m
2022-08-31 18:42:48,098 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-31 18:42:48,307] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 18:42:48,307] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-31 18:42:48,307] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 18:42:48,307] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-31 18:42:48,308] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-31 18:42:48,308] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-31 18:42:48,308] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-31 18:42:48,308] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-31 18:42:48,308] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-31 18:42:48,308] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-31 18:42:48,308] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-31 18:42:48,308] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-31 18:42:48,308] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-31 18:42:48,308] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-31 18:42:48,308] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-31 18:42:48,308] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-31 18:42:48,309] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-31 18:42:48,309] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-31 18:42:48,309] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-31 18:42:48,309] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-31 18:42:48,309] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-31 18:42:48,309] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-31 18:42:48,309] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-31 18:42:48,309] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-31 18:42:48,309] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-31 18:42:48,309] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-31 18:42:48,309] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-31 18:42:48,309] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-31 18:42:48,309] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-31 18:42:48,310] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-31 18:42:48,310] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-31 18:42:48,310] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-31 18:42:48,310] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-31 18:42:48,310] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-31 18:42:48,310] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-31 18:42:48,310] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-31 18:42:48,310] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-31 18:42:48,310] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug31_18-42-45_instance-3bwob41y-01[0m
[32m[2022-08-31 18:42:48,310] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-31 18:42:48,310] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-31 18:42:48,310] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-31 18:42:48,310] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-31 18:42:48,311] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-31 18:42:48,311] [    INFO][0m - max_seq_length                :64[0m
[32m[2022-08-31 18:42:48,311] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-31 18:42:48,311] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-31 18:42:48,311] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-31 18:42:48,311] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-31 18:42:48,311] [    INFO][0m - num_train_epochs              :100.0[0m
[32m[2022-08-31 18:42:48,311] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-31 18:42:48,311] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-31 18:42:48,311] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-31 18:42:48,311] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-31 18:42:48,311] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-31 18:42:48,311] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-31 18:42:48,312] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-31 18:42:48,312] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-31 18:42:48,312] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-31 18:42:48,312] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-31 18:42:48,312] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-31 18:42:48,312] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-31 18:42:48,312] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-31 18:42:48,312] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-31 18:42:48,312] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-31 18:42:48,312] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-31 18:42:48,312] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-31 18:42:48,312] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-31 18:42:48,312] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-31 18:42:48,313] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-31 18:42:48,313] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-31 18:42:48,313] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-31 18:42:48,313] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-31 18:42:48,313] [    INFO][0m - seed                          :42[0m
[32m[2022-08-31 18:42:48,313] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-31 18:42:48,313] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-31 18:42:48,313] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-31 18:42:48,313] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-31 18:42:48,313] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-31 18:42:48,313] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-31 18:42:48,313] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-31 18:42:48,313] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-31 18:42:48,313] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-31 18:42:48,314] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-31 18:42:48,314] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-31 18:42:48,314] [    INFO][0m - [0m
[32m[2022-08-31 18:42:48,315] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-31 18:42:48,315] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:42:48,316] [    INFO][0m -   Num Epochs = 100[0m
[32m[2022-08-31 18:42:48,316] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-31 18:42:48,316] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-31 18:42:48,316] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-31 18:42:48,316] [    INFO][0m -   Total optimization steps = 2000.0[0m
[32m[2022-08-31 18:42:48,316] [    INFO][0m -   Total num train samples = 16000[0m
[32m[2022-08-31 18:42:49,765] [    INFO][0m - loss: 1.16256266, learning_rate: 2.985e-05, global_step: 10, interval_runtime: 1.4483, interval_samples_per_second: 5.524, interval_steps_per_second: 6.905, epoch: 0.5[0m
[32m[2022-08-31 18:42:50,399] [    INFO][0m - loss: 0.90855885, learning_rate: 2.97e-05, global_step: 20, interval_runtime: 0.6339, interval_samples_per_second: 12.62, interval_steps_per_second: 15.774, epoch: 1.0[0m
[32m[2022-08-31 18:42:51,170] [    INFO][0m - loss: 0.60976782, learning_rate: 2.955e-05, global_step: 30, interval_runtime: 0.7706, interval_samples_per_second: 10.382, interval_steps_per_second: 12.977, epoch: 1.5[0m
[32m[2022-08-31 18:42:51,845] [    INFO][0m - loss: 0.42072854, learning_rate: 2.94e-05, global_step: 40, interval_runtime: 0.6751, interval_samples_per_second: 11.85, interval_steps_per_second: 14.812, epoch: 2.0[0m
[32m[2022-08-31 18:42:52,578] [    INFO][0m - loss: 0.13146275, learning_rate: 2.925e-05, global_step: 50, interval_runtime: 0.7337, interval_samples_per_second: 10.904, interval_steps_per_second: 13.63, epoch: 2.5[0m
[32m[2022-08-31 18:42:53,218] [    INFO][0m - loss: 0.19348569, learning_rate: 2.91e-05, global_step: 60, interval_runtime: 0.6396, interval_samples_per_second: 12.508, interval_steps_per_second: 15.635, epoch: 3.0[0m
[32m[2022-08-31 18:42:54,005] [    INFO][0m - loss: 0.02997322, learning_rate: 2.895e-05, global_step: 70, interval_runtime: 0.7871, interval_samples_per_second: 10.163, interval_steps_per_second: 12.704, epoch: 3.5[0m
[32m[2022-08-31 18:42:54,755] [    INFO][0m - loss: 0.03465876, learning_rate: 2.88e-05, global_step: 80, interval_runtime: 0.7491, interval_samples_per_second: 10.68, interval_steps_per_second: 13.35, epoch: 4.0[0m
[32m[2022-08-31 18:42:55,496] [    INFO][0m - loss: 0.02343138, learning_rate: 2.865e-05, global_step: 90, interval_runtime: 0.7412, interval_samples_per_second: 10.793, interval_steps_per_second: 13.492, epoch: 4.5[0m
[32m[2022-08-31 18:42:56,175] [    INFO][0m - loss: 0.00328904, learning_rate: 2.8499999999999998e-05, global_step: 100, interval_runtime: 0.6788, interval_samples_per_second: 11.786, interval_steps_per_second: 14.732, epoch: 5.0[0m
[32m[2022-08-31 18:42:56,176] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:42:56,176] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:42:56,176] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:42:56,176] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:42:56,176] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:42:56,513] [    INFO][0m - eval_loss: 1.9794031381607056, eval_accuracy: 0.60625, eval_runtime: 0.3368, eval_samples_per_second: 475.019, eval_steps_per_second: 14.844, epoch: 5.0[0m
[32m[2022-08-31 18:42:56,513] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-31 18:42:56,513] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:42:58,360] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-31 18:42:58,360] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-31 18:43:03,318] [    INFO][0m - loss: 0.00215306, learning_rate: 2.8349999999999998e-05, global_step: 110, interval_runtime: 7.1437, interval_samples_per_second: 1.12, interval_steps_per_second: 1.4, epoch: 5.5[0m
[32m[2022-08-31 18:43:03,974] [    INFO][0m - loss: 0.00046957, learning_rate: 2.8199999999999998e-05, global_step: 120, interval_runtime: 0.6551, interval_samples_per_second: 12.212, interval_steps_per_second: 15.265, epoch: 6.0[0m
[32m[2022-08-31 18:43:04,691] [    INFO][0m - loss: 0.03185987, learning_rate: 2.805e-05, global_step: 130, interval_runtime: 0.7178, interval_samples_per_second: 11.145, interval_steps_per_second: 13.931, epoch: 6.5[0m
[32m[2022-08-31 18:43:05,317] [    INFO][0m - loss: 0.0002556, learning_rate: 2.79e-05, global_step: 140, interval_runtime: 0.6262, interval_samples_per_second: 12.775, interval_steps_per_second: 15.969, epoch: 7.0[0m
[32m[2022-08-31 18:43:06,087] [    INFO][0m - loss: 0.00011694, learning_rate: 2.7750000000000004e-05, global_step: 150, interval_runtime: 0.7699, interval_samples_per_second: 10.391, interval_steps_per_second: 12.989, epoch: 7.5[0m
[32m[2022-08-31 18:43:06,759] [    INFO][0m - loss: 9.66e-05, learning_rate: 2.7600000000000003e-05, global_step: 160, interval_runtime: 0.672, interval_samples_per_second: 11.904, interval_steps_per_second: 14.88, epoch: 8.0[0m
[32m[2022-08-31 18:43:07,498] [    INFO][0m - loss: 5.172e-05, learning_rate: 2.7450000000000003e-05, global_step: 170, interval_runtime: 0.7387, interval_samples_per_second: 10.83, interval_steps_per_second: 13.537, epoch: 8.5[0m
[32m[2022-08-31 18:43:08,184] [    INFO][0m - loss: 0.00527632, learning_rate: 2.7300000000000003e-05, global_step: 180, interval_runtime: 0.6863, interval_samples_per_second: 11.656, interval_steps_per_second: 14.57, epoch: 9.0[0m
[32m[2022-08-31 18:43:08,883] [    INFO][0m - loss: 0.00016031, learning_rate: 2.7150000000000003e-05, global_step: 190, interval_runtime: 0.6991, interval_samples_per_second: 11.443, interval_steps_per_second: 14.304, epoch: 9.5[0m
[32m[2022-08-31 18:43:09,546] [    INFO][0m - loss: 0.00281799, learning_rate: 2.7000000000000002e-05, global_step: 200, interval_runtime: 0.6626, interval_samples_per_second: 12.073, interval_steps_per_second: 15.091, epoch: 10.0[0m
[32m[2022-08-31 18:43:09,547] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:43:09,547] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:43:09,547] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:43:09,547] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:43:09,547] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:43:09,892] [    INFO][0m - eval_loss: 1.9575908184051514, eval_accuracy: 0.625, eval_runtime: 0.3449, eval_samples_per_second: 463.927, eval_steps_per_second: 14.498, epoch: 10.0[0m
[32m[2022-08-31 18:43:09,893] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-31 18:43:09,893] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:43:11,748] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-31 18:43:11,749] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-31 18:43:16,857] [    INFO][0m - loss: 6.454e-05, learning_rate: 2.6850000000000002e-05, global_step: 210, interval_runtime: 7.3113, interval_samples_per_second: 1.094, interval_steps_per_second: 1.368, epoch: 10.5[0m
[32m[2022-08-31 18:43:17,500] [    INFO][0m - loss: 0.00185847, learning_rate: 2.6700000000000002e-05, global_step: 220, interval_runtime: 0.6424, interval_samples_per_second: 12.453, interval_steps_per_second: 15.566, epoch: 11.0[0m
[32m[2022-08-31 18:43:18,261] [    INFO][0m - loss: 1.09e-05, learning_rate: 2.655e-05, global_step: 230, interval_runtime: 0.7612, interval_samples_per_second: 10.509, interval_steps_per_second: 13.137, epoch: 11.5[0m
[32m[2022-08-31 18:43:19,044] [    INFO][0m - loss: 0.00028444, learning_rate: 2.64e-05, global_step: 240, interval_runtime: 0.7829, interval_samples_per_second: 10.218, interval_steps_per_second: 12.772, epoch: 12.0[0m
[32m[2022-08-31 18:43:19,739] [    INFO][0m - loss: 0.00049281, learning_rate: 2.625e-05, global_step: 250, interval_runtime: 0.6949, interval_samples_per_second: 11.512, interval_steps_per_second: 14.39, epoch: 12.5[0m
[32m[2022-08-31 18:43:20,360] [    INFO][0m - loss: 2.926e-05, learning_rate: 2.61e-05, global_step: 260, interval_runtime: 0.6208, interval_samples_per_second: 12.886, interval_steps_per_second: 16.107, epoch: 13.0[0m
[32m[2022-08-31 18:43:21,112] [    INFO][0m - loss: 0.00023444, learning_rate: 2.595e-05, global_step: 270, interval_runtime: 0.7523, interval_samples_per_second: 10.635, interval_steps_per_second: 13.293, epoch: 13.5[0m
[32m[2022-08-31 18:43:21,769] [    INFO][0m - loss: 1.376e-05, learning_rate: 2.58e-05, global_step: 280, interval_runtime: 0.657, interval_samples_per_second: 12.176, interval_steps_per_second: 15.22, epoch: 14.0[0m
[32m[2022-08-31 18:43:22,491] [    INFO][0m - loss: 0.00011365, learning_rate: 2.565e-05, global_step: 290, interval_runtime: 0.7215, interval_samples_per_second: 11.088, interval_steps_per_second: 13.86, epoch: 14.5[0m
[32m[2022-08-31 18:43:23,159] [    INFO][0m - loss: 2.093e-05, learning_rate: 2.55e-05, global_step: 300, interval_runtime: 0.668, interval_samples_per_second: 11.976, interval_steps_per_second: 14.97, epoch: 15.0[0m
[32m[2022-08-31 18:43:23,159] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:43:23,160] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:43:23,160] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:43:23,160] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:43:23,160] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:43:23,498] [    INFO][0m - eval_loss: 2.2884838581085205, eval_accuracy: 0.64375, eval_runtime: 0.3381, eval_samples_per_second: 473.167, eval_steps_per_second: 14.786, epoch: 15.0[0m
[32m[2022-08-31 18:43:23,499] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-31 18:43:23,503] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:43:25,825] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-31 18:43:25,826] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-31 18:43:31,293] [    INFO][0m - loss: 9.59e-06, learning_rate: 2.535e-05, global_step: 310, interval_runtime: 8.1346, interval_samples_per_second: 0.983, interval_steps_per_second: 1.229, epoch: 15.5[0m
[32m[2022-08-31 18:43:31,892] [    INFO][0m - loss: 2.081e-05, learning_rate: 2.52e-05, global_step: 320, interval_runtime: 0.599, interval_samples_per_second: 13.355, interval_steps_per_second: 16.693, epoch: 16.0[0m
[32m[2022-08-31 18:43:32,565] [    INFO][0m - loss: 8.43e-06, learning_rate: 2.505e-05, global_step: 330, interval_runtime: 0.673, interval_samples_per_second: 11.888, interval_steps_per_second: 14.859, epoch: 16.5[0m
[32m[2022-08-31 18:43:33,191] [    INFO][0m - loss: 3.414e-05, learning_rate: 2.49e-05, global_step: 340, interval_runtime: 0.6249, interval_samples_per_second: 12.803, interval_steps_per_second: 16.003, epoch: 17.0[0m
[32m[2022-08-31 18:43:33,917] [    INFO][0m - loss: 0.00010401, learning_rate: 2.475e-05, global_step: 350, interval_runtime: 0.7269, interval_samples_per_second: 11.005, interval_steps_per_second: 13.757, epoch: 17.5[0m
[32m[2022-08-31 18:43:34,522] [    INFO][0m - loss: 3.786e-05, learning_rate: 2.4599999999999998e-05, global_step: 360, interval_runtime: 0.6052, interval_samples_per_second: 13.22, interval_steps_per_second: 16.525, epoch: 18.0[0m
[32m[2022-08-31 18:43:35,209] [    INFO][0m - loss: 4.16e-05, learning_rate: 2.4449999999999998e-05, global_step: 370, interval_runtime: 0.687, interval_samples_per_second: 11.645, interval_steps_per_second: 14.556, epoch: 18.5[0m
[32m[2022-08-31 18:43:35,821] [    INFO][0m - loss: 9.94e-06, learning_rate: 2.43e-05, global_step: 380, interval_runtime: 0.612, interval_samples_per_second: 13.072, interval_steps_per_second: 16.34, epoch: 19.0[0m
[32m[2022-08-31 18:43:36,489] [    INFO][0m - loss: 1.783e-05, learning_rate: 2.415e-05, global_step: 390, interval_runtime: 0.6674, interval_samples_per_second: 11.987, interval_steps_per_second: 14.984, epoch: 19.5[0m
[32m[2022-08-31 18:43:37,098] [    INFO][0m - loss: 9.19e-06, learning_rate: 2.4e-05, global_step: 400, interval_runtime: 0.6096, interval_samples_per_second: 13.124, interval_steps_per_second: 16.405, epoch: 20.0[0m
[32m[2022-08-31 18:43:37,099] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:43:37,099] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:43:37,099] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:43:37,099] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:43:37,099] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:43:37,431] [    INFO][0m - eval_loss: 2.4476771354675293, eval_accuracy: 0.625, eval_runtime: 0.3315, eval_samples_per_second: 482.706, eval_steps_per_second: 15.085, epoch: 20.0[0m
[32m[2022-08-31 18:43:37,431] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-31 18:43:37,431] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:43:39,364] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-31 18:43:39,364] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-31 18:43:44,528] [    INFO][0m - loss: 9.55e-06, learning_rate: 2.385e-05, global_step: 410, interval_runtime: 7.4294, interval_samples_per_second: 1.077, interval_steps_per_second: 1.346, epoch: 20.5[0m
[32m[2022-08-31 18:43:45,128] [    INFO][0m - loss: 2.46e-06, learning_rate: 2.37e-05, global_step: 420, interval_runtime: 0.6007, interval_samples_per_second: 13.318, interval_steps_per_second: 16.648, epoch: 21.0[0m
[32m[2022-08-31 18:43:45,788] [    INFO][0m - loss: 3.63e-06, learning_rate: 2.3550000000000003e-05, global_step: 430, interval_runtime: 0.6594, interval_samples_per_second: 12.131, interval_steps_per_second: 15.164, epoch: 21.5[0m
[32m[2022-08-31 18:43:46,369] [    INFO][0m - loss: 3.42e-06, learning_rate: 2.3400000000000003e-05, global_step: 440, interval_runtime: 0.5811, interval_samples_per_second: 13.766, interval_steps_per_second: 17.208, epoch: 22.0[0m
[32m[2022-08-31 18:43:47,052] [    INFO][0m - loss: 3.68e-06, learning_rate: 2.3250000000000003e-05, global_step: 450, interval_runtime: 0.683, interval_samples_per_second: 11.714, interval_steps_per_second: 14.642, epoch: 22.5[0m
[32m[2022-08-31 18:43:47,668] [    INFO][0m - loss: 6.55e-06, learning_rate: 2.3100000000000002e-05, global_step: 460, interval_runtime: 0.6161, interval_samples_per_second: 12.986, interval_steps_per_second: 16.232, epoch: 23.0[0m
[32m[2022-08-31 18:43:48,341] [    INFO][0m - loss: 7.71e-06, learning_rate: 2.2950000000000002e-05, global_step: 470, interval_runtime: 0.6725, interval_samples_per_second: 11.896, interval_steps_per_second: 14.87, epoch: 23.5[0m
[32m[2022-08-31 18:43:48,941] [    INFO][0m - loss: 3.37e-06, learning_rate: 2.2800000000000002e-05, global_step: 480, interval_runtime: 0.6009, interval_samples_per_second: 13.313, interval_steps_per_second: 16.641, epoch: 24.0[0m
[32m[2022-08-31 18:43:49,624] [    INFO][0m - loss: 5.2e-06, learning_rate: 2.265e-05, global_step: 490, interval_runtime: 0.6825, interval_samples_per_second: 11.722, interval_steps_per_second: 14.652, epoch: 24.5[0m
[32m[2022-08-31 18:43:50,215] [    INFO][0m - loss: 5.931e-05, learning_rate: 2.25e-05, global_step: 500, interval_runtime: 0.5914, interval_samples_per_second: 13.527, interval_steps_per_second: 16.909, epoch: 25.0[0m
[32m[2022-08-31 18:43:50,216] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:43:50,216] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:43:50,216] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:43:50,216] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:43:50,216] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:43:50,624] [    INFO][0m - eval_loss: 2.4036803245544434, eval_accuracy: 0.64375, eval_runtime: 0.4067, eval_samples_per_second: 393.376, eval_steps_per_second: 12.293, epoch: 25.0[0m
[32m[2022-08-31 18:43:50,625] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-08-31 18:43:50,625] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:43:52,606] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-08-31 18:43:52,606] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-08-31 18:43:58,176] [    INFO][0m - loss: 4.23e-06, learning_rate: 2.235e-05, global_step: 510, interval_runtime: 7.9606, interval_samples_per_second: 1.005, interval_steps_per_second: 1.256, epoch: 25.5[0m
[32m[2022-08-31 18:43:58,757] [    INFO][0m - loss: 5.47e-06, learning_rate: 2.22e-05, global_step: 520, interval_runtime: 0.5815, interval_samples_per_second: 13.758, interval_steps_per_second: 17.198, epoch: 26.0[0m
[32m[2022-08-31 18:43:59,417] [    INFO][0m - loss: 4.75e-06, learning_rate: 2.205e-05, global_step: 530, interval_runtime: 0.6596, interval_samples_per_second: 12.128, interval_steps_per_second: 15.16, epoch: 26.5[0m
[32m[2022-08-31 18:44:00,014] [    INFO][0m - loss: 2.21e-06, learning_rate: 2.19e-05, global_step: 540, interval_runtime: 0.5972, interval_samples_per_second: 13.397, interval_steps_per_second: 16.746, epoch: 27.0[0m
[32m[2022-08-31 18:44:00,653] [    INFO][0m - loss: 3.56e-06, learning_rate: 2.175e-05, global_step: 550, interval_runtime: 0.6389, interval_samples_per_second: 12.521, interval_steps_per_second: 15.651, epoch: 27.5[0m
[32m[2022-08-31 18:44:01,260] [    INFO][0m - loss: 3.19e-06, learning_rate: 2.16e-05, global_step: 560, interval_runtime: 0.607, interval_samples_per_second: 13.179, interval_steps_per_second: 16.474, epoch: 28.0[0m
[32m[2022-08-31 18:44:01,990] [    INFO][0m - loss: 5.26e-06, learning_rate: 2.145e-05, global_step: 570, interval_runtime: 0.7303, interval_samples_per_second: 10.955, interval_steps_per_second: 13.694, epoch: 28.5[0m
[32m[2022-08-31 18:44:02,596] [    INFO][0m - loss: 6.99e-06, learning_rate: 2.13e-05, global_step: 580, interval_runtime: 0.6053, interval_samples_per_second: 13.217, interval_steps_per_second: 16.521, epoch: 29.0[0m
[32m[2022-08-31 18:44:03,303] [    INFO][0m - loss: 2.36e-06, learning_rate: 2.115e-05, global_step: 590, interval_runtime: 0.7075, interval_samples_per_second: 11.308, interval_steps_per_second: 14.135, epoch: 29.5[0m
[32m[2022-08-31 18:44:03,901] [    INFO][0m - loss: 4.84e-06, learning_rate: 2.1e-05, global_step: 600, interval_runtime: 0.5981, interval_samples_per_second: 13.375, interval_steps_per_second: 16.719, epoch: 30.0[0m
[32m[2022-08-31 18:44:03,902] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:44:03,902] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:44:03,902] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:44:03,902] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:44:03,902] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:44:04,220] [    INFO][0m - eval_loss: 2.4446451663970947, eval_accuracy: 0.6375, eval_runtime: 0.3175, eval_samples_per_second: 503.933, eval_steps_per_second: 15.748, epoch: 30.0[0m
[32m[2022-08-31 18:44:04,220] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-08-31 18:44:04,220] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:44:06,223] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-08-31 18:44:06,223] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-08-31 18:44:11,594] [    INFO][0m - loss: 3.47e-06, learning_rate: 2.085e-05, global_step: 610, interval_runtime: 7.6927, interval_samples_per_second: 1.04, interval_steps_per_second: 1.3, epoch: 30.5[0m
[32m[2022-08-31 18:44:12,180] [    INFO][0m - loss: 2.975e-05, learning_rate: 2.07e-05, global_step: 620, interval_runtime: 0.5859, interval_samples_per_second: 13.655, interval_steps_per_second: 17.069, epoch: 31.0[0m
[32m[2022-08-31 18:44:12,883] [    INFO][0m - loss: 4.01e-06, learning_rate: 2.055e-05, global_step: 630, interval_runtime: 0.7026, interval_samples_per_second: 11.387, interval_steps_per_second: 14.234, epoch: 31.5[0m
[32m[2022-08-31 18:44:13,498] [    INFO][0m - loss: 1.58e-06, learning_rate: 2.04e-05, global_step: 640, interval_runtime: 0.6153, interval_samples_per_second: 13.002, interval_steps_per_second: 16.253, epoch: 32.0[0m
[32m[2022-08-31 18:44:14,156] [    INFO][0m - loss: 3.47e-06, learning_rate: 2.025e-05, global_step: 650, interval_runtime: 0.6582, interval_samples_per_second: 12.155, interval_steps_per_second: 15.194, epoch: 32.5[0m
[32m[2022-08-31 18:44:14,809] [    INFO][0m - loss: 1.92e-06, learning_rate: 2.01e-05, global_step: 660, interval_runtime: 0.6533, interval_samples_per_second: 12.246, interval_steps_per_second: 15.308, epoch: 33.0[0m
[32m[2022-08-31 18:44:15,490] [    INFO][0m - loss: 1.31e-06, learning_rate: 1.995e-05, global_step: 670, interval_runtime: 0.6809, interval_samples_per_second: 11.75, interval_steps_per_second: 14.687, epoch: 33.5[0m
[32m[2022-08-31 18:44:16,065] [    INFO][0m - loss: 3.31e-06, learning_rate: 1.98e-05, global_step: 680, interval_runtime: 0.5754, interval_samples_per_second: 13.903, interval_steps_per_second: 17.379, epoch: 34.0[0m
[32m[2022-08-31 18:44:16,725] [    INFO][0m - loss: 2.102e-05, learning_rate: 1.965e-05, global_step: 690, interval_runtime: 0.6601, interval_samples_per_second: 12.119, interval_steps_per_second: 15.149, epoch: 34.5[0m
[32m[2022-08-31 18:44:17,337] [    INFO][0m - loss: 3.92e-06, learning_rate: 1.95e-05, global_step: 700, interval_runtime: 0.6114, interval_samples_per_second: 13.084, interval_steps_per_second: 16.356, epoch: 35.0[0m
[32m[2022-08-31 18:44:17,338] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:44:17,338] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:44:17,338] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:44:17,338] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:44:17,338] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:44:17,668] [    INFO][0m - eval_loss: 2.5491092205047607, eval_accuracy: 0.64375, eval_runtime: 0.3302, eval_samples_per_second: 484.608, eval_steps_per_second: 15.144, epoch: 35.0[0m
[32m[2022-08-31 18:44:17,669] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-700[0m
[32m[2022-08-31 18:44:17,669] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:44:19,891] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-700/tokenizer_config.json[0m
[32m[2022-08-31 18:44:19,891] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-700/special_tokens_map.json[0m
[32m[2022-08-31 18:44:25,433] [    INFO][0m - loss: 3.26e-06, learning_rate: 1.935e-05, global_step: 710, interval_runtime: 8.0959, interval_samples_per_second: 0.988, interval_steps_per_second: 1.235, epoch: 35.5[0m
[32m[2022-08-31 18:44:26,045] [    INFO][0m - loss: 2.19e-06, learning_rate: 1.9200000000000003e-05, global_step: 720, interval_runtime: 0.6121, interval_samples_per_second: 13.069, interval_steps_per_second: 16.337, epoch: 36.0[0m
[32m[2022-08-31 18:44:26,709] [    INFO][0m - loss: 2.09e-06, learning_rate: 1.9050000000000002e-05, global_step: 730, interval_runtime: 0.6642, interval_samples_per_second: 12.044, interval_steps_per_second: 15.055, epoch: 36.5[0m
[32m[2022-08-31 18:44:27,320] [    INFO][0m - loss: 3.06e-06, learning_rate: 1.8900000000000002e-05, global_step: 740, interval_runtime: 0.6108, interval_samples_per_second: 13.098, interval_steps_per_second: 16.372, epoch: 37.0[0m
[32m[2022-08-31 18:44:28,040] [    INFO][0m - loss: 2.07e-06, learning_rate: 1.8750000000000002e-05, global_step: 750, interval_runtime: 0.7195, interval_samples_per_second: 11.119, interval_steps_per_second: 13.899, epoch: 37.5[0m
[32m[2022-08-31 18:44:28,632] [    INFO][0m - loss: 5.81e-06, learning_rate: 1.86e-05, global_step: 760, interval_runtime: 0.5927, interval_samples_per_second: 13.497, interval_steps_per_second: 16.871, epoch: 38.0[0m
[32m[2022-08-31 18:44:29,301] [    INFO][0m - loss: 2.12e-06, learning_rate: 1.845e-05, global_step: 770, interval_runtime: 0.6687, interval_samples_per_second: 11.964, interval_steps_per_second: 14.955, epoch: 38.5[0m
[32m[2022-08-31 18:44:29,892] [    INFO][0m - loss: 2.09e-06, learning_rate: 1.83e-05, global_step: 780, interval_runtime: 0.5905, interval_samples_per_second: 13.548, interval_steps_per_second: 16.935, epoch: 39.0[0m
[32m[2022-08-31 18:44:30,583] [    INFO][0m - loss: 1.4e-06, learning_rate: 1.815e-05, global_step: 790, interval_runtime: 0.6916, interval_samples_per_second: 11.567, interval_steps_per_second: 14.459, epoch: 39.5[0m
[32m[2022-08-31 18:44:31,192] [    INFO][0m - loss: 2.11e-06, learning_rate: 1.8e-05, global_step: 800, interval_runtime: 0.6086, interval_samples_per_second: 13.145, interval_steps_per_second: 16.432, epoch: 40.0[0m
[32m[2022-08-31 18:44:31,192] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:44:31,192] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:44:31,192] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:44:31,192] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:44:31,192] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:44:31,520] [    INFO][0m - eval_loss: 2.5497260093688965, eval_accuracy: 0.64375, eval_runtime: 0.3275, eval_samples_per_second: 488.573, eval_steps_per_second: 15.268, epoch: 40.0[0m
[32m[2022-08-31 18:44:31,520] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-800[0m
[32m[2022-08-31 18:44:31,521] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:44:33,750] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-800/tokenizer_config.json[0m
[32m[2022-08-31 18:44:33,750] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-800/special_tokens_map.json[0m
[32m[2022-08-31 18:44:39,478] [    INFO][0m - loss: 2.01e-06, learning_rate: 1.785e-05, global_step: 810, interval_runtime: 8.2866, interval_samples_per_second: 0.965, interval_steps_per_second: 1.207, epoch: 40.5[0m
[32m[2022-08-31 18:44:40,082] [    INFO][0m - loss: 2.77e-06, learning_rate: 1.77e-05, global_step: 820, interval_runtime: 0.6034, interval_samples_per_second: 13.258, interval_steps_per_second: 16.573, epoch: 41.0[0m
[32m[2022-08-31 18:44:40,726] [    INFO][0m - loss: 2.42e-06, learning_rate: 1.755e-05, global_step: 830, interval_runtime: 0.6448, interval_samples_per_second: 12.407, interval_steps_per_second: 15.509, epoch: 41.5[0m
[32m[2022-08-31 18:44:41,313] [    INFO][0m - loss: 1.92e-06, learning_rate: 1.74e-05, global_step: 840, interval_runtime: 0.5864, interval_samples_per_second: 13.642, interval_steps_per_second: 17.053, epoch: 42.0[0m
[32m[2022-08-31 18:44:41,969] [    INFO][0m - loss: 3.02e-06, learning_rate: 1.725e-05, global_step: 850, interval_runtime: 0.6561, interval_samples_per_second: 12.192, interval_steps_per_second: 15.24, epoch: 42.5[0m
[32m[2022-08-31 18:44:42,563] [    INFO][0m - loss: 2.62e-06, learning_rate: 1.71e-05, global_step: 860, interval_runtime: 0.594, interval_samples_per_second: 13.467, interval_steps_per_second: 16.834, epoch: 43.0[0m
[32m[2022-08-31 18:44:43,231] [    INFO][0m - loss: 2.21e-06, learning_rate: 1.695e-05, global_step: 870, interval_runtime: 0.6678, interval_samples_per_second: 11.98, interval_steps_per_second: 14.975, epoch: 43.5[0m
[32m[2022-08-31 18:44:43,820] [    INFO][0m - loss: 1.73e-06, learning_rate: 1.6800000000000002e-05, global_step: 880, interval_runtime: 0.5887, interval_samples_per_second: 13.59, interval_steps_per_second: 16.987, epoch: 44.0[0m
[32m[2022-08-31 18:44:44,483] [    INFO][0m - loss: 2.37e-06, learning_rate: 1.665e-05, global_step: 890, interval_runtime: 0.6635, interval_samples_per_second: 12.058, interval_steps_per_second: 15.072, epoch: 44.5[0m
[32m[2022-08-31 18:44:45,070] [    INFO][0m - loss: 4.98e-06, learning_rate: 1.65e-05, global_step: 900, interval_runtime: 0.5864, interval_samples_per_second: 13.643, interval_steps_per_second: 17.054, epoch: 45.0[0m
[32m[2022-08-31 18:44:45,071] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:44:45,071] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:44:45,071] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:44:45,071] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:44:45,072] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:44:45,397] [    INFO][0m - eval_loss: 2.569244861602783, eval_accuracy: 0.64375, eval_runtime: 0.3259, eval_samples_per_second: 490.917, eval_steps_per_second: 15.341, epoch: 45.0[0m
[32m[2022-08-31 18:44:45,398] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-900[0m
[32m[2022-08-31 18:44:45,398] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:44:47,852] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-900/tokenizer_config.json[0m
[32m[2022-08-31 18:44:47,852] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-900/special_tokens_map.json[0m
[32m[2022-08-31 18:44:53,423] [    INFO][0m - loss: 2.55e-06, learning_rate: 1.635e-05, global_step: 910, interval_runtime: 8.353, interval_samples_per_second: 0.958, interval_steps_per_second: 1.197, epoch: 45.5[0m
[32m[2022-08-31 18:44:54,039] [    INFO][0m - loss: 1.41e-06, learning_rate: 1.62e-05, global_step: 920, interval_runtime: 0.617, interval_samples_per_second: 12.967, interval_steps_per_second: 16.208, epoch: 46.0[0m
[32m[2022-08-31 18:44:54,740] [    INFO][0m - loss: 2.1e-06, learning_rate: 1.605e-05, global_step: 930, interval_runtime: 0.6994, interval_samples_per_second: 11.439, interval_steps_per_second: 14.299, epoch: 46.5[0m
[32m[2022-08-31 18:44:55,395] [    INFO][0m - loss: 1.07e-06, learning_rate: 1.59e-05, global_step: 940, interval_runtime: 0.6561, interval_samples_per_second: 12.194, interval_steps_per_second: 15.242, epoch: 47.0[0m
[32m[2022-08-31 18:44:56,122] [    INFO][0m - loss: 2.47e-06, learning_rate: 1.575e-05, global_step: 950, interval_runtime: 0.7267, interval_samples_per_second: 11.009, interval_steps_per_second: 13.762, epoch: 47.5[0m
[32m[2022-08-31 18:44:56,754] [    INFO][0m - loss: 2.57e-06, learning_rate: 1.56e-05, global_step: 960, interval_runtime: 0.6322, interval_samples_per_second: 12.654, interval_steps_per_second: 15.818, epoch: 48.0[0m
[32m[2022-08-31 18:44:57,500] [    INFO][0m - loss: 2.31e-06, learning_rate: 1.545e-05, global_step: 970, interval_runtime: 0.7457, interval_samples_per_second: 10.729, interval_steps_per_second: 13.411, epoch: 48.5[0m
[32m[2022-08-31 18:44:58,118] [    INFO][0m - loss: 2.15e-06, learning_rate: 1.53e-05, global_step: 980, interval_runtime: 0.6188, interval_samples_per_second: 12.928, interval_steps_per_second: 16.16, epoch: 49.0[0m
[32m[2022-08-31 18:44:58,808] [    INFO][0m - loss: 5.44e-06, learning_rate: 1.515e-05, global_step: 990, interval_runtime: 0.69, interval_samples_per_second: 11.594, interval_steps_per_second: 14.493, epoch: 49.5[0m
[32m[2022-08-31 18:44:59,401] [    INFO][0m - loss: 1.04e-06, learning_rate: 1.5e-05, global_step: 1000, interval_runtime: 0.5922, interval_samples_per_second: 13.508, interval_steps_per_second: 16.885, epoch: 50.0[0m
[32m[2022-08-31 18:44:59,401] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:44:59,402] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:44:59,402] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:44:59,402] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:44:59,402] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:44:59,725] [    INFO][0m - eval_loss: 2.5889885425567627, eval_accuracy: 0.64375, eval_runtime: 0.3234, eval_samples_per_second: 494.734, eval_steps_per_second: 15.46, epoch: 50.0[0m
[32m[2022-08-31 18:44:59,726] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1000[0m
[32m[2022-08-31 18:44:59,726] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:45:01,912] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1000/tokenizer_config.json[0m
[32m[2022-08-31 18:45:01,912] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1000/special_tokens_map.json[0m
[32m[2022-08-31 18:45:07,633] [    INFO][0m - loss: 9.8e-07, learning_rate: 1.485e-05, global_step: 1010, interval_runtime: 8.2315, interval_samples_per_second: 0.972, interval_steps_per_second: 1.215, epoch: 50.5[0m
[32m[2022-08-31 18:45:08,201] [    INFO][0m - loss: 2.36e-06, learning_rate: 1.47e-05, global_step: 1020, interval_runtime: 0.5693, interval_samples_per_second: 14.053, interval_steps_per_second: 17.566, epoch: 51.0[0m
[32m[2022-08-31 18:45:08,860] [    INFO][0m - loss: 1.45e-06, learning_rate: 1.455e-05, global_step: 1030, interval_runtime: 0.6585, interval_samples_per_second: 12.148, interval_steps_per_second: 15.186, epoch: 51.5[0m
[32m[2022-08-31 18:45:09,466] [    INFO][0m - loss: 1.64e-06, learning_rate: 1.44e-05, global_step: 1040, interval_runtime: 0.6066, interval_samples_per_second: 13.189, interval_steps_per_second: 16.486, epoch: 52.0[0m
[32m[2022-08-31 18:45:10,142] [    INFO][0m - loss: 1.85e-06, learning_rate: 1.4249999999999999e-05, global_step: 1050, interval_runtime: 0.675, interval_samples_per_second: 11.851, interval_steps_per_second: 14.814, epoch: 52.5[0m
[32m[2022-08-31 18:45:10,740] [    INFO][0m - loss: 2.08e-06, learning_rate: 1.4099999999999999e-05, global_step: 1060, interval_runtime: 0.5991, interval_samples_per_second: 13.354, interval_steps_per_second: 16.692, epoch: 53.0[0m
[32m[2022-08-31 18:45:11,393] [    INFO][0m - loss: 1.59e-06, learning_rate: 1.395e-05, global_step: 1070, interval_runtime: 0.6523, interval_samples_per_second: 12.265, interval_steps_per_second: 15.332, epoch: 53.5[0m
[32m[2022-08-31 18:45:11,971] [    INFO][0m - loss: 1.15e-06, learning_rate: 1.3800000000000002e-05, global_step: 1080, interval_runtime: 0.5784, interval_samples_per_second: 13.832, interval_steps_per_second: 17.29, epoch: 54.0[0m
[32m[2022-08-31 18:45:12,648] [    INFO][0m - loss: 2.42e-06, learning_rate: 1.3650000000000001e-05, global_step: 1090, interval_runtime: 0.6772, interval_samples_per_second: 11.813, interval_steps_per_second: 14.766, epoch: 54.5[0m
[32m[2022-08-31 18:45:13,248] [    INFO][0m - loss: 2.03e-06, learning_rate: 1.3500000000000001e-05, global_step: 1100, interval_runtime: 0.5998, interval_samples_per_second: 13.337, interval_steps_per_second: 16.672, epoch: 55.0[0m
[32m[2022-08-31 18:45:13,249] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:45:13,249] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:45:13,249] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:45:13,249] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:45:13,249] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:45:13,585] [    INFO][0m - eval_loss: 2.605538845062256, eval_accuracy: 0.64375, eval_runtime: 0.3362, eval_samples_per_second: 475.974, eval_steps_per_second: 14.874, epoch: 55.0[0m
[32m[2022-08-31 18:45:13,586] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1100[0m
[32m[2022-08-31 18:45:13,586] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:45:15,665] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1100/tokenizer_config.json[0m
[32m[2022-08-31 18:45:15,665] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1100/special_tokens_map.json[0m
[32m[2022-08-31 18:45:21,172] [    INFO][0m - loss: 1.02e-06, learning_rate: 1.3350000000000001e-05, global_step: 1110, interval_runtime: 7.9234, interval_samples_per_second: 1.01, interval_steps_per_second: 1.262, epoch: 55.5[0m
[32m[2022-08-31 18:45:21,760] [    INFO][0m - loss: 2.05e-06, learning_rate: 1.32e-05, global_step: 1120, interval_runtime: 0.5883, interval_samples_per_second: 13.598, interval_steps_per_second: 16.997, epoch: 56.0[0m
[32m[2022-08-31 18:45:22,449] [    INFO][0m - loss: 1.4e-06, learning_rate: 1.305e-05, global_step: 1130, interval_runtime: 0.689, interval_samples_per_second: 11.612, interval_steps_per_second: 14.514, epoch: 56.5[0m
[32m[2022-08-31 18:45:23,051] [    INFO][0m - loss: 1.61e-06, learning_rate: 1.29e-05, global_step: 1140, interval_runtime: 0.6026, interval_samples_per_second: 13.277, interval_steps_per_second: 16.596, epoch: 57.0[0m
[32m[2022-08-31 18:45:23,731] [    INFO][0m - loss: 2.04e-06, learning_rate: 1.275e-05, global_step: 1150, interval_runtime: 0.6796, interval_samples_per_second: 11.771, interval_steps_per_second: 14.714, epoch: 57.5[0m
[32m[2022-08-31 18:45:24,376] [    INFO][0m - loss: 1.73e-06, learning_rate: 1.26e-05, global_step: 1160, interval_runtime: 0.6446, interval_samples_per_second: 12.41, interval_steps_per_second: 15.513, epoch: 58.0[0m
[32m[2022-08-31 18:45:25,069] [    INFO][0m - loss: 2.11e-06, learning_rate: 1.245e-05, global_step: 1170, interval_runtime: 0.6936, interval_samples_per_second: 11.534, interval_steps_per_second: 14.417, epoch: 58.5[0m
[32m[2022-08-31 18:45:25,661] [    INFO][0m - loss: 1.5e-06, learning_rate: 1.2299999999999999e-05, global_step: 1180, interval_runtime: 0.5916, interval_samples_per_second: 13.524, interval_steps_per_second: 16.904, epoch: 59.0[0m
[32m[2022-08-31 18:45:26,350] [    INFO][0m - loss: 3.44e-06, learning_rate: 1.215e-05, global_step: 1190, interval_runtime: 0.6891, interval_samples_per_second: 11.609, interval_steps_per_second: 14.512, epoch: 59.5[0m
[32m[2022-08-31 18:45:26,958] [    INFO][0m - loss: 1.17e-06, learning_rate: 1.2e-05, global_step: 1200, interval_runtime: 0.6081, interval_samples_per_second: 13.156, interval_steps_per_second: 16.445, epoch: 60.0[0m
[32m[2022-08-31 18:45:26,959] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:45:26,959] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:45:26,959] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:45:26,959] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:45:26,959] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:45:27,282] [    INFO][0m - eval_loss: 2.6195080280303955, eval_accuracy: 0.65, eval_runtime: 0.322, eval_samples_per_second: 496.917, eval_steps_per_second: 15.529, epoch: 60.0[0m
[32m[2022-08-31 18:45:27,282] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1200[0m
[32m[2022-08-31 18:45:27,282] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:45:29,370] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1200/tokenizer_config.json[0m
[32m[2022-08-31 18:45:29,371] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1200/special_tokens_map.json[0m
[32m[2022-08-31 18:45:35,053] [    INFO][0m - loss: 1.28e-06, learning_rate: 1.185e-05, global_step: 1210, interval_runtime: 8.095, interval_samples_per_second: 0.988, interval_steps_per_second: 1.235, epoch: 60.5[0m
[32m[2022-08-31 18:45:35,648] [    INFO][0m - loss: 1.59e-06, learning_rate: 1.1700000000000001e-05, global_step: 1220, interval_runtime: 0.5948, interval_samples_per_second: 13.45, interval_steps_per_second: 16.813, epoch: 61.0[0m
[32m[2022-08-31 18:45:36,388] [    INFO][0m - loss: 1.23e-06, learning_rate: 1.1550000000000001e-05, global_step: 1230, interval_runtime: 0.74, interval_samples_per_second: 10.811, interval_steps_per_second: 13.513, epoch: 61.5[0m
[32m[2022-08-31 18:45:37,035] [    INFO][0m - loss: 1.84e-06, learning_rate: 1.1400000000000001e-05, global_step: 1240, interval_runtime: 0.6464, interval_samples_per_second: 12.377, interval_steps_per_second: 15.471, epoch: 62.0[0m
[32m[2022-08-31 18:45:37,739] [    INFO][0m - loss: 1.7e-06, learning_rate: 1.125e-05, global_step: 1250, interval_runtime: 0.7049, interval_samples_per_second: 11.349, interval_steps_per_second: 14.187, epoch: 62.5[0m
[32m[2022-08-31 18:45:38,335] [    INFO][0m - loss: 1.73e-06, learning_rate: 1.11e-05, global_step: 1260, interval_runtime: 0.595, interval_samples_per_second: 13.445, interval_steps_per_second: 16.806, epoch: 63.0[0m
[32m[2022-08-31 18:45:39,024] [    INFO][0m - loss: 4.29e-06, learning_rate: 1.095e-05, global_step: 1270, interval_runtime: 0.6896, interval_samples_per_second: 11.601, interval_steps_per_second: 14.502, epoch: 63.5[0m
[32m[2022-08-31 18:45:39,626] [    INFO][0m - loss: 1.17e-06, learning_rate: 1.08e-05, global_step: 1280, interval_runtime: 0.6019, interval_samples_per_second: 13.291, interval_steps_per_second: 16.614, epoch: 64.0[0m
[32m[2022-08-31 18:45:40,337] [    INFO][0m - loss: 2.33e-06, learning_rate: 1.065e-05, global_step: 1290, interval_runtime: 0.7112, interval_samples_per_second: 11.248, interval_steps_per_second: 14.061, epoch: 64.5[0m
[32m[2022-08-31 18:45:40,935] [    INFO][0m - loss: 1.66e-06, learning_rate: 1.05e-05, global_step: 1300, interval_runtime: 0.5973, interval_samples_per_second: 13.393, interval_steps_per_second: 16.741, epoch: 65.0[0m
[32m[2022-08-31 18:45:40,935] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:45:40,936] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:45:40,936] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:45:40,936] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:45:40,936] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:45:41,272] [    INFO][0m - eval_loss: 2.6391446590423584, eval_accuracy: 0.65, eval_runtime: 0.3357, eval_samples_per_second: 476.62, eval_steps_per_second: 14.894, epoch: 65.0[0m
[32m[2022-08-31 18:45:41,272] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1300[0m
[32m[2022-08-31 18:45:41,272] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:45:44,941] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1300/tokenizer_config.json[0m
[32m[2022-08-31 18:45:44,942] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1300/special_tokens_map.json[0m
[32m[2022-08-31 18:45:50,643] [    INFO][0m - loss: 1.42e-06, learning_rate: 1.035e-05, global_step: 1310, interval_runtime: 9.7081, interval_samples_per_second: 0.824, interval_steps_per_second: 1.03, epoch: 65.5[0m
[32m[2022-08-31 18:45:51,242] [    INFO][0m - loss: 1.2e-06, learning_rate: 1.02e-05, global_step: 1320, interval_runtime: 0.5994, interval_samples_per_second: 13.346, interval_steps_per_second: 16.683, epoch: 66.0[0m
[32m[2022-08-31 18:45:51,907] [    INFO][0m - loss: 1.14e-06, learning_rate: 1.005e-05, global_step: 1330, interval_runtime: 0.6647, interval_samples_per_second: 12.035, interval_steps_per_second: 15.044, epoch: 66.5[0m
[32m[2022-08-31 18:45:52,496] [    INFO][0m - loss: 1.92e-06, learning_rate: 9.9e-06, global_step: 1340, interval_runtime: 0.5896, interval_samples_per_second: 13.569, interval_steps_per_second: 16.961, epoch: 67.0[0m
[32m[2022-08-31 18:45:53,213] [    INFO][0m - loss: 8.9e-07, learning_rate: 9.75e-06, global_step: 1350, interval_runtime: 0.7168, interval_samples_per_second: 11.161, interval_steps_per_second: 13.951, epoch: 67.5[0m
[32m[2022-08-31 18:45:53,856] [    INFO][0m - loss: 3.86e-06, learning_rate: 9.600000000000001e-06, global_step: 1360, interval_runtime: 0.6432, interval_samples_per_second: 12.438, interval_steps_per_second: 15.547, epoch: 68.0[0m
[32m[2022-08-31 18:45:54,592] [    INFO][0m - loss: 9.4e-07, learning_rate: 9.450000000000001e-06, global_step: 1370, interval_runtime: 0.7355, interval_samples_per_second: 10.876, interval_steps_per_second: 13.596, epoch: 68.5[0m
[32m[2022-08-31 18:45:55,244] [    INFO][0m - loss: 4.2e-06, learning_rate: 9.3e-06, global_step: 1380, interval_runtime: 0.6516, interval_samples_per_second: 12.278, interval_steps_per_second: 15.348, epoch: 69.0[0m
[32m[2022-08-31 18:45:55,924] [    INFO][0m - loss: 9e-07, learning_rate: 9.15e-06, global_step: 1390, interval_runtime: 0.6804, interval_samples_per_second: 11.758, interval_steps_per_second: 14.698, epoch: 69.5[0m
[32m[2022-08-31 18:45:56,527] [    INFO][0m - loss: 1.24e-06, learning_rate: 9e-06, global_step: 1400, interval_runtime: 0.6029, interval_samples_per_second: 13.27, interval_steps_per_second: 16.588, epoch: 70.0[0m
[32m[2022-08-31 18:45:56,527] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:45:56,527] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:45:56,527] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:45:56,528] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:45:56,528] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:45:56,875] [    INFO][0m - eval_loss: 2.636094570159912, eval_accuracy: 0.65, eval_runtime: 0.3468, eval_samples_per_second: 461.303, eval_steps_per_second: 14.416, epoch: 70.0[0m
[32m[2022-08-31 18:45:56,875] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1400[0m
[32m[2022-08-31 18:45:56,876] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:45:58,791] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1400/tokenizer_config.json[0m
[32m[2022-08-31 18:45:59,143] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1400/special_tokens_map.json[0m
[32m[2022-08-31 18:46:05,132] [    INFO][0m - loss: 1.9e-06, learning_rate: 8.85e-06, global_step: 1410, interval_runtime: 8.6048, interval_samples_per_second: 0.93, interval_steps_per_second: 1.162, epoch: 70.5[0m
[32m[2022-08-31 18:46:05,772] [    INFO][0m - loss: 1.508e-05, learning_rate: 8.7e-06, global_step: 1420, interval_runtime: 0.6402, interval_samples_per_second: 12.495, interval_steps_per_second: 15.619, epoch: 71.0[0m
[32m[2022-08-31 18:46:06,477] [    INFO][0m - loss: 4.35e-06, learning_rate: 8.55e-06, global_step: 1430, interval_runtime: 0.7056, interval_samples_per_second: 11.337, interval_steps_per_second: 14.172, epoch: 71.5[0m
[32m[2022-08-31 18:46:07,099] [    INFO][0m - loss: 1.03e-06, learning_rate: 8.400000000000001e-06, global_step: 1440, interval_runtime: 0.6217, interval_samples_per_second: 12.868, interval_steps_per_second: 16.085, epoch: 72.0[0m
[32m[2022-08-31 18:46:07,788] [    INFO][0m - loss: 2.2e-06, learning_rate: 8.25e-06, global_step: 1450, interval_runtime: 0.6888, interval_samples_per_second: 11.614, interval_steps_per_second: 14.517, epoch: 72.5[0m
[32m[2022-08-31 18:46:08,385] [    INFO][0m - loss: 1.27e-06, learning_rate: 8.1e-06, global_step: 1460, interval_runtime: 0.5964, interval_samples_per_second: 13.413, interval_steps_per_second: 16.767, epoch: 73.0[0m
[32m[2022-08-31 18:46:09,137] [    INFO][0m - loss: 1.96e-06, learning_rate: 7.95e-06, global_step: 1470, interval_runtime: 0.753, interval_samples_per_second: 10.623, interval_steps_per_second: 13.279, epoch: 73.5[0m
[32m[2022-08-31 18:46:09,816] [    INFO][0m - loss: 1.31e-06, learning_rate: 7.8e-06, global_step: 1480, interval_runtime: 0.6785, interval_samples_per_second: 11.791, interval_steps_per_second: 14.739, epoch: 74.0[0m
[32m[2022-08-31 18:46:10,513] [    INFO][0m - loss: 1.06e-06, learning_rate: 7.65e-06, global_step: 1490, interval_runtime: 0.6967, interval_samples_per_second: 11.483, interval_steps_per_second: 14.353, epoch: 74.5[0m
[32m[2022-08-31 18:46:11,115] [    INFO][0m - loss: 9.4e-07, learning_rate: 7.5e-06, global_step: 1500, interval_runtime: 0.5939, interval_samples_per_second: 13.471, interval_steps_per_second: 16.838, epoch: 75.0[0m
[32m[2022-08-31 18:46:11,116] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:46:11,116] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:46:11,116] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:46:11,116] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:46:11,116] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:46:11,453] [    INFO][0m - eval_loss: 2.6952099800109863, eval_accuracy: 0.6625, eval_runtime: 0.3366, eval_samples_per_second: 475.315, eval_steps_per_second: 14.854, epoch: 75.0[0m
[32m[2022-08-31 18:46:11,453] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1500[0m
[32m[2022-08-31 18:46:11,454] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:46:13,528] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1500/tokenizer_config.json[0m
[32m[2022-08-31 18:46:13,528] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1500/special_tokens_map.json[0m
[32m[2022-08-31 18:46:20,697] [    INFO][0m - loss: 1.15e-06, learning_rate: 7.35e-06, global_step: 1510, interval_runtime: 9.5905, interval_samples_per_second: 0.834, interval_steps_per_second: 1.043, epoch: 75.5[0m
[32m[2022-08-31 18:46:21,315] [    INFO][0m - loss: 2.33e-06, learning_rate: 7.2e-06, global_step: 1520, interval_runtime: 0.6174, interval_samples_per_second: 12.958, interval_steps_per_second: 16.198, epoch: 76.0[0m
[32m[2022-08-31 18:46:22,057] [    INFO][0m - loss: 8.6e-07, learning_rate: 7.049999999999999e-06, global_step: 1530, interval_runtime: 0.742, interval_samples_per_second: 10.781, interval_steps_per_second: 13.476, epoch: 76.5[0m
[32m[2022-08-31 18:46:22,706] [    INFO][0m - loss: 1.72e-06, learning_rate: 6.900000000000001e-06, global_step: 1540, interval_runtime: 0.6492, interval_samples_per_second: 12.322, interval_steps_per_second: 15.403, epoch: 77.0[0m
[32m[2022-08-31 18:46:23,453] [    INFO][0m - loss: 2.43e-06, learning_rate: 6.750000000000001e-06, global_step: 1550, interval_runtime: 0.7473, interval_samples_per_second: 10.705, interval_steps_per_second: 13.382, epoch: 77.5[0m
[32m[2022-08-31 18:46:24,178] [    INFO][0m - loss: 1.24e-06, learning_rate: 6.6e-06, global_step: 1560, interval_runtime: 0.7244, interval_samples_per_second: 11.044, interval_steps_per_second: 13.805, epoch: 78.0[0m
[32m[2022-08-31 18:46:24,909] [    INFO][0m - loss: 1.52e-06, learning_rate: 6.45e-06, global_step: 1570, interval_runtime: 0.7316, interval_samples_per_second: 10.934, interval_steps_per_second: 13.668, epoch: 78.5[0m
[32m[2022-08-31 18:46:25,567] [    INFO][0m - loss: 1.18e-06, learning_rate: 6.3e-06, global_step: 1580, interval_runtime: 0.6577, interval_samples_per_second: 12.164, interval_steps_per_second: 15.205, epoch: 79.0[0m
[32m[2022-08-31 18:46:26,287] [    INFO][0m - loss: 7.6e-07, learning_rate: 6.1499999999999996e-06, global_step: 1590, interval_runtime: 0.72, interval_samples_per_second: 11.111, interval_steps_per_second: 13.889, epoch: 79.5[0m
[32m[2022-08-31 18:46:26,940] [    INFO][0m - loss: 1.34e-06, learning_rate: 6e-06, global_step: 1600, interval_runtime: 0.6524, interval_samples_per_second: 12.262, interval_steps_per_second: 15.328, epoch: 80.0[0m
[32m[2022-08-31 18:46:26,940] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:46:26,940] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:46:26,940] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:46:26,940] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:46:26,940] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:46:27,291] [    INFO][0m - eval_loss: 2.6971592903137207, eval_accuracy: 0.6625, eval_runtime: 0.3498, eval_samples_per_second: 457.383, eval_steps_per_second: 14.293, epoch: 80.0[0m
[32m[2022-08-31 18:46:27,292] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1600[0m
[32m[2022-08-31 18:46:27,292] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:46:29,579] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1600/tokenizer_config.json[0m
[32m[2022-08-31 18:46:29,580] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1600/special_tokens_map.json[0m
[32m[2022-08-31 18:46:34,876] [    INFO][0m - loss: 1.64e-06, learning_rate: 5.850000000000001e-06, global_step: 1610, interval_runtime: 7.9368, interval_samples_per_second: 1.008, interval_steps_per_second: 1.26, epoch: 80.5[0m
[32m[2022-08-31 18:46:35,507] [    INFO][0m - loss: 1.3e-06, learning_rate: 5.7000000000000005e-06, global_step: 1620, interval_runtime: 0.6305, interval_samples_per_second: 12.688, interval_steps_per_second: 15.86, epoch: 81.0[0m
[32m[2022-08-31 18:46:36,190] [    INFO][0m - loss: 1.44e-06, learning_rate: 5.55e-06, global_step: 1630, interval_runtime: 0.6837, interval_samples_per_second: 11.702, interval_steps_per_second: 14.627, epoch: 81.5[0m
[32m[2022-08-31 18:46:36,841] [    INFO][0m - loss: 8.5e-07, learning_rate: 5.4e-06, global_step: 1640, interval_runtime: 0.6502, interval_samples_per_second: 12.303, interval_steps_per_second: 15.379, epoch: 82.0[0m
[32m[2022-08-31 18:46:37,565] [    INFO][0m - loss: 6.2e-07, learning_rate: 5.25e-06, global_step: 1650, interval_runtime: 0.7244, interval_samples_per_second: 11.043, interval_steps_per_second: 13.804, epoch: 82.5[0m
[32m[2022-08-31 18:46:38,241] [    INFO][0m - loss: 8e-07, learning_rate: 5.1e-06, global_step: 1660, interval_runtime: 0.6759, interval_samples_per_second: 11.836, interval_steps_per_second: 14.795, epoch: 83.0[0m
[32m[2022-08-31 18:46:38,948] [    INFO][0m - loss: 1.43e-06, learning_rate: 4.95e-06, global_step: 1670, interval_runtime: 0.7068, interval_samples_per_second: 11.319, interval_steps_per_second: 14.148, epoch: 83.5[0m
[32m[2022-08-31 18:46:39,623] [    INFO][0m - loss: 1.02e-06, learning_rate: 4.800000000000001e-06, global_step: 1680, interval_runtime: 0.6746, interval_samples_per_second: 11.858, interval_steps_per_second: 14.823, epoch: 84.0[0m
[32m[2022-08-31 18:46:40,381] [    INFO][0m - loss: 6.3e-07, learning_rate: 4.65e-06, global_step: 1690, interval_runtime: 0.7582, interval_samples_per_second: 10.551, interval_steps_per_second: 13.189, epoch: 84.5[0m
[32m[2022-08-31 18:46:41,034] [    INFO][0m - loss: 1.04e-06, learning_rate: 4.5e-06, global_step: 1700, interval_runtime: 0.6534, interval_samples_per_second: 12.244, interval_steps_per_second: 15.305, epoch: 85.0[0m
[32m[2022-08-31 18:46:41,035] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:46:41,036] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:46:41,036] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:46:41,036] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:46:41,036] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:46:41,387] [    INFO][0m - eval_loss: 2.699037551879883, eval_accuracy: 0.65625, eval_runtime: 0.3506, eval_samples_per_second: 456.366, eval_steps_per_second: 14.261, epoch: 85.0[0m
[32m[2022-08-31 18:46:41,387] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1700[0m
[32m[2022-08-31 18:46:41,388] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:46:44,942] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1700/tokenizer_config.json[0m
[32m[2022-08-31 18:46:44,942] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1700/special_tokens_map.json[0m
[32m[2022-08-31 18:46:50,432] [    INFO][0m - loss: 1.99e-06, learning_rate: 4.35e-06, global_step: 1710, interval_runtime: 9.3979, interval_samples_per_second: 0.851, interval_steps_per_second: 1.064, epoch: 85.5[0m
[32m[2022-08-31 18:46:51,082] [    INFO][0m - loss: 1.33e-06, learning_rate: 4.2000000000000004e-06, global_step: 1720, interval_runtime: 0.6491, interval_samples_per_second: 12.324, interval_steps_per_second: 15.405, epoch: 86.0[0m
[32m[2022-08-31 18:46:51,826] [    INFO][0m - loss: 2.11e-06, learning_rate: 4.05e-06, global_step: 1730, interval_runtime: 0.7447, interval_samples_per_second: 10.742, interval_steps_per_second: 13.428, epoch: 86.5[0m
[32m[2022-08-31 18:46:52,489] [    INFO][0m - loss: 1.11e-06, learning_rate: 3.9e-06, global_step: 1740, interval_runtime: 0.6633, interval_samples_per_second: 12.061, interval_steps_per_second: 15.077, epoch: 87.0[0m
[32m[2022-08-31 18:46:53,249] [    INFO][0m - loss: 2.19e-06, learning_rate: 3.75e-06, global_step: 1750, interval_runtime: 0.7595, interval_samples_per_second: 10.533, interval_steps_per_second: 13.166, epoch: 87.5[0m
[32m[2022-08-31 18:46:53,962] [    INFO][0m - loss: 9.9e-07, learning_rate: 3.6e-06, global_step: 1760, interval_runtime: 0.7135, interval_samples_per_second: 11.213, interval_steps_per_second: 14.016, epoch: 88.0[0m
[32m[2022-08-31 18:46:54,690] [    INFO][0m - loss: 9.7e-07, learning_rate: 3.4500000000000004e-06, global_step: 1770, interval_runtime: 0.7278, interval_samples_per_second: 10.993, interval_steps_per_second: 13.741, epoch: 88.5[0m
[32m[2022-08-31 18:46:55,334] [    INFO][0m - loss: 2.57e-06, learning_rate: 3.3e-06, global_step: 1780, interval_runtime: 0.6446, interval_samples_per_second: 12.412, interval_steps_per_second: 15.515, epoch: 89.0[0m
[32m[2022-08-31 18:46:56,044] [    INFO][0m - loss: 7.3e-07, learning_rate: 3.15e-06, global_step: 1790, interval_runtime: 0.7091, interval_samples_per_second: 11.282, interval_steps_per_second: 14.102, epoch: 89.5[0m
[32m[2022-08-31 18:46:56,689] [    INFO][0m - loss: 1.02e-06, learning_rate: 3e-06, global_step: 1800, interval_runtime: 0.6451, interval_samples_per_second: 12.4, interval_steps_per_second: 15.5, epoch: 90.0[0m
[32m[2022-08-31 18:46:56,690] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:46:56,690] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:46:56,690] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:46:56,690] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:46:56,690] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:46:57,075] [    INFO][0m - eval_loss: 2.703159809112549, eval_accuracy: 0.65625, eval_runtime: 0.385, eval_samples_per_second: 415.618, eval_steps_per_second: 12.988, epoch: 90.0[0m
[32m[2022-08-31 18:46:57,075] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1800[0m
[32m[2022-08-31 18:46:57,076] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:46:58,986] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1800/tokenizer_config.json[0m
[32m[2022-08-31 18:46:58,987] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1800/special_tokens_map.json[0m
[32m[2022-08-31 18:47:04,169] [    INFO][0m - loss: 1.51e-06, learning_rate: 2.8500000000000002e-06, global_step: 1810, interval_runtime: 7.4799, interval_samples_per_second: 1.07, interval_steps_per_second: 1.337, epoch: 90.5[0m
[32m[2022-08-31 18:47:04,798] [    INFO][0m - loss: 7.4e-07, learning_rate: 2.7e-06, global_step: 1820, interval_runtime: 0.629, interval_samples_per_second: 12.718, interval_steps_per_second: 15.897, epoch: 91.0[0m
[32m[2022-08-31 18:47:05,526] [    INFO][0m - loss: 1.41e-06, learning_rate: 2.55e-06, global_step: 1830, interval_runtime: 0.7285, interval_samples_per_second: 10.981, interval_steps_per_second: 13.726, epoch: 91.5[0m
[32m[2022-08-31 18:47:06,261] [    INFO][0m - loss: 1.52e-06, learning_rate: 2.4000000000000003e-06, global_step: 1840, interval_runtime: 0.7346, interval_samples_per_second: 10.89, interval_steps_per_second: 13.612, epoch: 92.0[0m
[32m[2022-08-31 18:47:06,990] [    INFO][0m - loss: 1.24e-06, learning_rate: 2.25e-06, global_step: 1850, interval_runtime: 0.7297, interval_samples_per_second: 10.963, interval_steps_per_second: 13.704, epoch: 92.5[0m
[32m[2022-08-31 18:47:07,602] [    INFO][0m - loss: 1.02e-06, learning_rate: 2.1000000000000002e-06, global_step: 1860, interval_runtime: 0.6113, interval_samples_per_second: 13.087, interval_steps_per_second: 16.359, epoch: 93.0[0m
[32m[2022-08-31 18:47:08,344] [    INFO][0m - loss: 2.37e-06, learning_rate: 1.95e-06, global_step: 1870, interval_runtime: 0.7417, interval_samples_per_second: 10.786, interval_steps_per_second: 13.483, epoch: 93.5[0m
[32m[2022-08-31 18:47:09,016] [    INFO][0m - loss: 7.5e-07, learning_rate: 1.8e-06, global_step: 1880, interval_runtime: 0.6725, interval_samples_per_second: 11.896, interval_steps_per_second: 14.871, epoch: 94.0[0m
[32m[2022-08-31 18:47:09,743] [    INFO][0m - loss: 9.2e-07, learning_rate: 1.65e-06, global_step: 1890, interval_runtime: 0.7265, interval_samples_per_second: 11.011, interval_steps_per_second: 13.764, epoch: 94.5[0m
[32m[2022-08-31 18:47:10,354] [    INFO][0m - loss: 9.4e-07, learning_rate: 1.5e-06, global_step: 1900, interval_runtime: 0.6117, interval_samples_per_second: 13.077, interval_steps_per_second: 16.347, epoch: 95.0[0m
[32m[2022-08-31 18:47:10,355] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:47:10,355] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:47:10,355] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:47:10,355] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:47:10,355] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:47:10,685] [    INFO][0m - eval_loss: 2.704148530960083, eval_accuracy: 0.65625, eval_runtime: 0.3293, eval_samples_per_second: 485.936, eval_steps_per_second: 15.186, epoch: 95.0[0m
[32m[2022-08-31 18:47:10,685] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1900[0m
[32m[2022-08-31 18:47:10,685] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:47:12,692] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1900/tokenizer_config.json[0m
[32m[2022-08-31 18:47:12,692] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1900/special_tokens_map.json[0m
[32m[2022-08-31 18:47:18,445] [    INFO][0m - loss: 8.9e-07, learning_rate: 1.35e-06, global_step: 1910, interval_runtime: 8.0902, interval_samples_per_second: 0.989, interval_steps_per_second: 1.236, epoch: 95.5[0m
[32m[2022-08-31 18:47:19,046] [    INFO][0m - loss: 1.03e-06, learning_rate: 1.2000000000000002e-06, global_step: 1920, interval_runtime: 0.6015, interval_samples_per_second: 13.3, interval_steps_per_second: 16.625, epoch: 96.0[0m
[32m[2022-08-31 18:47:19,739] [    INFO][0m - loss: 9.3e-07, learning_rate: 1.0500000000000001e-06, global_step: 1930, interval_runtime: 0.6934, interval_samples_per_second: 11.538, interval_steps_per_second: 14.422, epoch: 96.5[0m
[32m[2022-08-31 18:47:20,444] [    INFO][0m - loss: 1.06e-06, learning_rate: 9e-07, global_step: 1940, interval_runtime: 0.705, interval_samples_per_second: 11.348, interval_steps_per_second: 14.185, epoch: 97.0[0m
[32m[2022-08-31 18:47:21,146] [    INFO][0m - loss: 1.48e-06, learning_rate: 7.5e-07, global_step: 1950, interval_runtime: 0.7021, interval_samples_per_second: 11.395, interval_steps_per_second: 14.244, epoch: 97.5[0m
[32m[2022-08-31 18:47:21,786] [    INFO][0m - loss: 1.25e-06, learning_rate: 6.000000000000001e-07, global_step: 1960, interval_runtime: 0.6396, interval_samples_per_second: 12.508, interval_steps_per_second: 15.635, epoch: 98.0[0m
[32m[2022-08-31 18:47:22,488] [    INFO][0m - loss: 1.02e-06, learning_rate: 4.5e-07, global_step: 1970, interval_runtime: 0.7014, interval_samples_per_second: 11.407, interval_steps_per_second: 14.258, epoch: 98.5[0m
[32m[2022-08-31 18:47:23,132] [    INFO][0m - loss: 9.6e-07, learning_rate: 3.0000000000000004e-07, global_step: 1980, interval_runtime: 0.645, interval_samples_per_second: 12.403, interval_steps_per_second: 15.503, epoch: 99.0[0m
[32m[2022-08-31 18:47:23,865] [    INFO][0m - loss: 8.24e-06, learning_rate: 1.5000000000000002e-07, global_step: 1990, interval_runtime: 0.7324, interval_samples_per_second: 10.923, interval_steps_per_second: 13.654, epoch: 99.5[0m
[32m[2022-08-31 18:47:24,491] [    INFO][0m - loss: 7.2e-07, learning_rate: 0.0, global_step: 2000, interval_runtime: 0.6255, interval_samples_per_second: 12.79, interval_steps_per_second: 15.988, epoch: 100.0[0m
[32m[2022-08-31 18:47:24,492] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:47:24,492] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:47:24,492] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:47:24,492] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:47:24,492] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:47:24,831] [    INFO][0m - eval_loss: 2.704615354537964, eval_accuracy: 0.65625, eval_runtime: 0.3391, eval_samples_per_second: 471.857, eval_steps_per_second: 14.746, epoch: 100.0[0m
[32m[2022-08-31 18:47:24,832] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2000[0m
[32m[2022-08-31 18:47:24,832] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:47:26,697] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2000/tokenizer_config.json[0m
[32m[2022-08-31 18:47:26,697] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2000/special_tokens_map.json[0m
[32m[2022-08-31 18:47:31,236] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-31 18:47:31,237] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-1500 (score: 0.6625).[0m
[32m[2022-08-31 18:47:31,964] [    INFO][0m - train_runtime: 283.6469, train_samples_per_second: 56.408, train_steps_per_second: 7.051, train_loss: 0.017825318132809572, epoch: 100.0[0m
[32m[2022-08-31 18:47:31,965] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-31 18:47:31,965] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:47:34,035] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-31 18:47:34,035] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-31 18:47:34,037] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-31 18:47:34,037] [    INFO][0m -   epoch                    =      100.0[0m
[32m[2022-08-31 18:47:34,038] [    INFO][0m -   train_loss               =     0.0178[0m
[32m[2022-08-31 18:47:34,038] [    INFO][0m -   train_runtime            = 0:04:43.64[0m
[32m[2022-08-31 18:47:34,038] [    INFO][0m -   train_samples_per_second =     56.408[0m
[32m[2022-08-31 18:47:34,038] [    INFO][0m -   train_steps_per_second   =      7.051[0m
[32m[2022-08-31 18:47:34,047] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-31 18:47:34,047] [    INFO][0m -   Num examples = 2520[0m
[32m[2022-08-31 18:47:34,047] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:47:34,047] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:47:34,047] [    INFO][0m -   Total prediction steps = 79[0m
[32m[2022-08-31 18:47:39,453] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-31 18:47:39,454] [    INFO][0m -   test_accuracy           =     0.6587[0m
[32m[2022-08-31 18:47:39,454] [    INFO][0m -   test_loss               =     2.4081[0m
[32m[2022-08-31 18:47:39,454] [    INFO][0m -   test_runtime            = 0:00:05.40[0m
[32m[2022-08-31 18:47:39,454] [    INFO][0m -   test_samples_per_second =    466.133[0m
[32m[2022-08-31 18:47:39,454] [    INFO][0m -   test_steps_per_second   =     14.613[0m
[32m[2022-08-31 18:47:39,454] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-31 18:47:39,454] [    INFO][0m -   Num examples = 3000[0m
[32m[2022-08-31 18:47:39,455] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:47:39,455] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:47:39,455] [    INFO][0m -   Total prediction steps = 94[0m
[32m[2022-08-31 18:47:48,182] [    INFO][0m - Predictions for ocnlif saved to ./fewclue_submit_examples.[0m
run.sh: line 65: --model_name_or_path: command not found
 
==========
bustm
==========
 
[33m[2022-08-31 18:47:52,577] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-31 18:47:52,577] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-31 18:47:52,578] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 18:47:52,578] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-31 18:47:52,578] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 18:47:52,578] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-31 18:47:52,578] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-31 18:47:52,578] [    INFO][0m - early_stop_patience           :20[0m
[32m[2022-08-31 18:47:52,578] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-31 18:47:52,578] [    INFO][0m - model_name_or_path            :roformer_v2_chinese_char_base[0m
[32m[2022-08-31 18:47:52,578] [    INFO][0m - [0m
[32m[2022-08-31 18:47:52,578] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 18:47:52,579] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-31 18:47:52,579] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 18:47:52,579] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-31 18:47:52,579] [    INFO][0m - prompt                        :{'text':'text_a'}{'sep'}{'text':'text_b'}{'sep'}{'hard':'å‰ä¸¤å¥è¯'}{'mask'}{'hard':'åƒ'}[0m
[32m[2022-08-31 18:47:52,579] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-31 18:47:52,579] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-31 18:47:52,579] [    INFO][0m - task_name                     :bustm[0m
[32m[2022-08-31 18:47:52,579] [    INFO][0m - [0m
[32m[2022-08-31 18:47:52,579] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/roformer_v2_chinese_char_base/model_state.pdparams[0m
W0831 18:47:52.580724 55866 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0831 18:47:52.584568 55866 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-31 18:47:55,450] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/roformer_v2_chinese_char_base/vocab.txt[0m
[32m[2022-08-31 18:47:55,458] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/roformer_v2_chinese_char_base/tokenizer_config.json[0m
[32m[2022-08-31 18:47:55,459] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/roformer_v2_chinese_char_base/special_tokens_map.json[0m
[32m[2022-08-31 18:47:55,459] [    INFO][0m - Using template: [{'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'sep': None}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'sep': None}, {'add_prefix_space': '', 'hard': 'å‰ä¸¤å¥è¯'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'åƒ'}][0m
2022-08-31 18:47:55,465 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-31 18:47:55,570] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 18:47:55,570] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-31 18:47:55,571] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 18:47:55,571] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-31 18:47:55,571] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-31 18:47:55,571] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-31 18:47:55,571] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-31 18:47:55,571] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-31 18:47:55,571] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-31 18:47:55,571] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-31 18:47:55,571] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-31 18:47:55,571] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-31 18:47:55,571] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-31 18:47:55,572] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-31 18:47:55,572] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-31 18:47:55,572] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-31 18:47:55,572] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-31 18:47:55,572] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-31 18:47:55,572] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-31 18:47:55,572] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-31 18:47:55,572] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-31 18:47:55,572] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-31 18:47:55,572] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-31 18:47:55,572] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-31 18:47:55,572] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-31 18:47:55,572] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-31 18:47:55,573] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-31 18:47:55,573] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-31 18:47:55,573] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-31 18:47:55,573] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-31 18:47:55,573] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-31 18:47:55,573] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-31 18:47:55,573] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-31 18:47:55,573] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-31 18:47:55,573] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-31 18:47:55,573] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-31 18:47:55,573] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-31 18:47:55,573] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug31_18-47-52_instance-3bwob41y-01[0m
[32m[2022-08-31 18:47:55,573] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-31 18:47:55,573] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-31 18:47:55,574] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-31 18:47:55,574] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-31 18:47:55,574] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-31 18:47:55,574] [    INFO][0m - max_seq_length                :40[0m
[32m[2022-08-31 18:47:55,574] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-31 18:47:55,574] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-31 18:47:55,574] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-31 18:47:55,574] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-31 18:47:55,574] [    INFO][0m - num_train_epochs              :100.0[0m
[32m[2022-08-31 18:47:55,574] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-31 18:47:55,574] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-31 18:47:55,574] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-31 18:47:55,574] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-31 18:47:55,575] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-31 18:47:55,575] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-31 18:47:55,575] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-31 18:47:55,575] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-31 18:47:55,575] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-31 18:47:55,575] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-31 18:47:55,575] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-31 18:47:55,575] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-31 18:47:55,575] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-31 18:47:55,575] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-31 18:47:55,575] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-31 18:47:55,575] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-31 18:47:55,575] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-31 18:47:55,576] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-31 18:47:55,576] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-31 18:47:55,576] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-31 18:47:55,576] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-31 18:47:55,576] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-31 18:47:55,576] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-31 18:47:55,576] [    INFO][0m - seed                          :42[0m
[32m[2022-08-31 18:47:55,576] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-31 18:47:55,576] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-31 18:47:55,576] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-31 18:47:55,576] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-31 18:47:55,576] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-31 18:47:55,576] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-31 18:47:55,577] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-31 18:47:55,577] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-31 18:47:55,577] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-31 18:47:55,577] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-31 18:47:55,577] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-31 18:47:55,577] [    INFO][0m - [0m
[32m[2022-08-31 18:47:55,578] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-31 18:47:55,579] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:47:55,579] [    INFO][0m -   Num Epochs = 100[0m
[32m[2022-08-31 18:47:55,579] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-31 18:47:55,579] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-31 18:47:55,579] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-31 18:47:55,579] [    INFO][0m -   Total optimization steps = 2000.0[0m
[32m[2022-08-31 18:47:55,579] [    INFO][0m -   Total num train samples = 16000[0m
[32m[2022-08-31 18:47:57,025] [    INFO][0m - loss: 0.73114076, learning_rate: 2.985e-05, global_step: 10, interval_runtime: 1.4458, interval_samples_per_second: 5.533, interval_steps_per_second: 6.917, epoch: 0.5[0m
[32m[2022-08-31 18:47:57,636] [    INFO][0m - loss: 0.52815542, learning_rate: 2.97e-05, global_step: 20, interval_runtime: 0.6102, interval_samples_per_second: 13.111, interval_steps_per_second: 16.389, epoch: 1.0[0m
[32m[2022-08-31 18:47:58,313] [    INFO][0m - loss: 0.23909347, learning_rate: 2.955e-05, global_step: 30, interval_runtime: 0.6772, interval_samples_per_second: 11.814, interval_steps_per_second: 14.767, epoch: 1.5[0m
[32m[2022-08-31 18:47:58,918] [    INFO][0m - loss: 0.33866427, learning_rate: 2.94e-05, global_step: 40, interval_runtime: 0.6048, interval_samples_per_second: 13.228, interval_steps_per_second: 16.535, epoch: 2.0[0m
[32m[2022-08-31 18:47:59,588] [    INFO][0m - loss: 0.16615089, learning_rate: 2.925e-05, global_step: 50, interval_runtime: 0.6705, interval_samples_per_second: 11.932, interval_steps_per_second: 14.915, epoch: 2.5[0m
[32m[2022-08-31 18:48:00,181] [    INFO][0m - loss: 0.21213701, learning_rate: 2.91e-05, global_step: 60, interval_runtime: 0.5924, interval_samples_per_second: 13.505, interval_steps_per_second: 16.881, epoch: 3.0[0m
[32m[2022-08-31 18:48:00,862] [    INFO][0m - loss: 0.01481189, learning_rate: 2.895e-05, global_step: 70, interval_runtime: 0.6812, interval_samples_per_second: 11.745, interval_steps_per_second: 14.681, epoch: 3.5[0m
[32m[2022-08-31 18:48:01,443] [    INFO][0m - loss: 0.03697253, learning_rate: 2.88e-05, global_step: 80, interval_runtime: 0.5812, interval_samples_per_second: 13.766, interval_steps_per_second: 17.207, epoch: 4.0[0m
[32m[2022-08-31 18:48:02,107] [    INFO][0m - loss: 0.08608651, learning_rate: 2.865e-05, global_step: 90, interval_runtime: 0.6637, interval_samples_per_second: 12.054, interval_steps_per_second: 15.068, epoch: 4.5[0m
[32m[2022-08-31 18:48:02,683] [    INFO][0m - loss: 0.00053789, learning_rate: 2.8499999999999998e-05, global_step: 100, interval_runtime: 0.5767, interval_samples_per_second: 13.872, interval_steps_per_second: 17.34, epoch: 5.0[0m
[32m[2022-08-31 18:48:02,684] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:48:02,684] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:48:02,684] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:48:02,684] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:48:02,684] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:48:02,985] [    INFO][0m - eval_loss: 1.7941604852676392, eval_accuracy: 0.80625, eval_runtime: 0.3012, eval_samples_per_second: 531.164, eval_steps_per_second: 16.599, epoch: 5.0[0m
[32m[2022-08-31 18:48:02,986] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-31 18:48:02,986] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:48:04,923] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-31 18:48:04,923] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-31 18:48:10,042] [    INFO][0m - loss: 0.00012986, learning_rate: 2.8349999999999998e-05, global_step: 110, interval_runtime: 7.3592, interval_samples_per_second: 1.087, interval_steps_per_second: 1.359, epoch: 5.5[0m
[32m[2022-08-31 18:48:10,635] [    INFO][0m - loss: 3.19e-06, learning_rate: 2.8199999999999998e-05, global_step: 120, interval_runtime: 0.5926, interval_samples_per_second: 13.5, interval_steps_per_second: 16.874, epoch: 6.0[0m
[32m[2022-08-31 18:48:11,308] [    INFO][0m - loss: 1.799e-05, learning_rate: 2.805e-05, global_step: 130, interval_runtime: 0.6724, interval_samples_per_second: 11.897, interval_steps_per_second: 14.871, epoch: 6.5[0m
[32m[2022-08-31 18:48:11,924] [    INFO][0m - loss: 0.00012534, learning_rate: 2.79e-05, global_step: 140, interval_runtime: 0.6169, interval_samples_per_second: 12.968, interval_steps_per_second: 16.21, epoch: 7.0[0m
[32m[2022-08-31 18:48:12,579] [    INFO][0m - loss: 2.661e-05, learning_rate: 2.7750000000000004e-05, global_step: 150, interval_runtime: 0.6545, interval_samples_per_second: 12.224, interval_steps_per_second: 15.28, epoch: 7.5[0m
[32m[2022-08-31 18:48:13,197] [    INFO][0m - loss: 0.0002971, learning_rate: 2.7600000000000003e-05, global_step: 160, interval_runtime: 0.6177, interval_samples_per_second: 12.951, interval_steps_per_second: 16.188, epoch: 8.0[0m
[32m[2022-08-31 18:48:13,855] [    INFO][0m - loss: 5.8e-07, learning_rate: 2.7450000000000003e-05, global_step: 170, interval_runtime: 0.6587, interval_samples_per_second: 12.145, interval_steps_per_second: 15.182, epoch: 8.5[0m
[32m[2022-08-31 18:48:14,426] [    INFO][0m - loss: 2.09e-06, learning_rate: 2.7300000000000003e-05, global_step: 180, interval_runtime: 0.5705, interval_samples_per_second: 14.023, interval_steps_per_second: 17.529, epoch: 9.0[0m
[32m[2022-08-31 18:48:15,099] [    INFO][0m - loss: 9.171e-05, learning_rate: 2.7150000000000003e-05, global_step: 190, interval_runtime: 0.6731, interval_samples_per_second: 11.886, interval_steps_per_second: 14.857, epoch: 9.5[0m
[32m[2022-08-31 18:48:15,679] [    INFO][0m - loss: 1.399e-05, learning_rate: 2.7000000000000002e-05, global_step: 200, interval_runtime: 0.5798, interval_samples_per_second: 13.798, interval_steps_per_second: 17.247, epoch: 10.0[0m
[32m[2022-08-31 18:48:15,679] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:48:15,679] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:48:15,679] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:48:15,680] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:48:15,680] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:48:15,946] [    INFO][0m - eval_loss: 2.4871342182159424, eval_accuracy: 0.7875, eval_runtime: 0.2662, eval_samples_per_second: 601.023, eval_steps_per_second: 18.782, epoch: 10.0[0m
[32m[2022-08-31 18:48:15,946] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-31 18:48:15,946] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:48:17,890] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-31 18:48:17,890] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-31 18:48:24,278] [    INFO][0m - loss: 1.221e-05, learning_rate: 2.6850000000000002e-05, global_step: 210, interval_runtime: 8.5992, interval_samples_per_second: 0.93, interval_steps_per_second: 1.163, epoch: 10.5[0m
[32m[2022-08-31 18:48:24,863] [    INFO][0m - loss: 1e-07, learning_rate: 2.6700000000000002e-05, global_step: 220, interval_runtime: 0.585, interval_samples_per_second: 13.675, interval_steps_per_second: 17.094, epoch: 11.0[0m
[32m[2022-08-31 18:48:25,537] [    INFO][0m - loss: 1.645e-05, learning_rate: 2.655e-05, global_step: 230, interval_runtime: 0.674, interval_samples_per_second: 11.869, interval_steps_per_second: 14.837, epoch: 11.5[0m
[32m[2022-08-31 18:48:26,127] [    INFO][0m - loss: 7e-08, learning_rate: 2.64e-05, global_step: 240, interval_runtime: 0.5901, interval_samples_per_second: 13.557, interval_steps_per_second: 16.947, epoch: 12.0[0m
[32m[2022-08-31 18:48:26,846] [    INFO][0m - loss: 2e-08, learning_rate: 2.625e-05, global_step: 250, interval_runtime: 0.7184, interval_samples_per_second: 11.136, interval_steps_per_second: 13.92, epoch: 12.5[0m
[32m[2022-08-31 18:48:27,456] [    INFO][0m - loss: 8e-07, learning_rate: 2.61e-05, global_step: 260, interval_runtime: 0.6108, interval_samples_per_second: 13.097, interval_steps_per_second: 16.371, epoch: 13.0[0m
[32m[2022-08-31 18:48:28,167] [    INFO][0m - loss: 3.85e-06, learning_rate: 2.595e-05, global_step: 270, interval_runtime: 0.7104, interval_samples_per_second: 11.261, interval_steps_per_second: 14.076, epoch: 13.5[0m
[32m[2022-08-31 18:48:28,767] [    INFO][0m - loss: 1.3e-07, learning_rate: 2.58e-05, global_step: 280, interval_runtime: 0.6005, interval_samples_per_second: 13.323, interval_steps_per_second: 16.654, epoch: 14.0[0m
[32m[2022-08-31 18:48:29,417] [    INFO][0m - loss: 1.7e-07, learning_rate: 2.565e-05, global_step: 290, interval_runtime: 0.6503, interval_samples_per_second: 12.301, interval_steps_per_second: 15.377, epoch: 14.5[0m
[32m[2022-08-31 18:48:30,018] [    INFO][0m - loss: 4e-08, learning_rate: 2.55e-05, global_step: 300, interval_runtime: 0.6002, interval_samples_per_second: 13.328, interval_steps_per_second: 16.66, epoch: 15.0[0m
[32m[2022-08-31 18:48:30,018] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:48:30,018] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:48:30,018] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:48:30,019] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:48:30,019] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:48:30,273] [    INFO][0m - eval_loss: 2.3867974281311035, eval_accuracy: 0.83125, eval_runtime: 0.2537, eval_samples_per_second: 630.656, eval_steps_per_second: 19.708, epoch: 15.0[0m
[32m[2022-08-31 18:48:30,273] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-31 18:48:30,273] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:48:32,275] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-31 18:48:32,275] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-31 18:48:37,515] [    INFO][0m - loss: 1.519e-05, learning_rate: 2.535e-05, global_step: 310, interval_runtime: 7.497, interval_samples_per_second: 1.067, interval_steps_per_second: 1.334, epoch: 15.5[0m
[32m[2022-08-31 18:48:38,145] [    INFO][0m - loss: 2e-08, learning_rate: 2.52e-05, global_step: 320, interval_runtime: 0.63, interval_samples_per_second: 12.699, interval_steps_per_second: 15.874, epoch: 16.0[0m
[32m[2022-08-31 18:48:38,863] [    INFO][0m - loss: 4e-08, learning_rate: 2.505e-05, global_step: 330, interval_runtime: 0.718, interval_samples_per_second: 11.141, interval_steps_per_second: 13.927, epoch: 16.5[0m
[32m[2022-08-31 18:48:39,525] [    INFO][0m - loss: 4e-08, learning_rate: 2.49e-05, global_step: 340, interval_runtime: 0.6624, interval_samples_per_second: 12.077, interval_steps_per_second: 15.096, epoch: 17.0[0m
[32m[2022-08-31 18:48:40,183] [    INFO][0m - loss: 1.6e-07, learning_rate: 2.475e-05, global_step: 350, interval_runtime: 0.6583, interval_samples_per_second: 12.152, interval_steps_per_second: 15.19, epoch: 17.5[0m
[32m[2022-08-31 18:48:40,771] [    INFO][0m - loss: 3e-08, learning_rate: 2.4599999999999998e-05, global_step: 360, interval_runtime: 0.5877, interval_samples_per_second: 13.613, interval_steps_per_second: 17.016, epoch: 18.0[0m
[32m[2022-08-31 18:48:41,435] [    INFO][0m - loss: 7.1e-07, learning_rate: 2.4449999999999998e-05, global_step: 370, interval_runtime: 0.6639, interval_samples_per_second: 12.05, interval_steps_per_second: 15.062, epoch: 18.5[0m
[32m[2022-08-31 18:48:42,029] [    INFO][0m - loss: 4e-08, learning_rate: 2.43e-05, global_step: 380, interval_runtime: 0.5938, interval_samples_per_second: 13.473, interval_steps_per_second: 16.841, epoch: 19.0[0m
[32m[2022-08-31 18:48:42,716] [    INFO][0m - loss: 1.6e-07, learning_rate: 2.415e-05, global_step: 390, interval_runtime: 0.6872, interval_samples_per_second: 11.641, interval_steps_per_second: 14.551, epoch: 19.5[0m
[32m[2022-08-31 18:48:43,292] [    INFO][0m - loss: 1.337e-05, learning_rate: 2.4e-05, global_step: 400, interval_runtime: 0.5757, interval_samples_per_second: 13.896, interval_steps_per_second: 17.37, epoch: 20.0[0m
[32m[2022-08-31 18:48:43,293] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:48:43,293] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:48:43,293] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:48:43,293] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:48:43,294] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:48:43,564] [    INFO][0m - eval_loss: 2.612842082977295, eval_accuracy: 0.7875, eval_runtime: 0.27, eval_samples_per_second: 592.66, eval_steps_per_second: 18.521, epoch: 20.0[0m
[32m[2022-08-31 18:48:43,564] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-31 18:48:43,564] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:48:45,443] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-31 18:48:45,444] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-31 18:48:51,001] [    INFO][0m - loss: 1.5e-07, learning_rate: 2.385e-05, global_step: 410, interval_runtime: 7.709, interval_samples_per_second: 1.038, interval_steps_per_second: 1.297, epoch: 20.5[0m
[32m[2022-08-31 18:48:51,617] [    INFO][0m - loss: 2e-08, learning_rate: 2.37e-05, global_step: 420, interval_runtime: 0.6158, interval_samples_per_second: 12.991, interval_steps_per_second: 16.238, epoch: 21.0[0m
[32m[2022-08-31 18:48:52,272] [    INFO][0m - loss: 2e-08, learning_rate: 2.3550000000000003e-05, global_step: 430, interval_runtime: 0.6549, interval_samples_per_second: 12.216, interval_steps_per_second: 15.27, epoch: 21.5[0m
[32m[2022-08-31 18:48:52,864] [    INFO][0m - loss: 7.3e-07, learning_rate: 2.3400000000000003e-05, global_step: 440, interval_runtime: 0.5919, interval_samples_per_second: 13.516, interval_steps_per_second: 16.895, epoch: 22.0[0m
[32m[2022-08-31 18:48:53,522] [    INFO][0m - loss: 1.1e-07, learning_rate: 2.3250000000000003e-05, global_step: 450, interval_runtime: 0.6587, interval_samples_per_second: 12.146, interval_steps_per_second: 15.182, epoch: 22.5[0m
[32m[2022-08-31 18:48:54,146] [    INFO][0m - loss: 7.7e-07, learning_rate: 2.3100000000000002e-05, global_step: 460, interval_runtime: 0.6239, interval_samples_per_second: 12.822, interval_steps_per_second: 16.027, epoch: 23.0[0m
[32m[2022-08-31 18:48:54,931] [    INFO][0m - loss: 8.74e-06, learning_rate: 2.2950000000000002e-05, global_step: 470, interval_runtime: 0.7854, interval_samples_per_second: 10.186, interval_steps_per_second: 12.733, epoch: 23.5[0m
[32m[2022-08-31 18:48:55,540] [    INFO][0m - loss: 4e-08, learning_rate: 2.2800000000000002e-05, global_step: 480, interval_runtime: 0.6084, interval_samples_per_second: 13.149, interval_steps_per_second: 16.436, epoch: 24.0[0m
[32m[2022-08-31 18:48:56,196] [    INFO][0m - loss: 3e-08, learning_rate: 2.265e-05, global_step: 490, interval_runtime: 0.6557, interval_samples_per_second: 12.201, interval_steps_per_second: 15.251, epoch: 24.5[0m
[32m[2022-08-31 18:48:56,792] [    INFO][0m - loss: 0.0, learning_rate: 2.25e-05, global_step: 500, interval_runtime: 0.5964, interval_samples_per_second: 13.413, interval_steps_per_second: 16.767, epoch: 25.0[0m
[32m[2022-08-31 18:48:56,793] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:48:56,793] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:48:56,793] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:48:56,793] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:48:56,793] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:48:57,049] [    INFO][0m - eval_loss: 2.572798252105713, eval_accuracy: 0.8, eval_runtime: 0.2555, eval_samples_per_second: 626.126, eval_steps_per_second: 19.566, epoch: 25.0[0m
[32m[2022-08-31 18:48:57,049] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-08-31 18:48:57,049] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:48:59,912] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-08-31 18:48:59,913] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-08-31 18:49:05,788] [    INFO][0m - loss: 1e-08, learning_rate: 2.235e-05, global_step: 510, interval_runtime: 8.996, interval_samples_per_second: 0.889, interval_steps_per_second: 1.112, epoch: 25.5[0m
[32m[2022-08-31 18:49:06,402] [    INFO][0m - loss: 3e-08, learning_rate: 2.22e-05, global_step: 520, interval_runtime: 0.6139, interval_samples_per_second: 13.031, interval_steps_per_second: 16.288, epoch: 26.0[0m
[32m[2022-08-31 18:49:07,044] [    INFO][0m - loss: 3e-08, learning_rate: 2.205e-05, global_step: 530, interval_runtime: 0.6416, interval_samples_per_second: 12.469, interval_steps_per_second: 15.587, epoch: 26.5[0m
[32m[2022-08-31 18:49:07,674] [    INFO][0m - loss: 1e-08, learning_rate: 2.19e-05, global_step: 540, interval_runtime: 0.6298, interval_samples_per_second: 12.702, interval_steps_per_second: 15.877, epoch: 27.0[0m
[32m[2022-08-31 18:49:08,358] [    INFO][0m - loss: 1.5e-07, learning_rate: 2.175e-05, global_step: 550, interval_runtime: 0.684, interval_samples_per_second: 11.696, interval_steps_per_second: 14.62, epoch: 27.5[0m
[32m[2022-08-31 18:49:08,944] [    INFO][0m - loss: 1e-08, learning_rate: 2.16e-05, global_step: 560, interval_runtime: 0.5862, interval_samples_per_second: 13.648, interval_steps_per_second: 17.059, epoch: 28.0[0m
[32m[2022-08-31 18:49:09,619] [    INFO][0m - loss: 4e-08, learning_rate: 2.145e-05, global_step: 570, interval_runtime: 0.6758, interval_samples_per_second: 11.838, interval_steps_per_second: 14.798, epoch: 28.5[0m
[32m[2022-08-31 18:49:10,285] [    INFO][0m - loss: 2e-08, learning_rate: 2.13e-05, global_step: 580, interval_runtime: 0.6652, interval_samples_per_second: 12.026, interval_steps_per_second: 15.032, epoch: 29.0[0m
[32m[2022-08-31 18:49:11,029] [    INFO][0m - loss: 2e-08, learning_rate: 2.115e-05, global_step: 590, interval_runtime: 0.7442, interval_samples_per_second: 10.75, interval_steps_per_second: 13.438, epoch: 29.5[0m
[32m[2022-08-31 18:49:11,634] [    INFO][0m - loss: 1e-08, learning_rate: 2.1e-05, global_step: 600, interval_runtime: 0.6047, interval_samples_per_second: 13.23, interval_steps_per_second: 16.538, epoch: 30.0[0m
[32m[2022-08-31 18:49:11,634] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:49:11,634] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:49:11,634] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:49:11,634] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:49:11,634] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:49:11,907] [    INFO][0m - eval_loss: 2.5746548175811768, eval_accuracy: 0.8, eval_runtime: 0.2723, eval_samples_per_second: 587.551, eval_steps_per_second: 18.361, epoch: 30.0[0m
[32m[2022-08-31 18:49:11,907] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-08-31 18:49:11,907] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:49:13,931] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-08-31 18:49:13,931] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-08-31 18:49:19,597] [    INFO][0m - loss: 0.0001298, learning_rate: 2.085e-05, global_step: 610, interval_runtime: 7.9631, interval_samples_per_second: 1.005, interval_steps_per_second: 1.256, epoch: 30.5[0m
[32m[2022-08-31 18:49:20,208] [    INFO][0m - loss: 0.00058928, learning_rate: 2.07e-05, global_step: 620, interval_runtime: 0.6109, interval_samples_per_second: 13.095, interval_steps_per_second: 16.369, epoch: 31.0[0m
[32m[2022-08-31 18:49:20,864] [    INFO][0m - loss: 1e-08, learning_rate: 2.055e-05, global_step: 630, interval_runtime: 0.6562, interval_samples_per_second: 12.191, interval_steps_per_second: 15.239, epoch: 31.5[0m
[32m[2022-08-31 18:49:21,477] [    INFO][0m - loss: 1e-08, learning_rate: 2.04e-05, global_step: 640, interval_runtime: 0.6132, interval_samples_per_second: 13.047, interval_steps_per_second: 16.309, epoch: 32.0[0m
[32m[2022-08-31 18:49:22,153] [    INFO][0m - loss: 2.93e-06, learning_rate: 2.025e-05, global_step: 650, interval_runtime: 0.6762, interval_samples_per_second: 11.831, interval_steps_per_second: 14.789, epoch: 32.5[0m
[32m[2022-08-31 18:49:22,800] [    INFO][0m - loss: 1e-08, learning_rate: 2.01e-05, global_step: 660, interval_runtime: 0.6462, interval_samples_per_second: 12.38, interval_steps_per_second: 15.475, epoch: 33.0[0m
[32m[2022-08-31 18:49:23,453] [    INFO][0m - loss: 1.86e-06, learning_rate: 1.995e-05, global_step: 670, interval_runtime: 0.6539, interval_samples_per_second: 12.234, interval_steps_per_second: 15.293, epoch: 33.5[0m
[32m[2022-08-31 18:49:24,078] [    INFO][0m - loss: 8.047e-05, learning_rate: 1.98e-05, global_step: 680, interval_runtime: 0.6248, interval_samples_per_second: 12.804, interval_steps_per_second: 16.005, epoch: 34.0[0m
[32m[2022-08-31 18:49:24,758] [    INFO][0m - loss: 1e-08, learning_rate: 1.965e-05, global_step: 690, interval_runtime: 0.6789, interval_samples_per_second: 11.784, interval_steps_per_second: 14.73, epoch: 34.5[0m
[32m[2022-08-31 18:49:25,361] [    INFO][0m - loss: 0.22282338, learning_rate: 1.95e-05, global_step: 700, interval_runtime: 0.6045, interval_samples_per_second: 13.235, interval_steps_per_second: 16.544, epoch: 35.0[0m
[32m[2022-08-31 18:49:25,362] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:49:25,362] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:49:25,362] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:49:25,362] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:49:25,362] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:49:25,637] [    INFO][0m - eval_loss: 2.6312880516052246, eval_accuracy: 0.7875, eval_runtime: 0.2742, eval_samples_per_second: 583.569, eval_steps_per_second: 18.237, epoch: 35.0[0m
[32m[2022-08-31 18:49:25,637] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-700[0m
[32m[2022-08-31 18:49:25,637] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:49:28,063] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-700/tokenizer_config.json[0m
[32m[2022-08-31 18:49:28,063] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-700/special_tokens_map.json[0m
[32m[2022-08-31 18:49:33,867] [    INFO][0m - loss: 0.04743277, learning_rate: 1.935e-05, global_step: 710, interval_runtime: 8.5055, interval_samples_per_second: 0.941, interval_steps_per_second: 1.176, epoch: 35.5[0m
[32m[2022-08-31 18:49:34,484] [    INFO][0m - loss: 1.35e-06, learning_rate: 1.9200000000000003e-05, global_step: 720, interval_runtime: 0.6175, interval_samples_per_second: 12.956, interval_steps_per_second: 16.195, epoch: 36.0[0m
[32m[2022-08-31 18:49:35,139] [    INFO][0m - loss: 5.958e-05, learning_rate: 1.9050000000000002e-05, global_step: 730, interval_runtime: 0.6543, interval_samples_per_second: 12.227, interval_steps_per_second: 15.284, epoch: 36.5[0m
[32m[2022-08-31 18:49:35,742] [    INFO][0m - loss: 9.42e-06, learning_rate: 1.8900000000000002e-05, global_step: 740, interval_runtime: 0.6038, interval_samples_per_second: 13.249, interval_steps_per_second: 16.562, epoch: 37.0[0m
[32m[2022-08-31 18:49:36,413] [    INFO][0m - loss: 2.4e-07, learning_rate: 1.8750000000000002e-05, global_step: 750, interval_runtime: 0.6705, interval_samples_per_second: 11.931, interval_steps_per_second: 14.913, epoch: 37.5[0m
[32m[2022-08-31 18:49:37,019] [    INFO][0m - loss: 7.7e-07, learning_rate: 1.86e-05, global_step: 760, interval_runtime: 0.6061, interval_samples_per_second: 13.199, interval_steps_per_second: 16.499, epoch: 38.0[0m
[32m[2022-08-31 18:49:37,667] [    INFO][0m - loss: 7e-08, learning_rate: 1.845e-05, global_step: 770, interval_runtime: 0.6473, interval_samples_per_second: 12.358, interval_steps_per_second: 15.448, epoch: 38.5[0m
[32m[2022-08-31 18:49:38,253] [    INFO][0m - loss: 5.4e-07, learning_rate: 1.83e-05, global_step: 780, interval_runtime: 0.5862, interval_samples_per_second: 13.646, interval_steps_per_second: 17.058, epoch: 39.0[0m
[32m[2022-08-31 18:49:38,913] [    INFO][0m - loss: 2.03e-06, learning_rate: 1.815e-05, global_step: 790, interval_runtime: 0.6603, interval_samples_per_second: 12.115, interval_steps_per_second: 15.144, epoch: 39.5[0m
[32m[2022-08-31 18:49:39,528] [    INFO][0m - loss: 2.63e-06, learning_rate: 1.8e-05, global_step: 800, interval_runtime: 0.6151, interval_samples_per_second: 13.005, interval_steps_per_second: 16.257, epoch: 40.0[0m
[32m[2022-08-31 18:49:39,529] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:49:39,529] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:49:39,529] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:49:39,529] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:49:39,529] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:49:39,791] [    INFO][0m - eval_loss: 2.1356139183044434, eval_accuracy: 0.80625, eval_runtime: 0.262, eval_samples_per_second: 610.647, eval_steps_per_second: 19.083, epoch: 40.0[0m
[32m[2022-08-31 18:49:39,792] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-800[0m
[32m[2022-08-31 18:49:39,792] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:49:41,789] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-800/tokenizer_config.json[0m
[32m[2022-08-31 18:49:41,790] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-800/special_tokens_map.json[0m
[32m[2022-08-31 18:49:47,372] [    INFO][0m - loss: 1.22e-06, learning_rate: 1.785e-05, global_step: 810, interval_runtime: 7.8439, interval_samples_per_second: 1.02, interval_steps_per_second: 1.275, epoch: 40.5[0m
[32m[2022-08-31 18:49:47,995] [    INFO][0m - loss: 1.3e-07, learning_rate: 1.77e-05, global_step: 820, interval_runtime: 0.6221, interval_samples_per_second: 12.86, interval_steps_per_second: 16.075, epoch: 41.0[0m
[32m[2022-08-31 18:49:48,685] [    INFO][0m - loss: 5.32e-06, learning_rate: 1.755e-05, global_step: 830, interval_runtime: 0.6901, interval_samples_per_second: 11.593, interval_steps_per_second: 14.491, epoch: 41.5[0m
[32m[2022-08-31 18:49:49,311] [    INFO][0m - loss: 2.4e-07, learning_rate: 1.74e-05, global_step: 840, interval_runtime: 0.6264, interval_samples_per_second: 12.771, interval_steps_per_second: 15.963, epoch: 42.0[0m
[32m[2022-08-31 18:49:50,012] [    INFO][0m - loss: 1.7e-07, learning_rate: 1.725e-05, global_step: 850, interval_runtime: 0.7008, interval_samples_per_second: 11.415, interval_steps_per_second: 14.269, epoch: 42.5[0m
[32m[2022-08-31 18:49:50,618] [    INFO][0m - loss: 2.8e-07, learning_rate: 1.71e-05, global_step: 860, interval_runtime: 0.6058, interval_samples_per_second: 13.205, interval_steps_per_second: 16.507, epoch: 43.0[0m
[32m[2022-08-31 18:49:51,359] [    INFO][0m - loss: 2.7e-07, learning_rate: 1.695e-05, global_step: 870, interval_runtime: 0.741, interval_samples_per_second: 10.796, interval_steps_per_second: 13.495, epoch: 43.5[0m
[32m[2022-08-31 18:49:51,971] [    INFO][0m - loss: 1.3e-07, learning_rate: 1.6800000000000002e-05, global_step: 880, interval_runtime: 0.6123, interval_samples_per_second: 13.066, interval_steps_per_second: 16.333, epoch: 44.0[0m
[32m[2022-08-31 18:49:52,699] [    INFO][0m - loss: 1.4e-07, learning_rate: 1.665e-05, global_step: 890, interval_runtime: 0.7283, interval_samples_per_second: 10.984, interval_steps_per_second: 13.73, epoch: 44.5[0m
[32m[2022-08-31 18:49:53,357] [    INFO][0m - loss: 6e-08, learning_rate: 1.65e-05, global_step: 900, interval_runtime: 0.6578, interval_samples_per_second: 12.162, interval_steps_per_second: 15.202, epoch: 45.0[0m
[32m[2022-08-31 18:49:53,358] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:49:53,358] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:49:53,358] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:49:53,358] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:49:53,358] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:49:53,618] [    INFO][0m - eval_loss: 2.142658233642578, eval_accuracy: 0.80625, eval_runtime: 0.2593, eval_samples_per_second: 616.968, eval_steps_per_second: 19.28, epoch: 45.0[0m
[32m[2022-08-31 18:49:53,619] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-900[0m
[32m[2022-08-31 18:49:53,619] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:49:56,104] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-900/tokenizer_config.json[0m
[32m[2022-08-31 18:49:56,104] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-900/special_tokens_map.json[0m
[32m[2022-08-31 18:50:01,612] [    INFO][0m - loss: 1.6e-07, learning_rate: 1.635e-05, global_step: 910, interval_runtime: 8.254, interval_samples_per_second: 0.969, interval_steps_per_second: 1.212, epoch: 45.5[0m
[32m[2022-08-31 18:50:02,327] [    INFO][0m - loss: 2.1e-07, learning_rate: 1.62e-05, global_step: 920, interval_runtime: 0.7157, interval_samples_per_second: 11.178, interval_steps_per_second: 13.972, epoch: 46.0[0m
[32m[2022-08-31 18:50:03,065] [    INFO][0m - loss: 3.8e-07, learning_rate: 1.605e-05, global_step: 930, interval_runtime: 0.7383, interval_samples_per_second: 10.836, interval_steps_per_second: 13.545, epoch: 46.5[0m
[32m[2022-08-31 18:50:03,744] [    INFO][0m - loss: 8e-08, learning_rate: 1.59e-05, global_step: 940, interval_runtime: 0.6787, interval_samples_per_second: 11.787, interval_steps_per_second: 14.734, epoch: 47.0[0m
[32m[2022-08-31 18:50:04,454] [    INFO][0m - loss: 8e-08, learning_rate: 1.575e-05, global_step: 950, interval_runtime: 0.7105, interval_samples_per_second: 11.26, interval_steps_per_second: 14.075, epoch: 47.5[0m
[32m[2022-08-31 18:50:05,086] [    INFO][0m - loss: 2.8e-07, learning_rate: 1.56e-05, global_step: 960, interval_runtime: 0.6308, interval_samples_per_second: 12.682, interval_steps_per_second: 15.852, epoch: 48.0[0m
[32m[2022-08-31 18:50:05,778] [    INFO][0m - loss: 2.3e-07, learning_rate: 1.545e-05, global_step: 970, interval_runtime: 0.693, interval_samples_per_second: 11.544, interval_steps_per_second: 14.43, epoch: 48.5[0m
[32m[2022-08-31 18:50:06,418] [    INFO][0m - loss: 3.3e-07, learning_rate: 1.53e-05, global_step: 980, interval_runtime: 0.6401, interval_samples_per_second: 12.498, interval_steps_per_second: 15.622, epoch: 49.0[0m
[32m[2022-08-31 18:50:07,114] [    INFO][0m - loss: 1.4e-07, learning_rate: 1.515e-05, global_step: 990, interval_runtime: 0.6956, interval_samples_per_second: 11.5, interval_steps_per_second: 14.375, epoch: 49.5[0m
[32m[2022-08-31 18:50:07,737] [    INFO][0m - loss: 8.1e-07, learning_rate: 1.5e-05, global_step: 1000, interval_runtime: 0.6227, interval_samples_per_second: 12.847, interval_steps_per_second: 16.059, epoch: 50.0[0m
[32m[2022-08-31 18:50:07,737] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:50:07,738] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:50:07,738] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:50:07,738] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:50:07,738] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:50:08,022] [    INFO][0m - eval_loss: 2.153477191925049, eval_accuracy: 0.80625, eval_runtime: 0.2839, eval_samples_per_second: 563.514, eval_steps_per_second: 17.61, epoch: 50.0[0m
[32m[2022-08-31 18:50:08,022] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1000[0m
[32m[2022-08-31 18:50:08,022] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:50:10,001] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1000/tokenizer_config.json[0m
[32m[2022-08-31 18:50:10,001] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1000/special_tokens_map.json[0m
[32m[2022-08-31 18:50:15,093] [    INFO][0m - loss: 1.5e-07, learning_rate: 1.485e-05, global_step: 1010, interval_runtime: 7.3557, interval_samples_per_second: 1.088, interval_steps_per_second: 1.359, epoch: 50.5[0m
[32m[2022-08-31 18:50:15,732] [    INFO][0m - loss: 5e-08, learning_rate: 1.47e-05, global_step: 1020, interval_runtime: 0.6397, interval_samples_per_second: 12.507, interval_steps_per_second: 15.633, epoch: 51.0[0m
[32m[2022-08-31 18:50:16,429] [    INFO][0m - loss: 9.94e-06, learning_rate: 1.455e-05, global_step: 1030, interval_runtime: 0.6971, interval_samples_per_second: 11.476, interval_steps_per_second: 14.345, epoch: 51.5[0m
[32m[2022-08-31 18:50:17,045] [    INFO][0m - loss: 2.2e-07, learning_rate: 1.44e-05, global_step: 1040, interval_runtime: 0.6157, interval_samples_per_second: 12.994, interval_steps_per_second: 16.242, epoch: 52.0[0m
[32m[2022-08-31 18:50:17,754] [    INFO][0m - loss: 3.5e-07, learning_rate: 1.4249999999999999e-05, global_step: 1050, interval_runtime: 0.7094, interval_samples_per_second: 11.277, interval_steps_per_second: 14.097, epoch: 52.5[0m
[32m[2022-08-31 18:50:18,393] [    INFO][0m - loss: 1.6e-07, learning_rate: 1.4099999999999999e-05, global_step: 1060, interval_runtime: 0.6382, interval_samples_per_second: 12.535, interval_steps_per_second: 15.669, epoch: 53.0[0m
[32m[2022-08-31 18:50:19,048] [    INFO][0m - loss: 1.6e-07, learning_rate: 1.395e-05, global_step: 1070, interval_runtime: 0.656, interval_samples_per_second: 12.194, interval_steps_per_second: 15.243, epoch: 53.5[0m
[32m[2022-08-31 18:50:19,690] [    INFO][0m - loss: 3.8e-07, learning_rate: 1.3800000000000002e-05, global_step: 1080, interval_runtime: 0.6416, interval_samples_per_second: 12.469, interval_steps_per_second: 15.587, epoch: 54.0[0m
[32m[2022-08-31 18:50:20,413] [    INFO][0m - loss: 6.8e-07, learning_rate: 1.3650000000000001e-05, global_step: 1090, interval_runtime: 0.7224, interval_samples_per_second: 11.074, interval_steps_per_second: 13.842, epoch: 54.5[0m
[32m[2022-08-31 18:50:21,077] [    INFO][0m - loss: 3.9e-07, learning_rate: 1.3500000000000001e-05, global_step: 1100, interval_runtime: 0.6647, interval_samples_per_second: 12.035, interval_steps_per_second: 15.044, epoch: 55.0[0m
[32m[2022-08-31 18:50:21,078] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:50:21,078] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:50:21,078] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:50:21,078] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:50:21,079] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:50:21,355] [    INFO][0m - eval_loss: 2.314267158508301, eval_accuracy: 0.79375, eval_runtime: 0.2768, eval_samples_per_second: 577.958, eval_steps_per_second: 18.061, epoch: 55.0[0m
[32m[2022-08-31 18:50:21,356] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1100[0m
[32m[2022-08-31 18:50:21,356] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:50:23,395] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1100/tokenizer_config.json[0m
[32m[2022-08-31 18:50:23,396] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1100/special_tokens_map.json[0m
[32m[2022-08-31 18:50:28,518] [    INFO][0m - loss: 1.8e-07, learning_rate: 1.3350000000000001e-05, global_step: 1110, interval_runtime: 7.4411, interval_samples_per_second: 1.075, interval_steps_per_second: 1.344, epoch: 55.5[0m
[32m[2022-08-31 18:50:29,128] [    INFO][0m - loss: 2.3e-07, learning_rate: 1.32e-05, global_step: 1120, interval_runtime: 0.6098, interval_samples_per_second: 13.119, interval_steps_per_second: 16.399, epoch: 56.0[0m
[32m[2022-08-31 18:50:29,916] [    INFO][0m - loss: 2.29e-06, learning_rate: 1.305e-05, global_step: 1130, interval_runtime: 0.7879, interval_samples_per_second: 10.154, interval_steps_per_second: 12.693, epoch: 56.5[0m
[32m[2022-08-31 18:50:30,564] [    INFO][0m - loss: 1.21e-06, learning_rate: 1.29e-05, global_step: 1140, interval_runtime: 0.6477, interval_samples_per_second: 12.352, interval_steps_per_second: 15.44, epoch: 57.0[0m
[32m[2022-08-31 18:50:31,396] [    INFO][0m - loss: 2.23e-06, learning_rate: 1.275e-05, global_step: 1150, interval_runtime: 0.8322, interval_samples_per_second: 9.613, interval_steps_per_second: 12.016, epoch: 57.5[0m
[32m[2022-08-31 18:50:32,061] [    INFO][0m - loss: 3.6e-07, learning_rate: 1.26e-05, global_step: 1160, interval_runtime: 0.6647, interval_samples_per_second: 12.035, interval_steps_per_second: 15.043, epoch: 58.0[0m
[32m[2022-08-31 18:50:32,941] [    INFO][0m - loss: 6.176e-05, learning_rate: 1.245e-05, global_step: 1170, interval_runtime: 0.8787, interval_samples_per_second: 9.104, interval_steps_per_second: 11.38, epoch: 58.5[0m
[32m[2022-08-31 18:50:33,634] [    INFO][0m - loss: 1.1e-07, learning_rate: 1.2299999999999999e-05, global_step: 1180, interval_runtime: 0.6947, interval_samples_per_second: 11.516, interval_steps_per_second: 14.395, epoch: 59.0[0m
[32m[2022-08-31 18:50:34,327] [    INFO][0m - loss: 2.685e-05, learning_rate: 1.215e-05, global_step: 1190, interval_runtime: 0.6928, interval_samples_per_second: 11.548, interval_steps_per_second: 14.435, epoch: 59.5[0m
[32m[2022-08-31 18:50:35,002] [    INFO][0m - loss: 1.03e-06, learning_rate: 1.2e-05, global_step: 1200, interval_runtime: 0.6747, interval_samples_per_second: 11.857, interval_steps_per_second: 14.822, epoch: 60.0[0m
[32m[2022-08-31 18:50:35,003] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:50:35,003] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:50:35,003] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:50:35,003] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:50:35,003] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:50:35,285] [    INFO][0m - eval_loss: 2.31300687789917, eval_accuracy: 0.79375, eval_runtime: 0.2815, eval_samples_per_second: 568.382, eval_steps_per_second: 17.762, epoch: 60.0[0m
[32m[2022-08-31 18:50:35,285] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1200[0m
[32m[2022-08-31 18:50:35,286] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:50:37,281] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1200/tokenizer_config.json[0m
[32m[2022-08-31 18:50:37,282] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1200/special_tokens_map.json[0m
[32m[2022-08-31 18:50:42,909] [    INFO][0m - loss: 3.99e-06, learning_rate: 1.185e-05, global_step: 1210, interval_runtime: 7.9062, interval_samples_per_second: 1.012, interval_steps_per_second: 1.265, epoch: 60.5[0m
[32m[2022-08-31 18:50:43,460] [    INFO][0m - loss: 1e-08, learning_rate: 1.1700000000000001e-05, global_step: 1220, interval_runtime: 0.5522, interval_samples_per_second: 14.487, interval_steps_per_second: 18.109, epoch: 61.0[0m
[32m[2022-08-31 18:50:44,294] [    INFO][0m - loss: 9.7e-07, learning_rate: 1.1550000000000001e-05, global_step: 1230, interval_runtime: 0.8334, interval_samples_per_second: 9.599, interval_steps_per_second: 11.998, epoch: 61.5[0m
[32m[2022-08-31 18:50:44,913] [    INFO][0m - loss: 4e-08, learning_rate: 1.1400000000000001e-05, global_step: 1240, interval_runtime: 0.62, interval_samples_per_second: 12.904, interval_steps_per_second: 16.13, epoch: 62.0[0m
[32m[2022-08-31 18:50:45,603] [    INFO][0m - loss: 1e-08, learning_rate: 1.125e-05, global_step: 1250, interval_runtime: 0.6888, interval_samples_per_second: 11.614, interval_steps_per_second: 14.518, epoch: 62.5[0m
[32m[2022-08-31 18:50:46,208] [    INFO][0m - loss: 3e-08, learning_rate: 1.11e-05, global_step: 1260, interval_runtime: 0.6059, interval_samples_per_second: 13.204, interval_steps_per_second: 16.505, epoch: 63.0[0m
[32m[2022-08-31 18:50:46,889] [    INFO][0m - loss: 1e-08, learning_rate: 1.095e-05, global_step: 1270, interval_runtime: 0.6804, interval_samples_per_second: 11.758, interval_steps_per_second: 14.697, epoch: 63.5[0m
[32m[2022-08-31 18:50:47,534] [    INFO][0m - loss: 1e-08, learning_rate: 1.08e-05, global_step: 1280, interval_runtime: 0.6446, interval_samples_per_second: 12.411, interval_steps_per_second: 15.513, epoch: 64.0[0m
[32m[2022-08-31 18:50:48,264] [    INFO][0m - loss: 1e-08, learning_rate: 1.065e-05, global_step: 1290, interval_runtime: 0.7311, interval_samples_per_second: 10.942, interval_steps_per_second: 13.677, epoch: 64.5[0m
[32m[2022-08-31 18:50:48,899] [    INFO][0m - loss: 2e-08, learning_rate: 1.05e-05, global_step: 1300, interval_runtime: 0.6351, interval_samples_per_second: 12.597, interval_steps_per_second: 15.747, epoch: 65.0[0m
[32m[2022-08-31 18:50:48,900] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:50:48,900] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:50:48,900] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:50:48,901] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:50:48,901] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:50:49,184] [    INFO][0m - eval_loss: 2.3121821880340576, eval_accuracy: 0.8125, eval_runtime: 0.2834, eval_samples_per_second: 564.59, eval_steps_per_second: 17.643, epoch: 65.0[0m
[32m[2022-08-31 18:50:49,184] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1300[0m
[32m[2022-08-31 18:50:49,185] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:50:51,399] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1300/tokenizer_config.json[0m
[32m[2022-08-31 18:50:51,400] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1300/special_tokens_map.json[0m
[32m[2022-08-31 18:50:57,252] [    INFO][0m - loss: 3e-08, learning_rate: 1.035e-05, global_step: 1310, interval_runtime: 8.3524, interval_samples_per_second: 0.958, interval_steps_per_second: 1.197, epoch: 65.5[0m
[32m[2022-08-31 18:50:57,854] [    INFO][0m - loss: 2e-08, learning_rate: 1.02e-05, global_step: 1320, interval_runtime: 0.6018, interval_samples_per_second: 13.294, interval_steps_per_second: 16.617, epoch: 66.0[0m
[32m[2022-08-31 18:50:58,518] [    INFO][0m - loss: 2e-08, learning_rate: 1.005e-05, global_step: 1330, interval_runtime: 0.664, interval_samples_per_second: 12.049, interval_steps_per_second: 15.061, epoch: 66.5[0m
[32m[2022-08-31 18:50:59,142] [    INFO][0m - loss: 1e-08, learning_rate: 9.9e-06, global_step: 1340, interval_runtime: 0.6239, interval_samples_per_second: 12.822, interval_steps_per_second: 16.027, epoch: 67.0[0m
[32m[2022-08-31 18:50:59,820] [    INFO][0m - loss: 4e-08, learning_rate: 9.75e-06, global_step: 1350, interval_runtime: 0.6784, interval_samples_per_second: 11.793, interval_steps_per_second: 14.741, epoch: 67.5[0m
[32m[2022-08-31 18:51:00,433] [    INFO][0m - loss: 8.1e-07, learning_rate: 9.600000000000001e-06, global_step: 1360, interval_runtime: 0.6127, interval_samples_per_second: 13.056, interval_steps_per_second: 16.321, epoch: 68.0[0m
[32m[2022-08-31 18:51:01,114] [    INFO][0m - loss: 9e-08, learning_rate: 9.450000000000001e-06, global_step: 1370, interval_runtime: 0.681, interval_samples_per_second: 11.747, interval_steps_per_second: 14.684, epoch: 68.5[0m
[32m[2022-08-31 18:51:01,722] [    INFO][0m - loss: 2e-08, learning_rate: 9.3e-06, global_step: 1380, interval_runtime: 0.6078, interval_samples_per_second: 13.162, interval_steps_per_second: 16.452, epoch: 69.0[0m
[32m[2022-08-31 18:51:02,375] [    INFO][0m - loss: 2.7e-07, learning_rate: 9.15e-06, global_step: 1390, interval_runtime: 0.6538, interval_samples_per_second: 12.236, interval_steps_per_second: 15.295, epoch: 69.5[0m
[32m[2022-08-31 18:51:02,992] [    INFO][0m - loss: 1e-08, learning_rate: 9e-06, global_step: 1400, interval_runtime: 0.6167, interval_samples_per_second: 12.972, interval_steps_per_second: 16.215, epoch: 70.0[0m
[32m[2022-08-31 18:51:02,993] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:51:02,993] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:51:02,993] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:51:02,993] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:51:02,993] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:51:03,252] [    INFO][0m - eval_loss: 2.3112292289733887, eval_accuracy: 0.80625, eval_runtime: 0.2588, eval_samples_per_second: 618.28, eval_steps_per_second: 19.321, epoch: 70.0[0m
[32m[2022-08-31 18:51:03,252] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1400[0m
[32m[2022-08-31 18:51:03,252] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:51:05,127] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1400/tokenizer_config.json[0m
[32m[2022-08-31 18:51:05,127] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1400/special_tokens_map.json[0m
[32m[2022-08-31 18:51:10,026] [    INFO][0m - loss: 9.7e-07, learning_rate: 8.85e-06, global_step: 1410, interval_runtime: 7.0335, interval_samples_per_second: 1.137, interval_steps_per_second: 1.422, epoch: 70.5[0m
[32m[2022-08-31 18:51:10,680] [    INFO][0m - loss: 0.0, learning_rate: 8.7e-06, global_step: 1420, interval_runtime: 0.6548, interval_samples_per_second: 12.217, interval_steps_per_second: 15.271, epoch: 71.0[0m
[32m[2022-08-31 18:51:11,430] [    INFO][0m - loss: 6e-08, learning_rate: 8.55e-06, global_step: 1430, interval_runtime: 0.7495, interval_samples_per_second: 10.674, interval_steps_per_second: 13.342, epoch: 71.5[0m
[32m[2022-08-31 18:51:12,101] [    INFO][0m - loss: 5e-08, learning_rate: 8.400000000000001e-06, global_step: 1440, interval_runtime: 0.6712, interval_samples_per_second: 11.919, interval_steps_per_second: 14.899, epoch: 72.0[0m
[32m[2022-08-31 18:51:12,828] [    INFO][0m - loss: 1.9e-07, learning_rate: 8.25e-06, global_step: 1450, interval_runtime: 0.7272, interval_samples_per_second: 11.001, interval_steps_per_second: 13.751, epoch: 72.5[0m
[32m[2022-08-31 18:51:13,414] [    INFO][0m - loss: 3e-08, learning_rate: 8.1e-06, global_step: 1460, interval_runtime: 0.5854, interval_samples_per_second: 13.665, interval_steps_per_second: 17.082, epoch: 73.0[0m
[32m[2022-08-31 18:51:14,086] [    INFO][0m - loss: 4.46e-06, learning_rate: 7.95e-06, global_step: 1470, interval_runtime: 0.6722, interval_samples_per_second: 11.901, interval_steps_per_second: 14.877, epoch: 73.5[0m
[32m[2022-08-31 18:51:14,677] [    INFO][0m - loss: 0.0, learning_rate: 7.8e-06, global_step: 1480, interval_runtime: 0.5912, interval_samples_per_second: 13.531, interval_steps_per_second: 16.914, epoch: 74.0[0m
[32m[2022-08-31 18:51:15,330] [    INFO][0m - loss: 1e-08, learning_rate: 7.65e-06, global_step: 1490, interval_runtime: 0.6526, interval_samples_per_second: 12.26, interval_steps_per_second: 15.324, epoch: 74.5[0m
[32m[2022-08-31 18:51:15,919] [    INFO][0m - loss: 0.0, learning_rate: 7.5e-06, global_step: 1500, interval_runtime: 0.5888, interval_samples_per_second: 13.587, interval_steps_per_second: 16.984, epoch: 75.0[0m
[32m[2022-08-31 18:51:15,919] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:51:15,919] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:51:15,919] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:51:15,919] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:51:15,919] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:51:16,171] [    INFO][0m - eval_loss: 2.3165478706359863, eval_accuracy: 0.80625, eval_runtime: 0.251, eval_samples_per_second: 637.351, eval_steps_per_second: 19.917, epoch: 75.0[0m
[32m[2022-08-31 18:51:16,171] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1500[0m
[32m[2022-08-31 18:51:16,171] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:51:19,749] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1500/tokenizer_config.json[0m
[32m[2022-08-31 18:51:19,749] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1500/special_tokens_map.json[0m
[32m[2022-08-31 18:51:24,994] [    INFO][0m - loss: 2.08e-06, learning_rate: 7.35e-06, global_step: 1510, interval_runtime: 9.0752, interval_samples_per_second: 0.882, interval_steps_per_second: 1.102, epoch: 75.5[0m
[32m[2022-08-31 18:51:25,597] [    INFO][0m - loss: 2e-08, learning_rate: 7.2e-06, global_step: 1520, interval_runtime: 0.6031, interval_samples_per_second: 13.266, interval_steps_per_second: 16.582, epoch: 76.0[0m
[32m[2022-08-31 18:51:26,263] [    INFO][0m - loss: 1e-08, learning_rate: 7.049999999999999e-06, global_step: 1530, interval_runtime: 0.6658, interval_samples_per_second: 12.015, interval_steps_per_second: 15.019, epoch: 76.5[0m
[32m[2022-08-31 18:51:26,869] [    INFO][0m - loss: 1e-08, learning_rate: 6.900000000000001e-06, global_step: 1540, interval_runtime: 0.6066, interval_samples_per_second: 13.188, interval_steps_per_second: 16.485, epoch: 77.0[0m
[32m[2022-08-31 18:51:27,544] [    INFO][0m - loss: 1e-08, learning_rate: 6.750000000000001e-06, global_step: 1550, interval_runtime: 0.6741, interval_samples_per_second: 11.867, interval_steps_per_second: 14.834, epoch: 77.5[0m
[32m[2022-08-31 18:51:28,192] [    INFO][0m - loss: 1e-08, learning_rate: 6.6e-06, global_step: 1560, interval_runtime: 0.6486, interval_samples_per_second: 12.335, interval_steps_per_second: 15.418, epoch: 78.0[0m
[32m[2022-08-31 18:51:28,843] [    INFO][0m - loss: 1e-08, learning_rate: 6.45e-06, global_step: 1570, interval_runtime: 0.6508, interval_samples_per_second: 12.293, interval_steps_per_second: 15.366, epoch: 78.5[0m
[32m[2022-08-31 18:51:29,451] [    INFO][0m - loss: 1e-08, learning_rate: 6.3e-06, global_step: 1580, interval_runtime: 0.6082, interval_samples_per_second: 13.152, interval_steps_per_second: 16.441, epoch: 79.0[0m
[32m[2022-08-31 18:51:30,099] [    INFO][0m - loss: 1e-08, learning_rate: 6.1499999999999996e-06, global_step: 1590, interval_runtime: 0.6479, interval_samples_per_second: 12.347, interval_steps_per_second: 15.434, epoch: 79.5[0m
[32m[2022-08-31 18:51:30,691] [    INFO][0m - loss: 1e-08, learning_rate: 6e-06, global_step: 1600, interval_runtime: 0.5924, interval_samples_per_second: 13.505, interval_steps_per_second: 16.881, epoch: 80.0[0m
[32m[2022-08-31 18:51:30,692] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:51:30,692] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:51:30,692] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:51:30,692] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:51:30,692] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:51:30,946] [    INFO][0m - eval_loss: 2.3328468799591064, eval_accuracy: 0.8125, eval_runtime: 0.2536, eval_samples_per_second: 630.929, eval_steps_per_second: 19.717, epoch: 80.0[0m
[32m[2022-08-31 18:51:30,946] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1600[0m
[32m[2022-08-31 18:51:30,947] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:51:34,214] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1600/tokenizer_config.json[0m
[32m[2022-08-31 18:51:34,214] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1600/special_tokens_map.json[0m
[32m[2022-08-31 18:51:40,964] [    INFO][0m - loss: 3e-07, learning_rate: 5.850000000000001e-06, global_step: 1610, interval_runtime: 10.2721, interval_samples_per_second: 0.779, interval_steps_per_second: 0.974, epoch: 80.5[0m
[32m[2022-08-31 18:51:41,552] [    INFO][0m - loss: 0.0, learning_rate: 5.7000000000000005e-06, global_step: 1620, interval_runtime: 0.5889, interval_samples_per_second: 13.586, interval_steps_per_second: 16.982, epoch: 81.0[0m
[32m[2022-08-31 18:51:42,231] [    INFO][0m - loss: 0.0, learning_rate: 5.55e-06, global_step: 1630, interval_runtime: 0.6786, interval_samples_per_second: 11.789, interval_steps_per_second: 14.736, epoch: 81.5[0m
[32m[2022-08-31 18:51:42,820] [    INFO][0m - loss: 3e-08, learning_rate: 5.4e-06, global_step: 1640, interval_runtime: 0.5887, interval_samples_per_second: 13.59, interval_steps_per_second: 16.988, epoch: 82.0[0m
[32m[2022-08-31 18:51:43,473] [    INFO][0m - loss: 1e-08, learning_rate: 5.25e-06, global_step: 1650, interval_runtime: 0.6537, interval_samples_per_second: 12.238, interval_steps_per_second: 15.298, epoch: 82.5[0m
[32m[2022-08-31 18:51:44,090] [    INFO][0m - loss: 1e-08, learning_rate: 5.1e-06, global_step: 1660, interval_runtime: 0.6164, interval_samples_per_second: 12.979, interval_steps_per_second: 16.224, epoch: 83.0[0m
[32m[2022-08-31 18:51:44,777] [    INFO][0m - loss: 1e-08, learning_rate: 4.95e-06, global_step: 1670, interval_runtime: 0.6868, interval_samples_per_second: 11.648, interval_steps_per_second: 14.56, epoch: 83.5[0m
[32m[2022-08-31 18:51:45,362] [    INFO][0m - loss: 1e-08, learning_rate: 4.800000000000001e-06, global_step: 1680, interval_runtime: 0.5856, interval_samples_per_second: 13.662, interval_steps_per_second: 17.078, epoch: 84.0[0m
[32m[2022-08-31 18:51:46,079] [    INFO][0m - loss: 1e-08, learning_rate: 4.65e-06, global_step: 1690, interval_runtime: 0.7168, interval_samples_per_second: 11.161, interval_steps_per_second: 13.952, epoch: 84.5[0m
[32m[2022-08-31 18:51:46,662] [    INFO][0m - loss: 1e-08, learning_rate: 4.5e-06, global_step: 1700, interval_runtime: 0.5829, interval_samples_per_second: 13.724, interval_steps_per_second: 17.155, epoch: 85.0[0m
[32m[2022-08-31 18:51:46,662] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:51:46,662] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:51:46,663] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:51:46,663] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:51:46,663] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:51:46,930] [    INFO][0m - eval_loss: 2.3351292610168457, eval_accuracy: 0.8125, eval_runtime: 0.2672, eval_samples_per_second: 598.743, eval_steps_per_second: 18.711, epoch: 85.0[0m
[32m[2022-08-31 18:51:46,930] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1700[0m
[32m[2022-08-31 18:51:46,931] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:51:48,815] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1700/tokenizer_config.json[0m
[32m[2022-08-31 18:51:48,815] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1700/special_tokens_map.json[0m
[32m[2022-08-31 18:51:56,123] [    INFO][0m - loss: 1e-08, learning_rate: 4.35e-06, global_step: 1710, interval_runtime: 9.4606, interval_samples_per_second: 0.846, interval_steps_per_second: 1.057, epoch: 85.5[0m
[32m[2022-08-31 18:51:56,700] [    INFO][0m - loss: 0.0, learning_rate: 4.2000000000000004e-06, global_step: 1720, interval_runtime: 0.5771, interval_samples_per_second: 13.863, interval_steps_per_second: 17.328, epoch: 86.0[0m
[32m[2022-08-31 18:51:57,341] [    INFO][0m - loss: 1.1e-07, learning_rate: 4.05e-06, global_step: 1730, interval_runtime: 0.6411, interval_samples_per_second: 12.478, interval_steps_per_second: 15.598, epoch: 86.5[0m
[32m[2022-08-31 18:51:57,913] [    INFO][0m - loss: 7e-08, learning_rate: 3.9e-06, global_step: 1740, interval_runtime: 0.5725, interval_samples_per_second: 13.973, interval_steps_per_second: 17.466, epoch: 87.0[0m
[32m[2022-08-31 18:51:58,557] [    INFO][0m - loss: 4e-08, learning_rate: 3.75e-06, global_step: 1750, interval_runtime: 0.6434, interval_samples_per_second: 12.434, interval_steps_per_second: 15.543, epoch: 87.5[0m
[32m[2022-08-31 18:51:59,137] [    INFO][0m - loss: 0.0, learning_rate: 3.6e-06, global_step: 1760, interval_runtime: 0.5802, interval_samples_per_second: 13.789, interval_steps_per_second: 17.237, epoch: 88.0[0m
[32m[2022-08-31 18:51:59,775] [    INFO][0m - loss: 1.1e-07, learning_rate: 3.4500000000000004e-06, global_step: 1770, interval_runtime: 0.6385, interval_samples_per_second: 12.528, interval_steps_per_second: 15.66, epoch: 88.5[0m
[32m[2022-08-31 18:52:00,355] [    INFO][0m - loss: 0.0, learning_rate: 3.3e-06, global_step: 1780, interval_runtime: 0.5793, interval_samples_per_second: 13.809, interval_steps_per_second: 17.261, epoch: 89.0[0m
[32m[2022-08-31 18:52:01,000] [    INFO][0m - loss: 1e-08, learning_rate: 3.15e-06, global_step: 1790, interval_runtime: 0.6456, interval_samples_per_second: 12.392, interval_steps_per_second: 15.49, epoch: 89.5[0m
[32m[2022-08-31 18:52:01,562] [    INFO][0m - loss: 1e-08, learning_rate: 3e-06, global_step: 1800, interval_runtime: 0.5614, interval_samples_per_second: 14.25, interval_steps_per_second: 17.812, epoch: 90.0[0m
[32m[2022-08-31 18:52:01,563] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:52:01,563] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:52:01,564] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:52:01,564] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:52:01,564] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:52:01,818] [    INFO][0m - eval_loss: 2.335235357284546, eval_accuracy: 0.8125, eval_runtime: 0.2542, eval_samples_per_second: 629.478, eval_steps_per_second: 19.671, epoch: 90.0[0m
[32m[2022-08-31 18:52:01,818] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1800[0m
[32m[2022-08-31 18:52:01,818] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:52:03,714] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1800/tokenizer_config.json[0m
[32m[2022-08-31 18:52:03,714] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1800/special_tokens_map.json[0m
[32m[2022-08-31 18:52:08,914] [    INFO][0m - loss: 8e-08, learning_rate: 2.8500000000000002e-06, global_step: 1810, interval_runtime: 7.3527, interval_samples_per_second: 1.088, interval_steps_per_second: 1.36, epoch: 90.5[0m
[32m[2022-08-31 18:52:09,496] [    INFO][0m - loss: 3e-08, learning_rate: 2.7e-06, global_step: 1820, interval_runtime: 0.5818, interval_samples_per_second: 13.75, interval_steps_per_second: 17.188, epoch: 91.0[0m
[32m[2022-08-31 18:52:10,141] [    INFO][0m - loss: 3e-08, learning_rate: 2.55e-06, global_step: 1830, interval_runtime: 0.645, interval_samples_per_second: 12.404, interval_steps_per_second: 15.505, epoch: 91.5[0m
[32m[2022-08-31 18:52:10,724] [    INFO][0m - loss: 1e-08, learning_rate: 2.4000000000000003e-06, global_step: 1840, interval_runtime: 0.583, interval_samples_per_second: 13.722, interval_steps_per_second: 17.152, epoch: 92.0[0m
[32m[2022-08-31 18:52:11,375] [    INFO][0m - loss: 0.0, learning_rate: 2.25e-06, global_step: 1850, interval_runtime: 0.6507, interval_samples_per_second: 12.295, interval_steps_per_second: 15.369, epoch: 92.5[0m
[32m[2022-08-31 18:52:11,979] [    INFO][0m - loss: 3e-08, learning_rate: 2.1000000000000002e-06, global_step: 1860, interval_runtime: 0.604, interval_samples_per_second: 13.244, interval_steps_per_second: 16.555, epoch: 93.0[0m
[32m[2022-08-31 18:52:12,631] [    INFO][0m - loss: 1e-08, learning_rate: 1.95e-06, global_step: 1870, interval_runtime: 0.6521, interval_samples_per_second: 12.268, interval_steps_per_second: 15.335, epoch: 93.5[0m
[32m[2022-08-31 18:52:13,198] [    INFO][0m - loss: 0.0, learning_rate: 1.8e-06, global_step: 1880, interval_runtime: 0.5671, interval_samples_per_second: 14.108, interval_steps_per_second: 17.635, epoch: 94.0[0m
[32m[2022-08-31 18:52:13,849] [    INFO][0m - loss: 0.0, learning_rate: 1.65e-06, global_step: 1890, interval_runtime: 0.6512, interval_samples_per_second: 12.284, interval_steps_per_second: 15.355, epoch: 94.5[0m
[32m[2022-08-31 18:52:14,429] [    INFO][0m - loss: 0.0, learning_rate: 1.5e-06, global_step: 1900, interval_runtime: 0.5794, interval_samples_per_second: 13.808, interval_steps_per_second: 17.26, epoch: 95.0[0m
[32m[2022-08-31 18:52:14,429] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:52:14,429] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:52:14,429] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:52:14,429] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:52:14,430] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:52:14,697] [    INFO][0m - eval_loss: 2.33534574508667, eval_accuracy: 0.8125, eval_runtime: 0.267, eval_samples_per_second: 599.336, eval_steps_per_second: 18.729, epoch: 95.0[0m
[32m[2022-08-31 18:52:14,697] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1900[0m
[32m[2022-08-31 18:52:14,697] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:52:16,544] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1900/tokenizer_config.json[0m
[32m[2022-08-31 18:52:16,544] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1900/special_tokens_map.json[0m
[32m[2022-08-31 18:52:23,339] [    INFO][0m - loss: 1e-08, learning_rate: 1.35e-06, global_step: 1910, interval_runtime: 8.9102, interval_samples_per_second: 0.898, interval_steps_per_second: 1.122, epoch: 95.5[0m
[32m[2022-08-31 18:52:23,934] [    INFO][0m - loss: 5.9e-07, learning_rate: 1.2000000000000002e-06, global_step: 1920, interval_runtime: 0.5951, interval_samples_per_second: 13.443, interval_steps_per_second: 16.804, epoch: 96.0[0m
[32m[2022-08-31 18:52:24,593] [    INFO][0m - loss: 1e-08, learning_rate: 1.0500000000000001e-06, global_step: 1930, interval_runtime: 0.6583, interval_samples_per_second: 12.153, interval_steps_per_second: 15.191, epoch: 96.5[0m
[32m[2022-08-31 18:52:25,169] [    INFO][0m - loss: 1e-08, learning_rate: 9e-07, global_step: 1940, interval_runtime: 0.5768, interval_samples_per_second: 13.868, interval_steps_per_second: 17.336, epoch: 97.0[0m
[32m[2022-08-31 18:52:25,814] [    INFO][0m - loss: 2e-08, learning_rate: 7.5e-07, global_step: 1950, interval_runtime: 0.6444, interval_samples_per_second: 12.414, interval_steps_per_second: 15.518, epoch: 97.5[0m
[32m[2022-08-31 18:52:26,397] [    INFO][0m - loss: 1.2e-07, learning_rate: 6.000000000000001e-07, global_step: 1960, interval_runtime: 0.5836, interval_samples_per_second: 13.707, interval_steps_per_second: 17.134, epoch: 98.0[0m
[32m[2022-08-31 18:52:27,042] [    INFO][0m - loss: 1e-08, learning_rate: 4.5e-07, global_step: 1970, interval_runtime: 0.6448, interval_samples_per_second: 12.406, interval_steps_per_second: 15.508, epoch: 98.5[0m
[32m[2022-08-31 18:52:27,632] [    INFO][0m - loss: 3e-08, learning_rate: 3.0000000000000004e-07, global_step: 1980, interval_runtime: 0.5894, interval_samples_per_second: 13.572, interval_steps_per_second: 16.965, epoch: 99.0[0m
[32m[2022-08-31 18:52:28,283] [    INFO][0m - loss: 2e-08, learning_rate: 1.5000000000000002e-07, global_step: 1990, interval_runtime: 0.6513, interval_samples_per_second: 12.283, interval_steps_per_second: 15.354, epoch: 99.5[0m
[32m[2022-08-31 18:52:28,901] [    INFO][0m - loss: 2e-08, learning_rate: 0.0, global_step: 2000, interval_runtime: 0.6177, interval_samples_per_second: 12.951, interval_steps_per_second: 16.189, epoch: 100.0[0m
[32m[2022-08-31 18:52:28,901] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:52:28,901] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:52:28,901] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:52:28,901] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:52:28,901] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:52:29,158] [    INFO][0m - eval_loss: 2.3354334831237793, eval_accuracy: 0.8125, eval_runtime: 0.2564, eval_samples_per_second: 624.126, eval_steps_per_second: 19.504, epoch: 100.0[0m
[32m[2022-08-31 18:52:29,158] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2000[0m
[32m[2022-08-31 18:52:29,158] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:52:31,038] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2000/tokenizer_config.json[0m
[32m[2022-08-31 18:52:31,038] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2000/special_tokens_map.json[0m
[32m[2022-08-31 18:52:37,584] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-31 18:52:37,584] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-300 (score: 0.83125).[0m
[32m[2022-08-31 18:52:38,221] [    INFO][0m - train_runtime: 282.6412, train_samples_per_second: 56.609, train_steps_per_second: 7.076, train_loss: 0.013129031688255945, epoch: 100.0[0m
[32m[2022-08-31 18:52:38,222] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-31 18:52:38,222] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:52:43,092] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-31 18:52:43,092] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-31 18:52:43,097] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-31 18:52:43,097] [    INFO][0m -   epoch                    =      100.0[0m
[32m[2022-08-31 18:52:43,097] [    INFO][0m -   train_loss               =     0.0131[0m
[32m[2022-08-31 18:52:43,097] [    INFO][0m -   train_runtime            = 0:04:42.64[0m
[32m[2022-08-31 18:52:43,098] [    INFO][0m -   train_samples_per_second =     56.609[0m
[32m[2022-08-31 18:52:43,098] [    INFO][0m -   train_steps_per_second   =      7.076[0m
[32m[2022-08-31 18:52:43,109] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-31 18:52:43,110] [    INFO][0m -   Num examples = 1772[0m
[32m[2022-08-31 18:52:43,110] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:52:43,110] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:52:43,110] [    INFO][0m -   Total prediction steps = 56[0m
[32m[2022-08-31 18:52:46,230] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-31 18:52:46,231] [    INFO][0m -   test_accuracy           =     0.7884[0m
[32m[2022-08-31 18:52:46,231] [    INFO][0m -   test_loss               =     2.2634[0m
[32m[2022-08-31 18:52:46,231] [    INFO][0m -   test_runtime            = 0:00:03.12[0m
[32m[2022-08-31 18:52:46,231] [    INFO][0m -   test_samples_per_second =    567.814[0m
[32m[2022-08-31 18:52:46,231] [    INFO][0m -   test_steps_per_second   =     17.944[0m
[32m[2022-08-31 18:52:46,231] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-31 18:52:46,232] [    INFO][0m -   Num examples = 2000[0m
[32m[2022-08-31 18:52:46,232] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:52:46,232] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:52:46,232] [    INFO][0m -   Total prediction steps = 63[0m
[32m[2022-08-31 18:52:50,992] [    INFO][0m - Predictions for bustm saved to ./fewclue_submit_examples.[0m
run.sh: line 65: --model_name_or_path: command not found
 
==========
chid
==========
 
[33m[2022-08-31 18:52:55,511] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-31 18:52:55,511] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-31 18:52:55,511] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 18:52:55,511] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-31 18:52:55,512] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 18:52:55,512] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-31 18:52:55,512] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-31 18:52:55,512] [    INFO][0m - early_stop_patience           :20[0m
[32m[2022-08-31 18:52:55,512] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-31 18:52:55,512] [    INFO][0m - model_name_or_path            :roformer_v2_chinese_char_base[0m
[32m[2022-08-31 18:52:55,512] [    INFO][0m - [0m
[32m[2022-08-31 18:52:55,512] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 18:52:55,512] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-31 18:52:55,512] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 18:52:55,512] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-31 18:52:55,512] [    INFO][0m - prompt                        :{'text':'text_a'}{'sep'}{'hard':'è¿™å¥è¯'}{'mask'}{'hard':'é€šé¡ºã€‚'}[0m
[32m[2022-08-31 18:52:55,512] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-31 18:52:55,513] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-31 18:52:55,513] [    INFO][0m - task_name                     :chid[0m
[32m[2022-08-31 18:52:55,513] [    INFO][0m - [0m
[32m[2022-08-31 18:52:55,513] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/roformer_v2_chinese_char_base/model_state.pdparams[0m
W0831 18:52:55.514499  7521 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0831 18:52:55.519158  7521 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-31 18:52:58,623] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/roformer_v2_chinese_char_base/vocab.txt[0m
[32m[2022-08-31 18:52:58,631] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/roformer_v2_chinese_char_base/tokenizer_config.json[0m
[32m[2022-08-31 18:52:58,631] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/roformer_v2_chinese_char_base/special_tokens_map.json[0m
[32m[2022-08-31 18:52:58,632] [    INFO][0m - Using template: [{'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'sep': None}, {'add_prefix_space': '', 'hard': 'è¿™å¥è¯'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'é€šé¡ºã€‚'}][0m
2022-08-31 18:52:58,643 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-31 18:52:58,970] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 18:52:58,970] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-31 18:52:58,970] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 18:52:58,971] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-31 18:52:58,971] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-31 18:52:58,971] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-31 18:52:58,971] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-31 18:52:58,971] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-31 18:52:58,971] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-31 18:52:58,971] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-31 18:52:58,971] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-31 18:52:58,971] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-31 18:52:58,971] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-31 18:52:58,971] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-31 18:52:58,971] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-31 18:52:58,972] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-31 18:52:58,972] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-31 18:52:58,972] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-31 18:52:58,972] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-31 18:52:58,972] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-31 18:52:58,972] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-31 18:52:58,972] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-31 18:52:58,972] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-31 18:52:58,972] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-31 18:52:58,972] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-31 18:52:58,972] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-31 18:52:58,972] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-31 18:52:58,972] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-31 18:52:58,973] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-31 18:52:58,973] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-31 18:52:58,973] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-31 18:52:58,973] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-31 18:52:58,973] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-31 18:52:58,973] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-31 18:52:58,973] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-31 18:52:58,973] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-31 18:52:58,973] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-31 18:52:58,973] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug31_18-52-55_instance-3bwob41y-01[0m
[32m[2022-08-31 18:52:58,973] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-31 18:52:58,973] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-31 18:52:58,973] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-31 18:52:58,974] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-31 18:52:58,974] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-31 18:52:58,974] [    INFO][0m - max_seq_length                :256[0m
[32m[2022-08-31 18:52:58,974] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-31 18:52:58,974] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-31 18:52:58,974] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-31 18:52:58,974] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-31 18:52:58,974] [    INFO][0m - num_train_epochs              :100.0[0m
[32m[2022-08-31 18:52:58,974] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-31 18:52:58,974] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-31 18:52:58,974] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-31 18:52:58,974] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-31 18:52:58,974] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-31 18:52:58,975] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-31 18:52:58,975] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-31 18:52:58,975] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-31 18:52:58,975] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-31 18:52:58,975] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-31 18:52:58,975] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-31 18:52:58,975] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-31 18:52:58,975] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-31 18:52:58,975] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-31 18:52:58,975] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-31 18:52:58,975] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-31 18:52:58,975] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-31 18:52:58,975] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-31 18:52:58,975] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-31 18:52:58,976] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-31 18:52:58,976] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-31 18:52:58,976] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-31 18:52:58,976] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-31 18:52:58,976] [    INFO][0m - seed                          :42[0m
[32m[2022-08-31 18:52:58,976] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-31 18:52:58,976] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-31 18:52:58,976] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-31 18:52:58,976] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-31 18:52:58,976] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-31 18:52:58,976] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-31 18:52:58,976] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-31 18:52:58,976] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-31 18:52:58,976] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-31 18:52:58,977] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-31 18:52:58,977] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-31 18:52:58,977] [    INFO][0m - [0m
[32m[2022-08-31 18:52:58,978] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-31 18:52:58,978] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-08-31 18:52:58,979] [    INFO][0m -   Num Epochs = 100[0m
[32m[2022-08-31 18:52:58,979] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-31 18:52:58,979] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-31 18:52:58,979] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-31 18:52:58,979] [    INFO][0m -   Total optimization steps = 17700.0[0m
[32m[2022-08-31 18:52:58,979] [    INFO][0m -   Total num train samples = 141400[0m
[32m[2022-08-31 18:53:00,954] [    INFO][0m - loss: 0.84228354, learning_rate: 2.9983050847457627e-05, global_step: 10, interval_runtime: 1.9733, interval_samples_per_second: 4.054, interval_steps_per_second: 5.068, epoch: 0.0565[0m
[32m[2022-08-31 18:53:02,229] [    INFO][0m - loss: 0.39148569, learning_rate: 2.9966101694915256e-05, global_step: 20, interval_runtime: 1.2758, interval_samples_per_second: 6.271, interval_steps_per_second: 7.838, epoch: 0.113[0m
[32m[2022-08-31 18:53:03,497] [    INFO][0m - loss: 0.59226255, learning_rate: 2.9949152542372882e-05, global_step: 30, interval_runtime: 1.2682, interval_samples_per_second: 6.308, interval_steps_per_second: 7.885, epoch: 0.1695[0m
[32m[2022-08-31 18:53:04,771] [    INFO][0m - loss: 0.46545577, learning_rate: 2.9932203389830508e-05, global_step: 40, interval_runtime: 1.2737, interval_samples_per_second: 6.281, interval_steps_per_second: 7.851, epoch: 0.226[0m
[32m[2022-08-31 18:53:06,053] [    INFO][0m - loss: 0.35654504, learning_rate: 2.9915254237288134e-05, global_step: 50, interval_runtime: 1.2816, interval_samples_per_second: 6.242, interval_steps_per_second: 7.803, epoch: 0.2825[0m
[32m[2022-08-31 18:53:07,351] [    INFO][0m - loss: 0.26691196, learning_rate: 2.9898305084745767e-05, global_step: 60, interval_runtime: 1.2986, interval_samples_per_second: 6.161, interval_steps_per_second: 7.701, epoch: 0.339[0m
[32m[2022-08-31 18:53:08,626] [    INFO][0m - loss: 0.89680672, learning_rate: 2.9881355932203393e-05, global_step: 70, interval_runtime: 1.2745, interval_samples_per_second: 6.277, interval_steps_per_second: 7.846, epoch: 0.3955[0m
[32m[2022-08-31 18:53:09,919] [    INFO][0m - loss: 0.57905602, learning_rate: 2.986440677966102e-05, global_step: 80, interval_runtime: 1.2933, interval_samples_per_second: 6.186, interval_steps_per_second: 7.732, epoch: 0.452[0m
[32m[2022-08-31 18:53:11,196] [    INFO][0m - loss: 0.46980891, learning_rate: 2.9847457627118645e-05, global_step: 90, interval_runtime: 1.2771, interval_samples_per_second: 6.264, interval_steps_per_second: 7.83, epoch: 0.5085[0m
[32m[2022-08-31 18:53:12,471] [    INFO][0m - loss: 0.41962714, learning_rate: 2.9830508474576274e-05, global_step: 100, interval_runtime: 1.2749, interval_samples_per_second: 6.275, interval_steps_per_second: 7.844, epoch: 0.565[0m
[32m[2022-08-31 18:53:12,472] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:53:12,472] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-08-31 18:53:12,472] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:53:12,472] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:53:12,472] [    INFO][0m -   Total prediction steps = 45[0m
[32m[2022-08-31 18:53:21,019] [    INFO][0m - eval_loss: 0.4150516986846924, eval_accuracy: 0.28217821782178215, eval_runtime: 8.5466, eval_samples_per_second: 165.445, eval_steps_per_second: 5.265, epoch: 0.565[0m
[32m[2022-08-31 18:53:21,020] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-31 18:53:21,020] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:53:25,874] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-31 18:53:25,874] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-31 18:53:31,865] [    INFO][0m - loss: 0.42969389, learning_rate: 2.98135593220339e-05, global_step: 110, interval_runtime: 19.3943, interval_samples_per_second: 0.412, interval_steps_per_second: 0.516, epoch: 0.6215[0m
[32m[2022-08-31 18:53:33,139] [    INFO][0m - loss: 0.36119037, learning_rate: 2.9796610169491526e-05, global_step: 120, interval_runtime: 1.2732, interval_samples_per_second: 6.284, interval_steps_per_second: 7.854, epoch: 0.678[0m
[32m[2022-08-31 18:53:39,107] [    INFO][0m - loss: 0.45607109, learning_rate: 2.9779661016949152e-05, global_step: 130, interval_runtime: 5.9681, interval_samples_per_second: 1.34, interval_steps_per_second: 1.676, epoch: 0.7345[0m
[32m[2022-08-31 18:53:40,383] [    INFO][0m - loss: 0.49577098, learning_rate: 2.976271186440678e-05, global_step: 140, interval_runtime: 1.277, interval_samples_per_second: 6.265, interval_steps_per_second: 7.831, epoch: 0.791[0m
[32m[2022-08-31 18:53:41,651] [    INFO][0m - loss: 0.52192898, learning_rate: 2.9745762711864407e-05, global_step: 150, interval_runtime: 1.2678, interval_samples_per_second: 6.31, interval_steps_per_second: 7.888, epoch: 0.8475[0m
[32m[2022-08-31 18:53:42,922] [    INFO][0m - loss: 0.46501174, learning_rate: 2.9728813559322033e-05, global_step: 160, interval_runtime: 1.2706, interval_samples_per_second: 6.296, interval_steps_per_second: 7.87, epoch: 0.904[0m
[32m[2022-08-31 18:53:44,189] [    INFO][0m - loss: 0.40611348, learning_rate: 2.9711864406779662e-05, global_step: 170, interval_runtime: 1.267, interval_samples_per_second: 6.314, interval_steps_per_second: 7.893, epoch: 0.9605[0m
[32m[2022-08-31 18:53:45,497] [    INFO][0m - loss: 0.41428432, learning_rate: 2.9694915254237292e-05, global_step: 180, interval_runtime: 1.3061, interval_samples_per_second: 6.125, interval_steps_per_second: 7.657, epoch: 1.0169[0m
[32m[2022-08-31 18:53:46,771] [    INFO][0m - loss: 0.50358148, learning_rate: 2.9677966101694918e-05, global_step: 190, interval_runtime: 1.2766, interval_samples_per_second: 6.267, interval_steps_per_second: 7.834, epoch: 1.0734[0m
[32m[2022-08-31 18:53:48,081] [    INFO][0m - loss: 0.44019818, learning_rate: 2.9661016949152544e-05, global_step: 200, interval_runtime: 1.3098, interval_samples_per_second: 6.108, interval_steps_per_second: 7.635, epoch: 1.1299[0m
[32m[2022-08-31 18:53:48,082] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:53:48,082] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-08-31 18:53:48,082] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:53:48,082] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:53:48,082] [    INFO][0m -   Total prediction steps = 45[0m
[32m[2022-08-31 18:53:56,575] [    INFO][0m - eval_loss: 0.4101632535457611, eval_accuracy: 0.15841584158415842, eval_runtime: 8.4924, eval_samples_per_second: 166.501, eval_steps_per_second: 5.299, epoch: 1.1299[0m
[32m[2022-08-31 18:53:56,576] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-31 18:53:56,576] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:53:58,511] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-31 18:53:58,511] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-31 18:54:04,297] [    INFO][0m - loss: 0.38552067, learning_rate: 2.964406779661017e-05, global_step: 210, interval_runtime: 16.2156, interval_samples_per_second: 0.493, interval_steps_per_second: 0.617, epoch: 1.1864[0m
[32m[2022-08-31 18:54:05,564] [    INFO][0m - loss: 0.51324544, learning_rate: 2.96271186440678e-05, global_step: 220, interval_runtime: 1.2672, interval_samples_per_second: 6.313, interval_steps_per_second: 7.891, epoch: 1.2429[0m
[32m[2022-08-31 18:54:06,838] [    INFO][0m - loss: 0.46238632, learning_rate: 2.9610169491525425e-05, global_step: 230, interval_runtime: 1.2731, interval_samples_per_second: 6.284, interval_steps_per_second: 7.855, epoch: 1.2994[0m
[32m[2022-08-31 18:54:08,109] [    INFO][0m - loss: 0.4592133, learning_rate: 2.959322033898305e-05, global_step: 240, interval_runtime: 1.2713, interval_samples_per_second: 6.293, interval_steps_per_second: 7.866, epoch: 1.3559[0m
[32m[2022-08-31 18:54:09,386] [    INFO][0m - loss: 0.29505391, learning_rate: 2.9576271186440677e-05, global_step: 250, interval_runtime: 1.2773, interval_samples_per_second: 6.263, interval_steps_per_second: 7.829, epoch: 1.4124[0m
[32m[2022-08-31 18:54:10,655] [    INFO][0m - loss: 0.52447972, learning_rate: 2.9559322033898306e-05, global_step: 260, interval_runtime: 1.2686, interval_samples_per_second: 6.306, interval_steps_per_second: 7.883, epoch: 1.4689[0m
[32m[2022-08-31 18:54:11,930] [    INFO][0m - loss: 0.37582026, learning_rate: 2.9542372881355932e-05, global_step: 270, interval_runtime: 1.2756, interval_samples_per_second: 6.271, interval_steps_per_second: 7.839, epoch: 1.5254[0m
[32m[2022-08-31 18:54:13,226] [    INFO][0m - loss: 0.48937993, learning_rate: 2.9525423728813558e-05, global_step: 280, interval_runtime: 1.2963, interval_samples_per_second: 6.172, interval_steps_per_second: 7.715, epoch: 1.5819[0m
[32m[2022-08-31 18:54:14,503] [    INFO][0m - loss: 0.30693631, learning_rate: 2.9508474576271187e-05, global_step: 290, interval_runtime: 1.2758, interval_samples_per_second: 6.271, interval_steps_per_second: 7.838, epoch: 1.6384[0m
[32m[2022-08-31 18:54:15,770] [    INFO][0m - loss: 0.54220009, learning_rate: 2.9491525423728817e-05, global_step: 300, interval_runtime: 1.2673, interval_samples_per_second: 6.312, interval_steps_per_second: 7.891, epoch: 1.6949[0m
[32m[2022-08-31 18:54:15,774] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:54:15,774] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-08-31 18:54:15,774] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:54:15,775] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:54:15,775] [    INFO][0m -   Total prediction steps = 45[0m
[32m[2022-08-31 18:54:24,257] [    INFO][0m - eval_loss: 0.4113437235355377, eval_accuracy: 0.2623762376237624, eval_runtime: 8.4826, eval_samples_per_second: 166.694, eval_steps_per_second: 5.305, epoch: 1.6949[0m
[32m[2022-08-31 18:54:24,258] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-31 18:54:24,258] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:54:26,089] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-31 18:54:26,089] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-31 18:54:31,943] [    INFO][0m - loss: 0.37121377, learning_rate: 2.9474576271186443e-05, global_step: 310, interval_runtime: 16.1731, interval_samples_per_second: 0.495, interval_steps_per_second: 0.618, epoch: 1.7514[0m
[32m[2022-08-31 18:54:33,212] [    INFO][0m - loss: 0.38303576, learning_rate: 2.945762711864407e-05, global_step: 320, interval_runtime: 1.2694, interval_samples_per_second: 6.302, interval_steps_per_second: 7.877, epoch: 1.8079[0m
[32m[2022-08-31 18:54:34,487] [    INFO][0m - loss: 0.55302382, learning_rate: 2.9440677966101695e-05, global_step: 330, interval_runtime: 1.2747, interval_samples_per_second: 6.276, interval_steps_per_second: 7.845, epoch: 1.8644[0m
[32m[2022-08-31 18:54:35,752] [    INFO][0m - loss: 0.3439796, learning_rate: 2.9423728813559324e-05, global_step: 340, interval_runtime: 1.2652, interval_samples_per_second: 6.323, interval_steps_per_second: 7.904, epoch: 1.9209[0m
[32m[2022-08-31 18:54:37,021] [    INFO][0m - loss: 0.32916017, learning_rate: 2.940677966101695e-05, global_step: 350, interval_runtime: 1.2687, interval_samples_per_second: 6.306, interval_steps_per_second: 7.882, epoch: 1.9774[0m
[32m[2022-08-31 18:54:38,309] [    INFO][0m - loss: 0.4256042, learning_rate: 2.9389830508474576e-05, global_step: 360, interval_runtime: 1.2875, interval_samples_per_second: 6.214, interval_steps_per_second: 7.767, epoch: 2.0339[0m
[32m[2022-08-31 18:54:39,583] [    INFO][0m - loss: 0.35024164, learning_rate: 2.9372881355932202e-05, global_step: 370, interval_runtime: 1.2746, interval_samples_per_second: 6.276, interval_steps_per_second: 7.846, epoch: 2.0904[0m
[32m[2022-08-31 18:54:40,857] [    INFO][0m - loss: 0.52934113, learning_rate: 2.935593220338983e-05, global_step: 380, interval_runtime: 1.2736, interval_samples_per_second: 6.281, interval_steps_per_second: 7.852, epoch: 2.1469[0m
[32m[2022-08-31 18:54:42,144] [    INFO][0m - loss: 0.50209928, learning_rate: 2.9338983050847457e-05, global_step: 390, interval_runtime: 1.2871, interval_samples_per_second: 6.215, interval_steps_per_second: 7.769, epoch: 2.2034[0m
[32m[2022-08-31 18:54:43,417] [    INFO][0m - loss: 0.44873796, learning_rate: 2.9322033898305087e-05, global_step: 400, interval_runtime: 1.273, interval_samples_per_second: 6.284, interval_steps_per_second: 7.855, epoch: 2.2599[0m
[32m[2022-08-31 18:54:43,418] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:54:43,418] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-08-31 18:54:43,418] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:54:43,418] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:54:43,418] [    INFO][0m -   Total prediction steps = 45[0m
[32m[2022-08-31 18:54:52,103] [    INFO][0m - eval_loss: 0.45976144075393677, eval_accuracy: 0.2079207920792079, eval_runtime: 8.6845, eval_samples_per_second: 162.819, eval_steps_per_second: 5.182, epoch: 2.2599[0m
[32m[2022-08-31 18:54:52,104] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-31 18:54:52,104] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:54:54,462] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-31 18:54:54,462] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-31 18:55:00,342] [    INFO][0m - loss: 0.55260487, learning_rate: 2.9305084745762713e-05, global_step: 410, interval_runtime: 16.9252, interval_samples_per_second: 0.473, interval_steps_per_second: 0.591, epoch: 2.3164[0m
[32m[2022-08-31 18:55:01,611] [    INFO][0m - loss: 0.3462522, learning_rate: 2.9288135593220342e-05, global_step: 420, interval_runtime: 1.2684, interval_samples_per_second: 6.307, interval_steps_per_second: 7.884, epoch: 2.3729[0m
[32m[2022-08-31 18:55:02,886] [    INFO][0m - loss: 0.42210822, learning_rate: 2.9271186440677968e-05, global_step: 430, interval_runtime: 1.2754, interval_samples_per_second: 6.273, interval_steps_per_second: 7.841, epoch: 2.4294[0m
[32m[2022-08-31 18:55:04,163] [    INFO][0m - loss: 0.49078903, learning_rate: 2.9254237288135594e-05, global_step: 440, interval_runtime: 1.277, interval_samples_per_second: 6.264, interval_steps_per_second: 7.831, epoch: 2.4859[0m
[32m[2022-08-31 18:55:05,433] [    INFO][0m - loss: 0.46551561, learning_rate: 2.923728813559322e-05, global_step: 450, interval_runtime: 1.2696, interval_samples_per_second: 6.301, interval_steps_per_second: 7.877, epoch: 2.5424[0m
[32m[2022-08-31 18:55:06,704] [    INFO][0m - loss: 0.54994946, learning_rate: 2.922033898305085e-05, global_step: 460, interval_runtime: 1.2714, interval_samples_per_second: 6.292, interval_steps_per_second: 7.865, epoch: 2.5989[0m
[32m[2022-08-31 18:55:07,979] [    INFO][0m - loss: 0.49327755, learning_rate: 2.9203389830508475e-05, global_step: 470, interval_runtime: 1.275, interval_samples_per_second: 6.274, interval_steps_per_second: 7.843, epoch: 2.6554[0m
[32m[2022-08-31 18:55:09,287] [    INFO][0m - loss: 0.42410536, learning_rate: 2.91864406779661e-05, global_step: 480, interval_runtime: 1.3081, interval_samples_per_second: 6.116, interval_steps_per_second: 7.645, epoch: 2.7119[0m
[32m[2022-08-31 18:55:10,573] [    INFO][0m - loss: 0.41172881, learning_rate: 2.9169491525423727e-05, global_step: 490, interval_runtime: 1.2859, interval_samples_per_second: 6.221, interval_steps_per_second: 7.777, epoch: 2.7684[0m
[32m[2022-08-31 18:55:11,850] [    INFO][0m - loss: 0.35424778, learning_rate: 2.9152542372881356e-05, global_step: 500, interval_runtime: 1.277, interval_samples_per_second: 6.265, interval_steps_per_second: 7.831, epoch: 2.8249[0m
[32m[2022-08-31 18:55:11,851] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:55:11,851] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-08-31 18:55:11,851] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:55:11,851] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:55:11,851] [    INFO][0m -   Total prediction steps = 45[0m
[32m[2022-08-31 18:55:20,370] [    INFO][0m - eval_loss: 0.41006478667259216, eval_accuracy: 0.594059405940594, eval_runtime: 8.5182, eval_samples_per_second: 165.998, eval_steps_per_second: 5.283, epoch: 2.8249[0m
[32m[2022-08-31 18:55:20,371] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-08-31 18:55:20,371] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:55:22,702] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-08-31 18:55:22,703] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-08-31 18:55:31,087] [    INFO][0m - loss: 0.35354767, learning_rate: 2.9135593220338986e-05, global_step: 510, interval_runtime: 19.2364, interval_samples_per_second: 0.416, interval_steps_per_second: 0.52, epoch: 2.8814[0m
[32m[2022-08-31 18:55:32,366] [    INFO][0m - loss: 0.35985658, learning_rate: 2.911864406779661e-05, global_step: 520, interval_runtime: 1.2793, interval_samples_per_second: 6.253, interval_steps_per_second: 7.817, epoch: 2.9379[0m
[32m[2022-08-31 18:55:33,625] [    INFO][0m - loss: 0.33889167, learning_rate: 2.9101694915254238e-05, global_step: 530, interval_runtime: 1.2593, interval_samples_per_second: 6.353, interval_steps_per_second: 7.941, epoch: 2.9944[0m
[32m[2022-08-31 18:55:34,919] [    INFO][0m - loss: 0.33041, learning_rate: 2.9084745762711867e-05, global_step: 540, interval_runtime: 1.2933, interval_samples_per_second: 6.186, interval_steps_per_second: 7.732, epoch: 3.0508[0m
[32m[2022-08-31 18:55:36,202] [    INFO][0m - loss: 0.39507627, learning_rate: 2.9067796610169493e-05, global_step: 550, interval_runtime: 1.2833, interval_samples_per_second: 6.234, interval_steps_per_second: 7.792, epoch: 3.1073[0m
[32m[2022-08-31 18:55:37,474] [    INFO][0m - loss: 0.47307806, learning_rate: 2.905084745762712e-05, global_step: 560, interval_runtime: 1.2718, interval_samples_per_second: 6.29, interval_steps_per_second: 7.863, epoch: 3.1638[0m
[32m[2022-08-31 18:55:38,749] [    INFO][0m - loss: 0.42627945, learning_rate: 2.9033898305084745e-05, global_step: 570, interval_runtime: 1.2754, interval_samples_per_second: 6.272, interval_steps_per_second: 7.84, epoch: 3.2203[0m
[32m[2022-08-31 18:55:40,047] [    INFO][0m - loss: 0.39528527, learning_rate: 2.9016949152542374e-05, global_step: 580, interval_runtime: 1.2985, interval_samples_per_second: 6.161, interval_steps_per_second: 7.701, epoch: 3.2768[0m
[32m[2022-08-31 18:55:41,322] [    INFO][0m - loss: 0.45477548, learning_rate: 2.9e-05, global_step: 590, interval_runtime: 1.2742, interval_samples_per_second: 6.278, interval_steps_per_second: 7.848, epoch: 3.3333[0m
[32m[2022-08-31 18:55:42,620] [    INFO][0m - loss: 0.45789909, learning_rate: 2.8983050847457626e-05, global_step: 600, interval_runtime: 1.2974, interval_samples_per_second: 6.166, interval_steps_per_second: 7.708, epoch: 3.3898[0m
[32m[2022-08-31 18:55:42,621] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:55:42,621] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-08-31 18:55:42,621] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:55:42,621] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:55:42,621] [    INFO][0m -   Total prediction steps = 45[0m
[32m[2022-08-31 18:55:51,305] [    INFO][0m - eval_loss: 0.4092737138271332, eval_accuracy: 0.5346534653465347, eval_runtime: 8.6831, eval_samples_per_second: 162.844, eval_steps_per_second: 5.182, epoch: 3.3898[0m
[32m[2022-08-31 18:55:51,305] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-08-31 18:55:51,306] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:55:53,222] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-08-31 18:55:53,222] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-08-31 18:55:58,913] [    INFO][0m - loss: 0.37156498, learning_rate: 2.8966101694915252e-05, global_step: 610, interval_runtime: 16.2937, interval_samples_per_second: 0.491, interval_steps_per_second: 0.614, epoch: 3.4463[0m
[32m[2022-08-31 18:56:00,185] [    INFO][0m - loss: 0.43987622, learning_rate: 2.894915254237288e-05, global_step: 620, interval_runtime: 1.2723, interval_samples_per_second: 6.288, interval_steps_per_second: 7.859, epoch: 3.5028[0m
[32m[2022-08-31 18:56:01,458] [    INFO][0m - loss: 0.36390297, learning_rate: 2.893220338983051e-05, global_step: 630, interval_runtime: 1.2724, interval_samples_per_second: 6.287, interval_steps_per_second: 7.859, epoch: 3.5593[0m
[32m[2022-08-31 18:56:02,727] [    INFO][0m - loss: 0.55198984, learning_rate: 2.8915254237288137e-05, global_step: 640, interval_runtime: 1.2698, interval_samples_per_second: 6.3, interval_steps_per_second: 7.875, epoch: 3.6158[0m
[32m[2022-08-31 18:56:04,009] [    INFO][0m - loss: 0.38699901, learning_rate: 2.8898305084745763e-05, global_step: 650, interval_runtime: 1.2812, interval_samples_per_second: 6.244, interval_steps_per_second: 7.805, epoch: 3.6723[0m
[32m[2022-08-31 18:56:05,283] [    INFO][0m - loss: 0.44871049, learning_rate: 2.8881355932203392e-05, global_step: 660, interval_runtime: 1.2743, interval_samples_per_second: 6.278, interval_steps_per_second: 7.847, epoch: 3.7288[0m
[32m[2022-08-31 18:56:06,556] [    INFO][0m - loss: 0.3503747, learning_rate: 2.8864406779661018e-05, global_step: 670, interval_runtime: 1.2728, interval_samples_per_second: 6.286, interval_steps_per_second: 7.857, epoch: 3.7853[0m
[32m[2022-08-31 18:56:07,858] [    INFO][0m - loss: 0.39605672, learning_rate: 2.8847457627118644e-05, global_step: 680, interval_runtime: 1.3021, interval_samples_per_second: 6.144, interval_steps_per_second: 7.68, epoch: 3.8418[0m
[32m[2022-08-31 18:56:09,123] [    INFO][0m - loss: 0.40216541, learning_rate: 2.883050847457627e-05, global_step: 690, interval_runtime: 1.2656, interval_samples_per_second: 6.321, interval_steps_per_second: 7.902, epoch: 3.8983[0m
[32m[2022-08-31 18:56:10,394] [    INFO][0m - loss: 0.47948399, learning_rate: 2.88135593220339e-05, global_step: 700, interval_runtime: 1.2702, interval_samples_per_second: 6.298, interval_steps_per_second: 7.873, epoch: 3.9548[0m
[32m[2022-08-31 18:56:10,394] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:56:10,394] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-08-31 18:56:10,395] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:56:10,395] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:56:10,395] [    INFO][0m -   Total prediction steps = 45[0m
[32m[2022-08-31 18:56:18,988] [    INFO][0m - eval_loss: 0.40751776099205017, eval_accuracy: 0.3415841584158416, eval_runtime: 8.5929, eval_samples_per_second: 164.555, eval_steps_per_second: 5.237, epoch: 3.9548[0m
[32m[2022-08-31 18:56:18,989] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-700[0m
[32m[2022-08-31 18:56:18,989] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:56:20,784] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-700/tokenizer_config.json[0m
[32m[2022-08-31 18:56:20,784] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-700/special_tokens_map.json[0m
[32m[2022-08-31 18:56:26,505] [    INFO][0m - loss: 0.4938458, learning_rate: 2.8796610169491525e-05, global_step: 710, interval_runtime: 16.111, interval_samples_per_second: 0.497, interval_steps_per_second: 0.621, epoch: 4.0113[0m
[32m[2022-08-31 18:56:27,775] [    INFO][0m - loss: 0.38362148, learning_rate: 2.877966101694915e-05, global_step: 720, interval_runtime: 1.2699, interval_samples_per_second: 6.3, interval_steps_per_second: 7.875, epoch: 4.0678[0m
[32m[2022-08-31 18:56:29,059] [    INFO][0m - loss: 0.41493988, learning_rate: 2.8762711864406777e-05, global_step: 730, interval_runtime: 1.284, interval_samples_per_second: 6.23, interval_steps_per_second: 7.788, epoch: 4.1243[0m
[32m[2022-08-31 18:56:30,332] [    INFO][0m - loss: 0.35326688, learning_rate: 2.874576271186441e-05, global_step: 740, interval_runtime: 1.2739, interval_samples_per_second: 6.28, interval_steps_per_second: 7.85, epoch: 4.1808[0m
[32m[2022-08-31 18:56:31,609] [    INFO][0m - loss: 0.41817856, learning_rate: 2.8728813559322036e-05, global_step: 750, interval_runtime: 1.2761, interval_samples_per_second: 6.269, interval_steps_per_second: 7.836, epoch: 4.2373[0m
[32m[2022-08-31 18:56:32,887] [    INFO][0m - loss: 0.55544338, learning_rate: 2.8711864406779662e-05, global_step: 760, interval_runtime: 1.2785, interval_samples_per_second: 6.257, interval_steps_per_second: 7.822, epoch: 4.2938[0m
[32m[2022-08-31 18:56:34,168] [    INFO][0m - loss: 0.34151378, learning_rate: 2.8694915254237288e-05, global_step: 770, interval_runtime: 1.281, interval_samples_per_second: 6.245, interval_steps_per_second: 7.806, epoch: 4.3503[0m
[32m[2022-08-31 18:56:35,440] [    INFO][0m - loss: 0.42360907, learning_rate: 2.8677966101694917e-05, global_step: 780, interval_runtime: 1.2721, interval_samples_per_second: 6.289, interval_steps_per_second: 7.861, epoch: 4.4068[0m
[32m[2022-08-31 18:56:36,715] [    INFO][0m - loss: 0.46683459, learning_rate: 2.8661016949152543e-05, global_step: 790, interval_runtime: 1.2747, interval_samples_per_second: 6.276, interval_steps_per_second: 7.845, epoch: 4.4633[0m
[32m[2022-08-31 18:56:37,988] [    INFO][0m - loss: 0.35218711, learning_rate: 2.864406779661017e-05, global_step: 800, interval_runtime: 1.2733, interval_samples_per_second: 6.283, interval_steps_per_second: 7.854, epoch: 4.5198[0m
[32m[2022-08-31 18:56:37,989] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:56:37,989] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-08-31 18:56:37,989] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:56:37,989] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:56:37,989] [    INFO][0m -   Total prediction steps = 45[0m
[32m[2022-08-31 18:56:46,615] [    INFO][0m - eval_loss: 0.3818274736404419, eval_accuracy: 0.40594059405940597, eval_runtime: 8.6247, eval_samples_per_second: 163.948, eval_steps_per_second: 5.218, epoch: 4.5198[0m
[32m[2022-08-31 18:56:46,615] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-800[0m
[32m[2022-08-31 18:56:46,615] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:56:48,871] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-800/tokenizer_config.json[0m
[32m[2022-08-31 18:56:48,872] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-800/special_tokens_map.json[0m
[32m[2022-08-31 18:56:54,631] [    INFO][0m - loss: 0.25421543, learning_rate: 2.86271186440678e-05, global_step: 810, interval_runtime: 16.6423, interval_samples_per_second: 0.481, interval_steps_per_second: 0.601, epoch: 4.5763[0m
[32m[2022-08-31 18:56:55,907] [    INFO][0m - loss: 0.43792691, learning_rate: 2.8610169491525424e-05, global_step: 820, interval_runtime: 1.2757, interval_samples_per_second: 6.271, interval_steps_per_second: 7.839, epoch: 4.6328[0m
[32m[2022-08-31 18:56:57,188] [    INFO][0m - loss: 0.52431746, learning_rate: 2.859322033898305e-05, global_step: 830, interval_runtime: 1.2819, interval_samples_per_second: 6.241, interval_steps_per_second: 7.801, epoch: 4.6893[0m
[32m[2022-08-31 18:56:58,457] [    INFO][0m - loss: 0.39188547, learning_rate: 2.8576271186440676e-05, global_step: 840, interval_runtime: 1.2683, interval_samples_per_second: 6.308, interval_steps_per_second: 7.885, epoch: 4.7458[0m
[32m[2022-08-31 18:56:59,730] [    INFO][0m - loss: 0.40201149, learning_rate: 2.855932203389831e-05, global_step: 850, interval_runtime: 1.273, interval_samples_per_second: 6.284, interval_steps_per_second: 7.855, epoch: 4.8023[0m
[32m[2022-08-31 18:57:01,011] [    INFO][0m - loss: 0.5149858, learning_rate: 2.8542372881355935e-05, global_step: 860, interval_runtime: 1.2811, interval_samples_per_second: 6.244, interval_steps_per_second: 7.806, epoch: 4.8588[0m
[32m[2022-08-31 18:57:02,299] [    INFO][0m - loss: 0.36700509, learning_rate: 2.852542372881356e-05, global_step: 870, interval_runtime: 1.2882, interval_samples_per_second: 6.21, interval_steps_per_second: 7.763, epoch: 4.9153[0m
[32m[2022-08-31 18:57:03,568] [    INFO][0m - loss: 0.45345187, learning_rate: 2.8508474576271187e-05, global_step: 880, interval_runtime: 1.2692, interval_samples_per_second: 6.303, interval_steps_per_second: 7.879, epoch: 4.9718[0m
[32m[2022-08-31 18:57:04,853] [    INFO][0m - loss: 0.34755867, learning_rate: 2.8491525423728816e-05, global_step: 890, interval_runtime: 1.2842, interval_samples_per_second: 6.229, interval_steps_per_second: 7.787, epoch: 5.0282[0m
[32m[2022-08-31 18:57:06,142] [    INFO][0m - loss: 0.49812608, learning_rate: 2.8474576271186442e-05, global_step: 900, interval_runtime: 1.29, interval_samples_per_second: 6.202, interval_steps_per_second: 7.752, epoch: 5.0847[0m
[32m[2022-08-31 18:57:06,143] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:57:06,143] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-08-31 18:57:06,143] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:57:06,144] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:57:06,144] [    INFO][0m -   Total prediction steps = 45[0m
[32m[2022-08-31 18:57:14,600] [    INFO][0m - eval_loss: 0.4972793161869049, eval_accuracy: 0.3811881188118812, eval_runtime: 8.4553, eval_samples_per_second: 167.232, eval_steps_per_second: 5.322, epoch: 5.0847[0m
[32m[2022-08-31 18:57:14,600] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-900[0m
[32m[2022-08-31 18:57:14,600] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:57:16,963] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-900/tokenizer_config.json[0m
[32m[2022-08-31 18:57:16,963] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-900/special_tokens_map.json[0m
[32m[2022-08-31 18:57:23,091] [    INFO][0m - loss: 0.26979938, learning_rate: 2.8457627118644068e-05, global_step: 910, interval_runtime: 16.9482, interval_samples_per_second: 0.472, interval_steps_per_second: 0.59, epoch: 5.1412[0m
[32m[2022-08-31 18:57:24,383] [    INFO][0m - loss: 0.38410053, learning_rate: 2.8440677966101694e-05, global_step: 920, interval_runtime: 1.2926, interval_samples_per_second: 6.189, interval_steps_per_second: 7.737, epoch: 5.1977[0m
[32m[2022-08-31 18:57:25,667] [    INFO][0m - loss: 0.22747278, learning_rate: 2.8423728813559323e-05, global_step: 930, interval_runtime: 1.2833, interval_samples_per_second: 6.234, interval_steps_per_second: 7.793, epoch: 5.2542[0m
[32m[2022-08-31 18:57:26,945] [    INFO][0m - loss: 0.27192614, learning_rate: 2.840677966101695e-05, global_step: 940, interval_runtime: 1.2762, interval_samples_per_second: 6.268, interval_steps_per_second: 7.836, epoch: 5.3107[0m
[32m[2022-08-31 18:57:28,213] [    INFO][0m - loss: 0.71248927, learning_rate: 2.8389830508474575e-05, global_step: 950, interval_runtime: 1.2688, interval_samples_per_second: 6.305, interval_steps_per_second: 7.881, epoch: 5.3672[0m
[32m[2022-08-31 18:57:29,491] [    INFO][0m - loss: 0.32979198, learning_rate: 2.8372881355932205e-05, global_step: 960, interval_runtime: 1.28, interval_samples_per_second: 6.25, interval_steps_per_second: 7.813, epoch: 5.4237[0m
[32m[2022-08-31 18:57:30,767] [    INFO][0m - loss: 0.30224128, learning_rate: 2.8355932203389834e-05, global_step: 970, interval_runtime: 1.2753, interval_samples_per_second: 6.273, interval_steps_per_second: 7.841, epoch: 5.4802[0m
[32m[2022-08-31 18:57:32,057] [    INFO][0m - loss: 0.29275579, learning_rate: 2.833898305084746e-05, global_step: 980, interval_runtime: 1.2899, interval_samples_per_second: 6.202, interval_steps_per_second: 7.753, epoch: 5.5367[0m
[32m[2022-08-31 18:57:33,338] [    INFO][0m - loss: 0.44246254, learning_rate: 2.8322033898305086e-05, global_step: 990, interval_runtime: 1.2812, interval_samples_per_second: 6.244, interval_steps_per_second: 7.805, epoch: 5.5932[0m
[32m[2022-08-31 18:57:34,608] [    INFO][0m - loss: 0.4440774, learning_rate: 2.8305084745762712e-05, global_step: 1000, interval_runtime: 1.2706, interval_samples_per_second: 6.296, interval_steps_per_second: 7.87, epoch: 5.6497[0m
[32m[2022-08-31 18:57:34,609] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:57:34,609] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-08-31 18:57:34,609] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:57:34,609] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:57:34,610] [    INFO][0m -   Total prediction steps = 45[0m
[32m[2022-08-31 18:57:43,167] [    INFO][0m - eval_loss: 0.5969682931900024, eval_accuracy: 0.400990099009901, eval_runtime: 8.556, eval_samples_per_second: 165.264, eval_steps_per_second: 5.259, epoch: 5.6497[0m
[32m[2022-08-31 18:57:43,167] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1000[0m
[32m[2022-08-31 18:57:43,168] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:57:45,256] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1000/tokenizer_config.json[0m
[32m[2022-08-31 18:57:45,256] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1000/special_tokens_map.json[0m
[32m[2022-08-31 18:57:51,283] [    INFO][0m - loss: 0.38585827, learning_rate: 2.828813559322034e-05, global_step: 1010, interval_runtime: 16.6747, interval_samples_per_second: 0.48, interval_steps_per_second: 0.6, epoch: 5.7062[0m
[32m[2022-08-31 18:57:52,576] [    INFO][0m - loss: 0.3418998, learning_rate: 2.8271186440677967e-05, global_step: 1020, interval_runtime: 1.293, interval_samples_per_second: 6.187, interval_steps_per_second: 7.734, epoch: 5.7627[0m
[32m[2022-08-31 18:57:53,860] [    INFO][0m - loss: 0.32847164, learning_rate: 2.8254237288135593e-05, global_step: 1030, interval_runtime: 1.2826, interval_samples_per_second: 6.237, interval_steps_per_second: 7.797, epoch: 5.8192[0m
[32m[2022-08-31 18:57:55,144] [    INFO][0m - loss: 0.2464973, learning_rate: 2.823728813559322e-05, global_step: 1040, interval_runtime: 1.2855, interval_samples_per_second: 6.223, interval_steps_per_second: 7.779, epoch: 5.8757[0m
[32m[2022-08-31 18:57:56,414] [    INFO][0m - loss: 0.3911062, learning_rate: 2.822033898305085e-05, global_step: 1050, interval_runtime: 1.269, interval_samples_per_second: 6.304, interval_steps_per_second: 7.88, epoch: 5.9322[0m
[32m[2022-08-31 18:57:57,677] [    INFO][0m - loss: 0.45821075, learning_rate: 2.8203389830508475e-05, global_step: 1060, interval_runtime: 1.2629, interval_samples_per_second: 6.335, interval_steps_per_second: 7.918, epoch: 5.9887[0m
[32m[2022-08-31 18:57:58,964] [    INFO][0m - loss: 0.45358634, learning_rate: 2.81864406779661e-05, global_step: 1070, interval_runtime: 1.2877, interval_samples_per_second: 6.212, interval_steps_per_second: 7.766, epoch: 6.0452[0m
[32m[2022-08-31 18:58:00,233] [    INFO][0m - loss: 0.28918481, learning_rate: 2.816949152542373e-05, global_step: 1080, interval_runtime: 1.2688, interval_samples_per_second: 6.305, interval_steps_per_second: 7.882, epoch: 6.1017[0m
[32m[2022-08-31 18:58:01,503] [    INFO][0m - loss: 0.29828796, learning_rate: 2.815254237288136e-05, global_step: 1090, interval_runtime: 1.2706, interval_samples_per_second: 6.296, interval_steps_per_second: 7.87, epoch: 6.1582[0m
[32m[2022-08-31 18:58:02,782] [    INFO][0m - loss: 0.3124227, learning_rate: 2.8135593220338985e-05, global_step: 1100, interval_runtime: 1.278, interval_samples_per_second: 6.26, interval_steps_per_second: 7.825, epoch: 6.2147[0m
[32m[2022-08-31 18:58:02,783] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:58:02,783] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-08-31 18:58:02,783] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:58:02,783] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:58:02,783] [    INFO][0m -   Total prediction steps = 45[0m
[32m[2022-08-31 18:58:11,375] [    INFO][0m - eval_loss: 0.5031841993331909, eval_accuracy: 0.47029702970297027, eval_runtime: 8.5917, eval_samples_per_second: 164.578, eval_steps_per_second: 5.238, epoch: 6.2147[0m
[32m[2022-08-31 18:58:11,376] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1100[0m
[32m[2022-08-31 18:58:11,376] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:58:13,354] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1100/tokenizer_config.json[0m
[32m[2022-08-31 18:58:13,355] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1100/special_tokens_map.json[0m
[32m[2022-08-31 18:58:19,486] [    INFO][0m - loss: 0.17105045, learning_rate: 2.811864406779661e-05, global_step: 1110, interval_runtime: 16.7019, interval_samples_per_second: 0.479, interval_steps_per_second: 0.599, epoch: 6.2712[0m
[32m[2022-08-31 18:58:20,800] [    INFO][0m - loss: 0.24424345, learning_rate: 2.8101694915254237e-05, global_step: 1120, interval_runtime: 1.3163, interval_samples_per_second: 6.078, interval_steps_per_second: 7.597, epoch: 6.3277[0m
[32m[2022-08-31 18:58:22,075] [    INFO][0m - loss: 0.31585205, learning_rate: 2.8084745762711866e-05, global_step: 1130, interval_runtime: 1.275, interval_samples_per_second: 6.274, interval_steps_per_second: 7.843, epoch: 6.3842[0m
[32m[2022-08-31 18:58:23,360] [    INFO][0m - loss: 0.46693807, learning_rate: 2.8067796610169492e-05, global_step: 1140, interval_runtime: 1.285, interval_samples_per_second: 6.226, interval_steps_per_second: 7.782, epoch: 6.4407[0m
[32m[2022-08-31 18:58:24,651] [    INFO][0m - loss: 0.41057105, learning_rate: 2.805084745762712e-05, global_step: 1150, interval_runtime: 1.2911, interval_samples_per_second: 6.196, interval_steps_per_second: 7.745, epoch: 6.4972[0m
[32m[2022-08-31 18:58:25,927] [    INFO][0m - loss: 0.12442255, learning_rate: 2.8033898305084744e-05, global_step: 1160, interval_runtime: 1.2768, interval_samples_per_second: 6.266, interval_steps_per_second: 7.832, epoch: 6.5537[0m
[32m[2022-08-31 18:58:27,221] [    INFO][0m - loss: 0.3791579, learning_rate: 2.8016949152542374e-05, global_step: 1170, interval_runtime: 1.2932, interval_samples_per_second: 6.186, interval_steps_per_second: 7.733, epoch: 6.6102[0m
[32m[2022-08-31 18:58:28,500] [    INFO][0m - loss: 0.24133205, learning_rate: 2.8e-05, global_step: 1180, interval_runtime: 1.2797, interval_samples_per_second: 6.251, interval_steps_per_second: 7.814, epoch: 6.6667[0m
[32m[2022-08-31 18:58:29,780] [    INFO][0m - loss: 0.16055298, learning_rate: 2.798305084745763e-05, global_step: 1190, interval_runtime: 1.2795, interval_samples_per_second: 6.253, interval_steps_per_second: 7.816, epoch: 6.7232[0m
[32m[2022-08-31 18:58:31,059] [    INFO][0m - loss: 0.37617743, learning_rate: 2.7966101694915255e-05, global_step: 1200, interval_runtime: 1.2793, interval_samples_per_second: 6.254, interval_steps_per_second: 7.817, epoch: 6.7797[0m
[32m[2022-08-31 18:58:31,060] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:58:31,060] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-08-31 18:58:31,060] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:58:31,060] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:58:31,060] [    INFO][0m -   Total prediction steps = 45[0m
[32m[2022-08-31 18:58:39,688] [    INFO][0m - eval_loss: 0.8406813144683838, eval_accuracy: 0.4504950495049505, eval_runtime: 8.6268, eval_samples_per_second: 163.908, eval_steps_per_second: 5.216, epoch: 6.7797[0m
[32m[2022-08-31 18:58:39,689] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1200[0m
[32m[2022-08-31 18:58:39,690] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:58:41,752] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1200/tokenizer_config.json[0m
[32m[2022-08-31 18:58:41,752] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1200/special_tokens_map.json[0m
[32m[2022-08-31 18:58:48,038] [    INFO][0m - loss: 0.33791819, learning_rate: 2.7949152542372884e-05, global_step: 1210, interval_runtime: 16.9782, interval_samples_per_second: 0.471, interval_steps_per_second: 0.589, epoch: 6.8362[0m
[32m[2022-08-31 18:58:49,311] [    INFO][0m - loss: 0.4329464, learning_rate: 2.793220338983051e-05, global_step: 1220, interval_runtime: 1.273, interval_samples_per_second: 6.284, interval_steps_per_second: 7.855, epoch: 6.8927[0m
[32m[2022-08-31 18:58:50,585] [    INFO][0m - loss: 0.26513352, learning_rate: 2.7915254237288136e-05, global_step: 1230, interval_runtime: 1.2744, interval_samples_per_second: 6.278, interval_steps_per_second: 7.847, epoch: 6.9492[0m
[32m[2022-08-31 18:58:51,880] [    INFO][0m - loss: 0.33054731, learning_rate: 2.7898305084745762e-05, global_step: 1240, interval_runtime: 1.2949, interval_samples_per_second: 6.178, interval_steps_per_second: 7.723, epoch: 7.0056[0m
[32m[2022-08-31 18:58:53,155] [    INFO][0m - loss: 0.16774515, learning_rate: 2.788135593220339e-05, global_step: 1250, interval_runtime: 1.2752, interval_samples_per_second: 6.274, interval_steps_per_second: 7.842, epoch: 7.0621[0m
[32m[2022-08-31 18:58:54,438] [    INFO][0m - loss: 0.24800556, learning_rate: 2.7864406779661017e-05, global_step: 1260, interval_runtime: 1.2823, interval_samples_per_second: 6.239, interval_steps_per_second: 7.798, epoch: 7.1186[0m
[32m[2022-08-31 18:58:55,707] [    INFO][0m - loss: 0.50293922, learning_rate: 2.7847457627118643e-05, global_step: 1270, interval_runtime: 1.2695, interval_samples_per_second: 6.302, interval_steps_per_second: 7.877, epoch: 7.1751[0m
[32m[2022-08-31 18:58:56,983] [    INFO][0m - loss: 0.28452842, learning_rate: 2.783050847457627e-05, global_step: 1280, interval_runtime: 1.2756, interval_samples_per_second: 6.272, interval_steps_per_second: 7.84, epoch: 7.2316[0m
[32m[2022-08-31 18:58:58,256] [    INFO][0m - loss: 0.1087204, learning_rate: 2.78135593220339e-05, global_step: 1290, interval_runtime: 1.2733, interval_samples_per_second: 6.283, interval_steps_per_second: 7.854, epoch: 7.2881[0m
[32m[2022-08-31 18:58:59,534] [    INFO][0m - loss: 0.30927405, learning_rate: 2.7796610169491528e-05, global_step: 1300, interval_runtime: 1.2783, interval_samples_per_second: 6.258, interval_steps_per_second: 7.823, epoch: 7.3446[0m
[32m[2022-08-31 18:58:59,535] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:58:59,535] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-08-31 18:58:59,536] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:58:59,536] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:58:59,536] [    INFO][0m -   Total prediction steps = 45[0m
[32m[2022-08-31 18:59:08,144] [    INFO][0m - eval_loss: 0.7381966710090637, eval_accuracy: 0.5148514851485149, eval_runtime: 8.6082, eval_samples_per_second: 164.262, eval_steps_per_second: 5.228, epoch: 7.3446[0m
[32m[2022-08-31 18:59:08,145] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1300[0m
[32m[2022-08-31 18:59:08,145] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:59:09,718] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1300/tokenizer_config.json[0m
[32m[2022-08-31 18:59:09,719] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1300/special_tokens_map.json[0m
[32m[2022-08-31 18:59:15,520] [    INFO][0m - loss: 0.21713591, learning_rate: 2.7779661016949154e-05, global_step: 1310, interval_runtime: 15.9859, interval_samples_per_second: 0.5, interval_steps_per_second: 0.626, epoch: 7.4011[0m
[32m[2022-08-31 18:59:16,801] [    INFO][0m - loss: 0.06678227, learning_rate: 2.776271186440678e-05, global_step: 1320, interval_runtime: 1.281, interval_samples_per_second: 6.245, interval_steps_per_second: 7.806, epoch: 7.4576[0m
[32m[2022-08-31 18:59:18,081] [    INFO][0m - loss: 0.25138705, learning_rate: 2.774576271186441e-05, global_step: 1330, interval_runtime: 1.2803, interval_samples_per_second: 6.248, interval_steps_per_second: 7.81, epoch: 7.5141[0m
[32m[2022-08-31 18:59:19,355] [    INFO][0m - loss: 0.10123158, learning_rate: 2.7728813559322035e-05, global_step: 1340, interval_runtime: 1.2741, interval_samples_per_second: 6.279, interval_steps_per_second: 7.848, epoch: 7.5706[0m
[32m[2022-08-31 18:59:20,638] [    INFO][0m - loss: 0.15951736, learning_rate: 2.771186440677966e-05, global_step: 1350, interval_runtime: 1.2823, interval_samples_per_second: 6.239, interval_steps_per_second: 7.798, epoch: 7.6271[0m
[32m[2022-08-31 18:59:21,913] [    INFO][0m - loss: 0.18189782, learning_rate: 2.7694915254237287e-05, global_step: 1360, interval_runtime: 1.2754, interval_samples_per_second: 6.273, interval_steps_per_second: 7.841, epoch: 7.6836[0m
[32m[2022-08-31 18:59:23,180] [    INFO][0m - loss: 0.16386701, learning_rate: 2.7677966101694917e-05, global_step: 1370, interval_runtime: 1.2664, interval_samples_per_second: 6.317, interval_steps_per_second: 7.896, epoch: 7.7401[0m
[32m[2022-08-31 18:59:24,448] [    INFO][0m - loss: 0.26078656, learning_rate: 2.7661016949152542e-05, global_step: 1380, interval_runtime: 1.2682, interval_samples_per_second: 6.308, interval_steps_per_second: 7.885, epoch: 7.7966[0m
[32m[2022-08-31 18:59:25,718] [    INFO][0m - loss: 0.2438355, learning_rate: 2.764406779661017e-05, global_step: 1390, interval_runtime: 1.2706, interval_samples_per_second: 6.296, interval_steps_per_second: 7.87, epoch: 7.8531[0m
[32m[2022-08-31 18:59:26,992] [    INFO][0m - loss: 0.35081644, learning_rate: 2.7627118644067794e-05, global_step: 1400, interval_runtime: 1.2724, interval_samples_per_second: 6.287, interval_steps_per_second: 7.859, epoch: 7.9096[0m
[32m[2022-08-31 18:59:26,993] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:59:26,993] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-08-31 18:59:26,993] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:59:26,994] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:59:26,994] [    INFO][0m -   Total prediction steps = 45[0m
[32m[2022-08-31 18:59:35,546] [    INFO][0m - eval_loss: 0.5658089518547058, eval_accuracy: 0.4603960396039604, eval_runtime: 8.552, eval_samples_per_second: 165.341, eval_steps_per_second: 5.262, epoch: 7.9096[0m
[32m[2022-08-31 18:59:35,547] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1400[0m
[32m[2022-08-31 18:59:35,547] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:59:37,491] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1400/tokenizer_config.json[0m
[32m[2022-08-31 18:59:37,492] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1400/special_tokens_map.json[0m
[32m[2022-08-31 18:59:43,170] [    INFO][0m - loss: 0.08850827, learning_rate: 2.7610169491525427e-05, global_step: 1410, interval_runtime: 16.1788, interval_samples_per_second: 0.494, interval_steps_per_second: 0.618, epoch: 7.9661[0m
[32m[2022-08-31 18:59:44,476] [    INFO][0m - loss: 0.23033781, learning_rate: 2.7593220338983053e-05, global_step: 1420, interval_runtime: 1.3063, interval_samples_per_second: 6.124, interval_steps_per_second: 7.655, epoch: 8.0226[0m
[32m[2022-08-31 18:59:45,759] [    INFO][0m - loss: 0.24896073, learning_rate: 2.757627118644068e-05, global_step: 1430, interval_runtime: 1.2833, interval_samples_per_second: 6.234, interval_steps_per_second: 7.792, epoch: 8.0791[0m
[32m[2022-08-31 18:59:47,034] [    INFO][0m - loss: 0.08395635, learning_rate: 2.7559322033898305e-05, global_step: 1440, interval_runtime: 1.2744, interval_samples_per_second: 6.278, interval_steps_per_second: 7.847, epoch: 8.1356[0m
[32m[2022-08-31 18:59:48,307] [    INFO][0m - loss: 0.27966118, learning_rate: 2.7542372881355934e-05, global_step: 1450, interval_runtime: 1.2733, interval_samples_per_second: 6.283, interval_steps_per_second: 7.854, epoch: 8.1921[0m
[32m[2022-08-31 18:59:49,580] [    INFO][0m - loss: 0.23845308, learning_rate: 2.752542372881356e-05, global_step: 1460, interval_runtime: 1.2732, interval_samples_per_second: 6.283, interval_steps_per_second: 7.854, epoch: 8.2486[0m
[32m[2022-08-31 18:59:50,860] [    INFO][0m - loss: 0.1793721, learning_rate: 2.7508474576271186e-05, global_step: 1470, interval_runtime: 1.2792, interval_samples_per_second: 6.254, interval_steps_per_second: 7.817, epoch: 8.3051[0m
[32m[2022-08-31 18:59:52,132] [    INFO][0m - loss: 0.12459433, learning_rate: 2.7491525423728812e-05, global_step: 1480, interval_runtime: 1.2724, interval_samples_per_second: 6.287, interval_steps_per_second: 7.859, epoch: 8.3616[0m
[32m[2022-08-31 18:59:53,403] [    INFO][0m - loss: 0.18947245, learning_rate: 2.747457627118644e-05, global_step: 1490, interval_runtime: 1.2707, interval_samples_per_second: 6.296, interval_steps_per_second: 7.87, epoch: 8.4181[0m
[32m[2022-08-31 18:59:56,413] [    INFO][0m - loss: 0.06305928, learning_rate: 2.7457627118644068e-05, global_step: 1500, interval_runtime: 1.2744, interval_samples_per_second: 6.277, interval_steps_per_second: 7.847, epoch: 8.4746[0m
[32m[2022-08-31 18:59:56,415] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:59:56,415] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-08-31 18:59:56,415] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:59:56,415] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:59:56,415] [    INFO][0m -   Total prediction steps = 45[0m
[32m[2022-08-31 19:00:05,017] [    INFO][0m - eval_loss: 0.8148716688156128, eval_accuracy: 0.504950495049505, eval_runtime: 8.6008, eval_samples_per_second: 164.403, eval_steps_per_second: 5.232, epoch: 8.4746[0m
[32m[2022-08-31 19:00:05,017] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1500[0m
[32m[2022-08-31 19:00:05,017] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 19:00:08,311] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1500/tokenizer_config.json[0m
[32m[2022-08-31 19:00:08,312] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1500/special_tokens_map.json[0m
[32m[2022-08-31 19:00:14,044] [    INFO][0m - loss: 0.2389426, learning_rate: 2.7440677966101694e-05, global_step: 1510, interval_runtime: 19.3669, interval_samples_per_second: 0.413, interval_steps_per_second: 0.516, epoch: 8.5311[0m
[32m[2022-08-31 19:00:15,315] [    INFO][0m - loss: 0.13800591, learning_rate: 2.742372881355932e-05, global_step: 1520, interval_runtime: 1.2706, interval_samples_per_second: 6.296, interval_steps_per_second: 7.871, epoch: 8.5876[0m
[32m[2022-08-31 19:00:16,584] [    INFO][0m - loss: 0.17512708, learning_rate: 2.7406779661016952e-05, global_step: 1530, interval_runtime: 1.2689, interval_samples_per_second: 6.305, interval_steps_per_second: 7.881, epoch: 8.6441[0m
[32m[2022-08-31 19:00:17,851] [    INFO][0m - loss: 0.25246449, learning_rate: 2.7389830508474578e-05, global_step: 1540, interval_runtime: 1.2679, interval_samples_per_second: 6.309, interval_steps_per_second: 7.887, epoch: 8.7006[0m
[32m[2022-08-31 19:00:19,121] [    INFO][0m - loss: 0.08051835, learning_rate: 2.7372881355932204e-05, global_step: 1550, interval_runtime: 1.2697, interval_samples_per_second: 6.301, interval_steps_per_second: 7.876, epoch: 8.7571[0m
[32m[2022-08-31 19:00:20,394] [    INFO][0m - loss: 0.1835662, learning_rate: 2.735593220338983e-05, global_step: 1560, interval_runtime: 1.2734, interval_samples_per_second: 6.282, interval_steps_per_second: 7.853, epoch: 8.8136[0m
[32m[2022-08-31 19:00:21,666] [    INFO][0m - loss: 0.43056774, learning_rate: 2.733898305084746e-05, global_step: 1570, interval_runtime: 1.2712, interval_samples_per_second: 6.293, interval_steps_per_second: 7.867, epoch: 8.8701[0m
[32m[2022-08-31 19:00:22,934] [    INFO][0m - loss: 0.03039847, learning_rate: 2.7322033898305085e-05, global_step: 1580, interval_runtime: 1.2677, interval_samples_per_second: 6.311, interval_steps_per_second: 7.888, epoch: 8.9266[0m
[32m[2022-08-31 19:00:24,202] [    INFO][0m - loss: 0.28897564, learning_rate: 2.730508474576271e-05, global_step: 1590, interval_runtime: 1.2685, interval_samples_per_second: 6.306, interval_steps_per_second: 7.883, epoch: 8.9831[0m
[32m[2022-08-31 19:00:25,503] [    INFO][0m - loss: 0.23543601, learning_rate: 2.7288135593220337e-05, global_step: 1600, interval_runtime: 1.301, interval_samples_per_second: 6.149, interval_steps_per_second: 7.686, epoch: 9.0395[0m
[32m[2022-08-31 19:00:25,504] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 19:00:25,504] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-08-31 19:00:25,504] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 19:00:25,504] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 19:00:25,504] [    INFO][0m -   Total prediction steps = 45[0m
[32m[2022-08-31 19:00:34,034] [    INFO][0m - eval_loss: 1.366019606590271, eval_accuracy: 0.47029702970297027, eval_runtime: 8.5298, eval_samples_per_second: 165.772, eval_steps_per_second: 5.276, epoch: 9.0395[0m
[32m[2022-08-31 19:00:34,035] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1600[0m
[32m[2022-08-31 19:00:34,035] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 19:00:35,905] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1600/tokenizer_config.json[0m
[32m[2022-08-31 19:00:35,906] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1600/special_tokens_map.json[0m
[32m[2022-08-31 19:00:41,692] [    INFO][0m - loss: 0.00392144, learning_rate: 2.7271186440677967e-05, global_step: 1610, interval_runtime: 16.189, interval_samples_per_second: 0.494, interval_steps_per_second: 0.618, epoch: 9.096[0m
[32m[2022-08-31 19:00:42,970] [    INFO][0m - loss: 0.1510831, learning_rate: 2.7254237288135593e-05, global_step: 1620, interval_runtime: 1.2777, interval_samples_per_second: 6.261, interval_steps_per_second: 7.827, epoch: 9.1525[0m
[32m[2022-08-31 19:00:44,241] [    INFO][0m - loss: 0.16670635, learning_rate: 2.723728813559322e-05, global_step: 1630, interval_runtime: 1.2718, interval_samples_per_second: 6.29, interval_steps_per_second: 7.863, epoch: 9.209[0m
[32m[2022-08-31 19:00:45,503] [    INFO][0m - loss: 0.07765105, learning_rate: 2.722033898305085e-05, global_step: 1640, interval_runtime: 1.2609, interval_samples_per_second: 6.345, interval_steps_per_second: 7.931, epoch: 9.2655[0m
[32m[2022-08-31 19:00:46,762] [    INFO][0m - loss: 0.36769423, learning_rate: 2.7203389830508477e-05, global_step: 1650, interval_runtime: 1.26, interval_samples_per_second: 6.349, interval_steps_per_second: 7.937, epoch: 9.322[0m
[32m[2022-08-31 19:00:48,039] [    INFO][0m - loss: 0.25436566, learning_rate: 2.7186440677966103e-05, global_step: 1660, interval_runtime: 1.2765, interval_samples_per_second: 6.267, interval_steps_per_second: 7.834, epoch: 9.3785[0m
[32m[2022-08-31 19:00:49,345] [    INFO][0m - loss: 0.12106229, learning_rate: 2.716949152542373e-05, global_step: 1670, interval_runtime: 1.3057, interval_samples_per_second: 6.127, interval_steps_per_second: 7.658, epoch: 9.435[0m
[32m[2022-08-31 19:00:50,617] [    INFO][0m - loss: 0.00215028, learning_rate: 2.715254237288136e-05, global_step: 1680, interval_runtime: 1.2725, interval_samples_per_second: 6.287, interval_steps_per_second: 7.859, epoch: 9.4915[0m
[32m[2022-08-31 19:00:51,884] [    INFO][0m - loss: 0.35342376, learning_rate: 2.7135593220338985e-05, global_step: 1690, interval_runtime: 1.2667, interval_samples_per_second: 6.316, interval_steps_per_second: 7.895, epoch: 9.548[0m
[32m[2022-08-31 19:00:53,160] [    INFO][0m - loss: 0.12975973, learning_rate: 2.711864406779661e-05, global_step: 1700, interval_runtime: 1.2762, interval_samples_per_second: 6.269, interval_steps_per_second: 7.836, epoch: 9.6045[0m
[32m[2022-08-31 19:00:53,161] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 19:00:53,162] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-08-31 19:00:53,162] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 19:00:53,162] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 19:00:53,162] [    INFO][0m -   Total prediction steps = 45[0m
[32m[2022-08-31 19:01:01,700] [    INFO][0m - eval_loss: 0.6888177990913391, eval_accuracy: 0.4504950495049505, eval_runtime: 8.5375, eval_samples_per_second: 165.622, eval_steps_per_second: 5.271, epoch: 9.6045[0m
[32m[2022-08-31 19:01:01,700] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1700[0m
[32m[2022-08-31 19:01:01,701] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 19:01:03,570] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1700/tokenizer_config.json[0m
[32m[2022-08-31 19:01:03,570] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1700/special_tokens_map.json[0m
[32m[2022-08-31 19:01:09,287] [    INFO][0m - loss: 0.04325445, learning_rate: 2.7101694915254236e-05, global_step: 1710, interval_runtime: 16.1272, interval_samples_per_second: 0.496, interval_steps_per_second: 0.62, epoch: 9.661[0m
[32m[2022-08-31 19:01:10,555] [    INFO][0m - loss: 0.09932842, learning_rate: 2.7084745762711866e-05, global_step: 1720, interval_runtime: 1.2676, interval_samples_per_second: 6.311, interval_steps_per_second: 7.889, epoch: 9.7175[0m
[32m[2022-08-31 19:01:11,817] [    INFO][0m - loss: 0.24548054, learning_rate: 2.7067796610169492e-05, global_step: 1730, interval_runtime: 1.262, interval_samples_per_second: 6.339, interval_steps_per_second: 7.924, epoch: 9.774[0m
[32m[2022-08-31 19:01:13,113] [    INFO][0m - loss: 0.00091482, learning_rate: 2.7050847457627118e-05, global_step: 1740, interval_runtime: 1.2957, interval_samples_per_second: 6.174, interval_steps_per_second: 7.718, epoch: 9.8305[0m
[32m[2022-08-31 19:01:14,385] [    INFO][0m - loss: 0.31661406, learning_rate: 2.7033898305084747e-05, global_step: 1750, interval_runtime: 1.2723, interval_samples_per_second: 6.288, interval_steps_per_second: 7.859, epoch: 9.887[0m
[32m[2022-08-31 19:01:15,655] [    INFO][0m - loss: 0.05413143, learning_rate: 2.7016949152542376e-05, global_step: 1760, interval_runtime: 1.2698, interval_samples_per_second: 6.3, interval_steps_per_second: 7.875, epoch: 9.9435[0m
[32m[2022-08-31 19:01:16,881] [    INFO][0m - loss: 0.17529994, learning_rate: 2.7000000000000002e-05, global_step: 1770, interval_runtime: 1.2263, interval_samples_per_second: 6.524, interval_steps_per_second: 8.154, epoch: 10.0[0m
[32m[2022-08-31 19:01:18,219] [    INFO][0m - loss: 0.06590364, learning_rate: 2.698305084745763e-05, global_step: 1780, interval_runtime: 1.3381, interval_samples_per_second: 5.979, interval_steps_per_second: 7.474, epoch: 10.0565[0m
[32m[2022-08-31 19:01:19,487] [    INFO][0m - loss: 0.02342816, learning_rate: 2.6966101694915254e-05, global_step: 1790, interval_runtime: 1.2683, interval_samples_per_second: 6.308, interval_steps_per_second: 7.885, epoch: 10.113[0m
[32m[2022-08-31 19:01:20,759] [    INFO][0m - loss: 0.01432903, learning_rate: 2.6949152542372884e-05, global_step: 1800, interval_runtime: 1.2714, interval_samples_per_second: 6.292, interval_steps_per_second: 7.865, epoch: 10.1695[0m
[32m[2022-08-31 19:01:20,760] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 19:01:20,760] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-08-31 19:01:20,760] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 19:01:20,760] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 19:01:20,760] [    INFO][0m -   Total prediction steps = 45[0m
[32m[2022-08-31 19:01:29,365] [    INFO][0m - eval_loss: 1.4015988111495972, eval_accuracy: 0.4801980198019802, eval_runtime: 8.6042, eval_samples_per_second: 164.338, eval_steps_per_second: 5.23, epoch: 10.1695[0m
[32m[2022-08-31 19:01:29,366] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1800[0m
[32m[2022-08-31 19:01:29,366] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 19:01:31,619] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1800/tokenizer_config.json[0m
[32m[2022-08-31 19:01:31,620] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1800/special_tokens_map.json[0m
[32m[2022-08-31 19:01:37,370] [    INFO][0m - loss: 0.05068451, learning_rate: 2.693220338983051e-05, global_step: 1810, interval_runtime: 16.6109, interval_samples_per_second: 0.482, interval_steps_per_second: 0.602, epoch: 10.226[0m
[32m[2022-08-31 19:01:38,642] [    INFO][0m - loss: 0.23045683, learning_rate: 2.6915254237288136e-05, global_step: 1820, interval_runtime: 1.2719, interval_samples_per_second: 6.29, interval_steps_per_second: 7.862, epoch: 10.2825[0m
[32m[2022-08-31 19:01:39,915] [    INFO][0m - loss: 0.03919393, learning_rate: 2.689830508474576e-05, global_step: 1830, interval_runtime: 1.2727, interval_samples_per_second: 6.286, interval_steps_per_second: 7.857, epoch: 10.339[0m
[32m[2022-08-31 19:01:41,185] [    INFO][0m - loss: 0.05133284, learning_rate: 2.688135593220339e-05, global_step: 1840, interval_runtime: 1.2708, interval_samples_per_second: 6.295, interval_steps_per_second: 7.869, epoch: 10.3955[0m
[32m[2022-08-31 19:01:42,464] [    INFO][0m - loss: 0.16420946, learning_rate: 2.6864406779661017e-05, global_step: 1850, interval_runtime: 1.2739, interval_samples_per_second: 6.28, interval_steps_per_second: 7.85, epoch: 10.452[0m
[32m[2022-08-31 19:01:43,755] [    INFO][0m - loss: 0.11920959, learning_rate: 2.6847457627118643e-05, global_step: 1860, interval_runtime: 1.2953, interval_samples_per_second: 6.176, interval_steps_per_second: 7.72, epoch: 10.5085[0m
[32m[2022-08-31 19:01:45,035] [    INFO][0m - loss: 0.09927557, learning_rate: 2.6830508474576272e-05, global_step: 1870, interval_runtime: 1.2801, interval_samples_per_second: 6.249, interval_steps_per_second: 7.812, epoch: 10.565[0m
[32m[2022-08-31 19:01:46,301] [    INFO][0m - loss: 0.15081186, learning_rate: 2.68135593220339e-05, global_step: 1880, interval_runtime: 1.2663, interval_samples_per_second: 6.318, interval_steps_per_second: 7.897, epoch: 10.6215[0m
[32m[2022-08-31 19:01:47,571] [    INFO][0m - loss: 0.12426395, learning_rate: 2.6796610169491527e-05, global_step: 1890, interval_runtime: 1.27, interval_samples_per_second: 6.299, interval_steps_per_second: 7.874, epoch: 10.678[0m
[32m[2022-08-31 19:01:48,846] [    INFO][0m - loss: 0.09747754, learning_rate: 2.6779661016949153e-05, global_step: 1900, interval_runtime: 1.2746, interval_samples_per_second: 6.277, interval_steps_per_second: 7.846, epoch: 10.7345[0m
[32m[2022-08-31 19:01:48,846] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 19:01:48,847] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-08-31 19:01:48,847] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 19:01:48,847] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 19:01:48,847] [    INFO][0m -   Total prediction steps = 45[0m
[32m[2022-08-31 19:01:57,307] [    INFO][0m - eval_loss: 1.0365668535232544, eval_accuracy: 0.504950495049505, eval_runtime: 8.4602, eval_samples_per_second: 167.136, eval_steps_per_second: 5.319, epoch: 10.7345[0m
[32m[2022-08-31 19:01:57,308] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1900[0m
[32m[2022-08-31 19:01:57,308] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 19:01:59,324] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1900/tokenizer_config.json[0m
[32m[2022-08-31 19:01:59,325] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1900/special_tokens_map.json[0m
[32m[2022-08-31 19:02:05,152] [    INFO][0m - loss: 0.21716838, learning_rate: 2.676271186440678e-05, global_step: 1910, interval_runtime: 16.3059, interval_samples_per_second: 0.491, interval_steps_per_second: 0.613, epoch: 10.791[0m
[32m[2022-08-31 19:02:06,421] [    INFO][0m - loss: 0.00189395, learning_rate: 2.674576271186441e-05, global_step: 1920, interval_runtime: 1.2693, interval_samples_per_second: 6.303, interval_steps_per_second: 7.878, epoch: 10.8475[0m
[32m[2022-08-31 19:02:07,689] [    INFO][0m - loss: 0.20883286, learning_rate: 2.6728813559322035e-05, global_step: 1930, interval_runtime: 1.2685, interval_samples_per_second: 6.307, interval_steps_per_second: 7.884, epoch: 10.904[0m
[32m[2022-08-31 19:02:08,958] [    INFO][0m - loss: 0.1018567, learning_rate: 2.671186440677966e-05, global_step: 1940, interval_runtime: 1.2691, interval_samples_per_second: 6.304, interval_steps_per_second: 7.88, epoch: 10.9605[0m
[32m[2022-08-31 19:02:10,247] [    INFO][0m - loss: 0.09607537, learning_rate: 2.6694915254237287e-05, global_step: 1950, interval_runtime: 1.2886, interval_samples_per_second: 6.208, interval_steps_per_second: 7.76, epoch: 11.0169[0m
[32m[2022-08-31 19:02:11,515] [    INFO][0m - loss: 0.0011377, learning_rate: 2.6677966101694916e-05, global_step: 1960, interval_runtime: 1.2681, interval_samples_per_second: 6.309, interval_steps_per_second: 7.886, epoch: 11.0734[0m
[32m[2022-08-31 19:02:12,784] [    INFO][0m - loss: 0.08201163, learning_rate: 2.6661016949152542e-05, global_step: 1970, interval_runtime: 1.2687, interval_samples_per_second: 6.306, interval_steps_per_second: 7.882, epoch: 11.1299[0m
[32m[2022-08-31 19:02:14,050] [    INFO][0m - loss: 0.20690053, learning_rate: 2.664406779661017e-05, global_step: 1980, interval_runtime: 1.2666, interval_samples_per_second: 6.316, interval_steps_per_second: 7.895, epoch: 11.1864[0m
[32m[2022-08-31 19:02:15,321] [    INFO][0m - loss: 0.0011286, learning_rate: 2.6627118644067797e-05, global_step: 1990, interval_runtime: 1.2703, interval_samples_per_second: 6.298, interval_steps_per_second: 7.872, epoch: 11.2429[0m
[32m[2022-08-31 19:02:16,586] [    INFO][0m - loss: 0.00073258, learning_rate: 2.6610169491525427e-05, global_step: 2000, interval_runtime: 1.2647, interval_samples_per_second: 6.326, interval_steps_per_second: 7.907, epoch: 11.2994[0m
[32m[2022-08-31 19:02:16,587] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 19:02:16,587] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-08-31 19:02:16,587] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 19:02:16,587] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 19:02:16,588] [    INFO][0m -   Total prediction steps = 45[0m
[32m[2022-08-31 19:02:25,118] [    INFO][0m - eval_loss: 1.2131993770599365, eval_accuracy: 0.4603960396039604, eval_runtime: 8.5302, eval_samples_per_second: 165.763, eval_steps_per_second: 5.275, epoch: 11.2994[0m
[32m[2022-08-31 19:02:25,118] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2000[0m
[32m[2022-08-31 19:02:25,119] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 19:02:26,917] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2000/tokenizer_config.json[0m
[32m[2022-08-31 19:02:26,918] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2000/special_tokens_map.json[0m
[32m[2022-08-31 19:02:32,285] [    INFO][0m - loss: 0.046289, learning_rate: 2.6593220338983053e-05, global_step: 2010, interval_runtime: 15.6995, interval_samples_per_second: 0.51, interval_steps_per_second: 0.637, epoch: 11.3559[0m
[32m[2022-08-31 19:02:33,557] [    INFO][0m - loss: 0.19559959, learning_rate: 2.657627118644068e-05, global_step: 2020, interval_runtime: 1.2719, interval_samples_per_second: 6.29, interval_steps_per_second: 7.862, epoch: 11.4124[0m
[32m[2022-08-31 19:02:34,828] [    INFO][0m - loss: 0.16421012, learning_rate: 2.6559322033898304e-05, global_step: 2030, interval_runtime: 1.271, interval_samples_per_second: 6.294, interval_steps_per_second: 7.868, epoch: 11.4689[0m
[32m[2022-08-31 19:02:36,100] [    INFO][0m - loss: 0.01653557, learning_rate: 2.6542372881355934e-05, global_step: 2040, interval_runtime: 1.2718, interval_samples_per_second: 6.29, interval_steps_per_second: 7.863, epoch: 11.5254[0m
[32m[2022-08-31 19:02:37,367] [    INFO][0m - loss: 0.0016099, learning_rate: 2.652542372881356e-05, global_step: 2050, interval_runtime: 1.267, interval_samples_per_second: 6.314, interval_steps_per_second: 7.892, epoch: 11.5819[0m
[32m[2022-08-31 19:02:38,641] [    INFO][0m - loss: 0.12501698, learning_rate: 2.6508474576271186e-05, global_step: 2060, interval_runtime: 1.2746, interval_samples_per_second: 6.276, interval_steps_per_second: 7.846, epoch: 11.6384[0m
[32m[2022-08-31 19:02:39,936] [    INFO][0m - loss: 0.0808762, learning_rate: 2.649152542372881e-05, global_step: 2070, interval_runtime: 1.2942, interval_samples_per_second: 6.182, interval_steps_per_second: 7.727, epoch: 11.6949[0m
[32m[2022-08-31 19:02:41,204] [    INFO][0m - loss: 0.07768992, learning_rate: 2.647457627118644e-05, global_step: 2080, interval_runtime: 1.2684, interval_samples_per_second: 6.307, interval_steps_per_second: 7.884, epoch: 11.7514[0m
[32m[2022-08-31 19:02:42,481] [    INFO][0m - loss: 0.00097072, learning_rate: 2.645762711864407e-05, global_step: 2090, interval_runtime: 1.277, interval_samples_per_second: 6.265, interval_steps_per_second: 7.831, epoch: 11.8079[0m
[32m[2022-08-31 19:02:43,756] [    INFO][0m - loss: 0.16780212, learning_rate: 2.6440677966101696e-05, global_step: 2100, interval_runtime: 1.2743, interval_samples_per_second: 6.278, interval_steps_per_second: 7.847, epoch: 11.8644[0m
[32m[2022-08-31 19:02:43,757] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 19:02:43,757] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-08-31 19:02:43,757] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 19:02:43,757] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 19:02:43,757] [    INFO][0m -   Total prediction steps = 45[0m
[32m[2022-08-31 19:02:52,247] [    INFO][0m - eval_loss: 0.9240800142288208, eval_accuracy: 0.4603960396039604, eval_runtime: 8.49, eval_samples_per_second: 166.548, eval_steps_per_second: 5.3, epoch: 11.8644[0m
[32m[2022-08-31 19:02:52,248] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2100[0m
[32m[2022-08-31 19:02:52,248] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 19:02:54,095] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2100/tokenizer_config.json[0m
[32m[2022-08-31 19:02:54,095] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2100/special_tokens_map.json[0m
[32m[2022-08-31 19:02:59,663] [    INFO][0m - loss: 0.00445845, learning_rate: 2.6423728813559322e-05, global_step: 2110, interval_runtime: 15.9075, interval_samples_per_second: 0.503, interval_steps_per_second: 0.629, epoch: 11.9209[0m
[32m[2022-08-31 19:03:00,940] [    INFO][0m - loss: 0.07629554, learning_rate: 2.640677966101695e-05, global_step: 2120, interval_runtime: 1.2772, interval_samples_per_second: 6.264, interval_steps_per_second: 7.83, epoch: 11.9774[0m
[32m[2022-08-31 19:03:02,215] [    INFO][0m - loss: 0.00100109, learning_rate: 2.6389830508474578e-05, global_step: 2130, interval_runtime: 1.2745, interval_samples_per_second: 6.277, interval_steps_per_second: 7.846, epoch: 12.0339[0m
[32m[2022-08-31 19:03:03,479] [    INFO][0m - loss: 0.00070521, learning_rate: 2.6372881355932204e-05, global_step: 2140, interval_runtime: 1.2648, interval_samples_per_second: 6.325, interval_steps_per_second: 7.907, epoch: 12.0904[0m
[32m[2022-08-31 19:03:04,744] [    INFO][0m - loss: 0.07559968, learning_rate: 2.635593220338983e-05, global_step: 2150, interval_runtime: 1.2645, interval_samples_per_second: 6.326, interval_steps_per_second: 7.908, epoch: 12.1469[0m
[32m[2022-08-31 19:03:06,014] [    INFO][0m - loss: 0.00040619, learning_rate: 2.633898305084746e-05, global_step: 2160, interval_runtime: 1.27, interval_samples_per_second: 6.299, interval_steps_per_second: 7.874, epoch: 12.2034[0m
[32m[2022-08-31 19:03:07,321] [    INFO][0m - loss: 0.00037308, learning_rate: 2.6322033898305085e-05, global_step: 2170, interval_runtime: 1.3067, interval_samples_per_second: 6.122, interval_steps_per_second: 7.653, epoch: 12.2599[0m
[32m[2022-08-31 19:03:08,604] [    INFO][0m - loss: 0.00034618, learning_rate: 2.630508474576271e-05, global_step: 2180, interval_runtime: 1.2828, interval_samples_per_second: 6.236, interval_steps_per_second: 7.795, epoch: 12.3164[0m
[32m[2022-08-31 19:03:09,871] [    INFO][0m - loss: 0.00027181, learning_rate: 2.6288135593220337e-05, global_step: 2190, interval_runtime: 1.2678, interval_samples_per_second: 6.31, interval_steps_per_second: 7.888, epoch: 12.3729[0m
[32m[2022-08-31 19:03:11,142] [    INFO][0m - loss: 0.09749576, learning_rate: 2.627118644067797e-05, global_step: 2200, interval_runtime: 1.2706, interval_samples_per_second: 6.296, interval_steps_per_second: 7.87, epoch: 12.4294[0m
[32m[2022-08-31 19:03:11,143] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 19:03:11,143] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-08-31 19:03:11,143] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 19:03:11,143] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 19:03:11,143] [    INFO][0m -   Total prediction steps = 45[0m
[32m[2022-08-31 19:03:19,734] [    INFO][0m - eval_loss: 1.3111822605133057, eval_accuracy: 0.43564356435643564, eval_runtime: 8.5909, eval_samples_per_second: 164.592, eval_steps_per_second: 5.238, epoch: 12.4294[0m
[32m[2022-08-31 19:03:19,735] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2200[0m
[32m[2022-08-31 19:03:19,735] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 19:03:21,558] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2200/tokenizer_config.json[0m
[32m[2022-08-31 19:03:21,559] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2200/special_tokens_map.json[0m
[32m[2022-08-31 19:03:27,126] [    INFO][0m - loss: 0.08531412, learning_rate: 2.6254237288135595e-05, global_step: 2210, interval_runtime: 15.9842, interval_samples_per_second: 0.5, interval_steps_per_second: 0.626, epoch: 12.4859[0m
[32m[2022-08-31 19:03:28,391] [    INFO][0m - loss: 0.00072855, learning_rate: 2.623728813559322e-05, global_step: 2220, interval_runtime: 1.2644, interval_samples_per_second: 6.327, interval_steps_per_second: 7.909, epoch: 12.5424[0m
[32m[2022-08-31 19:03:29,665] [    INFO][0m - loss: 0.00108473, learning_rate: 2.6220338983050847e-05, global_step: 2230, interval_runtime: 1.2743, interval_samples_per_second: 6.278, interval_steps_per_second: 7.848, epoch: 12.5989[0m
[32m[2022-08-31 19:03:30,952] [    INFO][0m - loss: 0.07300424, learning_rate: 2.6203389830508477e-05, global_step: 2240, interval_runtime: 1.287, interval_samples_per_second: 6.216, interval_steps_per_second: 7.77, epoch: 12.6554[0m
[32m[2022-08-31 19:03:32,252] [    INFO][0m - loss: 0.00062613, learning_rate: 2.6186440677966103e-05, global_step: 2250, interval_runtime: 1.3001, interval_samples_per_second: 6.153, interval_steps_per_second: 7.692, epoch: 12.7119[0m
[32m[2022-08-31 19:03:33,527] [    INFO][0m - loss: 0.15844965, learning_rate: 2.616949152542373e-05, global_step: 2260, interval_runtime: 1.2754, interval_samples_per_second: 6.273, interval_steps_per_second: 7.841, epoch: 12.7684[0m
[32m[2022-08-31 19:03:34,804] [    INFO][0m - loss: 0.0010103, learning_rate: 2.6152542372881355e-05, global_step: 2270, interval_runtime: 1.276, interval_samples_per_second: 6.269, interval_steps_per_second: 7.837, epoch: 12.8249[0m
[32m[2022-08-31 19:03:36,073] [    INFO][0m - loss: 0.07915047, learning_rate: 2.6135593220338984e-05, global_step: 2280, interval_runtime: 1.2694, interval_samples_per_second: 6.302, interval_steps_per_second: 7.877, epoch: 12.8814[0m
[32m[2022-08-31 19:03:37,342] [    INFO][0m - loss: 0.03474925, learning_rate: 2.611864406779661e-05, global_step: 2290, interval_runtime: 1.2693, interval_samples_per_second: 6.303, interval_steps_per_second: 7.879, epoch: 12.9379[0m
[32m[2022-08-31 19:03:38,600] [    INFO][0m - loss: 0.00092316, learning_rate: 2.6101694915254236e-05, global_step: 2300, interval_runtime: 1.257, interval_samples_per_second: 6.364, interval_steps_per_second: 7.955, epoch: 12.9944[0m
[32m[2022-08-31 19:03:38,601] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 19:03:38,601] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-08-31 19:03:38,601] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 19:03:38,601] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 19:03:38,601] [    INFO][0m -   Total prediction steps = 45[0m
[32m[2022-08-31 19:03:47,146] [    INFO][0m - eval_loss: 1.2081319093704224, eval_accuracy: 0.47029702970297027, eval_runtime: 8.5451, eval_samples_per_second: 165.474, eval_steps_per_second: 5.266, epoch: 12.9944[0m
[32m[2022-08-31 19:03:47,147] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2300[0m
[32m[2022-08-31 19:03:47,147] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 19:03:49,323] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2300/tokenizer_config.json[0m
[32m[2022-08-31 19:03:49,324] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2300/special_tokens_map.json[0m
[32m[2022-08-31 19:03:55,466] [    INFO][0m - loss: 0.05474475, learning_rate: 2.6084745762711862e-05, global_step: 2310, interval_runtime: 16.8663, interval_samples_per_second: 0.474, interval_steps_per_second: 0.593, epoch: 13.0508[0m
[32m[2022-08-31 19:03:56,739] [    INFO][0m - loss: 0.01123856, learning_rate: 2.6067796610169495e-05, global_step: 2320, interval_runtime: 1.2739, interval_samples_per_second: 6.28, interval_steps_per_second: 7.85, epoch: 13.1073[0m
[32m[2022-08-31 19:03:58,008] [    INFO][0m - loss: 0.096883, learning_rate: 2.605084745762712e-05, global_step: 2330, interval_runtime: 1.2685, interval_samples_per_second: 6.307, interval_steps_per_second: 7.883, epoch: 13.1638[0m
[32m[2022-08-31 19:03:59,282] [    INFO][0m - loss: 0.00216534, learning_rate: 2.6033898305084746e-05, global_step: 2340, interval_runtime: 1.2743, interval_samples_per_second: 6.278, interval_steps_per_second: 7.848, epoch: 13.2203[0m
[32m[2022-08-31 19:04:00,563] [    INFO][0m - loss: 0.00081379, learning_rate: 2.6016949152542372e-05, global_step: 2350, interval_runtime: 1.2807, interval_samples_per_second: 6.246, interval_steps_per_second: 7.808, epoch: 13.2768[0m
[32m[2022-08-31 19:04:01,844] [    INFO][0m - loss: 0.04101442, learning_rate: 2.6000000000000002e-05, global_step: 2360, interval_runtime: 1.2813, interval_samples_per_second: 6.244, interval_steps_per_second: 7.804, epoch: 13.3333[0m
[32m[2022-08-31 19:04:03,110] [    INFO][0m - loss: 0.00049243, learning_rate: 2.5983050847457628e-05, global_step: 2370, interval_runtime: 1.2658, interval_samples_per_second: 6.32, interval_steps_per_second: 7.9, epoch: 13.3898[0m
[32m[2022-08-31 19:04:04,376] [    INFO][0m - loss: 0.00040864, learning_rate: 2.5966101694915254e-05, global_step: 2380, interval_runtime: 1.2649, interval_samples_per_second: 6.325, interval_steps_per_second: 7.906, epoch: 13.4463[0m
[32m[2022-08-31 19:04:05,648] [    INFO][0m - loss: 0.00091103, learning_rate: 2.594915254237288e-05, global_step: 2390, interval_runtime: 1.2727, interval_samples_per_second: 6.286, interval_steps_per_second: 7.857, epoch: 13.5028[0m
[32m[2022-08-31 19:04:06,919] [    INFO][0m - loss: 0.00027423, learning_rate: 2.593220338983051e-05, global_step: 2400, interval_runtime: 1.2713, interval_samples_per_second: 6.293, interval_steps_per_second: 7.866, epoch: 13.5593[0m
[32m[2022-08-31 19:04:06,920] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 19:04:06,920] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-08-31 19:04:06,920] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 19:04:06,920] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 19:04:06,920] [    INFO][0m -   Total prediction steps = 45[0m
[32m[2022-08-31 19:04:15,365] [    INFO][0m - eval_loss: 1.3506001234054565, eval_accuracy: 0.46534653465346537, eval_runtime: 8.4447, eval_samples_per_second: 167.443, eval_steps_per_second: 5.329, epoch: 13.5593[0m
[32m[2022-08-31 19:04:15,366] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2400[0m
[32m[2022-08-31 19:04:15,366] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 19:04:17,271] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2400/tokenizer_config.json[0m
[32m[2022-08-31 19:04:17,271] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2400/special_tokens_map.json[0m
[32m[2022-08-31 19:04:23,451] [    INFO][0m - loss: 0.00022648, learning_rate: 2.5915254237288135e-05, global_step: 2410, interval_runtime: 16.5312, interval_samples_per_second: 0.484, interval_steps_per_second: 0.605, epoch: 13.6158[0m
[32m[2022-08-31 19:04:24,722] [    INFO][0m - loss: 0.10211525, learning_rate: 2.589830508474576e-05, global_step: 2420, interval_runtime: 1.2712, interval_samples_per_second: 6.293, interval_steps_per_second: 7.867, epoch: 13.6723[0m
[32m[2022-08-31 19:04:25,991] [    INFO][0m - loss: 0.09457114, learning_rate: 2.5881355932203394e-05, global_step: 2430, interval_runtime: 1.27, interval_samples_per_second: 6.299, interval_steps_per_second: 7.874, epoch: 13.7288[0m
[32m[2022-08-31 19:04:27,273] [    INFO][0m - loss: 0.17777541, learning_rate: 2.586440677966102e-05, global_step: 2440, interval_runtime: 1.2813, interval_samples_per_second: 6.244, interval_steps_per_second: 7.804, epoch: 13.7853[0m
[32m[2022-08-31 19:04:28,544] [    INFO][0m - loss: 0.00311028, learning_rate: 2.5847457627118646e-05, global_step: 2450, interval_runtime: 1.2715, interval_samples_per_second: 6.292, interval_steps_per_second: 7.865, epoch: 13.8418[0m
[32m[2022-08-31 19:04:29,833] [    INFO][0m - loss: 0.00178784, learning_rate: 2.583050847457627e-05, global_step: 2460, interval_runtime: 1.2885, interval_samples_per_second: 6.209, interval_steps_per_second: 7.761, epoch: 13.8983[0m
[32m[2022-08-31 19:04:31,102] [    INFO][0m - loss: 0.07907425, learning_rate: 2.58135593220339e-05, global_step: 2470, interval_runtime: 1.2689, interval_samples_per_second: 6.305, interval_steps_per_second: 7.881, epoch: 13.9548[0m
[32m[2022-08-31 19:04:32,393] [    INFO][0m - loss: 0.00303356, learning_rate: 2.5796610169491527e-05, global_step: 2480, interval_runtime: 1.2906, interval_samples_per_second: 6.199, interval_steps_per_second: 7.748, epoch: 14.0113[0m
[32m[2022-08-31 19:04:33,660] [    INFO][0m - loss: 0.1214686, learning_rate: 2.5779661016949153e-05, global_step: 2490, interval_runtime: 1.2682, interval_samples_per_second: 6.308, interval_steps_per_second: 7.885, epoch: 14.0678[0m
[32m[2022-08-31 19:04:34,929] [    INFO][0m - loss: 0.02843051, learning_rate: 2.576271186440678e-05, global_step: 2500, interval_runtime: 1.2678, interval_samples_per_second: 6.31, interval_steps_per_second: 7.887, epoch: 14.1243[0m
[32m[2022-08-31 19:04:34,930] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 19:04:34,931] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-08-31 19:04:34,931] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 19:04:34,931] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 19:04:34,931] [    INFO][0m -   Total prediction steps = 45[0m
[32m[2022-08-31 19:04:43,477] [    INFO][0m - eval_loss: 1.14926016330719, eval_accuracy: 0.4801980198019802, eval_runtime: 8.5461, eval_samples_per_second: 165.456, eval_steps_per_second: 5.266, epoch: 14.1243[0m
[32m[2022-08-31 19:04:43,478] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2500[0m
[32m[2022-08-31 19:04:43,478] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 19:04:45,824] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2500/tokenizer_config.json[0m
[32m[2022-08-31 19:04:45,824] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2500/special_tokens_map.json[0m
[32m[2022-08-31 19:04:50,985] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-31 19:04:50,986] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-500 (score: 0.594059405940594).[0m
[32m[2022-08-31 19:04:51,755] [    INFO][0m - train_runtime: 712.7745, train_samples_per_second: 198.38, train_steps_per_second: 24.833, train_loss: 0.2615404532074928, epoch: 14.1243[0m
[32m[2022-08-31 19:04:51,757] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-31 19:04:51,757] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 19:04:53,680] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-31 19:04:53,680] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-31 19:04:53,681] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-31 19:04:53,682] [    INFO][0m -   epoch                    =    14.1243[0m
[32m[2022-08-31 19:04:53,682] [    INFO][0m -   train_loss               =     0.2615[0m
[32m[2022-08-31 19:04:53,682] [    INFO][0m -   train_runtime            = 0:11:52.77[0m
[32m[2022-08-31 19:04:53,682] [    INFO][0m -   train_samples_per_second =     198.38[0m
[32m[2022-08-31 19:04:53,682] [    INFO][0m -   train_steps_per_second   =     24.833[0m
[32m[2022-08-31 19:04:53,693] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-31 19:04:53,693] [    INFO][0m -   Num examples = 14014[0m
[32m[2022-08-31 19:04:53,693] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 19:04:53,693] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 19:04:53,693] [    INFO][0m -   Total prediction steps = 438[0m
[32m[2022-08-31 19:06:17,927] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-31 19:06:17,927] [    INFO][0m -   test_accuracy           =     0.5709[0m
[32m[2022-08-31 19:06:17,927] [    INFO][0m -   test_loss               =       0.41[0m
[32m[2022-08-31 19:06:17,928] [    INFO][0m -   test_runtime            = 0:01:24.23[0m
[32m[2022-08-31 19:06:17,928] [    INFO][0m -   test_samples_per_second =    166.371[0m
[32m[2022-08-31 19:06:17,928] [    INFO][0m -   test_steps_per_second   =        5.2[0m
[32m[2022-08-31 19:06:17,928] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-31 19:06:17,928] [    INFO][0m -   Num examples = 14000[0m
[32m[2022-08-31 19:06:17,928] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 19:06:17,928] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 19:06:17,928] [    INFO][0m -   Total prediction steps = 438[0m
[32m[2022-08-31 19:08:00,016] [    INFO][0m - Predictions for chidf saved to ./fewclue_submit_examples.[0m
run.sh: line 65: --model_name_or_path: command not found
 
==========
csl
==========
 
[33m[2022-08-31 19:08:04,394] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-31 19:08:04,394] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-31 19:08:04,395] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 19:08:04,395] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-31 19:08:04,395] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 19:08:04,395] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-31 19:08:04,395] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-31 19:08:04,395] [    INFO][0m - early_stop_patience           :20[0m
[32m[2022-08-31 19:08:04,395] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-31 19:08:04,395] [    INFO][0m - model_name_or_path            :roformer_v2_chinese_char_base[0m
[32m[2022-08-31 19:08:04,395] [    INFO][0m - [0m
[32m[2022-08-31 19:08:04,395] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 19:08:04,395] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-31 19:08:04,396] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 19:08:04,396] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-31 19:08:04,396] [    INFO][0m - prompt                        :{'text': 'text_a'}{'hard':'ä¸Šæ–‡ä¸­æ‰¾'}{'mask'}{'hard': 'å‡ºè¿™äº›å…³é”®è¯ï¼š'}{'text':'text_b'}[0m
[32m[2022-08-31 19:08:04,396] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-31 19:08:04,396] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-31 19:08:04,396] [    INFO][0m - task_name                     :csl[0m
[32m[2022-08-31 19:08:04,396] [    INFO][0m - [0m
[32m[2022-08-31 19:08:04,396] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/roformer_v2_chinese_char_base/model_state.pdparams[0m
W0831 19:08:04.397711 14579 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0831 19:08:04.402446 14579 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-31 19:08:07,464] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/roformer_v2_chinese_char_base/vocab.txt[0m
[32m[2022-08-31 19:08:07,471] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/roformer_v2_chinese_char_base/tokenizer_config.json[0m
[32m[2022-08-31 19:08:07,472] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/roformer_v2_chinese_char_base/special_tokens_map.json[0m
[32m[2022-08-31 19:08:07,472] [    INFO][0m - Using template: [{'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': 'ä¸Šæ–‡ä¸­æ‰¾'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'å‡ºè¿™äº›å…³é”®è¯ï¼š'}, {'add_prefix_space': '', 'text': 'text_b'}][0m
2022-08-31 19:08:07,479 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-31 19:08:07,643] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 19:08:07,644] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-31 19:08:07,644] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 19:08:07,644] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-31 19:08:07,644] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-31 19:08:07,644] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-31 19:08:07,644] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-31 19:08:07,644] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-31 19:08:07,644] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-31 19:08:07,645] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-31 19:08:07,645] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-31 19:08:07,645] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-31 19:08:07,645] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-31 19:08:07,645] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-31 19:08:07,645] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-31 19:08:07,645] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-31 19:08:07,645] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-31 19:08:07,645] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-31 19:08:07,645] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-31 19:08:07,645] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-31 19:08:07,645] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-31 19:08:07,645] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-31 19:08:07,646] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-31 19:08:07,646] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-31 19:08:07,646] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-31 19:08:07,646] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-31 19:08:07,646] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-31 19:08:07,646] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-31 19:08:07,646] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-31 19:08:07,646] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-31 19:08:07,646] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-31 19:08:07,646] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-31 19:08:07,646] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-31 19:08:07,646] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-31 19:08:07,646] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-31 19:08:07,647] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-31 19:08:07,647] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-31 19:08:07,647] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug31_19-08-04_instance-3bwob41y-01[0m
[32m[2022-08-31 19:08:07,647] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-31 19:08:07,647] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-31 19:08:07,647] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-31 19:08:07,647] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-31 19:08:07,647] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-31 19:08:07,647] [    INFO][0m - max_seq_length                :320[0m
[32m[2022-08-31 19:08:07,647] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-31 19:08:07,647] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-31 19:08:07,647] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-31 19:08:07,647] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-31 19:08:07,647] [    INFO][0m - num_train_epochs              :100.0[0m
[32m[2022-08-31 19:08:07,648] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-31 19:08:07,648] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-31 19:08:07,648] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-31 19:08:07,648] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-31 19:08:07,648] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-31 19:08:07,648] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-31 19:08:07,648] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-31 19:08:07,648] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-31 19:08:07,648] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-31 19:08:07,648] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-31 19:08:07,648] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-31 19:08:07,648] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-31 19:08:07,648] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-31 19:08:07,649] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-31 19:08:07,649] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-31 19:08:07,649] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-31 19:08:07,649] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-31 19:08:07,649] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-31 19:08:07,649] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-31 19:08:07,649] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-31 19:08:07,649] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-31 19:08:07,649] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-31 19:08:07,649] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-31 19:08:07,649] [    INFO][0m - seed                          :42[0m
[32m[2022-08-31 19:08:07,649] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-31 19:08:07,649] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-31 19:08:07,649] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-31 19:08:07,650] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-31 19:08:07,650] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-31 19:08:07,650] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-31 19:08:07,650] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-31 19:08:07,650] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-31 19:08:07,650] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-31 19:08:07,650] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-31 19:08:07,650] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-31 19:08:07,650] [    INFO][0m - [0m
[32m[2022-08-31 19:08:07,652] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-31 19:08:07,652] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 19:08:07,652] [    INFO][0m -   Num Epochs = 100[0m
[32m[2022-08-31 19:08:07,652] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-31 19:08:07,652] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-31 19:08:07,652] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-31 19:08:07,652] [    INFO][0m -   Total optimization steps = 2000.0[0m
[32m[2022-08-31 19:08:07,652] [    INFO][0m -   Total num train samples = 16000[0m
[32m[2022-08-31 19:08:09,958] [    INFO][0m - loss: 0.95018559, learning_rate: 2.985e-05, global_step: 10, interval_runtime: 2.3036, interval_samples_per_second: 3.473, interval_steps_per_second: 4.341, epoch: 0.5[0m
[32m[2022-08-31 19:08:11,565] [    INFO][0m - loss: 0.97467775, learning_rate: 2.97e-05, global_step: 20, interval_runtime: 1.6078, interval_samples_per_second: 4.976, interval_steps_per_second: 6.22, epoch: 1.0[0m
[32m[2022-08-31 19:08:13,210] [    INFO][0m - loss: 0.62396412, learning_rate: 2.955e-05, global_step: 30, interval_runtime: 1.6455, interval_samples_per_second: 4.862, interval_steps_per_second: 6.077, epoch: 1.5[0m
[32m[2022-08-31 19:08:14,745] [    INFO][0m - loss: 0.80947418, learning_rate: 2.94e-05, global_step: 40, interval_runtime: 1.5352, interval_samples_per_second: 5.211, interval_steps_per_second: 6.514, epoch: 2.0[0m
[32m[2022-08-31 19:08:16,417] [    INFO][0m - loss: 0.63252335, learning_rate: 2.925e-05, global_step: 50, interval_runtime: 1.6714, interval_samples_per_second: 4.786, interval_steps_per_second: 5.983, epoch: 2.5[0m
[32m[2022-08-31 19:08:17,947] [    INFO][0m - loss: 0.59678426, learning_rate: 2.91e-05, global_step: 60, interval_runtime: 1.5298, interval_samples_per_second: 5.229, interval_steps_per_second: 6.537, epoch: 3.0[0m
[32m[2022-08-31 19:08:19,606] [    INFO][0m - loss: 0.40669537, learning_rate: 2.895e-05, global_step: 70, interval_runtime: 1.6591, interval_samples_per_second: 4.822, interval_steps_per_second: 6.027, epoch: 3.5[0m
[32m[2022-08-31 19:08:21,138] [    INFO][0m - loss: 0.32743127, learning_rate: 2.88e-05, global_step: 80, interval_runtime: 1.5326, interval_samples_per_second: 5.22, interval_steps_per_second: 6.525, epoch: 4.0[0m
[32m[2022-08-31 19:08:22,793] [    INFO][0m - loss: 0.18119012, learning_rate: 2.865e-05, global_step: 90, interval_runtime: 1.6538, interval_samples_per_second: 4.837, interval_steps_per_second: 6.047, epoch: 4.5[0m
[32m[2022-08-31 19:08:24,332] [    INFO][0m - loss: 0.26949563, learning_rate: 2.8499999999999998e-05, global_step: 100, interval_runtime: 1.5396, interval_samples_per_second: 5.196, interval_steps_per_second: 6.495, epoch: 5.0[0m
[32m[2022-08-31 19:08:24,333] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 19:08:24,333] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 19:08:24,333] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 19:08:24,333] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 19:08:24,333] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 19:08:25,715] [    INFO][0m - eval_loss: 1.5226414203643799, eval_accuracy: 0.7, eval_runtime: 1.3821, eval_samples_per_second: 115.766, eval_steps_per_second: 3.618, epoch: 5.0[0m
[32m[2022-08-31 19:08:25,716] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-31 19:08:25,716] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 19:08:27,634] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-31 19:08:27,635] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-31 19:08:33,924] [    INFO][0m - loss: 0.05285782, learning_rate: 2.8349999999999998e-05, global_step: 110, interval_runtime: 9.5918, interval_samples_per_second: 0.834, interval_steps_per_second: 1.043, epoch: 5.5[0m
[32m[2022-08-31 19:08:35,465] [    INFO][0m - loss: 0.49462624, learning_rate: 2.8199999999999998e-05, global_step: 120, interval_runtime: 1.5408, interval_samples_per_second: 5.192, interval_steps_per_second: 6.49, epoch: 6.0[0m
[32m[2022-08-31 19:08:37,151] [    INFO][0m - loss: 0.28290222, learning_rate: 2.805e-05, global_step: 130, interval_runtime: 1.6866, interval_samples_per_second: 4.743, interval_steps_per_second: 5.929, epoch: 6.5[0m
[32m[2022-08-31 19:08:38,694] [    INFO][0m - loss: 0.04733637, learning_rate: 2.79e-05, global_step: 140, interval_runtime: 1.5424, interval_samples_per_second: 5.187, interval_steps_per_second: 6.483, epoch: 7.0[0m
[32m[2022-08-31 19:08:40,357] [    INFO][0m - loss: 0.00216324, learning_rate: 2.7750000000000004e-05, global_step: 150, interval_runtime: 1.6631, interval_samples_per_second: 4.81, interval_steps_per_second: 6.013, epoch: 7.5[0m
[32m[2022-08-31 19:08:41,896] [    INFO][0m - loss: 0.04961397, learning_rate: 2.7600000000000003e-05, global_step: 160, interval_runtime: 1.5389, interval_samples_per_second: 5.198, interval_steps_per_second: 6.498, epoch: 8.0[0m
[32m[2022-08-31 19:08:43,586] [    INFO][0m - loss: 0.2458128, learning_rate: 2.7450000000000003e-05, global_step: 170, interval_runtime: 1.6909, interval_samples_per_second: 4.731, interval_steps_per_second: 5.914, epoch: 8.5[0m
[32m[2022-08-31 19:08:45,130] [    INFO][0m - loss: 0.0090292, learning_rate: 2.7300000000000003e-05, global_step: 180, interval_runtime: 1.5433, interval_samples_per_second: 5.184, interval_steps_per_second: 6.48, epoch: 9.0[0m
[32m[2022-08-31 19:08:46,793] [    INFO][0m - loss: 0.00093005, learning_rate: 2.7150000000000003e-05, global_step: 190, interval_runtime: 1.6629, interval_samples_per_second: 4.811, interval_steps_per_second: 6.013, epoch: 9.5[0m
[32m[2022-08-31 19:08:48,331] [    INFO][0m - loss: 0.02566539, learning_rate: 2.7000000000000002e-05, global_step: 200, interval_runtime: 1.538, interval_samples_per_second: 5.202, interval_steps_per_second: 6.502, epoch: 10.0[0m
[32m[2022-08-31 19:08:48,331] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 19:08:48,332] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 19:08:48,332] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 19:08:48,332] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 19:08:48,332] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 19:08:49,723] [    INFO][0m - eval_loss: 2.5939784049987793, eval_accuracy: 0.68125, eval_runtime: 1.3912, eval_samples_per_second: 115.012, eval_steps_per_second: 3.594, epoch: 10.0[0m
[32m[2022-08-31 19:08:49,724] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-31 19:08:49,724] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 19:08:51,651] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-31 19:08:51,651] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-31 19:08:57,782] [    INFO][0m - loss: 3.202e-05, learning_rate: 2.6850000000000002e-05, global_step: 210, interval_runtime: 9.4512, interval_samples_per_second: 0.846, interval_steps_per_second: 1.058, epoch: 10.5[0m
[32m[2022-08-31 19:08:59,313] [    INFO][0m - loss: 0.00025421, learning_rate: 2.6700000000000002e-05, global_step: 220, interval_runtime: 1.5311, interval_samples_per_second: 5.225, interval_steps_per_second: 6.531, epoch: 11.0[0m
[32m[2022-08-31 19:09:00,972] [    INFO][0m - loss: 7.54e-06, learning_rate: 2.655e-05, global_step: 230, interval_runtime: 1.6588, interval_samples_per_second: 4.823, interval_steps_per_second: 6.028, epoch: 11.5[0m
[32m[2022-08-31 19:09:02,517] [    INFO][0m - loss: 0.00473937, learning_rate: 2.64e-05, global_step: 240, interval_runtime: 1.5446, interval_samples_per_second: 5.179, interval_steps_per_second: 6.474, epoch: 12.0[0m
[32m[2022-08-31 19:09:04,202] [    INFO][0m - loss: 1.129e-05, learning_rate: 2.625e-05, global_step: 250, interval_runtime: 1.6859, interval_samples_per_second: 4.745, interval_steps_per_second: 5.932, epoch: 12.5[0m
[32m[2022-08-31 19:09:05,727] [    INFO][0m - loss: 0.00177652, learning_rate: 2.61e-05, global_step: 260, interval_runtime: 1.5246, interval_samples_per_second: 5.247, interval_steps_per_second: 6.559, epoch: 13.0[0m
[32m[2022-08-31 19:09:07,373] [    INFO][0m - loss: 4.49e-06, learning_rate: 2.595e-05, global_step: 270, interval_runtime: 1.6462, interval_samples_per_second: 4.86, interval_steps_per_second: 6.075, epoch: 13.5[0m
[32m[2022-08-31 19:09:08,903] [    INFO][0m - loss: 0.1152583, learning_rate: 2.58e-05, global_step: 280, interval_runtime: 1.5295, interval_samples_per_second: 5.23, interval_steps_per_second: 6.538, epoch: 14.0[0m
[32m[2022-08-31 19:09:10,555] [    INFO][0m - loss: 3.01e-06, learning_rate: 2.565e-05, global_step: 290, interval_runtime: 1.6519, interval_samples_per_second: 4.843, interval_steps_per_second: 6.054, epoch: 14.5[0m
[32m[2022-08-31 19:09:12,094] [    INFO][0m - loss: 0.01757616, learning_rate: 2.55e-05, global_step: 300, interval_runtime: 1.5391, interval_samples_per_second: 5.198, interval_steps_per_second: 6.497, epoch: 15.0[0m
[32m[2022-08-31 19:09:12,095] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 19:09:12,095] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 19:09:12,095] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 19:09:12,095] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 19:09:12,095] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 19:09:13,483] [    INFO][0m - eval_loss: 2.2571938037872314, eval_accuracy: 0.7, eval_runtime: 1.3872, eval_samples_per_second: 115.339, eval_steps_per_second: 3.604, epoch: 15.0[0m
[32m[2022-08-31 19:09:13,483] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-31 19:09:13,484] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 19:09:15,727] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-31 19:09:15,728] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-31 19:09:22,173] [    INFO][0m - loss: 0.00014667, learning_rate: 2.535e-05, global_step: 310, interval_runtime: 10.0793, interval_samples_per_second: 0.794, interval_steps_per_second: 0.992, epoch: 15.5[0m
[32m[2022-08-31 19:09:23,705] [    INFO][0m - loss: 2.5e-06, learning_rate: 2.52e-05, global_step: 320, interval_runtime: 1.5323, interval_samples_per_second: 5.221, interval_steps_per_second: 6.526, epoch: 16.0[0m
[32m[2022-08-31 19:09:25,387] [    INFO][0m - loss: 3.91e-06, learning_rate: 2.505e-05, global_step: 330, interval_runtime: 1.6813, interval_samples_per_second: 4.758, interval_steps_per_second: 5.948, epoch: 16.5[0m
[32m[2022-08-31 19:09:26,917] [    INFO][0m - loss: 8.36e-06, learning_rate: 2.49e-05, global_step: 340, interval_runtime: 1.5302, interval_samples_per_second: 5.228, interval_steps_per_second: 6.535, epoch: 17.0[0m
[32m[2022-08-31 19:09:28,564] [    INFO][0m - loss: 6.795e-05, learning_rate: 2.475e-05, global_step: 350, interval_runtime: 1.6473, interval_samples_per_second: 4.856, interval_steps_per_second: 6.071, epoch: 17.5[0m
[32m[2022-08-31 19:09:30,108] [    INFO][0m - loss: 0.00010577, learning_rate: 2.4599999999999998e-05, global_step: 360, interval_runtime: 1.5437, interval_samples_per_second: 5.182, interval_steps_per_second: 6.478, epoch: 18.0[0m
[32m[2022-08-31 19:09:31,771] [    INFO][0m - loss: 2.54e-06, learning_rate: 2.4449999999999998e-05, global_step: 370, interval_runtime: 1.6626, interval_samples_per_second: 4.812, interval_steps_per_second: 6.015, epoch: 18.5[0m
[32m[2022-08-31 19:09:33,301] [    INFO][0m - loss: 0.0002167, learning_rate: 2.43e-05, global_step: 380, interval_runtime: 1.5309, interval_samples_per_second: 5.226, interval_steps_per_second: 6.532, epoch: 19.0[0m
[32m[2022-08-31 19:09:34,950] [    INFO][0m - loss: 1.8e-07, learning_rate: 2.415e-05, global_step: 390, interval_runtime: 1.6493, interval_samples_per_second: 4.851, interval_steps_per_second: 6.063, epoch: 19.5[0m
[32m[2022-08-31 19:09:36,482] [    INFO][0m - loss: 3.57e-06, learning_rate: 2.4e-05, global_step: 400, interval_runtime: 1.5314, interval_samples_per_second: 5.224, interval_steps_per_second: 6.53, epoch: 20.0[0m
[32m[2022-08-31 19:09:36,483] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 19:09:36,483] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 19:09:36,483] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 19:09:36,483] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 19:09:36,483] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 19:09:37,891] [    INFO][0m - eval_loss: 4.379105091094971, eval_accuracy: 0.675, eval_runtime: 1.4082, eval_samples_per_second: 113.619, eval_steps_per_second: 3.551, epoch: 20.0[0m
[32m[2022-08-31 19:09:37,892] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-31 19:09:37,892] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 19:09:39,765] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-31 19:09:39,766] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-31 19:09:45,564] [    INFO][0m - loss: 0.00340546, learning_rate: 2.385e-05, global_step: 410, interval_runtime: 9.0816, interval_samples_per_second: 0.881, interval_steps_per_second: 1.101, epoch: 20.5[0m
[32m[2022-08-31 19:09:47,106] [    INFO][0m - loss: 2.56e-06, learning_rate: 2.37e-05, global_step: 420, interval_runtime: 1.5424, interval_samples_per_second: 5.187, interval_steps_per_second: 6.484, epoch: 21.0[0m
[32m[2022-08-31 19:09:48,767] [    INFO][0m - loss: 0.00014863, learning_rate: 2.3550000000000003e-05, global_step: 430, interval_runtime: 1.6606, interval_samples_per_second: 4.817, interval_steps_per_second: 6.022, epoch: 21.5[0m
[32m[2022-08-31 19:09:50,294] [    INFO][0m - loss: 5.52e-06, learning_rate: 2.3400000000000003e-05, global_step: 440, interval_runtime: 1.528, interval_samples_per_second: 5.236, interval_steps_per_second: 6.544, epoch: 22.0[0m
[32m[2022-08-31 19:09:51,966] [    INFO][0m - loss: 3.101e-05, learning_rate: 2.3250000000000003e-05, global_step: 450, interval_runtime: 1.6711, interval_samples_per_second: 4.787, interval_steps_per_second: 5.984, epoch: 22.5[0m
[32m[2022-08-31 19:09:53,518] [    INFO][0m - loss: 8.07e-06, learning_rate: 2.3100000000000002e-05, global_step: 460, interval_runtime: 1.5526, interval_samples_per_second: 5.153, interval_steps_per_second: 6.441, epoch: 23.0[0m
[32m[2022-08-31 19:09:55,170] [    INFO][0m - loss: 1.8e-07, learning_rate: 2.2950000000000002e-05, global_step: 470, interval_runtime: 1.652, interval_samples_per_second: 4.843, interval_steps_per_second: 6.053, epoch: 23.5[0m
[32m[2022-08-31 19:09:56,711] [    INFO][0m - loss: 1.1e-07, learning_rate: 2.2800000000000002e-05, global_step: 480, interval_runtime: 1.5408, interval_samples_per_second: 5.192, interval_steps_per_second: 6.49, epoch: 24.0[0m
[32m[2022-08-31 19:09:58,396] [    INFO][0m - loss: 3.3e-07, learning_rate: 2.265e-05, global_step: 490, interval_runtime: 1.6848, interval_samples_per_second: 4.748, interval_steps_per_second: 5.935, epoch: 24.5[0m
[32m[2022-08-31 19:09:59,941] [    INFO][0m - loss: 3.5e-07, learning_rate: 2.25e-05, global_step: 500, interval_runtime: 1.5452, interval_samples_per_second: 5.177, interval_steps_per_second: 6.472, epoch: 25.0[0m
[32m[2022-08-31 19:09:59,942] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 19:09:59,943] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 19:09:59,943] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 19:09:59,943] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 19:09:59,943] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 19:10:01,319] [    INFO][0m - eval_loss: 3.6144955158233643, eval_accuracy: 0.7, eval_runtime: 1.3757, eval_samples_per_second: 116.308, eval_steps_per_second: 3.635, epoch: 25.0[0m
[32m[2022-08-31 19:10:01,320] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-08-31 19:10:01,320] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 19:10:03,228] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-08-31 19:10:03,229] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-08-31 19:10:09,410] [    INFO][0m - loss: 7.7e-07, learning_rate: 2.235e-05, global_step: 510, interval_runtime: 9.4685, interval_samples_per_second: 0.845, interval_steps_per_second: 1.056, epoch: 25.5[0m
[32m[2022-08-31 19:10:10,946] [    INFO][0m - loss: 2.3e-07, learning_rate: 2.22e-05, global_step: 520, interval_runtime: 1.5358, interval_samples_per_second: 5.209, interval_steps_per_second: 6.511, epoch: 26.0[0m
[32m[2022-08-31 19:10:12,605] [    INFO][0m - loss: 8e-08, learning_rate: 2.205e-05, global_step: 530, interval_runtime: 1.6596, interval_samples_per_second: 4.82, interval_steps_per_second: 6.025, epoch: 26.5[0m
[32m[2022-08-31 19:10:14,139] [    INFO][0m - loss: 7e-08, learning_rate: 2.19e-05, global_step: 540, interval_runtime: 1.5336, interval_samples_per_second: 5.216, interval_steps_per_second: 6.521, epoch: 27.0[0m
[32m[2022-08-31 19:10:15,804] [    INFO][0m - loss: 5.1e-07, learning_rate: 2.175e-05, global_step: 550, interval_runtime: 1.6651, interval_samples_per_second: 4.804, interval_steps_per_second: 6.006, epoch: 27.5[0m
[32m[2022-08-31 19:10:17,340] [    INFO][0m - loss: 6.8e-07, learning_rate: 2.16e-05, global_step: 560, interval_runtime: 1.5357, interval_samples_per_second: 5.209, interval_steps_per_second: 6.512, epoch: 28.0[0m
[32m[2022-08-31 19:10:19,003] [    INFO][0m - loss: 2e-07, learning_rate: 2.145e-05, global_step: 570, interval_runtime: 1.6633, interval_samples_per_second: 4.81, interval_steps_per_second: 6.012, epoch: 28.5[0m
[32m[2022-08-31 19:10:20,557] [    INFO][0m - loss: 1.6e-07, learning_rate: 2.13e-05, global_step: 580, interval_runtime: 1.553, interval_samples_per_second: 5.151, interval_steps_per_second: 6.439, epoch: 29.0[0m
[32m[2022-08-31 19:10:22,228] [    INFO][0m - loss: 5e-08, learning_rate: 2.115e-05, global_step: 590, interval_runtime: 1.6722, interval_samples_per_second: 4.784, interval_steps_per_second: 5.98, epoch: 29.5[0m
[32m[2022-08-31 19:10:23,770] [    INFO][0m - loss: 5e-08, learning_rate: 2.1e-05, global_step: 600, interval_runtime: 1.5423, interval_samples_per_second: 5.187, interval_steps_per_second: 6.484, epoch: 30.0[0m
[32m[2022-08-31 19:10:23,771] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 19:10:23,771] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 19:10:23,771] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 19:10:23,771] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 19:10:23,771] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 19:10:25,154] [    INFO][0m - eval_loss: 3.6350162029266357, eval_accuracy: 0.7, eval_runtime: 1.3814, eval_samples_per_second: 115.828, eval_steps_per_second: 3.62, epoch: 30.0[0m
[32m[2022-08-31 19:10:25,155] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-08-31 19:10:25,155] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 19:10:27,086] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-08-31 19:10:27,087] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-08-31 19:10:33,311] [    INFO][0m - loss: 9.5e-07, learning_rate: 2.085e-05, global_step: 610, interval_runtime: 9.5403, interval_samples_per_second: 0.839, interval_steps_per_second: 1.048, epoch: 30.5[0m
[32m[2022-08-31 19:10:34,842] [    INFO][0m - loss: 6.7e-07, learning_rate: 2.07e-05, global_step: 620, interval_runtime: 1.5316, interval_samples_per_second: 5.223, interval_steps_per_second: 6.529, epoch: 31.0[0m
[32m[2022-08-31 19:10:36,503] [    INFO][0m - loss: 1.4e-07, learning_rate: 2.055e-05, global_step: 630, interval_runtime: 1.6604, interval_samples_per_second: 4.818, interval_steps_per_second: 6.023, epoch: 31.5[0m
[32m[2022-08-31 19:10:38,032] [    INFO][0m - loss: 2.3e-07, learning_rate: 2.04e-05, global_step: 640, interval_runtime: 1.5289, interval_samples_per_second: 5.233, interval_steps_per_second: 6.541, epoch: 32.0[0m
[32m[2022-08-31 19:10:39,717] [    INFO][0m - loss: 2e-07, learning_rate: 2.025e-05, global_step: 650, interval_runtime: 1.685, interval_samples_per_second: 4.748, interval_steps_per_second: 5.935, epoch: 32.5[0m
[32m[2022-08-31 19:10:41,256] [    INFO][0m - loss: 4e-08, learning_rate: 2.01e-05, global_step: 660, interval_runtime: 1.5391, interval_samples_per_second: 5.198, interval_steps_per_second: 6.497, epoch: 33.0[0m
[32m[2022-08-31 19:10:42,911] [    INFO][0m - loss: 6e-08, learning_rate: 1.995e-05, global_step: 670, interval_runtime: 1.6551, interval_samples_per_second: 4.834, interval_steps_per_second: 6.042, epoch: 33.5[0m
[32m[2022-08-31 19:10:44,451] [    INFO][0m - loss: 8.41e-06, learning_rate: 1.98e-05, global_step: 680, interval_runtime: 1.54, interval_samples_per_second: 5.195, interval_steps_per_second: 6.493, epoch: 34.0[0m
[32m[2022-08-31 19:10:46,115] [    INFO][0m - loss: 9e-08, learning_rate: 1.965e-05, global_step: 690, interval_runtime: 1.6635, interval_samples_per_second: 4.809, interval_steps_per_second: 6.012, epoch: 34.5[0m
[32m[2022-08-31 19:10:47,648] [    INFO][0m - loss: 2.1e-07, learning_rate: 1.95e-05, global_step: 700, interval_runtime: 1.5336, interval_samples_per_second: 5.217, interval_steps_per_second: 6.521, epoch: 35.0[0m
[32m[2022-08-31 19:10:47,649] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 19:10:47,649] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 19:10:47,649] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 19:10:47,649] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 19:10:47,649] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 19:10:49,031] [    INFO][0m - eval_loss: 3.7750401496887207, eval_accuracy: 0.6875, eval_runtime: 1.3815, eval_samples_per_second: 115.818, eval_steps_per_second: 3.619, epoch: 35.0[0m
[32m[2022-08-31 19:10:49,031] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-700[0m
[32m[2022-08-31 19:10:49,032] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 19:10:50,964] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-700/tokenizer_config.json[0m
[32m[2022-08-31 19:10:50,965] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-700/special_tokens_map.json[0m
[32m[2022-08-31 19:10:56,927] [    INFO][0m - loss: 7e-08, learning_rate: 1.935e-05, global_step: 710, interval_runtime: 9.2793, interval_samples_per_second: 0.862, interval_steps_per_second: 1.078, epoch: 35.5[0m
[32m[2022-08-31 19:10:58,470] [    INFO][0m - loss: 7e-08, learning_rate: 1.9200000000000003e-05, global_step: 720, interval_runtime: 1.5422, interval_samples_per_second: 5.187, interval_steps_per_second: 6.484, epoch: 36.0[0m
[32m[2022-08-31 19:11:00,122] [    INFO][0m - loss: 3e-08, learning_rate: 1.9050000000000002e-05, global_step: 730, interval_runtime: 1.653, interval_samples_per_second: 4.84, interval_steps_per_second: 6.05, epoch: 36.5[0m
[32m[2022-08-31 19:11:01,660] [    INFO][0m - loss: 1e-07, learning_rate: 1.8900000000000002e-05, global_step: 740, interval_runtime: 1.5374, interval_samples_per_second: 5.204, interval_steps_per_second: 6.505, epoch: 37.0[0m
[32m[2022-08-31 19:11:03,318] [    INFO][0m - loss: 4e-08, learning_rate: 1.8750000000000002e-05, global_step: 750, interval_runtime: 1.6579, interval_samples_per_second: 4.825, interval_steps_per_second: 6.032, epoch: 37.5[0m
[32m[2022-08-31 19:11:04,853] [    INFO][0m - loss: 6e-08, learning_rate: 1.86e-05, global_step: 760, interval_runtime: 1.5347, interval_samples_per_second: 5.213, interval_steps_per_second: 6.516, epoch: 38.0[0m
[32m[2022-08-31 19:11:06,514] [    INFO][0m - loss: 1.22e-06, learning_rate: 1.845e-05, global_step: 770, interval_runtime: 1.6614, interval_samples_per_second: 4.815, interval_steps_per_second: 6.019, epoch: 38.5[0m
[32m[2022-08-31 19:11:08,044] [    INFO][0m - loss: 9e-08, learning_rate: 1.83e-05, global_step: 780, interval_runtime: 1.5306, interval_samples_per_second: 5.227, interval_steps_per_second: 6.533, epoch: 39.0[0m
[32m[2022-08-31 19:11:09,700] [    INFO][0m - loss: 3e-08, learning_rate: 1.815e-05, global_step: 790, interval_runtime: 1.6556, interval_samples_per_second: 4.832, interval_steps_per_second: 6.04, epoch: 39.5[0m
[32m[2022-08-31 19:11:11,230] [    INFO][0m - loss: 4.7e-07, learning_rate: 1.8e-05, global_step: 800, interval_runtime: 1.5295, interval_samples_per_second: 5.23, interval_steps_per_second: 6.538, epoch: 40.0[0m
[32m[2022-08-31 19:11:11,230] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 19:11:11,230] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 19:11:11,231] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 19:11:11,231] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 19:11:11,231] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 19:11:12,746] [    INFO][0m - eval_loss: 3.7673752307891846, eval_accuracy: 0.69375, eval_runtime: 1.5154, eval_samples_per_second: 105.583, eval_steps_per_second: 3.299, epoch: 40.0[0m
[32m[2022-08-31 19:11:12,747] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-800[0m
[32m[2022-08-31 19:11:12,747] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 19:11:14,734] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-800/tokenizer_config.json[0m
[32m[2022-08-31 19:11:14,735] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-800/special_tokens_map.json[0m
[32m[2022-08-31 19:11:21,198] [    INFO][0m - loss: 3e-08, learning_rate: 1.785e-05, global_step: 810, interval_runtime: 9.9686, interval_samples_per_second: 0.803, interval_steps_per_second: 1.003, epoch: 40.5[0m
[32m[2022-08-31 19:11:22,756] [    INFO][0m - loss: 3.4e-07, learning_rate: 1.77e-05, global_step: 820, interval_runtime: 1.557, interval_samples_per_second: 5.138, interval_steps_per_second: 6.422, epoch: 41.0[0m
[32m[2022-08-31 19:11:24,423] [    INFO][0m - loss: 2.4e-07, learning_rate: 1.755e-05, global_step: 830, interval_runtime: 1.6676, interval_samples_per_second: 4.797, interval_steps_per_second: 5.997, epoch: 41.5[0m
[32m[2022-08-31 19:11:25,959] [    INFO][0m - loss: 7e-08, learning_rate: 1.74e-05, global_step: 840, interval_runtime: 1.5361, interval_samples_per_second: 5.208, interval_steps_per_second: 6.51, epoch: 42.0[0m
[32m[2022-08-31 19:11:27,608] [    INFO][0m - loss: 8e-08, learning_rate: 1.725e-05, global_step: 850, interval_runtime: 1.6487, interval_samples_per_second: 4.852, interval_steps_per_second: 6.065, epoch: 42.5[0m
[32m[2022-08-31 19:11:29,143] [    INFO][0m - loss: 7e-08, learning_rate: 1.71e-05, global_step: 860, interval_runtime: 1.5353, interval_samples_per_second: 5.211, interval_steps_per_second: 6.514, epoch: 43.0[0m
[32m[2022-08-31 19:11:30,786] [    INFO][0m - loss: 3e-08, learning_rate: 1.695e-05, global_step: 870, interval_runtime: 1.6433, interval_samples_per_second: 4.868, interval_steps_per_second: 6.085, epoch: 43.5[0m
[32m[2022-08-31 19:11:32,321] [    INFO][0m - loss: 2.2e-07, learning_rate: 1.6800000000000002e-05, global_step: 880, interval_runtime: 1.5347, interval_samples_per_second: 5.213, interval_steps_per_second: 6.516, epoch: 44.0[0m
[32m[2022-08-31 19:11:34,012] [    INFO][0m - loss: 8e-08, learning_rate: 1.665e-05, global_step: 890, interval_runtime: 1.6909, interval_samples_per_second: 4.731, interval_steps_per_second: 5.914, epoch: 44.5[0m
[32m[2022-08-31 19:11:35,544] [    INFO][0m - loss: 8e-08, learning_rate: 1.65e-05, global_step: 900, interval_runtime: 1.5325, interval_samples_per_second: 5.22, interval_steps_per_second: 6.525, epoch: 45.0[0m
[32m[2022-08-31 19:11:35,545] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 19:11:35,545] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 19:11:35,545] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 19:11:35,545] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 19:11:35,546] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 19:11:36,939] [    INFO][0m - eval_loss: 3.7633984088897705, eval_accuracy: 0.69375, eval_runtime: 1.3848, eval_samples_per_second: 115.539, eval_steps_per_second: 3.611, epoch: 45.0[0m
[32m[2022-08-31 19:11:36,940] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-900[0m
[32m[2022-08-31 19:11:36,940] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 19:11:38,968] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-900/tokenizer_config.json[0m
[32m[2022-08-31 19:11:38,968] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-900/special_tokens_map.json[0m
[32m[2022-08-31 19:11:45,523] [    INFO][0m - loss: 1e-07, learning_rate: 1.635e-05, global_step: 910, interval_runtime: 9.9781, interval_samples_per_second: 0.802, interval_steps_per_second: 1.002, epoch: 45.5[0m
[32m[2022-08-31 19:11:47,062] [    INFO][0m - loss: 7.06e-06, learning_rate: 1.62e-05, global_step: 920, interval_runtime: 1.5388, interval_samples_per_second: 5.199, interval_steps_per_second: 6.498, epoch: 46.0[0m
[32m[2022-08-31 19:11:48,723] [    INFO][0m - loss: 1.4e-07, learning_rate: 1.605e-05, global_step: 930, interval_runtime: 1.6617, interval_samples_per_second: 4.814, interval_steps_per_second: 6.018, epoch: 46.5[0m
[32m[2022-08-31 19:11:50,260] [    INFO][0m - loss: 4e-08, learning_rate: 1.59e-05, global_step: 940, interval_runtime: 1.5369, interval_samples_per_second: 5.205, interval_steps_per_second: 6.507, epoch: 47.0[0m
[32m[2022-08-31 19:11:51,920] [    INFO][0m - loss: 6e-08, learning_rate: 1.575e-05, global_step: 950, interval_runtime: 1.6603, interval_samples_per_second: 4.818, interval_steps_per_second: 6.023, epoch: 47.5[0m
[32m[2022-08-31 19:11:53,459] [    INFO][0m - loss: 1.88e-06, learning_rate: 1.56e-05, global_step: 960, interval_runtime: 1.5389, interval_samples_per_second: 5.198, interval_steps_per_second: 6.498, epoch: 48.0[0m
[32m[2022-08-31 19:11:55,160] [    INFO][0m - loss: 4e-08, learning_rate: 1.545e-05, global_step: 970, interval_runtime: 1.7011, interval_samples_per_second: 4.703, interval_steps_per_second: 5.879, epoch: 48.5[0m
[32m[2022-08-31 19:11:56,696] [    INFO][0m - loss: 2e-08, learning_rate: 1.53e-05, global_step: 980, interval_runtime: 1.5359, interval_samples_per_second: 5.209, interval_steps_per_second: 6.511, epoch: 49.0[0m
[32m[2022-08-31 19:11:58,358] [    INFO][0m - loss: 6e-08, learning_rate: 1.515e-05, global_step: 990, interval_runtime: 1.6612, interval_samples_per_second: 4.816, interval_steps_per_second: 6.02, epoch: 49.5[0m
[32m[2022-08-31 19:11:59,886] [    INFO][0m - loss: 1.5e-07, learning_rate: 1.5e-05, global_step: 1000, interval_runtime: 1.5284, interval_samples_per_second: 5.234, interval_steps_per_second: 6.543, epoch: 50.0[0m
[32m[2022-08-31 19:11:59,886] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 19:11:59,886] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 19:11:59,887] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 19:11:59,887] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 19:11:59,887] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 19:12:01,278] [    INFO][0m - eval_loss: 3.8708205223083496, eval_accuracy: 0.675, eval_runtime: 1.3914, eval_samples_per_second: 114.995, eval_steps_per_second: 3.594, epoch: 50.0[0m
[32m[2022-08-31 19:12:01,279] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1000[0m
[32m[2022-08-31 19:12:01,279] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 19:12:03,325] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1000/tokenizer_config.json[0m
[32m[2022-08-31 19:12:03,326] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1000/special_tokens_map.json[0m
[32m[2022-08-31 19:12:09,734] [    INFO][0m - loss: 4e-08, learning_rate: 1.485e-05, global_step: 1010, interval_runtime: 9.8478, interval_samples_per_second: 0.812, interval_steps_per_second: 1.015, epoch: 50.5[0m
[32m[2022-08-31 19:12:11,267] [    INFO][0m - loss: 8e-08, learning_rate: 1.47e-05, global_step: 1020, interval_runtime: 1.5333, interval_samples_per_second: 5.218, interval_steps_per_second: 6.522, epoch: 51.0[0m
[32m[2022-08-31 19:12:12,929] [    INFO][0m - loss: 7e-08, learning_rate: 1.455e-05, global_step: 1030, interval_runtime: 1.662, interval_samples_per_second: 4.814, interval_steps_per_second: 6.017, epoch: 51.5[0m
[32m[2022-08-31 19:12:14,477] [    INFO][0m - loss: 1.2e-07, learning_rate: 1.44e-05, global_step: 1040, interval_runtime: 1.5479, interval_samples_per_second: 5.168, interval_steps_per_second: 6.46, epoch: 52.0[0m
[32m[2022-08-31 19:12:16,158] [    INFO][0m - loss: 1.7e-07, learning_rate: 1.4249999999999999e-05, global_step: 1050, interval_runtime: 1.6811, interval_samples_per_second: 4.759, interval_steps_per_second: 5.949, epoch: 52.5[0m
[32m[2022-08-31 19:12:17,696] [    INFO][0m - loss: 2.5e-07, learning_rate: 1.4099999999999999e-05, global_step: 1060, interval_runtime: 1.5372, interval_samples_per_second: 5.204, interval_steps_per_second: 6.505, epoch: 53.0[0m
[32m[2022-08-31 19:12:19,355] [    INFO][0m - loss: 6e-08, learning_rate: 1.395e-05, global_step: 1070, interval_runtime: 1.6598, interval_samples_per_second: 4.82, interval_steps_per_second: 6.025, epoch: 53.5[0m
[32m[2022-08-31 19:12:20,893] [    INFO][0m - loss: 6e-08, learning_rate: 1.3800000000000002e-05, global_step: 1080, interval_runtime: 1.5379, interval_samples_per_second: 5.202, interval_steps_per_second: 6.502, epoch: 54.0[0m
[32m[2022-08-31 19:12:22,555] [    INFO][0m - loss: 5e-08, learning_rate: 1.3650000000000001e-05, global_step: 1090, interval_runtime: 1.6613, interval_samples_per_second: 4.815, interval_steps_per_second: 6.019, epoch: 54.5[0m
[32m[2022-08-31 19:12:24,094] [    INFO][0m - loss: 2e-08, learning_rate: 1.3500000000000001e-05, global_step: 1100, interval_runtime: 1.5393, interval_samples_per_second: 5.197, interval_steps_per_second: 6.496, epoch: 55.0[0m
[32m[2022-08-31 19:12:24,095] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 19:12:24,095] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 19:12:24,095] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 19:12:24,095] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 19:12:24,095] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 19:12:25,531] [    INFO][0m - eval_loss: 3.867953062057495, eval_accuracy: 0.675, eval_runtime: 1.4349, eval_samples_per_second: 111.505, eval_steps_per_second: 3.485, epoch: 55.0[0m
[32m[2022-08-31 19:12:25,531] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1100[0m
[32m[2022-08-31 19:12:25,531] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 19:12:27,825] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1100/tokenizer_config.json[0m
[32m[2022-08-31 19:12:27,826] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1100/special_tokens_map.json[0m
[32m[2022-08-31 19:12:34,754] [    INFO][0m - loss: 1e-08, learning_rate: 1.3350000000000001e-05, global_step: 1110, interval_runtime: 10.6607, interval_samples_per_second: 0.75, interval_steps_per_second: 0.938, epoch: 55.5[0m
[32m[2022-08-31 19:12:36,302] [    INFO][0m - loss: 3e-08, learning_rate: 1.32e-05, global_step: 1120, interval_runtime: 1.5474, interval_samples_per_second: 5.17, interval_steps_per_second: 6.462, epoch: 56.0[0m
[32m[2022-08-31 19:12:37,989] [    INFO][0m - loss: 6e-08, learning_rate: 1.305e-05, global_step: 1130, interval_runtime: 1.6866, interval_samples_per_second: 4.743, interval_steps_per_second: 5.929, epoch: 56.5[0m
[32m[2022-08-31 19:12:39,542] [    INFO][0m - loss: 2.1e-07, learning_rate: 1.29e-05, global_step: 1140, interval_runtime: 1.5524, interval_samples_per_second: 5.153, interval_steps_per_second: 6.441, epoch: 57.0[0m
[32m[2022-08-31 19:12:41,210] [    INFO][0m - loss: 4e-08, learning_rate: 1.275e-05, global_step: 1150, interval_runtime: 1.6692, interval_samples_per_second: 4.793, interval_steps_per_second: 5.991, epoch: 57.5[0m
[32m[2022-08-31 19:12:42,748] [    INFO][0m - loss: 9.9e-07, learning_rate: 1.26e-05, global_step: 1160, interval_runtime: 1.538, interval_samples_per_second: 5.201, interval_steps_per_second: 6.502, epoch: 58.0[0m
[32m[2022-08-31 19:12:44,413] [    INFO][0m - loss: 4e-08, learning_rate: 1.245e-05, global_step: 1170, interval_runtime: 1.6636, interval_samples_per_second: 4.809, interval_steps_per_second: 6.011, epoch: 58.5[0m
[32m[2022-08-31 19:12:45,958] [    INFO][0m - loss: 3e-07, learning_rate: 1.2299999999999999e-05, global_step: 1180, interval_runtime: 1.5465, interval_samples_per_second: 5.173, interval_steps_per_second: 6.466, epoch: 59.0[0m
[32m[2022-08-31 19:12:47,622] [    INFO][0m - loss: 4.1e-07, learning_rate: 1.215e-05, global_step: 1190, interval_runtime: 1.6633, interval_samples_per_second: 4.81, interval_steps_per_second: 6.012, epoch: 59.5[0m
[32m[2022-08-31 19:12:49,163] [    INFO][0m - loss: 3e-08, learning_rate: 1.2e-05, global_step: 1200, interval_runtime: 1.541, interval_samples_per_second: 5.191, interval_steps_per_second: 6.489, epoch: 60.0[0m
[32m[2022-08-31 19:12:49,164] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 19:12:49,164] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 19:12:49,164] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 19:12:49,164] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 19:12:49,164] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 19:12:50,556] [    INFO][0m - eval_loss: 3.8555121421813965, eval_accuracy: 0.68125, eval_runtime: 1.3905, eval_samples_per_second: 115.07, eval_steps_per_second: 3.596, epoch: 60.0[0m
[32m[2022-08-31 19:12:50,556] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1200[0m
[32m[2022-08-31 19:12:50,557] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 19:12:52,602] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1200/tokenizer_config.json[0m
[32m[2022-08-31 19:12:52,603] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1200/special_tokens_map.json[0m
[32m[2022-08-31 19:12:58,857] [    INFO][0m - loss: 2e-08, learning_rate: 1.185e-05, global_step: 1210, interval_runtime: 9.6945, interval_samples_per_second: 0.825, interval_steps_per_second: 1.032, epoch: 60.5[0m
[32m[2022-08-31 19:13:00,395] [    INFO][0m - loss: 6e-08, learning_rate: 1.1700000000000001e-05, global_step: 1220, interval_runtime: 1.5378, interval_samples_per_second: 5.202, interval_steps_per_second: 6.503, epoch: 61.0[0m
[32m[2022-08-31 19:13:02,094] [    INFO][0m - loss: 9e-08, learning_rate: 1.1550000000000001e-05, global_step: 1230, interval_runtime: 1.6983, interval_samples_per_second: 4.71, interval_steps_per_second: 5.888, epoch: 61.5[0m
[32m[2022-08-31 19:13:03,638] [    INFO][0m - loss: 3e-08, learning_rate: 1.1400000000000001e-05, global_step: 1240, interval_runtime: 1.5447, interval_samples_per_second: 5.179, interval_steps_per_second: 6.474, epoch: 62.0[0m
[32m[2022-08-31 19:13:05,293] [    INFO][0m - loss: 1.9e-07, learning_rate: 1.125e-05, global_step: 1250, interval_runtime: 1.6549, interval_samples_per_second: 4.834, interval_steps_per_second: 6.042, epoch: 62.5[0m
[32m[2022-08-31 19:13:06,843] [    INFO][0m - loss: 1e-07, learning_rate: 1.11e-05, global_step: 1260, interval_runtime: 1.55, interval_samples_per_second: 5.161, interval_steps_per_second: 6.452, epoch: 63.0[0m
[32m[2022-08-31 19:13:08,521] [    INFO][0m - loss: 3e-08, learning_rate: 1.095e-05, global_step: 1270, interval_runtime: 1.6775, interval_samples_per_second: 4.769, interval_steps_per_second: 5.961, epoch: 63.5[0m
[32m[2022-08-31 19:13:10,074] [    INFO][0m - loss: 8e-08, learning_rate: 1.08e-05, global_step: 1280, interval_runtime: 1.5534, interval_samples_per_second: 5.15, interval_steps_per_second: 6.438, epoch: 64.0[0m
[32m[2022-08-31 19:13:11,737] [    INFO][0m - loss: 1.2e-07, learning_rate: 1.065e-05, global_step: 1290, interval_runtime: 1.6632, interval_samples_per_second: 4.81, interval_steps_per_second: 6.012, epoch: 64.5[0m
[32m[2022-08-31 19:13:13,279] [    INFO][0m - loss: 4.1e-07, learning_rate: 1.05e-05, global_step: 1300, interval_runtime: 1.541, interval_samples_per_second: 5.191, interval_steps_per_second: 6.489, epoch: 65.0[0m
[32m[2022-08-31 19:13:13,280] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 19:13:13,280] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 19:13:13,280] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 19:13:13,280] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 19:13:13,280] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 19:13:14,659] [    INFO][0m - eval_loss: 3.8532955646514893, eval_accuracy: 0.6875, eval_runtime: 1.379, eval_samples_per_second: 116.03, eval_steps_per_second: 3.626, epoch: 65.0[0m
[32m[2022-08-31 19:13:14,660] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1300[0m
[32m[2022-08-31 19:13:14,660] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 19:13:16,602] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1300/tokenizer_config.json[0m
[32m[2022-08-31 19:13:16,603] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1300/special_tokens_map.json[0m
[32m[2022-08-31 19:13:25,134] [    INFO][0m - loss: 5e-08, learning_rate: 1.035e-05, global_step: 1310, interval_runtime: 11.8553, interval_samples_per_second: 0.675, interval_steps_per_second: 0.844, epoch: 65.5[0m
[32m[2022-08-31 19:13:26,670] [    INFO][0m - loss: 1e-08, learning_rate: 1.02e-05, global_step: 1320, interval_runtime: 1.5365, interval_samples_per_second: 5.207, interval_steps_per_second: 6.508, epoch: 66.0[0m
[32m[2022-08-31 19:13:28,347] [    INFO][0m - loss: 3e-08, learning_rate: 1.005e-05, global_step: 1330, interval_runtime: 1.6772, interval_samples_per_second: 4.77, interval_steps_per_second: 5.962, epoch: 66.5[0m
[32m[2022-08-31 19:13:29,898] [    INFO][0m - loss: 4e-08, learning_rate: 9.9e-06, global_step: 1340, interval_runtime: 1.5507, interval_samples_per_second: 5.159, interval_steps_per_second: 6.449, epoch: 67.0[0m
[32m[2022-08-31 19:13:31,567] [    INFO][0m - loss: 4e-07, learning_rate: 9.75e-06, global_step: 1350, interval_runtime: 1.6686, interval_samples_per_second: 4.794, interval_steps_per_second: 5.993, epoch: 67.5[0m
[32m[2022-08-31 19:13:33,110] [    INFO][0m - loss: 5e-08, learning_rate: 9.600000000000001e-06, global_step: 1360, interval_runtime: 1.5431, interval_samples_per_second: 5.184, interval_steps_per_second: 6.48, epoch: 68.0[0m
[32m[2022-08-31 19:13:34,765] [    INFO][0m - loss: 2.2e-07, learning_rate: 9.450000000000001e-06, global_step: 1370, interval_runtime: 1.6555, interval_samples_per_second: 4.832, interval_steps_per_second: 6.041, epoch: 68.5[0m
[32m[2022-08-31 19:13:36,308] [    INFO][0m - loss: 4e-08, learning_rate: 9.3e-06, global_step: 1380, interval_runtime: 1.5429, interval_samples_per_second: 5.185, interval_steps_per_second: 6.481, epoch: 69.0[0m
[32m[2022-08-31 19:13:37,986] [    INFO][0m - loss: 8e-08, learning_rate: 9.15e-06, global_step: 1390, interval_runtime: 1.6777, interval_samples_per_second: 4.768, interval_steps_per_second: 5.96, epoch: 69.5[0m
[32m[2022-08-31 19:13:39,532] [    INFO][0m - loss: 7e-08, learning_rate: 9e-06, global_step: 1400, interval_runtime: 1.5461, interval_samples_per_second: 5.174, interval_steps_per_second: 6.468, epoch: 70.0[0m
[32m[2022-08-31 19:13:39,533] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 19:13:39,533] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 19:13:39,533] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 19:13:39,533] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 19:13:39,533] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 19:13:40,911] [    INFO][0m - eval_loss: 3.849973201751709, eval_accuracy: 0.6875, eval_runtime: 1.3767, eval_samples_per_second: 116.22, eval_steps_per_second: 3.632, epoch: 70.0[0m
[32m[2022-08-31 19:13:40,911] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1400[0m
[32m[2022-08-31 19:13:40,911] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 19:13:42,711] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1400/tokenizer_config.json[0m
[32m[2022-08-31 19:13:42,711] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1400/special_tokens_map.json[0m
[32m[2022-08-31 19:13:51,324] [    INFO][0m - loss: 3e-08, learning_rate: 8.85e-06, global_step: 1410, interval_runtime: 11.7918, interval_samples_per_second: 0.678, interval_steps_per_second: 0.848, epoch: 70.5[0m
[32m[2022-08-31 19:13:52,870] [    INFO][0m - loss: 3e-08, learning_rate: 8.7e-06, global_step: 1420, interval_runtime: 1.5458, interval_samples_per_second: 5.175, interval_steps_per_second: 6.469, epoch: 71.0[0m
[32m[2022-08-31 19:13:54,538] [    INFO][0m - loss: 1.3e-07, learning_rate: 8.55e-06, global_step: 1430, interval_runtime: 1.668, interval_samples_per_second: 4.796, interval_steps_per_second: 5.995, epoch: 71.5[0m
[32m[2022-08-31 19:13:56,081] [    INFO][0m - loss: 2e-08, learning_rate: 8.400000000000001e-06, global_step: 1440, interval_runtime: 1.5438, interval_samples_per_second: 5.182, interval_steps_per_second: 6.478, epoch: 72.0[0m
[32m[2022-08-31 19:13:57,759] [    INFO][0m - loss: 4e-08, learning_rate: 8.25e-06, global_step: 1450, interval_runtime: 1.6778, interval_samples_per_second: 4.768, interval_steps_per_second: 5.96, epoch: 72.5[0m
[32m[2022-08-31 19:14:00,785] [    INFO][0m - loss: 3e-08, learning_rate: 8.1e-06, global_step: 1460, interval_runtime: 3.026, interval_samples_per_second: 2.644, interval_steps_per_second: 3.305, epoch: 73.0[0m
[32m[2022-08-31 19:14:02,452] [    INFO][0m - loss: 8e-08, learning_rate: 7.95e-06, global_step: 1470, interval_runtime: 1.6671, interval_samples_per_second: 4.799, interval_steps_per_second: 5.999, epoch: 73.5[0m
[32m[2022-08-31 19:14:04,011] [    INFO][0m - loss: 4e-08, learning_rate: 7.8e-06, global_step: 1480, interval_runtime: 1.558, interval_samples_per_second: 5.135, interval_steps_per_second: 6.418, epoch: 74.0[0m
[32m[2022-08-31 19:14:05,680] [    INFO][0m - loss: 7e-08, learning_rate: 7.65e-06, global_step: 1490, interval_runtime: 1.67, interval_samples_per_second: 4.79, interval_steps_per_second: 5.988, epoch: 74.5[0m
[32m[2022-08-31 19:14:07,226] [    INFO][0m - loss: 5e-08, learning_rate: 7.5e-06, global_step: 1500, interval_runtime: 1.5454, interval_samples_per_second: 5.177, interval_steps_per_second: 6.471, epoch: 75.0[0m
[32m[2022-08-31 19:14:07,226] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 19:14:07,226] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 19:14:07,226] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 19:14:07,227] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 19:14:07,227] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 19:14:08,594] [    INFO][0m - eval_loss: 3.8501274585723877, eval_accuracy: 0.6875, eval_runtime: 1.3668, eval_samples_per_second: 117.064, eval_steps_per_second: 3.658, epoch: 75.0[0m
[32m[2022-08-31 19:14:08,594] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1500[0m
[32m[2022-08-31 19:14:08,594] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 19:14:10,643] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1500/tokenizer_config.json[0m
[32m[2022-08-31 19:14:10,644] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1500/special_tokens_map.json[0m
[32m[2022-08-31 19:14:16,494] [    INFO][0m - loss: 5e-08, learning_rate: 7.35e-06, global_step: 1510, interval_runtime: 9.2681, interval_samples_per_second: 0.863, interval_steps_per_second: 1.079, epoch: 75.5[0m
[32m[2022-08-31 19:14:18,040] [    INFO][0m - loss: 3e-08, learning_rate: 7.2e-06, global_step: 1520, interval_runtime: 1.5467, interval_samples_per_second: 5.172, interval_steps_per_second: 6.465, epoch: 76.0[0m
[32m[2022-08-31 19:14:19,705] [    INFO][0m - loss: 2e-08, learning_rate: 7.049999999999999e-06, global_step: 1530, interval_runtime: 1.664, interval_samples_per_second: 4.808, interval_steps_per_second: 6.009, epoch: 76.5[0m
[32m[2022-08-31 19:14:21,241] [    INFO][0m - loss: 1e-08, learning_rate: 6.900000000000001e-06, global_step: 1540, interval_runtime: 1.5367, interval_samples_per_second: 5.206, interval_steps_per_second: 6.507, epoch: 77.0[0m
[32m[2022-08-31 19:14:22,913] [    INFO][0m - loss: 5.5e-07, learning_rate: 6.750000000000001e-06, global_step: 1550, interval_runtime: 1.6716, interval_samples_per_second: 4.786, interval_steps_per_second: 5.982, epoch: 77.5[0m
[32m[2022-08-31 19:14:24,468] [    INFO][0m - loss: 1e-08, learning_rate: 6.6e-06, global_step: 1560, interval_runtime: 1.5547, interval_samples_per_second: 5.146, interval_steps_per_second: 6.432, epoch: 78.0[0m
[32m[2022-08-31 19:14:26,136] [    INFO][0m - loss: 1e-08, learning_rate: 6.45e-06, global_step: 1570, interval_runtime: 1.6682, interval_samples_per_second: 4.796, interval_steps_per_second: 5.994, epoch: 78.5[0m
[32m[2022-08-31 19:14:27,672] [    INFO][0m - loss: 1e-07, learning_rate: 6.3e-06, global_step: 1580, interval_runtime: 1.5357, interval_samples_per_second: 5.209, interval_steps_per_second: 6.512, epoch: 79.0[0m
[32m[2022-08-31 19:14:29,344] [    INFO][0m - loss: 4e-08, learning_rate: 6.1499999999999996e-06, global_step: 1590, interval_runtime: 1.6724, interval_samples_per_second: 4.784, interval_steps_per_second: 5.979, epoch: 79.5[0m
[32m[2022-08-31 19:14:30,886] [    INFO][0m - loss: 5e-08, learning_rate: 6e-06, global_step: 1600, interval_runtime: 1.5425, interval_samples_per_second: 5.186, interval_steps_per_second: 6.483, epoch: 80.0[0m
[32m[2022-08-31 19:14:30,888] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 19:14:30,888] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 19:14:30,888] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 19:14:30,888] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 19:14:30,888] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 19:14:32,335] [    INFO][0m - eval_loss: 3.847766876220703, eval_accuracy: 0.6875, eval_runtime: 1.4461, eval_samples_per_second: 110.643, eval_steps_per_second: 3.458, epoch: 80.0[0m
[32m[2022-08-31 19:14:32,335] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1600[0m
[32m[2022-08-31 19:14:32,335] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 19:14:34,401] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1600/tokenizer_config.json[0m
[32m[2022-08-31 19:14:34,401] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1600/special_tokens_map.json[0m
[32m[2022-08-31 19:14:40,300] [    INFO][0m - loss: 8e-08, learning_rate: 5.850000000000001e-06, global_step: 1610, interval_runtime: 9.4132, interval_samples_per_second: 0.85, interval_steps_per_second: 1.062, epoch: 80.5[0m
[32m[2022-08-31 19:14:41,837] [    INFO][0m - loss: 2e-08, learning_rate: 5.7000000000000005e-06, global_step: 1620, interval_runtime: 1.5369, interval_samples_per_second: 5.205, interval_steps_per_second: 6.507, epoch: 81.0[0m
[32m[2022-08-31 19:14:43,508] [    INFO][0m - loss: 4e-08, learning_rate: 5.55e-06, global_step: 1630, interval_runtime: 1.6711, interval_samples_per_second: 4.787, interval_steps_per_second: 5.984, epoch: 81.5[0m
[32m[2022-08-31 19:14:45,050] [    INFO][0m - loss: 1.1e-07, learning_rate: 5.4e-06, global_step: 1640, interval_runtime: 1.5427, interval_samples_per_second: 5.186, interval_steps_per_second: 6.482, epoch: 82.0[0m
[32m[2022-08-31 19:14:46,709] [    INFO][0m - loss: 6e-08, learning_rate: 5.25e-06, global_step: 1650, interval_runtime: 1.6579, interval_samples_per_second: 4.825, interval_steps_per_second: 6.032, epoch: 82.5[0m
[32m[2022-08-31 19:14:48,249] [    INFO][0m - loss: 1e-08, learning_rate: 5.1e-06, global_step: 1660, interval_runtime: 1.5411, interval_samples_per_second: 5.191, interval_steps_per_second: 6.489, epoch: 83.0[0m
[32m[2022-08-31 19:14:49,929] [    INFO][0m - loss: 2.1e-07, learning_rate: 4.95e-06, global_step: 1670, interval_runtime: 1.6792, interval_samples_per_second: 4.764, interval_steps_per_second: 5.955, epoch: 83.5[0m
[32m[2022-08-31 19:14:51,476] [    INFO][0m - loss: 8e-08, learning_rate: 4.800000000000001e-06, global_step: 1680, interval_runtime: 1.5476, interval_samples_per_second: 5.169, interval_steps_per_second: 6.461, epoch: 84.0[0m
[32m[2022-08-31 19:14:53,144] [    INFO][0m - loss: 2e-07, learning_rate: 4.65e-06, global_step: 1690, interval_runtime: 1.6673, interval_samples_per_second: 4.798, interval_steps_per_second: 5.998, epoch: 84.5[0m
[32m[2022-08-31 19:14:54,780] [    INFO][0m - loss: 2.2e-07, learning_rate: 4.5e-06, global_step: 1700, interval_runtime: 1.6357, interval_samples_per_second: 4.891, interval_steps_per_second: 6.114, epoch: 85.0[0m
[32m[2022-08-31 19:14:54,781] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 19:14:54,781] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 19:14:54,781] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 19:14:54,781] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 19:14:54,781] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 19:14:56,261] [    INFO][0m - eval_loss: 3.8467354774475098, eval_accuracy: 0.6875, eval_runtime: 1.4797, eval_samples_per_second: 108.132, eval_steps_per_second: 3.379, epoch: 85.0[0m
[32m[2022-08-31 19:14:56,261] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1700[0m
[32m[2022-08-31 19:14:56,261] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 19:14:58,501] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1700/tokenizer_config.json[0m
[32m[2022-08-31 19:14:58,502] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1700/special_tokens_map.json[0m
[32m[2022-08-31 19:15:04,525] [    INFO][0m - loss: 5.6e-07, learning_rate: 4.35e-06, global_step: 1710, interval_runtime: 9.7453, interval_samples_per_second: 0.821, interval_steps_per_second: 1.026, epoch: 85.5[0m
[32m[2022-08-31 19:15:06,065] [    INFO][0m - loss: 9e-08, learning_rate: 4.2000000000000004e-06, global_step: 1720, interval_runtime: 1.5404, interval_samples_per_second: 5.194, interval_steps_per_second: 6.492, epoch: 86.0[0m
[32m[2022-08-31 19:15:08,602] [    INFO][0m - loss: 3.8e-07, learning_rate: 4.05e-06, global_step: 1730, interval_runtime: 1.7404, interval_samples_per_second: 4.597, interval_steps_per_second: 5.746, epoch: 86.5[0m
[32m[2022-08-31 19:15:10,153] [    INFO][0m - loss: 2e-08, learning_rate: 3.9e-06, global_step: 1740, interval_runtime: 2.3474, interval_samples_per_second: 3.408, interval_steps_per_second: 4.26, epoch: 87.0[0m
[32m[2022-08-31 19:15:11,811] [    INFO][0m - loss: 7e-08, learning_rate: 3.75e-06, global_step: 1750, interval_runtime: 1.6576, interval_samples_per_second: 4.826, interval_steps_per_second: 6.033, epoch: 87.5[0m
[32m[2022-08-31 19:15:13,362] [    INFO][0m - loss: 3e-08, learning_rate: 3.6e-06, global_step: 1760, interval_runtime: 1.5513, interval_samples_per_second: 5.157, interval_steps_per_second: 6.446, epoch: 88.0[0m
[32m[2022-08-31 19:15:15,082] [    INFO][0m - loss: 3e-08, learning_rate: 3.4500000000000004e-06, global_step: 1770, interval_runtime: 1.72, interval_samples_per_second: 4.651, interval_steps_per_second: 5.814, epoch: 88.5[0m
[32m[2022-08-31 19:15:16,622] [    INFO][0m - loss: 5e-08, learning_rate: 3.3e-06, global_step: 1780, interval_runtime: 1.5401, interval_samples_per_second: 5.195, interval_steps_per_second: 6.493, epoch: 89.0[0m
[32m[2022-08-31 19:15:18,303] [    INFO][0m - loss: 5e-08, learning_rate: 3.15e-06, global_step: 1790, interval_runtime: 1.6805, interval_samples_per_second: 4.761, interval_steps_per_second: 5.951, epoch: 89.5[0m
[32m[2022-08-31 19:15:19,865] [    INFO][0m - loss: 1.4e-07, learning_rate: 3e-06, global_step: 1800, interval_runtime: 1.5628, interval_samples_per_second: 5.119, interval_steps_per_second: 6.399, epoch: 90.0[0m
[32m[2022-08-31 19:15:19,866] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 19:15:19,866] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 19:15:19,866] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 19:15:19,866] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 19:15:19,867] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 19:15:21,228] [    INFO][0m - eval_loss: 3.844026565551758, eval_accuracy: 0.6875, eval_runtime: 1.3606, eval_samples_per_second: 117.591, eval_steps_per_second: 3.675, epoch: 90.0[0m
[32m[2022-08-31 19:15:21,228] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1800[0m
[32m[2022-08-31 19:15:21,228] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 19:15:24,818] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1800/tokenizer_config.json[0m
[32m[2022-08-31 19:15:24,819] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1800/special_tokens_map.json[0m
[32m[2022-08-31 19:15:30,858] [    INFO][0m - loss: 2e-08, learning_rate: 2.8500000000000002e-06, global_step: 1810, interval_runtime: 10.9927, interval_samples_per_second: 0.728, interval_steps_per_second: 0.91, epoch: 90.5[0m
[32m[2022-08-31 19:15:32,400] [    INFO][0m - loss: 3e-08, learning_rate: 2.7e-06, global_step: 1820, interval_runtime: 1.5422, interval_samples_per_second: 5.187, interval_steps_per_second: 6.484, epoch: 91.0[0m
[32m[2022-08-31 19:15:34,063] [    INFO][0m - loss: 6e-08, learning_rate: 2.55e-06, global_step: 1830, interval_runtime: 1.6628, interval_samples_per_second: 4.811, interval_steps_per_second: 6.014, epoch: 91.5[0m
[32m[2022-08-31 19:15:35,602] [    INFO][0m - loss: 3e-08, learning_rate: 2.4000000000000003e-06, global_step: 1840, interval_runtime: 1.5385, interval_samples_per_second: 5.2, interval_steps_per_second: 6.5, epoch: 92.0[0m
[32m[2022-08-31 19:15:37,331] [    INFO][0m - loss: 2e-08, learning_rate: 2.25e-06, global_step: 1850, interval_runtime: 1.7292, interval_samples_per_second: 4.626, interval_steps_per_second: 5.783, epoch: 92.5[0m
[32m[2022-08-31 19:15:38,875] [    INFO][0m - loss: 3e-08, learning_rate: 2.1000000000000002e-06, global_step: 1860, interval_runtime: 1.5445, interval_samples_per_second: 5.18, interval_steps_per_second: 6.475, epoch: 93.0[0m
[32m[2022-08-31 19:15:40,529] [    INFO][0m - loss: 3e-08, learning_rate: 1.95e-06, global_step: 1870, interval_runtime: 1.6532, interval_samples_per_second: 4.839, interval_steps_per_second: 6.049, epoch: 93.5[0m
[32m[2022-08-31 19:15:42,071] [    INFO][0m - loss: 3.7e-07, learning_rate: 1.8e-06, global_step: 1880, interval_runtime: 1.5426, interval_samples_per_second: 5.186, interval_steps_per_second: 6.483, epoch: 94.0[0m
[32m[2022-08-31 19:15:43,739] [    INFO][0m - loss: 4e-08, learning_rate: 1.65e-06, global_step: 1890, interval_runtime: 1.667, interval_samples_per_second: 4.799, interval_steps_per_second: 5.999, epoch: 94.5[0m
[32m[2022-08-31 19:15:45,278] [    INFO][0m - loss: 5e-08, learning_rate: 1.5e-06, global_step: 1900, interval_runtime: 1.5394, interval_samples_per_second: 5.197, interval_steps_per_second: 6.496, epoch: 95.0[0m
[32m[2022-08-31 19:15:45,279] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 19:15:45,279] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 19:15:45,279] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 19:15:45,279] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 19:15:45,279] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 19:15:46,748] [    INFO][0m - eval_loss: 3.8434860706329346, eval_accuracy: 0.6875, eval_runtime: 1.4684, eval_samples_per_second: 108.962, eval_steps_per_second: 3.405, epoch: 95.0[0m
[32m[2022-08-31 19:15:46,748] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1900[0m
[32m[2022-08-31 19:15:46,748] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 19:15:48,639] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1900/tokenizer_config.json[0m
[32m[2022-08-31 19:15:48,639] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1900/special_tokens_map.json[0m
[32m[2022-08-31 19:15:55,858] [    INFO][0m - loss: 1.1e-07, learning_rate: 1.35e-06, global_step: 1910, interval_runtime: 10.5808, interval_samples_per_second: 0.756, interval_steps_per_second: 0.945, epoch: 95.5[0m
[32m[2022-08-31 19:15:57,948] [    INFO][0m - loss: 8e-08, learning_rate: 1.2000000000000002e-06, global_step: 1920, interval_runtime: 1.5403, interval_samples_per_second: 5.194, interval_steps_per_second: 6.492, epoch: 96.0[0m
[32m[2022-08-31 19:15:59,632] [    INFO][0m - loss: 2e-08, learning_rate: 1.0500000000000001e-06, global_step: 1930, interval_runtime: 2.2338, interval_samples_per_second: 3.581, interval_steps_per_second: 4.477, epoch: 96.5[0m
[32m[2022-08-31 19:16:01,183] [    INFO][0m - loss: 3e-08, learning_rate: 9e-07, global_step: 1940, interval_runtime: 1.5508, interval_samples_per_second: 5.159, interval_steps_per_second: 6.448, epoch: 97.0[0m
[32m[2022-08-31 19:16:02,841] [    INFO][0m - loss: 5e-08, learning_rate: 7.5e-07, global_step: 1950, interval_runtime: 1.6582, interval_samples_per_second: 4.825, interval_steps_per_second: 6.031, epoch: 97.5[0m
[32m[2022-08-31 19:16:04,389] [    INFO][0m - loss: 3e-08, learning_rate: 6.000000000000001e-07, global_step: 1960, interval_runtime: 1.5477, interval_samples_per_second: 5.169, interval_steps_per_second: 6.461, epoch: 98.0[0m
[32m[2022-08-31 19:16:06,076] [    INFO][0m - loss: 4e-08, learning_rate: 4.5e-07, global_step: 1970, interval_runtime: 1.6868, interval_samples_per_second: 4.743, interval_steps_per_second: 5.929, epoch: 98.5[0m
[32m[2022-08-31 19:16:07,625] [    INFO][0m - loss: 1.7e-07, learning_rate: 3.0000000000000004e-07, global_step: 1980, interval_runtime: 1.5485, interval_samples_per_second: 5.166, interval_steps_per_second: 6.458, epoch: 99.0[0m
[32m[2022-08-31 19:16:09,292] [    INFO][0m - loss: 1e-07, learning_rate: 1.5000000000000002e-07, global_step: 1990, interval_runtime: 1.6673, interval_samples_per_second: 4.798, interval_steps_per_second: 5.998, epoch: 99.5[0m
[32m[2022-08-31 19:16:10,846] [    INFO][0m - loss: 5e-08, learning_rate: 0.0, global_step: 2000, interval_runtime: 1.5544, interval_samples_per_second: 5.147, interval_steps_per_second: 6.433, epoch: 100.0[0m
[32m[2022-08-31 19:16:10,847] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 19:16:10,847] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 19:16:10,847] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 19:16:10,847] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 19:16:10,847] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 19:16:12,244] [    INFO][0m - eval_loss: 3.843566417694092, eval_accuracy: 0.6875, eval_runtime: 1.3961, eval_samples_per_second: 114.609, eval_steps_per_second: 3.582, epoch: 100.0[0m
[32m[2022-08-31 19:16:12,244] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2000[0m
[32m[2022-08-31 19:16:12,244] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 19:16:14,226] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2000/tokenizer_config.json[0m
[32m[2022-08-31 19:16:14,226] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2000/special_tokens_map.json[0m
[32m[2022-08-31 19:16:18,727] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-31 19:16:18,728] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-100 (score: 0.7).[0m
[32m[2022-08-31 19:16:19,408] [    INFO][0m - train_runtime: 491.7545, train_samples_per_second: 32.537, train_steps_per_second: 4.067, train_loss: 0.03563609664458977, epoch: 100.0[0m
[32m[2022-08-31 19:16:19,410] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-31 19:16:19,411] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 19:16:21,251] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-31 19:16:21,251] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-31 19:16:21,252] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-31 19:16:21,253] [    INFO][0m -   epoch                    =      100.0[0m
[32m[2022-08-31 19:16:21,253] [    INFO][0m -   train_loss               =     0.0356[0m
[32m[2022-08-31 19:16:21,253] [    INFO][0m -   train_runtime            = 0:08:11.75[0m
[32m[2022-08-31 19:16:21,253] [    INFO][0m -   train_samples_per_second =     32.537[0m
[32m[2022-08-31 19:16:21,253] [    INFO][0m -   train_steps_per_second   =      4.067[0m
[32m[2022-08-31 19:16:21,262] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-31 19:16:21,262] [    INFO][0m -   Num examples = 2838[0m
[32m[2022-08-31 19:16:21,262] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 19:16:21,262] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 19:16:21,262] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-08-31 19:16:45,844] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-31 19:16:45,845] [    INFO][0m -   test_accuracy           =     0.6815[0m
[32m[2022-08-31 19:16:45,845] [    INFO][0m -   test_loss               =     1.6227[0m
[32m[2022-08-31 19:16:45,845] [    INFO][0m -   test_runtime            = 0:00:24.58[0m
[32m[2022-08-31 19:16:45,845] [    INFO][0m -   test_samples_per_second =    115.449[0m
[32m[2022-08-31 19:16:45,845] [    INFO][0m -   test_steps_per_second   =       3.62[0m
[32m[2022-08-31 19:16:45,846] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-31 19:16:45,846] [    INFO][0m -   Num examples = 3000[0m
[32m[2022-08-31 19:16:45,846] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 19:16:45,846] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 19:16:45,846] [    INFO][0m -   Total prediction steps = 94[0m
[32m[2022-08-31 19:17:19,906] [    INFO][0m - Predictions for cslf saved to ./fewclue_submit_examples.[0m
run.sh: line 65: --model_name_or_path: command not found
 
==========
cluewsc
==========
 
[33m[2022-08-31 19:17:24,462] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-31 19:17:24,462] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-31 19:17:24,462] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 19:17:24,462] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-31 19:17:24,462] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 19:17:24,462] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-31 19:17:24,462] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-31 19:17:24,463] [    INFO][0m - early_stop_patience           :20[0m
[32m[2022-08-31 19:17:24,463] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-31 19:17:24,463] [    INFO][0m - model_name_or_path            :roformer_v2_chinese_char_base[0m
[32m[2022-08-31 19:17:24,463] [    INFO][0m - [0m
[32m[2022-08-31 19:17:24,463] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 19:17:24,463] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-31 19:17:24,463] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 19:17:24,463] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-31 19:17:24,463] [    INFO][0m - prompt                        :{'text':'text_a'}{'sep'}{'hard':'å…¶ä¸­ä»£è¯ç”¨'}{'mask'}{'hard':'äº†ã€‚'}[0m
[32m[2022-08-31 19:17:24,463] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-31 19:17:24,463] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-31 19:17:24,463] [    INFO][0m - task_name                     :cluewsc[0m
[32m[2022-08-31 19:17:24,463] [    INFO][0m - [0m
[32m[2022-08-31 19:17:24,464] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/roformer_v2_chinese_char_base/model_state.pdparams[0m
W0831 19:17:24.465775 76080 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0831 19:17:24.470232 76080 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-31 19:17:27,685] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/roformer_v2_chinese_char_base/vocab.txt[0m
[32m[2022-08-31 19:17:27,693] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/roformer_v2_chinese_char_base/tokenizer_config.json[0m
[32m[2022-08-31 19:17:27,694] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/roformer_v2_chinese_char_base/special_tokens_map.json[0m
[32m[2022-08-31 19:17:27,695] [    INFO][0m - Using template: [{'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'sep': None}, {'add_prefix_space': '', 'hard': 'å…¶ä¸­ä»£è¯ç”¨'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'äº†ã€‚'}][0m
2022-08-31 19:17:27,701 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-31 19:17:27,812] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 19:17:27,812] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-31 19:17:27,812] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 19:17:27,813] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-31 19:17:27,813] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-31 19:17:27,813] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-31 19:17:27,813] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-31 19:17:27,813] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-31 19:17:27,813] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-31 19:17:27,813] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-31 19:17:27,813] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-31 19:17:27,813] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-31 19:17:27,813] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-31 19:17:27,813] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-31 19:17:27,814] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-31 19:17:27,814] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-31 19:17:27,814] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-31 19:17:27,814] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-31 19:17:27,814] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-31 19:17:27,814] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-31 19:17:27,814] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-31 19:17:27,814] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-31 19:17:27,814] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-31 19:17:27,814] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-31 19:17:27,814] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-31 19:17:27,814] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-31 19:17:27,814] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-31 19:17:27,815] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-31 19:17:27,815] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-31 19:17:27,815] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-31 19:17:27,815] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-31 19:17:27,815] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-31 19:17:27,815] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-31 19:17:27,815] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-31 19:17:27,815] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-31 19:17:27,815] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-31 19:17:27,815] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-31 19:17:27,815] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug31_19-17-24_instance-3bwob41y-01[0m
[32m[2022-08-31 19:17:27,815] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-31 19:17:27,815] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-31 19:17:27,816] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-31 19:17:27,816] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-31 19:17:27,816] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-31 19:17:27,816] [    INFO][0m - max_seq_length                :128[0m
[32m[2022-08-31 19:17:27,816] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-31 19:17:27,816] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-31 19:17:27,816] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-31 19:17:27,816] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-31 19:17:27,816] [    INFO][0m - num_train_epochs              :100.0[0m
[32m[2022-08-31 19:17:27,816] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-31 19:17:27,816] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-31 19:17:27,816] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-31 19:17:27,816] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-31 19:17:27,817] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-31 19:17:27,817] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-31 19:17:27,817] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-31 19:17:27,817] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-31 19:17:27,817] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-31 19:17:27,817] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-31 19:17:27,817] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-31 19:17:27,817] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-31 19:17:27,817] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-31 19:17:27,817] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-31 19:17:27,817] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-31 19:17:27,817] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-31 19:17:27,817] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-31 19:17:27,817] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-31 19:17:27,818] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-31 19:17:27,818] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-31 19:17:27,818] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-31 19:17:27,818] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-31 19:17:27,818] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-31 19:17:27,818] [    INFO][0m - seed                          :42[0m
[32m[2022-08-31 19:17:27,818] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-31 19:17:27,818] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-31 19:17:27,818] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-31 19:17:27,818] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-31 19:17:27,818] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-31 19:17:27,818] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-31 19:17:27,818] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-31 19:17:27,819] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-31 19:17:27,819] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-31 19:17:27,819] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-31 19:17:27,819] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-31 19:17:27,819] [    INFO][0m - [0m
[32m[2022-08-31 19:17:27,821] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-31 19:17:27,821] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 19:17:27,822] [    INFO][0m -   Num Epochs = 100[0m
[32m[2022-08-31 19:17:27,822] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-31 19:17:27,822] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-31 19:17:27,822] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-31 19:17:27,822] [    INFO][0m -   Total optimization steps = 2000.0[0m
[32m[2022-08-31 19:17:27,822] [    INFO][0m -   Total num train samples = 16000[0m
[32m[2022-08-31 19:17:29,384] [    INFO][0m - loss: 1.0091197, learning_rate: 2.985e-05, global_step: 10, interval_runtime: 1.5607, interval_samples_per_second: 5.126, interval_steps_per_second: 6.407, epoch: 0.5[0m
[32m[2022-08-31 19:17:30,140] [    INFO][0m - loss: 0.80249195, learning_rate: 2.97e-05, global_step: 20, interval_runtime: 0.7568, interval_samples_per_second: 10.571, interval_steps_per_second: 13.213, epoch: 1.0[0m
[32m[2022-08-31 19:17:31,005] [    INFO][0m - loss: 0.774931, learning_rate: 2.955e-05, global_step: 30, interval_runtime: 0.865, interval_samples_per_second: 9.249, interval_steps_per_second: 11.561, epoch: 1.5[0m
[32m[2022-08-31 19:17:31,758] [    INFO][0m - loss: 0.77708802, learning_rate: 2.94e-05, global_step: 40, interval_runtime: 0.7524, interval_samples_per_second: 10.633, interval_steps_per_second: 13.292, epoch: 2.0[0m
[32m[2022-08-31 19:17:32,625] [    INFO][0m - loss: 0.85854445, learning_rate: 2.925e-05, global_step: 50, interval_runtime: 0.8668, interval_samples_per_second: 9.229, interval_steps_per_second: 11.536, epoch: 2.5[0m
[32m[2022-08-31 19:17:33,375] [    INFO][0m - loss: 0.78980699, learning_rate: 2.91e-05, global_step: 60, interval_runtime: 0.7504, interval_samples_per_second: 10.66, interval_steps_per_second: 13.326, epoch: 3.0[0m
[32m[2022-08-31 19:17:34,236] [    INFO][0m - loss: 0.60947261, learning_rate: 2.895e-05, global_step: 70, interval_runtime: 0.8611, interval_samples_per_second: 9.29, interval_steps_per_second: 11.613, epoch: 3.5[0m
[32m[2022-08-31 19:17:34,998] [    INFO][0m - loss: 0.75776048, learning_rate: 2.88e-05, global_step: 80, interval_runtime: 0.762, interval_samples_per_second: 10.499, interval_steps_per_second: 13.124, epoch: 4.0[0m
[32m[2022-08-31 19:17:35,843] [    INFO][0m - loss: 0.63086877, learning_rate: 2.865e-05, global_step: 90, interval_runtime: 0.845, interval_samples_per_second: 9.467, interval_steps_per_second: 11.834, epoch: 4.5[0m
[32m[2022-08-31 19:17:36,610] [    INFO][0m - loss: 0.57704735, learning_rate: 2.8499999999999998e-05, global_step: 100, interval_runtime: 0.7664, interval_samples_per_second: 10.438, interval_steps_per_second: 13.048, epoch: 5.0[0m
[32m[2022-08-31 19:17:36,611] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 19:17:36,611] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-31 19:17:36,611] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 19:17:36,611] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 19:17:36,611] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 19:17:37,161] [    INFO][0m - eval_loss: 0.9723784923553467, eval_accuracy: 0.48427672955974843, eval_runtime: 0.5494, eval_samples_per_second: 289.417, eval_steps_per_second: 9.101, epoch: 5.0[0m
[32m[2022-08-31 19:17:37,161] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-31 19:17:37,161] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 19:17:39,402] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-31 19:17:39,403] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-31 19:17:45,438] [    INFO][0m - loss: 0.43420091, learning_rate: 2.8349999999999998e-05, global_step: 110, interval_runtime: 8.8277, interval_samples_per_second: 0.906, interval_steps_per_second: 1.133, epoch: 5.5[0m
[32m[2022-08-31 19:17:46,191] [    INFO][0m - loss: 0.45274105, learning_rate: 2.8199999999999998e-05, global_step: 120, interval_runtime: 0.7539, interval_samples_per_second: 10.611, interval_steps_per_second: 13.264, epoch: 6.0[0m
[32m[2022-08-31 19:17:47,071] [    INFO][0m - loss: 0.30244448, learning_rate: 2.805e-05, global_step: 130, interval_runtime: 0.8796, interval_samples_per_second: 9.095, interval_steps_per_second: 11.369, epoch: 6.5[0m
[32m[2022-08-31 19:17:47,830] [    INFO][0m - loss: 0.35861878, learning_rate: 2.79e-05, global_step: 140, interval_runtime: 0.7589, interval_samples_per_second: 10.541, interval_steps_per_second: 13.176, epoch: 7.0[0m
[32m[2022-08-31 19:17:48,694] [    INFO][0m - loss: 0.37643404, learning_rate: 2.7750000000000004e-05, global_step: 150, interval_runtime: 0.8643, interval_samples_per_second: 9.256, interval_steps_per_second: 11.57, epoch: 7.5[0m
[32m[2022-08-31 19:17:49,457] [    INFO][0m - loss: 0.41014762, learning_rate: 2.7600000000000003e-05, global_step: 160, interval_runtime: 0.7625, interval_samples_per_second: 10.491, interval_steps_per_second: 13.114, epoch: 8.0[0m
[32m[2022-08-31 19:17:50,359] [    INFO][0m - loss: 0.40405712, learning_rate: 2.7450000000000003e-05, global_step: 170, interval_runtime: 0.9027, interval_samples_per_second: 8.863, interval_steps_per_second: 11.078, epoch: 8.5[0m
[32m[2022-08-31 19:17:51,152] [    INFO][0m - loss: 0.38676672, learning_rate: 2.7300000000000003e-05, global_step: 180, interval_runtime: 0.7929, interval_samples_per_second: 10.09, interval_steps_per_second: 12.612, epoch: 9.0[0m
[32m[2022-08-31 19:17:52,010] [    INFO][0m - loss: 0.50219736, learning_rate: 2.7150000000000003e-05, global_step: 190, interval_runtime: 0.8575, interval_samples_per_second: 9.33, interval_steps_per_second: 11.662, epoch: 9.5[0m
[32m[2022-08-31 19:17:52,765] [    INFO][0m - loss: 0.08366133, learning_rate: 2.7000000000000002e-05, global_step: 200, interval_runtime: 0.7556, interval_samples_per_second: 10.588, interval_steps_per_second: 13.235, epoch: 10.0[0m
[32m[2022-08-31 19:17:52,766] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 19:17:52,766] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-31 19:17:52,766] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 19:17:52,766] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 19:17:52,766] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 19:17:53,311] [    INFO][0m - eval_loss: 2.510923385620117, eval_accuracy: 0.5157232704402516, eval_runtime: 0.5448, eval_samples_per_second: 291.864, eval_steps_per_second: 9.178, epoch: 10.0[0m
[32m[2022-08-31 19:17:53,312] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-31 19:17:53,312] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 19:17:55,182] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-31 19:17:55,182] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-31 19:18:01,573] [    INFO][0m - loss: 0.20687413, learning_rate: 2.6850000000000002e-05, global_step: 210, interval_runtime: 8.8072, interval_samples_per_second: 0.908, interval_steps_per_second: 1.135, epoch: 10.5[0m
[32m[2022-08-31 19:18:02,368] [    INFO][0m - loss: 0.30657909, learning_rate: 2.6700000000000002e-05, global_step: 220, interval_runtime: 0.7958, interval_samples_per_second: 10.052, interval_steps_per_second: 12.565, epoch: 11.0[0m
[32m[2022-08-31 19:18:03,242] [    INFO][0m - loss: 0.03186432, learning_rate: 2.655e-05, global_step: 230, interval_runtime: 0.873, interval_samples_per_second: 9.164, interval_steps_per_second: 11.455, epoch: 11.5[0m
[32m[2022-08-31 19:18:04,374] [    INFO][0m - loss: 0.24203811, learning_rate: 2.64e-05, global_step: 240, interval_runtime: 0.7596, interval_samples_per_second: 10.532, interval_steps_per_second: 13.165, epoch: 12.0[0m
[32m[2022-08-31 19:18:05,260] [    INFO][0m - loss: 0.07388663, learning_rate: 2.625e-05, global_step: 250, interval_runtime: 1.2591, interval_samples_per_second: 6.354, interval_steps_per_second: 7.942, epoch: 12.5[0m
[32m[2022-08-31 19:18:06,024] [    INFO][0m - loss: 0.32290249, learning_rate: 2.61e-05, global_step: 260, interval_runtime: 0.7635, interval_samples_per_second: 10.478, interval_steps_per_second: 13.098, epoch: 13.0[0m
[32m[2022-08-31 19:18:06,931] [    INFO][0m - loss: 0.00633286, learning_rate: 2.595e-05, global_step: 270, interval_runtime: 0.9072, interval_samples_per_second: 8.819, interval_steps_per_second: 11.023, epoch: 13.5[0m
[32m[2022-08-31 19:18:07,685] [    INFO][0m - loss: 0.07058167, learning_rate: 2.58e-05, global_step: 280, interval_runtime: 0.7543, interval_samples_per_second: 10.606, interval_steps_per_second: 13.258, epoch: 14.0[0m
[32m[2022-08-31 19:18:08,600] [    INFO][0m - loss: 0.08088473, learning_rate: 2.565e-05, global_step: 290, interval_runtime: 0.9155, interval_samples_per_second: 8.738, interval_steps_per_second: 10.923, epoch: 14.5[0m
[32m[2022-08-31 19:18:09,394] [    INFO][0m - loss: 0.09361454, learning_rate: 2.55e-05, global_step: 300, interval_runtime: 0.7939, interval_samples_per_second: 10.077, interval_steps_per_second: 12.596, epoch: 15.0[0m
[32m[2022-08-31 19:18:09,395] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 19:18:09,395] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-31 19:18:09,395] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 19:18:09,395] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 19:18:09,395] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 19:18:09,940] [    INFO][0m - eval_loss: 3.665118455886841, eval_accuracy: 0.5345911949685535, eval_runtime: 0.5447, eval_samples_per_second: 291.9, eval_steps_per_second: 9.179, epoch: 15.0[0m
[32m[2022-08-31 19:18:09,941] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-31 19:18:09,941] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 19:18:11,834] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-31 19:18:11,834] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-31 19:18:18,422] [    INFO][0m - loss: 0.17266145, learning_rate: 2.535e-05, global_step: 310, interval_runtime: 9.028, interval_samples_per_second: 0.886, interval_steps_per_second: 1.108, epoch: 15.5[0m
[32m[2022-08-31 19:18:19,224] [    INFO][0m - loss: 0.18249347, learning_rate: 2.52e-05, global_step: 320, interval_runtime: 0.8009, interval_samples_per_second: 9.988, interval_steps_per_second: 12.485, epoch: 16.0[0m
[32m[2022-08-31 19:18:20,118] [    INFO][0m - loss: 0.04580708, learning_rate: 2.505e-05, global_step: 330, interval_runtime: 0.8951, interval_samples_per_second: 8.938, interval_steps_per_second: 11.172, epoch: 16.5[0m
[32m[2022-08-31 19:18:20,928] [    INFO][0m - loss: 0.11278321, learning_rate: 2.49e-05, global_step: 340, interval_runtime: 0.8097, interval_samples_per_second: 9.881, interval_steps_per_second: 12.351, epoch: 17.0[0m
[32m[2022-08-31 19:18:21,799] [    INFO][0m - loss: 0.12507285, learning_rate: 2.475e-05, global_step: 350, interval_runtime: 0.871, interval_samples_per_second: 9.185, interval_steps_per_second: 11.481, epoch: 17.5[0m
[32m[2022-08-31 19:18:22,576] [    INFO][0m - loss: 0.01319884, learning_rate: 2.4599999999999998e-05, global_step: 360, interval_runtime: 0.777, interval_samples_per_second: 10.296, interval_steps_per_second: 12.87, epoch: 18.0[0m
[32m[2022-08-31 19:18:23,483] [    INFO][0m - loss: 0.00511495, learning_rate: 2.4449999999999998e-05, global_step: 370, interval_runtime: 0.9067, interval_samples_per_second: 8.823, interval_steps_per_second: 11.028, epoch: 18.5[0m
[32m[2022-08-31 19:18:24,262] [    INFO][0m - loss: 0.00182042, learning_rate: 2.43e-05, global_step: 380, interval_runtime: 0.7791, interval_samples_per_second: 10.268, interval_steps_per_second: 12.835, epoch: 19.0[0m
[32m[2022-08-31 19:18:25,153] [    INFO][0m - loss: 8.835e-05, learning_rate: 2.415e-05, global_step: 390, interval_runtime: 0.8907, interval_samples_per_second: 8.981, interval_steps_per_second: 11.227, epoch: 19.5[0m
[32m[2022-08-31 19:18:25,987] [    INFO][0m - loss: 0.01123195, learning_rate: 2.4e-05, global_step: 400, interval_runtime: 0.8345, interval_samples_per_second: 9.586, interval_steps_per_second: 11.983, epoch: 20.0[0m
[32m[2022-08-31 19:18:25,988] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 19:18:25,989] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-31 19:18:25,989] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 19:18:25,989] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 19:18:25,989] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 19:18:26,553] [    INFO][0m - eval_loss: 5.778968334197998, eval_accuracy: 0.5283018867924528, eval_runtime: 0.5641, eval_samples_per_second: 281.886, eval_steps_per_second: 8.864, epoch: 20.0[0m
[32m[2022-08-31 19:18:26,554] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-31 19:18:26,554] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 19:18:28,435] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-31 19:18:28,435] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-31 19:18:35,806] [    INFO][0m - loss: 3.12e-06, learning_rate: 2.385e-05, global_step: 410, interval_runtime: 9.8186, interval_samples_per_second: 0.815, interval_steps_per_second: 1.018, epoch: 20.5[0m
[32m[2022-08-31 19:18:36,564] [    INFO][0m - loss: 0.03202333, learning_rate: 2.37e-05, global_step: 420, interval_runtime: 0.7583, interval_samples_per_second: 10.55, interval_steps_per_second: 13.188, epoch: 21.0[0m
[32m[2022-08-31 19:18:37,423] [    INFO][0m - loss: 0.00661885, learning_rate: 2.3550000000000003e-05, global_step: 430, interval_runtime: 0.8588, interval_samples_per_second: 9.316, interval_steps_per_second: 11.645, epoch: 21.5[0m
[32m[2022-08-31 19:18:38,184] [    INFO][0m - loss: 0.00024129, learning_rate: 2.3400000000000003e-05, global_step: 440, interval_runtime: 0.7611, interval_samples_per_second: 10.511, interval_steps_per_second: 13.139, epoch: 22.0[0m
[32m[2022-08-31 19:18:39,055] [    INFO][0m - loss: 9.1e-07, learning_rate: 2.3250000000000003e-05, global_step: 450, interval_runtime: 0.8702, interval_samples_per_second: 9.193, interval_steps_per_second: 11.491, epoch: 22.5[0m
[32m[2022-08-31 19:18:39,817] [    INFO][0m - loss: 4.97e-06, learning_rate: 2.3100000000000002e-05, global_step: 460, interval_runtime: 0.7625, interval_samples_per_second: 10.492, interval_steps_per_second: 13.115, epoch: 23.0[0m
[32m[2022-08-31 19:18:40,707] [    INFO][0m - loss: 3.5e-07, learning_rate: 2.2950000000000002e-05, global_step: 470, interval_runtime: 0.8906, interval_samples_per_second: 8.982, interval_steps_per_second: 11.228, epoch: 23.5[0m
[32m[2022-08-31 19:18:41,481] [    INFO][0m - loss: 0.00013243, learning_rate: 2.2800000000000002e-05, global_step: 480, interval_runtime: 0.7736, interval_samples_per_second: 10.341, interval_steps_per_second: 12.927, epoch: 24.0[0m
[32m[2022-08-31 19:18:42,360] [    INFO][0m - loss: 4.07e-06, learning_rate: 2.265e-05, global_step: 490, interval_runtime: 0.8795, interval_samples_per_second: 9.096, interval_steps_per_second: 11.37, epoch: 24.5[0m
[32m[2022-08-31 19:18:43,112] [    INFO][0m - loss: 0.00330306, learning_rate: 2.25e-05, global_step: 500, interval_runtime: 0.7513, interval_samples_per_second: 10.649, interval_steps_per_second: 13.311, epoch: 25.0[0m
[32m[2022-08-31 19:18:43,112] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 19:18:43,113] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-31 19:18:43,113] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 19:18:43,113] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 19:18:43,113] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 19:18:43,669] [    INFO][0m - eval_loss: 5.699644088745117, eval_accuracy: 0.6037735849056604, eval_runtime: 0.5554, eval_samples_per_second: 286.283, eval_steps_per_second: 9.003, epoch: 25.0[0m
[32m[2022-08-31 19:18:43,669] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-08-31 19:18:43,670] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 19:18:45,626] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-08-31 19:18:45,626] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-08-31 19:18:53,143] [    INFO][0m - loss: 7.81e-06, learning_rate: 2.235e-05, global_step: 510, interval_runtime: 10.031, interval_samples_per_second: 0.798, interval_steps_per_second: 0.997, epoch: 25.5[0m
[32m[2022-08-31 19:18:53,905] [    INFO][0m - loss: 7.28e-06, learning_rate: 2.22e-05, global_step: 520, interval_runtime: 0.7625, interval_samples_per_second: 10.492, interval_steps_per_second: 13.115, epoch: 26.0[0m
[32m[2022-08-31 19:18:54,768] [    INFO][0m - loss: 3.1e-07, learning_rate: 2.205e-05, global_step: 530, interval_runtime: 0.8624, interval_samples_per_second: 9.277, interval_steps_per_second: 11.596, epoch: 26.5[0m
[32m[2022-08-31 19:18:55,517] [    INFO][0m - loss: 2.11e-05, learning_rate: 2.19e-05, global_step: 540, interval_runtime: 0.7487, interval_samples_per_second: 10.685, interval_steps_per_second: 13.356, epoch: 27.0[0m
[32m[2022-08-31 19:18:56,526] [    INFO][0m - loss: 1.558e-05, learning_rate: 2.175e-05, global_step: 550, interval_runtime: 1.0094, interval_samples_per_second: 7.925, interval_steps_per_second: 9.907, epoch: 27.5[0m
[32m[2022-08-31 19:18:57,339] [    INFO][0m - loss: 2.03e-06, learning_rate: 2.16e-05, global_step: 560, interval_runtime: 0.8121, interval_samples_per_second: 9.851, interval_steps_per_second: 12.313, epoch: 28.0[0m
[32m[2022-08-31 19:18:58,268] [    INFO][0m - loss: 5e-08, learning_rate: 2.145e-05, global_step: 570, interval_runtime: 0.9301, interval_samples_per_second: 8.601, interval_steps_per_second: 10.751, epoch: 28.5[0m
[32m[2022-08-31 19:18:59,039] [    INFO][0m - loss: 2e-08, learning_rate: 2.13e-05, global_step: 580, interval_runtime: 0.7703, interval_samples_per_second: 10.386, interval_steps_per_second: 12.983, epoch: 29.0[0m
[32m[2022-08-31 19:18:59,931] [    INFO][0m - loss: 6e-08, learning_rate: 2.115e-05, global_step: 590, interval_runtime: 0.8924, interval_samples_per_second: 8.965, interval_steps_per_second: 11.206, epoch: 29.5[0m
[32m[2022-08-31 19:19:00,714] [    INFO][0m - loss: 4e-08, learning_rate: 2.1e-05, global_step: 600, interval_runtime: 0.7829, interval_samples_per_second: 10.219, interval_steps_per_second: 12.773, epoch: 30.0[0m
[32m[2022-08-31 19:19:00,718] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 19:19:00,718] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-31 19:19:00,718] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 19:19:00,718] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 19:19:00,718] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 19:19:01,292] [    INFO][0m - eval_loss: 6.770840167999268, eval_accuracy: 0.5031446540880503, eval_runtime: 0.5725, eval_samples_per_second: 277.747, eval_steps_per_second: 8.734, epoch: 30.0[0m
[32m[2022-08-31 19:19:01,293] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-08-31 19:19:01,293] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 19:19:03,285] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-08-31 19:19:03,286] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-08-31 19:19:09,195] [    INFO][0m - loss: 1e-08, learning_rate: 2.085e-05, global_step: 610, interval_runtime: 8.4814, interval_samples_per_second: 0.943, interval_steps_per_second: 1.179, epoch: 30.5[0m
[32m[2022-08-31 19:19:09,983] [    INFO][0m - loss: 3e-08, learning_rate: 2.07e-05, global_step: 620, interval_runtime: 0.7878, interval_samples_per_second: 10.155, interval_steps_per_second: 12.693, epoch: 31.0[0m
[32m[2022-08-31 19:19:10,881] [    INFO][0m - loss: 4e-08, learning_rate: 2.055e-05, global_step: 630, interval_runtime: 0.8967, interval_samples_per_second: 8.922, interval_steps_per_second: 11.153, epoch: 31.5[0m
[32m[2022-08-31 19:19:11,696] [    INFO][0m - loss: 4e-08, learning_rate: 2.04e-05, global_step: 640, interval_runtime: 0.8162, interval_samples_per_second: 9.801, interval_steps_per_second: 12.251, epoch: 32.0[0m
[32m[2022-08-31 19:19:12,580] [    INFO][0m - loss: 3e-08, learning_rate: 2.025e-05, global_step: 650, interval_runtime: 0.8842, interval_samples_per_second: 9.047, interval_steps_per_second: 11.309, epoch: 32.5[0m
[32m[2022-08-31 19:19:13,350] [    INFO][0m - loss: 7.6e-07, learning_rate: 2.01e-05, global_step: 660, interval_runtime: 0.7704, interval_samples_per_second: 10.385, interval_steps_per_second: 12.981, epoch: 33.0[0m
[32m[2022-08-31 19:19:14,273] [    INFO][0m - loss: 4e-08, learning_rate: 1.995e-05, global_step: 670, interval_runtime: 0.9219, interval_samples_per_second: 8.678, interval_steps_per_second: 10.847, epoch: 33.5[0m
[32m[2022-08-31 19:19:15,036] [    INFO][0m - loss: 5e-08, learning_rate: 1.98e-05, global_step: 680, interval_runtime: 0.7633, interval_samples_per_second: 10.48, interval_steps_per_second: 13.1, epoch: 34.0[0m
[32m[2022-08-31 19:19:15,931] [    INFO][0m - loss: 0.0, learning_rate: 1.965e-05, global_step: 690, interval_runtime: 0.8951, interval_samples_per_second: 8.938, interval_steps_per_second: 11.172, epoch: 34.5[0m
[32m[2022-08-31 19:19:16,694] [    INFO][0m - loss: 1.616e-05, learning_rate: 1.95e-05, global_step: 700, interval_runtime: 0.7635, interval_samples_per_second: 10.478, interval_steps_per_second: 13.098, epoch: 35.0[0m
[32m[2022-08-31 19:19:16,695] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 19:19:16,695] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-31 19:19:16,695] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 19:19:16,695] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 19:19:16,695] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 19:19:17,234] [    INFO][0m - eval_loss: 6.64040470123291, eval_accuracy: 0.5534591194968553, eval_runtime: 0.5384, eval_samples_per_second: 295.3, eval_steps_per_second: 9.286, epoch: 35.0[0m
[32m[2022-08-31 19:19:17,235] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-700[0m
[32m[2022-08-31 19:19:17,235] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 19:19:19,154] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-700/tokenizer_config.json[0m
[32m[2022-08-31 19:19:19,155] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-700/special_tokens_map.json[0m
[32m[2022-08-31 19:19:24,841] [    INFO][0m - loss: 1.1e-07, learning_rate: 1.935e-05, global_step: 710, interval_runtime: 8.146, interval_samples_per_second: 0.982, interval_steps_per_second: 1.228, epoch: 35.5[0m
[32m[2022-08-31 19:19:25,621] [    INFO][0m - loss: 1.989e-05, learning_rate: 1.9200000000000003e-05, global_step: 720, interval_runtime: 0.7801, interval_samples_per_second: 10.255, interval_steps_per_second: 12.819, epoch: 36.0[0m
[32m[2022-08-31 19:19:26,499] [    INFO][0m - loss: 3e-08, learning_rate: 1.9050000000000002e-05, global_step: 730, interval_runtime: 0.8789, interval_samples_per_second: 9.102, interval_steps_per_second: 11.378, epoch: 36.5[0m
[32m[2022-08-31 19:19:27,270] [    INFO][0m - loss: 1e-08, learning_rate: 1.8900000000000002e-05, global_step: 740, interval_runtime: 0.7709, interval_samples_per_second: 10.378, interval_steps_per_second: 12.972, epoch: 37.0[0m
[32m[2022-08-31 19:19:28,254] [    INFO][0m - loss: 5.92e-06, learning_rate: 1.8750000000000002e-05, global_step: 750, interval_runtime: 0.9838, interval_samples_per_second: 8.132, interval_steps_per_second: 10.164, epoch: 37.5[0m
[32m[2022-08-31 19:19:29,102] [    INFO][0m - loss: 0.0, learning_rate: 1.86e-05, global_step: 760, interval_runtime: 0.8481, interval_samples_per_second: 9.433, interval_steps_per_second: 11.791, epoch: 38.0[0m
[32m[2022-08-31 19:19:29,974] [    INFO][0m - loss: 0.0, learning_rate: 1.845e-05, global_step: 770, interval_runtime: 0.871, interval_samples_per_second: 9.185, interval_steps_per_second: 11.481, epoch: 38.5[0m
[32m[2022-08-31 19:19:30,729] [    INFO][0m - loss: 3.2e-07, learning_rate: 1.83e-05, global_step: 780, interval_runtime: 0.7561, interval_samples_per_second: 10.58, interval_steps_per_second: 13.225, epoch: 39.0[0m
[32m[2022-08-31 19:19:31,636] [    INFO][0m - loss: 0.0, learning_rate: 1.815e-05, global_step: 790, interval_runtime: 0.9067, interval_samples_per_second: 8.823, interval_steps_per_second: 11.029, epoch: 39.5[0m
[32m[2022-08-31 19:19:32,398] [    INFO][0m - loss: 5e-08, learning_rate: 1.8e-05, global_step: 800, interval_runtime: 0.7623, interval_samples_per_second: 10.495, interval_steps_per_second: 13.119, epoch: 40.0[0m
[32m[2022-08-31 19:19:32,399] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 19:19:32,399] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-31 19:19:32,399] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 19:19:32,399] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 19:19:32,399] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 19:19:32,941] [    INFO][0m - eval_loss: 6.906084060668945, eval_accuracy: 0.5094339622641509, eval_runtime: 0.5419, eval_samples_per_second: 293.43, eval_steps_per_second: 9.227, epoch: 40.0[0m
[32m[2022-08-31 19:19:32,942] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-800[0m
[32m[2022-08-31 19:19:32,942] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 19:19:34,879] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-800/tokenizer_config.json[0m
[32m[2022-08-31 19:19:34,880] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-800/special_tokens_map.json[0m
[32m[2022-08-31 19:19:40,401] [    INFO][0m - loss: 0.0, learning_rate: 1.785e-05, global_step: 810, interval_runtime: 8.0023, interval_samples_per_second: 1.0, interval_steps_per_second: 1.25, epoch: 40.5[0m
[32m[2022-08-31 19:19:41,196] [    INFO][0m - loss: 5e-08, learning_rate: 1.77e-05, global_step: 820, interval_runtime: 0.7953, interval_samples_per_second: 10.059, interval_steps_per_second: 12.574, epoch: 41.0[0m
[32m[2022-08-31 19:19:42,066] [    INFO][0m - loss: 8e-08, learning_rate: 1.755e-05, global_step: 830, interval_runtime: 0.87, interval_samples_per_second: 9.196, interval_steps_per_second: 11.495, epoch: 41.5[0m
[32m[2022-08-31 19:19:42,808] [    INFO][0m - loss: 0.0, learning_rate: 1.74e-05, global_step: 840, interval_runtime: 0.7422, interval_samples_per_second: 10.779, interval_steps_per_second: 13.474, epoch: 42.0[0m
[32m[2022-08-31 19:19:43,741] [    INFO][0m - loss: 5.99e-06, learning_rate: 1.725e-05, global_step: 850, interval_runtime: 0.9302, interval_samples_per_second: 8.601, interval_steps_per_second: 10.751, epoch: 42.5[0m
[32m[2022-08-31 19:19:44,498] [    INFO][0m - loss: 1e-08, learning_rate: 1.71e-05, global_step: 860, interval_runtime: 0.7597, interval_samples_per_second: 10.53, interval_steps_per_second: 13.162, epoch: 43.0[0m
[32m[2022-08-31 19:19:45,416] [    INFO][0m - loss: 1.3e-07, learning_rate: 1.695e-05, global_step: 870, interval_runtime: 0.918, interval_samples_per_second: 8.714, interval_steps_per_second: 10.893, epoch: 43.5[0m
[32m[2022-08-31 19:19:46,219] [    INFO][0m - loss: 0.0, learning_rate: 1.6800000000000002e-05, global_step: 880, interval_runtime: 0.8027, interval_samples_per_second: 9.966, interval_steps_per_second: 12.457, epoch: 44.0[0m
[32m[2022-08-31 19:19:47,093] [    INFO][0m - loss: 1e-08, learning_rate: 1.665e-05, global_step: 890, interval_runtime: 0.8741, interval_samples_per_second: 9.152, interval_steps_per_second: 11.44, epoch: 44.5[0m
[32m[2022-08-31 19:19:47,848] [    INFO][0m - loss: 4e-08, learning_rate: 1.65e-05, global_step: 900, interval_runtime: 0.7556, interval_samples_per_second: 10.588, interval_steps_per_second: 13.234, epoch: 45.0[0m
[32m[2022-08-31 19:19:47,849] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 19:19:47,849] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-31 19:19:47,850] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 19:19:47,850] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 19:19:47,850] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 19:19:48,402] [    INFO][0m - eval_loss: 6.971168518066406, eval_accuracy: 0.5094339622641509, eval_runtime: 0.5519, eval_samples_per_second: 288.119, eval_steps_per_second: 9.06, epoch: 45.0[0m
[32m[2022-08-31 19:19:48,402] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-900[0m
[32m[2022-08-31 19:19:48,402] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 19:19:51,367] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-900/tokenizer_config.json[0m
[32m[2022-08-31 19:19:51,700] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-900/special_tokens_map.json[0m
[32m[2022-08-31 19:19:57,155] [    INFO][0m - loss: 0.0, learning_rate: 1.635e-05, global_step: 910, interval_runtime: 9.3019, interval_samples_per_second: 0.86, interval_steps_per_second: 1.075, epoch: 45.5[0m
[32m[2022-08-31 19:19:57,924] [    INFO][0m - loss: 1e-08, learning_rate: 1.62e-05, global_step: 920, interval_runtime: 0.7731, interval_samples_per_second: 10.347, interval_steps_per_second: 12.934, epoch: 46.0[0m
[32m[2022-08-31 19:19:58,811] [    INFO][0m - loss: 5e-08, learning_rate: 1.605e-05, global_step: 930, interval_runtime: 0.8868, interval_samples_per_second: 9.021, interval_steps_per_second: 11.277, epoch: 46.5[0m
[32m[2022-08-31 19:19:59,557] [    INFO][0m - loss: 0.0, learning_rate: 1.59e-05, global_step: 940, interval_runtime: 0.7462, interval_samples_per_second: 10.72, interval_steps_per_second: 13.401, epoch: 47.0[0m
[32m[2022-08-31 19:20:00,460] [    INFO][0m - loss: 2.47e-06, learning_rate: 1.575e-05, global_step: 950, interval_runtime: 0.9031, interval_samples_per_second: 8.859, interval_steps_per_second: 11.073, epoch: 47.5[0m
[32m[2022-08-31 19:20:01,255] [    INFO][0m - loss: 0.0, learning_rate: 1.56e-05, global_step: 960, interval_runtime: 0.795, interval_samples_per_second: 10.063, interval_steps_per_second: 12.579, epoch: 48.0[0m
[32m[2022-08-31 19:20:02,155] [    INFO][0m - loss: 1e-08, learning_rate: 1.545e-05, global_step: 970, interval_runtime: 0.9006, interval_samples_per_second: 8.883, interval_steps_per_second: 11.104, epoch: 48.5[0m
[32m[2022-08-31 19:20:02,920] [    INFO][0m - loss: 5e-08, learning_rate: 1.53e-05, global_step: 980, interval_runtime: 0.7641, interval_samples_per_second: 10.47, interval_steps_per_second: 13.088, epoch: 49.0[0m
[32m[2022-08-31 19:20:03,812] [    INFO][0m - loss: 4e-08, learning_rate: 1.515e-05, global_step: 990, interval_runtime: 0.8929, interval_samples_per_second: 8.96, interval_steps_per_second: 11.2, epoch: 49.5[0m
[32m[2022-08-31 19:20:04,592] [    INFO][0m - loss: 0.0, learning_rate: 1.5e-05, global_step: 1000, interval_runtime: 0.78, interval_samples_per_second: 10.256, interval_steps_per_second: 12.82, epoch: 50.0[0m
[32m[2022-08-31 19:20:04,593] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 19:20:04,593] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-31 19:20:04,593] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 19:20:04,593] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 19:20:04,593] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 19:20:05,147] [    INFO][0m - eval_loss: 6.967649936676025, eval_accuracy: 0.5094339622641509, eval_runtime: 0.5475, eval_samples_per_second: 290.394, eval_steps_per_second: 9.132, epoch: 50.0[0m
[32m[2022-08-31 19:20:05,147] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1000[0m
[32m[2022-08-31 19:20:05,148] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 19:20:07,028] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1000/tokenizer_config.json[0m
[32m[2022-08-31 19:20:07,029] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1000/special_tokens_map.json[0m
[32m[2022-08-31 19:20:12,132] [    INFO][0m - loss: 0.0, learning_rate: 1.485e-05, global_step: 1010, interval_runtime: 7.5393, interval_samples_per_second: 1.061, interval_steps_per_second: 1.326, epoch: 50.5[0m
[32m[2022-08-31 19:20:12,893] [    INFO][0m - loss: 1e-08, learning_rate: 1.47e-05, global_step: 1020, interval_runtime: 0.7613, interval_samples_per_second: 10.509, interval_steps_per_second: 13.136, epoch: 51.0[0m
[32m[2022-08-31 19:20:13,763] [    INFO][0m - loss: 0.0, learning_rate: 1.455e-05, global_step: 1030, interval_runtime: 0.8704, interval_samples_per_second: 9.191, interval_steps_per_second: 11.489, epoch: 51.5[0m
[32m[2022-08-31 19:20:14,527] [    INFO][0m - loss: 6e-08, learning_rate: 1.44e-05, global_step: 1040, interval_runtime: 0.7634, interval_samples_per_second: 10.479, interval_steps_per_second: 13.099, epoch: 52.0[0m
[32m[2022-08-31 19:20:15,406] [    INFO][0m - loss: 1.34e-06, learning_rate: 1.4249999999999999e-05, global_step: 1050, interval_runtime: 0.8793, interval_samples_per_second: 9.098, interval_steps_per_second: 11.373, epoch: 52.5[0m
[32m[2022-08-31 19:20:16,170] [    INFO][0m - loss: 0.0, learning_rate: 1.4099999999999999e-05, global_step: 1060, interval_runtime: 0.7641, interval_samples_per_second: 10.469, interval_steps_per_second: 13.087, epoch: 53.0[0m
[32m[2022-08-31 19:20:17,087] [    INFO][0m - loss: 0.0, learning_rate: 1.395e-05, global_step: 1070, interval_runtime: 0.9165, interval_samples_per_second: 8.729, interval_steps_per_second: 10.911, epoch: 53.5[0m
[32m[2022-08-31 19:20:17,844] [    INFO][0m - loss: 1e-08, learning_rate: 1.3800000000000002e-05, global_step: 1080, interval_runtime: 0.7573, interval_samples_per_second: 10.563, interval_steps_per_second: 13.204, epoch: 54.0[0m
[32m[2022-08-31 19:20:18,751] [    INFO][0m - loss: 0.0, learning_rate: 1.3650000000000001e-05, global_step: 1090, interval_runtime: 0.907, interval_samples_per_second: 8.821, interval_steps_per_second: 11.026, epoch: 54.5[0m
[32m[2022-08-31 19:20:19,500] [    INFO][0m - loss: 0.0, learning_rate: 1.3500000000000001e-05, global_step: 1100, interval_runtime: 0.7489, interval_samples_per_second: 10.682, interval_steps_per_second: 13.352, epoch: 55.0[0m
[32m[2022-08-31 19:20:19,501] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 19:20:19,501] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-31 19:20:19,501] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 19:20:19,502] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 19:20:19,502] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 19:20:20,040] [    INFO][0m - eval_loss: 6.945559501647949, eval_accuracy: 0.5220125786163522, eval_runtime: 0.5388, eval_samples_per_second: 295.078, eval_steps_per_second: 9.279, epoch: 55.0[0m
[32m[2022-08-31 19:20:20,041] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1100[0m
[32m[2022-08-31 19:20:20,041] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 19:20:22,390] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1100/tokenizer_config.json[0m
[32m[2022-08-31 19:20:22,390] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1100/special_tokens_map.json[0m
[32m[2022-08-31 19:20:29,529] [    INFO][0m - loss: 0.0, learning_rate: 1.3350000000000001e-05, global_step: 1110, interval_runtime: 10.0287, interval_samples_per_second: 0.798, interval_steps_per_second: 0.997, epoch: 55.5[0m
[32m[2022-08-31 19:20:30,285] [    INFO][0m - loss: 3e-08, learning_rate: 1.32e-05, global_step: 1120, interval_runtime: 0.7569, interval_samples_per_second: 10.57, interval_steps_per_second: 13.212, epoch: 56.0[0m
[32m[2022-08-31 19:20:31,165] [    INFO][0m - loss: 1e-08, learning_rate: 1.305e-05, global_step: 1130, interval_runtime: 0.8791, interval_samples_per_second: 9.1, interval_steps_per_second: 11.375, epoch: 56.5[0m
[32m[2022-08-31 19:20:31,927] [    INFO][0m - loss: 3e-08, learning_rate: 1.29e-05, global_step: 1140, interval_runtime: 0.7628, interval_samples_per_second: 10.487, interval_steps_per_second: 13.109, epoch: 57.0[0m
[32m[2022-08-31 19:20:32,796] [    INFO][0m - loss: 0.0, learning_rate: 1.275e-05, global_step: 1150, interval_runtime: 0.8687, interval_samples_per_second: 9.209, interval_steps_per_second: 11.512, epoch: 57.5[0m
[32m[2022-08-31 19:20:33,568] [    INFO][0m - loss: 1e-08, learning_rate: 1.26e-05, global_step: 1160, interval_runtime: 0.7716, interval_samples_per_second: 10.369, interval_steps_per_second: 12.961, epoch: 58.0[0m
[32m[2022-08-31 19:20:34,450] [    INFO][0m - loss: 0.0, learning_rate: 1.245e-05, global_step: 1170, interval_runtime: 0.8817, interval_samples_per_second: 9.073, interval_steps_per_second: 11.342, epoch: 58.5[0m
[32m[2022-08-31 19:20:35,241] [    INFO][0m - loss: 1e-08, learning_rate: 1.2299999999999999e-05, global_step: 1180, interval_runtime: 0.7914, interval_samples_per_second: 10.109, interval_steps_per_second: 12.636, epoch: 59.0[0m
[32m[2022-08-31 19:20:36,155] [    INFO][0m - loss: 2e-08, learning_rate: 1.215e-05, global_step: 1190, interval_runtime: 0.9146, interval_samples_per_second: 8.747, interval_steps_per_second: 10.934, epoch: 59.5[0m
[32m[2022-08-31 19:20:36,920] [    INFO][0m - loss: 2e-08, learning_rate: 1.2e-05, global_step: 1200, interval_runtime: 0.7644, interval_samples_per_second: 10.465, interval_steps_per_second: 13.082, epoch: 60.0[0m
[32m[2022-08-31 19:20:36,920] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 19:20:36,921] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-31 19:20:36,921] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 19:20:36,921] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 19:20:36,921] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 19:20:37,480] [    INFO][0m - eval_loss: 6.946622371673584, eval_accuracy: 0.5220125786163522, eval_runtime: 0.5584, eval_samples_per_second: 284.728, eval_steps_per_second: 8.954, epoch: 60.0[0m
[32m[2022-08-31 19:20:37,480] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1200[0m
[32m[2022-08-31 19:20:37,480] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 19:20:39,426] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1200/tokenizer_config.json[0m
[32m[2022-08-31 19:20:39,426] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1200/special_tokens_map.json[0m
[32m[2022-08-31 19:20:44,543] [    INFO][0m - loss: 0.0, learning_rate: 1.185e-05, global_step: 1210, interval_runtime: 7.6232, interval_samples_per_second: 1.049, interval_steps_per_second: 1.312, epoch: 60.5[0m
[32m[2022-08-31 19:20:45,297] [    INFO][0m - loss: 1e-08, learning_rate: 1.1700000000000001e-05, global_step: 1220, interval_runtime: 0.7536, interval_samples_per_second: 10.616, interval_steps_per_second: 13.269, epoch: 61.0[0m
[32m[2022-08-31 19:20:46,154] [    INFO][0m - loss: 1e-08, learning_rate: 1.1550000000000001e-05, global_step: 1230, interval_runtime: 0.8572, interval_samples_per_second: 9.332, interval_steps_per_second: 11.665, epoch: 61.5[0m
[32m[2022-08-31 19:20:46,910] [    INFO][0m - loss: 9e-08, learning_rate: 1.1400000000000001e-05, global_step: 1240, interval_runtime: 0.7563, interval_samples_per_second: 10.578, interval_steps_per_second: 13.223, epoch: 62.0[0m
[32m[2022-08-31 19:20:47,790] [    INFO][0m - loss: 0.0, learning_rate: 1.125e-05, global_step: 1250, interval_runtime: 0.8796, interval_samples_per_second: 9.095, interval_steps_per_second: 11.369, epoch: 62.5[0m
[32m[2022-08-31 19:20:48,550] [    INFO][0m - loss: 0.0, learning_rate: 1.11e-05, global_step: 1260, interval_runtime: 0.7598, interval_samples_per_second: 10.528, interval_steps_per_second: 13.161, epoch: 63.0[0m
[32m[2022-08-31 19:20:49,427] [    INFO][0m - loss: 1e-08, learning_rate: 1.095e-05, global_step: 1270, interval_runtime: 0.877, interval_samples_per_second: 9.123, interval_steps_per_second: 11.403, epoch: 63.5[0m
[32m[2022-08-31 19:20:50,179] [    INFO][0m - loss: 1e-08, learning_rate: 1.08e-05, global_step: 1280, interval_runtime: 0.7526, interval_samples_per_second: 10.63, interval_steps_per_second: 13.287, epoch: 64.0[0m
[32m[2022-08-31 19:20:51,037] [    INFO][0m - loss: 1e-08, learning_rate: 1.065e-05, global_step: 1290, interval_runtime: 0.8577, interval_samples_per_second: 9.327, interval_steps_per_second: 11.659, epoch: 64.5[0m
[32m[2022-08-31 19:20:51,810] [    INFO][0m - loss: 0.0, learning_rate: 1.05e-05, global_step: 1300, interval_runtime: 0.7729, interval_samples_per_second: 10.351, interval_steps_per_second: 12.938, epoch: 65.0[0m
[32m[2022-08-31 19:20:51,810] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 19:20:51,810] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-31 19:20:51,811] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 19:20:51,811] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 19:20:51,811] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 19:20:52,370] [    INFO][0m - eval_loss: 6.9477410316467285, eval_accuracy: 0.5220125786163522, eval_runtime: 0.5591, eval_samples_per_second: 284.403, eval_steps_per_second: 8.943, epoch: 65.0[0m
[32m[2022-08-31 19:20:52,370] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1300[0m
[32m[2022-08-31 19:20:52,371] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 19:20:54,264] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1300/tokenizer_config.json[0m
[32m[2022-08-31 19:20:54,265] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1300/special_tokens_map.json[0m
[32m[2022-08-31 19:20:59,832] [    INFO][0m - loss: 2e-08, learning_rate: 1.035e-05, global_step: 1310, interval_runtime: 8.0223, interval_samples_per_second: 0.997, interval_steps_per_second: 1.247, epoch: 65.5[0m
[32m[2022-08-31 19:21:00,640] [    INFO][0m - loss: 2e-08, learning_rate: 1.02e-05, global_step: 1320, interval_runtime: 0.8079, interval_samples_per_second: 9.902, interval_steps_per_second: 12.377, epoch: 66.0[0m
[32m[2022-08-31 19:21:01,504] [    INFO][0m - loss: 1e-08, learning_rate: 1.005e-05, global_step: 1330, interval_runtime: 0.863, interval_samples_per_second: 9.27, interval_steps_per_second: 11.587, epoch: 66.5[0m
[32m[2022-08-31 19:21:02,280] [    INFO][0m - loss: 0.0, learning_rate: 9.9e-06, global_step: 1340, interval_runtime: 0.777, interval_samples_per_second: 10.296, interval_steps_per_second: 12.87, epoch: 67.0[0m
[32m[2022-08-31 19:21:03,164] [    INFO][0m - loss: 0.0, learning_rate: 9.75e-06, global_step: 1350, interval_runtime: 0.8836, interval_samples_per_second: 9.054, interval_steps_per_second: 11.317, epoch: 67.5[0m
[32m[2022-08-31 19:21:03,918] [    INFO][0m - loss: 6e-08, learning_rate: 9.600000000000001e-06, global_step: 1360, interval_runtime: 0.7544, interval_samples_per_second: 10.605, interval_steps_per_second: 13.256, epoch: 68.0[0m
[32m[2022-08-31 19:21:04,799] [    INFO][0m - loss: 0.0, learning_rate: 9.450000000000001e-06, global_step: 1370, interval_runtime: 0.8814, interval_samples_per_second: 9.077, interval_steps_per_second: 11.346, epoch: 68.5[0m
[32m[2022-08-31 19:21:05,548] [    INFO][0m - loss: 0.0, learning_rate: 9.3e-06, global_step: 1380, interval_runtime: 0.7488, interval_samples_per_second: 10.684, interval_steps_per_second: 13.355, epoch: 69.0[0m
[32m[2022-08-31 19:21:06,403] [    INFO][0m - loss: 1.3e-07, learning_rate: 9.15e-06, global_step: 1390, interval_runtime: 0.8547, interval_samples_per_second: 9.361, interval_steps_per_second: 11.701, epoch: 69.5[0m
[32m[2022-08-31 19:21:07,167] [    INFO][0m - loss: 1e-08, learning_rate: 9e-06, global_step: 1400, interval_runtime: 0.7638, interval_samples_per_second: 10.474, interval_steps_per_second: 13.092, epoch: 70.0[0m
[32m[2022-08-31 19:21:07,168] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 19:21:07,168] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-31 19:21:07,168] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 19:21:07,168] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 19:21:07,168] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 19:21:07,742] [    INFO][0m - eval_loss: 6.949770927429199, eval_accuracy: 0.5220125786163522, eval_runtime: 0.5734, eval_samples_per_second: 277.315, eval_steps_per_second: 8.721, epoch: 70.0[0m
[32m[2022-08-31 19:21:07,742] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1400[0m
[32m[2022-08-31 19:21:07,742] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 19:21:09,624] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1400/tokenizer_config.json[0m
[32m[2022-08-31 19:21:09,624] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1400/special_tokens_map.json[0m
[32m[2022-08-31 19:21:14,670] [    INFO][0m - loss: 1e-08, learning_rate: 8.85e-06, global_step: 1410, interval_runtime: 7.5027, interval_samples_per_second: 1.066, interval_steps_per_second: 1.333, epoch: 70.5[0m
[32m[2022-08-31 19:21:15,561] [    INFO][0m - loss: 0.0, learning_rate: 8.7e-06, global_step: 1420, interval_runtime: 0.8914, interval_samples_per_second: 8.974, interval_steps_per_second: 11.218, epoch: 71.0[0m
[32m[2022-08-31 19:21:17,154] [    INFO][0m - loss: 0.0, learning_rate: 8.55e-06, global_step: 1430, interval_runtime: 1.5931, interval_samples_per_second: 5.022, interval_steps_per_second: 6.277, epoch: 71.5[0m
[32m[2022-08-31 19:21:18,596] [    INFO][0m - loss: 1e-08, learning_rate: 8.400000000000001e-06, global_step: 1440, interval_runtime: 1.4415, interval_samples_per_second: 5.55, interval_steps_per_second: 6.937, epoch: 72.0[0m
[32m[2022-08-31 19:21:20,033] [    INFO][0m - loss: 0.0, learning_rate: 8.25e-06, global_step: 1450, interval_runtime: 1.4369, interval_samples_per_second: 5.568, interval_steps_per_second: 6.959, epoch: 72.5[0m
[32m[2022-08-31 19:21:21,435] [    INFO][0m - loss: 5e-08, learning_rate: 8.1e-06, global_step: 1460, interval_runtime: 1.4021, interval_samples_per_second: 5.706, interval_steps_per_second: 7.132, epoch: 73.0[0m
[32m[2022-08-31 19:21:22,819] [    INFO][0m - loss: 5e-08, learning_rate: 7.95e-06, global_step: 1470, interval_runtime: 1.3839, interval_samples_per_second: 5.781, interval_steps_per_second: 7.226, epoch: 73.5[0m
[32m[2022-08-31 19:21:24,234] [    INFO][0m - loss: 1e-08, learning_rate: 7.8e-06, global_step: 1480, interval_runtime: 1.4152, interval_samples_per_second: 5.653, interval_steps_per_second: 7.066, epoch: 74.0[0m
[32m[2022-08-31 19:21:25,686] [    INFO][0m - loss: 0.0, learning_rate: 7.65e-06, global_step: 1490, interval_runtime: 1.452, interval_samples_per_second: 5.51, interval_steps_per_second: 6.887, epoch: 74.5[0m
[32m[2022-08-31 19:21:26,932] [    INFO][0m - loss: 0.0, learning_rate: 7.5e-06, global_step: 1500, interval_runtime: 1.2463, interval_samples_per_second: 6.419, interval_steps_per_second: 8.024, epoch: 75.0[0m
[32m[2022-08-31 19:21:26,933] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 19:21:26,933] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-31 19:21:26,933] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 19:21:26,933] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 19:21:26,933] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 19:21:27,756] [    INFO][0m - eval_loss: 6.951115131378174, eval_accuracy: 0.5220125786163522, eval_runtime: 0.8226, eval_samples_per_second: 193.282, eval_steps_per_second: 6.078, epoch: 75.0[0m
[32m[2022-08-31 19:21:27,757] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1500[0m
[32m[2022-08-31 19:21:27,757] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 19:21:30,012] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1500/tokenizer_config.json[0m
[32m[2022-08-31 19:21:30,012] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1500/special_tokens_map.json[0m
[32m[2022-08-31 19:21:38,840] [    INFO][0m - loss: 0.0, learning_rate: 7.35e-06, global_step: 1510, interval_runtime: 11.9075, interval_samples_per_second: 0.672, interval_steps_per_second: 0.84, epoch: 75.5[0m
[32m[2022-08-31 19:21:40,240] [    INFO][0m - loss: 1e-07, learning_rate: 7.2e-06, global_step: 1520, interval_runtime: 1.4004, interval_samples_per_second: 5.712, interval_steps_per_second: 7.141, epoch: 76.0[0m
[32m[2022-08-31 19:21:41,438] [    INFO][0m - loss: 3e-08, learning_rate: 7.049999999999999e-06, global_step: 1530, interval_runtime: 1.1984, interval_samples_per_second: 6.675, interval_steps_per_second: 8.344, epoch: 76.5[0m
[32m[2022-08-31 19:21:42,517] [    INFO][0m - loss: 1e-08, learning_rate: 6.900000000000001e-06, global_step: 1540, interval_runtime: 1.0782, interval_samples_per_second: 7.42, interval_steps_per_second: 9.275, epoch: 77.0[0m
[32m[2022-08-31 19:21:43,820] [    INFO][0m - loss: 1e-08, learning_rate: 6.750000000000001e-06, global_step: 1550, interval_runtime: 1.3029, interval_samples_per_second: 6.14, interval_steps_per_second: 7.675, epoch: 77.5[0m
[32m[2022-08-31 19:21:44,920] [    INFO][0m - loss: 0.0, learning_rate: 6.6e-06, global_step: 1560, interval_runtime: 1.1008, interval_samples_per_second: 7.267, interval_steps_per_second: 9.084, epoch: 78.0[0m
[32m[2022-08-31 19:21:46,388] [    INFO][0m - loss: 8e-08, learning_rate: 6.45e-06, global_step: 1570, interval_runtime: 1.4671, interval_samples_per_second: 5.453, interval_steps_per_second: 6.816, epoch: 78.5[0m
[32m[2022-08-31 19:21:47,578] [    INFO][0m - loss: 0.0, learning_rate: 6.3e-06, global_step: 1580, interval_runtime: 1.1906, interval_samples_per_second: 6.719, interval_steps_per_second: 8.399, epoch: 79.0[0m
[32m[2022-08-31 19:21:48,697] [    INFO][0m - loss: 1e-08, learning_rate: 6.1499999999999996e-06, global_step: 1590, interval_runtime: 1.1189, interval_samples_per_second: 7.15, interval_steps_per_second: 8.937, epoch: 79.5[0m
[32m[2022-08-31 19:21:49,830] [    INFO][0m - loss: 0.0, learning_rate: 6e-06, global_step: 1600, interval_runtime: 1.1329, interval_samples_per_second: 7.061, interval_steps_per_second: 8.827, epoch: 80.0[0m
[32m[2022-08-31 19:21:49,831] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 19:21:49,831] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-31 19:21:49,831] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 19:21:49,831] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 19:21:49,831] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 19:21:50,591] [    INFO][0m - eval_loss: 6.952639579772949, eval_accuracy: 0.5220125786163522, eval_runtime: 0.7594, eval_samples_per_second: 209.369, eval_steps_per_second: 6.584, epoch: 80.0[0m
[32m[2022-08-31 19:21:50,591] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1600[0m
[32m[2022-08-31 19:21:50,591] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 19:21:52,494] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1600/tokenizer_config.json[0m
[32m[2022-08-31 19:21:52,495] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1600/special_tokens_map.json[0m
[32m[2022-08-31 19:21:57,991] [    INFO][0m - loss: 1e-08, learning_rate: 5.850000000000001e-06, global_step: 1610, interval_runtime: 8.1604, interval_samples_per_second: 0.98, interval_steps_per_second: 1.225, epoch: 80.5[0m
[32m[2022-08-31 19:21:58,857] [    INFO][0m - loss: 0.0, learning_rate: 5.7000000000000005e-06, global_step: 1620, interval_runtime: 0.8663, interval_samples_per_second: 9.235, interval_steps_per_second: 11.544, epoch: 81.0[0m
[32m[2022-08-31 19:22:00,194] [    INFO][0m - loss: 0.0, learning_rate: 5.55e-06, global_step: 1630, interval_runtime: 1.3375, interval_samples_per_second: 5.981, interval_steps_per_second: 7.476, epoch: 81.5[0m
[32m[2022-08-31 19:22:01,025] [    INFO][0m - loss: 0.0, learning_rate: 5.4e-06, global_step: 1640, interval_runtime: 0.8305, interval_samples_per_second: 9.632, interval_steps_per_second: 12.041, epoch: 82.0[0m
[32m[2022-08-31 19:22:01,908] [    INFO][0m - loss: 0.0, learning_rate: 5.25e-06, global_step: 1650, interval_runtime: 0.883, interval_samples_per_second: 9.06, interval_steps_per_second: 11.325, epoch: 82.5[0m
[32m[2022-08-31 19:22:02,662] [    INFO][0m - loss: 0.0, learning_rate: 5.1e-06, global_step: 1660, interval_runtime: 0.7539, interval_samples_per_second: 10.612, interval_steps_per_second: 13.265, epoch: 83.0[0m
[32m[2022-08-31 19:22:03,579] [    INFO][0m - loss: 1.4e-06, learning_rate: 4.95e-06, global_step: 1670, interval_runtime: 0.9165, interval_samples_per_second: 8.729, interval_steps_per_second: 10.911, epoch: 83.5[0m
[32m[2022-08-31 19:22:04,469] [    INFO][0m - loss: 3e-08, learning_rate: 4.800000000000001e-06, global_step: 1680, interval_runtime: 0.8908, interval_samples_per_second: 8.981, interval_steps_per_second: 11.226, epoch: 84.0[0m
[32m[2022-08-31 19:22:05,707] [    INFO][0m - loss: 1e-08, learning_rate: 4.65e-06, global_step: 1690, interval_runtime: 1.2376, interval_samples_per_second: 6.464, interval_steps_per_second: 8.08, epoch: 84.5[0m
[32m[2022-08-31 19:22:06,780] [    INFO][0m - loss: 0.0, learning_rate: 4.5e-06, global_step: 1700, interval_runtime: 1.0735, interval_samples_per_second: 7.452, interval_steps_per_second: 9.315, epoch: 85.0[0m
[32m[2022-08-31 19:22:06,781] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 19:22:06,781] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-31 19:22:06,781] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 19:22:06,781] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 19:22:06,781] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 19:22:07,485] [    INFO][0m - eval_loss: 6.968405246734619, eval_accuracy: 0.5094339622641509, eval_runtime: 0.7028, eval_samples_per_second: 226.239, eval_steps_per_second: 7.114, epoch: 85.0[0m
[32m[2022-08-31 19:22:07,486] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1700[0m
[32m[2022-08-31 19:22:07,486] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 19:22:11,937] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1700/tokenizer_config.json[0m
[32m[2022-08-31 19:22:11,937] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1700/special_tokens_map.json[0m
[32m[2022-08-31 19:22:18,570] [    INFO][0m - loss: 3e-08, learning_rate: 4.35e-06, global_step: 1710, interval_runtime: 11.7898, interval_samples_per_second: 0.679, interval_steps_per_second: 0.848, epoch: 85.5[0m
[32m[2022-08-31 19:22:19,554] [    INFO][0m - loss: 3e-08, learning_rate: 4.2000000000000004e-06, global_step: 1720, interval_runtime: 0.9841, interval_samples_per_second: 8.129, interval_steps_per_second: 10.161, epoch: 86.0[0m
[32m[2022-08-31 19:22:20,713] [    INFO][0m - loss: 0.0, learning_rate: 4.05e-06, global_step: 1730, interval_runtime: 1.1584, interval_samples_per_second: 6.906, interval_steps_per_second: 8.632, epoch: 86.5[0m
[32m[2022-08-31 19:22:21,766] [    INFO][0m - loss: 4.1e-07, learning_rate: 3.9e-06, global_step: 1740, interval_runtime: 1.0527, interval_samples_per_second: 7.599, interval_steps_per_second: 9.499, epoch: 87.0[0m
[32m[2022-08-31 19:22:22,968] [    INFO][0m - loss: 3e-08, learning_rate: 3.75e-06, global_step: 1750, interval_runtime: 1.2032, interval_samples_per_second: 6.649, interval_steps_per_second: 8.311, epoch: 87.5[0m
[32m[2022-08-31 19:22:24,032] [    INFO][0m - loss: 0.0, learning_rate: 3.6e-06, global_step: 1760, interval_runtime: 1.0629, interval_samples_per_second: 7.527, interval_steps_per_second: 9.409, epoch: 88.0[0m
[32m[2022-08-31 19:22:25,508] [    INFO][0m - loss: 4e-08, learning_rate: 3.4500000000000004e-06, global_step: 1770, interval_runtime: 1.4764, interval_samples_per_second: 5.418, interval_steps_per_second: 6.773, epoch: 88.5[0m
[32m[2022-08-31 19:22:26,429] [    INFO][0m - loss: 3e-08, learning_rate: 3.3e-06, global_step: 1780, interval_runtime: 0.921, interval_samples_per_second: 8.686, interval_steps_per_second: 10.858, epoch: 89.0[0m
[32m[2022-08-31 19:22:27,612] [    INFO][0m - loss: 1e-08, learning_rate: 3.15e-06, global_step: 1790, interval_runtime: 1.1829, interval_samples_per_second: 6.763, interval_steps_per_second: 8.454, epoch: 89.5[0m
[32m[2022-08-31 19:22:28,577] [    INFO][0m - loss: 2.3e-07, learning_rate: 3e-06, global_step: 1800, interval_runtime: 0.9657, interval_samples_per_second: 8.284, interval_steps_per_second: 10.355, epoch: 90.0[0m
[32m[2022-08-31 19:22:28,578] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 19:22:28,579] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-31 19:22:28,579] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 19:22:28,579] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 19:22:28,579] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 19:22:29,151] [    INFO][0m - eval_loss: 6.970396041870117, eval_accuracy: 0.5094339622641509, eval_runtime: 0.5721, eval_samples_per_second: 277.94, eval_steps_per_second: 8.74, epoch: 90.0[0m
[32m[2022-08-31 19:22:29,152] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1800[0m
[32m[2022-08-31 19:22:29,152] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 19:22:31,058] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1800/tokenizer_config.json[0m
[32m[2022-08-31 19:22:31,059] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1800/special_tokens_map.json[0m
[32m[2022-08-31 19:22:36,939] [    INFO][0m - loss: 1e-08, learning_rate: 2.8500000000000002e-06, global_step: 1810, interval_runtime: 8.3618, interval_samples_per_second: 0.957, interval_steps_per_second: 1.196, epoch: 90.5[0m
[32m[2022-08-31 19:22:39,161] [    INFO][0m - loss: 0.0, learning_rate: 2.7e-06, global_step: 1820, interval_runtime: 0.9584, interval_samples_per_second: 8.348, interval_steps_per_second: 10.435, epoch: 91.0[0m
[32m[2022-08-31 19:22:40,136] [    INFO][0m - loss: 0.0, learning_rate: 2.55e-06, global_step: 1830, interval_runtime: 2.2377, interval_samples_per_second: 3.575, interval_steps_per_second: 4.469, epoch: 91.5[0m
[32m[2022-08-31 19:22:41,106] [    INFO][0m - loss: 6e-08, learning_rate: 2.4000000000000003e-06, global_step: 1840, interval_runtime: 0.9707, interval_samples_per_second: 8.242, interval_steps_per_second: 10.302, epoch: 92.0[0m
[32m[2022-08-31 19:22:42,295] [    INFO][0m - loss: 1e-08, learning_rate: 2.25e-06, global_step: 1850, interval_runtime: 1.189, interval_samples_per_second: 6.728, interval_steps_per_second: 8.41, epoch: 92.5[0m
[32m[2022-08-31 19:22:43,386] [    INFO][0m - loss: 0.0, learning_rate: 2.1000000000000002e-06, global_step: 1860, interval_runtime: 1.0908, interval_samples_per_second: 7.334, interval_steps_per_second: 9.167, epoch: 93.0[0m
[32m[2022-08-31 19:22:44,574] [    INFO][0m - loss: 0.0, learning_rate: 1.95e-06, global_step: 1870, interval_runtime: 1.1877, interval_samples_per_second: 6.736, interval_steps_per_second: 8.42, epoch: 93.5[0m
[32m[2022-08-31 19:22:45,526] [    INFO][0m - loss: 0.0, learning_rate: 1.8e-06, global_step: 1880, interval_runtime: 0.9528, interval_samples_per_second: 8.397, interval_steps_per_second: 10.496, epoch: 94.0[0m
[32m[2022-08-31 19:22:46,721] [    INFO][0m - loss: 3e-08, learning_rate: 1.65e-06, global_step: 1890, interval_runtime: 1.1945, interval_samples_per_second: 6.698, interval_steps_per_second: 8.372, epoch: 94.5[0m
[32m[2022-08-31 19:22:47,660] [    INFO][0m - loss: 1e-08, learning_rate: 1.5e-06, global_step: 1900, interval_runtime: 0.939, interval_samples_per_second: 8.52, interval_steps_per_second: 10.649, epoch: 95.0[0m
[32m[2022-08-31 19:22:47,660] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 19:22:47,660] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-31 19:22:47,661] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 19:22:47,661] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 19:22:47,661] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 19:22:48,686] [    INFO][0m - eval_loss: 6.970788955688477, eval_accuracy: 0.5094339622641509, eval_runtime: 1.0246, eval_samples_per_second: 155.18, eval_steps_per_second: 4.88, epoch: 95.0[0m
[32m[2022-08-31 19:22:48,686] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1900[0m
[32m[2022-08-31 19:22:48,687] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 19:22:50,983] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1900/tokenizer_config.json[0m
[32m[2022-08-31 19:22:50,983] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1900/special_tokens_map.json[0m
[32m[2022-08-31 19:22:56,459] [    INFO][0m - loss: 1e-08, learning_rate: 1.35e-06, global_step: 1910, interval_runtime: 8.7985, interval_samples_per_second: 0.909, interval_steps_per_second: 1.137, epoch: 95.5[0m
[32m[2022-08-31 19:22:57,521] [    INFO][0m - loss: 4e-08, learning_rate: 1.2000000000000002e-06, global_step: 1920, interval_runtime: 1.063, interval_samples_per_second: 7.526, interval_steps_per_second: 9.407, epoch: 96.0[0m
[32m[2022-08-31 19:22:58,597] [    INFO][0m - loss: 1e-08, learning_rate: 1.0500000000000001e-06, global_step: 1930, interval_runtime: 1.0759, interval_samples_per_second: 7.436, interval_steps_per_second: 9.295, epoch: 96.5[0m
[32m[2022-08-31 19:22:59,356] [    INFO][0m - loss: 0.0, learning_rate: 9e-07, global_step: 1940, interval_runtime: 0.7585, interval_samples_per_second: 10.547, interval_steps_per_second: 13.183, epoch: 97.0[0m
[32m[2022-08-31 19:23:00,253] [    INFO][0m - loss: 7e-08, learning_rate: 7.5e-07, global_step: 1950, interval_runtime: 0.897, interval_samples_per_second: 8.918, interval_steps_per_second: 11.148, epoch: 97.5[0m
[32m[2022-08-31 19:23:01,011] [    INFO][0m - loss: 0.0, learning_rate: 6.000000000000001e-07, global_step: 1960, interval_runtime: 0.7579, interval_samples_per_second: 10.556, interval_steps_per_second: 13.195, epoch: 98.0[0m
[32m[2022-08-31 19:23:01,962] [    INFO][0m - loss: 1e-08, learning_rate: 4.5e-07, global_step: 1970, interval_runtime: 0.9509, interval_samples_per_second: 8.413, interval_steps_per_second: 10.516, epoch: 98.5[0m
[32m[2022-08-31 19:23:02,726] [    INFO][0m - loss: 1e-08, learning_rate: 3.0000000000000004e-07, global_step: 1980, interval_runtime: 0.7642, interval_samples_per_second: 10.469, interval_steps_per_second: 13.086, epoch: 99.0[0m
[32m[2022-08-31 19:23:03,711] [    INFO][0m - loss: 0.0, learning_rate: 1.5000000000000002e-07, global_step: 1990, interval_runtime: 0.9823, interval_samples_per_second: 8.144, interval_steps_per_second: 10.18, epoch: 99.5[0m
[32m[2022-08-31 19:23:04,601] [    INFO][0m - loss: 1e-08, learning_rate: 0.0, global_step: 2000, interval_runtime: 0.8923, interval_samples_per_second: 8.965, interval_steps_per_second: 11.207, epoch: 100.0[0m
[32m[2022-08-31 19:23:04,602] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 19:23:04,602] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-31 19:23:04,602] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 19:23:04,602] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 19:23:04,602] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 19:23:05,222] [    INFO][0m - eval_loss: 6.970869064331055, eval_accuracy: 0.5094339622641509, eval_runtime: 0.6197, eval_samples_per_second: 256.579, eval_steps_per_second: 8.069, epoch: 100.0[0m
[32m[2022-08-31 19:23:05,223] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2000[0m
[32m[2022-08-31 19:23:05,223] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 19:23:07,105] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2000/tokenizer_config.json[0m
[32m[2022-08-31 19:23:07,106] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2000/special_tokens_map.json[0m
[32m[2022-08-31 19:23:11,530] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-31 19:23:11,530] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-500 (score: 0.6037735849056604).[0m
[32m[2022-08-31 19:23:12,204] [    INFO][0m - train_runtime: 344.3813, train_samples_per_second: 46.46, train_steps_per_second: 5.808, train_loss: 0.06723337936804918, epoch: 100.0[0m
[32m[2022-08-31 19:23:12,205] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-31 19:23:12,205] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 19:23:14,097] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-31 19:23:14,098] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-31 19:23:14,099] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-31 19:23:14,099] [    INFO][0m -   epoch                    =      100.0[0m
[32m[2022-08-31 19:23:14,099] [    INFO][0m -   train_loss               =     0.0672[0m
[32m[2022-08-31 19:23:14,099] [    INFO][0m -   train_runtime            = 0:05:44.38[0m
[32m[2022-08-31 19:23:14,099] [    INFO][0m -   train_samples_per_second =      46.46[0m
[32m[2022-08-31 19:23:14,100] [    INFO][0m -   train_steps_per_second   =      5.808[0m
[32m[2022-08-31 19:23:14,116] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-31 19:23:14,116] [    INFO][0m -   Num examples = 976[0m
[32m[2022-08-31 19:23:14,116] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 19:23:14,116] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 19:23:14,116] [    INFO][0m -   Total prediction steps = 31[0m
[32m[2022-08-31 19:23:18,215] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-31 19:23:18,216] [    INFO][0m -   test_accuracy           =     0.5236[0m
[32m[2022-08-31 19:23:18,216] [    INFO][0m -   test_loss               =     7.5873[0m
[32m[2022-08-31 19:23:18,216] [    INFO][0m -   test_runtime            = 0:00:04.09[0m
[32m[2022-08-31 19:23:18,216] [    INFO][0m -   test_samples_per_second =    238.098[0m
[32m[2022-08-31 19:23:18,216] [    INFO][0m -   test_steps_per_second   =      7.563[0m
[32m[2022-08-31 19:23:18,216] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-31 19:23:18,216] [    INFO][0m -   Num examples = 290[0m
[32m[2022-08-31 19:23:18,216] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 19:23:18,216] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 19:23:18,217] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-08-31 19:23:20,109] [    INFO][0m - Predictions for cluewscf saved to ./fewclue_submit_examples.[0m
run.sh: line 65: --model_name_or_path: command not found
