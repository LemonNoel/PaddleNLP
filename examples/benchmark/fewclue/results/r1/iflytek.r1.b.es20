[33m[2022-08-31 16:26:30,340] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-31 16:26:30,340] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-31 16:26:30,340] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 16:26:30,340] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-31 16:26:30,340] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 16:26:30,341] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-31 16:26:30,341] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-31 16:26:30,341] [    INFO][0m - early_stop_patience           :20[0m
[32m[2022-08-31 16:26:30,341] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-31 16:26:30,341] [    INFO][0m - model_name_or_path            :roformer_v2_chinese_char_base[0m
[32m[2022-08-31 16:26:30,341] [    INFO][0m - [0m
[32m[2022-08-31 16:26:30,341] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 16:26:30,341] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-31 16:26:30,341] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 16:26:30,341] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-31 16:26:30,341] [    INFO][0m - prompt                        :‚Äú{'text':'text_a'}‚ÄùËøôÂè•ËØùÁöÑ‰∏ªÈ¢òÊòØ{'mask'}{'mask'}„ÄÇ[0m
[32m[2022-08-31 16:26:30,341] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-31 16:26:30,341] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-31 16:26:30,342] [    INFO][0m - task_name                     :iflytek[0m
[32m[2022-08-31 16:26:30,342] [    INFO][0m - [0m
[32m[2022-08-31 16:26:30,342] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/roformer_v2_chinese_char_base/model_state.pdparams[0m
W0831 16:26:30.343662 53315 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0831 16:26:30.347841 53315 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-31 16:26:33,298] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/roformer_v2_chinese_char_base/vocab.txt[0m
[32m[2022-08-31 16:26:33,307] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/roformer_v2_chinese_char_base/tokenizer_config.json[0m
[32m[2022-08-31 16:26:33,307] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/roformer_v2_chinese_char_base/special_tokens_map.json[0m
[32m[2022-08-31 16:26:33,308] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùËøôÂè•ËØùÁöÑ‰∏ªÈ¢òÊòØ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': '„ÄÇ'}][0m
2022-08-31 16:26:33,332 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-31 16:26:33,504] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 16:26:33,504] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-31 16:26:33,505] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 16:26:33,505] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-31 16:26:33,505] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-31 16:26:33,505] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-31 16:26:33,505] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-31 16:26:33,505] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-31 16:26:33,505] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-31 16:26:33,505] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-31 16:26:33,505] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-31 16:26:33,505] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-31 16:26:33,506] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-31 16:26:33,506] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-31 16:26:33,506] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-31 16:26:33,506] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-31 16:26:33,506] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-31 16:26:33,506] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-31 16:26:33,506] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-31 16:26:33,506] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-31 16:26:33,506] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-31 16:26:33,506] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-31 16:26:33,506] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-31 16:26:33,506] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-31 16:26:33,507] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-31 16:26:33,507] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-31 16:26:33,507] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-31 16:26:33,507] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-31 16:26:33,507] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-31 16:26:33,507] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-31 16:26:33,507] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-31 16:26:33,507] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-31 16:26:33,507] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-31 16:26:33,507] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-31 16:26:33,507] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-31 16:26:33,507] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-31 16:26:33,507] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-31 16:26:33,507] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug31_16-26-30_instance-3bwob41y-01[0m
[32m[2022-08-31 16:26:33,508] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-31 16:26:33,508] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-31 16:26:33,508] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-31 16:26:33,508] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-31 16:26:33,508] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-31 16:26:33,508] [    INFO][0m - max_seq_length                :320[0m
[32m[2022-08-31 16:26:33,508] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-31 16:26:33,508] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-31 16:26:33,508] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-31 16:26:33,508] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-31 16:26:33,508] [    INFO][0m - num_train_epochs              :100.0[0m
[32m[2022-08-31 16:26:33,508] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-31 16:26:33,508] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-31 16:26:33,509] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-31 16:26:33,509] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-31 16:26:33,509] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-31 16:26:33,509] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-31 16:26:33,509] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-31 16:26:33,509] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-31 16:26:33,509] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-31 16:26:33,509] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-31 16:26:33,509] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-31 16:26:33,509] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-31 16:26:33,509] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-31 16:26:33,509] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-31 16:26:33,509] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-31 16:26:33,509] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-31 16:26:33,510] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-31 16:26:33,510] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-31 16:26:33,510] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-31 16:26:33,510] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-31 16:26:33,510] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-31 16:26:33,510] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-31 16:26:33,510] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-31 16:26:33,510] [    INFO][0m - seed                          :42[0m
[32m[2022-08-31 16:26:33,510] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-31 16:26:33,510] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-31 16:26:33,510] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-31 16:26:33,510] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-31 16:26:33,510] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-31 16:26:33,511] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-31 16:26:33,511] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-31 16:26:33,511] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-31 16:26:33,511] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-31 16:26:33,511] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-31 16:26:33,511] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-31 16:26:33,511] [    INFO][0m - [0m
[32m[2022-08-31 16:26:33,513] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-31 16:26:33,513] [    INFO][0m -   Num examples = 3024[0m
[32m[2022-08-31 16:26:33,513] [    INFO][0m -   Num Epochs = 100[0m
[32m[2022-08-31 16:26:33,513] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-31 16:26:33,513] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-31 16:26:33,513] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-31 16:26:33,513] [    INFO][0m -   Total optimization steps = 37800.0[0m
[32m[2022-08-31 16:26:33,513] [    INFO][0m -   Total num train samples = 302400[0m
[32m[2022-08-31 16:26:35,755] [    INFO][0m - loss: 3.25898132, learning_rate: 2.999206349206349e-05, global_step: 10, interval_runtime: 2.2406, interval_samples_per_second: 3.57, interval_steps_per_second: 4.463, epoch: 0.0265[0m
[32m[2022-08-31 16:26:37,326] [    INFO][0m - loss: 2.78027935, learning_rate: 2.9984126984126986e-05, global_step: 20, interval_runtime: 1.5709, interval_samples_per_second: 5.093, interval_steps_per_second: 6.366, epoch: 0.0529[0m
[32m[2022-08-31 16:26:38,866] [    INFO][0m - loss: 2.88663139, learning_rate: 2.9976190476190477e-05, global_step: 30, interval_runtime: 1.5398, interval_samples_per_second: 5.195, interval_steps_per_second: 6.494, epoch: 0.0794[0m
[32m[2022-08-31 16:26:40,404] [    INFO][0m - loss: 2.46615429, learning_rate: 2.9968253968253967e-05, global_step: 40, interval_runtime: 1.5381, interval_samples_per_second: 5.201, interval_steps_per_second: 6.502, epoch: 0.1058[0m
[32m[2022-08-31 16:26:41,948] [    INFO][0m - loss: 2.83804111, learning_rate: 2.996031746031746e-05, global_step: 50, interval_runtime: 1.5441, interval_samples_per_second: 5.181, interval_steps_per_second: 6.476, epoch: 0.1323[0m
[32m[2022-08-31 16:26:43,490] [    INFO][0m - loss: 2.91769295, learning_rate: 2.9952380952380952e-05, global_step: 60, interval_runtime: 1.542, interval_samples_per_second: 5.188, interval_steps_per_second: 6.485, epoch: 0.1587[0m
[32m[2022-08-31 16:26:45,023] [    INFO][0m - loss: 2.81649971, learning_rate: 2.9944444444444443e-05, global_step: 70, interval_runtime: 1.5328, interval_samples_per_second: 5.219, interval_steps_per_second: 6.524, epoch: 0.1852[0m
[32m[2022-08-31 16:26:46,558] [    INFO][0m - loss: 2.07713776, learning_rate: 2.9936507936507937e-05, global_step: 80, interval_runtime: 1.5358, interval_samples_per_second: 5.209, interval_steps_per_second: 6.511, epoch: 0.2116[0m
[32m[2022-08-31 16:26:48,098] [    INFO][0m - loss: 2.27643909, learning_rate: 2.9928571428571428e-05, global_step: 90, interval_runtime: 1.5401, interval_samples_per_second: 5.195, interval_steps_per_second: 6.493, epoch: 0.2381[0m
[32m[2022-08-31 16:26:49,639] [    INFO][0m - loss: 2.33282318, learning_rate: 2.992063492063492e-05, global_step: 100, interval_runtime: 1.5405, interval_samples_per_second: 5.193, interval_steps_per_second: 6.491, epoch: 0.2646[0m
[32m[2022-08-31 16:26:49,640] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 16:26:49,640] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 16:26:49,640] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 16:26:49,640] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 16:26:49,640] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 16:27:01,441] [    INFO][0m - eval_loss: 2.116887092590332, eval_accuracy: 0.49745083758193737, eval_runtime: 11.8003, eval_samples_per_second: 116.353, eval_steps_per_second: 3.644, epoch: 0.2646[0m
[32m[2022-08-31 16:27:01,442] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-31 16:27:01,442] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 16:27:03,696] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-31 16:27:03,697] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-31 16:27:09,806] [    INFO][0m - loss: 2.06081848, learning_rate: 2.9912698412698416e-05, global_step: 110, interval_runtime: 20.1672, interval_samples_per_second: 0.397, interval_steps_per_second: 0.496, epoch: 0.291[0m
[32m[2022-08-31 16:27:11,351] [    INFO][0m - loss: 2.27537537, learning_rate: 2.9904761904761907e-05, global_step: 120, interval_runtime: 1.5453, interval_samples_per_second: 5.177, interval_steps_per_second: 6.471, epoch: 0.3175[0m
[32m[2022-08-31 16:27:12,887] [    INFO][0m - loss: 2.10109997, learning_rate: 2.9896825396825398e-05, global_step: 130, interval_runtime: 1.5362, interval_samples_per_second: 5.208, interval_steps_per_second: 6.509, epoch: 0.3439[0m
[32m[2022-08-31 16:27:14,428] [    INFO][0m - loss: 2.18250408, learning_rate: 2.9888888888888892e-05, global_step: 140, interval_runtime: 1.5408, interval_samples_per_second: 5.192, interval_steps_per_second: 6.49, epoch: 0.3704[0m
[32m[2022-08-31 16:27:15,965] [    INFO][0m - loss: 2.30416508, learning_rate: 2.9880952380952383e-05, global_step: 150, interval_runtime: 1.5363, interval_samples_per_second: 5.207, interval_steps_per_second: 6.509, epoch: 0.3968[0m
[32m[2022-08-31 16:27:17,505] [    INFO][0m - loss: 2.52104263, learning_rate: 2.9873015873015874e-05, global_step: 160, interval_runtime: 1.5401, interval_samples_per_second: 5.194, interval_steps_per_second: 6.493, epoch: 0.4233[0m
[32m[2022-08-31 16:27:19,043] [    INFO][0m - loss: 1.90954895, learning_rate: 2.9865079365079368e-05, global_step: 170, interval_runtime: 1.5378, interval_samples_per_second: 5.202, interval_steps_per_second: 6.503, epoch: 0.4497[0m
[32m[2022-08-31 16:27:20,584] [    INFO][0m - loss: 2.36899891, learning_rate: 2.985714285714286e-05, global_step: 180, interval_runtime: 1.5411, interval_samples_per_second: 5.191, interval_steps_per_second: 6.489, epoch: 0.4762[0m
[32m[2022-08-31 16:27:22,132] [    INFO][0m - loss: 1.87015114, learning_rate: 2.984920634920635e-05, global_step: 190, interval_runtime: 1.5485, interval_samples_per_second: 5.166, interval_steps_per_second: 6.458, epoch: 0.5026[0m
[32m[2022-08-31 16:27:23,678] [    INFO][0m - loss: 2.07663689, learning_rate: 2.984126984126984e-05, global_step: 200, interval_runtime: 1.5458, interval_samples_per_second: 5.175, interval_steps_per_second: 6.469, epoch: 0.5291[0m
[32m[2022-08-31 16:27:23,679] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 16:27:23,679] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 16:27:23,679] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 16:27:23,680] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 16:27:23,680] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 16:27:35,493] [    INFO][0m - eval_loss: 1.9733582735061646, eval_accuracy: 0.507647487254188, eval_runtime: 11.8131, eval_samples_per_second: 116.227, eval_steps_per_second: 3.64, epoch: 0.5291[0m
[32m[2022-08-31 16:27:35,494] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-31 16:27:35,494] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 16:27:37,583] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-31 16:27:37,584] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-31 16:27:43,548] [    INFO][0m - loss: 1.90034523, learning_rate: 2.9833333333333335e-05, global_step: 210, interval_runtime: 19.8695, interval_samples_per_second: 0.403, interval_steps_per_second: 0.503, epoch: 0.5556[0m
[32m[2022-08-31 16:27:45,096] [    INFO][0m - loss: 2.13618927, learning_rate: 2.9825396825396825e-05, global_step: 220, interval_runtime: 1.5479, interval_samples_per_second: 5.168, interval_steps_per_second: 6.46, epoch: 0.582[0m
[32m[2022-08-31 16:27:46,646] [    INFO][0m - loss: 1.94824181, learning_rate: 2.9817460317460316e-05, global_step: 230, interval_runtime: 1.5504, interval_samples_per_second: 5.16, interval_steps_per_second: 6.45, epoch: 0.6085[0m
[32m[2022-08-31 16:27:48,193] [    INFO][0m - loss: 1.78662682, learning_rate: 2.980952380952381e-05, global_step: 240, interval_runtime: 1.5472, interval_samples_per_second: 5.171, interval_steps_per_second: 6.463, epoch: 0.6349[0m
[32m[2022-08-31 16:27:49,744] [    INFO][0m - loss: 1.68384514, learning_rate: 2.98015873015873e-05, global_step: 250, interval_runtime: 1.5503, interval_samples_per_second: 5.16, interval_steps_per_second: 6.451, epoch: 0.6614[0m
[32m[2022-08-31 16:27:51,295] [    INFO][0m - loss: 1.85789261, learning_rate: 2.9793650793650792e-05, global_step: 260, interval_runtime: 1.5511, interval_samples_per_second: 5.158, interval_steps_per_second: 6.447, epoch: 0.6878[0m
[32m[2022-08-31 16:27:52,844] [    INFO][0m - loss: 1.7126894, learning_rate: 2.9785714285714286e-05, global_step: 270, interval_runtime: 1.5499, interval_samples_per_second: 5.162, interval_steps_per_second: 6.452, epoch: 0.7143[0m
[32m[2022-08-31 16:27:54,394] [    INFO][0m - loss: 2.07576923, learning_rate: 2.9777777777777777e-05, global_step: 280, interval_runtime: 1.5498, interval_samples_per_second: 5.162, interval_steps_per_second: 6.453, epoch: 0.7407[0m
[32m[2022-08-31 16:27:55,948] [    INFO][0m - loss: 1.87904205, learning_rate: 2.9769841269841268e-05, global_step: 290, interval_runtime: 1.5539, interval_samples_per_second: 5.148, interval_steps_per_second: 6.436, epoch: 0.7672[0m
[32m[2022-08-31 16:27:57,496] [    INFO][0m - loss: 1.79554787, learning_rate: 2.9761904761904762e-05, global_step: 300, interval_runtime: 1.5481, interval_samples_per_second: 5.168, interval_steps_per_second: 6.459, epoch: 0.7937[0m
[32m[2022-08-31 16:27:57,497] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 16:27:57,497] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 16:27:57,498] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 16:27:57,498] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 16:27:57,498] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 16:28:09,261] [    INFO][0m - eval_loss: 1.9043725728988647, eval_accuracy: 0.5258557902403496, eval_runtime: 11.7626, eval_samples_per_second: 116.726, eval_steps_per_second: 3.656, epoch: 0.7937[0m
[32m[2022-08-31 16:28:09,261] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-31 16:28:09,261] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 16:28:11,103] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-31 16:28:11,104] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-31 16:28:18,851] [    INFO][0m - loss: 1.94041252, learning_rate: 2.9753968253968256e-05, global_step: 310, interval_runtime: 21.3549, interval_samples_per_second: 0.375, interval_steps_per_second: 0.468, epoch: 0.8201[0m
[32m[2022-08-31 16:28:24,058] [    INFO][0m - loss: 1.64032917, learning_rate: 2.9746031746031747e-05, global_step: 320, interval_runtime: 5.2067, interval_samples_per_second: 1.536, interval_steps_per_second: 1.921, epoch: 0.8466[0m
[32m[2022-08-31 16:28:25,599] [    INFO][0m - loss: 1.86777878, learning_rate: 2.973809523809524e-05, global_step: 330, interval_runtime: 1.5414, interval_samples_per_second: 5.19, interval_steps_per_second: 6.488, epoch: 0.873[0m
[32m[2022-08-31 16:28:27,139] [    INFO][0m - loss: 1.78383808, learning_rate: 2.9730158730158732e-05, global_step: 340, interval_runtime: 1.5389, interval_samples_per_second: 5.199, interval_steps_per_second: 6.498, epoch: 0.8995[0m
[32m[2022-08-31 16:28:28,681] [    INFO][0m - loss: 2.146492, learning_rate: 2.9722222222222223e-05, global_step: 350, interval_runtime: 1.5422, interval_samples_per_second: 5.187, interval_steps_per_second: 6.484, epoch: 0.9259[0m
[32m[2022-08-31 16:28:30,210] [    INFO][0m - loss: 2.25149384, learning_rate: 2.9714285714285717e-05, global_step: 360, interval_runtime: 1.5295, interval_samples_per_second: 5.23, interval_steps_per_second: 6.538, epoch: 0.9524[0m
[32m[2022-08-31 16:28:31,746] [    INFO][0m - loss: 1.80003185, learning_rate: 2.9706349206349208e-05, global_step: 370, interval_runtime: 1.5361, interval_samples_per_second: 5.208, interval_steps_per_second: 6.51, epoch: 0.9788[0m
[32m[2022-08-31 16:28:33,419] [    INFO][0m - loss: 1.43075981, learning_rate: 2.96984126984127e-05, global_step: 380, interval_runtime: 1.6725, interval_samples_per_second: 4.783, interval_steps_per_second: 5.979, epoch: 1.0053[0m
[32m[2022-08-31 16:28:34,985] [    INFO][0m - loss: 1.32071285, learning_rate: 2.9690476190476193e-05, global_step: 390, interval_runtime: 1.566, interval_samples_per_second: 5.109, interval_steps_per_second: 6.386, epoch: 1.0317[0m
[32m[2022-08-31 16:28:36,526] [    INFO][0m - loss: 0.95992098, learning_rate: 2.9682539682539683e-05, global_step: 400, interval_runtime: 1.5419, interval_samples_per_second: 5.189, interval_steps_per_second: 6.486, epoch: 1.0582[0m
[32m[2022-08-31 16:28:36,527] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 16:28:36,527] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 16:28:36,527] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 16:28:36,527] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 16:28:36,527] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 16:28:48,369] [    INFO][0m - eval_loss: 1.9948184490203857, eval_accuracy: 0.5309541150764748, eval_runtime: 11.8413, eval_samples_per_second: 115.95, eval_steps_per_second: 3.631, epoch: 1.0582[0m
[32m[2022-08-31 16:28:48,370] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-31 16:28:48,370] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 16:28:50,179] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-31 16:28:50,180] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-31 16:28:56,053] [    INFO][0m - loss: 1.07943401, learning_rate: 2.9674603174603174e-05, global_step: 410, interval_runtime: 19.5265, interval_samples_per_second: 0.41, interval_steps_per_second: 0.512, epoch: 1.0847[0m
[32m[2022-08-31 16:28:57,592] [    INFO][0m - loss: 1.25128145, learning_rate: 2.966666666666667e-05, global_step: 420, interval_runtime: 1.5389, interval_samples_per_second: 5.198, interval_steps_per_second: 6.498, epoch: 1.1111[0m
[32m[2022-08-31 16:28:59,128] [    INFO][0m - loss: 1.09387321, learning_rate: 2.965873015873016e-05, global_step: 430, interval_runtime: 1.5362, interval_samples_per_second: 5.208, interval_steps_per_second: 6.509, epoch: 1.1376[0m
[32m[2022-08-31 16:29:00,667] [    INFO][0m - loss: 1.13894653, learning_rate: 2.965079365079365e-05, global_step: 440, interval_runtime: 1.5391, interval_samples_per_second: 5.198, interval_steps_per_second: 6.497, epoch: 1.164[0m
[32m[2022-08-31 16:29:02,206] [    INFO][0m - loss: 1.20811768, learning_rate: 2.9642857142857144e-05, global_step: 450, interval_runtime: 1.539, interval_samples_per_second: 5.198, interval_steps_per_second: 6.498, epoch: 1.1905[0m
[32m[2022-08-31 16:29:03,744] [    INFO][0m - loss: 1.29252605, learning_rate: 2.9634920634920635e-05, global_step: 460, interval_runtime: 1.5379, interval_samples_per_second: 5.202, interval_steps_per_second: 6.502, epoch: 1.2169[0m
[32m[2022-08-31 16:29:05,288] [    INFO][0m - loss: 1.28615618, learning_rate: 2.9626984126984126e-05, global_step: 470, interval_runtime: 1.5435, interval_samples_per_second: 5.183, interval_steps_per_second: 6.479, epoch: 1.2434[0m
[32m[2022-08-31 16:29:06,836] [    INFO][0m - loss: 1.13584023, learning_rate: 2.961904761904762e-05, global_step: 480, interval_runtime: 1.5482, interval_samples_per_second: 5.167, interval_steps_per_second: 6.459, epoch: 1.2698[0m
[32m[2022-08-31 16:29:08,378] [    INFO][0m - loss: 1.25216246, learning_rate: 2.961111111111111e-05, global_step: 490, interval_runtime: 1.5414, interval_samples_per_second: 5.19, interval_steps_per_second: 6.488, epoch: 1.2963[0m
[32m[2022-08-31 16:29:09,915] [    INFO][0m - loss: 1.692556, learning_rate: 2.96031746031746e-05, global_step: 500, interval_runtime: 1.5378, interval_samples_per_second: 5.202, interval_steps_per_second: 6.503, epoch: 1.3228[0m
[32m[2022-08-31 16:29:09,916] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 16:29:09,916] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 16:29:09,916] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 16:29:09,917] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 16:29:09,917] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 16:29:21,781] [    INFO][0m - eval_loss: 1.9202369451522827, eval_accuracy: 0.5455207574654042, eval_runtime: 11.8639, eval_samples_per_second: 115.729, eval_steps_per_second: 3.624, epoch: 1.3228[0m
[32m[2022-08-31 16:29:21,781] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-08-31 16:29:21,782] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 16:29:23,849] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-08-31 16:29:23,850] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-08-31 16:29:29,853] [    INFO][0m - loss: 1.65120964, learning_rate: 2.95952380952381e-05, global_step: 510, interval_runtime: 19.9379, interval_samples_per_second: 0.401, interval_steps_per_second: 0.502, epoch: 1.3492[0m
[32m[2022-08-31 16:29:31,391] [    INFO][0m - loss: 1.18495378, learning_rate: 2.958730158730159e-05, global_step: 520, interval_runtime: 1.5378, interval_samples_per_second: 5.202, interval_steps_per_second: 6.503, epoch: 1.3757[0m
[32m[2022-08-31 16:29:32,938] [    INFO][0m - loss: 1.07248945, learning_rate: 2.957936507936508e-05, global_step: 530, interval_runtime: 1.547, interval_samples_per_second: 5.171, interval_steps_per_second: 6.464, epoch: 1.4021[0m
[32m[2022-08-31 16:29:34,491] [    INFO][0m - loss: 1.31633444, learning_rate: 2.9571428571428575e-05, global_step: 540, interval_runtime: 1.5529, interval_samples_per_second: 5.152, interval_steps_per_second: 6.44, epoch: 1.4286[0m
[32m[2022-08-31 16:29:36,035] [    INFO][0m - loss: 1.2194994, learning_rate: 2.9563492063492066e-05, global_step: 550, interval_runtime: 1.544, interval_samples_per_second: 5.181, interval_steps_per_second: 6.477, epoch: 1.455[0m
[32m[2022-08-31 16:29:37,575] [    INFO][0m - loss: 1.28498631, learning_rate: 2.9555555555555556e-05, global_step: 560, interval_runtime: 1.54, interval_samples_per_second: 5.195, interval_steps_per_second: 6.494, epoch: 1.4815[0m
[32m[2022-08-31 16:29:39,104] [    INFO][0m - loss: 1.33323822, learning_rate: 2.954761904761905e-05, global_step: 570, interval_runtime: 1.5294, interval_samples_per_second: 5.231, interval_steps_per_second: 6.539, epoch: 1.5079[0m
[32m[2022-08-31 16:29:40,649] [    INFO][0m - loss: 1.31964569, learning_rate: 2.953968253968254e-05, global_step: 580, interval_runtime: 1.545, interval_samples_per_second: 5.178, interval_steps_per_second: 6.472, epoch: 1.5344[0m
[32m[2022-08-31 16:29:42,196] [    INFO][0m - loss: 1.69607773, learning_rate: 2.9531746031746032e-05, global_step: 590, interval_runtime: 1.5465, interval_samples_per_second: 5.173, interval_steps_per_second: 6.466, epoch: 1.5608[0m
[32m[2022-08-31 16:29:43,744] [    INFO][0m - loss: 1.38795795, learning_rate: 2.9523809523809523e-05, global_step: 600, interval_runtime: 1.5479, interval_samples_per_second: 5.168, interval_steps_per_second: 6.46, epoch: 1.5873[0m
[32m[2022-08-31 16:29:43,745] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 16:29:43,745] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 16:29:43,745] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 16:29:43,745] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 16:29:43,745] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 16:29:55,613] [    INFO][0m - eval_loss: 1.8691092729568481, eval_accuracy: 0.5593590677348871, eval_runtime: 11.8679, eval_samples_per_second: 115.69, eval_steps_per_second: 3.623, epoch: 1.5873[0m
[32m[2022-08-31 16:29:55,614] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-08-31 16:29:55,615] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 16:29:58,347] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-08-31 16:29:58,347] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-08-31 16:30:04,230] [    INFO][0m - loss: 1.35634794, learning_rate: 2.9515873015873017e-05, global_step: 610, interval_runtime: 20.4857, interval_samples_per_second: 0.391, interval_steps_per_second: 0.488, epoch: 1.6138[0m
[32m[2022-08-31 16:30:05,768] [    INFO][0m - loss: 1.3879734, learning_rate: 2.9507936507936508e-05, global_step: 620, interval_runtime: 1.5386, interval_samples_per_second: 5.199, interval_steps_per_second: 6.499, epoch: 1.6402[0m
[32m[2022-08-31 16:30:07,306] [    INFO][0m - loss: 1.08152189, learning_rate: 2.95e-05, global_step: 630, interval_runtime: 1.5376, interval_samples_per_second: 5.203, interval_steps_per_second: 6.504, epoch: 1.6667[0m
[32m[2022-08-31 16:30:08,852] [    INFO][0m - loss: 1.36469755, learning_rate: 2.9492063492063493e-05, global_step: 640, interval_runtime: 1.5462, interval_samples_per_second: 5.174, interval_steps_per_second: 6.468, epoch: 1.6931[0m
[32m[2022-08-31 16:30:10,394] [    INFO][0m - loss: 1.36016636, learning_rate: 2.9484126984126984e-05, global_step: 650, interval_runtime: 1.5424, interval_samples_per_second: 5.187, interval_steps_per_second: 6.484, epoch: 1.7196[0m
[32m[2022-08-31 16:30:11,939] [    INFO][0m - loss: 1.58899965, learning_rate: 2.9476190476190475e-05, global_step: 660, interval_runtime: 1.5445, interval_samples_per_second: 5.18, interval_steps_per_second: 6.475, epoch: 1.746[0m
[32m[2022-08-31 16:30:13,484] [    INFO][0m - loss: 1.33436565, learning_rate: 2.946825396825397e-05, global_step: 670, interval_runtime: 1.5442, interval_samples_per_second: 5.181, interval_steps_per_second: 6.476, epoch: 1.7725[0m
[32m[2022-08-31 16:30:15,036] [    INFO][0m - loss: 1.62144375, learning_rate: 2.946031746031746e-05, global_step: 680, interval_runtime: 1.5527, interval_samples_per_second: 5.152, interval_steps_per_second: 6.441, epoch: 1.7989[0m
[32m[2022-08-31 16:30:16,581] [    INFO][0m - loss: 1.29535494, learning_rate: 2.945238095238095e-05, global_step: 690, interval_runtime: 1.5453, interval_samples_per_second: 5.177, interval_steps_per_second: 6.471, epoch: 1.8254[0m
[32m[2022-08-31 16:30:18,124] [    INFO][0m - loss: 1.27209511, learning_rate: 2.9444444444444445e-05, global_step: 700, interval_runtime: 1.5429, interval_samples_per_second: 5.185, interval_steps_per_second: 6.481, epoch: 1.8519[0m
[32m[2022-08-31 16:30:18,125] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 16:30:18,125] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 16:30:18,125] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 16:30:18,125] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 16:30:18,125] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 16:30:30,016] [    INFO][0m - eval_loss: 1.9513115882873535, eval_accuracy: 0.5309541150764748, eval_runtime: 11.89, eval_samples_per_second: 115.475, eval_steps_per_second: 3.616, epoch: 1.8519[0m
[32m[2022-08-31 16:30:30,016] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-700[0m
[32m[2022-08-31 16:30:30,016] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 16:30:31,786] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-700/tokenizer_config.json[0m
[32m[2022-08-31 16:30:31,787] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-700/special_tokens_map.json[0m
[32m[2022-08-31 16:30:37,380] [    INFO][0m - loss: 1.3439436, learning_rate: 2.943650793650794e-05, global_step: 710, interval_runtime: 19.2556, interval_samples_per_second: 0.415, interval_steps_per_second: 0.519, epoch: 1.8783[0m
[32m[2022-08-31 16:30:38,920] [    INFO][0m - loss: 1.18975182, learning_rate: 2.942857142857143e-05, global_step: 720, interval_runtime: 1.5408, interval_samples_per_second: 5.192, interval_steps_per_second: 6.49, epoch: 1.9048[0m
[32m[2022-08-31 16:30:40,464] [    INFO][0m - loss: 1.43019791, learning_rate: 2.9420634920634924e-05, global_step: 730, interval_runtime: 1.5434, interval_samples_per_second: 5.184, interval_steps_per_second: 6.479, epoch: 1.9312[0m
[32m[2022-08-31 16:30:42,009] [    INFO][0m - loss: 1.42740316, learning_rate: 2.9412698412698414e-05, global_step: 740, interval_runtime: 1.5452, interval_samples_per_second: 5.177, interval_steps_per_second: 6.472, epoch: 1.9577[0m
[32m[2022-08-31 16:30:43,572] [    INFO][0m - loss: 1.05680828, learning_rate: 2.9404761904761905e-05, global_step: 750, interval_runtime: 1.5628, interval_samples_per_second: 5.119, interval_steps_per_second: 6.399, epoch: 1.9841[0m
[32m[2022-08-31 16:30:45,223] [    INFO][0m - loss: 0.86529598, learning_rate: 2.93968253968254e-05, global_step: 760, interval_runtime: 1.6513, interval_samples_per_second: 4.845, interval_steps_per_second: 6.056, epoch: 2.0106[0m
[32m[2022-08-31 16:30:46,766] [    INFO][0m - loss: 0.60332479, learning_rate: 2.938888888888889e-05, global_step: 770, interval_runtime: 1.543, interval_samples_per_second: 5.185, interval_steps_per_second: 6.481, epoch: 2.037[0m
[32m[2022-08-31 16:30:48,321] [    INFO][0m - loss: 0.73849783, learning_rate: 2.938095238095238e-05, global_step: 780, interval_runtime: 1.5544, interval_samples_per_second: 5.147, interval_steps_per_second: 6.433, epoch: 2.0635[0m
[32m[2022-08-31 16:30:49,879] [    INFO][0m - loss: 0.79543018, learning_rate: 2.9373015873015875e-05, global_step: 790, interval_runtime: 1.5576, interval_samples_per_second: 5.136, interval_steps_per_second: 6.42, epoch: 2.0899[0m
[32m[2022-08-31 16:30:51,420] [    INFO][0m - loss: 0.57479696, learning_rate: 2.9365079365079366e-05, global_step: 800, interval_runtime: 1.5417, interval_samples_per_second: 5.189, interval_steps_per_second: 6.487, epoch: 2.1164[0m
[32m[2022-08-31 16:30:51,420] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 16:30:51,421] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 16:30:51,421] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 16:30:51,421] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 16:30:51,421] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 16:31:03,199] [    INFO][0m - eval_loss: 2.102973699569702, eval_accuracy: 0.5484340859431901, eval_runtime: 11.7781, eval_samples_per_second: 116.572, eval_steps_per_second: 3.651, epoch: 2.1164[0m
[32m[2022-08-31 16:31:03,200] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-800[0m
[32m[2022-08-31 16:31:03,200] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 16:31:04,928] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-800/tokenizer_config.json[0m
[32m[2022-08-31 16:31:04,929] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-800/special_tokens_map.json[0m
[32m[2022-08-31 16:31:11,046] [    INFO][0m - loss: 0.6942606, learning_rate: 2.9357142857142857e-05, global_step: 810, interval_runtime: 19.626, interval_samples_per_second: 0.408, interval_steps_per_second: 0.51, epoch: 2.1429[0m
[32m[2022-08-31 16:31:12,584] [    INFO][0m - loss: 0.82782745, learning_rate: 2.934920634920635e-05, global_step: 820, interval_runtime: 1.5384, interval_samples_per_second: 5.2, interval_steps_per_second: 6.5, epoch: 2.1693[0m
[32m[2022-08-31 16:31:14,133] [    INFO][0m - loss: 0.71257033, learning_rate: 2.9341269841269842e-05, global_step: 830, interval_runtime: 1.5487, interval_samples_per_second: 5.166, interval_steps_per_second: 6.457, epoch: 2.1958[0m
[32m[2022-08-31 16:31:15,676] [    INFO][0m - loss: 0.71415124, learning_rate: 2.9333333333333333e-05, global_step: 840, interval_runtime: 1.5429, interval_samples_per_second: 5.185, interval_steps_per_second: 6.481, epoch: 2.2222[0m
[32m[2022-08-31 16:31:17,213] [    INFO][0m - loss: 0.87623081, learning_rate: 2.9325396825396827e-05, global_step: 850, interval_runtime: 1.5367, interval_samples_per_second: 5.206, interval_steps_per_second: 6.508, epoch: 2.2487[0m
[32m[2022-08-31 16:31:18,751] [    INFO][0m - loss: 0.74240751, learning_rate: 2.9317460317460318e-05, global_step: 860, interval_runtime: 1.5378, interval_samples_per_second: 5.202, interval_steps_per_second: 6.503, epoch: 2.2751[0m
[32m[2022-08-31 16:31:20,288] [    INFO][0m - loss: 0.74404092, learning_rate: 2.930952380952381e-05, global_step: 870, interval_runtime: 1.5379, interval_samples_per_second: 5.202, interval_steps_per_second: 6.502, epoch: 2.3016[0m
[32m[2022-08-31 16:31:21,827] [    INFO][0m - loss: 0.67985229, learning_rate: 2.9301587301587303e-05, global_step: 880, interval_runtime: 1.5382, interval_samples_per_second: 5.201, interval_steps_per_second: 6.501, epoch: 2.328[0m
[32m[2022-08-31 16:31:23,376] [    INFO][0m - loss: 0.7400094, learning_rate: 2.9293650793650793e-05, global_step: 890, interval_runtime: 1.549, interval_samples_per_second: 5.165, interval_steps_per_second: 6.456, epoch: 2.3545[0m
[32m[2022-08-31 16:31:24,916] [    INFO][0m - loss: 0.59632282, learning_rate: 2.9285714285714284e-05, global_step: 900, interval_runtime: 1.5399, interval_samples_per_second: 5.195, interval_steps_per_second: 6.494, epoch: 2.381[0m
[32m[2022-08-31 16:31:24,916] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 16:31:24,916] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 16:31:24,916] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 16:31:24,917] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 16:31:24,917] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 16:31:36,634] [    INFO][0m - eval_loss: 2.3127481937408447, eval_accuracy: 0.5120174799708667, eval_runtime: 11.7162, eval_samples_per_second: 117.189, eval_steps_per_second: 3.67, epoch: 2.381[0m
[32m[2022-08-31 16:31:36,635] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-900[0m
[32m[2022-08-31 16:31:36,635] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 16:31:38,726] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-900/tokenizer_config.json[0m
[32m[2022-08-31 16:31:38,726] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-900/special_tokens_map.json[0m
[32m[2022-08-31 16:31:45,078] [    INFO][0m - loss: 0.79376159, learning_rate: 2.9277777777777778e-05, global_step: 910, interval_runtime: 20.1623, interval_samples_per_second: 0.397, interval_steps_per_second: 0.496, epoch: 2.4074[0m
[32m[2022-08-31 16:31:46,623] [    INFO][0m - loss: 0.8658329, learning_rate: 2.9269841269841272e-05, global_step: 920, interval_runtime: 1.5455, interval_samples_per_second: 5.176, interval_steps_per_second: 6.47, epoch: 2.4339[0m
[32m[2022-08-31 16:31:48,169] [    INFO][0m - loss: 0.91561069, learning_rate: 2.9261904761904763e-05, global_step: 930, interval_runtime: 1.5457, interval_samples_per_second: 5.175, interval_steps_per_second: 6.469, epoch: 2.4603[0m
[32m[2022-08-31 16:31:49,725] [    INFO][0m - loss: 0.64173012, learning_rate: 2.9253968253968257e-05, global_step: 940, interval_runtime: 1.5558, interval_samples_per_second: 5.142, interval_steps_per_second: 6.428, epoch: 2.4868[0m
[32m[2022-08-31 16:31:51,288] [    INFO][0m - loss: 0.9976368, learning_rate: 2.9246031746031748e-05, global_step: 950, interval_runtime: 1.5618, interval_samples_per_second: 5.122, interval_steps_per_second: 6.403, epoch: 2.5132[0m
[32m[2022-08-31 16:31:52,846] [    INFO][0m - loss: 0.69757376, learning_rate: 2.923809523809524e-05, global_step: 960, interval_runtime: 1.5594, interval_samples_per_second: 5.13, interval_steps_per_second: 6.413, epoch: 2.5397[0m
[32m[2022-08-31 16:31:54,397] [    INFO][0m - loss: 0.842696, learning_rate: 2.9230158730158733e-05, global_step: 970, interval_runtime: 1.5507, interval_samples_per_second: 5.159, interval_steps_per_second: 6.449, epoch: 2.5661[0m
[32m[2022-08-31 16:31:55,952] [    INFO][0m - loss: 0.73820238, learning_rate: 2.9222222222222224e-05, global_step: 980, interval_runtime: 1.5544, interval_samples_per_second: 5.147, interval_steps_per_second: 6.433, epoch: 2.5926[0m
[32m[2022-08-31 16:31:57,502] [    INFO][0m - loss: 0.7496141, learning_rate: 2.9214285714285715e-05, global_step: 990, interval_runtime: 1.5501, interval_samples_per_second: 5.161, interval_steps_per_second: 6.451, epoch: 2.619[0m
[32m[2022-08-31 16:31:59,065] [    INFO][0m - loss: 0.88294134, learning_rate: 2.9206349206349206e-05, global_step: 1000, interval_runtime: 1.5634, interval_samples_per_second: 5.117, interval_steps_per_second: 6.396, epoch: 2.6455[0m
[32m[2022-08-31 16:31:59,066] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 16:31:59,066] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 16:31:59,066] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 16:31:59,066] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 16:31:59,066] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 16:32:11,064] [    INFO][0m - eval_loss: 2.310410737991333, eval_accuracy: 0.5309541150764748, eval_runtime: 11.998, eval_samples_per_second: 114.436, eval_steps_per_second: 3.584, epoch: 2.6455[0m
[32m[2022-08-31 16:32:11,065] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1000[0m
[32m[2022-08-31 16:32:11,065] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 16:32:13,273] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1000/tokenizer_config.json[0m
[32m[2022-08-31 16:32:13,273] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1000/special_tokens_map.json[0m
[32m[2022-08-31 16:32:19,406] [    INFO][0m - loss: 0.84129877, learning_rate: 2.91984126984127e-05, global_step: 1010, interval_runtime: 20.3407, interval_samples_per_second: 0.393, interval_steps_per_second: 0.492, epoch: 2.672[0m
[32m[2022-08-31 16:32:20,949] [    INFO][0m - loss: 0.94123096, learning_rate: 2.919047619047619e-05, global_step: 1020, interval_runtime: 1.5436, interval_samples_per_second: 5.183, interval_steps_per_second: 6.479, epoch: 2.6984[0m
[32m[2022-08-31 16:32:22,486] [    INFO][0m - loss: 0.8673214, learning_rate: 2.918253968253968e-05, global_step: 1030, interval_runtime: 1.5366, interval_samples_per_second: 5.206, interval_steps_per_second: 6.508, epoch: 2.7249[0m
[32m[2022-08-31 16:32:24,035] [    INFO][0m - loss: 1.02609262, learning_rate: 2.9174603174603176e-05, global_step: 1040, interval_runtime: 1.549, interval_samples_per_second: 5.165, interval_steps_per_second: 6.456, epoch: 2.7513[0m
[32m[2022-08-31 16:32:25,583] [    INFO][0m - loss: 1.1493372, learning_rate: 2.9166666666666666e-05, global_step: 1050, interval_runtime: 1.5484, interval_samples_per_second: 5.167, interval_steps_per_second: 6.458, epoch: 2.7778[0m
[32m[2022-08-31 16:32:27,134] [    INFO][0m - loss: 0.82619696, learning_rate: 2.9158730158730157e-05, global_step: 1060, interval_runtime: 1.5501, interval_samples_per_second: 5.161, interval_steps_per_second: 6.451, epoch: 2.8042[0m
[32m[2022-08-31 16:32:28,680] [    INFO][0m - loss: 0.81535225, learning_rate: 2.915079365079365e-05, global_step: 1070, interval_runtime: 1.5461, interval_samples_per_second: 5.174, interval_steps_per_second: 6.468, epoch: 2.8307[0m
[32m[2022-08-31 16:32:30,244] [    INFO][0m - loss: 0.88314142, learning_rate: 2.9142857142857142e-05, global_step: 1080, interval_runtime: 1.5639, interval_samples_per_second: 5.115, interval_steps_per_second: 6.394, epoch: 2.8571[0m
[32m[2022-08-31 16:32:31,790] [    INFO][0m - loss: 0.81441803, learning_rate: 2.9134920634920633e-05, global_step: 1090, interval_runtime: 1.5462, interval_samples_per_second: 5.174, interval_steps_per_second: 6.467, epoch: 2.8836[0m
[32m[2022-08-31 16:32:33,355] [    INFO][0m - loss: 0.75781698, learning_rate: 2.9126984126984127e-05, global_step: 1100, interval_runtime: 1.564, interval_samples_per_second: 5.115, interval_steps_per_second: 6.394, epoch: 2.9101[0m
[32m[2022-08-31 16:32:33,356] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 16:32:33,356] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 16:32:33,356] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 16:32:33,356] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 16:32:33,356] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 16:32:45,157] [    INFO][0m - eval_loss: 2.153599739074707, eval_accuracy: 0.5535324107793154, eval_runtime: 11.8011, eval_samples_per_second: 116.345, eval_steps_per_second: 3.644, epoch: 2.9101[0m
[32m[2022-08-31 16:32:45,158] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1100[0m
[32m[2022-08-31 16:32:45,158] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 16:32:47,145] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1100/tokenizer_config.json[0m
[32m[2022-08-31 16:32:47,145] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1100/special_tokens_map.json[0m
[32m[2022-08-31 16:32:53,481] [    INFO][0m - loss: 0.84458704, learning_rate: 2.9119047619047618e-05, global_step: 1110, interval_runtime: 20.1271, interval_samples_per_second: 0.397, interval_steps_per_second: 0.497, epoch: 2.9365[0m
[32m[2022-08-31 16:32:55,041] [    INFO][0m - loss: 0.81965342, learning_rate: 2.9111111111111112e-05, global_step: 1120, interval_runtime: 1.5606, interval_samples_per_second: 5.126, interval_steps_per_second: 6.408, epoch: 2.963[0m
[32m[2022-08-31 16:32:56,587] [    INFO][0m - loss: 0.9003315, learning_rate: 2.9103174603174606e-05, global_step: 1130, interval_runtime: 1.5456, interval_samples_per_second: 5.176, interval_steps_per_second: 6.47, epoch: 2.9894[0m
[32m[2022-08-31 16:32:58,223] [    INFO][0m - loss: 0.47347355, learning_rate: 2.9095238095238097e-05, global_step: 1140, interval_runtime: 1.6366, interval_samples_per_second: 4.888, interval_steps_per_second: 6.11, epoch: 3.0159[0m
[32m[2022-08-31 16:32:59,763] [    INFO][0m - loss: 0.44542346, learning_rate: 2.9087301587301588e-05, global_step: 1150, interval_runtime: 1.5382, interval_samples_per_second: 5.201, interval_steps_per_second: 6.501, epoch: 3.0423[0m
[32m[2022-08-31 16:33:01,304] [    INFO][0m - loss: 0.46322851, learning_rate: 2.9079365079365082e-05, global_step: 1160, interval_runtime: 1.5421, interval_samples_per_second: 5.188, interval_steps_per_second: 6.485, epoch: 3.0688[0m
[32m[2022-08-31 16:33:02,860] [    INFO][0m - loss: 0.37483134, learning_rate: 2.9071428571428573e-05, global_step: 1170, interval_runtime: 1.5563, interval_samples_per_second: 5.141, interval_steps_per_second: 6.426, epoch: 3.0952[0m
[32m[2022-08-31 16:33:04,423] [    INFO][0m - loss: 0.45575528, learning_rate: 2.9063492063492064e-05, global_step: 1180, interval_runtime: 1.5627, interval_samples_per_second: 5.119, interval_steps_per_second: 6.399, epoch: 3.1217[0m
[32m[2022-08-31 16:33:05,976] [    INFO][0m - loss: 0.32328103, learning_rate: 2.9055555555555558e-05, global_step: 1190, interval_runtime: 1.5531, interval_samples_per_second: 5.151, interval_steps_per_second: 6.439, epoch: 3.1481[0m
[32m[2022-08-31 16:33:07,692] [    INFO][0m - loss: 0.43831873, learning_rate: 2.904761904761905e-05, global_step: 1200, interval_runtime: 1.5473, interval_samples_per_second: 5.17, interval_steps_per_second: 6.463, epoch: 3.1746[0m
[32m[2022-08-31 16:33:07,693] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 16:33:07,694] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 16:33:07,694] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 16:33:07,694] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 16:33:07,694] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 16:33:19,513] [    INFO][0m - eval_loss: 2.5298616886138916, eval_accuracy: 0.526584122359796, eval_runtime: 11.8186, eval_samples_per_second: 116.173, eval_steps_per_second: 3.638, epoch: 3.1746[0m
[32m[2022-08-31 16:33:19,514] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1200[0m
[32m[2022-08-31 16:33:19,514] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 16:33:21,427] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1200/tokenizer_config.json[0m
[32m[2022-08-31 16:33:21,427] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1200/special_tokens_map.json[0m
[32m[2022-08-31 16:33:27,669] [    INFO][0m - loss: 0.50438933, learning_rate: 2.903968253968254e-05, global_step: 1210, interval_runtime: 20.1455, interval_samples_per_second: 0.397, interval_steps_per_second: 0.496, epoch: 3.2011[0m
[32m[2022-08-31 16:33:29,216] [    INFO][0m - loss: 0.44828758, learning_rate: 2.9031746031746034e-05, global_step: 1220, interval_runtime: 1.5472, interval_samples_per_second: 5.171, interval_steps_per_second: 6.463, epoch: 3.2275[0m
[32m[2022-08-31 16:33:30,770] [    INFO][0m - loss: 0.45660386, learning_rate: 2.9023809523809524e-05, global_step: 1230, interval_runtime: 1.5535, interval_samples_per_second: 5.15, interval_steps_per_second: 6.437, epoch: 3.254[0m
[32m[2022-08-31 16:33:32,333] [    INFO][0m - loss: 0.30937169, learning_rate: 2.9015873015873015e-05, global_step: 1240, interval_runtime: 1.5629, interval_samples_per_second: 5.119, interval_steps_per_second: 6.398, epoch: 3.2804[0m
[32m[2022-08-31 16:33:33,883] [    INFO][0m - loss: 0.57506609, learning_rate: 2.900793650793651e-05, global_step: 1250, interval_runtime: 1.5504, interval_samples_per_second: 5.16, interval_steps_per_second: 6.45, epoch: 3.3069[0m
[32m[2022-08-31 16:33:35,433] [    INFO][0m - loss: 0.5060648, learning_rate: 2.9e-05, global_step: 1260, interval_runtime: 1.5483, interval_samples_per_second: 5.167, interval_steps_per_second: 6.459, epoch: 3.3333[0m
[32m[2022-08-31 16:33:36,975] [    INFO][0m - loss: 0.35438552, learning_rate: 2.899206349206349e-05, global_step: 1270, interval_runtime: 1.5437, interval_samples_per_second: 5.182, interval_steps_per_second: 6.478, epoch: 3.3598[0m
[32m[2022-08-31 16:33:38,526] [    INFO][0m - loss: 0.39719162, learning_rate: 2.8984126984126985e-05, global_step: 1280, interval_runtime: 1.5514, interval_samples_per_second: 5.157, interval_steps_per_second: 6.446, epoch: 3.3862[0m
[32m[2022-08-31 16:33:40,082] [    INFO][0m - loss: 0.2732239, learning_rate: 2.8976190476190476e-05, global_step: 1290, interval_runtime: 1.5555, interval_samples_per_second: 5.143, interval_steps_per_second: 6.429, epoch: 3.4127[0m
[32m[2022-08-31 16:33:41,633] [    INFO][0m - loss: 0.20569754, learning_rate: 2.8968253968253967e-05, global_step: 1300, interval_runtime: 1.551, interval_samples_per_second: 5.158, interval_steps_per_second: 6.448, epoch: 3.4392[0m
[32m[2022-08-31 16:33:41,633] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 16:33:41,634] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 16:33:41,634] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 16:33:41,634] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 16:33:41,634] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 16:33:53,525] [    INFO][0m - eval_loss: 2.543318748474121, eval_accuracy: 0.5455207574654042, eval_runtime: 11.891, eval_samples_per_second: 115.466, eval_steps_per_second: 3.616, epoch: 3.4392[0m
[32m[2022-08-31 16:33:53,526] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1300[0m
[32m[2022-08-31 16:33:53,526] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 16:33:55,270] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1300/tokenizer_config.json[0m
[32m[2022-08-31 16:33:55,270] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1300/special_tokens_map.json[0m
[32m[2022-08-31 16:34:01,057] [    INFO][0m - loss: 0.25875506, learning_rate: 2.896031746031746e-05, global_step: 1310, interval_runtime: 19.4239, interval_samples_per_second: 0.412, interval_steps_per_second: 0.515, epoch: 3.4656[0m
[32m[2022-08-31 16:34:02,600] [    INFO][0m - loss: 0.45420527, learning_rate: 2.8952380952380955e-05, global_step: 1320, interval_runtime: 1.5429, interval_samples_per_second: 5.185, interval_steps_per_second: 6.481, epoch: 3.4921[0m
[32m[2022-08-31 16:34:04,156] [    INFO][0m - loss: 0.2735832, learning_rate: 2.8944444444444446e-05, global_step: 1330, interval_runtime: 1.5565, interval_samples_per_second: 5.14, interval_steps_per_second: 6.425, epoch: 3.5185[0m
[32m[2022-08-31 16:34:05,701] [    INFO][0m - loss: 0.46954851, learning_rate: 2.893650793650794e-05, global_step: 1340, interval_runtime: 1.5445, interval_samples_per_second: 5.18, interval_steps_per_second: 6.475, epoch: 3.545[0m
[32m[2022-08-31 16:34:07,254] [    INFO][0m - loss: 0.39621108, learning_rate: 2.892857142857143e-05, global_step: 1350, interval_runtime: 1.5534, interval_samples_per_second: 5.15, interval_steps_per_second: 6.437, epoch: 3.5714[0m
[32m[2022-08-31 16:34:08,807] [    INFO][0m - loss: 0.4388001, learning_rate: 2.892063492063492e-05, global_step: 1360, interval_runtime: 1.5529, interval_samples_per_second: 5.151, interval_steps_per_second: 6.439, epoch: 3.5979[0m
[32m[2022-08-31 16:34:10,367] [    INFO][0m - loss: 0.52719765, learning_rate: 2.8912698412698416e-05, global_step: 1370, interval_runtime: 1.5598, interval_samples_per_second: 5.129, interval_steps_per_second: 6.411, epoch: 3.6243[0m
[32m[2022-08-31 16:34:11,912] [    INFO][0m - loss: 0.59634686, learning_rate: 2.8904761904761907e-05, global_step: 1380, interval_runtime: 1.5452, interval_samples_per_second: 5.177, interval_steps_per_second: 6.472, epoch: 3.6508[0m
[32m[2022-08-31 16:34:13,462] [    INFO][0m - loss: 0.39557469, learning_rate: 2.8896825396825397e-05, global_step: 1390, interval_runtime: 1.55, interval_samples_per_second: 5.161, interval_steps_per_second: 6.452, epoch: 3.6772[0m
[32m[2022-08-31 16:34:15,006] [    INFO][0m - loss: 0.48923187, learning_rate: 2.8888888888888888e-05, global_step: 1400, interval_runtime: 1.5441, interval_samples_per_second: 5.181, interval_steps_per_second: 6.476, epoch: 3.7037[0m
[32m[2022-08-31 16:34:15,007] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 16:34:15,007] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 16:34:15,007] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 16:34:15,007] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 16:34:15,007] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 16:34:26,908] [    INFO][0m - eval_loss: 2.637554883956909, eval_accuracy: 0.5316824471959214, eval_runtime: 11.9006, eval_samples_per_second: 115.372, eval_steps_per_second: 3.613, epoch: 3.7037[0m
[32m[2022-08-31 16:34:26,909] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1400[0m
[32m[2022-08-31 16:34:26,909] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 16:34:29,208] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1400/tokenizer_config.json[0m
[32m[2022-08-31 16:34:29,208] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1400/special_tokens_map.json[0m
[32m[2022-08-31 16:34:35,165] [    INFO][0m - loss: 0.31794832, learning_rate: 2.8880952380952382e-05, global_step: 1410, interval_runtime: 20.1578, interval_samples_per_second: 0.397, interval_steps_per_second: 0.496, epoch: 3.7302[0m
[32m[2022-08-31 16:34:36,725] [    INFO][0m - loss: 0.37964554, learning_rate: 2.8873015873015873e-05, global_step: 1420, interval_runtime: 1.5594, interval_samples_per_second: 5.13, interval_steps_per_second: 6.413, epoch: 3.7566[0m
[32m[2022-08-31 16:34:38,283] [    INFO][0m - loss: 0.453652, learning_rate: 2.8865079365079364e-05, global_step: 1430, interval_runtime: 1.5585, interval_samples_per_second: 5.133, interval_steps_per_second: 6.416, epoch: 3.7831[0m
[32m[2022-08-31 16:34:39,844] [    INFO][0m - loss: 0.42187796, learning_rate: 2.8857142857142858e-05, global_step: 1440, interval_runtime: 1.5613, interval_samples_per_second: 5.124, interval_steps_per_second: 6.405, epoch: 3.8095[0m
[32m[2022-08-31 16:34:41,397] [    INFO][0m - loss: 0.8271841, learning_rate: 2.884920634920635e-05, global_step: 1450, interval_runtime: 1.5538, interval_samples_per_second: 5.149, interval_steps_per_second: 6.436, epoch: 3.836[0m
[32m[2022-08-31 16:34:42,966] [    INFO][0m - loss: 0.52173076, learning_rate: 2.884126984126984e-05, global_step: 1460, interval_runtime: 1.5682, interval_samples_per_second: 5.101, interval_steps_per_second: 6.377, epoch: 3.8624[0m
[32m[2022-08-31 16:34:44,528] [    INFO][0m - loss: 0.6218627, learning_rate: 2.8833333333333334e-05, global_step: 1470, interval_runtime: 1.5626, interval_samples_per_second: 5.12, interval_steps_per_second: 6.4, epoch: 3.8889[0m
[32m[2022-08-31 16:34:46,074] [    INFO][0m - loss: 0.73524694, learning_rate: 2.8825396825396825e-05, global_step: 1480, interval_runtime: 1.5455, interval_samples_per_second: 5.176, interval_steps_per_second: 6.47, epoch: 3.9153[0m
[32m[2022-08-31 16:34:47,631] [    INFO][0m - loss: 0.49991522, learning_rate: 2.8817460317460316e-05, global_step: 1490, interval_runtime: 1.557, interval_samples_per_second: 5.138, interval_steps_per_second: 6.422, epoch: 3.9418[0m
[32m[2022-08-31 16:34:49,188] [    INFO][0m - loss: 0.54521594, learning_rate: 2.880952380952381e-05, global_step: 1500, interval_runtime: 1.557, interval_samples_per_second: 5.138, interval_steps_per_second: 6.422, epoch: 3.9683[0m
[32m[2022-08-31 16:34:49,189] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 16:34:49,189] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 16:34:49,189] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 16:34:49,189] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 16:34:49,189] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 16:35:01,222] [    INFO][0m - eval_loss: 2.6321866512298584, eval_accuracy: 0.5273124544792426, eval_runtime: 12.0326, eval_samples_per_second: 114.107, eval_steps_per_second: 3.574, epoch: 3.9683[0m
[32m[2022-08-31 16:35:03,128] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1500[0m
[32m[2022-08-31 16:35:03,128] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 16:35:04,280] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1500/tokenizer_config.json[0m
[32m[2022-08-31 16:35:04,281] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1500/special_tokens_map.json[0m
[32m[2022-08-31 16:35:07,711] [    INFO][0m - loss: 0.52757974, learning_rate: 2.88015873015873e-05, global_step: 1510, interval_runtime: 18.5225, interval_samples_per_second: 0.432, interval_steps_per_second: 0.54, epoch: 3.9947[0m
[32m[2022-08-31 16:35:09,374] [    INFO][0m - loss: 0.33297024, learning_rate: 2.8793650793650795e-05, global_step: 1520, interval_runtime: 1.6617, interval_samples_per_second: 4.814, interval_steps_per_second: 6.018, epoch: 4.0212[0m
[32m[2022-08-31 16:35:10,923] [    INFO][0m - loss: 0.25293455, learning_rate: 2.878571428571429e-05, global_step: 1530, interval_runtime: 1.5504, interval_samples_per_second: 5.16, interval_steps_per_second: 6.45, epoch: 4.0476[0m
[32m[2022-08-31 16:35:12,482] [    INFO][0m - loss: 0.2782095, learning_rate: 2.877777777777778e-05, global_step: 1540, interval_runtime: 1.5589, interval_samples_per_second: 5.132, interval_steps_per_second: 6.415, epoch: 4.0741[0m
[32m[2022-08-31 16:35:14,142] [    INFO][0m - loss: 0.30484862, learning_rate: 2.876984126984127e-05, global_step: 1550, interval_runtime: 1.6611, interval_samples_per_second: 4.816, interval_steps_per_second: 6.02, epoch: 4.1005[0m
[32m[2022-08-31 16:35:15,691] [    INFO][0m - loss: 0.11587507, learning_rate: 2.8761904761904765e-05, global_step: 1560, interval_runtime: 1.5486, interval_samples_per_second: 5.166, interval_steps_per_second: 6.458, epoch: 4.127[0m
[32m[2022-08-31 16:35:17,254] [    INFO][0m - loss: 0.32446063, learning_rate: 2.8753968253968255e-05, global_step: 1570, interval_runtime: 1.5629, interval_samples_per_second: 5.119, interval_steps_per_second: 6.399, epoch: 4.1534[0m
[32m[2022-08-31 16:35:18,804] [    INFO][0m - loss: 0.17039375, learning_rate: 2.8746031746031746e-05, global_step: 1580, interval_runtime: 1.5502, interval_samples_per_second: 5.161, interval_steps_per_second: 6.451, epoch: 4.1799[0m
[32m[2022-08-31 16:35:20,354] [    INFO][0m - loss: 0.08481813, learning_rate: 2.873809523809524e-05, global_step: 1590, interval_runtime: 1.5499, interval_samples_per_second: 5.162, interval_steps_per_second: 6.452, epoch: 4.2063[0m
[32m[2022-08-31 16:35:21,917] [    INFO][0m - loss: 0.13712096, learning_rate: 2.873015873015873e-05, global_step: 1600, interval_runtime: 1.5632, interval_samples_per_second: 5.118, interval_steps_per_second: 6.397, epoch: 4.2328[0m
[32m[2022-08-31 16:35:21,919] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 16:35:21,919] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 16:35:21,919] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 16:35:21,919] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 16:35:21,919] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 16:35:33,904] [    INFO][0m - eval_loss: 2.8021678924560547, eval_accuracy: 0.5338674435542607, eval_runtime: 11.9831, eval_samples_per_second: 114.578, eval_steps_per_second: 3.588, epoch: 4.2328[0m
[32m[2022-08-31 16:35:33,905] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1600[0m
[32m[2022-08-31 16:35:33,905] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 16:35:34,865] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1600/tokenizer_config.json[0m
[32m[2022-08-31 16:35:34,866] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1600/special_tokens_map.json[0m
[32m[2022-08-31 16:35:38,537] [    INFO][0m - loss: 0.17086957, learning_rate: 2.8722222222222222e-05, global_step: 1610, interval_runtime: 16.6194, interval_samples_per_second: 0.481, interval_steps_per_second: 0.602, epoch: 4.2593[0m
[32m[2022-08-31 16:35:40,085] [    INFO][0m - loss: 0.17473292, learning_rate: 2.8714285714285716e-05, global_step: 1620, interval_runtime: 1.5483, interval_samples_per_second: 5.167, interval_steps_per_second: 6.459, epoch: 4.2857[0m
[32m[2022-08-31 16:35:41,632] [    INFO][0m - loss: 0.12448436, learning_rate: 2.8706349206349207e-05, global_step: 1630, interval_runtime: 1.5466, interval_samples_per_second: 5.173, interval_steps_per_second: 6.466, epoch: 4.3122[0m
[32m[2022-08-31 16:35:43,184] [    INFO][0m - loss: 0.15317458, learning_rate: 2.8698412698412698e-05, global_step: 1640, interval_runtime: 1.5525, interval_samples_per_second: 5.153, interval_steps_per_second: 6.441, epoch: 4.3386[0m
[32m[2022-08-31 16:35:44,734] [    INFO][0m - loss: 0.35418303, learning_rate: 2.8690476190476192e-05, global_step: 1650, interval_runtime: 1.5496, interval_samples_per_second: 5.163, interval_steps_per_second: 6.453, epoch: 4.3651[0m
[32m[2022-08-31 16:35:46,280] [    INFO][0m - loss: 0.280089, learning_rate: 2.8682539682539683e-05, global_step: 1660, interval_runtime: 1.5463, interval_samples_per_second: 5.173, interval_steps_per_second: 6.467, epoch: 4.3915[0m
[32m[2022-08-31 16:35:47,820] [    INFO][0m - loss: 0.17043879, learning_rate: 2.8674603174603174e-05, global_step: 1670, interval_runtime: 1.54, interval_samples_per_second: 5.195, interval_steps_per_second: 6.494, epoch: 4.418[0m
[32m[2022-08-31 16:35:49,363] [    INFO][0m - loss: 0.2428504, learning_rate: 2.8666666666666668e-05, global_step: 1680, interval_runtime: 1.5436, interval_samples_per_second: 5.183, interval_steps_per_second: 6.478, epoch: 4.4444[0m
[32m[2022-08-31 16:35:50,914] [    INFO][0m - loss: 0.20481598, learning_rate: 2.865873015873016e-05, global_step: 1690, interval_runtime: 1.5507, interval_samples_per_second: 5.159, interval_steps_per_second: 6.449, epoch: 4.4709[0m
[32m[2022-08-31 16:35:52,476] [    INFO][0m - loss: 0.13307004, learning_rate: 2.865079365079365e-05, global_step: 1700, interval_runtime: 1.5622, interval_samples_per_second: 5.121, interval_steps_per_second: 6.401, epoch: 4.4974[0m
[32m[2022-08-31 16:35:52,477] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 16:35:52,477] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 16:35:52,477] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 16:35:52,477] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 16:35:52,478] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 16:36:04,286] [    INFO][0m - eval_loss: 2.8415682315826416, eval_accuracy: 0.5258557902403496, eval_runtime: 11.8082, eval_samples_per_second: 116.275, eval_steps_per_second: 3.642, epoch: 4.4974[0m
[32m[2022-08-31 16:36:04,287] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1700[0m
[32m[2022-08-31 16:36:04,287] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 16:36:05,235] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1700/tokenizer_config.json[0m
[32m[2022-08-31 16:36:05,235] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1700/special_tokens_map.json[0m
[32m[2022-08-31 16:36:09,266] [    INFO][0m - loss: 0.31623321, learning_rate: 2.8642857142857144e-05, global_step: 1710, interval_runtime: 16.7898, interval_samples_per_second: 0.476, interval_steps_per_second: 0.596, epoch: 4.5238[0m
[32m[2022-08-31 16:36:10,825] [    INFO][0m - loss: 0.47453175, learning_rate: 2.8634920634920638e-05, global_step: 1720, interval_runtime: 1.5586, interval_samples_per_second: 5.133, interval_steps_per_second: 6.416, epoch: 4.5503[0m
[32m[2022-08-31 16:36:12,378] [    INFO][0m - loss: 0.24221084, learning_rate: 2.862698412698413e-05, global_step: 1730, interval_runtime: 1.5531, interval_samples_per_second: 5.151, interval_steps_per_second: 6.439, epoch: 4.5767[0m
[32m[2022-08-31 16:36:13,927] [    INFO][0m - loss: 0.32186539, learning_rate: 2.8619047619047623e-05, global_step: 1740, interval_runtime: 1.5494, interval_samples_per_second: 5.163, interval_steps_per_second: 6.454, epoch: 4.6032[0m
[32m[2022-08-31 16:36:15,480] [    INFO][0m - loss: 0.31258526, learning_rate: 2.8611111111111113e-05, global_step: 1750, interval_runtime: 1.5525, interval_samples_per_second: 5.153, interval_steps_per_second: 6.441, epoch: 4.6296[0m
[32m[2022-08-31 16:36:17,025] [    INFO][0m - loss: 0.30719371, learning_rate: 2.8603174603174604e-05, global_step: 1760, interval_runtime: 1.5444, interval_samples_per_second: 5.18, interval_steps_per_second: 6.475, epoch: 4.6561[0m
[32m[2022-08-31 16:36:18,567] [    INFO][0m - loss: 0.225488, learning_rate: 2.85952380952381e-05, global_step: 1770, interval_runtime: 1.5429, interval_samples_per_second: 5.185, interval_steps_per_second: 6.481, epoch: 4.6825[0m
[32m[2022-08-31 16:36:20,123] [    INFO][0m - loss: 0.26690013, learning_rate: 2.858730158730159e-05, global_step: 1780, interval_runtime: 1.5559, interval_samples_per_second: 5.142, interval_steps_per_second: 6.427, epoch: 4.709[0m
[32m[2022-08-31 16:36:21,684] [    INFO][0m - loss: 0.32436101, learning_rate: 2.857936507936508e-05, global_step: 1790, interval_runtime: 1.5606, interval_samples_per_second: 5.126, interval_steps_per_second: 6.408, epoch: 4.7354[0m
[32m[2022-08-31 16:36:23,233] [    INFO][0m - loss: 0.26292021, learning_rate: 2.857142857142857e-05, global_step: 1800, interval_runtime: 1.5489, interval_samples_per_second: 5.165, interval_steps_per_second: 6.456, epoch: 4.7619[0m
[32m[2022-08-31 16:36:23,234] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 16:36:23,234] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 16:36:23,234] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 16:36:23,234] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 16:36:23,234] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 16:36:35,131] [    INFO][0m - eval_loss: 2.890265941619873, eval_accuracy: 0.5251274581209031, eval_runtime: 11.8967, eval_samples_per_second: 115.41, eval_steps_per_second: 3.614, epoch: 4.7619[0m
[32m[2022-08-31 16:36:35,132] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1800[0m
[32m[2022-08-31 16:36:35,132] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 16:36:36,288] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1800/tokenizer_config.json[0m
[32m[2022-08-31 16:36:36,289] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1800/special_tokens_map.json[0m
[32m[2022-08-31 16:36:40,344] [    INFO][0m - loss: 0.28362091, learning_rate: 2.8563492063492065e-05, global_step: 1810, interval_runtime: 17.1113, interval_samples_per_second: 0.468, interval_steps_per_second: 0.584, epoch: 4.7884[0m
[32m[2022-08-31 16:36:41,895] [    INFO][0m - loss: 0.3371119, learning_rate: 2.8555555555555556e-05, global_step: 1820, interval_runtime: 1.551, interval_samples_per_second: 5.158, interval_steps_per_second: 6.447, epoch: 4.8148[0m
[32m[2022-08-31 16:36:43,442] [    INFO][0m - loss: 0.15362294, learning_rate: 2.8547619047619047e-05, global_step: 1830, interval_runtime: 1.5467, interval_samples_per_second: 5.172, interval_steps_per_second: 6.465, epoch: 4.8413[0m
[32m[2022-08-31 16:36:44,988] [    INFO][0m - loss: 0.132774, learning_rate: 2.853968253968254e-05, global_step: 1840, interval_runtime: 1.5462, interval_samples_per_second: 5.174, interval_steps_per_second: 6.467, epoch: 4.8677[0m
[32m[2022-08-31 16:36:46,581] [    INFO][0m - loss: 0.22655983, learning_rate: 2.853174603174603e-05, global_step: 1850, interval_runtime: 1.5924, interval_samples_per_second: 5.024, interval_steps_per_second: 6.28, epoch: 4.8942[0m
[32m[2022-08-31 16:36:48,150] [    INFO][0m - loss: 0.39786842, learning_rate: 2.8523809523809522e-05, global_step: 1860, interval_runtime: 1.57, interval_samples_per_second: 5.095, interval_steps_per_second: 6.369, epoch: 4.9206[0m
[32m[2022-08-31 16:36:49,722] [    INFO][0m - loss: 0.46622291, learning_rate: 2.8515873015873017e-05, global_step: 1870, interval_runtime: 1.5713, interval_samples_per_second: 5.091, interval_steps_per_second: 6.364, epoch: 4.9471[0m
[32m[2022-08-31 16:36:51,279] [    INFO][0m - loss: 0.21341827, learning_rate: 2.8507936507936507e-05, global_step: 1880, interval_runtime: 1.5567, interval_samples_per_second: 5.139, interval_steps_per_second: 6.424, epoch: 4.9735[0m
[32m[2022-08-31 16:36:52,821] [    INFO][0m - loss: 0.16527535, learning_rate: 2.8499999999999998e-05, global_step: 1890, interval_runtime: 1.5423, interval_samples_per_second: 5.187, interval_steps_per_second: 6.484, epoch: 5.0[0m
[32m[2022-08-31 16:36:54,489] [    INFO][0m - loss: 0.20856588, learning_rate: 2.8492063492063492e-05, global_step: 1900, interval_runtime: 1.6686, interval_samples_per_second: 4.795, interval_steps_per_second: 5.993, epoch: 5.0265[0m
[32m[2022-08-31 16:36:54,490] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 16:36:54,490] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 16:36:54,490] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 16:36:54,490] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 16:36:54,490] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 16:37:06,582] [    INFO][0m - eval_loss: 2.946197032928467, eval_accuracy: 0.5222141296431173, eval_runtime: 12.0912, eval_samples_per_second: 113.553, eval_steps_per_second: 3.556, epoch: 5.0265[0m
[32m[2022-08-31 16:37:06,583] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1900[0m
[32m[2022-08-31 16:37:06,583] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 16:37:07,948] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1900/tokenizer_config.json[0m
[32m[2022-08-31 16:37:07,948] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1900/special_tokens_map.json[0m
[32m[2022-08-31 16:37:12,039] [    INFO][0m - loss: 0.08037331, learning_rate: 2.8484126984126983e-05, global_step: 1910, interval_runtime: 17.5488, interval_samples_per_second: 0.456, interval_steps_per_second: 0.57, epoch: 5.0529[0m
[32m[2022-08-31 16:37:13,600] [    INFO][0m - loss: 0.15371722, learning_rate: 2.8476190476190477e-05, global_step: 1920, interval_runtime: 1.5618, interval_samples_per_second: 5.122, interval_steps_per_second: 6.403, epoch: 5.0794[0m
[32m[2022-08-31 16:37:15,152] [    INFO][0m - loss: 0.02982653, learning_rate: 2.846825396825397e-05, global_step: 1930, interval_runtime: 1.5515, interval_samples_per_second: 5.156, interval_steps_per_second: 6.445, epoch: 5.1058[0m
[32m[2022-08-31 16:37:16,705] [    INFO][0m - loss: 0.14875027, learning_rate: 2.8460317460317462e-05, global_step: 1940, interval_runtime: 1.5537, interval_samples_per_second: 5.149, interval_steps_per_second: 6.436, epoch: 5.1323[0m
[32m[2022-08-31 16:37:18,251] [    INFO][0m - loss: 0.15450876, learning_rate: 2.8452380952380953e-05, global_step: 1950, interval_runtime: 1.545, interval_samples_per_second: 5.178, interval_steps_per_second: 6.473, epoch: 5.1587[0m
[32m[2022-08-31 16:37:19,805] [    INFO][0m - loss: 0.06778213, learning_rate: 2.8444444444444447e-05, global_step: 1960, interval_runtime: 1.5549, interval_samples_per_second: 5.145, interval_steps_per_second: 6.431, epoch: 5.1852[0m
[32m[2022-08-31 16:37:21,348] [    INFO][0m - loss: 0.06293433, learning_rate: 2.8436507936507938e-05, global_step: 1970, interval_runtime: 1.5427, interval_samples_per_second: 5.186, interval_steps_per_second: 6.482, epoch: 5.2116[0m
[32m[2022-08-31 16:37:22,908] [    INFO][0m - loss: 0.10249772, learning_rate: 2.842857142857143e-05, global_step: 1980, interval_runtime: 1.5597, interval_samples_per_second: 5.129, interval_steps_per_second: 6.411, epoch: 5.2381[0m
[32m[2022-08-31 16:37:24,469] [    INFO][0m - loss: 0.2228133, learning_rate: 2.8420634920634923e-05, global_step: 1990, interval_runtime: 1.5609, interval_samples_per_second: 5.125, interval_steps_per_second: 6.407, epoch: 5.2646[0m
[32m[2022-08-31 16:37:26,028] [    INFO][0m - loss: 0.10624589, learning_rate: 2.8412698412698414e-05, global_step: 2000, interval_runtime: 1.5595, interval_samples_per_second: 5.13, interval_steps_per_second: 6.413, epoch: 5.291[0m
[32m[2022-08-31 16:37:26,029] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 16:37:26,029] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 16:37:26,029] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 16:37:26,029] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 16:37:26,029] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 16:37:38,077] [    INFO][0m - eval_loss: 3.0864953994750977, eval_accuracy: 0.5273124544792426, eval_runtime: 12.0464, eval_samples_per_second: 113.976, eval_steps_per_second: 3.57, epoch: 5.291[0m
[32m[2022-08-31 16:37:38,077] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2000[0m
[32m[2022-08-31 16:37:38,077] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 16:37:39,238] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2000/tokenizer_config.json[0m
[32m[2022-08-31 16:37:39,238] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2000/special_tokens_map.json[0m
[32m[2022-08-31 16:37:43,371] [    INFO][0m - loss: 0.10832026, learning_rate: 2.8404761904761905e-05, global_step: 2010, interval_runtime: 17.3427, interval_samples_per_second: 0.461, interval_steps_per_second: 0.577, epoch: 5.3175[0m
[32m[2022-08-31 16:37:44,917] [    INFO][0m - loss: 0.05633044, learning_rate: 2.83968253968254e-05, global_step: 2020, interval_runtime: 1.5461, interval_samples_per_second: 5.174, interval_steps_per_second: 6.468, epoch: 5.3439[0m
[32m[2022-08-31 16:37:46,469] [    INFO][0m - loss: 0.11090692, learning_rate: 2.838888888888889e-05, global_step: 2030, interval_runtime: 1.5515, interval_samples_per_second: 5.156, interval_steps_per_second: 6.445, epoch: 5.3704[0m
[32m[2022-08-31 16:37:48,028] [    INFO][0m - loss: 0.15125489, learning_rate: 2.838095238095238e-05, global_step: 2040, interval_runtime: 1.5598, interval_samples_per_second: 5.129, interval_steps_per_second: 6.411, epoch: 5.3968[0m
[32m[2022-08-31 16:37:49,596] [    INFO][0m - loss: 0.06342704, learning_rate: 2.8373015873015875e-05, global_step: 2050, interval_runtime: 1.5676, interval_samples_per_second: 5.103, interval_steps_per_second: 6.379, epoch: 5.4233[0m
[32m[2022-08-31 16:37:51,165] [    INFO][0m - loss: 0.07438819, learning_rate: 2.8365079365079365e-05, global_step: 2060, interval_runtime: 1.5694, interval_samples_per_second: 5.098, interval_steps_per_second: 6.372, epoch: 5.4497[0m
[32m[2022-08-31 16:37:52,726] [    INFO][0m - loss: 0.17387881, learning_rate: 2.8357142857142856e-05, global_step: 2070, interval_runtime: 1.5606, interval_samples_per_second: 5.126, interval_steps_per_second: 6.408, epoch: 5.4762[0m
[32m[2022-08-31 16:37:54,296] [    INFO][0m - loss: 0.13747647, learning_rate: 2.834920634920635e-05, global_step: 2080, interval_runtime: 1.5684, interval_samples_per_second: 5.101, interval_steps_per_second: 6.376, epoch: 5.5026[0m
[32m[2022-08-31 16:37:55,853] [    INFO][0m - loss: 0.15922649, learning_rate: 2.834126984126984e-05, global_step: 2090, interval_runtime: 1.5586, interval_samples_per_second: 5.133, interval_steps_per_second: 6.416, epoch: 5.5291[0m
[32m[2022-08-31 16:37:57,403] [    INFO][0m - loss: 0.1370568, learning_rate: 2.8333333333333332e-05, global_step: 2100, interval_runtime: 1.5502, interval_samples_per_second: 5.161, interval_steps_per_second: 6.451, epoch: 5.5556[0m
[32m[2022-08-31 16:37:57,404] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 16:37:57,404] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 16:37:57,404] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 16:37:57,404] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 16:37:57,404] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 16:38:09,357] [    INFO][0m - eval_loss: 3.0416486263275146, eval_accuracy: 0.5214857975236707, eval_runtime: 11.9518, eval_samples_per_second: 114.878, eval_steps_per_second: 3.598, epoch: 5.5556[0m
[32m[2022-08-31 16:38:09,357] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2100[0m
[32m[2022-08-31 16:38:09,357] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 16:38:10,716] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2100/tokenizer_config.json[0m
[32m[2022-08-31 16:38:13,313] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2100/special_tokens_map.json[0m
[32m[2022-08-31 16:38:17,425] [    INFO][0m - loss: 0.10438315, learning_rate: 2.8325396825396826e-05, global_step: 2110, interval_runtime: 20.0215, interval_samples_per_second: 0.4, interval_steps_per_second: 0.499, epoch: 5.582[0m
[32m[2022-08-31 16:38:18,964] [    INFO][0m - loss: 0.12110405, learning_rate: 2.8317460317460317e-05, global_step: 2120, interval_runtime: 1.5393, interval_samples_per_second: 5.197, interval_steps_per_second: 6.496, epoch: 5.6085[0m
[32m[2022-08-31 16:38:20,518] [    INFO][0m - loss: 0.08543775, learning_rate: 2.830952380952381e-05, global_step: 2130, interval_runtime: 1.5538, interval_samples_per_second: 5.149, interval_steps_per_second: 6.436, epoch: 5.6349[0m
[32m[2022-08-31 16:38:22,074] [    INFO][0m - loss: 0.11964741, learning_rate: 2.8301587301587305e-05, global_step: 2140, interval_runtime: 1.5565, interval_samples_per_second: 5.14, interval_steps_per_second: 6.424, epoch: 5.6614[0m
[32m[2022-08-31 16:38:23,631] [    INFO][0m - loss: 0.16909087, learning_rate: 2.8293650793650796e-05, global_step: 2150, interval_runtime: 1.5572, interval_samples_per_second: 5.137, interval_steps_per_second: 6.422, epoch: 5.6878[0m
[32m[2022-08-31 16:38:25,177] [    INFO][0m - loss: 0.06035618, learning_rate: 2.8285714285714287e-05, global_step: 2160, interval_runtime: 1.5461, interval_samples_per_second: 5.174, interval_steps_per_second: 6.468, epoch: 5.7143[0m
[32m[2022-08-31 16:38:26,724] [    INFO][0m - loss: 0.2318579, learning_rate: 2.8277777777777778e-05, global_step: 2170, interval_runtime: 1.5465, interval_samples_per_second: 5.173, interval_steps_per_second: 6.466, epoch: 5.7407[0m
[32m[2022-08-31 16:38:28,276] [    INFO][0m - loss: 0.13121153, learning_rate: 2.8269841269841272e-05, global_step: 2180, interval_runtime: 1.5524, interval_samples_per_second: 5.153, interval_steps_per_second: 6.442, epoch: 5.7672[0m
[32m[2022-08-31 16:38:29,820] [    INFO][0m - loss: 0.21970584, learning_rate: 2.8261904761904763e-05, global_step: 2190, interval_runtime: 1.5436, interval_samples_per_second: 5.183, interval_steps_per_second: 6.478, epoch: 5.7937[0m
[32m[2022-08-31 16:38:31,375] [    INFO][0m - loss: 0.20879378, learning_rate: 2.8253968253968253e-05, global_step: 2200, interval_runtime: 1.5545, interval_samples_per_second: 5.146, interval_steps_per_second: 6.433, epoch: 5.8201[0m
[32m[2022-08-31 16:38:31,375] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 16:38:31,375] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 16:38:31,375] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 16:38:31,375] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 16:38:31,376] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 16:38:43,128] [    INFO][0m - eval_loss: 3.100926160812378, eval_accuracy: 0.5360524399126001, eval_runtime: 11.7521, eval_samples_per_second: 116.83, eval_steps_per_second: 3.659, epoch: 5.8201[0m
[32m[2022-08-31 16:38:43,129] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2200[0m
[32m[2022-08-31 16:38:43,129] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 16:38:44,397] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2200/tokenizer_config.json[0m
[32m[2022-08-31 16:38:44,397] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2200/special_tokens_map.json[0m
[32m[2022-08-31 16:38:48,525] [    INFO][0m - loss: 0.04064157, learning_rate: 2.8246031746031748e-05, global_step: 2210, interval_runtime: 17.1503, interval_samples_per_second: 0.466, interval_steps_per_second: 0.583, epoch: 5.8466[0m
[32m[2022-08-31 16:38:50,081] [    INFO][0m - loss: 0.19175696, learning_rate: 2.823809523809524e-05, global_step: 2220, interval_runtime: 1.5557, interval_samples_per_second: 5.142, interval_steps_per_second: 6.428, epoch: 5.873[0m
[32m[2022-08-31 16:38:51,636] [    INFO][0m - loss: 0.21211963, learning_rate: 2.823015873015873e-05, global_step: 2230, interval_runtime: 1.5554, interval_samples_per_second: 5.143, interval_steps_per_second: 6.429, epoch: 5.8995[0m
[32m[2022-08-31 16:38:53,182] [    INFO][0m - loss: 0.07381508, learning_rate: 2.8222222222222223e-05, global_step: 2240, interval_runtime: 1.5464, interval_samples_per_second: 5.173, interval_steps_per_second: 6.467, epoch: 5.9259[0m
[32m[2022-08-31 16:38:54,728] [    INFO][0m - loss: 0.10979283, learning_rate: 2.8214285714285714e-05, global_step: 2250, interval_runtime: 1.5459, interval_samples_per_second: 5.175, interval_steps_per_second: 6.469, epoch: 5.9524[0m
[32m[2022-08-31 16:38:56,281] [    INFO][0m - loss: 0.16513829, learning_rate: 2.8206349206349205e-05, global_step: 2260, interval_runtime: 1.5527, interval_samples_per_second: 5.152, interval_steps_per_second: 6.44, epoch: 5.9788[0m
[32m[2022-08-31 16:38:57,937] [    INFO][0m - loss: 0.06666365, learning_rate: 2.81984126984127e-05, global_step: 2270, interval_runtime: 1.6565, interval_samples_per_second: 4.829, interval_steps_per_second: 6.037, epoch: 6.0053[0m
[32m[2022-08-31 16:38:59,493] [    INFO][0m - loss: 0.08911138, learning_rate: 2.819047619047619e-05, global_step: 2280, interval_runtime: 1.5552, interval_samples_per_second: 5.144, interval_steps_per_second: 6.43, epoch: 6.0317[0m
[32m[2022-08-31 16:39:01,054] [    INFO][0m - loss: 0.04828932, learning_rate: 2.818253968253968e-05, global_step: 2290, interval_runtime: 1.5608, interval_samples_per_second: 5.126, interval_steps_per_second: 6.407, epoch: 6.0582[0m
[32m[2022-08-31 16:39:02,630] [    INFO][0m - loss: 0.0340885, learning_rate: 2.8174603174603175e-05, global_step: 2300, interval_runtime: 1.5764, interval_samples_per_second: 5.075, interval_steps_per_second: 6.344, epoch: 6.0847[0m
[32m[2022-08-31 16:39:02,631] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 16:39:02,631] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 16:39:02,631] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 16:39:02,631] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 16:39:02,631] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 16:39:14,300] [    INFO][0m - eval_loss: 3.196225166320801, eval_accuracy: 0.5418790968681719, eval_runtime: 11.6685, eval_samples_per_second: 117.668, eval_steps_per_second: 3.685, epoch: 6.0847[0m
[32m[2022-08-31 16:39:14,301] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2300[0m
[32m[2022-08-31 16:39:14,301] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 16:39:15,591] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2300/tokenizer_config.json[0m
[32m[2022-08-31 16:39:15,591] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2300/special_tokens_map.json[0m
[32m[2022-08-31 16:39:19,577] [    INFO][0m - loss: 0.03236229, learning_rate: 2.8166666666666666e-05, global_step: 2310, interval_runtime: 16.9467, interval_samples_per_second: 0.472, interval_steps_per_second: 0.59, epoch: 6.1111[0m
[32m[2022-08-31 16:39:21,125] [    INFO][0m - loss: 0.08197283, learning_rate: 2.8158730158730157e-05, global_step: 2320, interval_runtime: 1.549, interval_samples_per_second: 5.165, interval_steps_per_second: 6.456, epoch: 6.1376[0m
[32m[2022-08-31 16:39:22,671] [    INFO][0m - loss: 0.11698953, learning_rate: 2.8150793650793654e-05, global_step: 2330, interval_runtime: 1.5454, interval_samples_per_second: 5.177, interval_steps_per_second: 6.471, epoch: 6.164[0m
[32m[2022-08-31 16:39:24,223] [    INFO][0m - loss: 0.01490709, learning_rate: 2.8142857142857145e-05, global_step: 2340, interval_runtime: 1.552, interval_samples_per_second: 5.154, interval_steps_per_second: 6.443, epoch: 6.1905[0m
[32m[2022-08-31 16:39:25,779] [    INFO][0m - loss: 0.013297, learning_rate: 2.8134920634920636e-05, global_step: 2350, interval_runtime: 1.5558, interval_samples_per_second: 5.142, interval_steps_per_second: 6.428, epoch: 6.2169[0m
[32m[2022-08-31 16:39:27,337] [    INFO][0m - loss: 0.03933641, learning_rate: 2.812698412698413e-05, global_step: 2360, interval_runtime: 1.5584, interval_samples_per_second: 5.133, interval_steps_per_second: 6.417, epoch: 6.2434[0m
[32m[2022-08-31 16:39:28,889] [    INFO][0m - loss: 0.05052884, learning_rate: 2.811904761904762e-05, global_step: 2370, interval_runtime: 1.5515, interval_samples_per_second: 5.156, interval_steps_per_second: 6.445, epoch: 6.2698[0m
[32m[2022-08-31 16:39:30,438] [    INFO][0m - loss: 0.08792042, learning_rate: 2.811111111111111e-05, global_step: 2380, interval_runtime: 1.5492, interval_samples_per_second: 5.164, interval_steps_per_second: 6.455, epoch: 6.2963[0m
[32m[2022-08-31 16:39:31,989] [    INFO][0m - loss: 0.09813594, learning_rate: 2.8103174603174606e-05, global_step: 2390, interval_runtime: 1.5513, interval_samples_per_second: 5.157, interval_steps_per_second: 6.446, epoch: 6.3228[0m
[32m[2022-08-31 16:39:33,535] [    INFO][0m - loss: 0.05846724, learning_rate: 2.8095238095238096e-05, global_step: 2400, interval_runtime: 1.5459, interval_samples_per_second: 5.175, interval_steps_per_second: 6.469, epoch: 6.3492[0m
[32m[2022-08-31 16:39:33,536] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 16:39:33,536] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 16:39:33,536] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 16:39:33,536] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 16:39:33,536] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 16:39:45,389] [    INFO][0m - eval_loss: 3.168095111846924, eval_accuracy: 0.5367807720320467, eval_runtime: 11.8526, eval_samples_per_second: 115.84, eval_steps_per_second: 3.628, epoch: 6.3492[0m
[32m[2022-08-31 16:39:45,390] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2400[0m
[32m[2022-08-31 16:39:45,390] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 16:39:46,493] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2400/tokenizer_config.json[0m
[32m[2022-08-31 16:39:46,494] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2400/special_tokens_map.json[0m
[32m[2022-08-31 16:39:51,459] [    INFO][0m - loss: 0.22370753, learning_rate: 2.8087301587301587e-05, global_step: 2410, interval_runtime: 16.9494, interval_samples_per_second: 0.472, interval_steps_per_second: 0.59, epoch: 6.3757[0m
[32m[2022-08-31 16:39:53,006] [    INFO][0m - loss: 0.07687657, learning_rate: 2.807936507936508e-05, global_step: 2420, interval_runtime: 2.5213, interval_samples_per_second: 3.173, interval_steps_per_second: 3.966, epoch: 6.4021[0m
[32m[2022-08-31 16:39:54,557] [    INFO][0m - loss: 0.17995381, learning_rate: 2.8071428571428572e-05, global_step: 2430, interval_runtime: 1.5514, interval_samples_per_second: 5.157, interval_steps_per_second: 6.446, epoch: 6.4286[0m
[32m[2022-08-31 16:39:56,114] [    INFO][0m - loss: 0.0999149, learning_rate: 2.8063492063492063e-05, global_step: 2440, interval_runtime: 1.5571, interval_samples_per_second: 5.138, interval_steps_per_second: 6.422, epoch: 6.455[0m
[32m[2022-08-31 16:39:57,672] [    INFO][0m - loss: 0.02734972, learning_rate: 2.8055555555555557e-05, global_step: 2450, interval_runtime: 1.5573, interval_samples_per_second: 5.137, interval_steps_per_second: 6.421, epoch: 6.4815[0m
[32m[2022-08-31 16:39:59,227] [    INFO][0m - loss: 0.01979509, learning_rate: 2.8047619047619048e-05, global_step: 2460, interval_runtime: 1.5551, interval_samples_per_second: 5.144, interval_steps_per_second: 6.43, epoch: 6.5079[0m
[32m[2022-08-31 16:40:00,789] [    INFO][0m - loss: 0.14428706, learning_rate: 2.803968253968254e-05, global_step: 2470, interval_runtime: 1.5618, interval_samples_per_second: 5.122, interval_steps_per_second: 6.403, epoch: 6.5344[0m
[32m[2022-08-31 16:40:02,343] [    INFO][0m - loss: 0.08178154, learning_rate: 2.8031746031746033e-05, global_step: 2480, interval_runtime: 1.5542, interval_samples_per_second: 5.147, interval_steps_per_second: 6.434, epoch: 6.5608[0m
[32m[2022-08-31 16:40:03,897] [    INFO][0m - loss: 0.1676268, learning_rate: 2.8023809523809524e-05, global_step: 2490, interval_runtime: 1.5544, interval_samples_per_second: 5.147, interval_steps_per_second: 6.433, epoch: 6.5873[0m
[32m[2022-08-31 16:40:05,438] [    INFO][0m - loss: 0.128785, learning_rate: 2.8015873015873015e-05, global_step: 2500, interval_runtime: 1.5408, interval_samples_per_second: 5.192, interval_steps_per_second: 6.49, epoch: 6.6138[0m
[32m[2022-08-31 16:40:05,439] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 16:40:05,439] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 16:40:05,439] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 16:40:05,439] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 16:40:05,439] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 16:40:17,447] [    INFO][0m - eval_loss: 3.260087490081787, eval_accuracy: 0.5375091041514931, eval_runtime: 12.0071, eval_samples_per_second: 114.349, eval_steps_per_second: 3.581, epoch: 6.6138[0m
[32m[2022-08-31 16:40:17,448] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2500[0m
[32m[2022-08-31 16:40:17,448] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 16:40:18,442] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2500/tokenizer_config.json[0m
[32m[2022-08-31 16:40:18,442] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2500/special_tokens_map.json[0m
[32m[2022-08-31 16:40:22,444] [    INFO][0m - loss: 0.04796256, learning_rate: 2.800793650793651e-05, global_step: 2510, interval_runtime: 17.0054, interval_samples_per_second: 0.47, interval_steps_per_second: 0.588, epoch: 6.6402[0m
[32m[2022-08-31 16:40:23,988] [    INFO][0m - loss: 0.01917291, learning_rate: 2.8e-05, global_step: 2520, interval_runtime: 1.5448, interval_samples_per_second: 5.179, interval_steps_per_second: 6.473, epoch: 6.6667[0m
[32m[2022-08-31 16:40:25,535] [    INFO][0m - loss: 0.10532094, learning_rate: 2.7992063492063494e-05, global_step: 2530, interval_runtime: 1.5462, interval_samples_per_second: 5.174, interval_steps_per_second: 6.468, epoch: 6.6931[0m
[32m[2022-08-31 16:40:27,075] [    INFO][0m - loss: 0.05984538, learning_rate: 2.7984126984126988e-05, global_step: 2540, interval_runtime: 1.54, interval_samples_per_second: 5.195, interval_steps_per_second: 6.494, epoch: 6.7196[0m
[32m[2022-08-31 16:40:28,643] [    INFO][0m - loss: 0.25728047, learning_rate: 2.797619047619048e-05, global_step: 2550, interval_runtime: 1.5684, interval_samples_per_second: 5.101, interval_steps_per_second: 6.376, epoch: 6.746[0m
[32m[2022-08-31 16:40:30,192] [    INFO][0m - loss: 0.02948678, learning_rate: 2.796825396825397e-05, global_step: 2560, interval_runtime: 1.5484, interval_samples_per_second: 5.167, interval_steps_per_second: 6.458, epoch: 6.7725[0m
[32m[2022-08-31 16:40:31,759] [    INFO][0m - loss: 0.07850046, learning_rate: 2.796031746031746e-05, global_step: 2570, interval_runtime: 1.5673, interval_samples_per_second: 5.104, interval_steps_per_second: 6.38, epoch: 6.7989[0m
[32m[2022-08-31 16:40:33,312] [    INFO][0m - loss: 0.03394576, learning_rate: 2.7952380952380955e-05, global_step: 2580, interval_runtime: 1.5535, interval_samples_per_second: 5.15, interval_steps_per_second: 6.437, epoch: 6.8254[0m
[32m[2022-08-31 16:40:34,866] [    INFO][0m - loss: 0.17079585, learning_rate: 2.7944444444444445e-05, global_step: 2590, interval_runtime: 1.554, interval_samples_per_second: 5.148, interval_steps_per_second: 6.435, epoch: 6.8519[0m
[32m[2022-08-31 16:40:36,417] [    INFO][0m - loss: 0.03365871, learning_rate: 2.7936507936507936e-05, global_step: 2600, interval_runtime: 1.5505, interval_samples_per_second: 5.16, interval_steps_per_second: 6.449, epoch: 6.8783[0m
[32m[2022-08-31 16:40:36,417] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 16:40:36,417] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 16:40:36,418] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 16:40:36,418] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 16:40:36,418] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 16:40:48,393] [    INFO][0m - eval_loss: 3.4113078117370605, eval_accuracy: 0.5258557902403496, eval_runtime: 11.9752, eval_samples_per_second: 114.654, eval_steps_per_second: 3.591, epoch: 6.8783[0m
[32m[2022-08-31 16:40:48,394] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2600[0m
[32m[2022-08-31 16:40:48,394] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 16:40:49,382] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2600/tokenizer_config.json[0m
[32m[2022-08-31 16:40:49,382] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2600/special_tokens_map.json[0m
[32m[2022-08-31 16:40:51,996] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-31 16:40:51,996] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-600 (score: 0.5593590677348871).[0m
[32m[2022-08-31 16:40:52,602] [    INFO][0m - train_runtime: 859.0876, train_samples_per_second: 352.001, train_steps_per_second: 44.0, train_loss: 0.7497499996710282, epoch: 6.8783[0m
[32m[2022-08-31 16:40:52,602] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-31 16:40:52,603] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 16:40:54,851] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-31 16:40:54,852] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-31 16:40:54,853] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-31 16:40:54,854] [    INFO][0m -   epoch                    =     6.8783[0m
[32m[2022-08-31 16:40:54,854] [    INFO][0m -   train_loss               =     0.7497[0m
[32m[2022-08-31 16:40:54,854] [    INFO][0m -   train_runtime            = 0:14:19.08[0m
[32m[2022-08-31 16:40:54,854] [    INFO][0m -   train_samples_per_second =    352.001[0m
[32m[2022-08-31 16:40:54,854] [    INFO][0m -   train_steps_per_second   =       44.0[0m
[32m[2022-08-31 16:40:54,865] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-31 16:40:54,865] [    INFO][0m -   Num examples = 1749[0m
[32m[2022-08-31 16:40:54,865] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 16:40:54,865] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 16:40:54,865] [    INFO][0m -   Total prediction steps = 55[0m
[32m[2022-08-31 16:41:10,103] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-31 16:41:10,103] [    INFO][0m -   test_accuracy           =     0.5563[0m
[32m[2022-08-31 16:41:10,104] [    INFO][0m -   test_loss               =     1.8771[0m
[32m[2022-08-31 16:41:10,104] [    INFO][0m -   test_runtime            = 0:00:15.23[0m
[32m[2022-08-31 16:41:10,104] [    INFO][0m -   test_samples_per_second =     114.78[0m
[32m[2022-08-31 16:41:10,104] [    INFO][0m -   test_steps_per_second   =      3.609[0m
[32m[2022-08-31 16:41:10,104] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-31 16:41:10,104] [    INFO][0m -   Num examples = 2600[0m
[32m[2022-08-31 16:41:10,104] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 16:41:10,104] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 16:41:10,104] [    INFO][0m -   Total prediction steps = 82[0m
[32m[2022-08-31 16:41:39,473] [    INFO][0m - Predictions for iflytekf saved to ./fewclue_submit_examples.[0m
run.sh: line 65: --model_name_or_path: command not found
