[33m[2022-09-01 14:11:46,993] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-09-01 14:11:46,993] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-01 14:11:46,993] [    INFO][0m - ============================================================[0m
[32m[2022-09-01 14:11:46,993] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-01 14:11:46,993] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-01 14:11:46,993] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-01 14:11:46,993] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-01 14:11:46,994] [    INFO][0m - early_stop_patience           :20[0m
[32m[2022-09-01 14:11:46,994] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-01 14:11:46,994] [    INFO][0m - model_name_or_path            :roformer_v2_chinese_char_base[0m
[32m[2022-09-01 14:11:46,994] [    INFO][0m - [0m
[32m[2022-09-01 14:11:46,994] [    INFO][0m - ============================================================[0m
[32m[2022-09-01 14:11:46,994] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-01 14:11:46,994] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-01 14:11:46,994] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-01 14:11:46,994] [    INFO][0m - prompt                        :‚Äú{'text':'text_a'}‚ÄùËøôÂè•ËØùÁöÑ‰∏ªÈ¢òÊòØ{'mask'}{'mask'}„ÄÇ[0m
[32m[2022-09-01 14:11:46,994] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-01 14:11:46,994] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-01 14:11:46,994] [    INFO][0m - task_name                     :iflytek[0m
[32m[2022-09-01 14:11:46,994] [    INFO][0m - [0m
[32m[2022-09-01 14:11:46,995] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/roformer_v2_chinese_char_base/model_state.pdparams[0m
W0901 14:11:46.996145 61424 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0901 14:11:47.000126 61424 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-01 14:11:51,320] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/roformer_v2_chinese_char_base/vocab.txt[0m
[32m[2022-09-01 14:11:51,329] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/roformer_v2_chinese_char_base/tokenizer_config.json[0m
[32m[2022-09-01 14:11:51,329] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/roformer_v2_chinese_char_base/special_tokens_map.json[0m
[32m[2022-09-01 14:11:51,330] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùËøôÂè•ËØùÁöÑ‰∏ªÈ¢òÊòØ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': '„ÄÇ'}][0m
2022-09-01 14:11:51,355 INFO [download.py:119] unique_endpoints {''}
[32m[2022-09-01 14:11:51,532] [    INFO][0m - ============================================================[0m
[32m[2022-09-01 14:11:51,532] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-01 14:11:51,532] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-01 14:11:51,532] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-01 14:11:51,532] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-01 14:11:51,533] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-01 14:11:51,533] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-01 14:11:51,533] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-01 14:11:51,533] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-01 14:11:51,533] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-01 14:11:51,533] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-01 14:11:51,533] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-01 14:11:51,533] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-01 14:11:51,533] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-01 14:11:51,533] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-01 14:11:51,533] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-01 14:11:51,533] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-01 14:11:51,533] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-01 14:11:51,534] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-09-01 14:11:51,534] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-09-01 14:11:51,534] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-09-01 14:11:51,534] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-01 14:11:51,534] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-01 14:11:51,534] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-01 14:11:51,534] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-01 14:11:51,534] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-01 14:11:51,534] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-01 14:11:51,534] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-01 14:11:51,534] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-01 14:11:51,534] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-01 14:11:51,535] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-09-01 14:11:51,535] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-01 14:11:51,535] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-01 14:11:51,535] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-01 14:11:51,535] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-01 14:11:51,535] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-01 14:11:51,535] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-01 14:11:51,535] [    INFO][0m - logging_dir                   :./checkpoints/runs/Sep01_14-11-46_instance-3bwob41y-01[0m
[32m[2022-09-01 14:11:51,535] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-01 14:11:51,535] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-01 14:11:51,535] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-01 14:11:51,535] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-01 14:11:51,535] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-01 14:11:51,536] [    INFO][0m - max_seq_length                :320[0m
[32m[2022-09-01 14:11:51,536] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-01 14:11:51,536] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-01 14:11:51,536] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-01 14:11:51,536] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-01 14:11:51,536] [    INFO][0m - num_train_epochs              :100.0[0m
[32m[2022-09-01 14:11:51,536] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-01 14:11:51,536] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-01 14:11:51,536] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-09-01 14:11:51,536] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-01 14:11:51,536] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-01 14:11:51,536] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-09-01 14:11:51,536] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-09-01 14:11:51,536] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-01 14:11:51,537] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-01 14:11:51,537] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-01 14:11:51,537] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-09-01 14:11:51,537] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-01 14:11:51,537] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-01 14:11:51,537] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-01 14:11:51,537] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-01 14:11:51,537] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-01 14:11:51,537] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-01 14:11:51,537] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-09-01 14:11:51,537] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-01 14:11:51,537] [    INFO][0m - save_steps                    :100[0m
[32m[2022-09-01 14:11:51,538] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-09-01 14:11:51,538] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-01 14:11:51,538] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-01 14:11:51,538] [    INFO][0m - seed                          :42[0m
[32m[2022-09-01 14:11:51,538] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-01 14:11:51,538] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-01 14:11:51,538] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-01 14:11:51,538] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-09-01 14:11:51,538] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-01 14:11:51,538] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-01 14:11:51,538] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-01 14:11:51,538] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-01 14:11:51,538] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-01 14:11:51,539] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-01 14:11:51,539] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-01 14:11:51,539] [    INFO][0m - [0m
[32m[2022-09-01 14:11:51,540] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-01 14:11:51,540] [    INFO][0m -   Num examples = 3024[0m
[32m[2022-09-01 14:11:51,540] [    INFO][0m -   Num Epochs = 100[0m
[32m[2022-09-01 14:11:51,541] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-09-01 14:11:51,541] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-09-01 14:11:51,541] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-01 14:11:51,541] [    INFO][0m -   Total optimization steps = 37800.0[0m
[32m[2022-09-01 14:11:51,541] [    INFO][0m -   Total num train samples = 302400[0m
[32m[2022-09-01 14:11:53,756] [    INFO][0m - loss: 3.2592247, learning_rate: 2.999206349206349e-05, global_step: 10, interval_runtime: 2.2145, interval_samples_per_second: 3.613, interval_steps_per_second: 4.516, epoch: 0.0265[0m
[32m[2022-09-01 14:11:55,293] [    INFO][0m - loss: 2.78882713, learning_rate: 2.9984126984126986e-05, global_step: 20, interval_runtime: 1.5369, interval_samples_per_second: 5.205, interval_steps_per_second: 6.507, epoch: 0.0529[0m
[32m[2022-09-01 14:11:56,836] [    INFO][0m - loss: 2.88782063, learning_rate: 2.9976190476190477e-05, global_step: 30, interval_runtime: 1.5426, interval_samples_per_second: 5.186, interval_steps_per_second: 6.483, epoch: 0.0794[0m
[32m[2022-09-01 14:11:58,367] [    INFO][0m - loss: 2.4666914, learning_rate: 2.9968253968253967e-05, global_step: 40, interval_runtime: 1.5315, interval_samples_per_second: 5.224, interval_steps_per_second: 6.529, epoch: 0.1058[0m
[32m[2022-09-01 14:11:59,904] [    INFO][0m - loss: 2.83278961, learning_rate: 2.996031746031746e-05, global_step: 50, interval_runtime: 1.537, interval_samples_per_second: 5.205, interval_steps_per_second: 6.506, epoch: 0.1323[0m
[32m[2022-09-01 14:12:01,437] [    INFO][0m - loss: 2.90789413, learning_rate: 2.9952380952380952e-05, global_step: 60, interval_runtime: 1.5329, interval_samples_per_second: 5.219, interval_steps_per_second: 6.524, epoch: 0.1587[0m
[32m[2022-09-01 14:12:02,958] [    INFO][0m - loss: 2.81969929, learning_rate: 2.9944444444444443e-05, global_step: 70, interval_runtime: 1.5212, interval_samples_per_second: 5.259, interval_steps_per_second: 6.574, epoch: 0.1852[0m
[32m[2022-09-01 14:12:04,485] [    INFO][0m - loss: 2.06190434, learning_rate: 2.9936507936507937e-05, global_step: 80, interval_runtime: 1.526, interval_samples_per_second: 5.243, interval_steps_per_second: 6.553, epoch: 0.2116[0m
[32m[2022-09-01 14:12:06,016] [    INFO][0m - loss: 2.27086601, learning_rate: 2.9928571428571428e-05, global_step: 90, interval_runtime: 1.531, interval_samples_per_second: 5.225, interval_steps_per_second: 6.532, epoch: 0.2381[0m
[32m[2022-09-01 14:12:07,547] [    INFO][0m - loss: 2.37829514, learning_rate: 2.992063492063492e-05, global_step: 100, interval_runtime: 1.5319, interval_samples_per_second: 5.222, interval_steps_per_second: 6.528, epoch: 0.2646[0m
[32m[2022-09-01 14:12:07,548] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 14:12:07,548] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-01 14:12:07,548] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 14:12:07,548] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 14:12:07,548] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-09-01 14:12:19,224] [    INFO][0m - eval_loss: 2.115597724914551, eval_accuracy: 0.493809176984705, eval_runtime: 11.6754, eval_samples_per_second: 117.598, eval_steps_per_second: 3.683, epoch: 0.2646[0m
[32m[2022-09-01 14:12:19,225] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-09-01 14:12:19,225] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 14:12:21,078] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-01 14:12:21,078] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-01 14:12:26,721] [    INFO][0m - loss: 2.06430416, learning_rate: 2.9912698412698416e-05, global_step: 110, interval_runtime: 19.1738, interval_samples_per_second: 0.417, interval_steps_per_second: 0.522, epoch: 0.291[0m
[32m[2022-09-01 14:12:28,257] [    INFO][0m - loss: 2.31661968, learning_rate: 2.9904761904761907e-05, global_step: 120, interval_runtime: 1.5362, interval_samples_per_second: 5.208, interval_steps_per_second: 6.51, epoch: 0.3175[0m
[32m[2022-09-01 14:12:29,783] [    INFO][0m - loss: 2.10916615, learning_rate: 2.9896825396825398e-05, global_step: 130, interval_runtime: 1.5259, interval_samples_per_second: 5.243, interval_steps_per_second: 6.554, epoch: 0.3439[0m
[32m[2022-09-01 14:12:31,319] [    INFO][0m - loss: 2.19597359, learning_rate: 2.9888888888888892e-05, global_step: 140, interval_runtime: 1.5356, interval_samples_per_second: 5.21, interval_steps_per_second: 6.512, epoch: 0.3704[0m
[32m[2022-09-01 14:12:32,854] [    INFO][0m - loss: 2.32777958, learning_rate: 2.9880952380952383e-05, global_step: 150, interval_runtime: 1.5352, interval_samples_per_second: 5.211, interval_steps_per_second: 6.514, epoch: 0.3968[0m
[32m[2022-09-01 14:12:34,384] [    INFO][0m - loss: 2.4642786, learning_rate: 2.9873015873015874e-05, global_step: 160, interval_runtime: 1.5299, interval_samples_per_second: 5.229, interval_steps_per_second: 6.537, epoch: 0.4233[0m
[32m[2022-09-01 14:12:35,916] [    INFO][0m - loss: 1.87936687, learning_rate: 2.9865079365079368e-05, global_step: 170, interval_runtime: 1.5322, interval_samples_per_second: 5.221, interval_steps_per_second: 6.527, epoch: 0.4497[0m
[32m[2022-09-01 14:12:37,444] [    INFO][0m - loss: 2.41119747, learning_rate: 2.985714285714286e-05, global_step: 180, interval_runtime: 1.5282, interval_samples_per_second: 5.235, interval_steps_per_second: 6.544, epoch: 0.4762[0m
[32m[2022-09-01 14:12:38,979] [    INFO][0m - loss: 1.87252617, learning_rate: 2.984920634920635e-05, global_step: 190, interval_runtime: 1.5343, interval_samples_per_second: 5.214, interval_steps_per_second: 6.518, epoch: 0.5026[0m
[32m[2022-09-01 14:12:40,509] [    INFO][0m - loss: 2.07058525, learning_rate: 2.984126984126984e-05, global_step: 200, interval_runtime: 1.5303, interval_samples_per_second: 5.228, interval_steps_per_second: 6.534, epoch: 0.5291[0m
[32m[2022-09-01 14:12:40,509] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 14:12:40,509] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-01 14:12:40,510] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 14:12:40,510] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 14:12:40,510] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-09-01 14:12:52,132] [    INFO][0m - eval_loss: 1.969077229499817, eval_accuracy: 0.507647487254188, eval_runtime: 11.6217, eval_samples_per_second: 118.141, eval_steps_per_second: 3.7, epoch: 0.5291[0m
[32m[2022-09-01 14:12:52,132] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-09-01 14:12:52,133] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 14:12:53,998] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-09-01 14:12:53,998] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-09-01 14:13:03,239] [    INFO][0m - loss: 1.91853886, learning_rate: 2.9833333333333335e-05, global_step: 210, interval_runtime: 22.7293, interval_samples_per_second: 0.352, interval_steps_per_second: 0.44, epoch: 0.5556[0m
[32m[2022-09-01 14:13:04,783] [    INFO][0m - loss: 2.12517776, learning_rate: 2.9825396825396825e-05, global_step: 220, interval_runtime: 1.5448, interval_samples_per_second: 5.179, interval_steps_per_second: 6.473, epoch: 0.582[0m
[32m[2022-09-01 14:13:06,334] [    INFO][0m - loss: 1.95145454, learning_rate: 2.9817460317460316e-05, global_step: 230, interval_runtime: 1.5508, interval_samples_per_second: 5.159, interval_steps_per_second: 6.448, epoch: 0.6085[0m
[32m[2022-09-01 14:13:07,874] [    INFO][0m - loss: 1.82327995, learning_rate: 2.980952380952381e-05, global_step: 240, interval_runtime: 1.5405, interval_samples_per_second: 5.193, interval_steps_per_second: 6.492, epoch: 0.6349[0m
[32m[2022-09-01 14:13:09,427] [    INFO][0m - loss: 1.75423985, learning_rate: 2.98015873015873e-05, global_step: 250, interval_runtime: 1.5525, interval_samples_per_second: 5.153, interval_steps_per_second: 6.441, epoch: 0.6614[0m
[32m[2022-09-01 14:13:10,965] [    INFO][0m - loss: 1.87354183, learning_rate: 2.9793650793650792e-05, global_step: 260, interval_runtime: 1.5382, interval_samples_per_second: 5.201, interval_steps_per_second: 6.501, epoch: 0.6878[0m
[32m[2022-09-01 14:13:12,508] [    INFO][0m - loss: 1.65876446, learning_rate: 2.9785714285714286e-05, global_step: 270, interval_runtime: 1.543, interval_samples_per_second: 5.185, interval_steps_per_second: 6.481, epoch: 0.7143[0m
[32m[2022-09-01 14:13:14,050] [    INFO][0m - loss: 2.0993063, learning_rate: 2.9777777777777777e-05, global_step: 280, interval_runtime: 1.5418, interval_samples_per_second: 5.189, interval_steps_per_second: 6.486, epoch: 0.7407[0m
[32m[2022-09-01 14:13:15,591] [    INFO][0m - loss: 1.88904991, learning_rate: 2.9769841269841268e-05, global_step: 290, interval_runtime: 1.5417, interval_samples_per_second: 5.189, interval_steps_per_second: 6.486, epoch: 0.7672[0m
[32m[2022-09-01 14:13:17,125] [    INFO][0m - loss: 1.80988598, learning_rate: 2.9761904761904762e-05, global_step: 300, interval_runtime: 1.5331, interval_samples_per_second: 5.218, interval_steps_per_second: 6.523, epoch: 0.7937[0m
[32m[2022-09-01 14:13:17,125] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 14:13:17,125] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-01 14:13:17,125] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 14:13:17,125] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 14:13:17,125] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-09-01 14:13:28,714] [    INFO][0m - eval_loss: 1.88689386844635, eval_accuracy: 0.5294974508375819, eval_runtime: 11.5881, eval_samples_per_second: 118.484, eval_steps_per_second: 3.711, epoch: 0.7937[0m
[32m[2022-09-01 14:13:28,715] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-09-01 14:13:28,715] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 14:13:30,582] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-09-01 14:13:30,583] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-09-01 14:13:36,161] [    INFO][0m - loss: 1.94528656, learning_rate: 2.9753968253968256e-05, global_step: 310, interval_runtime: 19.0368, interval_samples_per_second: 0.42, interval_steps_per_second: 0.525, epoch: 0.8201[0m
[32m[2022-09-01 14:13:37,692] [    INFO][0m - loss: 1.64635925, learning_rate: 2.9746031746031747e-05, global_step: 320, interval_runtime: 1.5309, interval_samples_per_second: 5.226, interval_steps_per_second: 6.532, epoch: 0.8466[0m
[32m[2022-09-01 14:13:39,220] [    INFO][0m - loss: 1.85745983, learning_rate: 2.973809523809524e-05, global_step: 330, interval_runtime: 1.5274, interval_samples_per_second: 5.238, interval_steps_per_second: 6.547, epoch: 0.873[0m
[32m[2022-09-01 14:13:40,776] [    INFO][0m - loss: 1.81357613, learning_rate: 2.9730158730158732e-05, global_step: 340, interval_runtime: 1.5563, interval_samples_per_second: 5.14, interval_steps_per_second: 6.425, epoch: 0.8995[0m
[32m[2022-09-01 14:13:42,307] [    INFO][0m - loss: 2.11502438, learning_rate: 2.9722222222222223e-05, global_step: 350, interval_runtime: 1.5314, interval_samples_per_second: 5.224, interval_steps_per_second: 6.53, epoch: 0.9259[0m
[32m[2022-09-01 14:13:43,835] [    INFO][0m - loss: 2.24745464, learning_rate: 2.9714285714285717e-05, global_step: 360, interval_runtime: 1.5273, interval_samples_per_second: 5.238, interval_steps_per_second: 6.548, epoch: 0.9524[0m
[32m[2022-09-01 14:13:45,366] [    INFO][0m - loss: 1.81403389, learning_rate: 2.9706349206349208e-05, global_step: 370, interval_runtime: 1.5309, interval_samples_per_second: 5.226, interval_steps_per_second: 6.532, epoch: 0.9788[0m
[32m[2022-09-01 14:13:47,014] [    INFO][0m - loss: 1.42935972, learning_rate: 2.96984126984127e-05, global_step: 380, interval_runtime: 1.648, interval_samples_per_second: 4.854, interval_steps_per_second: 6.068, epoch: 1.0053[0m
[32m[2022-09-01 14:13:48,548] [    INFO][0m - loss: 1.34950609, learning_rate: 2.9690476190476193e-05, global_step: 390, interval_runtime: 1.5347, interval_samples_per_second: 5.213, interval_steps_per_second: 6.516, epoch: 1.0317[0m
[32m[2022-09-01 14:13:50,082] [    INFO][0m - loss: 0.92820253, learning_rate: 2.9682539682539683e-05, global_step: 400, interval_runtime: 1.5337, interval_samples_per_second: 5.216, interval_steps_per_second: 6.52, epoch: 1.0582[0m
[32m[2022-09-01 14:13:50,083] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 14:13:50,083] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-01 14:13:50,083] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 14:13:50,083] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 14:13:50,083] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-09-01 14:14:01,706] [    INFO][0m - eval_loss: 1.9649407863616943, eval_accuracy: 0.5294974508375819, eval_runtime: 11.6226, eval_samples_per_second: 118.132, eval_steps_per_second: 3.7, epoch: 1.0582[0m
[32m[2022-09-01 14:14:01,707] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-09-01 14:14:01,707] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 14:14:03,549] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-09-01 14:14:03,550] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-09-01 14:14:09,433] [    INFO][0m - loss: 1.08780432, learning_rate: 2.9674603174603174e-05, global_step: 410, interval_runtime: 19.3511, interval_samples_per_second: 0.413, interval_steps_per_second: 0.517, epoch: 1.0847[0m
[32m[2022-09-01 14:14:10,969] [    INFO][0m - loss: 1.31559877, learning_rate: 2.966666666666667e-05, global_step: 420, interval_runtime: 1.5363, interval_samples_per_second: 5.207, interval_steps_per_second: 6.509, epoch: 1.1111[0m
[32m[2022-09-01 14:14:12,512] [    INFO][0m - loss: 1.12412167, learning_rate: 2.965873015873016e-05, global_step: 430, interval_runtime: 1.5422, interval_samples_per_second: 5.187, interval_steps_per_second: 6.484, epoch: 1.1376[0m
[32m[2022-09-01 14:14:14,059] [    INFO][0m - loss: 1.15369282, learning_rate: 2.965079365079365e-05, global_step: 440, interval_runtime: 1.5475, interval_samples_per_second: 5.17, interval_steps_per_second: 6.462, epoch: 1.164[0m
[32m[2022-09-01 14:14:15,588] [    INFO][0m - loss: 1.20946159, learning_rate: 2.9642857142857144e-05, global_step: 450, interval_runtime: 1.5288, interval_samples_per_second: 5.233, interval_steps_per_second: 6.541, epoch: 1.1905[0m
[32m[2022-09-01 14:14:17,124] [    INFO][0m - loss: 1.33155632, learning_rate: 2.9634920634920635e-05, global_step: 460, interval_runtime: 1.5359, interval_samples_per_second: 5.209, interval_steps_per_second: 6.511, epoch: 1.2169[0m
[32m[2022-09-01 14:14:18,666] [    INFO][0m - loss: 1.19870243, learning_rate: 2.9626984126984126e-05, global_step: 470, interval_runtime: 1.5425, interval_samples_per_second: 5.186, interval_steps_per_second: 6.483, epoch: 1.2434[0m
[32m[2022-09-01 14:14:20,213] [    INFO][0m - loss: 1.13759508, learning_rate: 2.961904761904762e-05, global_step: 480, interval_runtime: 1.5461, interval_samples_per_second: 5.174, interval_steps_per_second: 6.468, epoch: 1.2698[0m
[32m[2022-09-01 14:14:21,754] [    INFO][0m - loss: 1.22842255, learning_rate: 2.961111111111111e-05, global_step: 490, interval_runtime: 1.5414, interval_samples_per_second: 5.19, interval_steps_per_second: 6.488, epoch: 1.2963[0m
[32m[2022-09-01 14:14:23,295] [    INFO][0m - loss: 1.60907974, learning_rate: 2.96031746031746e-05, global_step: 500, interval_runtime: 1.541, interval_samples_per_second: 5.191, interval_steps_per_second: 6.489, epoch: 1.3228[0m
[32m[2022-09-01 14:14:23,296] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 14:14:23,296] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-01 14:14:23,296] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 14:14:23,296] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 14:14:23,296] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-09-01 14:14:34,947] [    INFO][0m - eval_loss: 1.914451003074646, eval_accuracy: 0.5418790968681719, eval_runtime: 11.6509, eval_samples_per_second: 117.845, eval_steps_per_second: 3.691, epoch: 1.3228[0m
[32m[2022-09-01 14:14:34,948] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-09-01 14:14:34,948] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 14:14:39,235] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-09-01 14:14:39,236] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-09-01 14:14:46,887] [    INFO][0m - loss: 1.70240326, learning_rate: 2.95952380952381e-05, global_step: 510, interval_runtime: 23.5921, interval_samples_per_second: 0.339, interval_steps_per_second: 0.424, epoch: 1.3492[0m
[32m[2022-09-01 14:14:48,423] [    INFO][0m - loss: 1.13265381, learning_rate: 2.958730158730159e-05, global_step: 520, interval_runtime: 1.5362, interval_samples_per_second: 5.208, interval_steps_per_second: 6.51, epoch: 1.3757[0m
[32m[2022-09-01 14:14:49,957] [    INFO][0m - loss: 1.11714725, learning_rate: 2.957936507936508e-05, global_step: 530, interval_runtime: 1.5334, interval_samples_per_second: 5.217, interval_steps_per_second: 6.522, epoch: 1.4021[0m
[32m[2022-09-01 14:14:51,506] [    INFO][0m - loss: 1.33510656, learning_rate: 2.9571428571428575e-05, global_step: 540, interval_runtime: 1.5494, interval_samples_per_second: 5.163, interval_steps_per_second: 6.454, epoch: 1.4286[0m
[32m[2022-09-01 14:14:53,050] [    INFO][0m - loss: 1.16094837, learning_rate: 2.9563492063492066e-05, global_step: 550, interval_runtime: 1.5436, interval_samples_per_second: 5.183, interval_steps_per_second: 6.478, epoch: 1.455[0m
[32m[2022-09-01 14:14:54,582] [    INFO][0m - loss: 1.30322399, learning_rate: 2.9555555555555556e-05, global_step: 560, interval_runtime: 1.5318, interval_samples_per_second: 5.223, interval_steps_per_second: 6.528, epoch: 1.4815[0m
[32m[2022-09-01 14:14:56,108] [    INFO][0m - loss: 1.38342533, learning_rate: 2.954761904761905e-05, global_step: 570, interval_runtime: 1.5263, interval_samples_per_second: 5.241, interval_steps_per_second: 6.552, epoch: 1.5079[0m
[32m[2022-09-01 14:14:57,640] [    INFO][0m - loss: 1.34900732, learning_rate: 2.953968253968254e-05, global_step: 580, interval_runtime: 1.5315, interval_samples_per_second: 5.224, interval_steps_per_second: 6.53, epoch: 1.5344[0m
[32m[2022-09-01 14:14:59,174] [    INFO][0m - loss: 1.64883347, learning_rate: 2.9531746031746032e-05, global_step: 590, interval_runtime: 1.5341, interval_samples_per_second: 5.215, interval_steps_per_second: 6.518, epoch: 1.5608[0m
[32m[2022-09-01 14:15:00,719] [    INFO][0m - loss: 1.41090593, learning_rate: 2.9523809523809523e-05, global_step: 600, interval_runtime: 1.5449, interval_samples_per_second: 5.178, interval_steps_per_second: 6.473, epoch: 1.5873[0m
[32m[2022-09-01 14:15:00,719] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 14:15:00,719] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-01 14:15:00,720] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 14:15:00,720] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 14:15:00,720] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-09-01 14:15:12,374] [    INFO][0m - eval_loss: 1.880864143371582, eval_accuracy: 0.5549890750182083, eval_runtime: 11.6539, eval_samples_per_second: 117.815, eval_steps_per_second: 3.69, epoch: 1.5873[0m
[32m[2022-09-01 14:15:12,375] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-09-01 14:15:12,375] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 14:15:14,196] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-09-01 14:15:14,196] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-09-01 14:15:19,984] [    INFO][0m - loss: 1.35048904, learning_rate: 2.9515873015873017e-05, global_step: 610, interval_runtime: 19.2654, interval_samples_per_second: 0.415, interval_steps_per_second: 0.519, epoch: 1.6138[0m
[32m[2022-09-01 14:15:21,518] [    INFO][0m - loss: 1.40398941, learning_rate: 2.9507936507936508e-05, global_step: 620, interval_runtime: 1.5346, interval_samples_per_second: 5.213, interval_steps_per_second: 6.517, epoch: 1.6402[0m
[32m[2022-09-01 14:15:23,059] [    INFO][0m - loss: 1.0630971, learning_rate: 2.95e-05, global_step: 630, interval_runtime: 1.5405, interval_samples_per_second: 5.193, interval_steps_per_second: 6.492, epoch: 1.6667[0m
[32m[2022-09-01 14:15:24,605] [    INFO][0m - loss: 1.39536495, learning_rate: 2.9492063492063493e-05, global_step: 640, interval_runtime: 1.5457, interval_samples_per_second: 5.175, interval_steps_per_second: 6.469, epoch: 1.6931[0m
[32m[2022-09-01 14:15:26,144] [    INFO][0m - loss: 1.35010719, learning_rate: 2.9484126984126984e-05, global_step: 650, interval_runtime: 1.5392, interval_samples_per_second: 5.198, interval_steps_per_second: 6.497, epoch: 1.7196[0m
[32m[2022-09-01 14:15:27,681] [    INFO][0m - loss: 1.58975782, learning_rate: 2.9476190476190475e-05, global_step: 660, interval_runtime: 1.5366, interval_samples_per_second: 5.206, interval_steps_per_second: 6.508, epoch: 1.746[0m
[32m[2022-09-01 14:15:29,206] [    INFO][0m - loss: 1.36823864, learning_rate: 2.946825396825397e-05, global_step: 670, interval_runtime: 1.5251, interval_samples_per_second: 5.246, interval_steps_per_second: 6.557, epoch: 1.7725[0m
[32m[2022-09-01 14:15:30,736] [    INFO][0m - loss: 1.61264153, learning_rate: 2.946031746031746e-05, global_step: 680, interval_runtime: 1.5305, interval_samples_per_second: 5.227, interval_steps_per_second: 6.534, epoch: 1.7989[0m
[32m[2022-09-01 14:15:32,275] [    INFO][0m - loss: 1.26665573, learning_rate: 2.945238095238095e-05, global_step: 690, interval_runtime: 1.5392, interval_samples_per_second: 5.198, interval_steps_per_second: 6.497, epoch: 1.8254[0m
[32m[2022-09-01 14:15:33,816] [    INFO][0m - loss: 1.21321497, learning_rate: 2.9444444444444445e-05, global_step: 700, interval_runtime: 1.5405, interval_samples_per_second: 5.193, interval_steps_per_second: 6.491, epoch: 1.8519[0m
[32m[2022-09-01 14:15:33,817] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 14:15:33,817] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-01 14:15:33,817] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 14:15:33,817] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 14:15:33,817] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-09-01 14:15:45,454] [    INFO][0m - eval_loss: 1.9384609460830688, eval_accuracy: 0.5404224326292789, eval_runtime: 11.6364, eval_samples_per_second: 117.992, eval_steps_per_second: 3.695, epoch: 1.8519[0m
[32m[2022-09-01 14:15:45,455] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-700[0m
[32m[2022-09-01 14:15:45,455] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 14:15:47,324] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-700/tokenizer_config.json[0m
[32m[2022-09-01 14:15:47,325] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-700/special_tokens_map.json[0m
[32m[2022-09-01 14:15:52,927] [    INFO][0m - loss: 1.31463652, learning_rate: 2.943650793650794e-05, global_step: 710, interval_runtime: 19.111, interval_samples_per_second: 0.419, interval_steps_per_second: 0.523, epoch: 1.8783[0m
[32m[2022-09-01 14:15:54,477] [    INFO][0m - loss: 1.17408104, learning_rate: 2.942857142857143e-05, global_step: 720, interval_runtime: 1.5496, interval_samples_per_second: 5.163, interval_steps_per_second: 6.453, epoch: 1.9048[0m
[32m[2022-09-01 14:15:56,026] [    INFO][0m - loss: 1.46274891, learning_rate: 2.9420634920634924e-05, global_step: 730, interval_runtime: 1.5501, interval_samples_per_second: 5.161, interval_steps_per_second: 6.451, epoch: 1.9312[0m
[32m[2022-09-01 14:15:57,573] [    INFO][0m - loss: 1.39576025, learning_rate: 2.9412698412698414e-05, global_step: 740, interval_runtime: 1.5469, interval_samples_per_second: 5.171, interval_steps_per_second: 6.464, epoch: 1.9577[0m
[32m[2022-09-01 14:15:59,121] [    INFO][0m - loss: 1.04794025, learning_rate: 2.9404761904761905e-05, global_step: 750, interval_runtime: 1.5478, interval_samples_per_second: 5.169, interval_steps_per_second: 6.461, epoch: 1.9841[0m
[32m[2022-09-01 14:16:00,768] [    INFO][0m - loss: 0.82822046, learning_rate: 2.93968253968254e-05, global_step: 760, interval_runtime: 1.6469, interval_samples_per_second: 4.858, interval_steps_per_second: 6.072, epoch: 2.0106[0m
[32m[2022-09-01 14:16:02,315] [    INFO][0m - loss: 0.63040347, learning_rate: 2.938888888888889e-05, global_step: 770, interval_runtime: 1.547, interval_samples_per_second: 5.171, interval_steps_per_second: 6.464, epoch: 2.037[0m
[32m[2022-09-01 14:16:03,869] [    INFO][0m - loss: 0.73905096, learning_rate: 2.938095238095238e-05, global_step: 780, interval_runtime: 1.5538, interval_samples_per_second: 5.149, interval_steps_per_second: 6.436, epoch: 2.0635[0m
[32m[2022-09-01 14:16:05,425] [    INFO][0m - loss: 0.79416428, learning_rate: 2.9373015873015875e-05, global_step: 790, interval_runtime: 1.5556, interval_samples_per_second: 5.143, interval_steps_per_second: 6.428, epoch: 2.0899[0m
[32m[2022-09-01 14:16:06,965] [    INFO][0m - loss: 0.57830596, learning_rate: 2.9365079365079366e-05, global_step: 800, interval_runtime: 1.5404, interval_samples_per_second: 5.193, interval_steps_per_second: 6.492, epoch: 2.1164[0m
[32m[2022-09-01 14:16:06,966] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 14:16:06,966] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-01 14:16:06,966] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 14:16:06,966] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 14:16:06,966] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-09-01 14:16:18,609] [    INFO][0m - eval_loss: 2.053922176361084, eval_accuracy: 0.5455207574654042, eval_runtime: 11.6421, eval_samples_per_second: 117.934, eval_steps_per_second: 3.694, epoch: 2.1164[0m
[32m[2022-09-01 14:16:18,609] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-800[0m
[32m[2022-09-01 14:16:18,610] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 14:16:20,776] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-800/tokenizer_config.json[0m
[32m[2022-09-01 14:16:20,777] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-800/special_tokens_map.json[0m
[32m[2022-09-01 14:16:26,330] [    INFO][0m - loss: 0.70765858, learning_rate: 2.9357142857142857e-05, global_step: 810, interval_runtime: 19.3647, interval_samples_per_second: 0.413, interval_steps_per_second: 0.516, epoch: 2.1429[0m
[32m[2022-09-01 14:16:27,856] [    INFO][0m - loss: 0.77595901, learning_rate: 2.934920634920635e-05, global_step: 820, interval_runtime: 1.5259, interval_samples_per_second: 5.243, interval_steps_per_second: 6.554, epoch: 2.1693[0m
[32m[2022-09-01 14:16:29,393] [    INFO][0m - loss: 0.69997549, learning_rate: 2.9341269841269842e-05, global_step: 830, interval_runtime: 1.537, interval_samples_per_second: 5.205, interval_steps_per_second: 6.506, epoch: 2.1958[0m
[32m[2022-09-01 14:16:30,924] [    INFO][0m - loss: 0.78655705, learning_rate: 2.9333333333333333e-05, global_step: 840, interval_runtime: 1.5309, interval_samples_per_second: 5.226, interval_steps_per_second: 6.532, epoch: 2.2222[0m
[32m[2022-09-01 14:16:32,456] [    INFO][0m - loss: 0.89499969, learning_rate: 2.9325396825396827e-05, global_step: 850, interval_runtime: 1.5322, interval_samples_per_second: 5.221, interval_steps_per_second: 6.527, epoch: 2.2487[0m
[32m[2022-09-01 14:16:33,985] [    INFO][0m - loss: 0.73915768, learning_rate: 2.9317460317460318e-05, global_step: 860, interval_runtime: 1.5296, interval_samples_per_second: 5.23, interval_steps_per_second: 6.538, epoch: 2.2751[0m
[32m[2022-09-01 14:16:35,516] [    INFO][0m - loss: 0.74757867, learning_rate: 2.930952380952381e-05, global_step: 870, interval_runtime: 1.5309, interval_samples_per_second: 5.226, interval_steps_per_second: 6.532, epoch: 2.3016[0m
[32m[2022-09-01 14:16:37,044] [    INFO][0m - loss: 0.61748376, learning_rate: 2.9301587301587303e-05, global_step: 880, interval_runtime: 1.5274, interval_samples_per_second: 5.238, interval_steps_per_second: 6.547, epoch: 2.328[0m
[32m[2022-09-01 14:16:38,577] [    INFO][0m - loss: 0.7943541, learning_rate: 2.9293650793650793e-05, global_step: 890, interval_runtime: 1.5329, interval_samples_per_second: 5.219, interval_steps_per_second: 6.524, epoch: 2.3545[0m
[32m[2022-09-01 14:16:40,105] [    INFO][0m - loss: 0.53237, learning_rate: 2.9285714285714284e-05, global_step: 900, interval_runtime: 1.529, interval_samples_per_second: 5.232, interval_steps_per_second: 6.54, epoch: 2.381[0m
[32m[2022-09-01 14:16:40,106] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 14:16:40,107] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-01 14:16:40,107] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 14:16:40,107] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 14:16:40,107] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-09-01 14:16:51,843] [    INFO][0m - eval_loss: 2.3003053665161133, eval_accuracy: 0.5207574654042243, eval_runtime: 11.7352, eval_samples_per_second: 116.998, eval_steps_per_second: 3.664, epoch: 2.381[0m
[32m[2022-09-01 14:16:51,843] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-900[0m
[32m[2022-09-01 14:16:51,843] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 14:16:53,711] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-900/tokenizer_config.json[0m
[32m[2022-09-01 14:16:53,711] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-900/special_tokens_map.json[0m
[32m[2022-09-01 14:16:59,809] [    INFO][0m - loss: 0.80338211, learning_rate: 2.9277777777777778e-05, global_step: 910, interval_runtime: 19.7028, interval_samples_per_second: 0.406, interval_steps_per_second: 0.508, epoch: 2.4074[0m
[32m[2022-09-01 14:17:01,355] [    INFO][0m - loss: 0.82429667, learning_rate: 2.9269841269841272e-05, global_step: 920, interval_runtime: 1.5463, interval_samples_per_second: 5.174, interval_steps_per_second: 6.467, epoch: 2.4339[0m
[32m[2022-09-01 14:17:02,908] [    INFO][0m - loss: 0.94666176, learning_rate: 2.9261904761904763e-05, global_step: 930, interval_runtime: 1.5529, interval_samples_per_second: 5.152, interval_steps_per_second: 6.439, epoch: 2.4603[0m
[32m[2022-09-01 14:17:04,452] [    INFO][0m - loss: 0.5690309, learning_rate: 2.9253968253968257e-05, global_step: 940, interval_runtime: 1.5449, interval_samples_per_second: 5.178, interval_steps_per_second: 6.473, epoch: 2.4868[0m
[32m[2022-09-01 14:17:06,003] [    INFO][0m - loss: 0.93949385, learning_rate: 2.9246031746031748e-05, global_step: 950, interval_runtime: 1.5502, interval_samples_per_second: 5.161, interval_steps_per_second: 6.451, epoch: 2.5132[0m
[32m[2022-09-01 14:17:07,563] [    INFO][0m - loss: 0.6409791, learning_rate: 2.923809523809524e-05, global_step: 960, interval_runtime: 1.5599, interval_samples_per_second: 5.129, interval_steps_per_second: 6.411, epoch: 2.5397[0m
[32m[2022-09-01 14:17:09,108] [    INFO][0m - loss: 0.89946442, learning_rate: 2.9230158730158733e-05, global_step: 970, interval_runtime: 1.5458, interval_samples_per_second: 5.175, interval_steps_per_second: 6.469, epoch: 2.5661[0m
[32m[2022-09-01 14:17:10,660] [    INFO][0m - loss: 0.70999956, learning_rate: 2.9222222222222224e-05, global_step: 980, interval_runtime: 1.5516, interval_samples_per_second: 5.156, interval_steps_per_second: 6.445, epoch: 2.5926[0m
[32m[2022-09-01 14:17:12,204] [    INFO][0m - loss: 0.70319438, learning_rate: 2.9214285714285715e-05, global_step: 990, interval_runtime: 1.5443, interval_samples_per_second: 5.18, interval_steps_per_second: 6.475, epoch: 2.619[0m
[32m[2022-09-01 14:17:13,751] [    INFO][0m - loss: 0.8628356, learning_rate: 2.9206349206349206e-05, global_step: 1000, interval_runtime: 1.5465, interval_samples_per_second: 5.173, interval_steps_per_second: 6.466, epoch: 2.6455[0m
[32m[2022-09-01 14:17:13,751] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 14:17:13,752] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-01 14:17:13,752] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 14:17:13,752] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 14:17:13,752] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-09-01 14:17:25,444] [    INFO][0m - eval_loss: 2.3351082801818848, eval_accuracy: 0.5127458120903132, eval_runtime: 11.6918, eval_samples_per_second: 117.433, eval_steps_per_second: 3.678, epoch: 2.6455[0m
[32m[2022-09-01 14:17:25,445] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1000[0m
[32m[2022-09-01 14:17:25,445] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 14:17:30,360] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1000/tokenizer_config.json[0m
[32m[2022-09-01 14:17:30,360] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1000/special_tokens_map.json[0m
[32m[2022-09-01 14:17:36,857] [    INFO][0m - loss: 0.81685143, learning_rate: 2.91984126984127e-05, global_step: 1010, interval_runtime: 23.1065, interval_samples_per_second: 0.346, interval_steps_per_second: 0.433, epoch: 2.672[0m
[32m[2022-09-01 14:17:38,397] [    INFO][0m - loss: 0.9099699, learning_rate: 2.919047619047619e-05, global_step: 1020, interval_runtime: 1.539, interval_samples_per_second: 5.198, interval_steps_per_second: 6.498, epoch: 2.6984[0m
[32m[2022-09-01 14:17:39,941] [    INFO][0m - loss: 0.92620087, learning_rate: 2.918253968253968e-05, global_step: 1030, interval_runtime: 1.5451, interval_samples_per_second: 5.178, interval_steps_per_second: 6.472, epoch: 2.7249[0m
[32m[2022-09-01 14:17:41,490] [    INFO][0m - loss: 0.92560844, learning_rate: 2.9174603174603176e-05, global_step: 1040, interval_runtime: 1.5485, interval_samples_per_second: 5.166, interval_steps_per_second: 6.458, epoch: 2.7513[0m
[32m[2022-09-01 14:17:43,035] [    INFO][0m - loss: 0.9992444, learning_rate: 2.9166666666666666e-05, global_step: 1050, interval_runtime: 1.5452, interval_samples_per_second: 5.177, interval_steps_per_second: 6.472, epoch: 2.7778[0m
[32m[2022-09-01 14:17:44,584] [    INFO][0m - loss: 0.82834196, learning_rate: 2.9158730158730157e-05, global_step: 1060, interval_runtime: 1.5482, interval_samples_per_second: 5.167, interval_steps_per_second: 6.459, epoch: 2.8042[0m
[32m[2022-09-01 14:17:46,126] [    INFO][0m - loss: 0.78470764, learning_rate: 2.915079365079365e-05, global_step: 1070, interval_runtime: 1.5423, interval_samples_per_second: 5.187, interval_steps_per_second: 6.484, epoch: 2.8307[0m
[32m[2022-09-01 14:17:47,670] [    INFO][0m - loss: 0.85846329, learning_rate: 2.9142857142857142e-05, global_step: 1080, interval_runtime: 1.5441, interval_samples_per_second: 5.181, interval_steps_per_second: 6.476, epoch: 2.8571[0m
[32m[2022-09-01 14:17:49,205] [    INFO][0m - loss: 0.89298496, learning_rate: 2.9134920634920633e-05, global_step: 1090, interval_runtime: 1.5356, interval_samples_per_second: 5.21, interval_steps_per_second: 6.512, epoch: 2.8836[0m
[32m[2022-09-01 14:17:50,751] [    INFO][0m - loss: 0.79604812, learning_rate: 2.9126984126984127e-05, global_step: 1100, interval_runtime: 1.5459, interval_samples_per_second: 5.175, interval_steps_per_second: 6.469, epoch: 2.9101[0m
[32m[2022-09-01 14:17:50,752] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 14:17:50,752] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-01 14:17:50,752] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 14:17:50,752] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 14:17:50,752] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-09-01 14:18:02,423] [    INFO][0m - eval_loss: 2.1908152103424072, eval_accuracy: 0.5557174071376547, eval_runtime: 11.6702, eval_samples_per_second: 117.65, eval_steps_per_second: 3.685, epoch: 2.9101[0m
[32m[2022-09-01 14:18:02,423] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1100[0m
[32m[2022-09-01 14:18:02,424] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 14:18:04,344] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1100/tokenizer_config.json[0m
[32m[2022-09-01 14:18:04,344] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1100/special_tokens_map.json[0m
[32m[2022-09-01 14:18:10,048] [    INFO][0m - loss: 0.8124218, learning_rate: 2.9119047619047618e-05, global_step: 1110, interval_runtime: 19.2966, interval_samples_per_second: 0.415, interval_steps_per_second: 0.518, epoch: 2.9365[0m
[32m[2022-09-01 14:18:11,588] [    INFO][0m - loss: 0.83467798, learning_rate: 2.9111111111111112e-05, global_step: 1120, interval_runtime: 1.5399, interval_samples_per_second: 5.195, interval_steps_per_second: 6.494, epoch: 2.963[0m
[32m[2022-09-01 14:18:13,134] [    INFO][0m - loss: 0.91376343, learning_rate: 2.9103174603174606e-05, global_step: 1130, interval_runtime: 1.5463, interval_samples_per_second: 5.174, interval_steps_per_second: 6.467, epoch: 2.9894[0m
[32m[2022-09-01 14:18:14,756] [    INFO][0m - loss: 0.51501746, learning_rate: 2.9095238095238097e-05, global_step: 1140, interval_runtime: 1.6224, interval_samples_per_second: 4.931, interval_steps_per_second: 6.164, epoch: 3.0159[0m
[32m[2022-09-01 14:18:16,301] [    INFO][0m - loss: 0.48869848, learning_rate: 2.9087301587301588e-05, global_step: 1150, interval_runtime: 1.545, interval_samples_per_second: 5.178, interval_steps_per_second: 6.473, epoch: 3.0423[0m
[32m[2022-09-01 14:18:17,839] [    INFO][0m - loss: 0.40601125, learning_rate: 2.9079365079365082e-05, global_step: 1160, interval_runtime: 1.5378, interval_samples_per_second: 5.202, interval_steps_per_second: 6.503, epoch: 3.0688[0m
[32m[2022-09-01 14:18:19,383] [    INFO][0m - loss: 0.42622809, learning_rate: 2.9071428571428573e-05, global_step: 1170, interval_runtime: 1.5437, interval_samples_per_second: 5.182, interval_steps_per_second: 6.478, epoch: 3.0952[0m
[32m[2022-09-01 14:18:20,931] [    INFO][0m - loss: 0.51130943, learning_rate: 2.9063492063492064e-05, global_step: 1180, interval_runtime: 1.5482, interval_samples_per_second: 5.167, interval_steps_per_second: 6.459, epoch: 3.1217[0m
[32m[2022-09-01 14:18:22,475] [    INFO][0m - loss: 0.34736063, learning_rate: 2.9055555555555558e-05, global_step: 1190, interval_runtime: 1.5442, interval_samples_per_second: 5.181, interval_steps_per_second: 6.476, epoch: 3.1481[0m
[32m[2022-09-01 14:18:24,020] [    INFO][0m - loss: 0.51085725, learning_rate: 2.904761904761905e-05, global_step: 1200, interval_runtime: 1.545, interval_samples_per_second: 5.178, interval_steps_per_second: 6.472, epoch: 3.1746[0m
[32m[2022-09-01 14:18:24,021] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 14:18:24,021] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-01 14:18:24,021] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 14:18:24,021] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 14:18:24,021] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-09-01 14:18:35,703] [    INFO][0m - eval_loss: 2.3820557594299316, eval_accuracy: 0.5178441369264385, eval_runtime: 11.6808, eval_samples_per_second: 117.544, eval_steps_per_second: 3.681, epoch: 3.1746[0m
[32m[2022-09-01 14:18:35,703] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1200[0m
[32m[2022-09-01 14:18:35,703] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 14:18:37,517] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1200/tokenizer_config.json[0m
[32m[2022-09-01 14:18:37,518] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1200/special_tokens_map.json[0m
[32m[2022-09-01 14:18:43,376] [    INFO][0m - loss: 0.507024, learning_rate: 2.903968253968254e-05, global_step: 1210, interval_runtime: 19.3561, interval_samples_per_second: 0.413, interval_steps_per_second: 0.517, epoch: 3.2011[0m
[32m[2022-09-01 14:18:44,920] [    INFO][0m - loss: 0.3951035, learning_rate: 2.9031746031746034e-05, global_step: 1220, interval_runtime: 1.5438, interval_samples_per_second: 5.182, interval_steps_per_second: 6.477, epoch: 3.2275[0m
[32m[2022-09-01 14:18:46,472] [    INFO][0m - loss: 0.49582434, learning_rate: 2.9023809523809524e-05, global_step: 1230, interval_runtime: 1.5512, interval_samples_per_second: 5.157, interval_steps_per_second: 6.447, epoch: 3.254[0m
[32m[2022-09-01 14:18:48,018] [    INFO][0m - loss: 0.28598838, learning_rate: 2.9015873015873015e-05, global_step: 1240, interval_runtime: 1.5465, interval_samples_per_second: 5.173, interval_steps_per_second: 6.466, epoch: 3.2804[0m
[32m[2022-09-01 14:18:49,567] [    INFO][0m - loss: 0.5458972, learning_rate: 2.900793650793651e-05, global_step: 1250, interval_runtime: 1.5489, interval_samples_per_second: 5.165, interval_steps_per_second: 6.456, epoch: 3.3069[0m
[32m[2022-09-01 14:18:51,116] [    INFO][0m - loss: 0.45794435, learning_rate: 2.9e-05, global_step: 1260, interval_runtime: 1.5493, interval_samples_per_second: 5.164, interval_steps_per_second: 6.455, epoch: 3.3333[0m
[32m[2022-09-01 14:18:52,665] [    INFO][0m - loss: 0.23687603, learning_rate: 2.899206349206349e-05, global_step: 1270, interval_runtime: 1.5489, interval_samples_per_second: 5.165, interval_steps_per_second: 6.456, epoch: 3.3598[0m
[32m[2022-09-01 14:18:54,214] [    INFO][0m - loss: 0.52444315, learning_rate: 2.8984126984126985e-05, global_step: 1280, interval_runtime: 1.5486, interval_samples_per_second: 5.166, interval_steps_per_second: 6.458, epoch: 3.3862[0m
[32m[2022-09-01 14:18:55,756] [    INFO][0m - loss: 0.37295589, learning_rate: 2.8976190476190476e-05, global_step: 1290, interval_runtime: 1.5424, interval_samples_per_second: 5.187, interval_steps_per_second: 6.484, epoch: 3.4127[0m
[32m[2022-09-01 14:18:57,298] [    INFO][0m - loss: 0.22393911, learning_rate: 2.8968253968253967e-05, global_step: 1300, interval_runtime: 1.5416, interval_samples_per_second: 5.189, interval_steps_per_second: 6.487, epoch: 3.4392[0m
[32m[2022-09-01 14:18:57,298] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 14:18:57,299] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-01 14:18:57,299] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 14:18:57,299] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 14:18:57,299] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-09-01 14:19:08,961] [    INFO][0m - eval_loss: 2.474618911743164, eval_accuracy: 0.5455207574654042, eval_runtime: 11.6621, eval_samples_per_second: 117.731, eval_steps_per_second: 3.687, epoch: 3.4392[0m
[32m[2022-09-01 14:19:08,962] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1300[0m
[32m[2022-09-01 14:19:08,962] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 14:19:10,755] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1300/tokenizer_config.json[0m
[32m[2022-09-01 14:19:10,755] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1300/special_tokens_map.json[0m
[32m[2022-09-01 14:19:16,829] [    INFO][0m - loss: 0.2975492, learning_rate: 2.896031746031746e-05, global_step: 1310, interval_runtime: 19.5308, interval_samples_per_second: 0.41, interval_steps_per_second: 0.512, epoch: 3.4656[0m
[32m[2022-09-01 14:19:18,363] [    INFO][0m - loss: 0.4952312, learning_rate: 2.8952380952380955e-05, global_step: 1320, interval_runtime: 1.535, interval_samples_per_second: 5.212, interval_steps_per_second: 6.515, epoch: 3.4921[0m
[32m[2022-09-01 14:19:19,898] [    INFO][0m - loss: 0.27275112, learning_rate: 2.8944444444444446e-05, global_step: 1330, interval_runtime: 1.5342, interval_samples_per_second: 5.214, interval_steps_per_second: 6.518, epoch: 3.5185[0m
[32m[2022-09-01 14:19:21,445] [    INFO][0m - loss: 0.49560747, learning_rate: 2.893650793650794e-05, global_step: 1340, interval_runtime: 1.5474, interval_samples_per_second: 5.17, interval_steps_per_second: 6.462, epoch: 3.545[0m
[32m[2022-09-01 14:19:22,994] [    INFO][0m - loss: 0.4408596, learning_rate: 2.892857142857143e-05, global_step: 1350, interval_runtime: 1.5482, interval_samples_per_second: 5.167, interval_steps_per_second: 6.459, epoch: 3.5714[0m
[32m[2022-09-01 14:19:24,539] [    INFO][0m - loss: 0.32407415, learning_rate: 2.892063492063492e-05, global_step: 1360, interval_runtime: 1.546, interval_samples_per_second: 5.175, interval_steps_per_second: 6.468, epoch: 3.5979[0m
[32m[2022-09-01 14:19:26,077] [    INFO][0m - loss: 0.46423378, learning_rate: 2.8912698412698416e-05, global_step: 1370, interval_runtime: 1.5378, interval_samples_per_second: 5.202, interval_steps_per_second: 6.503, epoch: 3.6243[0m
[32m[2022-09-01 14:19:27,617] [    INFO][0m - loss: 0.60005412, learning_rate: 2.8904761904761907e-05, global_step: 1380, interval_runtime: 1.5393, interval_samples_per_second: 5.197, interval_steps_per_second: 6.496, epoch: 3.6508[0m
[32m[2022-09-01 14:19:29,164] [    INFO][0m - loss: 0.36595936, learning_rate: 2.8896825396825397e-05, global_step: 1390, interval_runtime: 1.5476, interval_samples_per_second: 5.169, interval_steps_per_second: 6.462, epoch: 3.6772[0m
[32m[2022-09-01 14:19:30,703] [    INFO][0m - loss: 0.57030134, learning_rate: 2.8888888888888888e-05, global_step: 1400, interval_runtime: 1.5387, interval_samples_per_second: 5.199, interval_steps_per_second: 6.499, epoch: 3.7037[0m
[32m[2022-09-01 14:19:30,704] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 14:19:30,704] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-01 14:19:30,704] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 14:19:30,704] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 14:19:30,704] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-09-01 14:19:42,351] [    INFO][0m - eval_loss: 2.6124014854431152, eval_accuracy: 0.5294974508375819, eval_runtime: 11.6469, eval_samples_per_second: 117.885, eval_steps_per_second: 3.692, epoch: 3.7037[0m
[32m[2022-09-01 14:19:42,352] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1400[0m
[32m[2022-09-01 14:19:42,352] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 14:19:44,388] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1400/tokenizer_config.json[0m
[32m[2022-09-01 14:19:44,389] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1400/special_tokens_map.json[0m
[32m[2022-09-01 14:19:53,230] [    INFO][0m - loss: 0.32145019, learning_rate: 2.8880952380952382e-05, global_step: 1410, interval_runtime: 22.5271, interval_samples_per_second: 0.355, interval_steps_per_second: 0.444, epoch: 3.7302[0m
[32m[2022-09-01 14:19:54,788] [    INFO][0m - loss: 0.3952508, learning_rate: 2.8873015873015873e-05, global_step: 1420, interval_runtime: 1.5581, interval_samples_per_second: 5.134, interval_steps_per_second: 6.418, epoch: 3.7566[0m
[32m[2022-09-01 14:19:56,336] [    INFO][0m - loss: 0.37749665, learning_rate: 2.8865079365079364e-05, global_step: 1430, interval_runtime: 1.548, interval_samples_per_second: 5.168, interval_steps_per_second: 6.46, epoch: 3.7831[0m
[32m[2022-09-01 14:19:57,887] [    INFO][0m - loss: 0.30296443, learning_rate: 2.8857142857142858e-05, global_step: 1440, interval_runtime: 1.551, interval_samples_per_second: 5.158, interval_steps_per_second: 6.447, epoch: 3.8095[0m
[32m[2022-09-01 14:19:59,432] [    INFO][0m - loss: 0.65408802, learning_rate: 2.884920634920635e-05, global_step: 1450, interval_runtime: 1.5452, interval_samples_per_second: 5.177, interval_steps_per_second: 6.472, epoch: 3.836[0m
[32m[2022-09-01 14:20:00,980] [    INFO][0m - loss: 0.48424625, learning_rate: 2.884126984126984e-05, global_step: 1460, interval_runtime: 1.5475, interval_samples_per_second: 5.17, interval_steps_per_second: 6.462, epoch: 3.8624[0m
[32m[2022-09-01 14:20:02,537] [    INFO][0m - loss: 0.67547941, learning_rate: 2.8833333333333334e-05, global_step: 1470, interval_runtime: 1.5566, interval_samples_per_second: 5.139, interval_steps_per_second: 6.424, epoch: 3.8889[0m
[32m[2022-09-01 14:20:04,080] [    INFO][0m - loss: 0.64911633, learning_rate: 2.8825396825396825e-05, global_step: 1480, interval_runtime: 1.5433, interval_samples_per_second: 5.184, interval_steps_per_second: 6.48, epoch: 3.9153[0m
[32m[2022-09-01 14:20:05,629] [    INFO][0m - loss: 0.45776072, learning_rate: 2.8817460317460316e-05, global_step: 1490, interval_runtime: 1.5491, interval_samples_per_second: 5.164, interval_steps_per_second: 6.455, epoch: 3.9418[0m
[32m[2022-09-01 14:20:07,177] [    INFO][0m - loss: 0.41666579, learning_rate: 2.880952380952381e-05, global_step: 1500, interval_runtime: 1.5482, interval_samples_per_second: 5.167, interval_steps_per_second: 6.459, epoch: 3.9683[0m
[32m[2022-09-01 14:20:07,178] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 14:20:07,178] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-01 14:20:07,178] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 14:20:07,178] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 14:20:07,178] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-09-01 14:20:18,933] [    INFO][0m - eval_loss: 2.5795018672943115, eval_accuracy: 0.5302257829570284, eval_runtime: 11.7544, eval_samples_per_second: 116.808, eval_steps_per_second: 3.658, epoch: 3.9683[0m
[32m[2022-09-01 14:20:18,934] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1500[0m
[32m[2022-09-01 14:20:18,934] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 14:20:25,392] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1500/tokenizer_config.json[0m
[32m[2022-09-01 14:20:25,393] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1500/special_tokens_map.json[0m
[32m[2022-09-01 14:20:31,135] [    INFO][0m - loss: 0.63215933, learning_rate: 2.88015873015873e-05, global_step: 1510, interval_runtime: 23.957, interval_samples_per_second: 0.334, interval_steps_per_second: 0.417, epoch: 3.9947[0m
[32m[2022-09-01 14:20:32,767] [    INFO][0m - loss: 0.3583957, learning_rate: 2.8793650793650795e-05, global_step: 1520, interval_runtime: 1.6331, interval_samples_per_second: 4.899, interval_steps_per_second: 6.123, epoch: 4.0212[0m
[32m[2022-09-01 14:20:34,307] [    INFO][0m - loss: 0.16435151, learning_rate: 2.878571428571429e-05, global_step: 1530, interval_runtime: 1.5391, interval_samples_per_second: 5.198, interval_steps_per_second: 6.497, epoch: 4.0476[0m
[32m[2022-09-01 14:20:35,852] [    INFO][0m - loss: 0.16842849, learning_rate: 2.877777777777778e-05, global_step: 1540, interval_runtime: 1.5455, interval_samples_per_second: 5.176, interval_steps_per_second: 6.47, epoch: 4.0741[0m
[32m[2022-09-01 14:20:37,385] [    INFO][0m - loss: 0.24658883, learning_rate: 2.876984126984127e-05, global_step: 1550, interval_runtime: 1.533, interval_samples_per_second: 5.219, interval_steps_per_second: 6.523, epoch: 4.1005[0m
[32m[2022-09-01 14:20:38,914] [    INFO][0m - loss: 0.1612457, learning_rate: 2.8761904761904765e-05, global_step: 1560, interval_runtime: 1.5289, interval_samples_per_second: 5.233, interval_steps_per_second: 6.541, epoch: 4.127[0m
[32m[2022-09-01 14:20:40,447] [    INFO][0m - loss: 0.22431421, learning_rate: 2.8753968253968255e-05, global_step: 1570, interval_runtime: 1.5327, interval_samples_per_second: 5.22, interval_steps_per_second: 6.524, epoch: 4.1534[0m
[32m[2022-09-01 14:20:41,988] [    INFO][0m - loss: 0.18536294, learning_rate: 2.8746031746031746e-05, global_step: 1580, interval_runtime: 1.5408, interval_samples_per_second: 5.192, interval_steps_per_second: 6.49, epoch: 4.1799[0m
[32m[2022-09-01 14:20:43,529] [    INFO][0m - loss: 0.08298215, learning_rate: 2.873809523809524e-05, global_step: 1590, interval_runtime: 1.5412, interval_samples_per_second: 5.191, interval_steps_per_second: 6.489, epoch: 4.2063[0m
[32m[2022-09-01 14:20:45,070] [    INFO][0m - loss: 0.16164755, learning_rate: 2.873015873015873e-05, global_step: 1600, interval_runtime: 1.541, interval_samples_per_second: 5.191, interval_steps_per_second: 6.489, epoch: 4.2328[0m
[32m[2022-09-01 14:20:45,070] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 14:20:45,070] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-01 14:20:45,070] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 14:20:45,070] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 14:20:45,071] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-09-01 14:20:56,919] [    INFO][0m - eval_loss: 2.740914821624756, eval_accuracy: 0.5302257829570284, eval_runtime: 11.8483, eval_samples_per_second: 115.881, eval_steps_per_second: 3.629, epoch: 4.2328[0m
[32m[2022-09-01 14:20:56,920] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1600[0m
[32m[2022-09-01 14:20:56,920] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 14:20:58,757] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1600/tokenizer_config.json[0m
[32m[2022-09-01 14:20:58,757] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1600/special_tokens_map.json[0m
[32m[2022-09-01 14:21:04,588] [    INFO][0m - loss: 0.20732894, learning_rate: 2.8722222222222222e-05, global_step: 1610, interval_runtime: 19.5178, interval_samples_per_second: 0.41, interval_steps_per_second: 0.512, epoch: 4.2593[0m
[32m[2022-09-01 14:21:06,125] [    INFO][0m - loss: 0.30801506, learning_rate: 2.8714285714285716e-05, global_step: 1620, interval_runtime: 1.5372, interval_samples_per_second: 5.204, interval_steps_per_second: 6.505, epoch: 4.2857[0m
[32m[2022-09-01 14:21:07,663] [    INFO][0m - loss: 0.15429301, learning_rate: 2.8706349206349207e-05, global_step: 1630, interval_runtime: 1.5382, interval_samples_per_second: 5.201, interval_steps_per_second: 6.501, epoch: 4.3122[0m
[32m[2022-09-01 14:21:09,201] [    INFO][0m - loss: 0.22944016, learning_rate: 2.8698412698412698e-05, global_step: 1640, interval_runtime: 1.5382, interval_samples_per_second: 5.201, interval_steps_per_second: 6.501, epoch: 4.3386[0m
[32m[2022-09-01 14:21:10,742] [    INFO][0m - loss: 0.48917375, learning_rate: 2.8690476190476192e-05, global_step: 1650, interval_runtime: 1.5414, interval_samples_per_second: 5.19, interval_steps_per_second: 6.488, epoch: 4.3651[0m
[32m[2022-09-01 14:21:12,279] [    INFO][0m - loss: 0.21541944, learning_rate: 2.8682539682539683e-05, global_step: 1660, interval_runtime: 1.536, interval_samples_per_second: 5.208, interval_steps_per_second: 6.51, epoch: 4.3915[0m
[32m[2022-09-01 14:21:13,811] [    INFO][0m - loss: 0.18304343, learning_rate: 2.8674603174603174e-05, global_step: 1670, interval_runtime: 1.5324, interval_samples_per_second: 5.221, interval_steps_per_second: 6.526, epoch: 4.418[0m
[32m[2022-09-01 14:21:15,351] [    INFO][0m - loss: 0.1781208, learning_rate: 2.8666666666666668e-05, global_step: 1680, interval_runtime: 1.5399, interval_samples_per_second: 5.195, interval_steps_per_second: 6.494, epoch: 4.4444[0m
[32m[2022-09-01 14:21:16,892] [    INFO][0m - loss: 0.10970395, learning_rate: 2.865873015873016e-05, global_step: 1690, interval_runtime: 1.5412, interval_samples_per_second: 5.191, interval_steps_per_second: 6.489, epoch: 4.4709[0m
[32m[2022-09-01 14:21:18,430] [    INFO][0m - loss: 0.22423339, learning_rate: 2.865079365079365e-05, global_step: 1700, interval_runtime: 1.5377, interval_samples_per_second: 5.202, interval_steps_per_second: 6.503, epoch: 4.4974[0m
[32m[2022-09-01 14:21:18,430] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 14:21:18,431] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-01 14:21:18,431] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 14:21:18,431] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 14:21:18,431] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-09-01 14:21:30,092] [    INFO][0m - eval_loss: 2.8296573162078857, eval_accuracy: 0.5236707938820102, eval_runtime: 11.661, eval_samples_per_second: 117.743, eval_steps_per_second: 3.688, epoch: 4.4974[0m
[32m[2022-09-01 14:21:30,093] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1700[0m
[32m[2022-09-01 14:21:30,093] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 14:21:31,925] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1700/tokenizer_config.json[0m
[32m[2022-09-01 14:21:31,926] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1700/special_tokens_map.json[0m
[32m[2022-09-01 14:21:43,345] [    INFO][0m - loss: 0.30308495, learning_rate: 2.8642857142857144e-05, global_step: 1710, interval_runtime: 24.9145, interval_samples_per_second: 0.321, interval_steps_per_second: 0.401, epoch: 4.5238[0m
[32m[2022-09-01 14:21:44,892] [    INFO][0m - loss: 0.35375044, learning_rate: 2.8634920634920638e-05, global_step: 1720, interval_runtime: 1.547, interval_samples_per_second: 5.171, interval_steps_per_second: 6.464, epoch: 4.5503[0m
[32m[2022-09-01 14:21:46,431] [    INFO][0m - loss: 0.16687928, learning_rate: 2.862698412698413e-05, global_step: 1730, interval_runtime: 1.5392, interval_samples_per_second: 5.197, interval_steps_per_second: 6.497, epoch: 4.5767[0m
[32m[2022-09-01 14:21:47,974] [    INFO][0m - loss: 0.17894709, learning_rate: 2.8619047619047623e-05, global_step: 1740, interval_runtime: 1.5435, interval_samples_per_second: 5.183, interval_steps_per_second: 6.479, epoch: 4.6032[0m
[32m[2022-09-01 14:21:49,520] [    INFO][0m - loss: 0.25466821, learning_rate: 2.8611111111111113e-05, global_step: 1750, interval_runtime: 1.545, interval_samples_per_second: 5.178, interval_steps_per_second: 6.473, epoch: 4.6296[0m
[32m[2022-09-01 14:21:51,055] [    INFO][0m - loss: 0.42441206, learning_rate: 2.8603174603174604e-05, global_step: 1760, interval_runtime: 1.5357, interval_samples_per_second: 5.209, interval_steps_per_second: 6.512, epoch: 4.6561[0m
[32m[2022-09-01 14:21:52,596] [    INFO][0m - loss: 0.23766274, learning_rate: 2.85952380952381e-05, global_step: 1770, interval_runtime: 1.5406, interval_samples_per_second: 5.193, interval_steps_per_second: 6.491, epoch: 4.6825[0m
[32m[2022-09-01 14:21:54,147] [    INFO][0m - loss: 0.27431273, learning_rate: 2.858730158730159e-05, global_step: 1780, interval_runtime: 1.5518, interval_samples_per_second: 5.155, interval_steps_per_second: 6.444, epoch: 4.709[0m
[32m[2022-09-01 14:21:55,693] [    INFO][0m - loss: 0.328863, learning_rate: 2.857936507936508e-05, global_step: 1790, interval_runtime: 1.5454, interval_samples_per_second: 5.177, interval_steps_per_second: 6.471, epoch: 4.7354[0m
[32m[2022-09-01 14:21:57,237] [    INFO][0m - loss: 0.20667682, learning_rate: 2.857142857142857e-05, global_step: 1800, interval_runtime: 1.5443, interval_samples_per_second: 5.18, interval_steps_per_second: 6.475, epoch: 4.7619[0m
[32m[2022-09-01 14:21:57,238] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 14:21:57,238] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-01 14:21:57,238] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 14:21:57,238] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 14:21:57,238] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-09-01 14:22:08,902] [    INFO][0m - eval_loss: 2.793457269668579, eval_accuracy: 0.528040786598689, eval_runtime: 11.6629, eval_samples_per_second: 117.724, eval_steps_per_second: 3.687, epoch: 4.7619[0m
[32m[2022-09-01 14:22:08,902] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1800[0m
[32m[2022-09-01 14:22:08,903] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 14:22:10,745] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1800/tokenizer_config.json[0m
[32m[2022-09-01 14:22:10,746] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1800/special_tokens_map.json[0m
[32m[2022-09-01 14:22:16,634] [    INFO][0m - loss: 0.25053134, learning_rate: 2.8563492063492065e-05, global_step: 1810, interval_runtime: 19.3969, interval_samples_per_second: 0.412, interval_steps_per_second: 0.516, epoch: 4.7884[0m
[32m[2022-09-01 14:22:18,174] [    INFO][0m - loss: 0.30430989, learning_rate: 2.8555555555555556e-05, global_step: 1820, interval_runtime: 1.5401, interval_samples_per_second: 5.194, interval_steps_per_second: 6.493, epoch: 4.8148[0m
[32m[2022-09-01 14:22:19,715] [    INFO][0m - loss: 0.20925989, learning_rate: 2.8547619047619047e-05, global_step: 1830, interval_runtime: 1.5405, interval_samples_per_second: 5.193, interval_steps_per_second: 6.491, epoch: 4.8413[0m
[32m[2022-09-01 14:22:21,255] [    INFO][0m - loss: 0.21598709, learning_rate: 2.853968253968254e-05, global_step: 1840, interval_runtime: 1.5406, interval_samples_per_second: 5.193, interval_steps_per_second: 6.491, epoch: 4.8677[0m
[32m[2022-09-01 14:22:22,804] [    INFO][0m - loss: 0.2729198, learning_rate: 2.853174603174603e-05, global_step: 1850, interval_runtime: 1.549, interval_samples_per_second: 5.165, interval_steps_per_second: 6.456, epoch: 4.8942[0m
[32m[2022-09-01 14:22:24,347] [    INFO][0m - loss: 0.28904545, learning_rate: 2.8523809523809522e-05, global_step: 1860, interval_runtime: 1.543, interval_samples_per_second: 5.185, interval_steps_per_second: 6.481, epoch: 4.9206[0m
[32m[2022-09-01 14:22:25,885] [    INFO][0m - loss: 0.49952202, learning_rate: 2.8515873015873017e-05, global_step: 1870, interval_runtime: 1.5381, interval_samples_per_second: 5.201, interval_steps_per_second: 6.501, epoch: 4.9471[0m
[32m[2022-09-01 14:22:27,428] [    INFO][0m - loss: 0.30435433, learning_rate: 2.8507936507936507e-05, global_step: 1880, interval_runtime: 1.5424, interval_samples_per_second: 5.187, interval_steps_per_second: 6.483, epoch: 4.9735[0m
[32m[2022-09-01 14:22:28,985] [    INFO][0m - loss: 0.15803325, learning_rate: 2.8499999999999998e-05, global_step: 1890, interval_runtime: 1.5576, interval_samples_per_second: 5.136, interval_steps_per_second: 6.42, epoch: 5.0[0m
[32m[2022-09-01 14:22:30,650] [    INFO][0m - loss: 0.0896345, learning_rate: 2.8492063492063492e-05, global_step: 1900, interval_runtime: 1.6641, interval_samples_per_second: 4.807, interval_steps_per_second: 6.009, epoch: 5.0265[0m
[32m[2022-09-01 14:22:30,651] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 14:22:30,651] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-01 14:22:30,651] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 14:22:30,651] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 14:22:30,651] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-09-01 14:22:42,274] [    INFO][0m - eval_loss: 2.8707520961761475, eval_accuracy: 0.5236707938820102, eval_runtime: 11.6228, eval_samples_per_second: 118.13, eval_steps_per_second: 3.7, epoch: 5.0265[0m
[32m[2022-09-01 14:22:42,275] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1900[0m
[32m[2022-09-01 14:22:42,275] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 14:22:44,106] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1900/tokenizer_config.json[0m
[32m[2022-09-01 14:22:44,106] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1900/special_tokens_map.json[0m
[32m[2022-09-01 14:22:49,627] [    INFO][0m - loss: 0.11972344, learning_rate: 2.8484126984126983e-05, global_step: 1910, interval_runtime: 18.977, interval_samples_per_second: 0.422, interval_steps_per_second: 0.527, epoch: 5.0529[0m
[32m[2022-09-01 14:22:51,178] [    INFO][0m - loss: 0.1080009, learning_rate: 2.8476190476190477e-05, global_step: 1920, interval_runtime: 1.551, interval_samples_per_second: 5.158, interval_steps_per_second: 6.447, epoch: 5.0794[0m
[32m[2022-09-01 14:22:52,725] [    INFO][0m - loss: 0.09963325, learning_rate: 2.846825396825397e-05, global_step: 1930, interval_runtime: 1.5472, interval_samples_per_second: 5.171, interval_steps_per_second: 6.463, epoch: 5.1058[0m
[32m[2022-09-01 14:22:54,270] [    INFO][0m - loss: 0.20009995, learning_rate: 2.8460317460317462e-05, global_step: 1940, interval_runtime: 1.5451, interval_samples_per_second: 5.178, interval_steps_per_second: 6.472, epoch: 5.1323[0m
[32m[2022-09-01 14:22:55,804] [    INFO][0m - loss: 0.23172281, learning_rate: 2.8452380952380953e-05, global_step: 1950, interval_runtime: 1.5343, interval_samples_per_second: 5.214, interval_steps_per_second: 6.517, epoch: 5.1587[0m
[32m[2022-09-01 14:22:57,340] [    INFO][0m - loss: 0.072724, learning_rate: 2.8444444444444447e-05, global_step: 1960, interval_runtime: 1.536, interval_samples_per_second: 5.208, interval_steps_per_second: 6.51, epoch: 5.1852[0m
[32m[2022-09-01 14:22:58,877] [    INFO][0m - loss: 0.14112433, learning_rate: 2.8436507936507938e-05, global_step: 1970, interval_runtime: 1.5362, interval_samples_per_second: 5.208, interval_steps_per_second: 6.51, epoch: 5.2116[0m
[32m[2022-09-01 14:23:00,420] [    INFO][0m - loss: 0.05961108, learning_rate: 2.842857142857143e-05, global_step: 1980, interval_runtime: 1.5433, interval_samples_per_second: 5.184, interval_steps_per_second: 6.48, epoch: 5.2381[0m
[32m[2022-09-01 14:23:01,972] [    INFO][0m - loss: 0.14188178, learning_rate: 2.8420634920634923e-05, global_step: 1990, interval_runtime: 1.5523, interval_samples_per_second: 5.154, interval_steps_per_second: 6.442, epoch: 5.2646[0m
[32m[2022-09-01 14:23:03,516] [    INFO][0m - loss: 0.11838722, learning_rate: 2.8412698412698414e-05, global_step: 2000, interval_runtime: 1.5441, interval_samples_per_second: 5.181, interval_steps_per_second: 6.476, epoch: 5.291[0m
[32m[2022-09-01 14:23:03,516] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 14:23:03,517] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-01 14:23:03,517] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 14:23:03,517] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 14:23:03,517] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-09-01 14:23:15,221] [    INFO][0m - eval_loss: 3.005160093307495, eval_accuracy: 0.5462490895848507, eval_runtime: 11.7036, eval_samples_per_second: 117.315, eval_steps_per_second: 3.674, epoch: 5.291[0m
[32m[2022-09-01 14:23:15,222] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2000[0m
[32m[2022-09-01 14:23:15,222] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 14:23:17,048] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2000/tokenizer_config.json[0m
[32m[2022-09-01 14:23:17,048] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2000/special_tokens_map.json[0m
[32m[2022-09-01 14:23:23,154] [    INFO][0m - loss: 0.03592678, learning_rate: 2.8404761904761905e-05, global_step: 2010, interval_runtime: 19.6365, interval_samples_per_second: 0.407, interval_steps_per_second: 0.509, epoch: 5.3175[0m
[32m[2022-09-01 14:23:24,694] [    INFO][0m - loss: 0.1290558, learning_rate: 2.83968253968254e-05, global_step: 2020, interval_runtime: 1.5412, interval_samples_per_second: 5.191, interval_steps_per_second: 6.488, epoch: 5.3439[0m
[32m[2022-09-01 14:23:26,242] [    INFO][0m - loss: 0.10873548, learning_rate: 2.838888888888889e-05, global_step: 2030, interval_runtime: 1.5472, interval_samples_per_second: 5.17, interval_steps_per_second: 6.463, epoch: 5.3704[0m
[32m[2022-09-01 14:23:27,786] [    INFO][0m - loss: 0.11288593, learning_rate: 2.838095238095238e-05, global_step: 2040, interval_runtime: 1.5445, interval_samples_per_second: 5.18, interval_steps_per_second: 6.474, epoch: 5.3968[0m
[32m[2022-09-01 14:23:29,334] [    INFO][0m - loss: 0.19957572, learning_rate: 2.8373015873015875e-05, global_step: 2050, interval_runtime: 1.5477, interval_samples_per_second: 5.169, interval_steps_per_second: 6.461, epoch: 5.4233[0m
[32m[2022-09-01 14:23:30,878] [    INFO][0m - loss: 0.05373381, learning_rate: 2.8365079365079365e-05, global_step: 2060, interval_runtime: 1.5445, interval_samples_per_second: 5.18, interval_steps_per_second: 6.475, epoch: 5.4497[0m
[32m[2022-09-01 14:23:32,426] [    INFO][0m - loss: 0.11377363, learning_rate: 2.8357142857142856e-05, global_step: 2070, interval_runtime: 1.5479, interval_samples_per_second: 5.168, interval_steps_per_second: 6.461, epoch: 5.4762[0m
[32m[2022-09-01 14:23:33,974] [    INFO][0m - loss: 0.09123718, learning_rate: 2.834920634920635e-05, global_step: 2080, interval_runtime: 1.5471, interval_samples_per_second: 5.171, interval_steps_per_second: 6.464, epoch: 5.5026[0m
[32m[2022-09-01 14:23:35,516] [    INFO][0m - loss: 0.39210291, learning_rate: 2.834126984126984e-05, global_step: 2090, interval_runtime: 1.5434, interval_samples_per_second: 5.183, interval_steps_per_second: 6.479, epoch: 5.5291[0m
[32m[2022-09-01 14:23:37,057] [    INFO][0m - loss: 0.14701144, learning_rate: 2.8333333333333332e-05, global_step: 2100, interval_runtime: 1.541, interval_samples_per_second: 5.192, interval_steps_per_second: 6.489, epoch: 5.5556[0m
[32m[2022-09-01 14:23:37,058] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 14:23:37,058] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-01 14:23:37,058] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 14:23:37,059] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 14:23:37,059] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-09-01 14:23:48,717] [    INFO][0m - eval_loss: 2.974357843399048, eval_accuracy: 0.5345957756737072, eval_runtime: 11.6576, eval_samples_per_second: 117.777, eval_steps_per_second: 3.689, epoch: 5.5556[0m
[32m[2022-09-01 14:23:48,718] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2100[0m
[32m[2022-09-01 14:23:48,718] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 14:23:50,874] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2100/tokenizer_config.json[0m
[32m[2022-09-01 14:23:50,875] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2100/special_tokens_map.json[0m
[32m[2022-09-01 14:23:56,740] [    INFO][0m - loss: 0.16289515, learning_rate: 2.8325396825396826e-05, global_step: 2110, interval_runtime: 19.6824, interval_samples_per_second: 0.406, interval_steps_per_second: 0.508, epoch: 5.582[0m
[32m[2022-09-01 14:23:58,271] [    INFO][0m - loss: 0.22207189, learning_rate: 2.8317460317460317e-05, global_step: 2120, interval_runtime: 1.5318, interval_samples_per_second: 5.223, interval_steps_per_second: 6.528, epoch: 5.6085[0m
[32m[2022-09-01 14:23:59,814] [    INFO][0m - loss: 0.1108362, learning_rate: 2.830952380952381e-05, global_step: 2130, interval_runtime: 1.5425, interval_samples_per_second: 5.186, interval_steps_per_second: 6.483, epoch: 5.6349[0m
[32m[2022-09-01 14:24:01,350] [    INFO][0m - loss: 0.24929819, learning_rate: 2.8301587301587305e-05, global_step: 2140, interval_runtime: 1.5365, interval_samples_per_second: 5.207, interval_steps_per_second: 6.508, epoch: 5.6614[0m
[32m[2022-09-01 14:24:02,889] [    INFO][0m - loss: 0.1031366, learning_rate: 2.8293650793650796e-05, global_step: 2150, interval_runtime: 1.5383, interval_samples_per_second: 5.201, interval_steps_per_second: 6.501, epoch: 5.6878[0m
[32m[2022-09-01 14:24:04,429] [    INFO][0m - loss: 0.10001223, learning_rate: 2.8285714285714287e-05, global_step: 2160, interval_runtime: 1.5399, interval_samples_per_second: 5.195, interval_steps_per_second: 6.494, epoch: 5.7143[0m
[32m[2022-09-01 14:24:05,966] [    INFO][0m - loss: 0.22184174, learning_rate: 2.8277777777777778e-05, global_step: 2170, interval_runtime: 1.5372, interval_samples_per_second: 5.204, interval_steps_per_second: 6.505, epoch: 5.7407[0m
[32m[2022-09-01 14:24:07,506] [    INFO][0m - loss: 0.14035496, learning_rate: 2.8269841269841272e-05, global_step: 2180, interval_runtime: 1.5405, interval_samples_per_second: 5.193, interval_steps_per_second: 6.492, epoch: 5.7672[0m
[32m[2022-09-01 14:24:09,044] [    INFO][0m - loss: 0.26224763, learning_rate: 2.8261904761904763e-05, global_step: 2190, interval_runtime: 1.5377, interval_samples_per_second: 5.203, interval_steps_per_second: 6.503, epoch: 5.7937[0m
[32m[2022-09-01 14:24:10,589] [    INFO][0m - loss: 0.25458264, learning_rate: 2.8253968253968253e-05, global_step: 2200, interval_runtime: 1.5451, interval_samples_per_second: 5.178, interval_steps_per_second: 6.472, epoch: 5.8201[0m
[32m[2022-09-01 14:24:10,590] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 14:24:10,590] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-01 14:24:10,590] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 14:24:10,590] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 14:24:10,590] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-09-01 14:24:22,261] [    INFO][0m - eval_loss: 3.139336109161377, eval_accuracy: 0.5127458120903132, eval_runtime: 11.6702, eval_samples_per_second: 117.65, eval_steps_per_second: 3.685, epoch: 5.8201[0m
[32m[2022-09-01 14:24:22,262] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2200[0m
[32m[2022-09-01 14:24:22,262] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 14:24:24,220] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2200/tokenizer_config.json[0m
[32m[2022-09-01 14:24:24,220] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2200/special_tokens_map.json[0m
[32m[2022-09-01 14:24:30,154] [    INFO][0m - loss: 0.17532444, learning_rate: 2.8246031746031748e-05, global_step: 2210, interval_runtime: 19.5645, interval_samples_per_second: 0.409, interval_steps_per_second: 0.511, epoch: 5.8466[0m
[32m[2022-09-01 14:24:31,696] [    INFO][0m - loss: 0.23937337, learning_rate: 2.823809523809524e-05, global_step: 2220, interval_runtime: 1.5429, interval_samples_per_second: 5.185, interval_steps_per_second: 6.481, epoch: 5.873[0m
[32m[2022-09-01 14:24:33,241] [    INFO][0m - loss: 0.1096751, learning_rate: 2.823015873015873e-05, global_step: 2230, interval_runtime: 1.5443, interval_samples_per_second: 5.18, interval_steps_per_second: 6.475, epoch: 5.8995[0m
[32m[2022-09-01 14:24:34,777] [    INFO][0m - loss: 0.06199538, learning_rate: 2.8222222222222223e-05, global_step: 2240, interval_runtime: 1.5361, interval_samples_per_second: 5.208, interval_steps_per_second: 6.51, epoch: 5.9259[0m
[32m[2022-09-01 14:24:36,312] [    INFO][0m - loss: 0.13731062, learning_rate: 2.8214285714285714e-05, global_step: 2250, interval_runtime: 1.5355, interval_samples_per_second: 5.21, interval_steps_per_second: 6.512, epoch: 5.9524[0m
[32m[2022-09-01 14:24:37,852] [    INFO][0m - loss: 0.29023519, learning_rate: 2.8206349206349205e-05, global_step: 2260, interval_runtime: 1.5394, interval_samples_per_second: 5.197, interval_steps_per_second: 6.496, epoch: 5.9788[0m
[32m[2022-09-01 14:24:39,497] [    INFO][0m - loss: 0.08654929, learning_rate: 2.81984126984127e-05, global_step: 2270, interval_runtime: 1.6449, interval_samples_per_second: 4.864, interval_steps_per_second: 6.08, epoch: 6.0053[0m
[32m[2022-09-01 14:24:41,044] [    INFO][0m - loss: 0.17468338, learning_rate: 2.819047619047619e-05, global_step: 2280, interval_runtime: 1.5472, interval_samples_per_second: 5.171, interval_steps_per_second: 6.463, epoch: 6.0317[0m
[32m[2022-09-01 14:24:42,585] [    INFO][0m - loss: 0.02769471, learning_rate: 2.818253968253968e-05, global_step: 2290, interval_runtime: 1.5416, interval_samples_per_second: 5.189, interval_steps_per_second: 6.487, epoch: 6.0582[0m
[32m[2022-09-01 14:24:44,135] [    INFO][0m - loss: 0.09938787, learning_rate: 2.8174603174603175e-05, global_step: 2300, interval_runtime: 1.5499, interval_samples_per_second: 5.162, interval_steps_per_second: 6.452, epoch: 6.0847[0m
[32m[2022-09-01 14:24:44,136] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 14:24:44,136] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-01 14:24:44,136] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 14:24:44,136] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 14:24:44,136] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-09-01 14:24:55,787] [    INFO][0m - eval_loss: 3.2407286167144775, eval_accuracy: 0.5302257829570284, eval_runtime: 11.6498, eval_samples_per_second: 117.856, eval_steps_per_second: 3.691, epoch: 6.0847[0m
[32m[2022-09-01 14:24:55,788] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2300[0m
[32m[2022-09-01 14:24:55,788] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 14:24:57,761] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2300/tokenizer_config.json[0m
[32m[2022-09-01 14:24:57,761] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2300/special_tokens_map.json[0m
[32m[2022-09-01 14:25:03,882] [    INFO][0m - loss: 0.06010152, learning_rate: 2.8166666666666666e-05, global_step: 2310, interval_runtime: 19.7469, interval_samples_per_second: 0.405, interval_steps_per_second: 0.506, epoch: 6.1111[0m
[32m[2022-09-01 14:25:05,419] [    INFO][0m - loss: 0.11296233, learning_rate: 2.8158730158730157e-05, global_step: 2320, interval_runtime: 1.536, interval_samples_per_second: 5.208, interval_steps_per_second: 6.51, epoch: 6.1376[0m
[32m[2022-09-01 14:25:06,954] [    INFO][0m - loss: 0.08044054, learning_rate: 2.8150793650793654e-05, global_step: 2330, interval_runtime: 1.5362, interval_samples_per_second: 5.208, interval_steps_per_second: 6.509, epoch: 6.164[0m
[32m[2022-09-01 14:25:08,495] [    INFO][0m - loss: 0.12196175, learning_rate: 2.8142857142857145e-05, global_step: 2340, interval_runtime: 1.5398, interval_samples_per_second: 5.196, interval_steps_per_second: 6.495, epoch: 6.1905[0m
[32m[2022-09-01 14:25:10,032] [    INFO][0m - loss: 0.12097886, learning_rate: 2.8134920634920636e-05, global_step: 2350, interval_runtime: 1.5378, interval_samples_per_second: 5.202, interval_steps_per_second: 6.503, epoch: 6.2169[0m
[32m[2022-09-01 14:25:11,572] [    INFO][0m - loss: 0.1566119, learning_rate: 2.812698412698413e-05, global_step: 2360, interval_runtime: 1.5396, interval_samples_per_second: 5.196, interval_steps_per_second: 6.495, epoch: 6.2434[0m
[32m[2022-09-01 14:25:13,113] [    INFO][0m - loss: 0.04428331, learning_rate: 2.811904761904762e-05, global_step: 2370, interval_runtime: 1.5412, interval_samples_per_second: 5.191, interval_steps_per_second: 6.489, epoch: 6.2698[0m
[32m[2022-09-01 14:25:14,649] [    INFO][0m - loss: 0.10060781, learning_rate: 2.811111111111111e-05, global_step: 2380, interval_runtime: 1.536, interval_samples_per_second: 5.208, interval_steps_per_second: 6.51, epoch: 6.2963[0m
[32m[2022-09-01 14:25:16,190] [    INFO][0m - loss: 0.08702517, learning_rate: 2.8103174603174606e-05, global_step: 2390, interval_runtime: 1.5408, interval_samples_per_second: 5.192, interval_steps_per_second: 6.49, epoch: 6.3228[0m
[32m[2022-09-01 14:25:17,733] [    INFO][0m - loss: 0.00929921, learning_rate: 2.8095238095238096e-05, global_step: 2400, interval_runtime: 1.5432, interval_samples_per_second: 5.184, interval_steps_per_second: 6.48, epoch: 6.3492[0m
[32m[2022-09-01 14:25:17,733] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 14:25:17,733] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-01 14:25:17,734] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 14:25:17,734] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 14:25:17,734] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-09-01 14:25:29,410] [    INFO][0m - eval_loss: 3.1210122108459473, eval_accuracy: 0.5440640932265113, eval_runtime: 11.676, eval_samples_per_second: 117.592, eval_steps_per_second: 3.683, epoch: 6.3492[0m
[32m[2022-09-01 14:25:29,411] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2400[0m
[32m[2022-09-01 14:25:29,411] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 14:25:31,758] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2400/tokenizer_config.json[0m
[32m[2022-09-01 14:25:31,759] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2400/special_tokens_map.json[0m
[32m[2022-09-01 14:25:37,928] [    INFO][0m - loss: 0.12751164, learning_rate: 2.8087301587301587e-05, global_step: 2410, interval_runtime: 20.195, interval_samples_per_second: 0.396, interval_steps_per_second: 0.495, epoch: 6.3757[0m
[32m[2022-09-01 14:25:39,465] [    INFO][0m - loss: 0.11776983, learning_rate: 2.807936507936508e-05, global_step: 2420, interval_runtime: 1.5366, interval_samples_per_second: 5.206, interval_steps_per_second: 6.508, epoch: 6.4021[0m
[32m[2022-09-01 14:25:41,004] [    INFO][0m - loss: 0.15598521, learning_rate: 2.8071428571428572e-05, global_step: 2430, interval_runtime: 1.5396, interval_samples_per_second: 5.196, interval_steps_per_second: 6.495, epoch: 6.4286[0m
[32m[2022-09-01 14:25:42,544] [    INFO][0m - loss: 0.10282046, learning_rate: 2.8063492063492063e-05, global_step: 2440, interval_runtime: 1.5393, interval_samples_per_second: 5.197, interval_steps_per_second: 6.496, epoch: 6.455[0m
[32m[2022-09-01 14:25:44,084] [    INFO][0m - loss: 0.03818308, learning_rate: 2.8055555555555557e-05, global_step: 2450, interval_runtime: 1.54, interval_samples_per_second: 5.195, interval_steps_per_second: 6.493, epoch: 6.4815[0m
[32m[2022-09-01 14:25:45,622] [    INFO][0m - loss: 0.02398412, learning_rate: 2.8047619047619048e-05, global_step: 2460, interval_runtime: 1.5381, interval_samples_per_second: 5.201, interval_steps_per_second: 6.501, epoch: 6.5079[0m
[32m[2022-09-01 14:25:47,157] [    INFO][0m - loss: 0.03545844, learning_rate: 2.803968253968254e-05, global_step: 2470, interval_runtime: 1.5356, interval_samples_per_second: 5.21, interval_steps_per_second: 6.512, epoch: 6.5344[0m
[32m[2022-09-01 14:25:48,699] [    INFO][0m - loss: 0.15936803, learning_rate: 2.8031746031746033e-05, global_step: 2480, interval_runtime: 1.5411, interval_samples_per_second: 5.191, interval_steps_per_second: 6.489, epoch: 6.5608[0m
[32m[2022-09-01 14:25:50,240] [    INFO][0m - loss: 0.04160484, learning_rate: 2.8023809523809524e-05, global_step: 2490, interval_runtime: 1.5418, interval_samples_per_second: 5.189, interval_steps_per_second: 6.486, epoch: 6.5873[0m
[32m[2022-09-01 14:25:51,774] [    INFO][0m - loss: 0.04348568, learning_rate: 2.8015873015873015e-05, global_step: 2500, interval_runtime: 1.5336, interval_samples_per_second: 5.216, interval_steps_per_second: 6.521, epoch: 6.6138[0m
[32m[2022-09-01 14:25:51,774] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 14:25:51,775] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-01 14:25:51,775] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 14:25:51,775] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 14:25:51,775] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-09-01 14:26:03,463] [    INFO][0m - eval_loss: 3.2211854457855225, eval_accuracy: 0.5287691187181355, eval_runtime: 11.6882, eval_samples_per_second: 117.468, eval_steps_per_second: 3.679, epoch: 6.6138[0m
[32m[2022-09-01 14:26:03,464] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2500[0m
[32m[2022-09-01 14:26:03,464] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 14:26:05,272] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2500/tokenizer_config.json[0m
[32m[2022-09-01 14:26:05,272] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2500/special_tokens_map.json[0m
[32m[2022-09-01 14:26:11,645] [    INFO][0m - loss: 0.01519181, learning_rate: 2.800793650793651e-05, global_step: 2510, interval_runtime: 19.8704, interval_samples_per_second: 0.403, interval_steps_per_second: 0.503, epoch: 6.6402[0m
[32m[2022-09-01 14:26:13,188] [    INFO][0m - loss: 0.02412873, learning_rate: 2.8e-05, global_step: 2520, interval_runtime: 1.5434, interval_samples_per_second: 5.183, interval_steps_per_second: 6.479, epoch: 6.6667[0m
[32m[2022-09-01 14:26:14,727] [    INFO][0m - loss: 0.15031757, learning_rate: 2.7992063492063494e-05, global_step: 2530, interval_runtime: 1.5384, interval_samples_per_second: 5.2, interval_steps_per_second: 6.5, epoch: 6.6931[0m
[32m[2022-09-01 14:26:16,266] [    INFO][0m - loss: 0.01873586, learning_rate: 2.7984126984126988e-05, global_step: 2540, interval_runtime: 1.5397, interval_samples_per_second: 5.196, interval_steps_per_second: 6.495, epoch: 6.7196[0m
[32m[2022-09-01 14:26:17,815] [    INFO][0m - loss: 0.19745878, learning_rate: 2.797619047619048e-05, global_step: 2550, interval_runtime: 1.5488, interval_samples_per_second: 5.165, interval_steps_per_second: 6.456, epoch: 6.746[0m
[32m[2022-09-01 14:26:19,349] [    INFO][0m - loss: 0.08090546, learning_rate: 2.796825396825397e-05, global_step: 2560, interval_runtime: 1.5342, interval_samples_per_second: 5.214, interval_steps_per_second: 6.518, epoch: 6.7725[0m
[32m[2022-09-01 14:26:20,891] [    INFO][0m - loss: 0.01680048, learning_rate: 2.796031746031746e-05, global_step: 2570, interval_runtime: 1.5414, interval_samples_per_second: 5.19, interval_steps_per_second: 6.488, epoch: 6.7989[0m
[32m[2022-09-01 14:26:22,437] [    INFO][0m - loss: 0.02819506, learning_rate: 2.7952380952380955e-05, global_step: 2580, interval_runtime: 1.5466, interval_samples_per_second: 5.173, interval_steps_per_second: 6.466, epoch: 6.8254[0m
[32m[2022-09-01 14:26:23,980] [    INFO][0m - loss: 0.2000771, learning_rate: 2.7944444444444445e-05, global_step: 2590, interval_runtime: 1.5427, interval_samples_per_second: 5.186, interval_steps_per_second: 6.482, epoch: 6.8519[0m
[32m[2022-09-01 14:26:25,519] [    INFO][0m - loss: 0.03396706, learning_rate: 2.7936507936507936e-05, global_step: 2600, interval_runtime: 1.5395, interval_samples_per_second: 5.197, interval_steps_per_second: 6.496, epoch: 6.8783[0m
[32m[2022-09-01 14:26:25,520] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 14:26:25,520] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-01 14:26:25,520] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 14:26:25,520] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 14:26:25,520] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-09-01 14:26:37,214] [    INFO][0m - eval_loss: 3.4531407356262207, eval_accuracy: 0.5360524399126001, eval_runtime: 11.6929, eval_samples_per_second: 117.421, eval_steps_per_second: 3.677, epoch: 6.8783[0m
[32m[2022-09-01 14:26:37,215] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2600[0m
[32m[2022-09-01 14:26:37,215] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 14:26:39,255] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2600/tokenizer_config.json[0m
[32m[2022-09-01 14:26:39,255] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2600/special_tokens_map.json[0m
[32m[2022-09-01 14:26:44,889] [    INFO][0m - loss: 0.03351396, learning_rate: 2.792857142857143e-05, global_step: 2610, interval_runtime: 19.3663, interval_samples_per_second: 0.413, interval_steps_per_second: 0.516, epoch: 6.9048[0m
[32m[2022-09-01 14:26:46,430] [    INFO][0m - loss: 0.18684123, learning_rate: 2.792063492063492e-05, global_step: 2620, interval_runtime: 1.5444, interval_samples_per_second: 5.18, interval_steps_per_second: 6.475, epoch: 6.9312[0m
[32m[2022-09-01 14:26:47,977] [    INFO][0m - loss: 0.09500635, learning_rate: 2.7912698412698412e-05, global_step: 2630, interval_runtime: 1.5466, interval_samples_per_second: 5.173, interval_steps_per_second: 6.466, epoch: 6.9577[0m
[32m[2022-09-01 14:26:49,517] [    INFO][0m - loss: 0.05809776, learning_rate: 2.7904761904761906e-05, global_step: 2640, interval_runtime: 1.5405, interval_samples_per_second: 5.193, interval_steps_per_second: 6.492, epoch: 6.9841[0m
[32m[2022-09-01 14:26:51,156] [    INFO][0m - loss: 0.0989884, learning_rate: 2.7896825396825397e-05, global_step: 2650, interval_runtime: 1.6384, interval_samples_per_second: 4.883, interval_steps_per_second: 6.103, epoch: 7.0106[0m
[32m[2022-09-01 14:26:52,694] [    INFO][0m - loss: 0.01031967, learning_rate: 2.7888888888888888e-05, global_step: 2660, interval_runtime: 1.5387, interval_samples_per_second: 5.199, interval_steps_per_second: 6.499, epoch: 7.037[0m
[32m[2022-09-01 14:26:54,237] [    INFO][0m - loss: 0.04738423, learning_rate: 2.7880952380952382e-05, global_step: 2670, interval_runtime: 1.5424, interval_samples_per_second: 5.187, interval_steps_per_second: 6.483, epoch: 7.0635[0m
[32m[2022-09-01 14:26:55,789] [    INFO][0m - loss: 0.04475205, learning_rate: 2.7873015873015873e-05, global_step: 2680, interval_runtime: 1.5517, interval_samples_per_second: 5.155, interval_steps_per_second: 6.444, epoch: 7.0899[0m
[32m[2022-09-01 14:26:57,348] [    INFO][0m - loss: 0.03913054, learning_rate: 2.7865079365079363e-05, global_step: 2690, interval_runtime: 1.5594, interval_samples_per_second: 5.13, interval_steps_per_second: 6.413, epoch: 7.1164[0m
[32m[2022-09-01 14:26:58,913] [    INFO][0m - loss: 0.02180586, learning_rate: 2.7857142857142858e-05, global_step: 2700, interval_runtime: 1.5649, interval_samples_per_second: 5.112, interval_steps_per_second: 6.39, epoch: 7.1429[0m
[32m[2022-09-01 14:26:58,914] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 14:26:58,914] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-01 14:26:58,914] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 14:26:58,914] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 14:26:58,915] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-09-01 14:27:10,641] [    INFO][0m - eval_loss: 3.3403871059417725, eval_accuracy: 0.5273124544792426, eval_runtime: 11.7258, eval_samples_per_second: 117.093, eval_steps_per_second: 3.667, epoch: 7.1429[0m
[32m[2022-09-01 14:27:10,642] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2700[0m
[32m[2022-09-01 14:27:10,642] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 14:27:12,211] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2700/tokenizer_config.json[0m
[32m[2022-09-01 14:27:12,211] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2700/special_tokens_map.json[0m
[32m[2022-09-01 14:27:15,891] [    INFO][0m - loss: 0.10413808, learning_rate: 2.784920634920635e-05, global_step: 2710, interval_runtime: 16.9788, interval_samples_per_second: 0.471, interval_steps_per_second: 0.589, epoch: 7.1693[0m
[32m[2022-09-01 14:27:17,438] [    INFO][0m - loss: 0.04410776, learning_rate: 2.784126984126984e-05, global_step: 2720, interval_runtime: 1.546, interval_samples_per_second: 5.175, interval_steps_per_second: 6.468, epoch: 7.1958[0m
[32m[2022-09-01 14:27:18,982] [    INFO][0m - loss: 0.02915067, learning_rate: 2.7833333333333337e-05, global_step: 2730, interval_runtime: 1.5442, interval_samples_per_second: 5.181, interval_steps_per_second: 6.476, epoch: 7.2222[0m
[32m[2022-09-01 14:27:20,527] [    INFO][0m - loss: 0.01481253, learning_rate: 2.7825396825396828e-05, global_step: 2740, interval_runtime: 1.5448, interval_samples_per_second: 5.179, interval_steps_per_second: 6.473, epoch: 7.2487[0m
[32m[2022-09-01 14:27:22,069] [    INFO][0m - loss: 0.01770155, learning_rate: 2.781746031746032e-05, global_step: 2750, interval_runtime: 1.542, interval_samples_per_second: 5.188, interval_steps_per_second: 6.485, epoch: 7.2751[0m
[32m[2022-09-01 14:27:23,614] [    INFO][0m - loss: 0.01374269, learning_rate: 2.7809523809523813e-05, global_step: 2760, interval_runtime: 1.5454, interval_samples_per_second: 5.177, interval_steps_per_second: 6.471, epoch: 7.3016[0m
[32m[2022-09-01 14:27:25,157] [    INFO][0m - loss: 0.14891618, learning_rate: 2.7801587301587303e-05, global_step: 2770, interval_runtime: 1.5433, interval_samples_per_second: 5.184, interval_steps_per_second: 6.48, epoch: 7.328[0m
[32m[2022-09-01 14:27:26,699] [    INFO][0m - loss: 0.14027468, learning_rate: 2.7793650793650794e-05, global_step: 2780, interval_runtime: 1.5414, interval_samples_per_second: 5.19, interval_steps_per_second: 6.487, epoch: 7.3545[0m
[32m[2022-09-01 14:27:28,246] [    INFO][0m - loss: 0.02355487, learning_rate: 2.778571428571429e-05, global_step: 2790, interval_runtime: 1.5473, interval_samples_per_second: 5.17, interval_steps_per_second: 6.463, epoch: 7.381[0m
[32m[2022-09-01 14:27:29,780] [    INFO][0m - loss: 0.02899173, learning_rate: 2.777777777777778e-05, global_step: 2800, interval_runtime: 1.534, interval_samples_per_second: 5.215, interval_steps_per_second: 6.519, epoch: 7.4074[0m
[32m[2022-09-01 14:27:29,781] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 14:27:29,781] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-01 14:27:29,781] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 14:27:29,781] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 14:27:29,781] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-09-01 14:27:41,486] [    INFO][0m - eval_loss: 3.4212682247161865, eval_accuracy: 0.5367807720320467, eval_runtime: 11.7046, eval_samples_per_second: 117.304, eval_steps_per_second: 3.674, epoch: 7.4074[0m
[32m[2022-09-01 14:27:41,487] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2800[0m
[32m[2022-09-01 14:27:41,487] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 14:27:42,687] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2800/tokenizer_config.json[0m
[32m[2022-09-01 14:27:42,687] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2800/special_tokens_map.json[0m
[32m[2022-09-01 14:27:49,393] [    INFO][0m - loss: 0.00700654, learning_rate: 2.776984126984127e-05, global_step: 2810, interval_runtime: 16.7673, interval_samples_per_second: 0.477, interval_steps_per_second: 0.596, epoch: 7.4339[0m
[32m[2022-09-01 14:27:50,935] [    INFO][0m - loss: 0.00762725, learning_rate: 2.7761904761904764e-05, global_step: 2820, interval_runtime: 4.3872, interval_samples_per_second: 1.823, interval_steps_per_second: 2.279, epoch: 7.4603[0m
[32m[2022-09-01 14:27:52,477] [    INFO][0m - loss: 0.06298308, learning_rate: 2.7753968253968255e-05, global_step: 2830, interval_runtime: 1.5425, interval_samples_per_second: 5.186, interval_steps_per_second: 6.483, epoch: 7.4868[0m
[32m[2022-09-01 14:27:54,032] [    INFO][0m - loss: 0.00979209, learning_rate: 2.7746031746031746e-05, global_step: 2840, interval_runtime: 1.5549, interval_samples_per_second: 5.145, interval_steps_per_second: 6.431, epoch: 7.5132[0m
[32m[2022-09-01 14:27:55,585] [    INFO][0m - loss: 0.09810059, learning_rate: 2.773809523809524e-05, global_step: 2850, interval_runtime: 1.5521, interval_samples_per_second: 5.154, interval_steps_per_second: 6.443, epoch: 7.5397[0m
[32m[2022-09-01 14:27:57,141] [    INFO][0m - loss: 0.00658662, learning_rate: 2.773015873015873e-05, global_step: 2860, interval_runtime: 1.5564, interval_samples_per_second: 5.14, interval_steps_per_second: 6.425, epoch: 7.5661[0m
[32m[2022-09-01 14:27:58,686] [    INFO][0m - loss: 0.10155425, learning_rate: 2.772222222222222e-05, global_step: 2870, interval_runtime: 1.545, interval_samples_per_second: 5.178, interval_steps_per_second: 6.473, epoch: 7.5926[0m
[32m[2022-09-01 14:28:00,235] [    INFO][0m - loss: 0.00892702, learning_rate: 2.7714285714285716e-05, global_step: 2880, interval_runtime: 1.549, interval_samples_per_second: 5.165, interval_steps_per_second: 6.456, epoch: 7.619[0m
[32m[2022-09-01 14:28:01,782] [    INFO][0m - loss: 0.04657135, learning_rate: 2.7706349206349206e-05, global_step: 2890, interval_runtime: 1.5467, interval_samples_per_second: 5.172, interval_steps_per_second: 6.465, epoch: 7.6455[0m
[32m[2022-09-01 14:28:03,336] [    INFO][0m - loss: 0.04554854, learning_rate: 2.7698412698412697e-05, global_step: 2900, interval_runtime: 1.5545, interval_samples_per_second: 5.146, interval_steps_per_second: 6.433, epoch: 7.672[0m
[32m[2022-09-01 14:28:03,337] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 14:28:03,337] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-01 14:28:03,337] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 14:28:03,337] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 14:28:03,337] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-09-01 14:28:15,092] [    INFO][0m - eval_loss: 3.4785735607147217, eval_accuracy: 0.5258557902403496, eval_runtime: 11.754, eval_samples_per_second: 116.811, eval_steps_per_second: 3.658, epoch: 7.672[0m
[32m[2022-09-01 14:28:15,092] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2900[0m
[32m[2022-09-01 14:28:15,092] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 14:28:16,128] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2900/tokenizer_config.json[0m
[32m[2022-09-01 14:28:16,128] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2900/special_tokens_map.json[0m
[32m[2022-09-01 14:28:20,094] [    INFO][0m - loss: 0.0103837, learning_rate: 2.769047619047619e-05, global_step: 2910, interval_runtime: 16.7578, interval_samples_per_second: 0.477, interval_steps_per_second: 0.597, epoch: 7.6984[0m
[32m[2022-09-01 14:28:21,643] [    INFO][0m - loss: 0.02494153, learning_rate: 2.7682539682539682e-05, global_step: 2920, interval_runtime: 1.5491, interval_samples_per_second: 5.164, interval_steps_per_second: 6.456, epoch: 7.7249[0m
[32m[2022-09-01 14:28:23,185] [    INFO][0m - loss: 0.06933246, learning_rate: 2.7674603174603173e-05, global_step: 2930, interval_runtime: 1.5424, interval_samples_per_second: 5.187, interval_steps_per_second: 6.483, epoch: 7.7513[0m
[32m[2022-09-01 14:28:24,732] [    INFO][0m - loss: 0.01281151, learning_rate: 2.766666666666667e-05, global_step: 2940, interval_runtime: 1.547, interval_samples_per_second: 5.171, interval_steps_per_second: 6.464, epoch: 7.7778[0m
[32m[2022-09-01 14:28:26,266] [    INFO][0m - loss: 0.11147068, learning_rate: 2.765873015873016e-05, global_step: 2950, interval_runtime: 1.5341, interval_samples_per_second: 5.215, interval_steps_per_second: 6.518, epoch: 7.8042[0m
[32m[2022-09-01 14:28:27,813] [    INFO][0m - loss: 0.15238154, learning_rate: 2.7650793650793652e-05, global_step: 2960, interval_runtime: 1.5463, interval_samples_per_second: 5.174, interval_steps_per_second: 6.467, epoch: 7.8307[0m
[32m[2022-09-01 14:28:29,356] [    INFO][0m - loss: 0.03797024, learning_rate: 2.7642857142857143e-05, global_step: 2970, interval_runtime: 1.5437, interval_samples_per_second: 5.182, interval_steps_per_second: 6.478, epoch: 7.8571[0m
[32m[2022-09-01 14:28:30,913] [    INFO][0m - loss: 0.01521443, learning_rate: 2.7634920634920637e-05, global_step: 2980, interval_runtime: 1.5559, interval_samples_per_second: 5.142, interval_steps_per_second: 6.427, epoch: 7.8836[0m
[32m[2022-09-01 14:28:32,461] [    INFO][0m - loss: 0.07280548, learning_rate: 2.7626984126984128e-05, global_step: 2990, interval_runtime: 1.549, interval_samples_per_second: 5.165, interval_steps_per_second: 6.456, epoch: 7.9101[0m
[32m[2022-09-01 14:28:34,008] [    INFO][0m - loss: 0.06554377, learning_rate: 2.761904761904762e-05, global_step: 3000, interval_runtime: 1.5469, interval_samples_per_second: 5.171, interval_steps_per_second: 6.464, epoch: 7.9365[0m
[32m[2022-09-01 14:28:34,009] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 14:28:34,009] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-01 14:28:34,009] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 14:28:34,010] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 14:28:34,010] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-09-01 14:28:45,665] [    INFO][0m - eval_loss: 3.432293653488159, eval_accuracy: 0.5302257829570284, eval_runtime: 11.6546, eval_samples_per_second: 117.808, eval_steps_per_second: 3.69, epoch: 7.9365[0m
[32m[2022-09-01 14:28:45,665] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-3000[0m
[32m[2022-09-01 14:28:45,665] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 14:28:46,698] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-3000/tokenizer_config.json[0m
[32m[2022-09-01 14:28:46,698] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-3000/special_tokens_map.json[0m
[32m[2022-09-01 14:28:50,278] [    INFO][0m - loss: 0.03118479, learning_rate: 2.7611111111111113e-05, global_step: 3010, interval_runtime: 16.2694, interval_samples_per_second: 0.492, interval_steps_per_second: 0.615, epoch: 7.963[0m
[32m[2022-09-01 14:28:51,826] [    INFO][0m - loss: 0.21746602, learning_rate: 2.7603174603174604e-05, global_step: 3020, interval_runtime: 1.5488, interval_samples_per_second: 5.165, interval_steps_per_second: 6.457, epoch: 7.9894[0m
[32m[2022-09-01 14:28:53,451] [    INFO][0m - loss: 0.14046714, learning_rate: 2.7595238095238094e-05, global_step: 3030, interval_runtime: 1.6243, interval_samples_per_second: 4.925, interval_steps_per_second: 6.157, epoch: 8.0159[0m
[32m[2022-09-01 14:28:55,009] [    INFO][0m - loss: 0.04646471, learning_rate: 2.758730158730159e-05, global_step: 3040, interval_runtime: 1.5576, interval_samples_per_second: 5.136, interval_steps_per_second: 6.42, epoch: 8.0423[0m
[32m[2022-09-01 14:28:56,563] [    INFO][0m - loss: 0.01131221, learning_rate: 2.757936507936508e-05, global_step: 3050, interval_runtime: 1.5546, interval_samples_per_second: 5.146, interval_steps_per_second: 6.432, epoch: 8.0688[0m
[32m[2022-09-01 14:28:58,114] [    INFO][0m - loss: 0.04048145, learning_rate: 2.757142857142857e-05, global_step: 3060, interval_runtime: 1.551, interval_samples_per_second: 5.158, interval_steps_per_second: 6.447, epoch: 8.0952[0m
[32m[2022-09-01 14:28:59,659] [    INFO][0m - loss: 0.03565266, learning_rate: 2.7563492063492064e-05, global_step: 3070, interval_runtime: 1.5454, interval_samples_per_second: 5.177, interval_steps_per_second: 6.471, epoch: 8.1217[0m
[32m[2022-09-01 14:29:01,205] [    INFO][0m - loss: 0.0011977, learning_rate: 2.7555555555555555e-05, global_step: 3080, interval_runtime: 1.5459, interval_samples_per_second: 5.175, interval_steps_per_second: 6.469, epoch: 8.1481[0m
[32m[2022-09-01 14:29:02,755] [    INFO][0m - loss: 0.00113265, learning_rate: 2.7547619047619046e-05, global_step: 3090, interval_runtime: 1.5493, interval_samples_per_second: 5.164, interval_steps_per_second: 6.455, epoch: 8.1746[0m
[32m[2022-09-01 14:29:04,300] [    INFO][0m - loss: 0.02332515, learning_rate: 2.753968253968254e-05, global_step: 3100, interval_runtime: 1.5449, interval_samples_per_second: 5.178, interval_steps_per_second: 6.473, epoch: 8.2011[0m
[32m[2022-09-01 14:29:04,300] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 14:29:04,300] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-01 14:29:04,301] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 14:29:04,301] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 14:29:04,301] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-09-01 14:29:16,042] [    INFO][0m - eval_loss: 3.470651149749756, eval_accuracy: 0.5491624180626365, eval_runtime: 11.7409, eval_samples_per_second: 116.941, eval_steps_per_second: 3.662, epoch: 8.2011[0m
[32m[2022-09-01 14:29:16,043] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-3100[0m
[32m[2022-09-01 14:29:16,043] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 14:29:17,075] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-3100/tokenizer_config.json[0m
[32m[2022-09-01 14:29:17,075] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-3100/special_tokens_map.json[0m
[32m[2022-09-01 14:29:19,221] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-01 14:29:19,221] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-1100 (score: 0.5557174071376547).[0m
[32m[2022-09-01 14:29:19,898] [    INFO][0m - train_runtime: 1048.356, train_samples_per_second: 288.452, train_steps_per_second: 36.056, train_loss: 0.6383821073943569, epoch: 8.2011[0m
[32m[2022-09-01 14:29:19,899] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-09-01 14:29:19,899] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 14:29:24,602] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-09-01 14:29:24,603] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-09-01 14:29:24,605] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-01 14:29:24,605] [    INFO][0m -   epoch                    =     8.2011[0m
[32m[2022-09-01 14:29:24,605] [    INFO][0m -   train_loss               =     0.6384[0m
[32m[2022-09-01 14:29:24,605] [    INFO][0m -   train_runtime            = 0:17:28.35[0m
[32m[2022-09-01 14:29:24,605] [    INFO][0m -   train_samples_per_second =    288.452[0m
[32m[2022-09-01 14:29:24,605] [    INFO][0m -   train_steps_per_second   =     36.056[0m
[32m[2022-09-01 14:29:24,619] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-01 14:29:24,619] [    INFO][0m -   Num examples = 1749[0m
[32m[2022-09-01 14:29:24,619] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 14:29:24,619] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 14:29:24,619] [    INFO][0m -   Total prediction steps = 55[0m
[32m[2022-09-01 14:29:39,501] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-01 14:29:39,502] [    INFO][0m -   test_accuracy           =     0.5437[0m
[32m[2022-09-01 14:29:39,502] [    INFO][0m -   test_loss               =     2.1808[0m
[32m[2022-09-01 14:29:39,502] [    INFO][0m -   test_runtime            = 0:00:14.88[0m
[32m[2022-09-01 14:29:39,502] [    INFO][0m -   test_samples_per_second =    117.518[0m
[32m[2022-09-01 14:29:39,503] [    INFO][0m -   test_steps_per_second   =      3.696[0m
[32m[2022-09-01 14:29:39,503] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-01 14:29:39,503] [    INFO][0m -   Num examples = 2600[0m
[32m[2022-09-01 14:29:39,503] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 14:29:39,503] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 14:29:39,503] [    INFO][0m -   Total prediction steps = 82[0m
[32m[2022-09-01 14:30:08,236] [    INFO][0m - Predictions for iflytekf saved to ./fewclue_submit_examples.[0m
run.sh: line 65: --model_name_or_path: command not found
