 
==========
eprstmt
==========
 
[32m[2022-09-19 20:32:03,396] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-19 20:32:03,396] [    INFO][0m - ============================================================[0m
[32m[2022-09-19 20:32:03,396] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-19 20:32:03,396] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-19 20:32:03,396] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-19 20:32:03,396] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-19 20:32:03,396] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-19 20:32:03,396] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-19 20:32:03,397] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-19 20:32:03,397] [    INFO][0m - [0m
[32m[2022-09-19 20:32:03,397] [    INFO][0m - ============================================================[0m
[32m[2022-09-19 20:32:03,397] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-19 20:32:03,397] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-19 20:32:03,397] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-19 20:32:03,397] [    INFO][0m - pretrained                    :/ssd2/wanghuijuan03/data/zero-shot/checkpoints_09191451/checkpoint-21000/model_state.pdparams[0m
[32m[2022-09-19 20:32:03,397] [    INFO][0m - prompt                        :‚Äú{'text':'text_a'}‚ÄùËøôÊù°ËØÑËÆ∫ÁöÑÊÉÖÊÑüÂÄæÂêëÊòØ{'mask'}{'mask'}ÁöÑ„ÄÇ[0m
[32m[2022-09-19 20:32:03,397] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-19 20:32:03,397] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-19 20:32:03,397] [    INFO][0m - task_name                     :eprstmt[0m
[32m[2022-09-19 20:32:03,397] [    INFO][0m - [0m
[32m[2022-09-19 20:32:03,398] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0919 20:32:03.399600 25880 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0919 20:32:03.403649 25880 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-19 20:32:08,031] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-19 20:32:08,047] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-19 20:32:08,048] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-19 20:32:08,049] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùËøôÊù°ËØÑËÆ∫ÁöÑÊÉÖÊÑüÂÄæÂêëÊòØ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'ÁöÑ„ÄÇ'}][0m
[32m[2022-09-19 20:32:09,937] [    INFO][0m - ============================================================[0m
[32m[2022-09-19 20:32:09,937] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-19 20:32:09,937] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-19 20:32:09,938] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-19 20:32:09,938] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-19 20:32:09,938] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-19 20:32:09,938] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-19 20:32:09,938] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-19 20:32:09,938] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-19 20:32:09,938] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-19 20:32:09,938] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-19 20:32:09,938] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-19 20:32:09,938] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-19 20:32:09,939] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-19 20:32:09,939] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-19 20:32:09,939] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-19 20:32:09,939] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-19 20:32:09,939] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-19 20:32:09,939] [    INFO][0m - eval_batch_size               :16[0m
[32m[2022-09-19 20:32:09,939] [    INFO][0m - eval_steps                    :200[0m
[32m[2022-09-19 20:32:09,939] [    INFO][0m - evaluation_strategy           :IntervalStrategy.EPOCH[0m
[32m[2022-09-19 20:32:09,939] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-19 20:32:09,939] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-19 20:32:09,939] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-19 20:32:09,939] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-19 20:32:09,939] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-19 20:32:09,940] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-19 20:32:09,940] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-19 20:32:09,940] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-19 20:32:09,940] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-19 20:32:09,940] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-09-19 20:32:09,940] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-19 20:32:09,940] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-19 20:32:09,940] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-19 20:32:09,940] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-19 20:32:09,940] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-19 20:32:09,940] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-19 20:32:09,940] [    INFO][0m - logging_dir                   :./checkpoints_eprstmt/runs/Sep19_20-32-03_instance-3bwob41y-01[0m
[32m[2022-09-19 20:32:09,940] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-19 20:32:09,940] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-19 20:32:09,941] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-19 20:32:09,941] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-19 20:32:09,941] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-19 20:32:09,941] [    INFO][0m - max_seq_length                :128[0m
[32m[2022-09-19 20:32:09,941] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-19 20:32:09,941] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-19 20:32:09,941] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-19 20:32:09,941] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-19 20:32:09,941] [    INFO][0m - num_train_epochs              :20.0[0m
[32m[2022-09-19 20:32:09,941] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-19 20:32:09,941] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-19 20:32:09,941] [    INFO][0m - output_dir                    :./checkpoints_eprstmt/[0m
[32m[2022-09-19 20:32:09,941] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-19 20:32:09,942] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-19 20:32:09,942] [    INFO][0m - per_device_eval_batch_size    :16[0m
[32m[2022-09-19 20:32:09,942] [    INFO][0m - per_device_train_batch_size   :16[0m
[32m[2022-09-19 20:32:09,942] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-19 20:32:09,942] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-19 20:32:09,942] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-19 20:32:09,942] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-09-19 20:32:09,942] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-19 20:32:09,942] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-19 20:32:09,942] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-19 20:32:09,942] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-19 20:32:09,942] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-19 20:32:09,942] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-19 20:32:09,943] [    INFO][0m - run_name                      :./checkpoints_eprstmt/[0m
[32m[2022-09-19 20:32:09,943] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-19 20:32:09,943] [    INFO][0m - save_steps                    :200[0m
[32m[2022-09-19 20:32:09,943] [    INFO][0m - save_strategy                 :IntervalStrategy.EPOCH[0m
[32m[2022-09-19 20:32:09,943] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-19 20:32:09,943] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-19 20:32:09,943] [    INFO][0m - seed                          :42[0m
[32m[2022-09-19 20:32:09,943] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-19 20:32:09,943] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-19 20:32:09,943] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-19 20:32:09,943] [    INFO][0m - train_batch_size              :16[0m
[32m[2022-09-19 20:32:09,943] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-19 20:32:09,943] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-19 20:32:09,943] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-19 20:32:09,944] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-19 20:32:09,944] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-19 20:32:09,944] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-19 20:32:09,944] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-19 20:32:09,944] [    INFO][0m - [0m
[32m[2022-09-19 20:32:09,948] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-19 20:32:09,948] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 20:32:09,948] [    INFO][0m -   Num Epochs = 20[0m
[32m[2022-09-19 20:32:09,948] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2022-09-19 20:32:09,948] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2022-09-19 20:32:09,948] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-19 20:32:09,948] [    INFO][0m -   Total optimization steps = 200.0[0m
[32m[2022-09-19 20:32:09,948] [    INFO][0m -   Total num train samples = 3200[0m
[32m[2022-09-19 20:32:14,546] [    INFO][0m - loss: 1.57420464, learning_rate: 2.8499999999999998e-05, global_step: 10, interval_runtime: 4.5967, interval_samples_per_second: 3.481, interval_steps_per_second: 2.175, epoch: 1.0[0m
[32m[2022-09-19 20:32:14,548] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 20:32:14,548] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 20:32:14,548] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 20:32:14,548] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 20:32:14,548] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 20:32:15,897] [    INFO][0m - eval_loss: 1.0075955390930176, eval_accuracy: 0.56875, eval_runtime: 1.348, eval_samples_per_second: 118.693, eval_steps_per_second: 7.418, epoch: 1.0[0m
[32m[2022-09-19 20:32:15,897] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-10[0m
[32m[2022-09-19 20:32:15,897] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 20:32:19,126] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-10/tokenizer_config.json[0m
[32m[2022-09-19 20:32:19,127] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-10/special_tokens_map.json[0m
[32m[2022-09-19 20:32:27,799] [    INFO][0m - loss: 0.42797503, learning_rate: 2.7000000000000002e-05, global_step: 20, interval_runtime: 13.2533, interval_samples_per_second: 1.207, interval_steps_per_second: 0.755, epoch: 2.0[0m
[32m[2022-09-19 20:32:27,800] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 20:32:27,801] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 20:32:27,801] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 20:32:27,801] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 20:32:27,801] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 20:32:29,127] [    INFO][0m - eval_loss: 0.4477425515651703, eval_accuracy: 0.8875, eval_runtime: 1.326, eval_samples_per_second: 120.659, eval_steps_per_second: 7.541, epoch: 2.0[0m
[32m[2022-09-19 20:32:29,127] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-20[0m
[32m[2022-09-19 20:32:29,128] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 20:32:32,007] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-20/tokenizer_config.json[0m
[32m[2022-09-19 20:32:32,007] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-20/special_tokens_map.json[0m
[32m[2022-09-19 20:32:40,429] [    INFO][0m - loss: 0.14672201, learning_rate: 2.55e-05, global_step: 30, interval_runtime: 12.6294, interval_samples_per_second: 1.267, interval_steps_per_second: 0.792, epoch: 3.0[0m
[32m[2022-09-19 20:32:40,429] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 20:32:40,430] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 20:32:40,430] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 20:32:40,430] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 20:32:40,430] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 20:32:41,765] [    INFO][0m - eval_loss: 0.5635570287704468, eval_accuracy: 0.875, eval_runtime: 1.3352, eval_samples_per_second: 119.832, eval_steps_per_second: 7.489, epoch: 3.0[0m
[32m[2022-09-19 20:32:41,766] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-30[0m
[32m[2022-09-19 20:32:41,766] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 20:32:44,430] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-30/tokenizer_config.json[0m
[32m[2022-09-19 20:32:44,431] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-30/special_tokens_map.json[0m
[32m[2022-09-19 20:32:53,615] [    INFO][0m - loss: 0.02901495, learning_rate: 2.4e-05, global_step: 40, interval_runtime: 13.186, interval_samples_per_second: 1.213, interval_steps_per_second: 0.758, epoch: 4.0[0m
[32m[2022-09-19 20:32:53,616] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 20:32:53,616] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 20:32:53,616] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 20:32:53,616] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 20:32:53,616] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 20:32:54,960] [    INFO][0m - eval_loss: 0.6658926606178284, eval_accuracy: 0.85625, eval_runtime: 1.3442, eval_samples_per_second: 119.028, eval_steps_per_second: 7.439, epoch: 4.0[0m
[32m[2022-09-19 20:32:54,961] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-40[0m
[32m[2022-09-19 20:32:54,961] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 20:32:58,738] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-40/tokenizer_config.json[0m
[32m[2022-09-19 20:32:58,739] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-40/special_tokens_map.json[0m
[32m[2022-09-19 20:33:07,214] [    INFO][0m - loss: 0.00025402, learning_rate: 2.25e-05, global_step: 50, interval_runtime: 13.5997, interval_samples_per_second: 1.176, interval_steps_per_second: 0.735, epoch: 5.0[0m
[32m[2022-09-19 20:33:07,215] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 20:33:07,215] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 20:33:07,215] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 20:33:07,215] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 20:33:07,215] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 20:33:08,539] [    INFO][0m - eval_loss: 0.9789627194404602, eval_accuracy: 0.86875, eval_runtime: 1.3227, eval_samples_per_second: 120.961, eval_steps_per_second: 7.56, epoch: 5.0[0m
[32m[2022-09-19 20:33:08,732] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-50[0m
[32m[2022-09-19 20:33:08,732] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 20:33:11,335] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-50/tokenizer_config.json[0m
[32m[2022-09-19 20:33:11,335] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-50/special_tokens_map.json[0m
[32m[2022-09-19 20:33:19,792] [    INFO][0m - loss: 2.609e-05, learning_rate: 2.1e-05, global_step: 60, interval_runtime: 12.5779, interval_samples_per_second: 1.272, interval_steps_per_second: 0.795, epoch: 6.0[0m
[32m[2022-09-19 20:33:19,793] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 20:33:19,793] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 20:33:19,793] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 20:33:19,793] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 20:33:19,793] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 20:33:21,127] [    INFO][0m - eval_loss: 1.1718125343322754, eval_accuracy: 0.86875, eval_runtime: 1.3338, eval_samples_per_second: 119.957, eval_steps_per_second: 7.497, epoch: 6.0[0m
[32m[2022-09-19 20:33:21,128] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-60[0m
[32m[2022-09-19 20:33:21,128] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 20:33:23,710] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-60/tokenizer_config.json[0m
[32m[2022-09-19 20:33:23,710] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-60/special_tokens_map.json[0m
[32m[2022-09-19 20:33:32,162] [    INFO][0m - loss: 1.352e-05, learning_rate: 1.95e-05, global_step: 70, interval_runtime: 12.3696, interval_samples_per_second: 1.293, interval_steps_per_second: 0.808, epoch: 7.0[0m
[32m[2022-09-19 20:33:32,162] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 20:33:32,162] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 20:33:32,162] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 20:33:32,162] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 20:33:32,163] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 20:33:33,486] [    INFO][0m - eval_loss: 1.2681405544281006, eval_accuracy: 0.86875, eval_runtime: 1.3233, eval_samples_per_second: 120.905, eval_steps_per_second: 7.557, epoch: 7.0[0m
[32m[2022-09-19 20:33:33,486] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-70[0m
[32m[2022-09-19 20:33:33,487] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 20:33:37,166] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-70/tokenizer_config.json[0m
[32m[2022-09-19 20:33:37,166] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-70/special_tokens_map.json[0m
[32m[2022-09-19 20:33:45,499] [    INFO][0m - loss: 6.31e-06, learning_rate: 1.8e-05, global_step: 80, interval_runtime: 13.3373, interval_samples_per_second: 1.2, interval_steps_per_second: 0.75, epoch: 8.0[0m
[32m[2022-09-19 20:33:45,817] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 20:33:45,817] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 20:33:45,817] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 20:33:45,817] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 20:33:45,817] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 20:33:47,129] [    INFO][0m - eval_loss: 1.3155467510223389, eval_accuracy: 0.86875, eval_runtime: 1.3119, eval_samples_per_second: 121.959, eval_steps_per_second: 7.622, epoch: 8.0[0m
[32m[2022-09-19 20:33:47,130] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-80[0m
[32m[2022-09-19 20:33:47,130] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 20:33:49,683] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-80/tokenizer_config.json[0m
[32m[2022-09-19 20:33:49,683] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-80/special_tokens_map.json[0m
[32m[2022-09-19 20:33:58,243] [    INFO][0m - loss: 1.056e-05, learning_rate: 1.65e-05, global_step: 90, interval_runtime: 12.7439, interval_samples_per_second: 1.256, interval_steps_per_second: 0.785, epoch: 9.0[0m
[32m[2022-09-19 20:33:58,244] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 20:33:58,244] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 20:33:58,244] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 20:33:58,244] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 20:33:58,244] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 20:33:59,588] [    INFO][0m - eval_loss: 1.3488099575042725, eval_accuracy: 0.86875, eval_runtime: 1.3438, eval_samples_per_second: 119.068, eval_steps_per_second: 7.442, epoch: 9.0[0m
[32m[2022-09-19 20:33:59,588] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-90[0m
[32m[2022-09-19 20:33:59,588] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 20:34:07,255] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-90/tokenizer_config.json[0m
[32m[2022-09-19 20:34:07,256] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-90/special_tokens_map.json[0m
[32m[2022-09-19 20:34:16,328] [    INFO][0m - loss: 3.05e-06, learning_rate: 1.5e-05, global_step: 100, interval_runtime: 18.0849, interval_samples_per_second: 0.885, interval_steps_per_second: 0.553, epoch: 10.0[0m
[32m[2022-09-19 20:34:16,329] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 20:34:16,329] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 20:34:16,329] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 20:34:16,329] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 20:34:16,329] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 20:34:17,646] [    INFO][0m - eval_loss: 1.3718445301055908, eval_accuracy: 0.86875, eval_runtime: 1.3171, eval_samples_per_second: 121.476, eval_steps_per_second: 7.592, epoch: 10.0[0m
[32m[2022-09-19 20:34:17,647] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-100[0m
[32m[2022-09-19 20:34:17,647] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 20:34:20,565] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-19 20:34:20,566] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-19 20:34:28,806] [    INFO][0m - loss: 3.51e-06, learning_rate: 1.3500000000000001e-05, global_step: 110, interval_runtime: 12.4779, interval_samples_per_second: 1.282, interval_steps_per_second: 0.801, epoch: 11.0[0m
[32m[2022-09-19 20:34:28,806] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 20:34:28,806] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 20:34:28,806] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 20:34:28,806] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 20:34:28,807] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 20:34:30,128] [    INFO][0m - eval_loss: 1.390703558921814, eval_accuracy: 0.86875, eval_runtime: 1.3209, eval_samples_per_second: 121.128, eval_steps_per_second: 7.571, epoch: 11.0[0m
[32m[2022-09-19 20:34:30,128] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-110[0m
[32m[2022-09-19 20:34:30,128] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 20:34:32,705] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-110/tokenizer_config.json[0m
[32m[2022-09-19 20:34:32,705] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-110/special_tokens_map.json[0m
[32m[2022-09-19 20:34:43,096] [    INFO][0m - loss: 3.26e-06, learning_rate: 1.2e-05, global_step: 120, interval_runtime: 14.2907, interval_samples_per_second: 1.12, interval_steps_per_second: 0.7, epoch: 12.0[0m
[32m[2022-09-19 20:34:43,097] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 20:34:43,097] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 20:34:43,097] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 20:34:43,097] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 20:34:43,097] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 20:34:44,426] [    INFO][0m - eval_loss: 1.4084011316299438, eval_accuracy: 0.86875, eval_runtime: 1.3284, eval_samples_per_second: 120.444, eval_steps_per_second: 7.528, epoch: 12.0[0m
[32m[2022-09-19 20:34:44,426] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-120[0m
[32m[2022-09-19 20:34:44,426] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 20:34:47,029] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-120/tokenizer_config.json[0m
[32m[2022-09-19 20:34:47,029] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-120/special_tokens_map.json[0m
[32m[2022-09-19 20:34:55,238] [    INFO][0m - loss: 6.33e-06, learning_rate: 1.05e-05, global_step: 130, interval_runtime: 12.1413, interval_samples_per_second: 1.318, interval_steps_per_second: 0.824, epoch: 13.0[0m
[32m[2022-09-19 20:34:55,239] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 20:34:55,239] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 20:34:55,239] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 20:34:55,239] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 20:34:55,239] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 20:34:56,555] [    INFO][0m - eval_loss: 1.4292309284210205, eval_accuracy: 0.86875, eval_runtime: 1.3164, eval_samples_per_second: 121.541, eval_steps_per_second: 7.596, epoch: 13.0[0m
[32m[2022-09-19 20:34:56,556] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-130[0m
[32m[2022-09-19 20:34:56,556] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 20:34:58,982] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-130/tokenizer_config.json[0m
[32m[2022-09-19 20:34:58,982] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-130/special_tokens_map.json[0m
[32m[2022-09-19 20:35:07,198] [    INFO][0m - loss: 2.18e-06, learning_rate: 9e-06, global_step: 140, interval_runtime: 11.9603, interval_samples_per_second: 1.338, interval_steps_per_second: 0.836, epoch: 14.0[0m
[32m[2022-09-19 20:35:07,199] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 20:35:07,199] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 20:35:07,199] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 20:35:07,199] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 20:35:07,199] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 20:35:08,529] [    INFO][0m - eval_loss: 1.445386290550232, eval_accuracy: 0.86875, eval_runtime: 1.3294, eval_samples_per_second: 120.352, eval_steps_per_second: 7.522, epoch: 14.0[0m
[32m[2022-09-19 20:35:08,529] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-140[0m
[32m[2022-09-19 20:35:08,529] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 20:35:10,985] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-140/tokenizer_config.json[0m
[32m[2022-09-19 20:35:10,986] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-140/special_tokens_map.json[0m
[32m[2022-09-19 20:35:19,211] [    INFO][0m - loss: 3.01e-06, learning_rate: 7.5e-06, global_step: 150, interval_runtime: 12.0127, interval_samples_per_second: 1.332, interval_steps_per_second: 0.832, epoch: 15.0[0m
[32m[2022-09-19 20:35:19,741] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 20:35:19,741] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 20:35:19,741] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 20:35:19,741] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 20:35:19,741] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 20:35:21,072] [    INFO][0m - eval_loss: 1.4579883813858032, eval_accuracy: 0.86875, eval_runtime: 1.3301, eval_samples_per_second: 120.289, eval_steps_per_second: 7.518, epoch: 15.0[0m
[32m[2022-09-19 20:35:21,072] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-150[0m
[32m[2022-09-19 20:35:21,072] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 20:35:23,499] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-150/tokenizer_config.json[0m
[32m[2022-09-19 20:35:23,500] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-150/special_tokens_map.json[0m
[32m[2022-09-19 20:35:31,705] [    INFO][0m - loss: 3.54e-06, learning_rate: 6e-06, global_step: 160, interval_runtime: 12.4943, interval_samples_per_second: 1.281, interval_steps_per_second: 0.8, epoch: 16.0[0m
[32m[2022-09-19 20:35:31,706] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 20:35:31,706] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 20:35:31,706] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 20:35:31,706] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 20:35:31,706] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 20:35:33,039] [    INFO][0m - eval_loss: 1.4671897888183594, eval_accuracy: 0.86875, eval_runtime: 1.3329, eval_samples_per_second: 120.039, eval_steps_per_second: 7.502, epoch: 16.0[0m
[32m[2022-09-19 20:35:33,040] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-160[0m
[32m[2022-09-19 20:35:33,040] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 20:35:35,440] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-160/tokenizer_config.json[0m
[32m[2022-09-19 20:35:35,440] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-160/special_tokens_map.json[0m
[32m[2022-09-19 20:35:43,704] [    INFO][0m - loss: 2.56e-06, learning_rate: 4.5e-06, global_step: 170, interval_runtime: 11.9771, interval_samples_per_second: 1.336, interval_steps_per_second: 0.835, epoch: 17.0[0m
[32m[2022-09-19 20:35:43,705] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 20:35:43,705] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 20:35:43,705] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 20:35:43,705] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 20:35:43,705] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 20:35:45,035] [    INFO][0m - eval_loss: 1.47398042678833, eval_accuracy: 0.86875, eval_runtime: 1.33, eval_samples_per_second: 120.302, eval_steps_per_second: 7.519, epoch: 17.0[0m
[32m[2022-09-19 20:35:45,081] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-170[0m
[32m[2022-09-19 20:35:45,081] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 20:35:47,536] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-170/tokenizer_config.json[0m
[32m[2022-09-19 20:35:47,536] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-170/special_tokens_map.json[0m
[32m[2022-09-19 20:35:55,746] [    INFO][0m - loss: 2.32e-06, learning_rate: 3e-06, global_step: 180, interval_runtime: 12.064, interval_samples_per_second: 1.326, interval_steps_per_second: 0.829, epoch: 18.0[0m
[32m[2022-09-19 20:35:55,747] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 20:35:55,747] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 20:35:55,747] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 20:35:55,747] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 20:35:55,747] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 20:35:57,078] [    INFO][0m - eval_loss: 1.4783090353012085, eval_accuracy: 0.86875, eval_runtime: 1.3302, eval_samples_per_second: 120.281, eval_steps_per_second: 7.518, epoch: 18.0[0m
[32m[2022-09-19 20:35:57,078] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-180[0m
[32m[2022-09-19 20:35:57,078] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 20:36:00,052] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-180/tokenizer_config.json[0m
[32m[2022-09-19 20:36:00,053] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-180/special_tokens_map.json[0m
[32m[2022-09-19 20:36:08,860] [    INFO][0m - loss: 2.47e-06, learning_rate: 1.5e-06, global_step: 190, interval_runtime: 13.1136, interval_samples_per_second: 1.22, interval_steps_per_second: 0.763, epoch: 19.0[0m
[32m[2022-09-19 20:36:08,861] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 20:36:08,861] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 20:36:08,861] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 20:36:08,861] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 20:36:08,861] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 20:36:10,175] [    INFO][0m - eval_loss: 1.4811686277389526, eval_accuracy: 0.86875, eval_runtime: 1.3142, eval_samples_per_second: 121.745, eval_steps_per_second: 7.609, epoch: 19.0[0m
[32m[2022-09-19 20:36:10,176] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-190[0m
[32m[2022-09-19 20:36:10,176] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 20:36:13,271] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-190/tokenizer_config.json[0m
[32m[2022-09-19 20:36:13,272] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-190/special_tokens_map.json[0m
[32m[2022-09-19 20:36:21,446] [    INFO][0m - loss: 6.17e-06, learning_rate: 0.0, global_step: 200, interval_runtime: 12.586, interval_samples_per_second: 1.271, interval_steps_per_second: 0.795, epoch: 20.0[0m
[32m[2022-09-19 20:36:21,446] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 20:36:21,447] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 20:36:21,447] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 20:36:21,447] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 20:36:21,447] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 20:36:22,818] [    INFO][0m - eval_loss: 1.4822967052459717, eval_accuracy: 0.86875, eval_runtime: 1.3715, eval_samples_per_second: 116.663, eval_steps_per_second: 7.291, epoch: 20.0[0m
[32m[2022-09-19 20:36:23,793] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-200[0m
[32m[2022-09-19 20:36:23,794] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 20:36:26,257] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-200/tokenizer_config.json[0m
[32m[2022-09-19 20:36:26,257] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-200/special_tokens_map.json[0m
[32m[2022-09-19 20:36:31,077] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-19 20:36:31,077] [    INFO][0m - Loading best model from ./checkpoints_eprstmt/checkpoint-20 (score: 0.8875).[0m
[32m[2022-09-19 20:36:32,746] [    INFO][0m - train_runtime: 262.7969, train_samples_per_second: 12.177, train_steps_per_second: 0.761, train_loss: 0.10891327560701938, epoch: 20.0[0m
[32m[2022-09-19 20:36:32,801] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/[0m
[32m[2022-09-19 20:36:32,801] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 20:36:35,218] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/tokenizer_config.json[0m
[32m[2022-09-19 20:36:35,219] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/special_tokens_map.json[0m
[32m[2022-09-19 20:36:35,219] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-19 20:36:35,220] [    INFO][0m -   epoch                    =       20.0[0m
[32m[2022-09-19 20:36:35,220] [    INFO][0m -   train_loss               =     0.1089[0m
[32m[2022-09-19 20:36:35,220] [    INFO][0m -   train_runtime            = 0:04:22.79[0m
[32m[2022-09-19 20:36:35,220] [    INFO][0m -   train_samples_per_second =     12.177[0m
[32m[2022-09-19 20:36:35,220] [    INFO][0m -   train_steps_per_second   =      0.761[0m
[32m[2022-09-19 20:36:35,222] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-19 20:36:35,222] [    INFO][0m -   Num examples = 610[0m
[32m[2022-09-19 20:36:35,222] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 20:36:35,222] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 20:36:35,222] [    INFO][0m -   Total prediction steps = 39[0m
[32m[2022-09-19 20:36:40,509] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-19 20:36:40,510] [    INFO][0m -   test_accuracy           =     0.8967[0m
[32m[2022-09-19 20:36:40,510] [    INFO][0m -   test_loss               =     0.4021[0m
[32m[2022-09-19 20:36:40,510] [    INFO][0m -   test_runtime            = 0:00:05.28[0m
[32m[2022-09-19 20:36:40,510] [    INFO][0m -   test_samples_per_second =    115.372[0m
[32m[2022-09-19 20:36:40,510] [    INFO][0m -   test_steps_per_second   =      7.376[0m
[32m[2022-09-19 20:36:40,510] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-19 20:36:40,510] [    INFO][0m -   Num examples = 753[0m
[32m[2022-09-19 20:36:40,511] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 20:36:40,511] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 20:36:40,511] [    INFO][0m -   Total prediction steps = 48[0m
[32m[2022-09-19 20:36:47,690] [    INFO][0m - Predictions for eprstmt saved to ./fewclue_submit_examples.[0m
{
  "labels": 0,
  "text_a": "\u7269\u6d41\u5f88\u5feb\uff0c\u65e9\u4e0a\u4e0b\u5355\uff0c\u4e0b\u5348\u5c31\u5230\u4e86\u3002\u5305\u88c5\u4e5f\u5f88\u9ad8\u6863\u3002\u5c31\u662f\u8033\u673a\u97f3\u8d28\u5f88\u5dee\uff0c\u7172\u4e86\u4e00\u767e\u591a\u5c0f\u65f6\uff0c\u97f3\u8d28\u548c\u540c\u4e8b\u7684\u4e00\u767e\u591a\u5143\u7684\u8033\u673a\u5dee\u4e0d\u591a\uff0c1580\u5143\u4e70\u8fd9\u8033\u673a\u4e8f\u5927\u4e86\u3002",
  "text_b": "",
  "uid": 0
}

Prediction done.
 
==========
csldcp
==========
 
[32m[2022-09-19 20:37:18,681] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-19 20:37:18,681] [    INFO][0m - ============================================================[0m
[32m[2022-09-19 20:37:18,681] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-19 20:37:18,681] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-19 20:37:18,681] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-19 20:37:18,681] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-19 20:37:18,682] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-19 20:37:18,682] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-19 20:37:18,682] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-19 20:37:18,682] [    INFO][0m - [0m
[32m[2022-09-19 20:37:18,682] [    INFO][0m - ============================================================[0m
[32m[2022-09-19 20:37:18,682] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-19 20:37:18,682] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-19 20:37:18,682] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-19 20:37:18,682] [    INFO][0m - pretrained                    :/ssd2/wanghuijuan03/data/zero-shot/checkpoints_09191451/checkpoint-21000/model_state.pdparams[0m
[32m[2022-09-19 20:37:18,682] [    INFO][0m - prompt                        :‚Äú{'text':'text_a'}‚ÄùËøôÁØáÊñáÁåÆÁöÑÁ±ªÂà´ÊòØ{'mask'}{'mask'}„ÄÇ[0m
[32m[2022-09-19 20:37:18,682] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-19 20:37:18,682] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-19 20:37:18,682] [    INFO][0m - task_name                     :csldcp[0m
[32m[2022-09-19 20:37:18,683] [    INFO][0m - [0m
[32m[2022-09-19 20:37:18,683] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0919 20:37:18.684353 33472 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0919 20:37:18.688431 33472 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-19 20:37:23,449] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-19 20:37:23,461] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-19 20:37:23,461] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-19 20:37:23,462] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùËøôÁØáÊñáÁåÆÁöÑÁ±ªÂà´ÊòØ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': '„ÄÇ'}][0m
[32m[2022-09-19 20:37:24,785] [    INFO][0m - ============================================================[0m
[32m[2022-09-19 20:37:24,785] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-19 20:37:24,785] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-19 20:37:24,785] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-19 20:37:24,785] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-19 20:37:24,785] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-19 20:37:24,785] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-19 20:37:24,786] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-19 20:37:24,786] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-19 20:37:24,786] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-19 20:37:24,786] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-19 20:37:24,786] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-19 20:37:24,786] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-19 20:37:24,786] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-19 20:37:24,786] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-19 20:37:24,786] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-19 20:37:24,786] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-19 20:37:24,786] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-19 20:37:24,786] [    INFO][0m - eval_batch_size               :16[0m
[32m[2022-09-19 20:37:24,786] [    INFO][0m - eval_steps                    :200[0m
[32m[2022-09-19 20:37:24,787] [    INFO][0m - evaluation_strategy           :IntervalStrategy.EPOCH[0m
[32m[2022-09-19 20:37:24,787] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-19 20:37:24,787] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-19 20:37:24,787] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-19 20:37:24,787] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-19 20:37:24,787] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-19 20:37:24,787] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-19 20:37:24,787] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-19 20:37:24,787] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-19 20:37:24,787] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-19 20:37:24,787] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-09-19 20:37:24,787] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-19 20:37:24,787] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-19 20:37:24,787] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-19 20:37:24,788] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-19 20:37:24,788] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-19 20:37:24,788] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-19 20:37:24,788] [    INFO][0m - logging_dir                   :./checkpoints_csldcp/runs/Sep19_20-37-18_instance-3bwob41y-01[0m
[32m[2022-09-19 20:37:24,788] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-19 20:37:24,788] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-19 20:37:24,788] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-19 20:37:24,788] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-19 20:37:24,788] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-19 20:37:24,788] [    INFO][0m - max_seq_length                :256[0m
[32m[2022-09-19 20:37:24,788] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-19 20:37:24,788] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-19 20:37:24,788] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-19 20:37:24,789] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-19 20:37:24,789] [    INFO][0m - num_train_epochs              :20.0[0m
[32m[2022-09-19 20:37:24,789] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-19 20:37:24,789] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-19 20:37:24,789] [    INFO][0m - output_dir                    :./checkpoints_csldcp/[0m
[32m[2022-09-19 20:37:24,789] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-19 20:37:24,789] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-19 20:37:24,789] [    INFO][0m - per_device_eval_batch_size    :16[0m
[32m[2022-09-19 20:37:24,789] [    INFO][0m - per_device_train_batch_size   :16[0m
[32m[2022-09-19 20:37:24,789] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-19 20:37:24,789] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-19 20:37:24,789] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-19 20:37:24,789] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-09-19 20:37:24,790] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-19 20:37:24,790] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-19 20:37:24,790] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-19 20:37:24,790] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-19 20:37:24,790] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-19 20:37:24,790] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-19 20:37:24,790] [    INFO][0m - run_name                      :./checkpoints_csldcp/[0m
[32m[2022-09-19 20:37:24,790] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-19 20:37:24,790] [    INFO][0m - save_steps                    :200[0m
[32m[2022-09-19 20:37:24,790] [    INFO][0m - save_strategy                 :IntervalStrategy.EPOCH[0m
[32m[2022-09-19 20:37:24,790] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-19 20:37:24,790] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-19 20:37:24,790] [    INFO][0m - seed                          :42[0m
[32m[2022-09-19 20:37:24,790] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-19 20:37:24,791] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-19 20:37:24,791] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-19 20:37:24,791] [    INFO][0m - train_batch_size              :16[0m
[32m[2022-09-19 20:37:24,791] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-19 20:37:24,791] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-19 20:37:24,791] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-19 20:37:24,791] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-19 20:37:24,791] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-19 20:37:24,791] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-19 20:37:24,791] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-19 20:37:24,791] [    INFO][0m - [0m
[32m[2022-09-19 20:37:24,794] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-19 20:37:24,794] [    INFO][0m -   Num examples = 2036[0m
[32m[2022-09-19 20:37:24,795] [    INFO][0m -   Num Epochs = 20[0m
[32m[2022-09-19 20:37:24,795] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2022-09-19 20:37:24,795] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2022-09-19 20:37:24,795] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-19 20:37:24,795] [    INFO][0m -   Total optimization steps = 2560.0[0m
[32m[2022-09-19 20:37:24,795] [    INFO][0m -   Total num train samples = 40720[0m
[33m[2022-09-19 20:37:24,829] [ WARNING][0m - Token indices sequence length is longer than the specified maximum sequence length for this model (517 > 512). Running this sequence through the model will result in indexing errors[0m
[32m[2022-09-19 20:37:32,247] [    INFO][0m - loss: 2.88834515, learning_rate: 2.9882812500000002e-05, global_step: 10, interval_runtime: 7.4505, interval_samples_per_second: 2.148, interval_steps_per_second: 1.342, epoch: 0.0781[0m
[32m[2022-09-19 20:37:38,691] [    INFO][0m - loss: 2.36543179, learning_rate: 2.9765625e-05, global_step: 20, interval_runtime: 6.4448, interval_samples_per_second: 2.483, interval_steps_per_second: 1.552, epoch: 0.1562[0m
[32m[2022-09-19 20:37:45,130] [    INFO][0m - loss: 2.51796207, learning_rate: 2.96484375e-05, global_step: 30, interval_runtime: 6.4383, interval_samples_per_second: 2.485, interval_steps_per_second: 1.553, epoch: 0.2344[0m
[32m[2022-09-19 20:37:51,630] [    INFO][0m - loss: 3.88783035, learning_rate: 2.953125e-05, global_step: 40, interval_runtime: 6.5009, interval_samples_per_second: 2.461, interval_steps_per_second: 1.538, epoch: 0.3125[0m
[32m[2022-09-19 20:37:58,102] [    INFO][0m - loss: 3.06612434, learning_rate: 2.94140625e-05, global_step: 50, interval_runtime: 6.4716, interval_samples_per_second: 2.472, interval_steps_per_second: 1.545, epoch: 0.3906[0m
[32m[2022-09-19 20:38:04,578] [    INFO][0m - loss: 2.74858246, learning_rate: 2.9296875000000002e-05, global_step: 60, interval_runtime: 6.4756, interval_samples_per_second: 2.471, interval_steps_per_second: 1.544, epoch: 0.4688[0m
[32m[2022-09-19 20:38:11,058] [    INFO][0m - loss: 2.64671841, learning_rate: 2.91796875e-05, global_step: 70, interval_runtime: 6.48, interval_samples_per_second: 2.469, interval_steps_per_second: 1.543, epoch: 0.5469[0m
[32m[2022-09-19 20:38:17,541] [    INFO][0m - loss: 2.57924976, learning_rate: 2.90625e-05, global_step: 80, interval_runtime: 6.4836, interval_samples_per_second: 2.468, interval_steps_per_second: 1.542, epoch: 0.625[0m
[32m[2022-09-19 20:38:23,989] [    INFO][0m - loss: 2.29765587, learning_rate: 2.89453125e-05, global_step: 90, interval_runtime: 6.4474, interval_samples_per_second: 2.482, interval_steps_per_second: 1.551, epoch: 0.7031[0m
[32m[2022-09-19 20:38:30,466] [    INFO][0m - loss: 2.52749367, learning_rate: 2.8828125e-05, global_step: 100, interval_runtime: 6.4772, interval_samples_per_second: 2.47, interval_steps_per_second: 1.544, epoch: 0.7812[0m
[32m[2022-09-19 20:38:36,952] [    INFO][0m - loss: 2.06621475, learning_rate: 2.87109375e-05, global_step: 110, interval_runtime: 6.486, interval_samples_per_second: 2.467, interval_steps_per_second: 1.542, epoch: 0.8594[0m
[32m[2022-09-19 20:38:43,441] [    INFO][0m - loss: 1.96182594, learning_rate: 2.859375e-05, global_step: 120, interval_runtime: 6.4894, interval_samples_per_second: 2.466, interval_steps_per_second: 1.541, epoch: 0.9375[0m
[32m[2022-09-19 20:38:48,048] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 20:38:48,048] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-19 20:38:48,048] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 20:38:48,048] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 20:38:48,048] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-19 20:39:21,792] [    INFO][0m - eval_loss: 1.8060353994369507, eval_accuracy: 0.4927466150870406, eval_runtime: 33.7438, eval_samples_per_second: 61.285, eval_steps_per_second: 3.853, epoch: 1.0[0m
[32m[2022-09-19 20:39:21,839] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-128[0m
[32m[2022-09-19 20:39:21,839] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 20:39:25,496] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-128/tokenizer_config.json[0m
[32m[2022-09-19 20:39:25,496] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-128/special_tokens_map.json[0m
[32m[2022-09-19 20:39:32,966] [    INFO][0m - loss: 1.79559021, learning_rate: 2.8476562500000002e-05, global_step: 130, interval_runtime: 49.5246, interval_samples_per_second: 0.323, interval_steps_per_second: 0.202, epoch: 1.0156[0m
[32m[2022-09-19 20:39:39,589] [    INFO][0m - loss: 1.30509853, learning_rate: 2.8359375e-05, global_step: 140, interval_runtime: 6.6229, interval_samples_per_second: 2.416, interval_steps_per_second: 1.51, epoch: 1.0938[0m
[32m[2022-09-19 20:39:46,128] [    INFO][0m - loss: 1.38682575, learning_rate: 2.82421875e-05, global_step: 150, interval_runtime: 6.5384, interval_samples_per_second: 2.447, interval_steps_per_second: 1.529, epoch: 1.1719[0m
[32m[2022-09-19 20:39:52,644] [    INFO][0m - loss: 1.379249, learning_rate: 2.8125e-05, global_step: 160, interval_runtime: 6.5161, interval_samples_per_second: 2.455, interval_steps_per_second: 1.535, epoch: 1.25[0m
[32m[2022-09-19 20:40:01,238] [    INFO][0m - loss: 1.29835539, learning_rate: 2.80078125e-05, global_step: 170, interval_runtime: 6.5436, interval_samples_per_second: 2.445, interval_steps_per_second: 1.528, epoch: 1.3281[0m
[32m[2022-09-19 20:40:07,763] [    INFO][0m - loss: 1.33217449, learning_rate: 2.7890625000000002e-05, global_step: 180, interval_runtime: 8.5765, interval_samples_per_second: 1.866, interval_steps_per_second: 1.166, epoch: 1.4062[0m
[32m[2022-09-19 20:40:14,276] [    INFO][0m - loss: 1.051478, learning_rate: 2.77734375e-05, global_step: 190, interval_runtime: 6.5127, interval_samples_per_second: 2.457, interval_steps_per_second: 1.535, epoch: 1.4844[0m
[32m[2022-09-19 20:40:20,779] [    INFO][0m - loss: 1.31115561, learning_rate: 2.765625e-05, global_step: 200, interval_runtime: 6.5028, interval_samples_per_second: 2.46, interval_steps_per_second: 1.538, epoch: 1.5625[0m
[32m[2022-09-19 20:40:27,289] [    INFO][0m - loss: 1.19164009, learning_rate: 2.75390625e-05, global_step: 210, interval_runtime: 6.5099, interval_samples_per_second: 2.458, interval_steps_per_second: 1.536, epoch: 1.6406[0m
[32m[2022-09-19 20:40:33,843] [    INFO][0m - loss: 1.15019703, learning_rate: 2.7421875e-05, global_step: 220, interval_runtime: 6.5543, interval_samples_per_second: 2.441, interval_steps_per_second: 1.526, epoch: 1.7188[0m
[32m[2022-09-19 20:40:40,341] [    INFO][0m - loss: 1.06748209, learning_rate: 2.7304687500000002e-05, global_step: 230, interval_runtime: 6.4979, interval_samples_per_second: 2.462, interval_steps_per_second: 1.539, epoch: 1.7969[0m
[32m[2022-09-19 20:40:46,878] [    INFO][0m - loss: 1.22823191, learning_rate: 2.71875e-05, global_step: 240, interval_runtime: 6.5374, interval_samples_per_second: 2.447, interval_steps_per_second: 1.53, epoch: 1.875[0m
[32m[2022-09-19 20:40:53,408] [    INFO][0m - loss: 1.1797533, learning_rate: 2.70703125e-05, global_step: 250, interval_runtime: 6.5294, interval_samples_per_second: 2.45, interval_steps_per_second: 1.532, epoch: 1.9531[0m
[32m[2022-09-19 20:40:56,735] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 20:40:56,735] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-19 20:40:56,735] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 20:40:56,736] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 20:40:56,736] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-19 20:41:30,634] [    INFO][0m - eval_loss: 1.4048486948013306, eval_accuracy: 0.5304642166344294, eval_runtime: 33.898, eval_samples_per_second: 61.007, eval_steps_per_second: 3.835, epoch: 2.0[0m
[32m[2022-09-19 20:41:30,669] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-256[0m
[32m[2022-09-19 20:41:30,669] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 20:41:33,457] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-256/tokenizer_config.json[0m
[32m[2022-09-19 20:41:33,457] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-256/special_tokens_map.json[0m
[32m[2022-09-19 20:41:41,513] [    INFO][0m - loss: 0.83430233, learning_rate: 2.6953125e-05, global_step: 260, interval_runtime: 48.1052, interval_samples_per_second: 0.333, interval_steps_per_second: 0.208, epoch: 2.0312[0m
[32m[2022-09-19 20:41:48,004] [    INFO][0m - loss: 0.67090168, learning_rate: 2.68359375e-05, global_step: 270, interval_runtime: 6.491, interval_samples_per_second: 2.465, interval_steps_per_second: 1.541, epoch: 2.1094[0m
[32m[2022-09-19 20:41:54,517] [    INFO][0m - loss: 0.62722917, learning_rate: 2.6718750000000002e-05, global_step: 280, interval_runtime: 6.5126, interval_samples_per_second: 2.457, interval_steps_per_second: 1.535, epoch: 2.1875[0m
[32m[2022-09-19 20:42:01,059] [    INFO][0m - loss: 0.52486596, learning_rate: 2.66015625e-05, global_step: 290, interval_runtime: 6.5419, interval_samples_per_second: 2.446, interval_steps_per_second: 1.529, epoch: 2.2656[0m
[32m[2022-09-19 20:42:11,976] [    INFO][0m - loss: 0.75467386, learning_rate: 2.6484375000000002e-05, global_step: 300, interval_runtime: 6.5382, interval_samples_per_second: 2.447, interval_steps_per_second: 1.529, epoch: 2.3438[0m
[32m[2022-09-19 20:42:18,455] [    INFO][0m - loss: 0.73618288, learning_rate: 2.63671875e-05, global_step: 310, interval_runtime: 10.8578, interval_samples_per_second: 1.474, interval_steps_per_second: 0.921, epoch: 2.4219[0m
[32m[2022-09-19 20:42:24,960] [    INFO][0m - loss: 0.63058524, learning_rate: 2.625e-05, global_step: 320, interval_runtime: 6.5055, interval_samples_per_second: 2.459, interval_steps_per_second: 1.537, epoch: 2.5[0m
[32m[2022-09-19 20:42:31,456] [    INFO][0m - loss: 0.56451273, learning_rate: 2.61328125e-05, global_step: 330, interval_runtime: 6.4967, interval_samples_per_second: 2.463, interval_steps_per_second: 1.539, epoch: 2.5781[0m
[32m[2022-09-19 20:42:37,963] [    INFO][0m - loss: 0.80774565, learning_rate: 2.6015625e-05, global_step: 340, interval_runtime: 6.5066, interval_samples_per_second: 2.459, interval_steps_per_second: 1.537, epoch: 2.6562[0m
[32m[2022-09-19 20:42:44,499] [    INFO][0m - loss: 0.71217508, learning_rate: 2.5898437500000002e-05, global_step: 350, interval_runtime: 6.5361, interval_samples_per_second: 2.448, interval_steps_per_second: 1.53, epoch: 2.7344[0m
[32m[2022-09-19 20:42:51,082] [    INFO][0m - loss: 0.80792589, learning_rate: 2.578125e-05, global_step: 360, interval_runtime: 6.5829, interval_samples_per_second: 2.431, interval_steps_per_second: 1.519, epoch: 2.8125[0m
[32m[2022-09-19 20:42:57,637] [    INFO][0m - loss: 0.77168603, learning_rate: 2.56640625e-05, global_step: 370, interval_runtime: 6.5546, interval_samples_per_second: 2.441, interval_steps_per_second: 1.526, epoch: 2.8906[0m
[32m[2022-09-19 20:43:04,121] [    INFO][0m - loss: 0.58784971, learning_rate: 2.5546875e-05, global_step: 380, interval_runtime: 6.4845, interval_samples_per_second: 2.467, interval_steps_per_second: 1.542, epoch: 2.9688[0m
[32m[2022-09-19 20:43:06,181] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 20:43:06,181] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-19 20:43:06,181] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 20:43:06,181] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 20:43:06,181] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-19 20:43:39,967] [    INFO][0m - eval_loss: 1.3622838258743286, eval_accuracy: 0.5647969052224371, eval_runtime: 33.7853, eval_samples_per_second: 61.21, eval_steps_per_second: 3.848, epoch: 3.0[0m
[32m[2022-09-19 20:43:40,002] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-384[0m
[32m[2022-09-19 20:43:40,003] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 20:43:43,029] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-384/tokenizer_config.json[0m
[32m[2022-09-19 20:43:43,029] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-384/special_tokens_map.json[0m
[32m[2022-09-19 20:43:52,093] [    INFO][0m - loss: 0.37277319, learning_rate: 2.54296875e-05, global_step: 390, interval_runtime: 47.9717, interval_samples_per_second: 0.334, interval_steps_per_second: 0.208, epoch: 3.0469[0m
[32m[2022-09-19 20:43:58,555] [    INFO][0m - loss: 0.38404095, learning_rate: 2.5312500000000002e-05, global_step: 400, interval_runtime: 6.4618, interval_samples_per_second: 2.476, interval_steps_per_second: 1.548, epoch: 3.125[0m
[32m[2022-09-19 20:44:05,041] [    INFO][0m - loss: 0.29331474, learning_rate: 2.51953125e-05, global_step: 410, interval_runtime: 6.4862, interval_samples_per_second: 2.467, interval_steps_per_second: 1.542, epoch: 3.2031[0m
[32m[2022-09-19 20:44:11,577] [    INFO][0m - loss: 0.23038669, learning_rate: 2.5078125e-05, global_step: 420, interval_runtime: 6.5358, interval_samples_per_second: 2.448, interval_steps_per_second: 1.53, epoch: 3.2812[0m
[32m[2022-09-19 20:44:19,310] [    INFO][0m - loss: 0.3930855, learning_rate: 2.49609375e-05, global_step: 430, interval_runtime: 6.5077, interval_samples_per_second: 2.459, interval_steps_per_second: 1.537, epoch: 3.3594[0m
[32m[2022-09-19 20:44:25,943] [    INFO][0m - loss: 0.43510103, learning_rate: 2.484375e-05, global_step: 440, interval_runtime: 7.8583, interval_samples_per_second: 2.036, interval_steps_per_second: 1.273, epoch: 3.4375[0m
[32m[2022-09-19 20:44:32,448] [    INFO][0m - loss: 0.34708076, learning_rate: 2.4726562500000002e-05, global_step: 450, interval_runtime: 6.5055, interval_samples_per_second: 2.459, interval_steps_per_second: 1.537, epoch: 3.5156[0m
[32m[2022-09-19 20:44:38,959] [    INFO][0m - loss: 0.35094044, learning_rate: 2.4609375e-05, global_step: 460, interval_runtime: 6.5108, interval_samples_per_second: 2.457, interval_steps_per_second: 1.536, epoch: 3.5938[0m
[32m[2022-09-19 20:44:45,514] [    INFO][0m - loss: 0.36439693, learning_rate: 2.44921875e-05, global_step: 470, interval_runtime: 6.5553, interval_samples_per_second: 2.441, interval_steps_per_second: 1.525, epoch: 3.6719[0m
[32m[2022-09-19 20:44:52,049] [    INFO][0m - loss: 0.45040455, learning_rate: 2.4375e-05, global_step: 480, interval_runtime: 6.5346, interval_samples_per_second: 2.448, interval_steps_per_second: 1.53, epoch: 3.75[0m
[32m[2022-09-19 20:44:58,592] [    INFO][0m - loss: 0.35081327, learning_rate: 2.42578125e-05, global_step: 490, interval_runtime: 6.5427, interval_samples_per_second: 2.445, interval_steps_per_second: 1.528, epoch: 3.8281[0m
[32m[2022-09-19 20:45:05,127] [    INFO][0m - loss: 0.38651097, learning_rate: 2.4140625e-05, global_step: 500, interval_runtime: 6.5346, interval_samples_per_second: 2.448, interval_steps_per_second: 1.53, epoch: 3.9062[0m
[32m[2022-09-19 20:45:11,562] [    INFO][0m - loss: 0.38370955, learning_rate: 2.40234375e-05, global_step: 510, interval_runtime: 6.4361, interval_samples_per_second: 2.486, interval_steps_per_second: 1.554, epoch: 3.9844[0m
[32m[2022-09-19 20:45:12,373] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 20:45:12,374] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-19 20:45:12,374] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 20:45:12,374] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 20:45:12,374] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-19 20:45:45,958] [    INFO][0m - eval_loss: 1.766398310661316, eval_accuracy: 0.5691489361702128, eval_runtime: 33.5839, eval_samples_per_second: 61.577, eval_steps_per_second: 3.871, epoch: 4.0[0m
[32m[2022-09-19 20:45:45,994] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-512[0m
[32m[2022-09-19 20:45:45,994] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 20:45:48,741] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-512/tokenizer_config.json[0m
[32m[2022-09-19 20:45:48,741] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-512/special_tokens_map.json[0m
[32m[2022-09-19 20:45:59,177] [    INFO][0m - loss: 0.18705053, learning_rate: 2.3906250000000002e-05, global_step: 520, interval_runtime: 47.6149, interval_samples_per_second: 0.336, interval_steps_per_second: 0.21, epoch: 4.0625[0m
[32m[2022-09-19 20:46:05,719] [    INFO][0m - loss: 0.10954145, learning_rate: 2.37890625e-05, global_step: 530, interval_runtime: 6.5417, interval_samples_per_second: 2.446, interval_steps_per_second: 1.529, epoch: 4.1406[0m
[32m[2022-09-19 20:46:12,221] [    INFO][0m - loss: 0.17266198, learning_rate: 2.3671875e-05, global_step: 540, interval_runtime: 6.5013, interval_samples_per_second: 2.461, interval_steps_per_second: 1.538, epoch: 4.2188[0m
[32m[2022-09-19 20:46:18,711] [    INFO][0m - loss: 0.21214929, learning_rate: 2.35546875e-05, global_step: 550, interval_runtime: 6.4906, interval_samples_per_second: 2.465, interval_steps_per_second: 1.541, epoch: 4.2969[0m
[32m[2022-09-19 20:46:25,182] [    INFO][0m - loss: 0.11190103, learning_rate: 2.34375e-05, global_step: 560, interval_runtime: 6.4705, interval_samples_per_second: 2.473, interval_steps_per_second: 1.545, epoch: 4.375[0m
[32m[2022-09-19 20:46:31,709] [    INFO][0m - loss: 0.30846169, learning_rate: 2.3320312500000002e-05, global_step: 570, interval_runtime: 6.5278, interval_samples_per_second: 2.451, interval_steps_per_second: 1.532, epoch: 4.4531[0m
[32m[2022-09-19 20:46:38,248] [    INFO][0m - loss: 0.23166511, learning_rate: 2.3203125e-05, global_step: 580, interval_runtime: 6.5388, interval_samples_per_second: 2.447, interval_steps_per_second: 1.529, epoch: 4.5312[0m
[32m[2022-09-19 20:46:44,742] [    INFO][0m - loss: 0.17826605, learning_rate: 2.30859375e-05, global_step: 590, interval_runtime: 6.4941, interval_samples_per_second: 2.464, interval_steps_per_second: 1.54, epoch: 4.6094[0m
[32m[2022-09-19 20:46:51,271] [    INFO][0m - loss: 0.16152126, learning_rate: 2.296875e-05, global_step: 600, interval_runtime: 6.5284, interval_samples_per_second: 2.451, interval_steps_per_second: 1.532, epoch: 4.6875[0m
[32m[2022-09-19 20:46:57,786] [    INFO][0m - loss: 0.14520227, learning_rate: 2.28515625e-05, global_step: 610, interval_runtime: 6.515, interval_samples_per_second: 2.456, interval_steps_per_second: 1.535, epoch: 4.7656[0m
[32m[2022-09-19 20:47:04,310] [    INFO][0m - loss: 0.33075235, learning_rate: 2.2734375000000002e-05, global_step: 620, interval_runtime: 6.5245, interval_samples_per_second: 2.452, interval_steps_per_second: 1.533, epoch: 4.8438[0m
[32m[2022-09-19 20:47:10,830] [    INFO][0m - loss: 0.1438494, learning_rate: 2.26171875e-05, global_step: 630, interval_runtime: 6.5199, interval_samples_per_second: 2.454, interval_steps_per_second: 1.534, epoch: 4.9219[0m
[32m[2022-09-19 20:47:16,757] [    INFO][0m - loss: 0.19653738, learning_rate: 2.25e-05, global_step: 640, interval_runtime: 5.9274, interval_samples_per_second: 2.699, interval_steps_per_second: 1.687, epoch: 5.0[0m
[32m[2022-09-19 20:47:16,758] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 20:47:16,758] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-19 20:47:16,758] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 20:47:16,758] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 20:47:16,758] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-19 20:47:50,513] [    INFO][0m - eval_loss: 2.370706558227539, eval_accuracy: 0.5667311411992263, eval_runtime: 33.7537, eval_samples_per_second: 61.267, eval_steps_per_second: 3.851, epoch: 5.0[0m
[32m[2022-09-19 20:47:50,548] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-640[0m
[32m[2022-09-19 20:47:50,548] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 20:47:53,197] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-640/tokenizer_config.json[0m
[32m[2022-09-19 20:47:53,197] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-640/special_tokens_map.json[0m
[32m[2022-09-19 20:48:04,885] [    INFO][0m - loss: 0.12175668, learning_rate: 2.23828125e-05, global_step: 650, interval_runtime: 48.127, interval_samples_per_second: 0.332, interval_steps_per_second: 0.208, epoch: 5.0781[0m
[32m[2022-09-19 20:48:11,366] [    INFO][0m - loss: 0.12620012, learning_rate: 2.2265625e-05, global_step: 660, interval_runtime: 6.4819, interval_samples_per_second: 2.468, interval_steps_per_second: 1.543, epoch: 5.1562[0m
[32m[2022-09-19 20:48:17,875] [    INFO][0m - loss: 0.10176166, learning_rate: 2.2148437500000002e-05, global_step: 670, interval_runtime: 6.5089, interval_samples_per_second: 2.458, interval_steps_per_second: 1.536, epoch: 5.2344[0m
[32m[2022-09-19 20:48:24,394] [    INFO][0m - loss: 0.09031423, learning_rate: 2.203125e-05, global_step: 680, interval_runtime: 6.5191, interval_samples_per_second: 2.454, interval_steps_per_second: 1.534, epoch: 5.3125[0m
[32m[2022-09-19 20:48:30,906] [    INFO][0m - loss: 0.09731458, learning_rate: 2.19140625e-05, global_step: 690, interval_runtime: 6.5118, interval_samples_per_second: 2.457, interval_steps_per_second: 1.536, epoch: 5.3906[0m
[32m[2022-09-19 20:48:37,411] [    INFO][0m - loss: 0.17864105, learning_rate: 2.1796875e-05, global_step: 700, interval_runtime: 6.5049, interval_samples_per_second: 2.46, interval_steps_per_second: 1.537, epoch: 5.4688[0m
[32m[2022-09-19 20:48:43,955] [    INFO][0m - loss: 0.17335377, learning_rate: 2.16796875e-05, global_step: 710, interval_runtime: 6.5436, interval_samples_per_second: 2.445, interval_steps_per_second: 1.528, epoch: 5.5469[0m
[32m[2022-09-19 20:48:50,460] [    INFO][0m - loss: 0.0842927, learning_rate: 2.15625e-05, global_step: 720, interval_runtime: 6.5056, interval_samples_per_second: 2.459, interval_steps_per_second: 1.537, epoch: 5.625[0m
[32m[2022-09-19 20:48:56,970] [    INFO][0m - loss: 0.1207345, learning_rate: 2.14453125e-05, global_step: 730, interval_runtime: 6.5102, interval_samples_per_second: 2.458, interval_steps_per_second: 1.536, epoch: 5.7031[0m
[32m[2022-09-19 20:49:03,477] [    INFO][0m - loss: 0.14152995, learning_rate: 2.1328125000000002e-05, global_step: 740, interval_runtime: 6.5065, interval_samples_per_second: 2.459, interval_steps_per_second: 1.537, epoch: 5.7812[0m
[32m[2022-09-19 20:49:10,002] [    INFO][0m - loss: 0.13629613, learning_rate: 2.12109375e-05, global_step: 750, interval_runtime: 6.5247, interval_samples_per_second: 2.452, interval_steps_per_second: 1.533, epoch: 5.8594[0m
[32m[2022-09-19 20:49:16,532] [    INFO][0m - loss: 0.2393254, learning_rate: 2.109375e-05, global_step: 760, interval_runtime: 6.5305, interval_samples_per_second: 2.45, interval_steps_per_second: 1.531, epoch: 5.9375[0m
[32m[2022-09-19 20:49:21,161] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 20:49:25,004] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-19 20:49:25,004] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 20:49:25,004] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 20:49:25,004] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-19 20:49:58,984] [    INFO][0m - eval_loss: 2.4725897312164307, eval_accuracy: 0.5701160541586073, eval_runtime: 37.8223, eval_samples_per_second: 54.677, eval_steps_per_second: 3.437, epoch: 6.0[0m
[32m[2022-09-19 20:49:59,022] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-768[0m
[32m[2022-09-19 20:49:59,023] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 20:50:01,692] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-768/tokenizer_config.json[0m
[32m[2022-09-19 20:50:01,693] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-768/special_tokens_map.json[0m
[32m[2022-09-19 20:50:08,201] [    INFO][0m - loss: 0.08495421, learning_rate: 2.09765625e-05, global_step: 770, interval_runtime: 51.6687, interval_samples_per_second: 0.31, interval_steps_per_second: 0.194, epoch: 6.0156[0m
[32m[2022-09-19 20:50:14,721] [    INFO][0m - loss: 0.09269234, learning_rate: 2.0859375e-05, global_step: 780, interval_runtime: 6.5198, interval_samples_per_second: 2.454, interval_steps_per_second: 1.534, epoch: 6.0938[0m
[32m[2022-09-19 20:50:21,226] [    INFO][0m - loss: 0.02723153, learning_rate: 2.0742187500000002e-05, global_step: 790, interval_runtime: 6.505, interval_samples_per_second: 2.46, interval_steps_per_second: 1.537, epoch: 6.1719[0m
[32m[2022-09-19 20:50:27,754] [    INFO][0m - loss: 0.0660218, learning_rate: 2.0625e-05, global_step: 800, interval_runtime: 6.5278, interval_samples_per_second: 2.451, interval_steps_per_second: 1.532, epoch: 6.25[0m
[32m[2022-09-19 20:50:34,293] [    INFO][0m - loss: 0.15243399, learning_rate: 2.05078125e-05, global_step: 810, interval_runtime: 6.539, interval_samples_per_second: 2.447, interval_steps_per_second: 1.529, epoch: 6.3281[0m
[32m[2022-09-19 20:50:40,819] [    INFO][0m - loss: 0.06265328, learning_rate: 2.0390625e-05, global_step: 820, interval_runtime: 6.5269, interval_samples_per_second: 2.451, interval_steps_per_second: 1.532, epoch: 6.4062[0m
[32m[2022-09-19 20:50:47,344] [    INFO][0m - loss: 0.04763644, learning_rate: 2.02734375e-05, global_step: 830, interval_runtime: 6.5247, interval_samples_per_second: 2.452, interval_steps_per_second: 1.533, epoch: 6.4844[0m
[32m[2022-09-19 20:50:53,866] [    INFO][0m - loss: 0.04756515, learning_rate: 2.0156250000000002e-05, global_step: 840, interval_runtime: 6.5219, interval_samples_per_second: 2.453, interval_steps_per_second: 1.533, epoch: 6.5625[0m
[32m[2022-09-19 20:51:00,383] [    INFO][0m - loss: 0.11220415, learning_rate: 2.00390625e-05, global_step: 850, interval_runtime: 6.517, interval_samples_per_second: 2.455, interval_steps_per_second: 1.534, epoch: 6.6406[0m
[32m[2022-09-19 20:51:06,888] [    INFO][0m - loss: 0.04372406, learning_rate: 1.9921875e-05, global_step: 860, interval_runtime: 6.5047, interval_samples_per_second: 2.46, interval_steps_per_second: 1.537, epoch: 6.7188[0m
[32m[2022-09-19 20:51:13,429] [    INFO][0m - loss: 0.05379401, learning_rate: 1.98046875e-05, global_step: 870, interval_runtime: 6.5407, interval_samples_per_second: 2.446, interval_steps_per_second: 1.529, epoch: 6.7969[0m
[32m[2022-09-19 20:51:19,981] [    INFO][0m - loss: 0.09960079, learning_rate: 1.96875e-05, global_step: 880, interval_runtime: 6.5526, interval_samples_per_second: 2.442, interval_steps_per_second: 1.526, epoch: 6.875[0m
[32m[2022-09-19 20:51:26,499] [    INFO][0m - loss: 0.07817285, learning_rate: 1.95703125e-05, global_step: 890, interval_runtime: 6.5182, interval_samples_per_second: 2.455, interval_steps_per_second: 1.534, epoch: 6.9531[0m
[32m[2022-09-19 20:51:29,824] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 20:51:29,824] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-19 20:51:29,824] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 20:51:29,824] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 20:51:29,825] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-19 20:52:03,502] [    INFO][0m - eval_loss: 2.971083402633667, eval_accuracy: 0.5705996131528046, eval_runtime: 33.6775, eval_samples_per_second: 61.406, eval_steps_per_second: 3.86, epoch: 7.0[0m
[32m[2022-09-19 20:52:03,538] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-896[0m
[32m[2022-09-19 20:52:03,538] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 20:52:06,413] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-896/tokenizer_config.json[0m
[32m[2022-09-19 20:52:06,413] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-896/special_tokens_map.json[0m
[32m[2022-09-19 20:52:14,350] [    INFO][0m - loss: 0.01389939, learning_rate: 1.9453125e-05, global_step: 900, interval_runtime: 47.8504, interval_samples_per_second: 0.334, interval_steps_per_second: 0.209, epoch: 7.0312[0m
[32m[2022-09-19 20:52:20,869] [    INFO][0m - loss: 0.0553992, learning_rate: 1.93359375e-05, global_step: 910, interval_runtime: 6.5193, interval_samples_per_second: 2.454, interval_steps_per_second: 1.534, epoch: 7.1094[0m
[32m[2022-09-19 20:52:27,376] [    INFO][0m - loss: 0.0529574, learning_rate: 1.921875e-05, global_step: 920, interval_runtime: 6.5066, interval_samples_per_second: 2.459, interval_steps_per_second: 1.537, epoch: 7.1875[0m
[32m[2022-09-19 20:52:33,933] [    INFO][0m - loss: 0.00577531, learning_rate: 1.91015625e-05, global_step: 930, interval_runtime: 6.5568, interval_samples_per_second: 2.44, interval_steps_per_second: 1.525, epoch: 7.2656[0m
[32m[2022-09-19 20:52:40,465] [    INFO][0m - loss: 0.01606534, learning_rate: 1.8984375e-05, global_step: 940, interval_runtime: 6.532, interval_samples_per_second: 2.449, interval_steps_per_second: 1.531, epoch: 7.3438[0m
[32m[2022-09-19 20:52:47,000] [    INFO][0m - loss: 0.04159242, learning_rate: 1.88671875e-05, global_step: 950, interval_runtime: 6.5354, interval_samples_per_second: 2.448, interval_steps_per_second: 1.53, epoch: 7.4219[0m
[32m[2022-09-19 20:52:53,524] [    INFO][0m - loss: 0.07011861, learning_rate: 1.8750000000000002e-05, global_step: 960, interval_runtime: 6.5236, interval_samples_per_second: 2.453, interval_steps_per_second: 1.533, epoch: 7.5[0m
[32m[2022-09-19 20:53:00,064] [    INFO][0m - loss: 0.07386281, learning_rate: 1.86328125e-05, global_step: 970, interval_runtime: 6.54, interval_samples_per_second: 2.446, interval_steps_per_second: 1.529, epoch: 7.5781[0m
[32m[2022-09-19 20:53:06,618] [    INFO][0m - loss: 0.002406, learning_rate: 1.8515625e-05, global_step: 980, interval_runtime: 6.5546, interval_samples_per_second: 2.441, interval_steps_per_second: 1.526, epoch: 7.6562[0m
[32m[2022-09-19 20:53:13,140] [    INFO][0m - loss: 0.01658347, learning_rate: 1.83984375e-05, global_step: 990, interval_runtime: 6.522, interval_samples_per_second: 2.453, interval_steps_per_second: 1.533, epoch: 7.7344[0m
[32m[2022-09-19 20:53:19,672] [    INFO][0m - loss: 0.05631257, learning_rate: 1.828125e-05, global_step: 1000, interval_runtime: 6.5322, interval_samples_per_second: 2.449, interval_steps_per_second: 1.531, epoch: 7.8125[0m
[32m[2022-09-19 20:53:26,183] [    INFO][0m - loss: 0.02961827, learning_rate: 1.8164062500000002e-05, global_step: 1010, interval_runtime: 6.511, interval_samples_per_second: 2.457, interval_steps_per_second: 1.536, epoch: 7.8906[0m
[32m[2022-09-19 20:53:32,651] [    INFO][0m - loss: 0.06463085, learning_rate: 1.8046875e-05, global_step: 1020, interval_runtime: 6.4677, interval_samples_per_second: 2.474, interval_steps_per_second: 1.546, epoch: 7.9688[0m
[32m[2022-09-19 20:53:34,717] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 20:53:34,718] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-19 20:53:34,718] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 20:53:34,718] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 20:53:34,718] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-19 20:54:08,366] [    INFO][0m - eval_loss: 3.3885457515716553, eval_accuracy: 0.5735009671179884, eval_runtime: 33.6481, eval_samples_per_second: 61.46, eval_steps_per_second: 3.864, epoch: 8.0[0m
[32m[2022-09-19 20:54:08,402] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-1024[0m
[32m[2022-09-19 20:54:08,402] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 20:54:11,035] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-1024/tokenizer_config.json[0m
[32m[2022-09-19 20:54:11,036] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-1024/special_tokens_map.json[0m
[32m[2022-09-19 20:54:20,072] [    INFO][0m - loss: 0.02633396, learning_rate: 1.79296875e-05, global_step: 1030, interval_runtime: 47.421, interval_samples_per_second: 0.337, interval_steps_per_second: 0.211, epoch: 8.0469[0m
[32m[2022-09-19 20:54:26,560] [    INFO][0m - loss: 0.01616102, learning_rate: 1.78125e-05, global_step: 1040, interval_runtime: 6.4874, interval_samples_per_second: 2.466, interval_steps_per_second: 1.541, epoch: 8.125[0m
[32m[2022-09-19 20:54:33,070] [    INFO][0m - loss: 0.00516082, learning_rate: 1.76953125e-05, global_step: 1050, interval_runtime: 6.51, interval_samples_per_second: 2.458, interval_steps_per_second: 1.536, epoch: 8.2031[0m
[32m[2022-09-19 20:54:39,574] [    INFO][0m - loss: 0.02507566, learning_rate: 1.7578125000000002e-05, global_step: 1060, interval_runtime: 6.5043, interval_samples_per_second: 2.46, interval_steps_per_second: 1.537, epoch: 8.2812[0m
[32m[2022-09-19 20:54:46,091] [    INFO][0m - loss: 0.03007488, learning_rate: 1.74609375e-05, global_step: 1070, interval_runtime: 6.5169, interval_samples_per_second: 2.455, interval_steps_per_second: 1.534, epoch: 8.3594[0m
[32m[2022-09-19 20:54:52,613] [    INFO][0m - loss: 0.05919817, learning_rate: 1.734375e-05, global_step: 1080, interval_runtime: 6.5225, interval_samples_per_second: 2.453, interval_steps_per_second: 1.533, epoch: 8.4375[0m
[32m[2022-09-19 20:54:59,134] [    INFO][0m - loss: 0.00719907, learning_rate: 1.72265625e-05, global_step: 1090, interval_runtime: 6.5205, interval_samples_per_second: 2.454, interval_steps_per_second: 1.534, epoch: 8.5156[0m
[32m[2022-09-19 20:55:05,660] [    INFO][0m - loss: 0.01451665, learning_rate: 1.7109375e-05, global_step: 1100, interval_runtime: 6.5257, interval_samples_per_second: 2.452, interval_steps_per_second: 1.532, epoch: 8.5938[0m
[32m[2022-09-19 20:55:12,190] [    INFO][0m - loss: 0.01291051, learning_rate: 1.69921875e-05, global_step: 1110, interval_runtime: 6.5306, interval_samples_per_second: 2.45, interval_steps_per_second: 1.531, epoch: 8.6719[0m
[32m[2022-09-19 20:55:18,711] [    INFO][0m - loss: 0.00768031, learning_rate: 1.6875e-05, global_step: 1120, interval_runtime: 6.5209, interval_samples_per_second: 2.454, interval_steps_per_second: 1.534, epoch: 8.75[0m
[32m[2022-09-19 20:55:25,246] [    INFO][0m - loss: 0.06887361, learning_rate: 1.67578125e-05, global_step: 1130, interval_runtime: 6.535, interval_samples_per_second: 2.448, interval_steps_per_second: 1.53, epoch: 8.8281[0m
[32m[2022-09-19 20:55:31,794] [    INFO][0m - loss: 0.00801786, learning_rate: 1.6640625e-05, global_step: 1140, interval_runtime: 6.548, interval_samples_per_second: 2.443, interval_steps_per_second: 1.527, epoch: 8.9062[0m
[32m[2022-09-19 20:55:38,284] [    INFO][0m - loss: 0.12281202, learning_rate: 1.65234375e-05, global_step: 1150, interval_runtime: 6.4904, interval_samples_per_second: 2.465, interval_steps_per_second: 1.541, epoch: 8.9844[0m
[32m[2022-09-19 20:55:39,095] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 20:55:39,095] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-19 20:55:39,095] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 20:55:39,095] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 20:55:39,095] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-19 20:56:12,648] [    INFO][0m - eval_loss: 3.2550594806671143, eval_accuracy: 0.5807543520309478, eval_runtime: 33.5524, eval_samples_per_second: 61.635, eval_steps_per_second: 3.875, epoch: 9.0[0m
[32m[2022-09-19 20:56:12,683] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-1152[0m
[32m[2022-09-19 20:56:12,683] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 20:56:15,314] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-1152/tokenizer_config.json[0m
[32m[2022-09-19 20:56:15,314] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-1152/special_tokens_map.json[0m
[32m[2022-09-19 20:56:25,635] [    INFO][0m - loss: 0.011728, learning_rate: 1.640625e-05, global_step: 1160, interval_runtime: 47.351, interval_samples_per_second: 0.338, interval_steps_per_second: 0.211, epoch: 9.0625[0m
[32m[2022-09-19 20:56:32,139] [    INFO][0m - loss: 0.02853258, learning_rate: 1.62890625e-05, global_step: 1170, interval_runtime: 6.5038, interval_samples_per_second: 2.46, interval_steps_per_second: 1.538, epoch: 9.1406[0m
[32m[2022-09-19 20:56:38,669] [    INFO][0m - loss: 0.00313791, learning_rate: 1.6171875000000002e-05, global_step: 1180, interval_runtime: 6.5294, interval_samples_per_second: 2.45, interval_steps_per_second: 1.532, epoch: 9.2188[0m
[32m[2022-09-19 20:56:45,161] [    INFO][0m - loss: 0.00040179, learning_rate: 1.60546875e-05, global_step: 1190, interval_runtime: 6.4928, interval_samples_per_second: 2.464, interval_steps_per_second: 1.54, epoch: 9.2969[0m
[32m[2022-09-19 20:56:55,019] [    INFO][0m - loss: 0.01189393, learning_rate: 1.59375e-05, global_step: 1200, interval_runtime: 6.5148, interval_samples_per_second: 2.456, interval_steps_per_second: 1.535, epoch: 9.375[0m
[32m[2022-09-19 20:57:01,531] [    INFO][0m - loss: 0.00064701, learning_rate: 1.58203125e-05, global_step: 1210, interval_runtime: 9.8542, interval_samples_per_second: 1.624, interval_steps_per_second: 1.015, epoch: 9.4531[0m
[32m[2022-09-19 20:57:08,046] [    INFO][0m - loss: 0.00369282, learning_rate: 1.5703125e-05, global_step: 1220, interval_runtime: 6.5156, interval_samples_per_second: 2.456, interval_steps_per_second: 1.535, epoch: 9.5312[0m
[32m[2022-09-19 20:57:14,568] [    INFO][0m - loss: 0.00186492, learning_rate: 1.5585937500000002e-05, global_step: 1230, interval_runtime: 6.5218, interval_samples_per_second: 2.453, interval_steps_per_second: 1.533, epoch: 9.6094[0m
[32m[2022-09-19 20:57:21,102] [    INFO][0m - loss: 0.0047345, learning_rate: 1.546875e-05, global_step: 1240, interval_runtime: 6.5344, interval_samples_per_second: 2.449, interval_steps_per_second: 1.53, epoch: 9.6875[0m
[32m[2022-09-19 20:57:27,633] [    INFO][0m - loss: 0.01147258, learning_rate: 1.53515625e-05, global_step: 1250, interval_runtime: 6.5306, interval_samples_per_second: 2.45, interval_steps_per_second: 1.531, epoch: 9.7656[0m
[32m[2022-09-19 20:57:34,229] [    INFO][0m - loss: 0.01207056, learning_rate: 1.5234375000000001e-05, global_step: 1260, interval_runtime: 6.5957, interval_samples_per_second: 2.426, interval_steps_per_second: 1.516, epoch: 9.8438[0m
[32m[2022-09-19 20:57:40,772] [    INFO][0m - loss: 0.00776584, learning_rate: 1.51171875e-05, global_step: 1270, interval_runtime: 6.5426, interval_samples_per_second: 2.446, interval_steps_per_second: 1.528, epoch: 9.9219[0m
[32m[2022-09-19 20:57:46,724] [    INFO][0m - loss: 0.00074001, learning_rate: 1.5e-05, global_step: 1280, interval_runtime: 5.9527, interval_samples_per_second: 2.688, interval_steps_per_second: 1.68, epoch: 10.0[0m
[32m[2022-09-19 20:57:46,725] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 20:57:46,725] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-19 20:57:46,725] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 20:57:46,725] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 20:57:46,725] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-19 20:58:20,418] [    INFO][0m - eval_loss: 3.350550651550293, eval_accuracy: 0.5817214700193424, eval_runtime: 33.6924, eval_samples_per_second: 61.379, eval_steps_per_second: 3.858, epoch: 10.0[0m
[32m[2022-09-19 20:58:20,457] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-1280[0m
[32m[2022-09-19 20:58:20,457] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 20:58:23,132] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-1280/tokenizer_config.json[0m
[32m[2022-09-19 20:58:23,133] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-1280/special_tokens_map.json[0m
[32m[2022-09-19 20:58:34,746] [    INFO][0m - loss: 0.03283893, learning_rate: 1.48828125e-05, global_step: 1290, interval_runtime: 48.0203, interval_samples_per_second: 0.333, interval_steps_per_second: 0.208, epoch: 10.0781[0m
[32m[2022-09-19 20:58:41,263] [    INFO][0m - loss: 0.00092234, learning_rate: 1.4765625e-05, global_step: 1300, interval_runtime: 6.5187, interval_samples_per_second: 2.454, interval_steps_per_second: 1.534, epoch: 10.1562[0m
[32m[2022-09-19 20:58:47,793] [    INFO][0m - loss: 0.00692284, learning_rate: 1.4648437500000001e-05, global_step: 1310, interval_runtime: 6.53, interval_samples_per_second: 2.45, interval_steps_per_second: 1.531, epoch: 10.2344[0m
[32m[2022-09-19 20:58:54,296] [    INFO][0m - loss: 0.00113803, learning_rate: 1.453125e-05, global_step: 1320, interval_runtime: 6.5027, interval_samples_per_second: 2.461, interval_steps_per_second: 1.538, epoch: 10.3125[0m
[32m[2022-09-19 20:59:00,826] [    INFO][0m - loss: 0.00022076, learning_rate: 1.44140625e-05, global_step: 1330, interval_runtime: 6.5304, interval_samples_per_second: 2.45, interval_steps_per_second: 1.531, epoch: 10.3906[0m
[32m[2022-09-19 20:59:07,358] [    INFO][0m - loss: 0.00021103, learning_rate: 1.4296875e-05, global_step: 1340, interval_runtime: 6.5319, interval_samples_per_second: 2.45, interval_steps_per_second: 1.531, epoch: 10.4688[0m
[32m[2022-09-19 20:59:13,857] [    INFO][0m - loss: 0.01432101, learning_rate: 1.41796875e-05, global_step: 1350, interval_runtime: 6.4994, interval_samples_per_second: 2.462, interval_steps_per_second: 1.539, epoch: 10.5469[0m
[32m[2022-09-19 20:59:20,399] [    INFO][0m - loss: 0.0002984, learning_rate: 1.40625e-05, global_step: 1360, interval_runtime: 6.5417, interval_samples_per_second: 2.446, interval_steps_per_second: 1.529, epoch: 10.625[0m
[32m[2022-09-19 20:59:26,941] [    INFO][0m - loss: 0.00019798, learning_rate: 1.3945312500000001e-05, global_step: 1370, interval_runtime: 6.5421, interval_samples_per_second: 2.446, interval_steps_per_second: 1.529, epoch: 10.7031[0m
[32m[2022-09-19 20:59:33,492] [    INFO][0m - loss: 0.00159301, learning_rate: 1.3828125e-05, global_step: 1380, interval_runtime: 6.5501, interval_samples_per_second: 2.443, interval_steps_per_second: 1.527, epoch: 10.7812[0m
[32m[2022-09-19 20:59:40,016] [    INFO][0m - loss: 0.00054808, learning_rate: 1.37109375e-05, global_step: 1390, interval_runtime: 6.5251, interval_samples_per_second: 2.452, interval_steps_per_second: 1.533, epoch: 10.8594[0m
[32m[2022-09-19 20:59:46,544] [    INFO][0m - loss: 0.00029522, learning_rate: 1.359375e-05, global_step: 1400, interval_runtime: 6.528, interval_samples_per_second: 2.451, interval_steps_per_second: 1.532, epoch: 10.9375[0m
[32m[2022-09-19 20:59:51,180] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 20:59:51,181] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-19 20:59:51,181] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 20:59:51,181] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 20:59:51,181] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-19 21:00:24,833] [    INFO][0m - eval_loss: 3.3407797813415527, eval_accuracy: 0.5952611218568665, eval_runtime: 33.6516, eval_samples_per_second: 61.453, eval_steps_per_second: 3.863, epoch: 11.0[0m
[32m[2022-09-19 21:00:24,872] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-1408[0m
[32m[2022-09-19 21:00:24,872] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 21:00:27,526] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-1408/tokenizer_config.json[0m
[32m[2022-09-19 21:00:27,527] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-1408/special_tokens_map.json[0m
[32m[2022-09-19 21:00:33,905] [    INFO][0m - loss: 0.00190306, learning_rate: 1.34765625e-05, global_step: 1410, interval_runtime: 47.3601, interval_samples_per_second: 0.338, interval_steps_per_second: 0.211, epoch: 11.0156[0m
[32m[2022-09-19 21:00:40,380] [    INFO][0m - loss: 7.407e-05, learning_rate: 1.3359375000000001e-05, global_step: 1420, interval_runtime: 6.4754, interval_samples_per_second: 2.471, interval_steps_per_second: 1.544, epoch: 11.0938[0m
[32m[2022-09-19 21:00:46,944] [    INFO][0m - loss: 0.00087433, learning_rate: 1.3242187500000001e-05, global_step: 1430, interval_runtime: 6.5642, interval_samples_per_second: 2.437, interval_steps_per_second: 1.523, epoch: 11.1719[0m
[32m[2022-09-19 21:00:53,488] [    INFO][0m - loss: 0.00225245, learning_rate: 1.3125e-05, global_step: 1440, interval_runtime: 6.5434, interval_samples_per_second: 2.445, interval_steps_per_second: 1.528, epoch: 11.25[0m
[32m[2022-09-19 21:01:00,034] [    INFO][0m - loss: 0.02390916, learning_rate: 1.30078125e-05, global_step: 1450, interval_runtime: 6.5468, interval_samples_per_second: 2.444, interval_steps_per_second: 1.527, epoch: 11.3281[0m
[32m[2022-09-19 21:01:06,571] [    INFO][0m - loss: 0.00076999, learning_rate: 1.2890625e-05, global_step: 1460, interval_runtime: 6.5366, interval_samples_per_second: 2.448, interval_steps_per_second: 1.53, epoch: 11.4062[0m
[32m[2022-09-19 21:01:16,123] [    INFO][0m - loss: 0.00016552, learning_rate: 1.27734375e-05, global_step: 1470, interval_runtime: 9.5516, interval_samples_per_second: 1.675, interval_steps_per_second: 1.047, epoch: 11.4844[0m
[32m[2022-09-19 21:01:22,645] [    INFO][0m - loss: 0.00026229, learning_rate: 1.2656250000000001e-05, global_step: 1480, interval_runtime: 6.5228, interval_samples_per_second: 2.453, interval_steps_per_second: 1.533, epoch: 11.5625[0m
[32m[2022-09-19 21:01:29,177] [    INFO][0m - loss: 0.0163146, learning_rate: 1.25390625e-05, global_step: 1490, interval_runtime: 6.5313, interval_samples_per_second: 2.45, interval_steps_per_second: 1.531, epoch: 11.6406[0m
[32m[2022-09-19 21:01:35,716] [    INFO][0m - loss: 0.00128018, learning_rate: 1.2421875e-05, global_step: 1500, interval_runtime: 6.5393, interval_samples_per_second: 2.447, interval_steps_per_second: 1.529, epoch: 11.7188[0m
[32m[2022-09-19 21:01:42,218] [    INFO][0m - loss: 9.105e-05, learning_rate: 1.23046875e-05, global_step: 1510, interval_runtime: 6.5023, interval_samples_per_second: 2.461, interval_steps_per_second: 1.538, epoch: 11.7969[0m
[32m[2022-09-19 21:01:48,732] [    INFO][0m - loss: 0.00014841, learning_rate: 1.21875e-05, global_step: 1520, interval_runtime: 6.5137, interval_samples_per_second: 2.456, interval_steps_per_second: 1.535, epoch: 11.875[0m
[32m[2022-09-19 21:01:55,259] [    INFO][0m - loss: 6.418e-05, learning_rate: 1.20703125e-05, global_step: 1530, interval_runtime: 6.5269, interval_samples_per_second: 2.451, interval_steps_per_second: 1.532, epoch: 11.9531[0m
[32m[2022-09-19 21:01:58,591] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 21:01:58,591] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-19 21:01:58,591] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 21:01:58,591] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 21:01:58,591] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-19 21:02:32,226] [    INFO][0m - eval_loss: 3.3990061283111572, eval_accuracy: 0.5870406189555126, eval_runtime: 33.6336, eval_samples_per_second: 61.486, eval_steps_per_second: 3.865, epoch: 12.0[0m
[32m[2022-09-19 21:02:32,248] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-1536[0m
[32m[2022-09-19 21:02:32,248] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 21:02:34,742] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-1536/tokenizer_config.json[0m
[32m[2022-09-19 21:02:34,743] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-1536/special_tokens_map.json[0m
[32m[2022-09-19 21:02:42,318] [    INFO][0m - loss: 9.915e-05, learning_rate: 1.1953125000000001e-05, global_step: 1540, interval_runtime: 47.0587, interval_samples_per_second: 0.34, interval_steps_per_second: 0.213, epoch: 12.0312[0m
[32m[2022-09-19 21:02:48,829] [    INFO][0m - loss: 7.573e-05, learning_rate: 1.18359375e-05, global_step: 1550, interval_runtime: 6.5116, interval_samples_per_second: 2.457, interval_steps_per_second: 1.536, epoch: 12.1094[0m
[32m[2022-09-19 21:02:55,326] [    INFO][0m - loss: 0.01054063, learning_rate: 1.171875e-05, global_step: 1560, interval_runtime: 6.4972, interval_samples_per_second: 2.463, interval_steps_per_second: 1.539, epoch: 12.1875[0m
[32m[2022-09-19 21:03:01,846] [    INFO][0m - loss: 5.59e-05, learning_rate: 1.16015625e-05, global_step: 1570, interval_runtime: 6.5199, interval_samples_per_second: 2.454, interval_steps_per_second: 1.534, epoch: 12.2656[0m
[32m[2022-09-19 21:03:08,380] [    INFO][0m - loss: 5.697e-05, learning_rate: 1.1484375e-05, global_step: 1580, interval_runtime: 6.5338, interval_samples_per_second: 2.449, interval_steps_per_second: 1.53, epoch: 12.3438[0m
[32m[2022-09-19 21:03:14,917] [    INFO][0m - loss: 5.719e-05, learning_rate: 1.1367187500000001e-05, global_step: 1590, interval_runtime: 6.5373, interval_samples_per_second: 2.447, interval_steps_per_second: 1.53, epoch: 12.4219[0m
[32m[2022-09-19 21:03:21,474] [    INFO][0m - loss: 8.209e-05, learning_rate: 1.125e-05, global_step: 1600, interval_runtime: 6.5566, interval_samples_per_second: 2.44, interval_steps_per_second: 1.525, epoch: 12.5[0m
[32m[2022-09-19 21:03:28,040] [    INFO][0m - loss: 5.824e-05, learning_rate: 1.11328125e-05, global_step: 1610, interval_runtime: 6.5663, interval_samples_per_second: 2.437, interval_steps_per_second: 1.523, epoch: 12.5781[0m
[32m[2022-09-19 21:03:34,590] [    INFO][0m - loss: 7.068e-05, learning_rate: 1.1015625e-05, global_step: 1620, interval_runtime: 6.5497, interval_samples_per_second: 2.443, interval_steps_per_second: 1.527, epoch: 12.6562[0m
[32m[2022-09-19 21:03:41,141] [    INFO][0m - loss: 6.707e-05, learning_rate: 1.08984375e-05, global_step: 1630, interval_runtime: 6.5508, interval_samples_per_second: 2.442, interval_steps_per_second: 1.527, epoch: 12.7344[0m
[32m[2022-09-19 21:03:47,688] [    INFO][0m - loss: 9.679e-05, learning_rate: 1.078125e-05, global_step: 1640, interval_runtime: 6.5477, interval_samples_per_second: 2.444, interval_steps_per_second: 1.527, epoch: 12.8125[0m
[32m[2022-09-19 21:03:57,194] [    INFO][0m - loss: 6.745e-05, learning_rate: 1.0664062500000001e-05, global_step: 1650, interval_runtime: 6.5425, interval_samples_per_second: 2.446, interval_steps_per_second: 1.528, epoch: 12.8906[0m
[32m[2022-09-19 21:04:03,674] [    INFO][0m - loss: 7.153e-05, learning_rate: 1.0546875e-05, global_step: 1660, interval_runtime: 9.4417, interval_samples_per_second: 1.695, interval_steps_per_second: 1.059, epoch: 12.9688[0m
[32m[2022-09-19 21:04:05,741] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 21:04:05,742] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-19 21:04:05,742] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 21:04:05,742] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 21:04:05,742] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-19 21:04:39,373] [    INFO][0m - eval_loss: 3.3857767581939697, eval_accuracy: 0.5909090909090909, eval_runtime: 33.6308, eval_samples_per_second: 61.491, eval_steps_per_second: 3.866, epoch: 13.0[0m
[32m[2022-09-19 21:04:39,410] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-1664[0m
[32m[2022-09-19 21:04:39,411] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 21:04:42,049] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-1664/tokenizer_config.json[0m
[32m[2022-09-19 21:04:42,050] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-1664/special_tokens_map.json[0m
[32m[2022-09-19 21:04:50,910] [    INFO][0m - loss: 6.222e-05, learning_rate: 1.04296875e-05, global_step: 1670, interval_runtime: 47.2375, interval_samples_per_second: 0.339, interval_steps_per_second: 0.212, epoch: 13.0469[0m
[32m[2022-09-19 21:04:57,406] [    INFO][0m - loss: 5.134e-05, learning_rate: 1.03125e-05, global_step: 1680, interval_runtime: 6.4955, interval_samples_per_second: 2.463, interval_steps_per_second: 1.54, epoch: 13.125[0m
[32m[2022-09-19 21:05:03,907] [    INFO][0m - loss: 5.516e-05, learning_rate: 1.01953125e-05, global_step: 1690, interval_runtime: 6.5008, interval_samples_per_second: 2.461, interval_steps_per_second: 1.538, epoch: 13.2031[0m
[32m[2022-09-19 21:05:10,426] [    INFO][0m - loss: 2.756e-05, learning_rate: 1.0078125000000001e-05, global_step: 1700, interval_runtime: 6.5195, interval_samples_per_second: 2.454, interval_steps_per_second: 1.534, epoch: 13.2812[0m
[32m[2022-09-19 21:05:16,962] [    INFO][0m - loss: 3.088e-05, learning_rate: 9.9609375e-06, global_step: 1710, interval_runtime: 6.5362, interval_samples_per_second: 2.448, interval_steps_per_second: 1.53, epoch: 13.3594[0m
[32m[2022-09-19 21:05:23,494] [    INFO][0m - loss: 4.524e-05, learning_rate: 9.84375e-06, global_step: 1720, interval_runtime: 6.5319, interval_samples_per_second: 2.45, interval_steps_per_second: 1.531, epoch: 13.4375[0m
[32m[2022-09-19 21:05:30,039] [    INFO][0m - loss: 3.315e-05, learning_rate: 9.7265625e-06, global_step: 1730, interval_runtime: 6.5445, interval_samples_per_second: 2.445, interval_steps_per_second: 1.528, epoch: 13.5156[0m
[32m[2022-09-19 21:05:36,628] [    INFO][0m - loss: 4.187e-05, learning_rate: 9.609375e-06, global_step: 1740, interval_runtime: 6.5892, interval_samples_per_second: 2.428, interval_steps_per_second: 1.518, epoch: 13.5938[0m
[32m[2022-09-19 21:05:43,157] [    INFO][0m - loss: 4.414e-05, learning_rate: 9.4921875e-06, global_step: 1750, interval_runtime: 6.5289, interval_samples_per_second: 2.451, interval_steps_per_second: 1.532, epoch: 13.6719[0m
[32m[2022-09-19 21:05:49,688] [    INFO][0m - loss: 2.589e-05, learning_rate: 9.375000000000001e-06, global_step: 1760, interval_runtime: 6.5317, interval_samples_per_second: 2.45, interval_steps_per_second: 1.531, epoch: 13.75[0m
[32m[2022-09-19 21:05:56,244] [    INFO][0m - loss: 3.381e-05, learning_rate: 9.2578125e-06, global_step: 1770, interval_runtime: 6.5556, interval_samples_per_second: 2.441, interval_steps_per_second: 1.525, epoch: 13.8281[0m
[32m[2022-09-19 21:06:02,794] [    INFO][0m - loss: 4.333e-05, learning_rate: 9.140625e-06, global_step: 1780, interval_runtime: 6.5501, interval_samples_per_second: 2.443, interval_steps_per_second: 1.527, epoch: 13.9062[0m
[32m[2022-09-19 21:06:09,241] [    INFO][0m - loss: 5.242e-05, learning_rate: 9.0234375e-06, global_step: 1790, interval_runtime: 6.4466, interval_samples_per_second: 2.482, interval_steps_per_second: 1.551, epoch: 13.9844[0m
[32m[2022-09-19 21:06:10,054] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 21:06:10,055] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-19 21:06:10,055] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 21:06:10,055] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 21:06:10,055] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-19 21:06:43,676] [    INFO][0m - eval_loss: 3.3815360069274902, eval_accuracy: 0.5899419729206963, eval_runtime: 33.6203, eval_samples_per_second: 61.511, eval_steps_per_second: 3.867, epoch: 14.0[0m
[32m[2022-09-19 21:06:43,709] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-1792[0m
[32m[2022-09-19 21:06:43,710] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 21:06:46,299] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-1792/tokenizer_config.json[0m
[32m[2022-09-19 21:06:46,300] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-1792/special_tokens_map.json[0m
[32m[2022-09-19 21:06:56,496] [    INFO][0m - loss: 3.702e-05, learning_rate: 8.90625e-06, global_step: 1800, interval_runtime: 47.2551, interval_samples_per_second: 0.339, interval_steps_per_second: 0.212, epoch: 14.0625[0m
[32m[2022-09-19 21:07:03,000] [    INFO][0m - loss: 7.409e-05, learning_rate: 8.789062500000001e-06, global_step: 1810, interval_runtime: 6.5037, interval_samples_per_second: 2.46, interval_steps_per_second: 1.538, epoch: 14.1406[0m
[32m[2022-09-19 21:07:09,492] [    INFO][0m - loss: 3.454e-05, learning_rate: 8.671875e-06, global_step: 1820, interval_runtime: 6.4927, interval_samples_per_second: 2.464, interval_steps_per_second: 1.54, epoch: 14.2188[0m
[32m[2022-09-19 21:07:16,015] [    INFO][0m - loss: 0.00043557, learning_rate: 8.5546875e-06, global_step: 1830, interval_runtime: 6.5222, interval_samples_per_second: 2.453, interval_steps_per_second: 1.533, epoch: 14.2969[0m
[32m[2022-09-19 21:07:22,540] [    INFO][0m - loss: 4.121e-05, learning_rate: 8.4375e-06, global_step: 1840, interval_runtime: 6.5253, interval_samples_per_second: 2.452, interval_steps_per_second: 1.532, epoch: 14.375[0m
[32m[2022-09-19 21:07:29,090] [    INFO][0m - loss: 2.287e-05, learning_rate: 8.3203125e-06, global_step: 1850, interval_runtime: 6.55, interval_samples_per_second: 2.443, interval_steps_per_second: 1.527, epoch: 14.4531[0m
[32m[2022-09-19 21:07:35,604] [    INFO][0m - loss: 5.213e-05, learning_rate: 8.203125e-06, global_step: 1860, interval_runtime: 6.5144, interval_samples_per_second: 2.456, interval_steps_per_second: 1.535, epoch: 14.5312[0m
[32m[2022-09-19 21:07:42,141] [    INFO][0m - loss: 4.292e-05, learning_rate: 8.085937500000001e-06, global_step: 1870, interval_runtime: 6.5363, interval_samples_per_second: 2.448, interval_steps_per_second: 1.53, epoch: 14.6094[0m
[32m[2022-09-19 21:07:48,676] [    INFO][0m - loss: 3.303e-05, learning_rate: 7.96875e-06, global_step: 1880, interval_runtime: 6.5354, interval_samples_per_second: 2.448, interval_steps_per_second: 1.53, epoch: 14.6875[0m
[32m[2022-09-19 21:07:55,211] [    INFO][0m - loss: 4.059e-05, learning_rate: 7.8515625e-06, global_step: 1890, interval_runtime: 6.5354, interval_samples_per_second: 2.448, interval_steps_per_second: 1.53, epoch: 14.7656[0m
[32m[2022-09-19 21:08:01,733] [    INFO][0m - loss: 6.049e-05, learning_rate: 7.734375e-06, global_step: 1900, interval_runtime: 6.5213, interval_samples_per_second: 2.453, interval_steps_per_second: 1.533, epoch: 14.8438[0m
[32m[2022-09-19 21:08:08,263] [    INFO][0m - loss: 6.032e-05, learning_rate: 7.6171875000000005e-06, global_step: 1910, interval_runtime: 6.5296, interval_samples_per_second: 2.45, interval_steps_per_second: 1.531, epoch: 14.9219[0m
[32m[2022-09-19 21:08:14,212] [    INFO][0m - loss: 3.078e-05, learning_rate: 7.5e-06, global_step: 1920, interval_runtime: 5.95, interval_samples_per_second: 2.689, interval_steps_per_second: 1.681, epoch: 15.0[0m
[32m[2022-09-19 21:08:14,213] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 21:08:14,213] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-19 21:08:14,213] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 21:08:14,213] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 21:08:14,214] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-19 21:08:47,776] [    INFO][0m - eval_loss: 3.4171924591064453, eval_accuracy: 0.5913926499032882, eval_runtime: 33.562, eval_samples_per_second: 61.617, eval_steps_per_second: 3.873, epoch: 15.0[0m
[32m[2022-09-19 21:08:47,810] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-1920[0m
[32m[2022-09-19 21:08:47,810] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 21:08:50,376] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-1920/tokenizer_config.json[0m
[32m[2022-09-19 21:08:50,377] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-1920/special_tokens_map.json[0m
[32m[2022-09-19 21:09:01,894] [    INFO][0m - loss: 2.805e-05, learning_rate: 7.3828125e-06, global_step: 1930, interval_runtime: 47.6814, interval_samples_per_second: 0.336, interval_steps_per_second: 0.21, epoch: 15.0781[0m
[32m[2022-09-19 21:09:08,372] [    INFO][0m - loss: 3.601e-05, learning_rate: 7.265625e-06, global_step: 1940, interval_runtime: 6.4781, interval_samples_per_second: 2.47, interval_steps_per_second: 1.544, epoch: 15.1562[0m
[32m[2022-09-19 21:09:14,918] [    INFO][0m - loss: 2.751e-05, learning_rate: 7.1484375e-06, global_step: 1950, interval_runtime: 6.5457, interval_samples_per_second: 2.444, interval_steps_per_second: 1.528, epoch: 15.2344[0m
[32m[2022-09-19 21:09:21,422] [    INFO][0m - loss: 3.298e-05, learning_rate: 7.03125e-06, global_step: 1960, interval_runtime: 6.5044, interval_samples_per_second: 2.46, interval_steps_per_second: 1.537, epoch: 15.3125[0m
[32m[2022-09-19 21:09:27,936] [    INFO][0m - loss: 3.129e-05, learning_rate: 6.9140625e-06, global_step: 1970, interval_runtime: 6.5144, interval_samples_per_second: 2.456, interval_steps_per_second: 1.535, epoch: 15.3906[0m
[32m[2022-09-19 21:09:34,499] [    INFO][0m - loss: 2.422e-05, learning_rate: 6.796875e-06, global_step: 1980, interval_runtime: 6.5628, interval_samples_per_second: 2.438, interval_steps_per_second: 1.524, epoch: 15.4688[0m
[32m[2022-09-19 21:09:41,016] [    INFO][0m - loss: 6.931e-05, learning_rate: 6.679687500000001e-06, global_step: 1990, interval_runtime: 6.5164, interval_samples_per_second: 2.455, interval_steps_per_second: 1.535, epoch: 15.5469[0m
[32m[2022-09-19 21:09:47,541] [    INFO][0m - loss: 2.41e-05, learning_rate: 6.5625e-06, global_step: 2000, interval_runtime: 6.5253, interval_samples_per_second: 2.452, interval_steps_per_second: 1.532, epoch: 15.625[0m
[32m[2022-09-19 21:09:54,173] [    INFO][0m - loss: 2.315e-05, learning_rate: 6.4453125e-06, global_step: 2010, interval_runtime: 6.6317, interval_samples_per_second: 2.413, interval_steps_per_second: 1.508, epoch: 15.7031[0m
[32m[2022-09-19 21:10:00,737] [    INFO][0m - loss: 2.211e-05, learning_rate: 6.3281250000000005e-06, global_step: 2020, interval_runtime: 6.5639, interval_samples_per_second: 2.438, interval_steps_per_second: 1.523, epoch: 15.7812[0m
[32m[2022-09-19 21:10:07,272] [    INFO][0m - loss: 3.473e-05, learning_rate: 6.2109375e-06, global_step: 2030, interval_runtime: 6.5348, interval_samples_per_second: 2.448, interval_steps_per_second: 1.53, epoch: 15.8594[0m
[32m[2022-09-19 21:10:13,807] [    INFO][0m - loss: 2.543e-05, learning_rate: 6.09375e-06, global_step: 2040, interval_runtime: 6.5356, interval_samples_per_second: 2.448, interval_steps_per_second: 1.53, epoch: 15.9375[0m
[32m[2022-09-19 21:10:18,439] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 21:10:18,439] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-19 21:10:18,439] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 21:10:18,440] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 21:10:18,440] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-19 21:10:51,999] [    INFO][0m - eval_loss: 3.417515993118286, eval_accuracy: 0.5913926499032882, eval_runtime: 33.5589, eval_samples_per_second: 61.623, eval_steps_per_second: 3.874, epoch: 16.0[0m
[32m[2022-09-19 21:10:52,033] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-2048[0m
[32m[2022-09-19 21:10:52,033] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 21:10:54,583] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-2048/tokenizer_config.json[0m
[32m[2022-09-19 21:10:54,583] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-2048/special_tokens_map.json[0m
[32m[2022-09-19 21:11:00,825] [    INFO][0m - loss: 3.985e-05, learning_rate: 5.9765625000000004e-06, global_step: 2050, interval_runtime: 47.0184, interval_samples_per_second: 0.34, interval_steps_per_second: 0.213, epoch: 16.0156[0m
[32m[2022-09-19 21:11:07,313] [    INFO][0m - loss: 4.07e-05, learning_rate: 5.859375e-06, global_step: 2060, interval_runtime: 6.4878, interval_samples_per_second: 2.466, interval_steps_per_second: 1.541, epoch: 16.0938[0m
[32m[2022-09-19 21:11:13,795] [    INFO][0m - loss: 3.028e-05, learning_rate: 5.7421875e-06, global_step: 2070, interval_runtime: 6.4818, interval_samples_per_second: 2.468, interval_steps_per_second: 1.543, epoch: 16.1719[0m
[32m[2022-09-19 21:11:20,302] [    INFO][0m - loss: 2.376e-05, learning_rate: 5.625e-06, global_step: 2080, interval_runtime: 6.5072, interval_samples_per_second: 2.459, interval_steps_per_second: 1.537, epoch: 16.25[0m
[32m[2022-09-19 21:11:26,798] [    INFO][0m - loss: 3.435e-05, learning_rate: 5.5078125e-06, global_step: 2090, interval_runtime: 6.4957, interval_samples_per_second: 2.463, interval_steps_per_second: 1.539, epoch: 16.3281[0m
[32m[2022-09-19 21:11:33,310] [    INFO][0m - loss: 2.998e-05, learning_rate: 5.390625e-06, global_step: 2100, interval_runtime: 6.5121, interval_samples_per_second: 2.457, interval_steps_per_second: 1.536, epoch: 16.4062[0m
[32m[2022-09-19 21:11:39,823] [    INFO][0m - loss: 3.448e-05, learning_rate: 5.2734375e-06, global_step: 2110, interval_runtime: 6.5132, interval_samples_per_second: 2.457, interval_steps_per_second: 1.535, epoch: 16.4844[0m
[32m[2022-09-19 21:11:46,343] [    INFO][0m - loss: 1.922e-05, learning_rate: 5.15625e-06, global_step: 2120, interval_runtime: 6.5195, interval_samples_per_second: 2.454, interval_steps_per_second: 1.534, epoch: 16.5625[0m
[32m[2022-09-19 21:11:52,873] [    INFO][0m - loss: 1.918e-05, learning_rate: 5.0390625000000005e-06, global_step: 2130, interval_runtime: 6.53, interval_samples_per_second: 2.45, interval_steps_per_second: 1.531, epoch: 16.6406[0m
[32m[2022-09-19 21:11:59,403] [    INFO][0m - loss: 3.546e-05, learning_rate: 4.921875e-06, global_step: 2140, interval_runtime: 6.53, interval_samples_per_second: 2.45, interval_steps_per_second: 1.531, epoch: 16.7188[0m
[32m[2022-09-19 21:12:05,917] [    INFO][0m - loss: 3.201e-05, learning_rate: 4.8046875e-06, global_step: 2150, interval_runtime: 6.514, interval_samples_per_second: 2.456, interval_steps_per_second: 1.535, epoch: 16.7969[0m
[32m[2022-09-19 21:12:12,454] [    INFO][0m - loss: 1.923e-05, learning_rate: 4.6875000000000004e-06, global_step: 2160, interval_runtime: 6.5374, interval_samples_per_second: 2.447, interval_steps_per_second: 1.53, epoch: 16.875[0m
[32m[2022-09-19 21:12:18,990] [    INFO][0m - loss: 3.713e-05, learning_rate: 4.5703125e-06, global_step: 2170, interval_runtime: 6.5352, interval_samples_per_second: 2.448, interval_steps_per_second: 1.53, epoch: 16.9531[0m
[32m[2022-09-19 21:12:22,312] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 21:12:22,312] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-19 21:12:22,312] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 21:12:22,312] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 21:12:22,312] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-19 21:12:55,872] [    INFO][0m - eval_loss: 3.4176759719848633, eval_accuracy: 0.5923597678916828, eval_runtime: 33.5596, eval_samples_per_second: 61.622, eval_steps_per_second: 3.874, epoch: 17.0[0m
[32m[2022-09-19 21:12:55,907] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-2176[0m
[32m[2022-09-19 21:12:55,907] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 21:12:58,473] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-2176/tokenizer_config.json[0m
[32m[2022-09-19 21:12:58,473] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-2176/special_tokens_map.json[0m
[32m[2022-09-19 21:13:06,057] [    INFO][0m - loss: 0.00011348, learning_rate: 4.453125e-06, global_step: 2180, interval_runtime: 47.0672, interval_samples_per_second: 0.34, interval_steps_per_second: 0.212, epoch: 17.0312[0m
[32m[2022-09-19 21:13:15,644] [    INFO][0m - loss: 1.685e-05, learning_rate: 4.3359375e-06, global_step: 2190, interval_runtime: 6.497, interval_samples_per_second: 2.463, interval_steps_per_second: 1.539, epoch: 17.1094[0m
[32m[2022-09-19 21:13:22,140] [    INFO][0m - loss: 2.074e-05, learning_rate: 4.21875e-06, global_step: 2200, interval_runtime: 9.5858, interval_samples_per_second: 1.669, interval_steps_per_second: 1.043, epoch: 17.1875[0m
[32m[2022-09-19 21:13:28,645] [    INFO][0m - loss: 2.683e-05, learning_rate: 4.1015625e-06, global_step: 2210, interval_runtime: 6.5057, interval_samples_per_second: 2.459, interval_steps_per_second: 1.537, epoch: 17.2656[0m
[32m[2022-09-19 21:13:35,154] [    INFO][0m - loss: 5.481e-05, learning_rate: 3.984375e-06, global_step: 2220, interval_runtime: 6.5085, interval_samples_per_second: 2.458, interval_steps_per_second: 1.536, epoch: 17.3438[0m
[32m[2022-09-19 21:13:41,782] [    INFO][0m - loss: 1.867e-05, learning_rate: 3.8671875e-06, global_step: 2230, interval_runtime: 6.6278, interval_samples_per_second: 2.414, interval_steps_per_second: 1.509, epoch: 17.4219[0m
[32m[2022-09-19 21:13:48,286] [    INFO][0m - loss: 3.278e-05, learning_rate: 3.75e-06, global_step: 2240, interval_runtime: 6.5045, interval_samples_per_second: 2.46, interval_steps_per_second: 1.537, epoch: 17.5[0m
[32m[2022-09-19 21:13:54,803] [    INFO][0m - loss: 2.574e-05, learning_rate: 3.6328125e-06, global_step: 2250, interval_runtime: 6.517, interval_samples_per_second: 2.455, interval_steps_per_second: 1.534, epoch: 17.5781[0m
[32m[2022-09-19 21:14:01,330] [    INFO][0m - loss: 2.875e-05, learning_rate: 3.515625e-06, global_step: 2260, interval_runtime: 6.5266, interval_samples_per_second: 2.451, interval_steps_per_second: 1.532, epoch: 17.6562[0m
[32m[2022-09-19 21:14:07,871] [    INFO][0m - loss: 2.828e-05, learning_rate: 3.3984375e-06, global_step: 2270, interval_runtime: 6.5413, interval_samples_per_second: 2.446, interval_steps_per_second: 1.529, epoch: 17.7344[0m
[32m[2022-09-19 21:14:14,450] [    INFO][0m - loss: 2.441e-05, learning_rate: 3.28125e-06, global_step: 2280, interval_runtime: 6.579, interval_samples_per_second: 2.432, interval_steps_per_second: 1.52, epoch: 17.8125[0m
[32m[2022-09-19 21:14:20,975] [    INFO][0m - loss: 4.339e-05, learning_rate: 3.1640625000000003e-06, global_step: 2290, interval_runtime: 6.5253, interval_samples_per_second: 2.452, interval_steps_per_second: 1.533, epoch: 17.8906[0m
[32m[2022-09-19 21:14:27,441] [    INFO][0m - loss: 5.919e-05, learning_rate: 3.046875e-06, global_step: 2300, interval_runtime: 6.4657, interval_samples_per_second: 2.475, interval_steps_per_second: 1.547, epoch: 17.9688[0m
[32m[2022-09-19 21:14:29,505] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 21:14:29,505] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-19 21:14:29,505] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 21:14:29,505] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 21:14:29,506] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-19 21:15:03,028] [    INFO][0m - eval_loss: 3.416177988052368, eval_accuracy: 0.5918762088974855, eval_runtime: 33.5217, eval_samples_per_second: 61.691, eval_steps_per_second: 3.878, epoch: 18.0[0m
[32m[2022-09-19 21:15:03,063] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-2304[0m
[32m[2022-09-19 21:15:03,063] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 21:15:05,614] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-2304/tokenizer_config.json[0m
[32m[2022-09-19 21:15:05,615] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-2304/special_tokens_map.json[0m
[32m[2022-09-19 21:15:14,490] [    INFO][0m - loss: 1.831e-05, learning_rate: 2.9296875e-06, global_step: 2310, interval_runtime: 47.0491, interval_samples_per_second: 0.34, interval_steps_per_second: 0.213, epoch: 18.0469[0m
[32m[2022-09-19 21:15:22,187] [    INFO][0m - loss: 2.795e-05, learning_rate: 2.8125e-06, global_step: 2320, interval_runtime: 6.499, interval_samples_per_second: 2.462, interval_steps_per_second: 1.539, epoch: 18.125[0m
[32m[2022-09-19 21:15:28,719] [    INFO][0m - loss: 4.45e-05, learning_rate: 2.6953125e-06, global_step: 2330, interval_runtime: 7.7295, interval_samples_per_second: 2.07, interval_steps_per_second: 1.294, epoch: 18.2031[0m
[32m[2022-09-19 21:15:35,317] [    INFO][0m - loss: 4.468e-05, learning_rate: 2.578125e-06, global_step: 2340, interval_runtime: 6.5976, interval_samples_per_second: 2.425, interval_steps_per_second: 1.516, epoch: 18.2812[0m
[32m[2022-09-19 21:15:41,838] [    INFO][0m - loss: 2.587e-05, learning_rate: 2.4609375e-06, global_step: 2350, interval_runtime: 6.5214, interval_samples_per_second: 2.453, interval_steps_per_second: 1.533, epoch: 18.3594[0m
[32m[2022-09-19 21:15:48,359] [    INFO][0m - loss: 2.769e-05, learning_rate: 2.3437500000000002e-06, global_step: 2360, interval_runtime: 6.5213, interval_samples_per_second: 2.453, interval_steps_per_second: 1.533, epoch: 18.4375[0m
[32m[2022-09-19 21:15:54,882] [    INFO][0m - loss: 5.087e-05, learning_rate: 2.2265625e-06, global_step: 2370, interval_runtime: 6.5227, interval_samples_per_second: 2.453, interval_steps_per_second: 1.533, epoch: 18.5156[0m
[32m[2022-09-19 21:16:01,403] [    INFO][0m - loss: 1.761e-05, learning_rate: 2.109375e-06, global_step: 2380, interval_runtime: 6.5211, interval_samples_per_second: 2.454, interval_steps_per_second: 1.533, epoch: 18.5938[0m
[32m[2022-09-19 21:16:07,927] [    INFO][0m - loss: 2.776e-05, learning_rate: 1.9921875e-06, global_step: 2390, interval_runtime: 6.5246, interval_samples_per_second: 2.452, interval_steps_per_second: 1.533, epoch: 18.6719[0m
[32m[2022-09-19 21:16:14,443] [    INFO][0m - loss: 2.964e-05, learning_rate: 1.875e-06, global_step: 2400, interval_runtime: 6.5157, interval_samples_per_second: 2.456, interval_steps_per_second: 1.535, epoch: 18.75[0m
[32m[2022-09-19 21:16:20,969] [    INFO][0m - loss: 1.615e-05, learning_rate: 1.7578125e-06, global_step: 2410, interval_runtime: 6.5256, interval_samples_per_second: 2.452, interval_steps_per_second: 1.532, epoch: 18.8281[0m
[32m[2022-09-19 21:16:27,509] [    INFO][0m - loss: 2.628e-05, learning_rate: 1.640625e-06, global_step: 2420, interval_runtime: 6.5404, interval_samples_per_second: 2.446, interval_steps_per_second: 1.529, epoch: 18.9062[0m
[32m[2022-09-19 21:16:33,926] [    INFO][0m - loss: 2.185e-05, learning_rate: 1.5234375e-06, global_step: 2430, interval_runtime: 6.4169, interval_samples_per_second: 2.493, interval_steps_per_second: 1.558, epoch: 18.9844[0m
[32m[2022-09-19 21:16:34,737] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 21:16:34,737] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-19 21:16:34,737] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 21:16:34,737] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 21:16:34,737] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-19 21:17:08,258] [    INFO][0m - eval_loss: 3.4169185161590576, eval_accuracy: 0.5918762088974855, eval_runtime: 33.5199, eval_samples_per_second: 61.695, eval_steps_per_second: 3.878, epoch: 19.0[0m
[32m[2022-09-19 21:17:08,293] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-2432[0m
[32m[2022-09-19 21:17:08,294] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 21:17:10,848] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-2432/tokenizer_config.json[0m
[32m[2022-09-19 21:17:10,849] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-2432/special_tokens_map.json[0m
[32m[2022-09-19 21:17:21,109] [    INFO][0m - loss: 5.179e-05, learning_rate: 1.40625e-06, global_step: 2440, interval_runtime: 47.1828, interval_samples_per_second: 0.339, interval_steps_per_second: 0.212, epoch: 19.0625[0m
[32m[2022-09-19 21:17:27,587] [    INFO][0m - loss: 3.191e-05, learning_rate: 1.2890625e-06, global_step: 2450, interval_runtime: 6.4785, interval_samples_per_second: 2.47, interval_steps_per_second: 1.544, epoch: 19.1406[0m
[32m[2022-09-19 21:17:34,100] [    INFO][0m - loss: 3.33e-05, learning_rate: 1.1718750000000001e-06, global_step: 2460, interval_runtime: 6.5122, interval_samples_per_second: 2.457, interval_steps_per_second: 1.536, epoch: 19.2188[0m
[32m[2022-09-19 21:17:40,614] [    INFO][0m - loss: 3.15e-05, learning_rate: 1.0546875e-06, global_step: 2470, interval_runtime: 6.5143, interval_samples_per_second: 2.456, interval_steps_per_second: 1.535, epoch: 19.2969[0m
[32m[2022-09-19 21:17:47,156] [    INFO][0m - loss: 0.00016412, learning_rate: 9.375e-07, global_step: 2480, interval_runtime: 6.542, interval_samples_per_second: 2.446, interval_steps_per_second: 1.529, epoch: 19.375[0m
[32m[2022-09-19 21:17:53,668] [    INFO][0m - loss: 3.292e-05, learning_rate: 8.203125e-07, global_step: 2490, interval_runtime: 6.5122, interval_samples_per_second: 2.457, interval_steps_per_second: 1.536, epoch: 19.4531[0m
[32m[2022-09-19 21:18:00,183] [    INFO][0m - loss: 2.374e-05, learning_rate: 7.03125e-07, global_step: 2500, interval_runtime: 6.5146, interval_samples_per_second: 2.456, interval_steps_per_second: 1.535, epoch: 19.5312[0m
[32m[2022-09-19 21:18:06,707] [    INFO][0m - loss: 2.374e-05, learning_rate: 5.859375000000001e-07, global_step: 2510, interval_runtime: 6.5244, interval_samples_per_second: 2.452, interval_steps_per_second: 1.533, epoch: 19.6094[0m
[32m[2022-09-19 21:18:13,233] [    INFO][0m - loss: 0.00482892, learning_rate: 4.6875e-07, global_step: 2520, interval_runtime: 6.5262, interval_samples_per_second: 2.452, interval_steps_per_second: 1.532, epoch: 19.6875[0m
[32m[2022-09-19 21:18:19,755] [    INFO][0m - loss: 1.879e-05, learning_rate: 3.515625e-07, global_step: 2530, interval_runtime: 6.5218, interval_samples_per_second: 2.453, interval_steps_per_second: 1.533, epoch: 19.7656[0m
[32m[2022-09-19 21:18:26,290] [    INFO][0m - loss: 2.011e-05, learning_rate: 2.34375e-07, global_step: 2540, interval_runtime: 6.5347, interval_samples_per_second: 2.448, interval_steps_per_second: 1.53, epoch: 19.8438[0m
[32m[2022-09-19 21:18:32,827] [    INFO][0m - loss: 2.774e-05, learning_rate: 1.171875e-07, global_step: 2550, interval_runtime: 6.5377, interval_samples_per_second: 2.447, interval_steps_per_second: 1.53, epoch: 19.9219[0m
[32m[2022-09-19 21:18:38,787] [    INFO][0m - loss: 5.357e-05, learning_rate: 0.0, global_step: 2560, interval_runtime: 5.9594, interval_samples_per_second: 2.685, interval_steps_per_second: 1.678, epoch: 20.0[0m
[32m[2022-09-19 21:18:38,787] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 21:18:38,787] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-19 21:18:38,787] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 21:18:38,788] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 21:18:38,788] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-19 21:19:12,307] [    INFO][0m - eval_loss: 3.414524793624878, eval_accuracy: 0.59284332688588, eval_runtime: 33.5186, eval_samples_per_second: 61.697, eval_steps_per_second: 3.878, epoch: 20.0[0m
[32m[2022-09-19 21:19:12,341] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-2560[0m
[32m[2022-09-19 21:19:12,341] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 21:19:14,912] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-2560/tokenizer_config.json[0m
[32m[2022-09-19 21:19:14,912] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-2560/special_tokens_map.json[0m
[32m[2022-09-19 21:19:19,712] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-19 21:19:19,712] [    INFO][0m - Loading best model from ./checkpoints_csldcp/checkpoint-1408 (score: 0.5952611218568665).[0m
[32m[2022-09-19 21:19:21,242] [    INFO][0m - train_runtime: 2516.447, train_samples_per_second: 16.182, train_steps_per_second: 1.017, train_loss: 0.26641821556745526, epoch: 20.0[0m
[32m[2022-09-19 21:19:21,244] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/[0m
[32m[2022-09-19 21:19:21,244] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 21:19:23,710] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/tokenizer_config.json[0m
[32m[2022-09-19 21:19:23,710] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/special_tokens_map.json[0m
[32m[2022-09-19 21:19:23,711] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-19 21:19:23,712] [    INFO][0m -   epoch                    =       20.0[0m
[32m[2022-09-19 21:19:23,712] [    INFO][0m -   train_loss               =     0.2664[0m
[32m[2022-09-19 21:19:23,712] [    INFO][0m -   train_runtime            = 0:41:56.44[0m
[32m[2022-09-19 21:19:23,712] [    INFO][0m -   train_samples_per_second =     16.182[0m
[32m[2022-09-19 21:19:23,712] [    INFO][0m -   train_steps_per_second   =      1.017[0m
[32m[2022-09-19 21:19:23,722] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-19 21:19:23,722] [    INFO][0m -   Num examples = 1784[0m
[32m[2022-09-19 21:19:23,722] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 21:19:23,722] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 21:19:23,722] [    INFO][0m -   Total prediction steps = 112[0m
[32m[2022-09-19 21:19:52,788] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-19 21:19:52,788] [    INFO][0m -   test_accuracy           =      0.583[0m
[32m[2022-09-19 21:19:52,788] [    INFO][0m -   test_loss               =     3.3407[0m
[32m[2022-09-19 21:19:52,788] [    INFO][0m -   test_runtime            = 0:00:29.06[0m
[32m[2022-09-19 21:19:52,788] [    INFO][0m -   test_samples_per_second =     61.379[0m
[32m[2022-09-19 21:19:52,789] [    INFO][0m -   test_steps_per_second   =      3.853[0m
[32m[2022-09-19 21:19:52,789] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-19 21:19:52,789] [    INFO][0m -   Num examples = 2999[0m
[32m[2022-09-19 21:19:52,789] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 21:19:52,789] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 21:19:52,789] [    INFO][0m -   Total prediction steps = 188[0m
[32m[2022-09-19 21:20:47,638] [    INFO][0m - Predictions for csldcp saved to ./fewclue_submit_examples.[0m
{
  "labels": 26,
  "text_a": "\u672c\u6587\u7814\u7a76\u4e86\u8d35\u5dde\u4e1c\u5357\u90e8\u7684\u5929\u67f1\u3001\u9526\u5c4f\u548c\u9ece\u5e73\u5730\u533a\u7684\u5730\u8d28\u6784\u9020\u7279\u5f81\u3001\u8be5\u533a\u91d1\u77ff\u5e8a\u7279\u5f81\u53ca\u5176\u4e0e\u5730\u8d28\u6784\u9020\u7684\u5173\u7cfb.\u8ba4\u4e3a\u533a\u5185\u5317\u90e8\u548c\u5357\u90e8\u76f8\u8ddd\u6570\u5341\u516c\u91cc\u7684\u4e24\u6761\u8fd1\u4e1c\u897f\u5411\u7684\u57fa\u5e95\u526a\u5207\u65ad\u88c2\u5e26\u6784\u6210\u4e86\u672c\u533a\u5730\u5811\u5f0f\u7684\u6784\u9020\u683c\u5c40.\u7531\u4e8e\u8fd9\u4e24\u6761\u4e1c\u897f\u5411\u526a\u5207\u65ad\u88c2\u7684\u526a\u5207\u4f5c\u7528,\u5f62\u6210\u4e86\u672c\u533a\u4ee5\u5317\u4e1c\u5411\u4e3a\u4e3b\u7684\u8936\u76b1\u548c\u526a\u5207\u5e26.\u5317\u4e1c\u5411\u8936\u76b1\u548c\u526a\u5207\u5e26\u6784\u9020\u662f\u5728\u52a0\u91cc\u4e1c\u671f\u5f62\u6210\u7684,\u540c\u65f6\u4e5f\u53d1\u751f\u4e86\u7eff\u7247\u5ca9\u76f8\u7684\u53d8\u8d28\u4f5c\u7528.\u5728\u80cc\u659c\u5f62\u6210\u7684\u540c\u65f6\u6216\u7a0d\u540e\u526a\u5207\u4f5c\u7528\u5f00\u59cb\u53d1\u751f,\u526a\u5207\u4f5c\u7528\u7ee7\u627f\u7740\u5317\u4e1c\u5411,\u5f62\u6210\u4e86\u82e5\u5e72\u6761\u4e0e\u8936\u76b1\u8f74\u5e73\u884c\u6216\u76f8\u4ea4\u7684\u526a\u5207\u5e26.\u6210\u77ff\u6d41\u4f53\u6cbf\u7740\u526a\u5207\u5e26\u4e0a\u5347,\u4e00\u65b9\u9762\u5728\u526a\u5207\u5e26\u4e2d\u6c89\u6dc0\u51fa\u7a7f\u5c42\u4ea7\u51fa\u7684\u4e0d\u6574\u5408\u7834\u788e\u5e26\u578b\u91d1\u77ff\u4f53,\u5373\u900f\u955c\u72b6\u542b\u91d1\u77f3\u82f1\u8109;\u540c\u65f6\u5927\u90e8\u5206\u5145\u586b\u5230\u7531\u80cc\u659c\u548c\u526a\u5207\u4f5c\u7528\u5f62\u6210\u7684\u5c42\u95f4\u88c2\u9699\u6216\u5c42\u95f4\u7834\u788e\u5e26\u4e2d,\u4ece\u800c\u5f62\u6210\u8d4b\u5b58\u4e8e\u6d4a\u79ef\u5ca9\u7684\u987a\u5c42\u4ea7\u51fa\u7684\u5c42\u72b6\u542b\u91d1\u77f3\u82f1\u8109\u91d1\u77ff\u5e8a.\u8fd9\u79cd\u7c7b\u578b\u7684\u542b\u91d1\u77f3\u82f1\u8109,\u65e0\u8bba\u5728\u54c1\u4f4d(\u591a\u6570\u53ef\u89c1\u660e\u91d1)\u3001\u50a8\u91cf\u548c\u4ea7\u91cf\u65b9\u9762\u5747\u5f88\u6709\u524d\u666f,\u5e76\u4e14\u6709\u7740\u5341\u5206\u91cd\u8981\u7684\u5b66\u672f\u610f\u4e49,\u56e0\u4e3a\u8fd9\u5728\u56fd\u5185\u8fd8\u662f\u9996\u6b21\u53d1\u73b0.",
  "text_b": "",
  "uid": 44
}

Prediction done.
 
==========
tnews
==========
 
[32m[2022-09-19 21:21:03,015] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-19 21:21:03,016] [    INFO][0m - ============================================================[0m
[32m[2022-09-19 21:21:03,016] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-19 21:21:03,016] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-19 21:21:03,016] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-19 21:21:03,016] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-19 21:21:03,016] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-19 21:21:03,016] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-19 21:21:03,017] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-19 21:21:03,017] [    INFO][0m - [0m
[32m[2022-09-19 21:21:03,017] [    INFO][0m - ============================================================[0m
[32m[2022-09-19 21:21:03,017] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-19 21:21:03,017] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-19 21:21:03,017] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-19 21:21:03,017] [    INFO][0m - pretrained                    :/ssd2/wanghuijuan03/data/zero-shot/checkpoints_09191451/checkpoint-21000/model_state.pdparams[0m
[32m[2022-09-19 21:21:03,017] [    INFO][0m - prompt                        :‚Äú{'text':'text_a'}‚Äù‰∏äËø∞Êñ∞ÈóªÈÄâËá™{'mask'}{'mask'}‰∏ìÊ†è„ÄÇ[0m
[32m[2022-09-19 21:21:03,017] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-19 21:21:03,017] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-19 21:21:03,017] [    INFO][0m - task_name                     :tnews[0m
[32m[2022-09-19 21:21:03,017] [    INFO][0m - [0m
[32m[2022-09-19 21:21:03,018] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0919 21:21:03.019217 10525 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0919 21:21:03.023252 10525 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-19 21:21:08,030] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-19 21:21:08,042] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-19 21:21:08,042] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-19 21:21:08,043] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚Äù‰∏äËø∞Êñ∞ÈóªÈÄâËá™'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': '‰∏ìÊ†è„ÄÇ'}][0m
[32m[2022-09-19 21:21:10,275] [    INFO][0m - ============================================================[0m
[32m[2022-09-19 21:21:10,276] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-19 21:21:10,276] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-19 21:21:10,276] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-19 21:21:10,276] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-19 21:21:10,277] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-19 21:21:10,277] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-19 21:21:10,277] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-19 21:21:10,277] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-19 21:21:10,277] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-19 21:21:10,277] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-19 21:21:10,277] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-19 21:21:10,277] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-19 21:21:10,277] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-19 21:21:10,277] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-19 21:21:10,277] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-19 21:21:10,277] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-19 21:21:10,277] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-19 21:21:10,278] [    INFO][0m - eval_batch_size               :16[0m
[32m[2022-09-19 21:21:10,278] [    INFO][0m - eval_steps                    :200[0m
[32m[2022-09-19 21:21:10,278] [    INFO][0m - evaluation_strategy           :IntervalStrategy.EPOCH[0m
[32m[2022-09-19 21:21:10,278] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-19 21:21:10,278] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-19 21:21:10,278] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-19 21:21:10,278] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-19 21:21:10,278] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-19 21:21:10,278] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-19 21:21:10,278] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-19 21:21:10,278] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-19 21:21:10,278] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-19 21:21:10,279] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-09-19 21:21:10,279] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-19 21:21:10,279] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-19 21:21:10,279] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-19 21:21:10,279] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-19 21:21:10,279] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-19 21:21:10,279] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-19 21:21:10,279] [    INFO][0m - logging_dir                   :./checkpoints_tnews/runs/Sep19_21-21-03_instance-3bwob41y-01[0m
[32m[2022-09-19 21:21:10,279] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-19 21:21:10,279] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-19 21:21:10,279] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-19 21:21:10,279] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-19 21:21:10,279] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-19 21:21:10,280] [    INFO][0m - max_seq_length                :64[0m
[32m[2022-09-19 21:21:10,280] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-19 21:21:10,280] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-19 21:21:10,280] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-19 21:21:10,280] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-19 21:21:10,280] [    INFO][0m - num_train_epochs              :20.0[0m
[32m[2022-09-19 21:21:10,280] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-19 21:21:10,280] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-19 21:21:10,280] [    INFO][0m - output_dir                    :./checkpoints_tnews/[0m
[32m[2022-09-19 21:21:10,280] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-19 21:21:10,280] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-19 21:21:10,280] [    INFO][0m - per_device_eval_batch_size    :16[0m
[32m[2022-09-19 21:21:10,280] [    INFO][0m - per_device_train_batch_size   :16[0m
[32m[2022-09-19 21:21:10,280] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-19 21:21:10,281] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-19 21:21:10,281] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-19 21:21:10,281] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-09-19 21:21:10,281] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-19 21:21:10,281] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-19 21:21:10,281] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-19 21:21:10,281] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-19 21:21:10,281] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-19 21:21:10,281] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-19 21:21:10,281] [    INFO][0m - run_name                      :./checkpoints_tnews/[0m
[32m[2022-09-19 21:21:10,281] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-19 21:21:10,281] [    INFO][0m - save_steps                    :200[0m
[32m[2022-09-19 21:21:10,281] [    INFO][0m - save_strategy                 :IntervalStrategy.EPOCH[0m
[32m[2022-09-19 21:21:10,282] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-19 21:21:10,282] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-19 21:21:10,282] [    INFO][0m - seed                          :42[0m
[32m[2022-09-19 21:21:10,282] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-19 21:21:10,282] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-19 21:21:10,282] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-19 21:21:10,282] [    INFO][0m - train_batch_size              :16[0m
[32m[2022-09-19 21:21:10,282] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-19 21:21:10,282] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-19 21:21:10,282] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-19 21:21:10,282] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-19 21:21:10,282] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-19 21:21:10,282] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-19 21:21:10,282] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-19 21:21:10,283] [    INFO][0m - [0m
[32m[2022-09-19 21:21:10,286] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-19 21:21:10,286] [    INFO][0m -   Num examples = 1185[0m
[32m[2022-09-19 21:21:10,286] [    INFO][0m -   Num Epochs = 20[0m
[32m[2022-09-19 21:21:10,286] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2022-09-19 21:21:10,286] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2022-09-19 21:21:10,286] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-19 21:21:10,286] [    INFO][0m -   Total optimization steps = 1500.0[0m
[32m[2022-09-19 21:21:10,286] [    INFO][0m -   Total num train samples = 23700[0m
[32m[2022-09-19 21:21:13,769] [    INFO][0m - loss: 2.87288132, learning_rate: 2.98e-05, global_step: 10, interval_runtime: 3.4814, interval_samples_per_second: 4.596, interval_steps_per_second: 2.872, epoch: 0.1333[0m
[32m[2022-09-19 21:21:15,657] [    INFO][0m - loss: 1.79432564, learning_rate: 2.96e-05, global_step: 20, interval_runtime: 1.8879, interval_samples_per_second: 8.475, interval_steps_per_second: 5.297, epoch: 0.2667[0m
[32m[2022-09-19 21:21:17,538] [    INFO][0m - loss: 1.4607482, learning_rate: 2.94e-05, global_step: 30, interval_runtime: 1.8818, interval_samples_per_second: 8.503, interval_steps_per_second: 5.314, epoch: 0.4[0m
[32m[2022-09-19 21:21:19,416] [    INFO][0m - loss: 1.57517796, learning_rate: 2.92e-05, global_step: 40, interval_runtime: 1.8776, interval_samples_per_second: 8.522, interval_steps_per_second: 5.326, epoch: 0.5333[0m
[32m[2022-09-19 21:21:21,296] [    INFO][0m - loss: 1.56975365, learning_rate: 2.9e-05, global_step: 50, interval_runtime: 1.88, interval_samples_per_second: 8.511, interval_steps_per_second: 5.319, epoch: 0.6667[0m
[32m[2022-09-19 21:21:23,173] [    INFO][0m - loss: 1.60133114, learning_rate: 2.88e-05, global_step: 60, interval_runtime: 1.8775, interval_samples_per_second: 8.522, interval_steps_per_second: 5.326, epoch: 0.8[0m
[32m[2022-09-19 21:21:25,047] [    INFO][0m - loss: 1.59355831, learning_rate: 2.86e-05, global_step: 70, interval_runtime: 1.8734, interval_samples_per_second: 8.541, interval_steps_per_second: 5.338, epoch: 0.9333[0m
[32m[2022-09-19 21:21:25,878] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 21:21:25,878] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-19 21:21:25,878] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 21:21:25,878] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 21:21:25,879] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-19 21:21:31,624] [    INFO][0m - eval_loss: 1.491665244102478, eval_accuracy: 0.51183970856102, eval_runtime: 5.7448, eval_samples_per_second: 191.131, eval_steps_per_second: 12.011, epoch: 1.0[0m
[32m[2022-09-19 21:21:31,646] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-75[0m
[32m[2022-09-19 21:21:31,647] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 21:21:35,226] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-75/tokenizer_config.json[0m
[32m[2022-09-19 21:21:35,228] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-75/special_tokens_map.json[0m
[32m[2022-09-19 21:21:42,844] [    INFO][0m - loss: 1.28348265, learning_rate: 2.84e-05, global_step: 80, interval_runtime: 17.7968, interval_samples_per_second: 0.899, interval_steps_per_second: 0.562, epoch: 1.0667[0m
[32m[2022-09-19 21:21:44,722] [    INFO][0m - loss: 0.99212475, learning_rate: 2.8199999999999998e-05, global_step: 90, interval_runtime: 1.8778, interval_samples_per_second: 8.521, interval_steps_per_second: 5.325, epoch: 1.2[0m
[32m[2022-09-19 21:21:46,615] [    INFO][0m - loss: 1.01107779, learning_rate: 2.8e-05, global_step: 100, interval_runtime: 1.8931, interval_samples_per_second: 8.452, interval_steps_per_second: 5.282, epoch: 1.3333[0m
[32m[2022-09-19 21:21:48,496] [    INFO][0m - loss: 1.21305656, learning_rate: 2.78e-05, global_step: 110, interval_runtime: 1.8799, interval_samples_per_second: 8.511, interval_steps_per_second: 5.319, epoch: 1.4667[0m
[32m[2022-09-19 21:21:50,389] [    INFO][0m - loss: 1.05937223, learning_rate: 2.7600000000000003e-05, global_step: 120, interval_runtime: 1.8952, interval_samples_per_second: 8.442, interval_steps_per_second: 5.277, epoch: 1.6[0m
[32m[2022-09-19 21:21:52,305] [    INFO][0m - loss: 0.98281908, learning_rate: 2.7400000000000002e-05, global_step: 130, interval_runtime: 1.9151, interval_samples_per_second: 8.355, interval_steps_per_second: 5.222, epoch: 1.7333[0m
[32m[2022-09-19 21:21:54,181] [    INFO][0m - loss: 0.94721823, learning_rate: 2.72e-05, global_step: 140, interval_runtime: 1.8768, interval_samples_per_second: 8.525, interval_steps_per_second: 5.328, epoch: 1.8667[0m
[32m[2022-09-19 21:21:55,954] [    INFO][0m - loss: 0.87847128, learning_rate: 2.7000000000000002e-05, global_step: 150, interval_runtime: 1.773, interval_samples_per_second: 9.024, interval_steps_per_second: 5.64, epoch: 2.0[0m
[32m[2022-09-19 21:21:55,955] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 21:21:55,955] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-19 21:21:55,955] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 21:21:55,955] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 21:21:55,955] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-19 21:22:01,598] [    INFO][0m - eval_loss: 1.6287802457809448, eval_accuracy: 0.5437158469945356, eval_runtime: 5.6417, eval_samples_per_second: 194.623, eval_steps_per_second: 12.23, epoch: 2.0[0m
[32m[2022-09-19 21:22:01,617] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-150[0m
[32m[2022-09-19 21:22:01,617] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 21:22:04,744] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-150/tokenizer_config.json[0m
[32m[2022-09-19 21:22:04,744] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-150/special_tokens_map.json[0m
[32m[2022-09-19 21:22:13,004] [    INFO][0m - loss: 0.5654108, learning_rate: 2.68e-05, global_step: 160, interval_runtime: 16.1285, interval_samples_per_second: 0.992, interval_steps_per_second: 0.62, epoch: 2.1333[0m
[32m[2022-09-19 21:22:14,924] [    INFO][0m - loss: 0.56397271, learning_rate: 2.6600000000000003e-05, global_step: 170, interval_runtime: 2.7973, interval_samples_per_second: 5.72, interval_steps_per_second: 3.575, epoch: 2.2667[0m
[32m[2022-09-19 21:22:16,795] [    INFO][0m - loss: 0.49810143, learning_rate: 2.64e-05, global_step: 180, interval_runtime: 1.9149, interval_samples_per_second: 8.355, interval_steps_per_second: 5.222, epoch: 2.4[0m
[32m[2022-09-19 21:22:18,666] [    INFO][0m - loss: 0.44809642, learning_rate: 2.62e-05, global_step: 190, interval_runtime: 1.8711, interval_samples_per_second: 8.551, interval_steps_per_second: 5.344, epoch: 2.5333[0m
[32m[2022-09-19 21:22:20,539] [    INFO][0m - loss: 0.62503443, learning_rate: 2.6000000000000002e-05, global_step: 200, interval_runtime: 1.8724, interval_samples_per_second: 8.545, interval_steps_per_second: 5.341, epoch: 2.6667[0m
[32m[2022-09-19 21:22:22,413] [    INFO][0m - loss: 0.64306087, learning_rate: 2.58e-05, global_step: 210, interval_runtime: 1.8743, interval_samples_per_second: 8.537, interval_steps_per_second: 5.335, epoch: 2.8[0m
[32m[2022-09-19 21:22:24,289] [    INFO][0m - loss: 0.68378658, learning_rate: 2.5600000000000002e-05, global_step: 220, interval_runtime: 1.8763, interval_samples_per_second: 8.527, interval_steps_per_second: 5.33, epoch: 2.9333[0m
[32m[2022-09-19 21:22:25,125] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 21:22:25,125] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-19 21:22:25,125] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 21:22:25,125] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 21:22:25,125] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-19 21:22:30,761] [    INFO][0m - eval_loss: 1.858341932296753, eval_accuracy: 0.5264116575591985, eval_runtime: 5.6355, eval_samples_per_second: 194.836, eval_steps_per_second: 12.244, epoch: 3.0[0m
[32m[2022-09-19 21:22:30,782] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-225[0m
[32m[2022-09-19 21:22:30,782] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 21:22:33,732] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-225/tokenizer_config.json[0m
[32m[2022-09-19 21:22:33,733] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-225/special_tokens_map.json[0m
[32m[2022-09-19 21:22:42,152] [    INFO][0m - loss: 0.37005978, learning_rate: 2.54e-05, global_step: 230, interval_runtime: 17.8628, interval_samples_per_second: 0.896, interval_steps_per_second: 0.56, epoch: 3.0667[0m
[32m[2022-09-19 21:22:44,020] [    INFO][0m - loss: 0.20644307, learning_rate: 2.52e-05, global_step: 240, interval_runtime: 1.8675, interval_samples_per_second: 8.568, interval_steps_per_second: 5.355, epoch: 3.2[0m
[32m[2022-09-19 21:22:45,897] [    INFO][0m - loss: 0.26266217, learning_rate: 2.5e-05, global_step: 250, interval_runtime: 1.8772, interval_samples_per_second: 8.523, interval_steps_per_second: 5.327, epoch: 3.3333[0m
[32m[2022-09-19 21:22:47,769] [    INFO][0m - loss: 0.26091309, learning_rate: 2.48e-05, global_step: 260, interval_runtime: 1.8721, interval_samples_per_second: 8.547, interval_steps_per_second: 5.342, epoch: 3.4667[0m
[32m[2022-09-19 21:22:49,641] [    INFO][0m - loss: 0.31187088, learning_rate: 2.4599999999999998e-05, global_step: 270, interval_runtime: 1.8728, interval_samples_per_second: 8.543, interval_steps_per_second: 5.339, epoch: 3.6[0m
[32m[2022-09-19 21:22:51,515] [    INFO][0m - loss: 0.26361947, learning_rate: 2.44e-05, global_step: 280, interval_runtime: 1.8735, interval_samples_per_second: 8.54, interval_steps_per_second: 5.338, epoch: 3.7333[0m
[32m[2022-09-19 21:22:53,386] [    INFO][0m - loss: 0.30457468, learning_rate: 2.42e-05, global_step: 290, interval_runtime: 1.871, interval_samples_per_second: 8.552, interval_steps_per_second: 5.345, epoch: 3.8667[0m
[32m[2022-09-19 21:22:55,162] [    INFO][0m - loss: 0.24265368, learning_rate: 2.4e-05, global_step: 300, interval_runtime: 1.7756, interval_samples_per_second: 9.011, interval_steps_per_second: 5.632, epoch: 4.0[0m
[32m[2022-09-19 21:22:55,162] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 21:22:55,162] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-19 21:22:55,162] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 21:22:55,163] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 21:22:55,163] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-19 21:23:00,703] [    INFO][0m - eval_loss: 2.836815357208252, eval_accuracy: 0.5336976320582878, eval_runtime: 5.54, eval_samples_per_second: 198.195, eval_steps_per_second: 12.455, epoch: 4.0[0m
[32m[2022-09-19 21:23:00,722] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-300[0m
[32m[2022-09-19 21:23:00,722] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 21:23:03,379] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-300/tokenizer_config.json[0m
[32m[2022-09-19 21:23:03,380] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-300/special_tokens_map.json[0m
[32m[2022-09-19 21:23:14,480] [    INFO][0m - loss: 0.14009413, learning_rate: 2.38e-05, global_step: 310, interval_runtime: 19.3177, interval_samples_per_second: 0.828, interval_steps_per_second: 0.518, epoch: 4.1333[0m
[32m[2022-09-19 21:23:16,348] [    INFO][0m - loss: 0.0748908, learning_rate: 2.3599999999999998e-05, global_step: 320, interval_runtime: 1.8687, interval_samples_per_second: 8.562, interval_steps_per_second: 5.351, epoch: 4.2667[0m
[32m[2022-09-19 21:23:18,210] [    INFO][0m - loss: 0.10083628, learning_rate: 2.3400000000000003e-05, global_step: 330, interval_runtime: 1.8618, interval_samples_per_second: 8.594, interval_steps_per_second: 5.371, epoch: 4.4[0m
[32m[2022-09-19 21:23:20,106] [    INFO][0m - loss: 0.107023, learning_rate: 2.32e-05, global_step: 340, interval_runtime: 1.8957, interval_samples_per_second: 8.44, interval_steps_per_second: 5.275, epoch: 4.5333[0m
[32m[2022-09-19 21:23:21,988] [    INFO][0m - loss: 0.16167231, learning_rate: 2.3000000000000003e-05, global_step: 350, interval_runtime: 1.8823, interval_samples_per_second: 8.5, interval_steps_per_second: 5.313, epoch: 4.6667[0m
[32m[2022-09-19 21:23:23,874] [    INFO][0m - loss: 0.18288842, learning_rate: 2.2800000000000002e-05, global_step: 360, interval_runtime: 1.8863, interval_samples_per_second: 8.482, interval_steps_per_second: 5.301, epoch: 4.8[0m
[32m[2022-09-19 21:23:25,742] [    INFO][0m - loss: 0.23531165, learning_rate: 2.26e-05, global_step: 370, interval_runtime: 1.8679, interval_samples_per_second: 8.566, interval_steps_per_second: 5.354, epoch: 4.9333[0m
[32m[2022-09-19 21:23:26,574] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 21:23:26,574] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-19 21:23:26,574] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 21:23:26,574] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 21:23:26,575] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-19 21:23:32,168] [    INFO][0m - eval_loss: 3.4184348583221436, eval_accuracy: 0.5318761384335154, eval_runtime: 5.5933, eval_samples_per_second: 196.306, eval_steps_per_second: 12.336, epoch: 5.0[0m
[32m[2022-09-19 21:23:32,187] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-375[0m
[32m[2022-09-19 21:23:32,187] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 21:23:34,960] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-375/tokenizer_config.json[0m
[32m[2022-09-19 21:23:34,960] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-375/special_tokens_map.json[0m
[32m[2022-09-19 21:23:40,970] [    INFO][0m - loss: 0.07278839, learning_rate: 2.2400000000000002e-05, global_step: 380, interval_runtime: 15.2278, interval_samples_per_second: 1.051, interval_steps_per_second: 0.657, epoch: 5.0667[0m
[32m[2022-09-19 21:23:44,034] [    INFO][0m - loss: 0.07382276, learning_rate: 2.22e-05, global_step: 390, interval_runtime: 1.8709, interval_samples_per_second: 8.552, interval_steps_per_second: 5.345, epoch: 5.2[0m
[32m[2022-09-19 21:23:45,895] [    INFO][0m - loss: 0.1051554, learning_rate: 2.2e-05, global_step: 400, interval_runtime: 3.054, interval_samples_per_second: 5.239, interval_steps_per_second: 3.274, epoch: 5.3333[0m
[32m[2022-09-19 21:23:47,795] [    INFO][0m - loss: 0.09737443, learning_rate: 2.18e-05, global_step: 410, interval_runtime: 1.8997, interval_samples_per_second: 8.422, interval_steps_per_second: 5.264, epoch: 5.4667[0m
[32m[2022-09-19 21:23:49,757] [    INFO][0m - loss: 0.10185965, learning_rate: 2.16e-05, global_step: 420, interval_runtime: 1.9625, interval_samples_per_second: 8.153, interval_steps_per_second: 5.095, epoch: 5.6[0m
[32m[2022-09-19 21:23:51,628] [    INFO][0m - loss: 0.09871534, learning_rate: 2.1400000000000002e-05, global_step: 430, interval_runtime: 1.8711, interval_samples_per_second: 8.551, interval_steps_per_second: 5.345, epoch: 5.7333[0m
[32m[2022-09-19 21:23:53,568] [    INFO][0m - loss: 0.1744895, learning_rate: 2.12e-05, global_step: 440, interval_runtime: 1.9396, interval_samples_per_second: 8.249, interval_steps_per_second: 5.156, epoch: 5.8667[0m
[32m[2022-09-19 21:23:55,375] [    INFO][0m - loss: 0.02653715, learning_rate: 2.1e-05, global_step: 450, interval_runtime: 1.8074, interval_samples_per_second: 8.853, interval_steps_per_second: 5.533, epoch: 6.0[0m
[32m[2022-09-19 21:23:55,376] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 21:23:55,376] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-19 21:23:55,376] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 21:23:55,376] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 21:23:55,376] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-19 21:24:00,912] [    INFO][0m - eval_loss: 3.970262050628662, eval_accuracy: 0.5428051001821493, eval_runtime: 5.5349, eval_samples_per_second: 198.378, eval_steps_per_second: 12.466, epoch: 6.0[0m
[32m[2022-09-19 21:24:00,930] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-450[0m
[32m[2022-09-19 21:24:00,931] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 21:24:03,832] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-450/tokenizer_config.json[0m
[32m[2022-09-19 21:24:03,833] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-450/special_tokens_map.json[0m
[32m[2022-09-19 21:24:10,835] [    INFO][0m - loss: 0.04323146, learning_rate: 2.08e-05, global_step: 460, interval_runtime: 15.4597, interval_samples_per_second: 1.035, interval_steps_per_second: 0.647, epoch: 6.1333[0m
[32m[2022-09-19 21:24:12,707] [    INFO][0m - loss: 0.00865295, learning_rate: 2.06e-05, global_step: 470, interval_runtime: 1.8719, interval_samples_per_second: 8.547, interval_steps_per_second: 5.342, epoch: 6.2667[0m
[32m[2022-09-19 21:24:14,575] [    INFO][0m - loss: 0.01188502, learning_rate: 2.04e-05, global_step: 480, interval_runtime: 1.8683, interval_samples_per_second: 8.564, interval_steps_per_second: 5.353, epoch: 6.4[0m
[32m[2022-09-19 21:24:16,455] [    INFO][0m - loss: 0.05330294, learning_rate: 2.02e-05, global_step: 490, interval_runtime: 1.8799, interval_samples_per_second: 8.511, interval_steps_per_second: 5.32, epoch: 6.5333[0m
[32m[2022-09-19 21:24:18,333] [    INFO][0m - loss: 0.01875377, learning_rate: 1.9999999999999998e-05, global_step: 500, interval_runtime: 1.8777, interval_samples_per_second: 8.521, interval_steps_per_second: 5.326, epoch: 6.6667[0m
[32m[2022-09-19 21:24:20,211] [    INFO][0m - loss: 0.02850882, learning_rate: 1.98e-05, global_step: 510, interval_runtime: 1.8784, interval_samples_per_second: 8.518, interval_steps_per_second: 5.324, epoch: 6.8[0m
[32m[2022-09-19 21:24:22,203] [    INFO][0m - loss: 0.0247775, learning_rate: 1.96e-05, global_step: 520, interval_runtime: 1.9915, interval_samples_per_second: 8.034, interval_steps_per_second: 5.021, epoch: 6.9333[0m
[32m[2022-09-19 21:24:23,032] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 21:24:23,032] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-19 21:24:23,032] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 21:24:23,032] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 21:24:23,032] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-19 21:24:28,556] [    INFO][0m - eval_loss: 4.925379276275635, eval_accuracy: 0.5409836065573771, eval_runtime: 5.5237, eval_samples_per_second: 198.779, eval_steps_per_second: 12.492, epoch: 7.0[0m
[32m[2022-09-19 21:24:28,575] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-525[0m
[32m[2022-09-19 21:24:28,575] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 21:24:31,386] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-525/tokenizer_config.json[0m
[32m[2022-09-19 21:24:31,387] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-525/special_tokens_map.json[0m
[32m[2022-09-19 21:24:37,461] [    INFO][0m - loss: 0.04156869, learning_rate: 1.9399999999999997e-05, global_step: 530, interval_runtime: 15.2588, interval_samples_per_second: 1.049, interval_steps_per_second: 0.655, epoch: 7.0667[0m
[32m[2022-09-19 21:24:39,324] [    INFO][0m - loss: 0.03907066, learning_rate: 1.9200000000000003e-05, global_step: 540, interval_runtime: 1.8623, interval_samples_per_second: 8.592, interval_steps_per_second: 5.37, epoch: 7.2[0m
[32m[2022-09-19 21:24:41,191] [    INFO][0m - loss: 0.06429166, learning_rate: 1.9e-05, global_step: 550, interval_runtime: 1.8671, interval_samples_per_second: 8.57, interval_steps_per_second: 5.356, epoch: 7.3333[0m
[32m[2022-09-19 21:24:43,062] [    INFO][0m - loss: 0.03013807, learning_rate: 1.8800000000000003e-05, global_step: 560, interval_runtime: 1.8708, interval_samples_per_second: 8.553, interval_steps_per_second: 5.345, epoch: 7.4667[0m
[32m[2022-09-19 21:24:44,934] [    INFO][0m - loss: 0.02782375, learning_rate: 1.86e-05, global_step: 570, interval_runtime: 1.8728, interval_samples_per_second: 8.543, interval_steps_per_second: 5.34, epoch: 7.6[0m
[32m[2022-09-19 21:24:46,805] [    INFO][0m - loss: 0.02849891, learning_rate: 1.84e-05, global_step: 580, interval_runtime: 1.8707, interval_samples_per_second: 8.553, interval_steps_per_second: 5.345, epoch: 7.7333[0m
[32m[2022-09-19 21:24:48,678] [    INFO][0m - loss: 0.01460403, learning_rate: 1.8200000000000002e-05, global_step: 590, interval_runtime: 1.8725, interval_samples_per_second: 8.545, interval_steps_per_second: 5.34, epoch: 7.8667[0m
[32m[2022-09-19 21:24:50,450] [    INFO][0m - loss: 0.01719482, learning_rate: 1.8e-05, global_step: 600, interval_runtime: 1.7721, interval_samples_per_second: 9.029, interval_steps_per_second: 5.643, epoch: 8.0[0m
[32m[2022-09-19 21:24:50,450] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 21:24:50,451] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-19 21:24:50,451] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 21:24:50,451] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 21:24:50,451] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-19 21:24:55,930] [    INFO][0m - eval_loss: 4.943414688110352, eval_accuracy: 0.5437158469945356, eval_runtime: 5.4786, eval_samples_per_second: 200.418, eval_steps_per_second: 12.595, epoch: 8.0[0m
[32m[2022-09-19 21:24:55,949] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-600[0m
[32m[2022-09-19 21:24:55,949] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 21:24:58,544] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-600/tokenizer_config.json[0m
[32m[2022-09-19 21:24:58,544] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-600/special_tokens_map.json[0m
[32m[2022-09-19 21:25:10,153] [    INFO][0m - loss: 0.00080656, learning_rate: 1.7800000000000002e-05, global_step: 610, interval_runtime: 19.7028, interval_samples_per_second: 0.812, interval_steps_per_second: 0.508, epoch: 8.1333[0m
[32m[2022-09-19 21:25:12,012] [    INFO][0m - loss: 0.02857523, learning_rate: 1.76e-05, global_step: 620, interval_runtime: 1.8593, interval_samples_per_second: 8.606, interval_steps_per_second: 5.379, epoch: 8.2667[0m
[32m[2022-09-19 21:25:13,879] [    INFO][0m - loss: 0.00613143, learning_rate: 1.74e-05, global_step: 630, interval_runtime: 1.8676, interval_samples_per_second: 8.567, interval_steps_per_second: 5.354, epoch: 8.4[0m
[32m[2022-09-19 21:25:15,743] [    INFO][0m - loss: 0.02304164, learning_rate: 1.72e-05, global_step: 640, interval_runtime: 1.8638, interval_samples_per_second: 8.584, interval_steps_per_second: 5.365, epoch: 8.5333[0m
[32m[2022-09-19 21:25:17,610] [    INFO][0m - loss: 0.00123833, learning_rate: 1.7e-05, global_step: 650, interval_runtime: 1.8664, interval_samples_per_second: 8.573, interval_steps_per_second: 5.358, epoch: 8.6667[0m
[32m[2022-09-19 21:25:19,482] [    INFO][0m - loss: 0.03136165, learning_rate: 1.6800000000000002e-05, global_step: 660, interval_runtime: 1.8718, interval_samples_per_second: 8.548, interval_steps_per_second: 5.342, epoch: 8.8[0m
[32m[2022-09-19 21:25:21,346] [    INFO][0m - loss: 0.01137968, learning_rate: 1.66e-05, global_step: 670, interval_runtime: 1.8645, interval_samples_per_second: 8.581, interval_steps_per_second: 5.363, epoch: 8.9333[0m
[32m[2022-09-19 21:25:22,174] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 21:25:22,174] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-19 21:25:22,174] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 21:25:22,174] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 21:25:22,175] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-19 21:25:27,613] [    INFO][0m - eval_loss: 5.292721271514893, eval_accuracy: 0.5373406193078324, eval_runtime: 5.4387, eval_samples_per_second: 201.887, eval_steps_per_second: 12.687, epoch: 9.0[0m
[32m[2022-09-19 21:25:27,632] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-675[0m
[32m[2022-09-19 21:25:27,632] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 21:25:30,235] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-675/tokenizer_config.json[0m
[32m[2022-09-19 21:25:30,235] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-675/special_tokens_map.json[0m
[32m[2022-09-19 21:25:39,078] [    INFO][0m - loss: 0.00259067, learning_rate: 1.64e-05, global_step: 680, interval_runtime: 17.7318, interval_samples_per_second: 0.902, interval_steps_per_second: 0.564, epoch: 9.0667[0m
[32m[2022-09-19 21:25:40,941] [    INFO][0m - loss: 0.00067132, learning_rate: 1.62e-05, global_step: 690, interval_runtime: 1.8628, interval_samples_per_second: 8.589, interval_steps_per_second: 5.368, epoch: 9.2[0m
[32m[2022-09-19 21:25:42,807] [    INFO][0m - loss: 0.0186066, learning_rate: 1.6e-05, global_step: 700, interval_runtime: 1.866, interval_samples_per_second: 8.575, interval_steps_per_second: 5.359, epoch: 9.3333[0m
[32m[2022-09-19 21:25:44,672] [    INFO][0m - loss: 0.00077841, learning_rate: 1.5799999999999998e-05, global_step: 710, interval_runtime: 1.8654, interval_samples_per_second: 8.577, interval_steps_per_second: 5.361, epoch: 9.4667[0m
[32m[2022-09-19 21:25:46,539] [    INFO][0m - loss: 0.00105722, learning_rate: 1.56e-05, global_step: 720, interval_runtime: 1.867, interval_samples_per_second: 8.57, interval_steps_per_second: 5.356, epoch: 9.6[0m
[32m[2022-09-19 21:25:48,403] [    INFO][0m - loss: 0.00362529, learning_rate: 1.5399999999999998e-05, global_step: 730, interval_runtime: 1.864, interval_samples_per_second: 8.584, interval_steps_per_second: 5.365, epoch: 9.7333[0m
[32m[2022-09-19 21:25:50,269] [    INFO][0m - loss: 0.00038315, learning_rate: 1.5200000000000002e-05, global_step: 740, interval_runtime: 1.8654, interval_samples_per_second: 8.577, interval_steps_per_second: 5.361, epoch: 9.8667[0m
[32m[2022-09-19 21:25:52,036] [    INFO][0m - loss: 0.00059598, learning_rate: 1.5e-05, global_step: 750, interval_runtime: 1.7675, interval_samples_per_second: 9.053, interval_steps_per_second: 5.658, epoch: 10.0[0m
[32m[2022-09-19 21:25:52,037] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 21:25:52,037] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-19 21:25:52,037] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 21:25:52,037] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 21:25:52,037] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-19 21:25:57,529] [    INFO][0m - eval_loss: 5.5537614822387695, eval_accuracy: 0.5373406193078324, eval_runtime: 5.4913, eval_samples_per_second: 199.953, eval_steps_per_second: 12.565, epoch: 10.0[0m
[32m[2022-09-19 21:25:57,549] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-750[0m
[32m[2022-09-19 21:25:57,549] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 21:26:00,174] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-750/tokenizer_config.json[0m
[32m[2022-09-19 21:26:00,174] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-750/special_tokens_map.json[0m
[32m[2022-09-19 21:26:06,937] [    INFO][0m - loss: 0.00020063, learning_rate: 1.48e-05, global_step: 760, interval_runtime: 14.9008, interval_samples_per_second: 1.074, interval_steps_per_second: 0.671, epoch: 10.1333[0m
[32m[2022-09-19 21:26:08,799] [    INFO][0m - loss: 0.00091036, learning_rate: 1.46e-05, global_step: 770, interval_runtime: 1.8622, interval_samples_per_second: 8.592, interval_steps_per_second: 5.37, epoch: 10.2667[0m
[32m[2022-09-19 21:26:10,664] [    INFO][0m - loss: 0.00017828, learning_rate: 1.44e-05, global_step: 780, interval_runtime: 1.8652, interval_samples_per_second: 8.578, interval_steps_per_second: 5.361, epoch: 10.4[0m
[32m[2022-09-19 21:26:12,569] [    INFO][0m - loss: 7.263e-05, learning_rate: 1.42e-05, global_step: 790, interval_runtime: 1.905, interval_samples_per_second: 8.399, interval_steps_per_second: 5.249, epoch: 10.5333[0m
[32m[2022-09-19 21:26:14,435] [    INFO][0m - loss: 0.0006957, learning_rate: 1.4e-05, global_step: 800, interval_runtime: 1.8655, interval_samples_per_second: 8.577, interval_steps_per_second: 5.361, epoch: 10.6667[0m
[32m[2022-09-19 21:26:16,297] [    INFO][0m - loss: 0.00015991, learning_rate: 1.3800000000000002e-05, global_step: 810, interval_runtime: 1.8625, interval_samples_per_second: 8.59, interval_steps_per_second: 5.369, epoch: 10.8[0m
[32m[2022-09-19 21:26:18,161] [    INFO][0m - loss: 0.00485381, learning_rate: 1.36e-05, global_step: 820, interval_runtime: 1.8638, interval_samples_per_second: 8.585, interval_steps_per_second: 5.365, epoch: 10.9333[0m
[32m[2022-09-19 21:26:18,996] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 21:26:18,996] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-19 21:26:18,997] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 21:26:18,997] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 21:26:18,997] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-19 21:26:24,428] [    INFO][0m - eval_loss: 5.442869186401367, eval_accuracy: 0.5209471766848816, eval_runtime: 5.4315, eval_samples_per_second: 202.154, eval_steps_per_second: 12.704, epoch: 11.0[0m
[32m[2022-09-19 21:26:24,447] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-825[0m
[32m[2022-09-19 21:26:24,447] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 21:26:26,962] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-825/tokenizer_config.json[0m
[32m[2022-09-19 21:26:26,963] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-825/special_tokens_map.json[0m
[32m[2022-09-19 21:26:32,781] [    INFO][0m - loss: 0.00759643, learning_rate: 1.34e-05, global_step: 830, interval_runtime: 14.6199, interval_samples_per_second: 1.094, interval_steps_per_second: 0.684, epoch: 11.0667[0m
[32m[2022-09-19 21:26:34,649] [    INFO][0m - loss: 0.00015333, learning_rate: 1.32e-05, global_step: 840, interval_runtime: 1.8679, interval_samples_per_second: 8.566, interval_steps_per_second: 5.354, epoch: 11.2[0m
[32m[2022-09-19 21:26:36,523] [    INFO][0m - loss: 0.00029607, learning_rate: 1.3000000000000001e-05, global_step: 850, interval_runtime: 1.8738, interval_samples_per_second: 8.539, interval_steps_per_second: 5.337, epoch: 11.3333[0m
[32m[2022-09-19 21:26:38,399] [    INFO][0m - loss: 0.00010338, learning_rate: 1.2800000000000001e-05, global_step: 860, interval_runtime: 1.8764, interval_samples_per_second: 8.527, interval_steps_per_second: 5.329, epoch: 11.4667[0m
[32m[2022-09-19 21:26:40,279] [    INFO][0m - loss: 0.00072087, learning_rate: 1.26e-05, global_step: 870, interval_runtime: 1.8796, interval_samples_per_second: 8.512, interval_steps_per_second: 5.32, epoch: 11.6[0m
[32m[2022-09-19 21:26:42,157] [    INFO][0m - loss: 0.0066324, learning_rate: 1.24e-05, global_step: 880, interval_runtime: 1.8784, interval_samples_per_second: 8.518, interval_steps_per_second: 5.324, epoch: 11.7333[0m
[32m[2022-09-19 21:26:44,033] [    INFO][0m - loss: 0.00076638, learning_rate: 1.22e-05, global_step: 890, interval_runtime: 1.8756, interval_samples_per_second: 8.53, interval_steps_per_second: 5.332, epoch: 11.8667[0m
[32m[2022-09-19 21:26:45,804] [    INFO][0m - loss: 6.138e-05, learning_rate: 1.2e-05, global_step: 900, interval_runtime: 1.7715, interval_samples_per_second: 9.032, interval_steps_per_second: 5.645, epoch: 12.0[0m
[32m[2022-09-19 21:26:45,805] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 21:26:45,805] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-19 21:26:45,805] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 21:26:45,805] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 21:26:45,805] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-19 21:26:51,276] [    INFO][0m - eval_loss: 5.634032726287842, eval_accuracy: 0.5255009107468124, eval_runtime: 5.4704, eval_samples_per_second: 200.715, eval_steps_per_second: 12.613, epoch: 12.0[0m
[32m[2022-09-19 21:26:51,295] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-900[0m
[32m[2022-09-19 21:26:51,295] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 21:26:53,787] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-900/tokenizer_config.json[0m
[32m[2022-09-19 21:26:53,788] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-900/special_tokens_map.json[0m
[32m[2022-09-19 21:27:04,941] [    INFO][0m - loss: 0.03620412, learning_rate: 1.1799999999999999e-05, global_step: 910, interval_runtime: 19.1371, interval_samples_per_second: 0.836, interval_steps_per_second: 0.523, epoch: 12.1333[0m
[32m[2022-09-19 21:27:06,813] [    INFO][0m - loss: 8.239e-05, learning_rate: 1.16e-05, global_step: 920, interval_runtime: 1.8719, interval_samples_per_second: 8.547, interval_steps_per_second: 5.342, epoch: 12.2667[0m
[32m[2022-09-19 21:27:08,676] [    INFO][0m - loss: 0.01550231, learning_rate: 1.1400000000000001e-05, global_step: 930, interval_runtime: 1.8631, interval_samples_per_second: 8.588, interval_steps_per_second: 5.367, epoch: 12.4[0m
[32m[2022-09-19 21:27:10,538] [    INFO][0m - loss: 7.331e-05, learning_rate: 1.1200000000000001e-05, global_step: 940, interval_runtime: 1.8615, interval_samples_per_second: 8.595, interval_steps_per_second: 5.372, epoch: 12.5333[0m
[32m[2022-09-19 21:27:12,403] [    INFO][0m - loss: 0.00064616, learning_rate: 1.1e-05, global_step: 950, interval_runtime: 1.8651, interval_samples_per_second: 8.579, interval_steps_per_second: 5.362, epoch: 12.6667[0m
[32m[2022-09-19 21:27:14,269] [    INFO][0m - loss: 9.41e-05, learning_rate: 1.08e-05, global_step: 960, interval_runtime: 1.8663, interval_samples_per_second: 8.573, interval_steps_per_second: 5.358, epoch: 12.8[0m
[32m[2022-09-19 21:27:16,137] [    INFO][0m - loss: 6.224e-05, learning_rate: 1.06e-05, global_step: 970, interval_runtime: 1.8672, interval_samples_per_second: 8.569, interval_steps_per_second: 5.355, epoch: 12.9333[0m
[32m[2022-09-19 21:27:16,967] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 21:27:16,967] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-19 21:27:16,967] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 21:27:16,967] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 21:27:16,967] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-19 21:27:22,431] [    INFO][0m - eval_loss: 5.602229118347168, eval_accuracy: 0.5382513661202186, eval_runtime: 5.4637, eval_samples_per_second: 200.962, eval_steps_per_second: 12.629, epoch: 13.0[0m
[32m[2022-09-19 21:27:22,449] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-975[0m
[32m[2022-09-19 21:27:22,450] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 21:27:24,882] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-975/tokenizer_config.json[0m
[32m[2022-09-19 21:27:24,882] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-975/special_tokens_map.json[0m
[32m[2022-09-19 21:27:32,878] [    INFO][0m - loss: 2.949e-05, learning_rate: 1.04e-05, global_step: 980, interval_runtime: 16.7415, interval_samples_per_second: 0.956, interval_steps_per_second: 0.597, epoch: 13.0667[0m
[32m[2022-09-19 21:27:34,743] [    INFO][0m - loss: 0.00046785, learning_rate: 1.02e-05, global_step: 990, interval_runtime: 1.8653, interval_samples_per_second: 8.578, interval_steps_per_second: 5.361, epoch: 13.2[0m
[32m[2022-09-19 21:27:36,742] [    INFO][0m - loss: 2.815e-05, learning_rate: 9.999999999999999e-06, global_step: 1000, interval_runtime: 1.8603, interval_samples_per_second: 8.601, interval_steps_per_second: 5.376, epoch: 13.3333[0m
[32m[2022-09-19 21:27:38,603] [    INFO][0m - loss: 0.00388708, learning_rate: 9.8e-06, global_step: 1010, interval_runtime: 1.9995, interval_samples_per_second: 8.002, interval_steps_per_second: 5.001, epoch: 13.4667[0m
[32m[2022-09-19 21:27:40,466] [    INFO][0m - loss: 8.45e-05, learning_rate: 9.600000000000001e-06, global_step: 1020, interval_runtime: 1.8628, interval_samples_per_second: 8.589, interval_steps_per_second: 5.368, epoch: 13.6[0m
[32m[2022-09-19 21:27:42,332] [    INFO][0m - loss: 0.00040857, learning_rate: 9.400000000000001e-06, global_step: 1030, interval_runtime: 1.8658, interval_samples_per_second: 8.575, interval_steps_per_second: 5.36, epoch: 13.7333[0m
[32m[2022-09-19 21:27:44,198] [    INFO][0m - loss: 0.00068796, learning_rate: 9.2e-06, global_step: 1040, interval_runtime: 1.8665, interval_samples_per_second: 8.572, interval_steps_per_second: 5.357, epoch: 13.8667[0m
[32m[2022-09-19 21:27:45,970] [    INFO][0m - loss: 4.05e-05, learning_rate: 9e-06, global_step: 1050, interval_runtime: 1.7716, interval_samples_per_second: 9.031, interval_steps_per_second: 5.644, epoch: 14.0[0m
[32m[2022-09-19 21:27:45,971] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 21:27:45,971] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-19 21:27:45,971] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 21:27:45,971] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 21:27:45,971] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-19 21:27:51,453] [    INFO][0m - eval_loss: 5.638358116149902, eval_accuracy: 0.5346083788706739, eval_runtime: 5.4819, eval_samples_per_second: 200.295, eval_steps_per_second: 12.587, epoch: 14.0[0m
[32m[2022-09-19 21:27:51,472] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-1050[0m
[32m[2022-09-19 21:27:51,472] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 21:27:53,967] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-1050/tokenizer_config.json[0m
[32m[2022-09-19 21:27:53,968] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-1050/special_tokens_map.json[0m
[32m[2022-09-19 21:28:00,644] [    INFO][0m - loss: 5.865e-05, learning_rate: 8.8e-06, global_step: 1060, interval_runtime: 14.6737, interval_samples_per_second: 1.09, interval_steps_per_second: 0.681, epoch: 14.1333[0m
[32m[2022-09-19 21:28:02,507] [    INFO][0m - loss: 0.00016657, learning_rate: 8.6e-06, global_step: 1070, interval_runtime: 1.8627, interval_samples_per_second: 8.59, interval_steps_per_second: 5.369, epoch: 14.2667[0m
[32m[2022-09-19 21:28:04,373] [    INFO][0m - loss: 1.273e-05, learning_rate: 8.400000000000001e-06, global_step: 1080, interval_runtime: 1.8662, interval_samples_per_second: 8.574, interval_steps_per_second: 5.359, epoch: 14.4[0m
[32m[2022-09-19 21:28:07,083] [    INFO][0m - loss: 5.832e-05, learning_rate: 8.2e-06, global_step: 1090, interval_runtime: 1.8733, interval_samples_per_second: 8.541, interval_steps_per_second: 5.338, epoch: 14.5333[0m
[32m[2022-09-19 21:28:08,962] [    INFO][0m - loss: 4.978e-05, learning_rate: 8e-06, global_step: 1100, interval_runtime: 2.7158, interval_samples_per_second: 5.891, interval_steps_per_second: 3.682, epoch: 14.6667[0m
[32m[2022-09-19 21:28:10,844] [    INFO][0m - loss: 1.204e-05, learning_rate: 7.8e-06, global_step: 1110, interval_runtime: 1.8818, interval_samples_per_second: 8.503, interval_steps_per_second: 5.314, epoch: 14.8[0m
[32m[2022-09-19 21:28:12,710] [    INFO][0m - loss: 0.00023076, learning_rate: 7.600000000000001e-06, global_step: 1120, interval_runtime: 1.8662, interval_samples_per_second: 8.574, interval_steps_per_second: 5.359, epoch: 14.9333[0m
[32m[2022-09-19 21:28:13,540] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 21:28:13,540] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-19 21:28:13,540] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 21:28:13,540] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 21:28:13,540] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-19 21:28:18,958] [    INFO][0m - eval_loss: 5.565683364868164, eval_accuracy: 0.5373406193078324, eval_runtime: 5.4182, eval_samples_per_second: 202.652, eval_steps_per_second: 12.735, epoch: 15.0[0m
[32m[2022-09-19 21:28:18,977] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-1125[0m
[32m[2022-09-19 21:28:18,977] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 21:28:21,474] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-1125/tokenizer_config.json[0m
[32m[2022-09-19 21:28:21,474] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-1125/special_tokens_map.json[0m
[32m[2022-09-19 21:28:27,249] [    INFO][0m - loss: 5.518e-05, learning_rate: 7.4e-06, global_step: 1130, interval_runtime: 14.539, interval_samples_per_second: 1.1, interval_steps_per_second: 0.688, epoch: 15.0667[0m
[32m[2022-09-19 21:28:29,111] [    INFO][0m - loss: 1.694e-05, learning_rate: 7.2e-06, global_step: 1140, interval_runtime: 1.8622, interval_samples_per_second: 8.592, interval_steps_per_second: 5.37, epoch: 15.2[0m
[32m[2022-09-19 21:28:30,976] [    INFO][0m - loss: 1.549e-05, learning_rate: 7e-06, global_step: 1150, interval_runtime: 1.8647, interval_samples_per_second: 8.581, interval_steps_per_second: 5.363, epoch: 15.3333[0m
[32m[2022-09-19 21:28:33,027] [    INFO][0m - loss: 1.99e-05, learning_rate: 6.8e-06, global_step: 1160, interval_runtime: 1.8641, interval_samples_per_second: 8.583, interval_steps_per_second: 5.365, epoch: 15.4667[0m
[32m[2022-09-19 21:28:34,891] [    INFO][0m - loss: 4.558e-05, learning_rate: 6.6e-06, global_step: 1170, interval_runtime: 2.0514, interval_samples_per_second: 7.8, interval_steps_per_second: 4.875, epoch: 15.6[0m
[32m[2022-09-19 21:28:36,762] [    INFO][0m - loss: 2.134e-05, learning_rate: 6.4000000000000006e-06, global_step: 1180, interval_runtime: 1.8701, interval_samples_per_second: 8.556, interval_steps_per_second: 5.347, epoch: 15.7333[0m
[32m[2022-09-19 21:28:38,633] [    INFO][0m - loss: 0.00097262, learning_rate: 6.2e-06, global_step: 1190, interval_runtime: 1.8719, interval_samples_per_second: 8.547, interval_steps_per_second: 5.342, epoch: 15.8667[0m
[32m[2022-09-19 21:28:40,414] [    INFO][0m - loss: 0.00010882, learning_rate: 6e-06, global_step: 1200, interval_runtime: 1.7807, interval_samples_per_second: 8.985, interval_steps_per_second: 5.616, epoch: 16.0[0m
[32m[2022-09-19 21:28:40,415] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 21:28:40,415] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-19 21:28:40,415] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 21:28:40,415] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 21:28:40,415] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-19 21:28:45,889] [    INFO][0m - eval_loss: 5.6046319007873535, eval_accuracy: 0.5428051001821493, eval_runtime: 5.4737, eval_samples_per_second: 200.597, eval_steps_per_second: 12.606, epoch: 16.0[0m
[32m[2022-09-19 21:28:45,908] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-1200[0m
[32m[2022-09-19 21:28:45,908] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 21:28:48,314] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-1200/tokenizer_config.json[0m
[32m[2022-09-19 21:28:48,314] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-1200/special_tokens_map.json[0m
[32m[2022-09-19 21:28:54,999] [    INFO][0m - loss: 1.086e-05, learning_rate: 5.8e-06, global_step: 1210, interval_runtime: 14.5847, interval_samples_per_second: 1.097, interval_steps_per_second: 0.686, epoch: 16.1333[0m
[32m[2022-09-19 21:28:57,772] [    INFO][0m - loss: 9.801e-05, learning_rate: 5.600000000000001e-06, global_step: 1220, interval_runtime: 2.7737, interval_samples_per_second: 5.768, interval_steps_per_second: 3.605, epoch: 16.2667[0m
[32m[2022-09-19 21:28:59,644] [    INFO][0m - loss: 0.00012737, learning_rate: 5.4e-06, global_step: 1230, interval_runtime: 1.8713, interval_samples_per_second: 8.55, interval_steps_per_second: 5.344, epoch: 16.4[0m
[32m[2022-09-19 21:29:01,511] [    INFO][0m - loss: 3.057e-05, learning_rate: 5.2e-06, global_step: 1240, interval_runtime: 1.8674, interval_samples_per_second: 8.568, interval_steps_per_second: 5.355, epoch: 16.5333[0m
[32m[2022-09-19 21:29:03,379] [    INFO][0m - loss: 1.149e-05, learning_rate: 4.9999999999999996e-06, global_step: 1250, interval_runtime: 1.8683, interval_samples_per_second: 8.564, interval_steps_per_second: 5.352, epoch: 16.6667[0m
[32m[2022-09-19 21:29:05,249] [    INFO][0m - loss: 4.066e-05, learning_rate: 4.800000000000001e-06, global_step: 1260, interval_runtime: 1.8695, interval_samples_per_second: 8.558, interval_steps_per_second: 5.349, epoch: 16.8[0m
[32m[2022-09-19 21:29:07,121] [    INFO][0m - loss: 0.00047077, learning_rate: 4.6e-06, global_step: 1270, interval_runtime: 1.8722, interval_samples_per_second: 8.546, interval_steps_per_second: 5.341, epoch: 16.9333[0m
[32m[2022-09-19 21:29:07,953] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 21:29:07,953] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-19 21:29:07,953] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 21:29:07,953] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 21:29:07,953] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-19 21:29:13,390] [    INFO][0m - eval_loss: 5.683000564575195, eval_accuracy: 0.5391621129326047, eval_runtime: 5.4369, eval_samples_per_second: 201.954, eval_steps_per_second: 12.691, epoch: 17.0[0m
[32m[2022-09-19 21:29:13,409] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-1275[0m
[32m[2022-09-19 21:29:13,409] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 21:29:15,900] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-1275/tokenizer_config.json[0m
[32m[2022-09-19 21:29:15,900] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-1275/special_tokens_map.json[0m
[32m[2022-09-19 21:29:21,610] [    INFO][0m - loss: 1.234e-05, learning_rate: 4.4e-06, global_step: 1280, interval_runtime: 14.4888, interval_samples_per_second: 1.104, interval_steps_per_second: 0.69, epoch: 17.0667[0m
[32m[2022-09-19 21:29:23,993] [    INFO][0m - loss: 3.662e-05, learning_rate: 4.2000000000000004e-06, global_step: 1290, interval_runtime: 1.8671, interval_samples_per_second: 8.57, interval_steps_per_second: 5.356, epoch: 17.2[0m
[32m[2022-09-19 21:29:25,861] [    INFO][0m - loss: 1.099e-05, learning_rate: 4e-06, global_step: 1300, interval_runtime: 2.3843, interval_samples_per_second: 6.711, interval_steps_per_second: 4.194, epoch: 17.3333[0m
[32m[2022-09-19 21:29:27,727] [    INFO][0m - loss: 1.841e-05, learning_rate: 3.8000000000000005e-06, global_step: 1310, interval_runtime: 1.8655, interval_samples_per_second: 8.577, interval_steps_per_second: 5.361, epoch: 17.4667[0m
[32m[2022-09-19 21:29:29,594] [    INFO][0m - loss: 1.763e-05, learning_rate: 3.6e-06, global_step: 1320, interval_runtime: 1.8673, interval_samples_per_second: 8.569, interval_steps_per_second: 5.355, epoch: 17.6[0m
[32m[2022-09-19 21:29:31,462] [    INFO][0m - loss: 6.24e-06, learning_rate: 3.4e-06, global_step: 1330, interval_runtime: 1.8681, interval_samples_per_second: 8.565, interval_steps_per_second: 5.353, epoch: 17.7333[0m
[32m[2022-09-19 21:29:33,338] [    INFO][0m - loss: 0.00077803, learning_rate: 3.2000000000000003e-06, global_step: 1340, interval_runtime: 1.8753, interval_samples_per_second: 8.532, interval_steps_per_second: 5.332, epoch: 17.8667[0m
[32m[2022-09-19 21:29:35,113] [    INFO][0m - loss: 2.608e-05, learning_rate: 3e-06, global_step: 1350, interval_runtime: 1.7752, interval_samples_per_second: 9.013, interval_steps_per_second: 5.633, epoch: 18.0[0m
[32m[2022-09-19 21:29:35,113] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 21:29:35,113] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-19 21:29:35,114] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 21:29:35,114] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 21:29:35,114] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-19 21:29:40,579] [    INFO][0m - eval_loss: 5.70653772354126, eval_accuracy: 0.5437158469945356, eval_runtime: 5.4649, eval_samples_per_second: 200.918, eval_steps_per_second: 12.626, epoch: 18.0[0m
[32m[2022-09-19 21:29:40,598] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-1350[0m
[32m[2022-09-19 21:29:40,598] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 21:29:43,116] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-1350/tokenizer_config.json[0m
[32m[2022-09-19 21:29:43,116] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-1350/special_tokens_map.json[0m
[32m[2022-09-19 21:29:49,781] [    INFO][0m - loss: 2.679e-05, learning_rate: 2.8000000000000003e-06, global_step: 1360, interval_runtime: 14.6677, interval_samples_per_second: 1.091, interval_steps_per_second: 0.682, epoch: 18.1333[0m
[32m[2022-09-19 21:29:51,652] [    INFO][0m - loss: 2.354e-05, learning_rate: 2.6e-06, global_step: 1370, interval_runtime: 1.8719, interval_samples_per_second: 8.548, interval_steps_per_second: 5.342, epoch: 18.2667[0m
[32m[2022-09-19 21:29:53,523] [    INFO][0m - loss: 0.00081714, learning_rate: 2.4000000000000003e-06, global_step: 1380, interval_runtime: 1.8705, interval_samples_per_second: 8.554, interval_steps_per_second: 5.346, epoch: 18.4[0m
[32m[2022-09-19 21:29:55,430] [    INFO][0m - loss: 0.0001195, learning_rate: 2.2e-06, global_step: 1390, interval_runtime: 1.9068, interval_samples_per_second: 8.391, interval_steps_per_second: 5.244, epoch: 18.5333[0m
[32m[2022-09-19 21:29:57,298] [    INFO][0m - loss: 4.268e-05, learning_rate: 2e-06, global_step: 1400, interval_runtime: 1.8681, interval_samples_per_second: 8.565, interval_steps_per_second: 5.353, epoch: 18.6667[0m
[32m[2022-09-19 21:29:59,166] [    INFO][0m - loss: 0.00061299, learning_rate: 1.8e-06, global_step: 1410, interval_runtime: 1.8683, interval_samples_per_second: 8.564, interval_steps_per_second: 5.353, epoch: 18.8[0m
[32m[2022-09-19 21:30:01,029] [    INFO][0m - loss: 1.648e-05, learning_rate: 1.6000000000000001e-06, global_step: 1420, interval_runtime: 1.863, interval_samples_per_second: 8.588, interval_steps_per_second: 5.368, epoch: 18.9333[0m
[32m[2022-09-19 21:30:01,863] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 21:30:01,863] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-19 21:30:01,863] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 21:30:01,863] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 21:30:01,863] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-19 21:30:07,340] [    INFO][0m - eval_loss: 5.7573723793029785, eval_accuracy: 0.5418943533697632, eval_runtime: 5.4765, eval_samples_per_second: 200.491, eval_steps_per_second: 12.599, epoch: 19.0[0m
[32m[2022-09-19 21:30:07,359] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-1425[0m
[32m[2022-09-19 21:30:07,359] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 21:30:09,844] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-1425/tokenizer_config.json[0m
[32m[2022-09-19 21:30:09,845] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-1425/special_tokens_map.json[0m
[32m[2022-09-19 21:30:15,626] [    INFO][0m - loss: 2.757e-05, learning_rate: 1.4000000000000001e-06, global_step: 1430, interval_runtime: 14.597, interval_samples_per_second: 1.096, interval_steps_per_second: 0.685, epoch: 19.0667[0m
[32m[2022-09-19 21:30:18,502] [    INFO][0m - loss: 2.009e-05, learning_rate: 1.2000000000000002e-06, global_step: 1440, interval_runtime: 2.8756, interval_samples_per_second: 5.564, interval_steps_per_second: 3.478, epoch: 19.2[0m
[32m[2022-09-19 21:30:20,368] [    INFO][0m - loss: 2.403e-05, learning_rate: 1e-06, global_step: 1450, interval_runtime: 1.8658, interval_samples_per_second: 8.575, interval_steps_per_second: 5.36, epoch: 19.3333[0m
[32m[2022-09-19 21:30:22,238] [    INFO][0m - loss: 3.909e-05, learning_rate: 8.000000000000001e-07, global_step: 1460, interval_runtime: 1.8705, interval_samples_per_second: 8.554, interval_steps_per_second: 5.346, epoch: 19.4667[0m
[32m[2022-09-19 21:30:24,114] [    INFO][0m - loss: 1.036e-05, learning_rate: 6.000000000000001e-07, global_step: 1470, interval_runtime: 1.8762, interval_samples_per_second: 8.528, interval_steps_per_second: 5.33, epoch: 19.6[0m
[32m[2022-09-19 21:30:25,983] [    INFO][0m - loss: 0.01062415, learning_rate: 4.0000000000000003e-07, global_step: 1480, interval_runtime: 1.8689, interval_samples_per_second: 8.561, interval_steps_per_second: 5.351, epoch: 19.7333[0m
[32m[2022-09-19 21:30:27,859] [    INFO][0m - loss: 1.089e-05, learning_rate: 2.0000000000000002e-07, global_step: 1490, interval_runtime: 1.8764, interval_samples_per_second: 8.527, interval_steps_per_second: 5.329, epoch: 19.8667[0m
[32m[2022-09-19 21:30:29,625] [    INFO][0m - loss: 2.056e-05, learning_rate: 0.0, global_step: 1500, interval_runtime: 1.7657, interval_samples_per_second: 9.061, interval_steps_per_second: 5.663, epoch: 20.0[0m
[32m[2022-09-19 21:30:29,626] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 21:30:29,626] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-19 21:30:29,626] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 21:30:29,626] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 21:30:29,626] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-19 21:30:35,122] [    INFO][0m - eval_loss: 5.755790710449219, eval_accuracy: 0.5418943533697632, eval_runtime: 5.4956, eval_samples_per_second: 199.798, eval_steps_per_second: 12.556, epoch: 20.0[0m
[32m[2022-09-19 21:30:35,140] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-1500[0m
[32m[2022-09-19 21:30:35,140] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 21:30:37,561] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-1500/tokenizer_config.json[0m
[32m[2022-09-19 21:30:37,562] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-1500/special_tokens_map.json[0m
[32m[2022-09-19 21:30:42,329] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-19 21:30:42,329] [    INFO][0m - Loading best model from ./checkpoints_tnews/checkpoint-150 (score: 0.5437158469945356).[0m
[32m[2022-09-19 21:30:43,757] [    INFO][0m - train_runtime: 573.4697, train_samples_per_second: 41.327, train_steps_per_second: 2.616, train_loss: 0.19679960783651526, epoch: 20.0[0m
[32m[2022-09-19 21:30:43,807] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/[0m
[32m[2022-09-19 21:30:43,808] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 21:30:46,229] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/tokenizer_config.json[0m
[32m[2022-09-19 21:30:46,229] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/special_tokens_map.json[0m
[32m[2022-09-19 21:30:46,230] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-19 21:30:46,230] [    INFO][0m -   epoch                    =       20.0[0m
[32m[2022-09-19 21:30:46,230] [    INFO][0m -   train_loss               =     0.1968[0m
[32m[2022-09-19 21:30:46,230] [    INFO][0m -   train_runtime            = 0:09:33.46[0m
[32m[2022-09-19 21:30:46,230] [    INFO][0m -   train_samples_per_second =     41.327[0m
[32m[2022-09-19 21:30:46,231] [    INFO][0m -   train_steps_per_second   =      2.616[0m
[32m[2022-09-19 21:30:46,237] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-19 21:30:46,237] [    INFO][0m -   Num examples = 2010[0m
[32m[2022-09-19 21:30:46,237] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 21:30:46,237] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 21:30:46,238] [    INFO][0m -   Total prediction steps = 126[0m
[32m[2022-09-19 21:30:56,573] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-19 21:30:56,577] [    INFO][0m -   test_accuracy           =     0.5801[0m
[32m[2022-09-19 21:30:56,577] [    INFO][0m -   test_loss               =     1.5272[0m
[32m[2022-09-19 21:30:56,578] [    INFO][0m -   test_runtime            = 0:00:10.33[0m
[32m[2022-09-19 21:30:56,578] [    INFO][0m -   test_samples_per_second =    194.474[0m
[32m[2022-09-19 21:30:56,578] [    INFO][0m -   test_steps_per_second   =     12.191[0m
[32m[2022-09-19 21:30:56,578] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-19 21:30:56,578] [    INFO][0m -   Num examples = 1500[0m
[32m[2022-09-19 21:30:56,578] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 21:30:56,578] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 21:30:56,579] [    INFO][0m -   Total prediction steps = 94[0m
[32m[2022-09-19 21:31:05,161] [    INFO][0m - Predictions for tnewsf saved to ./fewclue_submit_examples.[0m
{
  "labels": 11,
  "text_a": "\u5b69\u5b50\u8ddf\u8c01\u7761\uff0c\u5c31\u662f\u8c01\u7684\u5b69\u5b50",
  "text_b": "",
  "uid": 448
}

Prediction done.
 
==========
iflytek
==========
 
[32m[2022-09-19 21:31:30,739] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-19 21:31:30,739] [    INFO][0m - ============================================================[0m
[32m[2022-09-19 21:31:30,740] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-19 21:31:30,740] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-19 21:31:30,740] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-19 21:31:30,740] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-19 21:31:30,740] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-19 21:31:30,740] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-19 21:31:30,740] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-19 21:31:30,740] [    INFO][0m - [0m
[32m[2022-09-19 21:31:30,740] [    INFO][0m - ============================================================[0m
[32m[2022-09-19 21:31:30,740] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-19 21:31:30,740] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-19 21:31:30,740] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-19 21:31:30,741] [    INFO][0m - pretrained                    :/ssd2/wanghuijuan03/data/zero-shot/checkpoints_09191451/checkpoint-21000/model_state.pdparams[0m
[32m[2022-09-19 21:31:30,741] [    INFO][0m - prompt                        :‚Äú{'text':'text_a'}‚ÄùÂõ†Ê≠§ÔºåÂ∫îÁî®Á±ªÂà´ÊòØ{'mask'}{'mask'}„ÄÇ[0m
[32m[2022-09-19 21:31:30,741] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-19 21:31:30,741] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-19 21:31:30,741] [    INFO][0m - task_name                     :iflytek[0m
[32m[2022-09-19 21:31:30,741] [    INFO][0m - [0m
[32m[2022-09-19 21:31:30,741] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0919 21:31:30.743185 23729 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0919 21:31:30.747363 23729 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-19 21:31:35,579] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-19 21:31:35,590] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-19 21:31:35,590] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-19 21:31:35,590] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùÂõ†Ê≠§ÔºåÂ∫îÁî®Á±ªÂà´ÊòØ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': '„ÄÇ'}][0m
[32m[2022-09-19 21:31:36,868] [    INFO][0m - ============================================================[0m
[32m[2022-09-19 21:31:36,869] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-19 21:31:36,869] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-19 21:31:36,869] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-19 21:31:36,869] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-19 21:31:36,869] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-19 21:31:36,869] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-19 21:31:36,869] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-19 21:31:36,869] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-19 21:31:36,869] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-19 21:31:36,870] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-19 21:31:36,870] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-19 21:31:36,870] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-19 21:31:36,870] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-19 21:31:36,870] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-19 21:31:36,870] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-19 21:31:36,870] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-19 21:31:36,870] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-19 21:31:36,870] [    INFO][0m - eval_batch_size               :16[0m
[32m[2022-09-19 21:31:36,870] [    INFO][0m - eval_steps                    :200[0m
[32m[2022-09-19 21:31:36,870] [    INFO][0m - evaluation_strategy           :IntervalStrategy.EPOCH[0m
[32m[2022-09-19 21:31:36,870] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-19 21:31:36,870] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-19 21:31:36,871] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-19 21:31:36,871] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-19 21:31:36,871] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-19 21:31:36,871] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-19 21:31:36,871] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-19 21:31:36,871] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-19 21:31:36,871] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-19 21:31:36,871] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-09-19 21:31:36,871] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-19 21:31:36,871] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-19 21:31:36,871] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-19 21:31:36,871] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-19 21:31:36,871] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-19 21:31:36,872] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-19 21:31:36,872] [    INFO][0m - logging_dir                   :./checkpoints_iflytek/runs/Sep19_21-31-30_instance-3bwob41y-01[0m
[32m[2022-09-19 21:31:36,872] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-19 21:31:36,872] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-19 21:31:36,872] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-19 21:31:36,872] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-19 21:31:36,872] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-19 21:31:36,872] [    INFO][0m - max_seq_length                :320[0m
[32m[2022-09-19 21:31:36,872] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-19 21:31:36,872] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-19 21:31:36,872] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-19 21:31:36,872] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-19 21:31:36,872] [    INFO][0m - num_train_epochs              :20.0[0m
[32m[2022-09-19 21:31:36,872] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-19 21:31:36,873] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-19 21:31:36,873] [    INFO][0m - output_dir                    :./checkpoints_iflytek/[0m
[32m[2022-09-19 21:31:36,873] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-19 21:31:36,873] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-19 21:31:36,873] [    INFO][0m - per_device_eval_batch_size    :16[0m
[32m[2022-09-19 21:31:36,873] [    INFO][0m - per_device_train_batch_size   :16[0m
[32m[2022-09-19 21:31:36,873] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-19 21:31:36,873] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-19 21:31:36,873] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-19 21:31:36,873] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-09-19 21:31:36,873] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-19 21:31:36,873] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-19 21:31:36,873] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-19 21:31:36,874] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-19 21:31:36,874] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-19 21:31:36,874] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-19 21:31:36,874] [    INFO][0m - run_name                      :./checkpoints_iflytek/[0m
[32m[2022-09-19 21:31:36,874] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-19 21:31:36,874] [    INFO][0m - save_steps                    :200[0m
[32m[2022-09-19 21:31:36,874] [    INFO][0m - save_strategy                 :IntervalStrategy.EPOCH[0m
[32m[2022-09-19 21:31:36,874] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-19 21:31:36,874] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-19 21:31:36,874] [    INFO][0m - seed                          :42[0m
[32m[2022-09-19 21:31:36,874] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-19 21:31:36,874] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-19 21:31:36,874] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-19 21:31:36,875] [    INFO][0m - train_batch_size              :16[0m
[32m[2022-09-19 21:31:36,875] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-19 21:31:36,875] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-19 21:31:36,875] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-19 21:31:36,875] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-19 21:31:36,875] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-19 21:31:36,875] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-19 21:31:36,875] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-19 21:31:36,875] [    INFO][0m - [0m
[32m[2022-09-19 21:31:36,878] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-19 21:31:36,878] [    INFO][0m -   Num examples = 3024[0m
[32m[2022-09-19 21:31:36,878] [    INFO][0m -   Num Epochs = 20[0m
[32m[2022-09-19 21:31:36,878] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2022-09-19 21:31:36,878] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2022-09-19 21:31:36,878] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-19 21:31:36,878] [    INFO][0m -   Total optimization steps = 3780.0[0m
[32m[2022-09-19 21:31:36,878] [    INFO][0m -   Total num train samples = 60480[0m
[33m[2022-09-19 21:31:36,888] [ WARNING][0m - Token indices sequence length is longer than the specified maximum sequence length for this model (637 > 512). Running this sequence through the model will result in indexing errors[0m
[32m[2022-09-19 21:31:45,781] [    INFO][0m - loss: 3.56467896, learning_rate: 2.992063492063492e-05, global_step: 10, interval_runtime: 8.9018, interval_samples_per_second: 1.797, interval_steps_per_second: 1.123, epoch: 0.0529[0m
[32m[2022-09-19 21:31:53,665] [    INFO][0m - loss: 3.074123, learning_rate: 2.984126984126984e-05, global_step: 20, interval_runtime: 7.8837, interval_samples_per_second: 2.03, interval_steps_per_second: 1.268, epoch: 0.1058[0m
[32m[2022-09-19 21:32:01,655] [    INFO][0m - loss: 3.04026566, learning_rate: 2.9761904761904762e-05, global_step: 30, interval_runtime: 7.9903, interval_samples_per_second: 2.002, interval_steps_per_second: 1.252, epoch: 0.1587[0m
[32m[2022-09-19 21:32:09,566] [    INFO][0m - loss: 2.51518059, learning_rate: 2.9682539682539683e-05, global_step: 40, interval_runtime: 7.9113, interval_samples_per_second: 2.022, interval_steps_per_second: 1.264, epoch: 0.2116[0m
[32m[2022-09-19 21:32:17,495] [    INFO][0m - loss: 2.2788208, learning_rate: 2.96031746031746e-05, global_step: 50, interval_runtime: 7.9288, interval_samples_per_second: 2.018, interval_steps_per_second: 1.261, epoch: 0.2646[0m
[32m[2022-09-19 21:32:25,429] [    INFO][0m - loss: 2.14910202, learning_rate: 2.9523809523809523e-05, global_step: 60, interval_runtime: 7.9332, interval_samples_per_second: 2.017, interval_steps_per_second: 1.261, epoch: 0.3175[0m
[32m[2022-09-19 21:32:33,391] [    INFO][0m - loss: 2.2391737, learning_rate: 2.9444444444444445e-05, global_step: 70, interval_runtime: 7.9623, interval_samples_per_second: 2.009, interval_steps_per_second: 1.256, epoch: 0.3704[0m
[32m[2022-09-19 21:32:41,348] [    INFO][0m - loss: 2.43616772, learning_rate: 2.9365079365079366e-05, global_step: 80, interval_runtime: 7.957, interval_samples_per_second: 2.011, interval_steps_per_second: 1.257, epoch: 0.4233[0m
[32m[2022-09-19 21:32:49,327] [    INFO][0m - loss: 2.25354137, learning_rate: 2.9285714285714284e-05, global_step: 90, interval_runtime: 7.9797, interval_samples_per_second: 2.005, interval_steps_per_second: 1.253, epoch: 0.4762[0m
[32m[2022-09-19 21:32:57,308] [    INFO][0m - loss: 1.97880478, learning_rate: 2.9206349206349206e-05, global_step: 100, interval_runtime: 7.9806, interval_samples_per_second: 2.005, interval_steps_per_second: 1.253, epoch: 0.5291[0m
[32m[2022-09-19 21:33:05,288] [    INFO][0m - loss: 2.09109745, learning_rate: 2.9126984126984127e-05, global_step: 110, interval_runtime: 7.9798, interval_samples_per_second: 2.005, interval_steps_per_second: 1.253, epoch: 0.582[0m
[32m[2022-09-19 21:33:13,271] [    INFO][0m - loss: 1.9350626, learning_rate: 2.904761904761905e-05, global_step: 120, interval_runtime: 7.9834, interval_samples_per_second: 2.004, interval_steps_per_second: 1.253, epoch: 0.6349[0m
[32m[2022-09-19 21:33:21,253] [    INFO][0m - loss: 1.78673687, learning_rate: 2.8968253968253967e-05, global_step: 130, interval_runtime: 7.9815, interval_samples_per_second: 2.005, interval_steps_per_second: 1.253, epoch: 0.6878[0m
[32m[2022-09-19 21:33:29,217] [    INFO][0m - loss: 1.92865219, learning_rate: 2.8888888888888888e-05, global_step: 140, interval_runtime: 7.9642, interval_samples_per_second: 2.009, interval_steps_per_second: 1.256, epoch: 0.7407[0m
[32m[2022-09-19 21:33:37,161] [    INFO][0m - loss: 1.79095001, learning_rate: 2.880952380952381e-05, global_step: 150, interval_runtime: 7.9444, interval_samples_per_second: 2.014, interval_steps_per_second: 1.259, epoch: 0.7937[0m
[32m[2022-09-19 21:33:45,136] [    INFO][0m - loss: 1.85864525, learning_rate: 2.873015873015873e-05, global_step: 160, interval_runtime: 7.9749, interval_samples_per_second: 2.006, interval_steps_per_second: 1.254, epoch: 0.8466[0m
[32m[2022-09-19 21:33:53,100] [    INFO][0m - loss: 1.90664482, learning_rate: 2.865079365079365e-05, global_step: 170, interval_runtime: 7.9642, interval_samples_per_second: 2.009, interval_steps_per_second: 1.256, epoch: 0.8995[0m
[32m[2022-09-19 21:34:01,058] [    INFO][0m - loss: 2.15744553, learning_rate: 2.857142857142857e-05, global_step: 180, interval_runtime: 7.958, interval_samples_per_second: 2.011, interval_steps_per_second: 1.257, epoch: 0.9524[0m
[32m[2022-09-19 21:34:08,107] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 21:34:08,108] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-19 21:34:08,108] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 21:34:08,108] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 21:34:08,108] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-19 21:34:35,772] [    INFO][0m - eval_loss: 1.8808391094207764, eval_accuracy: 0.4297159504734159, eval_runtime: 27.6638, eval_samples_per_second: 49.632, eval_steps_per_second: 3.109, epoch: 1.0[0m
[32m[2022-09-19 21:34:35,796] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-189[0m
[32m[2022-09-19 21:34:35,796] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 21:34:38,636] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-189/tokenizer_config.json[0m
[32m[2022-09-19 21:34:38,637] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-189/special_tokens_map.json[0m
[32m[2022-09-19 21:34:44,872] [    INFO][0m - loss: 1.77955341, learning_rate: 2.8492063492063492e-05, global_step: 190, interval_runtime: 43.8134, interval_samples_per_second: 0.365, interval_steps_per_second: 0.228, epoch: 1.0053[0m
[32m[2022-09-19 21:34:52,798] [    INFO][0m - loss: 1.29042835, learning_rate: 2.8412698412698414e-05, global_step: 200, interval_runtime: 7.9266, interval_samples_per_second: 2.019, interval_steps_per_second: 1.262, epoch: 1.0582[0m
[32m[2022-09-19 21:35:00,780] [    INFO][0m - loss: 1.26103144, learning_rate: 2.8333333333333332e-05, global_step: 210, interval_runtime: 7.9815, interval_samples_per_second: 2.005, interval_steps_per_second: 1.253, epoch: 1.1111[0m
[32m[2022-09-19 21:35:08,758] [    INFO][0m - loss: 1.13744469, learning_rate: 2.8253968253968253e-05, global_step: 220, interval_runtime: 7.9786, interval_samples_per_second: 2.005, interval_steps_per_second: 1.253, epoch: 1.164[0m
[32m[2022-09-19 21:35:18,888] [    INFO][0m - loss: 1.4063343, learning_rate: 2.8174603174603175e-05, global_step: 230, interval_runtime: 8.0034, interval_samples_per_second: 1.999, interval_steps_per_second: 1.249, epoch: 1.2169[0m
[32m[2022-09-19 21:35:26,869] [    INFO][0m - loss: 1.25918427, learning_rate: 2.8095238095238096e-05, global_step: 240, interval_runtime: 10.1075, interval_samples_per_second: 1.583, interval_steps_per_second: 0.989, epoch: 1.2698[0m
[32m[2022-09-19 21:35:34,830] [    INFO][0m - loss: 1.48602476, learning_rate: 2.8015873015873015e-05, global_step: 250, interval_runtime: 7.9606, interval_samples_per_second: 2.01, interval_steps_per_second: 1.256, epoch: 1.3228[0m
[32m[2022-09-19 21:35:42,810] [    INFO][0m - loss: 1.62659264, learning_rate: 2.7936507936507936e-05, global_step: 260, interval_runtime: 7.98, interval_samples_per_second: 2.005, interval_steps_per_second: 1.253, epoch: 1.3757[0m
[32m[2022-09-19 21:35:50,860] [    INFO][0m - loss: 1.29978409, learning_rate: 2.7857142857142858e-05, global_step: 270, interval_runtime: 8.0497, interval_samples_per_second: 1.988, interval_steps_per_second: 1.242, epoch: 1.4286[0m
[32m[2022-09-19 21:35:58,842] [    INFO][0m - loss: 1.17875977, learning_rate: 2.777777777777778e-05, global_step: 280, interval_runtime: 7.982, interval_samples_per_second: 2.005, interval_steps_per_second: 1.253, epoch: 1.4815[0m
[32m[2022-09-19 21:36:06,828] [    INFO][0m - loss: 1.34981041, learning_rate: 2.7698412698412697e-05, global_step: 290, interval_runtime: 7.9866, interval_samples_per_second: 2.003, interval_steps_per_second: 1.252, epoch: 1.5344[0m
[32m[2022-09-19 21:36:14,850] [    INFO][0m - loss: 1.42956676, learning_rate: 2.761904761904762e-05, global_step: 300, interval_runtime: 8.0215, interval_samples_per_second: 1.995, interval_steps_per_second: 1.247, epoch: 1.5873[0m
[32m[2022-09-19 21:36:22,846] [    INFO][0m - loss: 1.48531551, learning_rate: 2.753968253968254e-05, global_step: 310, interval_runtime: 7.9967, interval_samples_per_second: 2.001, interval_steps_per_second: 1.251, epoch: 1.6402[0m
[32m[2022-09-19 21:36:30,859] [    INFO][0m - loss: 1.31742659, learning_rate: 2.7460317460317462e-05, global_step: 320, interval_runtime: 8.0129, interval_samples_per_second: 1.997, interval_steps_per_second: 1.248, epoch: 1.6931[0m
[32m[2022-09-19 21:36:38,848] [    INFO][0m - loss: 1.49747229, learning_rate: 2.738095238095238e-05, global_step: 330, interval_runtime: 7.9889, interval_samples_per_second: 2.003, interval_steps_per_second: 1.252, epoch: 1.746[0m
[32m[2022-09-19 21:36:46,851] [    INFO][0m - loss: 1.43635645, learning_rate: 2.73015873015873e-05, global_step: 340, interval_runtime: 8.0026, interval_samples_per_second: 1.999, interval_steps_per_second: 1.25, epoch: 1.7989[0m
[32m[2022-09-19 21:36:54,856] [    INFO][0m - loss: 1.20095482, learning_rate: 2.7222222222222223e-05, global_step: 350, interval_runtime: 8.0053, interval_samples_per_second: 1.999, interval_steps_per_second: 1.249, epoch: 1.8519[0m
[32m[2022-09-19 21:37:02,884] [    INFO][0m - loss: 1.3290616, learning_rate: 2.7142857142857144e-05, global_step: 360, interval_runtime: 8.028, interval_samples_per_second: 1.993, interval_steps_per_second: 1.246, epoch: 1.9048[0m
[32m[2022-09-19 21:37:10,912] [    INFO][0m - loss: 1.35846586, learning_rate: 2.7063492063492062e-05, global_step: 370, interval_runtime: 8.0281, interval_samples_per_second: 1.993, interval_steps_per_second: 1.246, epoch: 1.9577[0m
[32m[2022-09-19 21:37:17,158] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 21:37:17,159] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-19 21:37:17,159] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 21:37:17,159] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 21:37:17,159] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-19 21:37:44,732] [    INFO][0m - eval_loss: 1.79732084274292, eval_accuracy: 0.45520757465404227, eval_runtime: 27.5732, eval_samples_per_second: 49.795, eval_steps_per_second: 3.119, epoch: 2.0[0m
[32m[2022-09-19 21:37:44,757] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-378[0m
[32m[2022-09-19 21:37:44,757] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 21:37:47,481] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-378/tokenizer_config.json[0m
[32m[2022-09-19 21:37:47,481] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-378/special_tokens_map.json[0m
[32m[2022-09-19 21:37:54,380] [    INFO][0m - loss: 1.08677664, learning_rate: 2.6984126984126984e-05, global_step: 380, interval_runtime: 43.4679, interval_samples_per_second: 0.368, interval_steps_per_second: 0.23, epoch: 2.0106[0m
[32m[2022-09-19 21:38:02,355] [    INFO][0m - loss: 0.68098602, learning_rate: 2.6904761904761905e-05, global_step: 390, interval_runtime: 7.9747, interval_samples_per_second: 2.006, interval_steps_per_second: 1.254, epoch: 2.0635[0m
[32m[2022-09-19 21:38:10,338] [    INFO][0m - loss: 0.65091486, learning_rate: 2.6825396825396827e-05, global_step: 400, interval_runtime: 7.9835, interval_samples_per_second: 2.004, interval_steps_per_second: 1.253, epoch: 2.1164[0m
[32m[2022-09-19 21:38:18,295] [    INFO][0m - loss: 0.75630798, learning_rate: 2.6746031746031745e-05, global_step: 410, interval_runtime: 7.9566, interval_samples_per_second: 2.011, interval_steps_per_second: 1.257, epoch: 2.1693[0m
[32m[2022-09-19 21:38:26,290] [    INFO][0m - loss: 0.82391348, learning_rate: 2.6666666666666667e-05, global_step: 420, interval_runtime: 7.9946, interval_samples_per_second: 2.001, interval_steps_per_second: 1.251, epoch: 2.2222[0m
[32m[2022-09-19 21:38:34,293] [    INFO][0m - loss: 0.77663317, learning_rate: 2.6587301587301588e-05, global_step: 430, interval_runtime: 8.0032, interval_samples_per_second: 1.999, interval_steps_per_second: 1.249, epoch: 2.2751[0m
[32m[2022-09-19 21:38:42,288] [    INFO][0m - loss: 0.82846537, learning_rate: 2.650793650793651e-05, global_step: 440, interval_runtime: 7.9952, interval_samples_per_second: 2.001, interval_steps_per_second: 1.251, epoch: 2.328[0m
[32m[2022-09-19 21:38:50,299] [    INFO][0m - loss: 0.62012286, learning_rate: 2.6428571428571428e-05, global_step: 450, interval_runtime: 8.0103, interval_samples_per_second: 1.997, interval_steps_per_second: 1.248, epoch: 2.381[0m
[32m[2022-09-19 21:38:58,332] [    INFO][0m - loss: 0.82839117, learning_rate: 2.634920634920635e-05, global_step: 460, interval_runtime: 8.0337, interval_samples_per_second: 1.992, interval_steps_per_second: 1.245, epoch: 2.4339[0m
[32m[2022-09-19 21:39:06,357] [    INFO][0m - loss: 0.81118565, learning_rate: 2.626984126984127e-05, global_step: 470, interval_runtime: 8.0248, interval_samples_per_second: 1.994, interval_steps_per_second: 1.246, epoch: 2.4868[0m
[32m[2022-09-19 21:39:14,381] [    INFO][0m - loss: 1.01259727, learning_rate: 2.6190476190476192e-05, global_step: 480, interval_runtime: 8.0238, interval_samples_per_second: 1.994, interval_steps_per_second: 1.246, epoch: 2.5397[0m
[32m[2022-09-19 21:39:22,398] [    INFO][0m - loss: 0.87766962, learning_rate: 2.611111111111111e-05, global_step: 490, interval_runtime: 8.017, interval_samples_per_second: 1.996, interval_steps_per_second: 1.247, epoch: 2.5926[0m
[32m[2022-09-19 21:39:30,409] [    INFO][0m - loss: 0.82350979, learning_rate: 2.6031746031746032e-05, global_step: 500, interval_runtime: 8.0113, interval_samples_per_second: 1.997, interval_steps_per_second: 1.248, epoch: 2.6455[0m
[32m[2022-09-19 21:39:38,371] [    INFO][0m - loss: 0.86578884, learning_rate: 2.5952380952380953e-05, global_step: 510, interval_runtime: 7.9619, interval_samples_per_second: 2.01, interval_steps_per_second: 1.256, epoch: 2.6984[0m
[32m[2022-09-19 21:39:46,417] [    INFO][0m - loss: 0.94177618, learning_rate: 2.5873015873015875e-05, global_step: 520, interval_runtime: 8.0454, interval_samples_per_second: 1.989, interval_steps_per_second: 1.243, epoch: 2.7513[0m
[32m[2022-09-19 21:39:54,439] [    INFO][0m - loss: 0.89608688, learning_rate: 2.5793650793650793e-05, global_step: 530, interval_runtime: 8.0214, interval_samples_per_second: 1.995, interval_steps_per_second: 1.247, epoch: 2.8042[0m
[32m[2022-09-19 21:40:02,443] [    INFO][0m - loss: 0.9477458, learning_rate: 2.5714285714285714e-05, global_step: 540, interval_runtime: 8.0049, interval_samples_per_second: 1.999, interval_steps_per_second: 1.249, epoch: 2.8571[0m
[32m[2022-09-19 21:40:10,469] [    INFO][0m - loss: 0.8292243, learning_rate: 2.5634920634920636e-05, global_step: 550, interval_runtime: 8.0262, interval_samples_per_second: 1.993, interval_steps_per_second: 1.246, epoch: 2.9101[0m
[32m[2022-09-19 21:40:18,466] [    INFO][0m - loss: 0.8042038, learning_rate: 2.5555555555555557e-05, global_step: 560, interval_runtime: 7.9967, interval_samples_per_second: 2.001, interval_steps_per_second: 1.251, epoch: 2.963[0m
[32m[2022-09-19 21:40:23,896] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 21:40:23,896] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-19 21:40:23,897] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 21:40:23,897] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 21:40:23,897] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-19 21:40:51,515] [    INFO][0m - eval_loss: 2.151130437850952, eval_accuracy: 0.4493809176984705, eval_runtime: 27.6184, eval_samples_per_second: 49.713, eval_steps_per_second: 3.114, epoch: 3.0[0m
[32m[2022-09-19 21:40:51,539] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-567[0m
[32m[2022-09-19 21:40:51,540] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 21:40:54,402] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-567/tokenizer_config.json[0m
[32m[2022-09-19 21:40:54,403] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-567/special_tokens_map.json[0m
[32m[2022-09-19 21:41:02,303] [    INFO][0m - loss: 0.72570629, learning_rate: 2.5476190476190476e-05, global_step: 570, interval_runtime: 43.8373, interval_samples_per_second: 0.365, interval_steps_per_second: 0.228, epoch: 3.0159[0m
[32m[2022-09-19 21:41:10,254] [    INFO][0m - loss: 0.33083825, learning_rate: 2.5396825396825397e-05, global_step: 580, interval_runtime: 7.95, interval_samples_per_second: 2.013, interval_steps_per_second: 1.258, epoch: 3.0688[0m
[32m[2022-09-19 21:41:18,262] [    INFO][0m - loss: 0.39174902, learning_rate: 2.531746031746032e-05, global_step: 590, interval_runtime: 8.0086, interval_samples_per_second: 1.998, interval_steps_per_second: 1.249, epoch: 3.1217[0m
[32m[2022-09-19 21:41:26,282] [    INFO][0m - loss: 0.46610289, learning_rate: 2.523809523809524e-05, global_step: 600, interval_runtime: 8.0197, interval_samples_per_second: 1.995, interval_steps_per_second: 1.247, epoch: 3.1746[0m
[32m[2022-09-19 21:41:34,316] [    INFO][0m - loss: 0.56849647, learning_rate: 2.5158730158730158e-05, global_step: 610, interval_runtime: 8.0338, interval_samples_per_second: 1.992, interval_steps_per_second: 1.245, epoch: 3.2275[0m
[32m[2022-09-19 21:41:42,339] [    INFO][0m - loss: 0.4306313, learning_rate: 2.507936507936508e-05, global_step: 620, interval_runtime: 8.0237, interval_samples_per_second: 1.994, interval_steps_per_second: 1.246, epoch: 3.2804[0m
[32m[2022-09-19 21:41:50,364] [    INFO][0m - loss: 0.46752496, learning_rate: 2.5e-05, global_step: 630, interval_runtime: 8.0248, interval_samples_per_second: 1.994, interval_steps_per_second: 1.246, epoch: 3.3333[0m
[32m[2022-09-19 21:41:58,410] [    INFO][0m - loss: 0.28342063, learning_rate: 2.4920634920634923e-05, global_step: 640, interval_runtime: 8.0458, interval_samples_per_second: 1.989, interval_steps_per_second: 1.243, epoch: 3.3862[0m
[32m[2022-09-19 21:42:06,461] [    INFO][0m - loss: 0.35695667, learning_rate: 2.484126984126984e-05, global_step: 650, interval_runtime: 8.0514, interval_samples_per_second: 1.987, interval_steps_per_second: 1.242, epoch: 3.4392[0m
[32m[2022-09-19 21:42:14,490] [    INFO][0m - loss: 0.44454393, learning_rate: 2.4761904761904762e-05, global_step: 660, interval_runtime: 8.0292, interval_samples_per_second: 1.993, interval_steps_per_second: 1.245, epoch: 3.4921[0m
[32m[2022-09-19 21:42:22,499] [    INFO][0m - loss: 0.57733312, learning_rate: 2.4682539682539684e-05, global_step: 670, interval_runtime: 8.0087, interval_samples_per_second: 1.998, interval_steps_per_second: 1.249, epoch: 3.545[0m
[32m[2022-09-19 21:42:30,599] [    INFO][0m - loss: 0.52190342, learning_rate: 2.4603174603174605e-05, global_step: 680, interval_runtime: 8.0999, interval_samples_per_second: 1.975, interval_steps_per_second: 1.235, epoch: 3.5979[0m
[32m[2022-09-19 21:42:38,610] [    INFO][0m - loss: 0.50001273, learning_rate: 2.4523809523809523e-05, global_step: 690, interval_runtime: 8.0108, interval_samples_per_second: 1.997, interval_steps_per_second: 1.248, epoch: 3.6508[0m
[32m[2022-09-19 21:42:46,590] [    INFO][0m - loss: 0.52173038, learning_rate: 2.4444444444444445e-05, global_step: 700, interval_runtime: 7.981, interval_samples_per_second: 2.005, interval_steps_per_second: 1.253, epoch: 3.7037[0m
[32m[2022-09-19 21:42:54,596] [    INFO][0m - loss: 0.30652838, learning_rate: 2.4365079365079366e-05, global_step: 710, interval_runtime: 8.006, interval_samples_per_second: 1.999, interval_steps_per_second: 1.249, epoch: 3.7566[0m
[32m[2022-09-19 21:43:02,612] [    INFO][0m - loss: 0.35412881, learning_rate: 2.4285714285714288e-05, global_step: 720, interval_runtime: 8.0152, interval_samples_per_second: 1.996, interval_steps_per_second: 1.248, epoch: 3.8095[0m
[32m[2022-09-19 21:43:10,614] [    INFO][0m - loss: 0.57505083, learning_rate: 2.4206349206349206e-05, global_step: 730, interval_runtime: 8.0027, interval_samples_per_second: 1.999, interval_steps_per_second: 1.25, epoch: 3.8624[0m
[32m[2022-09-19 21:43:18,626] [    INFO][0m - loss: 0.58727789, learning_rate: 2.4126984126984128e-05, global_step: 740, interval_runtime: 8.0112, interval_samples_per_second: 1.997, interval_steps_per_second: 1.248, epoch: 3.9153[0m
[32m[2022-09-19 21:43:26,648] [    INFO][0m - loss: 0.59417372, learning_rate: 2.404761904761905e-05, global_step: 750, interval_runtime: 8.0225, interval_samples_per_second: 1.994, interval_steps_per_second: 1.246, epoch: 3.9683[0m
[32m[2022-09-19 21:43:31,267] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 21:43:31,268] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-19 21:43:31,268] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 21:43:31,268] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 21:43:31,268] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-19 21:43:59,043] [    INFO][0m - eval_loss: 2.605720281600952, eval_accuracy: 0.42607428987618357, eval_runtime: 27.7747, eval_samples_per_second: 49.434, eval_steps_per_second: 3.096, epoch: 4.0[0m
[32m[2022-09-19 21:43:59,067] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-756[0m
[32m[2022-09-19 21:43:59,067] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 21:44:02,207] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-756/tokenizer_config.json[0m
[32m[2022-09-19 21:44:02,208] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-756/special_tokens_map.json[0m
[32m[2022-09-19 21:44:10,955] [    INFO][0m - loss: 0.48619614, learning_rate: 2.396825396825397e-05, global_step: 760, interval_runtime: 44.3071, interval_samples_per_second: 0.361, interval_steps_per_second: 0.226, epoch: 4.0212[0m
[32m[2022-09-19 21:44:18,975] [    INFO][0m - loss: 0.331039, learning_rate: 2.388888888888889e-05, global_step: 770, interval_runtime: 8.0197, interval_samples_per_second: 1.995, interval_steps_per_second: 1.247, epoch: 4.0741[0m
[32m[2022-09-19 21:44:27,020] [    INFO][0m - loss: 0.16435543, learning_rate: 2.380952380952381e-05, global_step: 780, interval_runtime: 8.0452, interval_samples_per_second: 1.989, interval_steps_per_second: 1.243, epoch: 4.127[0m
[32m[2022-09-19 21:44:35,006] [    INFO][0m - loss: 0.23946764, learning_rate: 2.373015873015873e-05, global_step: 790, interval_runtime: 7.9855, interval_samples_per_second: 2.004, interval_steps_per_second: 1.252, epoch: 4.1799[0m
[32m[2022-09-19 21:44:43,045] [    INFO][0m - loss: 0.224121, learning_rate: 2.3650793650793653e-05, global_step: 800, interval_runtime: 8.0394, interval_samples_per_second: 1.99, interval_steps_per_second: 1.244, epoch: 4.2328[0m
[32m[2022-09-19 21:44:51,032] [    INFO][0m - loss: 0.17596539, learning_rate: 2.357142857142857e-05, global_step: 810, interval_runtime: 7.9865, interval_samples_per_second: 2.003, interval_steps_per_second: 1.252, epoch: 4.2857[0m
[32m[2022-09-19 21:44:59,045] [    INFO][0m - loss: 0.27077942, learning_rate: 2.3492063492063493e-05, global_step: 820, interval_runtime: 8.0137, interval_samples_per_second: 1.997, interval_steps_per_second: 1.248, epoch: 4.3386[0m
[32m[2022-09-19 21:45:07,120] [    INFO][0m - loss: 0.26214328, learning_rate: 2.3412698412698414e-05, global_step: 830, interval_runtime: 8.0739, interval_samples_per_second: 1.982, interval_steps_per_second: 1.239, epoch: 4.3915[0m
[32m[2022-09-19 21:45:15,100] [    INFO][0m - loss: 0.2420814, learning_rate: 2.3333333333333336e-05, global_step: 840, interval_runtime: 7.9804, interval_samples_per_second: 2.005, interval_steps_per_second: 1.253, epoch: 4.4444[0m
[32m[2022-09-19 21:45:23,109] [    INFO][0m - loss: 0.25321152, learning_rate: 2.3253968253968254e-05, global_step: 850, interval_runtime: 8.0096, interval_samples_per_second: 1.998, interval_steps_per_second: 1.249, epoch: 4.4974[0m
[32m[2022-09-19 21:45:31,107] [    INFO][0m - loss: 0.18308764, learning_rate: 2.3174603174603175e-05, global_step: 860, interval_runtime: 7.9978, interval_samples_per_second: 2.001, interval_steps_per_second: 1.25, epoch: 4.5503[0m
[32m[2022-09-19 21:45:39,128] [    INFO][0m - loss: 0.32902217, learning_rate: 2.3095238095238097e-05, global_step: 870, interval_runtime: 8.0208, interval_samples_per_second: 1.995, interval_steps_per_second: 1.247, epoch: 4.6032[0m
[32m[2022-09-19 21:45:47,106] [    INFO][0m - loss: 0.34261532, learning_rate: 2.301587301587302e-05, global_step: 880, interval_runtime: 7.9777, interval_samples_per_second: 2.006, interval_steps_per_second: 1.253, epoch: 4.6561[0m
[32m[2022-09-19 21:45:55,108] [    INFO][0m - loss: 0.34485879, learning_rate: 2.2936507936507937e-05, global_step: 890, interval_runtime: 8.0025, interval_samples_per_second: 1.999, interval_steps_per_second: 1.25, epoch: 4.709[0m
[32m[2022-09-19 21:46:03,090] [    INFO][0m - loss: 0.24807801, learning_rate: 2.2857142857142858e-05, global_step: 900, interval_runtime: 7.9819, interval_samples_per_second: 2.005, interval_steps_per_second: 1.253, epoch: 4.7619[0m
[32m[2022-09-19 21:46:11,131] [    INFO][0m - loss: 0.27429926, learning_rate: 2.277777777777778e-05, global_step: 910, interval_runtime: 8.0407, interval_samples_per_second: 1.99, interval_steps_per_second: 1.244, epoch: 4.8148[0m
[32m[2022-09-19 21:46:19,100] [    INFO][0m - loss: 0.33068559, learning_rate: 2.26984126984127e-05, global_step: 920, interval_runtime: 7.9698, interval_samples_per_second: 2.008, interval_steps_per_second: 1.255, epoch: 4.8677[0m
[32m[2022-09-19 21:46:27,112] [    INFO][0m - loss: 0.20519302, learning_rate: 2.261904761904762e-05, global_step: 930, interval_runtime: 8.0116, interval_samples_per_second: 1.997, interval_steps_per_second: 1.248, epoch: 4.9206[0m
[32m[2022-09-19 21:46:35,106] [    INFO][0m - loss: 0.32979703, learning_rate: 2.253968253968254e-05, global_step: 940, interval_runtime: 7.994, interval_samples_per_second: 2.001, interval_steps_per_second: 1.251, epoch: 4.9735[0m
[32m[2022-09-19 21:46:38,920] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 21:46:38,921] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-19 21:46:38,921] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 21:46:38,921] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 21:46:38,921] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-19 21:47:06,675] [    INFO][0m - eval_loss: 3.266807794570923, eval_accuracy: 0.4406409322651129, eval_runtime: 27.7541, eval_samples_per_second: 49.47, eval_steps_per_second: 3.099, epoch: 5.0[0m
[32m[2022-09-19 21:47:06,699] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-945[0m
[32m[2022-09-19 21:47:06,699] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 21:47:09,405] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-945/tokenizer_config.json[0m
[32m[2022-09-19 21:47:09,405] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-945/special_tokens_map.json[0m
[32m[2022-09-19 21:47:18,624] [    INFO][0m - loss: 0.24628713, learning_rate: 2.2460317460317462e-05, global_step: 950, interval_runtime: 43.5178, interval_samples_per_second: 0.368, interval_steps_per_second: 0.23, epoch: 5.0265[0m
[32m[2022-09-19 21:47:26,609] [    INFO][0m - loss: 0.17770352, learning_rate: 2.238095238095238e-05, global_step: 960, interval_runtime: 7.9846, interval_samples_per_second: 2.004, interval_steps_per_second: 1.252, epoch: 5.0794[0m
[32m[2022-09-19 21:47:34,580] [    INFO][0m - loss: 0.14181643, learning_rate: 2.2301587301587302e-05, global_step: 970, interval_runtime: 7.9708, interval_samples_per_second: 2.007, interval_steps_per_second: 1.255, epoch: 5.1323[0m
[32m[2022-09-19 21:47:42,549] [    INFO][0m - loss: 0.16291519, learning_rate: 2.222222222222222e-05, global_step: 980, interval_runtime: 7.97, interval_samples_per_second: 2.008, interval_steps_per_second: 1.255, epoch: 5.1852[0m
[32m[2022-09-19 21:47:50,546] [    INFO][0m - loss: 0.09729193, learning_rate: 2.2142857142857145e-05, global_step: 990, interval_runtime: 7.997, interval_samples_per_second: 2.001, interval_steps_per_second: 1.25, epoch: 5.2381[0m
[32m[2022-09-19 21:47:58,510] [    INFO][0m - loss: 0.19747179, learning_rate: 2.2063492063492063e-05, global_step: 1000, interval_runtime: 7.964, interval_samples_per_second: 2.009, interval_steps_per_second: 1.256, epoch: 5.291[0m
[32m[2022-09-19 21:48:06,498] [    INFO][0m - loss: 0.09123474, learning_rate: 2.1984126984126984e-05, global_step: 1010, interval_runtime: 7.9869, interval_samples_per_second: 2.003, interval_steps_per_second: 1.252, epoch: 5.3439[0m
[32m[2022-09-19 21:48:14,506] [    INFO][0m - loss: 0.10751179, learning_rate: 2.1904761904761903e-05, global_step: 1020, interval_runtime: 8.009, interval_samples_per_second: 1.998, interval_steps_per_second: 1.249, epoch: 5.3968[0m
[32m[2022-09-19 21:48:22,505] [    INFO][0m - loss: 0.13538482, learning_rate: 2.1825396825396827e-05, global_step: 1030, interval_runtime: 7.9983, interval_samples_per_second: 2.0, interval_steps_per_second: 1.25, epoch: 5.4497[0m
[32m[2022-09-19 21:48:30,471] [    INFO][0m - loss: 0.25129352, learning_rate: 2.1746031746031746e-05, global_step: 1040, interval_runtime: 7.9662, interval_samples_per_second: 2.008, interval_steps_per_second: 1.255, epoch: 5.5026[0m
[32m[2022-09-19 21:48:38,467] [    INFO][0m - loss: 0.28592505, learning_rate: 2.1666666666666667e-05, global_step: 1050, interval_runtime: 7.9961, interval_samples_per_second: 2.001, interval_steps_per_second: 1.251, epoch: 5.5556[0m
[32m[2022-09-19 21:48:46,492] [    INFO][0m - loss: 0.12965256, learning_rate: 2.1587301587301585e-05, global_step: 1060, interval_runtime: 8.0255, interval_samples_per_second: 1.994, interval_steps_per_second: 1.246, epoch: 5.6085[0m
[32m[2022-09-19 21:48:54,481] [    INFO][0m - loss: 0.13014838, learning_rate: 2.150793650793651e-05, global_step: 1070, interval_runtime: 7.9882, interval_samples_per_second: 2.003, interval_steps_per_second: 1.252, epoch: 5.6614[0m
[32m[2022-09-19 21:49:02,475] [    INFO][0m - loss: 0.16858402, learning_rate: 2.1428571428571428e-05, global_step: 1080, interval_runtime: 7.9937, interval_samples_per_second: 2.002, interval_steps_per_second: 1.251, epoch: 5.7143[0m
[32m[2022-09-19 21:49:10,479] [    INFO][0m - loss: 0.225296, learning_rate: 2.134920634920635e-05, global_step: 1090, interval_runtime: 8.004, interval_samples_per_second: 1.999, interval_steps_per_second: 1.249, epoch: 5.7672[0m
[32m[2022-09-19 21:49:18,550] [    INFO][0m - loss: 0.15695636, learning_rate: 2.1269841269841268e-05, global_step: 1100, interval_runtime: 8.0719, interval_samples_per_second: 1.982, interval_steps_per_second: 1.239, epoch: 5.8201[0m
[32m[2022-09-19 21:49:26,582] [    INFO][0m - loss: 0.18009675, learning_rate: 2.1190476190476193e-05, global_step: 1110, interval_runtime: 8.0312, interval_samples_per_second: 1.992, interval_steps_per_second: 1.245, epoch: 5.873[0m
[32m[2022-09-19 21:49:34,578] [    INFO][0m - loss: 0.11197618, learning_rate: 2.111111111111111e-05, global_step: 1120, interval_runtime: 7.9965, interval_samples_per_second: 2.001, interval_steps_per_second: 1.251, epoch: 5.9259[0m
[32m[2022-09-19 21:49:42,537] [    INFO][0m - loss: 0.20509443, learning_rate: 2.1031746031746032e-05, global_step: 1130, interval_runtime: 7.9585, interval_samples_per_second: 2.01, interval_steps_per_second: 1.257, epoch: 5.9788[0m
[32m[2022-09-19 21:49:45,575] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 21:49:45,576] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-19 21:49:45,576] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 21:49:45,576] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 21:49:45,576] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-19 21:50:13,136] [    INFO][0m - eval_loss: 3.781895160675049, eval_accuracy: 0.45302257829570286, eval_runtime: 27.5601, eval_samples_per_second: 49.818, eval_steps_per_second: 3.12, epoch: 6.0[0m
[32m[2022-09-19 21:50:13,160] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-1134[0m
[32m[2022-09-19 21:50:13,160] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 21:50:15,813] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-1134/tokenizer_config.json[0m
[32m[2022-09-19 21:50:15,813] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-1134/special_tokens_map.json[0m
[32m[2022-09-19 21:50:25,883] [    INFO][0m - loss: 0.12025826, learning_rate: 2.095238095238095e-05, global_step: 1140, interval_runtime: 43.3462, interval_samples_per_second: 0.369, interval_steps_per_second: 0.231, epoch: 6.0317[0m
[32m[2022-09-19 21:50:34,060] [    INFO][0m - loss: 0.09187288, learning_rate: 2.0873015873015875e-05, global_step: 1150, interval_runtime: 7.9691, interval_samples_per_second: 2.008, interval_steps_per_second: 1.255, epoch: 6.0847[0m
[32m[2022-09-19 21:50:42,150] [    INFO][0m - loss: 0.06747415, learning_rate: 2.0793650793650793e-05, global_step: 1160, interval_runtime: 8.2984, interval_samples_per_second: 1.928, interval_steps_per_second: 1.205, epoch: 6.1376[0m
[32m[2022-09-19 21:50:50,148] [    INFO][0m - loss: 0.21416712, learning_rate: 2.0714285714285715e-05, global_step: 1170, interval_runtime: 7.9979, interval_samples_per_second: 2.001, interval_steps_per_second: 1.25, epoch: 6.1905[0m
[32m[2022-09-19 21:50:58,138] [    INFO][0m - loss: 0.0435296, learning_rate: 2.0634920634920633e-05, global_step: 1180, interval_runtime: 7.9897, interval_samples_per_second: 2.003, interval_steps_per_second: 1.252, epoch: 6.2434[0m
[32m[2022-09-19 21:51:06,138] [    INFO][0m - loss: 0.16240361, learning_rate: 2.0555555555555558e-05, global_step: 1190, interval_runtime: 7.9999, interval_samples_per_second: 2.0, interval_steps_per_second: 1.25, epoch: 6.2963[0m
[32m[2022-09-19 21:51:14,150] [    INFO][0m - loss: 0.02965763, learning_rate: 2.0476190476190476e-05, global_step: 1200, interval_runtime: 8.0123, interval_samples_per_second: 1.997, interval_steps_per_second: 1.248, epoch: 6.3492[0m
[32m[2022-09-19 21:51:22,131] [    INFO][0m - loss: 0.18845737, learning_rate: 2.0396825396825398e-05, global_step: 1210, interval_runtime: 7.9811, interval_samples_per_second: 2.005, interval_steps_per_second: 1.253, epoch: 6.4021[0m
[32m[2022-09-19 21:51:30,153] [    INFO][0m - loss: 0.07328476, learning_rate: 2.0317460317460316e-05, global_step: 1220, interval_runtime: 8.0211, interval_samples_per_second: 1.995, interval_steps_per_second: 1.247, epoch: 6.455[0m
[32m[2022-09-19 21:51:38,126] [    INFO][0m - loss: 0.06861877, learning_rate: 2.023809523809524e-05, global_step: 1230, interval_runtime: 7.974, interval_samples_per_second: 2.007, interval_steps_per_second: 1.254, epoch: 6.5079[0m
[32m[2022-09-19 21:51:46,108] [    INFO][0m - loss: 0.08732034, learning_rate: 2.015873015873016e-05, global_step: 1240, interval_runtime: 7.981, interval_samples_per_second: 2.005, interval_steps_per_second: 1.253, epoch: 6.5608[0m
[32m[2022-09-19 21:51:54,114] [    INFO][0m - loss: 0.06144071, learning_rate: 2.007936507936508e-05, global_step: 1250, interval_runtime: 8.0064, interval_samples_per_second: 1.998, interval_steps_per_second: 1.249, epoch: 6.6138[0m
[32m[2022-09-19 21:52:02,077] [    INFO][0m - loss: 0.08016797, learning_rate: 1.9999999999999998e-05, global_step: 1260, interval_runtime: 7.9631, interval_samples_per_second: 2.009, interval_steps_per_second: 1.256, epoch: 6.6667[0m
[32m[2022-09-19 21:52:10,096] [    INFO][0m - loss: 0.07792116, learning_rate: 1.9920634920634923e-05, global_step: 1270, interval_runtime: 8.0195, interval_samples_per_second: 1.995, interval_steps_per_second: 1.247, epoch: 6.7196[0m
[32m[2022-09-19 21:52:18,165] [    INFO][0m - loss: 0.16299968, learning_rate: 1.984126984126984e-05, global_step: 1280, interval_runtime: 8.0686, interval_samples_per_second: 1.983, interval_steps_per_second: 1.239, epoch: 6.7725[0m
[32m[2022-09-19 21:52:26,163] [    INFO][0m - loss: 0.05079406, learning_rate: 1.9761904761904763e-05, global_step: 1290, interval_runtime: 7.9984, interval_samples_per_second: 2.0, interval_steps_per_second: 1.25, epoch: 6.8254[0m
[32m[2022-09-19 21:52:34,151] [    INFO][0m - loss: 0.24976838, learning_rate: 1.968253968253968e-05, global_step: 1300, interval_runtime: 7.9879, interval_samples_per_second: 2.003, interval_steps_per_second: 1.252, epoch: 6.8783[0m
[32m[2022-09-19 21:52:42,121] [    INFO][0m - loss: 0.14226658, learning_rate: 1.9603174603174606e-05, global_step: 1310, interval_runtime: 7.9701, interval_samples_per_second: 2.008, interval_steps_per_second: 1.255, epoch: 6.9312[0m
[32m[2022-09-19 21:52:50,050] [    INFO][0m - loss: 0.11998819, learning_rate: 1.9523809523809524e-05, global_step: 1320, interval_runtime: 7.9284, interval_samples_per_second: 2.018, interval_steps_per_second: 1.261, epoch: 6.9841[0m
[32m[2022-09-19 21:52:52,327] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 21:52:52,327] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-19 21:52:52,327] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 21:52:52,327] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 21:52:52,327] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-19 21:53:19,873] [    INFO][0m - eval_loss: 4.235479354858398, eval_accuracy: 0.44136926438455937, eval_runtime: 27.5457, eval_samples_per_second: 49.844, eval_steps_per_second: 3.122, epoch: 7.0[0m
[32m[2022-09-19 21:53:19,897] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-1323[0m
[32m[2022-09-19 21:53:19,897] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 21:53:22,540] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-1323/tokenizer_config.json[0m
[32m[2022-09-19 21:53:22,540] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-1323/special_tokens_map.json[0m
[32m[2022-09-19 21:53:33,325] [    INFO][0m - loss: 0.04490519, learning_rate: 1.9444444444444445e-05, global_step: 1330, interval_runtime: 43.2753, interval_samples_per_second: 0.37, interval_steps_per_second: 0.231, epoch: 7.037[0m
[32m[2022-09-19 21:53:41,298] [    INFO][0m - loss: 0.03357151, learning_rate: 1.9365079365079363e-05, global_step: 1340, interval_runtime: 7.9728, interval_samples_per_second: 2.007, interval_steps_per_second: 1.254, epoch: 7.0899[0m
[32m[2022-09-19 21:53:49,313] [    INFO][0m - loss: 0.0348244, learning_rate: 1.928571428571429e-05, global_step: 1350, interval_runtime: 8.0149, interval_samples_per_second: 1.996, interval_steps_per_second: 1.248, epoch: 7.1429[0m
[32m[2022-09-19 21:53:57,286] [    INFO][0m - loss: 0.08072593, learning_rate: 1.9206349206349206e-05, global_step: 1360, interval_runtime: 7.9734, interval_samples_per_second: 2.007, interval_steps_per_second: 1.254, epoch: 7.1958[0m
[32m[2022-09-19 21:54:05,266] [    INFO][0m - loss: 0.05971143, learning_rate: 1.9126984126984128e-05, global_step: 1370, interval_runtime: 7.9802, interval_samples_per_second: 2.005, interval_steps_per_second: 1.253, epoch: 7.2487[0m
[32m[2022-09-19 21:54:13,274] [    INFO][0m - loss: 0.0308679, learning_rate: 1.9047619047619046e-05, global_step: 1380, interval_runtime: 8.0076, interval_samples_per_second: 1.998, interval_steps_per_second: 1.249, epoch: 7.3016[0m
[32m[2022-09-19 21:54:21,280] [    INFO][0m - loss: 0.15801241, learning_rate: 1.896825396825397e-05, global_step: 1390, interval_runtime: 8.0055, interval_samples_per_second: 1.999, interval_steps_per_second: 1.249, epoch: 7.3545[0m
[32m[2022-09-19 21:54:29,261] [    INFO][0m - loss: 0.01426649, learning_rate: 1.888888888888889e-05, global_step: 1400, interval_runtime: 7.9819, interval_samples_per_second: 2.005, interval_steps_per_second: 1.253, epoch: 7.4074[0m
[32m[2022-09-19 21:54:37,255] [    INFO][0m - loss: 0.08868921, learning_rate: 1.880952380952381e-05, global_step: 1410, interval_runtime: 7.994, interval_samples_per_second: 2.001, interval_steps_per_second: 1.251, epoch: 7.4603[0m
[32m[2022-09-19 21:54:45,268] [    INFO][0m - loss: 0.11842165, learning_rate: 1.873015873015873e-05, global_step: 1420, interval_runtime: 8.0125, interval_samples_per_second: 1.997, interval_steps_per_second: 1.248, epoch: 7.5132[0m
[32m[2022-09-19 21:54:53,261] [    INFO][0m - loss: 0.06731367, learning_rate: 1.8650793650793654e-05, global_step: 1430, interval_runtime: 7.9929, interval_samples_per_second: 2.002, interval_steps_per_second: 1.251, epoch: 7.5661[0m
[32m[2022-09-19 21:55:01,294] [    INFO][0m - loss: 0.06848993, learning_rate: 1.8571428571428572e-05, global_step: 1440, interval_runtime: 8.0334, interval_samples_per_second: 1.992, interval_steps_per_second: 1.245, epoch: 7.619[0m
[32m[2022-09-19 21:55:09,344] [    INFO][0m - loss: 0.09595018, learning_rate: 1.8492063492063493e-05, global_step: 1450, interval_runtime: 8.0497, interval_samples_per_second: 1.988, interval_steps_per_second: 1.242, epoch: 7.672[0m
[32m[2022-09-19 21:55:17,327] [    INFO][0m - loss: 0.05850415, learning_rate: 1.841269841269841e-05, global_step: 1460, interval_runtime: 7.9826, interval_samples_per_second: 2.004, interval_steps_per_second: 1.253, epoch: 7.7249[0m
[32m[2022-09-19 21:55:25,316] [    INFO][0m - loss: 0.09264564, learning_rate: 1.8333333333333336e-05, global_step: 1470, interval_runtime: 7.9894, interval_samples_per_second: 2.003, interval_steps_per_second: 1.252, epoch: 7.7778[0m
[32m[2022-09-19 21:55:33,340] [    INFO][0m - loss: 0.12123735, learning_rate: 1.8253968253968254e-05, global_step: 1480, interval_runtime: 8.0241, interval_samples_per_second: 1.994, interval_steps_per_second: 1.246, epoch: 7.8307[0m
[32m[2022-09-19 21:55:41,368] [    INFO][0m - loss: 0.10825452, learning_rate: 1.8174603174603176e-05, global_step: 1490, interval_runtime: 8.0281, interval_samples_per_second: 1.993, interval_steps_per_second: 1.246, epoch: 7.8836[0m
[32m[2022-09-19 21:55:49,360] [    INFO][0m - loss: 0.14802722, learning_rate: 1.8095238095238094e-05, global_step: 1500, interval_runtime: 7.992, interval_samples_per_second: 2.002, interval_steps_per_second: 1.251, epoch: 7.9365[0m
[32m[2022-09-19 21:55:57,257] [    INFO][0m - loss: 0.09966832, learning_rate: 1.801587301587302e-05, global_step: 1510, interval_runtime: 7.8965, interval_samples_per_second: 2.026, interval_steps_per_second: 1.266, epoch: 7.9894[0m
[32m[2022-09-19 21:55:58,774] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 21:55:58,774] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-19 21:55:58,774] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 21:55:58,774] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 21:55:58,774] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-19 21:56:26,271] [    INFO][0m - eval_loss: 4.393617153167725, eval_accuracy: 0.42753095411507647, eval_runtime: 27.4968, eval_samples_per_second: 49.933, eval_steps_per_second: 3.128, epoch: 8.0[0m
[32m[2022-09-19 21:56:26,295] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-1512[0m
[32m[2022-09-19 21:56:26,295] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 21:56:28,922] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-1512/tokenizer_config.json[0m
[32m[2022-09-19 21:56:28,923] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-1512/special_tokens_map.json[0m
[32m[2022-09-19 21:56:40,588] [    INFO][0m - loss: 0.08563672, learning_rate: 1.7936507936507937e-05, global_step: 1520, interval_runtime: 43.3311, interval_samples_per_second: 0.369, interval_steps_per_second: 0.231, epoch: 8.0423[0m
[32m[2022-09-19 21:56:48,559] [    INFO][0m - loss: 0.09108961, learning_rate: 1.785714285714286e-05, global_step: 1530, interval_runtime: 7.9715, interval_samples_per_second: 2.007, interval_steps_per_second: 1.254, epoch: 8.0952[0m
[32m[2022-09-19 21:56:56,538] [    INFO][0m - loss: 0.02790735, learning_rate: 1.7777777777777777e-05, global_step: 1540, interval_runtime: 7.9784, interval_samples_per_second: 2.005, interval_steps_per_second: 1.253, epoch: 8.1481[0m
[32m[2022-09-19 21:57:04,548] [    INFO][0m - loss: 0.11412146, learning_rate: 1.76984126984127e-05, global_step: 1550, interval_runtime: 8.0103, interval_samples_per_second: 1.997, interval_steps_per_second: 1.248, epoch: 8.2011[0m
[32m[2022-09-19 21:57:12,528] [    INFO][0m - loss: 0.01987873, learning_rate: 1.761904761904762e-05, global_step: 1560, interval_runtime: 7.98, interval_samples_per_second: 2.005, interval_steps_per_second: 1.253, epoch: 8.254[0m
[32m[2022-09-19 21:57:20,494] [    INFO][0m - loss: 0.05959419, learning_rate: 1.753968253968254e-05, global_step: 1570, interval_runtime: 7.9658, interval_samples_per_second: 2.009, interval_steps_per_second: 1.255, epoch: 8.3069[0m
[32m[2022-09-19 21:57:28,545] [    INFO][0m - loss: 0.05229046, learning_rate: 1.746031746031746e-05, global_step: 1580, interval_runtime: 8.0513, interval_samples_per_second: 1.987, interval_steps_per_second: 1.242, epoch: 8.3598[0m
[32m[2022-09-19 21:57:36,571] [    INFO][0m - loss: 0.05892994, learning_rate: 1.7380952380952384e-05, global_step: 1590, interval_runtime: 8.0257, interval_samples_per_second: 1.994, interval_steps_per_second: 1.246, epoch: 8.4127[0m
[32m[2022-09-19 21:57:44,520] [    INFO][0m - loss: 0.05977581, learning_rate: 1.7301587301587302e-05, global_step: 1600, interval_runtime: 7.9491, interval_samples_per_second: 2.013, interval_steps_per_second: 1.258, epoch: 8.4656[0m
[32m[2022-09-19 21:57:52,519] [    INFO][0m - loss: 0.1319972, learning_rate: 1.7222222222222224e-05, global_step: 1610, interval_runtime: 7.999, interval_samples_per_second: 2.0, interval_steps_per_second: 1.25, epoch: 8.5185[0m
[32m[2022-09-19 21:58:00,500] [    INFO][0m - loss: 0.14655192, learning_rate: 1.7142857142857142e-05, global_step: 1620, interval_runtime: 7.9805, interval_samples_per_second: 2.005, interval_steps_per_second: 1.253, epoch: 8.5714[0m
[32m[2022-09-19 21:58:08,565] [    INFO][0m - loss: 0.01830368, learning_rate: 1.7063492063492067e-05, global_step: 1630, interval_runtime: 8.0652, interval_samples_per_second: 1.984, interval_steps_per_second: 1.24, epoch: 8.6243[0m
[32m[2022-09-19 21:58:16,550] [    INFO][0m - loss: 0.08391003, learning_rate: 1.6984126984126985e-05, global_step: 1640, interval_runtime: 7.9854, interval_samples_per_second: 2.004, interval_steps_per_second: 1.252, epoch: 8.6772[0m
[32m[2022-09-19 21:58:24,559] [    INFO][0m - loss: 0.08553384, learning_rate: 1.6904761904761906e-05, global_step: 1650, interval_runtime: 8.0088, interval_samples_per_second: 1.998, interval_steps_per_second: 1.249, epoch: 8.7302[0m
[32m[2022-09-19 21:58:32,547] [    INFO][0m - loss: 0.01745359, learning_rate: 1.6825396825396824e-05, global_step: 1660, interval_runtime: 7.988, interval_samples_per_second: 2.003, interval_steps_per_second: 1.252, epoch: 8.7831[0m
[32m[2022-09-19 21:58:40,582] [    INFO][0m - loss: 0.02410215, learning_rate: 1.674603174603175e-05, global_step: 1670, interval_runtime: 8.0352, interval_samples_per_second: 1.991, interval_steps_per_second: 1.245, epoch: 8.836[0m
[32m[2022-09-19 21:58:48,573] [    INFO][0m - loss: 0.02283885, learning_rate: 1.6666666666666667e-05, global_step: 1680, interval_runtime: 7.9904, interval_samples_per_second: 2.002, interval_steps_per_second: 1.251, epoch: 8.8889[0m
[32m[2022-09-19 21:58:56,588] [    INFO][0m - loss: 0.06179339, learning_rate: 1.658730158730159e-05, global_step: 1690, interval_runtime: 8.0154, interval_samples_per_second: 1.996, interval_steps_per_second: 1.248, epoch: 8.9418[0m
[32m[2022-09-19 21:59:04,461] [    INFO][0m - loss: 0.02695769, learning_rate: 1.6507936507936507e-05, global_step: 1700, interval_runtime: 7.8726, interval_samples_per_second: 2.032, interval_steps_per_second: 1.27, epoch: 8.9947[0m
[32m[2022-09-19 21:59:05,217] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 21:59:05,217] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-19 21:59:05,217] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 21:59:05,217] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 21:59:05,217] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-19 21:59:32,758] [    INFO][0m - eval_loss: 4.575902462005615, eval_accuracy: 0.4435542607428988, eval_runtime: 27.5406, eval_samples_per_second: 49.854, eval_steps_per_second: 3.123, epoch: 9.0[0m
[32m[2022-09-19 21:59:32,782] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-1701[0m
[32m[2022-09-19 21:59:32,783] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 21:59:35,446] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-1701/tokenizer_config.json[0m
[32m[2022-09-19 21:59:35,446] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-1701/special_tokens_map.json[0m
[32m[2022-09-19 21:59:47,865] [    INFO][0m - loss: 0.01583955, learning_rate: 1.6428571428571432e-05, global_step: 1710, interval_runtime: 43.4048, interval_samples_per_second: 0.369, interval_steps_per_second: 0.23, epoch: 9.0476[0m
[32m[2022-09-19 21:59:55,845] [    INFO][0m - loss: 0.03917543, learning_rate: 1.634920634920635e-05, global_step: 1720, interval_runtime: 7.9791, interval_samples_per_second: 2.005, interval_steps_per_second: 1.253, epoch: 9.1005[0m
[32m[2022-09-19 22:00:03,812] [    INFO][0m - loss: 0.03212941, learning_rate: 1.626984126984127e-05, global_step: 1730, interval_runtime: 7.967, interval_samples_per_second: 2.008, interval_steps_per_second: 1.255, epoch: 9.1534[0m
[32m[2022-09-19 22:00:11,813] [    INFO][0m - loss: 0.01187116, learning_rate: 1.619047619047619e-05, global_step: 1740, interval_runtime: 8.001, interval_samples_per_second: 2.0, interval_steps_per_second: 1.25, epoch: 9.2063[0m
[32m[2022-09-19 22:00:19,811] [    INFO][0m - loss: 0.00386662, learning_rate: 1.6111111111111115e-05, global_step: 1750, interval_runtime: 7.998, interval_samples_per_second: 2.0, interval_steps_per_second: 1.25, epoch: 9.2593[0m
[32m[2022-09-19 22:00:27,811] [    INFO][0m - loss: 0.06698177, learning_rate: 1.6031746031746033e-05, global_step: 1760, interval_runtime: 8.0003, interval_samples_per_second: 2.0, interval_steps_per_second: 1.25, epoch: 9.3122[0m
[32m[2022-09-19 22:00:35,821] [    INFO][0m - loss: 0.06747466, learning_rate: 1.5952380952380954e-05, global_step: 1770, interval_runtime: 8.0098, interval_samples_per_second: 1.998, interval_steps_per_second: 1.248, epoch: 9.3651[0m
[32m[2022-09-19 22:00:43,838] [    INFO][0m - loss: 0.0195053, learning_rate: 1.5873015873015872e-05, global_step: 1780, interval_runtime: 8.017, interval_samples_per_second: 1.996, interval_steps_per_second: 1.247, epoch: 9.418[0m
[32m[2022-09-19 22:00:51,853] [    INFO][0m - loss: 0.02023562, learning_rate: 1.5793650793650797e-05, global_step: 1790, interval_runtime: 8.0155, interval_samples_per_second: 1.996, interval_steps_per_second: 1.248, epoch: 9.4709[0m
[32m[2022-09-19 22:00:59,851] [    INFO][0m - loss: 0.00227428, learning_rate: 1.5714285714285715e-05, global_step: 1800, interval_runtime: 7.998, interval_samples_per_second: 2.001, interval_steps_per_second: 1.25, epoch: 9.5238[0m
[32m[2022-09-19 22:01:07,846] [    INFO][0m - loss: 0.02439472, learning_rate: 1.5634920634920637e-05, global_step: 1810, interval_runtime: 7.9951, interval_samples_per_second: 2.001, interval_steps_per_second: 1.251, epoch: 9.5767[0m
[32m[2022-09-19 22:01:15,819] [    INFO][0m - loss: 0.0047115, learning_rate: 1.5555555555555555e-05, global_step: 1820, interval_runtime: 7.9727, interval_samples_per_second: 2.007, interval_steps_per_second: 1.254, epoch: 9.6296[0m
[32m[2022-09-19 22:01:23,821] [    INFO][0m - loss: 0.02141319, learning_rate: 1.547619047619048e-05, global_step: 1830, interval_runtime: 8.0022, interval_samples_per_second: 1.999, interval_steps_per_second: 1.25, epoch: 9.6825[0m
[32m[2022-09-19 22:01:31,806] [    INFO][0m - loss: 0.10865189, learning_rate: 1.5396825396825398e-05, global_step: 1840, interval_runtime: 7.9847, interval_samples_per_second: 2.004, interval_steps_per_second: 1.252, epoch: 9.7354[0m
[32m[2022-09-19 22:01:39,798] [    INFO][0m - loss: 0.10052085, learning_rate: 1.531746031746032e-05, global_step: 1850, interval_runtime: 7.9924, interval_samples_per_second: 2.002, interval_steps_per_second: 1.251, epoch: 9.7884[0m
[32m[2022-09-19 22:01:47,805] [    INFO][0m - loss: 0.00105295, learning_rate: 1.5238095238095238e-05, global_step: 1860, interval_runtime: 8.0066, interval_samples_per_second: 1.998, interval_steps_per_second: 1.249, epoch: 9.8413[0m
[32m[2022-09-19 22:01:55,806] [    INFO][0m - loss: 0.06825877, learning_rate: 1.515873015873016e-05, global_step: 1870, interval_runtime: 8.0007, interval_samples_per_second: 2.0, interval_steps_per_second: 1.25, epoch: 9.8942[0m
[32m[2022-09-19 22:02:03,818] [    INFO][0m - loss: 0.0747418, learning_rate: 1.507936507936508e-05, global_step: 1880, interval_runtime: 8.0125, interval_samples_per_second: 1.997, interval_steps_per_second: 1.248, epoch: 9.9471[0m
[32m[2022-09-19 22:02:11,611] [    INFO][0m - loss: 0.0165357, learning_rate: 1.5e-05, global_step: 1890, interval_runtime: 7.793, interval_samples_per_second: 2.053, interval_steps_per_second: 1.283, epoch: 10.0[0m
[32m[2022-09-19 22:02:11,612] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 22:02:11,612] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-19 22:02:11,612] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 22:02:11,612] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 22:02:11,612] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-19 22:02:39,349] [    INFO][0m - eval_loss: 4.591861724853516, eval_accuracy: 0.4435542607428988, eval_runtime: 27.7364, eval_samples_per_second: 49.502, eval_steps_per_second: 3.101, epoch: 10.0[0m
[32m[2022-09-19 22:02:39,377] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-1890[0m
[32m[2022-09-19 22:02:39,378] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 22:02:42,252] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-1890/tokenizer_config.json[0m
[32m[2022-09-19 22:02:42,253] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-1890/special_tokens_map.json[0m
[32m[2022-09-19 22:02:55,873] [    INFO][0m - loss: 0.07377184, learning_rate: 1.492063492063492e-05, global_step: 1900, interval_runtime: 44.2615, interval_samples_per_second: 0.361, interval_steps_per_second: 0.226, epoch: 10.0529[0m
[32m[2022-09-19 22:03:03,841] [    INFO][0m - loss: 0.02868834, learning_rate: 1.4841269841269842e-05, global_step: 1910, interval_runtime: 7.9684, interval_samples_per_second: 2.008, interval_steps_per_second: 1.255, epoch: 10.1058[0m
[32m[2022-09-19 22:03:11,832] [    INFO][0m - loss: 0.00116072, learning_rate: 1.4761904761904761e-05, global_step: 1920, interval_runtime: 7.9909, interval_samples_per_second: 2.002, interval_steps_per_second: 1.251, epoch: 10.1587[0m
[32m[2022-09-19 22:03:19,852] [    INFO][0m - loss: 0.01007735, learning_rate: 1.4682539682539683e-05, global_step: 1930, interval_runtime: 8.0194, interval_samples_per_second: 1.995, interval_steps_per_second: 1.247, epoch: 10.2116[0m
[32m[2022-09-19 22:03:27,853] [    INFO][0m - loss: 0.09557722, learning_rate: 1.4603174603174603e-05, global_step: 1940, interval_runtime: 8.0019, interval_samples_per_second: 2.0, interval_steps_per_second: 1.25, epoch: 10.2646[0m
[32m[2022-09-19 22:03:35,876] [    INFO][0m - loss: 0.02978528, learning_rate: 1.4523809523809524e-05, global_step: 1950, interval_runtime: 8.0219, interval_samples_per_second: 1.995, interval_steps_per_second: 1.247, epoch: 10.3175[0m
[32m[2022-09-19 22:03:43,842] [    INFO][0m - loss: 0.06686135, learning_rate: 1.4444444444444444e-05, global_step: 1960, interval_runtime: 7.9671, interval_samples_per_second: 2.008, interval_steps_per_second: 1.255, epoch: 10.3704[0m
[32m[2022-09-19 22:03:51,889] [    INFO][0m - loss: 0.07614474, learning_rate: 1.4365079365079366e-05, global_step: 1970, interval_runtime: 8.0464, interval_samples_per_second: 1.988, interval_steps_per_second: 1.243, epoch: 10.4233[0m
[32m[2022-09-19 22:03:59,886] [    INFO][0m - loss: 0.00141922, learning_rate: 1.4285714285714285e-05, global_step: 1980, interval_runtime: 7.9976, interval_samples_per_second: 2.001, interval_steps_per_second: 1.25, epoch: 10.4762[0m
[32m[2022-09-19 22:04:07,891] [    INFO][0m - loss: 0.00178582, learning_rate: 1.4206349206349207e-05, global_step: 1990, interval_runtime: 8.0045, interval_samples_per_second: 1.999, interval_steps_per_second: 1.249, epoch: 10.5291[0m
[32m[2022-09-19 22:04:15,917] [    INFO][0m - loss: 0.03942381, learning_rate: 1.4126984126984127e-05, global_step: 2000, interval_runtime: 8.0261, interval_samples_per_second: 1.993, interval_steps_per_second: 1.246, epoch: 10.582[0m
[32m[2022-09-19 22:04:23,953] [    INFO][0m - loss: 0.00404711, learning_rate: 1.4047619047619048e-05, global_step: 2010, interval_runtime: 8.036, interval_samples_per_second: 1.991, interval_steps_per_second: 1.244, epoch: 10.6349[0m
[32m[2022-09-19 22:04:31,945] [    INFO][0m - loss: 0.00278063, learning_rate: 1.3968253968253968e-05, global_step: 2020, interval_runtime: 7.9919, interval_samples_per_second: 2.002, interval_steps_per_second: 1.251, epoch: 10.6878[0m
[32m[2022-09-19 22:04:39,927] [    INFO][0m - loss: 0.06290131, learning_rate: 1.388888888888889e-05, global_step: 2030, interval_runtime: 7.9823, interval_samples_per_second: 2.004, interval_steps_per_second: 1.253, epoch: 10.7407[0m
[32m[2022-09-19 22:04:47,942] [    INFO][0m - loss: 0.00812122, learning_rate: 1.380952380952381e-05, global_step: 2040, interval_runtime: 8.0149, interval_samples_per_second: 1.996, interval_steps_per_second: 1.248, epoch: 10.7937[0m
[32m[2022-09-19 22:04:55,934] [    INFO][0m - loss: 0.04740326, learning_rate: 1.3730158730158731e-05, global_step: 2050, interval_runtime: 7.9917, interval_samples_per_second: 2.002, interval_steps_per_second: 1.251, epoch: 10.8466[0m
[32m[2022-09-19 22:05:03,905] [    INFO][0m - loss: 0.00732928, learning_rate: 1.365079365079365e-05, global_step: 2060, interval_runtime: 7.9711, interval_samples_per_second: 2.007, interval_steps_per_second: 1.255, epoch: 10.8995[0m
[32m[2022-09-19 22:05:11,918] [    INFO][0m - loss: 0.01948284, learning_rate: 1.3571428571428572e-05, global_step: 2070, interval_runtime: 8.0127, interval_samples_per_second: 1.997, interval_steps_per_second: 1.248, epoch: 10.9524[0m
[32m[2022-09-19 22:05:18,905] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 22:05:18,906] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-19 22:05:18,906] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 22:05:18,906] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 22:05:18,906] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-19 22:05:46,449] [    INFO][0m - eval_loss: 4.84943962097168, eval_accuracy: 0.4450109249817917, eval_runtime: 27.5427, eval_samples_per_second: 49.85, eval_steps_per_second: 3.122, epoch: 11.0[0m
[32m[2022-09-19 22:05:46,473] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-2079[0m
[32m[2022-09-19 22:05:46,473] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 22:05:49,114] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-2079/tokenizer_config.json[0m
[32m[2022-09-19 22:05:49,114] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-2079/special_tokens_map.json[0m
[32m[2022-09-19 22:05:55,086] [    INFO][0m - loss: 0.00064026, learning_rate: 1.3492063492063492e-05, global_step: 2080, interval_runtime: 43.1682, interval_samples_per_second: 0.371, interval_steps_per_second: 0.232, epoch: 11.0053[0m
[32m[2022-09-19 22:06:03,026] [    INFO][0m - loss: 0.01467742, learning_rate: 1.3412698412698413e-05, global_step: 2090, interval_runtime: 7.9398, interval_samples_per_second: 2.015, interval_steps_per_second: 1.259, epoch: 11.0582[0m
[32m[2022-09-19 22:06:11,028] [    INFO][0m - loss: 0.00623029, learning_rate: 1.3333333333333333e-05, global_step: 2100, interval_runtime: 8.0023, interval_samples_per_second: 1.999, interval_steps_per_second: 1.25, epoch: 11.1111[0m
[32m[2022-09-19 22:06:19,041] [    INFO][0m - loss: 0.00697062, learning_rate: 1.3253968253968255e-05, global_step: 2110, interval_runtime: 8.0127, interval_samples_per_second: 1.997, interval_steps_per_second: 1.248, epoch: 11.164[0m
[32m[2022-09-19 22:06:27,021] [    INFO][0m - loss: 0.00028363, learning_rate: 1.3174603174603175e-05, global_step: 2120, interval_runtime: 7.98, interval_samples_per_second: 2.005, interval_steps_per_second: 1.253, epoch: 11.2169[0m
[32m[2022-09-19 22:06:35,046] [    INFO][0m - loss: 0.00273353, learning_rate: 1.3095238095238096e-05, global_step: 2130, interval_runtime: 8.025, interval_samples_per_second: 1.994, interval_steps_per_second: 1.246, epoch: 11.2698[0m
[32m[2022-09-19 22:06:43,025] [    INFO][0m - loss: 0.01249199, learning_rate: 1.3015873015873016e-05, global_step: 2140, interval_runtime: 7.9791, interval_samples_per_second: 2.005, interval_steps_per_second: 1.253, epoch: 11.3228[0m
[32m[2022-09-19 22:06:51,052] [    INFO][0m - loss: 0.00324782, learning_rate: 1.2936507936507937e-05, global_step: 2150, interval_runtime: 8.0274, interval_samples_per_second: 1.993, interval_steps_per_second: 1.246, epoch: 11.3757[0m
[32m[2022-09-19 22:06:59,061] [    INFO][0m - loss: 0.01553965, learning_rate: 1.2857142857142857e-05, global_step: 2160, interval_runtime: 8.0086, interval_samples_per_second: 1.998, interval_steps_per_second: 1.249, epoch: 11.4286[0m
[32m[2022-09-19 22:07:07,075] [    INFO][0m - loss: 0.03130051, learning_rate: 1.2777777777777779e-05, global_step: 2170, interval_runtime: 8.0144, interval_samples_per_second: 1.996, interval_steps_per_second: 1.248, epoch: 11.4815[0m
[32m[2022-09-19 22:07:15,083] [    INFO][0m - loss: 0.00471226, learning_rate: 1.2698412698412699e-05, global_step: 2180, interval_runtime: 8.008, interval_samples_per_second: 1.998, interval_steps_per_second: 1.249, epoch: 11.5344[0m
[32m[2022-09-19 22:07:23,089] [    INFO][0m - loss: 0.04550364, learning_rate: 1.261904761904762e-05, global_step: 2190, interval_runtime: 8.0057, interval_samples_per_second: 1.999, interval_steps_per_second: 1.249, epoch: 11.5873[0m
[32m[2022-09-19 22:07:31,070] [    INFO][0m - loss: 0.00058096, learning_rate: 1.253968253968254e-05, global_step: 2200, interval_runtime: 7.9812, interval_samples_per_second: 2.005, interval_steps_per_second: 1.253, epoch: 11.6402[0m
[32m[2022-09-19 22:07:39,053] [    INFO][0m - loss: 0.00905937, learning_rate: 1.2460317460317461e-05, global_step: 2210, interval_runtime: 7.9825, interval_samples_per_second: 2.004, interval_steps_per_second: 1.253, epoch: 11.6931[0m
[32m[2022-09-19 22:07:47,011] [    INFO][0m - loss: 0.05541569, learning_rate: 1.2380952380952381e-05, global_step: 2220, interval_runtime: 7.9587, interval_samples_per_second: 2.01, interval_steps_per_second: 1.256, epoch: 11.746[0m
[32m[2022-09-19 22:07:55,008] [    INFO][0m - loss: 0.00352554, learning_rate: 1.2301587301587303e-05, global_step: 2230, interval_runtime: 7.9964, interval_samples_per_second: 2.001, interval_steps_per_second: 1.251, epoch: 11.7989[0m
[32m[2022-09-19 22:08:02,981] [    INFO][0m - loss: 0.09841777, learning_rate: 1.2222222222222222e-05, global_step: 2240, interval_runtime: 7.9729, interval_samples_per_second: 2.007, interval_steps_per_second: 1.254, epoch: 11.8519[0m
[32m[2022-09-19 22:08:10,929] [    INFO][0m - loss: 0.00050553, learning_rate: 1.2142857142857144e-05, global_step: 2250, interval_runtime: 7.9483, interval_samples_per_second: 2.013, interval_steps_per_second: 1.258, epoch: 11.9048[0m
[32m[2022-09-19 22:08:18,905] [    INFO][0m - loss: 0.05792392, learning_rate: 1.2063492063492064e-05, global_step: 2260, interval_runtime: 7.9765, interval_samples_per_second: 2.006, interval_steps_per_second: 1.254, epoch: 11.9577[0m
[32m[2022-09-19 22:08:25,116] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 22:08:25,116] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-19 22:08:25,116] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 22:08:25,116] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 22:08:25,116] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-19 22:08:52,662] [    INFO][0m - eval_loss: 4.778552055358887, eval_accuracy: 0.4471959213401311, eval_runtime: 27.5456, eval_samples_per_second: 49.845, eval_steps_per_second: 3.122, epoch: 12.0[0m
[32m[2022-09-19 22:08:52,686] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-2268[0m
[32m[2022-09-19 22:08:52,686] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 22:08:55,309] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-2268/tokenizer_config.json[0m
[32m[2022-09-19 22:08:55,310] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-2268/special_tokens_map.json[0m
[32m[2022-09-19 22:09:01,940] [    INFO][0m - loss: 0.06748586, learning_rate: 1.1984126984126985e-05, global_step: 2270, interval_runtime: 43.0342, interval_samples_per_second: 0.372, interval_steps_per_second: 0.232, epoch: 12.0106[0m
[32m[2022-09-19 22:09:09,910] [    INFO][0m - loss: 0.00381719, learning_rate: 1.1904761904761905e-05, global_step: 2280, interval_runtime: 7.9707, interval_samples_per_second: 2.007, interval_steps_per_second: 1.255, epoch: 12.0635[0m
[32m[2022-09-19 22:09:17,912] [    INFO][0m - loss: 0.01247469, learning_rate: 1.1825396825396827e-05, global_step: 2290, interval_runtime: 8.0014, interval_samples_per_second: 2.0, interval_steps_per_second: 1.25, epoch: 12.1164[0m
[32m[2022-09-19 22:09:25,905] [    INFO][0m - loss: 0.01983108, learning_rate: 1.1746031746031746e-05, global_step: 2300, interval_runtime: 7.9937, interval_samples_per_second: 2.002, interval_steps_per_second: 1.251, epoch: 12.1693[0m
[32m[2022-09-19 22:09:33,914] [    INFO][0m - loss: 0.00189248, learning_rate: 1.1666666666666668e-05, global_step: 2310, interval_runtime: 8.0082, interval_samples_per_second: 1.998, interval_steps_per_second: 1.249, epoch: 12.2222[0m
[32m[2022-09-19 22:09:41,932] [    INFO][0m - loss: 0.00064609, learning_rate: 1.1587301587301588e-05, global_step: 2320, interval_runtime: 8.0187, interval_samples_per_second: 1.995, interval_steps_per_second: 1.247, epoch: 12.2751[0m
[32m[2022-09-19 22:09:49,911] [    INFO][0m - loss: 0.03089832, learning_rate: 1.150793650793651e-05, global_step: 2330, interval_runtime: 7.9782, interval_samples_per_second: 2.005, interval_steps_per_second: 1.253, epoch: 12.328[0m
[32m[2022-09-19 22:09:57,914] [    INFO][0m - loss: 0.00033735, learning_rate: 1.1428571428571429e-05, global_step: 2340, interval_runtime: 8.0036, interval_samples_per_second: 1.999, interval_steps_per_second: 1.249, epoch: 12.381[0m
[32m[2022-09-19 22:10:05,887] [    INFO][0m - loss: 0.00531615, learning_rate: 1.134920634920635e-05, global_step: 2350, interval_runtime: 7.9733, interval_samples_per_second: 2.007, interval_steps_per_second: 1.254, epoch: 12.4339[0m
[32m[2022-09-19 22:10:13,915] [    INFO][0m - loss: 0.01228974, learning_rate: 1.126984126984127e-05, global_step: 2360, interval_runtime: 8.0272, interval_samples_per_second: 1.993, interval_steps_per_second: 1.246, epoch: 12.4868[0m
[32m[2022-09-19 22:10:21,900] [    INFO][0m - loss: 0.00822198, learning_rate: 1.119047619047619e-05, global_step: 2370, interval_runtime: 7.985, interval_samples_per_second: 2.004, interval_steps_per_second: 1.252, epoch: 12.5397[0m
[32m[2022-09-19 22:10:29,892] [    INFO][0m - loss: 0.00185699, learning_rate: 1.111111111111111e-05, global_step: 2380, interval_runtime: 7.9925, interval_samples_per_second: 2.002, interval_steps_per_second: 1.251, epoch: 12.5926[0m
[32m[2022-09-19 22:10:37,892] [    INFO][0m - loss: 0.00068197, learning_rate: 1.1031746031746031e-05, global_step: 2390, interval_runtime: 7.9996, interval_samples_per_second: 2.0, interval_steps_per_second: 1.25, epoch: 12.6455[0m
[32m[2022-09-19 22:10:45,895] [    INFO][0m - loss: 0.00303034, learning_rate: 1.0952380952380951e-05, global_step: 2400, interval_runtime: 8.0035, interval_samples_per_second: 1.999, interval_steps_per_second: 1.249, epoch: 12.6984[0m
[32m[2022-09-19 22:10:53,909] [    INFO][0m - loss: 0.0301717, learning_rate: 1.0873015873015873e-05, global_step: 2410, interval_runtime: 8.0138, interval_samples_per_second: 1.997, interval_steps_per_second: 1.248, epoch: 12.7513[0m
[32m[2022-09-19 22:11:01,900] [    INFO][0m - loss: 0.00048701, learning_rate: 1.0793650793650793e-05, global_step: 2420, interval_runtime: 7.9902, interval_samples_per_second: 2.002, interval_steps_per_second: 1.252, epoch: 12.8042[0m
[32m[2022-09-19 22:11:09,918] [    INFO][0m - loss: 0.03570867, learning_rate: 1.0714285714285714e-05, global_step: 2430, interval_runtime: 8.0183, interval_samples_per_second: 1.995, interval_steps_per_second: 1.247, epoch: 12.8571[0m
[32m[2022-09-19 22:11:17,904] [    INFO][0m - loss: 0.02292683, learning_rate: 1.0634920634920634e-05, global_step: 2440, interval_runtime: 7.9862, interval_samples_per_second: 2.003, interval_steps_per_second: 1.252, epoch: 12.9101[0m
[32m[2022-09-19 22:11:25,865] [    INFO][0m - loss: 0.0291634, learning_rate: 1.0555555555555555e-05, global_step: 2450, interval_runtime: 7.9614, interval_samples_per_second: 2.01, interval_steps_per_second: 1.256, epoch: 12.963[0m
[32m[2022-09-19 22:11:31,273] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 22:11:31,273] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-19 22:11:31,273] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 22:11:31,273] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 22:11:31,273] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-19 22:11:58,840] [    INFO][0m - eval_loss: 4.929318904876709, eval_accuracy: 0.4559359067734887, eval_runtime: 27.566, eval_samples_per_second: 49.808, eval_steps_per_second: 3.12, epoch: 13.0[0m
[32m[2022-09-19 22:11:58,863] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-2457[0m
[32m[2022-09-19 22:11:58,864] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 22:12:01,480] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-2457/tokenizer_config.json[0m
[32m[2022-09-19 22:12:01,481] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-2457/special_tokens_map.json[0m
[32m[2022-09-19 22:12:08,965] [    INFO][0m - loss: 0.10180706, learning_rate: 1.0476190476190475e-05, global_step: 2460, interval_runtime: 43.0996, interval_samples_per_second: 0.371, interval_steps_per_second: 0.232, epoch: 13.0159[0m
[32m[2022-09-19 22:12:16,910] [    INFO][0m - loss: 0.00036239, learning_rate: 1.0396825396825397e-05, global_step: 2470, interval_runtime: 7.9453, interval_samples_per_second: 2.014, interval_steps_per_second: 1.259, epoch: 13.0688[0m
[32m[2022-09-19 22:12:24,904] [    INFO][0m - loss: 0.00013001, learning_rate: 1.0317460317460317e-05, global_step: 2480, interval_runtime: 7.9938, interval_samples_per_second: 2.002, interval_steps_per_second: 1.251, epoch: 13.1217[0m
[32m[2022-09-19 22:12:32,908] [    INFO][0m - loss: 0.00611637, learning_rate: 1.0238095238095238e-05, global_step: 2490, interval_runtime: 8.0043, interval_samples_per_second: 1.999, interval_steps_per_second: 1.249, epoch: 13.1746[0m
[32m[2022-09-19 22:12:40,929] [    INFO][0m - loss: 0.0004075, learning_rate: 1.0158730158730158e-05, global_step: 2500, interval_runtime: 8.0208, interval_samples_per_second: 1.995, interval_steps_per_second: 1.247, epoch: 13.2275[0m
[32m[2022-09-19 22:12:49,005] [    INFO][0m - loss: 0.00018717, learning_rate: 1.007936507936508e-05, global_step: 2510, interval_runtime: 8.0755, interval_samples_per_second: 1.981, interval_steps_per_second: 1.238, epoch: 13.2804[0m
[32m[2022-09-19 22:12:56,987] [    INFO][0m - loss: 0.0146952, learning_rate: 9.999999999999999e-06, global_step: 2520, interval_runtime: 7.9822, interval_samples_per_second: 2.004, interval_steps_per_second: 1.253, epoch: 13.3333[0m
[32m[2022-09-19 22:13:04,949] [    INFO][0m - loss: 0.00176767, learning_rate: 9.92063492063492e-06, global_step: 2530, interval_runtime: 7.9622, interval_samples_per_second: 2.009, interval_steps_per_second: 1.256, epoch: 13.3862[0m
[32m[2022-09-19 22:13:12,910] [    INFO][0m - loss: 0.00030803, learning_rate: 9.84126984126984e-06, global_step: 2540, interval_runtime: 7.961, interval_samples_per_second: 2.01, interval_steps_per_second: 1.256, epoch: 13.4392[0m
[32m[2022-09-19 22:13:20,876] [    INFO][0m - loss: 0.00278821, learning_rate: 9.761904761904762e-06, global_step: 2550, interval_runtime: 7.9663, interval_samples_per_second: 2.008, interval_steps_per_second: 1.255, epoch: 13.4921[0m
[32m[2022-09-19 22:13:28,868] [    INFO][0m - loss: 0.00690203, learning_rate: 9.682539682539682e-06, global_step: 2560, interval_runtime: 7.9922, interval_samples_per_second: 2.002, interval_steps_per_second: 1.251, epoch: 13.545[0m
[32m[2022-09-19 22:13:36,860] [    INFO][0m - loss: 0.02131908, learning_rate: 9.603174603174603e-06, global_step: 2570, interval_runtime: 7.9916, interval_samples_per_second: 2.002, interval_steps_per_second: 1.251, epoch: 13.5979[0m
[32m[2022-09-19 22:13:44,874] [    INFO][0m - loss: 0.00024714, learning_rate: 9.523809523809523e-06, global_step: 2580, interval_runtime: 8.0138, interval_samples_per_second: 1.997, interval_steps_per_second: 1.248, epoch: 13.6508[0m
[32m[2022-09-19 22:13:52,913] [    INFO][0m - loss: 0.00045071, learning_rate: 9.444444444444445e-06, global_step: 2590, interval_runtime: 8.039, interval_samples_per_second: 1.99, interval_steps_per_second: 1.244, epoch: 13.7037[0m
[32m[2022-09-19 22:14:00,901] [    INFO][0m - loss: 0.07828293, learning_rate: 9.365079365079364e-06, global_step: 2600, interval_runtime: 7.9885, interval_samples_per_second: 2.003, interval_steps_per_second: 1.252, epoch: 13.7566[0m
[32m[2022-09-19 22:14:08,890] [    INFO][0m - loss: 0.00113052, learning_rate: 9.285714285714286e-06, global_step: 2610, interval_runtime: 7.9884, interval_samples_per_second: 2.003, interval_steps_per_second: 1.252, epoch: 13.8095[0m
[32m[2022-09-19 22:14:16,891] [    INFO][0m - loss: 0.03053179, learning_rate: 9.206349206349206e-06, global_step: 2620, interval_runtime: 8.001, interval_samples_per_second: 2.0, interval_steps_per_second: 1.25, epoch: 13.8624[0m
[32m[2022-09-19 22:14:24,896] [    INFO][0m - loss: 0.0036864, learning_rate: 9.126984126984127e-06, global_step: 2630, interval_runtime: 8.005, interval_samples_per_second: 1.999, interval_steps_per_second: 1.249, epoch: 13.9153[0m
[32m[2022-09-19 22:14:32,900] [    INFO][0m - loss: 0.02394822, learning_rate: 9.047619047619047e-06, global_step: 2640, interval_runtime: 8.0046, interval_samples_per_second: 1.999, interval_steps_per_second: 1.249, epoch: 13.9683[0m
[32m[2022-09-19 22:14:37,515] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 22:14:37,515] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-19 22:14:37,515] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 22:14:37,515] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 22:14:37,515] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-19 22:15:05,206] [    INFO][0m - eval_loss: 5.055652141571045, eval_accuracy: 0.4442825928623452, eval_runtime: 27.6897, eval_samples_per_second: 49.585, eval_steps_per_second: 3.106, epoch: 14.0[0m
[32m[2022-09-19 22:15:05,229] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-2646[0m
[32m[2022-09-19 22:15:05,229] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 22:15:07,999] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-2646/tokenizer_config.json[0m
[32m[2022-09-19 22:15:07,999] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-2646/special_tokens_map.json[0m
[32m[2022-09-19 22:15:16,817] [    INFO][0m - loss: 0.05452952, learning_rate: 8.968253968253968e-06, global_step: 2650, interval_runtime: 43.917, interval_samples_per_second: 0.364, interval_steps_per_second: 0.228, epoch: 14.0212[0m
[32m[2022-09-19 22:15:24,755] [    INFO][0m - loss: 0.00018852, learning_rate: 8.888888888888888e-06, global_step: 2660, interval_runtime: 7.9377, interval_samples_per_second: 2.016, interval_steps_per_second: 1.26, epoch: 14.0741[0m
[32m[2022-09-19 22:15:32,779] [    INFO][0m - loss: 0.0118191, learning_rate: 8.80952380952381e-06, global_step: 2670, interval_runtime: 8.024, interval_samples_per_second: 1.994, interval_steps_per_second: 1.246, epoch: 14.127[0m
[32m[2022-09-19 22:15:40,840] [    INFO][0m - loss: 0.00037858, learning_rate: 8.73015873015873e-06, global_step: 2680, interval_runtime: 8.0612, interval_samples_per_second: 1.985, interval_steps_per_second: 1.241, epoch: 14.1799[0m
[32m[2022-09-19 22:15:48,830] [    INFO][0m - loss: 0.02163494, learning_rate: 8.650793650793651e-06, global_step: 2690, interval_runtime: 7.9903, interval_samples_per_second: 2.002, interval_steps_per_second: 1.252, epoch: 14.2328[0m
[32m[2022-09-19 22:15:56,851] [    INFO][0m - loss: 0.00015538, learning_rate: 8.571428571428571e-06, global_step: 2700, interval_runtime: 8.0206, interval_samples_per_second: 1.995, interval_steps_per_second: 1.247, epoch: 14.2857[0m
[32m[2022-09-19 22:16:04,890] [    INFO][0m - loss: 0.00205565, learning_rate: 8.492063492063492e-06, global_step: 2710, interval_runtime: 8.039, interval_samples_per_second: 1.99, interval_steps_per_second: 1.244, epoch: 14.3386[0m
[32m[2022-09-19 22:16:12,917] [    INFO][0m - loss: 8.914e-05, learning_rate: 8.412698412698412e-06, global_step: 2720, interval_runtime: 8.027, interval_samples_per_second: 1.993, interval_steps_per_second: 1.246, epoch: 14.3915[0m
[32m[2022-09-19 22:16:20,899] [    INFO][0m - loss: 0.03065827, learning_rate: 8.333333333333334e-06, global_step: 2730, interval_runtime: 7.9819, interval_samples_per_second: 2.005, interval_steps_per_second: 1.253, epoch: 14.4444[0m
[32m[2022-09-19 22:16:28,885] [    INFO][0m - loss: 0.00492505, learning_rate: 8.253968253968254e-06, global_step: 2740, interval_runtime: 7.9858, interval_samples_per_second: 2.004, interval_steps_per_second: 1.252, epoch: 14.4974[0m
[32m[2022-09-19 22:16:36,883] [    INFO][0m - loss: 0.02866386, learning_rate: 8.174603174603175e-06, global_step: 2750, interval_runtime: 7.9983, interval_samples_per_second: 2.0, interval_steps_per_second: 1.25, epoch: 14.5503[0m
[32m[2022-09-19 22:16:44,896] [    INFO][0m - loss: 0.00304659, learning_rate: 8.095238095238095e-06, global_step: 2760, interval_runtime: 8.0133, interval_samples_per_second: 1.997, interval_steps_per_second: 1.248, epoch: 14.6032[0m
[32m[2022-09-19 22:16:52,932] [    INFO][0m - loss: 0.00010415, learning_rate: 8.015873015873016e-06, global_step: 2770, interval_runtime: 8.0358, interval_samples_per_second: 1.991, interval_steps_per_second: 1.244, epoch: 14.6561[0m
[32m[2022-09-19 22:17:00,973] [    INFO][0m - loss: 0.0001222, learning_rate: 7.936507936507936e-06, global_step: 2780, interval_runtime: 8.0401, interval_samples_per_second: 1.99, interval_steps_per_second: 1.244, epoch: 14.709[0m
[32m[2022-09-19 22:17:08,942] [    INFO][0m - loss: 0.00013083, learning_rate: 7.857142857142858e-06, global_step: 2790, interval_runtime: 7.9698, interval_samples_per_second: 2.008, interval_steps_per_second: 1.255, epoch: 14.7619[0m
[32m[2022-09-19 22:17:16,965] [    INFO][0m - loss: 0.00526221, learning_rate: 7.777777777777777e-06, global_step: 2800, interval_runtime: 8.0228, interval_samples_per_second: 1.994, interval_steps_per_second: 1.246, epoch: 14.8148[0m
[32m[2022-09-19 22:17:24,945] [    INFO][0m - loss: 0.00011222, learning_rate: 7.698412698412699e-06, global_step: 2810, interval_runtime: 7.9798, interval_samples_per_second: 2.005, interval_steps_per_second: 1.253, epoch: 14.8677[0m
[32m[2022-09-19 22:17:32,982] [    INFO][0m - loss: 0.04043708, learning_rate: 7.619047619047619e-06, global_step: 2820, interval_runtime: 8.0372, interval_samples_per_second: 1.991, interval_steps_per_second: 1.244, epoch: 14.9206[0m
[32m[2022-09-19 22:17:40,995] [    INFO][0m - loss: 0.03065187, learning_rate: 7.53968253968254e-06, global_step: 2830, interval_runtime: 8.0125, interval_samples_per_second: 1.997, interval_steps_per_second: 1.248, epoch: 14.9735[0m
[32m[2022-09-19 22:17:44,807] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 22:17:44,808] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-19 22:17:44,808] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 22:17:44,808] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 22:17:44,808] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-19 22:18:12,404] [    INFO][0m - eval_loss: 4.9539008140563965, eval_accuracy: 0.4515659140568099, eval_runtime: 27.5958, eval_samples_per_second: 49.754, eval_steps_per_second: 3.116, epoch: 15.0[0m
[32m[2022-09-19 22:18:12,427] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-2835[0m
[32m[2022-09-19 22:18:12,427] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 22:18:14,980] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-2835/tokenizer_config.json[0m
[32m[2022-09-19 22:18:14,981] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-2835/special_tokens_map.json[0m
[32m[2022-09-19 22:18:24,492] [    INFO][0m - loss: 0.02448378, learning_rate: 7.46031746031746e-06, global_step: 2840, interval_runtime: 43.4969, interval_samples_per_second: 0.368, interval_steps_per_second: 0.23, epoch: 15.0265[0m
[32m[2022-09-19 22:18:32,441] [    INFO][0m - loss: 0.01544045, learning_rate: 7.380952380952381e-06, global_step: 2850, interval_runtime: 7.9493, interval_samples_per_second: 2.013, interval_steps_per_second: 1.258, epoch: 15.0794[0m
[32m[2022-09-19 22:18:40,442] [    INFO][0m - loss: 0.00467764, learning_rate: 7.301587301587301e-06, global_step: 2860, interval_runtime: 8.0011, interval_samples_per_second: 2.0, interval_steps_per_second: 1.25, epoch: 15.1323[0m
[32m[2022-09-19 22:18:48,547] [    INFO][0m - loss: 0.00436599, learning_rate: 7.222222222222222e-06, global_step: 2870, interval_runtime: 8.1055, interval_samples_per_second: 1.974, interval_steps_per_second: 1.234, epoch: 15.1852[0m
[32m[2022-09-19 22:18:56,694] [    INFO][0m - loss: 0.00694866, learning_rate: 7.142857142857143e-06, global_step: 2880, interval_runtime: 8.146, interval_samples_per_second: 1.964, interval_steps_per_second: 1.228, epoch: 15.2381[0m
[32m[2022-09-19 22:19:04,793] [    INFO][0m - loss: 0.00013599, learning_rate: 7.063492063492063e-06, global_step: 2890, interval_runtime: 8.0994, interval_samples_per_second: 1.975, interval_steps_per_second: 1.235, epoch: 15.291[0m
[32m[2022-09-19 22:19:12,814] [    INFO][0m - loss: 8.056e-05, learning_rate: 6.984126984126984e-06, global_step: 2900, interval_runtime: 8.0209, interval_samples_per_second: 1.995, interval_steps_per_second: 1.247, epoch: 15.3439[0m
[32m[2022-09-19 22:19:20,776] [    INFO][0m - loss: 0.00063438, learning_rate: 6.904761904761905e-06, global_step: 2910, interval_runtime: 7.9625, interval_samples_per_second: 2.009, interval_steps_per_second: 1.256, epoch: 15.3968[0m
[32m[2022-09-19 22:19:28,761] [    INFO][0m - loss: 4.889e-05, learning_rate: 6.825396825396825e-06, global_step: 2920, interval_runtime: 7.9848, interval_samples_per_second: 2.004, interval_steps_per_second: 1.252, epoch: 15.4497[0m
[32m[2022-09-19 22:19:36,776] [    INFO][0m - loss: 0.02632582, learning_rate: 6.746031746031746e-06, global_step: 2930, interval_runtime: 8.0152, interval_samples_per_second: 1.996, interval_steps_per_second: 1.248, epoch: 15.5026[0m
[32m[2022-09-19 22:19:44,757] [    INFO][0m - loss: 0.00013529, learning_rate: 6.666666666666667e-06, global_step: 2940, interval_runtime: 7.9805, interval_samples_per_second: 2.005, interval_steps_per_second: 1.253, epoch: 15.5556[0m
[32m[2022-09-19 22:19:52,758] [    INFO][0m - loss: 0.00316664, learning_rate: 6.587301587301587e-06, global_step: 2950, interval_runtime: 8.0011, interval_samples_per_second: 2.0, interval_steps_per_second: 1.25, epoch: 15.6085[0m
[32m[2022-09-19 22:20:00,764] [    INFO][0m - loss: 0.0294071, learning_rate: 6.507936507936508e-06, global_step: 2960, interval_runtime: 8.0058, interval_samples_per_second: 1.999, interval_steps_per_second: 1.249, epoch: 15.6614[0m
[32m[2022-09-19 22:20:08,748] [    INFO][0m - loss: 0.00558095, learning_rate: 6.428571428571429e-06, global_step: 2970, interval_runtime: 7.9842, interval_samples_per_second: 2.004, interval_steps_per_second: 1.252, epoch: 15.7143[0m
[32m[2022-09-19 22:20:16,762] [    INFO][0m - loss: 0.00012098, learning_rate: 6.349206349206349e-06, global_step: 2980, interval_runtime: 8.0144, interval_samples_per_second: 1.996, interval_steps_per_second: 1.248, epoch: 15.7672[0m
[32m[2022-09-19 22:20:24,742] [    INFO][0m - loss: 0.00011794, learning_rate: 6.26984126984127e-06, global_step: 2990, interval_runtime: 7.9802, interval_samples_per_second: 2.005, interval_steps_per_second: 1.253, epoch: 15.8201[0m
[32m[2022-09-19 22:20:32,741] [    INFO][0m - loss: 0.02265912, learning_rate: 6.190476190476191e-06, global_step: 3000, interval_runtime: 7.9983, interval_samples_per_second: 2.0, interval_steps_per_second: 1.25, epoch: 15.873[0m
[32m[2022-09-19 22:20:40,824] [    INFO][0m - loss: 0.00139479, learning_rate: 6.111111111111111e-06, global_step: 3010, interval_runtime: 8.0832, interval_samples_per_second: 1.979, interval_steps_per_second: 1.237, epoch: 15.9259[0m
[32m[2022-09-19 22:20:48,792] [    INFO][0m - loss: 0.00010786, learning_rate: 6.031746031746032e-06, global_step: 3020, interval_runtime: 7.9681, interval_samples_per_second: 2.008, interval_steps_per_second: 1.255, epoch: 15.9788[0m
[32m[2022-09-19 22:20:51,836] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 22:20:51,837] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-19 22:20:51,837] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 22:20:51,837] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 22:20:51,837] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-19 22:21:19,428] [    INFO][0m - eval_loss: 5.002303600311279, eval_accuracy: 0.4471959213401311, eval_runtime: 27.5905, eval_samples_per_second: 49.764, eval_steps_per_second: 3.117, epoch: 16.0[0m
[32m[2022-09-19 22:21:19,451] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-3024[0m
[32m[2022-09-19 22:21:19,451] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 22:21:22,017] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-3024/tokenizer_config.json[0m
[32m[2022-09-19 22:21:22,018] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-3024/special_tokens_map.json[0m
[32m[2022-09-19 22:21:31,858] [    INFO][0m - loss: 0.02179353, learning_rate: 5.9523809523809525e-06, global_step: 3030, interval_runtime: 43.0657, interval_samples_per_second: 0.372, interval_steps_per_second: 0.232, epoch: 16.0317[0m
[32m[2022-09-19 22:21:39,837] [    INFO][0m - loss: 0.00114914, learning_rate: 5.873015873015873e-06, global_step: 3040, interval_runtime: 7.9788, interval_samples_per_second: 2.005, interval_steps_per_second: 1.253, epoch: 16.0847[0m
[32m[2022-09-19 22:21:47,816] [    INFO][0m - loss: 0.00012133, learning_rate: 5.793650793650794e-06, global_step: 3050, interval_runtime: 7.9789, interval_samples_per_second: 2.005, interval_steps_per_second: 1.253, epoch: 16.1376[0m
[32m[2022-09-19 22:21:55,828] [    INFO][0m - loss: 7.976e-05, learning_rate: 5.7142857142857145e-06, global_step: 3060, interval_runtime: 8.0119, interval_samples_per_second: 1.997, interval_steps_per_second: 1.248, epoch: 16.1905[0m
[32m[2022-09-19 22:22:03,836] [    INFO][0m - loss: 5.716e-05, learning_rate: 5.634920634920635e-06, global_step: 3070, interval_runtime: 8.0087, interval_samples_per_second: 1.998, interval_steps_per_second: 1.249, epoch: 16.2434[0m
[32m[2022-09-19 22:22:11,896] [    INFO][0m - loss: 0.00366423, learning_rate: 5.555555555555555e-06, global_step: 3080, interval_runtime: 8.0598, interval_samples_per_second: 1.985, interval_steps_per_second: 1.241, epoch: 16.2963[0m
[32m[2022-09-19 22:22:19,890] [    INFO][0m - loss: 9.314e-05, learning_rate: 5.476190476190476e-06, global_step: 3090, interval_runtime: 7.9941, interval_samples_per_second: 2.001, interval_steps_per_second: 1.251, epoch: 16.3492[0m
[32m[2022-09-19 22:22:27,893] [    INFO][0m - loss: 7.938e-05, learning_rate: 5.396825396825396e-06, global_step: 3100, interval_runtime: 8.0027, interval_samples_per_second: 1.999, interval_steps_per_second: 1.25, epoch: 16.4021[0m
[32m[2022-09-19 22:22:35,915] [    INFO][0m - loss: 0.02754602, learning_rate: 5.317460317460317e-06, global_step: 3110, interval_runtime: 8.0225, interval_samples_per_second: 1.994, interval_steps_per_second: 1.247, epoch: 16.455[0m
[32m[2022-09-19 22:22:43,914] [    INFO][0m - loss: 7.812e-05, learning_rate: 5.238095238095238e-06, global_step: 3120, interval_runtime: 7.9987, interval_samples_per_second: 2.0, interval_steps_per_second: 1.25, epoch: 16.5079[0m
[32m[2022-09-19 22:22:51,913] [    INFO][0m - loss: 0.02280376, learning_rate: 5.158730158730158e-06, global_step: 3130, interval_runtime: 7.9992, interval_samples_per_second: 2.0, interval_steps_per_second: 1.25, epoch: 16.5608[0m
[32m[2022-09-19 22:22:59,942] [    INFO][0m - loss: 0.00902903, learning_rate: 5.079365079365079e-06, global_step: 3140, interval_runtime: 8.0286, interval_samples_per_second: 1.993, interval_steps_per_second: 1.246, epoch: 16.6138[0m
[32m[2022-09-19 22:23:08,007] [    INFO][0m - loss: 0.00703349, learning_rate: 4.9999999999999996e-06, global_step: 3150, interval_runtime: 8.0647, interval_samples_per_second: 1.984, interval_steps_per_second: 1.24, epoch: 16.6667[0m
[32m[2022-09-19 22:23:15,994] [    INFO][0m - loss: 0.00013352, learning_rate: 4.92063492063492e-06, global_step: 3160, interval_runtime: 7.9867, interval_samples_per_second: 2.003, interval_steps_per_second: 1.252, epoch: 16.7196[0m
[32m[2022-09-19 22:23:23,981] [    INFO][0m - loss: 6.893e-05, learning_rate: 4.841269841269841e-06, global_step: 3170, interval_runtime: 7.9874, interval_samples_per_second: 2.003, interval_steps_per_second: 1.252, epoch: 16.7725[0m
[32m[2022-09-19 22:23:31,982] [    INFO][0m - loss: 0.00011345, learning_rate: 4.7619047619047615e-06, global_step: 3180, interval_runtime: 8.0011, interval_samples_per_second: 2.0, interval_steps_per_second: 1.25, epoch: 16.8254[0m
[32m[2022-09-19 22:23:39,952] [    INFO][0m - loss: 0.02393302, learning_rate: 4.682539682539682e-06, global_step: 3190, interval_runtime: 7.9697, interval_samples_per_second: 2.008, interval_steps_per_second: 1.255, epoch: 16.8783[0m
[32m[2022-09-19 22:23:47,952] [    INFO][0m - loss: 0.01048904, learning_rate: 4.603174603174603e-06, global_step: 3200, interval_runtime: 8.0007, interval_samples_per_second: 2.0, interval_steps_per_second: 1.25, epoch: 16.9312[0m
[32m[2022-09-19 22:23:55,894] [    INFO][0m - loss: 0.01787912, learning_rate: 4.5238095238095235e-06, global_step: 3210, interval_runtime: 7.9416, interval_samples_per_second: 2.015, interval_steps_per_second: 1.259, epoch: 16.9841[0m
[32m[2022-09-19 22:23:58,179] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 22:23:58,179] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-19 22:23:58,179] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 22:23:58,179] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 22:23:58,179] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-19 22:24:25,792] [    INFO][0m - eval_loss: 5.03718900680542, eval_accuracy: 0.45010924981791695, eval_runtime: 27.6123, eval_samples_per_second: 49.724, eval_steps_per_second: 3.115, epoch: 17.0[0m
[32m[2022-09-19 22:24:25,815] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-3213[0m
[32m[2022-09-19 22:24:25,815] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 22:24:28,361] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-3213/tokenizer_config.json[0m
[32m[2022-09-19 22:24:28,361] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-3213/special_tokens_map.json[0m
[32m[2022-09-19 22:24:38,987] [    INFO][0m - loss: 0.00958083, learning_rate: 4.444444444444444e-06, global_step: 3220, interval_runtime: 43.0928, interval_samples_per_second: 0.371, interval_steps_per_second: 0.232, epoch: 17.037[0m
[32m[2022-09-19 22:24:46,948] [    INFO][0m - loss: 0.00012082, learning_rate: 4.365079365079365e-06, global_step: 3230, interval_runtime: 7.9612, interval_samples_per_second: 2.01, interval_steps_per_second: 1.256, epoch: 17.0899[0m
[32m[2022-09-19 22:24:54,951] [    INFO][0m - loss: 0.00121315, learning_rate: 4.2857142857142855e-06, global_step: 3240, interval_runtime: 8.0034, interval_samples_per_second: 1.999, interval_steps_per_second: 1.249, epoch: 17.1429[0m
[32m[2022-09-19 22:25:02,927] [    INFO][0m - loss: 0.01998583, learning_rate: 4.206349206349206e-06, global_step: 3250, interval_runtime: 7.9761, interval_samples_per_second: 2.006, interval_steps_per_second: 1.254, epoch: 17.1958[0m
[32m[2022-09-19 22:25:10,916] [    INFO][0m - loss: 0.01839872, learning_rate: 4.126984126984127e-06, global_step: 3260, interval_runtime: 7.9885, interval_samples_per_second: 2.003, interval_steps_per_second: 1.252, epoch: 17.2487[0m
[32m[2022-09-19 22:25:18,933] [    INFO][0m - loss: 0.01329281, learning_rate: 4.0476190476190474e-06, global_step: 3270, interval_runtime: 8.0169, interval_samples_per_second: 1.996, interval_steps_per_second: 1.247, epoch: 17.3016[0m
[32m[2022-09-19 22:25:26,951] [    INFO][0m - loss: 0.00310599, learning_rate: 3.968253968253968e-06, global_step: 3280, interval_runtime: 8.0184, interval_samples_per_second: 1.995, interval_steps_per_second: 1.247, epoch: 17.3545[0m
[32m[2022-09-19 22:25:34,968] [    INFO][0m - loss: 0.01193106, learning_rate: 3.888888888888889e-06, global_step: 3290, interval_runtime: 8.0164, interval_samples_per_second: 1.996, interval_steps_per_second: 1.247, epoch: 17.4074[0m
[32m[2022-09-19 22:25:42,923] [    INFO][0m - loss: 9.625e-05, learning_rate: 3.8095238095238094e-06, global_step: 3300, interval_runtime: 7.9552, interval_samples_per_second: 2.011, interval_steps_per_second: 1.257, epoch: 17.4603[0m
[32m[2022-09-19 22:25:50,933] [    INFO][0m - loss: 8.68e-05, learning_rate: 3.73015873015873e-06, global_step: 3310, interval_runtime: 8.01, interval_samples_per_second: 1.998, interval_steps_per_second: 1.248, epoch: 17.5132[0m
[32m[2022-09-19 22:25:58,965] [    INFO][0m - loss: 0.00441518, learning_rate: 3.6507936507936507e-06, global_step: 3320, interval_runtime: 8.0323, interval_samples_per_second: 1.992, interval_steps_per_second: 1.245, epoch: 17.5661[0m
[32m[2022-09-19 22:26:06,976] [    INFO][0m - loss: 6.568e-05, learning_rate: 3.5714285714285714e-06, global_step: 3330, interval_runtime: 8.0111, interval_samples_per_second: 1.997, interval_steps_per_second: 1.248, epoch: 17.619[0m
[32m[2022-09-19 22:26:14,968] [    INFO][0m - loss: 0.01139609, learning_rate: 3.492063492063492e-06, global_step: 3340, interval_runtime: 7.9914, interval_samples_per_second: 2.002, interval_steps_per_second: 1.251, epoch: 17.672[0m
[32m[2022-09-19 22:26:22,972] [    INFO][0m - loss: 0.01238923, learning_rate: 3.4126984126984127e-06, global_step: 3350, interval_runtime: 8.0041, interval_samples_per_second: 1.999, interval_steps_per_second: 1.249, epoch: 17.7249[0m
[32m[2022-09-19 22:26:30,948] [    INFO][0m - loss: 6.236e-05, learning_rate: 3.3333333333333333e-06, global_step: 3360, interval_runtime: 7.9767, interval_samples_per_second: 2.006, interval_steps_per_second: 1.254, epoch: 17.7778[0m
[32m[2022-09-19 22:26:38,971] [    INFO][0m - loss: 0.00015675, learning_rate: 3.253968253968254e-06, global_step: 3370, interval_runtime: 8.0218, interval_samples_per_second: 1.995, interval_steps_per_second: 1.247, epoch: 17.8307[0m
[32m[2022-09-19 22:26:46,989] [    INFO][0m - loss: 6.084e-05, learning_rate: 3.1746031746031746e-06, global_step: 3380, interval_runtime: 8.0182, interval_samples_per_second: 1.995, interval_steps_per_second: 1.247, epoch: 17.8836[0m
[32m[2022-09-19 22:26:54,972] [    INFO][0m - loss: 5.915e-05, learning_rate: 3.0952380952380953e-06, global_step: 3390, interval_runtime: 7.9839, interval_samples_per_second: 2.004, interval_steps_per_second: 1.253, epoch: 17.9365[0m
[32m[2022-09-19 22:27:02,866] [    INFO][0m - loss: 6.834e-05, learning_rate: 3.015873015873016e-06, global_step: 3400, interval_runtime: 7.8934, interval_samples_per_second: 2.027, interval_steps_per_second: 1.267, epoch: 17.9894[0m
[32m[2022-09-19 22:27:04,387] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 22:27:04,387] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-19 22:27:04,387] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 22:27:04,387] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 22:27:04,387] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-19 22:27:31,895] [    INFO][0m - eval_loss: 5.03147554397583, eval_accuracy: 0.4515659140568099, eval_runtime: 27.508, eval_samples_per_second: 49.913, eval_steps_per_second: 3.126, epoch: 18.0[0m
[32m[2022-09-19 22:27:31,919] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-3402[0m
[32m[2022-09-19 22:27:31,919] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 22:27:34,479] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-3402/tokenizer_config.json[0m
[32m[2022-09-19 22:27:34,479] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-3402/special_tokens_map.json[0m
[32m[2022-09-19 22:27:45,952] [    INFO][0m - loss: 0.00403241, learning_rate: 2.9365079365079366e-06, global_step: 3410, interval_runtime: 43.0855, interval_samples_per_second: 0.371, interval_steps_per_second: 0.232, epoch: 18.0423[0m
[32m[2022-09-19 22:27:53,936] [    INFO][0m - loss: 0.00142021, learning_rate: 2.8571428571428573e-06, global_step: 3420, interval_runtime: 7.9845, interval_samples_per_second: 2.004, interval_steps_per_second: 1.252, epoch: 18.0952[0m
[32m[2022-09-19 22:28:01,886] [    INFO][0m - loss: 4.33e-05, learning_rate: 2.7777777777777775e-06, global_step: 3430, interval_runtime: 7.9498, interval_samples_per_second: 2.013, interval_steps_per_second: 1.258, epoch: 18.1481[0m
[32m[2022-09-19 22:28:10,567] [    INFO][0m - loss: 9.152e-05, learning_rate: 2.698412698412698e-06, global_step: 3440, interval_runtime: 7.998, interval_samples_per_second: 2.0, interval_steps_per_second: 1.25, epoch: 18.2011[0m
[32m[2022-09-19 22:28:18,597] [    INFO][0m - loss: 4.423e-05, learning_rate: 2.619047619047619e-06, global_step: 3450, interval_runtime: 8.7134, interval_samples_per_second: 1.836, interval_steps_per_second: 1.148, epoch: 18.254[0m
[32m[2022-09-19 22:28:26,619] [    INFO][0m - loss: 5.511e-05, learning_rate: 2.5396825396825395e-06, global_step: 3460, interval_runtime: 8.0215, interval_samples_per_second: 1.995, interval_steps_per_second: 1.247, epoch: 18.3069[0m
[32m[2022-09-19 22:28:34,659] [    INFO][0m - loss: 0.00642115, learning_rate: 2.46031746031746e-06, global_step: 3470, interval_runtime: 8.0403, interval_samples_per_second: 1.99, interval_steps_per_second: 1.244, epoch: 18.3598[0m
[32m[2022-09-19 22:28:42,650] [    INFO][0m - loss: 0.01467983, learning_rate: 2.3809523809523808e-06, global_step: 3480, interval_runtime: 7.9908, interval_samples_per_second: 2.002, interval_steps_per_second: 1.251, epoch: 18.4127[0m
[32m[2022-09-19 22:28:50,660] [    INFO][0m - loss: 5.594e-05, learning_rate: 2.3015873015873014e-06, global_step: 3490, interval_runtime: 8.0096, interval_samples_per_second: 1.998, interval_steps_per_second: 1.249, epoch: 18.4656[0m
[32m[2022-09-19 22:28:58,651] [    INFO][0m - loss: 6.892e-05, learning_rate: 2.222222222222222e-06, global_step: 3500, interval_runtime: 7.9915, interval_samples_per_second: 2.002, interval_steps_per_second: 1.251, epoch: 18.5185[0m
[32m[2022-09-19 22:29:06,640] [    INFO][0m - loss: 6.526e-05, learning_rate: 2.1428571428571427e-06, global_step: 3510, interval_runtime: 7.9886, interval_samples_per_second: 2.003, interval_steps_per_second: 1.252, epoch: 18.5714[0m
[32m[2022-09-19 22:29:14,625] [    INFO][0m - loss: 0.00789728, learning_rate: 2.0634920634920634e-06, global_step: 3520, interval_runtime: 7.9858, interval_samples_per_second: 2.004, interval_steps_per_second: 1.252, epoch: 18.6243[0m
[32m[2022-09-19 22:29:22,602] [    INFO][0m - loss: 0.00269839, learning_rate: 1.984126984126984e-06, global_step: 3530, interval_runtime: 7.977, interval_samples_per_second: 2.006, interval_steps_per_second: 1.254, epoch: 18.6772[0m
[32m[2022-09-19 22:29:30,610] [    INFO][0m - loss: 6.78e-05, learning_rate: 1.9047619047619047e-06, global_step: 3540, interval_runtime: 8.008, interval_samples_per_second: 1.998, interval_steps_per_second: 1.249, epoch: 18.7302[0m
[32m[2022-09-19 22:29:38,609] [    INFO][0m - loss: 0.00652155, learning_rate: 1.8253968253968254e-06, global_step: 3550, interval_runtime: 7.9991, interval_samples_per_second: 2.0, interval_steps_per_second: 1.25, epoch: 18.7831[0m
[32m[2022-09-19 22:29:46,599] [    INFO][0m - loss: 0.02419716, learning_rate: 1.746031746031746e-06, global_step: 3560, interval_runtime: 7.9899, interval_samples_per_second: 2.003, interval_steps_per_second: 1.252, epoch: 18.836[0m
[32m[2022-09-19 22:29:54,600] [    INFO][0m - loss: 6.934e-05, learning_rate: 1.6666666666666667e-06, global_step: 3570, interval_runtime: 8.0003, interval_samples_per_second: 2.0, interval_steps_per_second: 1.25, epoch: 18.8889[0m
[32m[2022-09-19 22:30:02,644] [    INFO][0m - loss: 0.01081784, learning_rate: 1.5873015873015873e-06, global_step: 3580, interval_runtime: 8.0443, interval_samples_per_second: 1.989, interval_steps_per_second: 1.243, epoch: 18.9418[0m
[32m[2022-09-19 22:30:10,495] [    INFO][0m - loss: 0.01095777, learning_rate: 1.507936507936508e-06, global_step: 3590, interval_runtime: 7.8507, interval_samples_per_second: 2.038, interval_steps_per_second: 1.274, epoch: 18.9947[0m
[32m[2022-09-19 22:30:11,251] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 22:30:11,252] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-19 22:30:11,252] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 22:30:11,252] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 22:30:11,252] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-19 22:30:38,810] [    INFO][0m - eval_loss: 5.0414018630981445, eval_accuracy: 0.45302257829570286, eval_runtime: 27.5571, eval_samples_per_second: 49.824, eval_steps_per_second: 3.121, epoch: 19.0[0m
[32m[2022-09-19 22:30:38,836] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-3591[0m
[32m[2022-09-19 22:30:38,836] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 22:30:41,409] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-3591/tokenizer_config.json[0m
[32m[2022-09-19 22:30:41,409] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-3591/special_tokens_map.json[0m
[32m[2022-09-19 22:30:53,626] [    INFO][0m - loss: 0.00356325, learning_rate: 1.4285714285714286e-06, global_step: 3600, interval_runtime: 43.131, interval_samples_per_second: 0.371, interval_steps_per_second: 0.232, epoch: 19.0476[0m
[32m[2022-09-19 22:31:01,620] [    INFO][0m - loss: 0.00454412, learning_rate: 1.349206349206349e-06, global_step: 3610, interval_runtime: 7.9944, interval_samples_per_second: 2.001, interval_steps_per_second: 1.251, epoch: 19.1005[0m
[32m[2022-09-19 22:31:09,638] [    INFO][0m - loss: 0.00924493, learning_rate: 1.2698412698412697e-06, global_step: 3620, interval_runtime: 8.0177, interval_samples_per_second: 1.996, interval_steps_per_second: 1.247, epoch: 19.1534[0m
[32m[2022-09-19 22:31:17,666] [    INFO][0m - loss: 7.201e-05, learning_rate: 1.1904761904761904e-06, global_step: 3630, interval_runtime: 8.0281, interval_samples_per_second: 1.993, interval_steps_per_second: 1.246, epoch: 19.2063[0m
[32m[2022-09-19 22:31:25,657] [    INFO][0m - loss: 6.492e-05, learning_rate: 1.111111111111111e-06, global_step: 3640, interval_runtime: 7.9908, interval_samples_per_second: 2.002, interval_steps_per_second: 1.251, epoch: 19.2593[0m
[32m[2022-09-19 22:31:33,654] [    INFO][0m - loss: 0.00734695, learning_rate: 1.0317460317460317e-06, global_step: 3650, interval_runtime: 7.9978, interval_samples_per_second: 2.001, interval_steps_per_second: 1.25, epoch: 19.3122[0m
[32m[2022-09-19 22:31:41,659] [    INFO][0m - loss: 4.276e-05, learning_rate: 9.523809523809523e-07, global_step: 3660, interval_runtime: 8.0044, interval_samples_per_second: 1.999, interval_steps_per_second: 1.249, epoch: 19.3651[0m
[32m[2022-09-19 22:31:49,651] [    INFO][0m - loss: 0.00013825, learning_rate: 8.73015873015873e-07, global_step: 3670, interval_runtime: 7.9916, interval_samples_per_second: 2.002, interval_steps_per_second: 1.251, epoch: 19.418[0m
[32m[2022-09-19 22:31:57,653] [    INFO][0m - loss: 7.63e-05, learning_rate: 7.936507936507937e-07, global_step: 3680, interval_runtime: 8.0026, interval_samples_per_second: 1.999, interval_steps_per_second: 1.25, epoch: 19.4709[0m
[32m[2022-09-19 22:32:05,686] [    INFO][0m - loss: 4.566e-05, learning_rate: 7.142857142857143e-07, global_step: 3690, interval_runtime: 8.0329, interval_samples_per_second: 1.992, interval_steps_per_second: 1.245, epoch: 19.5238[0m
[32m[2022-09-19 22:32:13,721] [    INFO][0m - loss: 5.356e-05, learning_rate: 6.349206349206349e-07, global_step: 3700, interval_runtime: 8.0345, interval_samples_per_second: 1.991, interval_steps_per_second: 1.245, epoch: 19.5767[0m
[32m[2022-09-19 22:32:21,752] [    INFO][0m - loss: 0.00769742, learning_rate: 5.555555555555555e-07, global_step: 3710, interval_runtime: 8.0315, interval_samples_per_second: 1.992, interval_steps_per_second: 1.245, epoch: 19.6296[0m
[32m[2022-09-19 22:32:29,752] [    INFO][0m - loss: 5.591e-05, learning_rate: 4.761904761904762e-07, global_step: 3720, interval_runtime: 7.9994, interval_samples_per_second: 2.0, interval_steps_per_second: 1.25, epoch: 19.6825[0m
[32m[2022-09-19 22:32:37,776] [    INFO][0m - loss: 0.0210284, learning_rate: 3.9682539682539683e-07, global_step: 3730, interval_runtime: 8.0247, interval_samples_per_second: 1.994, interval_steps_per_second: 1.246, epoch: 19.7354[0m
[32m[2022-09-19 22:32:45,769] [    INFO][0m - loss: 7.005e-05, learning_rate: 3.1746031746031743e-07, global_step: 3740, interval_runtime: 7.9923, interval_samples_per_second: 2.002, interval_steps_per_second: 1.251, epoch: 19.7884[0m
[32m[2022-09-19 22:32:53,759] [    INFO][0m - loss: 0.00263875, learning_rate: 2.380952380952381e-07, global_step: 3750, interval_runtime: 7.9907, interval_samples_per_second: 2.002, interval_steps_per_second: 1.251, epoch: 19.8413[0m
[32m[2022-09-19 22:33:01,762] [    INFO][0m - loss: 0.00265734, learning_rate: 1.5873015873015872e-07, global_step: 3760, interval_runtime: 8.0024, interval_samples_per_second: 1.999, interval_steps_per_second: 1.25, epoch: 19.8942[0m
[32m[2022-09-19 22:33:09,753] [    INFO][0m - loss: 4.759e-05, learning_rate: 7.936507936507936e-08, global_step: 3770, interval_runtime: 7.9917, interval_samples_per_second: 2.002, interval_steps_per_second: 1.251, epoch: 19.9471[0m
[32m[2022-09-19 22:33:17,569] [    INFO][0m - loss: 0.01744836, learning_rate: 0.0, global_step: 3780, interval_runtime: 7.8152, interval_samples_per_second: 2.047, interval_steps_per_second: 1.28, epoch: 20.0[0m
[32m[2022-09-19 22:33:17,569] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 22:33:17,569] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-19 22:33:17,569] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 22:33:17,569] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 22:33:17,569] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-19 22:33:45,119] [    INFO][0m - eval_loss: 5.0379743576049805, eval_accuracy: 0.4537509104151493, eval_runtime: 27.5491, eval_samples_per_second: 49.838, eval_steps_per_second: 3.122, epoch: 20.0[0m
[32m[2022-09-19 22:33:45,143] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-3780[0m
[32m[2022-09-19 22:33:45,143] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 22:33:47,716] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-3780/tokenizer_config.json[0m
[32m[2022-09-19 22:33:47,717] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-3780/special_tokens_map.json[0m
[32m[2022-09-19 22:33:52,467] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-19 22:33:52,467] [    INFO][0m - Loading best model from ./checkpoints_iflytek/checkpoint-2457 (score: 0.4559359067734887).[0m
[32m[2022-09-19 22:33:54,078] [    INFO][0m - train_runtime: 3737.1994, train_samples_per_second: 16.183, train_steps_per_second: 1.011, train_loss: 0.2863883254586389, epoch: 20.0[0m
[32m[2022-09-19 22:33:54,133] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/[0m
[32m[2022-09-19 22:33:54,134] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 22:34:01,053] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/tokenizer_config.json[0m
[32m[2022-09-19 22:34:01,053] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/special_tokens_map.json[0m
[32m[2022-09-19 22:34:01,055] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-19 22:34:01,055] [    INFO][0m -   epoch                    =       20.0[0m
[32m[2022-09-19 22:34:01,055] [    INFO][0m -   train_loss               =     0.2864[0m
[32m[2022-09-19 22:34:01,055] [    INFO][0m -   train_runtime            = 1:02:17.19[0m
[32m[2022-09-19 22:34:01,055] [    INFO][0m -   train_samples_per_second =     16.183[0m
[32m[2022-09-19 22:34:01,055] [    INFO][0m -   train_steps_per_second   =      1.011[0m
[32m[2022-09-19 22:34:01,069] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-19 22:34:01,070] [    INFO][0m -   Num examples = 1749[0m
[32m[2022-09-19 22:34:01,070] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 22:34:01,070] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 22:34:01,070] [    INFO][0m -   Total prediction steps = 110[0m
[32m[2022-09-19 22:34:36,147] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-19 22:34:36,148] [    INFO][0m -   test_accuracy           =     0.4831[0m
[32m[2022-09-19 22:34:36,148] [    INFO][0m -   test_loss               =     4.6509[0m
[32m[2022-09-19 22:34:36,148] [    INFO][0m -   test_runtime            = 0:00:35.07[0m
[32m[2022-09-19 22:34:36,148] [    INFO][0m -   test_samples_per_second =     49.861[0m
[32m[2022-09-19 22:34:36,148] [    INFO][0m -   test_steps_per_second   =      3.136[0m
[32m[2022-09-19 22:34:36,149] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-19 22:34:36,149] [    INFO][0m -   Num examples = 2600[0m
[32m[2022-09-19 22:34:36,149] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 22:34:36,149] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 22:34:36,149] [    INFO][0m -   Total prediction steps = 163[0m
[32m[2022-09-19 22:35:34,594] [    INFO][0m - Predictions for iflytekf saved to ./fewclue_submit_examples.[0m
{
  "labels": 77,
  "text_a": "\u884c\u8baf\u901a\u662f\u5e7f\u5dde\u4ea4\u901a\u4fe1\u606f\u5316\u5efa\u8bbe\u6295\u8d44\u8425\u8fd0\u6709\u9650\u516c\u53f8\u5728\u5e7f\u5dde\u5e02\u4ea4\u901a\u8fd0\u8f93\u5c40\u7684\u6307\u5bfc\u4e0b\uff0c\u63a8\u51fa\u7684\u4e00\u6b3e\u63d0\u4f9b\u4ea4\u901a\u4fe1\u606f\u670d\u52a1\u7684\u624b\u673a\u7ec8\u7aef\u8f6f\u4ef6\uff0c\u652f\u6301Android\u548ciPhone\u7cfb\u7edf\u3002\u4e3b\u8981\u5305\u62ec\u8def\u51b5\u4fe1\u606f\u3001\u5b9e\u65f6\u516c\u4ea4\u3001\u505c\u8f66\u670d\u52a1\u3001\u7684\u58eb\u67e5\u8be2\u3001\u51fa\u884c\u89c4\u5212\u3001\u5730\u94c1\u4fe1\u606f\u3001\u822a\u7a7a\u4fe1\u606f\u3001\u94c1\u8def\u4fe1\u606f\u3001\u5ba2\u8fd0\u4fe1\u606f\u3001\u9a7e\u57f9\u4fe1\u606f\u3001\u4ea4\u901a\u8d44\u8baf\u3001WIFI\u70ed\u70b9\u7b49\u529f\u80fd\u6a21\u5757\u3002\u66f4\u65b0\u5185\u5bb91\u3001\u4f18\u5316\u5df2\u77e5bug\u3002",
  "text_b": "",
  "uid": 0
}

Prediction done.
 
==========
ocnli
==========
 
[32m[2022-09-19 22:35:50,123] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-19 22:35:50,123] [    INFO][0m - ============================================================[0m
[32m[2022-09-19 22:35:50,123] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-19 22:35:50,123] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-19 22:35:50,124] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-19 22:35:50,124] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-19 22:35:50,124] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-19 22:35:50,124] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-19 22:35:50,124] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-19 22:35:50,124] [    INFO][0m - [0m
[32m[2022-09-19 22:35:50,124] [    INFO][0m - ============================================================[0m
[32m[2022-09-19 22:35:50,124] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-19 22:35:50,124] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-19 22:35:50,124] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-19 22:35:50,124] [    INFO][0m - pretrained                    :/ssd2/wanghuijuan03/data/zero-shot/checkpoints_09191451/checkpoint-21000/model_state.pdparams[0m
[32m[2022-09-19 22:35:50,124] [    INFO][0m - prompt                        :‚Äú{'text':'text_a'}‚ÄùÂíå‚Äú{'text':'text_b'}‚Äù‰πãÈó¥ÁöÑÈÄªËæëÂÖ≥Á≥ªÊòØ{'mask'}{'mask'}„ÄÇ[0m
[32m[2022-09-19 22:35:50,124] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-19 22:35:50,125] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-19 22:35:50,125] [    INFO][0m - task_name                     :ocnli[0m
[32m[2022-09-19 22:35:50,125] [    INFO][0m - [0m
[32m[2022-09-19 22:35:50,125] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0919 22:35:50.126459 16812 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0919 22:35:50.130695 16812 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-19 22:35:55,343] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-19 22:35:55,354] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-19 22:35:55,354] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-19 22:35:55,355] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùÂíå‚Äú'}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': '‚Äù‰πãÈó¥ÁöÑÈÄªËæëÂÖ≥Á≥ªÊòØ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': '„ÄÇ'}][0m
{
  "labels": 2,
  "text_a": "\u4e03\u4e94\u671f\u95f4\u5f00\u59cb,\u56fd\u5bb6\u53c8\u6295\u8d44\u5c06\u6b66\u6c49\u5e02\u533a\u7684\u90e8\u5206\u571f\u5824\u6539\u5efa\u4e3a\u94a2\u7b4b\u6ce5\u51dd\u571f\u9632\u6c34\u5899",
  "text_b": "\u516b\u4e94\u671f\u95f4\u4f1a\u628a\u5269\u4e0b\u7684\u571f\u5824\u90fd\u6539\u5efa\u5b8c",
  "uid": 0
}

Traceback (most recent call last):
  File "train_single.py", line 232, in <module>
    main()
  File "train_single.py", line 115, in main
    state_dict = paddle.load(data_args.pretrained)
  File "/ssd2/wanghuijuan03/anaconda3/envs/prompt/lib/python3.8/site-packages/paddle/framework/io.py", line 1007, in load
    load_result = _legacy_load(path, **configs)
  File "/ssd2/wanghuijuan03/anaconda3/envs/prompt/lib/python3.8/site-packages/paddle/framework/io.py", line 1025, in _legacy_load
    model_path, config = _build_load_path_and_config(path, config)
  File "/ssd2/wanghuijuan03/anaconda3/envs/prompt/lib/python3.8/site-packages/paddle/framework/io.py", line 169, in _build_load_path_and_config
    raise ValueError(error_msg % path)
ValueError: The ``path`` (/ssd2/wanghuijuan03/data/zero-shot/checkpoints_09191451/checkpoint-21000/model_state.pdparams) to load model not exists.
 
==========
bustm
==========
 
[32m[2022-09-19 22:35:59,539] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-19 22:35:59,540] [    INFO][0m - ============================================================[0m
[32m[2022-09-19 22:35:59,540] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-19 22:35:59,540] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-19 22:35:59,540] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-19 22:35:59,540] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-19 22:35:59,540] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-19 22:35:59,540] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-19 22:35:59,540] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-19 22:35:59,540] [    INFO][0m - [0m
[32m[2022-09-19 22:35:59,540] [    INFO][0m - ============================================================[0m
[32m[2022-09-19 22:35:59,540] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-19 22:35:59,541] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-19 22:35:59,541] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-19 22:35:59,541] [    INFO][0m - pretrained                    :/ssd2/wanghuijuan03/data/zero-shot/checkpoints_09191451/checkpoint-21000/model_state.pdparams[0m
[32m[2022-09-19 22:35:59,541] [    INFO][0m - prompt                        :‚Äú{'text':'text_a'}‚ÄùÂíå‚Äú{'text':'text_b'}‚ÄùÊèèËø∞ÁöÑÊòØ{'mask'}{'mask'}ÁöÑ‰∫ãÊÉÖ„ÄÇ[0m
[32m[2022-09-19 22:35:59,541] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-19 22:35:59,541] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-19 22:35:59,541] [    INFO][0m - task_name                     :bustm[0m
[32m[2022-09-19 22:35:59,541] [    INFO][0m - [0m
[32m[2022-09-19 22:35:59,541] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0919 22:35:59.543412 17092 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0919 22:35:59.547675 17092 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-19 22:36:04,761] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-19 22:36:04,772] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-19 22:36:04,772] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-19 22:36:04,773] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùÂíå‚Äú'}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': '‚ÄùÊèèËø∞ÁöÑÊòØ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'ÁöÑ‰∫ãÊÉÖ„ÄÇ'}][0m
{
  "labels": 1,
  "text_a": "\u53eb\u7238\u7238\u53eb\u4e00\u58f0\u6211\u542c\u542c",
  "text_b": "\u90a3\u4f60\u53eb\u6211\u4e00\u58f0\u7238\u7238",
  "uid": 0
}

Traceback (most recent call last):
  File "train_single.py", line 232, in <module>
    main()
  File "train_single.py", line 115, in main
    state_dict = paddle.load(data_args.pretrained)
  File "/ssd2/wanghuijuan03/anaconda3/envs/prompt/lib/python3.8/site-packages/paddle/framework/io.py", line 1007, in load
    load_result = _legacy_load(path, **configs)
  File "/ssd2/wanghuijuan03/anaconda3/envs/prompt/lib/python3.8/site-packages/paddle/framework/io.py", line 1025, in _legacy_load
    model_path, config = _build_load_path_and_config(path, config)
  File "/ssd2/wanghuijuan03/anaconda3/envs/prompt/lib/python3.8/site-packages/paddle/framework/io.py", line 169, in _build_load_path_and_config
    raise ValueError(error_msg % path)
ValueError: The ``path`` (/ssd2/wanghuijuan03/data/zero-shot/checkpoints_09191451/checkpoint-21000/model_state.pdparams) to load model not exists.
 
==========
chid
==========
 
[32m[2022-09-19 22:36:08,724] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-19 22:36:08,724] [    INFO][0m - ============================================================[0m
[32m[2022-09-19 22:36:08,725] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-19 22:36:08,725] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-19 22:36:08,725] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-19 22:36:08,725] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-19 22:36:08,725] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-19 22:36:08,725] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-19 22:36:08,725] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-19 22:36:08,725] [    INFO][0m - [0m
[32m[2022-09-19 22:36:08,725] [    INFO][0m - ============================================================[0m
[32m[2022-09-19 22:36:08,725] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-19 22:36:08,725] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-19 22:36:08,725] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-19 22:36:08,726] [    INFO][0m - pretrained                    :/ssd2/wanghuijuan03/data/zero-shot/checkpoints_09191451/checkpoint-21000/model_state.pdparams[0m
[32m[2022-09-19 22:36:08,726] [    INFO][0m - prompt                        :‚Äú{'text':'text_a'}‚ÄùËøôÂè•ËØù‰∏≠ÊàêËØ≠[{'text':'text_b'}]ÁöÑÁêÜËß£Ê≠£Á°ÆÂêóÔºü{'mask'}{'mask'}„ÄÇ[0m
[32m[2022-09-19 22:36:08,726] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-19 22:36:08,726] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-19 22:36:08,726] [    INFO][0m - task_name                     :chid[0m
[32m[2022-09-19 22:36:08,726] [    INFO][0m - [0m
[32m[2022-09-19 22:36:08,726] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0919 22:36:08.728171 17326 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0919 22:36:08.732241 17326 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-19 22:36:13,721] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-19 22:36:13,732] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-19 22:36:13,732] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-19 22:36:13,733] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùËøôÂè•ËØù‰∏≠ÊàêËØ≠['}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': ']ÁöÑÁêÜËß£Ê≠£Á°ÆÂêóÔºü'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': '„ÄÇ'}][0m
{
  "labels": 0,
  "text_a": "\u5927\u90e8\u5206\u7684\u5927\u529f\u544a\u6210\u90fd\u662f\u7531\u65e0\u6570\u4e2a\u9020\u8c23\u751f\u4e8b\u7d2f\u79ef\u800c\u6765,\u505a\u597d\u6bcf\u4e00\u4ef6\u5e94\u8be5\u505a\u597d\u7684\u5c0f\u4e8b\u7edd\u4e0d\u662f\u4e00\u4ef6\u5c0f\u4e8b\u3002 \u300a\u4f60\u597d\u674e\u7115\u82f1\u300b\u4e3b\u8981\u8bb2\u4e86\u8d3e\u73b2\u610f\u5916\u7a7f\u8d8a\u5230\u4e86\u6bcd\u4eb2\u5e74\u8f7b\u7684\u65f6\u5019,\u548c\u6bcd\u4eb2\u4e00\u8d77\u611f\u53d7\u4e86\u5979\u7684\u751f\u6d3b\u548c\u60c5\u611f\u3002\u8fd9\u6545\u4e8b\u662f\u6839\u636e\u8d3e\u73b2\u7684\u4eb2\u751f\u7ecf\u5386\u6539\u7f16\u800c\u6765, ...",
  "text_b": "",
  "uid": 24
}

Traceback (most recent call last):
  File "train_single.py", line 232, in <module>
    main()
  File "train_single.py", line 115, in main
    state_dict = paddle.load(data_args.pretrained)
  File "/ssd2/wanghuijuan03/anaconda3/envs/prompt/lib/python3.8/site-packages/paddle/framework/io.py", line 1007, in load
    load_result = _legacy_load(path, **configs)
  File "/ssd2/wanghuijuan03/anaconda3/envs/prompt/lib/python3.8/site-packages/paddle/framework/io.py", line 1025, in _legacy_load
    model_path, config = _build_load_path_and_config(path, config)
  File "/ssd2/wanghuijuan03/anaconda3/envs/prompt/lib/python3.8/site-packages/paddle/framework/io.py", line 169, in _build_load_path_and_config
    raise ValueError(error_msg % path)
ValueError: The ``path`` (/ssd2/wanghuijuan03/data/zero-shot/checkpoints_09191451/checkpoint-21000/model_state.pdparams) to load model not exists.
 
==========
csl
==========
 
[32m[2022-09-19 22:36:17,740] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-19 22:36:17,740] [    INFO][0m - ============================================================[0m
[32m[2022-09-19 22:36:17,740] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-19 22:36:17,740] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-19 22:36:17,740] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-19 22:36:17,740] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-19 22:36:17,740] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-19 22:36:17,741] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-19 22:36:17,741] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-19 22:36:17,741] [    INFO][0m - [0m
[32m[2022-09-19 22:36:17,741] [    INFO][0m - ============================================================[0m
[32m[2022-09-19 22:36:17,741] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-19 22:36:17,741] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-19 22:36:17,741] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-19 22:36:17,741] [    INFO][0m - pretrained                    :/ssd2/wanghuijuan03/data/zero-shot/checkpoints_09191451/checkpoint-21000/model_state.pdparams[0m
[32m[2022-09-19 22:36:17,741] [    INFO][0m - prompt                        :‚Äú{'text':'text_a'}‚ÄùÊú¨ÊñáÁöÑÂÜÖÂÆπ{'mask'}{'mask'}‚Äú{'text':'text_b'}‚Äù[0m
[32m[2022-09-19 22:36:17,741] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-19 22:36:17,741] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-19 22:36:17,741] [    INFO][0m - task_name                     :csl[0m
[32m[2022-09-19 22:36:17,741] [    INFO][0m - [0m
[32m[2022-09-19 22:36:17,742] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0919 22:36:17.743669 17546 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0919 22:36:17.747877 17546 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-19 22:36:22,636] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-19 22:36:22,647] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-19 22:36:22,647] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-19 22:36:22,648] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùÊú¨ÊñáÁöÑÂÜÖÂÆπ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': '‚Äù'}][0m
{
  "labels": 1,
  "text_a": "\u65e0\u7ebf\u4f20\u611f\u5668\u7f51\u7edc\u4e2d\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u901a\u7528\u8fd1\u4f3c\u67e5\u8be2\u662f\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\uff0e\u6587\u4e2d\u63d0\u51fa\u4e00\u79cd\u65e0\u7ebf\u4f20\u611f\u5668\u7f51\u7edc\u4e2d\u9690\u79c1\u4fdd\u62a4\u901a\u7528\u8fd1\u4f3c\u67e5\u8be2\u534f\u8baePGAQ\uff0ePGAQ\u5c06\u4f20\u611f\u5668\u8282\u70b9\u7f16\u53f7\u548c\u5176\u91c7\u96c6\u6570\u636e\u9690\u85cf\u4e8e\u8bbe\u8ba1\u7684\u6570\u636e\u7ed3\u6784\u4e2d\uff0c\u5728\u57fa\u7ad9\u6784\u9020\u7ebf\u6027\u65b9\u7a0b\u7ec4\u89e3\u51fa\u76f4\u65b9\u56fe\uff0c\u6839\u636e\u76f4\u65b9\u56fe\u5177\u6709\u7684\u7edf\u8ba1\u4fe1\u606f\uff0c\u4e0d\u6cc4\u9732\u9690\u79c1\u5730\u5b8c\u6210Top-k\u67e5\u8be2\u3001\u8303\u56f4\u67e5\u8be2\u3001SUM\u3001MAX/MIN\u3001Median\u3001Histogram\u7b49\u8fd1\u4f3c\u67e5\u8be2\uff0ePGAQ\u4f7f\u7528\u7f51\u5185\u6c42\u548c\u805a\u96c6\u4ee5\u51cf\u5c11\u80fd\u91cf\u6d88\u8017\uff0c\u5e76\u4e14\u80fd\u591f\u901a\u8fc7\u8c03\u8282\u76f4\u65b9\u56fe\u5212\u5206\u7c92\u5ea6\u6765\u5e73\u8861\u67e5\u8be2\u7cbe\u5ea6\u4e0e\u80fd\u91cf\u6d88\u8017\uff0ePGAQ\u534f\u8bae\u5206\u4e3aH-PGAQ\u548cF-PGAQ\u4e24\u79cd\u6a21\u5f0f\uff0eH-PGAQ\u6a21\u5f0f\u4f7f\u7528\u6570\u636e\u6270\u52a8\u6280\u672f\u52a0\u5f3a\u6570\u636e\u5b89\u5168\u6027\uff0cF-PGAQ\u4f7f\u7528\u8fc7\u6ee4\u5668\u51cf\u5c11\u8fde\u7eed\u67e5\u8be2\u901a\u4fe1\u91cf\uff0e\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u4f7f\u7528\u771f\u5b9e\u6570\u636e\u96c6\u5b9e\u9a8c\u9a8c\u8bc1\u4e86PGAQ\u7684\u5b89\u5168\u6027\u548c\u6709\u6548\u6027\uff0e",
  "text_b": "\u65e0\u7ebf\u4f20\u611f\u5668\u7f51\u7edc\u3001\u6570\u636e\u805a\u96c6\u3001\u7269\u8054\u7f51\u3001\u8fd1\u4f3c\u67e5\u8be2",
  "uid": 0
}

Traceback (most recent call last):
  File "train_single.py", line 232, in <module>
    main()
  File "train_single.py", line 115, in main
    state_dict = paddle.load(data_args.pretrained)
  File "/ssd2/wanghuijuan03/anaconda3/envs/prompt/lib/python3.8/site-packages/paddle/framework/io.py", line 1007, in load
    load_result = _legacy_load(path, **configs)
  File "/ssd2/wanghuijuan03/anaconda3/envs/prompt/lib/python3.8/site-packages/paddle/framework/io.py", line 1025, in _legacy_load
    model_path, config = _build_load_path_and_config(path, config)
  File "/ssd2/wanghuijuan03/anaconda3/envs/prompt/lib/python3.8/site-packages/paddle/framework/io.py", line 169, in _build_load_path_and_config
    raise ValueError(error_msg % path)
ValueError: The ``path`` (/ssd2/wanghuijuan03/data/zero-shot/checkpoints_09191451/checkpoint-21000/model_state.pdparams) to load model not exists.
 
==========
cluewsc
==========
 
[32m[2022-09-19 22:36:26,903] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-19 22:36:26,903] [    INFO][0m - ============================================================[0m
[32m[2022-09-19 22:36:26,903] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-19 22:36:26,903] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-19 22:36:26,903] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-19 22:36:26,903] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-19 22:36:26,904] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-19 22:36:26,904] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-19 22:36:26,904] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-19 22:36:26,904] [    INFO][0m - [0m
[32m[2022-09-19 22:36:26,904] [    INFO][0m - ============================================================[0m
[32m[2022-09-19 22:36:26,904] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-19 22:36:26,904] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-19 22:36:26,904] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-19 22:36:26,904] [    INFO][0m - pretrained                    :/ssd2/wanghuijuan03/data/zero-shot/checkpoints_09191451/checkpoint-21000/model_state.pdparams[0m
[32m[2022-09-19 22:36:26,904] [    INFO][0m - prompt                        :‚Äú{'text':'text_a'}‚Äù{'text':'text_b'}ËøôÈáå‰ª£ËØç‰ΩøÁî®Ê≠£Á°ÆÂêóÔºü{'mask'}{'mask'}[0m
[32m[2022-09-19 22:36:26,904] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-19 22:36:26,904] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-19 22:36:26,904] [    INFO][0m - task_name                     :cluewsc[0m
[32m[2022-09-19 22:36:26,904] [    INFO][0m - [0m
[32m[2022-09-19 22:36:26,905] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0919 22:36:26.906283 17773 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0919 22:36:26.910456 17773 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-19 22:36:31,723] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-19 22:36:31,736] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-19 22:36:31,736] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-19 22:36:31,737] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚Äù'}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': 'ËøôÈáå‰ª£ËØç‰ΩøÁî®Ê≠£Á°ÆÂêóÔºü'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}][0m
{
  "labels": 0,
  "text_a": "\u4e3a\u4ec0\u4e48\u8981\u51fa\u73b0\u4e00\u4e2a\u8eab\u7a7f\u519b\u88c5\u7684\u9ad8\u5927\u7537\u4eba\uff1f\u5c31\u50cf\u4e00\u7247\u6811\u53f6\u98d8\u5165\u4e86\u6811\u6797\uff0c\u4ed6\u8d70\u5230\u4e86\u6211\u7684\u5bb6\u4eba\u4e2d\u95f4\u3002",
  "text_b": "\u5176\u4e2d\u4ed6\u6307\u7684\u662f\u6811\u53f6"
}

Traceback (most recent call last):
  File "train_single.py", line 232, in <module>
    main()
  File "train_single.py", line 115, in main
    state_dict = paddle.load(data_args.pretrained)
  File "/ssd2/wanghuijuan03/anaconda3/envs/prompt/lib/python3.8/site-packages/paddle/framework/io.py", line 1007, in load
    load_result = _legacy_load(path, **configs)
  File "/ssd2/wanghuijuan03/anaconda3/envs/prompt/lib/python3.8/site-packages/paddle/framework/io.py", line 1025, in _legacy_load
    model_path, config = _build_load_path_and_config(path, config)
  File "/ssd2/wanghuijuan03/anaconda3/envs/prompt/lib/python3.8/site-packages/paddle/framework/io.py", line 169, in _build_load_path_and_config
    raise ValueError(error_msg % path)
ValueError: The ``path`` (/ssd2/wanghuijuan03/data/zero-shot/checkpoints_09191451/checkpoint-21000/model_state.pdparams) to load model not exists.
