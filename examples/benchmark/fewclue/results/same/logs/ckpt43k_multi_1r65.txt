 
==========
eprstmt
==========
 
[32m[2022-09-20 10:16:22,840] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-20 10:16:22,840] [    INFO][0m - ============================================================[0m
[32m[2022-09-20 10:16:22,840] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-20 10:16:22,841] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-20 10:16:22,841] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-20 10:16:22,841] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-20 10:16:22,841] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-20 10:16:22,841] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-20 10:16:22,841] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-20 10:16:22,841] [    INFO][0m - [0m
[32m[2022-09-20 10:16:22,841] [    INFO][0m - ============================================================[0m
[32m[2022-09-20 10:16:22,841] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-20 10:16:22,841] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-20 10:16:22,841] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-20 10:16:22,841] [    INFO][0m - pretrained                    :/ssd2/wanghuijuan03/data/zero-shot/checkpoints_09191451/checkpoint-43000/model_state.pdparams[0m
[32m[2022-09-20 10:16:22,842] [    INFO][0m - prompt                        :‚Äú{'text':'text_a'}‚ÄùËøôÊù°ËØÑËÆ∫ÁöÑÊÉÖÊÑüÂÄæÂêëÊòØ{'mask'}{'mask'}ÁöÑ„ÄÇ[0m
[32m[2022-09-20 10:16:22,842] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-20 10:16:22,842] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-20 10:16:22,842] [    INFO][0m - task_name                     :eprstmt[0m
[32m[2022-09-20 10:16:22,842] [    INFO][0m - [0m
[32m[2022-09-20 10:16:22,842] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0920 10:16:22.844117  6016 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0920 10:16:22.848348  6016 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-20 10:16:27,618] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-20 10:16:27,629] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-20 10:16:27,629] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-20 10:16:27,630] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùËøôÊù°ËØÑËÆ∫ÁöÑÊÉÖÊÑüÂÄæÂêëÊòØ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'ÁöÑ„ÄÇ'}][0m
[32m[2022-09-20 10:16:29,584] [    INFO][0m - ============================================================[0m
[32m[2022-09-20 10:16:29,584] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-20 10:16:29,585] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-20 10:16:29,585] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-20 10:16:29,585] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-20 10:16:29,585] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-20 10:16:29,585] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-20 10:16:29,585] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-20 10:16:29,585] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-20 10:16:29,585] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-20 10:16:29,585] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-20 10:16:29,586] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-20 10:16:29,586] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-20 10:16:29,586] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-20 10:16:29,586] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-20 10:16:29,586] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-20 10:16:29,586] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-20 10:16:29,586] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-20 10:16:29,586] [    INFO][0m - eval_batch_size               :16[0m
[32m[2022-09-20 10:16:29,586] [    INFO][0m - eval_steps                    :200[0m
[32m[2022-09-20 10:16:29,586] [    INFO][0m - evaluation_strategy           :IntervalStrategy.EPOCH[0m
[32m[2022-09-20 10:16:29,586] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-20 10:16:29,586] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-20 10:16:29,587] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-20 10:16:29,587] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-20 10:16:29,587] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-20 10:16:29,587] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-20 10:16:29,587] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-20 10:16:29,587] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-20 10:16:29,587] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-20 10:16:29,587] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-09-20 10:16:29,587] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-20 10:16:29,587] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-20 10:16:29,587] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-20 10:16:29,587] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-20 10:16:29,587] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-20 10:16:29,587] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-20 10:16:29,588] [    INFO][0m - logging_dir                   :./checkpoints_eprstmt/runs/Sep20_10-16-22_instance-3bwob41y-01[0m
[32m[2022-09-20 10:16:29,588] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-20 10:16:29,588] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-20 10:16:29,588] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-20 10:16:29,588] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-20 10:16:29,588] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-20 10:16:29,588] [    INFO][0m - max_seq_length                :128[0m
[32m[2022-09-20 10:16:29,588] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-20 10:16:29,588] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-20 10:16:29,588] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-20 10:16:29,588] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-20 10:16:29,588] [    INFO][0m - num_train_epochs              :20.0[0m
[32m[2022-09-20 10:16:29,588] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-20 10:16:29,589] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-20 10:16:29,589] [    INFO][0m - output_dir                    :./checkpoints_eprstmt/[0m
[32m[2022-09-20 10:16:29,589] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-20 10:16:29,589] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-20 10:16:29,589] [    INFO][0m - per_device_eval_batch_size    :16[0m
[32m[2022-09-20 10:16:29,589] [    INFO][0m - per_device_train_batch_size   :16[0m
[32m[2022-09-20 10:16:29,589] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-20 10:16:29,589] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-20 10:16:29,589] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-20 10:16:29,589] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-09-20 10:16:29,589] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-20 10:16:29,589] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-20 10:16:29,589] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-20 10:16:29,589] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-20 10:16:29,590] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-20 10:16:29,590] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-20 10:16:29,590] [    INFO][0m - run_name                      :./checkpoints_eprstmt/[0m
[32m[2022-09-20 10:16:29,590] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-20 10:16:29,590] [    INFO][0m - save_steps                    :200[0m
[32m[2022-09-20 10:16:29,590] [    INFO][0m - save_strategy                 :IntervalStrategy.EPOCH[0m
[32m[2022-09-20 10:16:29,590] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-20 10:16:29,590] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-20 10:16:29,590] [    INFO][0m - seed                          :42[0m
[32m[2022-09-20 10:16:29,590] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-20 10:16:29,590] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-20 10:16:29,590] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-20 10:16:29,590] [    INFO][0m - train_batch_size              :16[0m
[32m[2022-09-20 10:16:29,591] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-20 10:16:29,591] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-20 10:16:29,591] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-20 10:16:29,591] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-20 10:16:29,591] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-20 10:16:29,591] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-20 10:16:29,591] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-20 10:16:29,591] [    INFO][0m - [0m
[32m[2022-09-20 10:16:29,594] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-20 10:16:29,595] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 10:16:29,595] [    INFO][0m -   Num Epochs = 20[0m
[32m[2022-09-20 10:16:29,595] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2022-09-20 10:16:29,595] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2022-09-20 10:16:29,595] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-20 10:16:29,595] [    INFO][0m -   Total optimization steps = 200.0[0m
[32m[2022-09-20 10:16:29,595] [    INFO][0m -   Total num train samples = 3200[0m
[32m[2022-09-20 10:16:33,991] [    INFO][0m - loss: 1.57690821, learning_rate: 2.8499999999999998e-05, global_step: 10, interval_runtime: 4.3949, interval_samples_per_second: 3.641, interval_steps_per_second: 2.275, epoch: 1.0[0m
[32m[2022-09-20 10:16:33,993] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 10:16:33,993] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 10:16:33,993] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 10:16:33,993] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 10:16:33,993] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 10:16:35,359] [    INFO][0m - eval_loss: 1.0190824270248413, eval_accuracy: 0.53125, eval_runtime: 1.3652, eval_samples_per_second: 117.198, eval_steps_per_second: 7.325, epoch: 1.0[0m
[32m[2022-09-20 10:16:35,359] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-10[0m
[32m[2022-09-20 10:16:35,360] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 10:16:38,432] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-10/tokenizer_config.json[0m
[32m[2022-09-20 10:16:38,433] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-10/special_tokens_map.json[0m
[32m[2022-09-20 10:16:47,810] [    INFO][0m - loss: 0.42450948, learning_rate: 2.7000000000000002e-05, global_step: 20, interval_runtime: 13.819, interval_samples_per_second: 1.158, interval_steps_per_second: 0.724, epoch: 2.0[0m
[32m[2022-09-20 10:16:47,811] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 10:16:47,811] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 10:16:47,811] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 10:16:47,812] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 10:16:47,812] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 10:16:49,163] [    INFO][0m - eval_loss: 0.4349232614040375, eval_accuracy: 0.88125, eval_runtime: 1.3514, eval_samples_per_second: 118.393, eval_steps_per_second: 7.4, epoch: 2.0[0m
[32m[2022-09-20 10:16:49,164] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-20[0m
[32m[2022-09-20 10:16:49,164] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 10:16:52,093] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-20/tokenizer_config.json[0m
[32m[2022-09-20 10:16:52,093] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-20/special_tokens_map.json[0m
[32m[2022-09-20 10:17:01,410] [    INFO][0m - loss: 0.18610442, learning_rate: 2.55e-05, global_step: 30, interval_runtime: 13.5997, interval_samples_per_second: 1.176, interval_steps_per_second: 0.735, epoch: 3.0[0m
[32m[2022-09-20 10:17:01,411] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 10:17:01,411] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 10:17:01,411] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 10:17:01,411] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 10:17:01,411] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 10:17:02,747] [    INFO][0m - eval_loss: 0.5736470818519592, eval_accuracy: 0.875, eval_runtime: 1.3352, eval_samples_per_second: 119.83, eval_steps_per_second: 7.489, epoch: 3.0[0m
[32m[2022-09-20 10:17:02,748] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-30[0m
[32m[2022-09-20 10:17:02,748] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 10:17:05,513] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-30/tokenizer_config.json[0m
[32m[2022-09-20 10:17:05,514] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-30/special_tokens_map.json[0m
[32m[2022-09-20 10:17:14,471] [    INFO][0m - loss: 0.06060318, learning_rate: 2.4e-05, global_step: 40, interval_runtime: 13.0616, interval_samples_per_second: 1.225, interval_steps_per_second: 0.766, epoch: 4.0[0m
[32m[2022-09-20 10:17:14,472] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 10:17:14,472] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 10:17:14,472] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 10:17:14,472] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 10:17:14,473] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 10:17:15,814] [    INFO][0m - eval_loss: 0.5128839015960693, eval_accuracy: 0.9, eval_runtime: 1.341, eval_samples_per_second: 119.318, eval_steps_per_second: 7.457, epoch: 4.0[0m
[32m[2022-09-20 10:17:15,815] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-40[0m
[32m[2022-09-20 10:17:15,815] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 10:17:18,835] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-40/tokenizer_config.json[0m
[32m[2022-09-20 10:17:18,835] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-40/special_tokens_map.json[0m
[32m[2022-09-20 10:17:30,372] [    INFO][0m - loss: 0.01082991, learning_rate: 2.25e-05, global_step: 50, interval_runtime: 15.9004, interval_samples_per_second: 1.006, interval_steps_per_second: 0.629, epoch: 5.0[0m
[32m[2022-09-20 10:17:30,372] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 10:17:30,372] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 10:17:30,373] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 10:17:30,373] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 10:17:30,373] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 10:17:31,711] [    INFO][0m - eval_loss: 1.2356489896774292, eval_accuracy: 0.875, eval_runtime: 1.3379, eval_samples_per_second: 119.587, eval_steps_per_second: 7.474, epoch: 5.0[0m
[32m[2022-09-20 10:17:31,712] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-50[0m
[32m[2022-09-20 10:17:31,712] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 10:17:34,308] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-50/tokenizer_config.json[0m
[32m[2022-09-20 10:17:34,308] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-50/special_tokens_map.json[0m
[32m[2022-09-20 10:17:43,202] [    INFO][0m - loss: 0.01475964, learning_rate: 2.1e-05, global_step: 60, interval_runtime: 12.8304, interval_samples_per_second: 1.247, interval_steps_per_second: 0.779, epoch: 6.0[0m
[32m[2022-09-20 10:17:43,203] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 10:17:43,203] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 10:17:43,203] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 10:17:43,203] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 10:17:43,203] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 10:17:44,548] [    INFO][0m - eval_loss: 1.3805820941925049, eval_accuracy: 0.89375, eval_runtime: 1.3447, eval_samples_per_second: 118.99, eval_steps_per_second: 7.437, epoch: 6.0[0m
[32m[2022-09-20 10:17:44,549] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-60[0m
[32m[2022-09-20 10:17:44,549] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 10:17:48,201] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-60/tokenizer_config.json[0m
[32m[2022-09-20 10:17:48,202] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-60/special_tokens_map.json[0m
[32m[2022-09-20 10:17:57,840] [    INFO][0m - loss: 3.104e-05, learning_rate: 1.95e-05, global_step: 70, interval_runtime: 13.5296, interval_samples_per_second: 1.183, interval_steps_per_second: 0.739, epoch: 7.0[0m
[32m[2022-09-20 10:17:57,841] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 10:17:57,841] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 10:17:57,841] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 10:17:57,842] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 10:17:57,842] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 10:17:59,180] [    INFO][0m - eval_loss: 1.784287452697754, eval_accuracy: 0.875, eval_runtime: 1.3378, eval_samples_per_second: 119.604, eval_steps_per_second: 7.475, epoch: 7.0[0m
[32m[2022-09-20 10:17:59,180] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-70[0m
[32m[2022-09-20 10:17:59,180] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 10:18:01,918] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-70/tokenizer_config.json[0m
[32m[2022-09-20 10:18:01,918] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-70/special_tokens_map.json[0m
[32m[2022-09-20 10:18:10,336] [    INFO][0m - loss: 4.1e-06, learning_rate: 1.8e-05, global_step: 80, interval_runtime: 13.6047, interval_samples_per_second: 1.176, interval_steps_per_second: 0.735, epoch: 8.0[0m
[32m[2022-09-20 10:18:10,337] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 10:18:10,337] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 10:18:10,337] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 10:18:10,337] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 10:18:10,337] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 10:18:11,673] [    INFO][0m - eval_loss: 1.9764066934585571, eval_accuracy: 0.86875, eval_runtime: 1.3353, eval_samples_per_second: 119.824, eval_steps_per_second: 7.489, epoch: 8.0[0m
[32m[2022-09-20 10:18:13,275] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-80[0m
[32m[2022-09-20 10:18:13,275] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 10:18:15,860] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-80/tokenizer_config.json[0m
[32m[2022-09-20 10:18:15,861] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-80/special_tokens_map.json[0m
[32m[2022-09-20 10:18:29,073] [    INFO][0m - loss: 2.1e-06, learning_rate: 1.65e-05, global_step: 90, interval_runtime: 18.7368, interval_samples_per_second: 0.854, interval_steps_per_second: 0.534, epoch: 9.0[0m
[32m[2022-09-20 10:18:29,074] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 10:18:29,074] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 10:18:29,074] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 10:18:29,074] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 10:18:29,074] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 10:18:30,404] [    INFO][0m - eval_loss: 2.0308496952056885, eval_accuracy: 0.88125, eval_runtime: 1.3302, eval_samples_per_second: 120.278, eval_steps_per_second: 7.517, epoch: 9.0[0m
[32m[2022-09-20 10:18:30,405] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-90[0m
[32m[2022-09-20 10:18:30,405] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 10:18:32,971] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-90/tokenizer_config.json[0m
[32m[2022-09-20 10:18:32,972] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-90/special_tokens_map.json[0m
[32m[2022-09-20 10:18:42,794] [    INFO][0m - loss: 2.41e-06, learning_rate: 1.5e-05, global_step: 100, interval_runtime: 13.7211, interval_samples_per_second: 1.166, interval_steps_per_second: 0.729, epoch: 10.0[0m
[32m[2022-09-20 10:18:42,795] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 10:18:42,795] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 10:18:42,795] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 10:18:42,795] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 10:18:42,796] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 10:18:44,116] [    INFO][0m - eval_loss: 2.056640148162842, eval_accuracy: 0.88125, eval_runtime: 1.3208, eval_samples_per_second: 121.142, eval_steps_per_second: 7.571, epoch: 10.0[0m
[32m[2022-09-20 10:18:44,117] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-100[0m
[32m[2022-09-20 10:18:44,117] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 10:18:46,534] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-20 10:18:46,535] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-20 10:18:54,686] [    INFO][0m - loss: 2.27e-06, learning_rate: 1.3500000000000001e-05, global_step: 110, interval_runtime: 11.8918, interval_samples_per_second: 1.345, interval_steps_per_second: 0.841, epoch: 11.0[0m
[32m[2022-09-20 10:18:56,828] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 10:18:56,828] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 10:18:56,828] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 10:18:56,831] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 10:18:56,832] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 10:18:58,173] [    INFO][0m - eval_loss: 2.072646379470825, eval_accuracy: 0.88125, eval_runtime: 1.3448, eval_samples_per_second: 118.975, eval_steps_per_second: 7.436, epoch: 11.0[0m
[32m[2022-09-20 10:18:58,174] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-110[0m
[32m[2022-09-20 10:18:58,174] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 10:19:00,991] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-110/tokenizer_config.json[0m
[32m[2022-09-20 10:19:00,992] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-110/special_tokens_map.json[0m
[32m[2022-09-20 10:19:09,777] [    INFO][0m - loss: 1.73e-06, learning_rate: 1.2e-05, global_step: 120, interval_runtime: 15.0908, interval_samples_per_second: 1.06, interval_steps_per_second: 0.663, epoch: 12.0[0m
[32m[2022-09-20 10:19:09,777] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 10:19:09,778] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 10:19:09,778] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 10:19:09,778] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 10:19:09,778] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 10:19:11,126] [    INFO][0m - eval_loss: 2.0869879722595215, eval_accuracy: 0.88125, eval_runtime: 1.3474, eval_samples_per_second: 118.75, eval_steps_per_second: 7.422, epoch: 12.0[0m
[32m[2022-09-20 10:19:11,126] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-120[0m
[32m[2022-09-20 10:19:11,126] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 10:19:13,599] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-120/tokenizer_config.json[0m
[32m[2022-09-20 10:19:13,599] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-120/special_tokens_map.json[0m
[32m[2022-09-20 10:19:21,773] [    INFO][0m - loss: 4.13e-06, learning_rate: 1.05e-05, global_step: 130, interval_runtime: 11.9958, interval_samples_per_second: 1.334, interval_steps_per_second: 0.834, epoch: 13.0[0m
[32m[2022-09-20 10:19:21,774] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 10:19:21,774] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 10:19:21,774] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 10:19:21,774] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 10:19:21,774] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 10:19:23,115] [    INFO][0m - eval_loss: 2.0983011722564697, eval_accuracy: 0.88125, eval_runtime: 1.341, eval_samples_per_second: 119.318, eval_steps_per_second: 7.457, epoch: 13.0[0m
[32m[2022-09-20 10:19:23,116] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-130[0m
[32m[2022-09-20 10:19:23,116] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 10:19:25,593] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-130/tokenizer_config.json[0m
[32m[2022-09-20 10:19:25,593] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-130/special_tokens_map.json[0m
[32m[2022-09-20 10:19:33,847] [    INFO][0m - loss: 1.37e-06, learning_rate: 9e-06, global_step: 140, interval_runtime: 12.0744, interval_samples_per_second: 1.325, interval_steps_per_second: 0.828, epoch: 14.0[0m
[32m[2022-09-20 10:19:33,874] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 10:19:33,874] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 10:19:33,874] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 10:19:33,874] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 10:19:33,875] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 10:19:35,202] [    INFO][0m - eval_loss: 2.1099884510040283, eval_accuracy: 0.88125, eval_runtime: 1.327, eval_samples_per_second: 120.57, eval_steps_per_second: 7.536, epoch: 14.0[0m
[32m[2022-09-20 10:19:35,202] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-140[0m
[32m[2022-09-20 10:19:35,202] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 10:19:37,574] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-140/tokenizer_config.json[0m
[32m[2022-09-20 10:19:37,574] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-140/special_tokens_map.json[0m
[32m[2022-09-20 10:19:45,733] [    INFO][0m - loss: 1.4e-06, learning_rate: 7.5e-06, global_step: 150, interval_runtime: 11.8858, interval_samples_per_second: 1.346, interval_steps_per_second: 0.841, epoch: 15.0[0m
[32m[2022-09-20 10:19:45,733] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 10:19:45,733] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 10:19:45,733] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 10:19:45,733] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 10:19:45,734] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 10:19:47,072] [    INFO][0m - eval_loss: 2.1186881065368652, eval_accuracy: 0.88125, eval_runtime: 1.3384, eval_samples_per_second: 119.542, eval_steps_per_second: 7.471, epoch: 15.0[0m
[32m[2022-09-20 10:19:47,073] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-150[0m
[32m[2022-09-20 10:19:47,073] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 10:19:49,529] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-150/tokenizer_config.json[0m
[32m[2022-09-20 10:19:50,022] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-150/special_tokens_map.json[0m
[32m[2022-09-20 10:19:58,180] [    INFO][0m - loss: 1.3e-06, learning_rate: 6e-06, global_step: 160, interval_runtime: 12.447, interval_samples_per_second: 1.285, interval_steps_per_second: 0.803, epoch: 16.0[0m
[32m[2022-09-20 10:19:58,180] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 10:19:58,181] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 10:19:58,181] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 10:19:58,181] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 10:19:58,181] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 10:19:59,504] [    INFO][0m - eval_loss: 2.124302387237549, eval_accuracy: 0.88125, eval_runtime: 1.3228, eval_samples_per_second: 120.959, eval_steps_per_second: 7.56, epoch: 16.0[0m
[32m[2022-09-20 10:19:59,504] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-160[0m
[32m[2022-09-20 10:19:59,504] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 10:20:01,884] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-160/tokenizer_config.json[0m
[32m[2022-09-20 10:20:01,884] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-160/special_tokens_map.json[0m
[32m[2022-09-20 10:20:10,150] [    INFO][0m - loss: 1.58e-06, learning_rate: 4.5e-06, global_step: 170, interval_runtime: 11.9702, interval_samples_per_second: 1.337, interval_steps_per_second: 0.835, epoch: 17.0[0m
[32m[2022-09-20 10:20:10,150] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 10:20:10,150] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 10:20:10,151] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 10:20:10,151] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 10:20:10,151] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 10:20:11,473] [    INFO][0m - eval_loss: 2.128437042236328, eval_accuracy: 0.88125, eval_runtime: 1.3217, eval_samples_per_second: 121.059, eval_steps_per_second: 7.566, epoch: 17.0[0m
[32m[2022-09-20 10:20:11,473] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-170[0m
[32m[2022-09-20 10:20:11,474] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 10:20:13,864] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-170/tokenizer_config.json[0m
[32m[2022-09-20 10:20:13,865] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-170/special_tokens_map.json[0m
[32m[2022-09-20 10:20:21,984] [    INFO][0m - loss: 1.02e-06, learning_rate: 3e-06, global_step: 180, interval_runtime: 11.8341, interval_samples_per_second: 1.352, interval_steps_per_second: 0.845, epoch: 18.0[0m
[32m[2022-09-20 10:20:21,985] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 10:20:21,985] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 10:20:21,985] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 10:20:21,985] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 10:20:21,985] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 10:20:23,303] [    INFO][0m - eval_loss: 2.1311848163604736, eval_accuracy: 0.88125, eval_runtime: 1.3176, eval_samples_per_second: 121.43, eval_steps_per_second: 7.589, epoch: 18.0[0m
[32m[2022-09-20 10:20:23,303] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-180[0m
[32m[2022-09-20 10:20:23,303] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 10:20:26,573] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-180/tokenizer_config.json[0m
[32m[2022-09-20 10:20:26,574] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-180/special_tokens_map.json[0m
[32m[2022-09-20 10:20:34,677] [    INFO][0m - loss: 1.48e-06, learning_rate: 1.5e-06, global_step: 190, interval_runtime: 12.6934, interval_samples_per_second: 1.26, interval_steps_per_second: 0.788, epoch: 19.0[0m
[32m[2022-09-20 10:20:34,678] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 10:20:34,678] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 10:20:34,678] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 10:20:34,678] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 10:20:34,678] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 10:20:37,383] [    INFO][0m - eval_loss: 2.1330032348632812, eval_accuracy: 0.88125, eval_runtime: 1.335, eval_samples_per_second: 119.848, eval_steps_per_second: 7.491, epoch: 19.0[0m
[32m[2022-09-20 10:20:37,383] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-190[0m
[32m[2022-09-20 10:20:37,383] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 10:20:39,782] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-190/tokenizer_config.json[0m
[32m[2022-09-20 10:20:39,782] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-190/special_tokens_map.json[0m
[32m[2022-09-20 10:20:47,911] [    INFO][0m - loss: 1.43e-06, learning_rate: 0.0, global_step: 200, interval_runtime: 13.2335, interval_samples_per_second: 1.209, interval_steps_per_second: 0.756, epoch: 20.0[0m
[32m[2022-09-20 10:20:47,911] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 10:20:47,911] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 10:20:47,911] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 10:20:47,912] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 10:20:47,912] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 10:20:49,235] [    INFO][0m - eval_loss: 2.133671998977661, eval_accuracy: 0.88125, eval_runtime: 1.3232, eval_samples_per_second: 120.915, eval_steps_per_second: 7.557, epoch: 20.0[0m
[32m[2022-09-20 10:20:50,520] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-200[0m
[32m[2022-09-20 10:20:50,520] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 10:20:52,880] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-200/tokenizer_config.json[0m
[32m[2022-09-20 10:20:52,881] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-200/special_tokens_map.json[0m
[32m[2022-09-20 10:20:57,635] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-20 10:20:57,636] [    INFO][0m - Loading best model from ./checkpoints_eprstmt/checkpoint-40 (score: 0.9).[0m
[32m[2022-09-20 10:20:59,214] [    INFO][0m - train_runtime: 269.6185, train_samples_per_second: 11.869, train_steps_per_second: 0.742, train_loss: 0.11368860911780303, epoch: 20.0[0m
[32m[2022-09-20 10:20:59,256] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/[0m
[32m[2022-09-20 10:20:59,256] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 10:21:01,583] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/tokenizer_config.json[0m
[32m[2022-09-20 10:21:01,584] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/special_tokens_map.json[0m
[32m[2022-09-20 10:21:01,584] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-20 10:21:01,585] [    INFO][0m -   epoch                    =       20.0[0m
[32m[2022-09-20 10:21:01,585] [    INFO][0m -   train_loss               =     0.1137[0m
[32m[2022-09-20 10:21:01,585] [    INFO][0m -   train_runtime            = 0:04:29.61[0m
[32m[2022-09-20 10:21:01,585] [    INFO][0m -   train_samples_per_second =     11.869[0m
[32m[2022-09-20 10:21:01,585] [    INFO][0m -   train_steps_per_second   =      0.742[0m
[32m[2022-09-20 10:21:01,587] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-20 10:21:01,588] [    INFO][0m -   Num examples = 610[0m
[32m[2022-09-20 10:21:01,588] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 10:21:01,588] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 10:21:01,588] [    INFO][0m -   Total prediction steps = 39[0m
[32m[2022-09-20 10:21:06,850] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-20 10:21:06,851] [    INFO][0m -   test_accuracy           =     0.9131[0m
[32m[2022-09-20 10:21:06,851] [    INFO][0m -   test_loss               =     0.4115[0m
[32m[2022-09-20 10:21:06,851] [    INFO][0m -   test_runtime            = 0:00:05.26[0m
[32m[2022-09-20 10:21:06,851] [    INFO][0m -   test_samples_per_second =    115.905[0m
[32m[2022-09-20 10:21:06,851] [    INFO][0m -   test_steps_per_second   =       7.41[0m
[32m[2022-09-20 10:21:06,851] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-20 10:21:06,851] [    INFO][0m -   Num examples = 753[0m
[32m[2022-09-20 10:21:06,851] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 10:21:06,852] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 10:21:06,852] [    INFO][0m -   Total prediction steps = 48[0m
[32m[2022-09-20 10:21:14,024] [    INFO][0m - Predictions for eprstmt saved to ./fewclue_submit_examples.[0m
{
  "labels": 0,
  "text_a": "\u7269\u6d41\u5f88\u5feb\uff0c\u65e9\u4e0a\u4e0b\u5355\uff0c\u4e0b\u5348\u5c31\u5230\u4e86\u3002\u5305\u88c5\u4e5f\u5f88\u9ad8\u6863\u3002\u5c31\u662f\u8033\u673a\u97f3\u8d28\u5f88\u5dee\uff0c\u7172\u4e86\u4e00\u767e\u591a\u5c0f\u65f6\uff0c\u97f3\u8d28\u548c\u540c\u4e8b\u7684\u4e00\u767e\u591a\u5143\u7684\u8033\u673a\u5dee\u4e0d\u591a\uff0c1580\u5143\u4e70\u8fd9\u8033\u673a\u4e8f\u5927\u4e86\u3002",
  "text_b": "",
  "uid": 0
}

Prediction done.
 
==========
csldcp
==========
 
[32m[2022-09-20 10:21:30,641] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-20 10:21:30,642] [    INFO][0m - ============================================================[0m
[32m[2022-09-20 10:21:30,642] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-20 10:21:30,642] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-20 10:21:30,642] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-20 10:21:30,642] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-20 10:21:30,642] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-20 10:21:30,642] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-20 10:21:30,642] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-20 10:21:30,643] [    INFO][0m - [0m
[32m[2022-09-20 10:21:30,643] [    INFO][0m - ============================================================[0m
[32m[2022-09-20 10:21:30,643] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-20 10:21:30,643] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-20 10:21:30,643] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-20 10:21:30,643] [    INFO][0m - pretrained                    :/ssd2/wanghuijuan03/data/zero-shot/checkpoints_09191451/checkpoint-43000/model_state.pdparams[0m
[32m[2022-09-20 10:21:30,643] [    INFO][0m - prompt                        :‚Äú{'text':'text_a'}‚ÄùËøôÁØáÊñáÁåÆÁöÑÁ±ªÂà´ÊòØ{'mask'}{'mask'}„ÄÇ[0m
[32m[2022-09-20 10:21:30,643] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-20 10:21:30,643] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-20 10:21:30,643] [    INFO][0m - task_name                     :csldcp[0m
[32m[2022-09-20 10:21:30,643] [    INFO][0m - [0m
[32m[2022-09-20 10:21:30,644] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0920 10:21:30.645649 14998 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0920 10:21:30.649699 14998 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-20 10:21:35,448] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-20 10:21:35,459] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-20 10:21:35,460] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-20 10:21:35,460] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùËøôÁØáÊñáÁåÆÁöÑÁ±ªÂà´ÊòØ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': '„ÄÇ'}][0m
[32m[2022-09-20 10:21:36,859] [    INFO][0m - ============================================================[0m
[32m[2022-09-20 10:21:36,860] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-20 10:21:36,860] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-20 10:21:36,860] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-20 10:21:36,860] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-20 10:21:36,860] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-20 10:21:36,862] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-20 10:21:36,862] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-20 10:21:36,862] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-20 10:21:36,862] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-20 10:21:36,862] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-20 10:21:36,862] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-20 10:21:36,862] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-20 10:21:36,862] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-20 10:21:36,862] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-20 10:21:36,862] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-20 10:21:36,862] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-20 10:21:36,862] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-20 10:21:36,863] [    INFO][0m - eval_batch_size               :16[0m
[32m[2022-09-20 10:21:36,863] [    INFO][0m - eval_steps                    :200[0m
[32m[2022-09-20 10:21:36,863] [    INFO][0m - evaluation_strategy           :IntervalStrategy.EPOCH[0m
[32m[2022-09-20 10:21:36,863] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-20 10:21:36,863] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-20 10:21:36,863] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-20 10:21:36,863] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-20 10:21:36,863] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-20 10:21:36,863] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-20 10:21:36,863] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-20 10:21:36,863] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-20 10:21:36,863] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-20 10:21:36,863] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-09-20 10:21:36,864] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-20 10:21:36,864] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-20 10:21:36,864] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-20 10:21:36,864] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-20 10:21:36,864] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-20 10:21:36,864] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-20 10:21:36,864] [    INFO][0m - logging_dir                   :./checkpoints_csldcp/runs/Sep20_10-21-30_instance-3bwob41y-01[0m
[32m[2022-09-20 10:21:36,864] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-20 10:21:36,864] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-20 10:21:36,864] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-20 10:21:36,864] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-20 10:21:36,864] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-20 10:21:36,864] [    INFO][0m - max_seq_length                :256[0m
[32m[2022-09-20 10:21:36,865] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-20 10:21:36,865] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-20 10:21:36,865] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-20 10:21:36,865] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-20 10:21:36,865] [    INFO][0m - num_train_epochs              :20.0[0m
[32m[2022-09-20 10:21:36,865] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-20 10:21:36,865] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-20 10:21:36,865] [    INFO][0m - output_dir                    :./checkpoints_csldcp/[0m
[32m[2022-09-20 10:21:36,865] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-20 10:21:36,865] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-20 10:21:36,865] [    INFO][0m - per_device_eval_batch_size    :16[0m
[32m[2022-09-20 10:21:36,865] [    INFO][0m - per_device_train_batch_size   :16[0m
[32m[2022-09-20 10:21:36,865] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-20 10:21:36,866] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-20 10:21:36,866] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-20 10:21:36,866] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-09-20 10:21:36,866] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-20 10:21:36,866] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-20 10:21:36,866] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-20 10:21:36,866] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-20 10:21:36,866] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-20 10:21:36,866] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-20 10:21:36,866] [    INFO][0m - run_name                      :./checkpoints_csldcp/[0m
[32m[2022-09-20 10:21:36,866] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-20 10:21:36,866] [    INFO][0m - save_steps                    :200[0m
[32m[2022-09-20 10:21:36,866] [    INFO][0m - save_strategy                 :IntervalStrategy.EPOCH[0m
[32m[2022-09-20 10:21:36,866] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-20 10:21:36,867] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-20 10:21:36,867] [    INFO][0m - seed                          :42[0m
[32m[2022-09-20 10:21:36,867] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-20 10:21:36,867] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-20 10:21:36,867] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-20 10:21:36,867] [    INFO][0m - train_batch_size              :16[0m
[32m[2022-09-20 10:21:36,867] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-20 10:21:36,867] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-20 10:21:36,867] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-20 10:21:36,867] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-20 10:21:36,867] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-20 10:21:36,867] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-20 10:21:36,867] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-20 10:21:36,868] [    INFO][0m - [0m
[32m[2022-09-20 10:21:36,871] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-20 10:21:36,871] [    INFO][0m -   Num examples = 2036[0m
[32m[2022-09-20 10:21:36,871] [    INFO][0m -   Num Epochs = 20[0m
[32m[2022-09-20 10:21:36,872] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2022-09-20 10:21:36,872] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2022-09-20 10:21:36,872] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-20 10:21:36,872] [    INFO][0m -   Total optimization steps = 2560.0[0m
[32m[2022-09-20 10:21:36,872] [    INFO][0m -   Total num train samples = 40720[0m
[33m[2022-09-20 10:21:36,906] [ WARNING][0m - Token indices sequence length is longer than the specified maximum sequence length for this model (517 > 512). Running this sequence through the model will result in indexing errors[0m
[32m[2022-09-20 10:21:44,355] [    INFO][0m - loss: 2.92556648, learning_rate: 2.9882812500000002e-05, global_step: 10, interval_runtime: 7.4823, interval_samples_per_second: 2.138, interval_steps_per_second: 1.336, epoch: 0.0781[0m
[32m[2022-09-20 10:21:50,818] [    INFO][0m - loss: 2.31910629, learning_rate: 2.9765625e-05, global_step: 20, interval_runtime: 6.4619, interval_samples_per_second: 2.476, interval_steps_per_second: 1.548, epoch: 0.1562[0m
[32m[2022-09-20 10:21:57,301] [    INFO][0m - loss: 1.84943047, learning_rate: 2.96484375e-05, global_step: 30, interval_runtime: 6.4843, interval_samples_per_second: 2.467, interval_steps_per_second: 1.542, epoch: 0.2344[0m
[32m[2022-09-20 10:22:03,803] [    INFO][0m - loss: 1.91598339, learning_rate: 2.953125e-05, global_step: 40, interval_runtime: 6.5018, interval_samples_per_second: 2.461, interval_steps_per_second: 1.538, epoch: 0.3125[0m
[32m[2022-09-20 10:22:10,264] [    INFO][0m - loss: 1.81809139, learning_rate: 2.94140625e-05, global_step: 50, interval_runtime: 6.4611, interval_samples_per_second: 2.476, interval_steps_per_second: 1.548, epoch: 0.3906[0m
[32m[2022-09-20 10:22:16,720] [    INFO][0m - loss: 1.96566143, learning_rate: 2.9296875000000002e-05, global_step: 60, interval_runtime: 6.456, interval_samples_per_second: 2.478, interval_steps_per_second: 1.549, epoch: 0.4688[0m
[32m[2022-09-20 10:22:23,234] [    INFO][0m - loss: 2.10156345, learning_rate: 2.91796875e-05, global_step: 70, interval_runtime: 6.5133, interval_samples_per_second: 2.457, interval_steps_per_second: 1.535, epoch: 0.5469[0m
[32m[2022-09-20 10:22:29,720] [    INFO][0m - loss: 1.62603359, learning_rate: 2.90625e-05, global_step: 80, interval_runtime: 6.4869, interval_samples_per_second: 2.467, interval_steps_per_second: 1.542, epoch: 0.625[0m
[32m[2022-09-20 10:22:36,188] [    INFO][0m - loss: 1.52774229, learning_rate: 2.89453125e-05, global_step: 90, interval_runtime: 6.4677, interval_samples_per_second: 2.474, interval_steps_per_second: 1.546, epoch: 0.7031[0m
[32m[2022-09-20 10:22:42,693] [    INFO][0m - loss: 1.75808334, learning_rate: 2.8828125e-05, global_step: 100, interval_runtime: 6.5049, interval_samples_per_second: 2.46, interval_steps_per_second: 1.537, epoch: 0.7812[0m
[32m[2022-09-20 10:22:49,196] [    INFO][0m - loss: 1.52964573, learning_rate: 2.87109375e-05, global_step: 110, interval_runtime: 6.5026, interval_samples_per_second: 2.461, interval_steps_per_second: 1.538, epoch: 0.8594[0m
[32m[2022-09-20 10:22:55,680] [    INFO][0m - loss: 1.67823753, learning_rate: 2.859375e-05, global_step: 120, interval_runtime: 6.4847, interval_samples_per_second: 2.467, interval_steps_per_second: 1.542, epoch: 0.9375[0m
[32m[2022-09-20 10:23:00,283] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 10:23:00,283] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-20 10:23:00,283] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 10:23:00,283] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 10:23:00,283] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-20 10:23:33,664] [    INFO][0m - eval_loss: 1.5446507930755615, eval_accuracy: 0.4995164410058027, eval_runtime: 33.381, eval_samples_per_second: 61.951, eval_steps_per_second: 3.894, epoch: 1.0[0m
[32m[2022-09-20 10:23:33,690] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-128[0m
[32m[2022-09-20 10:23:33,690] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 10:23:36,459] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-128/tokenizer_config.json[0m
[32m[2022-09-20 10:23:36,459] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-128/special_tokens_map.json[0m
[32m[2022-09-20 10:23:43,021] [    INFO][0m - loss: 1.47261324, learning_rate: 2.8476562500000002e-05, global_step: 130, interval_runtime: 47.3411, interval_samples_per_second: 0.338, interval_steps_per_second: 0.211, epoch: 1.0156[0m
[32m[2022-09-20 10:23:49,463] [    INFO][0m - loss: 0.88587637, learning_rate: 2.8359375e-05, global_step: 140, interval_runtime: 6.4414, interval_samples_per_second: 2.484, interval_steps_per_second: 1.552, epoch: 1.0938[0m
[32m[2022-09-20 10:23:55,942] [    INFO][0m - loss: 0.86702423, learning_rate: 2.82421875e-05, global_step: 150, interval_runtime: 6.4788, interval_samples_per_second: 2.47, interval_steps_per_second: 1.543, epoch: 1.1719[0m
[32m[2022-09-20 10:24:02,410] [    INFO][0m - loss: 1.07618093, learning_rate: 2.8125e-05, global_step: 160, interval_runtime: 6.4683, interval_samples_per_second: 2.474, interval_steps_per_second: 1.546, epoch: 1.25[0m
[32m[2022-09-20 10:24:08,903] [    INFO][0m - loss: 0.96365566, learning_rate: 2.80078125e-05, global_step: 170, interval_runtime: 6.4932, interval_samples_per_second: 2.464, interval_steps_per_second: 1.54, epoch: 1.3281[0m
[32m[2022-09-20 10:24:15,393] [    INFO][0m - loss: 1.05845318, learning_rate: 2.7890625000000002e-05, global_step: 180, interval_runtime: 6.4902, interval_samples_per_second: 2.465, interval_steps_per_second: 1.541, epoch: 1.4062[0m
[32m[2022-09-20 10:24:22,142] [    INFO][0m - loss: 0.8197814, learning_rate: 2.77734375e-05, global_step: 190, interval_runtime: 6.7484, interval_samples_per_second: 2.371, interval_steps_per_second: 1.482, epoch: 1.4844[0m
[32m[2022-09-20 10:24:28,618] [    INFO][0m - loss: 0.97580719, learning_rate: 2.765625e-05, global_step: 200, interval_runtime: 6.4766, interval_samples_per_second: 2.47, interval_steps_per_second: 1.544, epoch: 1.5625[0m
[32m[2022-09-20 10:24:35,094] [    INFO][0m - loss: 1.00562372, learning_rate: 2.75390625e-05, global_step: 210, interval_runtime: 6.4761, interval_samples_per_second: 2.471, interval_steps_per_second: 1.544, epoch: 1.6406[0m
[32m[2022-09-20 10:24:41,583] [    INFO][0m - loss: 0.8619791, learning_rate: 2.7421875e-05, global_step: 220, interval_runtime: 6.4882, interval_samples_per_second: 2.466, interval_steps_per_second: 1.541, epoch: 1.7188[0m
[32m[2022-09-20 10:24:48,048] [    INFO][0m - loss: 0.82576685, learning_rate: 2.7304687500000002e-05, global_step: 230, interval_runtime: 6.4652, interval_samples_per_second: 2.475, interval_steps_per_second: 1.547, epoch: 1.7969[0m
[32m[2022-09-20 10:24:54,517] [    INFO][0m - loss: 0.91834812, learning_rate: 2.71875e-05, global_step: 240, interval_runtime: 6.4688, interval_samples_per_second: 2.473, interval_steps_per_second: 1.546, epoch: 1.875[0m
[32m[2022-09-20 10:25:00,998] [    INFO][0m - loss: 0.98108292, learning_rate: 2.70703125e-05, global_step: 250, interval_runtime: 6.4814, interval_samples_per_second: 2.469, interval_steps_per_second: 1.543, epoch: 1.9531[0m
[32m[2022-09-20 10:25:04,297] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 10:25:04,298] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-20 10:25:04,298] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 10:25:04,298] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 10:25:04,298] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-20 10:25:37,733] [    INFO][0m - eval_loss: 1.389407992362976, eval_accuracy: 0.543036750483559, eval_runtime: 33.4352, eval_samples_per_second: 61.851, eval_steps_per_second: 3.888, epoch: 2.0[0m
[32m[2022-09-20 10:25:37,759] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-256[0m
[32m[2022-09-20 10:25:37,760] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 10:25:40,404] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-256/tokenizer_config.json[0m
[32m[2022-09-20 10:25:40,404] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-256/special_tokens_map.json[0m
[32m[2022-09-20 10:25:48,320] [    INFO][0m - loss: 0.67564759, learning_rate: 2.6953125e-05, global_step: 260, interval_runtime: 47.3214, interval_samples_per_second: 0.338, interval_steps_per_second: 0.211, epoch: 2.0312[0m
[32m[2022-09-20 10:25:54,794] [    INFO][0m - loss: 0.45353546, learning_rate: 2.68359375e-05, global_step: 270, interval_runtime: 6.474, interval_samples_per_second: 2.471, interval_steps_per_second: 1.545, epoch: 2.1094[0m
[32m[2022-09-20 10:26:01,253] [    INFO][0m - loss: 0.44609585, learning_rate: 2.6718750000000002e-05, global_step: 280, interval_runtime: 6.4598, interval_samples_per_second: 2.477, interval_steps_per_second: 1.548, epoch: 2.1875[0m
[32m[2022-09-20 10:26:07,739] [    INFO][0m - loss: 0.4036387, learning_rate: 2.66015625e-05, global_step: 290, interval_runtime: 6.4858, interval_samples_per_second: 2.467, interval_steps_per_second: 1.542, epoch: 2.2656[0m
[32m[2022-09-20 10:26:14,218] [    INFO][0m - loss: 0.445716, learning_rate: 2.6484375000000002e-05, global_step: 300, interval_runtime: 6.4791, interval_samples_per_second: 2.469, interval_steps_per_second: 1.543, epoch: 2.3438[0m
[32m[2022-09-20 10:26:20,703] [    INFO][0m - loss: 0.58842416, learning_rate: 2.63671875e-05, global_step: 310, interval_runtime: 6.4843, interval_samples_per_second: 2.468, interval_steps_per_second: 1.542, epoch: 2.4219[0m
[32m[2022-09-20 10:26:27,173] [    INFO][0m - loss: 0.52641296, learning_rate: 2.625e-05, global_step: 320, interval_runtime: 6.4703, interval_samples_per_second: 2.473, interval_steps_per_second: 1.546, epoch: 2.5[0m
[32m[2022-09-20 10:26:33,620] [    INFO][0m - loss: 0.46423283, learning_rate: 2.61328125e-05, global_step: 330, interval_runtime: 6.4471, interval_samples_per_second: 2.482, interval_steps_per_second: 1.551, epoch: 2.5781[0m
[32m[2022-09-20 10:26:40,095] [    INFO][0m - loss: 0.589012, learning_rate: 2.6015625e-05, global_step: 340, interval_runtime: 6.4745, interval_samples_per_second: 2.471, interval_steps_per_second: 1.545, epoch: 2.6562[0m
[32m[2022-09-20 10:26:46,567] [    INFO][0m - loss: 0.57193189, learning_rate: 2.5898437500000002e-05, global_step: 350, interval_runtime: 6.4727, interval_samples_per_second: 2.472, interval_steps_per_second: 1.545, epoch: 2.7344[0m
[32m[2022-09-20 10:26:53,051] [    INFO][0m - loss: 0.61532884, learning_rate: 2.578125e-05, global_step: 360, interval_runtime: 6.4837, interval_samples_per_second: 2.468, interval_steps_per_second: 1.542, epoch: 2.8125[0m
[32m[2022-09-20 10:26:59,520] [    INFO][0m - loss: 0.47925615, learning_rate: 2.56640625e-05, global_step: 370, interval_runtime: 6.4692, interval_samples_per_second: 2.473, interval_steps_per_second: 1.546, epoch: 2.8906[0m
[32m[2022-09-20 10:27:05,981] [    INFO][0m - loss: 0.50064597, learning_rate: 2.5546875e-05, global_step: 380, interval_runtime: 6.4602, interval_samples_per_second: 2.477, interval_steps_per_second: 1.548, epoch: 2.9688[0m
[32m[2022-09-20 10:27:08,044] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 10:27:08,045] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-20 10:27:08,045] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 10:27:08,045] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 10:27:08,045] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-20 10:27:41,561] [    INFO][0m - eval_loss: 1.5535117387771606, eval_accuracy: 0.5667311411992263, eval_runtime: 33.5167, eval_samples_per_second: 61.701, eval_steps_per_second: 3.879, epoch: 3.0[0m
[32m[2022-09-20 10:27:41,596] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-384[0m
[32m[2022-09-20 10:27:41,597] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 10:27:44,251] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-384/tokenizer_config.json[0m
[32m[2022-09-20 10:27:44,252] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-384/special_tokens_map.json[0m
[32m[2022-09-20 10:27:53,492] [    INFO][0m - loss: 0.3411149, learning_rate: 2.54296875e-05, global_step: 390, interval_runtime: 47.5111, interval_samples_per_second: 0.337, interval_steps_per_second: 0.21, epoch: 3.0469[0m
[32m[2022-09-20 10:27:59,960] [    INFO][0m - loss: 0.28207397, learning_rate: 2.5312500000000002e-05, global_step: 400, interval_runtime: 6.4683, interval_samples_per_second: 2.474, interval_steps_per_second: 1.546, epoch: 3.125[0m
[32m[2022-09-20 10:28:06,513] [    INFO][0m - loss: 0.27917428, learning_rate: 2.51953125e-05, global_step: 410, interval_runtime: 6.5528, interval_samples_per_second: 2.442, interval_steps_per_second: 1.526, epoch: 3.2031[0m
[32m[2022-09-20 10:28:13,005] [    INFO][0m - loss: 0.16880071, learning_rate: 2.5078125e-05, global_step: 420, interval_runtime: 6.492, interval_samples_per_second: 2.465, interval_steps_per_second: 1.54, epoch: 3.2812[0m
[32m[2022-09-20 10:28:19,480] [    INFO][0m - loss: 0.23177862, learning_rate: 2.49609375e-05, global_step: 430, interval_runtime: 6.4757, interval_samples_per_second: 2.471, interval_steps_per_second: 1.544, epoch: 3.3594[0m
[32m[2022-09-20 10:28:25,959] [    INFO][0m - loss: 0.37642748, learning_rate: 2.484375e-05, global_step: 440, interval_runtime: 6.4791, interval_samples_per_second: 2.469, interval_steps_per_second: 1.543, epoch: 3.4375[0m
[32m[2022-09-20 10:28:32,423] [    INFO][0m - loss: 0.20075934, learning_rate: 2.4726562500000002e-05, global_step: 450, interval_runtime: 6.4639, interval_samples_per_second: 2.475, interval_steps_per_second: 1.547, epoch: 3.5156[0m
[32m[2022-09-20 10:28:38,893] [    INFO][0m - loss: 0.18630571, learning_rate: 2.4609375e-05, global_step: 460, interval_runtime: 6.4693, interval_samples_per_second: 2.473, interval_steps_per_second: 1.546, epoch: 3.5938[0m
[32m[2022-09-20 10:28:45,337] [    INFO][0m - loss: 0.23245184, learning_rate: 2.44921875e-05, global_step: 470, interval_runtime: 6.4442, interval_samples_per_second: 2.483, interval_steps_per_second: 1.552, epoch: 3.6719[0m
[32m[2022-09-20 10:28:51,804] [    INFO][0m - loss: 0.33988438, learning_rate: 2.4375e-05, global_step: 480, interval_runtime: 6.4669, interval_samples_per_second: 2.474, interval_steps_per_second: 1.546, epoch: 3.75[0m
[32m[2022-09-20 10:28:58,288] [    INFO][0m - loss: 0.27926176, learning_rate: 2.42578125e-05, global_step: 490, interval_runtime: 6.4842, interval_samples_per_second: 2.468, interval_steps_per_second: 1.542, epoch: 3.8281[0m
[32m[2022-09-20 10:29:04,776] [    INFO][0m - loss: 0.26805971, learning_rate: 2.4140625e-05, global_step: 500, interval_runtime: 6.4878, interval_samples_per_second: 2.466, interval_steps_per_second: 1.541, epoch: 3.9062[0m
[32m[2022-09-20 10:29:11,149] [    INFO][0m - loss: 0.27214184, learning_rate: 2.40234375e-05, global_step: 510, interval_runtime: 6.3738, interval_samples_per_second: 2.51, interval_steps_per_second: 1.569, epoch: 3.9844[0m
[32m[2022-09-20 10:29:11,950] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 10:29:11,951] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-20 10:29:11,951] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 10:29:11,951] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 10:29:11,951] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-20 10:29:45,215] [    INFO][0m - eval_loss: 2.058335065841675, eval_accuracy: 0.5672147001934236, eval_runtime: 33.2635, eval_samples_per_second: 62.17, eval_steps_per_second: 3.908, epoch: 4.0[0m
[32m[2022-09-20 10:29:45,249] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-512[0m
[32m[2022-09-20 10:29:45,249] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 10:29:47,823] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-512/tokenizer_config.json[0m
[32m[2022-09-20 10:29:47,824] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-512/special_tokens_map.json[0m
[32m[2022-09-20 10:29:58,124] [    INFO][0m - loss: 0.11056027, learning_rate: 2.3906250000000002e-05, global_step: 520, interval_runtime: 46.9746, interval_samples_per_second: 0.341, interval_steps_per_second: 0.213, epoch: 4.0625[0m
[32m[2022-09-20 10:30:04,609] [    INFO][0m - loss: 0.11029732, learning_rate: 2.37890625e-05, global_step: 530, interval_runtime: 6.4846, interval_samples_per_second: 2.467, interval_steps_per_second: 1.542, epoch: 4.1406[0m
[32m[2022-09-20 10:30:11,082] [    INFO][0m - loss: 0.04689704, learning_rate: 2.3671875e-05, global_step: 540, interval_runtime: 6.4731, interval_samples_per_second: 2.472, interval_steps_per_second: 1.545, epoch: 4.2188[0m
[32m[2022-09-20 10:30:17,546] [    INFO][0m - loss: 0.10883839, learning_rate: 2.35546875e-05, global_step: 550, interval_runtime: 6.4642, interval_samples_per_second: 2.475, interval_steps_per_second: 1.547, epoch: 4.2969[0m
[32m[2022-09-20 10:30:23,996] [    INFO][0m - loss: 0.09175029, learning_rate: 2.34375e-05, global_step: 560, interval_runtime: 6.4497, interval_samples_per_second: 2.481, interval_steps_per_second: 1.55, epoch: 4.375[0m
[32m[2022-09-20 10:30:30,486] [    INFO][0m - loss: 0.15809569, learning_rate: 2.3320312500000002e-05, global_step: 570, interval_runtime: 6.4901, interval_samples_per_second: 2.465, interval_steps_per_second: 1.541, epoch: 4.4531[0m
[32m[2022-09-20 10:30:36,992] [    INFO][0m - loss: 0.13298115, learning_rate: 2.3203125e-05, global_step: 580, interval_runtime: 6.5063, interval_samples_per_second: 2.459, interval_steps_per_second: 1.537, epoch: 4.5312[0m
[32m[2022-09-20 10:30:43,460] [    INFO][0m - loss: 0.13967204, learning_rate: 2.30859375e-05, global_step: 590, interval_runtime: 6.4674, interval_samples_per_second: 2.474, interval_steps_per_second: 1.546, epoch: 4.6094[0m
[32m[2022-09-20 10:30:49,930] [    INFO][0m - loss: 0.1049842, learning_rate: 2.296875e-05, global_step: 600, interval_runtime: 6.4701, interval_samples_per_second: 2.473, interval_steps_per_second: 1.546, epoch: 4.6875[0m
[32m[2022-09-20 10:30:56,394] [    INFO][0m - loss: 0.11560881, learning_rate: 2.28515625e-05, global_step: 610, interval_runtime: 6.4646, interval_samples_per_second: 2.475, interval_steps_per_second: 1.547, epoch: 4.7656[0m
[32m[2022-09-20 10:31:02,861] [    INFO][0m - loss: 0.17130355, learning_rate: 2.2734375000000002e-05, global_step: 620, interval_runtime: 6.4664, interval_samples_per_second: 2.474, interval_steps_per_second: 1.546, epoch: 4.8438[0m
[32m[2022-09-20 10:31:09,326] [    INFO][0m - loss: 0.18221213, learning_rate: 2.26171875e-05, global_step: 630, interval_runtime: 6.465, interval_samples_per_second: 2.475, interval_steps_per_second: 1.547, epoch: 4.9219[0m
[32m[2022-09-20 10:31:15,210] [    INFO][0m - loss: 0.07739874, learning_rate: 2.25e-05, global_step: 640, interval_runtime: 5.8849, interval_samples_per_second: 2.719, interval_steps_per_second: 1.699, epoch: 5.0[0m
[32m[2022-09-20 10:31:15,211] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 10:31:15,211] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-20 10:31:15,211] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 10:31:15,212] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 10:31:15,212] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-20 10:31:48,388] [    INFO][0m - eval_loss: 2.3696846961975098, eval_accuracy: 0.559477756286267, eval_runtime: 33.1762, eval_samples_per_second: 62.334, eval_steps_per_second: 3.918, epoch: 5.0[0m
[32m[2022-09-20 10:31:48,423] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-640[0m
[32m[2022-09-20 10:31:48,423] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 10:31:51,043] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-640/tokenizer_config.json[0m
[32m[2022-09-20 10:31:51,044] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-640/special_tokens_map.json[0m
[32m[2022-09-20 10:32:02,619] [    INFO][0m - loss: 0.04425769, learning_rate: 2.23828125e-05, global_step: 650, interval_runtime: 47.4081, interval_samples_per_second: 0.337, interval_steps_per_second: 0.211, epoch: 5.0781[0m
[32m[2022-09-20 10:32:09,071] [    INFO][0m - loss: 0.11078432, learning_rate: 2.2265625e-05, global_step: 660, interval_runtime: 6.4525, interval_samples_per_second: 2.48, interval_steps_per_second: 1.55, epoch: 5.1562[0m
[32m[2022-09-20 10:32:15,546] [    INFO][0m - loss: 0.07280442, learning_rate: 2.2148437500000002e-05, global_step: 670, interval_runtime: 6.4749, interval_samples_per_second: 2.471, interval_steps_per_second: 1.544, epoch: 5.2344[0m
[32m[2022-09-20 10:32:22,033] [    INFO][0m - loss: 0.07617981, learning_rate: 2.203125e-05, global_step: 680, interval_runtime: 6.487, interval_samples_per_second: 2.466, interval_steps_per_second: 1.542, epoch: 5.3125[0m
[32m[2022-09-20 10:32:28,504] [    INFO][0m - loss: 0.05184574, learning_rate: 2.19140625e-05, global_step: 690, interval_runtime: 6.4714, interval_samples_per_second: 2.472, interval_steps_per_second: 1.545, epoch: 5.3906[0m
[32m[2022-09-20 10:32:34,970] [    INFO][0m - loss: 0.06422214, learning_rate: 2.1796875e-05, global_step: 700, interval_runtime: 6.4653, interval_samples_per_second: 2.475, interval_steps_per_second: 1.547, epoch: 5.4688[0m
[32m[2022-09-20 10:32:41,439] [    INFO][0m - loss: 0.07124143, learning_rate: 2.16796875e-05, global_step: 710, interval_runtime: 6.4689, interval_samples_per_second: 2.473, interval_steps_per_second: 1.546, epoch: 5.5469[0m
[32m[2022-09-20 10:32:47,886] [    INFO][0m - loss: 0.12558694, learning_rate: 2.15625e-05, global_step: 720, interval_runtime: 6.4471, interval_samples_per_second: 2.482, interval_steps_per_second: 1.551, epoch: 5.625[0m
[32m[2022-09-20 10:32:54,362] [    INFO][0m - loss: 0.04099183, learning_rate: 2.14453125e-05, global_step: 730, interval_runtime: 6.4761, interval_samples_per_second: 2.471, interval_steps_per_second: 1.544, epoch: 5.7031[0m
[32m[2022-09-20 10:33:00,823] [    INFO][0m - loss: 0.13214149, learning_rate: 2.1328125000000002e-05, global_step: 740, interval_runtime: 6.4612, interval_samples_per_second: 2.476, interval_steps_per_second: 1.548, epoch: 5.7812[0m
[32m[2022-09-20 10:33:07,296] [    INFO][0m - loss: 0.01904085, learning_rate: 2.12109375e-05, global_step: 750, interval_runtime: 6.4733, interval_samples_per_second: 2.472, interval_steps_per_second: 1.545, epoch: 5.8594[0m
[32m[2022-09-20 10:33:13,772] [    INFO][0m - loss: 0.07460344, learning_rate: 2.109375e-05, global_step: 760, interval_runtime: 6.4761, interval_samples_per_second: 2.471, interval_steps_per_second: 1.544, epoch: 5.9375[0m
[32m[2022-09-20 10:33:18,371] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 10:33:18,371] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-20 10:33:18,371] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 10:33:18,371] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 10:33:18,371] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-20 10:33:51,642] [    INFO][0m - eval_loss: 2.7502007484436035, eval_accuracy: 0.5730174081237911, eval_runtime: 33.2707, eval_samples_per_second: 62.157, eval_steps_per_second: 3.907, epoch: 6.0[0m
[32m[2022-09-20 10:33:51,677] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-768[0m
[32m[2022-09-20 10:33:51,677] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 10:33:54,249] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-768/tokenizer_config.json[0m
[32m[2022-09-20 10:33:54,249] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-768/special_tokens_map.json[0m
[32m[2022-09-20 10:34:00,623] [    INFO][0m - loss: 0.01068857, learning_rate: 2.09765625e-05, global_step: 770, interval_runtime: 46.8511, interval_samples_per_second: 0.342, interval_steps_per_second: 0.213, epoch: 6.0156[0m
[32m[2022-09-20 10:34:07,099] [    INFO][0m - loss: 0.02789341, learning_rate: 2.0859375e-05, global_step: 780, interval_runtime: 6.4757, interval_samples_per_second: 2.471, interval_steps_per_second: 1.544, epoch: 6.0938[0m
[32m[2022-09-20 10:34:13,554] [    INFO][0m - loss: 0.1460485, learning_rate: 2.0742187500000002e-05, global_step: 790, interval_runtime: 6.455, interval_samples_per_second: 2.479, interval_steps_per_second: 1.549, epoch: 6.1719[0m
[32m[2022-09-20 10:34:20,033] [    INFO][0m - loss: 0.03181912, learning_rate: 2.0625e-05, global_step: 800, interval_runtime: 6.4787, interval_samples_per_second: 2.47, interval_steps_per_second: 1.544, epoch: 6.25[0m
[32m[2022-09-20 10:34:26,498] [    INFO][0m - loss: 0.03691537, learning_rate: 2.05078125e-05, global_step: 810, interval_runtime: 6.4654, interval_samples_per_second: 2.475, interval_steps_per_second: 1.547, epoch: 6.3281[0m
[32m[2022-09-20 10:34:32,961] [    INFO][0m - loss: 0.04617247, learning_rate: 2.0390625e-05, global_step: 820, interval_runtime: 6.4623, interval_samples_per_second: 2.476, interval_steps_per_second: 1.547, epoch: 6.4062[0m
[32m[2022-09-20 10:34:39,430] [    INFO][0m - loss: 0.01420732, learning_rate: 2.02734375e-05, global_step: 830, interval_runtime: 6.4696, interval_samples_per_second: 2.473, interval_steps_per_second: 1.546, epoch: 6.4844[0m
[32m[2022-09-20 10:34:45,909] [    INFO][0m - loss: 0.08449574, learning_rate: 2.0156250000000002e-05, global_step: 840, interval_runtime: 6.4794, interval_samples_per_second: 2.469, interval_steps_per_second: 1.543, epoch: 6.5625[0m
[32m[2022-09-20 10:34:52,362] [    INFO][0m - loss: 0.00979338, learning_rate: 2.00390625e-05, global_step: 850, interval_runtime: 6.453, interval_samples_per_second: 2.479, interval_steps_per_second: 1.55, epoch: 6.6406[0m
[32m[2022-09-20 10:34:58,811] [    INFO][0m - loss: 0.03476619, learning_rate: 1.9921875e-05, global_step: 860, interval_runtime: 6.4486, interval_samples_per_second: 2.481, interval_steps_per_second: 1.551, epoch: 6.7188[0m
[32m[2022-09-20 10:35:05,279] [    INFO][0m - loss: 0.11326827, learning_rate: 1.98046875e-05, global_step: 870, interval_runtime: 6.4681, interval_samples_per_second: 2.474, interval_steps_per_second: 1.546, epoch: 6.7969[0m
[32m[2022-09-20 10:35:11,768] [    INFO][0m - loss: 0.0435317, learning_rate: 1.96875e-05, global_step: 880, interval_runtime: 6.4884, interval_samples_per_second: 2.466, interval_steps_per_second: 1.541, epoch: 6.875[0m
[32m[2022-09-20 10:35:18,238] [    INFO][0m - loss: 0.02117181, learning_rate: 1.95703125e-05, global_step: 890, interval_runtime: 6.4707, interval_samples_per_second: 2.473, interval_steps_per_second: 1.545, epoch: 6.9531[0m
[32m[2022-09-20 10:35:21,543] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 10:35:21,544] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-20 10:35:21,544] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 10:35:21,544] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 10:35:21,544] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-20 10:35:54,799] [    INFO][0m - eval_loss: 2.976865768432617, eval_accuracy: 0.574468085106383, eval_runtime: 33.2547, eval_samples_per_second: 62.187, eval_steps_per_second: 3.909, epoch: 7.0[0m
[32m[2022-09-20 10:35:54,834] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-896[0m
[32m[2022-09-20 10:35:54,834] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 10:35:57,457] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-896/tokenizer_config.json[0m
[32m[2022-09-20 10:35:57,457] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-896/special_tokens_map.json[0m
[32m[2022-09-20 10:36:05,228] [    INFO][0m - loss: 0.04711215, learning_rate: 1.9453125e-05, global_step: 900, interval_runtime: 46.9895, interval_samples_per_second: 0.341, interval_steps_per_second: 0.213, epoch: 7.0312[0m
[32m[2022-09-20 10:36:11,704] [    INFO][0m - loss: 0.04946605, learning_rate: 1.93359375e-05, global_step: 910, interval_runtime: 6.4759, interval_samples_per_second: 2.471, interval_steps_per_second: 1.544, epoch: 7.1094[0m
[32m[2022-09-20 10:36:18,162] [    INFO][0m - loss: 0.0239831, learning_rate: 1.921875e-05, global_step: 920, interval_runtime: 6.4581, interval_samples_per_second: 2.478, interval_steps_per_second: 1.548, epoch: 7.1875[0m
[32m[2022-09-20 10:36:24,660] [    INFO][0m - loss: 0.01854251, learning_rate: 1.91015625e-05, global_step: 930, interval_runtime: 6.4977, interval_samples_per_second: 2.462, interval_steps_per_second: 1.539, epoch: 7.2656[0m
[32m[2022-09-20 10:36:31,139] [    INFO][0m - loss: 0.00800878, learning_rate: 1.8984375e-05, global_step: 940, interval_runtime: 6.4797, interval_samples_per_second: 2.469, interval_steps_per_second: 1.543, epoch: 7.3438[0m
[32m[2022-09-20 10:36:38,199] [    INFO][0m - loss: 0.02149022, learning_rate: 1.88671875e-05, global_step: 950, interval_runtime: 6.466, interval_samples_per_second: 2.474, interval_steps_per_second: 1.547, epoch: 7.4219[0m
[32m[2022-09-20 10:36:44,674] [    INFO][0m - loss: 0.05562533, learning_rate: 1.8750000000000002e-05, global_step: 960, interval_runtime: 7.0683, interval_samples_per_second: 2.264, interval_steps_per_second: 1.415, epoch: 7.5[0m
[32m[2022-09-20 10:36:51,139] [    INFO][0m - loss: 0.10117481, learning_rate: 1.86328125e-05, global_step: 970, interval_runtime: 6.4656, interval_samples_per_second: 2.475, interval_steps_per_second: 1.547, epoch: 7.5781[0m
[32m[2022-09-20 10:36:57,656] [    INFO][0m - loss: 0.0365156, learning_rate: 1.8515625e-05, global_step: 980, interval_runtime: 6.5169, interval_samples_per_second: 2.455, interval_steps_per_second: 1.534, epoch: 7.6562[0m
[32m[2022-09-20 10:37:04,129] [    INFO][0m - loss: 0.00943786, learning_rate: 1.83984375e-05, global_step: 990, interval_runtime: 6.4724, interval_samples_per_second: 2.472, interval_steps_per_second: 1.545, epoch: 7.7344[0m
[32m[2022-09-20 10:37:10,599] [    INFO][0m - loss: 0.00373431, learning_rate: 1.828125e-05, global_step: 1000, interval_runtime: 6.4711, interval_samples_per_second: 2.473, interval_steps_per_second: 1.545, epoch: 7.8125[0m
[32m[2022-09-20 10:37:17,060] [    INFO][0m - loss: 0.02182323, learning_rate: 1.8164062500000002e-05, global_step: 1010, interval_runtime: 6.4601, interval_samples_per_second: 2.477, interval_steps_per_second: 1.548, epoch: 7.8906[0m
[32m[2022-09-20 10:37:23,482] [    INFO][0m - loss: 0.03065539, learning_rate: 1.8046875e-05, global_step: 1020, interval_runtime: 6.4223, interval_samples_per_second: 2.491, interval_steps_per_second: 1.557, epoch: 7.9688[0m
[32m[2022-09-20 10:37:25,524] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 10:37:25,524] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-20 10:37:25,524] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 10:37:25,525] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 10:37:25,525] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-20 10:37:59,070] [    INFO][0m - eval_loss: 2.91798734664917, eval_accuracy: 0.5851063829787234, eval_runtime: 33.5446, eval_samples_per_second: 61.649, eval_steps_per_second: 3.875, epoch: 8.0[0m
[32m[2022-09-20 10:37:59,104] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-1024[0m
[32m[2022-09-20 10:37:59,104] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 10:38:01,672] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-1024/tokenizer_config.json[0m
[32m[2022-09-20 10:38:01,672] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-1024/special_tokens_map.json[0m
[32m[2022-09-20 10:38:10,559] [    INFO][0m - loss: 0.00199115, learning_rate: 1.79296875e-05, global_step: 1030, interval_runtime: 47.0766, interval_samples_per_second: 0.34, interval_steps_per_second: 0.212, epoch: 8.0469[0m
[32m[2022-09-20 10:38:17,008] [    INFO][0m - loss: 0.00359579, learning_rate: 1.78125e-05, global_step: 1040, interval_runtime: 6.4498, interval_samples_per_second: 2.481, interval_steps_per_second: 1.55, epoch: 8.125[0m
[32m[2022-09-20 10:38:23,476] [    INFO][0m - loss: 0.00301451, learning_rate: 1.76953125e-05, global_step: 1050, interval_runtime: 6.4676, interval_samples_per_second: 2.474, interval_steps_per_second: 1.546, epoch: 8.2031[0m
[32m[2022-09-20 10:38:29,942] [    INFO][0m - loss: 0.01435296, learning_rate: 1.7578125000000002e-05, global_step: 1060, interval_runtime: 6.4661, interval_samples_per_second: 2.474, interval_steps_per_second: 1.547, epoch: 8.2812[0m
[32m[2022-09-20 10:38:36,423] [    INFO][0m - loss: 0.03756289, learning_rate: 1.74609375e-05, global_step: 1070, interval_runtime: 6.4812, interval_samples_per_second: 2.469, interval_steps_per_second: 1.543, epoch: 8.3594[0m
[32m[2022-09-20 10:38:42,901] [    INFO][0m - loss: 0.00791752, learning_rate: 1.734375e-05, global_step: 1080, interval_runtime: 6.4777, interval_samples_per_second: 2.47, interval_steps_per_second: 1.544, epoch: 8.4375[0m
[32m[2022-09-20 10:38:49,364] [    INFO][0m - loss: 0.00334755, learning_rate: 1.72265625e-05, global_step: 1090, interval_runtime: 6.4628, interval_samples_per_second: 2.476, interval_steps_per_second: 1.547, epoch: 8.5156[0m
[32m[2022-09-20 10:38:55,828] [    INFO][0m - loss: 0.00145852, learning_rate: 1.7109375e-05, global_step: 1100, interval_runtime: 6.4647, interval_samples_per_second: 2.475, interval_steps_per_second: 1.547, epoch: 8.5938[0m
[32m[2022-09-20 10:39:02,306] [    INFO][0m - loss: 0.10608169, learning_rate: 1.69921875e-05, global_step: 1110, interval_runtime: 6.4775, interval_samples_per_second: 2.47, interval_steps_per_second: 1.544, epoch: 8.6719[0m
[32m[2022-09-20 10:39:08,782] [    INFO][0m - loss: 0.04510999, learning_rate: 1.6875e-05, global_step: 1120, interval_runtime: 6.4759, interval_samples_per_second: 2.471, interval_steps_per_second: 1.544, epoch: 8.75[0m
[32m[2022-09-20 10:39:15,265] [    INFO][0m - loss: 0.02707983, learning_rate: 1.67578125e-05, global_step: 1130, interval_runtime: 6.4835, interval_samples_per_second: 2.468, interval_steps_per_second: 1.542, epoch: 8.8281[0m
[32m[2022-09-20 10:39:21,754] [    INFO][0m - loss: 0.03861219, learning_rate: 1.6640625e-05, global_step: 1140, interval_runtime: 6.489, interval_samples_per_second: 2.466, interval_steps_per_second: 1.541, epoch: 8.9062[0m
[32m[2022-09-20 10:39:28,134] [    INFO][0m - loss: 0.01244101, learning_rate: 1.65234375e-05, global_step: 1150, interval_runtime: 6.3803, interval_samples_per_second: 2.508, interval_steps_per_second: 1.567, epoch: 8.9844[0m
[32m[2022-09-20 10:39:28,940] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 10:39:28,940] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-20 10:39:28,940] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 10:39:28,941] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 10:39:28,941] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-20 10:40:02,330] [    INFO][0m - eval_loss: 3.1491804122924805, eval_accuracy: 0.5812379110251451, eval_runtime: 33.3896, eval_samples_per_second: 61.936, eval_steps_per_second: 3.893, epoch: 9.0[0m
[32m[2022-09-20 10:40:02,365] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-1152[0m
[32m[2022-09-20 10:40:02,365] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 10:40:04,971] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-1152/tokenizer_config.json[0m
[32m[2022-09-20 10:40:04,971] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-1152/special_tokens_map.json[0m
[32m[2022-09-20 10:40:15,168] [    INFO][0m - loss: 0.00814871, learning_rate: 1.640625e-05, global_step: 1160, interval_runtime: 47.0336, interval_samples_per_second: 0.34, interval_steps_per_second: 0.213, epoch: 9.0625[0m
[32m[2022-09-20 10:40:22,577] [    INFO][0m - loss: 0.0005952, learning_rate: 1.62890625e-05, global_step: 1170, interval_runtime: 6.4782, interval_samples_per_second: 2.47, interval_steps_per_second: 1.544, epoch: 9.1406[0m
[32m[2022-09-20 10:40:29,068] [    INFO][0m - loss: 0.00113176, learning_rate: 1.6171875000000002e-05, global_step: 1180, interval_runtime: 7.4221, interval_samples_per_second: 2.156, interval_steps_per_second: 1.347, epoch: 9.2188[0m
[32m[2022-09-20 10:40:35,516] [    INFO][0m - loss: 0.00324997, learning_rate: 1.60546875e-05, global_step: 1190, interval_runtime: 6.4475, interval_samples_per_second: 2.482, interval_steps_per_second: 1.551, epoch: 9.2969[0m
[32m[2022-09-20 10:40:41,996] [    INFO][0m - loss: 0.01758764, learning_rate: 1.59375e-05, global_step: 1200, interval_runtime: 6.4802, interval_samples_per_second: 2.469, interval_steps_per_second: 1.543, epoch: 9.375[0m
[32m[2022-09-20 10:40:48,484] [    INFO][0m - loss: 0.00076133, learning_rate: 1.58203125e-05, global_step: 1210, interval_runtime: 6.4876, interval_samples_per_second: 2.466, interval_steps_per_second: 1.541, epoch: 9.4531[0m
[32m[2022-09-20 10:40:54,952] [    INFO][0m - loss: 0.00064573, learning_rate: 1.5703125e-05, global_step: 1220, interval_runtime: 6.4686, interval_samples_per_second: 2.473, interval_steps_per_second: 1.546, epoch: 9.5312[0m
[32m[2022-09-20 10:41:01,435] [    INFO][0m - loss: 0.00422473, learning_rate: 1.5585937500000002e-05, global_step: 1230, interval_runtime: 6.483, interval_samples_per_second: 2.468, interval_steps_per_second: 1.543, epoch: 9.6094[0m
[32m[2022-09-20 10:41:07,892] [    INFO][0m - loss: 0.0023871, learning_rate: 1.546875e-05, global_step: 1240, interval_runtime: 6.4565, interval_samples_per_second: 2.478, interval_steps_per_second: 1.549, epoch: 9.6875[0m
[32m[2022-09-20 10:41:14,365] [    INFO][0m - loss: 0.00366404, learning_rate: 1.53515625e-05, global_step: 1250, interval_runtime: 6.4732, interval_samples_per_second: 2.472, interval_steps_per_second: 1.545, epoch: 9.7656[0m
[32m[2022-09-20 10:41:20,832] [    INFO][0m - loss: 0.00190546, learning_rate: 1.5234375000000001e-05, global_step: 1260, interval_runtime: 6.4673, interval_samples_per_second: 2.474, interval_steps_per_second: 1.546, epoch: 9.8438[0m
[32m[2022-09-20 10:41:27,300] [    INFO][0m - loss: 0.00056967, learning_rate: 1.51171875e-05, global_step: 1270, interval_runtime: 6.4674, interval_samples_per_second: 2.474, interval_steps_per_second: 1.546, epoch: 9.9219[0m
[32m[2022-09-20 10:41:33,206] [    INFO][0m - loss: 0.01362666, learning_rate: 1.5e-05, global_step: 1280, interval_runtime: 5.9066, interval_samples_per_second: 2.709, interval_steps_per_second: 1.693, epoch: 10.0[0m
[32m[2022-09-20 10:41:33,207] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 10:41:33,207] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-20 10:41:33,207] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 10:41:33,207] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 10:41:33,207] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-20 10:42:06,481] [    INFO][0m - eval_loss: 3.275238037109375, eval_accuracy: 0.586073500967118, eval_runtime: 33.2732, eval_samples_per_second: 62.152, eval_steps_per_second: 3.907, epoch: 10.0[0m
[32m[2022-09-20 10:42:06,516] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-1280[0m
[32m[2022-09-20 10:42:06,516] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 10:42:09,086] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-1280/tokenizer_config.json[0m
[32m[2022-09-20 10:42:09,087] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-1280/special_tokens_map.json[0m
[32m[2022-09-20 10:42:20,535] [    INFO][0m - loss: 0.01101419, learning_rate: 1.48828125e-05, global_step: 1290, interval_runtime: 47.3289, interval_samples_per_second: 0.338, interval_steps_per_second: 0.211, epoch: 10.0781[0m
[32m[2022-09-20 10:42:27,015] [    INFO][0m - loss: 0.00036458, learning_rate: 1.4765625e-05, global_step: 1300, interval_runtime: 6.4803, interval_samples_per_second: 2.469, interval_steps_per_second: 1.543, epoch: 10.1562[0m
[32m[2022-09-20 10:42:33,506] [    INFO][0m - loss: 0.00319536, learning_rate: 1.4648437500000001e-05, global_step: 1310, interval_runtime: 6.4903, interval_samples_per_second: 2.465, interval_steps_per_second: 1.541, epoch: 10.2344[0m
[32m[2022-09-20 10:42:39,967] [    INFO][0m - loss: 0.00010095, learning_rate: 1.453125e-05, global_step: 1320, interval_runtime: 6.4609, interval_samples_per_second: 2.476, interval_steps_per_second: 1.548, epoch: 10.3125[0m
[32m[2022-09-20 10:42:50,075] [    INFO][0m - loss: 0.00396871, learning_rate: 1.44140625e-05, global_step: 1330, interval_runtime: 6.4705, interval_samples_per_second: 2.473, interval_steps_per_second: 1.545, epoch: 10.3906[0m
[32m[2022-09-20 10:42:56,542] [    INFO][0m - loss: 0.00013071, learning_rate: 1.4296875e-05, global_step: 1340, interval_runtime: 10.105, interval_samples_per_second: 1.583, interval_steps_per_second: 0.99, epoch: 10.4688[0m
[32m[2022-09-20 10:43:02,990] [    INFO][0m - loss: 0.00016554, learning_rate: 1.41796875e-05, global_step: 1350, interval_runtime: 6.4482, interval_samples_per_second: 2.481, interval_steps_per_second: 1.551, epoch: 10.5469[0m
[32m[2022-09-20 10:43:09,460] [    INFO][0m - loss: 0.00781261, learning_rate: 1.40625e-05, global_step: 1360, interval_runtime: 6.4695, interval_samples_per_second: 2.473, interval_steps_per_second: 1.546, epoch: 10.625[0m
[32m[2022-09-20 10:43:15,933] [    INFO][0m - loss: 0.00568086, learning_rate: 1.3945312500000001e-05, global_step: 1370, interval_runtime: 6.4732, interval_samples_per_second: 2.472, interval_steps_per_second: 1.545, epoch: 10.7031[0m
[32m[2022-09-20 10:43:22,414] [    INFO][0m - loss: 0.00031986, learning_rate: 1.3828125e-05, global_step: 1380, interval_runtime: 6.4807, interval_samples_per_second: 2.469, interval_steps_per_second: 1.543, epoch: 10.7812[0m
[32m[2022-09-20 10:43:28,887] [    INFO][0m - loss: 0.00310911, learning_rate: 1.37109375e-05, global_step: 1390, interval_runtime: 6.4727, interval_samples_per_second: 2.472, interval_steps_per_second: 1.545, epoch: 10.8594[0m
[32m[2022-09-20 10:43:35,348] [    INFO][0m - loss: 0.00366892, learning_rate: 1.359375e-05, global_step: 1400, interval_runtime: 6.4617, interval_samples_per_second: 2.476, interval_steps_per_second: 1.548, epoch: 10.9375[0m
[32m[2022-09-20 10:43:39,935] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 10:43:39,935] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-20 10:43:39,935] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 10:43:39,935] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 10:43:39,935] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-20 10:44:13,232] [    INFO][0m - eval_loss: 3.2086780071258545, eval_accuracy: 0.5846228239845261, eval_runtime: 33.2967, eval_samples_per_second: 62.108, eval_steps_per_second: 3.904, epoch: 11.0[0m
[32m[2022-09-20 10:44:13,258] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-1408[0m
[32m[2022-09-20 10:44:13,258] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 10:44:15,934] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-1408/tokenizer_config.json[0m
[32m[2022-09-20 10:44:15,935] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-1408/special_tokens_map.json[0m
[32m[2022-09-20 10:44:23,097] [    INFO][0m - loss: 7.357e-05, learning_rate: 1.34765625e-05, global_step: 1410, interval_runtime: 47.7483, interval_samples_per_second: 0.335, interval_steps_per_second: 0.209, epoch: 11.0156[0m
[32m[2022-09-20 10:44:29,557] [    INFO][0m - loss: 0.00017395, learning_rate: 1.3359375000000001e-05, global_step: 1420, interval_runtime: 6.4603, interval_samples_per_second: 2.477, interval_steps_per_second: 1.548, epoch: 11.0938[0m
[32m[2022-09-20 10:44:36,048] [    INFO][0m - loss: 3.921e-05, learning_rate: 1.3242187500000001e-05, global_step: 1430, interval_runtime: 6.4905, interval_samples_per_second: 2.465, interval_steps_per_second: 1.541, epoch: 11.1719[0m
[32m[2022-09-20 10:44:42,513] [    INFO][0m - loss: 6.324e-05, learning_rate: 1.3125e-05, global_step: 1440, interval_runtime: 6.4652, interval_samples_per_second: 2.475, interval_steps_per_second: 1.547, epoch: 11.25[0m
[32m[2022-09-20 10:44:52,887] [    INFO][0m - loss: 0.00018823, learning_rate: 1.30078125e-05, global_step: 1450, interval_runtime: 6.4865, interval_samples_per_second: 2.467, interval_steps_per_second: 1.542, epoch: 11.3281[0m
[32m[2022-09-20 10:45:01,764] [    INFO][0m - loss: 0.00234213, learning_rate: 1.2890625e-05, global_step: 1460, interval_runtime: 10.3487, interval_samples_per_second: 1.546, interval_steps_per_second: 0.966, epoch: 11.4062[0m
[32m[2022-09-20 10:45:08,238] [    INFO][0m - loss: 0.00711593, learning_rate: 1.27734375e-05, global_step: 1470, interval_runtime: 8.8903, interval_samples_per_second: 1.8, interval_steps_per_second: 1.125, epoch: 11.4844[0m
[32m[2022-09-20 10:45:15,923] [    INFO][0m - loss: 0.00018369, learning_rate: 1.2656250000000001e-05, global_step: 1480, interval_runtime: 6.4727, interval_samples_per_second: 2.472, interval_steps_per_second: 1.545, epoch: 11.5625[0m
[32m[2022-09-20 10:45:22,417] [    INFO][0m - loss: 6.493e-05, learning_rate: 1.25390625e-05, global_step: 1490, interval_runtime: 7.7061, interval_samples_per_second: 2.076, interval_steps_per_second: 1.298, epoch: 11.6406[0m
[32m[2022-09-20 10:45:28,900] [    INFO][0m - loss: 0.03104879, learning_rate: 1.2421875e-05, global_step: 1500, interval_runtime: 6.4833, interval_samples_per_second: 2.468, interval_steps_per_second: 1.542, epoch: 11.7188[0m
[32m[2022-09-20 10:45:38,044] [    INFO][0m - loss: 7.134e-05, learning_rate: 1.23046875e-05, global_step: 1510, interval_runtime: 6.4727, interval_samples_per_second: 2.472, interval_steps_per_second: 1.545, epoch: 11.7969[0m
[32m[2022-09-20 10:45:44,525] [    INFO][0m - loss: 0.02425945, learning_rate: 1.21875e-05, global_step: 1520, interval_runtime: 9.1524, interval_samples_per_second: 1.748, interval_steps_per_second: 1.093, epoch: 11.875[0m
[32m[2022-09-20 10:45:51,008] [    INFO][0m - loss: 0.00533721, learning_rate: 1.20703125e-05, global_step: 1530, interval_runtime: 6.4826, interval_samples_per_second: 2.468, interval_steps_per_second: 1.543, epoch: 11.9531[0m
[32m[2022-09-20 10:45:54,313] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 10:45:56,247] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-20 10:45:56,248] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 10:45:56,248] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 10:45:56,248] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-20 10:46:29,680] [    INFO][0m - eval_loss: 3.1234445571899414, eval_accuracy: 0.5899419729206963, eval_runtime: 35.3656, eval_samples_per_second: 58.475, eval_steps_per_second: 3.676, epoch: 12.0[0m
[32m[2022-09-20 10:46:29,701] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-1536[0m
[32m[2022-09-20 10:46:29,702] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 10:46:32,259] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-1536/tokenizer_config.json[0m
[32m[2022-09-20 10:46:32,260] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-1536/special_tokens_map.json[0m
[32m[2022-09-20 10:46:41,395] [    INFO][0m - loss: 0.0004833, learning_rate: 1.1953125000000001e-05, global_step: 1540, interval_runtime: 50.387, interval_samples_per_second: 0.318, interval_steps_per_second: 0.198, epoch: 12.0312[0m
[32m[2022-09-20 10:46:47,911] [    INFO][0m - loss: 0.00021805, learning_rate: 1.18359375e-05, global_step: 1550, interval_runtime: 6.5163, interval_samples_per_second: 2.455, interval_steps_per_second: 1.535, epoch: 12.1094[0m
[32m[2022-09-20 10:46:54,394] [    INFO][0m - loss: 0.00243402, learning_rate: 1.171875e-05, global_step: 1560, interval_runtime: 6.4827, interval_samples_per_second: 2.468, interval_steps_per_second: 1.543, epoch: 12.1875[0m
[32m[2022-09-20 10:47:00,916] [    INFO][0m - loss: 0.00013563, learning_rate: 1.16015625e-05, global_step: 1570, interval_runtime: 6.522, interval_samples_per_second: 2.453, interval_steps_per_second: 1.533, epoch: 12.2656[0m
[32m[2022-09-20 10:47:07,412] [    INFO][0m - loss: 0.00125806, learning_rate: 1.1484375e-05, global_step: 1580, interval_runtime: 6.4963, interval_samples_per_second: 2.463, interval_steps_per_second: 1.539, epoch: 12.3438[0m
[32m[2022-09-20 10:47:14,388] [    INFO][0m - loss: 6.434e-05, learning_rate: 1.1367187500000001e-05, global_step: 1590, interval_runtime: 6.9761, interval_samples_per_second: 2.294, interval_steps_per_second: 1.433, epoch: 12.4219[0m
[32m[2022-09-20 10:47:20,885] [    INFO][0m - loss: 0.00022098, learning_rate: 1.125e-05, global_step: 1600, interval_runtime: 6.4966, interval_samples_per_second: 2.463, interval_steps_per_second: 1.539, epoch: 12.5[0m
[32m[2022-09-20 10:47:27,388] [    INFO][0m - loss: 9.15e-05, learning_rate: 1.11328125e-05, global_step: 1610, interval_runtime: 6.5026, interval_samples_per_second: 2.461, interval_steps_per_second: 1.538, epoch: 12.5781[0m
[32m[2022-09-20 10:47:33,876] [    INFO][0m - loss: 4.197e-05, learning_rate: 1.1015625e-05, global_step: 1620, interval_runtime: 6.4879, interval_samples_per_second: 2.466, interval_steps_per_second: 1.541, epoch: 12.6562[0m
[32m[2022-09-20 10:47:43,031] [    INFO][0m - loss: 0.00012423, learning_rate: 1.08984375e-05, global_step: 1630, interval_runtime: 6.4793, interval_samples_per_second: 2.469, interval_steps_per_second: 1.543, epoch: 12.7344[0m
[32m[2022-09-20 10:47:49,568] [    INFO][0m - loss: 0.00061001, learning_rate: 1.078125e-05, global_step: 1640, interval_runtime: 9.2123, interval_samples_per_second: 1.737, interval_steps_per_second: 1.086, epoch: 12.8125[0m
[32m[2022-09-20 10:47:56,043] [    INFO][0m - loss: 8.527e-05, learning_rate: 1.0664062500000001e-05, global_step: 1650, interval_runtime: 6.4758, interval_samples_per_second: 2.471, interval_steps_per_second: 1.544, epoch: 12.8906[0m
[32m[2022-09-20 10:48:02,471] [    INFO][0m - loss: 0.00014537, learning_rate: 1.0546875e-05, global_step: 1660, interval_runtime: 6.4274, interval_samples_per_second: 2.489, interval_steps_per_second: 1.556, epoch: 12.9688[0m
[32m[2022-09-20 10:48:04,522] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 10:48:04,522] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-20 10:48:04,522] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 10:48:04,522] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 10:48:04,522] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-20 10:48:38,035] [    INFO][0m - eval_loss: 3.2122931480407715, eval_accuracy: 0.5889748549323017, eval_runtime: 33.5118, eval_samples_per_second: 61.71, eval_steps_per_second: 3.879, epoch: 13.0[0m
[32m[2022-09-20 10:48:38,067] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-1664[0m
[32m[2022-09-20 10:48:38,067] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 10:48:40,910] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-1664/tokenizer_config.json[0m
[32m[2022-09-20 10:48:40,911] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-1664/special_tokens_map.json[0m
[32m[2022-09-20 10:48:50,498] [    INFO][0m - loss: 0.00021711, learning_rate: 1.04296875e-05, global_step: 1670, interval_runtime: 48.0268, interval_samples_per_second: 0.333, interval_steps_per_second: 0.208, epoch: 13.0469[0m
[32m[2022-09-20 10:48:57,012] [    INFO][0m - loss: 4.303e-05, learning_rate: 1.03125e-05, global_step: 1680, interval_runtime: 6.5137, interval_samples_per_second: 2.456, interval_steps_per_second: 1.535, epoch: 13.125[0m
[32m[2022-09-20 10:49:03,495] [    INFO][0m - loss: 9.606e-05, learning_rate: 1.01953125e-05, global_step: 1690, interval_runtime: 6.4835, interval_samples_per_second: 2.468, interval_steps_per_second: 1.542, epoch: 13.2031[0m
[32m[2022-09-20 10:49:09,990] [    INFO][0m - loss: 4.64e-05, learning_rate: 1.0078125000000001e-05, global_step: 1700, interval_runtime: 6.4954, interval_samples_per_second: 2.463, interval_steps_per_second: 1.54, epoch: 13.2812[0m
[32m[2022-09-20 10:49:16,779] [    INFO][0m - loss: 4.68e-05, learning_rate: 9.9609375e-06, global_step: 1710, interval_runtime: 6.5053, interval_samples_per_second: 2.46, interval_steps_per_second: 1.537, epoch: 13.3594[0m
[32m[2022-09-20 10:49:23,299] [    INFO][0m - loss: 7.27e-05, learning_rate: 9.84375e-06, global_step: 1720, interval_runtime: 6.8035, interval_samples_per_second: 2.352, interval_steps_per_second: 1.47, epoch: 13.4375[0m
[32m[2022-09-20 10:49:29,802] [    INFO][0m - loss: 0.00015749, learning_rate: 9.7265625e-06, global_step: 1730, interval_runtime: 6.5026, interval_samples_per_second: 2.461, interval_steps_per_second: 1.538, epoch: 13.5156[0m
[32m[2022-09-20 10:49:36,302] [    INFO][0m - loss: 5.365e-05, learning_rate: 9.609375e-06, global_step: 1740, interval_runtime: 6.5001, interval_samples_per_second: 2.461, interval_steps_per_second: 1.538, epoch: 13.5938[0m
[32m[2022-09-20 10:49:42,793] [    INFO][0m - loss: 3.327e-05, learning_rate: 9.4921875e-06, global_step: 1750, interval_runtime: 6.4912, interval_samples_per_second: 2.465, interval_steps_per_second: 1.541, epoch: 13.6719[0m
[32m[2022-09-20 10:49:49,283] [    INFO][0m - loss: 4.961e-05, learning_rate: 9.375000000000001e-06, global_step: 1760, interval_runtime: 6.4896, interval_samples_per_second: 2.465, interval_steps_per_second: 1.541, epoch: 13.75[0m
[32m[2022-09-20 10:49:55,780] [    INFO][0m - loss: 2.838e-05, learning_rate: 9.2578125e-06, global_step: 1770, interval_runtime: 6.4948, interval_samples_per_second: 2.464, interval_steps_per_second: 1.54, epoch: 13.8281[0m
[32m[2022-09-20 10:50:02,289] [    INFO][0m - loss: 7.298e-05, learning_rate: 9.140625e-06, global_step: 1780, interval_runtime: 6.5116, interval_samples_per_second: 2.457, interval_steps_per_second: 1.536, epoch: 13.9062[0m
[32m[2022-09-20 10:50:08,674] [    INFO][0m - loss: 2.367e-05, learning_rate: 9.0234375e-06, global_step: 1790, interval_runtime: 6.385, interval_samples_per_second: 2.506, interval_steps_per_second: 1.566, epoch: 13.9844[0m
[32m[2022-09-20 10:50:09,479] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 10:50:09,479] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-20 10:50:09,480] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 10:50:09,480] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 10:50:09,480] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-20 10:50:42,741] [    INFO][0m - eval_loss: 3.2056632041931152, eval_accuracy: 0.5913926499032882, eval_runtime: 33.2611, eval_samples_per_second: 62.175, eval_steps_per_second: 3.908, epoch: 14.0[0m
[32m[2022-09-20 10:50:42,770] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-1792[0m
[32m[2022-09-20 10:50:42,770] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 10:50:45,307] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-1792/tokenizer_config.json[0m
[32m[2022-09-20 10:50:45,308] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-1792/special_tokens_map.json[0m
[32m[2022-09-20 10:50:55,385] [    INFO][0m - loss: 3.78e-05, learning_rate: 8.90625e-06, global_step: 1800, interval_runtime: 46.7108, interval_samples_per_second: 0.343, interval_steps_per_second: 0.214, epoch: 14.0625[0m
[32m[2022-09-20 10:51:01,865] [    INFO][0m - loss: 2.896e-05, learning_rate: 8.789062500000001e-06, global_step: 1810, interval_runtime: 6.4805, interval_samples_per_second: 2.469, interval_steps_per_second: 1.543, epoch: 14.1406[0m
[32m[2022-09-20 10:51:08,329] [    INFO][0m - loss: 4.046e-05, learning_rate: 8.671875e-06, global_step: 1820, interval_runtime: 6.4635, interval_samples_per_second: 2.475, interval_steps_per_second: 1.547, epoch: 14.2188[0m
[32m[2022-09-20 10:51:14,822] [    INFO][0m - loss: 4.324e-05, learning_rate: 8.5546875e-06, global_step: 1830, interval_runtime: 6.4931, interval_samples_per_second: 2.464, interval_steps_per_second: 1.54, epoch: 14.2969[0m
[32m[2022-09-20 10:51:21,315] [    INFO][0m - loss: 0.00014019, learning_rate: 8.4375e-06, global_step: 1840, interval_runtime: 6.4936, interval_samples_per_second: 2.464, interval_steps_per_second: 1.54, epoch: 14.375[0m
[32m[2022-09-20 10:51:27,815] [    INFO][0m - loss: 2.592e-05, learning_rate: 8.3203125e-06, global_step: 1850, interval_runtime: 6.5, interval_samples_per_second: 2.462, interval_steps_per_second: 1.538, epoch: 14.4531[0m
[32m[2022-09-20 10:51:34,297] [    INFO][0m - loss: 6.178e-05, learning_rate: 8.203125e-06, global_step: 1860, interval_runtime: 6.481, interval_samples_per_second: 2.469, interval_steps_per_second: 1.543, epoch: 14.5312[0m
[32m[2022-09-20 10:51:40,788] [    INFO][0m - loss: 2.186e-05, learning_rate: 8.085937500000001e-06, global_step: 1870, interval_runtime: 6.4913, interval_samples_per_second: 2.465, interval_steps_per_second: 1.541, epoch: 14.6094[0m
[32m[2022-09-20 10:51:47,290] [    INFO][0m - loss: 2.639e-05, learning_rate: 7.96875e-06, global_step: 1880, interval_runtime: 6.5026, interval_samples_per_second: 2.461, interval_steps_per_second: 1.538, epoch: 14.6875[0m
[32m[2022-09-20 10:51:53,798] [    INFO][0m - loss: 5.986e-05, learning_rate: 7.8515625e-06, global_step: 1890, interval_runtime: 6.5079, interval_samples_per_second: 2.459, interval_steps_per_second: 1.537, epoch: 14.7656[0m
[32m[2022-09-20 10:52:00,317] [    INFO][0m - loss: 0.0001037, learning_rate: 7.734375e-06, global_step: 1900, interval_runtime: 6.5191, interval_samples_per_second: 2.454, interval_steps_per_second: 1.534, epoch: 14.8438[0m
[32m[2022-09-20 10:52:06,808] [    INFO][0m - loss: 2.73e-05, learning_rate: 7.6171875000000005e-06, global_step: 1910, interval_runtime: 6.4909, interval_samples_per_second: 2.465, interval_steps_per_second: 1.541, epoch: 14.9219[0m
[32m[2022-09-20 10:52:12,728] [    INFO][0m - loss: 4.162e-05, learning_rate: 7.5e-06, global_step: 1920, interval_runtime: 5.9197, interval_samples_per_second: 2.703, interval_steps_per_second: 1.689, epoch: 15.0[0m
[32m[2022-09-20 10:52:12,729] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 10:52:12,729] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-20 10:52:12,729] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 10:52:12,729] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 10:52:12,729] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-20 10:52:46,204] [    INFO][0m - eval_loss: 3.2260775566101074, eval_accuracy: 0.5923597678916828, eval_runtime: 33.4742, eval_samples_per_second: 61.779, eval_steps_per_second: 3.884, epoch: 15.0[0m
[32m[2022-09-20 10:52:46,239] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-1920[0m
[32m[2022-09-20 10:52:46,239] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 10:52:49,006] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-1920/tokenizer_config.json[0m
[32m[2022-09-20 10:52:49,007] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-1920/special_tokens_map.json[0m
[32m[2022-09-20 10:53:00,977] [    INFO][0m - loss: 3.971e-05, learning_rate: 7.3828125e-06, global_step: 1930, interval_runtime: 48.2486, interval_samples_per_second: 0.332, interval_steps_per_second: 0.207, epoch: 15.0781[0m
[32m[2022-09-20 10:53:07,449] [    INFO][0m - loss: 2.4e-05, learning_rate: 7.265625e-06, global_step: 1940, interval_runtime: 6.4721, interval_samples_per_second: 2.472, interval_steps_per_second: 1.545, epoch: 15.1562[0m
[32m[2022-09-20 10:53:13,958] [    INFO][0m - loss: 3.057e-05, learning_rate: 7.1484375e-06, global_step: 1950, interval_runtime: 6.5092, interval_samples_per_second: 2.458, interval_steps_per_second: 1.536, epoch: 15.2344[0m
[32m[2022-09-20 10:53:20,457] [    INFO][0m - loss: 4.922e-05, learning_rate: 7.03125e-06, global_step: 1960, interval_runtime: 6.4994, interval_samples_per_second: 2.462, interval_steps_per_second: 1.539, epoch: 15.3125[0m
[32m[2022-09-20 10:53:26,956] [    INFO][0m - loss: 3.047e-05, learning_rate: 6.9140625e-06, global_step: 1970, interval_runtime: 6.4988, interval_samples_per_second: 2.462, interval_steps_per_second: 1.539, epoch: 15.3906[0m
[32m[2022-09-20 10:53:33,453] [    INFO][0m - loss: 2.986e-05, learning_rate: 6.796875e-06, global_step: 1980, interval_runtime: 6.4967, interval_samples_per_second: 2.463, interval_steps_per_second: 1.539, epoch: 15.4688[0m
[32m[2022-09-20 10:53:39,948] [    INFO][0m - loss: 5.108e-05, learning_rate: 6.679687500000001e-06, global_step: 1990, interval_runtime: 6.4949, interval_samples_per_second: 2.463, interval_steps_per_second: 1.54, epoch: 15.5469[0m
[32m[2022-09-20 10:53:46,445] [    INFO][0m - loss: 5.78e-05, learning_rate: 6.5625e-06, global_step: 2000, interval_runtime: 6.497, interval_samples_per_second: 2.463, interval_steps_per_second: 1.539, epoch: 15.625[0m
[32m[2022-09-20 10:53:52,968] [    INFO][0m - loss: 3.302e-05, learning_rate: 6.4453125e-06, global_step: 2010, interval_runtime: 6.5232, interval_samples_per_second: 2.453, interval_steps_per_second: 1.533, epoch: 15.7031[0m
[32m[2022-09-20 10:53:59,454] [    INFO][0m - loss: 3.272e-05, learning_rate: 6.3281250000000005e-06, global_step: 2020, interval_runtime: 6.4855, interval_samples_per_second: 2.467, interval_steps_per_second: 1.542, epoch: 15.7812[0m
[32m[2022-09-20 10:54:05,958] [    INFO][0m - loss: 5.065e-05, learning_rate: 6.2109375e-06, global_step: 2030, interval_runtime: 6.5046, interval_samples_per_second: 2.46, interval_steps_per_second: 1.537, epoch: 15.8594[0m
[32m[2022-09-20 10:54:12,466] [    INFO][0m - loss: 3.847e-05, learning_rate: 6.09375e-06, global_step: 2040, interval_runtime: 6.5082, interval_samples_per_second: 2.458, interval_steps_per_second: 1.537, epoch: 15.9375[0m
[32m[2022-09-20 10:54:17,080] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 10:54:17,080] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-20 10:54:17,080] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 10:54:17,080] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 10:54:17,080] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-20 10:54:50,499] [    INFO][0m - eval_loss: 3.2312355041503906, eval_accuracy: 0.594294003868472, eval_runtime: 33.4183, eval_samples_per_second: 61.882, eval_steps_per_second: 3.89, epoch: 16.0[0m
[32m[2022-09-20 10:54:50,536] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-2048[0m
[32m[2022-09-20 10:54:50,536] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 10:54:55,122] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-2048/tokenizer_config.json[0m
[32m[2022-09-20 10:54:55,122] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-2048/special_tokens_map.json[0m
[32m[2022-09-20 10:55:01,909] [    INFO][0m - loss: 3.143e-05, learning_rate: 5.9765625000000004e-06, global_step: 2050, interval_runtime: 49.4428, interval_samples_per_second: 0.324, interval_steps_per_second: 0.202, epoch: 16.0156[0m
[32m[2022-09-20 10:55:08,448] [    INFO][0m - loss: 3.331e-05, learning_rate: 5.859375e-06, global_step: 2060, interval_runtime: 6.5385, interval_samples_per_second: 2.447, interval_steps_per_second: 1.529, epoch: 16.0938[0m
[32m[2022-09-20 10:55:14,979] [    INFO][0m - loss: 4.33e-05, learning_rate: 5.7421875e-06, global_step: 2070, interval_runtime: 6.5311, interval_samples_per_second: 2.45, interval_steps_per_second: 1.531, epoch: 16.1719[0m
[32m[2022-09-20 10:55:21,470] [    INFO][0m - loss: 1.992e-05, learning_rate: 5.625e-06, global_step: 2080, interval_runtime: 6.4913, interval_samples_per_second: 2.465, interval_steps_per_second: 1.541, epoch: 16.25[0m
[32m[2022-09-20 10:55:27,952] [    INFO][0m - loss: 1.674e-05, learning_rate: 5.5078125e-06, global_step: 2090, interval_runtime: 6.4819, interval_samples_per_second: 2.468, interval_steps_per_second: 1.543, epoch: 16.3281[0m
[32m[2022-09-20 10:55:34,440] [    INFO][0m - loss: 1.349e-05, learning_rate: 5.390625e-06, global_step: 2100, interval_runtime: 6.4878, interval_samples_per_second: 2.466, interval_steps_per_second: 1.541, epoch: 16.4062[0m
[32m[2022-09-20 10:55:40,927] [    INFO][0m - loss: 1.778e-05, learning_rate: 5.2734375e-06, global_step: 2110, interval_runtime: 6.4867, interval_samples_per_second: 2.467, interval_steps_per_second: 1.542, epoch: 16.4844[0m
[32m[2022-09-20 10:55:47,425] [    INFO][0m - loss: 2.346e-05, learning_rate: 5.15625e-06, global_step: 2120, interval_runtime: 6.4989, interval_samples_per_second: 2.462, interval_steps_per_second: 1.539, epoch: 16.5625[0m
[32m[2022-09-20 10:55:53,936] [    INFO][0m - loss: 3.847e-05, learning_rate: 5.0390625000000005e-06, global_step: 2130, interval_runtime: 6.51, interval_samples_per_second: 2.458, interval_steps_per_second: 1.536, epoch: 16.6406[0m
[32m[2022-09-20 10:56:00,435] [    INFO][0m - loss: 2.68e-05, learning_rate: 4.921875e-06, global_step: 2140, interval_runtime: 6.4995, interval_samples_per_second: 2.462, interval_steps_per_second: 1.539, epoch: 16.7188[0m
[32m[2022-09-20 10:56:06,923] [    INFO][0m - loss: 2.452e-05, learning_rate: 4.8046875e-06, global_step: 2150, interval_runtime: 6.488, interval_samples_per_second: 2.466, interval_steps_per_second: 1.541, epoch: 16.7969[0m
[32m[2022-09-20 10:56:13,422] [    INFO][0m - loss: 3.225e-05, learning_rate: 4.6875000000000004e-06, global_step: 2160, interval_runtime: 6.4989, interval_samples_per_second: 2.462, interval_steps_per_second: 1.539, epoch: 16.875[0m
[32m[2022-09-20 10:56:19,944] [    INFO][0m - loss: 1.958e-05, learning_rate: 4.5703125e-06, global_step: 2170, interval_runtime: 6.5218, interval_samples_per_second: 2.453, interval_steps_per_second: 1.533, epoch: 16.9531[0m
[32m[2022-09-20 10:56:23,258] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 10:56:23,259] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-20 10:56:23,259] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 10:56:23,259] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 10:56:23,259] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-20 10:56:56,731] [    INFO][0m - eval_loss: 3.23158597946167, eval_accuracy: 0.594294003868472, eval_runtime: 33.4722, eval_samples_per_second: 61.783, eval_steps_per_second: 3.884, epoch: 17.0[0m
[32m[2022-09-20 10:56:56,766] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-2176[0m
[32m[2022-09-20 10:56:56,767] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 10:56:59,528] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-2176/tokenizer_config.json[0m
[32m[2022-09-20 10:56:59,528] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-2176/special_tokens_map.json[0m
[32m[2022-09-20 10:57:07,561] [    INFO][0m - loss: 1.933e-05, learning_rate: 4.453125e-06, global_step: 2180, interval_runtime: 47.6174, interval_samples_per_second: 0.336, interval_steps_per_second: 0.21, epoch: 17.0312[0m
[32m[2022-09-20 10:57:14,026] [    INFO][0m - loss: 0.00041508, learning_rate: 4.3359375e-06, global_step: 2190, interval_runtime: 6.4648, interval_samples_per_second: 2.475, interval_steps_per_second: 1.547, epoch: 17.1094[0m
[32m[2022-09-20 10:57:20,518] [    INFO][0m - loss: 1.841e-05, learning_rate: 4.21875e-06, global_step: 2200, interval_runtime: 6.4918, interval_samples_per_second: 2.465, interval_steps_per_second: 1.54, epoch: 17.1875[0m
[32m[2022-09-20 10:57:27,021] [    INFO][0m - loss: 1.744e-05, learning_rate: 4.1015625e-06, global_step: 2210, interval_runtime: 6.503, interval_samples_per_second: 2.46, interval_steps_per_second: 1.538, epoch: 17.2656[0m
[32m[2022-09-20 10:57:33,534] [    INFO][0m - loss: 2.477e-05, learning_rate: 3.984375e-06, global_step: 2220, interval_runtime: 6.5135, interval_samples_per_second: 2.456, interval_steps_per_second: 1.535, epoch: 17.3438[0m
[32m[2022-09-20 10:57:42,461] [    INFO][0m - loss: 2.568e-05, learning_rate: 3.8671875e-06, global_step: 2230, interval_runtime: 6.5089, interval_samples_per_second: 2.458, interval_steps_per_second: 1.536, epoch: 17.4219[0m
[32m[2022-09-20 10:57:48,937] [    INFO][0m - loss: 2.989e-05, learning_rate: 3.75e-06, global_step: 2240, interval_runtime: 8.894, interval_samples_per_second: 1.799, interval_steps_per_second: 1.124, epoch: 17.5[0m
[32m[2022-09-20 10:57:55,460] [    INFO][0m - loss: 2.306e-05, learning_rate: 3.6328125e-06, global_step: 2250, interval_runtime: 6.5229, interval_samples_per_second: 2.453, interval_steps_per_second: 1.533, epoch: 17.5781[0m
[32m[2022-09-20 10:58:01,965] [    INFO][0m - loss: 1.559e-05, learning_rate: 3.515625e-06, global_step: 2260, interval_runtime: 6.5051, interval_samples_per_second: 2.46, interval_steps_per_second: 1.537, epoch: 17.6562[0m
[32m[2022-09-20 10:58:08,498] [    INFO][0m - loss: 2.388e-05, learning_rate: 3.3984375e-06, global_step: 2270, interval_runtime: 6.5325, interval_samples_per_second: 2.449, interval_steps_per_second: 1.531, epoch: 17.7344[0m
[32m[2022-09-20 10:58:15,039] [    INFO][0m - loss: 1.634e-05, learning_rate: 3.28125e-06, global_step: 2280, interval_runtime: 6.5413, interval_samples_per_second: 2.446, interval_steps_per_second: 1.529, epoch: 17.8125[0m
[32m[2022-09-20 10:58:21,555] [    INFO][0m - loss: 1.862e-05, learning_rate: 3.1640625000000003e-06, global_step: 2290, interval_runtime: 6.5158, interval_samples_per_second: 2.456, interval_steps_per_second: 1.535, epoch: 17.8906[0m
[32m[2022-09-20 10:58:28,008] [    INFO][0m - loss: 2.389e-05, learning_rate: 3.046875e-06, global_step: 2300, interval_runtime: 6.4537, interval_samples_per_second: 2.479, interval_steps_per_second: 1.549, epoch: 17.9688[0m
[32m[2022-09-20 10:58:30,074] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 10:58:30,415] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-20 10:58:30,415] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 10:58:30,415] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 10:58:30,415] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-20 10:59:03,837] [    INFO][0m - eval_loss: 3.2267134189605713, eval_accuracy: 0.59284332688588, eval_runtime: 33.7617, eval_samples_per_second: 61.253, eval_steps_per_second: 3.851, epoch: 18.0[0m
[32m[2022-09-20 10:59:03,871] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-2304[0m
[32m[2022-09-20 10:59:03,871] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 10:59:06,603] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-2304/tokenizer_config.json[0m
[32m[2022-09-20 10:59:06,603] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-2304/special_tokens_map.json[0m
[32m[2022-09-20 10:59:15,680] [    INFO][0m - loss: 2.103e-05, learning_rate: 2.9296875e-06, global_step: 2310, interval_runtime: 47.6716, interval_samples_per_second: 0.336, interval_steps_per_second: 0.21, epoch: 18.0469[0m
[32m[2022-09-20 10:59:22,185] [    INFO][0m - loss: 3.429e-05, learning_rate: 2.8125e-06, global_step: 2320, interval_runtime: 6.5052, interval_samples_per_second: 2.46, interval_steps_per_second: 1.537, epoch: 18.125[0m
[32m[2022-09-20 10:59:28,768] [    INFO][0m - loss: 2.48e-05, learning_rate: 2.6953125e-06, global_step: 2330, interval_runtime: 6.5831, interval_samples_per_second: 2.43, interval_steps_per_second: 1.519, epoch: 18.2031[0m
[32m[2022-09-20 10:59:35,287] [    INFO][0m - loss: 2.175e-05, learning_rate: 2.578125e-06, global_step: 2340, interval_runtime: 6.5185, interval_samples_per_second: 2.455, interval_steps_per_second: 1.534, epoch: 18.2812[0m
[32m[2022-09-20 10:59:41,805] [    INFO][0m - loss: 2.276e-05, learning_rate: 2.4609375e-06, global_step: 2350, interval_runtime: 6.518, interval_samples_per_second: 2.455, interval_steps_per_second: 1.534, epoch: 18.3594[0m
[32m[2022-09-20 10:59:48,308] [    INFO][0m - loss: 3.333e-05, learning_rate: 2.3437500000000002e-06, global_step: 2360, interval_runtime: 6.5033, interval_samples_per_second: 2.46, interval_steps_per_second: 1.538, epoch: 18.4375[0m
[32m[2022-09-20 10:59:54,817] [    INFO][0m - loss: 1.643e-05, learning_rate: 2.2265625e-06, global_step: 2370, interval_runtime: 6.5083, interval_samples_per_second: 2.458, interval_steps_per_second: 1.537, epoch: 18.5156[0m
[32m[2022-09-20 11:00:01,322] [    INFO][0m - loss: 1.774e-05, learning_rate: 2.109375e-06, global_step: 2380, interval_runtime: 6.5056, interval_samples_per_second: 2.459, interval_steps_per_second: 1.537, epoch: 18.5938[0m
[32m[2022-09-20 11:00:07,813] [    INFO][0m - loss: 3.363e-05, learning_rate: 1.9921875e-06, global_step: 2390, interval_runtime: 6.4904, interval_samples_per_second: 2.465, interval_steps_per_second: 1.541, epoch: 18.6719[0m
[32m[2022-09-20 11:00:14,314] [    INFO][0m - loss: 2.272e-05, learning_rate: 1.875e-06, global_step: 2400, interval_runtime: 6.5013, interval_samples_per_second: 2.461, interval_steps_per_second: 1.538, epoch: 18.75[0m
[32m[2022-09-20 11:00:20,823] [    INFO][0m - loss: 4.118e-05, learning_rate: 1.7578125e-06, global_step: 2410, interval_runtime: 6.5086, interval_samples_per_second: 2.458, interval_steps_per_second: 1.536, epoch: 18.8281[0m
[32m[2022-09-20 11:00:27,321] [    INFO][0m - loss: 1.687e-05, learning_rate: 1.640625e-06, global_step: 2420, interval_runtime: 6.499, interval_samples_per_second: 2.462, interval_steps_per_second: 1.539, epoch: 18.9062[0m
[32m[2022-09-20 11:00:33,721] [    INFO][0m - loss: 0.00011894, learning_rate: 1.5234375e-06, global_step: 2430, interval_runtime: 6.3993, interval_samples_per_second: 2.5, interval_steps_per_second: 1.563, epoch: 18.9844[0m
[32m[2022-09-20 11:00:34,531] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 11:00:34,531] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-20 11:00:34,531] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 11:00:34,531] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 11:00:34,531] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-20 11:01:07,976] [    INFO][0m - eval_loss: 3.231022834777832, eval_accuracy: 0.594294003868472, eval_runtime: 33.4447, eval_samples_per_second: 61.833, eval_steps_per_second: 3.887, epoch: 19.0[0m
[32m[2022-09-20 11:01:08,013] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-2432[0m
[32m[2022-09-20 11:01:08,013] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 11:01:10,685] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-2432/tokenizer_config.json[0m
[32m[2022-09-20 11:01:10,685] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-2432/special_tokens_map.json[0m
[32m[2022-09-20 11:01:21,025] [    INFO][0m - loss: 2.244e-05, learning_rate: 1.40625e-06, global_step: 2440, interval_runtime: 47.3041, interval_samples_per_second: 0.338, interval_steps_per_second: 0.211, epoch: 19.0625[0m
[32m[2022-09-20 11:01:31,124] [    INFO][0m - loss: 2.607e-05, learning_rate: 1.2890625e-06, global_step: 2450, interval_runtime: 6.4631, interval_samples_per_second: 2.476, interval_steps_per_second: 1.547, epoch: 19.1406[0m
[32m[2022-09-20 11:01:37,601] [    INFO][0m - loss: 2.878e-05, learning_rate: 1.1718750000000001e-06, global_step: 2460, interval_runtime: 10.1126, interval_samples_per_second: 1.582, interval_steps_per_second: 0.989, epoch: 19.2188[0m
[32m[2022-09-20 11:01:44,087] [    INFO][0m - loss: 1.81e-05, learning_rate: 1.0546875e-06, global_step: 2470, interval_runtime: 6.4867, interval_samples_per_second: 2.467, interval_steps_per_second: 1.542, epoch: 19.2969[0m
[32m[2022-09-20 11:01:50,577] [    INFO][0m - loss: 2.169e-05, learning_rate: 9.375e-07, global_step: 2480, interval_runtime: 6.49, interval_samples_per_second: 2.465, interval_steps_per_second: 1.541, epoch: 19.375[0m
[32m[2022-09-20 11:01:57,070] [    INFO][0m - loss: 1.96e-05, learning_rate: 8.203125e-07, global_step: 2490, interval_runtime: 6.4926, interval_samples_per_second: 2.464, interval_steps_per_second: 1.54, epoch: 19.4531[0m
[32m[2022-09-20 11:02:03,585] [    INFO][0m - loss: 2.702e-05, learning_rate: 7.03125e-07, global_step: 2500, interval_runtime: 6.5148, interval_samples_per_second: 2.456, interval_steps_per_second: 1.535, epoch: 19.5312[0m
[32m[2022-09-20 11:02:10,091] [    INFO][0m - loss: 2.301e-05, learning_rate: 5.859375000000001e-07, global_step: 2510, interval_runtime: 6.5062, interval_samples_per_second: 2.459, interval_steps_per_second: 1.537, epoch: 19.6094[0m
[32m[2022-09-20 11:02:16,582] [    INFO][0m - loss: 1.916e-05, learning_rate: 4.6875e-07, global_step: 2520, interval_runtime: 6.491, interval_samples_per_second: 2.465, interval_steps_per_second: 1.541, epoch: 19.6875[0m
[32m[2022-09-20 11:02:23,070] [    INFO][0m - loss: 2.24e-05, learning_rate: 3.515625e-07, global_step: 2530, interval_runtime: 6.4881, interval_samples_per_second: 2.466, interval_steps_per_second: 1.541, epoch: 19.7656[0m
[32m[2022-09-20 11:02:29,576] [    INFO][0m - loss: 2.113e-05, learning_rate: 2.34375e-07, global_step: 2540, interval_runtime: 6.5062, interval_samples_per_second: 2.459, interval_steps_per_second: 1.537, epoch: 19.8438[0m
[32m[2022-09-20 11:02:36,071] [    INFO][0m - loss: 2.808e-05, learning_rate: 1.171875e-07, global_step: 2550, interval_runtime: 6.4948, interval_samples_per_second: 2.464, interval_steps_per_second: 1.54, epoch: 19.9219[0m
[32m[2022-09-20 11:02:42,005] [    INFO][0m - loss: 2.436e-05, learning_rate: 0.0, global_step: 2560, interval_runtime: 5.9342, interval_samples_per_second: 2.696, interval_steps_per_second: 1.685, epoch: 20.0[0m
[32m[2022-09-20 11:02:42,006] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 11:02:42,006] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-20 11:02:42,006] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 11:02:42,006] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 11:02:42,006] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-20 11:03:15,429] [    INFO][0m - eval_loss: 3.2332468032836914, eval_accuracy: 0.5947775628626693, eval_runtime: 33.4222, eval_samples_per_second: 61.875, eval_steps_per_second: 3.89, epoch: 20.0[0m
[32m[2022-09-20 11:03:15,468] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-2560[0m
[32m[2022-09-20 11:03:15,468] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 11:03:18,155] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-2560/tokenizer_config.json[0m
[32m[2022-09-20 11:03:18,155] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-2560/special_tokens_map.json[0m
[32m[2022-09-20 11:03:23,138] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-20 11:03:23,138] [    INFO][0m - Loading best model from ./checkpoints_csldcp/checkpoint-2560 (score: 0.5947775628626693).[0m
[32m[2022-09-20 11:03:24,864] [    INFO][0m - train_runtime: 2507.9911, train_samples_per_second: 16.236, train_steps_per_second: 1.021, train_loss: 0.1949584753673946, epoch: 20.0[0m
[32m[2022-09-20 11:03:24,865] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/[0m
[32m[2022-09-20 11:03:24,866] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 11:03:27,388] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/tokenizer_config.json[0m
[32m[2022-09-20 11:03:27,389] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/special_tokens_map.json[0m
[32m[2022-09-20 11:03:27,390] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-20 11:03:27,390] [    INFO][0m -   epoch                    =       20.0[0m
[32m[2022-09-20 11:03:27,390] [    INFO][0m -   train_loss               =      0.195[0m
[32m[2022-09-20 11:03:27,390] [    INFO][0m -   train_runtime            = 0:41:47.99[0m
[32m[2022-09-20 11:03:27,390] [    INFO][0m -   train_samples_per_second =     16.236[0m
[32m[2022-09-20 11:03:27,390] [    INFO][0m -   train_steps_per_second   =      1.021[0m
[32m[2022-09-20 11:03:27,401] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-20 11:03:27,401] [    INFO][0m -   Num examples = 1784[0m
[32m[2022-09-20 11:03:27,401] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 11:03:27,401] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 11:03:27,401] [    INFO][0m -   Total prediction steps = 112[0m
[32m[2022-09-20 11:03:56,096] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-20 11:03:56,696] [    INFO][0m -   test_accuracy           =     0.5891[0m
[32m[2022-09-20 11:03:56,696] [    INFO][0m -   test_loss               =     3.2096[0m
[32m[2022-09-20 11:03:56,696] [    INFO][0m -   test_runtime            = 0:00:28.69[0m
[32m[2022-09-20 11:03:56,696] [    INFO][0m -   test_samples_per_second =     62.171[0m
[32m[2022-09-20 11:03:56,696] [    INFO][0m -   test_steps_per_second   =      3.903[0m
[32m[2022-09-20 11:03:56,697] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-20 11:03:56,697] [    INFO][0m -   Num examples = 2999[0m
[32m[2022-09-20 11:03:56,697] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 11:03:56,697] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 11:03:56,697] [    INFO][0m -   Total prediction steps = 188[0m
[32m[2022-09-20 11:04:50,974] [    INFO][0m - Predictions for csldcp saved to ./fewclue_submit_examples.[0m
{
  "labels": 26,
  "text_a": "\u672c\u6587\u7814\u7a76\u4e86\u8d35\u5dde\u4e1c\u5357\u90e8\u7684\u5929\u67f1\u3001\u9526\u5c4f\u548c\u9ece\u5e73\u5730\u533a\u7684\u5730\u8d28\u6784\u9020\u7279\u5f81\u3001\u8be5\u533a\u91d1\u77ff\u5e8a\u7279\u5f81\u53ca\u5176\u4e0e\u5730\u8d28\u6784\u9020\u7684\u5173\u7cfb.\u8ba4\u4e3a\u533a\u5185\u5317\u90e8\u548c\u5357\u90e8\u76f8\u8ddd\u6570\u5341\u516c\u91cc\u7684\u4e24\u6761\u8fd1\u4e1c\u897f\u5411\u7684\u57fa\u5e95\u526a\u5207\u65ad\u88c2\u5e26\u6784\u6210\u4e86\u672c\u533a\u5730\u5811\u5f0f\u7684\u6784\u9020\u683c\u5c40.\u7531\u4e8e\u8fd9\u4e24\u6761\u4e1c\u897f\u5411\u526a\u5207\u65ad\u88c2\u7684\u526a\u5207\u4f5c\u7528,\u5f62\u6210\u4e86\u672c\u533a\u4ee5\u5317\u4e1c\u5411\u4e3a\u4e3b\u7684\u8936\u76b1\u548c\u526a\u5207\u5e26.\u5317\u4e1c\u5411\u8936\u76b1\u548c\u526a\u5207\u5e26\u6784\u9020\u662f\u5728\u52a0\u91cc\u4e1c\u671f\u5f62\u6210\u7684,\u540c\u65f6\u4e5f\u53d1\u751f\u4e86\u7eff\u7247\u5ca9\u76f8\u7684\u53d8\u8d28\u4f5c\u7528.\u5728\u80cc\u659c\u5f62\u6210\u7684\u540c\u65f6\u6216\u7a0d\u540e\u526a\u5207\u4f5c\u7528\u5f00\u59cb\u53d1\u751f,\u526a\u5207\u4f5c\u7528\u7ee7\u627f\u7740\u5317\u4e1c\u5411,\u5f62\u6210\u4e86\u82e5\u5e72\u6761\u4e0e\u8936\u76b1\u8f74\u5e73\u884c\u6216\u76f8\u4ea4\u7684\u526a\u5207\u5e26.\u6210\u77ff\u6d41\u4f53\u6cbf\u7740\u526a\u5207\u5e26\u4e0a\u5347,\u4e00\u65b9\u9762\u5728\u526a\u5207\u5e26\u4e2d\u6c89\u6dc0\u51fa\u7a7f\u5c42\u4ea7\u51fa\u7684\u4e0d\u6574\u5408\u7834\u788e\u5e26\u578b\u91d1\u77ff\u4f53,\u5373\u900f\u955c\u72b6\u542b\u91d1\u77f3\u82f1\u8109;\u540c\u65f6\u5927\u90e8\u5206\u5145\u586b\u5230\u7531\u80cc\u659c\u548c\u526a\u5207\u4f5c\u7528\u5f62\u6210\u7684\u5c42\u95f4\u88c2\u9699\u6216\u5c42\u95f4\u7834\u788e\u5e26\u4e2d,\u4ece\u800c\u5f62\u6210\u8d4b\u5b58\u4e8e\u6d4a\u79ef\u5ca9\u7684\u987a\u5c42\u4ea7\u51fa\u7684\u5c42\u72b6\u542b\u91d1\u77f3\u82f1\u8109\u91d1\u77ff\u5e8a.\u8fd9\u79cd\u7c7b\u578b\u7684\u542b\u91d1\u77f3\u82f1\u8109,\u65e0\u8bba\u5728\u54c1\u4f4d(\u591a\u6570\u53ef\u89c1\u660e\u91d1)\u3001\u50a8\u91cf\u548c\u4ea7\u91cf\u65b9\u9762\u5747\u5f88\u6709\u524d\u666f,\u5e76\u4e14\u6709\u7740\u5341\u5206\u91cd\u8981\u7684\u5b66\u672f\u610f\u4e49,\u56e0\u4e3a\u8fd9\u5728\u56fd\u5185\u8fd8\u662f\u9996\u6b21\u53d1\u73b0.",
  "text_b": "",
  "uid": 44
}

Prediction done.
 
==========
tnews
==========
 
[32m[2022-09-20 11:05:06,996] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-20 11:05:06,996] [    INFO][0m - ============================================================[0m
[32m[2022-09-20 11:05:06,996] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-20 11:05:06,996] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-20 11:05:06,996] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-20 11:05:06,996] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-20 11:05:06,997] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-20 11:05:06,997] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-20 11:05:06,997] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-20 11:05:06,997] [    INFO][0m - [0m
[32m[2022-09-20 11:05:06,997] [    INFO][0m - ============================================================[0m
[32m[2022-09-20 11:05:06,997] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-20 11:05:06,997] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-20 11:05:06,997] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-20 11:05:06,997] [    INFO][0m - pretrained                    :/ssd2/wanghuijuan03/data/zero-shot/checkpoints_09191451/checkpoint-43000/model_state.pdparams[0m
[32m[2022-09-20 11:05:06,997] [    INFO][0m - prompt                        :‚Äú{'text':'text_a'}‚Äù‰∏äËø∞Êñ∞ÈóªÈÄâËá™{'mask'}{'mask'}‰∏ìÊ†è„ÄÇ[0m
[32m[2022-09-20 11:05:06,997] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-20 11:05:06,997] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-20 11:05:06,997] [    INFO][0m - task_name                     :tnews[0m
[32m[2022-09-20 11:05:06,997] [    INFO][0m - [0m
[32m[2022-09-20 11:05:06,998] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0920 11:05:06.999241 68473 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0920 11:05:07.003223 68473 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-20 11:05:11,773] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-20 11:05:11,784] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-20 11:05:11,784] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-20 11:05:11,785] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚Äù‰∏äËø∞Êñ∞ÈóªÈÄâËá™'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': '‰∏ìÊ†è„ÄÇ'}][0m
[32m[2022-09-20 11:05:14,029] [    INFO][0m - ============================================================[0m
[32m[2022-09-20 11:05:14,030] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-20 11:05:14,030] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-20 11:05:14,030] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-20 11:05:14,030] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-20 11:05:14,030] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-20 11:05:14,030] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-20 11:05:14,031] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-20 11:05:14,031] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-20 11:05:14,031] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-20 11:05:14,031] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-20 11:05:14,031] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-20 11:05:14,031] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-20 11:05:14,031] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-20 11:05:14,031] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-20 11:05:14,031] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-20 11:05:14,031] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-20 11:05:14,031] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-20 11:05:14,031] [    INFO][0m - eval_batch_size               :16[0m
[32m[2022-09-20 11:05:14,032] [    INFO][0m - eval_steps                    :200[0m
[32m[2022-09-20 11:05:14,032] [    INFO][0m - evaluation_strategy           :IntervalStrategy.EPOCH[0m
[32m[2022-09-20 11:05:14,032] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-20 11:05:14,032] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-20 11:05:14,032] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-20 11:05:14,032] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-20 11:05:14,032] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-20 11:05:14,032] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-20 11:05:14,032] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-20 11:05:14,032] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-20 11:05:14,032] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-20 11:05:14,032] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-09-20 11:05:14,032] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-20 11:05:14,033] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-20 11:05:14,033] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-20 11:05:14,033] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-20 11:05:14,033] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-20 11:05:14,033] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-20 11:05:14,033] [    INFO][0m - logging_dir                   :./checkpoints_tnews/runs/Sep20_11-05-06_instance-3bwob41y-01[0m
[32m[2022-09-20 11:05:14,033] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-20 11:05:14,033] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-20 11:05:14,033] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-20 11:05:14,033] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-20 11:05:14,033] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-20 11:05:14,033] [    INFO][0m - max_seq_length                :64[0m
[32m[2022-09-20 11:05:14,034] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-20 11:05:14,034] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-20 11:05:14,034] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-20 11:05:14,034] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-20 11:05:14,034] [    INFO][0m - num_train_epochs              :20.0[0m
[32m[2022-09-20 11:05:14,034] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-20 11:05:14,034] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-20 11:05:14,034] [    INFO][0m - output_dir                    :./checkpoints_tnews/[0m
[32m[2022-09-20 11:05:14,034] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-20 11:05:14,034] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-20 11:05:14,034] [    INFO][0m - per_device_eval_batch_size    :16[0m
[32m[2022-09-20 11:05:14,034] [    INFO][0m - per_device_train_batch_size   :16[0m
[32m[2022-09-20 11:05:14,034] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-20 11:05:14,035] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-20 11:05:14,035] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-20 11:05:14,035] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-09-20 11:05:14,035] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-20 11:05:14,035] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-20 11:05:14,035] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-20 11:05:14,035] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-20 11:05:14,035] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-20 11:05:14,035] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-20 11:05:14,035] [    INFO][0m - run_name                      :./checkpoints_tnews/[0m
[32m[2022-09-20 11:05:14,035] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-20 11:05:14,035] [    INFO][0m - save_steps                    :200[0m
[32m[2022-09-20 11:05:14,035] [    INFO][0m - save_strategy                 :IntervalStrategy.EPOCH[0m
[32m[2022-09-20 11:05:14,036] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-20 11:05:14,036] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-20 11:05:14,036] [    INFO][0m - seed                          :42[0m
[32m[2022-09-20 11:05:14,036] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-20 11:05:14,036] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-20 11:05:14,036] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-20 11:05:14,036] [    INFO][0m - train_batch_size              :16[0m
[32m[2022-09-20 11:05:14,036] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-20 11:05:14,036] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-20 11:05:14,036] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-20 11:05:14,036] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-20 11:05:14,036] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-20 11:05:14,036] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-20 11:05:14,037] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-20 11:05:14,037] [    INFO][0m - [0m
[32m[2022-09-20 11:05:14,040] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-20 11:05:14,040] [    INFO][0m -   Num examples = 1185[0m
[32m[2022-09-20 11:05:14,040] [    INFO][0m -   Num Epochs = 20[0m
[32m[2022-09-20 11:05:14,040] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2022-09-20 11:05:14,040] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2022-09-20 11:05:14,040] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-20 11:05:14,040] [    INFO][0m -   Total optimization steps = 1500.0[0m
[32m[2022-09-20 11:05:14,041] [    INFO][0m -   Total num train samples = 23700[0m
[32m[2022-09-20 11:05:17,100] [    INFO][0m - loss: 2.890201, learning_rate: 2.98e-05, global_step: 10, interval_runtime: 3.0581, interval_samples_per_second: 5.232, interval_steps_per_second: 3.27, epoch: 0.1333[0m
[32m[2022-09-20 11:05:18,984] [    INFO][0m - loss: 1.74913845, learning_rate: 2.96e-05, global_step: 20, interval_runtime: 1.8837, interval_samples_per_second: 8.494, interval_steps_per_second: 5.309, epoch: 0.2667[0m
[32m[2022-09-20 11:05:20,861] [    INFO][0m - loss: 1.42567921, learning_rate: 2.94e-05, global_step: 30, interval_runtime: 1.8775, interval_samples_per_second: 8.522, interval_steps_per_second: 5.326, epoch: 0.4[0m
[32m[2022-09-20 11:05:22,747] [    INFO][0m - loss: 1.51413145, learning_rate: 2.92e-05, global_step: 40, interval_runtime: 1.8863, interval_samples_per_second: 8.482, interval_steps_per_second: 5.301, epoch: 0.5333[0m
[32m[2022-09-20 11:05:24,620] [    INFO][0m - loss: 1.51236324, learning_rate: 2.9e-05, global_step: 50, interval_runtime: 1.8727, interval_samples_per_second: 8.544, interval_steps_per_second: 5.34, epoch: 0.6667[0m
[32m[2022-09-20 11:05:26,495] [    INFO][0m - loss: 1.61090679, learning_rate: 2.88e-05, global_step: 60, interval_runtime: 1.8747, interval_samples_per_second: 8.534, interval_steps_per_second: 5.334, epoch: 0.8[0m
[32m[2022-09-20 11:05:28,357] [    INFO][0m - loss: 1.62276039, learning_rate: 2.86e-05, global_step: 70, interval_runtime: 1.8627, interval_samples_per_second: 8.59, interval_steps_per_second: 5.368, epoch: 0.9333[0m
[32m[2022-09-20 11:05:29,190] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 11:05:29,190] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-20 11:05:29,191] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 11:05:29,191] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 11:05:29,191] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-20 11:05:34,798] [    INFO][0m - eval_loss: 1.5368518829345703, eval_accuracy: 0.5127504553734062, eval_runtime: 5.6065, eval_samples_per_second: 195.845, eval_steps_per_second: 12.307, epoch: 1.0[0m
[32m[2022-09-20 11:05:34,816] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-75[0m
[32m[2022-09-20 11:05:34,816] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 11:05:37,969] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-75/tokenizer_config.json[0m
[32m[2022-09-20 11:05:37,969] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-75/special_tokens_map.json[0m
[32m[2022-09-20 11:05:44,354] [    INFO][0m - loss: 1.23753557, learning_rate: 2.84e-05, global_step: 80, interval_runtime: 15.9962, interval_samples_per_second: 1.0, interval_steps_per_second: 0.625, epoch: 1.0667[0m
[32m[2022-09-20 11:05:46,212] [    INFO][0m - loss: 0.99421558, learning_rate: 2.8199999999999998e-05, global_step: 90, interval_runtime: 1.8585, interval_samples_per_second: 8.609, interval_steps_per_second: 5.381, epoch: 1.2[0m
[32m[2022-09-20 11:05:48,077] [    INFO][0m - loss: 1.01984921, learning_rate: 2.8e-05, global_step: 100, interval_runtime: 1.8644, interval_samples_per_second: 8.582, interval_steps_per_second: 5.364, epoch: 1.3333[0m
[32m[2022-09-20 11:05:49,937] [    INFO][0m - loss: 1.15798092, learning_rate: 2.78e-05, global_step: 110, interval_runtime: 1.8609, interval_samples_per_second: 8.598, interval_steps_per_second: 5.374, epoch: 1.4667[0m
[32m[2022-09-20 11:05:51,798] [    INFO][0m - loss: 1.08288317, learning_rate: 2.7600000000000003e-05, global_step: 120, interval_runtime: 1.861, interval_samples_per_second: 8.597, interval_steps_per_second: 5.373, epoch: 1.6[0m
[32m[2022-09-20 11:05:53,666] [    INFO][0m - loss: 1.02872286, learning_rate: 2.7400000000000002e-05, global_step: 130, interval_runtime: 1.8681, interval_samples_per_second: 8.565, interval_steps_per_second: 5.353, epoch: 1.7333[0m
[32m[2022-09-20 11:05:55,539] [    INFO][0m - loss: 0.96448469, learning_rate: 2.72e-05, global_step: 140, interval_runtime: 1.8729, interval_samples_per_second: 8.543, interval_steps_per_second: 5.339, epoch: 1.8667[0m
[32m[2022-09-20 11:05:57,310] [    INFO][0m - loss: 0.85941658, learning_rate: 2.7000000000000002e-05, global_step: 150, interval_runtime: 1.771, interval_samples_per_second: 9.034, interval_steps_per_second: 5.647, epoch: 2.0[0m
[32m[2022-09-20 11:05:57,311] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 11:05:57,311] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-20 11:05:57,311] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 11:05:57,311] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 11:05:57,311] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-20 11:06:02,888] [    INFO][0m - eval_loss: 1.655661940574646, eval_accuracy: 0.5437158469945356, eval_runtime: 5.5757, eval_samples_per_second: 196.925, eval_steps_per_second: 12.375, epoch: 2.0[0m
[32m[2022-09-20 11:06:02,907] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-150[0m
[32m[2022-09-20 11:06:02,907] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 11:06:05,684] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-150/tokenizer_config.json[0m
[32m[2022-09-20 11:06:05,685] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-150/special_tokens_map.json[0m
[32m[2022-09-20 11:06:12,938] [    INFO][0m - loss: 0.51411715, learning_rate: 2.68e-05, global_step: 160, interval_runtime: 15.6279, interval_samples_per_second: 1.024, interval_steps_per_second: 0.64, epoch: 2.1333[0m
[32m[2022-09-20 11:06:14,802] [    INFO][0m - loss: 0.61409459, learning_rate: 2.6600000000000003e-05, global_step: 170, interval_runtime: 1.8636, interval_samples_per_second: 8.586, interval_steps_per_second: 5.366, epoch: 2.2667[0m
[32m[2022-09-20 11:06:16,666] [    INFO][0m - loss: 0.52570562, learning_rate: 2.64e-05, global_step: 180, interval_runtime: 1.8638, interval_samples_per_second: 8.585, interval_steps_per_second: 5.365, epoch: 2.4[0m
[32m[2022-09-20 11:06:18,535] [    INFO][0m - loss: 0.37833955, learning_rate: 2.62e-05, global_step: 190, interval_runtime: 1.8688, interval_samples_per_second: 8.562, interval_steps_per_second: 5.351, epoch: 2.5333[0m
[32m[2022-09-20 11:06:20,397] [    INFO][0m - loss: 0.6690557, learning_rate: 2.6000000000000002e-05, global_step: 200, interval_runtime: 1.8627, interval_samples_per_second: 8.59, interval_steps_per_second: 5.369, epoch: 2.6667[0m
[32m[2022-09-20 11:06:22,268] [    INFO][0m - loss: 0.54785128, learning_rate: 2.58e-05, global_step: 210, interval_runtime: 1.8707, interval_samples_per_second: 8.553, interval_steps_per_second: 5.345, epoch: 2.8[0m
[32m[2022-09-20 11:06:24,132] [    INFO][0m - loss: 0.6185554, learning_rate: 2.5600000000000002e-05, global_step: 220, interval_runtime: 1.8643, interval_samples_per_second: 8.582, interval_steps_per_second: 5.364, epoch: 2.9333[0m
[32m[2022-09-20 11:06:24,964] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 11:06:24,965] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-20 11:06:24,965] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 11:06:24,965] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 11:06:24,965] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-20 11:06:30,493] [    INFO][0m - eval_loss: 1.8266003131866455, eval_accuracy: 0.5437158469945356, eval_runtime: 5.5284, eval_samples_per_second: 198.611, eval_steps_per_second: 12.481, epoch: 3.0[0m
[32m[2022-09-20 11:06:30,512] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-225[0m
[32m[2022-09-20 11:06:30,512] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 11:06:33,289] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-225/tokenizer_config.json[0m
[32m[2022-09-20 11:06:33,289] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-225/special_tokens_map.json[0m
[32m[2022-09-20 11:06:39,383] [    INFO][0m - loss: 0.3614944, learning_rate: 2.54e-05, global_step: 230, interval_runtime: 15.2503, interval_samples_per_second: 1.049, interval_steps_per_second: 0.656, epoch: 3.0667[0m
[32m[2022-09-20 11:06:41,247] [    INFO][0m - loss: 0.15324961, learning_rate: 2.52e-05, global_step: 240, interval_runtime: 1.8649, interval_samples_per_second: 8.58, interval_steps_per_second: 5.362, epoch: 3.2[0m
[32m[2022-09-20 11:06:43,121] [    INFO][0m - loss: 0.20433426, learning_rate: 2.5e-05, global_step: 250, interval_runtime: 1.8733, interval_samples_per_second: 8.541, interval_steps_per_second: 5.338, epoch: 3.3333[0m
[32m[2022-09-20 11:06:44,984] [    INFO][0m - loss: 0.29658926, learning_rate: 2.48e-05, global_step: 260, interval_runtime: 1.863, interval_samples_per_second: 8.588, interval_steps_per_second: 5.368, epoch: 3.4667[0m
[32m[2022-09-20 11:06:46,851] [    INFO][0m - loss: 0.24232993, learning_rate: 2.4599999999999998e-05, global_step: 270, interval_runtime: 1.8674, interval_samples_per_second: 8.568, interval_steps_per_second: 5.355, epoch: 3.6[0m
[32m[2022-09-20 11:06:48,719] [    INFO][0m - loss: 0.22839031, learning_rate: 2.44e-05, global_step: 280, interval_runtime: 1.8684, interval_samples_per_second: 8.564, interval_steps_per_second: 5.352, epoch: 3.7333[0m
[32m[2022-09-20 11:06:50,581] [    INFO][0m - loss: 0.24119151, learning_rate: 2.42e-05, global_step: 290, interval_runtime: 1.8612, interval_samples_per_second: 8.597, interval_steps_per_second: 5.373, epoch: 3.8667[0m
[32m[2022-09-20 11:06:52,364] [    INFO][0m - loss: 0.30196457, learning_rate: 2.4e-05, global_step: 300, interval_runtime: 1.7829, interval_samples_per_second: 8.974, interval_steps_per_second: 5.609, epoch: 4.0[0m
[32m[2022-09-20 11:06:52,364] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 11:06:52,364] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-20 11:06:52,364] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 11:06:52,364] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 11:06:52,364] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-20 11:06:57,853] [    INFO][0m - eval_loss: 2.7596211433410645, eval_accuracy: 0.5418943533697632, eval_runtime: 5.4884, eval_samples_per_second: 200.058, eval_steps_per_second: 12.572, epoch: 4.0[0m
[32m[2022-09-20 11:06:57,872] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-300[0m
[32m[2022-09-20 11:06:57,872] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 11:07:00,503] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-300/tokenizer_config.json[0m
[32m[2022-09-20 11:07:00,504] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-300/special_tokens_map.json[0m
[32m[2022-09-20 11:07:07,484] [    INFO][0m - loss: 0.12732618, learning_rate: 2.38e-05, global_step: 310, interval_runtime: 15.1208, interval_samples_per_second: 1.058, interval_steps_per_second: 0.661, epoch: 4.1333[0m
[32m[2022-09-20 11:07:09,346] [    INFO][0m - loss: 0.06571711, learning_rate: 2.3599999999999998e-05, global_step: 320, interval_runtime: 1.8615, interval_samples_per_second: 8.595, interval_steps_per_second: 5.372, epoch: 4.2667[0m
[32m[2022-09-20 11:07:11,204] [    INFO][0m - loss: 0.24459326, learning_rate: 2.3400000000000003e-05, global_step: 330, interval_runtime: 1.8583, interval_samples_per_second: 8.61, interval_steps_per_second: 5.381, epoch: 4.4[0m
[32m[2022-09-20 11:07:13,066] [    INFO][0m - loss: 0.120297, learning_rate: 2.32e-05, global_step: 340, interval_runtime: 1.8618, interval_samples_per_second: 8.594, interval_steps_per_second: 5.371, epoch: 4.5333[0m
[32m[2022-09-20 11:07:14,931] [    INFO][0m - loss: 0.1066682, learning_rate: 2.3000000000000003e-05, global_step: 350, interval_runtime: 1.8646, interval_samples_per_second: 8.581, interval_steps_per_second: 5.363, epoch: 4.6667[0m
[32m[2022-09-20 11:07:16,817] [    INFO][0m - loss: 0.23368742, learning_rate: 2.2800000000000002e-05, global_step: 360, interval_runtime: 1.886, interval_samples_per_second: 8.484, interval_steps_per_second: 5.302, epoch: 4.8[0m
[32m[2022-09-20 11:07:18,678] [    INFO][0m - loss: 0.22479124, learning_rate: 2.26e-05, global_step: 370, interval_runtime: 1.8609, interval_samples_per_second: 8.598, interval_steps_per_second: 5.374, epoch: 4.9333[0m
[32m[2022-09-20 11:07:19,504] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 11:07:19,504] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-20 11:07:19,504] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 11:07:19,504] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 11:07:19,505] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-20 11:07:24,998] [    INFO][0m - eval_loss: 3.3712406158447266, eval_accuracy: 0.5409836065573771, eval_runtime: 5.4926, eval_samples_per_second: 199.904, eval_steps_per_second: 12.562, epoch: 5.0[0m
[32m[2022-09-20 11:07:25,016] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-375[0m
[32m[2022-09-20 11:07:25,016] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 11:07:27,771] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-375/tokenizer_config.json[0m
[32m[2022-09-20 11:07:27,772] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-375/special_tokens_map.json[0m
[32m[2022-09-20 11:07:34,734] [    INFO][0m - loss: 0.0738287, learning_rate: 2.2400000000000002e-05, global_step: 380, interval_runtime: 16.0561, interval_samples_per_second: 0.997, interval_steps_per_second: 0.623, epoch: 5.0667[0m
[32m[2022-09-20 11:07:36,613] [    INFO][0m - loss: 0.05579465, learning_rate: 2.22e-05, global_step: 390, interval_runtime: 1.8792, interval_samples_per_second: 8.514, interval_steps_per_second: 5.321, epoch: 5.2[0m
[32m[2022-09-20 11:07:38,481] [    INFO][0m - loss: 0.07207732, learning_rate: 2.2e-05, global_step: 400, interval_runtime: 1.8679, interval_samples_per_second: 8.566, interval_steps_per_second: 5.354, epoch: 5.3333[0m
[32m[2022-09-20 11:07:40,356] [    INFO][0m - loss: 0.05726611, learning_rate: 2.18e-05, global_step: 410, interval_runtime: 1.8748, interval_samples_per_second: 8.534, interval_steps_per_second: 5.334, epoch: 5.4667[0m
[32m[2022-09-20 11:07:42,234] [    INFO][0m - loss: 0.06733714, learning_rate: 2.16e-05, global_step: 420, interval_runtime: 1.878, interval_samples_per_second: 8.52, interval_steps_per_second: 5.325, epoch: 5.6[0m
[32m[2022-09-20 11:07:44,100] [    INFO][0m - loss: 0.05833312, learning_rate: 2.1400000000000002e-05, global_step: 430, interval_runtime: 1.8669, interval_samples_per_second: 8.57, interval_steps_per_second: 5.356, epoch: 5.7333[0m
[32m[2022-09-20 11:07:45,967] [    INFO][0m - loss: 0.04253098, learning_rate: 2.12e-05, global_step: 440, interval_runtime: 1.8661, interval_samples_per_second: 8.574, interval_steps_per_second: 5.359, epoch: 5.8667[0m
[32m[2022-09-20 11:07:47,743] [    INFO][0m - loss: 0.03834534, learning_rate: 2.1e-05, global_step: 450, interval_runtime: 1.7763, interval_samples_per_second: 9.008, interval_steps_per_second: 5.63, epoch: 6.0[0m
[32m[2022-09-20 11:07:47,744] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 11:07:47,744] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-20 11:07:47,744] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 11:07:47,744] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 11:07:47,744] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-20 11:07:53,263] [    INFO][0m - eval_loss: 4.630578517913818, eval_accuracy: 0.5273224043715847, eval_runtime: 5.5183, eval_samples_per_second: 198.974, eval_steps_per_second: 12.504, epoch: 6.0[0m
[32m[2022-09-20 11:07:53,281] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-450[0m
[32m[2022-09-20 11:07:53,281] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 11:07:56,023] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-450/tokenizer_config.json[0m
[32m[2022-09-20 11:07:56,023] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-450/special_tokens_map.json[0m
[32m[2022-09-20 11:08:02,930] [    INFO][0m - loss: 0.00985568, learning_rate: 2.08e-05, global_step: 460, interval_runtime: 15.1867, interval_samples_per_second: 1.054, interval_steps_per_second: 0.658, epoch: 6.1333[0m
[32m[2022-09-20 11:08:04,797] [    INFO][0m - loss: 0.03626758, learning_rate: 2.06e-05, global_step: 470, interval_runtime: 1.8675, interval_samples_per_second: 8.568, interval_steps_per_second: 5.355, epoch: 6.2667[0m
[32m[2022-09-20 11:08:06,671] [    INFO][0m - loss: 0.00984424, learning_rate: 2.04e-05, global_step: 480, interval_runtime: 1.8736, interval_samples_per_second: 8.54, interval_steps_per_second: 5.337, epoch: 6.4[0m
[32m[2022-09-20 11:08:08,548] [    INFO][0m - loss: 0.11606963, learning_rate: 2.02e-05, global_step: 490, interval_runtime: 1.8773, interval_samples_per_second: 8.523, interval_steps_per_second: 5.327, epoch: 6.5333[0m
[32m[2022-09-20 11:08:10,426] [    INFO][0m - loss: 0.03843099, learning_rate: 1.9999999999999998e-05, global_step: 500, interval_runtime: 1.8772, interval_samples_per_second: 8.523, interval_steps_per_second: 5.327, epoch: 6.6667[0m
[32m[2022-09-20 11:08:12,293] [    INFO][0m - loss: 0.02658932, learning_rate: 1.98e-05, global_step: 510, interval_runtime: 1.8673, interval_samples_per_second: 8.569, interval_steps_per_second: 5.355, epoch: 6.8[0m
[32m[2022-09-20 11:08:14,161] [    INFO][0m - loss: 0.19288861, learning_rate: 1.96e-05, global_step: 520, interval_runtime: 1.8689, interval_samples_per_second: 8.561, interval_steps_per_second: 5.351, epoch: 6.9333[0m
[32m[2022-09-20 11:08:14,997] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 11:08:14,997] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-20 11:08:14,997] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 11:08:14,997] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 11:08:14,997] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-20 11:08:20,488] [    INFO][0m - eval_loss: 5.1162428855896, eval_accuracy: 0.5318761384335154, eval_runtime: 5.4912, eval_samples_per_second: 199.957, eval_steps_per_second: 12.566, epoch: 7.0[0m
[32m[2022-09-20 11:08:20,507] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-525[0m
[32m[2022-09-20 11:08:20,507] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 11:08:23,213] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-525/tokenizer_config.json[0m
[32m[2022-09-20 11:08:23,213] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-525/special_tokens_map.json[0m
[32m[2022-09-20 11:08:29,301] [    INFO][0m - loss: 0.00800393, learning_rate: 1.9399999999999997e-05, global_step: 530, interval_runtime: 15.1397, interval_samples_per_second: 1.057, interval_steps_per_second: 0.661, epoch: 7.0667[0m
[32m[2022-09-20 11:08:35,442] [    INFO][0m - loss: 0.08595312, learning_rate: 1.9200000000000003e-05, global_step: 540, interval_runtime: 1.8634, interval_samples_per_second: 8.586, interval_steps_per_second: 5.367, epoch: 7.2[0m
[32m[2022-09-20 11:08:37,326] [    INFO][0m - loss: 0.01976958, learning_rate: 1.9e-05, global_step: 550, interval_runtime: 6.1613, interval_samples_per_second: 2.597, interval_steps_per_second: 1.623, epoch: 7.3333[0m
[32m[2022-09-20 11:08:39,195] [    INFO][0m - loss: 0.03309641, learning_rate: 1.8800000000000003e-05, global_step: 560, interval_runtime: 1.8689, interval_samples_per_second: 8.561, interval_steps_per_second: 5.351, epoch: 7.4667[0m
[32m[2022-09-20 11:08:41,074] [    INFO][0m - loss: 0.11296202, learning_rate: 1.86e-05, global_step: 570, interval_runtime: 1.8788, interval_samples_per_second: 8.516, interval_steps_per_second: 5.322, epoch: 7.6[0m
[32m[2022-09-20 11:08:42,951] [    INFO][0m - loss: 0.01618958, learning_rate: 1.84e-05, global_step: 580, interval_runtime: 1.8773, interval_samples_per_second: 8.523, interval_steps_per_second: 5.327, epoch: 7.7333[0m
[32m[2022-09-20 11:08:44,838] [    INFO][0m - loss: 0.05928693, learning_rate: 1.8200000000000002e-05, global_step: 590, interval_runtime: 1.8868, interval_samples_per_second: 8.48, interval_steps_per_second: 5.3, epoch: 7.8667[0m
[32m[2022-09-20 11:08:46,612] [    INFO][0m - loss: 0.01406271, learning_rate: 1.8e-05, global_step: 600, interval_runtime: 1.7739, interval_samples_per_second: 9.02, interval_steps_per_second: 5.637, epoch: 8.0[0m
[32m[2022-09-20 11:08:46,612] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 11:08:46,613] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-20 11:08:46,613] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 11:08:46,613] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 11:08:46,613] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-20 11:08:52,096] [    INFO][0m - eval_loss: 4.959817886352539, eval_accuracy: 0.5391621129326047, eval_runtime: 5.4827, eval_samples_per_second: 200.267, eval_steps_per_second: 12.585, epoch: 8.0[0m
[32m[2022-09-20 11:08:52,114] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-600[0m
[32m[2022-09-20 11:08:52,115] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 11:08:54,673] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-600/tokenizer_config.json[0m
[32m[2022-09-20 11:08:54,674] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-600/special_tokens_map.json[0m
[32m[2022-09-20 11:09:02,449] [    INFO][0m - loss: 0.00703967, learning_rate: 1.7800000000000002e-05, global_step: 610, interval_runtime: 15.0259, interval_samples_per_second: 1.065, interval_steps_per_second: 0.666, epoch: 8.1333[0m
[32m[2022-09-20 11:09:04,326] [    INFO][0m - loss: 0.00267, learning_rate: 1.76e-05, global_step: 620, interval_runtime: 2.6879, interval_samples_per_second: 5.953, interval_steps_per_second: 3.72, epoch: 8.2667[0m
[32m[2022-09-20 11:09:06,192] [    INFO][0m - loss: 0.012453, learning_rate: 1.74e-05, global_step: 630, interval_runtime: 1.8666, interval_samples_per_second: 8.572, interval_steps_per_second: 5.357, epoch: 8.4[0m
[32m[2022-09-20 11:09:08,073] [    INFO][0m - loss: 0.0221086, learning_rate: 1.72e-05, global_step: 640, interval_runtime: 1.8808, interval_samples_per_second: 8.507, interval_steps_per_second: 5.317, epoch: 8.5333[0m
[32m[2022-09-20 11:09:09,952] [    INFO][0m - loss: 0.00694269, learning_rate: 1.7e-05, global_step: 650, interval_runtime: 1.8784, interval_samples_per_second: 8.518, interval_steps_per_second: 5.324, epoch: 8.6667[0m
[32m[2022-09-20 11:09:11,822] [    INFO][0m - loss: 0.00140847, learning_rate: 1.6800000000000002e-05, global_step: 660, interval_runtime: 1.8701, interval_samples_per_second: 8.556, interval_steps_per_second: 5.347, epoch: 8.8[0m
[32m[2022-09-20 11:09:13,695] [    INFO][0m - loss: 0.05425683, learning_rate: 1.66e-05, global_step: 670, interval_runtime: 1.873, interval_samples_per_second: 8.543, interval_steps_per_second: 5.339, epoch: 8.9333[0m
[32m[2022-09-20 11:09:14,527] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 11:09:14,527] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-20 11:09:14,527] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 11:09:14,527] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 11:09:14,527] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-20 11:09:20,065] [    INFO][0m - eval_loss: 5.290275573730469, eval_accuracy: 0.5373406193078324, eval_runtime: 5.5381, eval_samples_per_second: 198.262, eval_steps_per_second: 12.459, epoch: 9.0[0m
[32m[2022-09-20 11:09:20,084] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-675[0m
[32m[2022-09-20 11:09:20,084] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 11:09:22,650] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-675/tokenizer_config.json[0m
[32m[2022-09-20 11:09:22,651] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-675/special_tokens_map.json[0m
[32m[2022-09-20 11:09:28,610] [    INFO][0m - loss: 0.0014293, learning_rate: 1.64e-05, global_step: 680, interval_runtime: 14.9159, interval_samples_per_second: 1.073, interval_steps_per_second: 0.67, epoch: 9.0667[0m
[32m[2022-09-20 11:09:30,475] [    INFO][0m - loss: 0.00884748, learning_rate: 1.62e-05, global_step: 690, interval_runtime: 1.8643, interval_samples_per_second: 8.582, interval_steps_per_second: 5.364, epoch: 9.2[0m
[32m[2022-09-20 11:09:32,344] [    INFO][0m - loss: 0.00516456, learning_rate: 1.6e-05, global_step: 700, interval_runtime: 1.8695, interval_samples_per_second: 8.558, interval_steps_per_second: 5.349, epoch: 9.3333[0m
[32m[2022-09-20 11:09:34,212] [    INFO][0m - loss: 0.00649461, learning_rate: 1.5799999999999998e-05, global_step: 710, interval_runtime: 1.8675, interval_samples_per_second: 8.568, interval_steps_per_second: 5.355, epoch: 9.4667[0m
[32m[2022-09-20 11:09:36,082] [    INFO][0m - loss: 0.00070827, learning_rate: 1.56e-05, global_step: 720, interval_runtime: 1.8698, interval_samples_per_second: 8.557, interval_steps_per_second: 5.348, epoch: 9.6[0m
[32m[2022-09-20 11:09:37,957] [    INFO][0m - loss: 0.00774679, learning_rate: 1.5399999999999998e-05, global_step: 730, interval_runtime: 1.8753, interval_samples_per_second: 8.532, interval_steps_per_second: 5.332, epoch: 9.7333[0m
[32m[2022-09-20 11:09:39,844] [    INFO][0m - loss: 0.00010483, learning_rate: 1.5200000000000002e-05, global_step: 740, interval_runtime: 1.8872, interval_samples_per_second: 8.478, interval_steps_per_second: 5.299, epoch: 9.8667[0m
[32m[2022-09-20 11:09:41,611] [    INFO][0m - loss: 0.02124321, learning_rate: 1.5e-05, global_step: 750, interval_runtime: 1.7664, interval_samples_per_second: 9.058, interval_steps_per_second: 5.661, epoch: 10.0[0m
[32m[2022-09-20 11:09:41,611] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 11:09:41,611] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-20 11:09:41,611] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 11:09:41,611] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 11:09:41,611] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-20 11:09:47,107] [    INFO][0m - eval_loss: 5.402775764465332, eval_accuracy: 0.5491803278688525, eval_runtime: 5.4954, eval_samples_per_second: 199.804, eval_steps_per_second: 12.556, epoch: 10.0[0m
[32m[2022-09-20 11:09:47,126] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-750[0m
[32m[2022-09-20 11:09:47,126] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 11:09:49,744] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-750/tokenizer_config.json[0m
[32m[2022-09-20 11:09:49,744] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-750/special_tokens_map.json[0m
[32m[2022-09-20 11:09:56,601] [    INFO][0m - loss: 0.00050673, learning_rate: 1.48e-05, global_step: 760, interval_runtime: 14.9906, interval_samples_per_second: 1.067, interval_steps_per_second: 0.667, epoch: 10.1333[0m
[32m[2022-09-20 11:09:58,480] [    INFO][0m - loss: 0.00057294, learning_rate: 1.46e-05, global_step: 770, interval_runtime: 1.8787, interval_samples_per_second: 8.517, interval_steps_per_second: 5.323, epoch: 10.2667[0m
[32m[2022-09-20 11:10:04,016] [    INFO][0m - loss: 0.00027111, learning_rate: 1.44e-05, global_step: 780, interval_runtime: 1.8699, interval_samples_per_second: 8.557, interval_steps_per_second: 5.348, epoch: 10.4[0m
[32m[2022-09-20 11:10:05,885] [    INFO][0m - loss: 0.02024735, learning_rate: 1.42e-05, global_step: 790, interval_runtime: 5.5355, interval_samples_per_second: 2.89, interval_steps_per_second: 1.807, epoch: 10.5333[0m
[32m[2022-09-20 11:10:07,751] [    INFO][0m - loss: 0.0006193, learning_rate: 1.4e-05, global_step: 800, interval_runtime: 1.8662, interval_samples_per_second: 8.574, interval_steps_per_second: 5.359, epoch: 10.6667[0m
[32m[2022-09-20 11:10:09,615] [    INFO][0m - loss: 8.578e-05, learning_rate: 1.3800000000000002e-05, global_step: 810, interval_runtime: 1.8633, interval_samples_per_second: 8.587, interval_steps_per_second: 5.367, epoch: 10.8[0m
[32m[2022-09-20 11:10:11,495] [    INFO][0m - loss: 4.067e-05, learning_rate: 1.36e-05, global_step: 820, interval_runtime: 1.8801, interval_samples_per_second: 8.51, interval_steps_per_second: 5.319, epoch: 10.9333[0m
[32m[2022-09-20 11:10:12,324] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 11:10:12,324] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-20 11:10:12,325] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 11:10:12,325] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 11:10:12,325] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-20 11:10:17,789] [    INFO][0m - eval_loss: 5.520888328552246, eval_accuracy: 0.5418943533697632, eval_runtime: 5.4638, eval_samples_per_second: 200.96, eval_steps_per_second: 12.629, epoch: 11.0[0m
[32m[2022-09-20 11:10:17,808] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-825[0m
[32m[2022-09-20 11:10:17,808] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 11:10:20,307] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-825/tokenizer_config.json[0m
[32m[2022-09-20 11:10:20,307] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-825/special_tokens_map.json[0m
[32m[2022-09-20 11:10:26,403] [    INFO][0m - loss: 0.00483072, learning_rate: 1.34e-05, global_step: 830, interval_runtime: 14.908, interval_samples_per_second: 1.073, interval_steps_per_second: 0.671, epoch: 11.0667[0m
[32m[2022-09-20 11:10:29,086] [    INFO][0m - loss: 6.667e-05, learning_rate: 1.32e-05, global_step: 840, interval_runtime: 1.876, interval_samples_per_second: 8.529, interval_steps_per_second: 5.331, epoch: 11.2[0m
[32m[2022-09-20 11:10:30,959] [    INFO][0m - loss: 8.723e-05, learning_rate: 1.3000000000000001e-05, global_step: 850, interval_runtime: 2.6804, interval_samples_per_second: 5.969, interval_steps_per_second: 3.731, epoch: 11.3333[0m
[32m[2022-09-20 11:10:32,832] [    INFO][0m - loss: 0.00013454, learning_rate: 1.2800000000000001e-05, global_step: 860, interval_runtime: 1.8713, interval_samples_per_second: 8.55, interval_steps_per_second: 5.344, epoch: 11.4667[0m
[32m[2022-09-20 11:10:34,699] [    INFO][0m - loss: 2.291e-05, learning_rate: 1.26e-05, global_step: 870, interval_runtime: 1.8687, interval_samples_per_second: 8.562, interval_steps_per_second: 5.351, epoch: 11.6[0m
[32m[2022-09-20 11:10:36,570] [    INFO][0m - loss: 0.00011623, learning_rate: 1.24e-05, global_step: 880, interval_runtime: 1.8709, interval_samples_per_second: 8.552, interval_steps_per_second: 5.345, epoch: 11.7333[0m
[32m[2022-09-20 11:10:38,437] [    INFO][0m - loss: 0.00031365, learning_rate: 1.22e-05, global_step: 890, interval_runtime: 1.867, interval_samples_per_second: 8.57, interval_steps_per_second: 5.356, epoch: 11.8667[0m
[32m[2022-09-20 11:10:40,203] [    INFO][0m - loss: 7.143e-05, learning_rate: 1.2e-05, global_step: 900, interval_runtime: 1.7651, interval_samples_per_second: 9.064, interval_steps_per_second: 5.665, epoch: 12.0[0m
[32m[2022-09-20 11:10:40,203] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 11:10:40,203] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-20 11:10:40,203] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 11:10:40,203] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 11:10:40,203] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-20 11:10:45,659] [    INFO][0m - eval_loss: 5.531952381134033, eval_accuracy: 0.5519125683060109, eval_runtime: 5.456, eval_samples_per_second: 201.247, eval_steps_per_second: 12.647, epoch: 12.0[0m
[32m[2022-09-20 11:10:45,674] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-900[0m
[32m[2022-09-20 11:10:45,674] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 11:10:48,208] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-900/tokenizer_config.json[0m
[32m[2022-09-20 11:10:48,208] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-900/special_tokens_map.json[0m
[32m[2022-09-20 11:10:54,995] [    INFO][0m - loss: 0.00012921, learning_rate: 1.1799999999999999e-05, global_step: 910, interval_runtime: 14.7928, interval_samples_per_second: 1.082, interval_steps_per_second: 0.676, epoch: 12.1333[0m
[32m[2022-09-20 11:10:56,859] [    INFO][0m - loss: 4.782e-05, learning_rate: 1.16e-05, global_step: 920, interval_runtime: 1.8635, interval_samples_per_second: 8.586, interval_steps_per_second: 5.366, epoch: 12.2667[0m
[32m[2022-09-20 11:10:58,725] [    INFO][0m - loss: 3.124e-05, learning_rate: 1.1400000000000001e-05, global_step: 930, interval_runtime: 1.8658, interval_samples_per_second: 8.575, interval_steps_per_second: 5.36, epoch: 12.4[0m
[32m[2022-09-20 11:11:00,591] [    INFO][0m - loss: 3.522e-05, learning_rate: 1.1200000000000001e-05, global_step: 940, interval_runtime: 1.8664, interval_samples_per_second: 8.573, interval_steps_per_second: 5.358, epoch: 12.5333[0m
[32m[2022-09-20 11:11:02,460] [    INFO][0m - loss: 0.00783907, learning_rate: 1.1e-05, global_step: 950, interval_runtime: 1.8689, interval_samples_per_second: 8.561, interval_steps_per_second: 5.351, epoch: 12.6667[0m
[32m[2022-09-20 11:11:04,341] [    INFO][0m - loss: 5.468e-05, learning_rate: 1.08e-05, global_step: 960, interval_runtime: 1.8812, interval_samples_per_second: 8.505, interval_steps_per_second: 5.316, epoch: 12.8[0m
[32m[2022-09-20 11:11:06,206] [    INFO][0m - loss: 0.00514466, learning_rate: 1.06e-05, global_step: 970, interval_runtime: 1.865, interval_samples_per_second: 8.579, interval_steps_per_second: 5.362, epoch: 12.9333[0m
[32m[2022-09-20 11:11:07,037] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 11:11:07,037] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-20 11:11:07,037] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 11:11:07,037] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 11:11:07,037] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-20 11:11:12,496] [    INFO][0m - eval_loss: 5.65096378326416, eval_accuracy: 0.5555555555555556, eval_runtime: 5.458, eval_samples_per_second: 201.172, eval_steps_per_second: 12.642, epoch: 13.0[0m
[32m[2022-09-20 11:11:12,510] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-975[0m
[32m[2022-09-20 11:11:12,510] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 11:11:15,348] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-975/tokenizer_config.json[0m
[32m[2022-09-20 11:11:15,348] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-975/special_tokens_map.json[0m
[32m[2022-09-20 11:11:21,794] [    INFO][0m - loss: 2.673e-05, learning_rate: 1.04e-05, global_step: 980, interval_runtime: 15.5881, interval_samples_per_second: 1.026, interval_steps_per_second: 0.642, epoch: 13.0667[0m
[32m[2022-09-20 11:11:23,673] [    INFO][0m - loss: 0.00021532, learning_rate: 1.02e-05, global_step: 990, interval_runtime: 1.8787, interval_samples_per_second: 8.516, interval_steps_per_second: 5.323, epoch: 13.2[0m
[32m[2022-09-20 11:11:25,539] [    INFO][0m - loss: 5.38e-05, learning_rate: 9.999999999999999e-06, global_step: 1000, interval_runtime: 1.866, interval_samples_per_second: 8.574, interval_steps_per_second: 5.359, epoch: 13.3333[0m
[32m[2022-09-20 11:11:27,407] [    INFO][0m - loss: 0.00843608, learning_rate: 9.8e-06, global_step: 1010, interval_runtime: 1.868, interval_samples_per_second: 8.565, interval_steps_per_second: 5.353, epoch: 13.4667[0m
[32m[2022-09-20 11:11:29,283] [    INFO][0m - loss: 2.836e-05, learning_rate: 9.600000000000001e-06, global_step: 1020, interval_runtime: 1.8755, interval_samples_per_second: 8.531, interval_steps_per_second: 5.332, epoch: 13.6[0m
[32m[2022-09-20 11:11:31,155] [    INFO][0m - loss: 2.129e-05, learning_rate: 9.400000000000001e-06, global_step: 1030, interval_runtime: 1.8727, interval_samples_per_second: 8.544, interval_steps_per_second: 5.34, epoch: 13.7333[0m
[32m[2022-09-20 11:11:33,055] [    INFO][0m - loss: 1.533e-05, learning_rate: 9.2e-06, global_step: 1040, interval_runtime: 1.8996, interval_samples_per_second: 8.423, interval_steps_per_second: 5.264, epoch: 13.8667[0m
[32m[2022-09-20 11:11:34,823] [    INFO][0m - loss: 3.638e-05, learning_rate: 9e-06, global_step: 1050, interval_runtime: 1.7683, interval_samples_per_second: 9.048, interval_steps_per_second: 5.655, epoch: 14.0[0m
[32m[2022-09-20 11:11:34,824] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 11:11:34,824] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-20 11:11:34,824] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 11:11:34,824] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 11:11:34,824] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-20 11:11:40,278] [    INFO][0m - eval_loss: 5.680507183074951, eval_accuracy: 0.5473588342440802, eval_runtime: 5.4534, eval_samples_per_second: 201.343, eval_steps_per_second: 12.653, epoch: 14.0[0m
[32m[2022-09-20 11:11:40,292] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-1050[0m
[32m[2022-09-20 11:11:40,292] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 11:11:42,943] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-1050/tokenizer_config.json[0m
[32m[2022-09-20 11:11:42,943] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-1050/special_tokens_map.json[0m
[32m[2022-09-20 11:11:49,862] [    INFO][0m - loss: 0.03675567, learning_rate: 8.8e-06, global_step: 1060, interval_runtime: 15.0386, interval_samples_per_second: 1.064, interval_steps_per_second: 0.665, epoch: 14.1333[0m
[32m[2022-09-20 11:11:52,217] [    INFO][0m - loss: 8.415e-05, learning_rate: 8.6e-06, global_step: 1070, interval_runtime: 1.876, interval_samples_per_second: 8.529, interval_steps_per_second: 5.33, epoch: 14.2667[0m
[32m[2022-09-20 11:11:54,087] [    INFO][0m - loss: 0.00014994, learning_rate: 8.400000000000001e-06, global_step: 1080, interval_runtime: 2.3487, interval_samples_per_second: 6.812, interval_steps_per_second: 4.258, epoch: 14.4[0m
[32m[2022-09-20 11:11:55,955] [    INFO][0m - loss: 4.994e-05, learning_rate: 8.2e-06, global_step: 1090, interval_runtime: 1.8683, interval_samples_per_second: 8.564, interval_steps_per_second: 5.352, epoch: 14.5333[0m
[32m[2022-09-20 11:11:57,822] [    INFO][0m - loss: 8.561e-05, learning_rate: 8e-06, global_step: 1100, interval_runtime: 1.8668, interval_samples_per_second: 8.571, interval_steps_per_second: 5.357, epoch: 14.6667[0m
[32m[2022-09-20 11:11:59,688] [    INFO][0m - loss: 9.33e-06, learning_rate: 7.8e-06, global_step: 1110, interval_runtime: 1.866, interval_samples_per_second: 8.574, interval_steps_per_second: 5.359, epoch: 14.8[0m
[32m[2022-09-20 11:12:01,548] [    INFO][0m - loss: 0.00012998, learning_rate: 7.600000000000001e-06, global_step: 1120, interval_runtime: 1.86, interval_samples_per_second: 8.602, interval_steps_per_second: 5.376, epoch: 14.9333[0m
[32m[2022-09-20 11:12:02,378] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 11:12:02,378] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-20 11:12:02,378] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 11:12:02,378] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 11:12:02,378] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-20 11:12:07,811] [    INFO][0m - eval_loss: 5.712134838104248, eval_accuracy: 0.5473588342440802, eval_runtime: 5.4329, eval_samples_per_second: 202.101, eval_steps_per_second: 12.7, epoch: 15.0[0m
[32m[2022-09-20 11:12:07,825] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-1125[0m
[32m[2022-09-20 11:12:07,825] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 11:12:10,432] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-1125/tokenizer_config.json[0m
[32m[2022-09-20 11:12:10,432] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-1125/special_tokens_map.json[0m
[32m[2022-09-20 11:12:16,563] [    INFO][0m - loss: 0.00010127, learning_rate: 7.4e-06, global_step: 1130, interval_runtime: 15.0158, interval_samples_per_second: 1.066, interval_steps_per_second: 0.666, epoch: 15.0667[0m
[32m[2022-09-20 11:12:18,515] [    INFO][0m - loss: 5.57e-05, learning_rate: 7.2e-06, global_step: 1140, interval_runtime: 1.8663, interval_samples_per_second: 8.573, interval_steps_per_second: 5.358, epoch: 15.2[0m
[32m[2022-09-20 11:12:20,381] [    INFO][0m - loss: 0.01135977, learning_rate: 7e-06, global_step: 1150, interval_runtime: 1.9515, interval_samples_per_second: 8.199, interval_steps_per_second: 5.124, epoch: 15.3333[0m
[32m[2022-09-20 11:12:22,257] [    INFO][0m - loss: 6.589e-05, learning_rate: 6.8e-06, global_step: 1160, interval_runtime: 1.8751, interval_samples_per_second: 8.533, interval_steps_per_second: 5.333, epoch: 15.4667[0m
[32m[2022-09-20 11:12:24,134] [    INFO][0m - loss: 2.74e-05, learning_rate: 6.6e-06, global_step: 1170, interval_runtime: 1.8781, interval_samples_per_second: 8.519, interval_steps_per_second: 5.325, epoch: 15.6[0m
[32m[2022-09-20 11:12:26,008] [    INFO][0m - loss: 2.046e-05, learning_rate: 6.4000000000000006e-06, global_step: 1180, interval_runtime: 1.8732, interval_samples_per_second: 8.541, interval_steps_per_second: 5.338, epoch: 15.7333[0m
[32m[2022-09-20 11:12:27,883] [    INFO][0m - loss: 2.502e-05, learning_rate: 6.2e-06, global_step: 1190, interval_runtime: 1.8753, interval_samples_per_second: 8.532, interval_steps_per_second: 5.332, epoch: 15.8667[0m
[32m[2022-09-20 11:12:29,654] [    INFO][0m - loss: 7.566e-05, learning_rate: 6e-06, global_step: 1200, interval_runtime: 1.7706, interval_samples_per_second: 9.036, interval_steps_per_second: 5.648, epoch: 16.0[0m
[32m[2022-09-20 11:12:29,654] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 11:12:29,654] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-20 11:12:29,654] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 11:12:29,655] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 11:12:29,655] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-20 11:12:35,085] [    INFO][0m - eval_loss: 5.592779636383057, eval_accuracy: 0.5510018214936248, eval_runtime: 5.4299, eval_samples_per_second: 202.214, eval_steps_per_second: 12.707, epoch: 16.0[0m
[32m[2022-09-20 11:12:35,099] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-1200[0m
[32m[2022-09-20 11:12:35,099] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 11:12:37,872] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-1200/tokenizer_config.json[0m
[32m[2022-09-20 11:12:37,873] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-1200/special_tokens_map.json[0m
[32m[2022-09-20 11:12:45,751] [    INFO][0m - loss: 7.23e-06, learning_rate: 5.8e-06, global_step: 1210, interval_runtime: 16.0975, interval_samples_per_second: 0.994, interval_steps_per_second: 0.621, epoch: 16.1333[0m
[32m[2022-09-20 11:12:47,620] [    INFO][0m - loss: 2.795e-05, learning_rate: 5.600000000000001e-06, global_step: 1220, interval_runtime: 1.8687, interval_samples_per_second: 8.562, interval_steps_per_second: 5.351, epoch: 16.2667[0m
[32m[2022-09-20 11:12:49,490] [    INFO][0m - loss: 1.254e-05, learning_rate: 5.4e-06, global_step: 1230, interval_runtime: 1.8699, interval_samples_per_second: 8.556, interval_steps_per_second: 5.348, epoch: 16.4[0m
[32m[2022-09-20 11:12:51,355] [    INFO][0m - loss: 7.979e-05, learning_rate: 5.2e-06, global_step: 1240, interval_runtime: 1.8654, interval_samples_per_second: 8.577, interval_steps_per_second: 5.361, epoch: 16.5333[0m
[32m[2022-09-20 11:12:53,223] [    INFO][0m - loss: 8.77e-06, learning_rate: 4.9999999999999996e-06, global_step: 1250, interval_runtime: 1.8674, interval_samples_per_second: 8.568, interval_steps_per_second: 5.355, epoch: 16.6667[0m
[32m[2022-09-20 11:12:55,092] [    INFO][0m - loss: 1.093e-05, learning_rate: 4.800000000000001e-06, global_step: 1260, interval_runtime: 1.869, interval_samples_per_second: 8.561, interval_steps_per_second: 5.35, epoch: 16.8[0m
[32m[2022-09-20 11:12:56,967] [    INFO][0m - loss: 0.00127139, learning_rate: 4.6e-06, global_step: 1270, interval_runtime: 1.8758, interval_samples_per_second: 8.529, interval_steps_per_second: 5.331, epoch: 16.9333[0m
[32m[2022-09-20 11:12:57,794] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 11:12:57,794] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-20 11:12:57,794] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 11:12:57,794] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 11:12:57,794] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-20 11:13:03,320] [    INFO][0m - eval_loss: 5.643221378326416, eval_accuracy: 0.5537340619307832, eval_runtime: 5.5254, eval_samples_per_second: 198.719, eval_steps_per_second: 12.488, epoch: 17.0[0m
[32m[2022-09-20 11:13:03,339] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-1275[0m
[32m[2022-09-20 11:13:03,339] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 11:13:05,951] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-1275/tokenizer_config.json[0m
[32m[2022-09-20 11:13:05,951] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-1275/special_tokens_map.json[0m
[32m[2022-09-20 11:13:11,774] [    INFO][0m - loss: 8.01e-06, learning_rate: 4.4e-06, global_step: 1280, interval_runtime: 14.8068, interval_samples_per_second: 1.081, interval_steps_per_second: 0.675, epoch: 17.0667[0m
[32m[2022-09-20 11:13:13,640] [    INFO][0m - loss: 4.879e-05, learning_rate: 4.2000000000000004e-06, global_step: 1290, interval_runtime: 1.8661, interval_samples_per_second: 8.574, interval_steps_per_second: 5.359, epoch: 17.2[0m
[32m[2022-09-20 11:13:15,501] [    INFO][0m - loss: 0.00014899, learning_rate: 4e-06, global_step: 1300, interval_runtime: 1.8605, interval_samples_per_second: 8.6, interval_steps_per_second: 5.375, epoch: 17.3333[0m
[32m[2022-09-20 11:13:17,367] [    INFO][0m - loss: 7.38e-06, learning_rate: 3.8000000000000005e-06, global_step: 1310, interval_runtime: 1.8658, interval_samples_per_second: 8.576, interval_steps_per_second: 5.36, epoch: 17.4667[0m
[32m[2022-09-20 11:13:19,595] [    INFO][0m - loss: 8.44e-06, learning_rate: 3.6e-06, global_step: 1320, interval_runtime: 1.8676, interval_samples_per_second: 8.567, interval_steps_per_second: 5.354, epoch: 17.6[0m
[32m[2022-09-20 11:13:21,475] [    INFO][0m - loss: 9.64e-06, learning_rate: 3.4e-06, global_step: 1330, interval_runtime: 2.2412, interval_samples_per_second: 7.139, interval_steps_per_second: 4.462, epoch: 17.7333[0m
[32m[2022-09-20 11:13:23,348] [    INFO][0m - loss: 6.271e-05, learning_rate: 3.2000000000000003e-06, global_step: 1340, interval_runtime: 1.8729, interval_samples_per_second: 8.543, interval_steps_per_second: 5.339, epoch: 17.8667[0m
[32m[2022-09-20 11:13:25,120] [    INFO][0m - loss: 7.05e-06, learning_rate: 3e-06, global_step: 1350, interval_runtime: 1.7718, interval_samples_per_second: 9.03, interval_steps_per_second: 5.644, epoch: 18.0[0m
[32m[2022-09-20 11:13:25,121] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 11:13:25,121] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-20 11:13:25,121] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 11:13:25,121] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 11:13:25,121] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-20 11:13:30,644] [    INFO][0m - eval_loss: 5.6904988288879395, eval_accuracy: 0.5528233151183971, eval_runtime: 5.5227, eval_samples_per_second: 198.817, eval_steps_per_second: 12.494, epoch: 18.0[0m
[32m[2022-09-20 11:13:30,663] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-1350[0m
[32m[2022-09-20 11:13:30,663] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 11:13:33,280] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-1350/tokenizer_config.json[0m
[32m[2022-09-20 11:13:33,280] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-1350/special_tokens_map.json[0m
[32m[2022-09-20 11:13:40,101] [    INFO][0m - loss: 1.724e-05, learning_rate: 2.8000000000000003e-06, global_step: 1360, interval_runtime: 14.9808, interval_samples_per_second: 1.068, interval_steps_per_second: 0.668, epoch: 18.1333[0m
[32m[2022-09-20 11:13:41,969] [    INFO][0m - loss: 2.579e-05, learning_rate: 2.6e-06, global_step: 1370, interval_runtime: 1.8683, interval_samples_per_second: 8.564, interval_steps_per_second: 5.352, epoch: 18.2667[0m
[32m[2022-09-20 11:13:43,849] [    INFO][0m - loss: 1.174e-05, learning_rate: 2.4000000000000003e-06, global_step: 1380, interval_runtime: 1.8791, interval_samples_per_second: 8.515, interval_steps_per_second: 5.322, epoch: 18.4[0m
[32m[2022-09-20 11:13:45,727] [    INFO][0m - loss: 1.084e-05, learning_rate: 2.2e-06, global_step: 1390, interval_runtime: 1.8787, interval_samples_per_second: 8.517, interval_steps_per_second: 5.323, epoch: 18.5333[0m
[32m[2022-09-20 11:13:47,597] [    INFO][0m - loss: 2.077e-05, learning_rate: 2e-06, global_step: 1400, interval_runtime: 1.8702, interval_samples_per_second: 8.555, interval_steps_per_second: 5.347, epoch: 18.6667[0m
[32m[2022-09-20 11:13:49,471] [    INFO][0m - loss: 2.087e-05, learning_rate: 1.8e-06, global_step: 1410, interval_runtime: 1.8738, interval_samples_per_second: 8.539, interval_steps_per_second: 5.337, epoch: 18.8[0m
[32m[2022-09-20 11:13:51,339] [    INFO][0m - loss: 1.168e-05, learning_rate: 1.6000000000000001e-06, global_step: 1420, interval_runtime: 1.8683, interval_samples_per_second: 8.564, interval_steps_per_second: 5.353, epoch: 18.9333[0m
[32m[2022-09-20 11:13:52,174] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 11:13:52,174] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-20 11:13:52,175] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 11:13:52,175] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 11:13:52,175] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-20 11:13:57,667] [    INFO][0m - eval_loss: 5.694109916687012, eval_accuracy: 0.5537340619307832, eval_runtime: 5.4923, eval_samples_per_second: 199.917, eval_steps_per_second: 12.563, epoch: 19.0[0m
[32m[2022-09-20 11:13:57,686] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-1425[0m
[32m[2022-09-20 11:13:57,686] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 11:14:00,156] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-1425/tokenizer_config.json[0m
[32m[2022-09-20 11:14:00,156] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-1425/special_tokens_map.json[0m
[32m[2022-09-20 11:14:05,995] [    INFO][0m - loss: 4.538e-05, learning_rate: 1.4000000000000001e-06, global_step: 1430, interval_runtime: 14.6559, interval_samples_per_second: 1.092, interval_steps_per_second: 0.682, epoch: 19.0667[0m
[32m[2022-09-20 11:14:07,860] [    INFO][0m - loss: 5.483e-05, learning_rate: 1.2000000000000002e-06, global_step: 1440, interval_runtime: 1.8649, interval_samples_per_second: 8.579, interval_steps_per_second: 5.362, epoch: 19.2[0m
[32m[2022-09-20 11:14:09,736] [    INFO][0m - loss: 6.28e-06, learning_rate: 1e-06, global_step: 1450, interval_runtime: 1.8756, interval_samples_per_second: 8.53, interval_steps_per_second: 5.332, epoch: 19.3333[0m
[32m[2022-09-20 11:14:11,604] [    INFO][0m - loss: 1.805e-05, learning_rate: 8.000000000000001e-07, global_step: 1460, interval_runtime: 1.868, interval_samples_per_second: 8.565, interval_steps_per_second: 5.353, epoch: 19.4667[0m
[32m[2022-09-20 11:14:14,735] [    INFO][0m - loss: 0.0197561, learning_rate: 6.000000000000001e-07, global_step: 1470, interval_runtime: 1.8693, interval_samples_per_second: 8.56, interval_steps_per_second: 5.35, epoch: 19.6[0m
[32m[2022-09-20 11:14:16,601] [    INFO][0m - loss: 1.597e-05, learning_rate: 4.0000000000000003e-07, global_step: 1480, interval_runtime: 3.1278, interval_samples_per_second: 5.115, interval_steps_per_second: 3.197, epoch: 19.7333[0m
[32m[2022-09-20 11:14:18,473] [    INFO][0m - loss: 2.802e-05, learning_rate: 2.0000000000000002e-07, global_step: 1490, interval_runtime: 1.8715, interval_samples_per_second: 8.549, interval_steps_per_second: 5.343, epoch: 19.8667[0m
[32m[2022-09-20 11:14:20,243] [    INFO][0m - loss: 2.101e-05, learning_rate: 0.0, global_step: 1500, interval_runtime: 1.7703, interval_samples_per_second: 9.038, interval_steps_per_second: 5.649, epoch: 20.0[0m
[32m[2022-09-20 11:14:20,243] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 11:14:20,244] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-20 11:14:20,244] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 11:14:20,244] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 11:14:20,244] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-20 11:14:25,733] [    INFO][0m - eval_loss: 5.6921515464782715, eval_accuracy: 0.5528233151183971, eval_runtime: 5.4889, eval_samples_per_second: 200.041, eval_steps_per_second: 12.571, epoch: 20.0[0m
[32m[2022-09-20 11:14:25,752] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-1500[0m
[32m[2022-09-20 11:14:25,752] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 11:14:28,233] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-1500/tokenizer_config.json[0m
[32m[2022-09-20 11:14:28,234] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-1500/special_tokens_map.json[0m
[32m[2022-09-20 11:14:32,965] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-20 11:14:32,966] [    INFO][0m - Loading best model from ./checkpoints_tnews/checkpoint-975 (score: 0.5555555555555556).[0m
[32m[2022-09-20 11:14:35,901] [    INFO][0m - train_runtime: 560.5188, train_samples_per_second: 42.282, train_steps_per_second: 2.676, train_loss: 0.19476598074401652, epoch: 20.0[0m
[32m[2022-09-20 11:14:35,955] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/[0m
[32m[2022-09-20 11:14:35,956] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 11:14:38,357] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/tokenizer_config.json[0m
[32m[2022-09-20 11:14:38,357] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/special_tokens_map.json[0m
[32m[2022-09-20 11:14:38,358] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-20 11:14:38,358] [    INFO][0m -   epoch                    =       20.0[0m
[32m[2022-09-20 11:14:38,358] [    INFO][0m -   train_loss               =     0.1948[0m
[32m[2022-09-20 11:14:38,358] [    INFO][0m -   train_runtime            = 0:09:20.51[0m
[32m[2022-09-20 11:14:38,358] [    INFO][0m -   train_samples_per_second =     42.282[0m
[32m[2022-09-20 11:14:38,359] [    INFO][0m -   train_steps_per_second   =      2.676[0m
[32m[2022-09-20 11:14:38,365] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-20 11:14:38,365] [    INFO][0m -   Num examples = 2010[0m
[32m[2022-09-20 11:14:38,365] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 11:14:38,365] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 11:14:38,365] [    INFO][0m -   Total prediction steps = 126[0m
[32m[2022-09-20 11:14:48,373] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-20 11:14:48,373] [    INFO][0m -   test_accuracy           =     0.5547[0m
[32m[2022-09-20 11:14:48,373] [    INFO][0m -   test_loss               =     5.2919[0m
[32m[2022-09-20 11:14:48,373] [    INFO][0m -   test_runtime            = 0:00:10.00[0m
[32m[2022-09-20 11:14:48,374] [    INFO][0m -   test_samples_per_second =    200.843[0m
[32m[2022-09-20 11:14:48,374] [    INFO][0m -   test_steps_per_second   =      12.59[0m
[32m[2022-09-20 11:14:48,374] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-20 11:14:48,374] [    INFO][0m -   Num examples = 1500[0m
[32m[2022-09-20 11:14:48,374] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 11:14:48,374] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 11:14:48,374] [    INFO][0m -   Total prediction steps = 94[0m
[32m[2022-09-20 11:14:56,813] [    INFO][0m - Predictions for tnewsf saved to ./fewclue_submit_examples.[0m
{
  "labels": 11,
  "text_a": "\u5b69\u5b50\u8ddf\u8c01\u7761\uff0c\u5c31\u662f\u8c01\u7684\u5b69\u5b50",
  "text_b": "",
  "uid": 448
}

Prediction done.
 
==========
iflytek
==========
 
[32m[2022-09-20 11:15:19,910] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-20 11:15:19,910] [    INFO][0m - ============================================================[0m
[32m[2022-09-20 11:15:19,910] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-20 11:15:19,911] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-20 11:15:19,911] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-20 11:15:19,911] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-20 11:15:19,911] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-20 11:15:19,911] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-20 11:15:19,911] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-20 11:15:19,911] [    INFO][0m - [0m
[32m[2022-09-20 11:15:19,911] [    INFO][0m - ============================================================[0m
[32m[2022-09-20 11:15:19,911] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-20 11:15:19,911] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-20 11:15:19,911] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-20 11:15:19,911] [    INFO][0m - pretrained                    :/ssd2/wanghuijuan03/data/zero-shot/checkpoints_09191451/checkpoint-43000/model_state.pdparams[0m
[32m[2022-09-20 11:15:19,912] [    INFO][0m - prompt                        :‚Äú{'text':'text_a'}‚ÄùÂõ†Ê≠§ÔºåÂ∫îÁî®Á±ªÂà´ÊòØ{'mask'}{'mask'}„ÄÇ[0m
[32m[2022-09-20 11:15:19,912] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-20 11:15:19,912] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-20 11:15:19,912] [    INFO][0m - task_name                     :iflytek[0m
[32m[2022-09-20 11:15:19,912] [    INFO][0m - [0m
[32m[2022-09-20 11:15:19,912] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0920 11:15:19.914112   874 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0920 11:15:19.918265   874 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-20 11:15:24,753] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-20 11:15:24,764] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-20 11:15:24,765] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-20 11:15:24,765] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùÂõ†Ê≠§ÔºåÂ∫îÁî®Á±ªÂà´ÊòØ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': '„ÄÇ'}][0m
[32m[2022-09-20 11:15:26,094] [    INFO][0m - ============================================================[0m
[32m[2022-09-20 11:15:26,094] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-20 11:15:26,094] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-20 11:15:26,094] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-20 11:15:26,094] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-20 11:15:26,094] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-20 11:15:26,094] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-20 11:15:26,095] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-20 11:15:26,095] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-20 11:15:26,095] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-20 11:15:26,095] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-20 11:15:26,095] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-20 11:15:26,095] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-20 11:15:26,095] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-20 11:15:26,095] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-20 11:15:26,095] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-20 11:15:26,095] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-20 11:15:26,095] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-20 11:15:26,095] [    INFO][0m - eval_batch_size               :16[0m
[32m[2022-09-20 11:15:26,095] [    INFO][0m - eval_steps                    :200[0m
[32m[2022-09-20 11:15:26,096] [    INFO][0m - evaluation_strategy           :IntervalStrategy.EPOCH[0m
[32m[2022-09-20 11:15:26,096] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-20 11:15:26,096] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-20 11:15:26,096] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-20 11:15:26,096] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-20 11:15:26,096] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-20 11:15:26,096] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-20 11:15:26,096] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-20 11:15:26,096] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-20 11:15:26,096] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-20 11:15:26,096] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-09-20 11:15:26,096] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-20 11:15:26,096] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-20 11:15:26,097] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-20 11:15:26,097] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-20 11:15:26,097] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-20 11:15:26,097] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-20 11:15:26,097] [    INFO][0m - logging_dir                   :./checkpoints_iflytek/runs/Sep20_11-15-19_instance-3bwob41y-01[0m
[32m[2022-09-20 11:15:26,097] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-20 11:15:26,097] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-20 11:15:26,097] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-20 11:15:26,097] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-20 11:15:26,097] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-20 11:15:26,097] [    INFO][0m - max_seq_length                :320[0m
[32m[2022-09-20 11:15:26,097] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-20 11:15:26,097] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-20 11:15:26,098] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-20 11:15:26,098] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-20 11:15:26,098] [    INFO][0m - num_train_epochs              :20.0[0m
[32m[2022-09-20 11:15:26,098] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-20 11:15:26,098] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-20 11:15:26,098] [    INFO][0m - output_dir                    :./checkpoints_iflytek/[0m
[32m[2022-09-20 11:15:26,098] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-20 11:15:26,098] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-20 11:15:26,098] [    INFO][0m - per_device_eval_batch_size    :16[0m
[32m[2022-09-20 11:15:26,098] [    INFO][0m - per_device_train_batch_size   :16[0m
[32m[2022-09-20 11:15:26,098] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-20 11:15:26,098] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-20 11:15:26,098] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-20 11:15:26,099] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-09-20 11:15:26,099] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-20 11:15:26,099] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-20 11:15:26,099] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-20 11:15:26,099] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-20 11:15:26,099] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-20 11:15:26,099] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-20 11:15:26,099] [    INFO][0m - run_name                      :./checkpoints_iflytek/[0m
[32m[2022-09-20 11:15:26,099] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-20 11:15:26,099] [    INFO][0m - save_steps                    :200[0m
[32m[2022-09-20 11:15:26,099] [    INFO][0m - save_strategy                 :IntervalStrategy.EPOCH[0m
[32m[2022-09-20 11:15:26,099] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-20 11:15:26,099] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-20 11:15:26,099] [    INFO][0m - seed                          :42[0m
[32m[2022-09-20 11:15:26,100] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-20 11:15:26,100] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-20 11:15:26,100] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-20 11:15:26,100] [    INFO][0m - train_batch_size              :16[0m
[32m[2022-09-20 11:15:26,100] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-20 11:15:26,100] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-20 11:15:26,100] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-20 11:15:26,100] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-20 11:15:26,100] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-20 11:15:26,100] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-20 11:15:26,100] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-20 11:15:26,100] [    INFO][0m - [0m
[32m[2022-09-20 11:15:26,103] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-20 11:15:26,103] [    INFO][0m -   Num examples = 3024[0m
[32m[2022-09-20 11:15:26,103] [    INFO][0m -   Num Epochs = 20[0m
[32m[2022-09-20 11:15:26,103] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2022-09-20 11:15:26,103] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2022-09-20 11:15:26,104] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-20 11:15:26,104] [    INFO][0m -   Total optimization steps = 3780.0[0m
[32m[2022-09-20 11:15:26,104] [    INFO][0m -   Total num train samples = 60480[0m
[33m[2022-09-20 11:15:26,112] [ WARNING][0m - Token indices sequence length is longer than the specified maximum sequence length for this model (637 > 512). Running this sequence through the model will result in indexing errors[0m
[32m[2022-09-20 11:15:35,126] [    INFO][0m - loss: 3.54978752, learning_rate: 2.992063492063492e-05, global_step: 10, interval_runtime: 9.0217, interval_samples_per_second: 1.774, interval_steps_per_second: 1.108, epoch: 0.0529[0m
[32m[2022-09-20 11:15:43,060] [    INFO][0m - loss: 3.11930046, learning_rate: 2.984126984126984e-05, global_step: 20, interval_runtime: 7.9341, interval_samples_per_second: 2.017, interval_steps_per_second: 1.26, epoch: 0.1058[0m
[32m[2022-09-20 11:15:51,047] [    INFO][0m - loss: 3.04221935, learning_rate: 2.9761904761904762e-05, global_step: 30, interval_runtime: 7.9862, interval_samples_per_second: 2.003, interval_steps_per_second: 1.252, epoch: 0.1587[0m
[32m[2022-09-20 11:15:58,977] [    INFO][0m - loss: 2.53534546, learning_rate: 2.9682539682539683e-05, global_step: 40, interval_runtime: 7.9301, interval_samples_per_second: 2.018, interval_steps_per_second: 1.261, epoch: 0.2116[0m
[32m[2022-09-20 11:16:06,932] [    INFO][0m - loss: 2.23683357, learning_rate: 2.96031746031746e-05, global_step: 50, interval_runtime: 7.9552, interval_samples_per_second: 2.011, interval_steps_per_second: 1.257, epoch: 0.2646[0m
[32m[2022-09-20 11:16:14,916] [    INFO][0m - loss: 2.12462063, learning_rate: 2.9523809523809523e-05, global_step: 60, interval_runtime: 7.9836, interval_samples_per_second: 2.004, interval_steps_per_second: 1.253, epoch: 0.3175[0m
[32m[2022-09-20 11:16:22,904] [    INFO][0m - loss: 2.29289131, learning_rate: 2.9444444444444445e-05, global_step: 70, interval_runtime: 7.9885, interval_samples_per_second: 2.003, interval_steps_per_second: 1.252, epoch: 0.3704[0m
[32m[2022-09-20 11:16:30,912] [    INFO][0m - loss: 2.40479698, learning_rate: 2.9365079365079366e-05, global_step: 80, interval_runtime: 8.0083, interval_samples_per_second: 1.998, interval_steps_per_second: 1.249, epoch: 0.4233[0m
[32m[2022-09-20 11:16:38,930] [    INFO][0m - loss: 2.23802261, learning_rate: 2.9285714285714284e-05, global_step: 90, interval_runtime: 8.018, interval_samples_per_second: 1.996, interval_steps_per_second: 1.247, epoch: 0.4762[0m
[32m[2022-09-20 11:16:46,926] [    INFO][0m - loss: 2.02591095, learning_rate: 2.9206349206349206e-05, global_step: 100, interval_runtime: 7.9954, interval_samples_per_second: 2.001, interval_steps_per_second: 1.251, epoch: 0.5291[0m
[32m[2022-09-20 11:16:54,914] [    INFO][0m - loss: 2.08595924, learning_rate: 2.9126984126984127e-05, global_step: 110, interval_runtime: 7.9885, interval_samples_per_second: 2.003, interval_steps_per_second: 1.252, epoch: 0.582[0m
[32m[2022-09-20 11:17:02,919] [    INFO][0m - loss: 1.90117455, learning_rate: 2.904761904761905e-05, global_step: 120, interval_runtime: 8.0051, interval_samples_per_second: 1.999, interval_steps_per_second: 1.249, epoch: 0.6349[0m
[32m[2022-09-20 11:17:10,912] [    INFO][0m - loss: 1.77337074, learning_rate: 2.8968253968253967e-05, global_step: 130, interval_runtime: 7.9928, interval_samples_per_second: 2.002, interval_steps_per_second: 1.251, epoch: 0.6878[0m
[32m[2022-09-20 11:17:18,898] [    INFO][0m - loss: 1.98485985, learning_rate: 2.8888888888888888e-05, global_step: 140, interval_runtime: 7.9856, interval_samples_per_second: 2.004, interval_steps_per_second: 1.252, epoch: 0.7407[0m
[32m[2022-09-20 11:17:26,881] [    INFO][0m - loss: 1.80244102, learning_rate: 2.880952380952381e-05, global_step: 150, interval_runtime: 7.9834, interval_samples_per_second: 2.004, interval_steps_per_second: 1.253, epoch: 0.7937[0m
[32m[2022-09-20 11:17:34,875] [    INFO][0m - loss: 1.91361008, learning_rate: 2.873015873015873e-05, global_step: 160, interval_runtime: 7.9936, interval_samples_per_second: 2.002, interval_steps_per_second: 1.251, epoch: 0.8466[0m
[32m[2022-09-20 11:17:42,850] [    INFO][0m - loss: 1.82804375, learning_rate: 2.865079365079365e-05, global_step: 170, interval_runtime: 7.9758, interval_samples_per_second: 2.006, interval_steps_per_second: 1.254, epoch: 0.8995[0m
[32m[2022-09-20 11:17:50,795] [    INFO][0m - loss: 2.17292709, learning_rate: 2.857142857142857e-05, global_step: 180, interval_runtime: 7.9446, interval_samples_per_second: 2.014, interval_steps_per_second: 1.259, epoch: 0.9524[0m
[32m[2022-09-20 11:17:57,809] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 11:17:57,809] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-20 11:17:57,809] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 11:17:57,809] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 11:17:57,809] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-20 11:18:25,438] [    INFO][0m - eval_loss: 1.8595175743103027, eval_accuracy: 0.4450109249817917, eval_runtime: 27.6278, eval_samples_per_second: 49.696, eval_steps_per_second: 3.113, epoch: 1.0[0m
[32m[2022-09-20 11:18:25,465] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-189[0m
[32m[2022-09-20 11:18:25,465] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 11:18:28,440] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-189/tokenizer_config.json[0m
[32m[2022-09-20 11:18:28,440] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-189/special_tokens_map.json[0m
[32m[2022-09-20 11:18:35,185] [    INFO][0m - loss: 1.83557739, learning_rate: 2.8492063492063492e-05, global_step: 190, interval_runtime: 44.3901, interval_samples_per_second: 0.36, interval_steps_per_second: 0.225, epoch: 1.0053[0m
[32m[2022-09-20 11:18:43,125] [    INFO][0m - loss: 1.24353704, learning_rate: 2.8412698412698414e-05, global_step: 200, interval_runtime: 7.9399, interval_samples_per_second: 2.015, interval_steps_per_second: 1.259, epoch: 1.0582[0m
[32m[2022-09-20 11:18:51,086] [    INFO][0m - loss: 1.26853352, learning_rate: 2.8333333333333332e-05, global_step: 210, interval_runtime: 7.9612, interval_samples_per_second: 2.01, interval_steps_per_second: 1.256, epoch: 1.1111[0m
[32m[2022-09-20 11:18:59,045] [    INFO][0m - loss: 1.16272449, learning_rate: 2.8253968253968253e-05, global_step: 220, interval_runtime: 7.9589, interval_samples_per_second: 2.01, interval_steps_per_second: 1.256, epoch: 1.164[0m
[32m[2022-09-20 11:19:09,501] [    INFO][0m - loss: 1.341292, learning_rate: 2.8174603174603175e-05, global_step: 230, interval_runtime: 7.9784, interval_samples_per_second: 2.005, interval_steps_per_second: 1.253, epoch: 1.2169[0m
[32m[2022-09-20 11:19:17,463] [    INFO][0m - loss: 1.18212395, learning_rate: 2.8095238095238096e-05, global_step: 240, interval_runtime: 10.4391, interval_samples_per_second: 1.533, interval_steps_per_second: 0.958, epoch: 1.2698[0m
[32m[2022-09-20 11:19:25,409] [    INFO][0m - loss: 1.56119404, learning_rate: 2.8015873015873015e-05, global_step: 250, interval_runtime: 7.9464, interval_samples_per_second: 2.014, interval_steps_per_second: 1.258, epoch: 1.3228[0m
[32m[2022-09-20 11:19:33,367] [    INFO][0m - loss: 1.58373671, learning_rate: 2.7936507936507936e-05, global_step: 260, interval_runtime: 7.9579, interval_samples_per_second: 2.011, interval_steps_per_second: 1.257, epoch: 1.3757[0m
[32m[2022-09-20 11:19:41,401] [    INFO][0m - loss: 1.28564758, learning_rate: 2.7857142857142858e-05, global_step: 270, interval_runtime: 8.0336, interval_samples_per_second: 1.992, interval_steps_per_second: 1.245, epoch: 1.4286[0m
[32m[2022-09-20 11:19:51,740] [    INFO][0m - loss: 1.14560823, learning_rate: 2.777777777777778e-05, global_step: 280, interval_runtime: 7.9858, interval_samples_per_second: 2.004, interval_steps_per_second: 1.252, epoch: 1.4815[0m
[32m[2022-09-20 11:19:59,703] [    INFO][0m - loss: 1.33683558, learning_rate: 2.7698412698412697e-05, global_step: 290, interval_runtime: 10.3164, interval_samples_per_second: 1.551, interval_steps_per_second: 0.969, epoch: 1.5344[0m
[32m[2022-09-20 11:20:07,719] [    INFO][0m - loss: 1.36313019, learning_rate: 2.761904761904762e-05, global_step: 300, interval_runtime: 8.0163, interval_samples_per_second: 1.996, interval_steps_per_second: 1.247, epoch: 1.5873[0m
[32m[2022-09-20 11:20:15,709] [    INFO][0m - loss: 1.45038738, learning_rate: 2.753968253968254e-05, global_step: 310, interval_runtime: 7.9899, interval_samples_per_second: 2.003, interval_steps_per_second: 1.252, epoch: 1.6402[0m
[32m[2022-09-20 11:20:23,707] [    INFO][0m - loss: 1.27692423, learning_rate: 2.7460317460317462e-05, global_step: 320, interval_runtime: 7.9983, interval_samples_per_second: 2.0, interval_steps_per_second: 1.25, epoch: 1.6931[0m
[32m[2022-09-20 11:20:31,696] [    INFO][0m - loss: 1.53579636, learning_rate: 2.738095238095238e-05, global_step: 330, interval_runtime: 7.9885, interval_samples_per_second: 2.003, interval_steps_per_second: 1.252, epoch: 1.746[0m
[32m[2022-09-20 11:20:39,667] [    INFO][0m - loss: 1.46035337, learning_rate: 2.73015873015873e-05, global_step: 340, interval_runtime: 7.9709, interval_samples_per_second: 2.007, interval_steps_per_second: 1.255, epoch: 1.7989[0m
[32m[2022-09-20 11:20:47,652] [    INFO][0m - loss: 1.21600313, learning_rate: 2.7222222222222223e-05, global_step: 350, interval_runtime: 7.9848, interval_samples_per_second: 2.004, interval_steps_per_second: 1.252, epoch: 1.8519[0m
[32m[2022-09-20 11:20:55,659] [    INFO][0m - loss: 1.38133326, learning_rate: 2.7142857142857144e-05, global_step: 360, interval_runtime: 8.0071, interval_samples_per_second: 1.998, interval_steps_per_second: 1.249, epoch: 1.9048[0m
[32m[2022-09-20 11:21:03,647] [    INFO][0m - loss: 1.37104578, learning_rate: 2.7063492063492062e-05, global_step: 370, interval_runtime: 7.9879, interval_samples_per_second: 2.003, interval_steps_per_second: 1.252, epoch: 1.9577[0m
[32m[2022-09-20 11:21:09,874] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 11:21:09,874] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-20 11:21:09,874] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 11:21:09,874] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 11:21:09,874] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-20 11:21:37,570] [    INFO][0m - eval_loss: 1.7903263568878174, eval_accuracy: 0.46176256372906044, eval_runtime: 27.6956, eval_samples_per_second: 49.575, eval_steps_per_second: 3.105, epoch: 2.0[0m
[32m[2022-09-20 11:21:39,801] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-378[0m
[32m[2022-09-20 11:21:39,802] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 11:21:42,761] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-378/tokenizer_config.json[0m
[32m[2022-09-20 11:21:42,761] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-378/special_tokens_map.json[0m
[32m[2022-09-20 11:21:50,407] [    INFO][0m - loss: 1.01073914, learning_rate: 2.6984126984126984e-05, global_step: 380, interval_runtime: 46.7608, interval_samples_per_second: 0.342, interval_steps_per_second: 0.214, epoch: 2.0106[0m
[32m[2022-09-20 11:21:58,373] [    INFO][0m - loss: 0.70402102, learning_rate: 2.6904761904761905e-05, global_step: 390, interval_runtime: 7.9651, interval_samples_per_second: 2.009, interval_steps_per_second: 1.255, epoch: 2.0635[0m
[32m[2022-09-20 11:22:06,328] [    INFO][0m - loss: 0.62105093, learning_rate: 2.6825396825396827e-05, global_step: 400, interval_runtime: 7.9557, interval_samples_per_second: 2.011, interval_steps_per_second: 1.257, epoch: 2.1164[0m
[32m[2022-09-20 11:22:14,265] [    INFO][0m - loss: 0.78277063, learning_rate: 2.6746031746031745e-05, global_step: 410, interval_runtime: 7.9362, interval_samples_per_second: 2.016, interval_steps_per_second: 1.26, epoch: 2.1693[0m
[32m[2022-09-20 11:22:22,227] [    INFO][0m - loss: 0.79846382, learning_rate: 2.6666666666666667e-05, global_step: 420, interval_runtime: 7.9625, interval_samples_per_second: 2.009, interval_steps_per_second: 1.256, epoch: 2.2222[0m
[32m[2022-09-20 11:22:30,212] [    INFO][0m - loss: 0.78079448, learning_rate: 2.6587301587301588e-05, global_step: 430, interval_runtime: 7.9845, interval_samples_per_second: 2.004, interval_steps_per_second: 1.252, epoch: 2.2751[0m
[32m[2022-09-20 11:22:38,206] [    INFO][0m - loss: 0.77088456, learning_rate: 2.650793650793651e-05, global_step: 440, interval_runtime: 7.9942, interval_samples_per_second: 2.001, interval_steps_per_second: 1.251, epoch: 2.328[0m
[32m[2022-09-20 11:22:46,218] [    INFO][0m - loss: 0.69305415, learning_rate: 2.6428571428571428e-05, global_step: 450, interval_runtime: 8.012, interval_samples_per_second: 1.997, interval_steps_per_second: 1.248, epoch: 2.381[0m
[32m[2022-09-20 11:22:54,240] [    INFO][0m - loss: 0.79198408, learning_rate: 2.634920634920635e-05, global_step: 460, interval_runtime: 8.0223, interval_samples_per_second: 1.994, interval_steps_per_second: 1.247, epoch: 2.4339[0m
[32m[2022-09-20 11:23:02,258] [    INFO][0m - loss: 0.80752106, learning_rate: 2.626984126984127e-05, global_step: 470, interval_runtime: 8.0182, interval_samples_per_second: 1.995, interval_steps_per_second: 1.247, epoch: 2.4868[0m
[32m[2022-09-20 11:23:10,271] [    INFO][0m - loss: 0.91252317, learning_rate: 2.6190476190476192e-05, global_step: 480, interval_runtime: 8.013, interval_samples_per_second: 1.997, interval_steps_per_second: 1.248, epoch: 2.5397[0m
[32m[2022-09-20 11:23:18,256] [    INFO][0m - loss: 0.88439741, learning_rate: 2.611111111111111e-05, global_step: 490, interval_runtime: 7.9845, interval_samples_per_second: 2.004, interval_steps_per_second: 1.252, epoch: 2.5926[0m
[32m[2022-09-20 11:23:26,235] [    INFO][0m - loss: 0.7239656, learning_rate: 2.6031746031746032e-05, global_step: 500, interval_runtime: 7.979, interval_samples_per_second: 2.005, interval_steps_per_second: 1.253, epoch: 2.6455[0m
[32m[2022-09-20 11:23:34,172] [    INFO][0m - loss: 0.92765503, learning_rate: 2.5952380952380953e-05, global_step: 510, interval_runtime: 7.9368, interval_samples_per_second: 2.016, interval_steps_per_second: 1.26, epoch: 2.6984[0m
[32m[2022-09-20 11:23:42,149] [    INFO][0m - loss: 0.9474102, learning_rate: 2.5873015873015875e-05, global_step: 520, interval_runtime: 7.9776, interval_samples_per_second: 2.006, interval_steps_per_second: 1.254, epoch: 2.7513[0m
[32m[2022-09-20 11:23:50,143] [    INFO][0m - loss: 0.91384916, learning_rate: 2.5793650793650793e-05, global_step: 530, interval_runtime: 7.9935, interval_samples_per_second: 2.002, interval_steps_per_second: 1.251, epoch: 2.8042[0m
[32m[2022-09-20 11:23:58,123] [    INFO][0m - loss: 0.92399883, learning_rate: 2.5714285714285714e-05, global_step: 540, interval_runtime: 7.9805, interval_samples_per_second: 2.005, interval_steps_per_second: 1.253, epoch: 2.8571[0m
[32m[2022-09-20 11:24:06,138] [    INFO][0m - loss: 0.78950791, learning_rate: 2.5634920634920636e-05, global_step: 550, interval_runtime: 8.0144, interval_samples_per_second: 1.996, interval_steps_per_second: 1.248, epoch: 2.9101[0m
[32m[2022-09-20 11:24:14,147] [    INFO][0m - loss: 0.83909369, learning_rate: 2.5555555555555557e-05, global_step: 560, interval_runtime: 8.0095, interval_samples_per_second: 1.998, interval_steps_per_second: 1.249, epoch: 2.963[0m
[32m[2022-09-20 11:24:19,561] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 11:24:19,561] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-20 11:24:19,562] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 11:24:19,562] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 11:24:19,562] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-20 11:24:47,206] [    INFO][0m - eval_loss: 2.0737972259521484, eval_accuracy: 0.45447924253459576, eval_runtime: 27.6443, eval_samples_per_second: 49.667, eval_steps_per_second: 3.111, epoch: 3.0[0m
[32m[2022-09-20 11:24:47,233] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-567[0m
[32m[2022-09-20 11:24:47,233] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 11:24:50,066] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-567/tokenizer_config.json[0m
[32m[2022-09-20 11:24:50,066] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-567/special_tokens_map.json[0m
[32m[2022-09-20 11:24:58,055] [    INFO][0m - loss: 0.7019608, learning_rate: 2.5476190476190476e-05, global_step: 570, interval_runtime: 43.9075, interval_samples_per_second: 0.364, interval_steps_per_second: 0.228, epoch: 3.0159[0m
[32m[2022-09-20 11:25:05,993] [    INFO][0m - loss: 0.3358567, learning_rate: 2.5396825396825397e-05, global_step: 580, interval_runtime: 7.9383, interval_samples_per_second: 2.016, interval_steps_per_second: 1.26, epoch: 3.0688[0m
[32m[2022-09-20 11:25:13,954] [    INFO][0m - loss: 0.42473512, learning_rate: 2.531746031746032e-05, global_step: 590, interval_runtime: 7.9611, interval_samples_per_second: 2.01, interval_steps_per_second: 1.256, epoch: 3.1217[0m
[32m[2022-09-20 11:25:21,920] [    INFO][0m - loss: 0.48478031, learning_rate: 2.523809523809524e-05, global_step: 600, interval_runtime: 7.9655, interval_samples_per_second: 2.009, interval_steps_per_second: 1.255, epoch: 3.1746[0m
[32m[2022-09-20 11:25:29,893] [    INFO][0m - loss: 0.46354671, learning_rate: 2.5158730158730158e-05, global_step: 610, interval_runtime: 7.9726, interval_samples_per_second: 2.007, interval_steps_per_second: 1.254, epoch: 3.2275[0m
[32m[2022-09-20 11:25:37,859] [    INFO][0m - loss: 0.37939734, learning_rate: 2.507936507936508e-05, global_step: 620, interval_runtime: 7.9668, interval_samples_per_second: 2.008, interval_steps_per_second: 1.255, epoch: 3.2804[0m
[32m[2022-09-20 11:25:45,813] [    INFO][0m - loss: 0.54448471, learning_rate: 2.5e-05, global_step: 630, interval_runtime: 7.9533, interval_samples_per_second: 2.012, interval_steps_per_second: 1.257, epoch: 3.3333[0m
[32m[2022-09-20 11:25:53,803] [    INFO][0m - loss: 0.26394026, learning_rate: 2.4920634920634923e-05, global_step: 640, interval_runtime: 7.9905, interval_samples_per_second: 2.002, interval_steps_per_second: 1.251, epoch: 3.3862[0m
[32m[2022-09-20 11:26:01,820] [    INFO][0m - loss: 0.26085248, learning_rate: 2.484126984126984e-05, global_step: 650, interval_runtime: 8.0169, interval_samples_per_second: 1.996, interval_steps_per_second: 1.247, epoch: 3.4392[0m
[32m[2022-09-20 11:26:10,293] [    INFO][0m - loss: 0.41527762, learning_rate: 2.4761904761904762e-05, global_step: 660, interval_runtime: 8.0147, interval_samples_per_second: 1.996, interval_steps_per_second: 1.248, epoch: 3.4921[0m
[32m[2022-09-20 11:26:18,306] [    INFO][0m - loss: 0.46330552, learning_rate: 2.4682539682539684e-05, global_step: 670, interval_runtime: 8.4715, interval_samples_per_second: 1.889, interval_steps_per_second: 1.18, epoch: 3.545[0m
[32m[2022-09-20 11:26:26,321] [    INFO][0m - loss: 0.46637316, learning_rate: 2.4603174603174605e-05, global_step: 680, interval_runtime: 8.015, interval_samples_per_second: 1.996, interval_steps_per_second: 1.248, epoch: 3.5979[0m
[32m[2022-09-20 11:26:34,328] [    INFO][0m - loss: 0.49418259, learning_rate: 2.4523809523809523e-05, global_step: 690, interval_runtime: 8.0071, interval_samples_per_second: 1.998, interval_steps_per_second: 1.249, epoch: 3.6508[0m
[32m[2022-09-20 11:26:42,320] [    INFO][0m - loss: 0.39797816, learning_rate: 2.4444444444444445e-05, global_step: 700, interval_runtime: 7.9916, interval_samples_per_second: 2.002, interval_steps_per_second: 1.251, epoch: 3.7037[0m
[32m[2022-09-20 11:26:50,335] [    INFO][0m - loss: 0.35252817, learning_rate: 2.4365079365079366e-05, global_step: 710, interval_runtime: 8.0143, interval_samples_per_second: 1.996, interval_steps_per_second: 1.248, epoch: 3.7566[0m
[32m[2022-09-20 11:26:58,419] [    INFO][0m - loss: 0.3747447, learning_rate: 2.4285714285714288e-05, global_step: 720, interval_runtime: 8.0845, interval_samples_per_second: 1.979, interval_steps_per_second: 1.237, epoch: 3.8095[0m
[32m[2022-09-20 11:27:06,424] [    INFO][0m - loss: 0.53677235, learning_rate: 2.4206349206349206e-05, global_step: 730, interval_runtime: 8.0049, interval_samples_per_second: 1.999, interval_steps_per_second: 1.249, epoch: 3.8624[0m
[32m[2022-09-20 11:27:14,437] [    INFO][0m - loss: 0.62417288, learning_rate: 2.4126984126984128e-05, global_step: 740, interval_runtime: 8.0125, interval_samples_per_second: 1.997, interval_steps_per_second: 1.248, epoch: 3.9153[0m
[32m[2022-09-20 11:27:22,492] [    INFO][0m - loss: 0.52227206, learning_rate: 2.404761904761905e-05, global_step: 750, interval_runtime: 8.0551, interval_samples_per_second: 1.986, interval_steps_per_second: 1.241, epoch: 3.9683[0m
[32m[2022-09-20 11:27:27,110] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 11:27:27,110] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-20 11:27:27,110] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 11:27:27,110] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 11:27:27,110] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-20 11:27:55,037] [    INFO][0m - eval_loss: 2.6441140174865723, eval_accuracy: 0.4224326292789512, eval_runtime: 27.926, eval_samples_per_second: 49.166, eval_steps_per_second: 3.08, epoch: 4.0[0m
[32m[2022-09-20 11:27:55,063] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-756[0m
[32m[2022-09-20 11:27:55,063] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 11:27:58,250] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-756/tokenizer_config.json[0m
[32m[2022-09-20 11:27:58,251] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-756/special_tokens_map.json[0m
[32m[2022-09-20 11:28:07,368] [    INFO][0m - loss: 0.47543559, learning_rate: 2.396825396825397e-05, global_step: 760, interval_runtime: 44.8761, interval_samples_per_second: 0.357, interval_steps_per_second: 0.223, epoch: 4.0212[0m
[32m[2022-09-20 11:28:15,604] [    INFO][0m - loss: 0.29543231, learning_rate: 2.388888888888889e-05, global_step: 770, interval_runtime: 8.0146, interval_samples_per_second: 1.996, interval_steps_per_second: 1.248, epoch: 4.0741[0m
[32m[2022-09-20 11:28:23,622] [    INFO][0m - loss: 0.1770528, learning_rate: 2.380952380952381e-05, global_step: 780, interval_runtime: 8.2397, interval_samples_per_second: 1.942, interval_steps_per_second: 1.214, epoch: 4.127[0m
[32m[2022-09-20 11:28:31,590] [    INFO][0m - loss: 0.21583428, learning_rate: 2.373015873015873e-05, global_step: 790, interval_runtime: 7.9677, interval_samples_per_second: 2.008, interval_steps_per_second: 1.255, epoch: 4.1799[0m
[32m[2022-09-20 11:28:39,618] [    INFO][0m - loss: 0.15792612, learning_rate: 2.3650793650793653e-05, global_step: 800, interval_runtime: 8.0288, interval_samples_per_second: 1.993, interval_steps_per_second: 1.246, epoch: 4.2328[0m
[32m[2022-09-20 11:28:47,656] [    INFO][0m - loss: 0.18319867, learning_rate: 2.357142857142857e-05, global_step: 810, interval_runtime: 8.0374, interval_samples_per_second: 1.991, interval_steps_per_second: 1.244, epoch: 4.2857[0m
[32m[2022-09-20 11:28:55,665] [    INFO][0m - loss: 0.25267854, learning_rate: 2.3492063492063493e-05, global_step: 820, interval_runtime: 8.0092, interval_samples_per_second: 1.998, interval_steps_per_second: 1.249, epoch: 4.3386[0m
[32m[2022-09-20 11:29:03,651] [    INFO][0m - loss: 0.36529346, learning_rate: 2.3412698412698414e-05, global_step: 830, interval_runtime: 7.9855, interval_samples_per_second: 2.004, interval_steps_per_second: 1.252, epoch: 4.3915[0m
[32m[2022-09-20 11:29:11,631] [    INFO][0m - loss: 0.25636415, learning_rate: 2.3333333333333336e-05, global_step: 840, interval_runtime: 7.9804, interval_samples_per_second: 2.005, interval_steps_per_second: 1.253, epoch: 4.4444[0m
[32m[2022-09-20 11:29:19,651] [    INFO][0m - loss: 0.16261455, learning_rate: 2.3253968253968254e-05, global_step: 850, interval_runtime: 8.0198, interval_samples_per_second: 1.995, interval_steps_per_second: 1.247, epoch: 4.4974[0m
[32m[2022-09-20 11:29:27,665] [    INFO][0m - loss: 0.24780023, learning_rate: 2.3174603174603175e-05, global_step: 860, interval_runtime: 8.0142, interval_samples_per_second: 1.996, interval_steps_per_second: 1.248, epoch: 4.5503[0m
[32m[2022-09-20 11:29:35,676] [    INFO][0m - loss: 0.24411421, learning_rate: 2.3095238095238097e-05, global_step: 870, interval_runtime: 8.0113, interval_samples_per_second: 1.997, interval_steps_per_second: 1.248, epoch: 4.6032[0m
[32m[2022-09-20 11:29:43,637] [    INFO][0m - loss: 0.35746284, learning_rate: 2.301587301587302e-05, global_step: 880, interval_runtime: 7.9611, interval_samples_per_second: 2.01, interval_steps_per_second: 1.256, epoch: 4.6561[0m
[32m[2022-09-20 11:29:51,672] [    INFO][0m - loss: 0.34988084, learning_rate: 2.2936507936507937e-05, global_step: 890, interval_runtime: 8.0346, interval_samples_per_second: 1.991, interval_steps_per_second: 1.245, epoch: 4.709[0m
[32m[2022-09-20 11:29:59,659] [    INFO][0m - loss: 0.19934784, learning_rate: 2.2857142857142858e-05, global_step: 900, interval_runtime: 7.9869, interval_samples_per_second: 2.003, interval_steps_per_second: 1.252, epoch: 4.7619[0m
[32m[2022-09-20 11:30:07,715] [    INFO][0m - loss: 0.22999678, learning_rate: 2.277777777777778e-05, global_step: 910, interval_runtime: 8.0557, interval_samples_per_second: 1.986, interval_steps_per_second: 1.241, epoch: 4.8148[0m
[32m[2022-09-20 11:30:15,687] [    INFO][0m - loss: 0.2756484, learning_rate: 2.26984126984127e-05, global_step: 920, interval_runtime: 7.9722, interval_samples_per_second: 2.007, interval_steps_per_second: 1.254, epoch: 4.8677[0m
[32m[2022-09-20 11:30:23,720] [    INFO][0m - loss: 0.21021628, learning_rate: 2.261904761904762e-05, global_step: 930, interval_runtime: 8.0329, interval_samples_per_second: 1.992, interval_steps_per_second: 1.245, epoch: 4.9206[0m
[32m[2022-09-20 11:30:31,737] [    INFO][0m - loss: 0.26040168, learning_rate: 2.253968253968254e-05, global_step: 940, interval_runtime: 8.0172, interval_samples_per_second: 1.996, interval_steps_per_second: 1.247, epoch: 4.9735[0m
[32m[2022-09-20 11:30:35,536] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 11:30:35,536] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-20 11:30:35,536] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 11:30:35,536] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 11:30:35,536] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-20 11:31:03,293] [    INFO][0m - eval_loss: 3.3552041053771973, eval_accuracy: 0.44646758922068464, eval_runtime: 27.7563, eval_samples_per_second: 49.466, eval_steps_per_second: 3.098, epoch: 5.0[0m
[32m[2022-09-20 11:31:03,319] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-945[0m
[32m[2022-09-20 11:31:03,319] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 11:31:06,238] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-945/tokenizer_config.json[0m
[32m[2022-09-20 11:31:06,238] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-945/special_tokens_map.json[0m
[32m[2022-09-20 11:31:16,008] [    INFO][0m - loss: 0.18878332, learning_rate: 2.2460317460317462e-05, global_step: 950, interval_runtime: 44.2707, interval_samples_per_second: 0.361, interval_steps_per_second: 0.226, epoch: 5.0265[0m
[32m[2022-09-20 11:31:23,988] [    INFO][0m - loss: 0.13875525, learning_rate: 2.238095238095238e-05, global_step: 960, interval_runtime: 7.9805, interval_samples_per_second: 2.005, interval_steps_per_second: 1.253, epoch: 5.0794[0m
[32m[2022-09-20 11:31:31,959] [    INFO][0m - loss: 0.22517598, learning_rate: 2.2301587301587302e-05, global_step: 970, interval_runtime: 7.9704, interval_samples_per_second: 2.007, interval_steps_per_second: 1.255, epoch: 5.1323[0m
[32m[2022-09-20 11:31:39,907] [    INFO][0m - loss: 0.15991424, learning_rate: 2.222222222222222e-05, global_step: 980, interval_runtime: 7.9484, interval_samples_per_second: 2.013, interval_steps_per_second: 1.258, epoch: 5.1852[0m
[32m[2022-09-20 11:31:47,884] [    INFO][0m - loss: 0.08617178, learning_rate: 2.2142857142857145e-05, global_step: 990, interval_runtime: 7.9768, interval_samples_per_second: 2.006, interval_steps_per_second: 1.254, epoch: 5.2381[0m
[32m[2022-09-20 11:31:55,833] [    INFO][0m - loss: 0.15300753, learning_rate: 2.2063492063492063e-05, global_step: 1000, interval_runtime: 7.9496, interval_samples_per_second: 2.013, interval_steps_per_second: 1.258, epoch: 5.291[0m
[32m[2022-09-20 11:32:03,795] [    INFO][0m - loss: 0.09976354, learning_rate: 2.1984126984126984e-05, global_step: 1010, interval_runtime: 7.9612, interval_samples_per_second: 2.01, interval_steps_per_second: 1.256, epoch: 5.3439[0m
[32m[2022-09-20 11:32:11,802] [    INFO][0m - loss: 0.10102746, learning_rate: 2.1904761904761903e-05, global_step: 1020, interval_runtime: 8.0071, interval_samples_per_second: 1.998, interval_steps_per_second: 1.249, epoch: 5.3968[0m
[32m[2022-09-20 11:32:22,795] [    INFO][0m - loss: 0.08730534, learning_rate: 2.1825396825396827e-05, global_step: 1030, interval_runtime: 7.9812, interval_samples_per_second: 2.005, interval_steps_per_second: 1.253, epoch: 5.4497[0m
[32m[2022-09-20 11:32:30,761] [    INFO][0m - loss: 0.16740528, learning_rate: 2.1746031746031746e-05, global_step: 1040, interval_runtime: 10.9784, interval_samples_per_second: 1.457, interval_steps_per_second: 0.911, epoch: 5.5026[0m
[32m[2022-09-20 11:32:38,740] [    INFO][0m - loss: 0.17858746, learning_rate: 2.1666666666666667e-05, global_step: 1050, interval_runtime: 7.9788, interval_samples_per_second: 2.005, interval_steps_per_second: 1.253, epoch: 5.5556[0m
[32m[2022-09-20 11:32:46,725] [    INFO][0m - loss: 0.15683159, learning_rate: 2.1587301587301585e-05, global_step: 1060, interval_runtime: 7.9845, interval_samples_per_second: 2.004, interval_steps_per_second: 1.252, epoch: 5.6085[0m
[32m[2022-09-20 11:32:54,685] [    INFO][0m - loss: 0.19003808, learning_rate: 2.150793650793651e-05, global_step: 1070, interval_runtime: 7.9596, interval_samples_per_second: 2.01, interval_steps_per_second: 1.256, epoch: 5.6614[0m
[32m[2022-09-20 11:33:02,634] [    INFO][0m - loss: 0.13418896, learning_rate: 2.1428571428571428e-05, global_step: 1080, interval_runtime: 7.95, interval_samples_per_second: 2.013, interval_steps_per_second: 1.258, epoch: 5.7143[0m
[32m[2022-09-20 11:33:10,604] [    INFO][0m - loss: 0.13034011, learning_rate: 2.134920634920635e-05, global_step: 1090, interval_runtime: 7.9697, interval_samples_per_second: 2.008, interval_steps_per_second: 1.255, epoch: 5.7672[0m
[32m[2022-09-20 11:33:18,595] [    INFO][0m - loss: 0.14632885, learning_rate: 2.1269841269841268e-05, global_step: 1100, interval_runtime: 7.9905, interval_samples_per_second: 2.002, interval_steps_per_second: 1.251, epoch: 5.8201[0m
[32m[2022-09-20 11:33:26,617] [    INFO][0m - loss: 0.15489126, learning_rate: 2.1190476190476193e-05, global_step: 1110, interval_runtime: 8.0226, interval_samples_per_second: 1.994, interval_steps_per_second: 1.246, epoch: 5.873[0m
[32m[2022-09-20 11:33:34,598] [    INFO][0m - loss: 0.09541692, learning_rate: 2.111111111111111e-05, global_step: 1120, interval_runtime: 7.9803, interval_samples_per_second: 2.005, interval_steps_per_second: 1.253, epoch: 5.9259[0m
[32m[2022-09-20 11:33:42,532] [    INFO][0m - loss: 0.26426964, learning_rate: 2.1031746031746032e-05, global_step: 1130, interval_runtime: 7.9347, interval_samples_per_second: 2.016, interval_steps_per_second: 1.26, epoch: 5.9788[0m
[32m[2022-09-20 11:33:45,571] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 11:33:45,571] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-20 11:33:45,571] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 11:33:45,571] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 11:33:45,571] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-20 11:34:13,165] [    INFO][0m - eval_loss: 3.846270799636841, eval_accuracy: 0.4362709395484341, eval_runtime: 27.593, eval_samples_per_second: 49.759, eval_steps_per_second: 3.117, epoch: 6.0[0m
[32m[2022-09-20 11:34:13,189] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-1134[0m
[32m[2022-09-20 11:34:13,189] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 11:34:16,054] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-1134/tokenizer_config.json[0m
[32m[2022-09-20 11:34:16,054] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-1134/special_tokens_map.json[0m
[32m[2022-09-20 11:34:26,532] [    INFO][0m - loss: 0.07122974, learning_rate: 2.095238095238095e-05, global_step: 1140, interval_runtime: 43.9997, interval_samples_per_second: 0.364, interval_steps_per_second: 0.227, epoch: 6.0317[0m
[32m[2022-09-20 11:34:34,488] [    INFO][0m - loss: 0.03522133, learning_rate: 2.0873015873015875e-05, global_step: 1150, interval_runtime: 7.956, interval_samples_per_second: 2.011, interval_steps_per_second: 1.257, epoch: 6.0847[0m
[32m[2022-09-20 11:34:42,509] [    INFO][0m - loss: 0.02519478, learning_rate: 2.0793650793650793e-05, global_step: 1160, interval_runtime: 8.0207, interval_samples_per_second: 1.995, interval_steps_per_second: 1.247, epoch: 6.1376[0m
[32m[2022-09-20 11:34:50,492] [    INFO][0m - loss: 0.16457597, learning_rate: 2.0714285714285715e-05, global_step: 1170, interval_runtime: 7.9829, interval_samples_per_second: 2.004, interval_steps_per_second: 1.253, epoch: 6.1905[0m
[32m[2022-09-20 11:34:59,546] [    INFO][0m - loss: 0.10570968, learning_rate: 2.0634920634920633e-05, global_step: 1180, interval_runtime: 7.9695, interval_samples_per_second: 2.008, interval_steps_per_second: 1.255, epoch: 6.2434[0m
[32m[2022-09-20 11:35:07,501] [    INFO][0m - loss: 0.06570038, learning_rate: 2.0555555555555558e-05, global_step: 1190, interval_runtime: 9.0396, interval_samples_per_second: 1.77, interval_steps_per_second: 1.106, epoch: 6.2963[0m
[32m[2022-09-20 11:35:15,486] [    INFO][0m - loss: 0.04919134, learning_rate: 2.0476190476190476e-05, global_step: 1200, interval_runtime: 7.9857, interval_samples_per_second: 2.004, interval_steps_per_second: 1.252, epoch: 6.3492[0m
[32m[2022-09-20 11:35:23,455] [    INFO][0m - loss: 0.1510213, learning_rate: 2.0396825396825398e-05, global_step: 1210, interval_runtime: 7.9687, interval_samples_per_second: 2.008, interval_steps_per_second: 1.255, epoch: 6.4021[0m
[32m[2022-09-20 11:35:31,465] [    INFO][0m - loss: 0.08419442, learning_rate: 2.0317460317460316e-05, global_step: 1220, interval_runtime: 8.0101, interval_samples_per_second: 1.997, interval_steps_per_second: 1.248, epoch: 6.455[0m
[32m[2022-09-20 11:35:39,414] [    INFO][0m - loss: 0.14273655, learning_rate: 2.023809523809524e-05, global_step: 1230, interval_runtime: 7.9488, interval_samples_per_second: 2.013, interval_steps_per_second: 1.258, epoch: 6.5079[0m
[32m[2022-09-20 11:35:47,390] [    INFO][0m - loss: 0.13559408, learning_rate: 2.015873015873016e-05, global_step: 1240, interval_runtime: 7.9762, interval_samples_per_second: 2.006, interval_steps_per_second: 1.254, epoch: 6.5608[0m
[32m[2022-09-20 11:35:55,400] [    INFO][0m - loss: 0.11674139, learning_rate: 2.007936507936508e-05, global_step: 1250, interval_runtime: 8.0096, interval_samples_per_second: 1.998, interval_steps_per_second: 1.248, epoch: 6.6138[0m
[32m[2022-09-20 11:36:03,361] [    INFO][0m - loss: 0.02104424, learning_rate: 1.9999999999999998e-05, global_step: 1260, interval_runtime: 7.961, interval_samples_per_second: 2.01, interval_steps_per_second: 1.256, epoch: 6.6667[0m
[32m[2022-09-20 11:36:11,369] [    INFO][0m - loss: 0.06804748, learning_rate: 1.9920634920634923e-05, global_step: 1270, interval_runtime: 8.0078, interval_samples_per_second: 1.998, interval_steps_per_second: 1.249, epoch: 6.7196[0m
[32m[2022-09-20 11:36:19,404] [    INFO][0m - loss: 0.14092605, learning_rate: 1.984126984126984e-05, global_step: 1280, interval_runtime: 8.0353, interval_samples_per_second: 1.991, interval_steps_per_second: 1.245, epoch: 6.7725[0m
[32m[2022-09-20 11:36:29,610] [    INFO][0m - loss: 0.10403192, learning_rate: 1.9761904761904763e-05, global_step: 1290, interval_runtime: 7.9971, interval_samples_per_second: 2.001, interval_steps_per_second: 1.25, epoch: 6.8254[0m
[32m[2022-09-20 11:36:37,611] [    INFO][0m - loss: 0.19136947, learning_rate: 1.968253968253968e-05, global_step: 1300, interval_runtime: 10.21, interval_samples_per_second: 1.567, interval_steps_per_second: 0.979, epoch: 6.8783[0m
[32m[2022-09-20 11:36:45,580] [    INFO][0m - loss: 0.04583087, learning_rate: 1.9603174603174606e-05, global_step: 1310, interval_runtime: 7.9686, interval_samples_per_second: 2.008, interval_steps_per_second: 1.255, epoch: 6.9312[0m
[32m[2022-09-20 11:36:53,493] [    INFO][0m - loss: 0.06011311, learning_rate: 1.9523809523809524e-05, global_step: 1320, interval_runtime: 7.9129, interval_samples_per_second: 2.022, interval_steps_per_second: 1.264, epoch: 6.9841[0m
[32m[2022-09-20 11:36:55,773] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 11:36:55,773] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-20 11:36:55,773] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 11:36:55,773] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 11:36:55,773] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-20 11:37:23,331] [    INFO][0m - eval_loss: 4.249548435211182, eval_accuracy: 0.437727603787327, eval_runtime: 27.5578, eval_samples_per_second: 49.823, eval_steps_per_second: 3.121, epoch: 7.0[0m
[32m[2022-09-20 11:37:23,356] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-1323[0m
[32m[2022-09-20 11:37:23,356] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 11:37:26,207] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-1323/tokenizer_config.json[0m
[32m[2022-09-20 11:37:26,208] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-1323/special_tokens_map.json[0m
[32m[2022-09-20 11:37:37,395] [    INFO][0m - loss: 0.04791054, learning_rate: 1.9444444444444445e-05, global_step: 1330, interval_runtime: 43.9025, interval_samples_per_second: 0.364, interval_steps_per_second: 0.228, epoch: 7.037[0m
[32m[2022-09-20 11:37:45,331] [    INFO][0m - loss: 0.01806491, learning_rate: 1.9365079365079363e-05, global_step: 1340, interval_runtime: 7.9359, interval_samples_per_second: 2.016, interval_steps_per_second: 1.26, epoch: 7.0899[0m
[32m[2022-09-20 11:37:53,323] [    INFO][0m - loss: 0.02861938, learning_rate: 1.928571428571429e-05, global_step: 1350, interval_runtime: 7.9916, interval_samples_per_second: 2.002, interval_steps_per_second: 1.251, epoch: 7.1429[0m
[32m[2022-09-20 11:38:01,280] [    INFO][0m - loss: 0.07809936, learning_rate: 1.9206349206349206e-05, global_step: 1360, interval_runtime: 7.9575, interval_samples_per_second: 2.011, interval_steps_per_second: 1.257, epoch: 7.1958[0m
[32m[2022-09-20 11:38:09,240] [    INFO][0m - loss: 0.0038019, learning_rate: 1.9126984126984128e-05, global_step: 1370, interval_runtime: 7.9603, interval_samples_per_second: 2.01, interval_steps_per_second: 1.256, epoch: 7.2487[0m
[32m[2022-09-20 11:38:17,233] [    INFO][0m - loss: 0.04450712, learning_rate: 1.9047619047619046e-05, global_step: 1380, interval_runtime: 7.9928, interval_samples_per_second: 2.002, interval_steps_per_second: 1.251, epoch: 7.3016[0m
[32m[2022-09-20 11:38:25,246] [    INFO][0m - loss: 0.18543839, learning_rate: 1.896825396825397e-05, global_step: 1390, interval_runtime: 8.0126, interval_samples_per_second: 1.997, interval_steps_per_second: 1.248, epoch: 7.3545[0m
[32m[2022-09-20 11:38:33,198] [    INFO][0m - loss: 0.00894265, learning_rate: 1.888888888888889e-05, global_step: 1400, interval_runtime: 7.9525, interval_samples_per_second: 2.012, interval_steps_per_second: 1.257, epoch: 7.4074[0m
[32m[2022-09-20 11:38:41,186] [    INFO][0m - loss: 0.01258653, learning_rate: 1.880952380952381e-05, global_step: 1410, interval_runtime: 7.9877, interval_samples_per_second: 2.003, interval_steps_per_second: 1.252, epoch: 7.4603[0m
[32m[2022-09-20 11:38:49,190] [    INFO][0m - loss: 0.06112882, learning_rate: 1.873015873015873e-05, global_step: 1420, interval_runtime: 8.0043, interval_samples_per_second: 1.999, interval_steps_per_second: 1.249, epoch: 7.5132[0m
[32m[2022-09-20 11:38:57,218] [    INFO][0m - loss: 0.03657035, learning_rate: 1.8650793650793654e-05, global_step: 1430, interval_runtime: 8.0275, interval_samples_per_second: 1.993, interval_steps_per_second: 1.246, epoch: 7.5661[0m
[32m[2022-09-20 11:39:05,168] [    INFO][0m - loss: 0.07781481, learning_rate: 1.8571428571428572e-05, global_step: 1440, interval_runtime: 7.9504, interval_samples_per_second: 2.012, interval_steps_per_second: 1.258, epoch: 7.619[0m
[32m[2022-09-20 11:39:13,174] [    INFO][0m - loss: 0.03933013, learning_rate: 1.8492063492063493e-05, global_step: 1450, interval_runtime: 8.0056, interval_samples_per_second: 1.999, interval_steps_per_second: 1.249, epoch: 7.672[0m
[32m[2022-09-20 11:39:21,151] [    INFO][0m - loss: 0.02979536, learning_rate: 1.841269841269841e-05, global_step: 1460, interval_runtime: 7.9765, interval_samples_per_second: 2.006, interval_steps_per_second: 1.254, epoch: 7.7249[0m
[32m[2022-09-20 11:39:29,136] [    INFO][0m - loss: 0.12530619, learning_rate: 1.8333333333333336e-05, global_step: 1470, interval_runtime: 7.9858, interval_samples_per_second: 2.004, interval_steps_per_second: 1.252, epoch: 7.7778[0m
[32m[2022-09-20 11:39:37,141] [    INFO][0m - loss: 0.16864648, learning_rate: 1.8253968253968254e-05, global_step: 1480, interval_runtime: 8.0047, interval_samples_per_second: 1.999, interval_steps_per_second: 1.249, epoch: 7.8307[0m
[32m[2022-09-20 11:39:45,163] [    INFO][0m - loss: 0.12648953, learning_rate: 1.8174603174603176e-05, global_step: 1490, interval_runtime: 8.0224, interval_samples_per_second: 1.994, interval_steps_per_second: 1.247, epoch: 7.8836[0m
[32m[2022-09-20 11:39:53,122] [    INFO][0m - loss: 0.09891095, learning_rate: 1.8095238095238094e-05, global_step: 1500, interval_runtime: 7.9593, interval_samples_per_second: 2.01, interval_steps_per_second: 1.256, epoch: 7.9365[0m
[32m[2022-09-20 11:40:00,994] [    INFO][0m - loss: 0.13943168, learning_rate: 1.801587301587302e-05, global_step: 1510, interval_runtime: 7.8718, interval_samples_per_second: 2.033, interval_steps_per_second: 1.27, epoch: 7.9894[0m
[32m[2022-09-20 11:40:02,515] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 11:40:02,515] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-20 11:40:02,515] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 11:40:02,515] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 11:40:02,515] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-20 11:40:30,227] [    INFO][0m - eval_loss: 4.516872406005859, eval_accuracy: 0.4355426074289876, eval_runtime: 27.711, eval_samples_per_second: 49.547, eval_steps_per_second: 3.103, epoch: 8.0[0m
[32m[2022-09-20 11:40:30,250] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-1512[0m
[32m[2022-09-20 11:40:30,251] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 11:40:33,067] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-1512/tokenizer_config.json[0m
[32m[2022-09-20 11:40:33,067] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-1512/special_tokens_map.json[0m
[32m[2022-09-20 11:40:47,667] [    INFO][0m - loss: 0.01235263, learning_rate: 1.7936507936507937e-05, global_step: 1520, interval_runtime: 46.6728, interval_samples_per_second: 0.343, interval_steps_per_second: 0.214, epoch: 8.0423[0m
[32m[2022-09-20 11:40:55,641] [    INFO][0m - loss: 0.00961946, learning_rate: 1.785714285714286e-05, global_step: 1530, interval_runtime: 7.9738, interval_samples_per_second: 2.007, interval_steps_per_second: 1.254, epoch: 8.0952[0m
[32m[2022-09-20 11:41:03,606] [    INFO][0m - loss: 0.01315953, learning_rate: 1.7777777777777777e-05, global_step: 1540, interval_runtime: 7.965, interval_samples_per_second: 2.009, interval_steps_per_second: 1.255, epoch: 8.1481[0m
[32m[2022-09-20 11:41:11,612] [    INFO][0m - loss: 0.01570393, learning_rate: 1.76984126984127e-05, global_step: 1550, interval_runtime: 8.0053, interval_samples_per_second: 1.999, interval_steps_per_second: 1.249, epoch: 8.2011[0m
[32m[2022-09-20 11:41:19,576] [    INFO][0m - loss: 0.02148719, learning_rate: 1.761904761904762e-05, global_step: 1560, interval_runtime: 7.9642, interval_samples_per_second: 2.009, interval_steps_per_second: 1.256, epoch: 8.254[0m
[32m[2022-09-20 11:41:27,543] [    INFO][0m - loss: 0.04632443, learning_rate: 1.753968253968254e-05, global_step: 1570, interval_runtime: 7.9676, interval_samples_per_second: 2.008, interval_steps_per_second: 1.255, epoch: 8.3069[0m
[32m[2022-09-20 11:41:35,563] [    INFO][0m - loss: 0.13834943, learning_rate: 1.746031746031746e-05, global_step: 1580, interval_runtime: 8.0203, interval_samples_per_second: 1.995, interval_steps_per_second: 1.247, epoch: 8.3598[0m
[32m[2022-09-20 11:41:43,579] [    INFO][0m - loss: 0.02465202, learning_rate: 1.7380952380952384e-05, global_step: 1590, interval_runtime: 8.0154, interval_samples_per_second: 1.996, interval_steps_per_second: 1.248, epoch: 8.4127[0m
[32m[2022-09-20 11:41:51,530] [    INFO][0m - loss: 0.01516675, learning_rate: 1.7301587301587302e-05, global_step: 1600, interval_runtime: 7.9508, interval_samples_per_second: 2.012, interval_steps_per_second: 1.258, epoch: 8.4656[0m
[32m[2022-09-20 11:41:59,504] [    INFO][0m - loss: 0.09790776, learning_rate: 1.7222222222222224e-05, global_step: 1610, interval_runtime: 7.9746, interval_samples_per_second: 2.006, interval_steps_per_second: 1.254, epoch: 8.5185[0m
[32m[2022-09-20 11:42:07,484] [    INFO][0m - loss: 0.12003536, learning_rate: 1.7142857142857142e-05, global_step: 1620, interval_runtime: 7.9795, interval_samples_per_second: 2.005, interval_steps_per_second: 1.253, epoch: 8.5714[0m
[32m[2022-09-20 11:42:15,478] [    INFO][0m - loss: 0.01231205, learning_rate: 1.7063492063492067e-05, global_step: 1630, interval_runtime: 7.9938, interval_samples_per_second: 2.002, interval_steps_per_second: 1.251, epoch: 8.6243[0m
[32m[2022-09-20 11:42:23,463] [    INFO][0m - loss: 0.11817112, learning_rate: 1.6984126984126985e-05, global_step: 1640, interval_runtime: 7.9852, interval_samples_per_second: 2.004, interval_steps_per_second: 1.252, epoch: 8.6772[0m
[32m[2022-09-20 11:42:31,462] [    INFO][0m - loss: 0.12414922, learning_rate: 1.6904761904761906e-05, global_step: 1650, interval_runtime: 7.9989, interval_samples_per_second: 2.0, interval_steps_per_second: 1.25, epoch: 8.7302[0m
[32m[2022-09-20 11:42:39,448] [    INFO][0m - loss: 0.02138354, learning_rate: 1.6825396825396824e-05, global_step: 1660, interval_runtime: 7.986, interval_samples_per_second: 2.003, interval_steps_per_second: 1.252, epoch: 8.7831[0m
[32m[2022-09-20 11:42:47,448] [    INFO][0m - loss: 0.01126763, learning_rate: 1.674603174603175e-05, global_step: 1670, interval_runtime: 8.0005, interval_samples_per_second: 2.0, interval_steps_per_second: 1.25, epoch: 8.836[0m
[32m[2022-09-20 11:42:55,413] [    INFO][0m - loss: 0.09081162, learning_rate: 1.6666666666666667e-05, global_step: 1680, interval_runtime: 7.9646, interval_samples_per_second: 2.009, interval_steps_per_second: 1.256, epoch: 8.8889[0m
[32m[2022-09-20 11:43:03,423] [    INFO][0m - loss: 0.07717208, learning_rate: 1.658730158730159e-05, global_step: 1690, interval_runtime: 8.0097, interval_samples_per_second: 1.998, interval_steps_per_second: 1.248, epoch: 8.9418[0m
[32m[2022-09-20 11:43:11,280] [    INFO][0m - loss: 0.0382028, learning_rate: 1.6507936507936507e-05, global_step: 1700, interval_runtime: 7.8573, interval_samples_per_second: 2.036, interval_steps_per_second: 1.273, epoch: 8.9947[0m
[32m[2022-09-20 11:43:12,040] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 11:43:12,040] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-20 11:43:12,040] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 11:43:12,041] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 11:43:12,041] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-20 11:43:39,714] [    INFO][0m - eval_loss: 4.580006122589111, eval_accuracy: 0.44136926438455937, eval_runtime: 27.673, eval_samples_per_second: 49.615, eval_steps_per_second: 3.108, epoch: 9.0[0m
[32m[2022-09-20 11:43:39,738] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-1701[0m
[32m[2022-09-20 11:43:39,738] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 11:43:42,647] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-1701/tokenizer_config.json[0m
[32m[2022-09-20 11:43:42,647] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-1701/special_tokens_map.json[0m
[32m[2022-09-20 11:43:55,328] [    INFO][0m - loss: 0.02247512, learning_rate: 1.6428571428571432e-05, global_step: 1710, interval_runtime: 44.0481, interval_samples_per_second: 0.363, interval_steps_per_second: 0.227, epoch: 9.0476[0m
[32m[2022-09-20 11:44:03,383] [    INFO][0m - loss: 0.02641031, learning_rate: 1.634920634920635e-05, global_step: 1720, interval_runtime: 8.0543, interval_samples_per_second: 1.987, interval_steps_per_second: 1.242, epoch: 9.1005[0m
[32m[2022-09-20 11:44:11,339] [    INFO][0m - loss: 0.0031155, learning_rate: 1.626984126984127e-05, global_step: 1730, interval_runtime: 7.9564, interval_samples_per_second: 2.011, interval_steps_per_second: 1.257, epoch: 9.1534[0m
[32m[2022-09-20 11:44:19,321] [    INFO][0m - loss: 0.02299751, learning_rate: 1.619047619047619e-05, global_step: 1740, interval_runtime: 7.9826, interval_samples_per_second: 2.004, interval_steps_per_second: 1.253, epoch: 9.2063[0m
[32m[2022-09-20 11:44:27,327] [    INFO][0m - loss: 0.00860204, learning_rate: 1.6111111111111115e-05, global_step: 1750, interval_runtime: 8.0053, interval_samples_per_second: 1.999, interval_steps_per_second: 1.249, epoch: 9.2593[0m
[32m[2022-09-20 11:44:35,323] [    INFO][0m - loss: 0.0454558, learning_rate: 1.6031746031746033e-05, global_step: 1760, interval_runtime: 7.996, interval_samples_per_second: 2.001, interval_steps_per_second: 1.251, epoch: 9.3122[0m
[32m[2022-09-20 11:44:43,332] [    INFO][0m - loss: 0.03018194, learning_rate: 1.5952380952380954e-05, global_step: 1770, interval_runtime: 8.0089, interval_samples_per_second: 1.998, interval_steps_per_second: 1.249, epoch: 9.3651[0m
[32m[2022-09-20 11:44:51,342] [    INFO][0m - loss: 0.06162459, learning_rate: 1.5873015873015872e-05, global_step: 1780, interval_runtime: 8.0103, interval_samples_per_second: 1.997, interval_steps_per_second: 1.248, epoch: 9.418[0m
[32m[2022-09-20 11:44:59,371] [    INFO][0m - loss: 0.02410728, learning_rate: 1.5793650793650797e-05, global_step: 1790, interval_runtime: 8.029, interval_samples_per_second: 1.993, interval_steps_per_second: 1.245, epoch: 9.4709[0m
[32m[2022-09-20 11:45:08,083] [    INFO][0m - loss: 0.00217708, learning_rate: 1.5714285714285715e-05, global_step: 1800, interval_runtime: 8.0099, interval_samples_per_second: 1.998, interval_steps_per_second: 1.248, epoch: 9.5238[0m
[32m[2022-09-20 11:45:16,047] [    INFO][0m - loss: 0.00272365, learning_rate: 1.5634920634920637e-05, global_step: 1810, interval_runtime: 8.6659, interval_samples_per_second: 1.846, interval_steps_per_second: 1.154, epoch: 9.5767[0m
[32m[2022-09-20 11:45:24,011] [    INFO][0m - loss: 0.00158919, learning_rate: 1.5555555555555555e-05, global_step: 1820, interval_runtime: 7.9645, interval_samples_per_second: 2.009, interval_steps_per_second: 1.256, epoch: 9.6296[0m
[32m[2022-09-20 11:45:31,990] [    INFO][0m - loss: 0.00517962, learning_rate: 1.547619047619048e-05, global_step: 1830, interval_runtime: 7.9786, interval_samples_per_second: 2.005, interval_steps_per_second: 1.253, epoch: 9.6825[0m
[32m[2022-09-20 11:45:39,974] [    INFO][0m - loss: 0.02704547, learning_rate: 1.5396825396825398e-05, global_step: 1840, interval_runtime: 7.9843, interval_samples_per_second: 2.004, interval_steps_per_second: 1.252, epoch: 9.7354[0m
[32m[2022-09-20 11:45:47,959] [    INFO][0m - loss: 0.18369489, learning_rate: 1.531746031746032e-05, global_step: 1850, interval_runtime: 7.985, interval_samples_per_second: 2.004, interval_steps_per_second: 1.252, epoch: 9.7884[0m
[32m[2022-09-20 11:45:56,000] [    INFO][0m - loss: 0.00451619, learning_rate: 1.5238095238095238e-05, global_step: 1860, interval_runtime: 8.0411, interval_samples_per_second: 1.99, interval_steps_per_second: 1.244, epoch: 9.8413[0m
[32m[2022-09-20 11:46:03,994] [    INFO][0m - loss: 0.0928602, learning_rate: 1.515873015873016e-05, global_step: 1870, interval_runtime: 7.9935, interval_samples_per_second: 2.002, interval_steps_per_second: 1.251, epoch: 9.8942[0m
[32m[2022-09-20 11:46:12,000] [    INFO][0m - loss: 0.06920766, learning_rate: 1.507936507936508e-05, global_step: 1880, interval_runtime: 8.006, interval_samples_per_second: 1.999, interval_steps_per_second: 1.249, epoch: 9.9471[0m
[32m[2022-09-20 11:46:19,785] [    INFO][0m - loss: 0.01164948, learning_rate: 1.5e-05, global_step: 1890, interval_runtime: 7.785, interval_samples_per_second: 2.055, interval_steps_per_second: 1.285, epoch: 10.0[0m
[32m[2022-09-20 11:46:19,785] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 11:46:19,786] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-20 11:46:19,786] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 11:46:19,786] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 11:46:19,786] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-20 11:46:47,388] [    INFO][0m - eval_loss: 4.7439422607421875, eval_accuracy: 0.4479242534595776, eval_runtime: 27.6017, eval_samples_per_second: 49.743, eval_steps_per_second: 3.116, epoch: 10.0[0m
[32m[2022-09-20 11:46:47,415] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-1890[0m
[32m[2022-09-20 11:46:47,415] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 11:46:50,201] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-1890/tokenizer_config.json[0m
[32m[2022-09-20 11:46:50,201] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-1890/special_tokens_map.json[0m
[32m[2022-09-20 11:47:03,530] [    INFO][0m - loss: 0.05044215, learning_rate: 1.492063492063492e-05, global_step: 1900, interval_runtime: 43.7449, interval_samples_per_second: 0.366, interval_steps_per_second: 0.229, epoch: 10.0529[0m
[32m[2022-09-20 11:47:11,521] [    INFO][0m - loss: 0.00076417, learning_rate: 1.4841269841269842e-05, global_step: 1910, interval_runtime: 7.9912, interval_samples_per_second: 2.002, interval_steps_per_second: 1.251, epoch: 10.1058[0m
[32m[2022-09-20 11:47:19,487] [    INFO][0m - loss: 0.00104659, learning_rate: 1.4761904761904761e-05, global_step: 1920, interval_runtime: 7.9658, interval_samples_per_second: 2.009, interval_steps_per_second: 1.255, epoch: 10.1587[0m
[32m[2022-09-20 11:47:27,477] [    INFO][0m - loss: 0.00092096, learning_rate: 1.4682539682539683e-05, global_step: 1930, interval_runtime: 7.9908, interval_samples_per_second: 2.002, interval_steps_per_second: 1.251, epoch: 10.2116[0m
[32m[2022-09-20 11:47:35,471] [    INFO][0m - loss: 0.07806407, learning_rate: 1.4603174603174603e-05, global_step: 1940, interval_runtime: 7.9935, interval_samples_per_second: 2.002, interval_steps_per_second: 1.251, epoch: 10.2646[0m
[32m[2022-09-20 11:47:43,486] [    INFO][0m - loss: 0.01003411, learning_rate: 1.4523809523809524e-05, global_step: 1950, interval_runtime: 8.0145, interval_samples_per_second: 1.996, interval_steps_per_second: 1.248, epoch: 10.3175[0m
[32m[2022-09-20 11:47:51,455] [    INFO][0m - loss: 0.04254351, learning_rate: 1.4444444444444444e-05, global_step: 1960, interval_runtime: 7.9696, interval_samples_per_second: 2.008, interval_steps_per_second: 1.255, epoch: 10.3704[0m
[32m[2022-09-20 11:47:59,494] [    INFO][0m - loss: 0.06081005, learning_rate: 1.4365079365079366e-05, global_step: 1970, interval_runtime: 8.0386, interval_samples_per_second: 1.99, interval_steps_per_second: 1.244, epoch: 10.4233[0m
[32m[2022-09-20 11:48:07,486] [    INFO][0m - loss: 0.00165422, learning_rate: 1.4285714285714285e-05, global_step: 1980, interval_runtime: 7.9923, interval_samples_per_second: 2.002, interval_steps_per_second: 1.251, epoch: 10.4762[0m
[32m[2022-09-20 11:48:15,484] [    INFO][0m - loss: 0.00037908, learning_rate: 1.4206349206349207e-05, global_step: 1990, interval_runtime: 7.9978, interval_samples_per_second: 2.001, interval_steps_per_second: 1.25, epoch: 10.5291[0m
[32m[2022-09-20 11:48:23,514] [    INFO][0m - loss: 0.04362014, learning_rate: 1.4126984126984127e-05, global_step: 2000, interval_runtime: 8.0304, interval_samples_per_second: 1.992, interval_steps_per_second: 1.245, epoch: 10.582[0m
[32m[2022-09-20 11:48:31,545] [    INFO][0m - loss: 0.02435831, learning_rate: 1.4047619047619048e-05, global_step: 2010, interval_runtime: 8.0304, interval_samples_per_second: 1.992, interval_steps_per_second: 1.245, epoch: 10.6349[0m
[32m[2022-09-20 11:48:39,557] [    INFO][0m - loss: 0.00142811, learning_rate: 1.3968253968253968e-05, global_step: 2020, interval_runtime: 8.0127, interval_samples_per_second: 1.997, interval_steps_per_second: 1.248, epoch: 10.6878[0m
[32m[2022-09-20 11:48:47,546] [    INFO][0m - loss: 0.0706957, learning_rate: 1.388888888888889e-05, global_step: 2030, interval_runtime: 7.989, interval_samples_per_second: 2.003, interval_steps_per_second: 1.252, epoch: 10.7407[0m
[32m[2022-09-20 11:48:55,560] [    INFO][0m - loss: 0.04106153, learning_rate: 1.380952380952381e-05, global_step: 2040, interval_runtime: 8.0139, interval_samples_per_second: 1.997, interval_steps_per_second: 1.248, epoch: 10.7937[0m
[32m[2022-09-20 11:49:03,580] [    INFO][0m - loss: 0.0732908, learning_rate: 1.3730158730158731e-05, global_step: 2050, interval_runtime: 8.0192, interval_samples_per_second: 1.995, interval_steps_per_second: 1.247, epoch: 10.8466[0m
[32m[2022-09-20 11:49:11,573] [    INFO][0m - loss: 0.00097842, learning_rate: 1.365079365079365e-05, global_step: 2060, interval_runtime: 7.9929, interval_samples_per_second: 2.002, interval_steps_per_second: 1.251, epoch: 10.8995[0m
[32m[2022-09-20 11:49:19,605] [    INFO][0m - loss: 0.00935397, learning_rate: 1.3571428571428572e-05, global_step: 2070, interval_runtime: 8.0327, interval_samples_per_second: 1.992, interval_steps_per_second: 1.245, epoch: 10.9524[0m
[32m[2022-09-20 11:49:26,598] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 11:49:26,598] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-20 11:49:26,598] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 11:49:26,598] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 11:49:26,598] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-20 11:49:54,128] [    INFO][0m - eval_loss: 4.8538336753845215, eval_accuracy: 0.44282592862345227, eval_runtime: 27.5298, eval_samples_per_second: 49.873, eval_steps_per_second: 3.124, epoch: 11.0[0m
[32m[2022-09-20 11:49:54,154] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-2079[0m
[32m[2022-09-20 11:49:54,154] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 11:49:56,863] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-2079/tokenizer_config.json[0m
[32m[2022-09-20 11:49:56,863] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-2079/special_tokens_map.json[0m
[32m[2022-09-20 11:50:02,969] [    INFO][0m - loss: 0.00071266, learning_rate: 1.3492063492063492e-05, global_step: 2080, interval_runtime: 43.364, interval_samples_per_second: 0.369, interval_steps_per_second: 0.231, epoch: 11.0053[0m
[32m[2022-09-20 11:50:10,903] [    INFO][0m - loss: 0.00111428, learning_rate: 1.3412698412698413e-05, global_step: 2090, interval_runtime: 7.9336, interval_samples_per_second: 2.017, interval_steps_per_second: 1.26, epoch: 11.0582[0m
[32m[2022-09-20 11:50:18,898] [    INFO][0m - loss: 0.00045482, learning_rate: 1.3333333333333333e-05, global_step: 2100, interval_runtime: 7.9956, interval_samples_per_second: 2.001, interval_steps_per_second: 1.251, epoch: 11.1111[0m
[32m[2022-09-20 11:50:26,914] [    INFO][0m - loss: 0.00074651, learning_rate: 1.3253968253968255e-05, global_step: 2110, interval_runtime: 8.0158, interval_samples_per_second: 1.996, interval_steps_per_second: 1.248, epoch: 11.164[0m
[32m[2022-09-20 11:50:34,890] [    INFO][0m - loss: 0.00036482, learning_rate: 1.3174603174603175e-05, global_step: 2120, interval_runtime: 7.9763, interval_samples_per_second: 2.006, interval_steps_per_second: 1.254, epoch: 11.2169[0m
[32m[2022-09-20 11:50:42,891] [    INFO][0m - loss: 0.0039567, learning_rate: 1.3095238095238096e-05, global_step: 2130, interval_runtime: 8.0003, interval_samples_per_second: 2.0, interval_steps_per_second: 1.25, epoch: 11.2698[0m
[32m[2022-09-20 11:50:50,872] [    INFO][0m - loss: 0.00573362, learning_rate: 1.3015873015873016e-05, global_step: 2140, interval_runtime: 7.9814, interval_samples_per_second: 2.005, interval_steps_per_second: 1.253, epoch: 11.3228[0m
[32m[2022-09-20 11:50:58,886] [    INFO][0m - loss: 0.00148839, learning_rate: 1.2936507936507937e-05, global_step: 2150, interval_runtime: 8.014, interval_samples_per_second: 1.997, interval_steps_per_second: 1.248, epoch: 11.3757[0m
[32m[2022-09-20 11:51:06,877] [    INFO][0m - loss: 0.03052557, learning_rate: 1.2857142857142857e-05, global_step: 2160, interval_runtime: 7.9903, interval_samples_per_second: 2.002, interval_steps_per_second: 1.252, epoch: 11.4286[0m
[32m[2022-09-20 11:51:15,495] [    INFO][0m - loss: 0.02279742, learning_rate: 1.2777777777777779e-05, global_step: 2170, interval_runtime: 8.023, interval_samples_per_second: 1.994, interval_steps_per_second: 1.246, epoch: 11.4815[0m
[32m[2022-09-20 11:51:23,499] [    INFO][0m - loss: 0.00096494, learning_rate: 1.2698412698412699e-05, global_step: 2180, interval_runtime: 8.5992, interval_samples_per_second: 1.861, interval_steps_per_second: 1.163, epoch: 11.5344[0m
[32m[2022-09-20 11:51:31,508] [    INFO][0m - loss: 0.05650329, learning_rate: 1.261904761904762e-05, global_step: 2190, interval_runtime: 8.0091, interval_samples_per_second: 1.998, interval_steps_per_second: 1.249, epoch: 11.5873[0m
[32m[2022-09-20 11:51:39,517] [    INFO][0m - loss: 0.0004171, learning_rate: 1.253968253968254e-05, global_step: 2200, interval_runtime: 8.0094, interval_samples_per_second: 1.998, interval_steps_per_second: 1.249, epoch: 11.6402[0m
[32m[2022-09-20 11:51:47,526] [    INFO][0m - loss: 0.00480407, learning_rate: 1.2460317460317461e-05, global_step: 2210, interval_runtime: 8.0087, interval_samples_per_second: 1.998, interval_steps_per_second: 1.249, epoch: 11.6931[0m
[32m[2022-09-20 11:51:55,527] [    INFO][0m - loss: 0.07143345, learning_rate: 1.2380952380952381e-05, global_step: 2220, interval_runtime: 8.0007, interval_samples_per_second: 2.0, interval_steps_per_second: 1.25, epoch: 11.746[0m
[32m[2022-09-20 11:52:03,579] [    INFO][0m - loss: 0.00179528, learning_rate: 1.2301587301587303e-05, global_step: 2230, interval_runtime: 8.0522, interval_samples_per_second: 1.987, interval_steps_per_second: 1.242, epoch: 11.7989[0m
[32m[2022-09-20 11:52:11,577] [    INFO][0m - loss: 0.05251981, learning_rate: 1.2222222222222222e-05, global_step: 2240, interval_runtime: 7.9983, interval_samples_per_second: 2.0, interval_steps_per_second: 1.25, epoch: 11.8519[0m
[32m[2022-09-20 11:52:19,556] [    INFO][0m - loss: 0.00093652, learning_rate: 1.2142857142857144e-05, global_step: 2250, interval_runtime: 7.979, interval_samples_per_second: 2.005, interval_steps_per_second: 1.253, epoch: 11.9048[0m
[32m[2022-09-20 11:52:27,575] [    INFO][0m - loss: 0.06029897, learning_rate: 1.2063492063492064e-05, global_step: 2260, interval_runtime: 8.0188, interval_samples_per_second: 1.995, interval_steps_per_second: 1.247, epoch: 11.9577[0m
[32m[2022-09-20 11:52:33,803] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 11:52:33,804] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-20 11:52:33,804] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 11:52:33,804] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 11:52:33,804] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-20 11:53:01,406] [    INFO][0m - eval_loss: 4.882024765014648, eval_accuracy: 0.4479242534595776, eval_runtime: 27.6021, eval_samples_per_second: 49.743, eval_steps_per_second: 3.116, epoch: 12.0[0m
[32m[2022-09-20 11:53:01,430] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-2268[0m
[32m[2022-09-20 11:53:01,430] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 11:53:04,141] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-2268/tokenizer_config.json[0m
[32m[2022-09-20 11:53:04,142] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-2268/special_tokens_map.json[0m
[32m[2022-09-20 11:53:11,142] [    INFO][0m - loss: 0.09084142, learning_rate: 1.1984126984126985e-05, global_step: 2270, interval_runtime: 43.567, interval_samples_per_second: 0.367, interval_steps_per_second: 0.23, epoch: 12.0106[0m
[32m[2022-09-20 11:53:19,112] [    INFO][0m - loss: 0.00619166, learning_rate: 1.1904761904761905e-05, global_step: 2280, interval_runtime: 7.9702, interval_samples_per_second: 2.007, interval_steps_per_second: 1.255, epoch: 12.0635[0m
[32m[2022-09-20 11:53:27,086] [    INFO][0m - loss: 0.0169091, learning_rate: 1.1825396825396827e-05, global_step: 2290, interval_runtime: 7.974, interval_samples_per_second: 2.007, interval_steps_per_second: 1.254, epoch: 12.1164[0m
[32m[2022-09-20 11:53:35,076] [    INFO][0m - loss: 0.01234032, learning_rate: 1.1746031746031746e-05, global_step: 2300, interval_runtime: 7.9902, interval_samples_per_second: 2.002, interval_steps_per_second: 1.252, epoch: 12.1693[0m
[32m[2022-09-20 11:53:43,063] [    INFO][0m - loss: 0.03247536, learning_rate: 1.1666666666666668e-05, global_step: 2310, interval_runtime: 7.9867, interval_samples_per_second: 2.003, interval_steps_per_second: 1.252, epoch: 12.2222[0m
[32m[2022-09-20 11:53:51,083] [    INFO][0m - loss: 0.00086623, learning_rate: 1.1587301587301588e-05, global_step: 2320, interval_runtime: 8.0199, interval_samples_per_second: 1.995, interval_steps_per_second: 1.247, epoch: 12.2751[0m
[32m[2022-09-20 11:53:59,045] [    INFO][0m - loss: 0.03444962, learning_rate: 1.150793650793651e-05, global_step: 2330, interval_runtime: 7.9622, interval_samples_per_second: 2.009, interval_steps_per_second: 1.256, epoch: 12.328[0m
[32m[2022-09-20 11:54:07,030] [    INFO][0m - loss: 0.00040304, learning_rate: 1.1428571428571429e-05, global_step: 2340, interval_runtime: 7.9849, interval_samples_per_second: 2.004, interval_steps_per_second: 1.252, epoch: 12.381[0m
[32m[2022-09-20 11:54:14,992] [    INFO][0m - loss: 0.01453864, learning_rate: 1.134920634920635e-05, global_step: 2350, interval_runtime: 7.9617, interval_samples_per_second: 2.01, interval_steps_per_second: 1.256, epoch: 12.4339[0m
[32m[2022-09-20 11:54:22,953] [    INFO][0m - loss: 0.00231159, learning_rate: 1.126984126984127e-05, global_step: 2360, interval_runtime: 7.9611, interval_samples_per_second: 2.01, interval_steps_per_second: 1.256, epoch: 12.4868[0m
[32m[2022-09-20 11:54:30,942] [    INFO][0m - loss: 0.0495349, learning_rate: 1.119047619047619e-05, global_step: 2370, interval_runtime: 7.989, interval_samples_per_second: 2.003, interval_steps_per_second: 1.252, epoch: 12.5397[0m
[32m[2022-09-20 11:54:38,922] [    INFO][0m - loss: 0.000395, learning_rate: 1.111111111111111e-05, global_step: 2380, interval_runtime: 7.9796, interval_samples_per_second: 2.005, interval_steps_per_second: 1.253, epoch: 12.5926[0m
[32m[2022-09-20 11:54:46,912] [    INFO][0m - loss: 0.00024702, learning_rate: 1.1031746031746031e-05, global_step: 2390, interval_runtime: 7.9901, interval_samples_per_second: 2.002, interval_steps_per_second: 1.252, epoch: 12.6455[0m
[32m[2022-09-20 11:54:54,896] [    INFO][0m - loss: 0.00176576, learning_rate: 1.0952380952380951e-05, global_step: 2400, interval_runtime: 7.9849, interval_samples_per_second: 2.004, interval_steps_per_second: 1.252, epoch: 12.6984[0m
[32m[2022-09-20 11:55:02,904] [    INFO][0m - loss: 0.03983935, learning_rate: 1.0873015873015873e-05, global_step: 2410, interval_runtime: 8.0071, interval_samples_per_second: 1.998, interval_steps_per_second: 1.249, epoch: 12.7513[0m
[32m[2022-09-20 11:55:10,893] [    INFO][0m - loss: 0.00037447, learning_rate: 1.0793650793650793e-05, global_step: 2420, interval_runtime: 7.9896, interval_samples_per_second: 2.003, interval_steps_per_second: 1.252, epoch: 12.8042[0m
[32m[2022-09-20 11:55:18,897] [    INFO][0m - loss: 0.0366549, learning_rate: 1.0714285714285714e-05, global_step: 2430, interval_runtime: 8.0039, interval_samples_per_second: 1.999, interval_steps_per_second: 1.249, epoch: 12.8571[0m
[32m[2022-09-20 11:55:26,879] [    INFO][0m - loss: 0.00908029, learning_rate: 1.0634920634920634e-05, global_step: 2440, interval_runtime: 7.9814, interval_samples_per_second: 2.005, interval_steps_per_second: 1.253, epoch: 12.9101[0m
[32m[2022-09-20 11:55:34,838] [    INFO][0m - loss: 0.06027872, learning_rate: 1.0555555555555555e-05, global_step: 2450, interval_runtime: 7.9597, interval_samples_per_second: 2.01, interval_steps_per_second: 1.256, epoch: 12.963[0m
[32m[2022-09-20 11:55:40,243] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 11:55:40,244] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-20 11:55:40,244] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 11:55:40,244] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 11:55:40,244] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-20 11:56:07,757] [    INFO][0m - eval_loss: 4.9148850440979, eval_accuracy: 0.44646758922068464, eval_runtime: 27.5126, eval_samples_per_second: 49.904, eval_steps_per_second: 3.126, epoch: 13.0[0m
[32m[2022-09-20 11:56:07,781] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-2457[0m
[32m[2022-09-20 11:56:07,781] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 11:56:10,523] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-2457/tokenizer_config.json[0m
[32m[2022-09-20 11:56:10,524] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-2457/special_tokens_map.json[0m
[32m[2022-09-20 11:56:18,189] [    INFO][0m - loss: 0.03678713, learning_rate: 1.0476190476190475e-05, global_step: 2460, interval_runtime: 43.3508, interval_samples_per_second: 0.369, interval_steps_per_second: 0.231, epoch: 13.0159[0m
[32m[2022-09-20 11:56:26,140] [    INFO][0m - loss: 0.00039535, learning_rate: 1.0396825396825397e-05, global_step: 2470, interval_runtime: 7.951, interval_samples_per_second: 2.012, interval_steps_per_second: 1.258, epoch: 13.0688[0m
[32m[2022-09-20 11:56:34,152] [    INFO][0m - loss: 0.00146407, learning_rate: 1.0317460317460317e-05, global_step: 2480, interval_runtime: 8.0117, interval_samples_per_second: 1.997, interval_steps_per_second: 1.248, epoch: 13.1217[0m
[32m[2022-09-20 11:56:42,147] [    INFO][0m - loss: 0.0023441, learning_rate: 1.0238095238095238e-05, global_step: 2490, interval_runtime: 7.9955, interval_samples_per_second: 2.001, interval_steps_per_second: 1.251, epoch: 13.1746[0m
[32m[2022-09-20 11:56:50,165] [    INFO][0m - loss: 0.00022816, learning_rate: 1.0158730158730158e-05, global_step: 2500, interval_runtime: 8.0176, interval_samples_per_second: 1.996, interval_steps_per_second: 1.247, epoch: 13.2275[0m
[32m[2022-09-20 11:56:58,150] [    INFO][0m - loss: 0.00024233, learning_rate: 1.007936507936508e-05, global_step: 2510, interval_runtime: 7.9856, interval_samples_per_second: 2.004, interval_steps_per_second: 1.252, epoch: 13.2804[0m
[32m[2022-09-20 11:57:06,118] [    INFO][0m - loss: 0.01586288, learning_rate: 9.999999999999999e-06, global_step: 2520, interval_runtime: 7.9678, interval_samples_per_second: 2.008, interval_steps_per_second: 1.255, epoch: 13.3333[0m
[32m[2022-09-20 11:57:14,069] [    INFO][0m - loss: 0.00024056, learning_rate: 9.92063492063492e-06, global_step: 2530, interval_runtime: 7.9511, interval_samples_per_second: 2.012, interval_steps_per_second: 1.258, epoch: 13.3862[0m
[32m[2022-09-20 11:57:22,028] [    INFO][0m - loss: 0.00516753, learning_rate: 9.84126984126984e-06, global_step: 2540, interval_runtime: 7.9591, interval_samples_per_second: 2.01, interval_steps_per_second: 1.256, epoch: 13.4392[0m
[32m[2022-09-20 11:57:29,998] [    INFO][0m - loss: 0.03540903, learning_rate: 9.761904761904762e-06, global_step: 2550, interval_runtime: 7.969, interval_samples_per_second: 2.008, interval_steps_per_second: 1.255, epoch: 13.4921[0m
[32m[2022-09-20 11:57:37,980] [    INFO][0m - loss: 0.0064613, learning_rate: 9.682539682539682e-06, global_step: 2560, interval_runtime: 7.9821, interval_samples_per_second: 2.004, interval_steps_per_second: 1.253, epoch: 13.545[0m
[32m[2022-09-20 11:57:45,963] [    INFO][0m - loss: 0.00871825, learning_rate: 9.603174603174603e-06, global_step: 2570, interval_runtime: 7.9836, interval_samples_per_second: 2.004, interval_steps_per_second: 1.253, epoch: 13.5979[0m
[32m[2022-09-20 11:57:53,952] [    INFO][0m - loss: 0.00013599, learning_rate: 9.523809523809523e-06, global_step: 2580, interval_runtime: 7.9883, interval_samples_per_second: 2.003, interval_steps_per_second: 1.252, epoch: 13.6508[0m
[32m[2022-09-20 11:58:01,974] [    INFO][0m - loss: 0.0004244, learning_rate: 9.444444444444445e-06, global_step: 2590, interval_runtime: 8.023, interval_samples_per_second: 1.994, interval_steps_per_second: 1.246, epoch: 13.7037[0m
[32m[2022-09-20 11:58:09,938] [    INFO][0m - loss: 0.0643801, learning_rate: 9.365079365079364e-06, global_step: 2600, interval_runtime: 7.9632, interval_samples_per_second: 2.009, interval_steps_per_second: 1.256, epoch: 13.7566[0m
[32m[2022-09-20 11:58:17,926] [    INFO][0m - loss: 9.828e-05, learning_rate: 9.285714285714286e-06, global_step: 2610, interval_runtime: 7.9882, interval_samples_per_second: 2.003, interval_steps_per_second: 1.252, epoch: 13.8095[0m
[32m[2022-09-20 11:58:25,920] [    INFO][0m - loss: 0.01504093, learning_rate: 9.206349206349206e-06, global_step: 2620, interval_runtime: 7.9942, interval_samples_per_second: 2.001, interval_steps_per_second: 1.251, epoch: 13.8624[0m
[32m[2022-09-20 11:58:33,912] [    INFO][0m - loss: 0.01076748, learning_rate: 9.126984126984127e-06, global_step: 2630, interval_runtime: 7.9916, interval_samples_per_second: 2.002, interval_steps_per_second: 1.251, epoch: 13.9153[0m
[32m[2022-09-20 11:58:41,913] [    INFO][0m - loss: 0.03629419, learning_rate: 9.047619047619047e-06, global_step: 2640, interval_runtime: 8.0017, interval_samples_per_second: 2.0, interval_steps_per_second: 1.25, epoch: 13.9683[0m
[32m[2022-09-20 11:58:46,522] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 11:58:46,522] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-20 11:58:46,522] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 11:58:46,522] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 11:58:46,522] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-20 11:59:14,117] [    INFO][0m - eval_loss: 5.007646083831787, eval_accuracy: 0.4515659140568099, eval_runtime: 27.5946, eval_samples_per_second: 49.756, eval_steps_per_second: 3.117, epoch: 14.0[0m
[32m[2022-09-20 11:59:14,141] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-2646[0m
[32m[2022-09-20 11:59:14,157] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 11:59:17,266] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-2646/tokenizer_config.json[0m
[32m[2022-09-20 11:59:17,266] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-2646/special_tokens_map.json[0m
[32m[2022-09-20 11:59:25,709] [    INFO][0m - loss: 0.0391171, learning_rate: 8.968253968253968e-06, global_step: 2650, interval_runtime: 43.7951, interval_samples_per_second: 0.365, interval_steps_per_second: 0.228, epoch: 14.0212[0m
[32m[2022-09-20 11:59:33,626] [    INFO][0m - loss: 8.884e-05, learning_rate: 8.888888888888888e-06, global_step: 2660, interval_runtime: 7.9177, interval_samples_per_second: 2.021, interval_steps_per_second: 1.263, epoch: 14.0741[0m
[32m[2022-09-20 11:59:41,615] [    INFO][0m - loss: 0.01147075, learning_rate: 8.80952380952381e-06, global_step: 2670, interval_runtime: 7.9887, interval_samples_per_second: 2.003, interval_steps_per_second: 1.252, epoch: 14.127[0m
[32m[2022-09-20 11:59:49,604] [    INFO][0m - loss: 0.00159517, learning_rate: 8.73015873015873e-06, global_step: 2680, interval_runtime: 7.9892, interval_samples_per_second: 2.003, interval_steps_per_second: 1.252, epoch: 14.1799[0m
[32m[2022-09-20 11:59:57,595] [    INFO][0m - loss: 0.04672176, learning_rate: 8.650793650793651e-06, global_step: 2690, interval_runtime: 7.9909, interval_samples_per_second: 2.002, interval_steps_per_second: 1.251, epoch: 14.2328[0m
[32m[2022-09-20 12:00:05,586] [    INFO][0m - loss: 0.00015308, learning_rate: 8.571428571428571e-06, global_step: 2700, interval_runtime: 7.9911, interval_samples_per_second: 2.002, interval_steps_per_second: 1.251, epoch: 14.2857[0m
[32m[2022-09-20 12:00:13,601] [    INFO][0m - loss: 0.01358709, learning_rate: 8.492063492063492e-06, global_step: 2710, interval_runtime: 8.014, interval_samples_per_second: 1.996, interval_steps_per_second: 1.248, epoch: 14.3386[0m
[32m[2022-09-20 12:00:21,612] [    INFO][0m - loss: 8.854e-05, learning_rate: 8.412698412698412e-06, global_step: 2720, interval_runtime: 8.0112, interval_samples_per_second: 1.997, interval_steps_per_second: 1.248, epoch: 14.3915[0m
[32m[2022-09-20 12:00:29,595] [    INFO][0m - loss: 0.02180867, learning_rate: 8.333333333333334e-06, global_step: 2730, interval_runtime: 7.9835, interval_samples_per_second: 2.004, interval_steps_per_second: 1.253, epoch: 14.4444[0m
[32m[2022-09-20 12:00:37,564] [    INFO][0m - loss: 0.01324376, learning_rate: 8.253968253968254e-06, global_step: 2740, interval_runtime: 7.9695, interval_samples_per_second: 2.008, interval_steps_per_second: 1.255, epoch: 14.4974[0m
[32m[2022-09-20 12:00:45,533] [    INFO][0m - loss: 0.0120966, learning_rate: 8.174603174603175e-06, global_step: 2750, interval_runtime: 7.9684, interval_samples_per_second: 2.008, interval_steps_per_second: 1.255, epoch: 14.5503[0m
[32m[2022-09-20 12:00:53,486] [    INFO][0m - loss: 0.00022577, learning_rate: 8.095238095238095e-06, global_step: 2760, interval_runtime: 7.9529, interval_samples_per_second: 2.012, interval_steps_per_second: 1.257, epoch: 14.6032[0m
[32m[2022-09-20 12:01:01,494] [    INFO][0m - loss: 7.775e-05, learning_rate: 8.015873015873016e-06, global_step: 2770, interval_runtime: 8.0083, interval_samples_per_second: 1.998, interval_steps_per_second: 1.249, epoch: 14.6561[0m
[32m[2022-09-20 12:01:09,504] [    INFO][0m - loss: 0.00013017, learning_rate: 7.936507936507936e-06, global_step: 2780, interval_runtime: 8.0102, interval_samples_per_second: 1.997, interval_steps_per_second: 1.248, epoch: 14.709[0m
[32m[2022-09-20 12:01:17,448] [    INFO][0m - loss: 7.783e-05, learning_rate: 7.857142857142858e-06, global_step: 2790, interval_runtime: 7.9436, interval_samples_per_second: 2.014, interval_steps_per_second: 1.259, epoch: 14.7619[0m
[32m[2022-09-20 12:01:25,434] [    INFO][0m - loss: 0.00866265, learning_rate: 7.777777777777777e-06, global_step: 2800, interval_runtime: 7.9857, interval_samples_per_second: 2.004, interval_steps_per_second: 1.252, epoch: 14.8148[0m
[32m[2022-09-20 12:01:33,407] [    INFO][0m - loss: 8.402e-05, learning_rate: 7.698412698412699e-06, global_step: 2810, interval_runtime: 7.9728, interval_samples_per_second: 2.007, interval_steps_per_second: 1.254, epoch: 14.8677[0m
[32m[2022-09-20 12:01:41,411] [    INFO][0m - loss: 0.02015867, learning_rate: 7.619047619047619e-06, global_step: 2820, interval_runtime: 8.0042, interval_samples_per_second: 1.999, interval_steps_per_second: 1.249, epoch: 14.9206[0m
[32m[2022-09-20 12:01:49,405] [    INFO][0m - loss: 9.381e-05, learning_rate: 7.53968253968254e-06, global_step: 2830, interval_runtime: 7.9944, interval_samples_per_second: 2.001, interval_steps_per_second: 1.251, epoch: 14.9735[0m
[32m[2022-09-20 12:01:53,214] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:01:53,214] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-20 12:01:53,214] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:01:53,214] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:01:53,214] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-20 12:02:20,751] [    INFO][0m - eval_loss: 5.019495487213135, eval_accuracy: 0.45010924981791695, eval_runtime: 27.5368, eval_samples_per_second: 49.861, eval_steps_per_second: 3.123, epoch: 15.0[0m
[32m[2022-09-20 12:02:20,775] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-2835[0m
[32m[2022-09-20 12:02:20,775] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:02:23,458] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-2835/tokenizer_config.json[0m
[32m[2022-09-20 12:02:23,459] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-2835/special_tokens_map.json[0m
[32m[2022-09-20 12:02:32,712] [    INFO][0m - loss: 0.0135627, learning_rate: 7.46031746031746e-06, global_step: 2840, interval_runtime: 43.3068, interval_samples_per_second: 0.369, interval_steps_per_second: 0.231, epoch: 15.0265[0m
[32m[2022-09-20 12:02:40,697] [    INFO][0m - loss: 0.00171964, learning_rate: 7.380952380952381e-06, global_step: 2850, interval_runtime: 7.9854, interval_samples_per_second: 2.004, interval_steps_per_second: 1.252, epoch: 15.0794[0m
[32m[2022-09-20 12:02:48,689] [    INFO][0m - loss: 0.01539593, learning_rate: 7.301587301587301e-06, global_step: 2860, interval_runtime: 7.9913, interval_samples_per_second: 2.002, interval_steps_per_second: 1.251, epoch: 15.1323[0m
[32m[2022-09-20 12:02:56,701] [    INFO][0m - loss: 0.00746289, learning_rate: 7.222222222222222e-06, global_step: 2870, interval_runtime: 8.0115, interval_samples_per_second: 1.997, interval_steps_per_second: 1.248, epoch: 15.1852[0m
[32m[2022-09-20 12:03:04,681] [    INFO][0m - loss: 0.03164254, learning_rate: 7.142857142857143e-06, global_step: 2880, interval_runtime: 7.9808, interval_samples_per_second: 2.005, interval_steps_per_second: 1.253, epoch: 15.2381[0m
[32m[2022-09-20 12:03:12,688] [    INFO][0m - loss: 0.00089971, learning_rate: 7.063492063492063e-06, global_step: 2890, interval_runtime: 8.007, interval_samples_per_second: 1.998, interval_steps_per_second: 1.249, epoch: 15.291[0m
[32m[2022-09-20 12:03:20,695] [    INFO][0m - loss: 0.0001344, learning_rate: 6.984126984126984e-06, global_step: 2900, interval_runtime: 8.0071, interval_samples_per_second: 1.998, interval_steps_per_second: 1.249, epoch: 15.3439[0m
[32m[2022-09-20 12:03:28,628] [    INFO][0m - loss: 5.463e-05, learning_rate: 6.904761904761905e-06, global_step: 2910, interval_runtime: 7.9333, interval_samples_per_second: 2.017, interval_steps_per_second: 1.261, epoch: 15.3968[0m
[32m[2022-09-20 12:03:36,598] [    INFO][0m - loss: 0.00015946, learning_rate: 6.825396825396825e-06, global_step: 2920, interval_runtime: 7.9696, interval_samples_per_second: 2.008, interval_steps_per_second: 1.255, epoch: 15.4497[0m
[32m[2022-09-20 12:03:44,611] [    INFO][0m - loss: 0.02216831, learning_rate: 6.746031746031746e-06, global_step: 2930, interval_runtime: 8.013, interval_samples_per_second: 1.997, interval_steps_per_second: 1.248, epoch: 15.5026[0m
[32m[2022-09-20 12:03:52,573] [    INFO][0m - loss: 0.00014884, learning_rate: 6.666666666666667e-06, global_step: 2940, interval_runtime: 7.9624, interval_samples_per_second: 2.009, interval_steps_per_second: 1.256, epoch: 15.5556[0m
[32m[2022-09-20 12:04:00,573] [    INFO][0m - loss: 0.00030962, learning_rate: 6.587301587301587e-06, global_step: 2950, interval_runtime: 8.0, interval_samples_per_second: 2.0, interval_steps_per_second: 1.25, epoch: 15.6085[0m
[32m[2022-09-20 12:04:08,543] [    INFO][0m - loss: 0.01951492, learning_rate: 6.507936507936508e-06, global_step: 2960, interval_runtime: 7.97, interval_samples_per_second: 2.008, interval_steps_per_second: 1.255, epoch: 15.6614[0m
[32m[2022-09-20 12:04:16,501] [    INFO][0m - loss: 0.00444627, learning_rate: 6.428571428571429e-06, global_step: 2970, interval_runtime: 7.9577, interval_samples_per_second: 2.011, interval_steps_per_second: 1.257, epoch: 15.7143[0m
[32m[2022-09-20 12:04:24,500] [    INFO][0m - loss: 7.621e-05, learning_rate: 6.349206349206349e-06, global_step: 2980, interval_runtime: 7.9985, interval_samples_per_second: 2.0, interval_steps_per_second: 1.25, epoch: 15.7672[0m
[32m[2022-09-20 12:04:32,476] [    INFO][0m - loss: 0.00017987, learning_rate: 6.26984126984127e-06, global_step: 2990, interval_runtime: 7.9768, interval_samples_per_second: 2.006, interval_steps_per_second: 1.254, epoch: 15.8201[0m
[32m[2022-09-20 12:04:40,474] [    INFO][0m - loss: 0.01451186, learning_rate: 6.190476190476191e-06, global_step: 3000, interval_runtime: 7.9981, interval_samples_per_second: 2.0, interval_steps_per_second: 1.25, epoch: 15.873[0m
[32m[2022-09-20 12:04:48,445] [    INFO][0m - loss: 0.0001182, learning_rate: 6.111111111111111e-06, global_step: 3010, interval_runtime: 7.9707, interval_samples_per_second: 2.007, interval_steps_per_second: 1.255, epoch: 15.9259[0m
[32m[2022-09-20 12:04:56,406] [    INFO][0m - loss: 6.876e-05, learning_rate: 6.031746031746032e-06, global_step: 3020, interval_runtime: 7.9604, interval_samples_per_second: 2.01, interval_steps_per_second: 1.256, epoch: 15.9788[0m
[32m[2022-09-20 12:04:59,446] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:04:59,446] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-20 12:04:59,446] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:04:59,446] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:04:59,446] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-20 12:05:26,957] [    INFO][0m - eval_loss: 5.054357528686523, eval_accuracy: 0.4435542607428988, eval_runtime: 27.5106, eval_samples_per_second: 49.908, eval_steps_per_second: 3.126, epoch: 16.0[0m
[32m[2022-09-20 12:05:26,981] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-3024[0m
[32m[2022-09-20 12:05:26,981] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:05:29,904] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-3024/tokenizer_config.json[0m
[32m[2022-09-20 12:05:29,904] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-3024/special_tokens_map.json[0m
[32m[2022-09-20 12:05:40,061] [    INFO][0m - loss: 0.03774723, learning_rate: 5.9523809523809525e-06, global_step: 3030, interval_runtime: 43.6556, interval_samples_per_second: 0.367, interval_steps_per_second: 0.229, epoch: 16.0317[0m
[32m[2022-09-20 12:05:48,032] [    INFO][0m - loss: 0.00103931, learning_rate: 5.873015873015873e-06, global_step: 3040, interval_runtime: 7.9709, interval_samples_per_second: 2.007, interval_steps_per_second: 1.255, epoch: 16.0847[0m
[32m[2022-09-20 12:05:55,964] [    INFO][0m - loss: 4.327e-05, learning_rate: 5.793650793650794e-06, global_step: 3050, interval_runtime: 7.9313, interval_samples_per_second: 2.017, interval_steps_per_second: 1.261, epoch: 16.1376[0m
[32m[2022-09-20 12:06:03,952] [    INFO][0m - loss: 6.665e-05, learning_rate: 5.7142857142857145e-06, global_step: 3060, interval_runtime: 7.9887, interval_samples_per_second: 2.003, interval_steps_per_second: 1.252, epoch: 16.1905[0m
[32m[2022-09-20 12:06:12,003] [    INFO][0m - loss: 0.00011317, learning_rate: 5.634920634920635e-06, global_step: 3070, interval_runtime: 8.0507, interval_samples_per_second: 1.987, interval_steps_per_second: 1.242, epoch: 16.2434[0m
[32m[2022-09-20 12:06:20,019] [    INFO][0m - loss: 0.00182887, learning_rate: 5.555555555555555e-06, global_step: 3080, interval_runtime: 8.0159, interval_samples_per_second: 1.996, interval_steps_per_second: 1.248, epoch: 16.2963[0m
[32m[2022-09-20 12:06:27,999] [    INFO][0m - loss: 4.832e-05, learning_rate: 5.476190476190476e-06, global_step: 3090, interval_runtime: 7.9802, interval_samples_per_second: 2.005, interval_steps_per_second: 1.253, epoch: 16.3492[0m
[32m[2022-09-20 12:06:36,006] [    INFO][0m - loss: 7.262e-05, learning_rate: 5.396825396825396e-06, global_step: 3100, interval_runtime: 8.0066, interval_samples_per_second: 1.998, interval_steps_per_second: 1.249, epoch: 16.4021[0m
[32m[2022-09-20 12:06:44,023] [    INFO][0m - loss: 0.03480214, learning_rate: 5.317460317460317e-06, global_step: 3110, interval_runtime: 8.0168, interval_samples_per_second: 1.996, interval_steps_per_second: 1.247, epoch: 16.455[0m
[32m[2022-09-20 12:06:52,007] [    INFO][0m - loss: 5.802e-05, learning_rate: 5.238095238095238e-06, global_step: 3120, interval_runtime: 7.9848, interval_samples_per_second: 2.004, interval_steps_per_second: 1.252, epoch: 16.5079[0m
[32m[2022-09-20 12:06:59,999] [    INFO][0m - loss: 0.02296832, learning_rate: 5.158730158730158e-06, global_step: 3130, interval_runtime: 7.992, interval_samples_per_second: 2.002, interval_steps_per_second: 1.251, epoch: 16.5608[0m
[32m[2022-09-20 12:07:07,984] [    INFO][0m - loss: 0.00714386, learning_rate: 5.079365079365079e-06, global_step: 3140, interval_runtime: 7.985, interval_samples_per_second: 2.004, interval_steps_per_second: 1.252, epoch: 16.6138[0m
[32m[2022-09-20 12:07:15,988] [    INFO][0m - loss: 0.00966083, learning_rate: 4.9999999999999996e-06, global_step: 3150, interval_runtime: 8.0038, interval_samples_per_second: 1.999, interval_steps_per_second: 1.249, epoch: 16.6667[0m
[32m[2022-09-20 12:07:23,969] [    INFO][0m - loss: 0.00014363, learning_rate: 4.92063492063492e-06, global_step: 3160, interval_runtime: 7.9806, interval_samples_per_second: 2.005, interval_steps_per_second: 1.253, epoch: 16.7196[0m
[32m[2022-09-20 12:07:31,946] [    INFO][0m - loss: 9.403e-05, learning_rate: 4.841269841269841e-06, global_step: 3170, interval_runtime: 7.9771, interval_samples_per_second: 2.006, interval_steps_per_second: 1.254, epoch: 16.7725[0m
[32m[2022-09-20 12:07:39,943] [    INFO][0m - loss: 0.00013922, learning_rate: 4.7619047619047615e-06, global_step: 3180, interval_runtime: 7.997, interval_samples_per_second: 2.001, interval_steps_per_second: 1.25, epoch: 16.8254[0m
[32m[2022-09-20 12:07:47,914] [    INFO][0m - loss: 0.00914325, learning_rate: 4.682539682539682e-06, global_step: 3190, interval_runtime: 7.971, interval_samples_per_second: 2.007, interval_steps_per_second: 1.255, epoch: 16.8783[0m
[32m[2022-09-20 12:07:55,875] [    INFO][0m - loss: 0.00508856, learning_rate: 4.603174603174603e-06, global_step: 3200, interval_runtime: 7.9617, interval_samples_per_second: 2.01, interval_steps_per_second: 1.256, epoch: 16.9312[0m
[32m[2022-09-20 12:08:03,804] [    INFO][0m - loss: 0.02082288, learning_rate: 4.5238095238095235e-06, global_step: 3210, interval_runtime: 7.9282, interval_samples_per_second: 2.018, interval_steps_per_second: 1.261, epoch: 16.9841[0m
[32m[2022-09-20 12:08:06,082] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:08:06,082] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-20 12:08:06,082] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:08:06,082] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:08:06,083] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-20 12:08:33,595] [    INFO][0m - eval_loss: 5.087226867675781, eval_accuracy: 0.4515659140568099, eval_runtime: 27.5127, eval_samples_per_second: 49.904, eval_steps_per_second: 3.126, epoch: 17.0[0m
[32m[2022-09-20 12:08:33,618] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-3213[0m
[32m[2022-09-20 12:08:33,618] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:08:36,243] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-3213/tokenizer_config.json[0m
[32m[2022-09-20 12:08:36,243] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-3213/special_tokens_map.json[0m
[32m[2022-09-20 12:08:47,100] [    INFO][0m - loss: 0.00580558, learning_rate: 4.444444444444444e-06, global_step: 3220, interval_runtime: 43.2963, interval_samples_per_second: 0.37, interval_steps_per_second: 0.231, epoch: 17.037[0m
[32m[2022-09-20 12:08:55,173] [    INFO][0m - loss: 0.00011304, learning_rate: 4.365079365079365e-06, global_step: 3230, interval_runtime: 8.0728, interval_samples_per_second: 1.982, interval_steps_per_second: 1.239, epoch: 17.0899[0m
[32m[2022-09-20 12:09:03,163] [    INFO][0m - loss: 0.00747571, learning_rate: 4.2857142857142855e-06, global_step: 3240, interval_runtime: 7.9898, interval_samples_per_second: 2.003, interval_steps_per_second: 1.252, epoch: 17.1429[0m
[32m[2022-09-20 12:09:11,142] [    INFO][0m - loss: 0.01634245, learning_rate: 4.206349206349206e-06, global_step: 3250, interval_runtime: 7.9789, interval_samples_per_second: 2.005, interval_steps_per_second: 1.253, epoch: 17.1958[0m
[32m[2022-09-20 12:09:19,119] [    INFO][0m - loss: 0.00849276, learning_rate: 4.126984126984127e-06, global_step: 3260, interval_runtime: 7.9779, interval_samples_per_second: 2.006, interval_steps_per_second: 1.253, epoch: 17.2487[0m
[32m[2022-09-20 12:09:27,126] [    INFO][0m - loss: 0.01288012, learning_rate: 4.0476190476190474e-06, global_step: 3270, interval_runtime: 8.0065, interval_samples_per_second: 1.998, interval_steps_per_second: 1.249, epoch: 17.3016[0m
[32m[2022-09-20 12:09:35,106] [    INFO][0m - loss: 0.00872228, learning_rate: 3.968253968253968e-06, global_step: 3280, interval_runtime: 7.9798, interval_samples_per_second: 2.005, interval_steps_per_second: 1.253, epoch: 17.3545[0m
[32m[2022-09-20 12:09:43,117] [    INFO][0m - loss: 0.00597011, learning_rate: 3.888888888888889e-06, global_step: 3290, interval_runtime: 8.0109, interval_samples_per_second: 1.997, interval_steps_per_second: 1.248, epoch: 17.4074[0m
[32m[2022-09-20 12:09:51,062] [    INFO][0m - loss: 7.582e-05, learning_rate: 3.8095238095238094e-06, global_step: 3300, interval_runtime: 7.9449, interval_samples_per_second: 2.014, interval_steps_per_second: 1.259, epoch: 17.4603[0m
[32m[2022-09-20 12:10:00,003] [    INFO][0m - loss: 9.907e-05, learning_rate: 3.73015873015873e-06, global_step: 3310, interval_runtime: 7.9838, interval_samples_per_second: 2.004, interval_steps_per_second: 1.253, epoch: 17.5132[0m
[32m[2022-09-20 12:10:07,986] [    INFO][0m - loss: 0.00108941, learning_rate: 3.6507936507936507e-06, global_step: 3320, interval_runtime: 8.9401, interval_samples_per_second: 1.79, interval_steps_per_second: 1.119, epoch: 17.5661[0m
[32m[2022-09-20 12:10:15,958] [    INFO][0m - loss: 6.607e-05, learning_rate: 3.5714285714285714e-06, global_step: 3330, interval_runtime: 7.9727, interval_samples_per_second: 2.007, interval_steps_per_second: 1.254, epoch: 17.619[0m
[32m[2022-09-20 12:10:23,934] [    INFO][0m - loss: 0.0174086, learning_rate: 3.492063492063492e-06, global_step: 3340, interval_runtime: 7.9755, interval_samples_per_second: 2.006, interval_steps_per_second: 1.254, epoch: 17.672[0m
[32m[2022-09-20 12:10:31,965] [    INFO][0m - loss: 0.00456693, learning_rate: 3.4126984126984127e-06, global_step: 3350, interval_runtime: 8.0311, interval_samples_per_second: 1.992, interval_steps_per_second: 1.245, epoch: 17.7249[0m
[32m[2022-09-20 12:10:39,919] [    INFO][0m - loss: 9.052e-05, learning_rate: 3.3333333333333333e-06, global_step: 3360, interval_runtime: 7.9544, interval_samples_per_second: 2.011, interval_steps_per_second: 1.257, epoch: 17.7778[0m
[32m[2022-09-20 12:10:47,914] [    INFO][0m - loss: 5.029e-05, learning_rate: 3.253968253968254e-06, global_step: 3370, interval_runtime: 7.9947, interval_samples_per_second: 2.001, interval_steps_per_second: 1.251, epoch: 17.8307[0m
[32m[2022-09-20 12:10:55,949] [    INFO][0m - loss: 4.615e-05, learning_rate: 3.1746031746031746e-06, global_step: 3380, interval_runtime: 8.0347, interval_samples_per_second: 1.991, interval_steps_per_second: 1.245, epoch: 17.8836[0m
[32m[2022-09-20 12:11:03,936] [    INFO][0m - loss: 5.626e-05, learning_rate: 3.0952380952380953e-06, global_step: 3390, interval_runtime: 7.9871, interval_samples_per_second: 2.003, interval_steps_per_second: 1.252, epoch: 17.9365[0m
[32m[2022-09-20 12:11:11,825] [    INFO][0m - loss: 8.205e-05, learning_rate: 3.015873015873016e-06, global_step: 3400, interval_runtime: 7.8894, interval_samples_per_second: 2.028, interval_steps_per_second: 1.268, epoch: 17.9894[0m
[32m[2022-09-20 12:11:13,345] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:11:13,345] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-20 12:11:13,345] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:11:13,345] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:11:13,345] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-20 12:11:41,045] [    INFO][0m - eval_loss: 5.101685523986816, eval_accuracy: 0.45229424617625635, eval_runtime: 27.6993, eval_samples_per_second: 49.568, eval_steps_per_second: 3.105, epoch: 18.0[0m
[32m[2022-09-20 12:11:41,071] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-3402[0m
[32m[2022-09-20 12:11:41,071] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:11:43,752] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-3402/tokenizer_config.json[0m
[32m[2022-09-20 12:11:43,752] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-3402/special_tokens_map.json[0m
[32m[2022-09-20 12:11:55,306] [    INFO][0m - loss: 5.303e-05, learning_rate: 2.9365079365079366e-06, global_step: 3410, interval_runtime: 43.4801, interval_samples_per_second: 0.368, interval_steps_per_second: 0.23, epoch: 18.0423[0m
[32m[2022-09-20 12:12:03,251] [    INFO][0m - loss: 0.00711673, learning_rate: 2.8571428571428573e-06, global_step: 3420, interval_runtime: 7.9455, interval_samples_per_second: 2.014, interval_steps_per_second: 1.259, epoch: 18.0952[0m
[32m[2022-09-20 12:12:11,182] [    INFO][0m - loss: 3.975e-05, learning_rate: 2.7777777777777775e-06, global_step: 3430, interval_runtime: 7.931, interval_samples_per_second: 2.017, interval_steps_per_second: 1.261, epoch: 18.1481[0m
[32m[2022-09-20 12:12:19,161] [    INFO][0m - loss: 8.766e-05, learning_rate: 2.698412698412698e-06, global_step: 3440, interval_runtime: 7.9789, interval_samples_per_second: 2.005, interval_steps_per_second: 1.253, epoch: 18.2011[0m
[32m[2022-09-20 12:12:27,156] [    INFO][0m - loss: 5.359e-05, learning_rate: 2.619047619047619e-06, global_step: 3450, interval_runtime: 7.9949, interval_samples_per_second: 2.001, interval_steps_per_second: 1.251, epoch: 18.254[0m
[32m[2022-09-20 12:12:35,148] [    INFO][0m - loss: 3.484e-05, learning_rate: 2.5396825396825395e-06, global_step: 3460, interval_runtime: 7.9921, interval_samples_per_second: 2.002, interval_steps_per_second: 1.251, epoch: 18.3069[0m
[32m[2022-09-20 12:12:43,185] [    INFO][0m - loss: 0.00493108, learning_rate: 2.46031746031746e-06, global_step: 3470, interval_runtime: 8.0377, interval_samples_per_second: 1.991, interval_steps_per_second: 1.244, epoch: 18.3598[0m
[32m[2022-09-20 12:12:51,147] [    INFO][0m - loss: 0.02333694, learning_rate: 2.3809523809523808e-06, global_step: 3480, interval_runtime: 7.9617, interval_samples_per_second: 2.01, interval_steps_per_second: 1.256, epoch: 18.4127[0m
[32m[2022-09-20 12:12:59,115] [    INFO][0m - loss: 5.046e-05, learning_rate: 2.3015873015873014e-06, global_step: 3490, interval_runtime: 7.9674, interval_samples_per_second: 2.008, interval_steps_per_second: 1.255, epoch: 18.4656[0m
[32m[2022-09-20 12:13:07,090] [    INFO][0m - loss: 5.798e-05, learning_rate: 2.222222222222222e-06, global_step: 3500, interval_runtime: 7.9757, interval_samples_per_second: 2.006, interval_steps_per_second: 1.254, epoch: 18.5185[0m
[32m[2022-09-20 12:13:15,058] [    INFO][0m - loss: 3.827e-05, learning_rate: 2.1428571428571427e-06, global_step: 3510, interval_runtime: 7.9678, interval_samples_per_second: 2.008, interval_steps_per_second: 1.255, epoch: 18.5714[0m
[32m[2022-09-20 12:13:23,075] [    INFO][0m - loss: 0.00349588, learning_rate: 2.0634920634920634e-06, global_step: 3520, interval_runtime: 8.0173, interval_samples_per_second: 1.996, interval_steps_per_second: 1.247, epoch: 18.6243[0m
[32m[2022-09-20 12:13:31,062] [    INFO][0m - loss: 0.00451422, learning_rate: 1.984126984126984e-06, global_step: 3530, interval_runtime: 7.9871, interval_samples_per_second: 2.003, interval_steps_per_second: 1.252, epoch: 18.6772[0m
[32m[2022-09-20 12:13:39,048] [    INFO][0m - loss: 5.661e-05, learning_rate: 1.9047619047619047e-06, global_step: 3540, interval_runtime: 7.9851, interval_samples_per_second: 2.004, interval_steps_per_second: 1.252, epoch: 18.7302[0m
[32m[2022-09-20 12:13:47,041] [    INFO][0m - loss: 0.00955008, learning_rate: 1.8253968253968254e-06, global_step: 3550, interval_runtime: 7.9928, interval_samples_per_second: 2.002, interval_steps_per_second: 1.251, epoch: 18.7831[0m
[32m[2022-09-20 12:13:59,230] [    INFO][0m - loss: 0.00738434, learning_rate: 1.746031746031746e-06, global_step: 3560, interval_runtime: 7.9778, interval_samples_per_second: 2.006, interval_steps_per_second: 1.253, epoch: 18.836[0m
[32m[2022-09-20 12:14:07,225] [    INFO][0m - loss: 4.634e-05, learning_rate: 1.6666666666666667e-06, global_step: 3570, interval_runtime: 12.2064, interval_samples_per_second: 1.311, interval_steps_per_second: 0.819, epoch: 18.8889[0m
[32m[2022-09-20 12:14:15,221] [    INFO][0m - loss: 0.00990065, learning_rate: 1.5873015873015873e-06, global_step: 3580, interval_runtime: 7.9964, interval_samples_per_second: 2.001, interval_steps_per_second: 1.251, epoch: 18.9418[0m
[32m[2022-09-20 12:14:23,047] [    INFO][0m - loss: 0.00608868, learning_rate: 1.507936507936508e-06, global_step: 3590, interval_runtime: 7.8257, interval_samples_per_second: 2.045, interval_steps_per_second: 1.278, epoch: 18.9947[0m
[32m[2022-09-20 12:14:23,805] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:14:23,806] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-20 12:14:23,806] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:14:23,806] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:14:23,806] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-20 12:14:51,483] [    INFO][0m - eval_loss: 5.106213569641113, eval_accuracy: 0.45083758193736345, eval_runtime: 27.677, eval_samples_per_second: 49.608, eval_steps_per_second: 3.107, epoch: 19.0[0m
[32m[2022-09-20 12:14:51,507] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-3591[0m
[32m[2022-09-20 12:14:51,507] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:14:54,113] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-3591/tokenizer_config.json[0m
[32m[2022-09-20 12:14:54,114] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-3591/special_tokens_map.json[0m
[32m[2022-09-20 12:15:06,408] [    INFO][0m - loss: 0.0061774, learning_rate: 1.4285714285714286e-06, global_step: 3600, interval_runtime: 43.3612, interval_samples_per_second: 0.369, interval_steps_per_second: 0.231, epoch: 19.0476[0m
[32m[2022-09-20 12:15:14,384] [    INFO][0m - loss: 0.00018613, learning_rate: 1.349206349206349e-06, global_step: 3610, interval_runtime: 7.9757, interval_samples_per_second: 2.006, interval_steps_per_second: 1.254, epoch: 19.1005[0m
[32m[2022-09-20 12:15:22,411] [    INFO][0m - loss: 0.00410195, learning_rate: 1.2698412698412697e-06, global_step: 3620, interval_runtime: 8.027, interval_samples_per_second: 1.993, interval_steps_per_second: 1.246, epoch: 19.1534[0m
[32m[2022-09-20 12:15:30,436] [    INFO][0m - loss: 4.226e-05, learning_rate: 1.1904761904761904e-06, global_step: 3630, interval_runtime: 8.0257, interval_samples_per_second: 1.994, interval_steps_per_second: 1.246, epoch: 19.2063[0m
[32m[2022-09-20 12:15:38,426] [    INFO][0m - loss: 5.537e-05, learning_rate: 1.111111111111111e-06, global_step: 3640, interval_runtime: 7.9896, interval_samples_per_second: 2.003, interval_steps_per_second: 1.252, epoch: 19.2593[0m
[32m[2022-09-20 12:15:46,403] [    INFO][0m - loss: 0.00682515, learning_rate: 1.0317460317460317e-06, global_step: 3650, interval_runtime: 7.9771, interval_samples_per_second: 2.006, interval_steps_per_second: 1.254, epoch: 19.3122[0m
[32m[2022-09-20 12:15:54,364] [    INFO][0m - loss: 3.385e-05, learning_rate: 9.523809523809523e-07, global_step: 3660, interval_runtime: 7.9607, interval_samples_per_second: 2.01, interval_steps_per_second: 1.256, epoch: 19.3651[0m
[32m[2022-09-20 12:16:02,325] [    INFO][0m - loss: 7.754e-05, learning_rate: 8.73015873015873e-07, global_step: 3670, interval_runtime: 7.961, interval_samples_per_second: 2.01, interval_steps_per_second: 1.256, epoch: 19.418[0m
[32m[2022-09-20 12:16:10,307] [    INFO][0m - loss: 5.636e-05, learning_rate: 7.936507936507937e-07, global_step: 3680, interval_runtime: 7.9823, interval_samples_per_second: 2.004, interval_steps_per_second: 1.253, epoch: 19.4709[0m
[32m[2022-09-20 12:16:20,560] [    INFO][0m - loss: 6.194e-05, learning_rate: 7.142857142857143e-07, global_step: 3690, interval_runtime: 10.2524, interval_samples_per_second: 1.561, interval_steps_per_second: 0.975, epoch: 19.5238[0m
[32m[2022-09-20 12:16:28,603] [    INFO][0m - loss: 5.786e-05, learning_rate: 6.349206349206349e-07, global_step: 3700, interval_runtime: 8.0429, interval_samples_per_second: 1.989, interval_steps_per_second: 1.243, epoch: 19.5767[0m
[32m[2022-09-20 12:16:36,638] [    INFO][0m - loss: 0.00634979, learning_rate: 5.555555555555555e-07, global_step: 3710, interval_runtime: 8.0352, interval_samples_per_second: 1.991, interval_steps_per_second: 1.245, epoch: 19.6296[0m
[32m[2022-09-20 12:16:44,643] [    INFO][0m - loss: 4.02e-05, learning_rate: 4.761904761904762e-07, global_step: 3720, interval_runtime: 8.0055, interval_samples_per_second: 1.999, interval_steps_per_second: 1.249, epoch: 19.6825[0m
[32m[2022-09-20 12:16:52,652] [    INFO][0m - loss: 0.02186182, learning_rate: 3.9682539682539683e-07, global_step: 3730, interval_runtime: 8.0085, interval_samples_per_second: 1.998, interval_steps_per_second: 1.249, epoch: 19.7354[0m
[32m[2022-09-20 12:17:00,640] [    INFO][0m - loss: 4.647e-05, learning_rate: 3.1746031746031743e-07, global_step: 3740, interval_runtime: 7.9882, interval_samples_per_second: 2.003, interval_steps_per_second: 1.252, epoch: 19.7884[0m
[32m[2022-09-20 12:17:08,645] [    INFO][0m - loss: 0.00601029, learning_rate: 2.380952380952381e-07, global_step: 3750, interval_runtime: 8.0053, interval_samples_per_second: 1.999, interval_steps_per_second: 1.249, epoch: 19.8413[0m
[32m[2022-09-20 12:17:16,656] [    INFO][0m - loss: 0.00335148, learning_rate: 1.5873015873015872e-07, global_step: 3760, interval_runtime: 7.9971, interval_samples_per_second: 2.001, interval_steps_per_second: 1.25, epoch: 19.8942[0m
[32m[2022-09-20 12:17:24,663] [    INFO][0m - loss: 4.12e-05, learning_rate: 7.936507936507936e-08, global_step: 3770, interval_runtime: 8.0205, interval_samples_per_second: 1.995, interval_steps_per_second: 1.247, epoch: 19.9471[0m
[32m[2022-09-20 12:17:32,494] [    INFO][0m - loss: 0.00519304, learning_rate: 0.0, global_step: 3780, interval_runtime: 7.8314, interval_samples_per_second: 2.043, interval_steps_per_second: 1.277, epoch: 20.0[0m
[32m[2022-09-20 12:17:32,495] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:17:32,495] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-20 12:17:32,495] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:17:32,495] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:17:32,495] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-20 12:18:00,036] [    INFO][0m - eval_loss: 5.111141681671143, eval_accuracy: 0.44865258557902404, eval_runtime: 27.5401, eval_samples_per_second: 49.855, eval_steps_per_second: 3.123, epoch: 20.0[0m
[32m[2022-09-20 12:18:00,059] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-3780[0m
[32m[2022-09-20 12:18:00,060] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:18:02,759] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-3780/tokenizer_config.json[0m
[32m[2022-09-20 12:18:02,760] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-3780/special_tokens_map.json[0m
[32m[2022-09-20 12:18:07,763] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-20 12:18:07,763] [    INFO][0m - Loading best model from ./checkpoints_iflytek/checkpoint-378 (score: 0.46176256372906044).[0m
[32m[2022-09-20 12:18:09,239] [    INFO][0m - train_runtime: 3763.1351, train_samples_per_second: 16.072, train_steps_per_second: 1.004, train_loss: 0.2801117839728942, epoch: 20.0[0m
[32m[2022-09-20 12:18:09,295] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/[0m
[32m[2022-09-20 12:18:09,295] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:18:11,826] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/tokenizer_config.json[0m
[32m[2022-09-20 12:18:11,826] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/special_tokens_map.json[0m
[32m[2022-09-20 12:18:11,828] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-20 12:18:11,828] [    INFO][0m -   epoch                    =       20.0[0m
[32m[2022-09-20 12:18:11,829] [    INFO][0m -   train_loss               =     0.2801[0m
[32m[2022-09-20 12:18:11,829] [    INFO][0m -   train_runtime            = 1:02:43.13[0m
[32m[2022-09-20 12:18:11,829] [    INFO][0m -   train_samples_per_second =     16.072[0m
[32m[2022-09-20 12:18:11,829] [    INFO][0m -   train_steps_per_second   =      1.004[0m
[32m[2022-09-20 12:18:11,843] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-20 12:18:11,843] [    INFO][0m -   Num examples = 1749[0m
[32m[2022-09-20 12:18:11,844] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:18:11,844] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:18:11,844] [    INFO][0m -   Total prediction steps = 110[0m
[32m[2022-09-20 12:18:46,889] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-20 12:18:46,889] [    INFO][0m -   test_accuracy           =     0.4694[0m
[32m[2022-09-20 12:18:46,889] [    INFO][0m -   test_loss               =     1.7701[0m
[32m[2022-09-20 12:18:46,890] [    INFO][0m -   test_runtime            = 0:00:35.04[0m
[32m[2022-09-20 12:18:46,890] [    INFO][0m -   test_samples_per_second =     49.907[0m
[32m[2022-09-20 12:18:46,890] [    INFO][0m -   test_steps_per_second   =      3.139[0m
[32m[2022-09-20 12:18:46,890] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-20 12:18:46,890] [    INFO][0m -   Num examples = 2600[0m
[32m[2022-09-20 12:18:46,890] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:18:46,890] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:18:46,890] [    INFO][0m -   Total prediction steps = 163[0m
[32m[2022-09-20 12:19:45,408] [    INFO][0m - Predictions for iflytekf saved to ./fewclue_submit_examples.[0m
{
  "labels": 77,
  "text_a": "\u884c\u8baf\u901a\u662f\u5e7f\u5dde\u4ea4\u901a\u4fe1\u606f\u5316\u5efa\u8bbe\u6295\u8d44\u8425\u8fd0\u6709\u9650\u516c\u53f8\u5728\u5e7f\u5dde\u5e02\u4ea4\u901a\u8fd0\u8f93\u5c40\u7684\u6307\u5bfc\u4e0b\uff0c\u63a8\u51fa\u7684\u4e00\u6b3e\u63d0\u4f9b\u4ea4\u901a\u4fe1\u606f\u670d\u52a1\u7684\u624b\u673a\u7ec8\u7aef\u8f6f\u4ef6\uff0c\u652f\u6301Android\u548ciPhone\u7cfb\u7edf\u3002\u4e3b\u8981\u5305\u62ec\u8def\u51b5\u4fe1\u606f\u3001\u5b9e\u65f6\u516c\u4ea4\u3001\u505c\u8f66\u670d\u52a1\u3001\u7684\u58eb\u67e5\u8be2\u3001\u51fa\u884c\u89c4\u5212\u3001\u5730\u94c1\u4fe1\u606f\u3001\u822a\u7a7a\u4fe1\u606f\u3001\u94c1\u8def\u4fe1\u606f\u3001\u5ba2\u8fd0\u4fe1\u606f\u3001\u9a7e\u57f9\u4fe1\u606f\u3001\u4ea4\u901a\u8d44\u8baf\u3001WIFI\u70ed\u70b9\u7b49\u529f\u80fd\u6a21\u5757\u3002\u66f4\u65b0\u5185\u5bb91\u3001\u4f18\u5316\u5df2\u77e5bug\u3002",
  "text_b": "",
  "uid": 0
}

Prediction done.
 
==========
ocnli
==========
 
[32m[2022-09-20 12:20:02,333] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-20 12:20:02,334] [    INFO][0m - ============================================================[0m
[32m[2022-09-20 12:20:02,334] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-20 12:20:02,334] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-20 12:20:02,334] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-20 12:20:02,334] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-20 12:20:02,334] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-20 12:20:02,334] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-20 12:20:02,334] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-20 12:20:02,334] [    INFO][0m - [0m
[32m[2022-09-20 12:20:02,334] [    INFO][0m - ============================================================[0m
[32m[2022-09-20 12:20:02,334] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-20 12:20:02,335] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-20 12:20:02,335] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-20 12:20:02,335] [    INFO][0m - pretrained                    :/ssd2/wanghuijuan03/data/zero-shot/checkpoints_09191451/checkpoint-43000/model_state.pdparams[0m
[32m[2022-09-20 12:20:02,335] [    INFO][0m - prompt                        :‚Äú{'text':'text_a'}‚ÄùÂíå‚Äú{'text':'text_b'}‚Äù‰πãÈó¥ÁöÑÈÄªËæëÂÖ≥Á≥ªÊòØ{'mask'}{'mask'}„ÄÇ[0m
[32m[2022-09-20 12:20:02,335] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-20 12:20:02,335] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-20 12:20:02,335] [    INFO][0m - task_name                     :ocnli[0m
[32m[2022-09-20 12:20:02,335] [    INFO][0m - [0m
[32m[2022-09-20 12:20:02,335] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0920 12:20:02.336925 81686 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0920 12:20:02.340838 81686 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-20 12:20:07,061] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-20 12:20:07,072] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-20 12:20:07,072] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-20 12:20:07,073] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùÂíå‚Äú'}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': '‚Äù‰πãÈó¥ÁöÑÈÄªËæëÂÖ≥Á≥ªÊòØ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': '„ÄÇ'}][0m
[32m[2022-09-20 12:20:08,442] [    INFO][0m - ============================================================[0m
[32m[2022-09-20 12:20:08,442] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-20 12:20:08,442] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-20 12:20:08,442] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-20 12:20:08,442] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-20 12:20:08,442] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-20 12:20:08,442] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-20 12:20:08,443] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-20 12:20:08,443] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-20 12:20:08,443] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-20 12:20:08,443] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-20 12:20:08,443] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-20 12:20:08,443] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-20 12:20:08,443] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-20 12:20:08,443] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-20 12:20:08,443] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-20 12:20:08,443] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-20 12:20:08,443] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-20 12:20:08,443] [    INFO][0m - eval_batch_size               :16[0m
[32m[2022-09-20 12:20:08,443] [    INFO][0m - eval_steps                    :200[0m
[32m[2022-09-20 12:20:08,444] [    INFO][0m - evaluation_strategy           :IntervalStrategy.EPOCH[0m
[32m[2022-09-20 12:20:08,444] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-20 12:20:08,444] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-20 12:20:08,444] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-20 12:20:08,444] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-20 12:20:08,444] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-20 12:20:08,444] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-20 12:20:08,444] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-20 12:20:08,444] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-20 12:20:08,444] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-20 12:20:08,444] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-09-20 12:20:08,444] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-20 12:20:08,444] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-20 12:20:08,444] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-20 12:20:08,445] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-20 12:20:08,445] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-20 12:20:08,445] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-20 12:20:08,445] [    INFO][0m - logging_dir                   :./checkpoints_ocnli/runs/Sep20_12-20-02_instance-3bwob41y-01[0m
[32m[2022-09-20 12:20:08,445] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-20 12:20:08,445] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-20 12:20:08,445] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-20 12:20:08,445] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-20 12:20:08,445] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-20 12:20:08,445] [    INFO][0m - max_seq_length                :128[0m
[32m[2022-09-20 12:20:08,445] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-20 12:20:08,445] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-20 12:20:08,445] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-20 12:20:08,445] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-20 12:20:08,446] [    INFO][0m - num_train_epochs              :20.0[0m
[32m[2022-09-20 12:20:08,446] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-20 12:20:08,446] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-20 12:20:08,446] [    INFO][0m - output_dir                    :./checkpoints_ocnli/[0m
[32m[2022-09-20 12:20:08,446] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-20 12:20:08,446] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-20 12:20:08,446] [    INFO][0m - per_device_eval_batch_size    :16[0m
[32m[2022-09-20 12:20:08,446] [    INFO][0m - per_device_train_batch_size   :16[0m
[32m[2022-09-20 12:20:08,446] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-20 12:20:08,446] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-20 12:20:08,446] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-20 12:20:08,446] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-09-20 12:20:08,446] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-20 12:20:08,447] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-20 12:20:08,447] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-20 12:20:08,447] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-20 12:20:08,447] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-20 12:20:08,447] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-20 12:20:08,447] [    INFO][0m - run_name                      :./checkpoints_ocnli/[0m
[32m[2022-09-20 12:20:08,447] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-20 12:20:08,447] [    INFO][0m - save_steps                    :200[0m
[32m[2022-09-20 12:20:08,447] [    INFO][0m - save_strategy                 :IntervalStrategy.EPOCH[0m
[32m[2022-09-20 12:20:08,447] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-20 12:20:08,447] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-20 12:20:08,447] [    INFO][0m - seed                          :42[0m
[32m[2022-09-20 12:20:08,447] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-20 12:20:08,447] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-20 12:20:08,448] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-20 12:20:08,448] [    INFO][0m - train_batch_size              :16[0m
[32m[2022-09-20 12:20:08,448] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-20 12:20:08,448] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-20 12:20:08,448] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-20 12:20:08,448] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-20 12:20:08,448] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-20 12:20:08,448] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-20 12:20:08,448] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-20 12:20:08,448] [    INFO][0m - [0m
[32m[2022-09-20 12:20:08,451] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-20 12:20:08,451] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 12:20:08,451] [    INFO][0m -   Num Epochs = 20[0m
[32m[2022-09-20 12:20:08,451] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2022-09-20 12:20:08,451] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2022-09-20 12:20:08,451] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-20 12:20:08,451] [    INFO][0m -   Total optimization steps = 200.0[0m
[32m[2022-09-20 12:20:08,451] [    INFO][0m -   Total num train samples = 3200[0m
[32m[2022-09-20 12:20:12,889] [    INFO][0m - loss: 3.29170952, learning_rate: 2.8499999999999998e-05, global_step: 10, interval_runtime: 4.4368, interval_samples_per_second: 3.606, interval_steps_per_second: 2.254, epoch: 1.0[0m
[32m[2022-09-20 12:20:12,890] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:20:12,890] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 12:20:12,890] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:20:12,890] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:20:12,891] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 12:20:14,270] [    INFO][0m - eval_loss: 1.6108801364898682, eval_accuracy: 0.38125, eval_runtime: 1.3787, eval_samples_per_second: 116.05, eval_steps_per_second: 7.253, epoch: 1.0[0m
[32m[2022-09-20 12:20:14,270] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-10[0m
[32m[2022-09-20 12:20:14,270] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:20:17,507] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-10/tokenizer_config.json[0m
[32m[2022-09-20 12:20:17,508] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-10/special_tokens_map.json[0m
[32m[2022-09-20 12:20:27,150] [    INFO][0m - loss: 1.20918522, learning_rate: 2.7000000000000002e-05, global_step: 20, interval_runtime: 14.2611, interval_samples_per_second: 1.122, interval_steps_per_second: 0.701, epoch: 2.0[0m
[32m[2022-09-20 12:20:27,151] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:20:27,151] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 12:20:27,151] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:20:27,151] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:20:27,152] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 12:20:28,516] [    INFO][0m - eval_loss: 1.1716231107711792, eval_accuracy: 0.45, eval_runtime: 1.3646, eval_samples_per_second: 117.252, eval_steps_per_second: 7.328, epoch: 2.0[0m
[32m[2022-09-20 12:20:28,517] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-20[0m
[32m[2022-09-20 12:20:28,517] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:20:31,531] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-20/tokenizer_config.json[0m
[32m[2022-09-20 12:20:31,531] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-20/special_tokens_map.json[0m
[32m[2022-09-20 12:20:40,946] [    INFO][0m - loss: 0.79349089, learning_rate: 2.55e-05, global_step: 30, interval_runtime: 13.7961, interval_samples_per_second: 1.16, interval_steps_per_second: 0.725, epoch: 3.0[0m
[32m[2022-09-20 12:20:40,947] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:20:40,947] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 12:20:40,947] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:20:40,947] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:20:40,947] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 12:20:42,297] [    INFO][0m - eval_loss: 1.219562292098999, eval_accuracy: 0.49375, eval_runtime: 1.3491, eval_samples_per_second: 118.599, eval_steps_per_second: 7.412, epoch: 3.0[0m
[32m[2022-09-20 12:20:42,297] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-30[0m
[32m[2022-09-20 12:20:42,297] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:20:45,127] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-30/tokenizer_config.json[0m
[32m[2022-09-20 12:20:45,127] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-30/special_tokens_map.json[0m
[32m[2022-09-20 12:20:53,999] [    INFO][0m - loss: 0.41358171, learning_rate: 2.4e-05, global_step: 40, interval_runtime: 13.0527, interval_samples_per_second: 1.226, interval_steps_per_second: 0.766, epoch: 4.0[0m
[32m[2022-09-20 12:20:53,999] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:20:53,999] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 12:20:54,000] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:20:54,000] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:20:54,000] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 12:20:55,367] [    INFO][0m - eval_loss: 2.11881947517395, eval_accuracy: 0.45625, eval_runtime: 1.3671, eval_samples_per_second: 117.036, eval_steps_per_second: 7.315, epoch: 4.0[0m
[32m[2022-09-20 12:20:55,368] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-40[0m
[32m[2022-09-20 12:20:55,368] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:20:58,208] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-40/tokenizer_config.json[0m
[32m[2022-09-20 12:20:58,208] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-40/special_tokens_map.json[0m
[32m[2022-09-20 12:21:12,009] [    INFO][0m - loss: 0.14333928, learning_rate: 2.25e-05, global_step: 50, interval_runtime: 18.0101, interval_samples_per_second: 0.888, interval_steps_per_second: 0.555, epoch: 5.0[0m
[32m[2022-09-20 12:21:12,010] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:21:12,010] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 12:21:12,010] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:21:12,010] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:21:12,010] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 12:21:13,363] [    INFO][0m - eval_loss: 2.699300765991211, eval_accuracy: 0.4875, eval_runtime: 1.3531, eval_samples_per_second: 118.251, eval_steps_per_second: 7.391, epoch: 5.0[0m
[32m[2022-09-20 12:21:13,364] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-50[0m
[32m[2022-09-20 12:21:13,364] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:21:16,221] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-50/tokenizer_config.json[0m
[32m[2022-09-20 12:21:16,222] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-50/special_tokens_map.json[0m
[32m[2022-09-20 12:21:25,328] [    INFO][0m - loss: 0.15666505, learning_rate: 2.1e-05, global_step: 60, interval_runtime: 13.319, interval_samples_per_second: 1.201, interval_steps_per_second: 0.751, epoch: 6.0[0m
[32m[2022-09-20 12:21:25,329] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:21:25,329] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 12:21:25,330] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:21:25,330] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:21:25,330] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 12:21:26,682] [    INFO][0m - eval_loss: 2.625614881515503, eval_accuracy: 0.5375, eval_runtime: 1.3523, eval_samples_per_second: 118.317, eval_steps_per_second: 7.395, epoch: 6.0[0m
[32m[2022-09-20 12:21:26,683] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-60[0m
[32m[2022-09-20 12:21:26,683] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:21:29,443] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-60/tokenizer_config.json[0m
[32m[2022-09-20 12:21:29,443] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-60/special_tokens_map.json[0m
[32m[2022-09-20 12:21:38,436] [    INFO][0m - loss: 0.07027754, learning_rate: 1.95e-05, global_step: 70, interval_runtime: 13.1083, interval_samples_per_second: 1.221, interval_steps_per_second: 0.763, epoch: 7.0[0m
[32m[2022-09-20 12:21:38,437] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:21:38,437] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 12:21:38,437] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:21:38,437] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:21:38,437] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 12:21:39,803] [    INFO][0m - eval_loss: 3.1701667308807373, eval_accuracy: 0.59375, eval_runtime: 1.3651, eval_samples_per_second: 117.204, eval_steps_per_second: 7.325, epoch: 7.0[0m
[32m[2022-09-20 12:21:39,803] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-70[0m
[32m[2022-09-20 12:21:39,803] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:21:42,605] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-70/tokenizer_config.json[0m
[32m[2022-09-20 12:21:42,605] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-70/special_tokens_map.json[0m
[32m[2022-09-20 12:21:55,705] [    INFO][0m - loss: 0.00602172, learning_rate: 1.8e-05, global_step: 80, interval_runtime: 13.1941, interval_samples_per_second: 1.213, interval_steps_per_second: 0.758, epoch: 8.0[0m
[32m[2022-09-20 12:21:55,706] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:21:55,706] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 12:21:55,706] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:21:55,706] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:21:55,706] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 12:21:57,050] [    INFO][0m - eval_loss: 3.996123790740967, eval_accuracy: 0.51875, eval_runtime: 1.3433, eval_samples_per_second: 119.111, eval_steps_per_second: 7.444, epoch: 8.0[0m
[32m[2022-09-20 12:21:57,050] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-80[0m
[32m[2022-09-20 12:21:57,050] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:21:59,847] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-80/tokenizer_config.json[0m
[32m[2022-09-20 12:21:59,847] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-80/special_tokens_map.json[0m
[32m[2022-09-20 12:22:13,315] [    INFO][0m - loss: 0.05276814, learning_rate: 1.65e-05, global_step: 90, interval_runtime: 21.6843, interval_samples_per_second: 0.738, interval_steps_per_second: 0.461, epoch: 9.0[0m
[32m[2022-09-20 12:22:13,316] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:22:13,316] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 12:22:13,316] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:22:13,316] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:22:13,316] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 12:22:14,686] [    INFO][0m - eval_loss: 5.099034786224365, eval_accuracy: 0.575, eval_runtime: 1.369, eval_samples_per_second: 116.871, eval_steps_per_second: 7.304, epoch: 9.0[0m
[32m[2022-09-20 12:22:14,686] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-90[0m
[32m[2022-09-20 12:22:14,686] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:22:17,534] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-90/tokenizer_config.json[0m
[32m[2022-09-20 12:22:17,535] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-90/special_tokens_map.json[0m
[32m[2022-09-20 12:22:27,635] [    INFO][0m - loss: 0.00449259, learning_rate: 1.5e-05, global_step: 100, interval_runtime: 14.3188, interval_samples_per_second: 1.117, interval_steps_per_second: 0.698, epoch: 10.0[0m
[32m[2022-09-20 12:22:27,636] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:22:27,636] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 12:22:27,636] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:22:27,636] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:22:27,636] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 12:22:29,003] [    INFO][0m - eval_loss: 4.845981597900391, eval_accuracy: 0.51875, eval_runtime: 1.3665, eval_samples_per_second: 117.092, eval_steps_per_second: 7.318, epoch: 10.0[0m
[32m[2022-09-20 12:22:29,004] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-100[0m
[32m[2022-09-20 12:22:29,004] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:22:32,051] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-20 12:22:32,051] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-20 12:22:40,669] [    INFO][0m - loss: 0.00012116, learning_rate: 1.3500000000000001e-05, global_step: 110, interval_runtime: 13.0354, interval_samples_per_second: 1.227, interval_steps_per_second: 0.767, epoch: 11.0[0m
[32m[2022-09-20 12:22:40,670] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:22:40,670] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 12:22:40,670] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:22:40,670] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:22:40,670] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 12:22:42,028] [    INFO][0m - eval_loss: 5.092278480529785, eval_accuracy: 0.525, eval_runtime: 1.357, eval_samples_per_second: 117.905, eval_steps_per_second: 7.369, epoch: 11.0[0m
[32m[2022-09-20 12:22:43,868] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-110[0m
[32m[2022-09-20 12:22:43,868] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:22:46,734] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-110/tokenizer_config.json[0m
[32m[2022-09-20 12:22:46,735] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-110/special_tokens_map.json[0m
[32m[2022-09-20 12:22:55,829] [    INFO][0m - loss: 0.00033493, learning_rate: 1.2e-05, global_step: 120, interval_runtime: 15.1599, interval_samples_per_second: 1.055, interval_steps_per_second: 0.66, epoch: 12.0[0m
[32m[2022-09-20 12:22:55,830] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:22:55,830] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 12:22:55,830] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:22:55,830] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:22:55,830] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 12:22:57,191] [    INFO][0m - eval_loss: 4.941973686218262, eval_accuracy: 0.5625, eval_runtime: 1.3604, eval_samples_per_second: 117.613, eval_steps_per_second: 7.351, epoch: 12.0[0m
[32m[2022-09-20 12:22:57,192] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-120[0m
[32m[2022-09-20 12:22:57,192] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:22:59,952] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-120/tokenizer_config.json[0m
[32m[2022-09-20 12:22:59,952] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-120/special_tokens_map.json[0m
[32m[2022-09-20 12:23:08,659] [    INFO][0m - loss: 0.0007803, learning_rate: 1.05e-05, global_step: 130, interval_runtime: 12.8297, interval_samples_per_second: 1.247, interval_steps_per_second: 0.779, epoch: 13.0[0m
[32m[2022-09-20 12:23:08,659] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:23:08,660] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 12:23:08,660] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:23:08,660] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:23:08,660] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 12:23:10,026] [    INFO][0m - eval_loss: 5.4405083656311035, eval_accuracy: 0.4875, eval_runtime: 1.3656, eval_samples_per_second: 117.166, eval_steps_per_second: 7.323, epoch: 13.0[0m
[32m[2022-09-20 12:23:10,026] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-130[0m
[32m[2022-09-20 12:23:10,026] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:23:12,908] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-130/tokenizer_config.json[0m
[32m[2022-09-20 12:23:12,908] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-130/special_tokens_map.json[0m
[32m[2022-09-20 12:23:21,441] [    INFO][0m - loss: 0.00015581, learning_rate: 9e-06, global_step: 140, interval_runtime: 12.7818, interval_samples_per_second: 1.252, interval_steps_per_second: 0.782, epoch: 14.0[0m
[32m[2022-09-20 12:23:25,040] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:23:25,040] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 12:23:25,040] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:23:25,040] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:23:25,040] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 12:23:26,397] [    INFO][0m - eval_loss: 5.354302406311035, eval_accuracy: 0.525, eval_runtime: 1.3565, eval_samples_per_second: 117.949, eval_steps_per_second: 7.372, epoch: 14.0[0m
[32m[2022-09-20 12:23:26,398] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-140[0m
[32m[2022-09-20 12:23:26,398] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:23:29,413] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-140/tokenizer_config.json[0m
[32m[2022-09-20 12:23:29,413] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-140/special_tokens_map.json[0m
[32m[2022-09-20 12:23:38,699] [    INFO][0m - loss: 8.19e-06, learning_rate: 7.5e-06, global_step: 150, interval_runtime: 17.2581, interval_samples_per_second: 0.927, interval_steps_per_second: 0.579, epoch: 15.0[0m
[32m[2022-09-20 12:23:38,699] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:23:38,700] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 12:23:38,700] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:23:38,700] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:23:38,700] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 12:23:40,065] [    INFO][0m - eval_loss: 5.7479939460754395, eval_accuracy: 0.55625, eval_runtime: 1.3645, eval_samples_per_second: 117.259, eval_steps_per_second: 7.329, epoch: 15.0[0m
[32m[2022-09-20 12:23:40,065] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-150[0m
[32m[2022-09-20 12:23:40,066] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:23:42,754] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-150/tokenizer_config.json[0m
[32m[2022-09-20 12:23:42,755] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-150/special_tokens_map.json[0m
[32m[2022-09-20 12:23:51,224] [    INFO][0m - loss: 1.268e-05, learning_rate: 6e-06, global_step: 160, interval_runtime: 12.5255, interval_samples_per_second: 1.277, interval_steps_per_second: 0.798, epoch: 16.0[0m
[32m[2022-09-20 12:23:51,225] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:23:51,225] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 12:23:51,225] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:23:51,225] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:23:51,225] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 12:23:52,581] [    INFO][0m - eval_loss: 5.882090091705322, eval_accuracy: 0.5625, eval_runtime: 1.3553, eval_samples_per_second: 118.058, eval_steps_per_second: 7.379, epoch: 16.0[0m
[32m[2022-09-20 12:23:52,581] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-160[0m
[32m[2022-09-20 12:23:52,581] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:23:55,187] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-160/tokenizer_config.json[0m
[32m[2022-09-20 12:23:55,188] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-160/special_tokens_map.json[0m
[32m[2022-09-20 12:24:03,781] [    INFO][0m - loss: 3.081e-05, learning_rate: 4.5e-06, global_step: 170, interval_runtime: 12.5573, interval_samples_per_second: 1.274, interval_steps_per_second: 0.796, epoch: 17.0[0m
[32m[2022-09-20 12:24:03,782] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:24:03,782] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 12:24:03,782] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:24:03,782] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:24:03,783] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 12:24:05,123] [    INFO][0m - eval_loss: 5.852601528167725, eval_accuracy: 0.55625, eval_runtime: 1.3403, eval_samples_per_second: 119.374, eval_steps_per_second: 7.461, epoch: 17.0[0m
[32m[2022-09-20 12:24:07,289] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-170[0m
[32m[2022-09-20 12:24:07,290] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:24:10,206] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-170/tokenizer_config.json[0m
[32m[2022-09-20 12:24:10,206] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-170/special_tokens_map.json[0m
[32m[2022-09-20 12:24:18,794] [    INFO][0m - loss: 4.777e-05, learning_rate: 3e-06, global_step: 180, interval_runtime: 15.0127, interval_samples_per_second: 1.066, interval_steps_per_second: 0.666, epoch: 18.0[0m
[32m[2022-09-20 12:24:18,795] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:24:18,795] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 12:24:18,795] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:24:18,795] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:24:18,795] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 12:24:20,159] [    INFO][0m - eval_loss: 5.7878737449646, eval_accuracy: 0.55625, eval_runtime: 1.363, eval_samples_per_second: 117.392, eval_steps_per_second: 7.337, epoch: 18.0[0m
[32m[2022-09-20 12:24:22,834] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-180[0m
[32m[2022-09-20 12:24:22,834] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:24:26,730] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-180/tokenizer_config.json[0m
[32m[2022-09-20 12:24:26,730] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-180/special_tokens_map.json[0m
[32m[2022-09-20 12:24:35,634] [    INFO][0m - loss: 5.42e-06, learning_rate: 1.5e-06, global_step: 190, interval_runtime: 16.8403, interval_samples_per_second: 0.95, interval_steps_per_second: 0.594, epoch: 19.0[0m
[32m[2022-09-20 12:24:35,635] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:24:35,635] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 12:24:35,636] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:24:35,636] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:24:35,636] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 12:24:36,999] [    INFO][0m - eval_loss: 5.755780220031738, eval_accuracy: 0.55, eval_runtime: 1.3625, eval_samples_per_second: 117.429, eval_steps_per_second: 7.339, epoch: 19.0[0m
[32m[2022-09-20 12:24:36,999] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-190[0m
[32m[2022-09-20 12:24:36,999] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:24:40,102] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-190/tokenizer_config.json[0m
[32m[2022-09-20 12:24:40,103] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-190/special_tokens_map.json[0m
[32m[2022-09-20 12:24:48,429] [    INFO][0m - loss: 5.32e-06, learning_rate: 0.0, global_step: 200, interval_runtime: 12.7949, interval_samples_per_second: 1.25, interval_steps_per_second: 0.782, epoch: 20.0[0m
[32m[2022-09-20 12:24:48,430] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:24:48,430] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 12:24:48,430] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:24:48,430] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:24:48,430] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 12:24:49,782] [    INFO][0m - eval_loss: 5.752390384674072, eval_accuracy: 0.54375, eval_runtime: 1.3515, eval_samples_per_second: 118.383, eval_steps_per_second: 7.399, epoch: 20.0[0m
[32m[2022-09-20 12:24:49,783] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-200[0m
[32m[2022-09-20 12:24:49,783] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:24:52,375] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-200/tokenizer_config.json[0m
[32m[2022-09-20 12:24:52,375] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-200/special_tokens_map.json[0m
[32m[2022-09-20 12:24:57,321] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-20 12:24:57,322] [    INFO][0m - Loading best model from ./checkpoints_ocnli/checkpoint-70 (score: 0.59375).[0m
[32m[2022-09-20 12:24:58,989] [    INFO][0m - train_runtime: 290.5362, train_samples_per_second: 11.014, train_steps_per_second: 0.688, train_loss: 0.3071517014937308, epoch: 20.0[0m
[32m[2022-09-20 12:24:58,990] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/[0m
[32m[2022-09-20 12:24:58,991] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:25:01,441] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/tokenizer_config.json[0m
[32m[2022-09-20 12:25:01,442] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/special_tokens_map.json[0m
[32m[2022-09-20 12:25:01,443] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-20 12:25:01,443] [    INFO][0m -   epoch                    =       20.0[0m
[32m[2022-09-20 12:25:01,443] [    INFO][0m -   train_loss               =     0.3072[0m
[32m[2022-09-20 12:25:01,443] [    INFO][0m -   train_runtime            = 0:04:50.53[0m
[32m[2022-09-20 12:25:01,443] [    INFO][0m -   train_samples_per_second =     11.014[0m
[32m[2022-09-20 12:25:01,443] [    INFO][0m -   train_steps_per_second   =      0.688[0m
[32m[2022-09-20 12:25:01,446] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-20 12:25:01,446] [    INFO][0m -   Num examples = 2520[0m
[32m[2022-09-20 12:25:01,446] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:25:01,446] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:25:01,446] [    INFO][0m -   Total prediction steps = 158[0m
[32m[2022-09-20 12:25:23,837] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-20 12:25:23,837] [    INFO][0m -   test_accuracy           =     0.5873[0m
[32m[2022-09-20 12:25:23,837] [    INFO][0m -   test_loss               =     2.7945[0m
[32m[2022-09-20 12:25:23,838] [    INFO][0m -   test_runtime            = 0:00:22.39[0m
[32m[2022-09-20 12:25:23,838] [    INFO][0m -   test_samples_per_second =    112.546[0m
[32m[2022-09-20 12:25:23,838] [    INFO][0m -   test_steps_per_second   =      7.056[0m
[32m[2022-09-20 12:25:23,838] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-20 12:25:23,838] [    INFO][0m -   Num examples = 3000[0m
[32m[2022-09-20 12:25:23,838] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:25:23,838] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:25:23,838] [    INFO][0m -   Total prediction steps = 188[0m
[32m[2022-09-20 12:25:53,290] [    INFO][0m - Predictions for ocnlif saved to ./fewclue_submit_examples.[0m
{
  "labels": 2,
  "text_a": "\u4e03\u4e94\u671f\u95f4\u5f00\u59cb,\u56fd\u5bb6\u53c8\u6295\u8d44\u5c06\u6b66\u6c49\u5e02\u533a\u7684\u90e8\u5206\u571f\u5824\u6539\u5efa\u4e3a\u94a2\u7b4b\u6ce5\u51dd\u571f\u9632\u6c34\u5899",
  "text_b": "\u516b\u4e94\u671f\u95f4\u4f1a\u628a\u5269\u4e0b\u7684\u571f\u5824\u90fd\u6539\u5efa\u5b8c",
  "uid": 0
}

Prediction done.
 
==========
bustm
==========
 
[32m[2022-09-20 12:26:10,192] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-20 12:26:10,193] [    INFO][0m - ============================================================[0m
[32m[2022-09-20 12:26:10,193] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-20 12:26:10,193] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-20 12:26:10,193] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-20 12:26:10,193] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-20 12:26:10,193] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-20 12:26:10,193] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-20 12:26:10,193] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-20 12:26:10,193] [    INFO][0m - [0m
[32m[2022-09-20 12:26:10,193] [    INFO][0m - ============================================================[0m
[32m[2022-09-20 12:26:10,193] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-20 12:26:10,193] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-20 12:26:10,194] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-20 12:26:10,194] [    INFO][0m - pretrained                    :/ssd2/wanghuijuan03/data/zero-shot/checkpoints_09191451/checkpoint-43000/model_state.pdparams[0m
[32m[2022-09-20 12:26:10,194] [    INFO][0m - prompt                        :‚Äú{'text':'text_a'}‚ÄùÂíå‚Äú{'text':'text_b'}‚ÄùÊèèËø∞ÁöÑÊòØ{'mask'}{'mask'}ÁöÑ‰∫ãÊÉÖ„ÄÇ[0m
[32m[2022-09-20 12:26:10,194] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-20 12:26:10,194] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-20 12:26:10,194] [    INFO][0m - task_name                     :bustm[0m
[32m[2022-09-20 12:26:10,194] [    INFO][0m - [0m
[32m[2022-09-20 12:26:10,194] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0920 12:26:10.196130  6174 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0920 12:26:10.200206  6174 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-20 12:26:14,869] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-20 12:26:14,880] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-20 12:26:14,881] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-20 12:26:14,881] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùÂíå‚Äú'}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': '‚ÄùÊèèËø∞ÁöÑÊòØ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'ÁöÑ‰∫ãÊÉÖ„ÄÇ'}][0m
[32m[2022-09-20 12:26:16,088] [    INFO][0m - ============================================================[0m
[32m[2022-09-20 12:26:16,088] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-20 12:26:16,088] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-20 12:26:16,088] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-20 12:26:16,088] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-20 12:26:16,088] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-20 12:26:16,088] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-20 12:26:16,088] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-20 12:26:16,089] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-20 12:26:16,089] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-20 12:26:16,089] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-20 12:26:16,089] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-20 12:26:16,089] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-20 12:26:16,089] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-20 12:26:16,089] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-20 12:26:16,089] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-20 12:26:16,089] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-20 12:26:16,089] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-20 12:26:16,089] [    INFO][0m - eval_batch_size               :16[0m
[32m[2022-09-20 12:26:16,089] [    INFO][0m - eval_steps                    :200[0m
[32m[2022-09-20 12:26:16,089] [    INFO][0m - evaluation_strategy           :IntervalStrategy.EPOCH[0m
[32m[2022-09-20 12:26:16,090] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-20 12:26:16,090] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-20 12:26:16,090] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-20 12:26:16,090] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-20 12:26:16,090] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-20 12:26:16,090] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-20 12:26:16,090] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-20 12:26:16,090] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-20 12:26:16,090] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-20 12:26:16,090] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-09-20 12:26:16,090] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-20 12:26:16,090] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-20 12:26:16,090] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-20 12:26:16,091] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-20 12:26:16,091] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-20 12:26:16,091] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-20 12:26:16,091] [    INFO][0m - logging_dir                   :./checkpoints_bustm/runs/Sep20_12-26-10_instance-3bwob41y-01[0m
[32m[2022-09-20 12:26:16,091] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-20 12:26:16,091] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-20 12:26:16,091] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-20 12:26:16,091] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-20 12:26:16,091] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-20 12:26:16,091] [    INFO][0m - max_seq_length                :64[0m
[32m[2022-09-20 12:26:16,091] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-20 12:26:16,091] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-20 12:26:16,091] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-20 12:26:16,091] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-20 12:26:16,092] [    INFO][0m - num_train_epochs              :20.0[0m
[32m[2022-09-20 12:26:16,092] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-20 12:26:16,092] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-20 12:26:16,092] [    INFO][0m - output_dir                    :./checkpoints_bustm/[0m
[32m[2022-09-20 12:26:16,092] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-20 12:26:16,092] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-20 12:26:16,092] [    INFO][0m - per_device_eval_batch_size    :16[0m
[32m[2022-09-20 12:26:16,092] [    INFO][0m - per_device_train_batch_size   :16[0m
[32m[2022-09-20 12:26:16,092] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-20 12:26:16,092] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-20 12:26:16,092] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-20 12:26:16,092] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-09-20 12:26:16,092] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-20 12:26:16,092] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-20 12:26:16,093] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-20 12:26:16,093] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-20 12:26:16,093] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-20 12:26:16,093] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-20 12:26:16,093] [    INFO][0m - run_name                      :./checkpoints_bustm/[0m
[32m[2022-09-20 12:26:16,093] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-20 12:26:16,093] [    INFO][0m - save_steps                    :200[0m
[32m[2022-09-20 12:26:16,093] [    INFO][0m - save_strategy                 :IntervalStrategy.EPOCH[0m
[32m[2022-09-20 12:26:16,093] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-20 12:26:16,093] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-20 12:26:16,093] [    INFO][0m - seed                          :42[0m
[32m[2022-09-20 12:26:16,093] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-20 12:26:16,093] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-20 12:26:16,094] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-20 12:26:16,094] [    INFO][0m - train_batch_size              :16[0m
[32m[2022-09-20 12:26:16,094] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-20 12:26:16,094] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-20 12:26:16,094] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-20 12:26:16,094] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-20 12:26:16,094] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-20 12:26:16,094] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-20 12:26:16,094] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-20 12:26:16,094] [    INFO][0m - [0m
[32m[2022-09-20 12:26:16,097] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-20 12:26:16,097] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 12:26:16,097] [    INFO][0m -   Num Epochs = 20[0m
[32m[2022-09-20 12:26:16,097] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2022-09-20 12:26:16,097] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2022-09-20 12:26:16,097] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-20 12:26:16,097] [    INFO][0m -   Total optimization steps = 200.0[0m
[32m[2022-09-20 12:26:16,097] [    INFO][0m -   Total num train samples = 3200[0m
[32m[2022-09-20 12:26:19,084] [    INFO][0m - loss: 0.6002862, learning_rate: 2.8499999999999998e-05, global_step: 10, interval_runtime: 2.9863, interval_samples_per_second: 5.358, interval_steps_per_second: 3.349, epoch: 1.0[0m
[32m[2022-09-20 12:26:19,085] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:26:19,085] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 12:26:19,085] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:26:19,086] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:26:19,086] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 12:26:19,854] [    INFO][0m - eval_loss: 0.34200242161750793, eval_accuracy: 0.575, eval_runtime: 0.7686, eval_samples_per_second: 208.177, eval_steps_per_second: 13.011, epoch: 1.0[0m
[32m[2022-09-20 12:26:19,855] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-10[0m
[32m[2022-09-20 12:26:19,855] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:26:22,785] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-10/tokenizer_config.json[0m
[32m[2022-09-20 12:26:22,785] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-10/special_tokens_map.json[0m
[32m[2022-09-20 12:26:30,908] [    INFO][0m - loss: 0.41170602, learning_rate: 2.7000000000000002e-05, global_step: 20, interval_runtime: 11.8235, interval_samples_per_second: 1.353, interval_steps_per_second: 0.846, epoch: 2.0[0m
[32m[2022-09-20 12:26:30,910] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:26:30,910] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 12:26:30,910] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:26:30,910] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:26:30,910] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 12:26:31,704] [    INFO][0m - eval_loss: 0.47383221983909607, eval_accuracy: 0.5125, eval_runtime: 0.794, eval_samples_per_second: 201.521, eval_steps_per_second: 12.595, epoch: 2.0[0m
[32m[2022-09-20 12:26:31,705] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-20[0m
[32m[2022-09-20 12:26:31,705] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:26:34,631] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-20/tokenizer_config.json[0m
[32m[2022-09-20 12:26:34,631] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-20/special_tokens_map.json[0m
[32m[2022-09-20 12:26:43,300] [    INFO][0m - loss: 0.38181648, learning_rate: 2.55e-05, global_step: 30, interval_runtime: 12.3922, interval_samples_per_second: 1.291, interval_steps_per_second: 0.807, epoch: 3.0[0m
[32m[2022-09-20 12:26:43,301] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:26:43,301] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 12:26:43,301] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:26:43,301] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:26:43,301] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 12:26:44,112] [    INFO][0m - eval_loss: 0.3127591013908386, eval_accuracy: 0.59375, eval_runtime: 0.8096, eval_samples_per_second: 197.627, eval_steps_per_second: 12.352, epoch: 3.0[0m
[32m[2022-09-20 12:26:44,112] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-30[0m
[32m[2022-09-20 12:26:44,112] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:26:47,265] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-30/tokenizer_config.json[0m
[32m[2022-09-20 12:26:47,266] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-30/special_tokens_map.json[0m
[32m[2022-09-20 12:26:55,566] [    INFO][0m - loss: 0.22794352, learning_rate: 2.4e-05, global_step: 40, interval_runtime: 12.2661, interval_samples_per_second: 1.304, interval_steps_per_second: 0.815, epoch: 4.0[0m
[32m[2022-09-20 12:26:55,567] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:26:55,567] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 12:26:55,568] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:26:55,568] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:26:55,568] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 12:26:56,417] [    INFO][0m - eval_loss: 0.41429704427719116, eval_accuracy: 0.6125, eval_runtime: 0.8485, eval_samples_per_second: 188.578, eval_steps_per_second: 11.786, epoch: 4.0[0m
[32m[2022-09-20 12:26:56,418] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-40[0m
[32m[2022-09-20 12:26:56,418] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:26:59,836] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-40/tokenizer_config.json[0m
[32m[2022-09-20 12:26:59,934] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-40/special_tokens_map.json[0m
[32m[2022-09-20 12:27:08,544] [    INFO][0m - loss: 0.15981089, learning_rate: 2.25e-05, global_step: 50, interval_runtime: 12.9779, interval_samples_per_second: 1.233, interval_steps_per_second: 0.771, epoch: 5.0[0m
[32m[2022-09-20 12:27:08,545] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:27:08,546] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 12:27:08,546] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:27:08,546] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:27:08,546] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 12:27:09,364] [    INFO][0m - eval_loss: 0.5752121210098267, eval_accuracy: 0.675, eval_runtime: 0.8178, eval_samples_per_second: 195.64, eval_steps_per_second: 12.227, epoch: 5.0[0m
[32m[2022-09-20 12:27:11,679] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-50[0m
[32m[2022-09-20 12:27:11,679] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:27:15,541] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-50/tokenizer_config.json[0m
[32m[2022-09-20 12:27:15,541] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-50/special_tokens_map.json[0m
[32m[2022-09-20 12:27:28,900] [    INFO][0m - loss: 0.05667922, learning_rate: 2.1e-05, global_step: 60, interval_runtime: 20.3554, interval_samples_per_second: 0.786, interval_steps_per_second: 0.491, epoch: 6.0[0m
[32m[2022-09-20 12:27:28,901] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:27:28,901] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 12:27:28,901] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:27:28,901] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:27:28,901] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 12:27:29,694] [    INFO][0m - eval_loss: 0.8721100091934204, eval_accuracy: 0.7125, eval_runtime: 0.7922, eval_samples_per_second: 201.962, eval_steps_per_second: 12.623, epoch: 6.0[0m
[32m[2022-09-20 12:27:29,695] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-60[0m
[32m[2022-09-20 12:27:29,695] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:27:32,628] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-60/tokenizer_config.json[0m
[32m[2022-09-20 12:27:32,629] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-60/special_tokens_map.json[0m
[32m[2022-09-20 12:27:40,360] [    INFO][0m - loss: 0.00417625, learning_rate: 1.95e-05, global_step: 70, interval_runtime: 11.4607, interval_samples_per_second: 1.396, interval_steps_per_second: 0.873, epoch: 7.0[0m
[32m[2022-09-20 12:27:40,361] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:27:40,361] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 12:27:40,361] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:27:40,362] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:27:40,362] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 12:27:41,162] [    INFO][0m - eval_loss: 1.4099489450454712, eval_accuracy: 0.66875, eval_runtime: 0.7998, eval_samples_per_second: 200.043, eval_steps_per_second: 12.503, epoch: 7.0[0m
[32m[2022-09-20 12:27:41,162] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-70[0m
[32m[2022-09-20 12:27:41,162] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:27:44,087] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-70/tokenizer_config.json[0m
[32m[2022-09-20 12:27:44,087] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-70/special_tokens_map.json[0m
[32m[2022-09-20 12:27:55,442] [    INFO][0m - loss: 0.00042522, learning_rate: 1.8e-05, global_step: 80, interval_runtime: 15.0817, interval_samples_per_second: 1.061, interval_steps_per_second: 0.663, epoch: 8.0[0m
[32m[2022-09-20 12:27:55,443] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:27:55,443] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 12:27:55,443] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:27:55,443] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:27:55,443] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 12:27:56,223] [    INFO][0m - eval_loss: 1.8206899166107178, eval_accuracy: 0.66875, eval_runtime: 0.7799, eval_samples_per_second: 205.146, eval_steps_per_second: 12.822, epoch: 8.0[0m
[32m[2022-09-20 12:27:56,224] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-80[0m
[32m[2022-09-20 12:27:56,224] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:27:59,110] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-80/tokenizer_config.json[0m
[32m[2022-09-20 12:27:59,110] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-80/special_tokens_map.json[0m
[32m[2022-09-20 12:28:06,672] [    INFO][0m - loss: 0.03950934, learning_rate: 1.65e-05, global_step: 90, interval_runtime: 11.2301, interval_samples_per_second: 1.425, interval_steps_per_second: 0.89, epoch: 9.0[0m
[32m[2022-09-20 12:28:06,673] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:28:06,673] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 12:28:06,673] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:28:06,673] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:28:06,673] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 12:28:07,436] [    INFO][0m - eval_loss: 1.8775326013565063, eval_accuracy: 0.68125, eval_runtime: 0.763, eval_samples_per_second: 209.707, eval_steps_per_second: 13.107, epoch: 9.0[0m
[32m[2022-09-20 12:28:07,437] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-90[0m
[32m[2022-09-20 12:28:07,437] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:28:11,611] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-90/tokenizer_config.json[0m
[32m[2022-09-20 12:28:11,612] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-90/special_tokens_map.json[0m
[32m[2022-09-20 12:28:18,843] [    INFO][0m - loss: 0.00058842, learning_rate: 1.5e-05, global_step: 100, interval_runtime: 12.1715, interval_samples_per_second: 1.315, interval_steps_per_second: 0.822, epoch: 10.0[0m
[32m[2022-09-20 12:28:18,844] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:28:18,844] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 12:28:18,844] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:28:18,844] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:28:18,844] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 12:28:19,619] [    INFO][0m - eval_loss: 1.7269290685653687, eval_accuracy: 0.68125, eval_runtime: 0.7743, eval_samples_per_second: 206.643, eval_steps_per_second: 12.915, epoch: 10.0[0m
[32m[2022-09-20 12:28:19,620] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-100[0m
[32m[2022-09-20 12:28:19,620] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:28:22,350] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-20 12:28:22,350] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-20 12:28:30,118] [    INFO][0m - loss: 0.02078846, learning_rate: 1.3500000000000001e-05, global_step: 110, interval_runtime: 11.2749, interval_samples_per_second: 1.419, interval_steps_per_second: 0.887, epoch: 11.0[0m
[32m[2022-09-20 12:28:30,119] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:28:30,119] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 12:28:30,120] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:28:30,120] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:28:30,120] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 12:28:30,902] [    INFO][0m - eval_loss: 1.7764079570770264, eval_accuracy: 0.68125, eval_runtime: 0.782, eval_samples_per_second: 204.602, eval_steps_per_second: 12.788, epoch: 11.0[0m
[32m[2022-09-20 12:28:30,902] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-110[0m
[32m[2022-09-20 12:28:30,902] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:28:33,623] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-110/tokenizer_config.json[0m
[32m[2022-09-20 12:28:33,624] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-110/special_tokens_map.json[0m
[32m[2022-09-20 12:28:42,688] [    INFO][0m - loss: 2.397e-05, learning_rate: 1.2e-05, global_step: 120, interval_runtime: 12.5699, interval_samples_per_second: 1.273, interval_steps_per_second: 0.796, epoch: 12.0[0m
[32m[2022-09-20 12:28:42,689] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:28:42,689] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 12:28:42,689] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:28:42,689] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:28:42,689] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 12:28:43,443] [    INFO][0m - eval_loss: 1.9136745929718018, eval_accuracy: 0.6375, eval_runtime: 0.7538, eval_samples_per_second: 212.245, eval_steps_per_second: 13.265, epoch: 12.0[0m
[32m[2022-09-20 12:28:43,444] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-120[0m
[32m[2022-09-20 12:28:43,444] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:28:46,027] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-120/tokenizer_config.json[0m
[32m[2022-09-20 12:28:46,028] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-120/special_tokens_map.json[0m
[32m[2022-09-20 12:28:53,622] [    INFO][0m - loss: 0.00014421, learning_rate: 1.05e-05, global_step: 130, interval_runtime: 10.9332, interval_samples_per_second: 1.463, interval_steps_per_second: 0.915, epoch: 13.0[0m
[32m[2022-09-20 12:28:53,622] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:28:53,623] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 12:28:53,623] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:28:53,623] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:28:53,623] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 12:28:56,345] [    INFO][0m - eval_loss: 1.988494634628296, eval_accuracy: 0.675, eval_runtime: 0.7723, eval_samples_per_second: 207.16, eval_steps_per_second: 12.948, epoch: 13.0[0m
[32m[2022-09-20 12:28:56,345] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-130[0m
[32m[2022-09-20 12:28:56,346] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:28:59,272] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-130/tokenizer_config.json[0m
[32m[2022-09-20 12:28:59,272] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-130/special_tokens_map.json[0m
[32m[2022-09-20 12:29:07,741] [    INFO][0m - loss: 0.03115759, learning_rate: 9e-06, global_step: 140, interval_runtime: 14.1193, interval_samples_per_second: 1.133, interval_steps_per_second: 0.708, epoch: 14.0[0m
[32m[2022-09-20 12:29:07,742] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:29:07,742] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 12:29:07,742] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:29:07,742] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:29:07,742] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 12:29:08,508] [    INFO][0m - eval_loss: 1.8984596729278564, eval_accuracy: 0.61875, eval_runtime: 0.7664, eval_samples_per_second: 208.78, eval_steps_per_second: 13.049, epoch: 14.0[0m
[32m[2022-09-20 12:29:08,509] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-140[0m
[32m[2022-09-20 12:29:08,509] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:29:11,329] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-140/tokenizer_config.json[0m
[32m[2022-09-20 12:29:11,330] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-140/special_tokens_map.json[0m
[32m[2022-09-20 12:29:18,309] [    INFO][0m - loss: 0.00046304, learning_rate: 7.5e-06, global_step: 150, interval_runtime: 10.5677, interval_samples_per_second: 1.514, interval_steps_per_second: 0.946, epoch: 15.0[0m
[32m[2022-09-20 12:29:18,309] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:29:18,309] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 12:29:18,310] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:29:18,310] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:29:18,310] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 12:29:19,067] [    INFO][0m - eval_loss: 1.6971502304077148, eval_accuracy: 0.6625, eval_runtime: 0.7568, eval_samples_per_second: 211.42, eval_steps_per_second: 13.214, epoch: 15.0[0m
[32m[2022-09-20 12:29:19,897] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-150[0m
[32m[2022-09-20 12:29:19,897] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:29:22,496] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-150/tokenizer_config.json[0m
[32m[2022-09-20 12:29:22,497] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-150/special_tokens_map.json[0m
[32m[2022-09-20 12:29:29,751] [    INFO][0m - loss: 0.00043021, learning_rate: 6e-06, global_step: 160, interval_runtime: 11.4424, interval_samples_per_second: 1.398, interval_steps_per_second: 0.874, epoch: 16.0[0m
[32m[2022-09-20 12:29:29,752] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:29:29,752] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 12:29:29,752] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:29:29,752] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:29:29,752] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 12:29:30,500] [    INFO][0m - eval_loss: 1.8128191232681274, eval_accuracy: 0.65625, eval_runtime: 0.748, eval_samples_per_second: 213.897, eval_steps_per_second: 13.369, epoch: 16.0[0m
[32m[2022-09-20 12:29:30,501] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-160[0m
[32m[2022-09-20 12:29:30,501] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:29:33,042] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-160/tokenizer_config.json[0m
[32m[2022-09-20 12:29:33,042] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-160/special_tokens_map.json[0m
[32m[2022-09-20 12:29:40,012] [    INFO][0m - loss: 3.772e-05, learning_rate: 4.5e-06, global_step: 170, interval_runtime: 10.2611, interval_samples_per_second: 1.559, interval_steps_per_second: 0.975, epoch: 17.0[0m
[32m[2022-09-20 12:29:40,012] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:29:40,013] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 12:29:40,013] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:29:40,013] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:29:40,013] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 12:29:40,787] [    INFO][0m - eval_loss: 1.816469430923462, eval_accuracy: 0.6625, eval_runtime: 0.7737, eval_samples_per_second: 206.807, eval_steps_per_second: 12.925, epoch: 17.0[0m
[32m[2022-09-20 12:29:40,787] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-170[0m
[32m[2022-09-20 12:29:40,787] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:29:43,426] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-170/tokenizer_config.json[0m
[32m[2022-09-20 12:29:43,426] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-170/special_tokens_map.json[0m
[32m[2022-09-20 12:29:52,434] [    INFO][0m - loss: 4.63e-06, learning_rate: 3e-06, global_step: 180, interval_runtime: 12.4226, interval_samples_per_second: 1.288, interval_steps_per_second: 0.805, epoch: 18.0[0m
[32m[2022-09-20 12:29:52,435] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:29:52,435] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 12:29:52,435] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:29:52,435] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:29:52,435] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 12:29:53,186] [    INFO][0m - eval_loss: 1.8327938318252563, eval_accuracy: 0.65625, eval_runtime: 0.7506, eval_samples_per_second: 213.149, eval_steps_per_second: 13.322, epoch: 18.0[0m
[32m[2022-09-20 12:29:53,187] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-180[0m
[32m[2022-09-20 12:29:53,187] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:29:58,376] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-180/tokenizer_config.json[0m
[32m[2022-09-20 12:29:58,377] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-180/special_tokens_map.json[0m
[32m[2022-09-20 12:30:06,166] [    INFO][0m - loss: 4.46e-06, learning_rate: 1.5e-06, global_step: 190, interval_runtime: 13.7315, interval_samples_per_second: 1.165, interval_steps_per_second: 0.728, epoch: 19.0[0m
[32m[2022-09-20 12:30:06,167] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:30:06,167] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 12:30:06,167] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:30:06,167] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:30:06,167] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 12:30:06,940] [    INFO][0m - eval_loss: 1.8381016254425049, eval_accuracy: 0.65625, eval_runtime: 0.7728, eval_samples_per_second: 207.042, eval_steps_per_second: 12.94, epoch: 19.0[0m
[32m[2022-09-20 12:30:06,941] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-190[0m
[32m[2022-09-20 12:30:06,941] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:30:14,943] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-190/tokenizer_config.json[0m
[32m[2022-09-20 12:30:14,943] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-190/special_tokens_map.json[0m
[32m[2022-09-20 12:30:22,707] [    INFO][0m - loss: 3.63e-06, learning_rate: 0.0, global_step: 200, interval_runtime: 16.5409, interval_samples_per_second: 0.967, interval_steps_per_second: 0.605, epoch: 20.0[0m
[32m[2022-09-20 12:30:22,708] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:30:22,708] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 12:30:22,708] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:30:22,708] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:30:22,708] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 12:30:23,474] [    INFO][0m - eval_loss: 1.8392632007598877, eval_accuracy: 0.65625, eval_runtime: 0.7659, eval_samples_per_second: 208.91, eval_steps_per_second: 13.057, epoch: 20.0[0m
[32m[2022-09-20 12:30:26,612] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-200[0m
[32m[2022-09-20 12:30:26,612] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:30:29,477] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-200/tokenizer_config.json[0m
[32m[2022-09-20 12:30:29,478] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-200/special_tokens_map.json[0m
[32m[2022-09-20 12:30:34,640] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-20 12:30:34,641] [    INFO][0m - Loading best model from ./checkpoints_bustm/checkpoint-60 (score: 0.7125).[0m
[32m[2022-09-20 12:30:36,515] [    INFO][0m - train_runtime: 260.4171, train_samples_per_second: 12.288, train_steps_per_second: 0.768, train_loss: 0.0967999736262027, epoch: 20.0[0m
[32m[2022-09-20 12:30:36,570] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/[0m
[32m[2022-09-20 12:30:36,570] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:30:39,373] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/tokenizer_config.json[0m
[32m[2022-09-20 12:30:39,373] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/special_tokens_map.json[0m
[32m[2022-09-20 12:30:39,374] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-20 12:30:39,375] [    INFO][0m -   epoch                    =       20.0[0m
[32m[2022-09-20 12:30:39,375] [    INFO][0m -   train_loss               =     0.0968[0m
[32m[2022-09-20 12:30:39,375] [    INFO][0m -   train_runtime            = 0:04:20.41[0m
[32m[2022-09-20 12:30:39,375] [    INFO][0m -   train_samples_per_second =     12.288[0m
[32m[2022-09-20 12:30:39,375] [    INFO][0m -   train_steps_per_second   =      0.768[0m
[32m[2022-09-20 12:30:39,377] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-20 12:30:39,377] [    INFO][0m -   Num examples = 1772[0m
[32m[2022-09-20 12:30:39,378] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:30:39,378] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:30:39,378] [    INFO][0m -   Total prediction steps = 111[0m
[32m[2022-09-20 12:30:48,726] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-20 12:30:48,727] [    INFO][0m -   test_accuracy           =     0.6862[0m
[32m[2022-09-20 12:30:48,727] [    INFO][0m -   test_loss               =     0.9302[0m
[32m[2022-09-20 12:30:48,727] [    INFO][0m -   test_runtime            = 0:00:09.34[0m
[32m[2022-09-20 12:30:48,727] [    INFO][0m -   test_samples_per_second =    189.538[0m
[32m[2022-09-20 12:30:48,727] [    INFO][0m -   test_steps_per_second   =     11.873[0m
[32m[2022-09-20 12:30:48,728] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-20 12:30:48,728] [    INFO][0m -   Num examples = 2000[0m
[32m[2022-09-20 12:30:48,728] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:30:48,728] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:30:48,728] [    INFO][0m -   Total prediction steps = 125[0m
[32m[2022-09-20 12:31:00,800] [    INFO][0m - Predictions for bustm saved to ./fewclue_submit_examples.[0m
{
  "labels": 1,
  "text_a": "\u53eb\u7238\u7238\u53eb\u4e00\u58f0\u6211\u542c\u542c",
  "text_b": "\u90a3\u4f60\u53eb\u6211\u4e00\u58f0\u7238\u7238",
  "uid": 0
}

Prediction done.
 
==========
chid
==========
 
[32m[2022-09-20 12:31:22,808] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-20 12:31:22,809] [    INFO][0m - ============================================================[0m
[32m[2022-09-20 12:31:22,809] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-20 12:31:22,809] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-20 12:31:22,809] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-20 12:31:22,809] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-20 12:31:22,809] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-20 12:31:22,809] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-20 12:31:22,809] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-20 12:31:22,809] [    INFO][0m - [0m
[32m[2022-09-20 12:31:22,809] [    INFO][0m - ============================================================[0m
[32m[2022-09-20 12:31:22,809] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-20 12:31:22,809] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-20 12:31:22,810] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-20 12:31:22,810] [    INFO][0m - pretrained                    :/ssd2/wanghuijuan03/data/zero-shot/checkpoints_09191451/checkpoint-43000/model_state.pdparams[0m
[32m[2022-09-20 12:31:22,810] [    INFO][0m - prompt                        :‚Äú{'text':'text_a'}‚ÄùËøôÂè•ËØù‰∏≠ÊàêËØ≠[{'text':'text_b'}]ÁöÑÁêÜËß£Ê≠£Á°ÆÂêóÔºü{'mask'}{'mask'}„ÄÇ[0m
[32m[2022-09-20 12:31:22,810] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-20 12:31:22,810] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-20 12:31:22,810] [    INFO][0m - task_name                     :chid[0m
[32m[2022-09-20 12:31:22,810] [    INFO][0m - [0m
[32m[2022-09-20 12:31:22,810] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0920 12:31:22.812247 14729 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0920 12:31:22.816303 14729 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-20 12:31:27,593] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-20 12:31:27,604] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-20 12:31:27,604] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-20 12:31:27,605] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùËøôÂè•ËØù‰∏≠ÊàêËØ≠['}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': ']ÁöÑÁêÜËß£Ê≠£Á°ÆÂêóÔºü'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': '„ÄÇ'}][0m
[32m[2022-09-20 12:31:28,783] [    INFO][0m - ============================================================[0m
[32m[2022-09-20 12:31:28,783] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-20 12:31:28,784] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-20 12:31:28,784] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-20 12:31:28,784] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-20 12:31:28,784] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-20 12:31:28,784] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-20 12:31:28,784] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-20 12:31:28,784] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-20 12:31:28,784] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-20 12:31:28,784] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-20 12:31:28,784] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-20 12:31:28,785] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-20 12:31:28,785] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-20 12:31:28,785] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-20 12:31:28,785] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-20 12:31:28,785] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-20 12:31:28,785] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-20 12:31:28,785] [    INFO][0m - eval_batch_size               :16[0m
[32m[2022-09-20 12:31:28,785] [    INFO][0m - eval_steps                    :200[0m
[32m[2022-09-20 12:31:28,785] [    INFO][0m - evaluation_strategy           :IntervalStrategy.EPOCH[0m
[32m[2022-09-20 12:31:28,785] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-20 12:31:28,785] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-20 12:31:28,785] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-20 12:31:28,785] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-20 12:31:28,785] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-20 12:31:28,786] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-20 12:31:28,786] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-20 12:31:28,786] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-20 12:31:28,786] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-20 12:31:28,786] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-09-20 12:31:28,786] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-20 12:31:28,786] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-20 12:31:28,786] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-20 12:31:28,786] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-20 12:31:28,786] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-20 12:31:28,786] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-20 12:31:28,786] [    INFO][0m - logging_dir                   :./checkpoints_chid/runs/Sep20_12-31-22_instance-3bwob41y-01[0m
[32m[2022-09-20 12:31:28,786] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-20 12:31:28,786] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-20 12:31:28,787] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-20 12:31:28,787] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-20 12:31:28,787] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-20 12:31:28,787] [    INFO][0m - max_seq_length                :256[0m
[32m[2022-09-20 12:31:28,787] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-20 12:31:28,787] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-20 12:31:28,787] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-20 12:31:28,787] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-20 12:31:28,787] [    INFO][0m - num_train_epochs              :20.0[0m
[32m[2022-09-20 12:31:28,787] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-20 12:31:28,787] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-20 12:31:28,787] [    INFO][0m - output_dir                    :./checkpoints_chid/[0m
[32m[2022-09-20 12:31:28,787] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-20 12:31:28,788] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-20 12:31:28,788] [    INFO][0m - per_device_eval_batch_size    :16[0m
[32m[2022-09-20 12:31:28,788] [    INFO][0m - per_device_train_batch_size   :16[0m
[32m[2022-09-20 12:31:28,788] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-20 12:31:28,788] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-20 12:31:28,788] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-20 12:31:28,788] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-09-20 12:31:28,788] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-20 12:31:28,788] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-20 12:31:28,788] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-20 12:31:28,788] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-20 12:31:28,788] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-20 12:31:28,788] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-20 12:31:28,788] [    INFO][0m - run_name                      :./checkpoints_chid/[0m
[32m[2022-09-20 12:31:28,789] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-20 12:31:28,789] [    INFO][0m - save_steps                    :200[0m
[32m[2022-09-20 12:31:28,789] [    INFO][0m - save_strategy                 :IntervalStrategy.EPOCH[0m
[32m[2022-09-20 12:31:28,789] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-20 12:31:28,789] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-20 12:31:28,789] [    INFO][0m - seed                          :42[0m
[32m[2022-09-20 12:31:28,789] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-20 12:31:28,789] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-20 12:31:28,789] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-20 12:31:28,789] [    INFO][0m - train_batch_size              :16[0m
[32m[2022-09-20 12:31:28,789] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-20 12:31:28,789] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-20 12:31:28,789] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-20 12:31:28,790] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-20 12:31:28,790] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-20 12:31:28,790] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-20 12:31:28,790] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-20 12:31:28,790] [    INFO][0m - [0m
[32m[2022-09-20 12:31:28,793] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-20 12:31:28,793] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-20 12:31:28,794] [    INFO][0m -   Num Epochs = 20[0m
[32m[2022-09-20 12:31:28,794] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2022-09-20 12:31:28,794] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2022-09-20 12:31:28,794] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-20 12:31:28,794] [    INFO][0m -   Total optimization steps = 1780.0[0m
[32m[2022-09-20 12:31:28,794] [    INFO][0m -   Total num train samples = 28280[0m
[32m[2022-09-20 12:31:36,171] [    INFO][0m - loss: 0.95046644, learning_rate: 2.9831460674157305e-05, global_step: 10, interval_runtime: 7.3758, interval_samples_per_second: 2.169, interval_steps_per_second: 1.356, epoch: 0.1124[0m
[32m[2022-09-20 12:31:42,551] [    INFO][0m - loss: 0.52971172, learning_rate: 2.9662921348314606e-05, global_step: 20, interval_runtime: 6.3807, interval_samples_per_second: 2.508, interval_steps_per_second: 1.567, epoch: 0.2247[0m
[32m[2022-09-20 12:31:48,910] [    INFO][0m - loss: 0.31843944, learning_rate: 2.949438202247191e-05, global_step: 30, interval_runtime: 6.3589, interval_samples_per_second: 2.516, interval_steps_per_second: 1.573, epoch: 0.3371[0m
[32m[2022-09-20 12:31:55,284] [    INFO][0m - loss: 0.59353585, learning_rate: 2.932584269662921e-05, global_step: 40, interval_runtime: 6.3737, interval_samples_per_second: 2.51, interval_steps_per_second: 1.569, epoch: 0.4494[0m
[32m[2022-09-20 12:32:01,657] [    INFO][0m - loss: 0.45728908, learning_rate: 2.915730337078652e-05, global_step: 50, interval_runtime: 6.3731, interval_samples_per_second: 2.511, interval_steps_per_second: 1.569, epoch: 0.5618[0m
[32m[2022-09-20 12:32:08,031] [    INFO][0m - loss: 0.41952462, learning_rate: 2.8988764044943823e-05, global_step: 60, interval_runtime: 6.374, interval_samples_per_second: 2.51, interval_steps_per_second: 1.569, epoch: 0.6742[0m
[32m[2022-09-20 12:32:14,400] [    INFO][0m - loss: 0.47521801, learning_rate: 2.8820224719101124e-05, global_step: 70, interval_runtime: 6.3692, interval_samples_per_second: 2.512, interval_steps_per_second: 1.57, epoch: 0.7865[0m
[32m[2022-09-20 12:32:20,839] [    INFO][0m - loss: 0.4995625, learning_rate: 2.865168539325843e-05, global_step: 80, interval_runtime: 6.4387, interval_samples_per_second: 2.485, interval_steps_per_second: 1.553, epoch: 0.8989[0m
[32m[2022-09-20 12:32:26,136] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:32:26,137] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-20 12:32:26,137] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:32:26,137] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:32:26,137] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-20 12:32:48,467] [    INFO][0m - eval_loss: 0.4093727171421051, eval_accuracy: 0.38613861386138615, eval_runtime: 22.33, eval_samples_per_second: 63.323, eval_steps_per_second: 3.986, epoch: 1.0[0m
[32m[2022-09-20 12:32:48,491] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-89[0m
[32m[2022-09-20 12:32:48,492] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:32:51,471] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-89/tokenizer_config.json[0m
[32m[2022-09-20 12:32:51,472] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-89/special_tokens_map.json[0m
[32m[2022-09-20 12:32:57,905] [    INFO][0m - loss: 0.42013707, learning_rate: 2.848314606741573e-05, global_step: 90, interval_runtime: 37.0661, interval_samples_per_second: 0.432, interval_steps_per_second: 0.27, epoch: 1.0112[0m
[32m[2022-09-20 12:33:04,270] [    INFO][0m - loss: 0.46649075, learning_rate: 2.8314606741573034e-05, global_step: 100, interval_runtime: 6.3652, interval_samples_per_second: 2.514, interval_steps_per_second: 1.571, epoch: 1.1236[0m
[32m[2022-09-20 12:33:10,650] [    INFO][0m - loss: 0.43378782, learning_rate: 2.8146067415730338e-05, global_step: 110, interval_runtime: 6.3795, interval_samples_per_second: 2.508, interval_steps_per_second: 1.568, epoch: 1.236[0m
[32m[2022-09-20 12:33:17,050] [    INFO][0m - loss: 0.43822546, learning_rate: 2.797752808988764e-05, global_step: 120, interval_runtime: 6.4005, interval_samples_per_second: 2.5, interval_steps_per_second: 1.562, epoch: 1.3483[0m
[32m[2022-09-20 12:33:23,442] [    INFO][0m - loss: 0.44390936, learning_rate: 2.7808988764044946e-05, global_step: 130, interval_runtime: 6.3915, interval_samples_per_second: 2.503, interval_steps_per_second: 1.565, epoch: 1.4607[0m
[32m[2022-09-20 12:33:29,841] [    INFO][0m - loss: 0.52689404, learning_rate: 2.7640449438202247e-05, global_step: 140, interval_runtime: 6.3987, interval_samples_per_second: 2.501, interval_steps_per_second: 1.563, epoch: 1.573[0m
[32m[2022-09-20 12:33:36,248] [    INFO][0m - loss: 0.40704064, learning_rate: 2.7471910112359552e-05, global_step: 150, interval_runtime: 6.4077, interval_samples_per_second: 2.497, interval_steps_per_second: 1.561, epoch: 1.6854[0m
[32m[2022-09-20 12:33:42,651] [    INFO][0m - loss: 0.34309201, learning_rate: 2.7303370786516856e-05, global_step: 160, interval_runtime: 6.4033, interval_samples_per_second: 2.499, interval_steps_per_second: 1.562, epoch: 1.7978[0m
[32m[2022-09-20 12:33:49,070] [    INFO][0m - loss: 0.4802103, learning_rate: 2.7134831460674157e-05, global_step: 170, interval_runtime: 6.4187, interval_samples_per_second: 2.493, interval_steps_per_second: 1.558, epoch: 1.9101[0m
[32m[2022-09-20 12:33:53,723] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:33:53,724] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-20 12:33:53,724] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:33:53,724] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:33:53,724] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-20 12:34:16,239] [    INFO][0m - eval_loss: 0.31461599469184875, eval_accuracy: 0.7326732673267327, eval_runtime: 22.5148, eval_samples_per_second: 62.803, eval_steps_per_second: 3.953, epoch: 2.0[0m
[32m[2022-09-20 12:34:16,263] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-178[0m
[32m[2022-09-20 12:34:16,263] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:34:19,354] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-178/tokenizer_config.json[0m
[32m[2022-09-20 12:34:19,355] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-178/special_tokens_map.json[0m
[32m[2022-09-20 12:34:26,938] [    INFO][0m - loss: 0.38146062, learning_rate: 2.696629213483146e-05, global_step: 180, interval_runtime: 37.8682, interval_samples_per_second: 0.423, interval_steps_per_second: 0.264, epoch: 2.0225[0m
[32m[2022-09-20 12:34:33,320] [    INFO][0m - loss: 0.2416575, learning_rate: 2.6797752808988762e-05, global_step: 190, interval_runtime: 6.3811, interval_samples_per_second: 2.507, interval_steps_per_second: 1.567, epoch: 2.1348[0m
[32m[2022-09-20 12:34:39,714] [    INFO][0m - loss: 0.2744338, learning_rate: 2.6629213483146066e-05, global_step: 200, interval_runtime: 6.3944, interval_samples_per_second: 2.502, interval_steps_per_second: 1.564, epoch: 2.2472[0m
[32m[2022-09-20 12:34:46,107] [    INFO][0m - loss: 0.35101264, learning_rate: 2.6460674157303374e-05, global_step: 210, interval_runtime: 6.3932, interval_samples_per_second: 2.503, interval_steps_per_second: 1.564, epoch: 2.3596[0m
[32m[2022-09-20 12:34:52,495] [    INFO][0m - loss: 0.3293633, learning_rate: 2.6292134831460675e-05, global_step: 220, interval_runtime: 6.3882, interval_samples_per_second: 2.505, interval_steps_per_second: 1.565, epoch: 2.4719[0m
[32m[2022-09-20 12:34:58,892] [    INFO][0m - loss: 0.36324778, learning_rate: 2.612359550561798e-05, global_step: 230, interval_runtime: 6.3965, interval_samples_per_second: 2.501, interval_steps_per_second: 1.563, epoch: 2.5843[0m
[32m[2022-09-20 12:35:05,288] [    INFO][0m - loss: 0.29650273, learning_rate: 2.595505617977528e-05, global_step: 240, interval_runtime: 6.3965, interval_samples_per_second: 2.501, interval_steps_per_second: 1.563, epoch: 2.6966[0m
[32m[2022-09-20 12:35:11,693] [    INFO][0m - loss: 0.25959246, learning_rate: 2.5786516853932585e-05, global_step: 250, interval_runtime: 6.4051, interval_samples_per_second: 2.498, interval_steps_per_second: 1.561, epoch: 2.809[0m
[32m[2022-09-20 12:35:18,082] [    INFO][0m - loss: 0.36928973, learning_rate: 2.561797752808989e-05, global_step: 260, interval_runtime: 6.389, interval_samples_per_second: 2.504, interval_steps_per_second: 1.565, epoch: 2.9213[0m
[32m[2022-09-20 12:35:22,098] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:35:22,098] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-20 12:35:22,098] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:35:22,098] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:35:22,098] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-20 12:35:44,398] [    INFO][0m - eval_loss: 0.3276788294315338, eval_accuracy: 0.7524752475247525, eval_runtime: 22.2997, eval_samples_per_second: 63.409, eval_steps_per_second: 3.991, epoch: 3.0[0m
[32m[2022-09-20 12:35:44,422] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-267[0m
[32m[2022-09-20 12:35:44,423] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:35:47,277] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-267/tokenizer_config.json[0m
[32m[2022-09-20 12:35:47,277] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-267/special_tokens_map.json[0m
[32m[2022-09-20 12:35:54,837] [    INFO][0m - loss: 0.2625643, learning_rate: 2.544943820224719e-05, global_step: 270, interval_runtime: 36.7549, interval_samples_per_second: 0.435, interval_steps_per_second: 0.272, epoch: 3.0337[0m
[32m[2022-09-20 12:36:01,214] [    INFO][0m - loss: 0.20538218, learning_rate: 2.5280898876404494e-05, global_step: 280, interval_runtime: 6.377, interval_samples_per_second: 2.509, interval_steps_per_second: 1.568, epoch: 3.1461[0m
[32m[2022-09-20 12:36:07,601] [    INFO][0m - loss: 0.12146283, learning_rate: 2.51123595505618e-05, global_step: 290, interval_runtime: 6.3872, interval_samples_per_second: 2.505, interval_steps_per_second: 1.566, epoch: 3.2584[0m
[32m[2022-09-20 12:36:13,993] [    INFO][0m - loss: 0.21463332, learning_rate: 2.4943820224719103e-05, global_step: 300, interval_runtime: 6.3917, interval_samples_per_second: 2.503, interval_steps_per_second: 1.565, epoch: 3.3708[0m
[32m[2022-09-20 12:36:20,389] [    INFO][0m - loss: 0.33142548, learning_rate: 2.4775280898876407e-05, global_step: 310, interval_runtime: 6.3961, interval_samples_per_second: 2.502, interval_steps_per_second: 1.563, epoch: 3.4831[0m
[32m[2022-09-20 12:36:26,804] [    INFO][0m - loss: 0.1804878, learning_rate: 2.4606741573033708e-05, global_step: 320, interval_runtime: 6.4149, interval_samples_per_second: 2.494, interval_steps_per_second: 1.559, epoch: 3.5955[0m
[32m[2022-09-20 12:36:33,203] [    INFO][0m - loss: 0.30988226, learning_rate: 2.4438202247191012e-05, global_step: 330, interval_runtime: 6.3994, interval_samples_per_second: 2.5, interval_steps_per_second: 1.563, epoch: 3.7079[0m
[32m[2022-09-20 12:36:39,610] [    INFO][0m - loss: 0.13425891, learning_rate: 2.4269662921348313e-05, global_step: 340, interval_runtime: 6.4068, interval_samples_per_second: 2.497, interval_steps_per_second: 1.561, epoch: 3.8202[0m
[32m[2022-09-20 12:36:46,014] [    INFO][0m - loss: 0.2620213, learning_rate: 2.4101123595505617e-05, global_step: 350, interval_runtime: 6.4038, interval_samples_per_second: 2.499, interval_steps_per_second: 1.562, epoch: 3.9326[0m
[32m[2022-09-20 12:36:49,383] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:36:49,383] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-20 12:36:49,383] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:36:49,384] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:36:49,384] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-20 12:37:11,701] [    INFO][0m - eval_loss: 0.3799543082714081, eval_accuracy: 0.6831683168316832, eval_runtime: 22.3172, eval_samples_per_second: 63.359, eval_steps_per_second: 3.988, epoch: 4.0[0m
[32m[2022-09-20 12:37:11,728] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-356[0m
[32m[2022-09-20 12:37:11,728] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:37:14,797] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-356/tokenizer_config.json[0m
[32m[2022-09-20 12:37:14,798] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-356/special_tokens_map.json[0m
[32m[2022-09-20 12:37:22,912] [    INFO][0m - loss: 0.23255026, learning_rate: 2.393258426966292e-05, global_step: 360, interval_runtime: 36.8983, interval_samples_per_second: 0.434, interval_steps_per_second: 0.271, epoch: 4.0449[0m
[32m[2022-09-20 12:37:29,284] [    INFO][0m - loss: 0.14432639, learning_rate: 2.3764044943820226e-05, global_step: 370, interval_runtime: 6.3713, interval_samples_per_second: 2.511, interval_steps_per_second: 1.57, epoch: 4.1573[0m
[32m[2022-09-20 12:37:35,678] [    INFO][0m - loss: 0.06920512, learning_rate: 2.359550561797753e-05, global_step: 380, interval_runtime: 6.3937, interval_samples_per_second: 2.502, interval_steps_per_second: 1.564, epoch: 4.2697[0m
[32m[2022-09-20 12:37:42,066] [    INFO][0m - loss: 0.29532104, learning_rate: 2.342696629213483e-05, global_step: 390, interval_runtime: 6.3885, interval_samples_per_second: 2.505, interval_steps_per_second: 1.565, epoch: 4.382[0m
[32m[2022-09-20 12:37:48,458] [    INFO][0m - loss: 0.22400498, learning_rate: 2.3258426966292135e-05, global_step: 400, interval_runtime: 6.3917, interval_samples_per_second: 2.503, interval_steps_per_second: 1.565, epoch: 4.4944[0m
[32m[2022-09-20 12:37:54,862] [    INFO][0m - loss: 0.04821869, learning_rate: 2.308988764044944e-05, global_step: 410, interval_runtime: 6.4044, interval_samples_per_second: 2.498, interval_steps_per_second: 1.561, epoch: 4.6067[0m
[32m[2022-09-20 12:38:01,267] [    INFO][0m - loss: 0.2187355, learning_rate: 2.292134831460674e-05, global_step: 420, interval_runtime: 6.4048, interval_samples_per_second: 2.498, interval_steps_per_second: 1.561, epoch: 4.7191[0m
[32m[2022-09-20 12:38:07,672] [    INFO][0m - loss: 0.17106036, learning_rate: 2.2752808988764045e-05, global_step: 430, interval_runtime: 6.405, interval_samples_per_second: 2.498, interval_steps_per_second: 1.561, epoch: 4.8315[0m
[32m[2022-09-20 12:38:14,054] [    INFO][0m - loss: 0.16020579, learning_rate: 2.2584269662921346e-05, global_step: 440, interval_runtime: 6.3817, interval_samples_per_second: 2.507, interval_steps_per_second: 1.567, epoch: 4.9438[0m
[32m[2022-09-20 12:38:16,800] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:38:16,801] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-20 12:38:16,801] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:38:16,801] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:38:16,801] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-20 12:38:39,111] [    INFO][0m - eval_loss: 0.5054502487182617, eval_accuracy: 0.7475247524752475, eval_runtime: 22.3101, eval_samples_per_second: 63.379, eval_steps_per_second: 3.989, epoch: 5.0[0m
[32m[2022-09-20 12:38:39,135] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-445[0m
[32m[2022-09-20 12:38:39,135] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:38:42,003] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-445/tokenizer_config.json[0m
[32m[2022-09-20 12:38:42,003] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-445/special_tokens_map.json[0m
[32m[2022-09-20 12:38:50,774] [    INFO][0m - loss: 0.01577537, learning_rate: 2.2415730337078654e-05, global_step: 450, interval_runtime: 36.7203, interval_samples_per_second: 0.436, interval_steps_per_second: 0.272, epoch: 5.0562[0m
[32m[2022-09-20 12:38:57,148] [    INFO][0m - loss: 0.11519701, learning_rate: 2.2247191011235958e-05, global_step: 460, interval_runtime: 6.3739, interval_samples_per_second: 2.51, interval_steps_per_second: 1.569, epoch: 5.1685[0m
[32m[2022-09-20 12:39:03,547] [    INFO][0m - loss: 0.02658695, learning_rate: 2.207865168539326e-05, global_step: 470, interval_runtime: 6.3992, interval_samples_per_second: 2.5, interval_steps_per_second: 1.563, epoch: 5.2809[0m
[32m[2022-09-20 12:39:09,937] [    INFO][0m - loss: 0.05075536, learning_rate: 2.1910112359550563e-05, global_step: 480, interval_runtime: 6.3903, interval_samples_per_second: 2.504, interval_steps_per_second: 1.565, epoch: 5.3933[0m
[32m[2022-09-20 12:39:16,329] [    INFO][0m - loss: 0.12121505, learning_rate: 2.1741573033707864e-05, global_step: 490, interval_runtime: 6.3913, interval_samples_per_second: 2.503, interval_steps_per_second: 1.565, epoch: 5.5056[0m
[32m[2022-09-20 12:39:22,730] [    INFO][0m - loss: 0.09521431, learning_rate: 2.1573033707865168e-05, global_step: 500, interval_runtime: 6.4011, interval_samples_per_second: 2.5, interval_steps_per_second: 1.562, epoch: 5.618[0m
[32m[2022-09-20 12:39:29,128] [    INFO][0m - loss: 0.10523065, learning_rate: 2.1404494382022473e-05, global_step: 510, interval_runtime: 6.3986, interval_samples_per_second: 2.501, interval_steps_per_second: 1.563, epoch: 5.7303[0m
[32m[2022-09-20 12:39:35,531] [    INFO][0m - loss: 0.07313164, learning_rate: 2.1235955056179773e-05, global_step: 520, interval_runtime: 6.4026, interval_samples_per_second: 2.499, interval_steps_per_second: 1.562, epoch: 5.8427[0m
[32m[2022-09-20 12:39:41,905] [    INFO][0m - loss: 0.0707296, learning_rate: 2.106741573033708e-05, global_step: 530, interval_runtime: 6.3742, interval_samples_per_second: 2.51, interval_steps_per_second: 1.569, epoch: 5.9551[0m
[32m[2022-09-20 12:39:44,028] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:39:44,028] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-20 12:39:44,029] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:39:44,029] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:39:44,029] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-20 12:40:06,332] [    INFO][0m - eval_loss: 0.39595362544059753, eval_accuracy: 0.6831683168316832, eval_runtime: 22.303, eval_samples_per_second: 63.399, eval_steps_per_second: 3.99, epoch: 6.0[0m
[32m[2022-09-20 12:40:06,356] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-534[0m
[32m[2022-09-20 12:40:06,356] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:40:09,199] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-534/tokenizer_config.json[0m
[32m[2022-09-20 12:40:09,199] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-534/special_tokens_map.json[0m
[32m[2022-09-20 12:40:18,588] [    INFO][0m - loss: 0.16291323, learning_rate: 2.0898876404494382e-05, global_step: 540, interval_runtime: 36.683, interval_samples_per_second: 0.436, interval_steps_per_second: 0.273, epoch: 6.0674[0m
[32m[2022-09-20 12:40:24,990] [    INFO][0m - loss: 0.02048205, learning_rate: 2.0730337078651686e-05, global_step: 550, interval_runtime: 6.4019, interval_samples_per_second: 2.499, interval_steps_per_second: 1.562, epoch: 6.1798[0m
[32m[2022-09-20 12:40:31,395] [    INFO][0m - loss: 0.04197102, learning_rate: 2.056179775280899e-05, global_step: 560, interval_runtime: 6.4042, interval_samples_per_second: 2.498, interval_steps_per_second: 1.561, epoch: 6.2921[0m
[32m[2022-09-20 12:40:37,793] [    INFO][0m - loss: 0.0865625, learning_rate: 2.039325842696629e-05, global_step: 570, interval_runtime: 6.3988, interval_samples_per_second: 2.5, interval_steps_per_second: 1.563, epoch: 6.4045[0m
[32m[2022-09-20 12:40:44,200] [    INFO][0m - loss: 0.00056019, learning_rate: 2.0224719101123596e-05, global_step: 580, interval_runtime: 6.4066, interval_samples_per_second: 2.497, interval_steps_per_second: 1.561, epoch: 6.5169[0m
[32m[2022-09-20 12:40:50,593] [    INFO][0m - loss: 0.05171123, learning_rate: 2.0056179775280897e-05, global_step: 590, interval_runtime: 6.3933, interval_samples_per_second: 2.503, interval_steps_per_second: 1.564, epoch: 6.6292[0m
[32m[2022-09-20 12:40:56,998] [    INFO][0m - loss: 0.13140316, learning_rate: 1.98876404494382e-05, global_step: 600, interval_runtime: 6.4048, interval_samples_per_second: 2.498, interval_steps_per_second: 1.561, epoch: 6.7416[0m
[32m[2022-09-20 12:41:03,398] [    INFO][0m - loss: 0.00092244, learning_rate: 1.971910112359551e-05, global_step: 610, interval_runtime: 6.4002, interval_samples_per_second: 2.5, interval_steps_per_second: 1.562, epoch: 6.8539[0m
[32m[2022-09-20 12:41:09,740] [    INFO][0m - loss: 0.0778069, learning_rate: 1.955056179775281e-05, global_step: 620, interval_runtime: 6.3417, interval_samples_per_second: 2.523, interval_steps_per_second: 1.577, epoch: 6.9663[0m
[32m[2022-09-20 12:41:11,242] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:41:11,243] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-20 12:41:11,243] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:41:11,243] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:41:11,243] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-20 12:41:33,533] [    INFO][0m - eval_loss: 0.6073131561279297, eval_accuracy: 0.7623762376237624, eval_runtime: 22.2896, eval_samples_per_second: 63.438, eval_steps_per_second: 3.993, epoch: 7.0[0m
[32m[2022-09-20 12:41:33,557] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-623[0m
[32m[2022-09-20 12:41:33,557] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:41:36,459] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-623/tokenizer_config.json[0m
[32m[2022-09-20 12:41:36,459] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-623/special_tokens_map.json[0m
[32m[2022-09-20 12:41:46,485] [    INFO][0m - loss: 0.00116475, learning_rate: 1.9382022471910114e-05, global_step: 630, interval_runtime: 36.745, interval_samples_per_second: 0.435, interval_steps_per_second: 0.272, epoch: 7.0787[0m
[32m[2022-09-20 12:41:53,103] [    INFO][0m - loss: 0.05980459, learning_rate: 1.9213483146067415e-05, global_step: 640, interval_runtime: 6.3959, interval_samples_per_second: 2.502, interval_steps_per_second: 1.564, epoch: 7.191[0m
[32m[2022-09-20 12:41:59,492] [    INFO][0m - loss: 0.03980439, learning_rate: 1.904494382022472e-05, global_step: 650, interval_runtime: 6.6106, interval_samples_per_second: 2.42, interval_steps_per_second: 1.513, epoch: 7.3034[0m
[32m[2022-09-20 12:42:05,880] [    INFO][0m - loss: 0.0771754, learning_rate: 1.8876404494382024e-05, global_step: 660, interval_runtime: 6.3885, interval_samples_per_second: 2.505, interval_steps_per_second: 1.565, epoch: 7.4157[0m
[32m[2022-09-20 12:42:12,266] [    INFO][0m - loss: 0.00018145, learning_rate: 1.8707865168539324e-05, global_step: 670, interval_runtime: 6.3858, interval_samples_per_second: 2.506, interval_steps_per_second: 1.566, epoch: 7.5281[0m
[32m[2022-09-20 12:42:18,682] [    INFO][0m - loss: 0.00046108, learning_rate: 1.853932584269663e-05, global_step: 680, interval_runtime: 6.4158, interval_samples_per_second: 2.494, interval_steps_per_second: 1.559, epoch: 7.6404[0m
[32m[2022-09-20 12:42:25,082] [    INFO][0m - loss: 0.01368093, learning_rate: 1.8370786516853933e-05, global_step: 690, interval_runtime: 6.4006, interval_samples_per_second: 2.5, interval_steps_per_second: 1.562, epoch: 7.7528[0m
[32m[2022-09-20 12:42:35,544] [    INFO][0m - loss: 0.03986203, learning_rate: 1.8202247191011237e-05, global_step: 700, interval_runtime: 6.3978, interval_samples_per_second: 2.501, interval_steps_per_second: 1.563, epoch: 7.8652[0m
[32m[2022-09-20 12:42:41,898] [    INFO][0m - loss: 0.00062344, learning_rate: 1.803370786516854e-05, global_step: 710, interval_runtime: 10.4179, interval_samples_per_second: 1.536, interval_steps_per_second: 0.96, epoch: 7.9775[0m
[32m[2022-09-20 12:42:42,770] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:42:42,771] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-20 12:42:42,771] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:42:42,771] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:42:42,771] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-20 12:43:05,067] [    INFO][0m - eval_loss: 0.8008748292922974, eval_accuracy: 0.7425742574257426, eval_runtime: 22.2959, eval_samples_per_second: 63.42, eval_steps_per_second: 3.992, epoch: 8.0[0m
[32m[2022-09-20 12:43:05,091] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-712[0m
[32m[2022-09-20 12:43:05,092] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:43:07,930] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-712/tokenizer_config.json[0m
[32m[2022-09-20 12:43:07,930] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-712/special_tokens_map.json[0m
[32m[2022-09-20 12:43:18,343] [    INFO][0m - loss: 0.00068828, learning_rate: 1.7865168539325843e-05, global_step: 720, interval_runtime: 36.4449, interval_samples_per_second: 0.439, interval_steps_per_second: 0.274, epoch: 8.0899[0m
[32m[2022-09-20 12:43:24,719] [    INFO][0m - loss: 0.03197657, learning_rate: 1.7696629213483147e-05, global_step: 730, interval_runtime: 6.3759, interval_samples_per_second: 2.509, interval_steps_per_second: 1.568, epoch: 8.2022[0m
[32m[2022-09-20 12:43:31,115] [    INFO][0m - loss: 4.885e-05, learning_rate: 1.7528089887640448e-05, global_step: 740, interval_runtime: 6.3961, interval_samples_per_second: 2.502, interval_steps_per_second: 1.563, epoch: 8.3146[0m
[32m[2022-09-20 12:43:37,515] [    INFO][0m - loss: 0.03511254, learning_rate: 1.7359550561797752e-05, global_step: 750, interval_runtime: 6.4002, interval_samples_per_second: 2.5, interval_steps_per_second: 1.562, epoch: 8.427[0m
[32m[2022-09-20 12:43:43,902] [    INFO][0m - loss: 0.04940696, learning_rate: 1.7191011235955056e-05, global_step: 760, interval_runtime: 6.3871, interval_samples_per_second: 2.505, interval_steps_per_second: 1.566, epoch: 8.5393[0m
[32m[2022-09-20 12:43:50,298] [    INFO][0m - loss: 0.0007636, learning_rate: 1.702247191011236e-05, global_step: 770, interval_runtime: 6.3955, interval_samples_per_second: 2.502, interval_steps_per_second: 1.564, epoch: 8.6517[0m
[32m[2022-09-20 12:43:56,703] [    INFO][0m - loss: 0.00514061, learning_rate: 1.6853932584269665e-05, global_step: 780, interval_runtime: 6.4051, interval_samples_per_second: 2.498, interval_steps_per_second: 1.561, epoch: 8.764[0m
[32m[2022-09-20 12:44:03,100] [    INFO][0m - loss: 0.01644075, learning_rate: 1.6685393258426966e-05, global_step: 790, interval_runtime: 6.3975, interval_samples_per_second: 2.501, interval_steps_per_second: 1.563, epoch: 8.8764[0m
[32m[2022-09-20 12:44:09,413] [    INFO][0m - loss: 0.00321951, learning_rate: 1.651685393258427e-05, global_step: 800, interval_runtime: 6.3132, interval_samples_per_second: 2.534, interval_steps_per_second: 1.584, epoch: 8.9888[0m
[32m[2022-09-20 12:44:09,670] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:44:09,670] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-20 12:44:09,670] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:44:09,670] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:44:09,670] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-20 12:44:31,985] [    INFO][0m - eval_loss: 0.9467601776123047, eval_accuracy: 0.7475247524752475, eval_runtime: 22.3148, eval_samples_per_second: 63.366, eval_steps_per_second: 3.988, epoch: 9.0[0m
[32m[2022-09-20 12:44:32,009] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-801[0m
[32m[2022-09-20 12:44:32,010] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:44:34,781] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-801/tokenizer_config.json[0m
[32m[2022-09-20 12:44:34,781] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-801/special_tokens_map.json[0m
[32m[2022-09-20 12:44:45,754] [    INFO][0m - loss: 0.02646503, learning_rate: 1.6348314606741574e-05, global_step: 810, interval_runtime: 36.3399, interval_samples_per_second: 0.44, interval_steps_per_second: 0.275, epoch: 9.1011[0m
[32m[2022-09-20 12:44:52,144] [    INFO][0m - loss: 0.07777998, learning_rate: 1.6179775280898875e-05, global_step: 820, interval_runtime: 6.3912, interval_samples_per_second: 2.503, interval_steps_per_second: 1.565, epoch: 9.2135[0m
[32m[2022-09-20 12:44:58,541] [    INFO][0m - loss: 0.00014879, learning_rate: 1.601123595505618e-05, global_step: 830, interval_runtime: 6.3967, interval_samples_per_second: 2.501, interval_steps_per_second: 1.563, epoch: 9.3258[0m
[32m[2022-09-20 12:45:04,960] [    INFO][0m - loss: 0.04385864, learning_rate: 1.5842696629213484e-05, global_step: 840, interval_runtime: 6.4188, interval_samples_per_second: 2.493, interval_steps_per_second: 1.558, epoch: 9.4382[0m
[32m[2022-09-20 12:45:11,397] [    INFO][0m - loss: 0.00029726, learning_rate: 1.5674157303370788e-05, global_step: 850, interval_runtime: 6.4372, interval_samples_per_second: 2.486, interval_steps_per_second: 1.553, epoch: 9.5506[0m
[32m[2022-09-20 12:45:17,821] [    INFO][0m - loss: 8.299e-05, learning_rate: 1.5505617977528093e-05, global_step: 860, interval_runtime: 6.4237, interval_samples_per_second: 2.491, interval_steps_per_second: 1.557, epoch: 9.6629[0m
[32m[2022-09-20 12:45:24,233] [    INFO][0m - loss: 0.00109251, learning_rate: 1.5337078651685393e-05, global_step: 870, interval_runtime: 6.4116, interval_samples_per_second: 2.495, interval_steps_per_second: 1.56, epoch: 9.7753[0m
[32m[2022-09-20 12:45:30,635] [    INFO][0m - loss: 0.00010356, learning_rate: 1.5168539325842698e-05, global_step: 880, interval_runtime: 6.4022, interval_samples_per_second: 2.499, interval_steps_per_second: 1.562, epoch: 9.8876[0m
[32m[2022-09-20 12:45:36,576] [    INFO][0m - loss: 4.232e-05, learning_rate: 1.5e-05, global_step: 890, interval_runtime: 5.9411, interval_samples_per_second: 2.693, interval_steps_per_second: 1.683, epoch: 10.0[0m
[32m[2022-09-20 12:45:36,577] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:45:36,577] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-20 12:45:36,577] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:45:36,577] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:45:36,577] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-20 12:45:58,979] [    INFO][0m - eval_loss: 0.9583412408828735, eval_accuracy: 0.7178217821782178, eval_runtime: 22.4021, eval_samples_per_second: 63.119, eval_steps_per_second: 3.973, epoch: 10.0[0m
[32m[2022-09-20 12:45:59,004] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-890[0m
[32m[2022-09-20 12:45:59,004] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:46:01,762] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-890/tokenizer_config.json[0m
[32m[2022-09-20 12:46:01,763] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-890/special_tokens_map.json[0m
[32m[2022-09-20 12:46:13,374] [    INFO][0m - loss: 5.773e-05, learning_rate: 1.4831460674157303e-05, global_step: 900, interval_runtime: 36.7978, interval_samples_per_second: 0.435, interval_steps_per_second: 0.272, epoch: 10.1124[0m
[32m[2022-09-20 12:46:19,768] [    INFO][0m - loss: 2.481e-05, learning_rate: 1.4662921348314606e-05, global_step: 910, interval_runtime: 6.3947, interval_samples_per_second: 2.502, interval_steps_per_second: 1.564, epoch: 10.2247[0m
[32m[2022-09-20 12:46:26,168] [    INFO][0m - loss: 2.874e-05, learning_rate: 1.4494382022471912e-05, global_step: 920, interval_runtime: 6.3997, interval_samples_per_second: 2.5, interval_steps_per_second: 1.563, epoch: 10.3371[0m
[32m[2022-09-20 12:46:32,570] [    INFO][0m - loss: 0.09347998, learning_rate: 1.4325842696629214e-05, global_step: 930, interval_runtime: 6.4021, interval_samples_per_second: 2.499, interval_steps_per_second: 1.562, epoch: 10.4494[0m
[32m[2022-09-20 12:46:38,974] [    INFO][0m - loss: 0.00114933, learning_rate: 1.4157303370786517e-05, global_step: 940, interval_runtime: 6.4036, interval_samples_per_second: 2.499, interval_steps_per_second: 1.562, epoch: 10.5618[0m
[32m[2022-09-20 12:46:45,379] [    INFO][0m - loss: 0.00267008, learning_rate: 1.398876404494382e-05, global_step: 950, interval_runtime: 6.4036, interval_samples_per_second: 2.499, interval_steps_per_second: 1.562, epoch: 10.6742[0m
[32m[2022-09-20 12:46:51,786] [    INFO][0m - loss: 2.951e-05, learning_rate: 1.3820224719101124e-05, global_step: 960, interval_runtime: 6.4079, interval_samples_per_second: 2.497, interval_steps_per_second: 1.561, epoch: 10.7865[0m
[32m[2022-09-20 12:46:58,188] [    INFO][0m - loss: 0.01417714, learning_rate: 1.3651685393258428e-05, global_step: 970, interval_runtime: 6.4029, interval_samples_per_second: 2.499, interval_steps_per_second: 1.562, epoch: 10.8989[0m
[32m[2022-09-20 12:47:03,477] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:47:03,477] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-20 12:47:03,477] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:47:03,477] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:47:03,477] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-20 12:47:25,753] [    INFO][0m - eval_loss: 0.9228904247283936, eval_accuracy: 0.7178217821782178, eval_runtime: 22.2755, eval_samples_per_second: 63.478, eval_steps_per_second: 3.995, epoch: 11.0[0m
[32m[2022-09-20 12:47:25,777] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-979[0m
[32m[2022-09-20 12:47:25,778] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:47:28,489] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-979/tokenizer_config.json[0m
[32m[2022-09-20 12:47:28,490] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-979/special_tokens_map.json[0m
[32m[2022-09-20 12:47:34,372] [    INFO][0m - loss: 6.709e-05, learning_rate: 1.348314606741573e-05, global_step: 980, interval_runtime: 36.1833, interval_samples_per_second: 0.442, interval_steps_per_second: 0.276, epoch: 11.0112[0m
[32m[2022-09-20 12:47:40,747] [    INFO][0m - loss: 0.00011332, learning_rate: 1.3314606741573033e-05, global_step: 990, interval_runtime: 6.3757, interval_samples_per_second: 2.51, interval_steps_per_second: 1.568, epoch: 11.1236[0m
[32m[2022-09-20 12:47:47,134] [    INFO][0m - loss: 2.72e-05, learning_rate: 1.3146067415730338e-05, global_step: 1000, interval_runtime: 6.3864, interval_samples_per_second: 2.505, interval_steps_per_second: 1.566, epoch: 11.236[0m
[32m[2022-09-20 12:47:53,543] [    INFO][0m - loss: 3.699e-05, learning_rate: 1.297752808988764e-05, global_step: 1010, interval_runtime: 6.4085, interval_samples_per_second: 2.497, interval_steps_per_second: 1.56, epoch: 11.3483[0m
[32m[2022-09-20 12:47:59,950] [    INFO][0m - loss: 2.498e-05, learning_rate: 1.2808988764044944e-05, global_step: 1020, interval_runtime: 6.4081, interval_samples_per_second: 2.497, interval_steps_per_second: 1.561, epoch: 11.4607[0m
[32m[2022-09-20 12:48:06,340] [    INFO][0m - loss: 2.686e-05, learning_rate: 1.2640449438202247e-05, global_step: 1030, interval_runtime: 6.3895, interval_samples_per_second: 2.504, interval_steps_per_second: 1.565, epoch: 11.573[0m
[32m[2022-09-20 12:48:12,908] [    INFO][0m - loss: 0.04377491, learning_rate: 1.2471910112359551e-05, global_step: 1040, interval_runtime: 6.5676, interval_samples_per_second: 2.436, interval_steps_per_second: 1.523, epoch: 11.6854[0m
[32m[2022-09-20 12:48:19,309] [    INFO][0m - loss: 3.174e-05, learning_rate: 1.2303370786516854e-05, global_step: 1050, interval_runtime: 6.4015, interval_samples_per_second: 2.499, interval_steps_per_second: 1.562, epoch: 11.7978[0m
[32m[2022-09-20 12:48:25,727] [    INFO][0m - loss: 2.847e-05, learning_rate: 1.2134831460674157e-05, global_step: 1060, interval_runtime: 6.4177, interval_samples_per_second: 2.493, interval_steps_per_second: 1.558, epoch: 11.9101[0m
[32m[2022-09-20 12:48:30,378] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:48:30,378] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-20 12:48:30,378] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:48:30,378] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:48:30,378] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-20 12:48:52,754] [    INFO][0m - eval_loss: 0.923709511756897, eval_accuracy: 0.7227722772277227, eval_runtime: 22.3756, eval_samples_per_second: 63.194, eval_steps_per_second: 3.978, epoch: 12.0[0m
[32m[2022-09-20 12:48:52,778] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-1068[0m
[32m[2022-09-20 12:48:52,778] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:48:55,431] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-1068/tokenizer_config.json[0m
[32m[2022-09-20 12:48:55,432] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-1068/special_tokens_map.json[0m
[32m[2022-09-20 12:49:01,849] [    INFO][0m - loss: 7.021e-05, learning_rate: 1.196629213483146e-05, global_step: 1070, interval_runtime: 36.1217, interval_samples_per_second: 0.443, interval_steps_per_second: 0.277, epoch: 12.0225[0m
[32m[2022-09-20 12:49:08,223] [    INFO][0m - loss: 2.715e-05, learning_rate: 1.1797752808988765e-05, global_step: 1080, interval_runtime: 6.3742, interval_samples_per_second: 2.51, interval_steps_per_second: 1.569, epoch: 12.1348[0m
[32m[2022-09-20 12:49:14,640] [    INFO][0m - loss: 2.213e-05, learning_rate: 1.1629213483146068e-05, global_step: 1090, interval_runtime: 6.4171, interval_samples_per_second: 2.493, interval_steps_per_second: 1.558, epoch: 12.2472[0m
[32m[2022-09-20 12:49:21,038] [    INFO][0m - loss: 0.03830827, learning_rate: 1.146067415730337e-05, global_step: 1100, interval_runtime: 6.3986, interval_samples_per_second: 2.501, interval_steps_per_second: 1.563, epoch: 12.3596[0m
[32m[2022-09-20 12:49:27,453] [    INFO][0m - loss: 9.293e-05, learning_rate: 1.1292134831460673e-05, global_step: 1110, interval_runtime: 6.414, interval_samples_per_second: 2.495, interval_steps_per_second: 1.559, epoch: 12.4719[0m
[32m[2022-09-20 12:49:37,250] [    INFO][0m - loss: 0.00011687, learning_rate: 1.1123595505617979e-05, global_step: 1120, interval_runtime: 6.4516, interval_samples_per_second: 2.48, interval_steps_per_second: 1.55, epoch: 12.5843[0m
[32m[2022-09-20 12:49:43,644] [    INFO][0m - loss: 5.256e-05, learning_rate: 1.0955056179775282e-05, global_step: 1130, interval_runtime: 9.74, interval_samples_per_second: 1.643, interval_steps_per_second: 1.027, epoch: 12.6966[0m
[32m[2022-09-20 12:49:50,036] [    INFO][0m - loss: 0.00032149, learning_rate: 1.0786516853932584e-05, global_step: 1140, interval_runtime: 6.3919, interval_samples_per_second: 2.503, interval_steps_per_second: 1.564, epoch: 12.809[0m
[32m[2022-09-20 12:49:56,436] [    INFO][0m - loss: 2.261e-05, learning_rate: 1.0617977528089887e-05, global_step: 1150, interval_runtime: 6.4, interval_samples_per_second: 2.5, interval_steps_per_second: 1.562, epoch: 12.9213[0m
[32m[2022-09-20 12:50:00,441] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:50:00,442] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-20 12:50:00,442] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:50:00,442] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:50:00,442] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-20 12:50:22,798] [    INFO][0m - eval_loss: 0.9748234748840332, eval_accuracy: 0.7475247524752475, eval_runtime: 22.356, eval_samples_per_second: 63.249, eval_steps_per_second: 3.981, epoch: 13.0[0m
[32m[2022-09-20 12:50:22,825] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-1157[0m
[32m[2022-09-20 12:50:22,825] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:50:25,540] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-1157/tokenizer_config.json[0m
[32m[2022-09-20 12:50:25,540] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-1157/special_tokens_map.json[0m
[32m[2022-09-20 12:50:32,692] [    INFO][0m - loss: 0.00274281, learning_rate: 1.0449438202247191e-05, global_step: 1160, interval_runtime: 36.2563, interval_samples_per_second: 0.441, interval_steps_per_second: 0.276, epoch: 13.0337[0m
[32m[2022-09-20 12:50:39,072] [    INFO][0m - loss: 1.781e-05, learning_rate: 1.0280898876404495e-05, global_step: 1170, interval_runtime: 6.3794, interval_samples_per_second: 2.508, interval_steps_per_second: 1.568, epoch: 13.1461[0m
[32m[2022-09-20 12:50:45,471] [    INFO][0m - loss: 1.716e-05, learning_rate: 1.0112359550561798e-05, global_step: 1180, interval_runtime: 6.3995, interval_samples_per_second: 2.5, interval_steps_per_second: 1.563, epoch: 13.2584[0m
[32m[2022-09-20 12:50:51,853] [    INFO][0m - loss: 2.556e-05, learning_rate: 9.9438202247191e-06, global_step: 1190, interval_runtime: 6.3821, interval_samples_per_second: 2.507, interval_steps_per_second: 1.567, epoch: 13.3708[0m
[32m[2022-09-20 12:51:00,769] [    INFO][0m - loss: 1.216e-05, learning_rate: 9.775280898876405e-06, global_step: 1200, interval_runtime: 6.4026, interval_samples_per_second: 2.499, interval_steps_per_second: 1.562, epoch: 13.4831[0m
[32m[2022-09-20 12:51:07,159] [    INFO][0m - loss: 0.00550727, learning_rate: 9.606741573033707e-06, global_step: 1210, interval_runtime: 8.9031, interval_samples_per_second: 1.797, interval_steps_per_second: 1.123, epoch: 13.5955[0m
[32m[2022-09-20 12:51:13,554] [    INFO][0m - loss: 0.00021034, learning_rate: 9.438202247191012e-06, global_step: 1220, interval_runtime: 6.3946, interval_samples_per_second: 2.502, interval_steps_per_second: 1.564, epoch: 13.7079[0m
[32m[2022-09-20 12:51:19,956] [    INFO][0m - loss: 0.04233985, learning_rate: 9.269662921348314e-06, global_step: 1230, interval_runtime: 6.4022, interval_samples_per_second: 2.499, interval_steps_per_second: 1.562, epoch: 13.8202[0m
[32m[2022-09-20 12:51:26,365] [    INFO][0m - loss: 1.511e-05, learning_rate: 9.101123595505619e-06, global_step: 1240, interval_runtime: 6.4088, interval_samples_per_second: 2.497, interval_steps_per_second: 1.56, epoch: 13.9326[0m
[32m[2022-09-20 12:51:29,736] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:51:29,736] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-20 12:51:29,736] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:51:29,736] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:51:29,736] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-20 12:51:52,027] [    INFO][0m - eval_loss: 0.8915468454360962, eval_accuracy: 0.7475247524752475, eval_runtime: 22.29, eval_samples_per_second: 63.436, eval_steps_per_second: 3.993, epoch: 14.0[0m
[32m[2022-09-20 12:51:52,051] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-1246[0m
[32m[2022-09-20 12:51:52,051] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:51:54,755] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-1246/tokenizer_config.json[0m
[32m[2022-09-20 12:51:54,755] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-1246/special_tokens_map.json[0m
[32m[2022-09-20 12:52:02,544] [    INFO][0m - loss: 0.00587598, learning_rate: 8.932584269662921e-06, global_step: 1250, interval_runtime: 36.1792, interval_samples_per_second: 0.442, interval_steps_per_second: 0.276, epoch: 14.0449[0m
[32m[2022-09-20 12:52:08,923] [    INFO][0m - loss: 1.352e-05, learning_rate: 8.764044943820224e-06, global_step: 1260, interval_runtime: 6.3784, interval_samples_per_second: 2.508, interval_steps_per_second: 1.568, epoch: 14.1573[0m
[32m[2022-09-20 12:52:15,316] [    INFO][0m - loss: 1.3e-05, learning_rate: 8.595505617977528e-06, global_step: 1270, interval_runtime: 6.394, interval_samples_per_second: 2.502, interval_steps_per_second: 1.564, epoch: 14.2697[0m
[32m[2022-09-20 12:52:21,721] [    INFO][0m - loss: 1.681e-05, learning_rate: 8.426966292134832e-06, global_step: 1280, interval_runtime: 6.4044, interval_samples_per_second: 2.498, interval_steps_per_second: 1.561, epoch: 14.382[0m
[32m[2022-09-20 12:52:28,124] [    INFO][0m - loss: 0.00298106, learning_rate: 8.258426966292135e-06, global_step: 1290, interval_runtime: 6.4036, interval_samples_per_second: 2.499, interval_steps_per_second: 1.562, epoch: 14.4944[0m
[32m[2022-09-20 12:52:34,531] [    INFO][0m - loss: 5.791e-05, learning_rate: 8.089887640449438e-06, global_step: 1300, interval_runtime: 6.4068, interval_samples_per_second: 2.497, interval_steps_per_second: 1.561, epoch: 14.6067[0m
[32m[2022-09-20 12:52:40,934] [    INFO][0m - loss: 0.03279244, learning_rate: 7.921348314606742e-06, global_step: 1310, interval_runtime: 6.4026, interval_samples_per_second: 2.499, interval_steps_per_second: 1.562, epoch: 14.7191[0m
[32m[2022-09-20 12:52:47,351] [    INFO][0m - loss: 0.01339512, learning_rate: 7.752808988764046e-06, global_step: 1320, interval_runtime: 6.4167, interval_samples_per_second: 2.494, interval_steps_per_second: 1.558, epoch: 14.8315[0m
[32m[2022-09-20 12:52:53,744] [    INFO][0m - loss: 1.42e-05, learning_rate: 7.584269662921349e-06, global_step: 1330, interval_runtime: 6.3933, interval_samples_per_second: 2.503, interval_steps_per_second: 1.564, epoch: 14.9438[0m
[32m[2022-09-20 12:52:56,484] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:52:56,485] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-20 12:52:56,485] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:52:56,485] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:52:56,485] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-20 12:53:18,764] [    INFO][0m - eval_loss: 0.9047732353210449, eval_accuracy: 0.7524752475247525, eval_runtime: 22.2787, eval_samples_per_second: 63.469, eval_steps_per_second: 3.995, epoch: 15.0[0m
[32m[2022-09-20 12:53:18,788] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-1335[0m
[32m[2022-09-20 12:53:18,788] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:53:21,472] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-1335/tokenizer_config.json[0m
[32m[2022-09-20 12:53:21,472] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-1335/special_tokens_map.json[0m
[32m[2022-09-20 12:53:29,967] [    INFO][0m - loss: 0.00663996, learning_rate: 7.4157303370786515e-06, global_step: 1340, interval_runtime: 36.223, interval_samples_per_second: 0.442, interval_steps_per_second: 0.276, epoch: 15.0562[0m
[32m[2022-09-20 12:53:38,440] [    INFO][0m - loss: 2.19e-05, learning_rate: 7.247191011235956e-06, global_step: 1350, interval_runtime: 6.3853, interval_samples_per_second: 2.506, interval_steps_per_second: 1.566, epoch: 15.1685[0m
[32m[2022-09-20 12:53:44,823] [    INFO][0m - loss: 2.298e-05, learning_rate: 7.078651685393258e-06, global_step: 1360, interval_runtime: 8.4708, interval_samples_per_second: 1.889, interval_steps_per_second: 1.181, epoch: 15.2809[0m
[32m[2022-09-20 12:53:51,210] [    INFO][0m - loss: 3.235e-05, learning_rate: 6.910112359550562e-06, global_step: 1370, interval_runtime: 6.3875, interval_samples_per_second: 2.505, interval_steps_per_second: 1.566, epoch: 15.3933[0m
[32m[2022-09-20 12:53:57,604] [    INFO][0m - loss: 0.02191899, learning_rate: 6.741573033707865e-06, global_step: 1380, interval_runtime: 6.394, interval_samples_per_second: 2.502, interval_steps_per_second: 1.564, epoch: 15.5056[0m
[32m[2022-09-20 12:54:03,995] [    INFO][0m - loss: 1.62e-05, learning_rate: 6.573033707865169e-06, global_step: 1390, interval_runtime: 6.3906, interval_samples_per_second: 2.504, interval_steps_per_second: 1.565, epoch: 15.618[0m
[32m[2022-09-20 12:54:10,393] [    INFO][0m - loss: 2.297e-05, learning_rate: 6.404494382022472e-06, global_step: 1400, interval_runtime: 6.3978, interval_samples_per_second: 2.501, interval_steps_per_second: 1.563, epoch: 15.7303[0m
[32m[2022-09-20 12:54:16,794] [    INFO][0m - loss: 1.405e-05, learning_rate: 6.235955056179776e-06, global_step: 1410, interval_runtime: 6.4015, interval_samples_per_second: 2.499, interval_steps_per_second: 1.562, epoch: 15.8427[0m
[32m[2022-09-20 12:54:23,167] [    INFO][0m - loss: 1.974e-05, learning_rate: 6.067415730337078e-06, global_step: 1420, interval_runtime: 6.3723, interval_samples_per_second: 2.511, interval_steps_per_second: 1.569, epoch: 15.9551[0m
[32m[2022-09-20 12:54:25,292] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:54:25,293] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-20 12:54:25,293] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:54:25,293] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:54:25,293] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-20 12:54:47,604] [    INFO][0m - eval_loss: 0.8720380067825317, eval_accuracy: 0.7475247524752475, eval_runtime: 22.3108, eval_samples_per_second: 63.377, eval_steps_per_second: 3.989, epoch: 16.0[0m
[32m[2022-09-20 12:54:47,629] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-1424[0m
[32m[2022-09-20 12:54:47,629] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:54:50,325] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-1424/tokenizer_config.json[0m
[32m[2022-09-20 12:54:50,326] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-1424/special_tokens_map.json[0m
[32m[2022-09-20 12:54:59,355] [    INFO][0m - loss: 1.594e-05, learning_rate: 5.8988764044943826e-06, global_step: 1430, interval_runtime: 36.1881, interval_samples_per_second: 0.442, interval_steps_per_second: 0.276, epoch: 16.0674[0m
[32m[2022-09-20 12:55:05,730] [    INFO][0m - loss: 1.721e-05, learning_rate: 5.730337078651685e-06, global_step: 1440, interval_runtime: 6.3757, interval_samples_per_second: 2.51, interval_steps_per_second: 1.568, epoch: 16.1798[0m
[32m[2022-09-20 12:55:12,116] [    INFO][0m - loss: 1.184e-05, learning_rate: 5.5617977528089895e-06, global_step: 1450, interval_runtime: 6.386, interval_samples_per_second: 2.505, interval_steps_per_second: 1.566, epoch: 16.2921[0m
[32m[2022-09-20 12:55:18,531] [    INFO][0m - loss: 1.78e-05, learning_rate: 5.393258426966292e-06, global_step: 1460, interval_runtime: 6.4146, interval_samples_per_second: 2.494, interval_steps_per_second: 1.559, epoch: 16.4045[0m
[32m[2022-09-20 12:55:24,928] [    INFO][0m - loss: 0.0099467, learning_rate: 5.2247191011235955e-06, global_step: 1470, interval_runtime: 6.397, interval_samples_per_second: 2.501, interval_steps_per_second: 1.563, epoch: 16.5169[0m
[32m[2022-09-20 12:55:31,345] [    INFO][0m - loss: 1.473e-05, learning_rate: 5.056179775280899e-06, global_step: 1480, interval_runtime: 6.4172, interval_samples_per_second: 2.493, interval_steps_per_second: 1.558, epoch: 16.6292[0m
[32m[2022-09-20 12:55:37,783] [    INFO][0m - loss: 3.54e-05, learning_rate: 4.8876404494382024e-06, global_step: 1490, interval_runtime: 6.4374, interval_samples_per_second: 2.485, interval_steps_per_second: 1.553, epoch: 16.7416[0m
[32m[2022-09-20 12:55:44,217] [    INFO][0m - loss: 1.612e-05, learning_rate: 4.719101123595506e-06, global_step: 1500, interval_runtime: 6.434, interval_samples_per_second: 2.487, interval_steps_per_second: 1.554, epoch: 16.8539[0m
[32m[2022-09-20 12:55:50,589] [    INFO][0m - loss: 1.974e-05, learning_rate: 4.550561797752809e-06, global_step: 1510, interval_runtime: 6.3727, interval_samples_per_second: 2.511, interval_steps_per_second: 1.569, epoch: 16.9663[0m
[32m[2022-09-20 12:55:52,092] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:55:52,092] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-20 12:55:52,092] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:55:52,092] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:55:52,093] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-20 12:56:14,419] [    INFO][0m - eval_loss: 0.8983380198478699, eval_accuracy: 0.7475247524752475, eval_runtime: 22.3264, eval_samples_per_second: 63.333, eval_steps_per_second: 3.986, epoch: 17.0[0m
[32m[2022-09-20 12:56:14,444] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-1513[0m
[32m[2022-09-20 12:56:14,444] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:56:17,168] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-1513/tokenizer_config.json[0m
[32m[2022-09-20 12:56:17,168] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-1513/special_tokens_map.json[0m
[32m[2022-09-20 12:56:26,882] [    INFO][0m - loss: 4.52e-05, learning_rate: 4.382022471910112e-06, global_step: 1520, interval_runtime: 36.2927, interval_samples_per_second: 0.441, interval_steps_per_second: 0.276, epoch: 17.0787[0m
[32m[2022-09-20 12:56:34,892] [    INFO][0m - loss: 1.323e-05, learning_rate: 4.213483146067416e-06, global_step: 1530, interval_runtime: 6.3867, interval_samples_per_second: 2.505, interval_steps_per_second: 1.566, epoch: 17.191[0m
[32m[2022-09-20 12:56:41,277] [    INFO][0m - loss: 1.557e-05, learning_rate: 4.044943820224719e-06, global_step: 1540, interval_runtime: 8.008, interval_samples_per_second: 1.998, interval_steps_per_second: 1.249, epoch: 17.3034[0m
[32m[2022-09-20 12:56:47,674] [    INFO][0m - loss: 1.659e-05, learning_rate: 3.876404494382023e-06, global_step: 1550, interval_runtime: 6.3973, interval_samples_per_second: 2.501, interval_steps_per_second: 1.563, epoch: 17.4157[0m
[32m[2022-09-20 12:56:54,074] [    INFO][0m - loss: 0.01130075, learning_rate: 3.7078651685393257e-06, global_step: 1560, interval_runtime: 6.3998, interval_samples_per_second: 2.5, interval_steps_per_second: 1.563, epoch: 17.5281[0m
[32m[2022-09-20 12:57:00,480] [    INFO][0m - loss: 9.79e-06, learning_rate: 3.539325842696629e-06, global_step: 1570, interval_runtime: 6.4058, interval_samples_per_second: 2.498, interval_steps_per_second: 1.561, epoch: 17.6404[0m
[32m[2022-09-20 12:57:06,882] [    INFO][0m - loss: 1.137e-05, learning_rate: 3.3707865168539327e-06, global_step: 1580, interval_runtime: 6.4019, interval_samples_per_second: 2.499, interval_steps_per_second: 1.562, epoch: 17.7528[0m
[32m[2022-09-20 12:57:13,286] [    INFO][0m - loss: 1.051e-05, learning_rate: 3.202247191011236e-06, global_step: 1590, interval_runtime: 6.404, interval_samples_per_second: 2.498, interval_steps_per_second: 1.562, epoch: 17.8652[0m
[32m[2022-09-20 12:57:19,639] [    INFO][0m - loss: 1.205e-05, learning_rate: 3.033707865168539e-06, global_step: 1600, interval_runtime: 6.3529, interval_samples_per_second: 2.519, interval_steps_per_second: 1.574, epoch: 17.9775[0m
[32m[2022-09-20 12:57:20,517] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:57:20,517] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-20 12:57:20,517] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:57:20,517] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:57:20,518] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-20 12:57:42,901] [    INFO][0m - eval_loss: 0.9112317562103271, eval_accuracy: 0.7524752475247525, eval_runtime: 22.3835, eval_samples_per_second: 63.172, eval_steps_per_second: 3.976, epoch: 18.0[0m
[32m[2022-09-20 12:57:42,926] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-1602[0m
[32m[2022-09-20 12:57:42,926] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:57:45,618] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-1602/tokenizer_config.json[0m
[32m[2022-09-20 12:57:45,619] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-1602/special_tokens_map.json[0m
[32m[2022-09-20 12:57:55,831] [    INFO][0m - loss: 1.144e-05, learning_rate: 2.8651685393258426e-06, global_step: 1610, interval_runtime: 36.1928, interval_samples_per_second: 0.442, interval_steps_per_second: 0.276, epoch: 18.0899[0m
[32m[2022-09-20 12:58:02,204] [    INFO][0m - loss: 0.00204168, learning_rate: 2.696629213483146e-06, global_step: 1620, interval_runtime: 6.3729, interval_samples_per_second: 2.511, interval_steps_per_second: 1.569, epoch: 18.2022[0m
[32m[2022-09-20 12:58:08,587] [    INFO][0m - loss: 8.51e-06, learning_rate: 2.5280898876404495e-06, global_step: 1630, interval_runtime: 6.3823, interval_samples_per_second: 2.507, interval_steps_per_second: 1.567, epoch: 18.3146[0m
[32m[2022-09-20 12:58:14,985] [    INFO][0m - loss: 1.328e-05, learning_rate: 2.359550561797753e-06, global_step: 1640, interval_runtime: 6.3982, interval_samples_per_second: 2.501, interval_steps_per_second: 1.563, epoch: 18.427[0m
[32m[2022-09-20 12:58:21,381] [    INFO][0m - loss: 1.759e-05, learning_rate: 2.191011235955056e-06, global_step: 1650, interval_runtime: 6.3962, interval_samples_per_second: 2.501, interval_steps_per_second: 1.563, epoch: 18.5393[0m
[32m[2022-09-20 12:58:27,783] [    INFO][0m - loss: 1.061e-05, learning_rate: 2.0224719101123594e-06, global_step: 1660, interval_runtime: 6.4016, interval_samples_per_second: 2.499, interval_steps_per_second: 1.562, epoch: 18.6517[0m
[32m[2022-09-20 12:58:34,192] [    INFO][0m - loss: 1.326e-05, learning_rate: 1.8539325842696629e-06, global_step: 1670, interval_runtime: 6.4091, interval_samples_per_second: 2.496, interval_steps_per_second: 1.56, epoch: 18.764[0m
[32m[2022-09-20 12:58:40,585] [    INFO][0m - loss: 0.00067037, learning_rate: 1.6853932584269663e-06, global_step: 1680, interval_runtime: 6.3935, interval_samples_per_second: 2.503, interval_steps_per_second: 1.564, epoch: 18.8764[0m
[32m[2022-09-20 12:58:46,905] [    INFO][0m - loss: 1.18e-05, learning_rate: 1.5168539325842696e-06, global_step: 1690, interval_runtime: 6.3193, interval_samples_per_second: 2.532, interval_steps_per_second: 1.582, epoch: 18.9888[0m
[32m[2022-09-20 12:58:47,160] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 12:58:47,161] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-20 12:58:47,161] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 12:58:47,161] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 12:58:47,161] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-20 12:59:09,442] [    INFO][0m - eval_loss: 0.9119732975959778, eval_accuracy: 0.7376237623762376, eval_runtime: 22.281, eval_samples_per_second: 63.462, eval_steps_per_second: 3.994, epoch: 19.0[0m
[32m[2022-09-20 12:59:09,467] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-1691[0m
[32m[2022-09-20 12:59:09,467] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 12:59:12,180] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-1691/tokenizer_config.json[0m
[32m[2022-09-20 12:59:12,180] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-1691/special_tokens_map.json[0m
[32m[2022-09-20 12:59:22,957] [    INFO][0m - loss: 9.61e-06, learning_rate: 1.348314606741573e-06, global_step: 1700, interval_runtime: 36.0519, interval_samples_per_second: 0.444, interval_steps_per_second: 0.277, epoch: 19.1011[0m
[32m[2022-09-20 12:59:29,329] [    INFO][0m - loss: 2.225e-05, learning_rate: 1.1797752808988765e-06, global_step: 1710, interval_runtime: 6.3728, interval_samples_per_second: 2.511, interval_steps_per_second: 1.569, epoch: 19.2135[0m
[32m[2022-09-20 12:59:35,720] [    INFO][0m - loss: 1.075e-05, learning_rate: 1.0112359550561797e-06, global_step: 1720, interval_runtime: 6.3903, interval_samples_per_second: 2.504, interval_steps_per_second: 1.565, epoch: 19.3258[0m
[32m[2022-09-20 12:59:42,124] [    INFO][0m - loss: 0.00092339, learning_rate: 8.426966292134832e-07, global_step: 1730, interval_runtime: 6.4041, interval_samples_per_second: 2.498, interval_steps_per_second: 1.562, epoch: 19.4382[0m
[32m[2022-09-20 12:59:48,532] [    INFO][0m - loss: 7.61e-06, learning_rate: 6.741573033707865e-07, global_step: 1740, interval_runtime: 6.4083, interval_samples_per_second: 2.497, interval_steps_per_second: 1.56, epoch: 19.5506[0m
[32m[2022-09-20 12:59:54,944] [    INFO][0m - loss: 9.43e-06, learning_rate: 5.056179775280899e-07, global_step: 1750, interval_runtime: 6.4112, interval_samples_per_second: 2.496, interval_steps_per_second: 1.56, epoch: 19.6629[0m
[32m[2022-09-20 13:00:01,347] [    INFO][0m - loss: 1.309e-05, learning_rate: 3.3707865168539325e-07, global_step: 1760, interval_runtime: 6.4036, interval_samples_per_second: 2.499, interval_steps_per_second: 1.562, epoch: 19.7753[0m
[32m[2022-09-20 13:00:07,744] [    INFO][0m - loss: 1.047e-05, learning_rate: 1.6853932584269663e-07, global_step: 1770, interval_runtime: 6.3975, interval_samples_per_second: 2.501, interval_steps_per_second: 1.563, epoch: 19.8876[0m
[32m[2022-09-20 13:00:13,684] [    INFO][0m - loss: 1.276e-05, learning_rate: 0.0, global_step: 1780, interval_runtime: 5.9397, interval_samples_per_second: 2.694, interval_steps_per_second: 1.684, epoch: 20.0[0m
[32m[2022-09-20 13:00:13,685] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 13:00:13,685] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-20 13:00:13,685] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 13:00:13,685] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 13:00:13,685] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-20 13:00:36,098] [    INFO][0m - eval_loss: 0.9141749739646912, eval_accuracy: 0.7376237623762376, eval_runtime: 22.4124, eval_samples_per_second: 63.09, eval_steps_per_second: 3.971, epoch: 20.0[0m
[32m[2022-09-20 13:00:36,122] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-1780[0m
[32m[2022-09-20 13:00:36,123] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 13:00:38,861] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-1780/tokenizer_config.json[0m
[32m[2022-09-20 13:00:38,861] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-1780/special_tokens_map.json[0m
[32m[2022-09-20 13:00:44,013] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-20 13:00:44,013] [    INFO][0m - Loading best model from ./checkpoints_chid/checkpoint-623 (score: 0.7623762376237624).[0m
[32m[2022-09-20 13:00:45,486] [    INFO][0m - train_runtime: 1756.6917, train_samples_per_second: 16.098, train_steps_per_second: 1.013, train_loss: 0.09429396870682978, epoch: 20.0[0m
[32m[2022-09-20 13:00:45,541] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/[0m
[32m[2022-09-20 13:00:45,542] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 13:00:48,005] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/tokenizer_config.json[0m
[32m[2022-09-20 13:00:48,005] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/special_tokens_map.json[0m
[32m[2022-09-20 13:00:48,006] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-20 13:00:48,006] [    INFO][0m -   epoch                    =       20.0[0m
[32m[2022-09-20 13:00:48,006] [    INFO][0m -   train_loss               =     0.0943[0m
[32m[2022-09-20 13:00:48,006] [    INFO][0m -   train_runtime            = 0:29:16.69[0m
[32m[2022-09-20 13:00:48,006] [    INFO][0m -   train_samples_per_second =     16.098[0m
[32m[2022-09-20 13:00:48,006] [    INFO][0m -   train_steps_per_second   =      1.013[0m
[32m[2022-09-20 13:00:48,014] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-20 13:00:48,014] [    INFO][0m -   Num examples = 14014[0m
[32m[2022-09-20 13:00:48,014] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 13:00:48,014] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 13:00:48,015] [    INFO][0m -   Total prediction steps = 876[0m
[32m[2022-09-20 13:04:41,314] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-20 13:04:41,315] [    INFO][0m -   test_accuracy           =     0.7612[0m
[32m[2022-09-20 13:04:41,315] [    INFO][0m -   test_loss               =     0.6374[0m
[32m[2022-09-20 13:04:41,315] [    INFO][0m -   test_runtime            = 0:03:53.29[0m
[32m[2022-09-20 13:04:41,315] [    INFO][0m -   test_samples_per_second =     60.069[0m
[32m[2022-09-20 13:04:41,315] [    INFO][0m -   test_steps_per_second   =      3.755[0m
[32m[2022-09-20 13:04:41,316] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-20 13:04:41,316] [    INFO][0m -   Num examples = 14000[0m
[32m[2022-09-20 13:04:41,316] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 13:04:41,316] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 13:04:41,316] [    INFO][0m -   Total prediction steps = 875[0m
[32m[2022-09-20 13:08:45,007] [    INFO][0m - Predictions for chidf saved to ./fewclue_submit_examples.[0m
{
  "labels": 0,
  "text_a": "\u5927\u90e8\u5206\u7684\u5927\u529f\u544a\u6210\u90fd\u662f\u7531\u65e0\u6570\u4e2a\u9020\u8c23\u751f\u4e8b\u7d2f\u79ef\u800c\u6765,\u505a\u597d\u6bcf\u4e00\u4ef6\u5e94\u8be5\u505a\u597d\u7684\u5c0f\u4e8b\u7edd\u4e0d\u662f\u4e00\u4ef6\u5c0f\u4e8b\u3002 \u300a\u4f60\u597d\u674e\u7115\u82f1\u300b\u4e3b\u8981\u8bb2\u4e86\u8d3e\u73b2\u610f\u5916\u7a7f\u8d8a\u5230\u4e86\u6bcd\u4eb2\u5e74\u8f7b\u7684\u65f6\u5019,\u548c\u6bcd\u4eb2\u4e00\u8d77\u611f\u53d7\u4e86\u5979\u7684\u751f\u6d3b\u548c\u60c5\u611f\u3002\u8fd9\u6545\u4e8b\u662f\u6839\u636e\u8d3e\u73b2\u7684\u4eb2\u751f\u7ecf\u5386\u6539\u7f16\u800c\u6765, ...",
  "text_b": "",
  "uid": 24
}

Prediction done.
 
==========
csl
==========
 
[32m[2022-09-20 13:09:01,126] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-20 13:09:01,127] [    INFO][0m - ============================================================[0m
[32m[2022-09-20 13:09:01,127] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-20 13:09:01,127] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-20 13:09:01,127] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-20 13:09:01,127] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-20 13:09:01,127] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-20 13:09:01,127] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-20 13:09:01,127] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-20 13:09:01,127] [    INFO][0m - [0m
[32m[2022-09-20 13:09:01,127] [    INFO][0m - ============================================================[0m
[32m[2022-09-20 13:09:01,128] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-20 13:09:01,128] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-20 13:09:01,128] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-20 13:09:01,128] [    INFO][0m - pretrained                    :/ssd2/wanghuijuan03/data/zero-shot/checkpoints_09191451/checkpoint-43000/model_state.pdparams[0m
[32m[2022-09-20 13:09:01,128] [    INFO][0m - prompt                        :‚Äú{'text':'text_a'}‚ÄùÊú¨ÊñáÁöÑÂÜÖÂÆπ{'mask'}{'mask'}‚Äú{'text':'text_b'}‚Äù[0m
[32m[2022-09-20 13:09:01,128] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-20 13:09:01,128] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-20 13:09:01,128] [    INFO][0m - task_name                     :csl[0m
[32m[2022-09-20 13:09:01,128] [    INFO][0m - [0m
[32m[2022-09-20 13:09:01,128] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0920 13:09:01.129951 57557 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0920 13:09:01.133991 57557 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-20 13:09:06,072] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-20 13:09:06,083] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-20 13:09:06,083] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-20 13:09:06,083] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùÊú¨ÊñáÁöÑÂÜÖÂÆπ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': '‚Äù'}][0m
[32m[2022-09-20 13:09:07,488] [    INFO][0m - ============================================================[0m
[32m[2022-09-20 13:09:07,488] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-20 13:09:07,488] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-20 13:09:07,488] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-20 13:09:07,488] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-20 13:09:07,488] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-20 13:09:07,488] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-20 13:09:07,488] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-20 13:09:07,488] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-20 13:09:07,489] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-20 13:09:07,489] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-20 13:09:07,489] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-20 13:09:07,489] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-20 13:09:07,489] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-20 13:09:07,489] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-20 13:09:07,489] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-20 13:09:07,489] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-20 13:09:07,489] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-20 13:09:07,489] [    INFO][0m - eval_batch_size               :16[0m
[32m[2022-09-20 13:09:07,489] [    INFO][0m - eval_steps                    :200[0m
[32m[2022-09-20 13:09:07,489] [    INFO][0m - evaluation_strategy           :IntervalStrategy.EPOCH[0m
[32m[2022-09-20 13:09:07,489] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-20 13:09:07,490] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-20 13:09:07,490] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-20 13:09:07,490] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-20 13:09:07,490] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-20 13:09:07,490] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-20 13:09:07,490] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-20 13:09:07,490] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-20 13:09:07,490] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-20 13:09:07,490] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-09-20 13:09:07,490] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-20 13:09:07,490] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-20 13:09:07,490] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-20 13:09:07,490] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-20 13:09:07,490] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-20 13:09:07,491] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-20 13:09:07,491] [    INFO][0m - logging_dir                   :./checkpoints_csl/runs/Sep20_13-09-01_instance-3bwob41y-01[0m
[32m[2022-09-20 13:09:07,491] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-20 13:09:07,491] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-20 13:09:07,491] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-20 13:09:07,491] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-20 13:09:07,491] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-20 13:09:07,491] [    INFO][0m - max_seq_length                :320[0m
[32m[2022-09-20 13:09:07,491] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-20 13:09:07,491] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-20 13:09:07,491] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-20 13:09:07,491] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-20 13:09:07,491] [    INFO][0m - num_train_epochs              :20.0[0m
[32m[2022-09-20 13:09:07,492] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-20 13:09:07,492] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-20 13:09:07,492] [    INFO][0m - output_dir                    :./checkpoints_csl/[0m
[32m[2022-09-20 13:09:07,492] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-20 13:09:07,492] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-20 13:09:07,492] [    INFO][0m - per_device_eval_batch_size    :16[0m
[32m[2022-09-20 13:09:07,492] [    INFO][0m - per_device_train_batch_size   :16[0m
[32m[2022-09-20 13:09:07,492] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-20 13:09:07,492] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-20 13:09:07,492] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-20 13:09:07,492] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-09-20 13:09:07,492] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-20 13:09:07,492] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-20 13:09:07,492] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-20 13:09:07,493] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-20 13:09:07,493] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-20 13:09:07,493] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-20 13:09:07,493] [    INFO][0m - run_name                      :./checkpoints_csl/[0m
[32m[2022-09-20 13:09:07,493] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-20 13:09:07,493] [    INFO][0m - save_steps                    :200[0m
[32m[2022-09-20 13:09:07,493] [    INFO][0m - save_strategy                 :IntervalStrategy.EPOCH[0m
[32m[2022-09-20 13:09:07,493] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-20 13:09:07,493] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-20 13:09:07,493] [    INFO][0m - seed                          :42[0m
[32m[2022-09-20 13:09:07,493] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-20 13:09:07,493] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-20 13:09:07,493] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-20 13:09:07,494] [    INFO][0m - train_batch_size              :16[0m
[32m[2022-09-20 13:09:07,494] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-20 13:09:07,494] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-20 13:09:07,494] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-20 13:09:07,494] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-20 13:09:07,494] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-20 13:09:07,494] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-20 13:09:07,494] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-20 13:09:07,494] [    INFO][0m - [0m
[32m[2022-09-20 13:09:07,497] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-20 13:09:07,497] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 13:09:07,497] [    INFO][0m -   Num Epochs = 20[0m
[32m[2022-09-20 13:09:07,497] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2022-09-20 13:09:07,497] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2022-09-20 13:09:07,497] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-20 13:09:07,497] [    INFO][0m -   Total optimization steps = 200.0[0m
[32m[2022-09-20 13:09:07,497] [    INFO][0m -   Total num train samples = 3200[0m
[33m[2022-09-20 13:09:07,671] [ WARNING][0m - Token indices sequence length is longer than the specified maximum sequence length for this model (608 > 512). Running this sequence through the model will result in indexing errors[0m
[32m[2022-09-20 13:09:16,320] [    INFO][0m - loss: 1.50791826, learning_rate: 2.8499999999999998e-05, global_step: 10, interval_runtime: 8.8209, interval_samples_per_second: 1.814, interval_steps_per_second: 1.134, epoch: 1.0[0m
[32m[2022-09-20 13:09:16,321] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 13:09:16,321] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 13:09:16,321] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 13:09:16,321] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 13:09:16,322] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 13:09:19,512] [    INFO][0m - eval_loss: 0.707259476184845, eval_accuracy: 0.53125, eval_runtime: 3.1902, eval_samples_per_second: 50.153, eval_steps_per_second: 3.135, epoch: 1.0[0m
[32m[2022-09-20 13:09:19,513] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-10[0m
[32m[2022-09-20 13:09:19,513] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 13:09:22,490] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-10/tokenizer_config.json[0m
[32m[2022-09-20 13:09:22,491] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-10/special_tokens_map.json[0m
[32m[2022-09-20 13:09:36,042] [    INFO][0m - loss: 0.68511605, learning_rate: 2.7000000000000002e-05, global_step: 20, interval_runtime: 19.7229, interval_samples_per_second: 0.811, interval_steps_per_second: 0.507, epoch: 2.0[0m
[32m[2022-09-20 13:09:36,043] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 13:09:36,043] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 13:09:36,044] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 13:09:36,044] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 13:09:36,044] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 13:09:39,211] [    INFO][0m - eval_loss: 0.773779034614563, eval_accuracy: 0.54375, eval_runtime: 3.1666, eval_samples_per_second: 50.528, eval_steps_per_second: 3.158, epoch: 2.0[0m
[32m[2022-09-20 13:09:39,212] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-20[0m
[32m[2022-09-20 13:09:39,212] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 13:09:42,098] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-20/tokenizer_config.json[0m
[32m[2022-09-20 13:09:42,099] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-20/special_tokens_map.json[0m
[32m[2022-09-20 13:09:55,665] [    INFO][0m - loss: 0.46856503, learning_rate: 2.55e-05, global_step: 30, interval_runtime: 19.623, interval_samples_per_second: 0.815, interval_steps_per_second: 0.51, epoch: 3.0[0m
[32m[2022-09-20 13:09:55,666] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 13:09:55,666] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 13:09:55,666] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 13:09:55,667] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 13:09:55,667] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 13:09:58,859] [    INFO][0m - eval_loss: 0.8847559094429016, eval_accuracy: 0.58125, eval_runtime: 3.1917, eval_samples_per_second: 50.131, eval_steps_per_second: 3.133, epoch: 3.0[0m
[32m[2022-09-20 13:09:58,859] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-30[0m
[32m[2022-09-20 13:09:58,860] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 13:10:01,763] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-30/tokenizer_config.json[0m
[32m[2022-09-20 13:10:01,764] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-30/special_tokens_map.json[0m
[32m[2022-09-20 13:10:15,332] [    INFO][0m - loss: 0.33731718, learning_rate: 2.4e-05, global_step: 40, interval_runtime: 19.6663, interval_samples_per_second: 0.814, interval_steps_per_second: 0.508, epoch: 4.0[0m
[32m[2022-09-20 13:10:15,332] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 13:10:15,333] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 13:10:15,333] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 13:10:15,333] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 13:10:15,333] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 13:10:18,504] [    INFO][0m - eval_loss: 1.1829826831817627, eval_accuracy: 0.56875, eval_runtime: 3.1713, eval_samples_per_second: 50.452, eval_steps_per_second: 3.153, epoch: 4.0[0m
[32m[2022-09-20 13:10:18,505] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-40[0m
[32m[2022-09-20 13:10:18,505] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 13:10:21,431] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-40/tokenizer_config.json[0m
[32m[2022-09-20 13:10:21,431] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-40/special_tokens_map.json[0m
[32m[2022-09-20 13:10:35,056] [    INFO][0m - loss: 0.14062833, learning_rate: 2.25e-05, global_step: 50, interval_runtime: 19.724, interval_samples_per_second: 0.811, interval_steps_per_second: 0.507, epoch: 5.0[0m
[32m[2022-09-20 13:10:35,057] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 13:10:35,057] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 13:10:35,057] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 13:10:35,057] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 13:10:35,057] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 13:10:38,223] [    INFO][0m - eval_loss: 1.1771453619003296, eval_accuracy: 0.68125, eval_runtime: 3.1659, eval_samples_per_second: 50.538, eval_steps_per_second: 3.159, epoch: 5.0[0m
[32m[2022-09-20 13:10:39,439] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-50[0m
[32m[2022-09-20 13:10:39,440] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 13:10:42,300] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-50/tokenizer_config.json[0m
[32m[2022-09-20 13:10:42,300] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-50/special_tokens_map.json[0m
[32m[2022-09-20 13:10:56,703] [    INFO][0m - loss: 0.03062438, learning_rate: 2.1e-05, global_step: 60, interval_runtime: 21.6472, interval_samples_per_second: 0.739, interval_steps_per_second: 0.462, epoch: 6.0[0m
[32m[2022-09-20 13:10:56,704] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 13:10:56,704] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 13:10:56,704] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 13:10:56,704] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 13:10:56,704] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 13:10:59,883] [    INFO][0m - eval_loss: 2.1910481452941895, eval_accuracy: 0.6375, eval_runtime: 3.1781, eval_samples_per_second: 50.345, eval_steps_per_second: 3.147, epoch: 6.0[0m
[32m[2022-09-20 13:10:59,883] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-60[0m
[32m[2022-09-20 13:10:59,883] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 13:11:02,855] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-60/tokenizer_config.json[0m
[32m[2022-09-20 13:11:02,855] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-60/special_tokens_map.json[0m
[32m[2022-09-20 13:11:16,437] [    INFO][0m - loss: 0.05564634, learning_rate: 1.95e-05, global_step: 70, interval_runtime: 19.734, interval_samples_per_second: 0.811, interval_steps_per_second: 0.507, epoch: 7.0[0m
[32m[2022-09-20 13:11:16,438] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 13:11:16,438] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 13:11:16,438] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 13:11:16,438] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 13:11:16,438] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 13:11:19,626] [    INFO][0m - eval_loss: 4.258542537689209, eval_accuracy: 0.625, eval_runtime: 3.1874, eval_samples_per_second: 50.198, eval_steps_per_second: 3.137, epoch: 7.0[0m
[32m[2022-09-20 13:11:19,626] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-70[0m
[32m[2022-09-20 13:11:19,626] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 13:11:22,532] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-70/tokenizer_config.json[0m
[32m[2022-09-20 13:11:22,532] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-70/special_tokens_map.json[0m
[32m[2022-09-20 13:11:36,104] [    INFO][0m - loss: 0.01129219, learning_rate: 1.8e-05, global_step: 80, interval_runtime: 19.6676, interval_samples_per_second: 0.814, interval_steps_per_second: 0.508, epoch: 8.0[0m
[32m[2022-09-20 13:11:36,105] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 13:11:36,106] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 13:11:36,106] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 13:11:36,106] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 13:11:36,106] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 13:11:39,302] [    INFO][0m - eval_loss: 4.003885746002197, eval_accuracy: 0.64375, eval_runtime: 3.1963, eval_samples_per_second: 50.057, eval_steps_per_second: 3.129, epoch: 8.0[0m
[32m[2022-09-20 13:11:39,303] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-80[0m
[32m[2022-09-20 13:11:39,303] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 13:11:42,186] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-80/tokenizer_config.json[0m
[32m[2022-09-20 13:11:42,187] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-80/special_tokens_map.json[0m
[32m[2022-09-20 13:11:55,444] [    INFO][0m - loss: 0.00046466, learning_rate: 1.65e-05, global_step: 90, interval_runtime: 19.3397, interval_samples_per_second: 0.827, interval_steps_per_second: 0.517, epoch: 9.0[0m
[32m[2022-09-20 13:11:55,445] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 13:11:55,445] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 13:11:55,445] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 13:11:55,445] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 13:11:55,445] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 13:11:58,706] [    INFO][0m - eval_loss: 4.6464524269104, eval_accuracy: 0.63125, eval_runtime: 3.1738, eval_samples_per_second: 50.412, eval_steps_per_second: 3.151, epoch: 9.0[0m
[32m[2022-09-20 13:11:58,707] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-90[0m
[32m[2022-09-20 13:11:58,707] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 13:12:01,474] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-90/tokenizer_config.json[0m
[32m[2022-09-20 13:12:01,474] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-90/special_tokens_map.json[0m
[32m[2022-09-20 13:12:14,539] [    INFO][0m - loss: 7.873e-05, learning_rate: 1.5e-05, global_step: 100, interval_runtime: 19.0955, interval_samples_per_second: 0.838, interval_steps_per_second: 0.524, epoch: 10.0[0m
[32m[2022-09-20 13:12:14,540] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 13:12:14,540] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 13:12:14,540] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 13:12:14,540] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 13:12:14,540] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 13:12:17,718] [    INFO][0m - eval_loss: 4.692601680755615, eval_accuracy: 0.6125, eval_runtime: 3.1772, eval_samples_per_second: 50.359, eval_steps_per_second: 3.147, epoch: 10.0[0m
[32m[2022-09-20 13:12:17,719] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-100[0m
[32m[2022-09-20 13:12:17,719] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 13:12:23,774] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-20 13:12:23,778] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-20 13:12:37,584] [    INFO][0m - loss: 1.439e-05, learning_rate: 1.3500000000000001e-05, global_step: 110, interval_runtime: 23.0444, interval_samples_per_second: 0.694, interval_steps_per_second: 0.434, epoch: 11.0[0m
[32m[2022-09-20 13:12:37,585] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 13:12:37,585] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 13:12:37,585] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 13:12:37,585] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 13:12:37,585] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 13:12:40,761] [    INFO][0m - eval_loss: 4.7486572265625, eval_accuracy: 0.61875, eval_runtime: 3.1755, eval_samples_per_second: 50.386, eval_steps_per_second: 3.149, epoch: 11.0[0m
[32m[2022-09-20 13:12:40,761] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-110[0m
[32m[2022-09-20 13:12:40,762] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 13:12:43,595] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-110/tokenizer_config.json[0m
[32m[2022-09-20 13:12:43,596] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-110/special_tokens_map.json[0m
[32m[2022-09-20 13:12:56,704] [    INFO][0m - loss: 0.00029495, learning_rate: 1.2e-05, global_step: 120, interval_runtime: 19.1198, interval_samples_per_second: 0.837, interval_steps_per_second: 0.523, epoch: 12.0[0m
[32m[2022-09-20 13:12:56,705] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 13:12:56,705] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 13:12:56,705] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 13:12:56,705] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 13:12:56,705] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 13:12:59,876] [    INFO][0m - eval_loss: 5.832347869873047, eval_accuracy: 0.625, eval_runtime: 3.1704, eval_samples_per_second: 50.467, eval_steps_per_second: 3.154, epoch: 12.0[0m
[32m[2022-09-20 13:12:59,876] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-120[0m
[32m[2022-09-20 13:12:59,876] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 13:13:02,697] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-120/tokenizer_config.json[0m
[32m[2022-09-20 13:13:02,697] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-120/special_tokens_map.json[0m
[32m[2022-09-20 13:13:15,861] [    INFO][0m - loss: 8.299e-05, learning_rate: 1.05e-05, global_step: 130, interval_runtime: 19.1575, interval_samples_per_second: 0.835, interval_steps_per_second: 0.522, epoch: 13.0[0m
[32m[2022-09-20 13:13:15,862] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 13:13:15,862] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 13:13:15,862] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 13:13:15,862] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 13:13:15,863] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 13:13:19,037] [    INFO][0m - eval_loss: 6.548500061035156, eval_accuracy: 0.59375, eval_runtime: 3.1746, eval_samples_per_second: 50.4, eval_steps_per_second: 3.15, epoch: 13.0[0m
[32m[2022-09-20 13:13:19,038] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-130[0m
[32m[2022-09-20 13:13:19,038] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 13:13:21,715] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-130/tokenizer_config.json[0m
[32m[2022-09-20 13:13:21,715] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-130/special_tokens_map.json[0m
[32m[2022-09-20 13:13:35,131] [    INFO][0m - loss: 0.00023015, learning_rate: 9e-06, global_step: 140, interval_runtime: 19.0293, interval_samples_per_second: 0.841, interval_steps_per_second: 0.526, epoch: 14.0[0m
[32m[2022-09-20 13:13:35,132] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 13:13:35,132] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 13:13:35,132] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 13:13:35,132] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 13:13:35,132] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 13:13:38,304] [    INFO][0m - eval_loss: 5.008358478546143, eval_accuracy: 0.6625, eval_runtime: 3.1714, eval_samples_per_second: 50.451, eval_steps_per_second: 3.153, epoch: 14.0[0m
[32m[2022-09-20 13:13:40,317] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-140[0m
[32m[2022-09-20 13:13:40,318] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 13:13:42,970] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-140/tokenizer_config.json[0m
[32m[2022-09-20 13:13:42,970] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-140/special_tokens_map.json[0m
[32m[2022-09-20 13:13:56,037] [    INFO][0m - loss: 0.02661505, learning_rate: 7.5e-06, global_step: 150, interval_runtime: 21.1466, interval_samples_per_second: 0.757, interval_steps_per_second: 0.473, epoch: 15.0[0m
[32m[2022-09-20 13:13:56,037] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 13:13:56,038] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 13:13:56,038] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 13:13:56,038] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 13:13:56,038] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 13:13:59,190] [    INFO][0m - eval_loss: 4.305624485015869, eval_accuracy: 0.625, eval_runtime: 3.1518, eval_samples_per_second: 50.765, eval_steps_per_second: 3.173, epoch: 15.0[0m
[32m[2022-09-20 13:13:59,190] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-150[0m
[32m[2022-09-20 13:13:59,190] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 13:14:01,797] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-150/tokenizer_config.json[0m
[32m[2022-09-20 13:14:01,798] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-150/special_tokens_map.json[0m
[32m[2022-09-20 13:14:14,882] [    INFO][0m - loss: 6.276e-05, learning_rate: 6e-06, global_step: 160, interval_runtime: 18.8451, interval_samples_per_second: 0.849, interval_steps_per_second: 0.531, epoch: 16.0[0m
[32m[2022-09-20 13:14:14,883] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 13:14:14,883] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 13:14:14,883] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 13:14:14,883] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 13:14:14,884] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 13:14:18,047] [    INFO][0m - eval_loss: 6.044432640075684, eval_accuracy: 0.6625, eval_runtime: 3.1632, eval_samples_per_second: 50.581, eval_steps_per_second: 3.161, epoch: 16.0[0m
[32m[2022-09-20 13:14:18,047] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-160[0m
[32m[2022-09-20 13:14:18,047] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 13:14:20,643] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-160/tokenizer_config.json[0m
[32m[2022-09-20 13:14:20,643] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-160/special_tokens_map.json[0m
[32m[2022-09-20 13:14:33,690] [    INFO][0m - loss: 1.104e-05, learning_rate: 4.5e-06, global_step: 170, interval_runtime: 18.8081, interval_samples_per_second: 0.851, interval_steps_per_second: 0.532, epoch: 17.0[0m
[32m[2022-09-20 13:14:33,691] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 13:14:33,691] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 13:14:33,691] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 13:14:33,691] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 13:14:33,691] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 13:14:36,834] [    INFO][0m - eval_loss: 6.068107604980469, eval_accuracy: 0.64375, eval_runtime: 3.1431, eval_samples_per_second: 50.905, eval_steps_per_second: 3.182, epoch: 17.0[0m
[32m[2022-09-20 13:14:36,835] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-170[0m
[32m[2022-09-20 13:14:36,835] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 13:14:39,396] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-170/tokenizer_config.json[0m
[32m[2022-09-20 13:14:39,397] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-170/special_tokens_map.json[0m
[32m[2022-09-20 13:14:52,511] [    INFO][0m - loss: 8.96e-06, learning_rate: 3e-06, global_step: 180, interval_runtime: 18.8213, interval_samples_per_second: 0.85, interval_steps_per_second: 0.531, epoch: 18.0[0m
[32m[2022-09-20 13:14:52,512] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 13:14:52,512] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 13:14:52,512] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 13:14:52,512] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 13:14:52,512] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 13:14:55,673] [    INFO][0m - eval_loss: 5.9088826179504395, eval_accuracy: 0.63125, eval_runtime: 3.1602, eval_samples_per_second: 50.629, eval_steps_per_second: 3.164, epoch: 18.0[0m
[32m[2022-09-20 13:14:59,679] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-180[0m
[32m[2022-09-20 13:14:59,679] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 13:15:02,405] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-180/tokenizer_config.json[0m
[32m[2022-09-20 13:15:02,406] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-180/special_tokens_map.json[0m
[32m[2022-09-20 13:15:16,026] [    INFO][0m - loss: 1.244e-05, learning_rate: 1.5e-06, global_step: 190, interval_runtime: 23.5141, interval_samples_per_second: 0.68, interval_steps_per_second: 0.425, epoch: 19.0[0m
[32m[2022-09-20 13:15:16,027] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 13:15:16,027] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 13:15:16,027] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 13:15:16,027] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 13:15:16,027] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 13:15:19,199] [    INFO][0m - eval_loss: 5.742199897766113, eval_accuracy: 0.63125, eval_runtime: 3.1716, eval_samples_per_second: 50.448, eval_steps_per_second: 3.153, epoch: 19.0[0m
[32m[2022-09-20 13:15:19,200] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-190[0m
[32m[2022-09-20 13:15:19,200] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 13:15:21,887] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-190/tokenizer_config.json[0m
[32m[2022-09-20 13:15:21,888] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-190/special_tokens_map.json[0m
[32m[2022-09-20 13:15:34,785] [    INFO][0m - loss: 3.408e-05, learning_rate: 0.0, global_step: 200, interval_runtime: 18.7587, interval_samples_per_second: 0.853, interval_steps_per_second: 0.533, epoch: 20.0[0m
[32m[2022-09-20 13:15:34,785] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 13:15:34,785] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 13:15:34,785] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 13:15:34,785] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 13:15:34,786] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 13:15:40,083] [    INFO][0m - eval_loss: 5.672101974487305, eval_accuracy: 0.61875, eval_runtime: 3.1773, eval_samples_per_second: 50.358, eval_steps_per_second: 3.147, epoch: 20.0[0m
[32m[2022-09-20 13:15:40,084] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-200[0m
[32m[2022-09-20 13:15:40,084] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 13:15:42,714] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-200/tokenizer_config.json[0m
[32m[2022-09-20 13:15:42,715] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-200/special_tokens_map.json[0m
[32m[2022-09-20 13:15:47,656] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-20 13:15:47,656] [    INFO][0m - Loading best model from ./checkpoints_csl/checkpoint-50 (score: 0.68125).[0m
[32m[2022-09-20 13:15:49,293] [    INFO][0m - train_runtime: 401.7952, train_samples_per_second: 7.964, train_steps_per_second: 0.498, train_loss: 0.16325089815527463, epoch: 20.0[0m
[32m[2022-09-20 13:15:49,295] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/[0m
[32m[2022-09-20 13:15:49,295] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 13:15:51,764] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/tokenizer_config.json[0m
[32m[2022-09-20 13:15:51,764] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/special_tokens_map.json[0m
[32m[2022-09-20 13:15:51,766] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-20 13:15:51,766] [    INFO][0m -   epoch                    =       20.0[0m
[32m[2022-09-20 13:15:51,766] [    INFO][0m -   train_loss               =     0.1633[0m
[32m[2022-09-20 13:15:51,766] [    INFO][0m -   train_runtime            = 0:06:41.79[0m
[32m[2022-09-20 13:15:51,766] [    INFO][0m -   train_samples_per_second =      7.964[0m
[32m[2022-09-20 13:15:51,766] [    INFO][0m -   train_steps_per_second   =      0.498[0m
[32m[2022-09-20 13:15:51,769] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-20 13:15:51,769] [    INFO][0m -   Num examples = 2838[0m
[32m[2022-09-20 13:15:51,769] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 13:15:51,769] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 13:15:51,769] [    INFO][0m -   Total prediction steps = 178[0m
[32m[2022-09-20 13:16:49,147] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-20 13:16:49,147] [    INFO][0m -   test_accuracy           =     0.6427[0m
[32m[2022-09-20 13:16:49,147] [    INFO][0m -   test_loss               =     1.3682[0m
[32m[2022-09-20 13:16:49,147] [    INFO][0m -   test_runtime            = 0:00:57.37[0m
[32m[2022-09-20 13:16:49,147] [    INFO][0m -   test_samples_per_second =     49.462[0m
[32m[2022-09-20 13:16:49,147] [    INFO][0m -   test_steps_per_second   =      3.102[0m
[32m[2022-09-20 13:16:49,148] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-20 13:16:49,148] [    INFO][0m -   Num examples = 3000[0m
[32m[2022-09-20 13:16:49,148] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 13:16:49,148] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 13:16:49,148] [    INFO][0m -   Total prediction steps = 188[0m
[32m[2022-09-20 13:17:57,237] [    INFO][0m - Predictions for cslf saved to ./fewclue_submit_examples.[0m
{
  "labels": 1,
  "text_a": "\u65e0\u7ebf\u4f20\u611f\u5668\u7f51\u7edc\u4e2d\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u901a\u7528\u8fd1\u4f3c\u67e5\u8be2\u662f\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\uff0e\u6587\u4e2d\u63d0\u51fa\u4e00\u79cd\u65e0\u7ebf\u4f20\u611f\u5668\u7f51\u7edc\u4e2d\u9690\u79c1\u4fdd\u62a4\u901a\u7528\u8fd1\u4f3c\u67e5\u8be2\u534f\u8baePGAQ\uff0ePGAQ\u5c06\u4f20\u611f\u5668\u8282\u70b9\u7f16\u53f7\u548c\u5176\u91c7\u96c6\u6570\u636e\u9690\u85cf\u4e8e\u8bbe\u8ba1\u7684\u6570\u636e\u7ed3\u6784\u4e2d\uff0c\u5728\u57fa\u7ad9\u6784\u9020\u7ebf\u6027\u65b9\u7a0b\u7ec4\u89e3\u51fa\u76f4\u65b9\u56fe\uff0c\u6839\u636e\u76f4\u65b9\u56fe\u5177\u6709\u7684\u7edf\u8ba1\u4fe1\u606f\uff0c\u4e0d\u6cc4\u9732\u9690\u79c1\u5730\u5b8c\u6210Top-k\u67e5\u8be2\u3001\u8303\u56f4\u67e5\u8be2\u3001SUM\u3001MAX/MIN\u3001Median\u3001Histogram\u7b49\u8fd1\u4f3c\u67e5\u8be2\uff0ePGAQ\u4f7f\u7528\u7f51\u5185\u6c42\u548c\u805a\u96c6\u4ee5\u51cf\u5c11\u80fd\u91cf\u6d88\u8017\uff0c\u5e76\u4e14\u80fd\u591f\u901a\u8fc7\u8c03\u8282\u76f4\u65b9\u56fe\u5212\u5206\u7c92\u5ea6\u6765\u5e73\u8861\u67e5\u8be2\u7cbe\u5ea6\u4e0e\u80fd\u91cf\u6d88\u8017\uff0ePGAQ\u534f\u8bae\u5206\u4e3aH-PGAQ\u548cF-PGAQ\u4e24\u79cd\u6a21\u5f0f\uff0eH-PGAQ\u6a21\u5f0f\u4f7f\u7528\u6570\u636e\u6270\u52a8\u6280\u672f\u52a0\u5f3a\u6570\u636e\u5b89\u5168\u6027\uff0cF-PGAQ\u4f7f\u7528\u8fc7\u6ee4\u5668\u51cf\u5c11\u8fde\u7eed\u67e5\u8be2\u901a\u4fe1\u91cf\uff0e\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u4f7f\u7528\u771f\u5b9e\u6570\u636e\u96c6\u5b9e\u9a8c\u9a8c\u8bc1\u4e86PGAQ\u7684\u5b89\u5168\u6027\u548c\u6709\u6548\u6027\uff0e",
  "text_b": "\u65e0\u7ebf\u4f20\u611f\u5668\u7f51\u7edc\u3001\u6570\u636e\u805a\u96c6\u3001\u7269\u8054\u7f51\u3001\u8fd1\u4f3c\u67e5\u8be2",
  "uid": 0
}

Prediction done.
 
==========
cluewsc
==========
 
[32m[2022-09-20 13:18:15,012] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-20 13:18:15,013] [    INFO][0m - ============================================================[0m
[32m[2022-09-20 13:18:15,013] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-20 13:18:15,013] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-20 13:18:15,013] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-20 13:18:15,013] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-20 13:18:15,013] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-20 13:18:15,013] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-20 13:18:15,013] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-20 13:18:15,013] [    INFO][0m - [0m
[32m[2022-09-20 13:18:15,014] [    INFO][0m - ============================================================[0m
[32m[2022-09-20 13:18:15,014] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-20 13:18:15,014] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-20 13:18:15,014] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-20 13:18:15,014] [    INFO][0m - pretrained                    :/ssd2/wanghuijuan03/data/zero-shot/checkpoints_09191451/checkpoint-43000/model_state.pdparams[0m
[32m[2022-09-20 13:18:15,014] [    INFO][0m - prompt                        :‚Äú{'text':'text_a'}‚Äù{'text':'text_b'}ËøôÈáå‰ª£ËØç‰ΩøÁî®Ê≠£Á°ÆÂêóÔºü{'mask'}{'mask'}[0m
[32m[2022-09-20 13:18:15,014] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-20 13:18:15,014] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-20 13:18:15,014] [    INFO][0m - task_name                     :cluewsc[0m
[32m[2022-09-20 13:18:15,014] [    INFO][0m - [0m
[32m[2022-09-20 13:18:15,014] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0920 13:18:15.015995 70320 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0920 13:18:15.020023 70320 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-20 13:18:19,721] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-20 13:18:19,732] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-20 13:18:19,732] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-20 13:18:19,733] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚Äù'}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': 'ËøôÈáå‰ª£ËØç‰ΩøÁî®Ê≠£Á°ÆÂêóÔºü'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}][0m
[32m[2022-09-20 13:18:21,034] [    INFO][0m - ============================================================[0m
[32m[2022-09-20 13:18:21,034] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-20 13:18:21,034] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-20 13:18:21,035] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-20 13:18:21,035] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-20 13:18:21,035] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-20 13:18:21,035] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-20 13:18:21,035] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-20 13:18:21,035] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-20 13:18:21,035] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-20 13:18:21,035] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-20 13:18:21,035] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-20 13:18:21,035] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-20 13:18:21,035] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-20 13:18:21,035] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-20 13:18:21,036] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-20 13:18:21,036] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-20 13:18:21,036] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-20 13:18:21,036] [    INFO][0m - eval_batch_size               :16[0m
[32m[2022-09-20 13:18:21,036] [    INFO][0m - eval_steps                    :200[0m
[32m[2022-09-20 13:18:21,036] [    INFO][0m - evaluation_strategy           :IntervalStrategy.EPOCH[0m
[32m[2022-09-20 13:18:21,036] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-20 13:18:21,036] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-20 13:18:21,036] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-20 13:18:21,036] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-20 13:18:21,036] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-20 13:18:21,036] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-20 13:18:21,036] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-20 13:18:21,036] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-20 13:18:21,037] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-20 13:18:21,037] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-09-20 13:18:21,037] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-20 13:18:21,037] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-20 13:18:21,037] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-20 13:18:21,037] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-20 13:18:21,037] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-20 13:18:21,037] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-20 13:18:21,037] [    INFO][0m - logging_dir                   :./checkpoints_cluewsc/runs/Sep20_13-18-15_instance-3bwob41y-01[0m
[32m[2022-09-20 13:18:21,037] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-20 13:18:21,037] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-20 13:18:21,037] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-20 13:18:21,037] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-20 13:18:21,037] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-20 13:18:21,038] [    INFO][0m - max_seq_length                :128[0m
[32m[2022-09-20 13:18:21,038] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-20 13:18:21,038] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-20 13:18:21,038] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-20 13:18:21,038] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-20 13:18:21,038] [    INFO][0m - num_train_epochs              :20.0[0m
[32m[2022-09-20 13:18:21,038] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-20 13:18:21,038] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-20 13:18:21,038] [    INFO][0m - output_dir                    :./checkpoints_cluewsc/[0m
[32m[2022-09-20 13:18:21,038] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-20 13:18:21,038] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-20 13:18:21,038] [    INFO][0m - per_device_eval_batch_size    :16[0m
[32m[2022-09-20 13:18:21,038] [    INFO][0m - per_device_train_batch_size   :16[0m
[32m[2022-09-20 13:18:21,039] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-20 13:18:21,039] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-20 13:18:21,039] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-20 13:18:21,039] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-09-20 13:18:21,039] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-20 13:18:21,039] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-20 13:18:21,039] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-20 13:18:21,039] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-20 13:18:21,039] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-20 13:18:21,039] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-20 13:18:21,039] [    INFO][0m - run_name                      :./checkpoints_cluewsc/[0m
[32m[2022-09-20 13:18:21,039] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-20 13:18:21,039] [    INFO][0m - save_steps                    :200[0m
[32m[2022-09-20 13:18:21,039] [    INFO][0m - save_strategy                 :IntervalStrategy.EPOCH[0m
[32m[2022-09-20 13:18:21,040] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-20 13:18:21,040] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-20 13:18:21,040] [    INFO][0m - seed                          :42[0m
[32m[2022-09-20 13:18:21,040] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-20 13:18:21,040] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-20 13:18:21,040] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-20 13:18:21,040] [    INFO][0m - train_batch_size              :16[0m
[32m[2022-09-20 13:18:21,040] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-20 13:18:21,040] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-20 13:18:21,040] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-20 13:18:21,040] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-20 13:18:21,040] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-20 13:18:21,040] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-20 13:18:21,040] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-20 13:18:21,041] [    INFO][0m - [0m
[32m[2022-09-20 13:18:21,043] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-20 13:18:21,043] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-20 13:18:21,043] [    INFO][0m -   Num Epochs = 20[0m
[32m[2022-09-20 13:18:21,043] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2022-09-20 13:18:21,043] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2022-09-20 13:18:21,044] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-20 13:18:21,044] [    INFO][0m -   Total optimization steps = 200.0[0m
[32m[2022-09-20 13:18:21,044] [    INFO][0m -   Total num train samples = 3200[0m
[32m[2022-09-20 13:18:25,535] [    INFO][0m - loss: 1.44639177, learning_rate: 2.8499999999999998e-05, global_step: 10, interval_runtime: 4.4894, interval_samples_per_second: 3.564, interval_steps_per_second: 2.227, epoch: 1.0[0m
[32m[2022-09-20 13:18:25,537] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 13:18:25,537] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-20 13:18:25,537] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 13:18:25,537] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 13:18:25,538] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 13:18:26,932] [    INFO][0m - eval_loss: 0.7814619541168213, eval_accuracy: 0.5031446540880503, eval_runtime: 1.3945, eval_samples_per_second: 114.022, eval_steps_per_second: 7.171, epoch: 1.0[0m
[32m[2022-09-20 13:18:26,933] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-10[0m
[32m[2022-09-20 13:18:26,933] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 13:18:30,170] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-10/tokenizer_config.json[0m
[32m[2022-09-20 13:18:30,170] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-10/special_tokens_map.json[0m
[32m[2022-09-20 13:18:39,870] [    INFO][0m - loss: 0.75864449, learning_rate: 2.7000000000000002e-05, global_step: 20, interval_runtime: 14.3351, interval_samples_per_second: 1.116, interval_steps_per_second: 0.698, epoch: 2.0[0m
[32m[2022-09-20 13:18:39,871] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 13:18:39,871] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-20 13:18:39,871] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 13:18:39,871] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 13:18:39,871] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 13:18:41,246] [    INFO][0m - eval_loss: 0.8903201222419739, eval_accuracy: 0.5031446540880503, eval_runtime: 1.3743, eval_samples_per_second: 115.696, eval_steps_per_second: 7.277, epoch: 2.0[0m
[32m[2022-09-20 13:18:41,246] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-20[0m
[32m[2022-09-20 13:18:41,246] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 13:18:44,328] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-20/tokenizer_config.json[0m
[32m[2022-09-20 13:18:44,329] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-20/special_tokens_map.json[0m
[32m[2022-09-20 13:18:53,239] [    INFO][0m - loss: 0.73322654, learning_rate: 2.55e-05, global_step: 30, interval_runtime: 13.3696, interval_samples_per_second: 1.197, interval_steps_per_second: 0.748, epoch: 3.0[0m
[32m[2022-09-20 13:18:53,240] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 13:18:53,240] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-20 13:18:53,240] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 13:18:53,240] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 13:18:53,240] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 13:18:54,629] [    INFO][0m - eval_loss: 0.8253832459449768, eval_accuracy: 0.5031446540880503, eval_runtime: 1.3885, eval_samples_per_second: 114.516, eval_steps_per_second: 7.202, epoch: 3.0[0m
[32m[2022-09-20 13:18:54,629] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-30[0m
[32m[2022-09-20 13:18:54,630] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 13:18:57,493] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-30/tokenizer_config.json[0m
[32m[2022-09-20 13:18:57,494] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-30/special_tokens_map.json[0m
[32m[2022-09-20 13:19:06,794] [    INFO][0m - loss: 0.55751548, learning_rate: 2.4e-05, global_step: 40, interval_runtime: 13.5556, interval_samples_per_second: 1.18, interval_steps_per_second: 0.738, epoch: 4.0[0m
[32m[2022-09-20 13:19:06,795] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 13:19:06,795] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-20 13:19:06,795] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 13:19:06,796] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 13:19:06,796] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 13:19:08,164] [    INFO][0m - eval_loss: 0.670682430267334, eval_accuracy: 0.6037735849056604, eval_runtime: 1.3683, eval_samples_per_second: 116.199, eval_steps_per_second: 7.308, epoch: 4.0[0m
[32m[2022-09-20 13:19:08,165] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-40[0m
[32m[2022-09-20 13:19:08,165] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 13:19:11,076] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-40/tokenizer_config.json[0m
[32m[2022-09-20 13:19:11,078] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-40/special_tokens_map.json[0m
[32m[2022-09-20 13:19:21,460] [    INFO][0m - loss: 0.41246476, learning_rate: 2.25e-05, global_step: 50, interval_runtime: 14.6659, interval_samples_per_second: 1.091, interval_steps_per_second: 0.682, epoch: 5.0[0m
[32m[2022-09-20 13:19:21,461] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 13:19:21,461] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-20 13:19:21,461] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 13:19:21,461] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 13:19:21,461] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 13:19:22,846] [    INFO][0m - eval_loss: 0.9323678612709045, eval_accuracy: 0.5723270440251572, eval_runtime: 1.3837, eval_samples_per_second: 114.909, eval_steps_per_second: 7.227, epoch: 5.0[0m
[32m[2022-09-20 13:19:24,666] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-50[0m
[32m[2022-09-20 13:19:24,667] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 13:19:27,835] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-50/tokenizer_config.json[0m
[32m[2022-09-20 13:19:27,835] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-50/special_tokens_map.json[0m
[32m[2022-09-20 13:19:39,188] [    INFO][0m - loss: 0.17621622, learning_rate: 2.1e-05, global_step: 60, interval_runtime: 17.7276, interval_samples_per_second: 0.903, interval_steps_per_second: 0.564, epoch: 6.0[0m
[32m[2022-09-20 13:19:39,189] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 13:19:39,189] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-20 13:19:39,189] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 13:19:39,189] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 13:19:39,189] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 13:19:40,567] [    INFO][0m - eval_loss: 0.9828766584396362, eval_accuracy: 0.6729559748427673, eval_runtime: 1.378, eval_samples_per_second: 115.381, eval_steps_per_second: 7.257, epoch: 6.0[0m
[32m[2022-09-20 13:19:41,243] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-60[0m
[32m[2022-09-20 13:19:41,243] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 13:19:44,157] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-60/tokenizer_config.json[0m
[32m[2022-09-20 13:19:44,158] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-60/special_tokens_map.json[0m
[32m[2022-09-20 13:19:53,145] [    INFO][0m - loss: 0.06764656, learning_rate: 1.95e-05, global_step: 70, interval_runtime: 13.9569, interval_samples_per_second: 1.146, interval_steps_per_second: 0.716, epoch: 7.0[0m
[32m[2022-09-20 13:19:53,146] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 13:19:53,146] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-20 13:19:53,146] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 13:19:53,146] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 13:19:53,146] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 13:19:54,532] [    INFO][0m - eval_loss: 1.7969475984573364, eval_accuracy: 0.7295597484276729, eval_runtime: 1.3858, eval_samples_per_second: 114.735, eval_steps_per_second: 7.216, epoch: 7.0[0m
[32m[2022-09-20 13:19:54,533] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-70[0m
[32m[2022-09-20 13:19:54,533] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 13:19:58,103] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-70/tokenizer_config.json[0m
[32m[2022-09-20 13:19:58,103] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-70/special_tokens_map.json[0m
[32m[2022-09-20 13:20:07,663] [    INFO][0m - loss: 0.02896805, learning_rate: 1.8e-05, global_step: 80, interval_runtime: 14.518, interval_samples_per_second: 1.102, interval_steps_per_second: 0.689, epoch: 8.0[0m
[32m[2022-09-20 13:20:07,664] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 13:20:07,664] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-20 13:20:07,664] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 13:20:07,664] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 13:20:07,664] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 13:20:09,048] [    INFO][0m - eval_loss: 1.998517632484436, eval_accuracy: 0.7232704402515723, eval_runtime: 1.3836, eval_samples_per_second: 114.917, eval_steps_per_second: 7.228, epoch: 8.0[0m
[32m[2022-09-20 13:20:09,049] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-80[0m
[32m[2022-09-20 13:20:09,049] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 13:20:12,150] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-80/tokenizer_config.json[0m
[32m[2022-09-20 13:20:12,151] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-80/special_tokens_map.json[0m
[32m[2022-09-20 13:20:20,989] [    INFO][0m - loss: 0.11516449, learning_rate: 1.65e-05, global_step: 90, interval_runtime: 13.3259, interval_samples_per_second: 1.201, interval_steps_per_second: 0.75, epoch: 9.0[0m
[32m[2022-09-20 13:20:20,990] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 13:20:20,990] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-20 13:20:20,990] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 13:20:20,990] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 13:20:20,990] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 13:20:22,374] [    INFO][0m - eval_loss: 2.0969491004943848, eval_accuracy: 0.7358490566037735, eval_runtime: 1.3836, eval_samples_per_second: 114.919, eval_steps_per_second: 7.228, epoch: 9.0[0m
[32m[2022-09-20 13:20:22,374] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-90[0m
[32m[2022-09-20 13:20:22,375] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 13:20:25,276] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-90/tokenizer_config.json[0m
[32m[2022-09-20 13:20:25,276] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-90/special_tokens_map.json[0m
[32m[2022-09-20 13:20:33,992] [    INFO][0m - loss: 0.00284467, learning_rate: 1.5e-05, global_step: 100, interval_runtime: 13.0027, interval_samples_per_second: 1.231, interval_steps_per_second: 0.769, epoch: 10.0[0m
[32m[2022-09-20 13:20:33,992] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 13:20:33,992] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-20 13:20:33,993] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 13:20:33,993] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 13:20:33,993] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 13:20:35,368] [    INFO][0m - eval_loss: 1.740485668182373, eval_accuracy: 0.7358490566037735, eval_runtime: 1.3747, eval_samples_per_second: 115.665, eval_steps_per_second: 7.275, epoch: 10.0[0m
[32m[2022-09-20 13:20:35,368] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-100[0m
[32m[2022-09-20 13:20:35,368] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 13:20:38,134] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-20 13:20:38,135] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-20 13:20:46,747] [    INFO][0m - loss: 0.02731248, learning_rate: 1.3500000000000001e-05, global_step: 110, interval_runtime: 12.7556, interval_samples_per_second: 1.254, interval_steps_per_second: 0.784, epoch: 11.0[0m
[32m[2022-09-20 13:20:46,748] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 13:20:46,748] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-20 13:20:46,748] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 13:20:46,748] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 13:20:46,748] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 13:20:48,111] [    INFO][0m - eval_loss: 2.4240427017211914, eval_accuracy: 0.7547169811320755, eval_runtime: 1.3629, eval_samples_per_second: 116.664, eval_steps_per_second: 7.337, epoch: 11.0[0m
[32m[2022-09-20 13:20:48,112] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-110[0m
[32m[2022-09-20 13:20:48,112] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 13:20:50,760] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-110/tokenizer_config.json[0m
[32m[2022-09-20 13:20:50,761] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-110/special_tokens_map.json[0m
[32m[2022-09-20 13:20:59,455] [    INFO][0m - loss: 8.497e-05, learning_rate: 1.2e-05, global_step: 120, interval_runtime: 12.7079, interval_samples_per_second: 1.259, interval_steps_per_second: 0.787, epoch: 12.0[0m
[32m[2022-09-20 13:20:59,456] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 13:20:59,456] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-20 13:20:59,456] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 13:20:59,456] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 13:20:59,456] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 13:21:00,822] [    INFO][0m - eval_loss: 2.232773780822754, eval_accuracy: 0.7547169811320755, eval_runtime: 1.3654, eval_samples_per_second: 116.446, eval_steps_per_second: 7.324, epoch: 12.0[0m
[32m[2022-09-20 13:21:00,822] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-120[0m
[32m[2022-09-20 13:21:00,822] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 13:21:03,640] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-120/tokenizer_config.json[0m
[32m[2022-09-20 13:21:03,641] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-120/special_tokens_map.json[0m
[32m[2022-09-20 13:21:12,457] [    INFO][0m - loss: 2.209e-05, learning_rate: 1.05e-05, global_step: 130, interval_runtime: 13.0019, interval_samples_per_second: 1.231, interval_steps_per_second: 0.769, epoch: 13.0[0m
[32m[2022-09-20 13:21:12,457] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 13:21:12,457] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-20 13:21:12,457] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 13:21:12,458] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 13:21:12,458] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 13:21:13,819] [    INFO][0m - eval_loss: 2.2757108211517334, eval_accuracy: 0.7610062893081762, eval_runtime: 1.3609, eval_samples_per_second: 116.834, eval_steps_per_second: 7.348, epoch: 13.0[0m
[32m[2022-09-20 13:21:13,819] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-130[0m
[32m[2022-09-20 13:21:13,819] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 13:21:20,499] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-130/tokenizer_config.json[0m
[32m[2022-09-20 13:21:20,499] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-130/special_tokens_map.json[0m
[32m[2022-09-20 13:21:30,212] [    INFO][0m - loss: 1.519e-05, learning_rate: 9e-06, global_step: 140, interval_runtime: 16.7572, interval_samples_per_second: 0.955, interval_steps_per_second: 0.597, epoch: 14.0[0m
[32m[2022-09-20 13:21:30,212] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 13:21:30,212] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-20 13:21:30,212] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 13:21:30,212] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 13:21:30,213] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 13:21:31,574] [    INFO][0m - eval_loss: 2.323219060897827, eval_accuracy: 0.7610062893081762, eval_runtime: 1.3614, eval_samples_per_second: 116.789, eval_steps_per_second: 7.345, epoch: 14.0[0m
[32m[2022-09-20 13:21:31,575] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-140[0m
[32m[2022-09-20 13:21:31,575] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 13:21:34,279] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-140/tokenizer_config.json[0m
[32m[2022-09-20 13:21:34,279] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-140/special_tokens_map.json[0m
[32m[2022-09-20 13:21:45,985] [    INFO][0m - loss: 7.86e-06, learning_rate: 7.5e-06, global_step: 150, interval_runtime: 16.7711, interval_samples_per_second: 0.954, interval_steps_per_second: 0.596, epoch: 15.0[0m
[32m[2022-09-20 13:21:45,986] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 13:21:45,986] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-20 13:21:45,986] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 13:21:45,986] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 13:21:45,986] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 13:21:47,346] [    INFO][0m - eval_loss: 2.350738048553467, eval_accuracy: 0.7610062893081762, eval_runtime: 1.3595, eval_samples_per_second: 116.951, eval_steps_per_second: 7.355, epoch: 15.0[0m
[32m[2022-09-20 13:21:47,346] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-150[0m
[32m[2022-09-20 13:21:47,346] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 13:21:50,066] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-150/tokenizer_config.json[0m
[32m[2022-09-20 13:21:50,066] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-150/special_tokens_map.json[0m
[32m[2022-09-20 13:21:58,980] [    INFO][0m - loss: 1.399e-05, learning_rate: 6e-06, global_step: 160, interval_runtime: 12.9949, interval_samples_per_second: 1.231, interval_steps_per_second: 0.77, epoch: 16.0[0m
[32m[2022-09-20 13:21:58,980] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 13:21:58,980] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-20 13:21:58,981] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 13:21:58,981] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 13:21:58,981] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 13:22:00,683] [    INFO][0m - eval_loss: 2.3656320571899414, eval_accuracy: 0.7610062893081762, eval_runtime: 1.3725, eval_samples_per_second: 115.848, eval_steps_per_second: 7.286, epoch: 16.0[0m
[32m[2022-09-20 13:22:00,684] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-160[0m
[32m[2022-09-20 13:22:00,685] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 13:22:03,434] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-160/tokenizer_config.json[0m
[32m[2022-09-20 13:22:03,435] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-160/special_tokens_map.json[0m
[32m[2022-09-20 13:22:12,804] [    INFO][0m - loss: 0.00141178, learning_rate: 4.5e-06, global_step: 170, interval_runtime: 13.8241, interval_samples_per_second: 1.157, interval_steps_per_second: 0.723, epoch: 17.0[0m
[32m[2022-09-20 13:22:12,805] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 13:22:12,805] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-20 13:22:12,805] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 13:22:12,805] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 13:22:12,806] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 13:22:14,272] [    INFO][0m - eval_loss: 2.3576629161834717, eval_accuracy: 0.7610062893081762, eval_runtime: 1.3962, eval_samples_per_second: 113.883, eval_steps_per_second: 7.162, epoch: 17.0[0m
[32m[2022-09-20 13:22:14,272] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-170[0m
[32m[2022-09-20 13:22:14,272] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 13:22:17,233] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-170/tokenizer_config.json[0m
[32m[2022-09-20 13:22:17,233] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-170/special_tokens_map.json[0m
[32m[2022-09-20 13:22:25,938] [    INFO][0m - loss: 4.629e-05, learning_rate: 3e-06, global_step: 180, interval_runtime: 13.0703, interval_samples_per_second: 1.224, interval_steps_per_second: 0.765, epoch: 18.0[0m
[32m[2022-09-20 13:22:25,939] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 13:22:25,939] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-20 13:22:25,939] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 13:22:25,939] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 13:22:25,939] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 13:22:27,321] [    INFO][0m - eval_loss: 2.3190248012542725, eval_accuracy: 0.7610062893081762, eval_runtime: 1.3812, eval_samples_per_second: 115.118, eval_steps_per_second: 7.24, epoch: 18.0[0m
[32m[2022-09-20 13:22:27,321] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-180[0m
[32m[2022-09-20 13:22:27,321] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 13:22:30,070] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-180/tokenizer_config.json[0m
[32m[2022-09-20 13:22:30,070] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-180/special_tokens_map.json[0m
[32m[2022-09-20 13:22:39,003] [    INFO][0m - loss: 2.197e-05, learning_rate: 1.5e-06, global_step: 190, interval_runtime: 13.1285, interval_samples_per_second: 1.219, interval_steps_per_second: 0.762, epoch: 19.0[0m
[32m[2022-09-20 13:22:39,004] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 13:22:39,004] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-20 13:22:39,004] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 13:22:39,004] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 13:22:39,004] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 13:22:40,367] [    INFO][0m - eval_loss: 2.343992233276367, eval_accuracy: 0.7672955974842768, eval_runtime: 1.3627, eval_samples_per_second: 116.684, eval_steps_per_second: 7.339, epoch: 19.0[0m
[32m[2022-09-20 13:22:40,367] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-190[0m
[32m[2022-09-20 13:22:40,368] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 13:22:42,887] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-190/tokenizer_config.json[0m
[32m[2022-09-20 13:22:42,888] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-190/special_tokens_map.json[0m
[32m[2022-09-20 13:22:51,539] [    INFO][0m - loss: 5.434e-05, learning_rate: 0.0, global_step: 200, interval_runtime: 12.5367, interval_samples_per_second: 1.276, interval_steps_per_second: 0.798, epoch: 20.0[0m
[32m[2022-09-20 13:22:51,540] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 13:22:51,540] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-20 13:22:51,540] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 13:22:51,540] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 13:22:51,540] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-20 13:22:52,922] [    INFO][0m - eval_loss: 2.351381301879883, eval_accuracy: 0.7672955974842768, eval_runtime: 1.3813, eval_samples_per_second: 115.112, eval_steps_per_second: 7.24, epoch: 20.0[0m
[32m[2022-09-20 13:22:52,922] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-200[0m
[32m[2022-09-20 13:22:52,923] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 13:22:55,525] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-200/tokenizer_config.json[0m
[32m[2022-09-20 13:22:55,525] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-200/special_tokens_map.json[0m
[32m[2022-09-20 13:23:00,655] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-20 13:23:00,655] [    INFO][0m - Loading best model from ./checkpoints_cluewsc/checkpoint-190 (score: 0.7672955974842768).[0m
[32m[2022-09-20 13:23:04,408] [    INFO][0m - train_runtime: 281.229, train_samples_per_second: 11.379, train_steps_per_second: 0.711, train_loss: 0.21640369968627055, epoch: 20.0[0m
[32m[2022-09-20 13:23:04,459] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/[0m
[32m[2022-09-20 13:23:04,459] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 13:23:07,004] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/tokenizer_config.json[0m
[32m[2022-09-20 13:23:07,005] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/special_tokens_map.json[0m
[32m[2022-09-20 13:23:07,006] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-20 13:23:07,006] [    INFO][0m -   epoch                    =       20.0[0m
[32m[2022-09-20 13:23:07,006] [    INFO][0m -   train_loss               =     0.2164[0m
[32m[2022-09-20 13:23:07,006] [    INFO][0m -   train_runtime            = 0:04:41.22[0m
[32m[2022-09-20 13:23:07,006] [    INFO][0m -   train_samples_per_second =     11.379[0m
[32m[2022-09-20 13:23:07,006] [    INFO][0m -   train_steps_per_second   =      0.711[0m
[32m[2022-09-20 13:23:07,009] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-20 13:23:07,009] [    INFO][0m -   Num examples = 976[0m
[32m[2022-09-20 13:23:07,009] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 13:23:07,009] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 13:23:07,009] [    INFO][0m -   Total prediction steps = 61[0m
[32m[2022-09-20 13:23:15,785] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-20 13:23:15,786] [    INFO][0m -   test_accuracy           =     0.7633[0m
[32m[2022-09-20 13:23:15,786] [    INFO][0m -   test_loss               =     2.2706[0m
[32m[2022-09-20 13:23:15,786] [    INFO][0m -   test_runtime            = 0:00:08.77[0m
[32m[2022-09-20 13:23:15,786] [    INFO][0m -   test_samples_per_second =    111.204[0m
[32m[2022-09-20 13:23:15,786] [    INFO][0m -   test_steps_per_second   =       6.95[0m
[32m[2022-09-20 13:23:15,786] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-20 13:23:15,786] [    INFO][0m -   Num examples = 290[0m
[32m[2022-09-20 13:23:15,786] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 13:23:15,787] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 13:23:15,787] [    INFO][0m -   Total prediction steps = 19[0m
[32m[2022-09-20 13:23:18,651] [    INFO][0m - Predictions for cluewscf saved to ./fewclue_submit_examples.[0m
{
  "labels": 0,
  "text_a": "\u4e3a\u4ec0\u4e48\u8981\u51fa\u73b0\u4e00\u4e2a\u8eab\u7a7f\u519b\u88c5\u7684\u9ad8\u5927\u7537\u4eba\uff1f\u5c31\u50cf\u4e00\u7247\u6811\u53f6\u98d8\u5165\u4e86\u6811\u6797\uff0c\u4ed6\u8d70\u5230\u4e86\u6211\u7684\u5bb6\u4eba\u4e2d\u95f4\u3002",
  "text_b": "\u5176\u4e2d\u4ed6\u6307\u7684\u662f\u6811\u53f6"
}

Prediction done.
