 
==========
eprstmt
==========
 
[32m[2022-09-16 16:38:54,219] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-16 16:38:54,220] [    INFO][0m - ============================================================[0m
[32m[2022-09-16 16:38:54,220] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-16 16:38:54,220] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-16 16:38:54,220] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-16 16:38:54,220] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-16 16:38:54,220] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-16 16:38:54,220] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-16 16:38:54,220] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-16 16:38:54,220] [    INFO][0m - [0m
[32m[2022-09-16 16:38:54,220] [    INFO][0m - ============================================================[0m
[32m[2022-09-16 16:38:54,220] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-16 16:38:54,221] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-16 16:38:54,221] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-16 16:38:54,221] [    INFO][0m - pretrained                    :../checkpoints_cmnli/checkpoint-22000/model_state.pdparams[0m
[32m[2022-09-16 16:38:54,221] [    INFO][0m - prompt                        :‚Äú{'text':'text_a'}‚ÄùËøôÊù°ËØÑËÆ∫ÁöÑÊÉÖÊÑüÂÄæÂêëÊòØ{'mask'}{'mask'}ÁöÑ„ÄÇ[0m
[32m[2022-09-16 16:38:54,221] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-16 16:38:54,221] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-16 16:38:54,221] [    INFO][0m - task_name                     :eprstmt[0m
[32m[2022-09-16 16:38:54,221] [    INFO][0m - [0m
[32m[2022-09-16 16:38:54,221] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0916 16:38:54.222864 62027 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0916 16:38:54.226894 62027 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-16 16:39:01,980] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-16 16:39:01,996] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-16 16:39:01,996] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-16 16:39:01,997] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùËøôÊù°ËØÑËÆ∫ÁöÑÊÉÖÊÑüÂÄæÂêëÊòØ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'ÁöÑ„ÄÇ'}][0m
[32m[2022-09-16 16:39:04,112] [    INFO][0m - ============================================================[0m
[32m[2022-09-16 16:39:04,112] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-16 16:39:04,112] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-16 16:39:04,113] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-16 16:39:04,113] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-16 16:39:04,113] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-16 16:39:04,113] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-16 16:39:04,113] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-16 16:39:04,113] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-16 16:39:04,113] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-16 16:39:04,113] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-16 16:39:04,113] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-16 16:39:04,113] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-16 16:39:04,113] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-16 16:39:04,114] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-16 16:39:04,114] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-16 16:39:04,114] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-16 16:39:04,114] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-16 16:39:04,114] [    INFO][0m - eval_batch_size               :16[0m
[32m[2022-09-16 16:39:04,114] [    INFO][0m - eval_steps                    :200[0m
[32m[2022-09-16 16:39:04,114] [    INFO][0m - evaluation_strategy           :IntervalStrategy.EPOCH[0m
[32m[2022-09-16 16:39:04,114] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-16 16:39:04,114] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-16 16:39:04,114] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-16 16:39:04,114] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-16 16:39:04,114] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-16 16:39:04,115] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-16 16:39:04,115] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-16 16:39:04,115] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-16 16:39:04,115] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-16 16:39:04,115] [    INFO][0m - learning_rate                 :3e-06[0m
[32m[2022-09-16 16:39:04,115] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-16 16:39:04,115] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-16 16:39:04,115] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-16 16:39:04,115] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-16 16:39:04,115] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-16 16:39:04,115] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-16 16:39:04,115] [    INFO][0m - logging_dir                   :./checkpoints_eprstmt/runs/Sep16_16-38-54_instance-3bwob41y-01[0m
[32m[2022-09-16 16:39:04,115] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-16 16:39:04,116] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-16 16:39:04,116] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-16 16:39:04,116] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-16 16:39:04,116] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-16 16:39:04,116] [    INFO][0m - max_seq_length                :128[0m
[32m[2022-09-16 16:39:04,116] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-16 16:39:04,116] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-16 16:39:04,116] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-16 16:39:04,116] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-16 16:39:04,116] [    INFO][0m - num_train_epochs              :30.0[0m
[32m[2022-09-16 16:39:04,116] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-16 16:39:04,116] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-16 16:39:04,116] [    INFO][0m - output_dir                    :./checkpoints_eprstmt/[0m
[32m[2022-09-16 16:39:04,117] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-16 16:39:04,117] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-16 16:39:04,117] [    INFO][0m - per_device_eval_batch_size    :16[0m
[32m[2022-09-16 16:39:04,117] [    INFO][0m - per_device_train_batch_size   :16[0m
[32m[2022-09-16 16:39:04,117] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-16 16:39:04,117] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-16 16:39:04,117] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-16 16:39:04,117] [    INFO][0m - ppt_learning_rate             :3e-05[0m
[32m[2022-09-16 16:39:04,117] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-16 16:39:04,117] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-16 16:39:04,117] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-16 16:39:04,117] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-16 16:39:04,117] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-16 16:39:04,118] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-16 16:39:04,118] [    INFO][0m - run_name                      :./checkpoints_eprstmt/[0m
[32m[2022-09-16 16:39:04,118] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-16 16:39:04,118] [    INFO][0m - save_steps                    :200[0m
[32m[2022-09-16 16:39:04,118] [    INFO][0m - save_strategy                 :IntervalStrategy.EPOCH[0m
[32m[2022-09-16 16:39:04,118] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-16 16:39:04,118] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-16 16:39:04,118] [    INFO][0m - seed                          :42[0m
[32m[2022-09-16 16:39:04,118] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-16 16:39:04,118] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-16 16:39:04,118] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-16 16:39:04,118] [    INFO][0m - train_batch_size              :16[0m
[32m[2022-09-16 16:39:04,118] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-16 16:39:04,118] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-16 16:39:04,119] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-16 16:39:04,119] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-16 16:39:04,119] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-16 16:39:04,119] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-16 16:39:04,119] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-16 16:39:04,119] [    INFO][0m - [0m
[32m[2022-09-16 16:39:04,122] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-16 16:39:04,122] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 16:39:04,122] [    INFO][0m -   Num Epochs = 30[0m
[32m[2022-09-16 16:39:04,122] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2022-09-16 16:39:04,122] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2022-09-16 16:39:04,122] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-16 16:39:04,122] [    INFO][0m -   Total optimization steps = 300.0[0m
[32m[2022-09-16 16:39:04,123] [    INFO][0m -   Total num train samples = 4800[0m
[32m[2022-09-16 16:39:08,419] [    INFO][0m - loss: 11.98670349, learning_rate: 2.9e-06, global_step: 10, interval_runtime: 4.2953, interval_samples_per_second: 3.725, interval_steps_per_second: 2.328, epoch: 1.0[0m
[32m[2022-09-16 16:39:08,420] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 16:39:08,420] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 16:39:08,420] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 16:39:08,421] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 16:39:08,421] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 16:39:09,759] [    INFO][0m - eval_loss: 9.007827758789062, eval_accuracy: 0.5125, eval_runtime: 1.3377, eval_samples_per_second: 119.612, eval_steps_per_second: 7.476, epoch: 1.0[0m
[32m[2022-09-16 16:39:09,759] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-10[0m
[32m[2022-09-16 16:39:09,760] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 16:39:12,535] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-10/tokenizer_config.json[0m
[32m[2022-09-16 16:39:12,535] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-10/special_tokens_map.json[0m
[32m[2022-09-16 16:39:21,570] [    INFO][0m - loss: 6.79923935, learning_rate: 2.8000000000000003e-06, global_step: 20, interval_runtime: 13.1514, interval_samples_per_second: 1.217, interval_steps_per_second: 0.76, epoch: 2.0[0m
[32m[2022-09-16 16:39:21,571] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 16:39:21,571] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 16:39:21,571] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 16:39:21,571] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 16:39:21,572] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 16:39:22,891] [    INFO][0m - eval_loss: 3.869274139404297, eval_accuracy: 0.58125, eval_runtime: 1.3191, eval_samples_per_second: 121.299, eval_steps_per_second: 7.581, epoch: 2.0[0m
[32m[2022-09-16 16:39:22,892] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-20[0m
[32m[2022-09-16 16:39:22,892] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 16:39:25,521] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-20/tokenizer_config.json[0m
[32m[2022-09-16 16:39:25,521] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-20/special_tokens_map.json[0m
[32m[2022-09-16 16:39:34,407] [    INFO][0m - loss: 2.82402592, learning_rate: 2.7e-06, global_step: 30, interval_runtime: 12.8362, interval_samples_per_second: 1.246, interval_steps_per_second: 0.779, epoch: 3.0[0m
[32m[2022-09-16 16:39:34,407] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 16:39:34,408] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 16:39:34,408] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 16:39:34,408] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 16:39:34,408] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 16:39:35,881] [    INFO][0m - eval_loss: 1.4764288663864136, eval_accuracy: 0.7625, eval_runtime: 1.3225, eval_samples_per_second: 120.986, eval_steps_per_second: 7.562, epoch: 3.0[0m
[32m[2022-09-16 16:39:35,882] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-30[0m
[32m[2022-09-16 16:39:35,882] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 16:39:38,604] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-30/tokenizer_config.json[0m
[32m[2022-09-16 16:39:38,605] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-30/special_tokens_map.json[0m
[32m[2022-09-16 16:39:47,684] [    INFO][0m - loss: 1.02907753, learning_rate: 2.6e-06, global_step: 40, interval_runtime: 13.2775, interval_samples_per_second: 1.205, interval_steps_per_second: 0.753, epoch: 4.0[0m
[32m[2022-09-16 16:39:47,685] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 16:39:47,685] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 16:39:47,685] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 16:39:47,685] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 16:39:47,685] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 16:39:49,008] [    INFO][0m - eval_loss: 0.4415934681892395, eval_accuracy: 0.86875, eval_runtime: 1.3229, eval_samples_per_second: 120.95, eval_steps_per_second: 7.559, epoch: 4.0[0m
[32m[2022-09-16 16:39:49,214] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-40[0m
[32m[2022-09-16 16:39:49,214] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 16:39:51,844] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-40/tokenizer_config.json[0m
[32m[2022-09-16 16:39:51,845] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-40/special_tokens_map.json[0m
[32m[2022-09-16 16:40:00,296] [    INFO][0m - loss: 0.35514529, learning_rate: 2.5e-06, global_step: 50, interval_runtime: 12.612, interval_samples_per_second: 1.269, interval_steps_per_second: 0.793, epoch: 5.0[0m
[32m[2022-09-16 16:40:00,297] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 16:40:00,297] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 16:40:00,297] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 16:40:00,297] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 16:40:00,297] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 16:40:03,812] [    INFO][0m - eval_loss: 0.29562050104141235, eval_accuracy: 0.90625, eval_runtime: 1.3186, eval_samples_per_second: 121.336, eval_steps_per_second: 7.584, epoch: 5.0[0m
[32m[2022-09-16 16:40:03,813] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-50[0m
[32m[2022-09-16 16:40:03,813] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 16:40:06,449] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-50/tokenizer_config.json[0m
[32m[2022-09-16 16:40:06,450] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-50/special_tokens_map.json[0m
[32m[2022-09-16 16:40:15,646] [    INFO][0m - loss: 0.21457882, learning_rate: 2.4000000000000003e-06, global_step: 60, interval_runtime: 15.0125, interval_samples_per_second: 1.066, interval_steps_per_second: 0.666, epoch: 6.0[0m
[32m[2022-09-16 16:40:15,647] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 16:40:15,647] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 16:40:15,647] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 16:40:15,648] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 16:40:15,648] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 16:40:16,972] [    INFO][0m - eval_loss: 0.2944781184196472, eval_accuracy: 0.8625, eval_runtime: 1.3241, eval_samples_per_second: 120.833, eval_steps_per_second: 7.552, epoch: 6.0[0m
[32m[2022-09-16 16:40:16,973] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-60[0m
[32m[2022-09-16 16:40:16,973] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 16:40:19,912] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-60/tokenizer_config.json[0m
[32m[2022-09-16 16:40:19,912] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-60/special_tokens_map.json[0m
[32m[2022-09-16 16:40:28,758] [    INFO][0m - loss: 0.14068346, learning_rate: 2.3000000000000004e-06, global_step: 70, interval_runtime: 13.4497, interval_samples_per_second: 1.19, interval_steps_per_second: 0.744, epoch: 7.0[0m
[32m[2022-09-16 16:40:28,759] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 16:40:28,759] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 16:40:28,759] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 16:40:28,759] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 16:40:28,760] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 16:40:30,083] [    INFO][0m - eval_loss: 0.31799715757369995, eval_accuracy: 0.86875, eval_runtime: 1.3235, eval_samples_per_second: 120.889, eval_steps_per_second: 7.556, epoch: 7.0[0m
[32m[2022-09-16 16:40:30,084] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-70[0m
[32m[2022-09-16 16:40:30,084] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 16:40:32,686] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-70/tokenizer_config.json[0m
[32m[2022-09-16 16:40:32,687] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-70/special_tokens_map.json[0m
[32m[2022-09-16 16:40:41,052] [    INFO][0m - loss: 0.10010716, learning_rate: 2.1999999999999997e-06, global_step: 80, interval_runtime: 12.2938, interval_samples_per_second: 1.301, interval_steps_per_second: 0.813, epoch: 8.0[0m
[32m[2022-09-16 16:40:41,053] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 16:40:41,053] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 16:40:41,053] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 16:40:41,053] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 16:40:41,053] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 16:40:43,765] [    INFO][0m - eval_loss: 0.39586496353149414, eval_accuracy: 0.89375, eval_runtime: 1.3281, eval_samples_per_second: 120.469, eval_steps_per_second: 7.529, epoch: 8.0[0m
[32m[2022-09-16 16:40:43,765] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-80[0m
[32m[2022-09-16 16:40:43,766] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 16:40:46,514] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-80/tokenizer_config.json[0m
[32m[2022-09-16 16:40:46,514] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-80/special_tokens_map.json[0m
[32m[2022-09-16 16:40:57,120] [    INFO][0m - loss: 0.08869336, learning_rate: 2.1e-06, global_step: 90, interval_runtime: 14.1543, interval_samples_per_second: 1.13, interval_steps_per_second: 0.706, epoch: 9.0[0m
[32m[2022-09-16 16:40:57,121] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 16:40:57,121] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 16:40:57,121] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 16:40:57,121] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 16:40:57,121] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 16:40:58,442] [    INFO][0m - eval_loss: 0.5097410082817078, eval_accuracy: 0.8625, eval_runtime: 1.3201, eval_samples_per_second: 121.203, eval_steps_per_second: 7.575, epoch: 9.0[0m
[32m[2022-09-16 16:40:58,442] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-90[0m
[32m[2022-09-16 16:40:58,442] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 16:41:01,231] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-90/tokenizer_config.json[0m
[32m[2022-09-16 16:41:01,231] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-90/special_tokens_map.json[0m
[32m[2022-09-16 16:41:13,796] [    INFO][0m - loss: 0.07718397, learning_rate: 2e-06, global_step: 100, interval_runtime: 18.5893, interval_samples_per_second: 0.861, interval_steps_per_second: 0.538, epoch: 10.0[0m
[32m[2022-09-16 16:41:13,796] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 16:41:13,797] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 16:41:13,797] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 16:41:13,797] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 16:41:13,797] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 16:41:15,124] [    INFO][0m - eval_loss: 0.5375559329986572, eval_accuracy: 0.875, eval_runtime: 1.3266, eval_samples_per_second: 120.61, eval_steps_per_second: 7.538, epoch: 10.0[0m
[32m[2022-09-16 16:41:15,124] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-100[0m
[32m[2022-09-16 16:41:15,124] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 16:41:17,774] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-16 16:41:17,774] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-16 16:41:27,193] [    INFO][0m - loss: 0.13126466, learning_rate: 1.9e-06, global_step: 110, interval_runtime: 13.3977, interval_samples_per_second: 1.194, interval_steps_per_second: 0.746, epoch: 11.0[0m
[32m[2022-09-16 16:41:27,194] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 16:41:27,195] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 16:41:27,195] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 16:41:27,195] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 16:41:27,195] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 16:41:28,522] [    INFO][0m - eval_loss: 0.7428551912307739, eval_accuracy: 0.85625, eval_runtime: 1.3271, eval_samples_per_second: 120.566, eval_steps_per_second: 7.535, epoch: 11.0[0m
[32m[2022-09-16 16:41:28,523] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-110[0m
[32m[2022-09-16 16:41:28,523] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 16:41:32,208] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-110/tokenizer_config.json[0m
[32m[2022-09-16 16:41:32,208] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-110/special_tokens_map.json[0m
[32m[2022-09-16 16:41:41,259] [    INFO][0m - loss: 0.0925964, learning_rate: 1.8e-06, global_step: 120, interval_runtime: 14.0655, interval_samples_per_second: 1.138, interval_steps_per_second: 0.711, epoch: 12.0[0m
[32m[2022-09-16 16:41:41,260] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 16:41:41,260] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 16:41:41,260] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 16:41:41,260] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 16:41:41,260] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 16:41:42,592] [    INFO][0m - eval_loss: 0.6989039182662964, eval_accuracy: 0.875, eval_runtime: 1.3321, eval_samples_per_second: 120.113, eval_steps_per_second: 7.507, epoch: 12.0[0m
[32m[2022-09-16 16:41:42,593] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-120[0m
[32m[2022-09-16 16:41:42,593] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 16:41:45,606] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-120/tokenizer_config.json[0m
[32m[2022-09-16 16:41:45,607] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-120/special_tokens_map.json[0m
[32m[2022-09-16 16:41:54,267] [    INFO][0m - loss: 0.08411547, learning_rate: 1.7e-06, global_step: 130, interval_runtime: 13.0081, interval_samples_per_second: 1.23, interval_steps_per_second: 0.769, epoch: 13.0[0m
[32m[2022-09-16 16:41:54,268] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 16:41:54,268] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 16:41:54,268] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 16:41:54,268] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 16:41:54,268] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 16:41:55,594] [    INFO][0m - eval_loss: 0.6448068022727966, eval_accuracy: 0.8875, eval_runtime: 1.3251, eval_samples_per_second: 120.743, eval_steps_per_second: 7.546, epoch: 13.0[0m
[32m[2022-09-16 16:41:56,796] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-130[0m
[32m[2022-09-16 16:41:56,796] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 16:41:59,688] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-130/tokenizer_config.json[0m
[32m[2022-09-16 16:41:59,688] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-130/special_tokens_map.json[0m
[32m[2022-09-16 16:42:08,456] [    INFO][0m - loss: 0.07143754, learning_rate: 1.6e-06, global_step: 140, interval_runtime: 14.1887, interval_samples_per_second: 1.128, interval_steps_per_second: 0.705, epoch: 14.0[0m
[32m[2022-09-16 16:42:08,457] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 16:42:08,457] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 16:42:08,457] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 16:42:08,457] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 16:42:08,457] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 16:42:09,788] [    INFO][0m - eval_loss: 0.7226930856704712, eval_accuracy: 0.8625, eval_runtime: 1.3303, eval_samples_per_second: 120.276, eval_steps_per_second: 7.517, epoch: 14.0[0m
[32m[2022-09-16 16:42:09,789] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-140[0m
[32m[2022-09-16 16:42:09,789] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 16:42:12,566] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-140/tokenizer_config.json[0m
[32m[2022-09-16 16:42:12,566] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-140/special_tokens_map.json[0m
[32m[2022-09-16 16:42:21,194] [    INFO][0m - loss: 0.0640445, learning_rate: 1.5e-06, global_step: 150, interval_runtime: 12.738, interval_samples_per_second: 1.256, interval_steps_per_second: 0.785, epoch: 15.0[0m
[32m[2022-09-16 16:42:21,194] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 16:42:21,195] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 16:42:21,195] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 16:42:21,195] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 16:42:21,195] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 16:42:22,549] [    INFO][0m - eval_loss: 0.6912941932678223, eval_accuracy: 0.86875, eval_runtime: 1.354, eval_samples_per_second: 118.169, eval_steps_per_second: 7.386, epoch: 15.0[0m
[32m[2022-09-16 16:42:22,549] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-150[0m
[32m[2022-09-16 16:42:22,550] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 16:42:25,428] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-150/tokenizer_config.json[0m
[32m[2022-09-16 16:42:25,429] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-150/special_tokens_map.json[0m
[32m[2022-09-16 16:42:35,356] [    INFO][0m - loss: 0.06254092, learning_rate: 1.4000000000000001e-06, global_step: 160, interval_runtime: 12.5557, interval_samples_per_second: 1.274, interval_steps_per_second: 0.796, epoch: 16.0[0m
[32m[2022-09-16 16:42:35,357] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 16:42:35,357] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 16:42:35,358] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 16:42:35,358] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 16:42:35,358] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 16:42:36,685] [    INFO][0m - eval_loss: 0.7302755117416382, eval_accuracy: 0.86875, eval_runtime: 1.3266, eval_samples_per_second: 120.61, eval_steps_per_second: 7.538, epoch: 16.0[0m
[32m[2022-09-16 16:42:36,685] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-160[0m
[32m[2022-09-16 16:42:36,685] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 16:42:39,306] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-160/tokenizer_config.json[0m
[32m[2022-09-16 16:42:39,306] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-160/special_tokens_map.json[0m
[32m[2022-09-16 16:42:51,783] [    INFO][0m - loss: 0.05870739, learning_rate: 1.3e-06, global_step: 170, interval_runtime: 18.0338, interval_samples_per_second: 0.887, interval_steps_per_second: 0.555, epoch: 17.0[0m
[32m[2022-09-16 16:42:51,784] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 16:42:51,784] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 16:42:51,784] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 16:42:51,784] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 16:42:51,785] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 16:42:53,116] [    INFO][0m - eval_loss: 0.7165374755859375, eval_accuracy: 0.88125, eval_runtime: 1.3307, eval_samples_per_second: 120.235, eval_steps_per_second: 7.515, epoch: 17.0[0m
[32m[2022-09-16 16:42:53,116] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-170[0m
[32m[2022-09-16 16:42:53,116] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 16:42:55,757] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-170/tokenizer_config.json[0m
[32m[2022-09-16 16:42:55,757] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-170/special_tokens_map.json[0m
[32m[2022-09-16 16:43:04,037] [    INFO][0m - loss: 0.05261009, learning_rate: 1.2000000000000002e-06, global_step: 180, interval_runtime: 12.2543, interval_samples_per_second: 1.306, interval_steps_per_second: 0.816, epoch: 18.0[0m
[32m[2022-09-16 16:43:04,038] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 16:43:04,038] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 16:43:04,038] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 16:43:04,038] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 16:43:04,039] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 16:43:05,358] [    INFO][0m - eval_loss: 0.7301039695739746, eval_accuracy: 0.8875, eval_runtime: 1.3197, eval_samples_per_second: 121.239, eval_steps_per_second: 7.577, epoch: 18.0[0m
[32m[2022-09-16 16:43:06,877] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-180[0m
[32m[2022-09-16 16:43:06,877] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 16:43:09,598] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-180/tokenizer_config.json[0m
[32m[2022-09-16 16:43:09,599] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-180/special_tokens_map.json[0m
[32m[2022-09-16 16:43:18,460] [    INFO][0m - loss: 0.04933577, learning_rate: 1.0999999999999998e-06, global_step: 190, interval_runtime: 14.422, interval_samples_per_second: 1.109, interval_steps_per_second: 0.693, epoch: 19.0[0m
[32m[2022-09-16 16:43:18,460] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 16:43:18,461] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 16:43:18,461] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 16:43:18,461] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 16:43:18,461] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 16:43:19,790] [    INFO][0m - eval_loss: 0.7535605430603027, eval_accuracy: 0.89375, eval_runtime: 1.3287, eval_samples_per_second: 120.415, eval_steps_per_second: 7.526, epoch: 19.0[0m
[32m[2022-09-16 16:43:19,804] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-190[0m
[32m[2022-09-16 16:43:19,804] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 16:43:22,418] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-190/tokenizer_config.json[0m
[32m[2022-09-16 16:43:22,419] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-190/special_tokens_map.json[0m
[32m[2022-09-16 16:43:31,152] [    INFO][0m - loss: 0.02918312, learning_rate: 1e-06, global_step: 200, interval_runtime: 12.693, interval_samples_per_second: 1.261, interval_steps_per_second: 0.788, epoch: 20.0[0m
[32m[2022-09-16 16:43:32,106] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 16:43:32,106] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 16:43:32,106] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 16:43:32,107] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 16:43:32,107] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 16:43:33,436] [    INFO][0m - eval_loss: 0.7716298699378967, eval_accuracy: 0.8875, eval_runtime: 1.3293, eval_samples_per_second: 120.365, eval_steps_per_second: 7.523, epoch: 20.0[0m
[32m[2022-09-16 16:43:33,437] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-200[0m
[32m[2022-09-16 16:43:33,437] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 16:43:36,338] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-200/tokenizer_config.json[0m
[32m[2022-09-16 16:43:36,339] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-200/special_tokens_map.json[0m
[32m[2022-09-16 16:43:45,049] [    INFO][0m - loss: 0.02500622, learning_rate: 9e-07, global_step: 210, interval_runtime: 13.8964, interval_samples_per_second: 1.151, interval_steps_per_second: 0.72, epoch: 21.0[0m
[32m[2022-09-16 16:43:45,050] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 16:43:45,050] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 16:43:45,050] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 16:43:45,050] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 16:43:45,050] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 16:43:46,372] [    INFO][0m - eval_loss: 0.8364835977554321, eval_accuracy: 0.875, eval_runtime: 1.3213, eval_samples_per_second: 121.092, eval_steps_per_second: 7.568, epoch: 21.0[0m
[32m[2022-09-16 16:43:46,372] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-210[0m
[32m[2022-09-16 16:43:46,372] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 16:43:48,944] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-210/tokenizer_config.json[0m
[32m[2022-09-16 16:43:48,944] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-210/special_tokens_map.json[0m
[32m[2022-09-16 16:43:57,520] [    INFO][0m - loss: 0.02523414, learning_rate: 8e-07, global_step: 220, interval_runtime: 12.4709, interval_samples_per_second: 1.283, interval_steps_per_second: 0.802, epoch: 22.0[0m
[32m[2022-09-16 16:43:57,521] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 16:43:57,521] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 16:43:57,521] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 16:43:57,521] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 16:43:57,521] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 16:43:58,838] [    INFO][0m - eval_loss: 0.8686038255691528, eval_accuracy: 0.875, eval_runtime: 1.3162, eval_samples_per_second: 121.567, eval_steps_per_second: 7.598, epoch: 22.0[0m
[32m[2022-09-16 16:43:58,838] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-220[0m
[32m[2022-09-16 16:43:58,838] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 16:44:01,841] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-220/tokenizer_config.json[0m
[32m[2022-09-16 16:44:01,841] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-220/special_tokens_map.json[0m
[32m[2022-09-16 16:44:10,518] [    INFO][0m - loss: 0.021965, learning_rate: 7.000000000000001e-07, global_step: 230, interval_runtime: 12.9987, interval_samples_per_second: 1.231, interval_steps_per_second: 0.769, epoch: 23.0[0m
[32m[2022-09-16 16:44:10,519] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 16:44:10,520] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 16:44:10,520] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 16:44:10,520] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 16:44:10,520] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 16:44:11,845] [    INFO][0m - eval_loss: 0.8798933029174805, eval_accuracy: 0.86875, eval_runtime: 1.325, eval_samples_per_second: 120.758, eval_steps_per_second: 7.547, epoch: 23.0[0m
[32m[2022-09-16 16:44:11,846] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-230[0m
[32m[2022-09-16 16:44:11,846] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 16:44:14,427] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-230/tokenizer_config.json[0m
[32m[2022-09-16 16:44:14,427] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-230/special_tokens_map.json[0m
[32m[2022-09-16 16:44:23,175] [    INFO][0m - loss: 0.01338461, learning_rate: 6.000000000000001e-07, global_step: 240, interval_runtime: 12.6569, interval_samples_per_second: 1.264, interval_steps_per_second: 0.79, epoch: 24.0[0m
[32m[2022-09-16 16:44:23,176] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 16:44:23,176] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 16:44:23,177] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 16:44:23,177] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 16:44:23,177] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 16:44:24,492] [    INFO][0m - eval_loss: 0.9029350280761719, eval_accuracy: 0.875, eval_runtime: 1.315, eval_samples_per_second: 121.677, eval_steps_per_second: 7.605, epoch: 24.0[0m
[32m[2022-09-16 16:44:24,492] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-240[0m
[32m[2022-09-16 16:44:24,492] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 16:44:27,251] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-240/tokenizer_config.json[0m
[32m[2022-09-16 16:44:27,252] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-240/special_tokens_map.json[0m
[32m[2022-09-16 16:44:36,555] [    INFO][0m - loss: 0.01126847, learning_rate: 5e-07, global_step: 250, interval_runtime: 13.3794, interval_samples_per_second: 1.196, interval_steps_per_second: 0.747, epoch: 25.0[0m
[32m[2022-09-16 16:44:36,555] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 16:44:36,556] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 16:44:36,556] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 16:44:36,556] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 16:44:36,556] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 16:44:37,892] [    INFO][0m - eval_loss: 0.9281266927719116, eval_accuracy: 0.8875, eval_runtime: 1.3354, eval_samples_per_second: 119.813, eval_steps_per_second: 7.488, epoch: 25.0[0m
[32m[2022-09-16 16:44:37,892] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-250[0m
[32m[2022-09-16 16:44:37,893] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 16:44:40,582] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-250/tokenizer_config.json[0m
[32m[2022-09-16 16:44:40,582] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-250/special_tokens_map.json[0m
[32m[2022-09-16 16:44:49,097] [    INFO][0m - loss: 0.00449607, learning_rate: 4e-07, global_step: 260, interval_runtime: 12.5425, interval_samples_per_second: 1.276, interval_steps_per_second: 0.797, epoch: 26.0[0m
[32m[2022-09-16 16:44:49,098] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 16:44:49,098] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 16:44:49,098] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 16:44:49,099] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 16:44:49,099] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 16:44:50,415] [    INFO][0m - eval_loss: 0.9588476419448853, eval_accuracy: 0.8875, eval_runtime: 1.3164, eval_samples_per_second: 121.544, eval_steps_per_second: 7.597, epoch: 26.0[0m
[32m[2022-09-16 16:44:50,416] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-260[0m
[32m[2022-09-16 16:44:50,416] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 16:44:53,364] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-260/tokenizer_config.json[0m
[32m[2022-09-16 16:44:53,364] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-260/special_tokens_map.json[0m
[32m[2022-09-16 16:45:01,534] [    INFO][0m - loss: 0.00588349, learning_rate: 3.0000000000000004e-07, global_step: 270, interval_runtime: 12.4365, interval_samples_per_second: 1.287, interval_steps_per_second: 0.804, epoch: 27.0[0m
[32m[2022-09-16 16:45:01,534] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 16:45:01,534] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 16:45:01,534] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 16:45:01,534] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 16:45:01,535] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 16:45:02,850] [    INFO][0m - eval_loss: 0.9806914329528809, eval_accuracy: 0.9, eval_runtime: 1.3152, eval_samples_per_second: 121.657, eval_steps_per_second: 7.604, epoch: 27.0[0m
[32m[2022-09-16 16:45:02,850] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-270[0m
[32m[2022-09-16 16:45:02,850] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 16:45:05,439] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-270/tokenizer_config.json[0m
[32m[2022-09-16 16:45:05,440] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-270/special_tokens_map.json[0m
[32m[2022-09-16 16:45:13,569] [    INFO][0m - loss: 0.00818122, learning_rate: 2e-07, global_step: 280, interval_runtime: 12.0353, interval_samples_per_second: 1.329, interval_steps_per_second: 0.831, epoch: 28.0[0m
[32m[2022-09-16 16:45:13,570] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 16:45:13,570] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 16:45:13,570] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 16:45:13,570] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 16:45:13,570] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 16:45:14,882] [    INFO][0m - eval_loss: 1.0004316568374634, eval_accuracy: 0.9, eval_runtime: 1.3113, eval_samples_per_second: 122.016, eval_steps_per_second: 7.626, epoch: 28.0[0m
[32m[2022-09-16 16:45:14,882] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-280[0m
[32m[2022-09-16 16:45:14,882] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 16:45:17,339] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-280/tokenizer_config.json[0m
[32m[2022-09-16 16:45:17,339] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-280/special_tokens_map.json[0m
[32m[2022-09-16 16:45:25,536] [    INFO][0m - loss: 0.00834839, learning_rate: 1e-07, global_step: 290, interval_runtime: 11.9666, interval_samples_per_second: 1.337, interval_steps_per_second: 0.836, epoch: 29.0[0m
[32m[2022-09-16 16:45:25,536] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 16:45:25,536] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 16:45:25,536] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 16:45:25,536] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 16:45:25,537] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 16:45:26,854] [    INFO][0m - eval_loss: 1.0089771747589111, eval_accuracy: 0.89375, eval_runtime: 1.3173, eval_samples_per_second: 121.461, eval_steps_per_second: 7.591, epoch: 29.0[0m
[32m[2022-09-16 16:45:26,854] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-290[0m
[32m[2022-09-16 16:45:26,854] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 16:45:29,376] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-290/tokenizer_config.json[0m
[32m[2022-09-16 16:45:29,376] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-290/special_tokens_map.json[0m
[32m[2022-09-16 16:45:37,507] [    INFO][0m - loss: 0.01026946, learning_rate: 0.0, global_step: 300, interval_runtime: 11.9713, interval_samples_per_second: 1.337, interval_steps_per_second: 0.835, epoch: 30.0[0m
[32m[2022-09-16 16:45:37,508] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 16:45:37,508] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 16:45:37,508] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 16:45:37,508] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 16:45:37,508] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 16:45:38,819] [    INFO][0m - eval_loss: 1.0115838050842285, eval_accuracy: 0.89375, eval_runtime: 1.3112, eval_samples_per_second: 122.022, eval_steps_per_second: 7.626, epoch: 30.0[0m
[32m[2022-09-16 16:45:38,820] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-300[0m
[32m[2022-09-16 16:45:38,820] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 16:45:41,272] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-300/tokenizer_config.json[0m
[32m[2022-09-16 16:45:41,272] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-300/special_tokens_map.json[0m
[32m[2022-09-16 16:45:46,020] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-16 16:45:46,020] [    INFO][0m - Loading best model from ./checkpoints_eprstmt/checkpoint-50 (score: 0.90625).[0m
[32m[2022-09-16 16:45:47,582] [    INFO][0m - train_runtime: 403.459, train_samples_per_second: 11.897, train_steps_per_second: 0.744, train_loss: 0.8148437094688415, epoch: 30.0[0m
[32m[2022-09-16 16:45:47,638] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/[0m
[32m[2022-09-16 16:45:47,639] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 16:45:50,140] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/tokenizer_config.json[0m
[32m[2022-09-16 16:45:50,141] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/special_tokens_map.json[0m
[32m[2022-09-16 16:45:50,142] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-16 16:45:50,142] [    INFO][0m -   epoch                    =       30.0[0m
[32m[2022-09-16 16:45:50,142] [    INFO][0m -   train_loss               =     0.8148[0m
[32m[2022-09-16 16:45:50,142] [    INFO][0m -   train_runtime            = 0:06:43.45[0m
[32m[2022-09-16 16:45:50,142] [    INFO][0m -   train_samples_per_second =     11.897[0m
[32m[2022-09-16 16:45:50,142] [    INFO][0m -   train_steps_per_second   =      0.744[0m
[32m[2022-09-16 16:45:50,145] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-16 16:45:50,145] [    INFO][0m -   Num examples = 610[0m
[32m[2022-09-16 16:45:50,145] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 16:45:50,145] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 16:45:50,145] [    INFO][0m -   Total prediction steps = 39[0m
[32m[2022-09-16 16:45:55,707] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-16 16:45:55,708] [    INFO][0m -   test_accuracy           =     0.9066[0m
[32m[2022-09-16 16:45:55,708] [    INFO][0m -   test_loss               =     0.2773[0m
[32m[2022-09-16 16:45:55,708] [    INFO][0m -   test_runtime            = 0:00:05.56[0m
[32m[2022-09-16 16:45:55,708] [    INFO][0m -   test_samples_per_second =    109.666[0m
[32m[2022-09-16 16:45:55,708] [    INFO][0m -   test_steps_per_second   =      7.011[0m
[32m[2022-09-16 16:45:55,708] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-16 16:45:55,708] [    INFO][0m -   Num examples = 753[0m
[32m[2022-09-16 16:45:55,709] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 16:45:55,709] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 16:45:55,709] [    INFO][0m -   Total prediction steps = 48[0m
terminate called after throwing an instance of 'paddle::memory::allocation::BadAlloc'
  what():  

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   full_dygraph_function(paddle::experimental::IntArrayBase<paddle::experimental::Tensor>, paddle::experimental::ScalarBase<paddle::experimental::Tensor>, paddle::experimental::DataType, phi::Place)
1   paddle::experimental::full(paddle::experimental::IntArrayBase<paddle::experimental::Tensor> const&, paddle::experimental::ScalarBase<paddle::experimental::Tensor> const&, paddle::experimental::DataType, phi::Place const&)
2   void phi::FullKernel<float, phi::GPUContext>(phi::GPUContext const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, paddle::experimental::ScalarBase<phi::DenseTensor> const&, paddle::experimental::DataType, phi::DenseTensor*)
3   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
4   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, paddle::experimental::DataType, unsigned long, bool) const
5   phi::DenseTensor::AllocateFrom(phi::Allocator*, paddle::experimental::DataType, unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  phi::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 910.073519GB memory on GPU 0, 5.416748GB memory has been allocated and available memory is only 26.331787GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
If the above ways do not solve the out of memory problem, you can try to use CUDA managed memory. The command is `export FLAGS_use_cuda_managed_memory=false`.
 (at /paddle/paddle/fluid/memory/allocation/cuda_allocator.cc:95)

run.sh: line 65: 62027 Aborted                 CUDA_VISIBLE_DEVICES=$device python train_single.py --output_dir ./checkpoints_$task_name/ --prompt "$prompt" --max_seq_length $max_length --learning_rate 3e-6 --ppt_learning_rate 3e-5 --num_train_epochs 30 --logging_steps 10 --do_save True --do_test --eval_steps 200 --save_steps 200 --per_device_eval_batch_size 16 --per_device_train_batch_size 16 --model_name_or_path ernie-1.0-large-zh-cw --split_id few_all --task_name $task_name --metric_for_best_model accuracy --disable_tqdm True --do_train --do_predict --do_eval --load_best_model_at_end --pretrained "../checkpoints_cmnli/checkpoint-22000/model_state.pdparams" --evaluation_strategy epoch --save_strategy epoch
 
==========
csldcp
==========
 
[32m[2022-09-16 16:46:35,638] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-16 16:46:35,638] [    INFO][0m - ============================================================[0m
[32m[2022-09-16 16:46:35,638] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-16 16:46:35,638] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-16 16:46:35,639] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-16 16:46:35,639] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-16 16:46:35,639] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-16 16:46:35,639] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-16 16:46:35,639] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-16 16:46:35,639] [    INFO][0m - [0m
[32m[2022-09-16 16:46:35,639] [    INFO][0m - ============================================================[0m
[32m[2022-09-16 16:46:35,639] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-16 16:46:35,639] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-16 16:46:35,639] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-16 16:46:35,639] [    INFO][0m - pretrained                    :../checkpoints_cmnli/checkpoint-22000/model_state.pdparams[0m
[32m[2022-09-16 16:46:35,639] [    INFO][0m - prompt                        :‚Äú{'text':'text_a'}‚ÄùËøôÁØáÊñáÁåÆÁöÑÁ±ªÂà´ÊòØ{'mask'}{'mask'}„ÄÇ[0m
[32m[2022-09-16 16:46:35,639] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-16 16:46:35,640] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-16 16:46:35,640] [    INFO][0m - task_name                     :csldcp[0m
[32m[2022-09-16 16:46:35,640] [    INFO][0m - [0m
[32m[2022-09-16 16:46:35,640] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0916 16:46:35.641916 70640 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0916 16:46:35.645953 70640 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-16 16:46:43,544] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-16 16:46:43,556] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-16 16:46:43,556] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-16 16:46:43,557] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùËøôÁØáÊñáÁåÆÁöÑÁ±ªÂà´ÊòØ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': '„ÄÇ'}][0m
[32m[2022-09-16 16:46:45,716] [    INFO][0m - ============================================================[0m
[32m[2022-09-16 16:46:45,716] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-16 16:46:45,716] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-16 16:46:45,717] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-16 16:46:45,717] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-16 16:46:45,717] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-16 16:46:45,717] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-16 16:46:45,717] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-16 16:46:45,717] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-16 16:46:45,717] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-16 16:46:45,717] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-16 16:46:45,717] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-16 16:46:45,717] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-16 16:46:45,718] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-16 16:46:45,718] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-16 16:46:45,718] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-16 16:46:45,718] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-16 16:46:45,718] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-16 16:46:45,718] [    INFO][0m - eval_batch_size               :16[0m
[32m[2022-09-16 16:46:45,718] [    INFO][0m - eval_steps                    :200[0m
[32m[2022-09-16 16:46:45,718] [    INFO][0m - evaluation_strategy           :IntervalStrategy.EPOCH[0m
[32m[2022-09-16 16:46:45,718] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-16 16:46:45,718] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-16 16:46:45,718] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-16 16:46:45,718] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-16 16:46:45,718] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-16 16:46:45,719] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-16 16:46:45,719] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-16 16:46:45,719] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-16 16:46:45,719] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-16 16:46:45,719] [    INFO][0m - learning_rate                 :3e-06[0m
[32m[2022-09-16 16:46:45,719] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-16 16:46:45,719] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-16 16:46:45,719] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-16 16:46:45,719] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-16 16:46:45,719] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-16 16:46:45,719] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-16 16:46:45,719] [    INFO][0m - logging_dir                   :./checkpoints_csldcp/runs/Sep16_16-46-35_instance-3bwob41y-01[0m
[32m[2022-09-16 16:46:45,719] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-16 16:46:45,720] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-16 16:46:45,720] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-16 16:46:45,720] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-16 16:46:45,720] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-16 16:46:45,720] [    INFO][0m - max_seq_length                :256[0m
[32m[2022-09-16 16:46:45,720] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-16 16:46:45,720] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-16 16:46:45,720] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-16 16:46:45,720] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-16 16:46:45,720] [    INFO][0m - num_train_epochs              :30.0[0m
[32m[2022-09-16 16:46:45,720] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-16 16:46:45,720] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-16 16:46:45,720] [    INFO][0m - output_dir                    :./checkpoints_csldcp/[0m
[32m[2022-09-16 16:46:45,721] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-16 16:46:45,721] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-16 16:46:45,721] [    INFO][0m - per_device_eval_batch_size    :16[0m
[32m[2022-09-16 16:46:45,721] [    INFO][0m - per_device_train_batch_size   :16[0m
[32m[2022-09-16 16:46:45,721] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-16 16:46:45,721] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-16 16:46:45,721] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-16 16:46:45,721] [    INFO][0m - ppt_learning_rate             :3e-05[0m
[32m[2022-09-16 16:46:45,721] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-16 16:46:45,721] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-16 16:46:45,721] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-16 16:46:45,721] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-16 16:46:45,721] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-16 16:46:45,721] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-16 16:46:45,722] [    INFO][0m - run_name                      :./checkpoints_csldcp/[0m
[32m[2022-09-16 16:46:45,722] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-16 16:46:45,722] [    INFO][0m - save_steps                    :200[0m
[32m[2022-09-16 16:46:45,722] [    INFO][0m - save_strategy                 :IntervalStrategy.EPOCH[0m
[32m[2022-09-16 16:46:45,722] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-16 16:46:45,722] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-16 16:46:45,722] [    INFO][0m - seed                          :42[0m
[32m[2022-09-16 16:46:45,722] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-16 16:46:45,722] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-16 16:46:45,722] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-16 16:46:45,722] [    INFO][0m - train_batch_size              :16[0m
[32m[2022-09-16 16:46:45,722] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-16 16:46:45,722] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-16 16:46:45,723] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-16 16:46:45,723] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-16 16:46:45,723] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-16 16:46:45,723] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-16 16:46:45,723] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-16 16:46:45,723] [    INFO][0m - [0m
[32m[2022-09-16 16:46:45,726] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-16 16:46:45,726] [    INFO][0m -   Num examples = 2036[0m
[32m[2022-09-16 16:46:45,726] [    INFO][0m -   Num Epochs = 30[0m
[32m[2022-09-16 16:46:45,726] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2022-09-16 16:46:45,726] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2022-09-16 16:46:45,727] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-16 16:46:45,727] [    INFO][0m -   Total optimization steps = 3840.0[0m
[32m[2022-09-16 16:46:45,727] [    INFO][0m -   Total num train samples = 61080[0m
[33m[2022-09-16 16:46:45,762] [ WARNING][0m - Token indices sequence length is longer than the specified maximum sequence length for this model (517 > 512). Running this sequence through the model will result in indexing errors[0m
[32m[2022-09-16 16:46:53,147] [    INFO][0m - loss: 7.20416794, learning_rate: 2.9921875000000003e-06, global_step: 10, interval_runtime: 7.4195, interval_samples_per_second: 2.156, interval_steps_per_second: 1.348, epoch: 0.0781[0m
[32m[2022-09-16 16:46:59,459] [    INFO][0m - loss: 3.52278748, learning_rate: 2.984375e-06, global_step: 20, interval_runtime: 6.3115, interval_samples_per_second: 2.535, interval_steps_per_second: 1.584, epoch: 0.1562[0m
[32m[2022-09-16 16:47:05,793] [    INFO][0m - loss: 2.85715046, learning_rate: 2.9765625e-06, global_step: 30, interval_runtime: 6.3341, interval_samples_per_second: 2.526, interval_steps_per_second: 1.579, epoch: 0.2344[0m
[32m[2022-09-16 16:47:12,144] [    INFO][0m - loss: 2.66256237, learning_rate: 2.96875e-06, global_step: 40, interval_runtime: 6.351, interval_samples_per_second: 2.519, interval_steps_per_second: 1.575, epoch: 0.3125[0m
[32m[2022-09-16 16:47:18,500] [    INFO][0m - loss: 2.43853035, learning_rate: 2.9609375e-06, global_step: 50, interval_runtime: 6.3565, interval_samples_per_second: 2.517, interval_steps_per_second: 1.573, epoch: 0.3906[0m
[32m[2022-09-16 16:47:24,859] [    INFO][0m - loss: 2.3709919, learning_rate: 2.953125e-06, global_step: 60, interval_runtime: 6.3584, interval_samples_per_second: 2.516, interval_steps_per_second: 1.573, epoch: 0.4688[0m
[32m[2022-09-16 16:47:31,238] [    INFO][0m - loss: 2.22511311, learning_rate: 2.9453125e-06, global_step: 70, interval_runtime: 6.3797, interval_samples_per_second: 2.508, interval_steps_per_second: 1.567, epoch: 0.5469[0m
[32m[2022-09-16 16:47:37,608] [    INFO][0m - loss: 2.0198925, learning_rate: 2.9375e-06, global_step: 80, interval_runtime: 6.3701, interval_samples_per_second: 2.512, interval_steps_per_second: 1.57, epoch: 0.625[0m
[32m[2022-09-16 16:47:43,977] [    INFO][0m - loss: 2.04300232, learning_rate: 2.9296875e-06, global_step: 90, interval_runtime: 6.3678, interval_samples_per_second: 2.513, interval_steps_per_second: 1.57, epoch: 0.7031[0m
[32m[2022-09-16 16:47:50,348] [    INFO][0m - loss: 2.05604095, learning_rate: 2.9218750000000003e-06, global_step: 100, interval_runtime: 6.3713, interval_samples_per_second: 2.511, interval_steps_per_second: 1.57, epoch: 0.7812[0m
[32m[2022-09-16 16:47:56,726] [    INFO][0m - loss: 1.92308559, learning_rate: 2.9140625e-06, global_step: 110, interval_runtime: 6.3787, interval_samples_per_second: 2.508, interval_steps_per_second: 1.568, epoch: 0.8594[0m
[32m[2022-09-16 16:48:03,125] [    INFO][0m - loss: 1.89526634, learning_rate: 2.90625e-06, global_step: 120, interval_runtime: 6.3985, interval_samples_per_second: 2.501, interval_steps_per_second: 1.563, epoch: 0.9375[0m
[32m[2022-09-16 16:48:07,655] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 16:48:07,656] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-16 16:48:07,656] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 16:48:07,656] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 16:48:07,656] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-16 16:48:40,595] [    INFO][0m - eval_loss: 1.667328119277954, eval_accuracy: 0.4985493230174081, eval_runtime: 32.9387, eval_samples_per_second: 62.783, eval_steps_per_second: 3.947, epoch: 1.0[0m
[32m[2022-09-16 16:48:40,622] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-128[0m
[32m[2022-09-16 16:48:40,623] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 16:48:43,240] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-128/tokenizer_config.json[0m
[32m[2022-09-16 16:48:43,241] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-128/special_tokens_map.json[0m
[32m[2022-09-16 16:48:49,869] [    INFO][0m - loss: 2.06733208, learning_rate: 2.8984375e-06, global_step: 130, interval_runtime: 46.7444, interval_samples_per_second: 0.342, interval_steps_per_second: 0.214, epoch: 1.0156[0m
[32m[2022-09-16 16:48:56,968] [    INFO][0m - loss: 1.51413898, learning_rate: 2.890625e-06, global_step: 140, interval_runtime: 6.3626, interval_samples_per_second: 2.515, interval_steps_per_second: 1.572, epoch: 1.0938[0m
[32m[2022-09-16 16:49:03,345] [    INFO][0m - loss: 1.69902267, learning_rate: 2.8828125e-06, global_step: 150, interval_runtime: 7.1132, interval_samples_per_second: 2.249, interval_steps_per_second: 1.406, epoch: 1.1719[0m
[32m[2022-09-16 16:49:09,715] [    INFO][0m - loss: 1.47323236, learning_rate: 2.875e-06, global_step: 160, interval_runtime: 6.3703, interval_samples_per_second: 2.512, interval_steps_per_second: 1.57, epoch: 1.25[0m
[32m[2022-09-16 16:49:16,127] [    INFO][0m - loss: 1.62083797, learning_rate: 2.8671875e-06, global_step: 170, interval_runtime: 6.4118, interval_samples_per_second: 2.495, interval_steps_per_second: 1.56, epoch: 1.3281[0m
[32m[2022-09-16 16:49:22,499] [    INFO][0m - loss: 1.66366997, learning_rate: 2.859375e-06, global_step: 180, interval_runtime: 6.3716, interval_samples_per_second: 2.511, interval_steps_per_second: 1.569, epoch: 1.4062[0m
[32m[2022-09-16 16:49:28,862] [    INFO][0m - loss: 1.58851614, learning_rate: 2.8515625000000003e-06, global_step: 190, interval_runtime: 6.3631, interval_samples_per_second: 2.515, interval_steps_per_second: 1.572, epoch: 1.4844[0m
[32m[2022-09-16 16:49:35,224] [    INFO][0m - loss: 1.72834415, learning_rate: 2.84375e-06, global_step: 200, interval_runtime: 6.3619, interval_samples_per_second: 2.515, interval_steps_per_second: 1.572, epoch: 1.5625[0m
[32m[2022-09-16 16:49:41,620] [    INFO][0m - loss: 1.67372017, learning_rate: 2.8359375e-06, global_step: 210, interval_runtime: 6.3966, interval_samples_per_second: 2.501, interval_steps_per_second: 1.563, epoch: 1.6406[0m
[32m[2022-09-16 16:49:48,056] [    INFO][0m - loss: 1.61373558, learning_rate: 2.828125e-06, global_step: 220, interval_runtime: 6.4356, interval_samples_per_second: 2.486, interval_steps_per_second: 1.554, epoch: 1.7188[0m
[32m[2022-09-16 16:49:54,453] [    INFO][0m - loss: 1.55951662, learning_rate: 2.8203125e-06, global_step: 230, interval_runtime: 6.397, interval_samples_per_second: 2.501, interval_steps_per_second: 1.563, epoch: 1.7969[0m
[32m[2022-09-16 16:50:00,869] [    INFO][0m - loss: 1.57817278, learning_rate: 2.8125e-06, global_step: 240, interval_runtime: 6.4156, interval_samples_per_second: 2.494, interval_steps_per_second: 1.559, epoch: 1.875[0m
[32m[2022-09-16 16:50:07,276] [    INFO][0m - loss: 1.6764183, learning_rate: 2.8046875e-06, global_step: 250, interval_runtime: 6.4071, interval_samples_per_second: 2.497, interval_steps_per_second: 1.561, epoch: 1.9531[0m
[32m[2022-09-16 16:50:10,530] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 16:50:10,531] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-16 16:50:10,531] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 16:50:10,531] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 16:50:10,531] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-16 16:50:43,644] [    INFO][0m - eval_loss: 1.42464017868042, eval_accuracy: 0.5333655705996132, eval_runtime: 33.113, eval_samples_per_second: 62.453, eval_steps_per_second: 3.926, epoch: 2.0[0m
[32m[2022-09-16 16:50:43,673] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-256[0m
[32m[2022-09-16 16:50:43,673] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 16:50:46,311] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-256/tokenizer_config.json[0m
[32m[2022-09-16 16:50:46,312] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-256/special_tokens_map.json[0m
[32m[2022-09-16 16:50:54,159] [    INFO][0m - loss: 1.35352974, learning_rate: 2.796875e-06, global_step: 260, interval_runtime: 46.8828, interval_samples_per_second: 0.341, interval_steps_per_second: 0.213, epoch: 2.0312[0m
[32m[2022-09-16 16:51:00,543] [    INFO][0m - loss: 1.38519688, learning_rate: 2.7890625e-06, global_step: 270, interval_runtime: 6.3841, interval_samples_per_second: 2.506, interval_steps_per_second: 1.566, epoch: 2.1094[0m
[32m[2022-09-16 16:51:06,926] [    INFO][0m - loss: 1.41614475, learning_rate: 2.7812500000000003e-06, global_step: 280, interval_runtime: 6.3834, interval_samples_per_second: 2.506, interval_steps_per_second: 1.567, epoch: 2.1875[0m
[32m[2022-09-16 16:51:13,321] [    INFO][0m - loss: 1.21451998, learning_rate: 2.7734375e-06, global_step: 290, interval_runtime: 6.3951, interval_samples_per_second: 2.502, interval_steps_per_second: 1.564, epoch: 2.2656[0m
[32m[2022-09-16 16:51:19,720] [    INFO][0m - loss: 1.48148766, learning_rate: 2.765625e-06, global_step: 300, interval_runtime: 6.3987, interval_samples_per_second: 2.501, interval_steps_per_second: 1.563, epoch: 2.3438[0m
[32m[2022-09-16 16:51:26,108] [    INFO][0m - loss: 1.34713602, learning_rate: 2.7578125e-06, global_step: 310, interval_runtime: 6.3879, interval_samples_per_second: 2.505, interval_steps_per_second: 1.565, epoch: 2.4219[0m
[32m[2022-09-16 16:51:32,496] [    INFO][0m - loss: 1.21778049, learning_rate: 2.75e-06, global_step: 320, interval_runtime: 6.3878, interval_samples_per_second: 2.505, interval_steps_per_second: 1.565, epoch: 2.5[0m
[32m[2022-09-16 16:51:38,883] [    INFO][0m - loss: 1.21941433, learning_rate: 2.7421875e-06, global_step: 330, interval_runtime: 6.3875, interval_samples_per_second: 2.505, interval_steps_per_second: 1.566, epoch: 2.5781[0m
[32m[2022-09-16 16:51:45,284] [    INFO][0m - loss: 1.35044689, learning_rate: 2.734375e-06, global_step: 340, interval_runtime: 6.4009, interval_samples_per_second: 2.5, interval_steps_per_second: 1.562, epoch: 2.6562[0m
[32m[2022-09-16 16:51:51,716] [    INFO][0m - loss: 1.32587681, learning_rate: 2.7265624999999998e-06, global_step: 350, interval_runtime: 6.4321, interval_samples_per_second: 2.488, interval_steps_per_second: 1.555, epoch: 2.7344[0m
[32m[2022-09-16 16:51:58,140] [    INFO][0m - loss: 1.3327795, learning_rate: 2.71875e-06, global_step: 360, interval_runtime: 6.4237, interval_samples_per_second: 2.491, interval_steps_per_second: 1.557, epoch: 2.8125[0m
[32m[2022-09-16 16:52:04,543] [    INFO][0m - loss: 1.36664925, learning_rate: 2.7109375000000002e-06, global_step: 370, interval_runtime: 6.4025, interval_samples_per_second: 2.499, interval_steps_per_second: 1.562, epoch: 2.8906[0m
[32m[2022-09-16 16:52:10,906] [    INFO][0m - loss: 1.26503592, learning_rate: 2.703125e-06, global_step: 380, interval_runtime: 6.3642, interval_samples_per_second: 2.514, interval_steps_per_second: 1.571, epoch: 2.9688[0m
[32m[2022-09-16 16:52:12,930] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 16:52:12,931] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-16 16:52:12,931] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 16:52:12,931] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 16:52:12,931] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-16 16:52:46,066] [    INFO][0m - eval_loss: 1.3166626691818237, eval_accuracy: 0.5444874274661509, eval_runtime: 33.1355, eval_samples_per_second: 62.41, eval_steps_per_second: 3.923, epoch: 3.0[0m
[32m[2022-09-16 16:52:46,094] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-384[0m
[32m[2022-09-16 16:52:46,094] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 16:52:48,715] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-384/tokenizer_config.json[0m
[32m[2022-09-16 16:52:48,716] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-384/special_tokens_map.json[0m
[32m[2022-09-16 16:52:57,740] [    INFO][0m - loss: 1.19864063, learning_rate: 2.6953125e-06, global_step: 390, interval_runtime: 46.8331, interval_samples_per_second: 0.342, interval_steps_per_second: 0.214, epoch: 3.0469[0m
[32m[2022-09-16 16:53:04,107] [    INFO][0m - loss: 1.22620697, learning_rate: 2.6875e-06, global_step: 400, interval_runtime: 6.3677, interval_samples_per_second: 2.513, interval_steps_per_second: 1.57, epoch: 3.125[0m
[32m[2022-09-16 16:53:10,486] [    INFO][0m - loss: 1.07577934, learning_rate: 2.6796875e-06, global_step: 410, interval_runtime: 6.3784, interval_samples_per_second: 2.508, interval_steps_per_second: 1.568, epoch: 3.2031[0m
[32m[2022-09-16 16:53:16,892] [    INFO][0m - loss: 1.01886539, learning_rate: 2.671875e-06, global_step: 420, interval_runtime: 6.406, interval_samples_per_second: 2.498, interval_steps_per_second: 1.561, epoch: 3.2812[0m
[32m[2022-09-16 16:53:23,307] [    INFO][0m - loss: 1.00628996, learning_rate: 2.6640625000000004e-06, global_step: 430, interval_runtime: 6.4146, interval_samples_per_second: 2.494, interval_steps_per_second: 1.559, epoch: 3.3594[0m
[32m[2022-09-16 16:53:29,702] [    INFO][0m - loss: 0.98608627, learning_rate: 2.6562499999999998e-06, global_step: 440, interval_runtime: 6.3956, interval_samples_per_second: 2.502, interval_steps_per_second: 1.564, epoch: 3.4375[0m
[32m[2022-09-16 16:53:36,096] [    INFO][0m - loss: 0.93259926, learning_rate: 2.6484375e-06, global_step: 450, interval_runtime: 6.3942, interval_samples_per_second: 2.502, interval_steps_per_second: 1.564, epoch: 3.5156[0m
[32m[2022-09-16 16:53:42,487] [    INFO][0m - loss: 1.254142, learning_rate: 2.6406250000000002e-06, global_step: 460, interval_runtime: 6.3905, interval_samples_per_second: 2.504, interval_steps_per_second: 1.565, epoch: 3.5938[0m
[32m[2022-09-16 16:53:48,867] [    INFO][0m - loss: 1.03294439, learning_rate: 2.6328125e-06, global_step: 470, interval_runtime: 6.3807, interval_samples_per_second: 2.508, interval_steps_per_second: 1.567, epoch: 3.6719[0m
[32m[2022-09-16 16:53:55,267] [    INFO][0m - loss: 1.20097466, learning_rate: 2.6250000000000003e-06, global_step: 480, interval_runtime: 6.3995, interval_samples_per_second: 2.5, interval_steps_per_second: 1.563, epoch: 3.75[0m
[32m[2022-09-16 16:54:01,675] [    INFO][0m - loss: 1.23101387, learning_rate: 2.6171875e-06, global_step: 490, interval_runtime: 6.4082, interval_samples_per_second: 2.497, interval_steps_per_second: 1.561, epoch: 3.8281[0m
[32m[2022-09-16 16:54:08,097] [    INFO][0m - loss: 1.10296192, learning_rate: 2.609375e-06, global_step: 500, interval_runtime: 6.4218, interval_samples_per_second: 2.492, interval_steps_per_second: 1.557, epoch: 3.9062[0m
[32m[2022-09-16 16:54:14,399] [    INFO][0m - loss: 1.04913492, learning_rate: 2.6015625e-06, global_step: 510, interval_runtime: 6.3025, interval_samples_per_second: 2.539, interval_steps_per_second: 1.587, epoch: 3.9844[0m
[32m[2022-09-16 16:54:15,191] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 16:54:15,192] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-16 16:54:15,192] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 16:54:15,192] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 16:54:15,192] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-16 16:54:48,258] [    INFO][0m - eval_loss: 1.310579776763916, eval_accuracy: 0.5638297872340425, eval_runtime: 33.066, eval_samples_per_second: 62.542, eval_steps_per_second: 3.932, epoch: 4.0[0m
[32m[2022-09-16 16:54:48,294] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-512[0m
[32m[2022-09-16 16:54:48,294] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 16:54:51,081] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-512/tokenizer_config.json[0m
[32m[2022-09-16 16:54:51,081] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-512/special_tokens_map.json[0m
[32m[2022-09-16 16:55:01,596] [    INFO][0m - loss: 1.1307663, learning_rate: 2.5937500000000004e-06, global_step: 520, interval_runtime: 47.1969, interval_samples_per_second: 0.339, interval_steps_per_second: 0.212, epoch: 4.0625[0m
[32m[2022-09-16 16:55:07,974] [    INFO][0m - loss: 0.89142904, learning_rate: 2.5859374999999998e-06, global_step: 530, interval_runtime: 6.3775, interval_samples_per_second: 2.509, interval_steps_per_second: 1.568, epoch: 4.1406[0m
[32m[2022-09-16 16:55:14,340] [    INFO][0m - loss: 0.90275011, learning_rate: 2.578125e-06, global_step: 540, interval_runtime: 6.3662, interval_samples_per_second: 2.513, interval_steps_per_second: 1.571, epoch: 4.2188[0m
[32m[2022-09-16 16:55:20,720] [    INFO][0m - loss: 0.85016661, learning_rate: 2.5703125000000002e-06, global_step: 550, interval_runtime: 6.3801, interval_samples_per_second: 2.508, interval_steps_per_second: 1.567, epoch: 4.2969[0m
[32m[2022-09-16 16:55:27,071] [    INFO][0m - loss: 0.84329319, learning_rate: 2.5625e-06, global_step: 560, interval_runtime: 6.3507, interval_samples_per_second: 2.519, interval_steps_per_second: 1.575, epoch: 4.375[0m
[32m[2022-09-16 16:55:33,463] [    INFO][0m - loss: 1.16157131, learning_rate: 2.5546875000000003e-06, global_step: 570, interval_runtime: 6.3922, interval_samples_per_second: 2.503, interval_steps_per_second: 1.564, epoch: 4.4531[0m
[32m[2022-09-16 16:55:39,870] [    INFO][0m - loss: 0.99106531, learning_rate: 2.546875e-06, global_step: 580, interval_runtime: 6.4072, interval_samples_per_second: 2.497, interval_steps_per_second: 1.561, epoch: 4.5312[0m
[32m[2022-09-16 16:55:46,248] [    INFO][0m - loss: 1.0528595, learning_rate: 2.5390625e-06, global_step: 590, interval_runtime: 6.3774, interval_samples_per_second: 2.509, interval_steps_per_second: 1.568, epoch: 4.6094[0m
[32m[2022-09-16 16:55:52,638] [    INFO][0m - loss: 0.98358297, learning_rate: 2.53125e-06, global_step: 600, interval_runtime: 6.3908, interval_samples_per_second: 2.504, interval_steps_per_second: 1.565, epoch: 4.6875[0m
[32m[2022-09-16 16:55:59,031] [    INFO][0m - loss: 0.80683136, learning_rate: 2.5234375000000004e-06, global_step: 610, interval_runtime: 6.3927, interval_samples_per_second: 2.503, interval_steps_per_second: 1.564, epoch: 4.7656[0m
[32m[2022-09-16 16:56:05,442] [    INFO][0m - loss: 0.93541508, learning_rate: 2.515625e-06, global_step: 620, interval_runtime: 6.4112, interval_samples_per_second: 2.496, interval_steps_per_second: 1.56, epoch: 4.8438[0m
[32m[2022-09-16 16:56:11,853] [    INFO][0m - loss: 0.90087585, learning_rate: 2.5078125e-06, global_step: 630, interval_runtime: 6.4107, interval_samples_per_second: 2.496, interval_steps_per_second: 1.56, epoch: 4.9219[0m
[32m[2022-09-16 16:56:17,678] [    INFO][0m - loss: 1.0097621, learning_rate: 2.5e-06, global_step: 640, interval_runtime: 5.8251, interval_samples_per_second: 2.747, interval_steps_per_second: 1.717, epoch: 5.0[0m
[32m[2022-09-16 16:56:17,679] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 16:56:17,679] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-16 16:56:17,679] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 16:56:17,679] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 16:56:17,679] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-16 16:56:51,028] [    INFO][0m - eval_loss: 1.2953996658325195, eval_accuracy: 0.5604448742746615, eval_runtime: 33.3484, eval_samples_per_second: 62.012, eval_steps_per_second: 3.898, epoch: 5.0[0m
[32m[2022-09-16 16:56:51,068] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-640[0m
[32m[2022-09-16 16:56:51,068] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 16:56:53,764] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-640/tokenizer_config.json[0m
[32m[2022-09-16 16:56:53,765] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-640/special_tokens_map.json[0m
[32m[2022-09-16 16:57:05,399] [    INFO][0m - loss: 0.86434231, learning_rate: 2.4921875e-06, global_step: 650, interval_runtime: 47.7202, interval_samples_per_second: 0.335, interval_steps_per_second: 0.21, epoch: 5.0781[0m
[32m[2022-09-16 16:57:11,797] [    INFO][0m - loss: 0.82149019, learning_rate: 2.4843750000000002e-06, global_step: 660, interval_runtime: 6.3983, interval_samples_per_second: 2.501, interval_steps_per_second: 1.563, epoch: 5.1562[0m
[32m[2022-09-16 16:57:18,228] [    INFO][0m - loss: 0.83128929, learning_rate: 2.4765625e-06, global_step: 670, interval_runtime: 6.4316, interval_samples_per_second: 2.488, interval_steps_per_second: 1.555, epoch: 5.2344[0m
[32m[2022-09-16 16:57:24,666] [    INFO][0m - loss: 0.84098053, learning_rate: 2.46875e-06, global_step: 680, interval_runtime: 6.4376, interval_samples_per_second: 2.485, interval_steps_per_second: 1.553, epoch: 5.3125[0m
[32m[2022-09-16 16:57:31,212] [    INFO][0m - loss: 0.87245512, learning_rate: 2.4609375e-06, global_step: 690, interval_runtime: 6.4071, interval_samples_per_second: 2.497, interval_steps_per_second: 1.561, epoch: 5.3906[0m
[32m[2022-09-16 16:57:37,608] [    INFO][0m - loss: 0.8154233, learning_rate: 2.4531250000000003e-06, global_step: 700, interval_runtime: 6.535, interval_samples_per_second: 2.448, interval_steps_per_second: 1.53, epoch: 5.4688[0m
[32m[2022-09-16 16:57:44,018] [    INFO][0m - loss: 0.79015589, learning_rate: 2.4453125e-06, global_step: 710, interval_runtime: 6.4099, interval_samples_per_second: 2.496, interval_steps_per_second: 1.56, epoch: 5.5469[0m
[32m[2022-09-16 16:57:50,408] [    INFO][0m - loss: 0.71180072, learning_rate: 2.4375e-06, global_step: 720, interval_runtime: 6.3896, interval_samples_per_second: 2.504, interval_steps_per_second: 1.565, epoch: 5.625[0m
[32m[2022-09-16 16:57:56,803] [    INFO][0m - loss: 0.80560598, learning_rate: 2.4296875e-06, global_step: 730, interval_runtime: 6.3952, interval_samples_per_second: 2.502, interval_steps_per_second: 1.564, epoch: 5.7031[0m
[32m[2022-09-16 16:58:03,211] [    INFO][0m - loss: 0.85200768, learning_rate: 2.421875e-06, global_step: 740, interval_runtime: 6.4077, interval_samples_per_second: 2.497, interval_steps_per_second: 1.561, epoch: 5.7812[0m
[32m[2022-09-16 16:58:09,613] [    INFO][0m - loss: 0.65576816, learning_rate: 2.4140625000000002e-06, global_step: 750, interval_runtime: 6.4017, interval_samples_per_second: 2.499, interval_steps_per_second: 1.562, epoch: 5.8594[0m
[32m[2022-09-16 16:58:16,034] [    INFO][0m - loss: 0.72363787, learning_rate: 2.40625e-06, global_step: 760, interval_runtime: 6.4213, interval_samples_per_second: 2.492, interval_steps_per_second: 1.557, epoch: 5.9375[0m
[32m[2022-09-16 16:58:20,570] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 16:58:20,571] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-16 16:58:20,571] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 16:58:20,571] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 16:58:20,571] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-16 16:58:53,673] [    INFO][0m - eval_loss: 1.3401433229446411, eval_accuracy: 0.5657640232108317, eval_runtime: 33.1019, eval_samples_per_second: 62.474, eval_steps_per_second: 3.927, epoch: 6.0[0m
[32m[2022-09-16 16:58:53,709] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-768[0m
[32m[2022-09-16 16:58:53,709] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 16:58:56,411] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-768/tokenizer_config.json[0m
[32m[2022-09-16 16:58:56,411] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-768/special_tokens_map.json[0m
[32m[2022-09-16 16:59:02,932] [    INFO][0m - loss: 0.6879519, learning_rate: 2.3984375e-06, global_step: 770, interval_runtime: 46.8981, interval_samples_per_second: 0.341, interval_steps_per_second: 0.213, epoch: 6.0156[0m
[32m[2022-09-16 16:59:09,305] [    INFO][0m - loss: 0.51053343, learning_rate: 2.390625e-06, global_step: 780, interval_runtime: 6.3731, interval_samples_per_second: 2.511, interval_steps_per_second: 1.569, epoch: 6.0938[0m
[32m[2022-09-16 16:59:15,675] [    INFO][0m - loss: 0.61238112, learning_rate: 2.3828125000000003e-06, global_step: 790, interval_runtime: 6.3697, interval_samples_per_second: 2.512, interval_steps_per_second: 1.57, epoch: 6.1719[0m
[32m[2022-09-16 16:59:22,059] [    INFO][0m - loss: 0.65174904, learning_rate: 2.375e-06, global_step: 800, interval_runtime: 6.3839, interval_samples_per_second: 2.506, interval_steps_per_second: 1.566, epoch: 6.25[0m
[32m[2022-09-16 16:59:28,435] [    INFO][0m - loss: 0.63879018, learning_rate: 2.3671875e-06, global_step: 810, interval_runtime: 6.3768, interval_samples_per_second: 2.509, interval_steps_per_second: 1.568, epoch: 6.3281[0m
[32m[2022-09-16 16:59:34,819] [    INFO][0m - loss: 0.67324405, learning_rate: 2.359375e-06, global_step: 820, interval_runtime: 6.3835, interval_samples_per_second: 2.506, interval_steps_per_second: 1.567, epoch: 6.4062[0m
[32m[2022-09-16 16:59:41,195] [    INFO][0m - loss: 0.67448421, learning_rate: 2.3515625e-06, global_step: 830, interval_runtime: 6.3757, interval_samples_per_second: 2.51, interval_steps_per_second: 1.568, epoch: 6.4844[0m
[32m[2022-09-16 16:59:47,580] [    INFO][0m - loss: 0.88722467, learning_rate: 2.3437500000000002e-06, global_step: 840, interval_runtime: 6.3854, interval_samples_per_second: 2.506, interval_steps_per_second: 1.566, epoch: 6.5625[0m
[32m[2022-09-16 16:59:53,967] [    INFO][0m - loss: 0.57169971, learning_rate: 2.3359375e-06, global_step: 850, interval_runtime: 6.3872, interval_samples_per_second: 2.505, interval_steps_per_second: 1.566, epoch: 6.6406[0m
[32m[2022-09-16 17:00:00,345] [    INFO][0m - loss: 0.68497319, learning_rate: 2.328125e-06, global_step: 860, interval_runtime: 6.3778, interval_samples_per_second: 2.509, interval_steps_per_second: 1.568, epoch: 6.7188[0m
[32m[2022-09-16 17:00:06,760] [    INFO][0m - loss: 0.68717151, learning_rate: 2.3203125e-06, global_step: 870, interval_runtime: 6.4151, interval_samples_per_second: 2.494, interval_steps_per_second: 1.559, epoch: 6.7969[0m
[32m[2022-09-16 17:00:13,157] [    INFO][0m - loss: 0.71088104, learning_rate: 2.3125000000000003e-06, global_step: 880, interval_runtime: 6.3973, interval_samples_per_second: 2.501, interval_steps_per_second: 1.563, epoch: 6.875[0m
[32m[2022-09-16 17:00:19,544] [    INFO][0m - loss: 0.81686935, learning_rate: 2.3046875e-06, global_step: 890, interval_runtime: 6.386, interval_samples_per_second: 2.505, interval_steps_per_second: 1.566, epoch: 6.9531[0m
[32m[2022-09-16 17:00:22,800] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 17:00:22,800] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-16 17:00:22,800] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 17:00:22,800] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 17:00:22,800] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-16 17:00:55,805] [    INFO][0m - eval_loss: 1.3378037214279175, eval_accuracy: 0.5676982591876208, eval_runtime: 33.0047, eval_samples_per_second: 62.658, eval_steps_per_second: 3.939, epoch: 7.0[0m
[32m[2022-09-16 17:00:55,841] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-896[0m
[32m[2022-09-16 17:00:55,841] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 17:00:58,524] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-896/tokenizer_config.json[0m
[32m[2022-09-16 17:00:58,524] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-896/special_tokens_map.json[0m
[32m[2022-09-16 17:01:06,204] [    INFO][0m - loss: 0.68319769, learning_rate: 2.296875e-06, global_step: 900, interval_runtime: 46.6606, interval_samples_per_second: 0.343, interval_steps_per_second: 0.214, epoch: 7.0312[0m
[32m[2022-09-16 17:01:12,567] [    INFO][0m - loss: 0.51376715, learning_rate: 2.2890625e-06, global_step: 910, interval_runtime: 6.3627, interval_samples_per_second: 2.515, interval_steps_per_second: 1.572, epoch: 7.1094[0m
[32m[2022-09-16 17:01:18,941] [    INFO][0m - loss: 0.52612114, learning_rate: 2.28125e-06, global_step: 920, interval_runtime: 6.3741, interval_samples_per_second: 2.51, interval_steps_per_second: 1.569, epoch: 7.1875[0m
[32m[2022-09-16 17:01:25,340] [    INFO][0m - loss: 0.53247318, learning_rate: 2.2734375e-06, global_step: 930, interval_runtime: 6.3992, interval_samples_per_second: 2.5, interval_steps_per_second: 1.563, epoch: 7.2656[0m
[32m[2022-09-16 17:01:31,739] [    INFO][0m - loss: 0.65272026, learning_rate: 2.265625e-06, global_step: 940, interval_runtime: 6.3989, interval_samples_per_second: 2.5, interval_steps_per_second: 1.563, epoch: 7.3438[0m
[32m[2022-09-16 17:01:38,119] [    INFO][0m - loss: 0.49102812, learning_rate: 2.2578125e-06, global_step: 950, interval_runtime: 6.3799, interval_samples_per_second: 2.508, interval_steps_per_second: 1.567, epoch: 7.4219[0m
[32m[2022-09-16 17:01:44,518] [    INFO][0m - loss: 0.59392748, learning_rate: 2.25e-06, global_step: 960, interval_runtime: 6.3988, interval_samples_per_second: 2.5, interval_steps_per_second: 1.563, epoch: 7.5[0m
[32m[2022-09-16 17:01:50,907] [    INFO][0m - loss: 0.48196621, learning_rate: 2.2421875000000003e-06, global_step: 970, interval_runtime: 6.3897, interval_samples_per_second: 2.504, interval_steps_per_second: 1.565, epoch: 7.5781[0m
[32m[2022-09-16 17:01:57,309] [    INFO][0m - loss: 0.50150185, learning_rate: 2.234375e-06, global_step: 980, interval_runtime: 6.4012, interval_samples_per_second: 2.5, interval_steps_per_second: 1.562, epoch: 7.6562[0m
[32m[2022-09-16 17:02:03,702] [    INFO][0m - loss: 0.58078465, learning_rate: 2.2265625e-06, global_step: 990, interval_runtime: 6.3937, interval_samples_per_second: 2.502, interval_steps_per_second: 1.564, epoch: 7.7344[0m
[32m[2022-09-16 17:02:10,114] [    INFO][0m - loss: 0.5300252, learning_rate: 2.21875e-06, global_step: 1000, interval_runtime: 6.4109, interval_samples_per_second: 2.496, interval_steps_per_second: 1.56, epoch: 7.8125[0m
[32m[2022-09-16 17:02:16,495] [    INFO][0m - loss: 0.59925799, learning_rate: 2.2109375e-06, global_step: 1010, interval_runtime: 6.3815, interval_samples_per_second: 2.507, interval_steps_per_second: 1.567, epoch: 7.8906[0m
[32m[2022-09-16 17:02:22,852] [    INFO][0m - loss: 0.71054134, learning_rate: 2.203125e-06, global_step: 1020, interval_runtime: 6.3571, interval_samples_per_second: 2.517, interval_steps_per_second: 1.573, epoch: 7.9688[0m
[32m[2022-09-16 17:02:24,882] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 17:02:24,883] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-16 17:02:24,883] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 17:02:24,883] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 17:02:24,883] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-16 17:02:58,024] [    INFO][0m - eval_loss: 1.4226558208465576, eval_accuracy: 0.561411992263056, eval_runtime: 33.1409, eval_samples_per_second: 62.4, eval_steps_per_second: 3.923, epoch: 8.0[0m
[32m[2022-09-16 17:02:58,062] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-1024[0m
[32m[2022-09-16 17:02:58,062] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 17:03:00,755] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-1024/tokenizer_config.json[0m
[32m[2022-09-16 17:03:00,756] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-1024/special_tokens_map.json[0m
[32m[2022-09-16 17:03:09,856] [    INFO][0m - loss: 0.46612496, learning_rate: 2.1953125e-06, global_step: 1030, interval_runtime: 47.0037, interval_samples_per_second: 0.34, interval_steps_per_second: 0.213, epoch: 8.0469[0m
[32m[2022-09-16 17:03:16,236] [    INFO][0m - loss: 0.46533694, learning_rate: 2.1875e-06, global_step: 1040, interval_runtime: 6.3806, interval_samples_per_second: 2.508, interval_steps_per_second: 1.567, epoch: 8.125[0m
[32m[2022-09-16 17:03:22,624] [    INFO][0m - loss: 0.41958799, learning_rate: 2.1796875e-06, global_step: 1050, interval_runtime: 6.3878, interval_samples_per_second: 2.505, interval_steps_per_second: 1.565, epoch: 8.2031[0m
[32m[2022-09-16 17:03:29,019] [    INFO][0m - loss: 0.42706313, learning_rate: 2.1718750000000003e-06, global_step: 1060, interval_runtime: 6.395, interval_samples_per_second: 2.502, interval_steps_per_second: 1.564, epoch: 8.2812[0m
[32m[2022-09-16 17:03:35,416] [    INFO][0m - loss: 0.50465055, learning_rate: 2.1640625e-06, global_step: 1070, interval_runtime: 6.3968, interval_samples_per_second: 2.501, interval_steps_per_second: 1.563, epoch: 8.3594[0m
[32m[2022-09-16 17:03:41,817] [    INFO][0m - loss: 0.47659235, learning_rate: 2.15625e-06, global_step: 1080, interval_runtime: 6.4011, interval_samples_per_second: 2.5, interval_steps_per_second: 1.562, epoch: 8.4375[0m
[32m[2022-09-16 17:03:48,206] [    INFO][0m - loss: 0.49666386, learning_rate: 2.1484375e-06, global_step: 1090, interval_runtime: 6.3894, interval_samples_per_second: 2.504, interval_steps_per_second: 1.565, epoch: 8.5156[0m
[32m[2022-09-16 17:03:54,607] [    INFO][0m - loss: 0.49410443, learning_rate: 2.140625e-06, global_step: 1100, interval_runtime: 6.4003, interval_samples_per_second: 2.5, interval_steps_per_second: 1.562, epoch: 8.5938[0m
[32m[2022-09-16 17:04:01,003] [    INFO][0m - loss: 0.37574809, learning_rate: 2.1328125e-06, global_step: 1110, interval_runtime: 6.3966, interval_samples_per_second: 2.501, interval_steps_per_second: 1.563, epoch: 8.6719[0m
[32m[2022-09-16 17:04:07,432] [    INFO][0m - loss: 0.53342237, learning_rate: 2.125e-06, global_step: 1120, interval_runtime: 6.4283, interval_samples_per_second: 2.489, interval_steps_per_second: 1.556, epoch: 8.75[0m
[32m[2022-09-16 17:04:13,876] [    INFO][0m - loss: 0.29262447, learning_rate: 2.1171875e-06, global_step: 1130, interval_runtime: 6.444, interval_samples_per_second: 2.483, interval_steps_per_second: 1.552, epoch: 8.8281[0m
[32m[2022-09-16 17:04:20,294] [    INFO][0m - loss: 0.44065814, learning_rate: 2.109375e-06, global_step: 1140, interval_runtime: 6.4186, interval_samples_per_second: 2.493, interval_steps_per_second: 1.558, epoch: 8.9062[0m
[32m[2022-09-16 17:04:26,601] [    INFO][0m - loss: 0.46836886, learning_rate: 2.1015625000000003e-06, global_step: 1150, interval_runtime: 6.3069, interval_samples_per_second: 2.537, interval_steps_per_second: 1.586, epoch: 8.9844[0m
[32m[2022-09-16 17:04:27,402] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 17:04:27,402] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-16 17:04:27,403] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 17:04:27,403] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 17:04:27,403] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-16 17:05:00,493] [    INFO][0m - eval_loss: 1.4661041498184204, eval_accuracy: 0.5754352030947776, eval_runtime: 33.0897, eval_samples_per_second: 62.497, eval_steps_per_second: 3.929, epoch: 9.0[0m
[32m[2022-09-16 17:05:00,528] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-1152[0m
[32m[2022-09-16 17:05:00,528] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 17:05:03,207] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-1152/tokenizer_config.json[0m
[32m[2022-09-16 17:05:03,207] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-1152/special_tokens_map.json[0m
[32m[2022-09-16 17:05:13,501] [    INFO][0m - loss: 0.39598584, learning_rate: 2.09375e-06, global_step: 1160, interval_runtime: 46.9, interval_samples_per_second: 0.341, interval_steps_per_second: 0.213, epoch: 9.0625[0m
[32m[2022-09-16 17:05:19,880] [    INFO][0m - loss: 0.29307981, learning_rate: 2.0859375e-06, global_step: 1170, interval_runtime: 6.3784, interval_samples_per_second: 2.508, interval_steps_per_second: 1.568, epoch: 9.1406[0m
[32m[2022-09-16 17:05:26,281] [    INFO][0m - loss: 0.34638622, learning_rate: 2.078125e-06, global_step: 1180, interval_runtime: 6.401, interval_samples_per_second: 2.5, interval_steps_per_second: 1.562, epoch: 9.2188[0m
[32m[2022-09-16 17:05:32,688] [    INFO][0m - loss: 0.29411047, learning_rate: 2.0703125e-06, global_step: 1190, interval_runtime: 6.4077, interval_samples_per_second: 2.497, interval_steps_per_second: 1.561, epoch: 9.2969[0m
[32m[2022-09-16 17:05:39,106] [    INFO][0m - loss: 0.36930618, learning_rate: 2.0625e-06, global_step: 1200, interval_runtime: 6.4171, interval_samples_per_second: 2.493, interval_steps_per_second: 1.558, epoch: 9.375[0m
[32m[2022-09-16 17:05:45,510] [    INFO][0m - loss: 0.42012701, learning_rate: 2.0546875e-06, global_step: 1210, interval_runtime: 6.4048, interval_samples_per_second: 2.498, interval_steps_per_second: 1.561, epoch: 9.4531[0m
[32m[2022-09-16 17:05:51,930] [    INFO][0m - loss: 0.3696785, learning_rate: 2.0468749999999998e-06, global_step: 1220, interval_runtime: 6.4202, interval_samples_per_second: 2.492, interval_steps_per_second: 1.558, epoch: 9.5312[0m
[32m[2022-09-16 17:05:58,349] [    INFO][0m - loss: 0.43755045, learning_rate: 2.0390625e-06, global_step: 1230, interval_runtime: 6.4188, interval_samples_per_second: 2.493, interval_steps_per_second: 1.558, epoch: 9.6094[0m
[32m[2022-09-16 17:06:04,759] [    INFO][0m - loss: 0.32410281, learning_rate: 2.0312500000000002e-06, global_step: 1240, interval_runtime: 6.41, interval_samples_per_second: 2.496, interval_steps_per_second: 1.56, epoch: 9.6875[0m
[32m[2022-09-16 17:06:11,181] [    INFO][0m - loss: 0.49042902, learning_rate: 2.0234375e-06, global_step: 1250, interval_runtime: 6.4214, interval_samples_per_second: 2.492, interval_steps_per_second: 1.557, epoch: 9.7656[0m
[32m[2022-09-16 17:06:17,588] [    INFO][0m - loss: 0.3471061, learning_rate: 2.015625e-06, global_step: 1260, interval_runtime: 6.4075, interval_samples_per_second: 2.497, interval_steps_per_second: 1.561, epoch: 9.8438[0m
[32m[2022-09-16 17:06:24,014] [    INFO][0m - loss: 0.41369386, learning_rate: 2.0078125e-06, global_step: 1270, interval_runtime: 6.4255, interval_samples_per_second: 2.49, interval_steps_per_second: 1.556, epoch: 9.9219[0m
[32m[2022-09-16 17:06:29,859] [    INFO][0m - loss: 0.43947277, learning_rate: 2e-06, global_step: 1280, interval_runtime: 5.8456, interval_samples_per_second: 2.737, interval_steps_per_second: 1.711, epoch: 10.0[0m
[32m[2022-09-16 17:06:29,860] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 17:06:29,860] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-16 17:06:29,860] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 17:06:29,860] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 17:06:29,860] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-16 17:07:02,999] [    INFO][0m - eval_loss: 1.5024183988571167, eval_accuracy: 0.5773694390715667, eval_runtime: 33.1379, eval_samples_per_second: 62.406, eval_steps_per_second: 3.923, epoch: 10.0[0m
[32m[2022-09-16 17:07:03,035] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-1280[0m
[32m[2022-09-16 17:07:03,035] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 17:07:05,810] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-1280/tokenizer_config.json[0m
[32m[2022-09-16 17:07:05,811] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-1280/special_tokens_map.json[0m
[32m[2022-09-16 17:07:17,422] [    INFO][0m - loss: 0.28657117, learning_rate: 1.9921875e-06, global_step: 1290, interval_runtime: 47.5628, interval_samples_per_second: 0.336, interval_steps_per_second: 0.21, epoch: 10.0781[0m
[32m[2022-09-16 17:07:23,843] [    INFO][0m - loss: 0.33508599, learning_rate: 1.984375e-06, global_step: 1300, interval_runtime: 6.421, interval_samples_per_second: 2.492, interval_steps_per_second: 1.557, epoch: 10.1562[0m
[32m[2022-09-16 17:07:30,247] [    INFO][0m - loss: 0.30582271, learning_rate: 1.9765624999999998e-06, global_step: 1310, interval_runtime: 6.4039, interval_samples_per_second: 2.498, interval_steps_per_second: 1.562, epoch: 10.2344[0m
[32m[2022-09-16 17:07:36,655] [    INFO][0m - loss: 0.28839872, learning_rate: 1.96875e-06, global_step: 1320, interval_runtime: 6.4077, interval_samples_per_second: 2.497, interval_steps_per_second: 1.561, epoch: 10.3125[0m
[32m[2022-09-16 17:07:43,056] [    INFO][0m - loss: 0.29768407, learning_rate: 1.9609375000000002e-06, global_step: 1330, interval_runtime: 6.4014, interval_samples_per_second: 2.499, interval_steps_per_second: 1.562, epoch: 10.3906[0m
[32m[2022-09-16 17:07:49,484] [    INFO][0m - loss: 0.28498423, learning_rate: 1.953125e-06, global_step: 1340, interval_runtime: 6.4282, interval_samples_per_second: 2.489, interval_steps_per_second: 1.556, epoch: 10.4688[0m
[32m[2022-09-16 17:07:55,892] [    INFO][0m - loss: 0.23153775, learning_rate: 1.9453125e-06, global_step: 1350, interval_runtime: 6.4072, interval_samples_per_second: 2.497, interval_steps_per_second: 1.561, epoch: 10.5469[0m
[32m[2022-09-16 17:08:02,323] [    INFO][0m - loss: 0.3079097, learning_rate: 1.9375e-06, global_step: 1360, interval_runtime: 6.4314, interval_samples_per_second: 2.488, interval_steps_per_second: 1.555, epoch: 10.625[0m
[32m[2022-09-16 17:08:08,753] [    INFO][0m - loss: 0.3423918, learning_rate: 1.9296875e-06, global_step: 1370, interval_runtime: 6.43, interval_samples_per_second: 2.488, interval_steps_per_second: 1.555, epoch: 10.7031[0m
[32m[2022-09-16 17:08:15,178] [    INFO][0m - loss: 0.27595932, learning_rate: 1.921875e-06, global_step: 1380, interval_runtime: 6.4251, interval_samples_per_second: 2.49, interval_steps_per_second: 1.556, epoch: 10.7812[0m
[32m[2022-09-16 17:08:21,594] [    INFO][0m - loss: 0.34956386, learning_rate: 1.9140625000000004e-06, global_step: 1390, interval_runtime: 6.4161, interval_samples_per_second: 2.494, interval_steps_per_second: 1.559, epoch: 10.8594[0m
[32m[2022-09-16 17:08:27,989] [    INFO][0m - loss: 0.32737851, learning_rate: 1.90625e-06, global_step: 1400, interval_runtime: 6.3945, interval_samples_per_second: 2.502, interval_steps_per_second: 1.564, epoch: 10.9375[0m
[32m[2022-09-16 17:08:32,522] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 17:08:32,522] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-16 17:08:32,522] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 17:08:32,522] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 17:08:32,522] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-16 17:09:05,721] [    INFO][0m - eval_loss: 1.5725349187850952, eval_accuracy: 0.5618955512572534, eval_runtime: 33.1987, eval_samples_per_second: 62.292, eval_steps_per_second: 3.916, epoch: 11.0[0m
[32m[2022-09-16 17:09:05,757] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-1408[0m
[32m[2022-09-16 17:09:05,757] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 17:09:08,651] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-1408/tokenizer_config.json[0m
[32m[2022-09-16 17:09:08,652] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-1408/special_tokens_map.json[0m
[32m[2022-09-16 17:09:15,228] [    INFO][0m - loss: 0.33138947, learning_rate: 1.8984375e-06, global_step: 1410, interval_runtime: 47.2387, interval_samples_per_second: 0.339, interval_steps_per_second: 0.212, epoch: 11.0156[0m
[32m[2022-09-16 17:09:21,589] [    INFO][0m - loss: 0.27536879, learning_rate: 1.8906250000000002e-06, global_step: 1420, interval_runtime: 6.3612, interval_samples_per_second: 2.515, interval_steps_per_second: 1.572, epoch: 11.0938[0m
[32m[2022-09-16 17:09:27,989] [    INFO][0m - loss: 0.18669167, learning_rate: 1.8828125e-06, global_step: 1430, interval_runtime: 6.4009, interval_samples_per_second: 2.5, interval_steps_per_second: 1.562, epoch: 11.1719[0m
[32m[2022-09-16 17:09:34,367] [    INFO][0m - loss: 0.20921566, learning_rate: 1.875e-06, global_step: 1440, interval_runtime: 6.3773, interval_samples_per_second: 2.509, interval_steps_per_second: 1.568, epoch: 11.25[0m
[32m[2022-09-16 17:09:40,779] [    INFO][0m - loss: 0.26037409, learning_rate: 1.8671875e-06, global_step: 1450, interval_runtime: 6.4119, interval_samples_per_second: 2.495, interval_steps_per_second: 1.56, epoch: 11.3281[0m
[32m[2022-09-16 17:09:47,185] [    INFO][0m - loss: 0.23027449, learning_rate: 1.8593749999999999e-06, global_step: 1460, interval_runtime: 6.4064, interval_samples_per_second: 2.498, interval_steps_per_second: 1.561, epoch: 11.4062[0m
[32m[2022-09-16 17:09:53,592] [    INFO][0m - loss: 0.30679274, learning_rate: 1.8515625000000001e-06, global_step: 1470, interval_runtime: 6.4073, interval_samples_per_second: 2.497, interval_steps_per_second: 1.561, epoch: 11.4844[0m
[32m[2022-09-16 17:10:00,015] [    INFO][0m - loss: 0.32163637, learning_rate: 1.8437500000000001e-06, global_step: 1480, interval_runtime: 6.4226, interval_samples_per_second: 2.491, interval_steps_per_second: 1.557, epoch: 11.5625[0m
[32m[2022-09-16 17:10:06,425] [    INFO][0m - loss: 0.29374902, learning_rate: 1.8359375e-06, global_step: 1490, interval_runtime: 6.41, interval_samples_per_second: 2.496, interval_steps_per_second: 1.56, epoch: 11.6406[0m
[32m[2022-09-16 17:10:12,839] [    INFO][0m - loss: 0.27443945, learning_rate: 1.828125e-06, global_step: 1500, interval_runtime: 6.414, interval_samples_per_second: 2.495, interval_steps_per_second: 1.559, epoch: 11.7188[0m
[32m[2022-09-16 17:10:19,224] [    INFO][0m - loss: 0.2178515, learning_rate: 1.8203125000000002e-06, global_step: 1510, interval_runtime: 6.3854, interval_samples_per_second: 2.506, interval_steps_per_second: 1.566, epoch: 11.7969[0m
[32m[2022-09-16 17:10:25,625] [    INFO][0m - loss: 0.24290502, learning_rate: 1.8125e-06, global_step: 1520, interval_runtime: 6.4007, interval_samples_per_second: 2.5, interval_steps_per_second: 1.562, epoch: 11.875[0m
[32m[2022-09-16 17:10:32,025] [    INFO][0m - loss: 0.19966837, learning_rate: 1.8046875e-06, global_step: 1530, interval_runtime: 6.3996, interval_samples_per_second: 2.5, interval_steps_per_second: 1.563, epoch: 11.9531[0m
[32m[2022-09-16 17:10:35,290] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 17:10:35,290] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-16 17:10:35,290] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 17:10:35,290] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 17:10:35,290] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-16 17:11:08,470] [    INFO][0m - eval_loss: 1.6178765296936035, eval_accuracy: 0.574468085106383, eval_runtime: 33.1796, eval_samples_per_second: 62.327, eval_steps_per_second: 3.918, epoch: 12.0[0m
[32m[2022-09-16 17:11:08,513] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-1536[0m
[32m[2022-09-16 17:11:08,513] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 17:11:11,170] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-1536/tokenizer_config.json[0m
[32m[2022-09-16 17:11:11,171] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-1536/special_tokens_map.json[0m
[32m[2022-09-16 17:11:19,021] [    INFO][0m - loss: 0.17595388, learning_rate: 1.796875e-06, global_step: 1540, interval_runtime: 46.9961, interval_samples_per_second: 0.34, interval_steps_per_second: 0.213, epoch: 12.0312[0m
[32m[2022-09-16 17:11:25,406] [    INFO][0m - loss: 0.20356665, learning_rate: 1.7890624999999999e-06, global_step: 1550, interval_runtime: 6.385, interval_samples_per_second: 2.506, interval_steps_per_second: 1.566, epoch: 12.1094[0m
[32m[2022-09-16 17:11:31,775] [    INFO][0m - loss: 0.18768849, learning_rate: 1.78125e-06, global_step: 1560, interval_runtime: 6.3688, interval_samples_per_second: 2.512, interval_steps_per_second: 1.57, epoch: 12.1875[0m
[32m[2022-09-16 17:11:38,170] [    INFO][0m - loss: 0.14899282, learning_rate: 1.7734375000000001e-06, global_step: 1570, interval_runtime: 6.395, interval_samples_per_second: 2.502, interval_steps_per_second: 1.564, epoch: 12.2656[0m
[32m[2022-09-16 17:11:44,569] [    INFO][0m - loss: 0.18470857, learning_rate: 1.765625e-06, global_step: 1580, interval_runtime: 6.399, interval_samples_per_second: 2.5, interval_steps_per_second: 1.563, epoch: 12.3438[0m
[32m[2022-09-16 17:11:50,973] [    INFO][0m - loss: 0.29442675, learning_rate: 1.7578125e-06, global_step: 1590, interval_runtime: 6.4046, interval_samples_per_second: 2.498, interval_steps_per_second: 1.561, epoch: 12.4219[0m
[32m[2022-09-16 17:11:57,384] [    INFO][0m - loss: 0.15767376, learning_rate: 1.7500000000000002e-06, global_step: 1600, interval_runtime: 6.4106, interval_samples_per_second: 2.496, interval_steps_per_second: 1.56, epoch: 12.5[0m
[32m[2022-09-16 17:12:03,804] [    INFO][0m - loss: 0.17224715, learning_rate: 1.7421875e-06, global_step: 1610, interval_runtime: 6.4204, interval_samples_per_second: 2.492, interval_steps_per_second: 1.558, epoch: 12.5781[0m
[32m[2022-09-16 17:12:10,218] [    INFO][0m - loss: 0.17619802, learning_rate: 1.734375e-06, global_step: 1620, interval_runtime: 6.413, interval_samples_per_second: 2.495, interval_steps_per_second: 1.559, epoch: 12.6562[0m
[32m[2022-09-16 17:12:16,628] [    INFO][0m - loss: 0.2467531, learning_rate: 1.7265625000000003e-06, global_step: 1630, interval_runtime: 6.4107, interval_samples_per_second: 2.496, interval_steps_per_second: 1.56, epoch: 12.7344[0m
[32m[2022-09-16 17:12:23,055] [    INFO][0m - loss: 0.20603757, learning_rate: 1.7187499999999998e-06, global_step: 1640, interval_runtime: 6.4264, interval_samples_per_second: 2.49, interval_steps_per_second: 1.556, epoch: 12.8125[0m
[32m[2022-09-16 17:12:29,459] [    INFO][0m - loss: 0.17952759, learning_rate: 1.7109375e-06, global_step: 1650, interval_runtime: 6.4045, interval_samples_per_second: 2.498, interval_steps_per_second: 1.561, epoch: 12.8906[0m
[32m[2022-09-16 17:12:35,818] [    INFO][0m - loss: 0.2773901, learning_rate: 1.703125e-06, global_step: 1660, interval_runtime: 6.3593, interval_samples_per_second: 2.516, interval_steps_per_second: 1.573, epoch: 12.9688[0m
[32m[2022-09-16 17:12:37,849] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 17:12:37,849] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-16 17:12:37,849] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 17:12:37,849] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 17:12:37,849] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-16 17:13:10,980] [    INFO][0m - eval_loss: 1.6958876848220825, eval_accuracy: 0.5739845261121856, eval_runtime: 33.1308, eval_samples_per_second: 62.419, eval_steps_per_second: 3.924, epoch: 13.0[0m
[32m[2022-09-16 17:13:11,020] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-1664[0m
[32m[2022-09-16 17:13:11,020] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 17:13:13,687] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-1664/tokenizer_config.json[0m
[32m[2022-09-16 17:13:13,688] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-1664/special_tokens_map.json[0m
[32m[2022-09-16 17:13:22,665] [    INFO][0m - loss: 0.15047187, learning_rate: 1.6953125e-06, global_step: 1670, interval_runtime: 46.8466, interval_samples_per_second: 0.342, interval_steps_per_second: 0.213, epoch: 13.0469[0m
[32m[2022-09-16 17:13:29,038] [    INFO][0m - loss: 0.14675986, learning_rate: 1.6875e-06, global_step: 1680, interval_runtime: 6.373, interval_samples_per_second: 2.511, interval_steps_per_second: 1.569, epoch: 13.125[0m
[32m[2022-09-16 17:13:35,410] [    INFO][0m - loss: 0.1558859, learning_rate: 1.6796875000000002e-06, global_step: 1690, interval_runtime: 6.3723, interval_samples_per_second: 2.511, interval_steps_per_second: 1.569, epoch: 13.2031[0m
[32m[2022-09-16 17:13:41,803] [    INFO][0m - loss: 0.16164842, learning_rate: 1.671875e-06, global_step: 1700, interval_runtime: 6.3928, interval_samples_per_second: 2.503, interval_steps_per_second: 1.564, epoch: 13.2812[0m
[32m[2022-09-16 17:13:48,188] [    INFO][0m - loss: 0.19674468, learning_rate: 1.6640625e-06, global_step: 1710, interval_runtime: 6.3845, interval_samples_per_second: 2.506, interval_steps_per_second: 1.566, epoch: 13.3594[0m
[32m[2022-09-16 17:13:54,578] [    INFO][0m - loss: 0.15700471, learning_rate: 1.6562500000000002e-06, global_step: 1720, interval_runtime: 6.3902, interval_samples_per_second: 2.504, interval_steps_per_second: 1.565, epoch: 13.4375[0m
[32m[2022-09-16 17:14:00,995] [    INFO][0m - loss: 0.15461738, learning_rate: 1.6484374999999998e-06, global_step: 1730, interval_runtime: 6.4176, interval_samples_per_second: 2.493, interval_steps_per_second: 1.558, epoch: 13.5156[0m
[32m[2022-09-16 17:14:07,383] [    INFO][0m - loss: 0.18806942, learning_rate: 1.640625e-06, global_step: 1740, interval_runtime: 6.3874, interval_samples_per_second: 2.505, interval_steps_per_second: 1.566, epoch: 13.5938[0m
[32m[2022-09-16 17:14:13,776] [    INFO][0m - loss: 0.14460049, learning_rate: 1.6328125e-06, global_step: 1750, interval_runtime: 6.3931, interval_samples_per_second: 2.503, interval_steps_per_second: 1.564, epoch: 13.6719[0m
[32m[2022-09-16 17:14:20,167] [    INFO][0m - loss: 0.12035296, learning_rate: 1.625e-06, global_step: 1760, interval_runtime: 6.3907, interval_samples_per_second: 2.504, interval_steps_per_second: 1.565, epoch: 13.75[0m
[32m[2022-09-16 17:14:26,577] [    INFO][0m - loss: 0.17661802, learning_rate: 1.6171875000000001e-06, global_step: 1770, interval_runtime: 6.4107, interval_samples_per_second: 2.496, interval_steps_per_second: 1.56, epoch: 13.8281[0m
[32m[2022-09-16 17:14:32,988] [    INFO][0m - loss: 0.13277178, learning_rate: 1.6093750000000002e-06, global_step: 1780, interval_runtime: 6.4106, interval_samples_per_second: 2.496, interval_steps_per_second: 1.56, epoch: 13.9062[0m
[32m[2022-09-16 17:14:39,297] [    INFO][0m - loss: 0.11948463, learning_rate: 1.6015625e-06, global_step: 1790, interval_runtime: 6.3084, interval_samples_per_second: 2.536, interval_steps_per_second: 1.585, epoch: 13.9844[0m
[32m[2022-09-16 17:14:40,098] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 17:14:40,098] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-16 17:14:40,098] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 17:14:40,098] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 17:14:40,098] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-16 17:15:13,159] [    INFO][0m - eval_loss: 1.7461875677108765, eval_accuracy: 0.5725338491295938, eval_runtime: 33.0598, eval_samples_per_second: 62.553, eval_steps_per_second: 3.932, epoch: 14.0[0m
[32m[2022-09-16 17:15:13,188] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-1792[0m
[32m[2022-09-16 17:15:13,189] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 17:15:15,794] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-1792/tokenizer_config.json[0m
[32m[2022-09-16 17:15:15,794] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-1792/special_tokens_map.json[0m
[32m[2022-09-16 17:15:26,076] [    INFO][0m - loss: 0.11658792, learning_rate: 1.59375e-06, global_step: 1800, interval_runtime: 46.78, interval_samples_per_second: 0.342, interval_steps_per_second: 0.214, epoch: 14.0625[0m
[32m[2022-09-16 17:15:32,454] [    INFO][0m - loss: 0.14728105, learning_rate: 1.5859375000000002e-06, global_step: 1810, interval_runtime: 6.3778, interval_samples_per_second: 2.509, interval_steps_per_second: 1.568, epoch: 14.1406[0m
[32m[2022-09-16 17:15:38,843] [    INFO][0m - loss: 0.12126832, learning_rate: 1.578125e-06, global_step: 1820, interval_runtime: 6.3888, interval_samples_per_second: 2.504, interval_steps_per_second: 1.565, epoch: 14.2188[0m
[32m[2022-09-16 17:15:45,250] [    INFO][0m - loss: 0.11181363, learning_rate: 1.5703125e-06, global_step: 1830, interval_runtime: 6.4071, interval_samples_per_second: 2.497, interval_steps_per_second: 1.561, epoch: 14.2969[0m
[32m[2022-09-16 17:15:51,688] [    INFO][0m - loss: 0.1197414, learning_rate: 1.5625e-06, global_step: 1840, interval_runtime: 6.4378, interval_samples_per_second: 2.485, interval_steps_per_second: 1.553, epoch: 14.375[0m
[32m[2022-09-16 17:15:58,116] [    INFO][0m - loss: 0.09693307, learning_rate: 1.5546874999999999e-06, global_step: 1850, interval_runtime: 6.4277, interval_samples_per_second: 2.489, interval_steps_per_second: 1.556, epoch: 14.4531[0m
[32m[2022-09-16 17:16:04,509] [    INFO][0m - loss: 0.1557531, learning_rate: 1.5468750000000001e-06, global_step: 1860, interval_runtime: 6.3932, interval_samples_per_second: 2.503, interval_steps_per_second: 1.564, epoch: 14.5312[0m
[32m[2022-09-16 17:16:10,925] [    INFO][0m - loss: 0.10829253, learning_rate: 1.5390625000000001e-06, global_step: 1870, interval_runtime: 6.4163, interval_samples_per_second: 2.494, interval_steps_per_second: 1.559, epoch: 14.6094[0m
[32m[2022-09-16 17:16:17,338] [    INFO][0m - loss: 0.12984976, learning_rate: 1.53125e-06, global_step: 1880, interval_runtime: 6.4133, interval_samples_per_second: 2.495, interval_steps_per_second: 1.559, epoch: 14.6875[0m
[32m[2022-09-16 17:16:23,761] [    INFO][0m - loss: 0.0727829, learning_rate: 1.5234375e-06, global_step: 1890, interval_runtime: 6.4219, interval_samples_per_second: 2.491, interval_steps_per_second: 1.557, epoch: 14.7656[0m
[32m[2022-09-16 17:16:30,169] [    INFO][0m - loss: 0.15647693, learning_rate: 1.5156250000000002e-06, global_step: 1900, interval_runtime: 6.4085, interval_samples_per_second: 2.497, interval_steps_per_second: 1.56, epoch: 14.8438[0m
[32m[2022-09-16 17:16:36,581] [    INFO][0m - loss: 0.11476419, learning_rate: 1.5078125e-06, global_step: 1910, interval_runtime: 6.4123, interval_samples_per_second: 2.495, interval_steps_per_second: 1.56, epoch: 14.9219[0m
[32m[2022-09-16 17:16:42,420] [    INFO][0m - loss: 0.21198301, learning_rate: 1.5e-06, global_step: 1920, interval_runtime: 5.8392, interval_samples_per_second: 2.74, interval_steps_per_second: 1.713, epoch: 15.0[0m
[32m[2022-09-16 17:16:42,421] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 17:16:42,421] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-16 17:16:42,421] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 17:16:42,421] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 17:16:42,421] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-16 17:17:15,585] [    INFO][0m - eval_loss: 1.8110452890396118, eval_accuracy: 0.5735009671179884, eval_runtime: 33.163, eval_samples_per_second: 62.359, eval_steps_per_second: 3.92, epoch: 15.0[0m
[32m[2022-09-16 17:17:15,613] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-1920[0m
[32m[2022-09-16 17:17:15,613] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 17:17:18,308] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-1920/tokenizer_config.json[0m
[32m[2022-09-16 17:17:18,308] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-1920/special_tokens_map.json[0m
[32m[2022-09-16 17:17:30,352] [    INFO][0m - loss: 0.10747867, learning_rate: 1.4921875e-06, global_step: 1930, interval_runtime: 47.9316, interval_samples_per_second: 0.334, interval_steps_per_second: 0.209, epoch: 15.0781[0m
[32m[2022-09-16 17:17:36,726] [    INFO][0m - loss: 0.07888973, learning_rate: 1.484375e-06, global_step: 1940, interval_runtime: 6.3744, interval_samples_per_second: 2.51, interval_steps_per_second: 1.569, epoch: 15.1562[0m
[32m[2022-09-16 17:17:43,128] [    INFO][0m - loss: 0.07638626, learning_rate: 1.4765625e-06, global_step: 1950, interval_runtime: 6.401, interval_samples_per_second: 2.5, interval_steps_per_second: 1.562, epoch: 15.2344[0m
[32m[2022-09-16 17:17:49,528] [    INFO][0m - loss: 0.10482233, learning_rate: 1.46875e-06, global_step: 1960, interval_runtime: 6.4002, interval_samples_per_second: 2.5, interval_steps_per_second: 1.562, epoch: 15.3125[0m
[32m[2022-09-16 17:17:55,909] [    INFO][0m - loss: 0.08312349, learning_rate: 1.4609375000000001e-06, global_step: 1970, interval_runtime: 6.381, interval_samples_per_second: 2.507, interval_steps_per_second: 1.567, epoch: 15.3906[0m
[32m[2022-09-16 17:18:02,325] [    INFO][0m - loss: 0.10916568, learning_rate: 1.453125e-06, global_step: 1980, interval_runtime: 6.416, interval_samples_per_second: 2.494, interval_steps_per_second: 1.559, epoch: 15.4688[0m
[32m[2022-09-16 17:18:08,722] [    INFO][0m - loss: 0.10570766, learning_rate: 1.4453125e-06, global_step: 1990, interval_runtime: 6.3972, interval_samples_per_second: 2.501, interval_steps_per_second: 1.563, epoch: 15.5469[0m
[32m[2022-09-16 17:18:15,144] [    INFO][0m - loss: 0.08864712, learning_rate: 1.4375e-06, global_step: 2000, interval_runtime: 6.422, interval_samples_per_second: 2.491, interval_steps_per_second: 1.557, epoch: 15.625[0m
[32m[2022-09-16 17:18:21,612] [    INFO][0m - loss: 0.13482038, learning_rate: 1.4296875e-06, global_step: 2010, interval_runtime: 6.4682, interval_samples_per_second: 2.474, interval_steps_per_second: 1.546, epoch: 15.7031[0m
[32m[2022-09-16 17:18:28,019] [    INFO][0m - loss: 0.13833749, learning_rate: 1.421875e-06, global_step: 2020, interval_runtime: 6.4069, interval_samples_per_second: 2.497, interval_steps_per_second: 1.561, epoch: 15.7812[0m
[32m[2022-09-16 17:18:34,448] [    INFO][0m - loss: 0.11399803, learning_rate: 1.4140625e-06, global_step: 2030, interval_runtime: 6.4286, interval_samples_per_second: 2.489, interval_steps_per_second: 1.556, epoch: 15.8594[0m
[32m[2022-09-16 17:18:40,880] [    INFO][0m - loss: 0.08339752, learning_rate: 1.40625e-06, global_step: 2040, interval_runtime: 6.4323, interval_samples_per_second: 2.487, interval_steps_per_second: 1.555, epoch: 15.9375[0m
[32m[2022-09-16 17:18:45,436] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 17:18:45,436] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-16 17:18:45,436] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 17:18:45,436] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 17:18:45,436] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-16 17:19:18,477] [    INFO][0m - eval_loss: 1.850369930267334, eval_accuracy: 0.5764023210831721, eval_runtime: 33.0402, eval_samples_per_second: 62.59, eval_steps_per_second: 3.935, epoch: 16.0[0m
[32m[2022-09-16 17:19:18,503] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-2048[0m
[32m[2022-09-16 17:19:18,503] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 17:19:21,017] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-2048/tokenizer_config.json[0m
[32m[2022-09-16 17:19:21,017] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-2048/special_tokens_map.json[0m
[32m[2022-09-16 17:19:27,554] [    INFO][0m - loss: 0.08458127, learning_rate: 1.3984375e-06, global_step: 2050, interval_runtime: 46.6737, interval_samples_per_second: 0.343, interval_steps_per_second: 0.214, epoch: 16.0156[0m
[32m[2022-09-16 17:19:33,922] [    INFO][0m - loss: 0.0942839, learning_rate: 1.3906250000000001e-06, global_step: 2060, interval_runtime: 6.3686, interval_samples_per_second: 2.512, interval_steps_per_second: 1.57, epoch: 16.0938[0m
[32m[2022-09-16 17:19:40,294] [    INFO][0m - loss: 0.08223968, learning_rate: 1.3828125e-06, global_step: 2070, interval_runtime: 6.372, interval_samples_per_second: 2.511, interval_steps_per_second: 1.569, epoch: 16.1719[0m
[32m[2022-09-16 17:19:46,690] [    INFO][0m - loss: 0.06269299, learning_rate: 1.375e-06, global_step: 2080, interval_runtime: 6.396, interval_samples_per_second: 2.502, interval_steps_per_second: 1.563, epoch: 16.25[0m
[32m[2022-09-16 17:19:53,092] [    INFO][0m - loss: 0.0910278, learning_rate: 1.3671875e-06, global_step: 2090, interval_runtime: 6.4018, interval_samples_per_second: 2.499, interval_steps_per_second: 1.562, epoch: 16.3281[0m
[32m[2022-09-16 17:19:59,493] [    INFO][0m - loss: 0.07958657, learning_rate: 1.359375e-06, global_step: 2100, interval_runtime: 6.4008, interval_samples_per_second: 2.5, interval_steps_per_second: 1.562, epoch: 16.4062[0m
[32m[2022-09-16 17:20:05,898] [    INFO][0m - loss: 0.08651289, learning_rate: 1.3515625e-06, global_step: 2110, interval_runtime: 6.4056, interval_samples_per_second: 2.498, interval_steps_per_second: 1.561, epoch: 16.4844[0m
[32m[2022-09-16 17:20:12,302] [    INFO][0m - loss: 0.05679557, learning_rate: 1.34375e-06, global_step: 2120, interval_runtime: 6.4034, interval_samples_per_second: 2.499, interval_steps_per_second: 1.562, epoch: 16.5625[0m
[32m[2022-09-16 17:20:18,719] [    INFO][0m - loss: 0.09008019, learning_rate: 1.3359375e-06, global_step: 2130, interval_runtime: 6.4168, interval_samples_per_second: 2.493, interval_steps_per_second: 1.558, epoch: 16.6406[0m
[32m[2022-09-16 17:20:25,159] [    INFO][0m - loss: 0.08910525, learning_rate: 1.3281249999999999e-06, global_step: 2140, interval_runtime: 6.4402, interval_samples_per_second: 2.484, interval_steps_per_second: 1.553, epoch: 16.7188[0m
[32m[2022-09-16 17:20:31,569] [    INFO][0m - loss: 0.0799395, learning_rate: 1.3203125000000001e-06, global_step: 2150, interval_runtime: 6.4101, interval_samples_per_second: 2.496, interval_steps_per_second: 1.56, epoch: 16.7969[0m
[32m[2022-09-16 17:20:37,996] [    INFO][0m - loss: 0.06454544, learning_rate: 1.3125000000000001e-06, global_step: 2160, interval_runtime: 6.4269, interval_samples_per_second: 2.49, interval_steps_per_second: 1.556, epoch: 16.875[0m
[32m[2022-09-16 17:20:44,439] [    INFO][0m - loss: 0.08113065, learning_rate: 1.3046875e-06, global_step: 2170, interval_runtime: 6.443, interval_samples_per_second: 2.483, interval_steps_per_second: 1.552, epoch: 16.9531[0m
[32m[2022-09-16 17:20:47,707] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 17:20:47,707] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-16 17:20:47,707] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 17:20:47,707] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 17:20:47,707] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-16 17:21:20,743] [    INFO][0m - eval_loss: 1.8960353136062622, eval_accuracy: 0.5715667311411993, eval_runtime: 33.0351, eval_samples_per_second: 62.6, eval_steps_per_second: 3.935, epoch: 17.0[0m
[32m[2022-09-16 17:21:20,765] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-2176[0m
[32m[2022-09-16 17:21:20,766] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 17:21:23,476] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-2176/tokenizer_config.json[0m
[32m[2022-09-16 17:21:23,476] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-2176/special_tokens_map.json[0m
[32m[2022-09-16 17:21:31,033] [    INFO][0m - loss: 0.06578911, learning_rate: 1.2968750000000002e-06, global_step: 2180, interval_runtime: 46.5942, interval_samples_per_second: 0.343, interval_steps_per_second: 0.215, epoch: 17.0312[0m
[32m[2022-09-16 17:21:37,421] [    INFO][0m - loss: 0.0843105, learning_rate: 1.2890625e-06, global_step: 2190, interval_runtime: 6.388, interval_samples_per_second: 2.505, interval_steps_per_second: 1.565, epoch: 17.1094[0m
[32m[2022-09-16 17:21:43,826] [    INFO][0m - loss: 0.04946228, learning_rate: 1.28125e-06, global_step: 2200, interval_runtime: 6.4052, interval_samples_per_second: 2.498, interval_steps_per_second: 1.561, epoch: 17.1875[0m
[32m[2022-09-16 17:21:50,233] [    INFO][0m - loss: 0.05375872, learning_rate: 1.2734375e-06, global_step: 2210, interval_runtime: 6.4065, interval_samples_per_second: 2.497, interval_steps_per_second: 1.561, epoch: 17.2656[0m
[32m[2022-09-16 17:21:56,641] [    INFO][0m - loss: 0.07265263, learning_rate: 1.265625e-06, global_step: 2220, interval_runtime: 6.4083, interval_samples_per_second: 2.497, interval_steps_per_second: 1.56, epoch: 17.3438[0m
[32m[2022-09-16 17:22:03,075] [    INFO][0m - loss: 0.09257629, learning_rate: 1.2578125e-06, global_step: 2230, interval_runtime: 6.4334, interval_samples_per_second: 2.487, interval_steps_per_second: 1.554, epoch: 17.4219[0m
[32m[2022-09-16 17:22:09,472] [    INFO][0m - loss: 0.05875472, learning_rate: 1.25e-06, global_step: 2240, interval_runtime: 6.3973, interval_samples_per_second: 2.501, interval_steps_per_second: 1.563, epoch: 17.5[0m
[32m[2022-09-16 17:22:15,898] [    INFO][0m - loss: 0.07028735, learning_rate: 1.2421875000000001e-06, global_step: 2250, interval_runtime: 6.426, interval_samples_per_second: 2.49, interval_steps_per_second: 1.556, epoch: 17.5781[0m
[32m[2022-09-16 17:22:22,301] [    INFO][0m - loss: 0.0526102, learning_rate: 1.234375e-06, global_step: 2260, interval_runtime: 6.4028, interval_samples_per_second: 2.499, interval_steps_per_second: 1.562, epoch: 17.6562[0m
[32m[2022-09-16 17:22:28,716] [    INFO][0m - loss: 0.0604001, learning_rate: 1.2265625000000002e-06, global_step: 2270, interval_runtime: 6.4151, interval_samples_per_second: 2.494, interval_steps_per_second: 1.559, epoch: 17.7344[0m
[32m[2022-09-16 17:22:35,131] [    INFO][0m - loss: 0.064292, learning_rate: 1.21875e-06, global_step: 2280, interval_runtime: 6.4147, interval_samples_per_second: 2.494, interval_steps_per_second: 1.559, epoch: 17.8125[0m
[32m[2022-09-16 17:22:41,542] [    INFO][0m - loss: 0.07863427, learning_rate: 1.2109375e-06, global_step: 2290, interval_runtime: 6.4115, interval_samples_per_second: 2.496, interval_steps_per_second: 1.56, epoch: 17.8906[0m
[32m[2022-09-16 17:22:47,893] [    INFO][0m - loss: 0.07614697, learning_rate: 1.203125e-06, global_step: 2300, interval_runtime: 6.3513, interval_samples_per_second: 2.519, interval_steps_per_second: 1.574, epoch: 17.9688[0m
[32m[2022-09-16 17:22:49,923] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 17:22:49,923] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-16 17:22:49,924] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 17:22:49,925] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 17:22:49,925] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-16 17:23:23,156] [    INFO][0m - eval_loss: 1.9540259838104248, eval_accuracy: 0.574468085106383, eval_runtime: 33.2327, eval_samples_per_second: 62.228, eval_steps_per_second: 3.912, epoch: 18.0[0m
[32m[2022-09-16 17:23:23,179] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-2304[0m
[32m[2022-09-16 17:23:23,179] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 17:23:26,174] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-2304/tokenizer_config.json[0m
[32m[2022-09-16 17:23:26,175] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-2304/special_tokens_map.json[0m
[32m[2022-09-16 17:23:35,703] [    INFO][0m - loss: 0.05174914, learning_rate: 1.1953125e-06, global_step: 2310, interval_runtime: 47.8096, interval_samples_per_second: 0.335, interval_steps_per_second: 0.209, epoch: 18.0469[0m
[32m[2022-09-16 17:23:42,562] [    INFO][0m - loss: 0.07319216, learning_rate: 1.1875e-06, global_step: 2320, interval_runtime: 6.3867, interval_samples_per_second: 2.505, interval_steps_per_second: 1.566, epoch: 18.125[0m
[32m[2022-09-16 17:23:48,994] [    INFO][0m - loss: 0.06495972, learning_rate: 1.1796875e-06, global_step: 2330, interval_runtime: 6.9043, interval_samples_per_second: 2.317, interval_steps_per_second: 1.448, epoch: 18.2031[0m
[32m[2022-09-16 17:23:55,428] [    INFO][0m - loss: 0.05806668, learning_rate: 1.1718750000000001e-06, global_step: 2340, interval_runtime: 6.4343, interval_samples_per_second: 2.487, interval_steps_per_second: 1.554, epoch: 18.2812[0m
[32m[2022-09-16 17:24:02,186] [    INFO][0m - loss: 0.0435263, learning_rate: 1.1640625e-06, global_step: 2350, interval_runtime: 6.4358, interval_samples_per_second: 2.486, interval_steps_per_second: 1.554, epoch: 18.3594[0m
[32m[2022-09-16 17:24:08,588] [    INFO][0m - loss: 0.03342235, learning_rate: 1.1562500000000002e-06, global_step: 2360, interval_runtime: 6.724, interval_samples_per_second: 2.38, interval_steps_per_second: 1.487, epoch: 18.4375[0m
[32m[2022-09-16 17:24:15,010] [    INFO][0m - loss: 0.06478451, learning_rate: 1.1484375e-06, global_step: 2370, interval_runtime: 6.4213, interval_samples_per_second: 2.492, interval_steps_per_second: 1.557, epoch: 18.5156[0m
[32m[2022-09-16 17:24:21,431] [    INFO][0m - loss: 0.04865138, learning_rate: 1.140625e-06, global_step: 2380, interval_runtime: 6.4215, interval_samples_per_second: 2.492, interval_steps_per_second: 1.557, epoch: 18.5938[0m
[32m[2022-09-16 17:24:27,851] [    INFO][0m - loss: 0.04340982, learning_rate: 1.1328125e-06, global_step: 2390, interval_runtime: 6.4202, interval_samples_per_second: 2.492, interval_steps_per_second: 1.558, epoch: 18.6719[0m
[32m[2022-09-16 17:24:34,264] [    INFO][0m - loss: 0.03872625, learning_rate: 1.125e-06, global_step: 2400, interval_runtime: 6.4126, interval_samples_per_second: 2.495, interval_steps_per_second: 1.559, epoch: 18.75[0m
[32m[2022-09-16 17:24:40,713] [    INFO][0m - loss: 0.04883215, learning_rate: 1.1171875e-06, global_step: 2410, interval_runtime: 6.4486, interval_samples_per_second: 2.481, interval_steps_per_second: 1.551, epoch: 18.8281[0m
[32m[2022-09-16 17:24:47,176] [    INFO][0m - loss: 0.06346088, learning_rate: 1.109375e-06, global_step: 2420, interval_runtime: 6.4631, interval_samples_per_second: 2.476, interval_steps_per_second: 1.547, epoch: 18.9062[0m
[32m[2022-09-16 17:24:53,503] [    INFO][0m - loss: 0.06588451, learning_rate: 1.1015625e-06, global_step: 2430, interval_runtime: 6.3271, interval_samples_per_second: 2.529, interval_steps_per_second: 1.581, epoch: 18.9844[0m
[32m[2022-09-16 17:24:54,303] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 17:24:54,303] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-16 17:24:54,304] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 17:24:54,304] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 17:24:54,304] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-16 17:25:27,501] [    INFO][0m - eval_loss: 1.9637824296951294, eval_accuracy: 0.574468085106383, eval_runtime: 33.1972, eval_samples_per_second: 62.294, eval_steps_per_second: 3.916, epoch: 19.0[0m
[32m[2022-09-16 17:25:27,525] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-2432[0m
[32m[2022-09-16 17:25:27,525] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 17:25:30,036] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-2432/tokenizer_config.json[0m
[32m[2022-09-16 17:25:30,036] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-2432/special_tokens_map.json[0m
[32m[2022-09-16 17:25:40,082] [    INFO][0m - loss: 0.04464802, learning_rate: 1.09375e-06, global_step: 2440, interval_runtime: 46.5796, interval_samples_per_second: 0.343, interval_steps_per_second: 0.215, epoch: 19.0625[0m
[32m[2022-09-16 17:25:46,445] [    INFO][0m - loss: 0.04327971, learning_rate: 1.0859375000000001e-06, global_step: 2450, interval_runtime: 6.3628, interval_samples_per_second: 2.515, interval_steps_per_second: 1.572, epoch: 19.1406[0m
[32m[2022-09-16 17:25:52,847] [    INFO][0m - loss: 0.04567421, learning_rate: 1.078125e-06, global_step: 2460, interval_runtime: 6.4015, interval_samples_per_second: 2.499, interval_steps_per_second: 1.562, epoch: 19.2188[0m
[32m[2022-09-16 17:25:59,248] [    INFO][0m - loss: 0.07399568, learning_rate: 1.0703125e-06, global_step: 2470, interval_runtime: 6.4013, interval_samples_per_second: 2.499, interval_steps_per_second: 1.562, epoch: 19.2969[0m
[32m[2022-09-16 17:26:05,649] [    INFO][0m - loss: 0.04247999, learning_rate: 1.0625e-06, global_step: 2480, interval_runtime: 6.4015, interval_samples_per_second: 2.499, interval_steps_per_second: 1.562, epoch: 19.375[0m
[32m[2022-09-16 17:26:12,043] [    INFO][0m - loss: 0.03495859, learning_rate: 1.0546875e-06, global_step: 2490, interval_runtime: 6.3941, interval_samples_per_second: 2.502, interval_steps_per_second: 1.564, epoch: 19.4531[0m
[32m[2022-09-16 17:26:18,439] [    INFO][0m - loss: 0.03617361, learning_rate: 1.046875e-06, global_step: 2500, interval_runtime: 6.3955, interval_samples_per_second: 2.502, interval_steps_per_second: 1.564, epoch: 19.5312[0m
[32m[2022-09-16 17:26:24,833] [    INFO][0m - loss: 0.04687016, learning_rate: 1.0390625e-06, global_step: 2510, interval_runtime: 6.3942, interval_samples_per_second: 2.502, interval_steps_per_second: 1.564, epoch: 19.6094[0m
[32m[2022-09-16 17:26:31,236] [    INFO][0m - loss: 0.05025409, learning_rate: 1.03125e-06, global_step: 2520, interval_runtime: 6.4027, interval_samples_per_second: 2.499, interval_steps_per_second: 1.562, epoch: 19.6875[0m
[32m[2022-09-16 17:26:37,629] [    INFO][0m - loss: 0.04054279, learning_rate: 1.0234374999999999e-06, global_step: 2530, interval_runtime: 6.3935, interval_samples_per_second: 2.503, interval_steps_per_second: 1.564, epoch: 19.7656[0m
[32m[2022-09-16 17:26:44,045] [    INFO][0m - loss: 0.03300165, learning_rate: 1.0156250000000001e-06, global_step: 2540, interval_runtime: 6.4159, interval_samples_per_second: 2.494, interval_steps_per_second: 1.559, epoch: 19.8438[0m
[32m[2022-09-16 17:26:50,455] [    INFO][0m - loss: 0.05614079, learning_rate: 1.0078125e-06, global_step: 2550, interval_runtime: 6.4096, interval_samples_per_second: 2.496, interval_steps_per_second: 1.56, epoch: 19.9219[0m
[32m[2022-09-16 17:26:56,301] [    INFO][0m - loss: 0.03977217, learning_rate: 1e-06, global_step: 2560, interval_runtime: 5.8458, interval_samples_per_second: 2.737, interval_steps_per_second: 1.711, epoch: 20.0[0m
[32m[2022-09-16 17:26:56,301] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 17:26:56,302] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-16 17:26:56,302] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 17:26:56,302] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 17:26:56,302] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-16 17:27:29,309] [    INFO][0m - eval_loss: 2.0017166137695312, eval_accuracy: 0.574468085106383, eval_runtime: 33.0067, eval_samples_per_second: 62.654, eval_steps_per_second: 3.939, epoch: 20.0[0m
[32m[2022-09-16 17:27:29,330] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-2560[0m
[32m[2022-09-16 17:27:29,330] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 17:27:31,807] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-2560/tokenizer_config.json[0m
[32m[2022-09-16 17:27:31,807] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-2560/special_tokens_map.json[0m
[32m[2022-09-16 17:27:43,052] [    INFO][0m - loss: 0.03114043, learning_rate: 9.921875e-07, global_step: 2570, interval_runtime: 46.7514, interval_samples_per_second: 0.342, interval_steps_per_second: 0.214, epoch: 20.0781[0m
[32m[2022-09-16 17:27:49,423] [    INFO][0m - loss: 0.04003339, learning_rate: 9.84375e-07, global_step: 2580, interval_runtime: 6.3711, interval_samples_per_second: 2.511, interval_steps_per_second: 1.57, epoch: 20.1562[0m
[32m[2022-09-16 17:27:55,840] [    INFO][0m - loss: 0.04599659, learning_rate: 9.765625e-07, global_step: 2590, interval_runtime: 6.4169, interval_samples_per_second: 2.493, interval_steps_per_second: 1.558, epoch: 20.2344[0m
[32m[2022-09-16 17:28:02,252] [    INFO][0m - loss: 0.03690259, learning_rate: 9.6875e-07, global_step: 2600, interval_runtime: 6.4123, interval_samples_per_second: 2.495, interval_steps_per_second: 1.56, epoch: 20.3125[0m
[32m[2022-09-16 17:28:08,677] [    INFO][0m - loss: 0.02513932, learning_rate: 9.609375e-07, global_step: 2610, interval_runtime: 6.4242, interval_samples_per_second: 2.491, interval_steps_per_second: 1.557, epoch: 20.3906[0m
[32m[2022-09-16 17:28:15,080] [    INFO][0m - loss: 0.02682516, learning_rate: 9.53125e-07, global_step: 2620, interval_runtime: 6.4033, interval_samples_per_second: 2.499, interval_steps_per_second: 1.562, epoch: 20.4688[0m
[32m[2022-09-16 17:28:21,476] [    INFO][0m - loss: 0.04555835, learning_rate: 9.453125000000001e-07, global_step: 2630, interval_runtime: 6.3958, interval_samples_per_second: 2.502, interval_steps_per_second: 1.564, epoch: 20.5469[0m
[32m[2022-09-16 17:28:27,889] [    INFO][0m - loss: 0.03007444, learning_rate: 9.375e-07, global_step: 2640, interval_runtime: 6.413, interval_samples_per_second: 2.495, interval_steps_per_second: 1.559, epoch: 20.625[0m
[32m[2022-09-16 17:28:34,300] [    INFO][0m - loss: 0.04933803, learning_rate: 9.296874999999999e-07, global_step: 2650, interval_runtime: 6.4114, interval_samples_per_second: 2.496, interval_steps_per_second: 1.56, epoch: 20.7031[0m
[32m[2022-09-16 17:28:40,706] [    INFO][0m - loss: 0.04347131, learning_rate: 9.218750000000001e-07, global_step: 2660, interval_runtime: 6.4063, interval_samples_per_second: 2.498, interval_steps_per_second: 1.561, epoch: 20.7812[0m
[32m[2022-09-16 17:28:47,099] [    INFO][0m - loss: 0.03814822, learning_rate: 9.140625e-07, global_step: 2670, interval_runtime: 6.3924, interval_samples_per_second: 2.503, interval_steps_per_second: 1.564, epoch: 20.8594[0m
[32m[2022-09-16 17:28:53,518] [    INFO][0m - loss: 0.02035309, learning_rate: 9.0625e-07, global_step: 2680, interval_runtime: 6.4185, interval_samples_per_second: 2.493, interval_steps_per_second: 1.558, epoch: 20.9375[0m
[32m[2022-09-16 17:28:58,062] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 17:28:58,062] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-16 17:28:58,062] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 17:28:58,062] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 17:28:58,062] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-16 17:29:31,203] [    INFO][0m - eval_loss: 2.027266025543213, eval_accuracy: 0.5749516441005803, eval_runtime: 33.1408, eval_samples_per_second: 62.4, eval_steps_per_second: 3.923, epoch: 21.0[0m
[32m[2022-09-16 17:29:31,238] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-2688[0m
[32m[2022-09-16 17:29:31,238] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 17:29:33,841] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-2688/tokenizer_config.json[0m
[32m[2022-09-16 17:29:33,841] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-2688/special_tokens_map.json[0m
[32m[2022-09-16 17:29:40,077] [    INFO][0m - loss: 0.0579718, learning_rate: 8.984375e-07, global_step: 2690, interval_runtime: 46.5594, interval_samples_per_second: 0.344, interval_steps_per_second: 0.215, epoch: 21.0156[0m
[32m[2022-09-16 17:29:46,452] [    INFO][0m - loss: 0.01916523, learning_rate: 8.90625e-07, global_step: 2700, interval_runtime: 6.3756, interval_samples_per_second: 2.51, interval_steps_per_second: 1.568, epoch: 21.0938[0m
[32m[2022-09-16 17:29:52,832] [    INFO][0m - loss: 0.02324691, learning_rate: 8.828125e-07, global_step: 2710, interval_runtime: 6.3795, interval_samples_per_second: 2.508, interval_steps_per_second: 1.568, epoch: 21.1719[0m
[32m[2022-09-16 17:29:59,199] [    INFO][0m - loss: 0.0394757, learning_rate: 8.750000000000001e-07, global_step: 2720, interval_runtime: 6.3667, interval_samples_per_second: 2.513, interval_steps_per_second: 1.571, epoch: 21.25[0m
[32m[2022-09-16 17:30:05,595] [    INFO][0m - loss: 0.03983814, learning_rate: 8.671875e-07, global_step: 2730, interval_runtime: 6.3957, interval_samples_per_second: 2.502, interval_steps_per_second: 1.564, epoch: 21.3281[0m
[32m[2022-09-16 17:30:11,979] [    INFO][0m - loss: 0.03392746, learning_rate: 8.593749999999999e-07, global_step: 2740, interval_runtime: 6.3845, interval_samples_per_second: 2.506, interval_steps_per_second: 1.566, epoch: 21.4062[0m
[32m[2022-09-16 17:30:18,379] [    INFO][0m - loss: 0.04201311, learning_rate: 8.515625e-07, global_step: 2750, interval_runtime: 6.4004, interval_samples_per_second: 2.5, interval_steps_per_second: 1.562, epoch: 21.4844[0m
[32m[2022-09-16 17:30:24,778] [    INFO][0m - loss: 0.02179307, learning_rate: 8.4375e-07, global_step: 2760, interval_runtime: 6.3984, interval_samples_per_second: 2.501, interval_steps_per_second: 1.563, epoch: 21.5625[0m
[32m[2022-09-16 17:30:31,170] [    INFO][0m - loss: 0.02743849, learning_rate: 8.359375e-07, global_step: 2770, interval_runtime: 6.3921, interval_samples_per_second: 2.503, interval_steps_per_second: 1.564, epoch: 21.6406[0m
[32m[2022-09-16 17:30:37,597] [    INFO][0m - loss: 0.02510828, learning_rate: 8.281250000000001e-07, global_step: 2780, interval_runtime: 6.427, interval_samples_per_second: 2.49, interval_steps_per_second: 1.556, epoch: 21.7188[0m
[32m[2022-09-16 17:30:44,012] [    INFO][0m - loss: 0.02421897, learning_rate: 8.203125e-07, global_step: 2790, interval_runtime: 6.4146, interval_samples_per_second: 2.494, interval_steps_per_second: 1.559, epoch: 21.7969[0m
[32m[2022-09-16 17:30:50,417] [    INFO][0m - loss: 0.02437572, learning_rate: 8.125e-07, global_step: 2800, interval_runtime: 6.4052, interval_samples_per_second: 2.498, interval_steps_per_second: 1.561, epoch: 21.875[0m
[32m[2022-09-16 17:30:56,837] [    INFO][0m - loss: 0.02836463, learning_rate: 8.046875000000001e-07, global_step: 2810, interval_runtime: 6.4204, interval_samples_per_second: 2.492, interval_steps_per_second: 1.558, epoch: 21.9531[0m
[32m[2022-09-16 17:31:00,105] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 17:31:00,105] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-16 17:31:00,105] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 17:31:00,105] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 17:31:00,105] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-16 17:31:33,134] [    INFO][0m - eval_loss: 2.0471417903900146, eval_accuracy: 0.5715667311411993, eval_runtime: 33.0287, eval_samples_per_second: 62.612, eval_steps_per_second: 3.936, epoch: 22.0[0m
[32m[2022-09-16 17:31:33,169] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-2816[0m
[32m[2022-09-16 17:31:33,170] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 17:31:35,696] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-2816/tokenizer_config.json[0m
[32m[2022-09-16 17:31:35,696] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-2816/special_tokens_map.json[0m
[32m[2022-09-16 17:31:43,197] [    INFO][0m - loss: 0.03692332, learning_rate: 7.96875e-07, global_step: 2820, interval_runtime: 46.3604, interval_samples_per_second: 0.345, interval_steps_per_second: 0.216, epoch: 22.0312[0m
[32m[2022-09-16 17:31:49,572] [    INFO][0m - loss: 0.01596821, learning_rate: 7.890625e-07, global_step: 2830, interval_runtime: 6.3742, interval_samples_per_second: 2.51, interval_steps_per_second: 1.569, epoch: 22.1094[0m
[32m[2022-09-16 17:31:55,963] [    INFO][0m - loss: 0.02561781, learning_rate: 7.8125e-07, global_step: 2840, interval_runtime: 6.391, interval_samples_per_second: 2.504, interval_steps_per_second: 1.565, epoch: 22.1875[0m
[32m[2022-09-16 17:32:02,356] [    INFO][0m - loss: 0.03898153, learning_rate: 7.734375000000001e-07, global_step: 2850, interval_runtime: 6.3937, interval_samples_per_second: 2.502, interval_steps_per_second: 1.564, epoch: 22.2656[0m
[32m[2022-09-16 17:32:08,776] [    INFO][0m - loss: 0.01921339, learning_rate: 7.65625e-07, global_step: 2860, interval_runtime: 6.4202, interval_samples_per_second: 2.492, interval_steps_per_second: 1.558, epoch: 22.3438[0m
[32m[2022-09-16 17:32:15,200] [    INFO][0m - loss: 0.02698714, learning_rate: 7.578125000000001e-07, global_step: 2870, interval_runtime: 6.4231, interval_samples_per_second: 2.491, interval_steps_per_second: 1.557, epoch: 22.4219[0m
[32m[2022-09-16 17:32:21,592] [    INFO][0m - loss: 0.02537999, learning_rate: 7.5e-07, global_step: 2880, interval_runtime: 6.3925, interval_samples_per_second: 2.503, interval_steps_per_second: 1.564, epoch: 22.5[0m
[32m[2022-09-16 17:32:27,988] [    INFO][0m - loss: 0.02243882, learning_rate: 7.421875e-07, global_step: 2890, interval_runtime: 6.3954, interval_samples_per_second: 2.502, interval_steps_per_second: 1.564, epoch: 22.5781[0m
[32m[2022-09-16 17:32:34,365] [    INFO][0m - loss: 0.02390041, learning_rate: 7.34375e-07, global_step: 2900, interval_runtime: 6.3774, interval_samples_per_second: 2.509, interval_steps_per_second: 1.568, epoch: 22.6562[0m
[32m[2022-09-16 17:32:40,767] [    INFO][0m - loss: 0.00802457, learning_rate: 7.265625e-07, global_step: 2910, interval_runtime: 6.4018, interval_samples_per_second: 2.499, interval_steps_per_second: 1.562, epoch: 22.7344[0m
[32m[2022-09-16 17:32:47,181] [    INFO][0m - loss: 0.02665495, learning_rate: 7.1875e-07, global_step: 2920, interval_runtime: 6.4136, interval_samples_per_second: 2.495, interval_steps_per_second: 1.559, epoch: 22.8125[0m
[32m[2022-09-16 17:32:53,573] [    INFO][0m - loss: 0.02253516, learning_rate: 7.109375e-07, global_step: 2930, interval_runtime: 6.3927, interval_samples_per_second: 2.503, interval_steps_per_second: 1.564, epoch: 22.8906[0m
[32m[2022-09-16 17:32:59,922] [    INFO][0m - loss: 0.02280518, learning_rate: 7.03125e-07, global_step: 2940, interval_runtime: 6.3486, interval_samples_per_second: 2.52, interval_steps_per_second: 1.575, epoch: 22.9688[0m
[32m[2022-09-16 17:33:01,947] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 17:33:01,947] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-16 17:33:01,947] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 17:33:01,947] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 17:33:01,948] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-16 17:33:35,116] [    INFO][0m - eval_loss: 2.0767133235931396, eval_accuracy: 0.5754352030947776, eval_runtime: 33.1683, eval_samples_per_second: 62.349, eval_steps_per_second: 3.919, epoch: 23.0[0m
[32m[2022-09-16 17:33:35,151] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-2944[0m
[32m[2022-09-16 17:33:35,151] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 17:33:37,777] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-2944/tokenizer_config.json[0m
[32m[2022-09-16 17:33:37,778] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-2944/special_tokens_map.json[0m
[32m[2022-09-16 17:33:46,606] [    INFO][0m - loss: 0.02362932, learning_rate: 6.953125000000001e-07, global_step: 2950, interval_runtime: 46.684, interval_samples_per_second: 0.343, interval_steps_per_second: 0.214, epoch: 23.0469[0m
[32m[2022-09-16 17:33:52,995] [    INFO][0m - loss: 0.02196116, learning_rate: 6.875e-07, global_step: 2960, interval_runtime: 6.389, interval_samples_per_second: 2.504, interval_steps_per_second: 1.565, epoch: 23.125[0m
[32m[2022-09-16 17:33:59,420] [    INFO][0m - loss: 0.02078149, learning_rate: 6.796875e-07, global_step: 2970, interval_runtime: 6.4251, interval_samples_per_second: 2.49, interval_steps_per_second: 1.556, epoch: 23.2031[0m
[32m[2022-09-16 17:34:05,836] [    INFO][0m - loss: 0.02554854, learning_rate: 6.71875e-07, global_step: 2980, interval_runtime: 6.4156, interval_samples_per_second: 2.494, interval_steps_per_second: 1.559, epoch: 23.2812[0m
[32m[2022-09-16 17:34:12,241] [    INFO][0m - loss: 0.02582177, learning_rate: 6.640624999999999e-07, global_step: 2990, interval_runtime: 6.4054, interval_samples_per_second: 2.498, interval_steps_per_second: 1.561, epoch: 23.3594[0m
[32m[2022-09-16 17:34:18,623] [    INFO][0m - loss: 0.0231749, learning_rate: 6.562500000000001e-07, global_step: 3000, interval_runtime: 6.3822, interval_samples_per_second: 2.507, interval_steps_per_second: 1.567, epoch: 23.4375[0m
[32m[2022-09-16 17:34:25,039] [    INFO][0m - loss: 0.04045327, learning_rate: 6.484375000000001e-07, global_step: 3010, interval_runtime: 6.416, interval_samples_per_second: 2.494, interval_steps_per_second: 1.559, epoch: 23.5156[0m
[32m[2022-09-16 17:34:31,445] [    INFO][0m - loss: 0.02001464, learning_rate: 6.40625e-07, global_step: 3020, interval_runtime: 6.4061, interval_samples_per_second: 2.498, interval_steps_per_second: 1.561, epoch: 23.5938[0m
[32m[2022-09-16 17:34:37,830] [    INFO][0m - loss: 0.01263907, learning_rate: 6.328125e-07, global_step: 3030, interval_runtime: 6.3843, interval_samples_per_second: 2.506, interval_steps_per_second: 1.566, epoch: 23.6719[0m
[32m[2022-09-16 17:34:44,245] [    INFO][0m - loss: 0.01823932, learning_rate: 6.25e-07, global_step: 3040, interval_runtime: 6.4145, interval_samples_per_second: 2.494, interval_steps_per_second: 1.559, epoch: 23.75[0m
[32m[2022-09-16 17:34:50,660] [    INFO][0m - loss: 0.03424033, learning_rate: 6.171875e-07, global_step: 3050, interval_runtime: 6.4162, interval_samples_per_second: 2.494, interval_steps_per_second: 1.559, epoch: 23.8281[0m
[32m[2022-09-16 17:34:57,084] [    INFO][0m - loss: 0.02702679, learning_rate: 6.09375e-07, global_step: 3060, interval_runtime: 6.4241, interval_samples_per_second: 2.491, interval_steps_per_second: 1.557, epoch: 23.9062[0m
[32m[2022-09-16 17:35:03,401] [    INFO][0m - loss: 0.01865573, learning_rate: 6.015625e-07, global_step: 3070, interval_runtime: 6.3168, interval_samples_per_second: 2.533, interval_steps_per_second: 1.583, epoch: 23.9844[0m
[32m[2022-09-16 17:35:04,196] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 17:35:04,196] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-16 17:35:04,196] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 17:35:04,196] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 17:35:04,196] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-16 17:35:37,212] [    INFO][0m - eval_loss: 2.0766754150390625, eval_accuracy: 0.5764023210831721, eval_runtime: 33.0155, eval_samples_per_second: 62.637, eval_steps_per_second: 3.938, epoch: 24.0[0m
[32m[2022-09-16 17:35:37,233] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-3072[0m
[32m[2022-09-16 17:35:37,234] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 17:35:39,950] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-3072/tokenizer_config.json[0m
[32m[2022-09-16 17:35:39,951] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-3072/special_tokens_map.json[0m
[32m[2022-09-16 17:35:50,588] [    INFO][0m - loss: 0.02398095, learning_rate: 5.9375e-07, global_step: 3080, interval_runtime: 47.1861, interval_samples_per_second: 0.339, interval_steps_per_second: 0.212, epoch: 24.0625[0m
[32m[2022-09-16 17:35:56,963] [    INFO][0m - loss: 0.01174599, learning_rate: 5.859375000000001e-07, global_step: 3090, interval_runtime: 6.375, interval_samples_per_second: 2.51, interval_steps_per_second: 1.569, epoch: 24.1406[0m
[32m[2022-09-16 17:36:03,335] [    INFO][0m - loss: 0.01682093, learning_rate: 5.781250000000001e-07, global_step: 3100, interval_runtime: 6.3728, interval_samples_per_second: 2.511, interval_steps_per_second: 1.569, epoch: 24.2188[0m
[32m[2022-09-16 17:36:09,697] [    INFO][0m - loss: 0.01619306, learning_rate: 5.703125e-07, global_step: 3110, interval_runtime: 6.3614, interval_samples_per_second: 2.515, interval_steps_per_second: 1.572, epoch: 24.2969[0m
[32m[2022-09-16 17:36:16,078] [    INFO][0m - loss: 0.02438854, learning_rate: 5.625e-07, global_step: 3120, interval_runtime: 6.3817, interval_samples_per_second: 2.507, interval_steps_per_second: 1.567, epoch: 24.375[0m
[32m[2022-09-16 17:36:22,466] [    INFO][0m - loss: 0.01933287, learning_rate: 5.546875e-07, global_step: 3130, interval_runtime: 6.3875, interval_samples_per_second: 2.505, interval_steps_per_second: 1.566, epoch: 24.4531[0m
[32m[2022-09-16 17:36:28,883] [    INFO][0m - loss: 0.02575489, learning_rate: 5.46875e-07, global_step: 3140, interval_runtime: 6.4167, interval_samples_per_second: 2.493, interval_steps_per_second: 1.558, epoch: 24.5312[0m
[32m[2022-09-16 17:36:35,328] [    INFO][0m - loss: 0.02207024, learning_rate: 5.390625e-07, global_step: 3150, interval_runtime: 6.4452, interval_samples_per_second: 2.482, interval_steps_per_second: 1.552, epoch: 24.6094[0m
[32m[2022-09-16 17:36:41,741] [    INFO][0m - loss: 0.0141345, learning_rate: 5.3125e-07, global_step: 3160, interval_runtime: 6.4129, interval_samples_per_second: 2.495, interval_steps_per_second: 1.559, epoch: 24.6875[0m
[32m[2022-09-16 17:36:48,217] [    INFO][0m - loss: 0.01976574, learning_rate: 5.234375e-07, global_step: 3170, interval_runtime: 6.4761, interval_samples_per_second: 2.471, interval_steps_per_second: 1.544, epoch: 24.7656[0m
[32m[2022-09-16 17:36:54,685] [    INFO][0m - loss: 0.02027847, learning_rate: 5.15625e-07, global_step: 3180, interval_runtime: 6.4674, interval_samples_per_second: 2.474, interval_steps_per_second: 1.546, epoch: 24.8438[0m
[32m[2022-09-16 17:37:01,114] [    INFO][0m - loss: 0.01664998, learning_rate: 5.078125000000001e-07, global_step: 3190, interval_runtime: 6.4298, interval_samples_per_second: 2.488, interval_steps_per_second: 1.555, epoch: 24.9219[0m
[32m[2022-09-16 17:37:06,975] [    INFO][0m - loss: 0.0144706, learning_rate: 5e-07, global_step: 3200, interval_runtime: 5.8613, interval_samples_per_second: 2.73, interval_steps_per_second: 1.706, epoch: 25.0[0m
[32m[2022-09-16 17:37:06,976] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 17:37:06,976] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-16 17:37:06,976] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 17:37:06,976] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 17:37:06,976] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-16 17:37:40,246] [    INFO][0m - eval_loss: 2.111285448074341, eval_accuracy: 0.5720502901353965, eval_runtime: 33.2692, eval_samples_per_second: 62.16, eval_steps_per_second: 3.908, epoch: 25.0[0m
[32m[2022-09-16 17:37:40,291] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-3200[0m
[32m[2022-09-16 17:37:40,292] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 17:37:42,942] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-3200/tokenizer_config.json[0m
[32m[2022-09-16 17:37:42,943] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-3200/special_tokens_map.json[0m
[32m[2022-09-16 17:37:56,070] [    INFO][0m - loss: 0.01262254, learning_rate: 4.921875e-07, global_step: 3210, interval_runtime: 49.0948, interval_samples_per_second: 0.326, interval_steps_per_second: 0.204, epoch: 25.0781[0m
[32m[2022-09-16 17:38:02,473] [    INFO][0m - loss: 0.01811042, learning_rate: 4.84375e-07, global_step: 3220, interval_runtime: 6.4023, interval_samples_per_second: 2.499, interval_steps_per_second: 1.562, epoch: 25.1562[0m
[32m[2022-09-16 17:38:08,864] [    INFO][0m - loss: 0.01834141, learning_rate: 4.765625e-07, global_step: 3230, interval_runtime: 6.391, interval_samples_per_second: 2.504, interval_steps_per_second: 1.565, epoch: 25.2344[0m
[32m[2022-09-16 17:38:15,253] [    INFO][0m - loss: 0.01788127, learning_rate: 4.6875e-07, global_step: 3240, interval_runtime: 6.3894, interval_samples_per_second: 2.504, interval_steps_per_second: 1.565, epoch: 25.3125[0m
[32m[2022-09-16 17:38:21,683] [    INFO][0m - loss: 0.01317778, learning_rate: 4.6093750000000003e-07, global_step: 3250, interval_runtime: 6.4298, interval_samples_per_second: 2.488, interval_steps_per_second: 1.555, epoch: 25.3906[0m
[32m[2022-09-16 17:38:28,091] [    INFO][0m - loss: 0.01349624, learning_rate: 4.53125e-07, global_step: 3260, interval_runtime: 6.4078, interval_samples_per_second: 2.497, interval_steps_per_second: 1.561, epoch: 25.4688[0m
[32m[2022-09-16 17:38:34,476] [    INFO][0m - loss: 0.01610654, learning_rate: 4.453125e-07, global_step: 3270, interval_runtime: 6.3856, interval_samples_per_second: 2.506, interval_steps_per_second: 1.566, epoch: 25.5469[0m
[32m[2022-09-16 17:38:40,868] [    INFO][0m - loss: 0.01678957, learning_rate: 4.3750000000000005e-07, global_step: 3280, interval_runtime: 6.3922, interval_samples_per_second: 2.503, interval_steps_per_second: 1.564, epoch: 25.625[0m
[32m[2022-09-16 17:38:47,243] [    INFO][0m - loss: 0.01262518, learning_rate: 4.2968749999999996e-07, global_step: 3290, interval_runtime: 6.3744, interval_samples_per_second: 2.51, interval_steps_per_second: 1.569, epoch: 25.7031[0m
[32m[2022-09-16 17:38:53,642] [    INFO][0m - loss: 0.01280788, learning_rate: 4.21875e-07, global_step: 3300, interval_runtime: 6.399, interval_samples_per_second: 2.5, interval_steps_per_second: 1.563, epoch: 25.7812[0m
[32m[2022-09-16 17:39:00,055] [    INFO][0m - loss: 0.01602975, learning_rate: 4.1406250000000006e-07, global_step: 3310, interval_runtime: 6.4133, interval_samples_per_second: 2.495, interval_steps_per_second: 1.559, epoch: 25.8594[0m
[32m[2022-09-16 17:39:06,449] [    INFO][0m - loss: 0.02424874, learning_rate: 4.0625e-07, global_step: 3320, interval_runtime: 6.3937, interval_samples_per_second: 2.502, interval_steps_per_second: 1.564, epoch: 25.9375[0m
[32m[2022-09-16 17:39:10,986] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 17:39:10,986] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-16 17:39:10,987] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 17:39:10,987] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 17:39:10,987] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-16 17:39:44,027] [    INFO][0m - eval_loss: 2.1397502422332764, eval_accuracy: 0.5754352030947776, eval_runtime: 33.0399, eval_samples_per_second: 62.591, eval_steps_per_second: 3.935, epoch: 26.0[0m
[32m[2022-09-16 17:39:44,062] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-3328[0m
[32m[2022-09-16 17:39:44,062] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 17:39:46,554] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-3328/tokenizer_config.json[0m
[32m[2022-09-16 17:39:46,554] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-3328/special_tokens_map.json[0m
[32m[2022-09-16 17:39:52,898] [    INFO][0m - loss: 0.0165384, learning_rate: 3.984375e-07, global_step: 3330, interval_runtime: 46.2649, interval_samples_per_second: 0.346, interval_steps_per_second: 0.216, epoch: 26.0156[0m
[32m[2022-09-16 17:39:59,256] [    INFO][0m - loss: 0.01013571, learning_rate: 3.90625e-07, global_step: 3340, interval_runtime: 6.5421, interval_samples_per_second: 2.446, interval_steps_per_second: 1.529, epoch: 26.0938[0m
[32m[2022-09-16 17:40:05,638] [    INFO][0m - loss: 0.02249135, learning_rate: 3.828125e-07, global_step: 3350, interval_runtime: 6.3824, interval_samples_per_second: 2.507, interval_steps_per_second: 1.567, epoch: 26.1719[0m
[32m[2022-09-16 17:40:12,044] [    INFO][0m - loss: 0.01640797, learning_rate: 3.75e-07, global_step: 3360, interval_runtime: 6.406, interval_samples_per_second: 2.498, interval_steps_per_second: 1.561, epoch: 26.25[0m
[32m[2022-09-16 17:40:18,455] [    INFO][0m - loss: 0.01209948, learning_rate: 3.671875e-07, global_step: 3370, interval_runtime: 6.4105, interval_samples_per_second: 2.496, interval_steps_per_second: 1.56, epoch: 26.3281[0m
[32m[2022-09-16 17:40:26,131] [    INFO][0m - loss: 0.01202657, learning_rate: 3.59375e-07, global_step: 3380, interval_runtime: 6.4004, interval_samples_per_second: 2.5, interval_steps_per_second: 1.562, epoch: 26.4062[0m
[32m[2022-09-16 17:40:32,545] [    INFO][0m - loss: 0.02366981, learning_rate: 3.515625e-07, global_step: 3390, interval_runtime: 7.6901, interval_samples_per_second: 2.081, interval_steps_per_second: 1.3, epoch: 26.4844[0m
[32m[2022-09-16 17:40:38,937] [    INFO][0m - loss: 0.0120564, learning_rate: 3.4375e-07, global_step: 3400, interval_runtime: 6.3918, interval_samples_per_second: 2.503, interval_steps_per_second: 1.565, epoch: 26.5625[0m
[32m[2022-09-16 17:40:45,341] [    INFO][0m - loss: 0.01433115, learning_rate: 3.359375e-07, global_step: 3410, interval_runtime: 6.4038, interval_samples_per_second: 2.499, interval_steps_per_second: 1.562, epoch: 26.6406[0m
[32m[2022-09-16 17:40:51,764] [    INFO][0m - loss: 0.0184484, learning_rate: 3.2812500000000003e-07, global_step: 3420, interval_runtime: 6.4225, interval_samples_per_second: 2.491, interval_steps_per_second: 1.557, epoch: 26.7188[0m
[32m[2022-09-16 17:40:58,181] [    INFO][0m - loss: 0.01860857, learning_rate: 3.203125e-07, global_step: 3430, interval_runtime: 6.4177, interval_samples_per_second: 2.493, interval_steps_per_second: 1.558, epoch: 26.7969[0m
[32m[2022-09-16 17:41:04,595] [    INFO][0m - loss: 0.01148407, learning_rate: 3.125e-07, global_step: 3440, interval_runtime: 6.4136, interval_samples_per_second: 2.495, interval_steps_per_second: 1.559, epoch: 26.875[0m
[32m[2022-09-16 17:41:11,018] [    INFO][0m - loss: 0.01279647, learning_rate: 3.046875e-07, global_step: 3450, interval_runtime: 6.4235, interval_samples_per_second: 2.491, interval_steps_per_second: 1.557, epoch: 26.9531[0m
[32m[2022-09-16 17:41:14,291] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 17:41:14,291] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-16 17:41:14,292] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 17:41:14,292] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 17:41:14,292] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-16 17:41:47,284] [    INFO][0m - eval_loss: 2.1533257961273193, eval_accuracy: 0.5735009671179884, eval_runtime: 32.992, eval_samples_per_second: 62.682, eval_steps_per_second: 3.94, epoch: 27.0[0m
[32m[2022-09-16 17:41:47,307] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-3456[0m
[32m[2022-09-16 17:41:47,307] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 17:41:49,796] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-3456/tokenizer_config.json[0m
[32m[2022-09-16 17:41:49,797] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-3456/special_tokens_map.json[0m
[32m[2022-09-16 17:41:57,310] [    INFO][0m - loss: 0.01186869, learning_rate: 2.96875e-07, global_step: 3460, interval_runtime: 46.292, interval_samples_per_second: 0.346, interval_steps_per_second: 0.216, epoch: 27.0312[0m
[32m[2022-09-16 17:42:07,926] [    INFO][0m - loss: 0.01105455, learning_rate: 2.8906250000000004e-07, global_step: 3470, interval_runtime: 6.3664, interval_samples_per_second: 2.513, interval_steps_per_second: 1.571, epoch: 27.1094[0m
[32m[2022-09-16 17:42:14,284] [    INFO][0m - loss: 0.01165905, learning_rate: 2.8125e-07, global_step: 3480, interval_runtime: 10.6065, interval_samples_per_second: 1.509, interval_steps_per_second: 0.943, epoch: 27.1875[0m
[32m[2022-09-16 17:42:20,650] [    INFO][0m - loss: 0.00915725, learning_rate: 2.734375e-07, global_step: 3490, interval_runtime: 6.3665, interval_samples_per_second: 2.513, interval_steps_per_second: 1.571, epoch: 27.2656[0m
[32m[2022-09-16 17:42:27,030] [    INFO][0m - loss: 0.01173596, learning_rate: 2.65625e-07, global_step: 3500, interval_runtime: 6.379, interval_samples_per_second: 2.508, interval_steps_per_second: 1.568, epoch: 27.3438[0m
[32m[2022-09-16 17:42:33,420] [    INFO][0m - loss: 0.01722234, learning_rate: 2.578125e-07, global_step: 3510, interval_runtime: 6.3915, interval_samples_per_second: 2.503, interval_steps_per_second: 1.565, epoch: 27.4219[0m
[32m[2022-09-16 17:42:39,850] [    INFO][0m - loss: 0.01149129, learning_rate: 2.5e-07, global_step: 3520, interval_runtime: 6.4303, interval_samples_per_second: 2.488, interval_steps_per_second: 1.555, epoch: 27.5[0m
[32m[2022-09-16 17:42:47,454] [    INFO][0m - loss: 0.00980577, learning_rate: 2.421875e-07, global_step: 3530, interval_runtime: 6.4101, interval_samples_per_second: 2.496, interval_steps_per_second: 1.56, epoch: 27.5781[0m
[32m[2022-09-16 17:42:53,861] [    INFO][0m - loss: 0.02121641, learning_rate: 2.34375e-07, global_step: 3540, interval_runtime: 7.6001, interval_samples_per_second: 2.105, interval_steps_per_second: 1.316, epoch: 27.6562[0m
[32m[2022-09-16 17:43:00,270] [    INFO][0m - loss: 0.00968993, learning_rate: 2.265625e-07, global_step: 3550, interval_runtime: 6.4091, interval_samples_per_second: 2.496, interval_steps_per_second: 1.56, epoch: 27.7344[0m
[32m[2022-09-16 17:43:06,680] [    INFO][0m - loss: 0.00663499, learning_rate: 2.1875000000000002e-07, global_step: 3560, interval_runtime: 6.4107, interval_samples_per_second: 2.496, interval_steps_per_second: 1.56, epoch: 27.8125[0m
[32m[2022-09-16 17:43:13,082] [    INFO][0m - loss: 0.02813747, learning_rate: 2.109375e-07, global_step: 3570, interval_runtime: 6.402, interval_samples_per_second: 2.499, interval_steps_per_second: 1.562, epoch: 27.8906[0m
[32m[2022-09-16 17:43:19,449] [    INFO][0m - loss: 0.01298293, learning_rate: 2.03125e-07, global_step: 3580, interval_runtime: 6.3667, interval_samples_per_second: 2.513, interval_steps_per_second: 1.571, epoch: 27.9688[0m
[32m[2022-09-16 17:43:21,473] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 17:43:21,473] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-16 17:43:21,473] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 17:43:21,473] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 17:43:21,473] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-16 17:43:54,388] [    INFO][0m - eval_loss: 2.1535303592681885, eval_accuracy: 0.5739845261121856, eval_runtime: 32.9147, eval_samples_per_second: 62.829, eval_steps_per_second: 3.95, epoch: 28.0[0m
[32m[2022-09-16 17:43:54,411] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-3584[0m
[32m[2022-09-16 17:43:54,411] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 17:43:56,890] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-3584/tokenizer_config.json[0m
[32m[2022-09-16 17:43:56,891] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-3584/special_tokens_map.json[0m
[32m[2022-09-16 17:44:05,539] [    INFO][0m - loss: 0.01802334, learning_rate: 1.953125e-07, global_step: 3590, interval_runtime: 46.0897, interval_samples_per_second: 0.347, interval_steps_per_second: 0.217, epoch: 28.0469[0m
[32m[2022-09-16 17:44:12,613] [    INFO][0m - loss: 0.01203694, learning_rate: 1.875e-07, global_step: 3600, interval_runtime: 6.3726, interval_samples_per_second: 2.511, interval_steps_per_second: 1.569, epoch: 28.125[0m
[32m[2022-09-16 17:44:18,981] [    INFO][0m - loss: 0.00875633, learning_rate: 1.796875e-07, global_step: 3610, interval_runtime: 7.0699, interval_samples_per_second: 2.263, interval_steps_per_second: 1.414, epoch: 28.2031[0m
[32m[2022-09-16 17:44:25,350] [    INFO][0m - loss: 0.01861341, learning_rate: 1.71875e-07, global_step: 3620, interval_runtime: 6.369, interval_samples_per_second: 2.512, interval_steps_per_second: 1.57, epoch: 28.2812[0m
[32m[2022-09-16 17:44:31,735] [    INFO][0m - loss: 0.00955689, learning_rate: 1.6406250000000002e-07, global_step: 3630, interval_runtime: 6.3847, interval_samples_per_second: 2.506, interval_steps_per_second: 1.566, epoch: 28.3594[0m
[32m[2022-09-16 17:44:38,131] [    INFO][0m - loss: 0.01506085, learning_rate: 1.5625e-07, global_step: 3640, interval_runtime: 6.3961, interval_samples_per_second: 2.502, interval_steps_per_second: 1.563, epoch: 28.4375[0m
[32m[2022-09-16 17:44:44,525] [    INFO][0m - loss: 0.00953552, learning_rate: 1.484375e-07, global_step: 3650, interval_runtime: 6.3939, interval_samples_per_second: 2.502, interval_steps_per_second: 1.564, epoch: 28.5156[0m
[32m[2022-09-16 17:44:50,928] [    INFO][0m - loss: 0.00956066, learning_rate: 1.40625e-07, global_step: 3660, interval_runtime: 6.403, interval_samples_per_second: 2.499, interval_steps_per_second: 1.562, epoch: 28.5938[0m
[32m[2022-09-16 17:44:57,353] [    INFO][0m - loss: 0.0295732, learning_rate: 1.328125e-07, global_step: 3670, interval_runtime: 6.4244, interval_samples_per_second: 2.491, interval_steps_per_second: 1.557, epoch: 28.6719[0m
[32m[2022-09-16 17:45:03,760] [    INFO][0m - loss: 0.00682125, learning_rate: 1.25e-07, global_step: 3680, interval_runtime: 6.4074, interval_samples_per_second: 2.497, interval_steps_per_second: 1.561, epoch: 28.75[0m
[32m[2022-09-16 17:45:10,166] [    INFO][0m - loss: 0.01149609, learning_rate: 1.171875e-07, global_step: 3690, interval_runtime: 6.4065, interval_samples_per_second: 2.497, interval_steps_per_second: 1.561, epoch: 28.8281[0m
[32m[2022-09-16 17:45:16,552] [    INFO][0m - loss: 0.0129605, learning_rate: 1.0937500000000001e-07, global_step: 3700, interval_runtime: 6.3856, interval_samples_per_second: 2.506, interval_steps_per_second: 1.566, epoch: 28.9062[0m
[32m[2022-09-16 17:45:22,860] [    INFO][0m - loss: 0.01330881, learning_rate: 1.015625e-07, global_step: 3710, interval_runtime: 6.3081, interval_samples_per_second: 2.536, interval_steps_per_second: 1.585, epoch: 28.9844[0m
[32m[2022-09-16 17:45:23,657] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 17:45:23,658] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-16 17:45:23,658] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 17:45:23,658] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 17:45:23,658] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-16 17:45:56,775] [    INFO][0m - eval_loss: 2.1603493690490723, eval_accuracy: 0.5739845261121856, eval_runtime: 33.1167, eval_samples_per_second: 62.446, eval_steps_per_second: 3.926, epoch: 29.0[0m
[32m[2022-09-16 17:45:56,795] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-3712[0m
[32m[2022-09-16 17:45:56,795] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 17:45:59,256] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-3712/tokenizer_config.json[0m
[32m[2022-09-16 17:45:59,256] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-3712/special_tokens_map.json[0m
[32m[2022-09-16 17:46:09,164] [    INFO][0m - loss: 0.01610835, learning_rate: 9.375e-08, global_step: 3720, interval_runtime: 46.3037, interval_samples_per_second: 0.346, interval_steps_per_second: 0.216, epoch: 29.0625[0m
[32m[2022-09-16 17:46:15,540] [    INFO][0m - loss: 0.00761219, learning_rate: 8.59375e-08, global_step: 3730, interval_runtime: 6.3762, interval_samples_per_second: 2.509, interval_steps_per_second: 1.568, epoch: 29.1406[0m
[32m[2022-09-16 17:46:21,960] [    INFO][0m - loss: 0.01795008, learning_rate: 7.8125e-08, global_step: 3740, interval_runtime: 6.419, interval_samples_per_second: 2.493, interval_steps_per_second: 1.558, epoch: 29.2188[0m
[32m[2022-09-16 17:46:28,397] [    INFO][0m - loss: 0.01505026, learning_rate: 7.03125e-08, global_step: 3750, interval_runtime: 6.4377, interval_samples_per_second: 2.485, interval_steps_per_second: 1.553, epoch: 29.2969[0m
[32m[2022-09-16 17:46:36,149] [    INFO][0m - loss: 0.01568098, learning_rate: 6.25e-08, global_step: 3760, interval_runtime: 6.4108, interval_samples_per_second: 2.496, interval_steps_per_second: 1.56, epoch: 29.375[0m
[32m[2022-09-16 17:46:42,555] [    INFO][0m - loss: 0.01321022, learning_rate: 5.4687500000000006e-08, global_step: 3770, interval_runtime: 7.7474, interval_samples_per_second: 2.065, interval_steps_per_second: 1.291, epoch: 29.4531[0m
[32m[2022-09-16 17:46:49,013] [    INFO][0m - loss: 0.01159567, learning_rate: 4.6875e-08, global_step: 3780, interval_runtime: 6.4582, interval_samples_per_second: 2.477, interval_steps_per_second: 1.548, epoch: 29.5312[0m
[32m[2022-09-16 17:46:55,424] [    INFO][0m - loss: 0.00945048, learning_rate: 3.90625e-08, global_step: 3790, interval_runtime: 6.4105, interval_samples_per_second: 2.496, interval_steps_per_second: 1.56, epoch: 29.6094[0m
[32m[2022-09-16 17:47:01,833] [    INFO][0m - loss: 0.01809183, learning_rate: 3.125e-08, global_step: 3800, interval_runtime: 6.4091, interval_samples_per_second: 2.496, interval_steps_per_second: 1.56, epoch: 29.6875[0m
[32m[2022-09-16 17:47:08,239] [    INFO][0m - loss: 0.00985838, learning_rate: 2.34375e-08, global_step: 3810, interval_runtime: 6.4059, interval_samples_per_second: 2.498, interval_steps_per_second: 1.561, epoch: 29.7656[0m
[32m[2022-09-16 17:47:14,669] [    INFO][0m - loss: 0.01159214, learning_rate: 1.5625e-08, global_step: 3820, interval_runtime: 6.4299, interval_samples_per_second: 2.488, interval_steps_per_second: 1.555, epoch: 29.8438[0m
[32m[2022-09-16 17:47:21,100] [    INFO][0m - loss: 0.01055018, learning_rate: 7.8125e-09, global_step: 3830, interval_runtime: 6.4307, interval_samples_per_second: 2.488, interval_steps_per_second: 1.555, epoch: 29.9219[0m
[32m[2022-09-16 17:47:26,933] [    INFO][0m - loss: 0.01556808, learning_rate: 0.0, global_step: 3840, interval_runtime: 5.8334, interval_samples_per_second: 2.743, interval_steps_per_second: 1.714, epoch: 30.0[0m
[32m[2022-09-16 17:47:26,933] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 17:47:26,933] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-16 17:47:26,934] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 17:47:26,934] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 17:47:26,934] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-16 17:48:00,439] [    INFO][0m - eval_loss: 2.159423351287842, eval_accuracy: 0.5739845261121856, eval_runtime: 33.5044, eval_samples_per_second: 61.723, eval_steps_per_second: 3.88, epoch: 30.0[0m
[32m[2022-09-16 17:48:00,476] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-3840[0m
[32m[2022-09-16 17:48:00,477] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 17:48:03,320] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-3840/tokenizer_config.json[0m
[32m[2022-09-16 17:48:03,320] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-3840/special_tokens_map.json[0m
[32m[2022-09-16 17:48:08,267] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-16 17:48:08,268] [    INFO][0m - Loading best model from ./checkpoints_csldcp/checkpoint-1280 (score: 0.5773694390715667).[0m
[32m[2022-09-16 17:48:09,982] [    INFO][0m - train_runtime: 3684.255, train_samples_per_second: 16.579, train_steps_per_second: 1.042, train_loss: 0.4053202649268011, epoch: 30.0[0m
[32m[2022-09-16 17:48:09,984] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/[0m
[32m[2022-09-16 17:48:09,985] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 17:48:12,347] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/tokenizer_config.json[0m
[32m[2022-09-16 17:48:12,347] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/special_tokens_map.json[0m
[32m[2022-09-16 17:48:12,349] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-16 17:48:12,349] [    INFO][0m -   epoch                    =       30.0[0m
[32m[2022-09-16 17:48:12,349] [    INFO][0m -   train_loss               =     0.4053[0m
[32m[2022-09-16 17:48:12,349] [    INFO][0m -   train_runtime            = 1:01:24.25[0m
[32m[2022-09-16 17:48:12,349] [    INFO][0m -   train_samples_per_second =     16.579[0m
[32m[2022-09-16 17:48:12,349] [    INFO][0m -   train_steps_per_second   =      1.042[0m
[32m[2022-09-16 17:48:12,365] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-16 17:48:12,365] [    INFO][0m -   Num examples = 1784[0m
[32m[2022-09-16 17:48:12,365] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 17:48:12,365] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 17:48:12,365] [    INFO][0m -   Total prediction steps = 112[0m
[32m[2022-09-16 17:48:40,972] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-16 17:48:40,973] [    INFO][0m -   test_accuracy           =     0.5779[0m
[32m[2022-09-16 17:48:40,973] [    INFO][0m -   test_loss               =      1.519[0m
[32m[2022-09-16 17:48:40,973] [    INFO][0m -   test_runtime            = 0:00:28.60[0m
[32m[2022-09-16 17:48:40,973] [    INFO][0m -   test_samples_per_second =     62.362[0m
[32m[2022-09-16 17:48:40,973] [    INFO][0m -   test_steps_per_second   =      3.915[0m
[32m[2022-09-16 17:48:40,973] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-16 17:48:40,973] [    INFO][0m -   Num examples = 2999[0m
[32m[2022-09-16 17:48:40,973] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 17:48:40,974] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 17:48:40,974] [    INFO][0m -   Total prediction steps = 188[0m
[32m[2022-09-16 17:49:34,415] [    INFO][0m - Predictions for csldcp saved to ./fewclue_submit_examples.[0m
{
  "labels": 26,
  "text_a": "\u672c\u6587\u7814\u7a76\u4e86\u8d35\u5dde\u4e1c\u5357\u90e8\u7684\u5929\u67f1\u3001\u9526\u5c4f\u548c\u9ece\u5e73\u5730\u533a\u7684\u5730\u8d28\u6784\u9020\u7279\u5f81\u3001\u8be5\u533a\u91d1\u77ff\u5e8a\u7279\u5f81\u53ca\u5176\u4e0e\u5730\u8d28\u6784\u9020\u7684\u5173\u7cfb.\u8ba4\u4e3a\u533a\u5185\u5317\u90e8\u548c\u5357\u90e8\u76f8\u8ddd\u6570\u5341\u516c\u91cc\u7684\u4e24\u6761\u8fd1\u4e1c\u897f\u5411\u7684\u57fa\u5e95\u526a\u5207\u65ad\u88c2\u5e26\u6784\u6210\u4e86\u672c\u533a\u5730\u5811\u5f0f\u7684\u6784\u9020\u683c\u5c40.\u7531\u4e8e\u8fd9\u4e24\u6761\u4e1c\u897f\u5411\u526a\u5207\u65ad\u88c2\u7684\u526a\u5207\u4f5c\u7528,\u5f62\u6210\u4e86\u672c\u533a\u4ee5\u5317\u4e1c\u5411\u4e3a\u4e3b\u7684\u8936\u76b1\u548c\u526a\u5207\u5e26.\u5317\u4e1c\u5411\u8936\u76b1\u548c\u526a\u5207\u5e26\u6784\u9020\u662f\u5728\u52a0\u91cc\u4e1c\u671f\u5f62\u6210\u7684,\u540c\u65f6\u4e5f\u53d1\u751f\u4e86\u7eff\u7247\u5ca9\u76f8\u7684\u53d8\u8d28\u4f5c\u7528.\u5728\u80cc\u659c\u5f62\u6210\u7684\u540c\u65f6\u6216\u7a0d\u540e\u526a\u5207\u4f5c\u7528\u5f00\u59cb\u53d1\u751f,\u526a\u5207\u4f5c\u7528\u7ee7\u627f\u7740\u5317\u4e1c\u5411,\u5f62\u6210\u4e86\u82e5\u5e72\u6761\u4e0e\u8936\u76b1\u8f74\u5e73\u884c\u6216\u76f8\u4ea4\u7684\u526a\u5207\u5e26.\u6210\u77ff\u6d41\u4f53\u6cbf\u7740\u526a\u5207\u5e26\u4e0a\u5347,\u4e00\u65b9\u9762\u5728\u526a\u5207\u5e26\u4e2d\u6c89\u6dc0\u51fa\u7a7f\u5c42\u4ea7\u51fa\u7684\u4e0d\u6574\u5408\u7834\u788e\u5e26\u578b\u91d1\u77ff\u4f53,\u5373\u900f\u955c\u72b6\u542b\u91d1\u77f3\u82f1\u8109;\u540c\u65f6\u5927\u90e8\u5206\u5145\u586b\u5230\u7531\u80cc\u659c\u548c\u526a\u5207\u4f5c\u7528\u5f62\u6210\u7684\u5c42\u95f4\u88c2\u9699\u6216\u5c42\u95f4\u7834\u788e\u5e26\u4e2d,\u4ece\u800c\u5f62\u6210\u8d4b\u5b58\u4e8e\u6d4a\u79ef\u5ca9\u7684\u987a\u5c42\u4ea7\u51fa\u7684\u5c42\u72b6\u542b\u91d1\u77f3\u82f1\u8109\u91d1\u77ff\u5e8a.\u8fd9\u79cd\u7c7b\u578b\u7684\u542b\u91d1\u77f3\u82f1\u8109,\u65e0\u8bba\u5728\u54c1\u4f4d(\u591a\u6570\u53ef\u89c1\u660e\u91d1)\u3001\u50a8\u91cf\u548c\u4ea7\u91cf\u65b9\u9762\u5747\u5f88\u6709\u524d\u666f,\u5e76\u4e14\u6709\u7740\u5341\u5206\u91cd\u8981\u7684\u5b66\u672f\u610f\u4e49,\u56e0\u4e3a\u8fd9\u5728\u56fd\u5185\u8fd8\u662f\u9996\u6b21\u53d1\u73b0.",
  "text_b": "",
  "uid": 44
}

 
==========
tnews
==========
 
[32m[2022-09-16 17:49:57,209] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-16 17:49:57,209] [    INFO][0m - ============================================================[0m
[32m[2022-09-16 17:49:57,209] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-16 17:49:57,209] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-16 17:49:57,209] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-16 17:49:57,209] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-16 17:49:57,209] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-16 17:49:57,210] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-16 17:49:57,210] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-16 17:49:57,210] [    INFO][0m - [0m
[32m[2022-09-16 17:49:57,210] [    INFO][0m - ============================================================[0m
[32m[2022-09-16 17:49:57,210] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-16 17:49:57,210] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-16 17:49:57,210] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-16 17:49:57,210] [    INFO][0m - pretrained                    :../checkpoints_cmnli/checkpoint-22000/model_state.pdparams[0m
[32m[2022-09-16 17:49:57,210] [    INFO][0m - prompt                        :‚Äú{'text':'text_a'}‚Äù‰∏äËø∞Êñ∞ÈóªÈÄâËá™{'mask'}{'mask'}‰∏ìÊ†è„ÄÇ[0m
[32m[2022-09-16 17:49:57,210] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-16 17:49:57,210] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-16 17:49:57,210] [    INFO][0m - task_name                     :tnews[0m
[32m[2022-09-16 17:49:57,210] [    INFO][0m - [0m
[32m[2022-09-16 17:49:57,211] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0916 17:49:57.212313 21578 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0916 17:49:57.216558 21578 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-16 17:50:04,981] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-16 17:50:04,992] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-16 17:50:04,993] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-16 17:50:04,993] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚Äù‰∏äËø∞Êñ∞ÈóªÈÄâËá™'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': '‰∏ìÊ†è„ÄÇ'}][0m
[32m[2022-09-16 17:50:06,930] [    INFO][0m - ============================================================[0m
[32m[2022-09-16 17:50:06,930] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-16 17:50:06,930] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-16 17:50:06,931] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-16 17:50:06,931] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-16 17:50:06,931] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-16 17:50:06,931] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-16 17:50:06,931] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-16 17:50:06,931] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-16 17:50:06,931] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-16 17:50:06,931] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-16 17:50:06,931] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-16 17:50:06,931] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-16 17:50:06,931] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-16 17:50:06,931] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-16 17:50:06,931] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-16 17:50:06,932] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-16 17:50:06,932] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-16 17:50:06,932] [    INFO][0m - eval_batch_size               :16[0m
[32m[2022-09-16 17:50:06,932] [    INFO][0m - eval_steps                    :200[0m
[32m[2022-09-16 17:50:06,932] [    INFO][0m - evaluation_strategy           :IntervalStrategy.EPOCH[0m
[32m[2022-09-16 17:50:06,932] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-16 17:50:06,932] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-16 17:50:06,932] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-16 17:50:06,932] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-16 17:50:06,932] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-16 17:50:06,932] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-16 17:50:06,932] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-16 17:50:06,932] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-16 17:50:06,933] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-16 17:50:06,933] [    INFO][0m - learning_rate                 :3e-06[0m
[32m[2022-09-16 17:50:06,933] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-16 17:50:06,933] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-16 17:50:06,933] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-16 17:50:06,933] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-16 17:50:06,933] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-16 17:50:06,933] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-16 17:50:06,933] [    INFO][0m - logging_dir                   :./checkpoints_tnews/runs/Sep16_17-49-57_instance-3bwob41y-01[0m
[32m[2022-09-16 17:50:06,933] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-16 17:50:06,933] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-16 17:50:06,933] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-16 17:50:06,933] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-16 17:50:06,934] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-16 17:50:06,934] [    INFO][0m - max_seq_length                :64[0m
[32m[2022-09-16 17:50:06,934] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-16 17:50:06,934] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-16 17:50:06,934] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-16 17:50:06,934] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-16 17:50:06,934] [    INFO][0m - num_train_epochs              :30.0[0m
[32m[2022-09-16 17:50:06,934] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-16 17:50:06,934] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-16 17:50:06,934] [    INFO][0m - output_dir                    :./checkpoints_tnews/[0m
[32m[2022-09-16 17:50:06,934] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-16 17:50:06,934] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-16 17:50:06,934] [    INFO][0m - per_device_eval_batch_size    :16[0m
[32m[2022-09-16 17:50:06,935] [    INFO][0m - per_device_train_batch_size   :16[0m
[32m[2022-09-16 17:50:06,935] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-16 17:50:06,935] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-16 17:50:06,935] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-16 17:50:06,935] [    INFO][0m - ppt_learning_rate             :3e-05[0m
[32m[2022-09-16 17:50:06,935] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-16 17:50:06,935] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-16 17:50:06,935] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-16 17:50:06,935] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-16 17:50:06,935] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-16 17:50:06,935] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-16 17:50:06,935] [    INFO][0m - run_name                      :./checkpoints_tnews/[0m
[32m[2022-09-16 17:50:06,935] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-16 17:50:06,935] [    INFO][0m - save_steps                    :200[0m
[32m[2022-09-16 17:50:06,936] [    INFO][0m - save_strategy                 :IntervalStrategy.EPOCH[0m
[32m[2022-09-16 17:50:06,936] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-16 17:50:06,936] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-16 17:50:06,936] [    INFO][0m - seed                          :42[0m
[32m[2022-09-16 17:50:06,936] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-16 17:50:06,936] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-16 17:50:06,936] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-16 17:50:06,936] [    INFO][0m - train_batch_size              :16[0m
[32m[2022-09-16 17:50:06,936] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-16 17:50:06,936] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-16 17:50:06,936] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-16 17:50:06,936] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-16 17:50:06,936] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-16 17:50:06,937] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-16 17:50:06,937] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-16 17:50:06,937] [    INFO][0m - [0m
[32m[2022-09-16 17:50:06,939] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-16 17:50:06,939] [    INFO][0m -   Num examples = 1185[0m
[32m[2022-09-16 17:50:06,939] [    INFO][0m -   Num Epochs = 30[0m
[32m[2022-09-16 17:50:06,939] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2022-09-16 17:50:06,940] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2022-09-16 17:50:06,940] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-16 17:50:06,940] [    INFO][0m -   Total optimization steps = 2250.0[0m
[32m[2022-09-16 17:50:06,940] [    INFO][0m -   Total num train samples = 35550[0m
[32m[2022-09-16 17:50:09,992] [    INFO][0m - loss: 5.07388496, learning_rate: 2.9866666666666667e-06, global_step: 10, interval_runtime: 3.0512, interval_samples_per_second: 5.244, interval_steps_per_second: 3.277, epoch: 0.1333[0m
[32m[2022-09-16 17:50:11,855] [    INFO][0m - loss: 2.94866962, learning_rate: 2.9733333333333334e-06, global_step: 20, interval_runtime: 1.8636, interval_samples_per_second: 8.585, interval_steps_per_second: 5.366, epoch: 0.2667[0m
[32m[2022-09-16 17:50:13,722] [    INFO][0m - loss: 2.36439362, learning_rate: 2.96e-06, global_step: 30, interval_runtime: 1.8668, interval_samples_per_second: 8.571, interval_steps_per_second: 5.357, epoch: 0.4[0m
[32m[2022-09-16 17:50:15,620] [    INFO][0m - loss: 2.12766685, learning_rate: 2.9466666666666667e-06, global_step: 40, interval_runtime: 1.8976, interval_samples_per_second: 8.432, interval_steps_per_second: 5.27, epoch: 0.5333[0m
[32m[2022-09-16 17:50:17,889] [    INFO][0m - loss: 1.94801521, learning_rate: 2.9333333333333333e-06, global_step: 50, interval_runtime: 1.9519, interval_samples_per_second: 8.197, interval_steps_per_second: 5.123, epoch: 0.6667[0m
[32m[2022-09-16 17:50:19,840] [    INFO][0m - loss: 1.8560009, learning_rate: 2.9200000000000004e-06, global_step: 60, interval_runtime: 2.2679, interval_samples_per_second: 7.055, interval_steps_per_second: 4.409, epoch: 0.8[0m
[32m[2022-09-16 17:50:21,718] [    INFO][0m - loss: 1.81731586, learning_rate: 2.9066666666666666e-06, global_step: 70, interval_runtime: 1.878, interval_samples_per_second: 8.52, interval_steps_per_second: 5.325, epoch: 0.9333[0m
[32m[2022-09-16 17:50:22,552] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 17:50:22,756] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-16 17:50:22,757] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 17:50:22,757] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 17:50:22,757] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-16 17:50:28,465] [    INFO][0m - eval_loss: 1.5396788120269775, eval_accuracy: 0.546448087431694, eval_runtime: 5.9125, eval_samples_per_second: 185.71, eval_steps_per_second: 11.67, epoch: 1.0[0m
[32m[2022-09-16 17:50:31,280] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-75[0m
[32m[2022-09-16 17:50:31,281] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 17:50:34,306] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-75/tokenizer_config.json[0m
[32m[2022-09-16 17:50:34,306] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-75/special_tokens_map.json[0m
[32m[2022-09-16 17:50:40,978] [    INFO][0m - loss: 1.44323196, learning_rate: 2.8933333333333333e-06, global_step: 80, interval_runtime: 19.2602, interval_samples_per_second: 0.831, interval_steps_per_second: 0.519, epoch: 1.0667[0m
[32m[2022-09-16 17:50:42,837] [    INFO][0m - loss: 1.42139101, learning_rate: 2.88e-06, global_step: 90, interval_runtime: 1.8587, interval_samples_per_second: 8.608, interval_steps_per_second: 5.38, epoch: 1.2[0m
[32m[2022-09-16 17:50:44,725] [    INFO][0m - loss: 1.41101103, learning_rate: 2.866666666666667e-06, global_step: 100, interval_runtime: 1.888, interval_samples_per_second: 8.475, interval_steps_per_second: 5.297, epoch: 1.3333[0m
[32m[2022-09-16 17:50:46,632] [    INFO][0m - loss: 1.64661617, learning_rate: 2.8533333333333333e-06, global_step: 110, interval_runtime: 1.9071, interval_samples_per_second: 8.39, interval_steps_per_second: 5.244, epoch: 1.4667[0m
[32m[2022-09-16 17:50:48,532] [    INFO][0m - loss: 1.42951736, learning_rate: 2.84e-06, global_step: 120, interval_runtime: 1.9, interval_samples_per_second: 8.421, interval_steps_per_second: 5.263, epoch: 1.6[0m
[32m[2022-09-16 17:50:50,412] [    INFO][0m - loss: 1.35447245, learning_rate: 2.8266666666666666e-06, global_step: 130, interval_runtime: 1.8802, interval_samples_per_second: 8.51, interval_steps_per_second: 5.319, epoch: 1.7333[0m
[32m[2022-09-16 17:50:52,293] [    INFO][0m - loss: 1.36787634, learning_rate: 2.8133333333333336e-06, global_step: 140, interval_runtime: 1.8814, interval_samples_per_second: 8.504, interval_steps_per_second: 5.315, epoch: 1.8667[0m
[32m[2022-09-16 17:50:54,056] [    INFO][0m - loss: 1.17560387, learning_rate: 2.8000000000000003e-06, global_step: 150, interval_runtime: 1.763, interval_samples_per_second: 9.076, interval_steps_per_second: 5.672, epoch: 2.0[0m
[32m[2022-09-16 17:50:54,057] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 17:50:54,057] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-16 17:50:54,057] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 17:50:54,057] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 17:50:54,057] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-16 17:50:59,592] [    INFO][0m - eval_loss: 1.3770776987075806, eval_accuracy: 0.569216757741348, eval_runtime: 5.534, eval_samples_per_second: 198.411, eval_steps_per_second: 12.468, epoch: 2.0[0m
[32m[2022-09-16 17:50:59,609] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-150[0m
[32m[2022-09-16 17:50:59,609] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 17:51:02,623] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-150/tokenizer_config.json[0m
[32m[2022-09-16 17:51:02,624] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-150/special_tokens_map.json[0m
[32m[2022-09-16 17:51:09,917] [    INFO][0m - loss: 1.24671049, learning_rate: 2.7866666666666665e-06, global_step: 160, interval_runtime: 15.8607, interval_samples_per_second: 1.009, interval_steps_per_second: 0.63, epoch: 2.1333[0m
[32m[2022-09-16 17:51:15,669] [    INFO][0m - loss: 1.36014166, learning_rate: 2.773333333333333e-06, global_step: 170, interval_runtime: 1.9311, interval_samples_per_second: 8.285, interval_steps_per_second: 5.178, epoch: 2.2667[0m
[32m[2022-09-16 17:51:17,537] [    INFO][0m - loss: 1.0623251, learning_rate: 2.7600000000000003e-06, global_step: 180, interval_runtime: 5.6889, interval_samples_per_second: 2.813, interval_steps_per_second: 1.758, epoch: 2.4[0m
[32m[2022-09-16 17:51:19,395] [    INFO][0m - loss: 1.04905119, learning_rate: 2.746666666666667e-06, global_step: 190, interval_runtime: 1.8576, interval_samples_per_second: 8.613, interval_steps_per_second: 5.383, epoch: 2.5333[0m
[32m[2022-09-16 17:51:21,265] [    INFO][0m - loss: 1.19315414, learning_rate: 2.733333333333333e-06, global_step: 200, interval_runtime: 1.8702, interval_samples_per_second: 8.555, interval_steps_per_second: 5.347, epoch: 2.6667[0m
[32m[2022-09-16 17:51:23,125] [    INFO][0m - loss: 1.3734149, learning_rate: 2.72e-06, global_step: 210, interval_runtime: 1.8603, interval_samples_per_second: 8.601, interval_steps_per_second: 5.376, epoch: 2.8[0m
[32m[2022-09-16 17:51:24,980] [    INFO][0m - loss: 1.1232625, learning_rate: 2.706666666666667e-06, global_step: 220, interval_runtime: 1.8546, interval_samples_per_second: 8.627, interval_steps_per_second: 5.392, epoch: 2.9333[0m
[32m[2022-09-16 17:51:25,805] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 17:51:25,805] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-16 17:51:25,805] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 17:51:25,806] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 17:51:25,806] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-16 17:51:31,396] [    INFO][0m - eval_loss: 1.3796055316925049, eval_accuracy: 0.5655737704918032, eval_runtime: 5.5902, eval_samples_per_second: 196.414, eval_steps_per_second: 12.343, epoch: 3.0[0m
[32m[2022-09-16 17:51:31,417] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-225[0m
[32m[2022-09-16 17:51:31,417] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 17:51:34,432] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-225/tokenizer_config.json[0m
[32m[2022-09-16 17:51:34,433] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-225/special_tokens_map.json[0m
[32m[2022-09-16 17:51:41,683] [    INFO][0m - loss: 1.08078079, learning_rate: 2.6933333333333335e-06, global_step: 230, interval_runtime: 16.7035, interval_samples_per_second: 0.958, interval_steps_per_second: 0.599, epoch: 3.0667[0m
[32m[2022-09-16 17:51:43,553] [    INFO][0m - loss: 1.00049648, learning_rate: 2.68e-06, global_step: 240, interval_runtime: 1.8697, interval_samples_per_second: 8.558, interval_steps_per_second: 5.348, epoch: 3.2[0m
[32m[2022-09-16 17:51:45,417] [    INFO][0m - loss: 1.0407568, learning_rate: 2.6666666666666664e-06, global_step: 250, interval_runtime: 1.8646, interval_samples_per_second: 8.581, interval_steps_per_second: 5.363, epoch: 3.3333[0m
[32m[2022-09-16 17:51:47,282] [    INFO][0m - loss: 1.29921446, learning_rate: 2.6533333333333335e-06, global_step: 260, interval_runtime: 1.8642, interval_samples_per_second: 8.583, interval_steps_per_second: 5.364, epoch: 3.4667[0m
[32m[2022-09-16 17:51:49,149] [    INFO][0m - loss: 1.08119202, learning_rate: 2.64e-06, global_step: 270, interval_runtime: 1.8669, interval_samples_per_second: 8.571, interval_steps_per_second: 5.357, epoch: 3.6[0m
[32m[2022-09-16 17:51:51,014] [    INFO][0m - loss: 1.07754602, learning_rate: 2.6266666666666668e-06, global_step: 280, interval_runtime: 1.8654, interval_samples_per_second: 8.577, interval_steps_per_second: 5.361, epoch: 3.7333[0m
[32m[2022-09-16 17:51:52,886] [    INFO][0m - loss: 1.02373457, learning_rate: 2.6133333333333334e-06, global_step: 290, interval_runtime: 1.8719, interval_samples_per_second: 8.547, interval_steps_per_second: 5.342, epoch: 3.8667[0m
[32m[2022-09-16 17:51:54,643] [    INFO][0m - loss: 1.10615101, learning_rate: 2.6e-06, global_step: 300, interval_runtime: 1.7574, interval_samples_per_second: 9.104, interval_steps_per_second: 5.69, epoch: 4.0[0m
[32m[2022-09-16 17:51:54,644] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 17:51:54,644] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-16 17:51:54,644] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 17:51:54,644] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 17:51:54,644] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-16 17:52:00,258] [    INFO][0m - eval_loss: 1.4153544902801514, eval_accuracy: 0.5619307832422586, eval_runtime: 5.6138, eval_samples_per_second: 195.591, eval_steps_per_second: 12.291, epoch: 4.0[0m
[32m[2022-09-16 17:52:00,285] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-300[0m
[32m[2022-09-16 17:52:00,285] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 17:52:03,487] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-300/tokenizer_config.json[0m
[32m[2022-09-16 17:52:03,487] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-300/special_tokens_map.json[0m
[32m[2022-09-16 17:52:11,106] [    INFO][0m - loss: 1.00920696, learning_rate: 2.5866666666666667e-06, global_step: 310, interval_runtime: 16.4623, interval_samples_per_second: 0.972, interval_steps_per_second: 0.607, epoch: 4.1333[0m
[32m[2022-09-16 17:52:12,976] [    INFO][0m - loss: 0.96071205, learning_rate: 2.5733333333333334e-06, global_step: 320, interval_runtime: 1.8705, interval_samples_per_second: 8.554, interval_steps_per_second: 5.346, epoch: 4.2667[0m
[32m[2022-09-16 17:52:14,847] [    INFO][0m - loss: 0.98261471, learning_rate: 2.56e-06, global_step: 330, interval_runtime: 1.8704, interval_samples_per_second: 8.554, interval_steps_per_second: 5.346, epoch: 4.4[0m
[32m[2022-09-16 17:52:16,714] [    INFO][0m - loss: 0.99696999, learning_rate: 2.5466666666666667e-06, global_step: 340, interval_runtime: 1.8675, interval_samples_per_second: 8.568, interval_steps_per_second: 5.355, epoch: 4.5333[0m
[32m[2022-09-16 17:52:18,580] [    INFO][0m - loss: 1.12974176, learning_rate: 2.5333333333333334e-06, global_step: 350, interval_runtime: 1.8655, interval_samples_per_second: 8.577, interval_steps_per_second: 5.361, epoch: 4.6667[0m
[32m[2022-09-16 17:52:20,446] [    INFO][0m - loss: 0.94502735, learning_rate: 2.52e-06, global_step: 360, interval_runtime: 1.8661, interval_samples_per_second: 8.574, interval_steps_per_second: 5.359, epoch: 4.8[0m
[32m[2022-09-16 17:52:22,306] [    INFO][0m - loss: 0.87863865, learning_rate: 2.506666666666667e-06, global_step: 370, interval_runtime: 1.8597, interval_samples_per_second: 8.603, interval_steps_per_second: 5.377, epoch: 4.9333[0m
[32m[2022-09-16 17:52:23,138] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 17:52:23,138] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-16 17:52:23,138] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 17:52:23,138] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 17:52:23,138] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-16 17:52:28,720] [    INFO][0m - eval_loss: 1.4282015562057495, eval_accuracy: 0.569216757741348, eval_runtime: 5.5815, eval_samples_per_second: 196.721, eval_steps_per_second: 12.362, epoch: 5.0[0m
[32m[2022-09-16 17:52:28,740] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-375[0m
[32m[2022-09-16 17:52:28,741] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 17:52:31,649] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-375/tokenizer_config.json[0m
[32m[2022-09-16 17:52:31,650] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-375/special_tokens_map.json[0m
[32m[2022-09-16 17:52:38,530] [    INFO][0m - loss: 0.72865877, learning_rate: 2.4933333333333333e-06, global_step: 380, interval_runtime: 16.2246, interval_samples_per_second: 0.986, interval_steps_per_second: 0.616, epoch: 5.0667[0m
[32m[2022-09-16 17:52:40,400] [    INFO][0m - loss: 0.81331892, learning_rate: 2.48e-06, global_step: 390, interval_runtime: 1.8702, interval_samples_per_second: 8.555, interval_steps_per_second: 5.347, epoch: 5.2[0m
[32m[2022-09-16 17:52:42,272] [    INFO][0m - loss: 1.12482052, learning_rate: 2.4666666666666666e-06, global_step: 400, interval_runtime: 1.8718, interval_samples_per_second: 8.548, interval_steps_per_second: 5.343, epoch: 5.3333[0m
[32m[2022-09-16 17:52:44,143] [    INFO][0m - loss: 0.93351421, learning_rate: 2.4533333333333337e-06, global_step: 410, interval_runtime: 1.8709, interval_samples_per_second: 8.552, interval_steps_per_second: 5.345, epoch: 5.4667[0m
[32m[2022-09-16 17:52:46,013] [    INFO][0m - loss: 0.74276161, learning_rate: 2.44e-06, global_step: 420, interval_runtime: 1.8704, interval_samples_per_second: 8.554, interval_steps_per_second: 5.346, epoch: 5.6[0m
[32m[2022-09-16 17:52:47,887] [    INFO][0m - loss: 0.86964016, learning_rate: 2.4266666666666666e-06, global_step: 430, interval_runtime: 1.8735, interval_samples_per_second: 8.54, interval_steps_per_second: 5.338, epoch: 5.7333[0m
[32m[2022-09-16 17:52:49,748] [    INFO][0m - loss: 0.86437712, learning_rate: 2.4133333333333332e-06, global_step: 440, interval_runtime: 1.8614, interval_samples_per_second: 8.596, interval_steps_per_second: 5.372, epoch: 5.8667[0m
[32m[2022-09-16 17:52:51,508] [    INFO][0m - loss: 0.83719568, learning_rate: 2.4000000000000003e-06, global_step: 450, interval_runtime: 1.7596, interval_samples_per_second: 9.093, interval_steps_per_second: 5.683, epoch: 6.0[0m
[32m[2022-09-16 17:52:51,509] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 17:52:51,509] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-16 17:52:51,509] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 17:52:51,509] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 17:52:51,509] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-16 17:52:57,139] [    INFO][0m - eval_loss: 1.469539999961853, eval_accuracy: 0.563752276867031, eval_runtime: 5.6297, eval_samples_per_second: 195.038, eval_steps_per_second: 12.256, epoch: 6.0[0m
[32m[2022-09-16 17:52:57,158] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-450[0m
[32m[2022-09-16 17:52:57,158] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 17:53:00,386] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-450/tokenizer_config.json[0m
[32m[2022-09-16 17:53:00,387] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-450/special_tokens_map.json[0m
[32m[2022-09-16 17:53:09,560] [    INFO][0m - loss: 0.86260967, learning_rate: 2.386666666666667e-06, global_step: 460, interval_runtime: 17.5267, interval_samples_per_second: 0.913, interval_steps_per_second: 0.571, epoch: 6.1333[0m
[32m[2022-09-16 17:53:11,478] [    INFO][0m - loss: 0.83043499, learning_rate: 2.373333333333333e-06, global_step: 470, interval_runtime: 2.4438, interval_samples_per_second: 6.547, interval_steps_per_second: 4.092, epoch: 6.2667[0m
[32m[2022-09-16 17:53:13,378] [    INFO][0m - loss: 0.84973793, learning_rate: 2.36e-06, global_step: 480, interval_runtime: 1.8991, interval_samples_per_second: 8.425, interval_steps_per_second: 5.266, epoch: 6.4[0m
[32m[2022-09-16 17:53:15,252] [    INFO][0m - loss: 0.75178313, learning_rate: 2.346666666666667e-06, global_step: 490, interval_runtime: 1.8742, interval_samples_per_second: 8.537, interval_steps_per_second: 5.336, epoch: 6.5333[0m
[32m[2022-09-16 17:53:17,133] [    INFO][0m - loss: 0.56803999, learning_rate: 2.3333333333333336e-06, global_step: 500, interval_runtime: 1.8808, interval_samples_per_second: 8.507, interval_steps_per_second: 5.317, epoch: 6.6667[0m
[32m[2022-09-16 17:53:19,003] [    INFO][0m - loss: 0.672188, learning_rate: 2.32e-06, global_step: 510, interval_runtime: 1.87, interval_samples_per_second: 8.556, interval_steps_per_second: 5.348, epoch: 6.8[0m
[32m[2022-09-16 17:53:20,865] [    INFO][0m - loss: 0.88422298, learning_rate: 2.3066666666666665e-06, global_step: 520, interval_runtime: 1.8624, interval_samples_per_second: 8.591, interval_steps_per_second: 5.37, epoch: 6.9333[0m
[32m[2022-09-16 17:53:21,691] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 17:53:21,691] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-16 17:53:21,691] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 17:53:21,692] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 17:53:21,692] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-16 17:53:27,399] [    INFO][0m - eval_loss: 1.588273286819458, eval_accuracy: 0.5655737704918032, eval_runtime: 5.7067, eval_samples_per_second: 192.406, eval_steps_per_second: 12.091, epoch: 7.0[0m
[32m[2022-09-16 17:53:27,421] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-525[0m
[32m[2022-09-16 17:53:27,422] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 17:53:30,562] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-525/tokenizer_config.json[0m
[32m[2022-09-16 17:53:30,563] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-525/special_tokens_map.json[0m
[32m[2022-09-16 17:53:36,827] [    INFO][0m - loss: 0.65193977, learning_rate: 2.2933333333333335e-06, global_step: 530, interval_runtime: 15.9617, interval_samples_per_second: 1.002, interval_steps_per_second: 0.626, epoch: 7.0667[0m
[32m[2022-09-16 17:53:38,698] [    INFO][0m - loss: 0.64212012, learning_rate: 2.28e-06, global_step: 540, interval_runtime: 1.871, interval_samples_per_second: 8.552, interval_steps_per_second: 5.345, epoch: 7.2[0m
[32m[2022-09-16 17:53:40,564] [    INFO][0m - loss: 0.67094202, learning_rate: 2.266666666666667e-06, global_step: 550, interval_runtime: 1.8661, interval_samples_per_second: 8.574, interval_steps_per_second: 5.359, epoch: 7.3333[0m
[32m[2022-09-16 17:53:42,431] [    INFO][0m - loss: 0.74357548, learning_rate: 2.253333333333333e-06, global_step: 560, interval_runtime: 1.8673, interval_samples_per_second: 8.569, interval_steps_per_second: 5.355, epoch: 7.4667[0m
[32m[2022-09-16 17:53:44,290] [    INFO][0m - loss: 0.68009005, learning_rate: 2.24e-06, global_step: 570, interval_runtime: 1.859, interval_samples_per_second: 8.607, interval_steps_per_second: 5.379, epoch: 7.6[0m
[32m[2022-09-16 17:53:46,158] [    INFO][0m - loss: 0.71107688, learning_rate: 2.226666666666667e-06, global_step: 580, interval_runtime: 1.8675, interval_samples_per_second: 8.568, interval_steps_per_second: 5.355, epoch: 7.7333[0m
[32m[2022-09-16 17:53:48,019] [    INFO][0m - loss: 0.64572339, learning_rate: 2.2133333333333335e-06, global_step: 590, interval_runtime: 1.8613, interval_samples_per_second: 8.596, interval_steps_per_second: 5.373, epoch: 7.8667[0m
[32m[2022-09-16 17:53:49,777] [    INFO][0m - loss: 0.58999281, learning_rate: 2.1999999999999997e-06, global_step: 600, interval_runtime: 1.7579, interval_samples_per_second: 9.102, interval_steps_per_second: 5.689, epoch: 8.0[0m
[32m[2022-09-16 17:53:49,777] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 17:53:49,778] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-16 17:53:49,778] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 17:53:49,778] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 17:53:49,778] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-16 17:53:55,307] [    INFO][0m - eval_loss: 1.694287896156311, eval_accuracy: 0.5510018214936248, eval_runtime: 5.529, eval_samples_per_second: 198.588, eval_steps_per_second: 12.48, epoch: 8.0[0m
[32m[2022-09-16 17:53:55,327] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-600[0m
[32m[2022-09-16 17:53:55,327] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 17:53:58,131] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-600/tokenizer_config.json[0m
[32m[2022-09-16 17:53:58,131] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-600/special_tokens_map.json[0m
[32m[2022-09-16 17:54:06,944] [    INFO][0m - loss: 0.49212842, learning_rate: 2.1866666666666668e-06, global_step: 610, interval_runtime: 17.1667, interval_samples_per_second: 0.932, interval_steps_per_second: 0.583, epoch: 8.1333[0m
[32m[2022-09-16 17:54:08,918] [    INFO][0m - loss: 0.64202914, learning_rate: 2.1733333333333334e-06, global_step: 620, interval_runtime: 1.9748, interval_samples_per_second: 8.102, interval_steps_per_second: 5.064, epoch: 8.2667[0m
[32m[2022-09-16 17:54:11,882] [    INFO][0m - loss: 0.60984907, learning_rate: 2.16e-06, global_step: 630, interval_runtime: 1.972, interval_samples_per_second: 8.114, interval_steps_per_second: 5.071, epoch: 8.4[0m
[32m[2022-09-16 17:54:13,777] [    INFO][0m - loss: 0.57950554, learning_rate: 2.1466666666666667e-06, global_step: 640, interval_runtime: 2.8866, interval_samples_per_second: 5.543, interval_steps_per_second: 3.464, epoch: 8.5333[0m
[32m[2022-09-16 17:54:15,657] [    INFO][0m - loss: 0.64555035, learning_rate: 2.1333333333333334e-06, global_step: 650, interval_runtime: 1.88, interval_samples_per_second: 8.51, interval_steps_per_second: 5.319, epoch: 8.6667[0m
[32m[2022-09-16 17:54:17,530] [    INFO][0m - loss: 0.58532944, learning_rate: 2.12e-06, global_step: 660, interval_runtime: 1.8732, interval_samples_per_second: 8.542, interval_steps_per_second: 5.339, epoch: 8.8[0m
[32m[2022-09-16 17:54:19,438] [    INFO][0m - loss: 0.59167957, learning_rate: 2.1066666666666667e-06, global_step: 670, interval_runtime: 1.9077, interval_samples_per_second: 8.387, interval_steps_per_second: 5.242, epoch: 8.9333[0m
[32m[2022-09-16 17:54:20,267] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 17:54:20,267] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-16 17:54:20,267] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 17:54:20,267] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 17:54:20,267] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-16 17:54:25,711] [    INFO][0m - eval_loss: 1.726608157157898, eval_accuracy: 0.5573770491803278, eval_runtime: 5.4443, eval_samples_per_second: 201.678, eval_steps_per_second: 12.674, epoch: 9.0[0m
[32m[2022-09-16 17:54:25,724] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-675[0m
[32m[2022-09-16 17:54:25,724] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 17:54:28,271] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-675/tokenizer_config.json[0m
[32m[2022-09-16 17:54:28,271] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-675/special_tokens_map.json[0m
[32m[2022-09-16 17:54:34,090] [    INFO][0m - loss: 0.47374005, learning_rate: 2.0933333333333333e-06, global_step: 680, interval_runtime: 14.6519, interval_samples_per_second: 1.092, interval_steps_per_second: 0.683, epoch: 9.0667[0m
[32m[2022-09-16 17:54:35,942] [    INFO][0m - loss: 0.48714485, learning_rate: 2.08e-06, global_step: 690, interval_runtime: 1.8519, interval_samples_per_second: 8.64, interval_steps_per_second: 5.4, epoch: 9.2[0m
[32m[2022-09-16 17:54:37,796] [    INFO][0m - loss: 0.47845607, learning_rate: 2.0666666666666666e-06, global_step: 700, interval_runtime: 1.8543, interval_samples_per_second: 8.628, interval_steps_per_second: 5.393, epoch: 9.3333[0m
[32m[2022-09-16 17:54:39,654] [    INFO][0m - loss: 0.5233284, learning_rate: 2.0533333333333333e-06, global_step: 710, interval_runtime: 1.8585, interval_samples_per_second: 8.609, interval_steps_per_second: 5.381, epoch: 9.4667[0m
[32m[2022-09-16 17:54:41,517] [    INFO][0m - loss: 0.54807806, learning_rate: 2.0400000000000004e-06, global_step: 720, interval_runtime: 1.8621, interval_samples_per_second: 8.592, interval_steps_per_second: 5.37, epoch: 9.6[0m
[32m[2022-09-16 17:54:43,381] [    INFO][0m - loss: 0.41457019, learning_rate: 2.0266666666666666e-06, global_step: 730, interval_runtime: 1.8647, interval_samples_per_second: 8.581, interval_steps_per_second: 5.363, epoch: 9.7333[0m
[32m[2022-09-16 17:54:45,253] [    INFO][0m - loss: 0.44687743, learning_rate: 2.0133333333333333e-06, global_step: 740, interval_runtime: 1.8713, interval_samples_per_second: 8.55, interval_steps_per_second: 5.344, epoch: 9.8667[0m
[32m[2022-09-16 17:54:47,017] [    INFO][0m - loss: 0.57389364, learning_rate: 2e-06, global_step: 750, interval_runtime: 1.7647, interval_samples_per_second: 9.067, interval_steps_per_second: 5.667, epoch: 10.0[0m
[32m[2022-09-16 17:54:47,018] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 17:54:47,018] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-16 17:54:47,018] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 17:54:47,018] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 17:54:47,018] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-16 17:54:52,539] [    INFO][0m - eval_loss: 1.900054693222046, eval_accuracy: 0.5601092896174863, eval_runtime: 5.5201, eval_samples_per_second: 198.91, eval_steps_per_second: 12.5, epoch: 10.0[0m
[32m[2022-09-16 17:54:52,557] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-750[0m
[32m[2022-09-16 17:54:52,557] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 17:54:55,346] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-750/tokenizer_config.json[0m
[32m[2022-09-16 17:54:55,346] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-750/special_tokens_map.json[0m
[32m[2022-09-16 17:55:02,199] [    INFO][0m - loss: 0.36756277, learning_rate: 1.986666666666667e-06, global_step: 760, interval_runtime: 15.1814, interval_samples_per_second: 1.054, interval_steps_per_second: 0.659, epoch: 10.1333[0m
[32m[2022-09-16 17:55:04,058] [    INFO][0m - loss: 0.28790693, learning_rate: 1.9733333333333336e-06, global_step: 770, interval_runtime: 1.8591, interval_samples_per_second: 8.606, interval_steps_per_second: 5.379, epoch: 10.2667[0m
[32m[2022-09-16 17:55:05,922] [    INFO][0m - loss: 0.36345944, learning_rate: 1.96e-06, global_step: 780, interval_runtime: 1.8642, interval_samples_per_second: 8.583, interval_steps_per_second: 5.364, epoch: 10.4[0m
[32m[2022-09-16 17:55:07,784] [    INFO][0m - loss: 0.41144218, learning_rate: 1.9466666666666665e-06, global_step: 790, interval_runtime: 1.8619, interval_samples_per_second: 8.593, interval_steps_per_second: 5.371, epoch: 10.5333[0m
[32m[2022-09-16 17:55:09,644] [    INFO][0m - loss: 0.46694322, learning_rate: 1.9333333333333336e-06, global_step: 800, interval_runtime: 1.8605, interval_samples_per_second: 8.6, interval_steps_per_second: 5.375, epoch: 10.6667[0m
[32m[2022-09-16 17:55:11,509] [    INFO][0m - loss: 0.37584651, learning_rate: 1.9200000000000003e-06, global_step: 810, interval_runtime: 1.8646, interval_samples_per_second: 8.581, interval_steps_per_second: 5.363, epoch: 10.8[0m
[32m[2022-09-16 17:55:13,380] [    INFO][0m - loss: 0.4251833, learning_rate: 1.9066666666666667e-06, global_step: 820, interval_runtime: 1.871, interval_samples_per_second: 8.552, interval_steps_per_second: 5.345, epoch: 10.9333[0m
[32m[2022-09-16 17:55:14,210] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 17:55:14,211] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-16 17:55:14,211] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 17:55:14,211] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 17:55:14,211] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-16 17:55:23,070] [    INFO][0m - eval_loss: 1.972801923751831, eval_accuracy: 0.5619307832422586, eval_runtime: 5.5668, eval_samples_per_second: 197.239, eval_steps_per_second: 12.395, epoch: 11.0[0m
[32m[2022-09-16 17:55:23,091] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-825[0m
[32m[2022-09-16 17:55:23,091] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 17:55:26,163] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-825/tokenizer_config.json[0m
[32m[2022-09-16 17:55:26,163] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-825/special_tokens_map.json[0m
[32m[2022-09-16 17:55:32,704] [    INFO][0m - loss: 0.36586118, learning_rate: 1.8933333333333333e-06, global_step: 830, interval_runtime: 19.324, interval_samples_per_second: 0.828, interval_steps_per_second: 0.517, epoch: 11.0667[0m
[32m[2022-09-16 17:55:34,615] [    INFO][0m - loss: 0.34051294, learning_rate: 1.8800000000000002e-06, global_step: 840, interval_runtime: 1.9105, interval_samples_per_second: 8.375, interval_steps_per_second: 5.234, epoch: 11.2[0m
[32m[2022-09-16 17:55:36,472] [    INFO][0m - loss: 0.33136041, learning_rate: 1.8666666666666667e-06, global_step: 850, interval_runtime: 1.8573, interval_samples_per_second: 8.615, interval_steps_per_second: 5.384, epoch: 11.3333[0m
[32m[2022-09-16 17:55:38,327] [    INFO][0m - loss: 0.30278957, learning_rate: 1.8533333333333333e-06, global_step: 860, interval_runtime: 1.8557, interval_samples_per_second: 8.622, interval_steps_per_second: 5.389, epoch: 11.4667[0m
[32m[2022-09-16 17:55:40,189] [    INFO][0m - loss: 0.3066294, learning_rate: 1.84e-06, global_step: 870, interval_runtime: 1.862, interval_samples_per_second: 8.593, interval_steps_per_second: 5.371, epoch: 11.6[0m
[32m[2022-09-16 17:55:42,054] [    INFO][0m - loss: 0.34612379, learning_rate: 1.8266666666666668e-06, global_step: 880, interval_runtime: 1.8644, interval_samples_per_second: 8.582, interval_steps_per_second: 5.364, epoch: 11.7333[0m
[32m[2022-09-16 17:55:43,932] [    INFO][0m - loss: 0.30058575, learning_rate: 1.8133333333333335e-06, global_step: 890, interval_runtime: 1.8777, interval_samples_per_second: 8.521, interval_steps_per_second: 5.326, epoch: 11.8667[0m
[32m[2022-09-16 17:55:45,696] [    INFO][0m - loss: 0.31600435, learning_rate: 1.8e-06, global_step: 900, interval_runtime: 1.7649, interval_samples_per_second: 9.066, interval_steps_per_second: 5.666, epoch: 12.0[0m
[32m[2022-09-16 17:55:45,697] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 17:55:45,697] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-16 17:55:45,697] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 17:55:45,697] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 17:55:45,698] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-16 17:55:51,151] [    INFO][0m - eval_loss: 2.2102303504943848, eval_accuracy: 0.5482695810564663, eval_runtime: 5.4538, eval_samples_per_second: 201.329, eval_steps_per_second: 12.652, epoch: 12.0[0m
[32m[2022-09-16 17:55:51,165] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-900[0m
[32m[2022-09-16 17:55:51,165] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 17:55:53,673] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-900/tokenizer_config.json[0m
[32m[2022-09-16 17:55:53,673] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-900/special_tokens_map.json[0m
[32m[2022-09-16 17:56:05,474] [    INFO][0m - loss: 0.35774775, learning_rate: 1.7866666666666666e-06, global_step: 910, interval_runtime: 19.7771, interval_samples_per_second: 0.809, interval_steps_per_second: 0.506, epoch: 12.1333[0m
[32m[2022-09-16 17:56:07,330] [    INFO][0m - loss: 0.22145412, learning_rate: 1.7733333333333334e-06, global_step: 920, interval_runtime: 1.8563, interval_samples_per_second: 8.619, interval_steps_per_second: 5.387, epoch: 12.2667[0m
[32m[2022-09-16 17:56:09,183] [    INFO][0m - loss: 0.26873083, learning_rate: 1.76e-06, global_step: 930, interval_runtime: 1.8535, interval_samples_per_second: 8.632, interval_steps_per_second: 5.395, epoch: 12.4[0m
[32m[2022-09-16 17:56:11,040] [    INFO][0m - loss: 0.25797791, learning_rate: 1.7466666666666665e-06, global_step: 940, interval_runtime: 1.8564, interval_samples_per_second: 8.619, interval_steps_per_second: 5.387, epoch: 12.5333[0m
[32m[2022-09-16 17:56:12,893] [    INFO][0m - loss: 0.24819701, learning_rate: 1.7333333333333332e-06, global_step: 950, interval_runtime: 1.8536, interval_samples_per_second: 8.632, interval_steps_per_second: 5.395, epoch: 12.6667[0m
[32m[2022-09-16 17:56:14,754] [    INFO][0m - loss: 0.29641552, learning_rate: 1.72e-06, global_step: 960, interval_runtime: 1.8606, interval_samples_per_second: 8.6, interval_steps_per_second: 5.375, epoch: 12.8[0m
[32m[2022-09-16 17:56:16,618] [    INFO][0m - loss: 0.25981889, learning_rate: 1.7066666666666667e-06, global_step: 970, interval_runtime: 1.8634, interval_samples_per_second: 8.586, interval_steps_per_second: 5.367, epoch: 12.9333[0m
[32m[2022-09-16 17:56:17,448] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 17:56:17,448] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-16 17:56:17,448] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 17:56:17,448] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 17:56:17,448] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-16 17:56:23,132] [    INFO][0m - eval_loss: 2.251242160797119, eval_accuracy: 0.5519125683060109, eval_runtime: 5.6834, eval_samples_per_second: 193.194, eval_steps_per_second: 12.141, epoch: 13.0[0m
[32m[2022-09-16 17:56:23,152] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-975[0m
[32m[2022-09-16 17:56:23,152] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 17:56:25,928] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-975/tokenizer_config.json[0m
[32m[2022-09-16 17:56:25,928] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-975/special_tokens_map.json[0m
[32m[2022-09-16 17:56:37,099] [    INFO][0m - loss: 0.20365231, learning_rate: 1.6933333333333334e-06, global_step: 980, interval_runtime: 20.4815, interval_samples_per_second: 0.781, interval_steps_per_second: 0.488, epoch: 13.0667[0m
[32m[2022-09-16 17:56:38,949] [    INFO][0m - loss: 0.15689899, learning_rate: 1.6800000000000002e-06, global_step: 990, interval_runtime: 1.8505, interval_samples_per_second: 8.646, interval_steps_per_second: 5.404, epoch: 13.2[0m
[32m[2022-09-16 17:56:40,806] [    INFO][0m - loss: 0.24407666, learning_rate: 1.6666666666666669e-06, global_step: 1000, interval_runtime: 1.8559, interval_samples_per_second: 8.621, interval_steps_per_second: 5.388, epoch: 13.3333[0m
[32m[2022-09-16 17:56:42,666] [    INFO][0m - loss: 0.17068161, learning_rate: 1.6533333333333333e-06, global_step: 1010, interval_runtime: 1.8605, interval_samples_per_second: 8.6, interval_steps_per_second: 5.375, epoch: 13.4667[0m
[32m[2022-09-16 17:56:44,529] [    INFO][0m - loss: 0.27432144, learning_rate: 1.64e-06, global_step: 1020, interval_runtime: 1.8627, interval_samples_per_second: 8.59, interval_steps_per_second: 5.369, epoch: 13.6[0m
[32m[2022-09-16 17:56:46,386] [    INFO][0m - loss: 0.17303352, learning_rate: 1.6266666666666668e-06, global_step: 1030, interval_runtime: 1.8572, interval_samples_per_second: 8.615, interval_steps_per_second: 5.385, epoch: 13.7333[0m
[32m[2022-09-16 17:56:48,251] [    INFO][0m - loss: 0.21343901, learning_rate: 1.6133333333333335e-06, global_step: 1040, interval_runtime: 1.8655, interval_samples_per_second: 8.577, interval_steps_per_second: 5.361, epoch: 13.8667[0m
[32m[2022-09-16 17:56:50,005] [    INFO][0m - loss: 0.21501853, learning_rate: 1.6e-06, global_step: 1050, interval_runtime: 1.7535, interval_samples_per_second: 9.125, interval_steps_per_second: 5.703, epoch: 14.0[0m
[32m[2022-09-16 17:56:50,005] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 17:56:50,006] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-16 17:56:50,006] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 17:56:50,006] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 17:56:50,006] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-16 17:56:55,515] [    INFO][0m - eval_loss: 2.3591673374176025, eval_accuracy: 0.5482695810564663, eval_runtime: 5.5084, eval_samples_per_second: 199.331, eval_steps_per_second: 12.526, epoch: 14.0[0m
[32m[2022-09-16 17:56:55,529] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-1050[0m
[32m[2022-09-16 17:56:55,530] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 17:56:58,225] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-1050/tokenizer_config.json[0m
[32m[2022-09-16 17:56:58,225] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-1050/special_tokens_map.json[0m
[32m[2022-09-16 17:57:05,120] [    INFO][0m - loss: 0.19444258, learning_rate: 1.5866666666666666e-06, global_step: 1060, interval_runtime: 15.1157, interval_samples_per_second: 1.059, interval_steps_per_second: 0.662, epoch: 14.1333[0m
[32m[2022-09-16 17:57:07,004] [    INFO][0m - loss: 0.186562, learning_rate: 1.5733333333333334e-06, global_step: 1070, interval_runtime: 1.883, interval_samples_per_second: 8.497, interval_steps_per_second: 5.311, epoch: 14.2667[0m
[32m[2022-09-16 17:57:08,874] [    INFO][0m - loss: 0.1490579, learning_rate: 1.56e-06, global_step: 1080, interval_runtime: 1.8703, interval_samples_per_second: 8.555, interval_steps_per_second: 5.347, epoch: 14.4[0m
[32m[2022-09-16 17:57:10,737] [    INFO][0m - loss: 0.23402185, learning_rate: 1.5466666666666668e-06, global_step: 1090, interval_runtime: 1.8636, interval_samples_per_second: 8.585, interval_steps_per_second: 5.366, epoch: 14.5333[0m
[32m[2022-09-16 17:57:12,605] [    INFO][0m - loss: 0.12679551, learning_rate: 1.5333333333333332e-06, global_step: 1100, interval_runtime: 1.8674, interval_samples_per_second: 8.568, interval_steps_per_second: 5.355, epoch: 14.6667[0m
[32m[2022-09-16 17:57:14,470] [    INFO][0m - loss: 0.1496907, learning_rate: 1.5200000000000003e-06, global_step: 1110, interval_runtime: 1.8651, interval_samples_per_second: 8.579, interval_steps_per_second: 5.362, epoch: 14.8[0m
[32m[2022-09-16 17:57:16,327] [    INFO][0m - loss: 0.16556658, learning_rate: 1.5066666666666667e-06, global_step: 1120, interval_runtime: 1.8572, interval_samples_per_second: 8.615, interval_steps_per_second: 5.384, epoch: 14.9333[0m
[32m[2022-09-16 17:57:17,155] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 17:57:17,155] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-16 17:57:17,155] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 17:57:17,155] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 17:57:17,155] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-16 17:57:22,771] [    INFO][0m - eval_loss: 2.461268424987793, eval_accuracy: 0.5482695810564663, eval_runtime: 5.6155, eval_samples_per_second: 195.532, eval_steps_per_second: 12.288, epoch: 15.0[0m
[32m[2022-09-16 17:57:22,787] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-1125[0m
[32m[2022-09-16 17:57:22,787] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 17:57:25,382] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-1125/tokenizer_config.json[0m
[32m[2022-09-16 17:57:25,382] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-1125/special_tokens_map.json[0m
[32m[2022-09-16 17:57:31,436] [    INFO][0m - loss: 0.13460078, learning_rate: 1.4933333333333334e-06, global_step: 1130, interval_runtime: 15.109, interval_samples_per_second: 1.059, interval_steps_per_second: 0.662, epoch: 15.0667[0m
[32m[2022-09-16 17:57:33,290] [    INFO][0m - loss: 0.12000296, learning_rate: 1.48e-06, global_step: 1140, interval_runtime: 1.8534, interval_samples_per_second: 8.633, interval_steps_per_second: 5.396, epoch: 15.2[0m
[32m[2022-09-16 17:57:35,146] [    INFO][0m - loss: 0.11690469, learning_rate: 1.4666666666666667e-06, global_step: 1150, interval_runtime: 1.8565, interval_samples_per_second: 8.618, interval_steps_per_second: 5.386, epoch: 15.3333[0m
[32m[2022-09-16 17:57:37,016] [    INFO][0m - loss: 0.07766246, learning_rate: 1.4533333333333333e-06, global_step: 1160, interval_runtime: 1.87, interval_samples_per_second: 8.556, interval_steps_per_second: 5.348, epoch: 15.4667[0m
[32m[2022-09-16 17:57:38,879] [    INFO][0m - loss: 0.13550109, learning_rate: 1.44e-06, global_step: 1170, interval_runtime: 1.863, interval_samples_per_second: 8.588, interval_steps_per_second: 5.368, epoch: 15.6[0m
[32m[2022-09-16 17:57:41,854] [    INFO][0m - loss: 0.17214136, learning_rate: 1.4266666666666666e-06, global_step: 1180, interval_runtime: 1.8632, interval_samples_per_second: 8.587, interval_steps_per_second: 5.367, epoch: 15.7333[0m
[32m[2022-09-16 17:57:43,716] [    INFO][0m - loss: 0.19437295, learning_rate: 1.4133333333333333e-06, global_step: 1190, interval_runtime: 2.9736, interval_samples_per_second: 5.381, interval_steps_per_second: 3.363, epoch: 15.8667[0m
[32m[2022-09-16 17:57:45,471] [    INFO][0m - loss: 0.13817314, learning_rate: 1.4000000000000001e-06, global_step: 1200, interval_runtime: 1.7553, interval_samples_per_second: 9.115, interval_steps_per_second: 5.697, epoch: 16.0[0m
[32m[2022-09-16 17:57:45,472] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 17:57:45,473] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-16 17:57:45,473] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 17:57:45,473] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 17:57:45,473] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-16 17:57:50,920] [    INFO][0m - eval_loss: 2.5627031326293945, eval_accuracy: 0.5428051001821493, eval_runtime: 5.4469, eval_samples_per_second: 201.581, eval_steps_per_second: 12.668, epoch: 16.0[0m
[32m[2022-09-16 17:57:50,934] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-1200[0m
[32m[2022-09-16 17:57:50,934] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 17:57:53,649] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-1200/tokenizer_config.json[0m
[32m[2022-09-16 17:57:53,650] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-1200/special_tokens_map.json[0m
[32m[2022-09-16 17:58:00,735] [    INFO][0m - loss: 0.06630152, learning_rate: 1.3866666666666666e-06, global_step: 1210, interval_runtime: 15.2629, interval_samples_per_second: 1.048, interval_steps_per_second: 0.655, epoch: 16.1333[0m
[32m[2022-09-16 17:58:02,602] [    INFO][0m - loss: 0.11355599, learning_rate: 1.3733333333333335e-06, global_step: 1220, interval_runtime: 1.8682, interval_samples_per_second: 8.564, interval_steps_per_second: 5.353, epoch: 16.2667[0m
[32m[2022-09-16 17:58:04,460] [    INFO][0m - loss: 0.14759016, learning_rate: 1.36e-06, global_step: 1230, interval_runtime: 1.8579, interval_samples_per_second: 8.612, interval_steps_per_second: 5.382, epoch: 16.4[0m
[32m[2022-09-16 17:58:08,296] [    INFO][0m - loss: 0.08985379, learning_rate: 1.3466666666666668e-06, global_step: 1240, interval_runtime: 1.8609, interval_samples_per_second: 8.598, interval_steps_per_second: 5.374, epoch: 16.5333[0m
[32m[2022-09-16 17:58:10,154] [    INFO][0m - loss: 0.09762847, learning_rate: 1.3333333333333332e-06, global_step: 1250, interval_runtime: 3.8333, interval_samples_per_second: 4.174, interval_steps_per_second: 2.609, epoch: 16.6667[0m
[32m[2022-09-16 17:58:12,015] [    INFO][0m - loss: 0.11216686, learning_rate: 1.32e-06, global_step: 1260, interval_runtime: 1.8606, interval_samples_per_second: 8.599, interval_steps_per_second: 5.374, epoch: 16.8[0m
[32m[2022-09-16 17:58:13,871] [    INFO][0m - loss: 0.07931616, learning_rate: 1.3066666666666667e-06, global_step: 1270, interval_runtime: 1.8559, interval_samples_per_second: 8.621, interval_steps_per_second: 5.388, epoch: 16.9333[0m
[32m[2022-09-16 17:58:14,697] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 17:58:14,697] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-16 17:58:14,697] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 17:58:14,697] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 17:58:14,697] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-16 17:58:20,183] [    INFO][0m - eval_loss: 2.658623456954956, eval_accuracy: 0.5473588342440802, eval_runtime: 5.4856, eval_samples_per_second: 200.159, eval_steps_per_second: 12.578, epoch: 17.0[0m
[32m[2022-09-16 17:58:20,201] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-1275[0m
[32m[2022-09-16 17:58:20,201] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 17:58:22,713] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-1275/tokenizer_config.json[0m
[32m[2022-09-16 17:58:22,822] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-1275/special_tokens_map.json[0m
[32m[2022-09-16 17:58:29,360] [    INFO][0m - loss: 0.14036647, learning_rate: 1.2933333333333334e-06, global_step: 1280, interval_runtime: 15.4884, interval_samples_per_second: 1.033, interval_steps_per_second: 0.646, epoch: 17.0667[0m
[32m[2022-09-16 17:58:31,221] [    INFO][0m - loss: 0.09024258, learning_rate: 1.28e-06, global_step: 1290, interval_runtime: 1.8616, interval_samples_per_second: 8.595, interval_steps_per_second: 5.372, epoch: 17.2[0m
[32m[2022-09-16 17:58:33,088] [    INFO][0m - loss: 0.09121373, learning_rate: 1.2666666666666667e-06, global_step: 1300, interval_runtime: 1.8664, interval_samples_per_second: 8.573, interval_steps_per_second: 5.358, epoch: 17.3333[0m
[32m[2022-09-16 17:58:34,957] [    INFO][0m - loss: 0.09239483, learning_rate: 1.2533333333333335e-06, global_step: 1310, interval_runtime: 1.8696, interval_samples_per_second: 8.558, interval_steps_per_second: 5.349, epoch: 17.4667[0m
[32m[2022-09-16 17:58:36,820] [    INFO][0m - loss: 0.08444362, learning_rate: 1.24e-06, global_step: 1320, interval_runtime: 1.8623, interval_samples_per_second: 8.592, interval_steps_per_second: 5.37, epoch: 17.6[0m
[32m[2022-09-16 17:58:38,676] [    INFO][0m - loss: 0.09664719, learning_rate: 1.2266666666666669e-06, global_step: 1330, interval_runtime: 1.8565, interval_samples_per_second: 8.618, interval_steps_per_second: 5.386, epoch: 17.7333[0m
[32m[2022-09-16 17:58:44,064] [    INFO][0m - loss: 0.11120138, learning_rate: 1.2133333333333333e-06, global_step: 1340, interval_runtime: 1.8655, interval_samples_per_second: 8.577, interval_steps_per_second: 5.361, epoch: 17.8667[0m
[32m[2022-09-16 17:58:45,831] [    INFO][0m - loss: 0.05561321, learning_rate: 1.2000000000000002e-06, global_step: 1350, interval_runtime: 5.2898, interval_samples_per_second: 3.025, interval_steps_per_second: 1.89, epoch: 18.0[0m
[32m[2022-09-16 17:58:45,832] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 17:58:45,832] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-16 17:58:45,832] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 17:58:45,832] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 17:58:45,832] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-16 17:58:51,301] [    INFO][0m - eval_loss: 2.7403833866119385, eval_accuracy: 0.5455373406193078, eval_runtime: 5.4686, eval_samples_per_second: 200.781, eval_steps_per_second: 12.617, epoch: 18.0[0m
[32m[2022-09-16 17:58:51,318] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-1350[0m
[32m[2022-09-16 17:58:51,318] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 17:58:53,853] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-1350/tokenizer_config.json[0m
[32m[2022-09-16 17:58:53,854] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-1350/special_tokens_map.json[0m
[32m[2022-09-16 17:59:00,544] [    INFO][0m - loss: 0.06868405, learning_rate: 1.1866666666666666e-06, global_step: 1360, interval_runtime: 14.7131, interval_samples_per_second: 1.087, interval_steps_per_second: 0.68, epoch: 18.1333[0m
[32m[2022-09-16 17:59:02,409] [    INFO][0m - loss: 0.06198574, learning_rate: 1.1733333333333335e-06, global_step: 1370, interval_runtime: 1.8645, interval_samples_per_second: 8.581, interval_steps_per_second: 5.363, epoch: 18.2667[0m
[32m[2022-09-16 17:59:04,273] [    INFO][0m - loss: 0.05237341, learning_rate: 1.16e-06, global_step: 1380, interval_runtime: 1.8642, interval_samples_per_second: 8.583, interval_steps_per_second: 5.364, epoch: 18.4[0m
[32m[2022-09-16 17:59:06,121] [    INFO][0m - loss: 0.06927273, learning_rate: 1.1466666666666668e-06, global_step: 1390, interval_runtime: 1.848, interval_samples_per_second: 8.658, interval_steps_per_second: 5.411, epoch: 18.5333[0m
[32m[2022-09-16 17:59:07,969] [    INFO][0m - loss: 0.07314332, learning_rate: 1.1333333333333334e-06, global_step: 1400, interval_runtime: 1.8477, interval_samples_per_second: 8.66, interval_steps_per_second: 5.412, epoch: 18.6667[0m
[32m[2022-09-16 17:59:09,823] [    INFO][0m - loss: 0.0526674, learning_rate: 1.12e-06, global_step: 1410, interval_runtime: 1.8542, interval_samples_per_second: 8.629, interval_steps_per_second: 5.393, epoch: 18.8[0m
[32m[2022-09-16 17:59:11,669] [    INFO][0m - loss: 0.08769571, learning_rate: 1.1066666666666667e-06, global_step: 1420, interval_runtime: 1.8464, interval_samples_per_second: 8.666, interval_steps_per_second: 5.416, epoch: 18.9333[0m
[32m[2022-09-16 17:59:12,495] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 17:59:12,496] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-16 17:59:12,496] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 17:59:12,496] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 17:59:12,496] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-16 17:59:17,979] [    INFO][0m - eval_loss: 2.8530290126800537, eval_accuracy: 0.546448087431694, eval_runtime: 5.4824, eval_samples_per_second: 200.276, eval_steps_per_second: 12.586, epoch: 19.0[0m
[32m[2022-09-16 17:59:17,998] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-1425[0m
[32m[2022-09-16 17:59:17,999] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 17:59:20,787] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-1425/tokenizer_config.json[0m
[32m[2022-09-16 17:59:20,788] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-1425/special_tokens_map.json[0m
[32m[2022-09-16 17:59:31,175] [    INFO][0m - loss: 0.03883839, learning_rate: 1.0933333333333334e-06, global_step: 1430, interval_runtime: 19.5061, interval_samples_per_second: 0.82, interval_steps_per_second: 0.513, epoch: 19.0667[0m
[32m[2022-09-16 17:59:33,031] [    INFO][0m - loss: 0.08604432, learning_rate: 1.08e-06, global_step: 1440, interval_runtime: 1.8557, interval_samples_per_second: 8.622, interval_steps_per_second: 5.389, epoch: 19.2[0m
[32m[2022-09-16 17:59:34,881] [    INFO][0m - loss: 0.08422982, learning_rate: 1.0666666666666667e-06, global_step: 1450, interval_runtime: 1.85, interval_samples_per_second: 8.649, interval_steps_per_second: 5.405, epoch: 19.3333[0m
[32m[2022-09-16 17:59:36,726] [    INFO][0m - loss: 0.04885768, learning_rate: 1.0533333333333333e-06, global_step: 1460, interval_runtime: 1.8447, interval_samples_per_second: 8.673, interval_steps_per_second: 5.421, epoch: 19.4667[0m
[32m[2022-09-16 17:59:38,581] [    INFO][0m - loss: 0.04481412, learning_rate: 1.04e-06, global_step: 1470, interval_runtime: 1.8546, interval_samples_per_second: 8.627, interval_steps_per_second: 5.392, epoch: 19.6[0m
[32m[2022-09-16 17:59:40,433] [    INFO][0m - loss: 0.07337679, learning_rate: 1.0266666666666666e-06, global_step: 1480, interval_runtime: 1.8519, interval_samples_per_second: 8.64, interval_steps_per_second: 5.4, epoch: 19.7333[0m
[32m[2022-09-16 17:59:42,291] [    INFO][0m - loss: 0.05669377, learning_rate: 1.0133333333333333e-06, global_step: 1490, interval_runtime: 1.8584, interval_samples_per_second: 8.61, interval_steps_per_second: 5.381, epoch: 19.8667[0m
[32m[2022-09-16 17:59:44,045] [    INFO][0m - loss: 0.06486073, learning_rate: 1e-06, global_step: 1500, interval_runtime: 1.7539, interval_samples_per_second: 9.122, interval_steps_per_second: 5.701, epoch: 20.0[0m
[32m[2022-09-16 17:59:44,046] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 17:59:44,046] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-16 17:59:44,046] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 17:59:44,046] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 17:59:44,046] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-16 17:59:49,555] [    INFO][0m - eval_loss: 2.938276529312134, eval_accuracy: 0.5473588342440802, eval_runtime: 5.5085, eval_samples_per_second: 199.33, eval_steps_per_second: 12.526, epoch: 20.0[0m
[32m[2022-09-16 17:59:49,573] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-1500[0m
[32m[2022-09-16 17:59:49,574] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 17:59:52,136] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-1500/tokenizer_config.json[0m
[32m[2022-09-16 17:59:52,137] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-1500/special_tokens_map.json[0m
[32m[2022-09-16 18:00:00,912] [    INFO][0m - loss: 0.08149309, learning_rate: 9.866666666666668e-07, global_step: 1510, interval_runtime: 16.8676, interval_samples_per_second: 0.949, interval_steps_per_second: 0.593, epoch: 20.1333[0m
[32m[2022-09-16 18:00:02,771] [    INFO][0m - loss: 0.05278373, learning_rate: 9.733333333333333e-07, global_step: 1520, interval_runtime: 1.8581, interval_samples_per_second: 8.611, interval_steps_per_second: 5.382, epoch: 20.2667[0m
[32m[2022-09-16 18:00:04,627] [    INFO][0m - loss: 0.08782851, learning_rate: 9.600000000000001e-07, global_step: 1530, interval_runtime: 1.8562, interval_samples_per_second: 8.62, interval_steps_per_second: 5.387, epoch: 20.4[0m
[32m[2022-09-16 18:00:06,474] [    INFO][0m - loss: 0.05752038, learning_rate: 9.466666666666667e-07, global_step: 1540, interval_runtime: 1.8473, interval_samples_per_second: 8.661, interval_steps_per_second: 5.413, epoch: 20.5333[0m
[32m[2022-09-16 18:00:08,334] [    INFO][0m - loss: 0.04221731, learning_rate: 9.333333333333333e-07, global_step: 1550, interval_runtime: 1.8602, interval_samples_per_second: 8.601, interval_steps_per_second: 5.376, epoch: 20.6667[0m
[32m[2022-09-16 18:00:10,183] [    INFO][0m - loss: 0.05657629, learning_rate: 9.2e-07, global_step: 1560, interval_runtime: 1.8485, interval_samples_per_second: 8.656, interval_steps_per_second: 5.41, epoch: 20.8[0m
[32m[2022-09-16 18:00:12,026] [    INFO][0m - loss: 0.03352951, learning_rate: 9.066666666666667e-07, global_step: 1570, interval_runtime: 1.8436, interval_samples_per_second: 8.679, interval_steps_per_second: 5.424, epoch: 20.9333[0m
[32m[2022-09-16 18:00:12,850] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 18:00:12,850] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-16 18:00:12,850] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 18:00:12,850] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 18:00:12,850] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-16 18:00:18,378] [    INFO][0m - eval_loss: 2.9836928844451904, eval_accuracy: 0.5482695810564663, eval_runtime: 5.5273, eval_samples_per_second: 198.65, eval_steps_per_second: 12.483, epoch: 21.0[0m
[32m[2022-09-16 18:00:18,396] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-1575[0m
[32m[2022-09-16 18:00:18,396] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 18:00:21,090] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-1575/tokenizer_config.json[0m
[32m[2022-09-16 18:00:21,090] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-1575/special_tokens_map.json[0m
[32m[2022-09-16 18:00:29,320] [    INFO][0m - loss: 0.04200142, learning_rate: 8.933333333333333e-07, global_step: 1580, interval_runtime: 17.2932, interval_samples_per_second: 0.925, interval_steps_per_second: 0.578, epoch: 21.0667[0m
[32m[2022-09-16 18:00:31,172] [    INFO][0m - loss: 0.05467571, learning_rate: 8.8e-07, global_step: 1590, interval_runtime: 1.8521, interval_samples_per_second: 8.639, interval_steps_per_second: 5.399, epoch: 21.2[0m
[32m[2022-09-16 18:00:33,026] [    INFO][0m - loss: 0.02639702, learning_rate: 8.666666666666666e-07, global_step: 1600, interval_runtime: 1.8543, interval_samples_per_second: 8.628, interval_steps_per_second: 5.393, epoch: 21.3333[0m
[32m[2022-09-16 18:00:34,925] [    INFO][0m - loss: 0.04487354, learning_rate: 8.533333333333334e-07, global_step: 1610, interval_runtime: 1.8988, interval_samples_per_second: 8.426, interval_steps_per_second: 5.266, epoch: 21.4667[0m
[32m[2022-09-16 18:00:36,783] [    INFO][0m - loss: 0.03674454, learning_rate: 8.400000000000001e-07, global_step: 1620, interval_runtime: 1.8575, interval_samples_per_second: 8.614, interval_steps_per_second: 5.384, epoch: 21.6[0m
[32m[2022-09-16 18:00:38,651] [    INFO][0m - loss: 0.02567792, learning_rate: 8.266666666666667e-07, global_step: 1630, interval_runtime: 1.8682, interval_samples_per_second: 8.564, interval_steps_per_second: 5.353, epoch: 21.7333[0m
[32m[2022-09-16 18:00:40,512] [    INFO][0m - loss: 0.05145885, learning_rate: 8.133333333333334e-07, global_step: 1640, interval_runtime: 1.861, interval_samples_per_second: 8.597, interval_steps_per_second: 5.373, epoch: 21.8667[0m
[32m[2022-09-16 18:00:42,268] [    INFO][0m - loss: 0.04119589, learning_rate: 8e-07, global_step: 1650, interval_runtime: 1.7565, interval_samples_per_second: 9.109, interval_steps_per_second: 5.693, epoch: 22.0[0m
[32m[2022-09-16 18:00:42,269] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 18:00:42,269] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-16 18:00:42,269] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 18:00:42,269] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 18:00:42,269] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-16 18:00:48,043] [    INFO][0m - eval_loss: 2.999335289001465, eval_accuracy: 0.5482695810564663, eval_runtime: 5.7736, eval_samples_per_second: 190.178, eval_steps_per_second: 11.951, epoch: 22.0[0m
[32m[2022-09-16 18:00:48,059] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-1650[0m
[32m[2022-09-16 18:00:48,059] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 18:00:51,031] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-1650/tokenizer_config.json[0m
[32m[2022-09-16 18:00:51,031] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-1650/special_tokens_map.json[0m
[32m[2022-09-16 18:01:00,362] [    INFO][0m - loss: 0.03821677, learning_rate: 7.866666666666667e-07, global_step: 1660, interval_runtime: 18.094, interval_samples_per_second: 0.884, interval_steps_per_second: 0.553, epoch: 22.1333[0m
[32m[2022-09-16 18:01:02,237] [    INFO][0m - loss: 0.0375946, learning_rate: 7.733333333333334e-07, global_step: 1670, interval_runtime: 1.8746, interval_samples_per_second: 8.535, interval_steps_per_second: 5.335, epoch: 22.2667[0m
[32m[2022-09-16 18:01:04,106] [    INFO][0m - loss: 0.03869747, learning_rate: 7.600000000000001e-07, global_step: 1680, interval_runtime: 1.8692, interval_samples_per_second: 8.56, interval_steps_per_second: 5.35, epoch: 22.4[0m
[32m[2022-09-16 18:01:05,966] [    INFO][0m - loss: 0.0390892, learning_rate: 7.466666666666667e-07, global_step: 1690, interval_runtime: 1.8596, interval_samples_per_second: 8.604, interval_steps_per_second: 5.378, epoch: 22.5333[0m
[32m[2022-09-16 18:01:07,828] [    INFO][0m - loss: 0.03579821, learning_rate: 7.333333333333333e-07, global_step: 1700, interval_runtime: 1.8625, interval_samples_per_second: 8.591, interval_steps_per_second: 5.369, epoch: 22.6667[0m
[32m[2022-09-16 18:01:09,706] [    INFO][0m - loss: 0.04331819, learning_rate: 7.2e-07, global_step: 1710, interval_runtime: 1.8774, interval_samples_per_second: 8.522, interval_steps_per_second: 5.326, epoch: 22.8[0m
[32m[2022-09-16 18:01:11,571] [    INFO][0m - loss: 0.03000297, learning_rate: 7.066666666666666e-07, global_step: 1720, interval_runtime: 1.8654, interval_samples_per_second: 8.577, interval_steps_per_second: 5.361, epoch: 22.9333[0m
[32m[2022-09-16 18:01:12,397] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 18:01:12,397] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-16 18:01:12,397] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 18:01:12,397] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 18:01:12,397] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-16 18:01:17,947] [    INFO][0m - eval_loss: 3.083725929260254, eval_accuracy: 0.546448087431694, eval_runtime: 5.5489, eval_samples_per_second: 197.878, eval_steps_per_second: 12.435, epoch: 23.0[0m
[32m[2022-09-16 18:01:17,968] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-1725[0m
[32m[2022-09-16 18:01:17,968] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 18:01:20,973] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-1725/tokenizer_config.json[0m
[32m[2022-09-16 18:01:20,973] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-1725/special_tokens_map.json[0m
[32m[2022-09-16 18:01:31,125] [    INFO][0m - loss: 0.03173226, learning_rate: 6.933333333333333e-07, global_step: 1730, interval_runtime: 19.5539, interval_samples_per_second: 0.818, interval_steps_per_second: 0.511, epoch: 23.0667[0m
[32m[2022-09-16 18:01:32,985] [    INFO][0m - loss: 0.0470754, learning_rate: 6.8e-07, global_step: 1740, interval_runtime: 1.8603, interval_samples_per_second: 8.601, interval_steps_per_second: 5.375, epoch: 23.2[0m
[32m[2022-09-16 18:01:34,843] [    INFO][0m - loss: 0.02168653, learning_rate: 6.666666666666666e-07, global_step: 1750, interval_runtime: 1.8579, interval_samples_per_second: 8.612, interval_steps_per_second: 5.382, epoch: 23.3333[0m
[32m[2022-09-16 18:01:36,696] [    INFO][0m - loss: 0.03414589, learning_rate: 6.533333333333334e-07, global_step: 1760, interval_runtime: 1.8528, interval_samples_per_second: 8.636, interval_steps_per_second: 5.397, epoch: 23.4667[0m
[32m[2022-09-16 18:01:38,542] [    INFO][0m - loss: 0.04352163, learning_rate: 6.4e-07, global_step: 1770, interval_runtime: 1.8463, interval_samples_per_second: 8.666, interval_steps_per_second: 5.416, epoch: 23.6[0m
[32m[2022-09-16 18:01:40,395] [    INFO][0m - loss: 0.04117078, learning_rate: 6.266666666666668e-07, global_step: 1780, interval_runtime: 1.8531, interval_samples_per_second: 8.634, interval_steps_per_second: 5.396, epoch: 23.7333[0m
[32m[2022-09-16 18:01:42,253] [    INFO][0m - loss: 0.10701091, learning_rate: 6.133333333333334e-07, global_step: 1790, interval_runtime: 1.8582, interval_samples_per_second: 8.61, interval_steps_per_second: 5.381, epoch: 23.8667[0m
[32m[2022-09-16 18:01:44,018] [    INFO][0m - loss: 0.04092084, learning_rate: 6.000000000000001e-07, global_step: 1800, interval_runtime: 1.7645, interval_samples_per_second: 9.067, interval_steps_per_second: 5.667, epoch: 24.0[0m
[32m[2022-09-16 18:01:44,019] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 18:01:44,019] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-16 18:01:44,019] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 18:01:44,019] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 18:01:44,020] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-16 18:01:49,591] [    INFO][0m - eval_loss: 3.0929923057556152, eval_accuracy: 0.5455373406193078, eval_runtime: 5.5704, eval_samples_per_second: 197.113, eval_steps_per_second: 12.387, epoch: 24.0[0m
[32m[2022-09-16 18:01:49,612] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-1800[0m
[32m[2022-09-16 18:01:49,612] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 18:01:52,473] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-1800/tokenizer_config.json[0m
[32m[2022-09-16 18:01:52,473] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-1800/special_tokens_map.json[0m
[32m[2022-09-16 18:02:00,666] [    INFO][0m - loss: 0.03210285, learning_rate: 5.866666666666667e-07, global_step: 1810, interval_runtime: 16.6475, interval_samples_per_second: 0.961, interval_steps_per_second: 0.601, epoch: 24.1333[0m
[32m[2022-09-16 18:02:02,533] [    INFO][0m - loss: 0.02249511, learning_rate: 5.733333333333334e-07, global_step: 1820, interval_runtime: 1.867, interval_samples_per_second: 8.57, interval_steps_per_second: 5.356, epoch: 24.2667[0m
[32m[2022-09-16 18:02:04,398] [    INFO][0m - loss: 0.03800118, learning_rate: 5.6e-07, global_step: 1830, interval_runtime: 1.8646, interval_samples_per_second: 8.581, interval_steps_per_second: 5.363, epoch: 24.4[0m
[32m[2022-09-16 18:02:06,256] [    INFO][0m - loss: 0.01917706, learning_rate: 5.466666666666667e-07, global_step: 1840, interval_runtime: 1.8583, interval_samples_per_second: 8.61, interval_steps_per_second: 5.381, epoch: 24.5333[0m
[32m[2022-09-16 18:02:08,150] [    INFO][0m - loss: 0.01934754, learning_rate: 5.333333333333333e-07, global_step: 1850, interval_runtime: 1.8945, interval_samples_per_second: 8.445, interval_steps_per_second: 5.278, epoch: 24.6667[0m
[32m[2022-09-16 18:02:10,050] [    INFO][0m - loss: 0.02539769, learning_rate: 5.2e-07, global_step: 1860, interval_runtime: 1.8998, interval_samples_per_second: 8.422, interval_steps_per_second: 5.264, epoch: 24.8[0m
[32m[2022-09-16 18:02:11,923] [    INFO][0m - loss: 0.03517972, learning_rate: 5.066666666666667e-07, global_step: 1870, interval_runtime: 1.8729, interval_samples_per_second: 8.543, interval_steps_per_second: 5.339, epoch: 24.9333[0m
[32m[2022-09-16 18:02:12,749] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 18:02:12,749] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-16 18:02:12,750] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 18:02:12,750] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 18:02:12,750] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-16 18:02:18,306] [    INFO][0m - eval_loss: 3.166106700897217, eval_accuracy: 0.5391621129326047, eval_runtime: 5.5559, eval_samples_per_second: 197.627, eval_steps_per_second: 12.419, epoch: 25.0[0m
[32m[2022-09-16 18:02:18,330] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-1875[0m
[32m[2022-09-16 18:02:18,330] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 18:02:21,430] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-1875/tokenizer_config.json[0m
[32m[2022-09-16 18:02:21,430] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-1875/special_tokens_map.json[0m
[32m[2022-09-16 18:02:28,502] [    INFO][0m - loss: 0.01374469, learning_rate: 4.933333333333334e-07, global_step: 1880, interval_runtime: 16.5791, interval_samples_per_second: 0.965, interval_steps_per_second: 0.603, epoch: 25.0667[0m
[32m[2022-09-16 18:02:30,354] [    INFO][0m - loss: 0.02401489, learning_rate: 4.800000000000001e-07, global_step: 1890, interval_runtime: 1.8516, interval_samples_per_second: 8.641, interval_steps_per_second: 5.401, epoch: 25.2[0m
[32m[2022-09-16 18:02:32,198] [    INFO][0m - loss: 0.02701077, learning_rate: 4.6666666666666666e-07, global_step: 1900, interval_runtime: 1.8439, interval_samples_per_second: 8.677, interval_steps_per_second: 5.423, epoch: 25.3333[0m
[32m[2022-09-16 18:02:34,053] [    INFO][0m - loss: 0.04254969, learning_rate: 4.5333333333333337e-07, global_step: 1910, interval_runtime: 1.8555, interval_samples_per_second: 8.623, interval_steps_per_second: 5.389, epoch: 25.4667[0m
[32m[2022-09-16 18:02:35,888] [    INFO][0m - loss: 0.04179544, learning_rate: 4.4e-07, global_step: 1920, interval_runtime: 1.8347, interval_samples_per_second: 8.721, interval_steps_per_second: 5.45, epoch: 25.6[0m
[32m[2022-09-16 18:02:37,729] [    INFO][0m - loss: 0.01997456, learning_rate: 4.266666666666667e-07, global_step: 1930, interval_runtime: 1.8418, interval_samples_per_second: 8.687, interval_steps_per_second: 5.43, epoch: 25.7333[0m
[32m[2022-09-16 18:02:39,560] [    INFO][0m - loss: 0.02432903, learning_rate: 4.1333333333333333e-07, global_step: 1940, interval_runtime: 1.83, interval_samples_per_second: 8.743, interval_steps_per_second: 5.464, epoch: 25.8667[0m
[32m[2022-09-16 18:02:41,309] [    INFO][0m - loss: 0.02197395, learning_rate: 4e-07, global_step: 1950, interval_runtime: 1.7498, interval_samples_per_second: 9.144, interval_steps_per_second: 5.715, epoch: 26.0[0m
[32m[2022-09-16 18:02:41,310] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 18:02:41,310] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-16 18:02:41,310] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 18:02:41,310] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 18:02:41,310] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-16 18:02:46,772] [    INFO][0m - eval_loss: 3.1717000007629395, eval_accuracy: 0.5437158469945356, eval_runtime: 5.4607, eval_samples_per_second: 201.073, eval_steps_per_second: 12.636, epoch: 26.0[0m
[32m[2022-09-16 18:02:46,791] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-1950[0m
[32m[2022-09-16 18:02:46,791] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 18:02:49,562] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-1950/tokenizer_config.json[0m
[32m[2022-09-16 18:02:49,562] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-1950/special_tokens_map.json[0m
[32m[2022-09-16 18:03:00,814] [    INFO][0m - loss: 0.03440477, learning_rate: 3.866666666666667e-07, global_step: 1960, interval_runtime: 19.5045, interval_samples_per_second: 0.82, interval_steps_per_second: 0.513, epoch: 26.1333[0m
[32m[2022-09-16 18:03:02,653] [    INFO][0m - loss: 0.01319905, learning_rate: 3.7333333333333334e-07, global_step: 1970, interval_runtime: 1.8397, interval_samples_per_second: 8.697, interval_steps_per_second: 5.436, epoch: 26.2667[0m
[32m[2022-09-16 18:03:04,498] [    INFO][0m - loss: 0.02347607, learning_rate: 3.6e-07, global_step: 1980, interval_runtime: 1.8444, interval_samples_per_second: 8.675, interval_steps_per_second: 5.422, epoch: 26.4[0m
[32m[2022-09-16 18:03:06,327] [    INFO][0m - loss: 0.02303141, learning_rate: 3.4666666666666665e-07, global_step: 1990, interval_runtime: 1.8287, interval_samples_per_second: 8.749, interval_steps_per_second: 5.468, epoch: 26.5333[0m
[32m[2022-09-16 18:03:08,155] [    INFO][0m - loss: 0.01918101, learning_rate: 3.333333333333333e-07, global_step: 2000, interval_runtime: 1.8284, interval_samples_per_second: 8.751, interval_steps_per_second: 5.469, epoch: 26.6667[0m
[32m[2022-09-16 18:03:09,989] [    INFO][0m - loss: 0.05349349, learning_rate: 3.2e-07, global_step: 2010, interval_runtime: 1.8341, interval_samples_per_second: 8.723, interval_steps_per_second: 5.452, epoch: 26.8[0m
[32m[2022-09-16 18:03:11,825] [    INFO][0m - loss: 0.03194406, learning_rate: 3.066666666666667e-07, global_step: 2020, interval_runtime: 1.8363, interval_samples_per_second: 8.713, interval_steps_per_second: 5.446, epoch: 26.9333[0m
[32m[2022-09-16 18:03:12,644] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 18:03:12,644] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-16 18:03:12,644] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 18:03:12,644] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 18:03:12,644] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-16 18:03:18,107] [    INFO][0m - eval_loss: 3.197185516357422, eval_accuracy: 0.546448087431694, eval_runtime: 5.4624, eval_samples_per_second: 201.011, eval_steps_per_second: 12.632, epoch: 27.0[0m
[32m[2022-09-16 18:03:18,128] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-2025[0m
[32m[2022-09-16 18:03:18,129] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 18:03:21,015] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-2025/tokenizer_config.json[0m
[32m[2022-09-16 18:03:21,015] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-2025/special_tokens_map.json[0m
[32m[2022-09-16 18:03:28,430] [    INFO][0m - loss: 0.02835875, learning_rate: 2.9333333333333337e-07, global_step: 2030, interval_runtime: 16.605, interval_samples_per_second: 0.964, interval_steps_per_second: 0.602, epoch: 27.0667[0m
[32m[2022-09-16 18:03:30,295] [    INFO][0m - loss: 0.01396756, learning_rate: 2.8e-07, global_step: 2040, interval_runtime: 1.8647, interval_samples_per_second: 8.581, interval_steps_per_second: 5.363, epoch: 27.2[0m
[32m[2022-09-16 18:03:32,228] [    INFO][0m - loss: 0.01887581, learning_rate: 2.6666666666666667e-07, global_step: 2050, interval_runtime: 1.9331, interval_samples_per_second: 8.277, interval_steps_per_second: 5.173, epoch: 27.3333[0m
[32m[2022-09-16 18:03:34,111] [    INFO][0m - loss: 0.02322245, learning_rate: 2.533333333333333e-07, global_step: 2060, interval_runtime: 1.883, interval_samples_per_second: 8.497, interval_steps_per_second: 5.311, epoch: 27.4667[0m
[32m[2022-09-16 18:03:35,995] [    INFO][0m - loss: 0.02133273, learning_rate: 2.4000000000000003e-07, global_step: 2070, interval_runtime: 1.8833, interval_samples_per_second: 8.496, interval_steps_per_second: 5.31, epoch: 27.6[0m
[32m[2022-09-16 18:03:37,873] [    INFO][0m - loss: 0.03158128, learning_rate: 2.2666666666666668e-07, global_step: 2080, interval_runtime: 1.8788, interval_samples_per_second: 8.516, interval_steps_per_second: 5.322, epoch: 27.7333[0m
[32m[2022-09-16 18:03:39,725] [    INFO][0m - loss: 0.01591132, learning_rate: 2.1333333333333334e-07, global_step: 2090, interval_runtime: 1.8515, interval_samples_per_second: 8.642, interval_steps_per_second: 5.401, epoch: 27.8667[0m
[32m[2022-09-16 18:03:41,465] [    INFO][0m - loss: 0.03289881, learning_rate: 2e-07, global_step: 2100, interval_runtime: 1.74, interval_samples_per_second: 9.195, interval_steps_per_second: 5.747, epoch: 28.0[0m
[32m[2022-09-16 18:03:41,466] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 18:03:41,466] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-16 18:03:41,466] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 18:03:41,466] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 18:03:41,466] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-16 18:03:46,951] [    INFO][0m - eval_loss: 3.219681978225708, eval_accuracy: 0.5510018214936248, eval_runtime: 5.485, eval_samples_per_second: 200.183, eval_steps_per_second: 12.58, epoch: 28.0[0m
[32m[2022-09-16 18:03:46,970] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-2100[0m
[32m[2022-09-16 18:03:46,970] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 18:03:49,676] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-2100/tokenizer_config.json[0m
[32m[2022-09-16 18:03:49,677] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-2100/special_tokens_map.json[0m
[32m[2022-09-16 18:03:57,566] [    INFO][0m - loss: 0.03570521, learning_rate: 1.8666666666666667e-07, global_step: 2110, interval_runtime: 16.1016, interval_samples_per_second: 0.994, interval_steps_per_second: 0.621, epoch: 28.1333[0m
[32m[2022-09-16 18:03:59,402] [    INFO][0m - loss: 0.03410428, learning_rate: 1.7333333333333332e-07, global_step: 2120, interval_runtime: 1.8357, interval_samples_per_second: 8.716, interval_steps_per_second: 5.448, epoch: 28.2667[0m
[32m[2022-09-16 18:04:01,242] [    INFO][0m - loss: 0.02508326, learning_rate: 1.6e-07, global_step: 2130, interval_runtime: 1.8394, interval_samples_per_second: 8.699, interval_steps_per_second: 5.437, epoch: 28.4[0m
[32m[2022-09-16 18:04:03,093] [    INFO][0m - loss: 0.01336277, learning_rate: 1.4666666666666668e-07, global_step: 2140, interval_runtime: 1.851, interval_samples_per_second: 8.644, interval_steps_per_second: 5.402, epoch: 28.5333[0m
[32m[2022-09-16 18:04:04,936] [    INFO][0m - loss: 0.01111124, learning_rate: 1.3333333333333334e-07, global_step: 2150, interval_runtime: 1.8438, interval_samples_per_second: 8.678, interval_steps_per_second: 5.424, epoch: 28.6667[0m
[32m[2022-09-16 18:04:06,766] [    INFO][0m - loss: 0.02040582, learning_rate: 1.2000000000000002e-07, global_step: 2160, interval_runtime: 1.8292, interval_samples_per_second: 8.747, interval_steps_per_second: 5.467, epoch: 28.8[0m
[32m[2022-09-16 18:04:08,598] [    INFO][0m - loss: 0.02173826, learning_rate: 1.0666666666666667e-07, global_step: 2170, interval_runtime: 1.8322, interval_samples_per_second: 8.732, interval_steps_per_second: 5.458, epoch: 28.9333[0m
[32m[2022-09-16 18:04:09,414] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 18:04:09,414] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-16 18:04:09,415] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 18:04:09,415] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 18:04:09,415] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-16 18:04:14,927] [    INFO][0m - eval_loss: 3.229734420776367, eval_accuracy: 0.5519125683060109, eval_runtime: 5.5123, eval_samples_per_second: 199.191, eval_steps_per_second: 12.517, epoch: 29.0[0m
[32m[2022-09-16 18:04:14,947] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-2175[0m
[32m[2022-09-16 18:04:14,947] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 18:04:17,886] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-2175/tokenizer_config.json[0m
[32m[2022-09-16 18:04:17,886] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-2175/special_tokens_map.json[0m
[32m[2022-09-16 18:04:24,861] [    INFO][0m - loss: 0.01567186, learning_rate: 9.333333333333334e-08, global_step: 2180, interval_runtime: 16.2631, interval_samples_per_second: 0.984, interval_steps_per_second: 0.615, epoch: 29.0667[0m
[32m[2022-09-16 18:04:26,696] [    INFO][0m - loss: 0.01955579, learning_rate: 8e-08, global_step: 2190, interval_runtime: 1.8349, interval_samples_per_second: 8.72, interval_steps_per_second: 5.45, epoch: 29.2[0m
[32m[2022-09-16 18:04:28,531] [    INFO][0m - loss: 0.01287838, learning_rate: 6.666666666666667e-08, global_step: 2200, interval_runtime: 1.8352, interval_samples_per_second: 8.718, interval_steps_per_second: 5.449, epoch: 29.3333[0m
[32m[2022-09-16 18:04:30,384] [    INFO][0m - loss: 0.01863015, learning_rate: 5.3333333333333334e-08, global_step: 2210, interval_runtime: 1.8532, interval_samples_per_second: 8.634, interval_steps_per_second: 5.396, epoch: 29.4667[0m
[32m[2022-09-16 18:04:32,227] [    INFO][0m - loss: 0.04651923, learning_rate: 4e-08, global_step: 2220, interval_runtime: 1.8429, interval_samples_per_second: 8.682, interval_steps_per_second: 5.426, epoch: 29.6[0m
[32m[2022-09-16 18:04:34,063] [    INFO][0m - loss: 0.01285827, learning_rate: 2.6666666666666667e-08, global_step: 2230, interval_runtime: 1.8361, interval_samples_per_second: 8.714, interval_steps_per_second: 5.446, epoch: 29.7333[0m
[32m[2022-09-16 18:04:35,896] [    INFO][0m - loss: 0.01196409, learning_rate: 1.3333333333333334e-08, global_step: 2240, interval_runtime: 1.8331, interval_samples_per_second: 8.729, interval_steps_per_second: 5.455, epoch: 29.8667[0m
[32m[2022-09-16 18:04:37,632] [    INFO][0m - loss: 0.06495092, learning_rate: 0.0, global_step: 2250, interval_runtime: 1.7356, interval_samples_per_second: 9.219, interval_steps_per_second: 5.762, epoch: 30.0[0m
[32m[2022-09-16 18:04:37,632] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 18:04:37,632] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-16 18:04:37,633] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 18:04:37,633] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 18:04:37,633] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-16 18:04:43,131] [    INFO][0m - eval_loss: 3.2336008548736572, eval_accuracy: 0.5500910746812386, eval_runtime: 5.4976, eval_samples_per_second: 199.723, eval_steps_per_second: 12.551, epoch: 30.0[0m
[32m[2022-09-16 18:04:43,149] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-2250[0m
[32m[2022-09-16 18:04:43,150] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 18:04:46,188] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-2250/tokenizer_config.json[0m
[32m[2022-09-16 18:04:46,188] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-2250/special_tokens_map.json[0m
[32m[2022-09-16 18:04:52,450] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-16 18:04:52,450] [    INFO][0m - Loading best model from ./checkpoints_tnews/checkpoint-150 (score: 0.569216757741348).[0m
[32m[2022-09-16 18:04:54,085] [    INFO][0m - train_runtime: 887.1444, train_samples_per_second: 40.072, train_steps_per_second: 2.536, train_loss: 0.4234939965274599, epoch: 30.0[0m
[32m[2022-09-16 18:04:55,717] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/[0m
[32m[2022-09-16 18:04:55,718] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 18:04:58,419] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/tokenizer_config.json[0m
[32m[2022-09-16 18:04:58,420] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/special_tokens_map.json[0m
[32m[2022-09-16 18:04:58,421] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-16 18:04:58,421] [    INFO][0m -   epoch                    =       30.0[0m
[32m[2022-09-16 18:04:58,421] [    INFO][0m -   train_loss               =     0.4235[0m
[32m[2022-09-16 18:04:58,421] [    INFO][0m -   train_runtime            = 0:14:47.14[0m
[32m[2022-09-16 18:04:58,421] [    INFO][0m -   train_samples_per_second =     40.072[0m
[32m[2022-09-16 18:04:58,421] [    INFO][0m -   train_steps_per_second   =      2.536[0m
[32m[2022-09-16 18:04:58,431] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-16 18:04:58,431] [    INFO][0m -   Num examples = 2010[0m
[32m[2022-09-16 18:04:58,431] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 18:04:58,431] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 18:04:58,431] [    INFO][0m -   Total prediction steps = 126[0m
[32m[2022-09-16 18:05:08,536] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-16 18:05:08,537] [    INFO][0m -   test_accuracy           =     0.5896[0m
[32m[2022-09-16 18:05:08,537] [    INFO][0m -   test_loss               =     1.3464[0m
[32m[2022-09-16 18:05:08,537] [    INFO][0m -   test_runtime            = 0:00:10.10[0m
[32m[2022-09-16 18:05:08,537] [    INFO][0m -   test_samples_per_second =    198.904[0m
[32m[2022-09-16 18:05:08,537] [    INFO][0m -   test_steps_per_second   =     12.469[0m
[32m[2022-09-16 18:05:08,538] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-16 18:05:08,538] [    INFO][0m -   Num examples = 1500[0m
[32m[2022-09-16 18:05:08,538] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 18:05:08,538] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 18:05:08,538] [    INFO][0m -   Total prediction steps = 94[0m
[32m[2022-09-16 18:05:16,402] [    INFO][0m - Predictions for tnewsf saved to ./fewclue_submit_examples.[0m
{
  "labels": 11,
  "text_a": "\u5b69\u5b50\u8ddf\u8c01\u7761\uff0c\u5c31\u662f\u8c01\u7684\u5b69\u5b50",
  "text_b": "",
  "uid": 448
}

 
==========
iflytek
==========
 
[32m[2022-09-16 18:05:40,690] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-16 18:05:40,691] [    INFO][0m - ============================================================[0m
[32m[2022-09-16 18:05:40,691] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-16 18:05:40,691] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-16 18:05:40,691] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-16 18:05:40,691] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-16 18:05:40,691] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-16 18:05:40,691] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-16 18:05:40,691] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-16 18:05:40,691] [    INFO][0m - [0m
[32m[2022-09-16 18:05:40,692] [    INFO][0m - ============================================================[0m
[32m[2022-09-16 18:05:40,692] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-16 18:05:40,692] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-16 18:05:40,692] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-16 18:05:40,692] [    INFO][0m - pretrained                    :../checkpoints_cmnli/checkpoint-22000/model_state.pdparams[0m
[32m[2022-09-16 18:05:40,692] [    INFO][0m - prompt                        :‚Äú{'text':'text_a'}‚ÄùÂõ†Ê≠§ÔºåÂ∫îÁî®Á±ªÂà´ÊòØ{'mask'}{'mask'}„ÄÇ[0m
[32m[2022-09-16 18:05:40,692] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-16 18:05:40,692] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-16 18:05:40,692] [    INFO][0m - task_name                     :iflytek[0m
[32m[2022-09-16 18:05:40,692] [    INFO][0m - [0m
[32m[2022-09-16 18:05:40,693] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0916 18:05:40.694567 69611 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0916 18:05:40.698709 69611 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-16 18:05:48,210] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-16 18:05:48,222] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-16 18:05:48,222] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-16 18:05:48,222] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùÂõ†Ê≠§ÔºåÂ∫îÁî®Á±ªÂà´ÊòØ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': '„ÄÇ'}][0m
[32m[2022-09-16 18:05:50,511] [    INFO][0m - ============================================================[0m
[32m[2022-09-16 18:05:50,511] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-16 18:05:50,511] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-16 18:05:50,511] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-16 18:05:50,511] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-16 18:05:50,511] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-16 18:05:50,511] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-16 18:05:50,511] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-16 18:05:50,512] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-16 18:05:50,512] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-16 18:05:50,512] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-16 18:05:50,512] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-16 18:05:50,512] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-16 18:05:50,512] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-16 18:05:50,512] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-16 18:05:50,512] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-16 18:05:50,512] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-16 18:05:50,512] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-16 18:05:50,512] [    INFO][0m - eval_batch_size               :16[0m
[32m[2022-09-16 18:05:50,512] [    INFO][0m - eval_steps                    :200[0m
[32m[2022-09-16 18:05:50,512] [    INFO][0m - evaluation_strategy           :IntervalStrategy.EPOCH[0m
[32m[2022-09-16 18:05:50,513] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-16 18:05:50,513] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-16 18:05:50,513] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-16 18:05:50,513] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-16 18:05:50,513] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-16 18:05:50,513] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-16 18:05:50,513] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-16 18:05:50,513] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-16 18:05:50,513] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-16 18:05:50,513] [    INFO][0m - learning_rate                 :3e-06[0m
[32m[2022-09-16 18:05:50,513] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-16 18:05:50,513] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-16 18:05:50,513] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-16 18:05:50,514] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-16 18:05:50,514] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-16 18:05:50,514] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-16 18:05:50,514] [    INFO][0m - logging_dir                   :./checkpoints_iflytek/runs/Sep16_18-05-40_instance-3bwob41y-01[0m
[32m[2022-09-16 18:05:50,514] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-16 18:05:50,514] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-16 18:05:50,514] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-16 18:05:50,514] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-16 18:05:50,514] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-16 18:05:50,514] [    INFO][0m - max_seq_length                :320[0m
[32m[2022-09-16 18:05:50,514] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-16 18:05:50,514] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-16 18:05:50,514] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-16 18:05:50,515] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-16 18:05:50,515] [    INFO][0m - num_train_epochs              :30.0[0m
[32m[2022-09-16 18:05:50,515] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-16 18:05:50,515] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-16 18:05:50,515] [    INFO][0m - output_dir                    :./checkpoints_iflytek/[0m
[32m[2022-09-16 18:05:50,515] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-16 18:05:50,515] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-16 18:05:50,515] [    INFO][0m - per_device_eval_batch_size    :16[0m
[32m[2022-09-16 18:05:50,515] [    INFO][0m - per_device_train_batch_size   :16[0m
[32m[2022-09-16 18:05:50,515] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-16 18:05:50,515] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-16 18:05:50,515] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-16 18:05:50,515] [    INFO][0m - ppt_learning_rate             :3e-05[0m
[32m[2022-09-16 18:05:50,515] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-16 18:05:50,516] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-16 18:05:50,516] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-16 18:05:50,516] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-16 18:05:50,516] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-16 18:05:50,516] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-16 18:05:50,516] [    INFO][0m - run_name                      :./checkpoints_iflytek/[0m
[32m[2022-09-16 18:05:50,516] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-16 18:05:50,516] [    INFO][0m - save_steps                    :200[0m
[32m[2022-09-16 18:05:50,516] [    INFO][0m - save_strategy                 :IntervalStrategy.EPOCH[0m
[32m[2022-09-16 18:05:50,516] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-16 18:05:50,516] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-16 18:05:50,516] [    INFO][0m - seed                          :42[0m
[32m[2022-09-16 18:05:50,516] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-16 18:05:50,517] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-16 18:05:50,517] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-16 18:05:50,517] [    INFO][0m - train_batch_size              :16[0m
[32m[2022-09-16 18:05:50,517] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-16 18:05:50,517] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-16 18:05:50,517] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-16 18:05:50,517] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-16 18:05:50,517] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-16 18:05:50,517] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-16 18:05:50,517] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-16 18:05:50,517] [    INFO][0m - [0m
[32m[2022-09-16 18:05:50,520] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-16 18:05:50,520] [    INFO][0m -   Num examples = 3024[0m
[32m[2022-09-16 18:05:50,520] [    INFO][0m -   Num Epochs = 30[0m
[32m[2022-09-16 18:05:50,520] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2022-09-16 18:05:50,520] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2022-09-16 18:05:50,520] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-16 18:05:50,520] [    INFO][0m -   Total optimization steps = 5670.0[0m
[32m[2022-09-16 18:05:50,520] [    INFO][0m -   Total num train samples = 90720[0m
[33m[2022-09-16 18:05:50,530] [ WARNING][0m - Token indices sequence length is longer than the specified maximum sequence length for this model (637 > 512). Running this sequence through the model will result in indexing errors[0m
[32m[2022-09-16 18:05:59,475] [    INFO][0m - loss: 7.55677261, learning_rate: 2.9947089947089946e-06, global_step: 10, interval_runtime: 8.9533, interval_samples_per_second: 1.787, interval_steps_per_second: 1.117, epoch: 0.0529[0m
[32m[2022-09-16 18:06:07,269] [    INFO][0m - loss: 4.42899361, learning_rate: 2.9894179894179896e-06, global_step: 20, interval_runtime: 7.7947, interval_samples_per_second: 2.053, interval_steps_per_second: 1.283, epoch: 0.1058[0m
[32m[2022-09-16 18:06:15,087] [    INFO][0m - loss: 3.92251167, learning_rate: 2.984126984126984e-06, global_step: 30, interval_runtime: 7.8181, interval_samples_per_second: 2.047, interval_steps_per_second: 1.279, epoch: 0.1587[0m
[32m[2022-09-16 18:06:22,896] [    INFO][0m - loss: 3.59458389, learning_rate: 2.9788359788359788e-06, global_step: 40, interval_runtime: 7.8087, interval_samples_per_second: 2.049, interval_steps_per_second: 1.281, epoch: 0.2116[0m
[32m[2022-09-16 18:06:30,713] [    INFO][0m - loss: 3.21243896, learning_rate: 2.9735449735449733e-06, global_step: 50, interval_runtime: 7.817, interval_samples_per_second: 2.047, interval_steps_per_second: 1.279, epoch: 0.2646[0m
[32m[2022-09-16 18:06:38,596] [    INFO][0m - loss: 3.08539505, learning_rate: 2.9682539682539683e-06, global_step: 60, interval_runtime: 7.8826, interval_samples_per_second: 2.03, interval_steps_per_second: 1.269, epoch: 0.3175[0m
[32m[2022-09-16 18:06:46,449] [    INFO][0m - loss: 3.14106102, learning_rate: 2.962962962962963e-06, global_step: 70, interval_runtime: 7.8535, interval_samples_per_second: 2.037, interval_steps_per_second: 1.273, epoch: 0.3704[0m
[32m[2022-09-16 18:06:54,327] [    INFO][0m - loss: 3.14912338, learning_rate: 2.957671957671958e-06, global_step: 80, interval_runtime: 7.8779, interval_samples_per_second: 2.031, interval_steps_per_second: 1.269, epoch: 0.4233[0m
[32m[2022-09-16 18:07:02,221] [    INFO][0m - loss: 2.97676697, learning_rate: 2.9523809523809525e-06, global_step: 90, interval_runtime: 7.8935, interval_samples_per_second: 2.027, interval_steps_per_second: 1.267, epoch: 0.4762[0m
[32m[2022-09-16 18:07:10,109] [    INFO][0m - loss: 2.71321678, learning_rate: 2.947089947089947e-06, global_step: 100, interval_runtime: 7.8882, interval_samples_per_second: 2.028, interval_steps_per_second: 1.268, epoch: 0.5291[0m
[32m[2022-09-16 18:07:17,979] [    INFO][0m - loss: 2.82073364, learning_rate: 2.9417989417989416e-06, global_step: 110, interval_runtime: 7.8698, interval_samples_per_second: 2.033, interval_steps_per_second: 1.271, epoch: 0.582[0m
[32m[2022-09-16 18:07:25,888] [    INFO][0m - loss: 2.50116425, learning_rate: 2.9365079365079366e-06, global_step: 120, interval_runtime: 7.9092, interval_samples_per_second: 2.023, interval_steps_per_second: 1.264, epoch: 0.6349[0m
[32m[2022-09-16 18:07:33,781] [    INFO][0m - loss: 2.33819656, learning_rate: 2.931216931216931e-06, global_step: 130, interval_runtime: 7.8934, interval_samples_per_second: 2.027, interval_steps_per_second: 1.267, epoch: 0.6878[0m
[32m[2022-09-16 18:07:41,673] [    INFO][0m - loss: 2.49874306, learning_rate: 2.925925925925926e-06, global_step: 140, interval_runtime: 7.8919, interval_samples_per_second: 2.027, interval_steps_per_second: 1.267, epoch: 0.7407[0m
[32m[2022-09-16 18:07:49,562] [    INFO][0m - loss: 2.27028599, learning_rate: 2.9206349206349207e-06, global_step: 150, interval_runtime: 7.889, interval_samples_per_second: 2.028, interval_steps_per_second: 1.268, epoch: 0.7937[0m
[32m[2022-09-16 18:07:58,864] [    INFO][0m - loss: 2.34674339, learning_rate: 2.9153439153439153e-06, global_step: 160, interval_runtime: 7.9256, interval_samples_per_second: 2.019, interval_steps_per_second: 1.262, epoch: 0.8466[0m
[32m[2022-09-16 18:08:06,744] [    INFO][0m - loss: 2.32957077, learning_rate: 2.91005291005291e-06, global_step: 170, interval_runtime: 9.2565, interval_samples_per_second: 1.729, interval_steps_per_second: 1.08, epoch: 0.8995[0m
[32m[2022-09-16 18:08:14,642] [    INFO][0m - loss: 2.63037376, learning_rate: 2.904761904761905e-06, global_step: 180, interval_runtime: 7.8978, interval_samples_per_second: 2.026, interval_steps_per_second: 1.266, epoch: 0.9524[0m
[32m[2022-09-16 18:08:21,577] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 18:08:21,577] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-16 18:08:21,577] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 18:08:21,577] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 18:08:21,578] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-16 18:08:49,045] [    INFO][0m - eval_loss: 2.0981945991516113, eval_accuracy: 0.4493809176984705, eval_runtime: 27.4671, eval_samples_per_second: 49.987, eval_steps_per_second: 3.131, epoch: 1.0[0m
[32m[2022-09-16 18:08:49,068] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-189[0m
[32m[2022-09-16 18:08:49,069] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 18:08:52,205] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-189/tokenizer_config.json[0m
[32m[2022-09-16 18:08:52,205] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-189/special_tokens_map.json[0m
[32m[2022-09-16 18:08:59,213] [    INFO][0m - loss: 2.13891296, learning_rate: 2.8994708994708994e-06, global_step: 190, interval_runtime: 44.5706, interval_samples_per_second: 0.359, interval_steps_per_second: 0.224, epoch: 1.0053[0m
[32m[2022-09-16 18:09:07,042] [    INFO][0m - loss: 1.83953247, learning_rate: 2.8941798941798944e-06, global_step: 200, interval_runtime: 7.8292, interval_samples_per_second: 2.044, interval_steps_per_second: 1.277, epoch: 1.0582[0m
[32m[2022-09-16 18:09:14,983] [    INFO][0m - loss: 2.04348793, learning_rate: 2.888888888888889e-06, global_step: 210, interval_runtime: 7.9409, interval_samples_per_second: 2.015, interval_steps_per_second: 1.259, epoch: 1.1111[0m
[32m[2022-09-16 18:09:22,886] [    INFO][0m - loss: 1.8278347, learning_rate: 2.8835978835978836e-06, global_step: 220, interval_runtime: 7.9034, interval_samples_per_second: 2.024, interval_steps_per_second: 1.265, epoch: 1.164[0m
[32m[2022-09-16 18:09:30,816] [    INFO][0m - loss: 2.09964314, learning_rate: 2.878306878306878e-06, global_step: 230, interval_runtime: 7.9293, interval_samples_per_second: 2.018, interval_steps_per_second: 1.261, epoch: 1.2169[0m
[32m[2022-09-16 18:09:38,775] [    INFO][0m - loss: 1.81381874, learning_rate: 2.873015873015873e-06, global_step: 240, interval_runtime: 7.9593, interval_samples_per_second: 2.01, interval_steps_per_second: 1.256, epoch: 1.2698[0m
[32m[2022-09-16 18:09:46,697] [    INFO][0m - loss: 2.05322361, learning_rate: 2.8677248677248677e-06, global_step: 250, interval_runtime: 7.922, interval_samples_per_second: 2.02, interval_steps_per_second: 1.262, epoch: 1.3228[0m
[32m[2022-09-16 18:09:54,672] [    INFO][0m - loss: 2.34227676, learning_rate: 2.8624338624338627e-06, global_step: 260, interval_runtime: 7.9744, interval_samples_per_second: 2.006, interval_steps_per_second: 1.254, epoch: 1.3757[0m
[32m[2022-09-16 18:10:02,661] [    INFO][0m - loss: 2.07583656, learning_rate: 2.8571428571428573e-06, global_step: 270, interval_runtime: 7.9883, interval_samples_per_second: 2.003, interval_steps_per_second: 1.252, epoch: 1.4286[0m
[32m[2022-09-16 18:10:10,576] [    INFO][0m - loss: 2.07210236, learning_rate: 2.851851851851852e-06, global_step: 280, interval_runtime: 7.9161, interval_samples_per_second: 2.021, interval_steps_per_second: 1.263, epoch: 1.4815[0m
[32m[2022-09-16 18:10:18,488] [    INFO][0m - loss: 1.98008213, learning_rate: 2.8465608465608464e-06, global_step: 290, interval_runtime: 7.9121, interval_samples_per_second: 2.022, interval_steps_per_second: 1.264, epoch: 1.5344[0m
[32m[2022-09-16 18:10:26,433] [    INFO][0m - loss: 1.97137947, learning_rate: 2.8412698412698414e-06, global_step: 300, interval_runtime: 7.9449, interval_samples_per_second: 2.014, interval_steps_per_second: 1.259, epoch: 1.5873[0m
[32m[2022-09-16 18:10:34,345] [    INFO][0m - loss: 2.15251884, learning_rate: 2.835978835978836e-06, global_step: 310, interval_runtime: 7.9113, interval_samples_per_second: 2.022, interval_steps_per_second: 1.264, epoch: 1.6402[0m
[32m[2022-09-16 18:10:47,399] [    INFO][0m - loss: 1.75872898, learning_rate: 2.830687830687831e-06, global_step: 320, interval_runtime: 7.9301, interval_samples_per_second: 2.018, interval_steps_per_second: 1.261, epoch: 1.6931[0m
[32m[2022-09-16 18:10:55,299] [    INFO][0m - loss: 2.06800671, learning_rate: 2.8253968253968255e-06, global_step: 330, interval_runtime: 13.0245, interval_samples_per_second: 1.228, interval_steps_per_second: 0.768, epoch: 1.746[0m
[32m[2022-09-16 18:11:03,182] [    INFO][0m - loss: 1.98325119, learning_rate: 2.82010582010582e-06, global_step: 340, interval_runtime: 7.883, interval_samples_per_second: 2.03, interval_steps_per_second: 1.269, epoch: 1.7989[0m
[32m[2022-09-16 18:11:11,070] [    INFO][0m - loss: 1.83145523, learning_rate: 2.8148148148148147e-06, global_step: 350, interval_runtime: 7.8883, interval_samples_per_second: 2.028, interval_steps_per_second: 1.268, epoch: 1.8519[0m
[32m[2022-09-16 18:11:18,970] [    INFO][0m - loss: 1.89616146, learning_rate: 2.8095238095238096e-06, global_step: 360, interval_runtime: 7.8998, interval_samples_per_second: 2.025, interval_steps_per_second: 1.266, epoch: 1.9048[0m
[32m[2022-09-16 18:11:29,363] [    INFO][0m - loss: 1.8790802, learning_rate: 2.8042328042328042e-06, global_step: 370, interval_runtime: 7.908, interval_samples_per_second: 2.023, interval_steps_per_second: 1.265, epoch: 1.9577[0m
[32m[2022-09-16 18:11:35,508] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 18:11:35,508] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-16 18:11:35,508] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 18:11:35,508] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 18:11:35,508] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-16 18:12:03,069] [    INFO][0m - eval_loss: 1.905476689338684, eval_accuracy: 0.4654042243262928, eval_runtime: 27.5604, eval_samples_per_second: 49.818, eval_steps_per_second: 3.12, epoch: 2.0[0m
[32m[2022-09-16 18:12:03,098] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-378[0m
[32m[2022-09-16 18:12:03,098] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 18:12:06,255] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-378/tokenizer_config.json[0m
[32m[2022-09-16 18:12:06,255] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-378/special_tokens_map.json[0m
[32m[2022-09-16 18:12:17,463] [    INFO][0m - loss: 1.52829857, learning_rate: 2.798941798941799e-06, global_step: 380, interval_runtime: 50.1515, interval_samples_per_second: 0.319, interval_steps_per_second: 0.199, epoch: 2.0106[0m
[32m[2022-09-16 18:12:25,385] [    INFO][0m - loss: 1.69610233, learning_rate: 2.7936507936507934e-06, global_step: 390, interval_runtime: 8.3561, interval_samples_per_second: 1.915, interval_steps_per_second: 1.197, epoch: 2.0635[0m
[32m[2022-09-16 18:12:33,269] [    INFO][0m - loss: 1.54283762, learning_rate: 2.7883597883597883e-06, global_step: 400, interval_runtime: 7.8837, interval_samples_per_second: 2.03, interval_steps_per_second: 1.268, epoch: 2.1164[0m
[32m[2022-09-16 18:12:41,140] [    INFO][0m - loss: 1.76497936, learning_rate: 2.783068783068783e-06, global_step: 410, interval_runtime: 7.8712, interval_samples_per_second: 2.033, interval_steps_per_second: 1.27, epoch: 2.1693[0m
[32m[2022-09-16 18:12:49,039] [    INFO][0m - loss: 1.66438789, learning_rate: 2.777777777777778e-06, global_step: 420, interval_runtime: 7.8991, interval_samples_per_second: 2.026, interval_steps_per_second: 1.266, epoch: 2.2222[0m
[32m[2022-09-16 18:12:56,945] [    INFO][0m - loss: 1.62132607, learning_rate: 2.7724867724867725e-06, global_step: 430, interval_runtime: 7.9057, interval_samples_per_second: 2.024, interval_steps_per_second: 1.265, epoch: 2.2751[0m
[32m[2022-09-16 18:13:07,425] [    INFO][0m - loss: 1.68370152, learning_rate: 2.7671957671957675e-06, global_step: 440, interval_runtime: 7.9012, interval_samples_per_second: 2.025, interval_steps_per_second: 1.266, epoch: 2.328[0m
[32m[2022-09-16 18:13:15,336] [    INFO][0m - loss: 1.48382244, learning_rate: 2.7619047619047616e-06, global_step: 450, interval_runtime: 10.4902, interval_samples_per_second: 1.525, interval_steps_per_second: 0.953, epoch: 2.381[0m
[32m[2022-09-16 18:13:23,280] [    INFO][0m - loss: 1.53427734, learning_rate: 2.7566137566137566e-06, global_step: 460, interval_runtime: 7.944, interval_samples_per_second: 2.014, interval_steps_per_second: 1.259, epoch: 2.4339[0m
[32m[2022-09-16 18:13:32,126] [    INFO][0m - loss: 1.55930119, learning_rate: 2.751322751322751e-06, global_step: 470, interval_runtime: 7.9214, interval_samples_per_second: 2.02, interval_steps_per_second: 1.262, epoch: 2.4868[0m
[32m[2022-09-16 18:13:40,096] [    INFO][0m - loss: 1.70260315, learning_rate: 2.746031746031746e-06, global_step: 480, interval_runtime: 8.8947, interval_samples_per_second: 1.799, interval_steps_per_second: 1.124, epoch: 2.5397[0m
[32m[2022-09-16 18:13:48,040] [    INFO][0m - loss: 1.70788307, learning_rate: 2.7407407407407407e-06, global_step: 490, interval_runtime: 7.9429, interval_samples_per_second: 2.014, interval_steps_per_second: 1.259, epoch: 2.5926[0m
[32m[2022-09-16 18:13:55,929] [    INFO][0m - loss: 1.62541447, learning_rate: 2.7354497354497357e-06, global_step: 500, interval_runtime: 7.8895, interval_samples_per_second: 2.028, interval_steps_per_second: 1.268, epoch: 2.6455[0m
[32m[2022-09-16 18:14:03,784] [    INFO][0m - loss: 1.75427914, learning_rate: 2.73015873015873e-06, global_step: 510, interval_runtime: 7.8552, interval_samples_per_second: 2.037, interval_steps_per_second: 1.273, epoch: 2.6984[0m
[32m[2022-09-16 18:14:12,597] [    INFO][0m - loss: 1.68110542, learning_rate: 2.724867724867725e-06, global_step: 520, interval_runtime: 7.8972, interval_samples_per_second: 2.026, interval_steps_per_second: 1.266, epoch: 2.7513[0m
[32m[2022-09-16 18:14:20,505] [    INFO][0m - loss: 1.7058672, learning_rate: 2.7195767195767194e-06, global_step: 530, interval_runtime: 8.8232, interval_samples_per_second: 1.813, interval_steps_per_second: 1.133, epoch: 2.8042[0m
[32m[2022-09-16 18:14:29,038] [    INFO][0m - loss: 1.61246185, learning_rate: 2.7142857142857144e-06, global_step: 540, interval_runtime: 7.9107, interval_samples_per_second: 2.023, interval_steps_per_second: 1.264, epoch: 2.8571[0m
[32m[2022-09-16 18:14:36,950] [    INFO][0m - loss: 1.50965652, learning_rate: 2.708994708994709e-06, global_step: 550, interval_runtime: 8.5347, interval_samples_per_second: 1.875, interval_steps_per_second: 1.172, epoch: 2.9101[0m
[32m[2022-09-16 18:14:44,889] [    INFO][0m - loss: 1.56764946, learning_rate: 2.703703703703704e-06, global_step: 560, interval_runtime: 7.9385, interval_samples_per_second: 2.016, interval_steps_per_second: 1.26, epoch: 2.963[0m
[32m[2022-09-16 18:14:50,275] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 18:14:50,275] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-16 18:14:50,275] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 18:14:50,276] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 18:14:50,276] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-16 18:15:18,437] [    INFO][0m - eval_loss: 1.8128684759140015, eval_accuracy: 0.4726875455207575, eval_runtime: 28.1607, eval_samples_per_second: 48.756, eval_steps_per_second: 3.054, epoch: 3.0[0m
[32m[2022-09-16 18:15:18,461] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-567[0m
[32m[2022-09-16 18:15:18,461] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 18:15:21,916] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-567/tokenizer_config.json[0m
[32m[2022-09-16 18:15:21,916] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-567/special_tokens_map.json[0m
[32m[2022-09-16 18:15:30,690] [    INFO][0m - loss: 1.59694443, learning_rate: 2.698412698412698e-06, global_step: 570, interval_runtime: 45.8018, interval_samples_per_second: 0.349, interval_steps_per_second: 0.218, epoch: 3.0159[0m
[32m[2022-09-16 18:15:39,642] [    INFO][0m - loss: 1.43882265, learning_rate: 2.693121693121693e-06, global_step: 580, interval_runtime: 7.8508, interval_samples_per_second: 2.038, interval_steps_per_second: 1.274, epoch: 3.0688[0m
[32m[2022-09-16 18:15:47,518] [    INFO][0m - loss: 1.63557167, learning_rate: 2.6878306878306877e-06, global_step: 590, interval_runtime: 8.9769, interval_samples_per_second: 1.782, interval_steps_per_second: 1.114, epoch: 3.1217[0m
[32m[2022-09-16 18:15:55,432] [    INFO][0m - loss: 1.48837185, learning_rate: 2.6825396825396827e-06, global_step: 600, interval_runtime: 7.9135, interval_samples_per_second: 2.022, interval_steps_per_second: 1.264, epoch: 3.1746[0m
[32m[2022-09-16 18:16:03,331] [    INFO][0m - loss: 1.45044308, learning_rate: 2.6772486772486773e-06, global_step: 610, interval_runtime: 7.8994, interval_samples_per_second: 2.025, interval_steps_per_second: 1.266, epoch: 3.2275[0m
[32m[2022-09-16 18:16:11,223] [    INFO][0m - loss: 1.26073093, learning_rate: 2.671957671957672e-06, global_step: 620, interval_runtime: 7.8917, interval_samples_per_second: 2.027, interval_steps_per_second: 1.267, epoch: 3.2804[0m
[32m[2022-09-16 18:16:19,114] [    INFO][0m - loss: 1.43465986, learning_rate: 2.6666666666666664e-06, global_step: 630, interval_runtime: 7.8911, interval_samples_per_second: 2.028, interval_steps_per_second: 1.267, epoch: 3.3333[0m
[32m[2022-09-16 18:16:27,035] [    INFO][0m - loss: 1.37705717, learning_rate: 2.6613756613756614e-06, global_step: 640, interval_runtime: 7.9209, interval_samples_per_second: 2.02, interval_steps_per_second: 1.262, epoch: 3.3862[0m
[32m[2022-09-16 18:16:34,971] [    INFO][0m - loss: 1.22173986, learning_rate: 2.656084656084656e-06, global_step: 650, interval_runtime: 7.9368, interval_samples_per_second: 2.016, interval_steps_per_second: 1.26, epoch: 3.4392[0m
[32m[2022-09-16 18:16:42,898] [    INFO][0m - loss: 1.31640806, learning_rate: 2.650793650793651e-06, global_step: 660, interval_runtime: 7.9262, interval_samples_per_second: 2.019, interval_steps_per_second: 1.262, epoch: 3.4921[0m
[32m[2022-09-16 18:16:50,802] [    INFO][0m - loss: 1.3789691, learning_rate: 2.6455026455026455e-06, global_step: 670, interval_runtime: 7.9041, interval_samples_per_second: 2.024, interval_steps_per_second: 1.265, epoch: 3.545[0m
[32m[2022-09-16 18:16:58,712] [    INFO][0m - loss: 1.30323696, learning_rate: 2.64021164021164e-06, global_step: 680, interval_runtime: 7.9107, interval_samples_per_second: 2.023, interval_steps_per_second: 1.264, epoch: 3.5979[0m
[32m[2022-09-16 18:17:07,577] [    INFO][0m - loss: 1.61739006, learning_rate: 2.6349206349206347e-06, global_step: 690, interval_runtime: 7.9081, interval_samples_per_second: 2.023, interval_steps_per_second: 1.265, epoch: 3.6508[0m
[32m[2022-09-16 18:17:15,475] [    INFO][0m - loss: 1.6023489, learning_rate: 2.6296296296296297e-06, global_step: 700, interval_runtime: 8.854, interval_samples_per_second: 1.807, interval_steps_per_second: 1.129, epoch: 3.7037[0m
[32m[2022-09-16 18:17:23,409] [    INFO][0m - loss: 1.06838818, learning_rate: 2.6243386243386242e-06, global_step: 710, interval_runtime: 7.9343, interval_samples_per_second: 2.017, interval_steps_per_second: 1.26, epoch: 3.7566[0m
[32m[2022-09-16 18:17:31,392] [    INFO][0m - loss: 1.26379747, learning_rate: 2.6190476190476192e-06, global_step: 720, interval_runtime: 7.9826, interval_samples_per_second: 2.004, interval_steps_per_second: 1.253, epoch: 3.8095[0m
[32m[2022-09-16 18:17:39,309] [    INFO][0m - loss: 1.50378895, learning_rate: 2.613756613756614e-06, global_step: 730, interval_runtime: 7.9176, interval_samples_per_second: 2.021, interval_steps_per_second: 1.263, epoch: 3.8624[0m
[32m[2022-09-16 18:17:49,296] [    INFO][0m - loss: 1.47596569, learning_rate: 2.6084656084656084e-06, global_step: 740, interval_runtime: 7.9102, interval_samples_per_second: 2.023, interval_steps_per_second: 1.264, epoch: 3.9153[0m
[32m[2022-09-16 18:17:57,203] [    INFO][0m - loss: 1.54174833, learning_rate: 2.603174603174603e-06, global_step: 750, interval_runtime: 9.9838, interval_samples_per_second: 1.603, interval_steps_per_second: 1.002, epoch: 3.9683[0m
[32m[2022-09-16 18:18:01,769] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 18:18:02,314] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-16 18:18:02,315] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 18:18:02,315] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 18:18:02,319] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-16 18:18:29,662] [    INFO][0m - eval_loss: 1.8464887142181396, eval_accuracy: 0.4697742170429716, eval_runtime: 27.892, eval_samples_per_second: 49.226, eval_steps_per_second: 3.083, epoch: 4.0[0m
[32m[2022-09-16 18:18:29,681] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-756[0m
[32m[2022-09-16 18:18:29,681] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 18:18:32,611] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-756/tokenizer_config.json[0m
[32m[2022-09-16 18:18:32,611] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-756/special_tokens_map.json[0m
[32m[2022-09-16 18:18:42,660] [    INFO][0m - loss: 1.40531378, learning_rate: 2.597883597883598e-06, global_step: 760, interval_runtime: 45.4568, interval_samples_per_second: 0.352, interval_steps_per_second: 0.22, epoch: 4.0212[0m
[32m[2022-09-16 18:18:50,567] [    INFO][0m - loss: 1.18742628, learning_rate: 2.5925925925925925e-06, global_step: 770, interval_runtime: 7.9065, interval_samples_per_second: 2.024, interval_steps_per_second: 1.265, epoch: 4.0741[0m
[32m[2022-09-16 18:18:58,502] [    INFO][0m - loss: 1.14005489, learning_rate: 2.5873015873015875e-06, global_step: 780, interval_runtime: 7.9352, interval_samples_per_second: 2.016, interval_steps_per_second: 1.26, epoch: 4.127[0m
[32m[2022-09-16 18:19:06,380] [    INFO][0m - loss: 1.16272907, learning_rate: 2.582010582010582e-06, global_step: 790, interval_runtime: 7.8786, interval_samples_per_second: 2.031, interval_steps_per_second: 1.269, epoch: 4.1799[0m
[32m[2022-09-16 18:19:14,305] [    INFO][0m - loss: 1.07059774, learning_rate: 2.5767195767195766e-06, global_step: 800, interval_runtime: 7.9253, interval_samples_per_second: 2.019, interval_steps_per_second: 1.262, epoch: 4.2328[0m
[32m[2022-09-16 18:19:22,206] [    INFO][0m - loss: 1.22485638, learning_rate: 2.571428571428571e-06, global_step: 810, interval_runtime: 7.9001, interval_samples_per_second: 2.025, interval_steps_per_second: 1.266, epoch: 4.2857[0m
[32m[2022-09-16 18:19:30,133] [    INFO][0m - loss: 1.09103079, learning_rate: 2.566137566137566e-06, global_step: 820, interval_runtime: 7.9274, interval_samples_per_second: 2.018, interval_steps_per_second: 1.261, epoch: 4.3386[0m
[32m[2022-09-16 18:19:38,019] [    INFO][0m - loss: 1.49046669, learning_rate: 2.5608465608465608e-06, global_step: 830, interval_runtime: 7.8856, interval_samples_per_second: 2.029, interval_steps_per_second: 1.268, epoch: 4.3915[0m
[32m[2022-09-16 18:19:45,909] [    INFO][0m - loss: 1.47392588, learning_rate: 2.5555555555555557e-06, global_step: 840, interval_runtime: 7.8898, interval_samples_per_second: 2.028, interval_steps_per_second: 1.267, epoch: 4.4444[0m
[32m[2022-09-16 18:19:55,098] [    INFO][0m - loss: 1.05011272, learning_rate: 2.5502645502645503e-06, global_step: 850, interval_runtime: 7.9612, interval_samples_per_second: 2.01, interval_steps_per_second: 1.256, epoch: 4.4974[0m
[32m[2022-09-16 18:20:03,002] [    INFO][0m - loss: 1.37138081, learning_rate: 2.544973544973545e-06, global_step: 860, interval_runtime: 9.1326, interval_samples_per_second: 1.752, interval_steps_per_second: 1.095, epoch: 4.5503[0m
[32m[2022-09-16 18:20:10,933] [    INFO][0m - loss: 1.16110506, learning_rate: 2.5396825396825395e-06, global_step: 870, interval_runtime: 7.9304, interval_samples_per_second: 2.018, interval_steps_per_second: 1.261, epoch: 4.6032[0m
[32m[2022-09-16 18:20:19,512] [    INFO][0m - loss: 1.60173836, learning_rate: 2.5343915343915344e-06, global_step: 880, interval_runtime: 7.8822, interval_samples_per_second: 2.03, interval_steps_per_second: 1.269, epoch: 4.6561[0m
[32m[2022-09-16 18:20:27,425] [    INFO][0m - loss: 1.30010128, learning_rate: 2.529100529100529e-06, global_step: 890, interval_runtime: 8.6101, interval_samples_per_second: 1.858, interval_steps_per_second: 1.161, epoch: 4.709[0m
[32m[2022-09-16 18:20:35,313] [    INFO][0m - loss: 1.21959906, learning_rate: 2.523809523809524e-06, global_step: 900, interval_runtime: 7.8874, interval_samples_per_second: 2.029, interval_steps_per_second: 1.268, epoch: 4.7619[0m
[32m[2022-09-16 18:20:43,290] [    INFO][0m - loss: 1.35730028, learning_rate: 2.5185185185185186e-06, global_step: 910, interval_runtime: 7.9773, interval_samples_per_second: 2.006, interval_steps_per_second: 1.254, epoch: 4.8148[0m
[32m[2022-09-16 18:20:52,702] [    INFO][0m - loss: 1.2170825, learning_rate: 2.513227513227513e-06, global_step: 920, interval_runtime: 7.9095, interval_samples_per_second: 2.023, interval_steps_per_second: 1.264, epoch: 4.8677[0m
[32m[2022-09-16 18:21:00,839] [    INFO][0m - loss: 1.23686867, learning_rate: 2.5079365079365077e-06, global_step: 930, interval_runtime: 9.4426, interval_samples_per_second: 1.694, interval_steps_per_second: 1.059, epoch: 4.9206[0m
[32m[2022-09-16 18:21:08,829] [    INFO][0m - loss: 1.17119589, learning_rate: 2.5026455026455027e-06, global_step: 940, interval_runtime: 8.1871, interval_samples_per_second: 1.954, interval_steps_per_second: 1.221, epoch: 4.9735[0m
[32m[2022-09-16 18:21:12,596] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 18:21:12,596] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-16 18:21:12,596] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 18:21:12,596] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 18:21:12,597] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-16 18:21:40,003] [    INFO][0m - eval_loss: 1.8901852369308472, eval_accuracy: 0.46394756008739985, eval_runtime: 27.4065, eval_samples_per_second: 50.098, eval_steps_per_second: 3.138, epoch: 5.0[0m
[32m[2022-09-16 18:21:40,030] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-945[0m
[32m[2022-09-16 18:21:40,030] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 18:21:42,804] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-945/tokenizer_config.json[0m
[32m[2022-09-16 18:21:42,804] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-945/special_tokens_map.json[0m
[32m[2022-09-16 18:21:53,104] [    INFO][0m - loss: 0.96120443, learning_rate: 2.4973544973544973e-06, global_step: 950, interval_runtime: 44.2744, interval_samples_per_second: 0.361, interval_steps_per_second: 0.226, epoch: 5.0265[0m
[32m[2022-09-16 18:22:01,001] [    INFO][0m - loss: 0.9729599, learning_rate: 2.4920634920634923e-06, global_step: 960, interval_runtime: 7.8969, interval_samples_per_second: 2.026, interval_steps_per_second: 1.266, epoch: 5.0794[0m
[32m[2022-09-16 18:22:08,890] [    INFO][0m - loss: 0.94026833, learning_rate: 2.486772486772487e-06, global_step: 970, interval_runtime: 7.889, interval_samples_per_second: 2.028, interval_steps_per_second: 1.268, epoch: 5.1323[0m
[32m[2022-09-16 18:22:16,783] [    INFO][0m - loss: 1.33045788, learning_rate: 2.4814814814814814e-06, global_step: 980, interval_runtime: 7.8937, interval_samples_per_second: 2.027, interval_steps_per_second: 1.267, epoch: 5.1852[0m
[32m[2022-09-16 18:22:24,680] [    INFO][0m - loss: 0.91823235, learning_rate: 2.476190476190476e-06, global_step: 990, interval_runtime: 7.8964, interval_samples_per_second: 2.026, interval_steps_per_second: 1.266, epoch: 5.2381[0m
[32m[2022-09-16 18:22:32,562] [    INFO][0m - loss: 1.0468091, learning_rate: 2.470899470899471e-06, global_step: 1000, interval_runtime: 7.8826, interval_samples_per_second: 2.03, interval_steps_per_second: 1.269, epoch: 5.291[0m
[32m[2022-09-16 18:22:40,480] [    INFO][0m - loss: 1.14677982, learning_rate: 2.4656084656084655e-06, global_step: 1010, interval_runtime: 7.9182, interval_samples_per_second: 2.021, interval_steps_per_second: 1.263, epoch: 5.3439[0m
[32m[2022-09-16 18:22:48,404] [    INFO][0m - loss: 0.91066933, learning_rate: 2.4603174603174605e-06, global_step: 1020, interval_runtime: 7.9229, interval_samples_per_second: 2.019, interval_steps_per_second: 1.262, epoch: 5.3968[0m
[32m[2022-09-16 18:22:56,316] [    INFO][0m - loss: 1.08730412, learning_rate: 2.455026455026455e-06, global_step: 1030, interval_runtime: 7.9131, interval_samples_per_second: 2.022, interval_steps_per_second: 1.264, epoch: 5.4497[0m
[32m[2022-09-16 18:23:05,056] [    INFO][0m - loss: 1.24609489, learning_rate: 2.4497354497354497e-06, global_step: 1040, interval_runtime: 7.8806, interval_samples_per_second: 2.03, interval_steps_per_second: 1.269, epoch: 5.5026[0m
[32m[2022-09-16 18:23:12,991] [    INFO][0m - loss: 1.27280397, learning_rate: 2.4444444444444442e-06, global_step: 1050, interval_runtime: 8.794, interval_samples_per_second: 1.819, interval_steps_per_second: 1.137, epoch: 5.5556[0m
[32m[2022-09-16 18:23:23,242] [    INFO][0m - loss: 1.42394218, learning_rate: 2.4391534391534392e-06, global_step: 1060, interval_runtime: 7.9225, interval_samples_per_second: 2.02, interval_steps_per_second: 1.262, epoch: 5.6085[0m
[32m[2022-09-16 18:23:34,487] [    INFO][0m - loss: 1.07257252, learning_rate: 2.433862433862434e-06, global_step: 1070, interval_runtime: 10.2075, interval_samples_per_second: 1.567, interval_steps_per_second: 0.98, epoch: 5.6614[0m
[32m[2022-09-16 18:23:42,385] [    INFO][0m - loss: 1.35322561, learning_rate: 2.428571428571429e-06, global_step: 1080, interval_runtime: 11.2636, interval_samples_per_second: 1.421, interval_steps_per_second: 0.888, epoch: 5.7143[0m
[32m[2022-09-16 18:23:52,466] [    INFO][0m - loss: 1.1201252, learning_rate: 2.4232804232804234e-06, global_step: 1090, interval_runtime: 7.9118, interval_samples_per_second: 2.022, interval_steps_per_second: 1.264, epoch: 5.7672[0m
[32m[2022-09-16 18:24:00,395] [    INFO][0m - loss: 1.04121237, learning_rate: 2.417989417989418e-06, global_step: 1100, interval_runtime: 10.0982, interval_samples_per_second: 1.584, interval_steps_per_second: 0.99, epoch: 5.8201[0m
[32m[2022-09-16 18:24:08,366] [    INFO][0m - loss: 1.09584799, learning_rate: 2.4126984126984125e-06, global_step: 1110, interval_runtime: 7.9712, interval_samples_per_second: 2.007, interval_steps_per_second: 1.255, epoch: 5.873[0m
[32m[2022-09-16 18:24:16,385] [    INFO][0m - loss: 1.0106637, learning_rate: 2.4074074074074075e-06, global_step: 1120, interval_runtime: 8.0183, interval_samples_per_second: 1.995, interval_steps_per_second: 1.247, epoch: 5.9259[0m
[32m[2022-09-16 18:24:24,336] [    INFO][0m - loss: 1.07367287, learning_rate: 2.402116402116402e-06, global_step: 1130, interval_runtime: 7.9513, interval_samples_per_second: 2.012, interval_steps_per_second: 1.258, epoch: 5.9788[0m
[32m[2022-09-16 18:24:27,349] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 18:24:27,350] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-16 18:24:27,350] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 18:24:27,350] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 18:24:27,350] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-16 18:24:55,163] [    INFO][0m - eval_loss: 1.959208607673645, eval_accuracy: 0.47050254916241807, eval_runtime: 27.8132, eval_samples_per_second: 49.365, eval_steps_per_second: 3.092, epoch: 6.0[0m
[32m[2022-09-16 18:24:55,189] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-1134[0m
[32m[2022-09-16 18:24:55,189] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 18:24:58,420] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-1134/tokenizer_config.json[0m
[32m[2022-09-16 18:24:58,421] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-1134/special_tokens_map.json[0m
[32m[2022-09-16 18:25:12,844] [    INFO][0m - loss: 0.99783154, learning_rate: 2.396825396825397e-06, global_step: 1140, interval_runtime: 48.5087, interval_samples_per_second: 0.33, interval_steps_per_second: 0.206, epoch: 6.0317[0m
[32m[2022-09-16 18:25:21,005] [    INFO][0m - loss: 0.9878232, learning_rate: 2.3915343915343916e-06, global_step: 1150, interval_runtime: 7.932, interval_samples_per_second: 2.017, interval_steps_per_second: 1.261, epoch: 6.0847[0m
[32m[2022-09-16 18:25:28,988] [    INFO][0m - loss: 0.8630497, learning_rate: 2.386243386243386e-06, global_step: 1160, interval_runtime: 8.2109, interval_samples_per_second: 1.949, interval_steps_per_second: 1.218, epoch: 6.1376[0m
[32m[2022-09-16 18:25:36,952] [    INFO][0m - loss: 1.15481834, learning_rate: 2.3809523809523808e-06, global_step: 1170, interval_runtime: 7.9643, interval_samples_per_second: 2.009, interval_steps_per_second: 1.256, epoch: 6.1905[0m
[32m[2022-09-16 18:25:44,921] [    INFO][0m - loss: 0.84581146, learning_rate: 2.3756613756613758e-06, global_step: 1180, interval_runtime: 7.9694, interval_samples_per_second: 2.008, interval_steps_per_second: 1.255, epoch: 6.2434[0m
[32m[2022-09-16 18:25:52,958] [    INFO][0m - loss: 0.9311986, learning_rate: 2.3703703703703703e-06, global_step: 1190, interval_runtime: 8.0373, interval_samples_per_second: 1.991, interval_steps_per_second: 1.244, epoch: 6.2963[0m
[32m[2022-09-16 18:26:00,871] [    INFO][0m - loss: 0.91005125, learning_rate: 2.3650793650793653e-06, global_step: 1200, interval_runtime: 7.9125, interval_samples_per_second: 2.022, interval_steps_per_second: 1.264, epoch: 6.3492[0m
[32m[2022-09-16 18:26:08,939] [    INFO][0m - loss: 0.79979806, learning_rate: 2.35978835978836e-06, global_step: 1210, interval_runtime: 8.0688, interval_samples_per_second: 1.983, interval_steps_per_second: 1.239, epoch: 6.4021[0m
[32m[2022-09-16 18:26:17,147] [    INFO][0m - loss: 1.03006639, learning_rate: 2.3544973544973545e-06, global_step: 1220, interval_runtime: 7.9152, interval_samples_per_second: 2.021, interval_steps_per_second: 1.263, epoch: 6.455[0m
[32m[2022-09-16 18:26:25,047] [    INFO][0m - loss: 1.07517672, learning_rate: 2.349206349206349e-06, global_step: 1230, interval_runtime: 8.1926, interval_samples_per_second: 1.953, interval_steps_per_second: 1.221, epoch: 6.5079[0m
[32m[2022-09-16 18:26:32,930] [    INFO][0m - loss: 1.04061689, learning_rate: 2.343915343915344e-06, global_step: 1240, interval_runtime: 7.8823, interval_samples_per_second: 2.03, interval_steps_per_second: 1.269, epoch: 6.5608[0m
[32m[2022-09-16 18:26:40,857] [    INFO][0m - loss: 1.12406979, learning_rate: 2.3386243386243386e-06, global_step: 1250, interval_runtime: 7.9267, interval_samples_per_second: 2.018, interval_steps_per_second: 1.262, epoch: 6.6138[0m
[32m[2022-09-16 18:26:48,929] [    INFO][0m - loss: 0.97177696, learning_rate: 2.3333333333333336e-06, global_step: 1260, interval_runtime: 7.8874, interval_samples_per_second: 2.029, interval_steps_per_second: 1.268, epoch: 6.6667[0m
[32m[2022-09-16 18:26:56,907] [    INFO][0m - loss: 0.98379784, learning_rate: 2.328042328042328e-06, global_step: 1270, interval_runtime: 8.1627, interval_samples_per_second: 1.96, interval_steps_per_second: 1.225, epoch: 6.7196[0m
[32m[2022-09-16 18:27:04,868] [    INFO][0m - loss: 0.97551537, learning_rate: 2.3227513227513227e-06, global_step: 1280, interval_runtime: 7.9612, interval_samples_per_second: 2.01, interval_steps_per_second: 1.256, epoch: 6.7725[0m
[32m[2022-09-16 18:27:12,780] [    INFO][0m - loss: 1.05558281, learning_rate: 2.3174603174603173e-06, global_step: 1290, interval_runtime: 7.9123, interval_samples_per_second: 2.022, interval_steps_per_second: 1.264, epoch: 6.8254[0m
[32m[2022-09-16 18:27:20,778] [    INFO][0m - loss: 1.00347118, learning_rate: 2.3121693121693123e-06, global_step: 1300, interval_runtime: 7.9978, interval_samples_per_second: 2.001, interval_steps_per_second: 1.25, epoch: 6.8783[0m
[32m[2022-09-16 18:27:28,704] [    INFO][0m - loss: 0.7792542, learning_rate: 2.306878306878307e-06, global_step: 1310, interval_runtime: 7.9256, interval_samples_per_second: 2.019, interval_steps_per_second: 1.262, epoch: 6.9312[0m
[32m[2022-09-16 18:27:36,530] [    INFO][0m - loss: 1.03796883, learning_rate: 2.301587301587302e-06, global_step: 1320, interval_runtime: 7.8263, interval_samples_per_second: 2.044, interval_steps_per_second: 1.278, epoch: 6.9841[0m
[32m[2022-09-16 18:27:38,786] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 18:27:38,786] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-16 18:27:38,786] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 18:27:38,786] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 18:27:38,787] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-16 18:28:06,220] [    INFO][0m - eval_loss: 1.9410825967788696, eval_accuracy: 0.4588492352512746, eval_runtime: 27.4333, eval_samples_per_second: 50.049, eval_steps_per_second: 3.135, epoch: 7.0[0m
[32m[2022-09-16 18:28:06,244] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-1323[0m
[32m[2022-09-16 18:28:06,244] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 18:28:09,325] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-1323/tokenizer_config.json[0m
[32m[2022-09-16 18:28:09,325] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-1323/special_tokens_map.json[0m
[32m[2022-09-16 18:28:22,156] [    INFO][0m - loss: 0.90116825, learning_rate: 2.2962962962962964e-06, global_step: 1330, interval_runtime: 45.6262, interval_samples_per_second: 0.351, interval_steps_per_second: 0.219, epoch: 7.037[0m
[32m[2022-09-16 18:28:29,985] [    INFO][0m - loss: 0.80512524, learning_rate: 2.291005291005291e-06, global_step: 1340, interval_runtime: 7.829, interval_samples_per_second: 2.044, interval_steps_per_second: 1.277, epoch: 7.0899[0m
[32m[2022-09-16 18:28:37,873] [    INFO][0m - loss: 0.75405207, learning_rate: 2.2857142857142856e-06, global_step: 1350, interval_runtime: 7.8878, interval_samples_per_second: 2.028, interval_steps_per_second: 1.268, epoch: 7.1429[0m
[32m[2022-09-16 18:28:45,709] [    INFO][0m - loss: 0.87873173, learning_rate: 2.2804232804232805e-06, global_step: 1360, interval_runtime: 7.8358, interval_samples_per_second: 2.042, interval_steps_per_second: 1.276, epoch: 7.1958[0m
[32m[2022-09-16 18:28:53,569] [    INFO][0m - loss: 0.96167402, learning_rate: 2.275132275132275e-06, global_step: 1370, interval_runtime: 7.8606, interval_samples_per_second: 2.035, interval_steps_per_second: 1.272, epoch: 7.2487[0m
[32m[2022-09-16 18:29:01,464] [    INFO][0m - loss: 0.9542614, learning_rate: 2.26984126984127e-06, global_step: 1380, interval_runtime: 7.8948, interval_samples_per_second: 2.027, interval_steps_per_second: 1.267, epoch: 7.3016[0m
[32m[2022-09-16 18:29:09,350] [    INFO][0m - loss: 0.96385899, learning_rate: 2.2645502645502647e-06, global_step: 1390, interval_runtime: 7.8862, interval_samples_per_second: 2.029, interval_steps_per_second: 1.268, epoch: 7.3545[0m
[32m[2022-09-16 18:29:17,231] [    INFO][0m - loss: 0.82544708, learning_rate: 2.2592592592592592e-06, global_step: 1400, interval_runtime: 7.881, interval_samples_per_second: 2.03, interval_steps_per_second: 1.269, epoch: 7.4074[0m
[32m[2022-09-16 18:29:25,134] [    INFO][0m - loss: 0.84515457, learning_rate: 2.253968253968254e-06, global_step: 1410, interval_runtime: 7.9025, interval_samples_per_second: 2.025, interval_steps_per_second: 1.265, epoch: 7.4603[0m
[32m[2022-09-16 18:29:33,066] [    INFO][0m - loss: 0.75882268, learning_rate: 2.248677248677249e-06, global_step: 1420, interval_runtime: 7.9327, interval_samples_per_second: 2.017, interval_steps_per_second: 1.261, epoch: 7.5132[0m
[32m[2022-09-16 18:29:40,971] [    INFO][0m - loss: 0.85022945, learning_rate: 2.2433862433862434e-06, global_step: 1430, interval_runtime: 7.9044, interval_samples_per_second: 2.024, interval_steps_per_second: 1.265, epoch: 7.5661[0m
[32m[2022-09-16 18:29:48,854] [    INFO][0m - loss: 0.85370941, learning_rate: 2.2380952380952384e-06, global_step: 1440, interval_runtime: 7.8826, interval_samples_per_second: 2.03, interval_steps_per_second: 1.269, epoch: 7.619[0m
[32m[2022-09-16 18:29:56,769] [    INFO][0m - loss: 0.89217548, learning_rate: 2.232804232804233e-06, global_step: 1450, interval_runtime: 7.9157, interval_samples_per_second: 2.021, interval_steps_per_second: 1.263, epoch: 7.672[0m
[32m[2022-09-16 18:30:04,667] [    INFO][0m - loss: 0.92783155, learning_rate: 2.2275132275132275e-06, global_step: 1460, interval_runtime: 7.8976, interval_samples_per_second: 2.026, interval_steps_per_second: 1.266, epoch: 7.7249[0m
[32m[2022-09-16 18:30:12,566] [    INFO][0m - loss: 0.86016235, learning_rate: 2.222222222222222e-06, global_step: 1470, interval_runtime: 7.8996, interval_samples_per_second: 2.025, interval_steps_per_second: 1.266, epoch: 7.7778[0m
[32m[2022-09-16 18:30:20,484] [    INFO][0m - loss: 0.78761749, learning_rate: 2.216931216931217e-06, global_step: 1480, interval_runtime: 7.9172, interval_samples_per_second: 2.021, interval_steps_per_second: 1.263, epoch: 7.8307[0m
[32m[2022-09-16 18:30:28,414] [    INFO][0m - loss: 0.81551046, learning_rate: 2.2116402116402116e-06, global_step: 1490, interval_runtime: 7.9309, interval_samples_per_second: 2.017, interval_steps_per_second: 1.261, epoch: 7.8836[0m
[32m[2022-09-16 18:30:36,291] [    INFO][0m - loss: 1.06715899, learning_rate: 2.2063492063492066e-06, global_step: 1500, interval_runtime: 7.8767, interval_samples_per_second: 2.031, interval_steps_per_second: 1.27, epoch: 7.9365[0m
[32m[2022-09-16 18:30:44,091] [    INFO][0m - loss: 0.93759203, learning_rate: 2.201058201058201e-06, global_step: 1510, interval_runtime: 7.7997, interval_samples_per_second: 2.051, interval_steps_per_second: 1.282, epoch: 7.9894[0m
[32m[2022-09-16 18:30:45,590] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 18:30:45,590] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-16 18:30:45,591] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 18:30:45,591] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 18:30:45,591] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-16 18:31:12,987] [    INFO][0m - eval_loss: 2.061413526535034, eval_accuracy: 0.4646758922068463, eval_runtime: 27.3957, eval_samples_per_second: 50.117, eval_steps_per_second: 3.139, epoch: 8.0[0m
[32m[2022-09-16 18:31:13,016] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-1512[0m
[32m[2022-09-16 18:31:13,016] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 18:31:16,393] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-1512/tokenizer_config.json[0m
[32m[2022-09-16 18:31:16,394] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-1512/special_tokens_map.json[0m
[32m[2022-09-16 18:31:30,519] [    INFO][0m - loss: 0.64608569, learning_rate: 2.1957671957671958e-06, global_step: 1520, interval_runtime: 46.4278, interval_samples_per_second: 0.345, interval_steps_per_second: 0.215, epoch: 8.0423[0m
[32m[2022-09-16 18:31:38,421] [    INFO][0m - loss: 0.72807698, learning_rate: 2.1904761904761903e-06, global_step: 1530, interval_runtime: 7.9019, interval_samples_per_second: 2.025, interval_steps_per_second: 1.266, epoch: 8.0952[0m
[32m[2022-09-16 18:31:46,281] [    INFO][0m - loss: 0.7322104, learning_rate: 2.1851851851851853e-06, global_step: 1540, interval_runtime: 7.8605, interval_samples_per_second: 2.035, interval_steps_per_second: 1.272, epoch: 8.1481[0m
[32m[2022-09-16 18:31:54,170] [    INFO][0m - loss: 0.708111, learning_rate: 2.17989417989418e-06, global_step: 1550, interval_runtime: 7.8883, interval_samples_per_second: 2.028, interval_steps_per_second: 1.268, epoch: 8.2011[0m
[32m[2022-09-16 18:32:02,008] [    INFO][0m - loss: 0.63161187, learning_rate: 2.174603174603175e-06, global_step: 1560, interval_runtime: 7.8386, interval_samples_per_second: 2.041, interval_steps_per_second: 1.276, epoch: 8.254[0m
[32m[2022-09-16 18:32:09,883] [    INFO][0m - loss: 0.74592781, learning_rate: 2.169312169312169e-06, global_step: 1570, interval_runtime: 7.874, interval_samples_per_second: 2.032, interval_steps_per_second: 1.27, epoch: 8.3069[0m
[32m[2022-09-16 18:32:17,799] [    INFO][0m - loss: 0.79539018, learning_rate: 2.164021164021164e-06, global_step: 1580, interval_runtime: 7.9159, interval_samples_per_second: 2.021, interval_steps_per_second: 1.263, epoch: 8.3598[0m
[32m[2022-09-16 18:32:25,705] [    INFO][0m - loss: 0.70044351, learning_rate: 2.1587301587301586e-06, global_step: 1590, interval_runtime: 7.9065, interval_samples_per_second: 2.024, interval_steps_per_second: 1.265, epoch: 8.4127[0m
[32m[2022-09-16 18:32:33,573] [    INFO][0m - loss: 0.80620184, learning_rate: 2.1534391534391536e-06, global_step: 1600, interval_runtime: 7.8683, interval_samples_per_second: 2.033, interval_steps_per_second: 1.271, epoch: 8.4656[0m
[32m[2022-09-16 18:32:41,477] [    INFO][0m - loss: 0.72899389, learning_rate: 2.148148148148148e-06, global_step: 1610, interval_runtime: 7.9042, interval_samples_per_second: 2.024, interval_steps_per_second: 1.265, epoch: 8.5185[0m
[32m[2022-09-16 18:32:49,364] [    INFO][0m - loss: 0.74384179, learning_rate: 2.142857142857143e-06, global_step: 1620, interval_runtime: 7.8866, interval_samples_per_second: 2.029, interval_steps_per_second: 1.268, epoch: 8.5714[0m
[32m[2022-09-16 18:32:57,276] [    INFO][0m - loss: 0.76850872, learning_rate: 2.1375661375661373e-06, global_step: 1630, interval_runtime: 7.9118, interval_samples_per_second: 2.022, interval_steps_per_second: 1.264, epoch: 8.6243[0m
[32m[2022-09-16 18:33:05,169] [    INFO][0m - loss: 0.91472702, learning_rate: 2.1322751322751323e-06, global_step: 1640, interval_runtime: 7.8935, interval_samples_per_second: 2.027, interval_steps_per_second: 1.267, epoch: 8.6772[0m
[32m[2022-09-16 18:33:13,065] [    INFO][0m - loss: 0.77424107, learning_rate: 2.126984126984127e-06, global_step: 1650, interval_runtime: 7.8954, interval_samples_per_second: 2.027, interval_steps_per_second: 1.267, epoch: 8.7302[0m
[32m[2022-09-16 18:33:20,963] [    INFO][0m - loss: 0.90233841, learning_rate: 2.121693121693122e-06, global_step: 1660, interval_runtime: 7.8983, interval_samples_per_second: 2.026, interval_steps_per_second: 1.266, epoch: 8.7831[0m
[32m[2022-09-16 18:33:28,879] [    INFO][0m - loss: 0.61462808, learning_rate: 2.1164021164021164e-06, global_step: 1670, interval_runtime: 7.9161, interval_samples_per_second: 2.021, interval_steps_per_second: 1.263, epoch: 8.836[0m
[32m[2022-09-16 18:33:36,776] [    INFO][0m - loss: 0.71861224, learning_rate: 2.1111111111111114e-06, global_step: 1680, interval_runtime: 7.8972, interval_samples_per_second: 2.026, interval_steps_per_second: 1.266, epoch: 8.8889[0m
[32m[2022-09-16 18:33:44,698] [    INFO][0m - loss: 0.7244626, learning_rate: 2.1058201058201056e-06, global_step: 1690, interval_runtime: 7.9217, interval_samples_per_second: 2.02, interval_steps_per_second: 1.262, epoch: 8.9418[0m
[32m[2022-09-16 18:33:52,458] [    INFO][0m - loss: 0.64851174, learning_rate: 2.1005291005291006e-06, global_step: 1700, interval_runtime: 7.76, interval_samples_per_second: 2.062, interval_steps_per_second: 1.289, epoch: 8.9947[0m
[32m[2022-09-16 18:33:53,205] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 18:33:53,205] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-16 18:33:53,205] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 18:33:53,205] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 18:33:53,205] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-16 18:34:20,425] [    INFO][0m - eval_loss: 2.1283762454986572, eval_accuracy: 0.46394756008739985, eval_runtime: 27.22, eval_samples_per_second: 50.441, eval_steps_per_second: 3.159, epoch: 9.0[0m
[32m[2022-09-16 18:34:20,449] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-1701[0m
[32m[2022-09-16 18:34:20,449] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 18:34:23,393] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-1701/tokenizer_config.json[0m
[32m[2022-09-16 18:34:23,393] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-1701/special_tokens_map.json[0m
[32m[2022-09-16 18:34:36,171] [    INFO][0m - loss: 0.65843153, learning_rate: 2.095238095238095e-06, global_step: 1710, interval_runtime: 43.7129, interval_samples_per_second: 0.366, interval_steps_per_second: 0.229, epoch: 9.0476[0m
[32m[2022-09-16 18:34:44,033] [    INFO][0m - loss: 0.49137888, learning_rate: 2.08994708994709e-06, global_step: 1720, interval_runtime: 7.8619, interval_samples_per_second: 2.035, interval_steps_per_second: 1.272, epoch: 9.1005[0m
[32m[2022-09-16 18:34:51,869] [    INFO][0m - loss: 0.83909712, learning_rate: 2.0846560846560847e-06, global_step: 1730, interval_runtime: 7.8357, interval_samples_per_second: 2.042, interval_steps_per_second: 1.276, epoch: 9.1534[0m
[32m[2022-09-16 18:34:59,762] [    INFO][0m - loss: 0.67002106, learning_rate: 2.0793650793650797e-06, global_step: 1740, interval_runtime: 7.8931, interval_samples_per_second: 2.027, interval_steps_per_second: 1.267, epoch: 9.2063[0m
[32m[2022-09-16 18:35:07,679] [    INFO][0m - loss: 0.65005322, learning_rate: 2.074074074074074e-06, global_step: 1750, interval_runtime: 7.9173, interval_samples_per_second: 2.021, interval_steps_per_second: 1.263, epoch: 9.2593[0m
[32m[2022-09-16 18:35:15,587] [    INFO][0m - loss: 0.67088461, learning_rate: 2.068783068783069e-06, global_step: 1760, interval_runtime: 7.9077, interval_samples_per_second: 2.023, interval_steps_per_second: 1.265, epoch: 9.3122[0m
[32m[2022-09-16 18:35:23,508] [    INFO][0m - loss: 0.60682516, learning_rate: 2.0634920634920634e-06, global_step: 1770, interval_runtime: 7.9205, interval_samples_per_second: 2.02, interval_steps_per_second: 1.263, epoch: 9.3651[0m
[32m[2022-09-16 18:35:31,423] [    INFO][0m - loss: 0.69861555, learning_rate: 2.0582010582010584e-06, global_step: 1780, interval_runtime: 7.9153, interval_samples_per_second: 2.021, interval_steps_per_second: 1.263, epoch: 9.418[0m
[32m[2022-09-16 18:35:39,359] [    INFO][0m - loss: 0.73097034, learning_rate: 2.052910052910053e-06, global_step: 1790, interval_runtime: 7.9361, interval_samples_per_second: 2.016, interval_steps_per_second: 1.26, epoch: 9.4709[0m
[32m[2022-09-16 18:35:47,263] [    INFO][0m - loss: 0.72421679, learning_rate: 2.0476190476190475e-06, global_step: 1800, interval_runtime: 7.9039, interval_samples_per_second: 2.024, interval_steps_per_second: 1.265, epoch: 9.5238[0m
[32m[2022-09-16 18:35:55,152] [    INFO][0m - loss: 0.54051657, learning_rate: 2.042328042328042e-06, global_step: 1810, interval_runtime: 7.8893, interval_samples_per_second: 2.028, interval_steps_per_second: 1.268, epoch: 9.5767[0m
[32m[2022-09-16 18:36:03,045] [    INFO][0m - loss: 0.67287374, learning_rate: 2.037037037037037e-06, global_step: 1820, interval_runtime: 7.8926, interval_samples_per_second: 2.027, interval_steps_per_second: 1.267, epoch: 9.6296[0m
[32m[2022-09-16 18:36:10,935] [    INFO][0m - loss: 0.68478651, learning_rate: 2.0317460317460316e-06, global_step: 1830, interval_runtime: 7.8907, interval_samples_per_second: 2.028, interval_steps_per_second: 1.267, epoch: 9.6825[0m
[32m[2022-09-16 18:36:18,830] [    INFO][0m - loss: 0.57336965, learning_rate: 2.0264550264550266e-06, global_step: 1840, interval_runtime: 7.8943, interval_samples_per_second: 2.027, interval_steps_per_second: 1.267, epoch: 9.7354[0m
[32m[2022-09-16 18:36:26,750] [    INFO][0m - loss: 0.75269823, learning_rate: 2.021164021164021e-06, global_step: 1850, interval_runtime: 7.9202, interval_samples_per_second: 2.02, interval_steps_per_second: 1.263, epoch: 9.7884[0m
[32m[2022-09-16 18:36:34,670] [    INFO][0m - loss: 0.41276455, learning_rate: 2.0158730158730158e-06, global_step: 1860, interval_runtime: 7.9201, interval_samples_per_second: 2.02, interval_steps_per_second: 1.263, epoch: 9.8413[0m
[32m[2022-09-16 18:36:42,609] [    INFO][0m - loss: 0.53154221, learning_rate: 2.0105820105820103e-06, global_step: 1870, interval_runtime: 7.939, interval_samples_per_second: 2.015, interval_steps_per_second: 1.26, epoch: 9.8942[0m
[32m[2022-09-16 18:36:50,548] [    INFO][0m - loss: 0.67915778, learning_rate: 2.0052910052910053e-06, global_step: 1880, interval_runtime: 7.9394, interval_samples_per_second: 2.015, interval_steps_per_second: 1.26, epoch: 9.9471[0m
[32m[2022-09-16 18:36:58,603] [    INFO][0m - loss: 0.62476373, learning_rate: 2e-06, global_step: 1890, interval_runtime: 7.7172, interval_samples_per_second: 2.073, interval_steps_per_second: 1.296, epoch: 10.0[0m
[32m[2022-09-16 18:36:58,604] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 18:36:58,604] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-16 18:36:58,604] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 18:36:58,604] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 18:36:58,604] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-16 18:37:25,850] [    INFO][0m - eval_loss: 2.2672672271728516, eval_accuracy: 0.46176256372906044, eval_runtime: 27.2453, eval_samples_per_second: 50.394, eval_steps_per_second: 3.157, epoch: 10.0[0m
[32m[2022-09-16 18:37:25,872] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-1890[0m
[32m[2022-09-16 18:37:25,872] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 18:37:28,773] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-1890/tokenizer_config.json[0m
[32m[2022-09-16 18:37:28,773] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-1890/special_tokens_map.json[0m
[32m[2022-09-16 18:37:42,523] [    INFO][0m - loss: 0.51295466, learning_rate: 1.994708994708995e-06, global_step: 1900, interval_runtime: 44.2571, interval_samples_per_second: 0.362, interval_steps_per_second: 0.226, epoch: 10.0529[0m
[32m[2022-09-16 18:37:50,407] [    INFO][0m - loss: 0.44673409, learning_rate: 1.9894179894179895e-06, global_step: 1910, interval_runtime: 7.8844, interval_samples_per_second: 2.029, interval_steps_per_second: 1.268, epoch: 10.1058[0m
[32m[2022-09-16 18:37:58,297] [    INFO][0m - loss: 0.44832997, learning_rate: 1.984126984126984e-06, global_step: 1920, interval_runtime: 7.8901, interval_samples_per_second: 2.028, interval_steps_per_second: 1.267, epoch: 10.1587[0m
[32m[2022-09-16 18:38:06,223] [    INFO][0m - loss: 0.61342888, learning_rate: 1.9788359788359786e-06, global_step: 1930, interval_runtime: 7.9257, interval_samples_per_second: 2.019, interval_steps_per_second: 1.262, epoch: 10.2116[0m
[32m[2022-09-16 18:38:14,139] [    INFO][0m - loss: 0.67148094, learning_rate: 1.9735449735449736e-06, global_step: 1940, interval_runtime: 7.9164, interval_samples_per_second: 2.021, interval_steps_per_second: 1.263, epoch: 10.2646[0m
[32m[2022-09-16 18:38:22,070] [    INFO][0m - loss: 0.45952516, learning_rate: 1.968253968253968e-06, global_step: 1950, interval_runtime: 7.9303, interval_samples_per_second: 2.018, interval_steps_per_second: 1.261, epoch: 10.3175[0m
[32m[2022-09-16 18:38:29,962] [    INFO][0m - loss: 0.65366826, learning_rate: 1.962962962962963e-06, global_step: 1960, interval_runtime: 7.8917, interval_samples_per_second: 2.027, interval_steps_per_second: 1.267, epoch: 10.3704[0m
[32m[2022-09-16 18:38:37,987] [    INFO][0m - loss: 0.5096406, learning_rate: 1.9576719576719577e-06, global_step: 1970, interval_runtime: 8.0252, interval_samples_per_second: 1.994, interval_steps_per_second: 1.246, epoch: 10.4233[0m
[32m[2022-09-16 18:38:45,889] [    INFO][0m - loss: 0.51435976, learning_rate: 1.9523809523809523e-06, global_step: 1980, interval_runtime: 7.9026, interval_samples_per_second: 2.025, interval_steps_per_second: 1.265, epoch: 10.4762[0m
[32m[2022-09-16 18:38:53,805] [    INFO][0m - loss: 0.52302461, learning_rate: 1.947089947089947e-06, global_step: 1990, interval_runtime: 7.9155, interval_samples_per_second: 2.021, interval_steps_per_second: 1.263, epoch: 10.5291[0m
[32m[2022-09-16 18:39:01,728] [    INFO][0m - loss: 0.49101772, learning_rate: 1.941798941798942e-06, global_step: 2000, interval_runtime: 7.9227, interval_samples_per_second: 2.02, interval_steps_per_second: 1.262, epoch: 10.582[0m
[32m[2022-09-16 18:39:09,656] [    INFO][0m - loss: 0.62330685, learning_rate: 1.9365079365079364e-06, global_step: 2010, interval_runtime: 7.9284, interval_samples_per_second: 2.018, interval_steps_per_second: 1.261, epoch: 10.6349[0m
[32m[2022-09-16 18:39:17,551] [    INFO][0m - loss: 0.51055856, learning_rate: 1.9312169312169314e-06, global_step: 2020, interval_runtime: 7.8947, interval_samples_per_second: 2.027, interval_steps_per_second: 1.267, epoch: 10.6878[0m
[32m[2022-09-16 18:39:25,446] [    INFO][0m - loss: 0.57805128, learning_rate: 1.925925925925926e-06, global_step: 2030, interval_runtime: 7.8955, interval_samples_per_second: 2.026, interval_steps_per_second: 1.267, epoch: 10.7407[0m
[32m[2022-09-16 18:39:33,353] [    INFO][0m - loss: 0.65087762, learning_rate: 1.9206349206349206e-06, global_step: 2040, interval_runtime: 7.9067, interval_samples_per_second: 2.024, interval_steps_per_second: 1.265, epoch: 10.7937[0m
[32m[2022-09-16 18:39:41,237] [    INFO][0m - loss: 0.59257307, learning_rate: 1.915343915343915e-06, global_step: 2050, interval_runtime: 7.8843, interval_samples_per_second: 2.029, interval_steps_per_second: 1.268, epoch: 10.8466[0m
[32m[2022-09-16 18:39:49,096] [    INFO][0m - loss: 0.55397358, learning_rate: 1.91005291005291e-06, global_step: 2060, interval_runtime: 7.8586, interval_samples_per_second: 2.036, interval_steps_per_second: 1.272, epoch: 10.8995[0m
[32m[2022-09-16 18:39:57,012] [    INFO][0m - loss: 0.54209223, learning_rate: 1.9047619047619047e-06, global_step: 2070, interval_runtime: 7.9167, interval_samples_per_second: 2.021, interval_steps_per_second: 1.263, epoch: 10.9524[0m
[32m[2022-09-16 18:40:03,914] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 18:40:03,915] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-16 18:40:03,915] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 18:40:03,915] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 18:40:03,915] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-16 18:40:31,092] [    INFO][0m - eval_loss: 2.3577797412872314, eval_accuracy: 0.45520757465404227, eval_runtime: 27.1766, eval_samples_per_second: 50.521, eval_steps_per_second: 3.164, epoch: 11.0[0m
[32m[2022-09-16 18:40:31,115] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-2079[0m
[32m[2022-09-16 18:40:31,116] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 18:40:33,963] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-2079/tokenizer_config.json[0m
[32m[2022-09-16 18:40:33,964] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-2079/special_tokens_map.json[0m
[32m[2022-09-16 18:40:40,300] [    INFO][0m - loss: 0.579916, learning_rate: 1.8994708994708995e-06, global_step: 2080, interval_runtime: 43.2878, interval_samples_per_second: 0.37, interval_steps_per_second: 0.231, epoch: 11.0053[0m
[32m[2022-09-16 18:40:48,148] [    INFO][0m - loss: 0.29548612, learning_rate: 1.894179894179894e-06, global_step: 2090, interval_runtime: 7.8481, interval_samples_per_second: 2.039, interval_steps_per_second: 1.274, epoch: 11.0582[0m
[32m[2022-09-16 18:40:56,044] [    INFO][0m - loss: 0.45278492, learning_rate: 1.888888888888889e-06, global_step: 2100, interval_runtime: 7.8955, interval_samples_per_second: 2.026, interval_steps_per_second: 1.267, epoch: 11.1111[0m
[32m[2022-09-16 18:41:03,965] [    INFO][0m - loss: 0.55317569, learning_rate: 1.8835978835978836e-06, global_step: 2110, interval_runtime: 7.9212, interval_samples_per_second: 2.02, interval_steps_per_second: 1.262, epoch: 11.164[0m
[32m[2022-09-16 18:41:11,841] [    INFO][0m - loss: 0.30413282, learning_rate: 1.8783068783068784e-06, global_step: 2120, interval_runtime: 7.8759, interval_samples_per_second: 2.032, interval_steps_per_second: 1.27, epoch: 11.2169[0m
[32m[2022-09-16 18:41:19,771] [    INFO][0m - loss: 0.40789599, learning_rate: 1.873015873015873e-06, global_step: 2130, interval_runtime: 7.9297, interval_samples_per_second: 2.018, interval_steps_per_second: 1.261, epoch: 11.2698[0m
[32m[2022-09-16 18:41:27,670] [    INFO][0m - loss: 0.53399472, learning_rate: 1.8677248677248677e-06, global_step: 2140, interval_runtime: 7.8994, interval_samples_per_second: 2.025, interval_steps_per_second: 1.266, epoch: 11.3228[0m
[32m[2022-09-16 18:41:35,612] [    INFO][0m - loss: 0.51688952, learning_rate: 1.8624338624338623e-06, global_step: 2150, interval_runtime: 7.9414, interval_samples_per_second: 2.015, interval_steps_per_second: 1.259, epoch: 11.3757[0m
[32m[2022-09-16 18:41:43,531] [    INFO][0m - loss: 0.54267397, learning_rate: 1.8571428571428573e-06, global_step: 2160, interval_runtime: 7.9195, interval_samples_per_second: 2.02, interval_steps_per_second: 1.263, epoch: 11.4286[0m
[32m[2022-09-16 18:41:51,453] [    INFO][0m - loss: 0.47913203, learning_rate: 1.8518518518518519e-06, global_step: 2170, interval_runtime: 7.9216, interval_samples_per_second: 2.02, interval_steps_per_second: 1.262, epoch: 11.4815[0m
[32m[2022-09-16 18:41:59,355] [    INFO][0m - loss: 0.47862778, learning_rate: 1.8465608465608467e-06, global_step: 2180, interval_runtime: 7.9024, interval_samples_per_second: 2.025, interval_steps_per_second: 1.265, epoch: 11.5344[0m
[32m[2022-09-16 18:42:07,279] [    INFO][0m - loss: 0.40593033, learning_rate: 1.8412698412698412e-06, global_step: 2190, interval_runtime: 7.9241, interval_samples_per_second: 2.019, interval_steps_per_second: 1.262, epoch: 11.5873[0m
[32m[2022-09-16 18:42:15,161] [    INFO][0m - loss: 0.36787841, learning_rate: 1.835978835978836e-06, global_step: 2200, interval_runtime: 7.8825, interval_samples_per_second: 2.03, interval_steps_per_second: 1.269, epoch: 11.6402[0m
[32m[2022-09-16 18:42:23,053] [    INFO][0m - loss: 0.59322433, learning_rate: 1.8306878306878306e-06, global_step: 2210, interval_runtime: 7.8919, interval_samples_per_second: 2.027, interval_steps_per_second: 1.267, epoch: 11.6931[0m
[32m[2022-09-16 18:42:30,948] [    INFO][0m - loss: 0.45422549, learning_rate: 1.8253968253968256e-06, global_step: 2220, interval_runtime: 7.8943, interval_samples_per_second: 2.027, interval_steps_per_second: 1.267, epoch: 11.746[0m
[32m[2022-09-16 18:42:38,865] [    INFO][0m - loss: 0.48064418, learning_rate: 1.8201058201058201e-06, global_step: 2230, interval_runtime: 7.9168, interval_samples_per_second: 2.021, interval_steps_per_second: 1.263, epoch: 11.7989[0m
[32m[2022-09-16 18:42:46,764] [    INFO][0m - loss: 0.5611444, learning_rate: 1.814814814814815e-06, global_step: 2240, interval_runtime: 7.8993, interval_samples_per_second: 2.026, interval_steps_per_second: 1.266, epoch: 11.8519[0m
[32m[2022-09-16 18:42:54,622] [    INFO][0m - loss: 0.4703227, learning_rate: 1.8095238095238095e-06, global_step: 2250, interval_runtime: 7.8577, interval_samples_per_second: 2.036, interval_steps_per_second: 1.273, epoch: 11.9048[0m
[32m[2022-09-16 18:43:02,514] [    INFO][0m - loss: 0.5544733, learning_rate: 1.8042328042328043e-06, global_step: 2260, interval_runtime: 7.8925, interval_samples_per_second: 2.027, interval_steps_per_second: 1.267, epoch: 11.9577[0m
[32m[2022-09-16 18:43:08,659] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 18:43:08,659] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-16 18:43:08,659] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 18:43:08,659] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 18:43:08,660] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-16 18:43:36,092] [    INFO][0m - eval_loss: 2.425096273422241, eval_accuracy: 0.4537509104151493, eval_runtime: 27.4321, eval_samples_per_second: 50.051, eval_steps_per_second: 3.135, epoch: 12.0[0m
[32m[2022-09-16 18:43:36,112] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-2268[0m
[32m[2022-09-16 18:43:36,112] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 18:43:38,971] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-2268/tokenizer_config.json[0m
[32m[2022-09-16 18:43:38,972] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-2268/special_tokens_map.json[0m
[32m[2022-09-16 18:43:45,921] [    INFO][0m - loss: 0.39955149, learning_rate: 1.7989417989417988e-06, global_step: 2270, interval_runtime: 43.4074, interval_samples_per_second: 0.369, interval_steps_per_second: 0.23, epoch: 12.0106[0m
[32m[2022-09-16 18:43:53,803] [    INFO][0m - loss: 0.43239117, learning_rate: 1.7936507936507938e-06, global_step: 2280, interval_runtime: 7.8813, interval_samples_per_second: 2.03, interval_steps_per_second: 1.269, epoch: 12.0635[0m
[32m[2022-09-16 18:44:01,704] [    INFO][0m - loss: 0.33849998, learning_rate: 1.7883597883597884e-06, global_step: 2290, interval_runtime: 7.9012, interval_samples_per_second: 2.025, interval_steps_per_second: 1.266, epoch: 12.1164[0m
[32m[2022-09-16 18:44:09,615] [    INFO][0m - loss: 0.38384476, learning_rate: 1.7830687830687832e-06, global_step: 2300, interval_runtime: 7.9108, interval_samples_per_second: 2.023, interval_steps_per_second: 1.264, epoch: 12.1693[0m
[32m[2022-09-16 18:44:17,566] [    INFO][0m - loss: 0.45375295, learning_rate: 1.7777777777777777e-06, global_step: 2310, interval_runtime: 7.9512, interval_samples_per_second: 2.012, interval_steps_per_second: 1.258, epoch: 12.2222[0m
[32m[2022-09-16 18:44:29,733] [    INFO][0m - loss: 0.3485759, learning_rate: 1.7724867724867725e-06, global_step: 2320, interval_runtime: 7.9768, interval_samples_per_second: 2.006, interval_steps_per_second: 1.254, epoch: 12.2751[0m
[32m[2022-09-16 18:44:37,631] [    INFO][0m - loss: 0.47070565, learning_rate: 1.767195767195767e-06, global_step: 2330, interval_runtime: 12.088, interval_samples_per_second: 1.324, interval_steps_per_second: 0.827, epoch: 12.328[0m
[32m[2022-09-16 18:44:45,542] [    INFO][0m - loss: 0.33572319, learning_rate: 1.761904761904762e-06, global_step: 2340, interval_runtime: 7.9113, interval_samples_per_second: 2.022, interval_steps_per_second: 1.264, epoch: 12.381[0m
[32m[2022-09-16 18:44:53,423] [    INFO][0m - loss: 0.50938315, learning_rate: 1.7566137566137567e-06, global_step: 2350, interval_runtime: 7.8803, interval_samples_per_second: 2.03, interval_steps_per_second: 1.269, epoch: 12.4339[0m
[32m[2022-09-16 18:45:01,312] [    INFO][0m - loss: 0.39516845, learning_rate: 1.7513227513227514e-06, global_step: 2360, interval_runtime: 7.89, interval_samples_per_second: 2.028, interval_steps_per_second: 1.267, epoch: 12.4868[0m
[32m[2022-09-16 18:45:09,197] [    INFO][0m - loss: 0.34849963, learning_rate: 1.746031746031746e-06, global_step: 2370, interval_runtime: 7.8845, interval_samples_per_second: 2.029, interval_steps_per_second: 1.268, epoch: 12.5397[0m
[32m[2022-09-16 18:45:17,095] [    INFO][0m - loss: 0.51604838, learning_rate: 1.7407407407407408e-06, global_step: 2380, interval_runtime: 7.8978, interval_samples_per_second: 2.026, interval_steps_per_second: 1.266, epoch: 12.5926[0m
[32m[2022-09-16 18:45:25,013] [    INFO][0m - loss: 0.34194133, learning_rate: 1.7354497354497354e-06, global_step: 2390, interval_runtime: 7.9187, interval_samples_per_second: 2.021, interval_steps_per_second: 1.263, epoch: 12.6455[0m
[32m[2022-09-16 18:45:32,949] [    INFO][0m - loss: 0.27380676, learning_rate: 1.7301587301587303e-06, global_step: 2400, interval_runtime: 7.9352, interval_samples_per_second: 2.016, interval_steps_per_second: 1.26, epoch: 12.6984[0m
[32m[2022-09-16 18:45:40,880] [    INFO][0m - loss: 0.25319872, learning_rate: 1.724867724867725e-06, global_step: 2410, interval_runtime: 7.9313, interval_samples_per_second: 2.017, interval_steps_per_second: 1.261, epoch: 12.7513[0m
[32m[2022-09-16 18:45:48,789] [    INFO][0m - loss: 0.43476787, learning_rate: 1.7195767195767197e-06, global_step: 2420, interval_runtime: 7.9092, interval_samples_per_second: 2.023, interval_steps_per_second: 1.264, epoch: 12.8042[0m
[32m[2022-09-16 18:45:56,718] [    INFO][0m - loss: 0.37881408, learning_rate: 1.7142857142857143e-06, global_step: 2430, interval_runtime: 7.9286, interval_samples_per_second: 2.018, interval_steps_per_second: 1.261, epoch: 12.8571[0m
[32m[2022-09-16 18:46:04,618] [    INFO][0m - loss: 0.43815136, learning_rate: 1.708994708994709e-06, global_step: 2440, interval_runtime: 7.9, interval_samples_per_second: 2.025, interval_steps_per_second: 1.266, epoch: 12.9101[0m
[32m[2022-09-16 18:46:12,511] [    INFO][0m - loss: 0.44962001, learning_rate: 1.7037037037037036e-06, global_step: 2450, interval_runtime: 7.8933, interval_samples_per_second: 2.027, interval_steps_per_second: 1.267, epoch: 12.963[0m
[32m[2022-09-16 18:46:17,859] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 18:46:17,859] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-16 18:46:17,859] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 18:46:17,859] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 18:46:17,859] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-16 18:46:45,381] [    INFO][0m - eval_loss: 2.493870496749878, eval_accuracy: 0.4588492352512746, eval_runtime: 27.5212, eval_samples_per_second: 49.889, eval_steps_per_second: 3.125, epoch: 13.0[0m
[32m[2022-09-16 18:46:45,404] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-2457[0m
[32m[2022-09-16 18:46:45,404] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 18:46:48,258] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-2457/tokenizer_config.json[0m
[32m[2022-09-16 18:46:48,259] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-2457/special_tokens_map.json[0m
[32m[2022-09-16 18:46:56,321] [    INFO][0m - loss: 0.40487213, learning_rate: 1.6984126984126986e-06, global_step: 2460, interval_runtime: 43.81, interval_samples_per_second: 0.365, interval_steps_per_second: 0.228, epoch: 13.0159[0m
[32m[2022-09-16 18:47:04,171] [    INFO][0m - loss: 0.29810457, learning_rate: 1.693121693121693e-06, global_step: 2470, interval_runtime: 7.85, interval_samples_per_second: 2.038, interval_steps_per_second: 1.274, epoch: 13.0688[0m
[32m[2022-09-16 18:47:12,532] [    INFO][0m - loss: 0.27131202, learning_rate: 1.687830687830688e-06, global_step: 2480, interval_runtime: 7.8824, interval_samples_per_second: 2.03, interval_steps_per_second: 1.269, epoch: 13.1217[0m
[32m[2022-09-16 18:47:20,430] [    INFO][0m - loss: 0.30104718, learning_rate: 1.6825396825396825e-06, global_step: 2490, interval_runtime: 8.3767, interval_samples_per_second: 1.91, interval_steps_per_second: 1.194, epoch: 13.1746[0m
[32m[2022-09-16 18:47:28,363] [    INFO][0m - loss: 0.27906036, learning_rate: 1.6772486772486773e-06, global_step: 2500, interval_runtime: 7.9328, interval_samples_per_second: 2.017, interval_steps_per_second: 1.261, epoch: 13.2275[0m
[32m[2022-09-16 18:47:36,628] [    INFO][0m - loss: 0.28095345, learning_rate: 1.6719576719576719e-06, global_step: 2510, interval_runtime: 7.8977, interval_samples_per_second: 2.026, interval_steps_per_second: 1.266, epoch: 13.2804[0m
[32m[2022-09-16 18:47:44,499] [    INFO][0m - loss: 0.26717069, learning_rate: 1.6666666666666669e-06, global_step: 2520, interval_runtime: 8.2379, interval_samples_per_second: 1.942, interval_steps_per_second: 1.214, epoch: 13.3333[0m
[32m[2022-09-16 18:47:52,349] [    INFO][0m - loss: 0.37278671, learning_rate: 1.6613756613756612e-06, global_step: 2530, interval_runtime: 7.8504, interval_samples_per_second: 2.038, interval_steps_per_second: 1.274, epoch: 13.3862[0m
[32m[2022-09-16 18:48:00,225] [    INFO][0m - loss: 0.28506522, learning_rate: 1.6560846560846562e-06, global_step: 2540, interval_runtime: 7.8752, interval_samples_per_second: 2.032, interval_steps_per_second: 1.27, epoch: 13.4392[0m
[32m[2022-09-16 18:48:09,243] [    INFO][0m - loss: 0.32763774, learning_rate: 1.6507936507936508e-06, global_step: 2550, interval_runtime: 7.8919, interval_samples_per_second: 2.027, interval_steps_per_second: 1.267, epoch: 13.4921[0m
[32m[2022-09-16 18:48:17,135] [    INFO][0m - loss: 0.27951751, learning_rate: 1.6455026455026456e-06, global_step: 2560, interval_runtime: 9.0184, interval_samples_per_second: 1.774, interval_steps_per_second: 1.109, epoch: 13.545[0m
[32m[2022-09-16 18:48:25,051] [    INFO][0m - loss: 0.3355726, learning_rate: 1.6402116402116401e-06, global_step: 2570, interval_runtime: 7.9159, interval_samples_per_second: 2.021, interval_steps_per_second: 1.263, epoch: 13.5979[0m
[32m[2022-09-16 18:48:32,963] [    INFO][0m - loss: 0.37282119, learning_rate: 1.6349206349206351e-06, global_step: 2580, interval_runtime: 7.9121, interval_samples_per_second: 2.022, interval_steps_per_second: 1.264, epoch: 13.6508[0m
[32m[2022-09-16 18:48:40,905] [    INFO][0m - loss: 0.2740907, learning_rate: 1.6296296296296295e-06, global_step: 2590, interval_runtime: 7.9424, interval_samples_per_second: 2.015, interval_steps_per_second: 1.259, epoch: 13.7037[0m
[32m[2022-09-16 18:48:48,799] [    INFO][0m - loss: 0.42828865, learning_rate: 1.6243386243386245e-06, global_step: 2600, interval_runtime: 7.8938, interval_samples_per_second: 2.027, interval_steps_per_second: 1.267, epoch: 13.7566[0m
[32m[2022-09-16 18:48:56,688] [    INFO][0m - loss: 0.33998656, learning_rate: 1.619047619047619e-06, global_step: 2610, interval_runtime: 7.889, interval_samples_per_second: 2.028, interval_steps_per_second: 1.268, epoch: 13.8095[0m
[32m[2022-09-16 18:49:04,591] [    INFO][0m - loss: 0.39372063, learning_rate: 1.6137566137566138e-06, global_step: 2620, interval_runtime: 7.9028, interval_samples_per_second: 2.025, interval_steps_per_second: 1.265, epoch: 13.8624[0m
[32m[2022-09-16 18:49:12,502] [    INFO][0m - loss: 0.32193213, learning_rate: 1.6084656084656084e-06, global_step: 2630, interval_runtime: 7.9116, interval_samples_per_second: 2.022, interval_steps_per_second: 1.264, epoch: 13.9153[0m
[32m[2022-09-16 18:49:20,396] [    INFO][0m - loss: 0.39475482, learning_rate: 1.6031746031746034e-06, global_step: 2640, interval_runtime: 7.8939, interval_samples_per_second: 2.027, interval_steps_per_second: 1.267, epoch: 13.9683[0m
[32m[2022-09-16 18:49:24,952] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 18:49:24,952] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-16 18:49:24,953] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 18:49:24,953] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 18:49:24,953] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-16 18:49:52,211] [    INFO][0m - eval_loss: 2.577751874923706, eval_accuracy: 0.45083758193736345, eval_runtime: 27.2577, eval_samples_per_second: 50.371, eval_steps_per_second: 3.155, epoch: 14.0[0m
[32m[2022-09-16 18:49:52,234] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-2646[0m
[32m[2022-09-16 18:49:52,235] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 18:49:55,005] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-2646/tokenizer_config.json[0m
[32m[2022-09-16 18:49:55,005] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-2646/special_tokens_map.json[0m
[32m[2022-09-16 18:50:03,549] [    INFO][0m - loss: 0.32297478, learning_rate: 1.5978835978835978e-06, global_step: 2650, interval_runtime: 43.1528, interval_samples_per_second: 0.371, interval_steps_per_second: 0.232, epoch: 14.0212[0m
[32m[2022-09-16 18:50:11,370] [    INFO][0m - loss: 0.27107041, learning_rate: 1.5925925925925927e-06, global_step: 2660, interval_runtime: 7.8206, interval_samples_per_second: 2.046, interval_steps_per_second: 1.279, epoch: 14.0741[0m
[32m[2022-09-16 18:50:19,262] [    INFO][0m - loss: 0.26769822, learning_rate: 1.5873015873015873e-06, global_step: 2670, interval_runtime: 7.8923, interval_samples_per_second: 2.027, interval_steps_per_second: 1.267, epoch: 14.127[0m
[32m[2022-09-16 18:50:27,157] [    INFO][0m - loss: 0.33185668, learning_rate: 1.582010582010582e-06, global_step: 2680, interval_runtime: 7.8947, interval_samples_per_second: 2.027, interval_steps_per_second: 1.267, epoch: 14.1799[0m
[32m[2022-09-16 18:50:35,047] [    INFO][0m - loss: 0.23319488, learning_rate: 1.5767195767195767e-06, global_step: 2690, interval_runtime: 7.8905, interval_samples_per_second: 2.028, interval_steps_per_second: 1.267, epoch: 14.2328[0m
[32m[2022-09-16 18:50:42,951] [    INFO][0m - loss: 0.24241502, learning_rate: 1.5714285714285714e-06, global_step: 2700, interval_runtime: 7.9041, interval_samples_per_second: 2.024, interval_steps_per_second: 1.265, epoch: 14.2857[0m
[32m[2022-09-16 18:50:50,891] [    INFO][0m - loss: 0.30504146, learning_rate: 1.566137566137566e-06, global_step: 2710, interval_runtime: 7.9397, interval_samples_per_second: 2.015, interval_steps_per_second: 1.26, epoch: 14.3386[0m
[32m[2022-09-16 18:50:58,820] [    INFO][0m - loss: 0.20816565, learning_rate: 1.560846560846561e-06, global_step: 2720, interval_runtime: 7.9287, interval_samples_per_second: 2.018, interval_steps_per_second: 1.261, epoch: 14.3915[0m
[32m[2022-09-16 18:51:06,710] [    INFO][0m - loss: 0.26213279, learning_rate: 1.5555555555555556e-06, global_step: 2730, interval_runtime: 7.8904, interval_samples_per_second: 2.028, interval_steps_per_second: 1.267, epoch: 14.4444[0m
[32m[2022-09-16 18:51:14,612] [    INFO][0m - loss: 0.21175487, learning_rate: 1.5502645502645504e-06, global_step: 2740, interval_runtime: 7.9011, interval_samples_per_second: 2.025, interval_steps_per_second: 1.266, epoch: 14.4974[0m
[32m[2022-09-16 18:51:22,511] [    INFO][0m - loss: 0.25316093, learning_rate: 1.544973544973545e-06, global_step: 2750, interval_runtime: 7.8993, interval_samples_per_second: 2.025, interval_steps_per_second: 1.266, epoch: 14.5503[0m
[32m[2022-09-16 18:51:30,388] [    INFO][0m - loss: 0.41341558, learning_rate: 1.5396825396825397e-06, global_step: 2760, interval_runtime: 7.8769, interval_samples_per_second: 2.031, interval_steps_per_second: 1.27, epoch: 14.6032[0m
[32m[2022-09-16 18:51:38,310] [    INFO][0m - loss: 0.2953424, learning_rate: 1.5343915343915343e-06, global_step: 2770, interval_runtime: 7.9222, interval_samples_per_second: 2.02, interval_steps_per_second: 1.262, epoch: 14.6561[0m
[32m[2022-09-16 18:51:46,239] [    INFO][0m - loss: 0.27110529, learning_rate: 1.5291005291005293e-06, global_step: 2780, interval_runtime: 7.9297, interval_samples_per_second: 2.018, interval_steps_per_second: 1.261, epoch: 14.709[0m
[32m[2022-09-16 18:51:54,096] [    INFO][0m - loss: 0.30828488, learning_rate: 1.5238095238095238e-06, global_step: 2790, interval_runtime: 7.8569, interval_samples_per_second: 2.036, interval_steps_per_second: 1.273, epoch: 14.7619[0m
[32m[2022-09-16 18:52:02,012] [    INFO][0m - loss: 0.29002266, learning_rate: 1.5185185185185186e-06, global_step: 2800, interval_runtime: 7.916, interval_samples_per_second: 2.021, interval_steps_per_second: 1.263, epoch: 14.8148[0m
[32m[2022-09-16 18:52:09,891] [    INFO][0m - loss: 0.25166659, learning_rate: 1.5132275132275132e-06, global_step: 2810, interval_runtime: 7.8787, interval_samples_per_second: 2.031, interval_steps_per_second: 1.269, epoch: 14.8677[0m
[32m[2022-09-16 18:52:17,804] [    INFO][0m - loss: 0.301703, learning_rate: 1.507936507936508e-06, global_step: 2820, interval_runtime: 7.9126, interval_samples_per_second: 2.022, interval_steps_per_second: 1.264, epoch: 14.9206[0m
[32m[2022-09-16 18:52:25,700] [    INFO][0m - loss: 0.37227221, learning_rate: 1.5026455026455025e-06, global_step: 2830, interval_runtime: 7.8958, interval_samples_per_second: 2.026, interval_steps_per_second: 1.266, epoch: 14.9735[0m
[32m[2022-09-16 18:52:29,464] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 18:52:29,465] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-16 18:52:29,465] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 18:52:29,465] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 18:52:29,465] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-16 18:52:56,795] [    INFO][0m - eval_loss: 2.68154239654541, eval_accuracy: 0.44865258557902404, eval_runtime: 27.3293, eval_samples_per_second: 50.239, eval_steps_per_second: 3.147, epoch: 15.0[0m
[32m[2022-09-16 18:52:56,825] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-2835[0m
[32m[2022-09-16 18:52:56,825] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 18:52:59,767] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-2835/tokenizer_config.json[0m
[32m[2022-09-16 18:52:59,768] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-2835/special_tokens_map.json[0m
[32m[2022-09-16 18:53:08,995] [    INFO][0m - loss: 0.20804555, learning_rate: 1.4973544973544973e-06, global_step: 2840, interval_runtime: 43.2957, interval_samples_per_second: 0.37, interval_steps_per_second: 0.231, epoch: 15.0265[0m
[32m[2022-09-16 18:53:16,850] [    INFO][0m - loss: 0.23610945, learning_rate: 1.492063492063492e-06, global_step: 2850, interval_runtime: 7.8548, interval_samples_per_second: 2.037, interval_steps_per_second: 1.273, epoch: 15.0794[0m
[32m[2022-09-16 18:53:24,804] [    INFO][0m - loss: 0.24825327, learning_rate: 1.4867724867724867e-06, global_step: 2860, interval_runtime: 7.9538, interval_samples_per_second: 2.012, interval_steps_per_second: 1.257, epoch: 15.1323[0m
[32m[2022-09-16 18:53:32,749] [    INFO][0m - loss: 0.20248954, learning_rate: 1.4814814814814815e-06, global_step: 2870, interval_runtime: 7.945, interval_samples_per_second: 2.014, interval_steps_per_second: 1.259, epoch: 15.1852[0m
[32m[2022-09-16 18:53:40,643] [    INFO][0m - loss: 0.26064143, learning_rate: 1.4761904761904762e-06, global_step: 2880, interval_runtime: 7.8937, interval_samples_per_second: 2.027, interval_steps_per_second: 1.267, epoch: 15.2381[0m
[32m[2022-09-16 18:53:48,584] [    INFO][0m - loss: 0.16542505, learning_rate: 1.4708994708994708e-06, global_step: 2890, interval_runtime: 7.941, interval_samples_per_second: 2.015, interval_steps_per_second: 1.259, epoch: 15.291[0m
[32m[2022-09-16 18:53:56,495] [    INFO][0m - loss: 0.24089379, learning_rate: 1.4656084656084656e-06, global_step: 2900, interval_runtime: 7.9117, interval_samples_per_second: 2.022, interval_steps_per_second: 1.264, epoch: 15.3439[0m
[32m[2022-09-16 18:54:04,367] [    INFO][0m - loss: 0.25421464, learning_rate: 1.4603174603174604e-06, global_step: 2910, interval_runtime: 7.8714, interval_samples_per_second: 2.033, interval_steps_per_second: 1.27, epoch: 15.3968[0m
[32m[2022-09-16 18:54:12,266] [    INFO][0m - loss: 0.28736238, learning_rate: 1.455026455026455e-06, global_step: 2920, interval_runtime: 7.8988, interval_samples_per_second: 2.026, interval_steps_per_second: 1.266, epoch: 15.4497[0m
[32m[2022-09-16 18:54:20,208] [    INFO][0m - loss: 0.21940887, learning_rate: 1.4497354497354497e-06, global_step: 2930, interval_runtime: 7.9426, interval_samples_per_second: 2.014, interval_steps_per_second: 1.259, epoch: 15.5026[0m
[32m[2022-09-16 18:54:28,102] [    INFO][0m - loss: 0.27118275, learning_rate: 1.4444444444444445e-06, global_step: 2940, interval_runtime: 7.8934, interval_samples_per_second: 2.027, interval_steps_per_second: 1.267, epoch: 15.5556[0m
[32m[2022-09-16 18:54:36,020] [    INFO][0m - loss: 0.24331191, learning_rate: 1.439153439153439e-06, global_step: 2950, interval_runtime: 7.9182, interval_samples_per_second: 2.021, interval_steps_per_second: 1.263, epoch: 15.6085[0m
[32m[2022-09-16 18:54:43,912] [    INFO][0m - loss: 0.24649849, learning_rate: 1.4338624338624338e-06, global_step: 2960, interval_runtime: 7.8926, interval_samples_per_second: 2.027, interval_steps_per_second: 1.267, epoch: 15.6614[0m
[32m[2022-09-16 18:54:51,794] [    INFO][0m - loss: 0.24248738, learning_rate: 1.4285714285714286e-06, global_step: 2970, interval_runtime: 7.8823, interval_samples_per_second: 2.03, interval_steps_per_second: 1.269, epoch: 15.7143[0m
[32m[2022-09-16 18:54:59,707] [    INFO][0m - loss: 0.21833208, learning_rate: 1.4232804232804232e-06, global_step: 2980, interval_runtime: 7.9121, interval_samples_per_second: 2.022, interval_steps_per_second: 1.264, epoch: 15.7672[0m
[32m[2022-09-16 18:55:07,601] [    INFO][0m - loss: 0.2041225, learning_rate: 1.417989417989418e-06, global_step: 2990, interval_runtime: 7.8946, interval_samples_per_second: 2.027, interval_steps_per_second: 1.267, epoch: 15.8201[0m
[32m[2022-09-16 18:55:15,535] [    INFO][0m - loss: 0.17027581, learning_rate: 1.4126984126984128e-06, global_step: 3000, interval_runtime: 7.9331, interval_samples_per_second: 2.017, interval_steps_per_second: 1.261, epoch: 15.873[0m
[32m[2022-09-16 18:55:23,450] [    INFO][0m - loss: 0.25859926, learning_rate: 1.4074074074074073e-06, global_step: 3010, interval_runtime: 7.9153, interval_samples_per_second: 2.021, interval_steps_per_second: 1.263, epoch: 15.9259[0m
[32m[2022-09-16 18:55:31,328] [    INFO][0m - loss: 0.29106202, learning_rate: 1.4021164021164021e-06, global_step: 3020, interval_runtime: 7.8782, interval_samples_per_second: 2.031, interval_steps_per_second: 1.269, epoch: 15.9788[0m
[32m[2022-09-16 18:55:34,331] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 18:55:34,331] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-16 18:55:34,332] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 18:55:34,332] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 18:55:34,332] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-16 18:56:01,514] [    INFO][0m - eval_loss: 2.725756883621216, eval_accuracy: 0.44865258557902404, eval_runtime: 27.1821, eval_samples_per_second: 50.511, eval_steps_per_second: 3.164, epoch: 16.0[0m
[32m[2022-09-16 18:56:01,538] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-3024[0m
[32m[2022-09-16 18:56:01,538] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 18:56:04,205] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-3024/tokenizer_config.json[0m
[32m[2022-09-16 18:56:04,205] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-3024/special_tokens_map.json[0m
[32m[2022-09-16 18:56:14,533] [    INFO][0m - loss: 0.19978759, learning_rate: 1.3968253968253967e-06, global_step: 3030, interval_runtime: 43.2052, interval_samples_per_second: 0.37, interval_steps_per_second: 0.231, epoch: 16.0317[0m
[32m[2022-09-16 18:56:22,380] [    INFO][0m - loss: 0.19639863, learning_rate: 1.3915343915343915e-06, global_step: 3040, interval_runtime: 7.8472, interval_samples_per_second: 2.039, interval_steps_per_second: 1.274, epoch: 16.0847[0m
[32m[2022-09-16 18:56:30,242] [    INFO][0m - loss: 0.15050533, learning_rate: 1.3862433862433862e-06, global_step: 3050, interval_runtime: 7.8614, interval_samples_per_second: 2.035, interval_steps_per_second: 1.272, epoch: 16.1376[0m
[32m[2022-09-16 18:56:42,069] [    INFO][0m - loss: 0.21427982, learning_rate: 1.3809523809523808e-06, global_step: 3060, interval_runtime: 7.8879, interval_samples_per_second: 2.028, interval_steps_per_second: 1.268, epoch: 16.1905[0m
[32m[2022-09-16 18:56:49,958] [    INFO][0m - loss: 0.20518355, learning_rate: 1.3756613756613756e-06, global_step: 3070, interval_runtime: 11.8283, interval_samples_per_second: 1.353, interval_steps_per_second: 0.845, epoch: 16.2434[0m
[32m[2022-09-16 18:56:57,867] [    INFO][0m - loss: 0.22127426, learning_rate: 1.3703703703703704e-06, global_step: 3080, interval_runtime: 7.9095, interval_samples_per_second: 2.023, interval_steps_per_second: 1.264, epoch: 16.2963[0m
[32m[2022-09-16 18:57:05,759] [    INFO][0m - loss: 0.20162857, learning_rate: 1.365079365079365e-06, global_step: 3090, interval_runtime: 7.8922, interval_samples_per_second: 2.027, interval_steps_per_second: 1.267, epoch: 16.3492[0m
[32m[2022-09-16 18:57:13,673] [    INFO][0m - loss: 0.19016171, learning_rate: 1.3597883597883597e-06, global_step: 3100, interval_runtime: 7.9134, interval_samples_per_second: 2.022, interval_steps_per_second: 1.264, epoch: 16.4021[0m
[32m[2022-09-16 18:57:21,714] [    INFO][0m - loss: 0.25997233, learning_rate: 1.3544973544973545e-06, global_step: 3110, interval_runtime: 7.9312, interval_samples_per_second: 2.017, interval_steps_per_second: 1.261, epoch: 16.455[0m
[32m[2022-09-16 18:57:29,627] [    INFO][0m - loss: 0.23308048, learning_rate: 1.349206349206349e-06, global_step: 3120, interval_runtime: 8.0228, interval_samples_per_second: 1.994, interval_steps_per_second: 1.246, epoch: 16.5079[0m
[32m[2022-09-16 18:57:37,544] [    INFO][0m - loss: 0.17670975, learning_rate: 1.3439153439153439e-06, global_step: 3130, interval_runtime: 7.9171, interval_samples_per_second: 2.021, interval_steps_per_second: 1.263, epoch: 16.5608[0m
[32m[2022-09-16 18:57:45,456] [    INFO][0m - loss: 0.21489892, learning_rate: 1.3386243386243386e-06, global_step: 3140, interval_runtime: 7.9118, interval_samples_per_second: 2.022, interval_steps_per_second: 1.264, epoch: 16.6138[0m
[32m[2022-09-16 18:57:53,397] [    INFO][0m - loss: 0.23210421, learning_rate: 1.3333333333333332e-06, global_step: 3150, interval_runtime: 7.9414, interval_samples_per_second: 2.015, interval_steps_per_second: 1.259, epoch: 16.6667[0m
[32m[2022-09-16 18:58:01,287] [    INFO][0m - loss: 0.23908994, learning_rate: 1.328042328042328e-06, global_step: 3160, interval_runtime: 7.8897, interval_samples_per_second: 2.028, interval_steps_per_second: 1.267, epoch: 16.7196[0m
[32m[2022-09-16 18:58:09,178] [    INFO][0m - loss: 0.16248231, learning_rate: 1.3227513227513228e-06, global_step: 3170, interval_runtime: 7.8908, interval_samples_per_second: 2.028, interval_steps_per_second: 1.267, epoch: 16.7725[0m
[32m[2022-09-16 18:58:17,101] [    INFO][0m - loss: 0.24227819, learning_rate: 1.3174603174603173e-06, global_step: 3180, interval_runtime: 7.9229, interval_samples_per_second: 2.019, interval_steps_per_second: 1.262, epoch: 16.8254[0m
[32m[2022-09-16 18:58:24,966] [    INFO][0m - loss: 0.27047465, learning_rate: 1.3121693121693121e-06, global_step: 3190, interval_runtime: 7.8654, interval_samples_per_second: 2.034, interval_steps_per_second: 1.271, epoch: 16.8783[0m
[32m[2022-09-16 18:58:32,847] [    INFO][0m - loss: 0.19377513, learning_rate: 1.306878306878307e-06, global_step: 3200, interval_runtime: 7.8807, interval_samples_per_second: 2.03, interval_steps_per_second: 1.269, epoch: 16.9312[0m
[32m[2022-09-16 18:58:40,684] [    INFO][0m - loss: 0.18212729, learning_rate: 1.3015873015873015e-06, global_step: 3210, interval_runtime: 7.8376, interval_samples_per_second: 2.041, interval_steps_per_second: 1.276, epoch: 16.9841[0m
[32m[2022-09-16 18:58:42,940] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 18:58:42,941] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-16 18:58:42,941] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 18:58:42,941] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 18:58:42,941] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-16 18:59:10,244] [    INFO][0m - eval_loss: 2.791870355606079, eval_accuracy: 0.44865258557902404, eval_runtime: 27.3026, eval_samples_per_second: 50.288, eval_steps_per_second: 3.15, epoch: 17.0[0m
[32m[2022-09-16 18:59:10,271] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-3213[0m
[32m[2022-09-16 18:59:10,271] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 18:59:14,065] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-3213/tokenizer_config.json[0m
[32m[2022-09-16 18:59:14,066] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-3213/special_tokens_map.json[0m
[32m[2022-09-16 18:59:27,809] [    INFO][0m - loss: 0.15504944, learning_rate: 1.2962962962962962e-06, global_step: 3220, interval_runtime: 47.1239, interval_samples_per_second: 0.34, interval_steps_per_second: 0.212, epoch: 17.037[0m
[32m[2022-09-16 18:59:35,662] [    INFO][0m - loss: 0.13884654, learning_rate: 1.291005291005291e-06, global_step: 3230, interval_runtime: 7.8536, interval_samples_per_second: 2.037, interval_steps_per_second: 1.273, epoch: 17.0899[0m
[32m[2022-09-16 18:59:43,554] [    INFO][0m - loss: 0.15435839, learning_rate: 1.2857142857142856e-06, global_step: 3240, interval_runtime: 7.8921, interval_samples_per_second: 2.027, interval_steps_per_second: 1.267, epoch: 17.1429[0m
[32m[2022-09-16 18:59:52,564] [    INFO][0m - loss: 0.15781864, learning_rate: 1.2804232804232804e-06, global_step: 3250, interval_runtime: 7.8593, interval_samples_per_second: 2.036, interval_steps_per_second: 1.272, epoch: 17.1958[0m
[32m[2022-09-16 19:00:00,578] [    INFO][0m - loss: 0.18908658, learning_rate: 1.2751322751322752e-06, global_step: 3260, interval_runtime: 9.1642, interval_samples_per_second: 1.746, interval_steps_per_second: 1.091, epoch: 17.2487[0m
[32m[2022-09-16 19:00:08,513] [    INFO][0m - loss: 0.1946445, learning_rate: 1.2698412698412697e-06, global_step: 3270, interval_runtime: 7.9357, interval_samples_per_second: 2.016, interval_steps_per_second: 1.26, epoch: 17.3016[0m
[32m[2022-09-16 19:00:16,440] [    INFO][0m - loss: 0.15843174, learning_rate: 1.2645502645502645e-06, global_step: 3280, interval_runtime: 7.9272, interval_samples_per_second: 2.018, interval_steps_per_second: 1.261, epoch: 17.3545[0m
[32m[2022-09-16 19:00:24,357] [    INFO][0m - loss: 0.16147368, learning_rate: 1.2592592592592593e-06, global_step: 3290, interval_runtime: 7.9162, interval_samples_per_second: 2.021, interval_steps_per_second: 1.263, epoch: 17.4074[0m
[32m[2022-09-16 19:00:32,208] [    INFO][0m - loss: 0.16934571, learning_rate: 1.2539682539682539e-06, global_step: 3300, interval_runtime: 7.8515, interval_samples_per_second: 2.038, interval_steps_per_second: 1.274, epoch: 17.4603[0m
[32m[2022-09-16 19:00:40,124] [    INFO][0m - loss: 0.20494325, learning_rate: 1.2486772486772486e-06, global_step: 3310, interval_runtime: 7.9161, interval_samples_per_second: 2.021, interval_steps_per_second: 1.263, epoch: 17.5132[0m
[32m[2022-09-16 19:00:48,035] [    INFO][0m - loss: 0.20948174, learning_rate: 1.2433862433862434e-06, global_step: 3320, interval_runtime: 7.9109, interval_samples_per_second: 2.023, interval_steps_per_second: 1.264, epoch: 17.5661[0m
[32m[2022-09-16 19:00:55,917] [    INFO][0m - loss: 0.19017972, learning_rate: 1.238095238095238e-06, global_step: 3330, interval_runtime: 7.8813, interval_samples_per_second: 2.03, interval_steps_per_second: 1.269, epoch: 17.619[0m
[32m[2022-09-16 19:01:03,821] [    INFO][0m - loss: 0.15789783, learning_rate: 1.2328042328042328e-06, global_step: 3340, interval_runtime: 7.9043, interval_samples_per_second: 2.024, interval_steps_per_second: 1.265, epoch: 17.672[0m
[32m[2022-09-16 19:01:11,736] [    INFO][0m - loss: 0.14183825, learning_rate: 1.2275132275132276e-06, global_step: 3350, interval_runtime: 7.9151, interval_samples_per_second: 2.021, interval_steps_per_second: 1.263, epoch: 17.7249[0m
[32m[2022-09-16 19:01:19,606] [    INFO][0m - loss: 0.16770864, learning_rate: 1.2222222222222221e-06, global_step: 3360, interval_runtime: 7.8704, interval_samples_per_second: 2.033, interval_steps_per_second: 1.271, epoch: 17.7778[0m
[32m[2022-09-16 19:01:27,514] [    INFO][0m - loss: 0.17004364, learning_rate: 1.216931216931217e-06, global_step: 3370, interval_runtime: 7.9072, interval_samples_per_second: 2.023, interval_steps_per_second: 1.265, epoch: 17.8307[0m
[32m[2022-09-16 19:01:35,434] [    INFO][0m - loss: 0.18349954, learning_rate: 1.2116402116402117e-06, global_step: 3380, interval_runtime: 7.9206, interval_samples_per_second: 2.02, interval_steps_per_second: 1.263, epoch: 17.8836[0m
[32m[2022-09-16 19:01:43,350] [    INFO][0m - loss: 0.12473714, learning_rate: 1.2063492063492063e-06, global_step: 3390, interval_runtime: 7.9151, interval_samples_per_second: 2.021, interval_steps_per_second: 1.263, epoch: 17.9365[0m
[32m[2022-09-16 19:01:51,144] [    INFO][0m - loss: 0.16102729, learning_rate: 1.201058201058201e-06, global_step: 3400, interval_runtime: 7.795, interval_samples_per_second: 2.053, interval_steps_per_second: 1.283, epoch: 17.9894[0m
[32m[2022-09-16 19:01:52,650] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:01:52,650] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-16 19:01:52,650] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:01:52,650] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:01:52,650] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-16 19:02:19,847] [    INFO][0m - eval_loss: 2.884396553039551, eval_accuracy: 0.45083758193736345, eval_runtime: 27.1966, eval_samples_per_second: 50.484, eval_steps_per_second: 3.162, epoch: 18.0[0m
[32m[2022-09-16 19:02:19,871] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-3402[0m
[32m[2022-09-16 19:02:19,871] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:02:23,566] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-3402/tokenizer_config.json[0m
[32m[2022-09-16 19:02:23,567] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-3402/special_tokens_map.json[0m
[32m[2022-09-16 19:02:37,759] [    INFO][0m - loss: 0.146428, learning_rate: 1.1957671957671958e-06, global_step: 3410, interval_runtime: 46.6141, interval_samples_per_second: 0.343, interval_steps_per_second: 0.215, epoch: 18.0423[0m
[32m[2022-09-16 19:02:45,641] [    INFO][0m - loss: 0.12200928, learning_rate: 1.1904761904761904e-06, global_step: 3420, interval_runtime: 7.8818, interval_samples_per_second: 2.03, interval_steps_per_second: 1.269, epoch: 18.0952[0m
[32m[2022-09-16 19:02:53,492] [    INFO][0m - loss: 0.09641935, learning_rate: 1.1851851851851852e-06, global_step: 3430, interval_runtime: 7.8516, interval_samples_per_second: 2.038, interval_steps_per_second: 1.274, epoch: 18.1481[0m
[32m[2022-09-16 19:03:01,941] [    INFO][0m - loss: 0.18044899, learning_rate: 1.17989417989418e-06, global_step: 3440, interval_runtime: 7.8947, interval_samples_per_second: 2.027, interval_steps_per_second: 1.267, epoch: 18.2011[0m
[32m[2022-09-16 19:03:09,838] [    INFO][0m - loss: 0.14393882, learning_rate: 1.1746031746031745e-06, global_step: 3450, interval_runtime: 8.4509, interval_samples_per_second: 1.893, interval_steps_per_second: 1.183, epoch: 18.254[0m
[32m[2022-09-16 19:03:17,740] [    INFO][0m - loss: 0.14034865, learning_rate: 1.1693121693121693e-06, global_step: 3460, interval_runtime: 7.902, interval_samples_per_second: 2.025, interval_steps_per_second: 1.266, epoch: 18.3069[0m
[32m[2022-09-16 19:03:25,676] [    INFO][0m - loss: 0.1488843, learning_rate: 1.164021164021164e-06, global_step: 3470, interval_runtime: 7.936, interval_samples_per_second: 2.016, interval_steps_per_second: 1.26, epoch: 18.3598[0m
[32m[2022-09-16 19:03:33,554] [    INFO][0m - loss: 0.14517548, learning_rate: 1.1587301587301586e-06, global_step: 3480, interval_runtime: 7.8778, interval_samples_per_second: 2.031, interval_steps_per_second: 1.269, epoch: 18.4127[0m
[32m[2022-09-16 19:03:41,448] [    INFO][0m - loss: 0.12628214, learning_rate: 1.1534391534391534e-06, global_step: 3490, interval_runtime: 7.8949, interval_samples_per_second: 2.027, interval_steps_per_second: 1.267, epoch: 18.4656[0m
[32m[2022-09-16 19:03:49,334] [    INFO][0m - loss: 0.15370467, learning_rate: 1.1481481481481482e-06, global_step: 3500, interval_runtime: 7.8853, interval_samples_per_second: 2.029, interval_steps_per_second: 1.268, epoch: 18.5185[0m
[32m[2022-09-16 19:03:57,222] [    INFO][0m - loss: 0.08607188, learning_rate: 1.1428571428571428e-06, global_step: 3510, interval_runtime: 7.8878, interval_samples_per_second: 2.028, interval_steps_per_second: 1.268, epoch: 18.5714[0m
[32m[2022-09-16 19:04:05,104] [    INFO][0m - loss: 0.15114732, learning_rate: 1.1375661375661376e-06, global_step: 3520, interval_runtime: 7.883, interval_samples_per_second: 2.03, interval_steps_per_second: 1.269, epoch: 18.6243[0m
[32m[2022-09-16 19:04:12,998] [    INFO][0m - loss: 0.13941348, learning_rate: 1.1322751322751323e-06, global_step: 3530, interval_runtime: 7.8934, interval_samples_per_second: 2.027, interval_steps_per_second: 1.267, epoch: 18.6772[0m
[32m[2022-09-16 19:04:20,906] [    INFO][0m - loss: 0.16243711, learning_rate: 1.126984126984127e-06, global_step: 3540, interval_runtime: 7.9082, interval_samples_per_second: 2.023, interval_steps_per_second: 1.265, epoch: 18.7302[0m
[32m[2022-09-16 19:04:28,820] [    INFO][0m - loss: 0.14200433, learning_rate: 1.1216931216931217e-06, global_step: 3550, interval_runtime: 7.9138, interval_samples_per_second: 2.022, interval_steps_per_second: 1.264, epoch: 18.7831[0m
[32m[2022-09-16 19:04:36,749] [    INFO][0m - loss: 0.15822704, learning_rate: 1.1164021164021165e-06, global_step: 3560, interval_runtime: 7.9295, interval_samples_per_second: 2.018, interval_steps_per_second: 1.261, epoch: 18.836[0m
[32m[2022-09-16 19:04:44,651] [    INFO][0m - loss: 0.13775537, learning_rate: 1.111111111111111e-06, global_step: 3570, interval_runtime: 7.9019, interval_samples_per_second: 2.025, interval_steps_per_second: 1.266, epoch: 18.8889[0m
[32m[2022-09-16 19:04:52,573] [    INFO][0m - loss: 0.18175933, learning_rate: 1.1058201058201058e-06, global_step: 3580, interval_runtime: 7.9222, interval_samples_per_second: 2.02, interval_steps_per_second: 1.262, epoch: 18.9418[0m
[32m[2022-09-16 19:05:00,305] [    INFO][0m - loss: 0.12921708, learning_rate: 1.1005291005291006e-06, global_step: 3590, interval_runtime: 7.7319, interval_samples_per_second: 2.069, interval_steps_per_second: 1.293, epoch: 18.9947[0m
[32m[2022-09-16 19:05:01,052] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:05:01,052] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-16 19:05:01,052] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:05:01,052] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:05:01,052] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-16 19:05:28,551] [    INFO][0m - eval_loss: 2.9618170261383057, eval_accuracy: 0.4457392571012382, eval_runtime: 27.4988, eval_samples_per_second: 49.93, eval_steps_per_second: 3.127, epoch: 19.0[0m
[32m[2022-09-16 19:05:28,575] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-3591[0m
[32m[2022-09-16 19:05:28,575] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:05:32,031] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-3591/tokenizer_config.json[0m
[32m[2022-09-16 19:05:32,031] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-3591/special_tokens_map.json[0m
[32m[2022-09-16 19:05:46,762] [    INFO][0m - loss: 0.12593423, learning_rate: 1.0952380952380952e-06, global_step: 3600, interval_runtime: 46.4571, interval_samples_per_second: 0.344, interval_steps_per_second: 0.215, epoch: 19.0476[0m
[32m[2022-09-16 19:05:54,640] [    INFO][0m - loss: 0.09917094, learning_rate: 1.08994708994709e-06, global_step: 3610, interval_runtime: 7.877, interval_samples_per_second: 2.031, interval_steps_per_second: 1.27, epoch: 19.1005[0m
[32m[2022-09-16 19:06:02,565] [    INFO][0m - loss: 0.12262573, learning_rate: 1.0846560846560845e-06, global_step: 3620, interval_runtime: 7.925, interval_samples_per_second: 2.019, interval_steps_per_second: 1.262, epoch: 19.1534[0m
[32m[2022-09-16 19:06:10,492] [    INFO][0m - loss: 0.13694798, learning_rate: 1.0793650793650793e-06, global_step: 3630, interval_runtime: 7.9274, interval_samples_per_second: 2.018, interval_steps_per_second: 1.261, epoch: 19.2063[0m
[32m[2022-09-16 19:06:18,780] [    INFO][0m - loss: 0.11054264, learning_rate: 1.074074074074074e-06, global_step: 3640, interval_runtime: 7.8963, interval_samples_per_second: 2.026, interval_steps_per_second: 1.266, epoch: 19.2593[0m
[32m[2022-09-16 19:06:26,650] [    INFO][0m - loss: 0.12029783, learning_rate: 1.0687830687830686e-06, global_step: 3650, interval_runtime: 8.2622, interval_samples_per_second: 1.937, interval_steps_per_second: 1.21, epoch: 19.3122[0m
[32m[2022-09-16 19:06:34,514] [    INFO][0m - loss: 0.17081351, learning_rate: 1.0634920634920634e-06, global_step: 3660, interval_runtime: 7.864, interval_samples_per_second: 2.035, interval_steps_per_second: 1.272, epoch: 19.3651[0m
[32m[2022-09-16 19:06:42,396] [    INFO][0m - loss: 0.1272645, learning_rate: 1.0582010582010582e-06, global_step: 3670, interval_runtime: 7.8815, interval_samples_per_second: 2.03, interval_steps_per_second: 1.269, epoch: 19.418[0m
[32m[2022-09-16 19:06:50,282] [    INFO][0m - loss: 0.16796241, learning_rate: 1.0529100529100528e-06, global_step: 3680, interval_runtime: 7.8863, interval_samples_per_second: 2.029, interval_steps_per_second: 1.268, epoch: 19.4709[0m
[32m[2022-09-16 19:06:58,160] [    INFO][0m - loss: 0.1005245, learning_rate: 1.0476190476190476e-06, global_step: 3690, interval_runtime: 7.8773, interval_samples_per_second: 2.031, interval_steps_per_second: 1.269, epoch: 19.5238[0m
[32m[2022-09-16 19:07:06,088] [    INFO][0m - loss: 0.11174055, learning_rate: 1.0423280423280423e-06, global_step: 3700, interval_runtime: 7.9286, interval_samples_per_second: 2.018, interval_steps_per_second: 1.261, epoch: 19.5767[0m
[32m[2022-09-16 19:07:14,019] [    INFO][0m - loss: 0.11916046, learning_rate: 1.037037037037037e-06, global_step: 3710, interval_runtime: 7.9305, interval_samples_per_second: 2.018, interval_steps_per_second: 1.261, epoch: 19.6296[0m
[32m[2022-09-16 19:07:21,921] [    INFO][0m - loss: 0.08199625, learning_rate: 1.0317460317460317e-06, global_step: 3720, interval_runtime: 7.9027, interval_samples_per_second: 2.025, interval_steps_per_second: 1.265, epoch: 19.6825[0m
[32m[2022-09-16 19:07:29,842] [    INFO][0m - loss: 0.15964816, learning_rate: 1.0264550264550265e-06, global_step: 3730, interval_runtime: 7.9203, interval_samples_per_second: 2.02, interval_steps_per_second: 1.263, epoch: 19.7354[0m
[32m[2022-09-16 19:07:37,725] [    INFO][0m - loss: 0.09834808, learning_rate: 1.021164021164021e-06, global_step: 3740, interval_runtime: 7.8833, interval_samples_per_second: 2.03, interval_steps_per_second: 1.268, epoch: 19.7884[0m
[32m[2022-09-16 19:07:45,619] [    INFO][0m - loss: 0.11828552, learning_rate: 1.0158730158730158e-06, global_step: 3750, interval_runtime: 7.8936, interval_samples_per_second: 2.027, interval_steps_per_second: 1.267, epoch: 19.8413[0m
[32m[2022-09-16 19:07:53,514] [    INFO][0m - loss: 0.15142443, learning_rate: 1.0105820105820106e-06, global_step: 3760, interval_runtime: 7.8952, interval_samples_per_second: 2.027, interval_steps_per_second: 1.267, epoch: 19.8942[0m
[32m[2022-09-16 19:08:01,409] [    INFO][0m - loss: 0.12095203, learning_rate: 1.0052910052910052e-06, global_step: 3770, interval_runtime: 7.895, interval_samples_per_second: 2.027, interval_steps_per_second: 1.267, epoch: 19.9471[0m
[32m[2022-09-16 19:08:09,119] [    INFO][0m - loss: 0.13364899, learning_rate: 1e-06, global_step: 3780, interval_runtime: 7.7104, interval_samples_per_second: 2.075, interval_steps_per_second: 1.297, epoch: 20.0[0m
[32m[2022-09-16 19:08:09,120] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:08:09,120] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-16 19:08:09,120] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:08:09,120] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:08:09,120] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-16 19:08:36,578] [    INFO][0m - eval_loss: 3.008350133895874, eval_accuracy: 0.45302257829570286, eval_runtime: 27.4571, eval_samples_per_second: 50.005, eval_steps_per_second: 3.132, epoch: 20.0[0m
[32m[2022-09-16 19:08:36,596] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-3780[0m
[32m[2022-09-16 19:08:36,596] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:08:40,727] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-3780/tokenizer_config.json[0m
[32m[2022-09-16 19:08:40,728] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-3780/special_tokens_map.json[0m
[32m[2022-09-16 19:08:56,065] [    INFO][0m - loss: 0.10915586, learning_rate: 9.947089947089947e-07, global_step: 3790, interval_runtime: 46.9458, interval_samples_per_second: 0.341, interval_steps_per_second: 0.213, epoch: 20.0529[0m
[32m[2022-09-16 19:09:03,911] [    INFO][0m - loss: 0.10345697, learning_rate: 9.894179894179893e-07, global_step: 3800, interval_runtime: 7.8461, interval_samples_per_second: 2.039, interval_steps_per_second: 1.275, epoch: 20.1058[0m
[32m[2022-09-16 19:09:11,786] [    INFO][0m - loss: 0.11555119, learning_rate: 9.84126984126984e-07, global_step: 3810, interval_runtime: 7.8752, interval_samples_per_second: 2.032, interval_steps_per_second: 1.27, epoch: 20.1587[0m
[32m[2022-09-16 19:09:19,648] [    INFO][0m - loss: 0.10247456, learning_rate: 9.788359788359789e-07, global_step: 3820, interval_runtime: 7.8617, interval_samples_per_second: 2.035, interval_steps_per_second: 1.272, epoch: 20.2116[0m
[32m[2022-09-16 19:09:27,528] [    INFO][0m - loss: 0.10527685, learning_rate: 9.735449735449734e-07, global_step: 3830, interval_runtime: 7.8802, interval_samples_per_second: 2.03, interval_steps_per_second: 1.269, epoch: 20.2646[0m
[32m[2022-09-16 19:09:35,454] [    INFO][0m - loss: 0.12055674, learning_rate: 9.682539682539682e-07, global_step: 3840, interval_runtime: 7.9258, interval_samples_per_second: 2.019, interval_steps_per_second: 1.262, epoch: 20.3175[0m
[32m[2022-09-16 19:09:43,358] [    INFO][0m - loss: 0.0738208, learning_rate: 9.62962962962963e-07, global_step: 3850, interval_runtime: 7.9042, interval_samples_per_second: 2.024, interval_steps_per_second: 1.265, epoch: 20.3704[0m
[32m[2022-09-16 19:09:51,257] [    INFO][0m - loss: 0.12260684, learning_rate: 9.576719576719576e-07, global_step: 3860, interval_runtime: 7.8987, interval_samples_per_second: 2.026, interval_steps_per_second: 1.266, epoch: 20.4233[0m
[32m[2022-09-16 19:09:59,145] [    INFO][0m - loss: 0.09915414, learning_rate: 9.523809523809523e-07, global_step: 3870, interval_runtime: 7.8878, interval_samples_per_second: 2.028, interval_steps_per_second: 1.268, epoch: 20.4762[0m
[32m[2022-09-16 19:10:07,062] [    INFO][0m - loss: 0.12304903, learning_rate: 9.47089947089947e-07, global_step: 3880, interval_runtime: 7.9165, interval_samples_per_second: 2.021, interval_steps_per_second: 1.263, epoch: 20.5291[0m
[32m[2022-09-16 19:10:14,953] [    INFO][0m - loss: 0.13051339, learning_rate: 9.417989417989418e-07, global_step: 3890, interval_runtime: 7.8909, interval_samples_per_second: 2.028, interval_steps_per_second: 1.267, epoch: 20.582[0m
[32m[2022-09-16 19:10:22,891] [    INFO][0m - loss: 0.10153358, learning_rate: 9.365079365079365e-07, global_step: 3900, interval_runtime: 7.9385, interval_samples_per_second: 2.016, interval_steps_per_second: 1.26, epoch: 20.6349[0m
[32m[2022-09-16 19:10:30,791] [    INFO][0m - loss: 0.09039064, learning_rate: 9.312169312169312e-07, global_step: 3910, interval_runtime: 7.9004, interval_samples_per_second: 2.025, interval_steps_per_second: 1.266, epoch: 20.6878[0m
[32m[2022-09-16 19:10:38,696] [    INFO][0m - loss: 0.11637845, learning_rate: 9.259259259259259e-07, global_step: 3920, interval_runtime: 7.9049, interval_samples_per_second: 2.024, interval_steps_per_second: 1.265, epoch: 20.7407[0m
[32m[2022-09-16 19:10:46,580] [    INFO][0m - loss: 0.12606406, learning_rate: 9.206349206349206e-07, global_step: 3930, interval_runtime: 7.8841, interval_samples_per_second: 2.029, interval_steps_per_second: 1.268, epoch: 20.7937[0m
[32m[2022-09-16 19:10:54,478] [    INFO][0m - loss: 0.11794467, learning_rate: 9.153439153439153e-07, global_step: 3940, interval_runtime: 7.8977, interval_samples_per_second: 2.026, interval_steps_per_second: 1.266, epoch: 20.8466[0m
[32m[2022-09-16 19:11:02,409] [    INFO][0m - loss: 0.15503335, learning_rate: 9.100529100529101e-07, global_step: 3950, interval_runtime: 7.9307, interval_samples_per_second: 2.017, interval_steps_per_second: 1.261, epoch: 20.8995[0m
[32m[2022-09-16 19:11:10,361] [    INFO][0m - loss: 0.0801829, learning_rate: 9.047619047619047e-07, global_step: 3960, interval_runtime: 7.9517, interval_samples_per_second: 2.012, interval_steps_per_second: 1.258, epoch: 20.9524[0m
[32m[2022-09-16 19:11:17,273] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:11:17,274] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-16 19:11:17,274] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:11:17,274] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:11:17,274] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-16 19:11:44,828] [    INFO][0m - eval_loss: 3.077223062515259, eval_accuracy: 0.45229424617625635, eval_runtime: 27.5541, eval_samples_per_second: 49.829, eval_steps_per_second: 3.121, epoch: 21.0[0m
[32m[2022-09-16 19:11:44,852] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-3969[0m
[32m[2022-09-16 19:11:44,853] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:11:48,710] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-3969/tokenizer_config.json[0m
[32m[2022-09-16 19:11:48,710] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-3969/special_tokens_map.json[0m
[32m[2022-09-16 19:11:57,186] [    INFO][0m - loss: 0.13522558, learning_rate: 8.994708994708994e-07, global_step: 3970, interval_runtime: 46.8253, interval_samples_per_second: 0.342, interval_steps_per_second: 0.214, epoch: 21.0053[0m
[32m[2022-09-16 19:12:05,025] [    INFO][0m - loss: 0.13899605, learning_rate: 8.941798941798942e-07, global_step: 3980, interval_runtime: 7.8393, interval_samples_per_second: 2.041, interval_steps_per_second: 1.276, epoch: 21.0582[0m
[32m[2022-09-16 19:12:12,858] [    INFO][0m - loss: 0.08576978, learning_rate: 8.888888888888889e-07, global_step: 3990, interval_runtime: 7.8328, interval_samples_per_second: 2.043, interval_steps_per_second: 1.277, epoch: 21.1111[0m
[32m[2022-09-16 19:12:20,714] [    INFO][0m - loss: 0.05757571, learning_rate: 8.835978835978835e-07, global_step: 4000, interval_runtime: 7.8563, interval_samples_per_second: 2.037, interval_steps_per_second: 1.273, epoch: 21.164[0m
[32m[2022-09-16 19:12:28,600] [    INFO][0m - loss: 0.093639, learning_rate: 8.783068783068783e-07, global_step: 4010, interval_runtime: 7.886, interval_samples_per_second: 2.029, interval_steps_per_second: 1.268, epoch: 21.2169[0m
[32m[2022-09-16 19:12:36,464] [    INFO][0m - loss: 0.07102291, learning_rate: 8.73015873015873e-07, global_step: 4020, interval_runtime: 7.8639, interval_samples_per_second: 2.035, interval_steps_per_second: 1.272, epoch: 21.2698[0m
[32m[2022-09-16 19:12:44,340] [    INFO][0m - loss: 0.06646199, learning_rate: 8.677248677248677e-07, global_step: 4030, interval_runtime: 7.8761, interval_samples_per_second: 2.031, interval_steps_per_second: 1.27, epoch: 21.3228[0m
[32m[2022-09-16 19:12:52,211] [    INFO][0m - loss: 0.14033408, learning_rate: 8.624338624338625e-07, global_step: 4040, interval_runtime: 7.8714, interval_samples_per_second: 2.033, interval_steps_per_second: 1.27, epoch: 21.3757[0m
[32m[2022-09-16 19:13:00,084] [    INFO][0m - loss: 0.07404903, learning_rate: 8.571428571428571e-07, global_step: 4050, interval_runtime: 7.8728, interval_samples_per_second: 2.032, interval_steps_per_second: 1.27, epoch: 21.4286[0m
[32m[2022-09-16 19:13:08,029] [    INFO][0m - loss: 0.08342664, learning_rate: 8.518518518518518e-07, global_step: 4060, interval_runtime: 7.945, interval_samples_per_second: 2.014, interval_steps_per_second: 1.259, epoch: 21.4815[0m
[32m[2022-09-16 19:13:15,921] [    INFO][0m - loss: 0.09095669, learning_rate: 8.465608465608465e-07, global_step: 4070, interval_runtime: 7.8921, interval_samples_per_second: 2.027, interval_steps_per_second: 1.267, epoch: 21.5344[0m
[32m[2022-09-16 19:13:23,817] [    INFO][0m - loss: 0.11925701, learning_rate: 8.412698412698413e-07, global_step: 4080, interval_runtime: 7.896, interval_samples_per_second: 2.026, interval_steps_per_second: 1.266, epoch: 21.5873[0m
[32m[2022-09-16 19:13:31,714] [    INFO][0m - loss: 0.06528174, learning_rate: 8.359788359788359e-07, global_step: 4090, interval_runtime: 7.896, interval_samples_per_second: 2.026, interval_steps_per_second: 1.266, epoch: 21.6402[0m
[32m[2022-09-16 19:13:39,600] [    INFO][0m - loss: 0.06654758, learning_rate: 8.306878306878306e-07, global_step: 4100, interval_runtime: 7.8872, interval_samples_per_second: 2.029, interval_steps_per_second: 1.268, epoch: 21.6931[0m
[32m[2022-09-16 19:13:47,486] [    INFO][0m - loss: 0.07931944, learning_rate: 8.253968253968254e-07, global_step: 4110, interval_runtime: 7.8857, interval_samples_per_second: 2.029, interval_steps_per_second: 1.268, epoch: 21.746[0m
[32m[2022-09-16 19:13:55,378] [    INFO][0m - loss: 0.09669026, learning_rate: 8.201058201058201e-07, global_step: 4120, interval_runtime: 7.8922, interval_samples_per_second: 2.027, interval_steps_per_second: 1.267, epoch: 21.7989[0m
[32m[2022-09-16 19:14:03,272] [    INFO][0m - loss: 0.11908346, learning_rate: 8.148148148148147e-07, global_step: 4130, interval_runtime: 7.894, interval_samples_per_second: 2.027, interval_steps_per_second: 1.267, epoch: 21.8519[0m
[32m[2022-09-16 19:14:11,183] [    INFO][0m - loss: 0.10819261, learning_rate: 8.095238095238095e-07, global_step: 4140, interval_runtime: 7.9109, interval_samples_per_second: 2.023, interval_steps_per_second: 1.264, epoch: 21.9048[0m
[32m[2022-09-16 19:14:19,085] [    INFO][0m - loss: 0.12746321, learning_rate: 8.042328042328042e-07, global_step: 4150, interval_runtime: 7.901, interval_samples_per_second: 2.025, interval_steps_per_second: 1.266, epoch: 21.9577[0m
[32m[2022-09-16 19:14:25,221] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:14:25,222] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-16 19:14:25,222] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:14:25,222] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:14:25,222] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-16 19:14:52,682] [    INFO][0m - eval_loss: 3.067946195602417, eval_accuracy: 0.4515659140568099, eval_runtime: 27.46, eval_samples_per_second: 50.0, eval_steps_per_second: 3.132, epoch: 22.0[0m
[32m[2022-09-16 19:14:52,709] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-4158[0m
[32m[2022-09-16 19:14:52,710] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:14:56,644] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-4158/tokenizer_config.json[0m
[32m[2022-09-16 19:14:56,645] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-4158/special_tokens_map.json[0m
[32m[2022-09-16 19:15:05,726] [    INFO][0m - loss: 0.131431, learning_rate: 7.989417989417989e-07, global_step: 4160, interval_runtime: 46.6415, interval_samples_per_second: 0.343, interval_steps_per_second: 0.214, epoch: 22.0106[0m
[32m[2022-09-16 19:15:13,581] [    INFO][0m - loss: 0.07217914, learning_rate: 7.936507936507937e-07, global_step: 4170, interval_runtime: 7.8546, interval_samples_per_second: 2.037, interval_steps_per_second: 1.273, epoch: 22.0635[0m
[32m[2022-09-16 19:15:21,431] [    INFO][0m - loss: 0.08736517, learning_rate: 7.883597883597883e-07, global_step: 4180, interval_runtime: 7.8507, interval_samples_per_second: 2.038, interval_steps_per_second: 1.274, epoch: 22.1164[0m
[32m[2022-09-16 19:15:29,255] [    INFO][0m - loss: 0.08030504, learning_rate: 7.83068783068783e-07, global_step: 4190, interval_runtime: 7.8236, interval_samples_per_second: 2.045, interval_steps_per_second: 1.278, epoch: 22.1693[0m
[32m[2022-09-16 19:15:37,271] [    INFO][0m - loss: 0.09042272, learning_rate: 7.777777777777778e-07, global_step: 4200, interval_runtime: 7.864, interval_samples_per_second: 2.035, interval_steps_per_second: 1.272, epoch: 22.2222[0m
[32m[2022-09-16 19:15:45,167] [    INFO][0m - loss: 0.08137882, learning_rate: 7.724867724867725e-07, global_step: 4210, interval_runtime: 8.0476, interval_samples_per_second: 1.988, interval_steps_per_second: 1.243, epoch: 22.2751[0m
[32m[2022-09-16 19:15:53,054] [    INFO][0m - loss: 0.07152944, learning_rate: 7.671957671957671e-07, global_step: 4220, interval_runtime: 7.8872, interval_samples_per_second: 2.029, interval_steps_per_second: 1.268, epoch: 22.328[0m
[32m[2022-09-16 19:16:00,928] [    INFO][0m - loss: 0.06094825, learning_rate: 7.619047619047619e-07, global_step: 4230, interval_runtime: 7.8741, interval_samples_per_second: 2.032, interval_steps_per_second: 1.27, epoch: 22.381[0m
[32m[2022-09-16 19:16:08,799] [    INFO][0m - loss: 0.0882988, learning_rate: 7.566137566137566e-07, global_step: 4240, interval_runtime: 7.8712, interval_samples_per_second: 2.033, interval_steps_per_second: 1.27, epoch: 22.4339[0m
[32m[2022-09-16 19:16:17,049] [    INFO][0m - loss: 0.06820889, learning_rate: 7.513227513227513e-07, global_step: 4250, interval_runtime: 7.9002, interval_samples_per_second: 2.025, interval_steps_per_second: 1.266, epoch: 22.4868[0m
[32m[2022-09-16 19:16:24,913] [    INFO][0m - loss: 0.06436305, learning_rate: 7.46031746031746e-07, global_step: 4260, interval_runtime: 8.2142, interval_samples_per_second: 1.948, interval_steps_per_second: 1.217, epoch: 22.5397[0m
[32m[2022-09-16 19:16:32,811] [    INFO][0m - loss: 0.07923707, learning_rate: 7.407407407407407e-07, global_step: 4270, interval_runtime: 7.8981, interval_samples_per_second: 2.026, interval_steps_per_second: 1.266, epoch: 22.5926[0m
[32m[2022-09-16 19:16:40,671] [    INFO][0m - loss: 0.05562925, learning_rate: 7.354497354497354e-07, global_step: 4280, interval_runtime: 7.86, interval_samples_per_second: 2.036, interval_steps_per_second: 1.272, epoch: 22.6455[0m
[32m[2022-09-16 19:16:48,561] [    INFO][0m - loss: 0.10885741, learning_rate: 7.301587301587302e-07, global_step: 4290, interval_runtime: 7.889, interval_samples_per_second: 2.028, interval_steps_per_second: 1.268, epoch: 22.6984[0m
[32m[2022-09-16 19:16:56,415] [    INFO][0m - loss: 0.06343164, learning_rate: 7.248677248677249e-07, global_step: 4300, interval_runtime: 7.8547, interval_samples_per_second: 2.037, interval_steps_per_second: 1.273, epoch: 22.7513[0m
[32m[2022-09-16 19:17:04,317] [    INFO][0m - loss: 0.09379265, learning_rate: 7.195767195767195e-07, global_step: 4310, interval_runtime: 7.9021, interval_samples_per_second: 2.025, interval_steps_per_second: 1.265, epoch: 22.8042[0m
[32m[2022-09-16 19:17:12,176] [    INFO][0m - loss: 0.149857, learning_rate: 7.142857142857143e-07, global_step: 4320, interval_runtime: 7.859, interval_samples_per_second: 2.036, interval_steps_per_second: 1.272, epoch: 22.8571[0m
[32m[2022-09-16 19:17:20,041] [    INFO][0m - loss: 0.10936576, learning_rate: 7.08994708994709e-07, global_step: 4330, interval_runtime: 7.8644, interval_samples_per_second: 2.034, interval_steps_per_second: 1.272, epoch: 22.9101[0m
[32m[2022-09-16 19:17:27,927] [    INFO][0m - loss: 0.11805322, learning_rate: 7.037037037037037e-07, global_step: 4340, interval_runtime: 7.8864, interval_samples_per_second: 2.029, interval_steps_per_second: 1.268, epoch: 22.963[0m
[32m[2022-09-16 19:17:33,274] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:17:33,274] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-16 19:17:33,274] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:17:33,274] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:17:33,275] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-16 19:18:00,712] [    INFO][0m - eval_loss: 3.104764699935913, eval_accuracy: 0.45229424617625635, eval_runtime: 27.4374, eval_samples_per_second: 50.041, eval_steps_per_second: 3.134, epoch: 23.0[0m
[32m[2022-09-16 19:18:00,734] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-4347[0m
[32m[2022-09-16 19:18:00,734] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:18:04,583] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-4347/tokenizer_config.json[0m
[32m[2022-09-16 19:18:04,583] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-4347/special_tokens_map.json[0m
[32m[2022-09-16 19:18:15,412] [    INFO][0m - loss: 0.05921189, learning_rate: 6.984126984126983e-07, global_step: 4350, interval_runtime: 47.4845, interval_samples_per_second: 0.337, interval_steps_per_second: 0.211, epoch: 23.0159[0m
[32m[2022-09-16 19:18:23,306] [    INFO][0m - loss: 0.07816586, learning_rate: 6.931216931216931e-07, global_step: 4360, interval_runtime: 7.8943, interval_samples_per_second: 2.027, interval_steps_per_second: 1.267, epoch: 23.0688[0m
[32m[2022-09-16 19:18:31,223] [    INFO][0m - loss: 0.06361455, learning_rate: 6.878306878306878e-07, global_step: 4370, interval_runtime: 7.9166, interval_samples_per_second: 2.021, interval_steps_per_second: 1.263, epoch: 23.1217[0m
[32m[2022-09-16 19:18:39,119] [    INFO][0m - loss: 0.0655305, learning_rate: 6.825396825396825e-07, global_step: 4380, interval_runtime: 7.8968, interval_samples_per_second: 2.026, interval_steps_per_second: 1.266, epoch: 23.1746[0m
[32m[2022-09-16 19:18:47,044] [    INFO][0m - loss: 0.05799478, learning_rate: 6.772486772486773e-07, global_step: 4390, interval_runtime: 7.9245, interval_samples_per_second: 2.019, interval_steps_per_second: 1.262, epoch: 23.2275[0m
[32m[2022-09-16 19:18:54,949] [    INFO][0m - loss: 0.06834532, learning_rate: 6.719576719576719e-07, global_step: 4400, interval_runtime: 7.9056, interval_samples_per_second: 2.024, interval_steps_per_second: 1.265, epoch: 23.2804[0m
[32m[2022-09-16 19:19:02,861] [    INFO][0m - loss: 0.09213768, learning_rate: 6.666666666666666e-07, global_step: 4410, interval_runtime: 7.9116, interval_samples_per_second: 2.022, interval_steps_per_second: 1.264, epoch: 23.3333[0m
[32m[2022-09-16 19:19:10,766] [    INFO][0m - loss: 0.06385635, learning_rate: 6.613756613756614e-07, global_step: 4420, interval_runtime: 7.9047, interval_samples_per_second: 2.024, interval_steps_per_second: 1.265, epoch: 23.3862[0m
[32m[2022-09-16 19:19:18,692] [    INFO][0m - loss: 0.08629122, learning_rate: 6.560846560846561e-07, global_step: 4430, interval_runtime: 7.9255, interval_samples_per_second: 2.019, interval_steps_per_second: 1.262, epoch: 23.4392[0m
[32m[2022-09-16 19:19:26,629] [    INFO][0m - loss: 0.06574113, learning_rate: 6.507936507936507e-07, global_step: 4440, interval_runtime: 7.937, interval_samples_per_second: 2.016, interval_steps_per_second: 1.26, epoch: 23.4921[0m
[32m[2022-09-16 19:19:34,534] [    INFO][0m - loss: 0.08621534, learning_rate: 6.455026455026455e-07, global_step: 4450, interval_runtime: 7.9057, interval_samples_per_second: 2.024, interval_steps_per_second: 1.265, epoch: 23.545[0m
[32m[2022-09-16 19:19:42,458] [    INFO][0m - loss: 0.10059209, learning_rate: 6.402116402116402e-07, global_step: 4460, interval_runtime: 7.9242, interval_samples_per_second: 2.019, interval_steps_per_second: 1.262, epoch: 23.5979[0m
[32m[2022-09-16 19:19:50,345] [    INFO][0m - loss: 0.0708638, learning_rate: 6.349206349206349e-07, global_step: 4470, interval_runtime: 7.8867, interval_samples_per_second: 2.029, interval_steps_per_second: 1.268, epoch: 23.6508[0m
[32m[2022-09-16 19:19:58,299] [    INFO][0m - loss: 0.08706506, learning_rate: 6.296296296296296e-07, global_step: 4480, interval_runtime: 7.9546, interval_samples_per_second: 2.011, interval_steps_per_second: 1.257, epoch: 23.7037[0m
[32m[2022-09-16 19:20:06,230] [    INFO][0m - loss: 0.0899094, learning_rate: 6.243386243386243e-07, global_step: 4490, interval_runtime: 7.9303, interval_samples_per_second: 2.018, interval_steps_per_second: 1.261, epoch: 23.7566[0m
[32m[2022-09-16 19:20:14,135] [    INFO][0m - loss: 0.08284059, learning_rate: 6.19047619047619e-07, global_step: 4500, interval_runtime: 7.9054, interval_samples_per_second: 2.024, interval_steps_per_second: 1.265, epoch: 23.8095[0m
[32m[2022-09-16 19:20:22,073] [    INFO][0m - loss: 0.1210157, learning_rate: 6.137566137566138e-07, global_step: 4510, interval_runtime: 7.9376, interval_samples_per_second: 2.016, interval_steps_per_second: 1.26, epoch: 23.8624[0m
[32m[2022-09-16 19:20:29,965] [    INFO][0m - loss: 0.05789696, learning_rate: 6.084656084656085e-07, global_step: 4520, interval_runtime: 7.8914, interval_samples_per_second: 2.028, interval_steps_per_second: 1.267, epoch: 23.9153[0m
[32m[2022-09-16 19:20:37,874] [    INFO][0m - loss: 0.06111463, learning_rate: 6.031746031746031e-07, global_step: 4530, interval_runtime: 7.91, interval_samples_per_second: 2.023, interval_steps_per_second: 1.264, epoch: 23.9683[0m
[32m[2022-09-16 19:20:42,429] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:20:42,967] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-16 19:20:42,968] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:20:42,968] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:20:42,968] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-16 19:21:10,214] [    INFO][0m - eval_loss: 3.148744821548462, eval_accuracy: 0.4515659140568099, eval_runtime: 27.7839, eval_samples_per_second: 49.417, eval_steps_per_second: 3.095, epoch: 24.0[0m
[32m[2022-09-16 19:21:10,237] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-4536[0m
[32m[2022-09-16 19:21:10,237] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:21:13,064] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-4536/tokenizer_config.json[0m
[32m[2022-09-16 19:21:13,064] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-4536/special_tokens_map.json[0m
[32m[2022-09-16 19:21:22,712] [    INFO][0m - loss: 0.06646225, learning_rate: 5.978835978835979e-07, global_step: 4540, interval_runtime: 44.838, interval_samples_per_second: 0.357, interval_steps_per_second: 0.223, epoch: 24.0212[0m
[32m[2022-09-16 19:21:30,579] [    INFO][0m - loss: 0.0527242, learning_rate: 5.925925925925926e-07, global_step: 4550, interval_runtime: 7.867, interval_samples_per_second: 2.034, interval_steps_per_second: 1.271, epoch: 24.0741[0m
[32m[2022-09-16 19:21:38,495] [    INFO][0m - loss: 0.04790784, learning_rate: 5.873015873015873e-07, global_step: 4560, interval_runtime: 7.9157, interval_samples_per_second: 2.021, interval_steps_per_second: 1.263, epoch: 24.127[0m
[32m[2022-09-16 19:21:46,412] [    INFO][0m - loss: 0.05027224, learning_rate: 5.82010582010582e-07, global_step: 4570, interval_runtime: 7.9171, interval_samples_per_second: 2.021, interval_steps_per_second: 1.263, epoch: 24.1799[0m
[32m[2022-09-16 19:21:54,335] [    INFO][0m - loss: 0.07373688, learning_rate: 5.767195767195767e-07, global_step: 4580, interval_runtime: 7.9231, interval_samples_per_second: 2.019, interval_steps_per_second: 1.262, epoch: 24.2328[0m
[32m[2022-09-16 19:22:02,261] [    INFO][0m - loss: 0.05629541, learning_rate: 5.714285714285714e-07, global_step: 4590, interval_runtime: 7.9259, interval_samples_per_second: 2.019, interval_steps_per_second: 1.262, epoch: 24.2857[0m
[32m[2022-09-16 19:22:10,151] [    INFO][0m - loss: 0.05600222, learning_rate: 5.661375661375662e-07, global_step: 4600, interval_runtime: 7.8899, interval_samples_per_second: 2.028, interval_steps_per_second: 1.267, epoch: 24.3386[0m
[32m[2022-09-16 19:22:18,072] [    INFO][0m - loss: 0.06792182, learning_rate: 5.608465608465608e-07, global_step: 4610, interval_runtime: 7.9208, interval_samples_per_second: 2.02, interval_steps_per_second: 1.263, epoch: 24.3915[0m
[32m[2022-09-16 19:22:26,012] [    INFO][0m - loss: 0.06844745, learning_rate: 5.555555555555555e-07, global_step: 4620, interval_runtime: 7.9402, interval_samples_per_second: 2.015, interval_steps_per_second: 1.259, epoch: 24.4444[0m
[32m[2022-09-16 19:22:33,921] [    INFO][0m - loss: 0.09018061, learning_rate: 5.502645502645503e-07, global_step: 4630, interval_runtime: 7.9093, interval_samples_per_second: 2.023, interval_steps_per_second: 1.264, epoch: 24.4974[0m
[32m[2022-09-16 19:22:41,821] [    INFO][0m - loss: 0.07566237, learning_rate: 5.44973544973545e-07, global_step: 4640, interval_runtime: 7.8999, interval_samples_per_second: 2.025, interval_steps_per_second: 1.266, epoch: 24.5503[0m
[32m[2022-09-16 19:22:49,763] [    INFO][0m - loss: 0.06722547, learning_rate: 5.396825396825396e-07, global_step: 4650, interval_runtime: 7.9414, interval_samples_per_second: 2.015, interval_steps_per_second: 1.259, epoch: 24.6032[0m
[32m[2022-09-16 19:22:57,695] [    INFO][0m - loss: 0.10952835, learning_rate: 5.343915343915343e-07, global_step: 4660, interval_runtime: 7.9326, interval_samples_per_second: 2.017, interval_steps_per_second: 1.261, epoch: 24.6561[0m
[32m[2022-09-16 19:23:05,565] [    INFO][0m - loss: 0.0429437, learning_rate: 5.291005291005291e-07, global_step: 4670, interval_runtime: 7.8699, interval_samples_per_second: 2.033, interval_steps_per_second: 1.271, epoch: 24.709[0m
[32m[2022-09-16 19:23:13,531] [    INFO][0m - loss: 0.09022937, learning_rate: 5.238095238095238e-07, global_step: 4680, interval_runtime: 7.9655, interval_samples_per_second: 2.009, interval_steps_per_second: 1.255, epoch: 24.7619[0m
[32m[2022-09-16 19:23:21,443] [    INFO][0m - loss: 0.10281619, learning_rate: 5.185185185185185e-07, global_step: 4690, interval_runtime: 7.912, interval_samples_per_second: 2.022, interval_steps_per_second: 1.264, epoch: 24.8148[0m
[32m[2022-09-16 19:23:29,338] [    INFO][0m - loss: 0.06497025, learning_rate: 5.132275132275132e-07, global_step: 4700, interval_runtime: 7.8951, interval_samples_per_second: 2.027, interval_steps_per_second: 1.267, epoch: 24.8677[0m
[32m[2022-09-16 19:23:37,282] [    INFO][0m - loss: 0.07508305, learning_rate: 5.079365079365079e-07, global_step: 4710, interval_runtime: 7.9444, interval_samples_per_second: 2.014, interval_steps_per_second: 1.259, epoch: 24.9206[0m
[32m[2022-09-16 19:23:45,169] [    INFO][0m - loss: 0.05587666, learning_rate: 5.026455026455026e-07, global_step: 4720, interval_runtime: 7.8867, interval_samples_per_second: 2.029, interval_steps_per_second: 1.268, epoch: 24.9735[0m
[32m[2022-09-16 19:23:48,929] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:23:48,929] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-16 19:23:48,930] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:23:48,930] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:23:48,930] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-16 19:24:16,076] [    INFO][0m - eval_loss: 3.1855878829956055, eval_accuracy: 0.45447924253459576, eval_runtime: 27.1463, eval_samples_per_second: 50.578, eval_steps_per_second: 3.168, epoch: 25.0[0m
[32m[2022-09-16 19:24:16,095] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-4725[0m
[32m[2022-09-16 19:24:16,095] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:24:19,021] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-4725/tokenizer_config.json[0m
[32m[2022-09-16 19:24:19,021] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-4725/special_tokens_map.json[0m
[32m[2022-09-16 19:24:28,420] [    INFO][0m - loss: 0.08290733, learning_rate: 4.973544973544974e-07, global_step: 4730, interval_runtime: 43.2505, interval_samples_per_second: 0.37, interval_steps_per_second: 0.231, epoch: 25.0265[0m
[32m[2022-09-16 19:24:36,287] [    INFO][0m - loss: 0.05667349, learning_rate: 4.92063492063492e-07, global_step: 4740, interval_runtime: 7.8671, interval_samples_per_second: 2.034, interval_steps_per_second: 1.271, epoch: 25.0794[0m
[32m[2022-09-16 19:24:44,199] [    INFO][0m - loss: 0.04974589, learning_rate: 4.867724867724867e-07, global_step: 4750, interval_runtime: 7.912, interval_samples_per_second: 2.022, interval_steps_per_second: 1.264, epoch: 25.1323[0m
[32m[2022-09-16 19:24:52,115] [    INFO][0m - loss: 0.1091229, learning_rate: 4.814814814814815e-07, global_step: 4760, interval_runtime: 7.9166, interval_samples_per_second: 2.021, interval_steps_per_second: 1.263, epoch: 25.1852[0m
[32m[2022-09-16 19:25:06,694] [    INFO][0m - loss: 0.05861727, learning_rate: 4.761904761904762e-07, global_step: 4770, interval_runtime: 12.7033, interval_samples_per_second: 1.26, interval_steps_per_second: 0.787, epoch: 25.2381[0m
[32m[2022-09-16 19:25:14,650] [    INFO][0m - loss: 0.06695886, learning_rate: 4.708994708994709e-07, global_step: 4780, interval_runtime: 9.8308, interval_samples_per_second: 1.628, interval_steps_per_second: 1.017, epoch: 25.291[0m
[32m[2022-09-16 19:25:22,599] [    INFO][0m - loss: 0.05336263, learning_rate: 4.656084656084656e-07, global_step: 4790, interval_runtime: 7.9489, interval_samples_per_second: 2.013, interval_steps_per_second: 1.258, epoch: 25.3439[0m
[32m[2022-09-16 19:25:30,603] [    INFO][0m - loss: 0.08276443, learning_rate: 4.603174603174603e-07, global_step: 4800, interval_runtime: 8.0042, interval_samples_per_second: 1.999, interval_steps_per_second: 1.249, epoch: 25.3968[0m
[32m[2022-09-16 19:25:38,529] [    INFO][0m - loss: 0.07443403, learning_rate: 4.5502645502645503e-07, global_step: 4810, interval_runtime: 7.9256, interval_samples_per_second: 2.019, interval_steps_per_second: 1.262, epoch: 25.4497[0m
[32m[2022-09-16 19:25:46,438] [    INFO][0m - loss: 0.07463958, learning_rate: 4.497354497354497e-07, global_step: 4820, interval_runtime: 7.9101, interval_samples_per_second: 2.023, interval_steps_per_second: 1.264, epoch: 25.5026[0m
[32m[2022-09-16 19:25:54,329] [    INFO][0m - loss: 0.06405895, learning_rate: 4.4444444444444444e-07, global_step: 4830, interval_runtime: 7.8907, interval_samples_per_second: 2.028, interval_steps_per_second: 1.267, epoch: 25.5556[0m
[32m[2022-09-16 19:26:02,239] [    INFO][0m - loss: 0.09407061, learning_rate: 4.3915343915343916e-07, global_step: 4840, interval_runtime: 7.9099, interval_samples_per_second: 2.023, interval_steps_per_second: 1.264, epoch: 25.6085[0m
[32m[2022-09-16 19:26:10,160] [    INFO][0m - loss: 0.04167008, learning_rate: 4.3386243386243384e-07, global_step: 4850, interval_runtime: 7.9209, interval_samples_per_second: 2.02, interval_steps_per_second: 1.262, epoch: 25.6614[0m
[32m[2022-09-16 19:26:19,392] [    INFO][0m - loss: 0.06207829, learning_rate: 4.2857142857142857e-07, global_step: 4860, interval_runtime: 7.8728, interval_samples_per_second: 2.032, interval_steps_per_second: 1.27, epoch: 25.7143[0m
[32m[2022-09-16 19:26:27,305] [    INFO][0m - loss: 0.05228812, learning_rate: 4.2328042328042324e-07, global_step: 4870, interval_runtime: 9.2721, interval_samples_per_second: 1.726, interval_steps_per_second: 1.078, epoch: 25.7672[0m
[32m[2022-09-16 19:26:35,182] [    INFO][0m - loss: 0.04586224, learning_rate: 4.1798941798941797e-07, global_step: 4880, interval_runtime: 7.877, interval_samples_per_second: 2.031, interval_steps_per_second: 1.27, epoch: 25.8201[0m
[32m[2022-09-16 19:26:43,071] [    INFO][0m - loss: 0.08212435, learning_rate: 4.126984126984127e-07, global_step: 4890, interval_runtime: 7.8896, interval_samples_per_second: 2.028, interval_steps_per_second: 1.267, epoch: 25.873[0m
[32m[2022-09-16 19:26:51,003] [    INFO][0m - loss: 0.04112041, learning_rate: 4.0740740740740737e-07, global_step: 4900, interval_runtime: 7.9314, interval_samples_per_second: 2.017, interval_steps_per_second: 1.261, epoch: 25.9259[0m
[32m[2022-09-16 19:26:58,892] [    INFO][0m - loss: 0.08367927, learning_rate: 4.021164021164021e-07, global_step: 4910, interval_runtime: 7.8886, interval_samples_per_second: 2.028, interval_steps_per_second: 1.268, epoch: 25.9788[0m
[32m[2022-09-16 19:27:01,902] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:27:01,902] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-16 19:27:01,902] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:27:01,902] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:27:01,902] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-16 19:27:29,199] [    INFO][0m - eval_loss: 3.2180209159851074, eval_accuracy: 0.4515659140568099, eval_runtime: 27.2964, eval_samples_per_second: 50.3, eval_steps_per_second: 3.151, epoch: 26.0[0m
[32m[2022-09-16 19:27:30,147] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-4914[0m
[32m[2022-09-16 19:27:30,147] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:27:33,972] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-4914/tokenizer_config.json[0m
[32m[2022-09-16 19:27:33,972] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-4914/special_tokens_map.json[0m
[32m[2022-09-16 19:27:45,641] [    INFO][0m - loss: 0.07211167, learning_rate: 3.9682539682539683e-07, global_step: 4920, interval_runtime: 46.7497, interval_samples_per_second: 0.342, interval_steps_per_second: 0.214, epoch: 26.0317[0m
[32m[2022-09-16 19:27:53,536] [    INFO][0m - loss: 0.0843142, learning_rate: 3.915343915343915e-07, global_step: 4930, interval_runtime: 7.8949, interval_samples_per_second: 2.027, interval_steps_per_second: 1.267, epoch: 26.0847[0m
[32m[2022-09-16 19:28:01,466] [    INFO][0m - loss: 0.06587306, learning_rate: 3.8624338624338623e-07, global_step: 4940, interval_runtime: 7.9297, interval_samples_per_second: 2.018, interval_steps_per_second: 1.261, epoch: 26.1376[0m
[32m[2022-09-16 19:28:09,391] [    INFO][0m - loss: 0.06550936, learning_rate: 3.8095238095238096e-07, global_step: 4950, interval_runtime: 7.9246, interval_samples_per_second: 2.019, interval_steps_per_second: 1.262, epoch: 26.1905[0m
[32m[2022-09-16 19:28:17,306] [    INFO][0m - loss: 0.04710406, learning_rate: 3.7566137566137564e-07, global_step: 4960, interval_runtime: 7.9151, interval_samples_per_second: 2.021, interval_steps_per_second: 1.263, epoch: 26.2434[0m
[32m[2022-09-16 19:28:25,222] [    INFO][0m - loss: 0.03672433, learning_rate: 3.7037037037037036e-07, global_step: 4970, interval_runtime: 7.916, interval_samples_per_second: 2.021, interval_steps_per_second: 1.263, epoch: 26.2963[0m
[32m[2022-09-16 19:28:33,103] [    INFO][0m - loss: 0.08440167, learning_rate: 3.650793650793651e-07, global_step: 4980, interval_runtime: 7.8817, interval_samples_per_second: 2.03, interval_steps_per_second: 1.269, epoch: 26.3492[0m
[32m[2022-09-16 19:28:41,027] [    INFO][0m - loss: 0.05029717, learning_rate: 3.5978835978835977e-07, global_step: 4990, interval_runtime: 7.9237, interval_samples_per_second: 2.019, interval_steps_per_second: 1.262, epoch: 26.4021[0m
[32m[2022-09-16 19:28:50,901] [    INFO][0m - loss: 0.05894041, learning_rate: 3.544973544973545e-07, global_step: 5000, interval_runtime: 7.9131, interval_samples_per_second: 2.022, interval_steps_per_second: 1.264, epoch: 26.455[0m
[32m[2022-09-16 19:29:00,929] [    INFO][0m - loss: 0.04572518, learning_rate: 3.4920634920634917e-07, global_step: 5010, interval_runtime: 9.8269, interval_samples_per_second: 1.628, interval_steps_per_second: 1.018, epoch: 26.5079[0m
[32m[2022-09-16 19:29:08,809] [    INFO][0m - loss: 0.05274822, learning_rate: 3.439153439153439e-07, global_step: 5020, interval_runtime: 10.0415, interval_samples_per_second: 1.593, interval_steps_per_second: 0.996, epoch: 26.5608[0m
[32m[2022-09-16 19:29:16,691] [    INFO][0m - loss: 0.04774912, learning_rate: 3.386243386243386e-07, global_step: 5030, interval_runtime: 7.8833, interval_samples_per_second: 2.03, interval_steps_per_second: 1.269, epoch: 26.6138[0m
[32m[2022-09-16 19:29:24,591] [    INFO][0m - loss: 0.04857178, learning_rate: 3.333333333333333e-07, global_step: 5040, interval_runtime: 7.9, interval_samples_per_second: 2.025, interval_steps_per_second: 1.266, epoch: 26.6667[0m
[32m[2022-09-16 19:29:32,470] [    INFO][0m - loss: 0.04574185, learning_rate: 3.2804232804232803e-07, global_step: 5050, interval_runtime: 7.8782, interval_samples_per_second: 2.031, interval_steps_per_second: 1.269, epoch: 26.7196[0m
[32m[2022-09-16 19:29:40,378] [    INFO][0m - loss: 0.05686905, learning_rate: 3.2275132275132276e-07, global_step: 5060, interval_runtime: 7.9086, interval_samples_per_second: 2.023, interval_steps_per_second: 1.264, epoch: 26.7725[0m
[32m[2022-09-16 19:29:48,283] [    INFO][0m - loss: 0.05323265, learning_rate: 3.1746031746031743e-07, global_step: 5070, interval_runtime: 7.905, interval_samples_per_second: 2.024, interval_steps_per_second: 1.265, epoch: 26.8254[0m
[32m[2022-09-16 19:29:56,232] [    INFO][0m - loss: 0.05617563, learning_rate: 3.1216931216931216e-07, global_step: 5080, interval_runtime: 7.9483, interval_samples_per_second: 2.013, interval_steps_per_second: 1.258, epoch: 26.8783[0m
[32m[2022-09-16 19:30:04,146] [    INFO][0m - loss: 0.05898752, learning_rate: 3.068783068783069e-07, global_step: 5090, interval_runtime: 7.9138, interval_samples_per_second: 2.022, interval_steps_per_second: 1.264, epoch: 26.9312[0m
[32m[2022-09-16 19:30:11,954] [    INFO][0m - loss: 0.05700929, learning_rate: 3.0158730158730156e-07, global_step: 5100, interval_runtime: 7.8084, interval_samples_per_second: 2.049, interval_steps_per_second: 1.281, epoch: 26.9841[0m
[32m[2022-09-16 19:30:14,204] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:30:14,204] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-16 19:30:14,204] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:30:14,204] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:30:14,204] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-16 19:30:41,362] [    INFO][0m - eval_loss: 3.2357754707336426, eval_accuracy: 0.45520757465404227, eval_runtime: 27.1577, eval_samples_per_second: 50.556, eval_steps_per_second: 3.167, epoch: 27.0[0m
[32m[2022-09-16 19:30:41,382] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-5103[0m
[32m[2022-09-16 19:30:41,382] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:30:44,969] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-5103/tokenizer_config.json[0m
[32m[2022-09-16 19:30:44,970] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-5103/special_tokens_map.json[0m
[32m[2022-09-16 19:30:57,356] [    INFO][0m - loss: 0.05944324, learning_rate: 2.962962962962963e-07, global_step: 5110, interval_runtime: 45.4022, interval_samples_per_second: 0.352, interval_steps_per_second: 0.22, epoch: 27.037[0m
[32m[2022-09-16 19:31:05,200] [    INFO][0m - loss: 0.05909737, learning_rate: 2.91005291005291e-07, global_step: 5120, interval_runtime: 7.8436, interval_samples_per_second: 2.04, interval_steps_per_second: 1.275, epoch: 27.0899[0m
[32m[2022-09-16 19:31:13,077] [    INFO][0m - loss: 0.06252713, learning_rate: 2.857142857142857e-07, global_step: 5130, interval_runtime: 7.8774, interval_samples_per_second: 2.031, interval_steps_per_second: 1.269, epoch: 27.1429[0m
[32m[2022-09-16 19:31:21,033] [    INFO][0m - loss: 0.06600582, learning_rate: 2.804232804232804e-07, global_step: 5140, interval_runtime: 7.9554, interval_samples_per_second: 2.011, interval_steps_per_second: 1.257, epoch: 27.1958[0m
[32m[2022-09-16 19:31:28,915] [    INFO][0m - loss: 0.05886845, learning_rate: 2.7513227513227515e-07, global_step: 5150, interval_runtime: 7.8818, interval_samples_per_second: 2.03, interval_steps_per_second: 1.269, epoch: 27.2487[0m
[32m[2022-09-16 19:31:36,820] [    INFO][0m - loss: 0.04750211, learning_rate: 2.698412698412698e-07, global_step: 5160, interval_runtime: 7.906, interval_samples_per_second: 2.024, interval_steps_per_second: 1.265, epoch: 27.3016[0m
[32m[2022-09-16 19:31:44,710] [    INFO][0m - loss: 0.05702514, learning_rate: 2.6455026455026455e-07, global_step: 5170, interval_runtime: 7.8893, interval_samples_per_second: 2.028, interval_steps_per_second: 1.268, epoch: 27.3545[0m
[32m[2022-09-16 19:31:52,597] [    INFO][0m - loss: 0.07920443, learning_rate: 2.5925925925925923e-07, global_step: 5180, interval_runtime: 7.8869, interval_samples_per_second: 2.029, interval_steps_per_second: 1.268, epoch: 27.4074[0m
[32m[2022-09-16 19:32:00,489] [    INFO][0m - loss: 0.07020521, learning_rate: 2.5396825396825396e-07, global_step: 5190, interval_runtime: 7.8927, interval_samples_per_second: 2.027, interval_steps_per_second: 1.267, epoch: 27.4603[0m
[32m[2022-09-16 19:32:08,393] [    INFO][0m - loss: 0.03496701, learning_rate: 2.486772486772487e-07, global_step: 5200, interval_runtime: 7.9037, interval_samples_per_second: 2.024, interval_steps_per_second: 1.265, epoch: 27.5132[0m
[32m[2022-09-16 19:32:16,300] [    INFO][0m - loss: 0.06983746, learning_rate: 2.4338624338624336e-07, global_step: 5210, interval_runtime: 7.9065, interval_samples_per_second: 2.024, interval_steps_per_second: 1.265, epoch: 27.5661[0m
[32m[2022-09-16 19:32:24,222] [    INFO][0m - loss: 0.05188677, learning_rate: 2.380952380952381e-07, global_step: 5220, interval_runtime: 7.9218, interval_samples_per_second: 2.02, interval_steps_per_second: 1.262, epoch: 27.619[0m
[32m[2022-09-16 19:32:32,111] [    INFO][0m - loss: 0.05547163, learning_rate: 2.328042328042328e-07, global_step: 5230, interval_runtime: 7.8893, interval_samples_per_second: 2.028, interval_steps_per_second: 1.268, epoch: 27.672[0m
[32m[2022-09-16 19:32:40,008] [    INFO][0m - loss: 0.08251234, learning_rate: 2.2751322751322752e-07, global_step: 5240, interval_runtime: 7.897, interval_samples_per_second: 2.026, interval_steps_per_second: 1.266, epoch: 27.7249[0m
[32m[2022-09-16 19:32:47,927] [    INFO][0m - loss: 0.04295819, learning_rate: 2.2222222222222222e-07, global_step: 5250, interval_runtime: 7.9185, interval_samples_per_second: 2.021, interval_steps_per_second: 1.263, epoch: 27.7778[0m
[32m[2022-09-16 19:32:55,813] [    INFO][0m - loss: 0.03893862, learning_rate: 2.1693121693121692e-07, global_step: 5260, interval_runtime: 7.8867, interval_samples_per_second: 2.029, interval_steps_per_second: 1.268, epoch: 27.8307[0m
[32m[2022-09-16 19:33:03,733] [    INFO][0m - loss: 0.05667893, learning_rate: 2.1164021164021162e-07, global_step: 5270, interval_runtime: 7.9201, interval_samples_per_second: 2.02, interval_steps_per_second: 1.263, epoch: 27.8836[0m
[32m[2022-09-16 19:33:12,444] [    INFO][0m - loss: 0.06546857, learning_rate: 2.0634920634920635e-07, global_step: 5280, interval_runtime: 7.9321, interval_samples_per_second: 2.017, interval_steps_per_second: 1.261, epoch: 27.9365[0m
[32m[2022-09-16 19:33:20,244] [    INFO][0m - loss: 0.07217513, learning_rate: 2.0105820105820105e-07, global_step: 5290, interval_runtime: 8.5783, interval_samples_per_second: 1.865, interval_steps_per_second: 1.166, epoch: 27.9894[0m
[32m[2022-09-16 19:33:21,748] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:33:21,748] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-16 19:33:21,749] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:33:21,749] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:33:21,749] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-16 19:33:49,088] [    INFO][0m - eval_loss: 3.2422358989715576, eval_accuracy: 0.45520757465404227, eval_runtime: 27.3394, eval_samples_per_second: 50.221, eval_steps_per_second: 3.146, epoch: 28.0[0m
[32m[2022-09-16 19:33:49,106] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-5292[0m
[32m[2022-09-16 19:33:49,106] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:33:52,096] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-5292/tokenizer_config.json[0m
[32m[2022-09-16 19:33:52,097] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-5292/special_tokens_map.json[0m
[32m[2022-09-16 19:34:05,063] [    INFO][0m - loss: 0.05361167, learning_rate: 1.9576719576719575e-07, global_step: 5300, interval_runtime: 44.8189, interval_samples_per_second: 0.357, interval_steps_per_second: 0.223, epoch: 28.0423[0m
[32m[2022-09-16 19:34:12,961] [    INFO][0m - loss: 0.06284164, learning_rate: 1.9047619047619048e-07, global_step: 5310, interval_runtime: 7.8986, interval_samples_per_second: 2.026, interval_steps_per_second: 1.266, epoch: 28.0952[0m
[32m[2022-09-16 19:34:20,864] [    INFO][0m - loss: 0.08012289, learning_rate: 1.8518518518518518e-07, global_step: 5320, interval_runtime: 7.9024, interval_samples_per_second: 2.025, interval_steps_per_second: 1.265, epoch: 28.1481[0m
[32m[2022-09-16 19:34:29,656] [    INFO][0m - loss: 0.05107691, learning_rate: 1.7989417989417988e-07, global_step: 5330, interval_runtime: 7.9254, interval_samples_per_second: 2.019, interval_steps_per_second: 1.262, epoch: 28.2011[0m
[32m[2022-09-16 19:34:37,556] [    INFO][0m - loss: 0.03855273, learning_rate: 1.7460317460317458e-07, global_step: 5340, interval_runtime: 8.7667, interval_samples_per_second: 1.825, interval_steps_per_second: 1.141, epoch: 28.254[0m
[32m[2022-09-16 19:34:45,476] [    INFO][0m - loss: 0.05024914, learning_rate: 1.693121693121693e-07, global_step: 5350, interval_runtime: 7.9207, interval_samples_per_second: 2.02, interval_steps_per_second: 1.263, epoch: 28.3069[0m
[32m[2022-09-16 19:34:53,412] [    INFO][0m - loss: 0.0565301, learning_rate: 1.6402116402116401e-07, global_step: 5360, interval_runtime: 7.9355, interval_samples_per_second: 2.016, interval_steps_per_second: 1.26, epoch: 28.3598[0m
[32m[2022-09-16 19:35:01,343] [    INFO][0m - loss: 0.04651484, learning_rate: 1.5873015873015872e-07, global_step: 5370, interval_runtime: 7.9309, interval_samples_per_second: 2.017, interval_steps_per_second: 1.261, epoch: 28.4127[0m
[32m[2022-09-16 19:35:09,241] [    INFO][0m - loss: 0.05478096, learning_rate: 1.5343915343915344e-07, global_step: 5380, interval_runtime: 7.8986, interval_samples_per_second: 2.026, interval_steps_per_second: 1.266, epoch: 28.4656[0m
[32m[2022-09-16 19:35:17,161] [    INFO][0m - loss: 0.06200567, learning_rate: 1.4814814814814815e-07, global_step: 5390, interval_runtime: 7.9195, interval_samples_per_second: 2.02, interval_steps_per_second: 1.263, epoch: 28.5185[0m
[32m[2022-09-16 19:35:25,101] [    INFO][0m - loss: 0.05031624, learning_rate: 1.4285714285714285e-07, global_step: 5400, interval_runtime: 7.9401, interval_samples_per_second: 2.015, interval_steps_per_second: 1.259, epoch: 28.5714[0m
[32m[2022-09-16 19:35:32,989] [    INFO][0m - loss: 0.03671548, learning_rate: 1.3756613756613757e-07, global_step: 5410, interval_runtime: 7.8882, interval_samples_per_second: 2.028, interval_steps_per_second: 1.268, epoch: 28.6243[0m
[32m[2022-09-16 19:35:40,913] [    INFO][0m - loss: 0.03152601, learning_rate: 1.3227513227513228e-07, global_step: 5420, interval_runtime: 7.9238, interval_samples_per_second: 2.019, interval_steps_per_second: 1.262, epoch: 28.6772[0m
[32m[2022-09-16 19:35:48,809] [    INFO][0m - loss: 0.04113642, learning_rate: 1.2698412698412698e-07, global_step: 5430, interval_runtime: 7.8964, interval_samples_per_second: 2.026, interval_steps_per_second: 1.266, epoch: 28.7302[0m
[32m[2022-09-16 19:35:56,730] [    INFO][0m - loss: 0.05893255, learning_rate: 1.2169312169312168e-07, global_step: 5440, interval_runtime: 7.9202, interval_samples_per_second: 2.02, interval_steps_per_second: 1.263, epoch: 28.7831[0m
[32m[2022-09-16 19:36:04,781] [    INFO][0m - loss: 0.0659403, learning_rate: 1.164021164021164e-07, global_step: 5450, interval_runtime: 7.9069, interval_samples_per_second: 2.024, interval_steps_per_second: 1.265, epoch: 28.836[0m
[32m[2022-09-16 19:36:17,018] [    INFO][0m - loss: 0.0343564, learning_rate: 1.1111111111111111e-07, global_step: 5460, interval_runtime: 8.0753, interval_samples_per_second: 1.981, interval_steps_per_second: 1.238, epoch: 28.8889[0m
[32m[2022-09-16 19:36:24,902] [    INFO][0m - loss: 0.05478712, learning_rate: 1.0582010582010581e-07, global_step: 5470, interval_runtime: 12.1903, interval_samples_per_second: 1.313, interval_steps_per_second: 0.82, epoch: 28.9418[0m
[32m[2022-09-16 19:36:32,633] [    INFO][0m - loss: 0.03760077, learning_rate: 1.0052910052910053e-07, global_step: 5480, interval_runtime: 7.7305, interval_samples_per_second: 2.07, interval_steps_per_second: 1.294, epoch: 28.9947[0m
[32m[2022-09-16 19:36:33,376] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:36:33,377] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-16 19:36:33,377] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:36:33,377] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:36:33,377] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-16 19:37:00,775] [    INFO][0m - eval_loss: 3.240525722503662, eval_accuracy: 0.45229424617625635, eval_runtime: 27.3981, eval_samples_per_second: 50.113, eval_steps_per_second: 3.139, epoch: 29.0[0m
[32m[2022-09-16 19:37:02,041] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-5481[0m
[32m[2022-09-16 19:37:02,042] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:37:05,783] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-5481/tokenizer_config.json[0m
[32m[2022-09-16 19:37:05,783] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-5481/special_tokens_map.json[0m
[32m[2022-09-16 19:37:20,292] [    INFO][0m - loss: 0.0523542, learning_rate: 9.523809523809524e-08, global_step: 5490, interval_runtime: 47.6587, interval_samples_per_second: 0.336, interval_steps_per_second: 0.21, epoch: 29.0476[0m
[32m[2022-09-16 19:37:28,714] [    INFO][0m - loss: 0.09911507, learning_rate: 8.994708994708994e-08, global_step: 5500, interval_runtime: 8.4224, interval_samples_per_second: 1.9, interval_steps_per_second: 1.187, epoch: 29.1005[0m
[32m[2022-09-16 19:37:37,246] [    INFO][0m - loss: 0.05551975, learning_rate: 8.465608465608466e-08, global_step: 5510, interval_runtime: 7.9219, interval_samples_per_second: 2.02, interval_steps_per_second: 1.262, epoch: 29.1534[0m
[32m[2022-09-16 19:37:45,146] [    INFO][0m - loss: 0.04947259, learning_rate: 7.936507936507936e-08, global_step: 5520, interval_runtime: 8.5101, interval_samples_per_second: 1.88, interval_steps_per_second: 1.175, epoch: 29.2063[0m
[32m[2022-09-16 19:37:53,071] [    INFO][0m - loss: 0.05371132, learning_rate: 7.407407407407407e-08, global_step: 5530, interval_runtime: 7.9252, interval_samples_per_second: 2.019, interval_steps_per_second: 1.262, epoch: 29.2593[0m
[32m[2022-09-16 19:38:00,981] [    INFO][0m - loss: 0.03685382, learning_rate: 6.878306878306879e-08, global_step: 5540, interval_runtime: 7.9103, interval_samples_per_second: 2.023, interval_steps_per_second: 1.264, epoch: 29.3122[0m
[32m[2022-09-16 19:38:08,890] [    INFO][0m - loss: 0.04341456, learning_rate: 6.349206349206349e-08, global_step: 5550, interval_runtime: 7.9086, interval_samples_per_second: 2.023, interval_steps_per_second: 1.264, epoch: 29.3651[0m
[32m[2022-09-16 19:38:16,800] [    INFO][0m - loss: 0.0244125, learning_rate: 5.82010582010582e-08, global_step: 5560, interval_runtime: 7.91, interval_samples_per_second: 2.023, interval_steps_per_second: 1.264, epoch: 29.418[0m
[32m[2022-09-16 19:38:24,723] [    INFO][0m - loss: 0.049556, learning_rate: 5.2910052910052905e-08, global_step: 5570, interval_runtime: 7.9234, interval_samples_per_second: 2.019, interval_steps_per_second: 1.262, epoch: 29.4709[0m
[32m[2022-09-16 19:38:32,612] [    INFO][0m - loss: 0.04348613, learning_rate: 4.761904761904762e-08, global_step: 5580, interval_runtime: 7.8892, interval_samples_per_second: 2.028, interval_steps_per_second: 1.268, epoch: 29.5238[0m
[32m[2022-09-16 19:38:40,518] [    INFO][0m - loss: 0.03490623, learning_rate: 4.232804232804233e-08, global_step: 5590, interval_runtime: 7.9056, interval_samples_per_second: 2.024, interval_steps_per_second: 1.265, epoch: 29.5767[0m
[32m[2022-09-16 19:38:48,422] [    INFO][0m - loss: 0.05122925, learning_rate: 3.7037037037037036e-08, global_step: 5600, interval_runtime: 7.9035, interval_samples_per_second: 2.024, interval_steps_per_second: 1.265, epoch: 29.6296[0m
[32m[2022-09-16 19:38:57,722] [    INFO][0m - loss: 0.03594581, learning_rate: 3.1746031746031744e-08, global_step: 5610, interval_runtime: 7.9306, interval_samples_per_second: 2.018, interval_steps_per_second: 1.261, epoch: 29.6825[0m
[32m[2022-09-16 19:39:05,582] [    INFO][0m - loss: 0.05758687, learning_rate: 2.6455026455026453e-08, global_step: 5620, interval_runtime: 9.2303, interval_samples_per_second: 1.733, interval_steps_per_second: 1.083, epoch: 29.7354[0m
[32m[2022-09-16 19:39:14,047] [    INFO][0m - loss: 0.06131917, learning_rate: 2.1164021164021164e-08, global_step: 5630, interval_runtime: 7.8757, interval_samples_per_second: 2.032, interval_steps_per_second: 1.27, epoch: 29.7884[0m
[32m[2022-09-16 19:39:21,933] [    INFO][0m - loss: 0.06380256, learning_rate: 1.5873015873015872e-08, global_step: 5640, interval_runtime: 8.4748, interval_samples_per_second: 1.888, interval_steps_per_second: 1.18, epoch: 29.8413[0m
[32m[2022-09-16 19:39:29,854] [    INFO][0m - loss: 0.05081418, learning_rate: 1.0582010582010582e-08, global_step: 5650, interval_runtime: 7.9211, interval_samples_per_second: 2.02, interval_steps_per_second: 1.262, epoch: 29.8942[0m
[32m[2022-09-16 19:39:37,794] [    INFO][0m - loss: 0.03414337, learning_rate: 5.291005291005291e-09, global_step: 5660, interval_runtime: 7.9399, interval_samples_per_second: 2.015, interval_steps_per_second: 1.259, epoch: 29.9471[0m
[32m[2022-09-16 19:39:45,517] [    INFO][0m - loss: 0.07431602, learning_rate: 0.0, global_step: 5670, interval_runtime: 7.7225, interval_samples_per_second: 2.072, interval_steps_per_second: 1.295, epoch: 30.0[0m
[32m[2022-09-16 19:39:45,517] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:39:45,517] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-16 19:39:45,517] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:39:45,517] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:39:45,518] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-16 19:40:12,810] [    INFO][0m - eval_loss: 3.24311900138855, eval_accuracy: 0.45229424617625635, eval_runtime: 27.2922, eval_samples_per_second: 50.307, eval_steps_per_second: 3.151, epoch: 30.0[0m
[32m[2022-09-16 19:40:12,834] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-5670[0m
[32m[2022-09-16 19:40:12,834] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:40:15,921] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-5670/tokenizer_config.json[0m
[32m[2022-09-16 19:40:15,921] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-5670/special_tokens_map.json[0m
[32m[2022-09-16 19:40:21,666] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-16 19:40:21,666] [    INFO][0m - Loading best model from ./checkpoints_iflytek/checkpoint-567 (score: 0.4726875455207575).[0m
[32m[2022-09-16 19:40:23,627] [    INFO][0m - train_runtime: 5673.1058, train_samples_per_second: 15.991, train_steps_per_second: 0.999, train_loss: 0.5787639871975522, epoch: 30.0[0m
[32m[2022-09-16 19:40:23,628] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/[0m
[32m[2022-09-16 19:40:23,629] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:40:26,719] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/tokenizer_config.json[0m
[32m[2022-09-16 19:40:29,823] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/special_tokens_map.json[0m
[32m[2022-09-16 19:40:29,825] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-16 19:40:29,825] [    INFO][0m -   epoch                    =       30.0[0m
[32m[2022-09-16 19:40:29,825] [    INFO][0m -   train_loss               =     0.5788[0m
[32m[2022-09-16 19:40:29,825] [    INFO][0m -   train_runtime            = 1:34:33.10[0m
[32m[2022-09-16 19:40:29,825] [    INFO][0m -   train_samples_per_second =     15.991[0m
[32m[2022-09-16 19:40:29,825] [    INFO][0m -   train_steps_per_second   =      0.999[0m
[32m[2022-09-16 19:40:29,847] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-16 19:40:29,847] [    INFO][0m -   Num examples = 1749[0m
[32m[2022-09-16 19:40:29,847] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:40:29,847] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:40:29,847] [    INFO][0m -   Total prediction steps = 110[0m
[32m[2022-09-16 19:41:04,534] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-16 19:41:04,534] [    INFO][0m -   test_accuracy           =     0.4786[0m
[32m[2022-09-16 19:41:04,535] [    INFO][0m -   test_loss               =     1.8222[0m
[32m[2022-09-16 19:41:04,535] [    INFO][0m -   test_runtime            = 0:00:34.68[0m
[32m[2022-09-16 19:41:04,535] [    INFO][0m -   test_samples_per_second =     50.422[0m
[32m[2022-09-16 19:41:04,535] [    INFO][0m -   test_steps_per_second   =      3.171[0m
[32m[2022-09-16 19:41:04,535] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-16 19:41:04,535] [    INFO][0m -   Num examples = 2600[0m
[32m[2022-09-16 19:41:04,535] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:41:04,535] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:41:04,536] [    INFO][0m -   Total prediction steps = 163[0m
[32m[2022-09-16 19:42:01,649] [    INFO][0m - Predictions for iflytekf saved to ./fewclue_submit_examples.[0m
{
  "labels": 77,
  "text_a": "\u884c\u8baf\u901a\u662f\u5e7f\u5dde\u4ea4\u901a\u4fe1\u606f\u5316\u5efa\u8bbe\u6295\u8d44\u8425\u8fd0\u6709\u9650\u516c\u53f8\u5728\u5e7f\u5dde\u5e02\u4ea4\u901a\u8fd0\u8f93\u5c40\u7684\u6307\u5bfc\u4e0b\uff0c\u63a8\u51fa\u7684\u4e00\u6b3e\u63d0\u4f9b\u4ea4\u901a\u4fe1\u606f\u670d\u52a1\u7684\u624b\u673a\u7ec8\u7aef\u8f6f\u4ef6\uff0c\u652f\u6301Android\u548ciPhone\u7cfb\u7edf\u3002\u4e3b\u8981\u5305\u62ec\u8def\u51b5\u4fe1\u606f\u3001\u5b9e\u65f6\u516c\u4ea4\u3001\u505c\u8f66\u670d\u52a1\u3001\u7684\u58eb\u67e5\u8be2\u3001\u51fa\u884c\u89c4\u5212\u3001\u5730\u94c1\u4fe1\u606f\u3001\u822a\u7a7a\u4fe1\u606f\u3001\u94c1\u8def\u4fe1\u606f\u3001\u5ba2\u8fd0\u4fe1\u606f\u3001\u9a7e\u57f9\u4fe1\u606f\u3001\u4ea4\u901a\u8d44\u8baf\u3001WIFI\u70ed\u70b9\u7b49\u529f\u80fd\u6a21\u5757\u3002\u66f4\u65b0\u5185\u5bb91\u3001\u4f18\u5316\u5df2\u77e5bug\u3002",
  "text_b": "",
  "uid": 0
}

 
==========
ocnli
==========
 
[32m[2022-09-16 19:42:23,772] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-16 19:42:23,773] [    INFO][0m - ============================================================[0m
[32m[2022-09-16 19:42:23,773] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-16 19:42:23,773] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-16 19:42:23,773] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-16 19:42:23,773] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-16 19:42:23,773] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-16 19:42:23,773] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-16 19:42:23,773] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-16 19:42:23,773] [    INFO][0m - [0m
[32m[2022-09-16 19:42:23,774] [    INFO][0m - ============================================================[0m
[32m[2022-09-16 19:42:23,774] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-16 19:42:23,774] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-16 19:42:23,774] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-16 19:42:23,774] [    INFO][0m - pretrained                    :../checkpoints_cmnli/checkpoint-22000/model_state.pdparams[0m
[32m[2022-09-16 19:42:23,774] [    INFO][0m - prompt                        :‚Äú{'text':'text_a'}‚ÄùÂíå‚Äú{'text':'text_b'}‚Äù‰πãÈó¥ÁöÑÈÄªËæëÂÖ≥Á≥ªÊòØ{'mask'}{'mask'}„ÄÇ[0m
[32m[2022-09-16 19:42:23,774] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-16 19:42:23,774] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-16 19:42:23,774] [    INFO][0m - task_name                     :ocnli[0m
[32m[2022-09-16 19:42:23,774] [    INFO][0m - [0m
[32m[2022-09-16 19:42:23,774] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0916 19:42:23.776047 11023 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0916 19:42:23.780148 11023 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-16 19:42:29,292] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-16 19:42:29,303] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-16 19:42:29,304] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-16 19:42:29,304] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùÂíå‚Äú'}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': '‚Äù‰πãÈó¥ÁöÑÈÄªËæëÂÖ≥Á≥ªÊòØ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': '„ÄÇ'}][0m
[32m[2022-09-16 19:42:32,585] [    INFO][0m - ============================================================[0m
[32m[2022-09-16 19:42:32,586] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-16 19:42:32,586] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-16 19:42:32,586] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-16 19:42:32,586] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-16 19:42:32,586] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-16 19:42:32,587] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-16 19:42:32,587] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-16 19:42:32,587] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-16 19:42:32,587] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-16 19:42:32,587] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-16 19:42:32,587] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-16 19:42:32,587] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-16 19:42:32,587] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-16 19:42:32,587] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-16 19:42:32,587] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-16 19:42:32,587] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-16 19:42:32,587] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-16 19:42:32,588] [    INFO][0m - eval_batch_size               :16[0m
[32m[2022-09-16 19:42:32,588] [    INFO][0m - eval_steps                    :200[0m
[32m[2022-09-16 19:42:32,588] [    INFO][0m - evaluation_strategy           :IntervalStrategy.EPOCH[0m
[32m[2022-09-16 19:42:32,588] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-16 19:42:32,588] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-16 19:42:32,588] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-16 19:42:32,588] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-16 19:42:32,588] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-16 19:42:32,588] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-16 19:42:32,588] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-16 19:42:32,588] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-16 19:42:32,588] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-16 19:42:32,589] [    INFO][0m - learning_rate                 :3e-06[0m
[32m[2022-09-16 19:42:32,589] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-16 19:42:32,589] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-16 19:42:32,589] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-16 19:42:32,589] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-16 19:42:32,589] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-16 19:42:32,589] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-16 19:42:32,589] [    INFO][0m - logging_dir                   :./checkpoints_ocnli/runs/Sep16_19-42-23_instance-3bwob41y-01[0m
[32m[2022-09-16 19:42:32,589] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-16 19:42:32,589] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-16 19:42:32,589] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-16 19:42:32,589] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-16 19:42:32,589] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-16 19:42:32,590] [    INFO][0m - max_seq_length                :128[0m
[32m[2022-09-16 19:42:32,590] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-16 19:42:32,590] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-16 19:42:32,590] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-16 19:42:32,590] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-16 19:42:32,590] [    INFO][0m - num_train_epochs              :30.0[0m
[32m[2022-09-16 19:42:32,590] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-16 19:42:32,590] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-16 19:42:32,590] [    INFO][0m - output_dir                    :./checkpoints_ocnli/[0m
[32m[2022-09-16 19:42:32,590] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-16 19:42:32,590] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-16 19:42:32,590] [    INFO][0m - per_device_eval_batch_size    :16[0m
[32m[2022-09-16 19:42:32,590] [    INFO][0m - per_device_train_batch_size   :16[0m
[32m[2022-09-16 19:42:32,591] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-16 19:42:32,591] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-16 19:42:32,591] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-16 19:42:32,591] [    INFO][0m - ppt_learning_rate             :3e-05[0m
[32m[2022-09-16 19:42:32,591] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-16 19:42:32,591] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-16 19:42:32,591] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-16 19:42:32,591] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-16 19:42:32,591] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-16 19:42:32,591] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-16 19:42:32,591] [    INFO][0m - run_name                      :./checkpoints_ocnli/[0m
[32m[2022-09-16 19:42:32,591] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-16 19:42:32,592] [    INFO][0m - save_steps                    :200[0m
[32m[2022-09-16 19:42:32,592] [    INFO][0m - save_strategy                 :IntervalStrategy.EPOCH[0m
[32m[2022-09-16 19:42:32,592] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-16 19:42:32,592] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-16 19:42:32,592] [    INFO][0m - seed                          :42[0m
[32m[2022-09-16 19:42:32,592] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-16 19:42:32,592] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-16 19:42:32,592] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-16 19:42:32,592] [    INFO][0m - train_batch_size              :16[0m
[32m[2022-09-16 19:42:32,592] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-16 19:42:32,592] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-16 19:42:32,592] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-16 19:42:32,592] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-16 19:42:32,593] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-16 19:42:32,593] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-16 19:42:32,593] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-16 19:42:32,593] [    INFO][0m - [0m
[32m[2022-09-16 19:42:32,596] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-16 19:42:32,596] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:42:32,596] [    INFO][0m -   Num Epochs = 30[0m
[32m[2022-09-16 19:42:32,596] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2022-09-16 19:42:32,596] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2022-09-16 19:42:32,596] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-16 19:42:32,596] [    INFO][0m -   Total optimization steps = 300.0[0m
[32m[2022-09-16 19:42:32,596] [    INFO][0m -   Total num train samples = 4800[0m
[32m[2022-09-16 19:42:37,019] [    INFO][0m - loss: 0.6339509, learning_rate: 2.9e-06, global_step: 10, interval_runtime: 4.4214, interval_samples_per_second: 3.619, interval_steps_per_second: 2.262, epoch: 1.0[0m
[32m[2022-09-16 19:42:37,020] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:42:37,021] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:42:37,021] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:42:37,021] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:42:37,021] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:42:38,374] [    INFO][0m - eval_loss: 0.6579482555389404, eval_accuracy: 0.75, eval_runtime: 1.3528, eval_samples_per_second: 118.271, eval_steps_per_second: 7.392, epoch: 1.0[0m
[32m[2022-09-16 19:42:38,375] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-10[0m
[32m[2022-09-16 19:42:38,375] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:42:41,866] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-10/tokenizer_config.json[0m
[32m[2022-09-16 19:42:41,866] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-10/special_tokens_map.json[0m
[32m[2022-09-16 19:42:51,631] [    INFO][0m - loss: 0.48952055, learning_rate: 2.8000000000000003e-06, global_step: 20, interval_runtime: 14.6119, interval_samples_per_second: 1.095, interval_steps_per_second: 0.684, epoch: 2.0[0m
[32m[2022-09-16 19:42:51,632] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:42:51,632] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:42:51,632] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:42:51,632] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:42:51,632] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:42:53,016] [    INFO][0m - eval_loss: 0.6377469897270203, eval_accuracy: 0.74375, eval_runtime: 1.3836, eval_samples_per_second: 115.643, eval_steps_per_second: 7.228, epoch: 2.0[0m
[32m[2022-09-16 19:42:53,017] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-20[0m
[32m[2022-09-16 19:42:53,017] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:42:55,856] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-20/tokenizer_config.json[0m
[32m[2022-09-16 19:42:55,857] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-20/special_tokens_map.json[0m
[32m[2022-09-16 19:43:04,719] [    INFO][0m - loss: 0.41134167, learning_rate: 2.7e-06, global_step: 30, interval_runtime: 13.0883, interval_samples_per_second: 1.222, interval_steps_per_second: 0.764, epoch: 3.0[0m
[32m[2022-09-16 19:43:04,720] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:43:04,720] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:43:04,720] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:43:04,720] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:43:04,720] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:43:06,066] [    INFO][0m - eval_loss: 0.624819815158844, eval_accuracy: 0.76875, eval_runtime: 1.3453, eval_samples_per_second: 118.936, eval_steps_per_second: 7.433, epoch: 3.0[0m
[32m[2022-09-16 19:43:06,066] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-30[0m
[32m[2022-09-16 19:43:06,066] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:43:08,834] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-30/tokenizer_config.json[0m
[32m[2022-09-16 19:43:08,834] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-30/special_tokens_map.json[0m
[32m[2022-09-16 19:43:18,080] [    INFO][0m - loss: 0.32677407, learning_rate: 2.6e-06, global_step: 40, interval_runtime: 13.3612, interval_samples_per_second: 1.198, interval_steps_per_second: 0.748, epoch: 4.0[0m
[32m[2022-09-16 19:43:18,081] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:43:18,081] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:43:18,081] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:43:18,081] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:43:18,081] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:43:19,400] [    INFO][0m - eval_loss: 0.6564428210258484, eval_accuracy: 0.76875, eval_runtime: 1.3181, eval_samples_per_second: 121.386, eval_steps_per_second: 7.587, epoch: 4.0[0m
[32m[2022-09-16 19:43:19,400] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-40[0m
[32m[2022-09-16 19:43:19,400] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:43:22,071] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-40/tokenizer_config.json[0m
[32m[2022-09-16 19:43:22,071] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-40/special_tokens_map.json[0m
[32m[2022-09-16 19:43:30,832] [    INFO][0m - loss: 0.26642528, learning_rate: 2.5e-06, global_step: 50, interval_runtime: 12.7517, interval_samples_per_second: 1.255, interval_steps_per_second: 0.784, epoch: 5.0[0m
[32m[2022-09-16 19:43:30,833] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:43:30,833] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:43:30,833] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:43:30,834] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:43:30,834] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:43:32,163] [    INFO][0m - eval_loss: 0.7095646858215332, eval_accuracy: 0.75625, eval_runtime: 1.3291, eval_samples_per_second: 120.379, eval_steps_per_second: 7.524, epoch: 5.0[0m
[32m[2022-09-16 19:43:32,163] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-50[0m
[32m[2022-09-16 19:43:32,163] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:43:34,904] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-50/tokenizer_config.json[0m
[32m[2022-09-16 19:43:34,905] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-50/special_tokens_map.json[0m
[32m[2022-09-16 19:43:43,621] [    INFO][0m - loss: 0.19000534, learning_rate: 2.4000000000000003e-06, global_step: 60, interval_runtime: 12.7891, interval_samples_per_second: 1.251, interval_steps_per_second: 0.782, epoch: 6.0[0m
[32m[2022-09-16 19:43:43,621] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:43:43,622] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:43:43,622] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:43:43,622] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:43:43,622] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:43:44,945] [    INFO][0m - eval_loss: 0.7304198145866394, eval_accuracy: 0.75, eval_runtime: 1.3228, eval_samples_per_second: 120.951, eval_steps_per_second: 7.559, epoch: 6.0[0m
[32m[2022-09-16 19:43:44,945] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-60[0m
[32m[2022-09-16 19:43:44,945] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:43:47,725] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-60/tokenizer_config.json[0m
[32m[2022-09-16 19:43:47,725] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-60/special_tokens_map.json[0m
[32m[2022-09-16 19:43:56,610] [    INFO][0m - loss: 0.15990683, learning_rate: 2.3000000000000004e-06, global_step: 70, interval_runtime: 12.9891, interval_samples_per_second: 1.232, interval_steps_per_second: 0.77, epoch: 7.0[0m
[32m[2022-09-16 19:43:56,611] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:43:56,611] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:43:56,611] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:43:56,611] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:43:56,612] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:43:57,922] [    INFO][0m - eval_loss: 0.7912464737892151, eval_accuracy: 0.75625, eval_runtime: 1.3103, eval_samples_per_second: 122.109, eval_steps_per_second: 7.632, epoch: 7.0[0m
[32m[2022-09-16 19:43:57,922] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-70[0m
[32m[2022-09-16 19:43:57,923] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:44:00,564] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-70/tokenizer_config.json[0m
[32m[2022-09-16 19:44:00,564] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-70/special_tokens_map.json[0m
[32m[2022-09-16 19:44:09,888] [    INFO][0m - loss: 0.11337812, learning_rate: 2.1999999999999997e-06, global_step: 80, interval_runtime: 13.2778, interval_samples_per_second: 1.205, interval_steps_per_second: 0.753, epoch: 8.0[0m
[32m[2022-09-16 19:44:09,888] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:44:09,888] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:44:09,889] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:44:09,889] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:44:09,889] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:44:11,197] [    INFO][0m - eval_loss: 0.8718236684799194, eval_accuracy: 0.7625, eval_runtime: 1.3079, eval_samples_per_second: 122.331, eval_steps_per_second: 7.646, epoch: 8.0[0m
[32m[2022-09-16 19:44:14,065] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-80[0m
[32m[2022-09-16 19:44:14,065] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:44:16,770] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-80/tokenizer_config.json[0m
[32m[2022-09-16 19:44:16,770] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-80/special_tokens_map.json[0m
[32m[2022-09-16 19:44:25,699] [    INFO][0m - loss: 0.08682978, learning_rate: 2.1e-06, global_step: 90, interval_runtime: 15.4368, interval_samples_per_second: 1.036, interval_steps_per_second: 0.648, epoch: 9.0[0m
[32m[2022-09-16 19:44:25,699] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:44:25,699] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:44:25,699] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:44:25,699] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:44:25,700] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:44:27,021] [    INFO][0m - eval_loss: 0.9653379321098328, eval_accuracy: 0.74375, eval_runtime: 1.3213, eval_samples_per_second: 121.09, eval_steps_per_second: 7.568, epoch: 9.0[0m
[32m[2022-09-16 19:44:27,021] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-90[0m
[32m[2022-09-16 19:44:27,022] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:44:29,680] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-90/tokenizer_config.json[0m
[32m[2022-09-16 19:44:29,680] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-90/special_tokens_map.json[0m
[32m[2022-09-16 19:44:38,638] [    INFO][0m - loss: 0.05599127, learning_rate: 2e-06, global_step: 100, interval_runtime: 13.3129, interval_samples_per_second: 1.202, interval_steps_per_second: 0.751, epoch: 10.0[0m
[32m[2022-09-16 19:44:38,638] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:44:38,638] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:44:38,638] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:44:38,639] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:44:38,639] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:44:39,950] [    INFO][0m - eval_loss: 1.0719945430755615, eval_accuracy: 0.75, eval_runtime: 1.3113, eval_samples_per_second: 122.013, eval_steps_per_second: 7.626, epoch: 10.0[0m
[32m[2022-09-16 19:44:39,951] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-100[0m
[32m[2022-09-16 19:44:39,951] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:44:42,581] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-16 19:44:42,581] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-16 19:44:51,253] [    INFO][0m - loss: 0.05157957, learning_rate: 1.9e-06, global_step: 110, interval_runtime: 12.6151, interval_samples_per_second: 1.268, interval_steps_per_second: 0.793, epoch: 11.0[0m
[32m[2022-09-16 19:44:53,275] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:44:53,276] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:44:53,276] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:44:53,276] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:44:53,276] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:44:54,600] [    INFO][0m - eval_loss: 1.1540985107421875, eval_accuracy: 0.74375, eval_runtime: 1.3245, eval_samples_per_second: 120.801, eval_steps_per_second: 7.55, epoch: 11.0[0m
[32m[2022-09-16 19:44:54,601] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-110[0m
[32m[2022-09-16 19:44:54,601] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:44:57,208] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-110/tokenizer_config.json[0m
[32m[2022-09-16 19:44:57,209] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-110/special_tokens_map.json[0m
[32m[2022-09-16 19:45:06,602] [    INFO][0m - loss: 0.06809844, learning_rate: 1.8e-06, global_step: 120, interval_runtime: 14.8227, interval_samples_per_second: 1.079, interval_steps_per_second: 0.675, epoch: 12.0[0m
[32m[2022-09-16 19:45:06,603] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:45:06,603] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:45:06,603] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:45:06,603] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:45:06,603] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:45:07,919] [    INFO][0m - eval_loss: 1.2340500354766846, eval_accuracy: 0.7375, eval_runtime: 1.3154, eval_samples_per_second: 121.632, eval_steps_per_second: 7.602, epoch: 12.0[0m
[32m[2022-09-16 19:45:07,919] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-120[0m
[32m[2022-09-16 19:45:07,920] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:45:14,188] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-120/tokenizer_config.json[0m
[32m[2022-09-16 19:45:14,189] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-120/special_tokens_map.json[0m
[32m[2022-09-16 19:45:23,161] [    INFO][0m - loss: 0.0423334, learning_rate: 1.7e-06, global_step: 130, interval_runtime: 17.085, interval_samples_per_second: 0.936, interval_steps_per_second: 0.585, epoch: 13.0[0m
[32m[2022-09-16 19:45:23,161] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:45:23,161] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:45:23,162] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:45:23,162] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:45:23,162] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:45:24,472] [    INFO][0m - eval_loss: 1.2956647872924805, eval_accuracy: 0.7375, eval_runtime: 1.31, eval_samples_per_second: 122.138, eval_steps_per_second: 7.634, epoch: 13.0[0m
[32m[2022-09-16 19:45:28,135] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-130[0m
[32m[2022-09-16 19:45:28,136] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:45:30,664] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-130/tokenizer_config.json[0m
[32m[2022-09-16 19:45:30,947] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-130/special_tokens_map.json[0m
[32m[2022-09-16 19:45:40,111] [    INFO][0m - loss: 0.03513905, learning_rate: 1.6e-06, global_step: 140, interval_runtime: 16.9508, interval_samples_per_second: 0.944, interval_steps_per_second: 0.59, epoch: 14.0[0m
[32m[2022-09-16 19:45:40,112] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:45:40,112] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:45:40,112] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:45:40,112] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:45:40,112] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:45:41,423] [    INFO][0m - eval_loss: 1.345572829246521, eval_accuracy: 0.74375, eval_runtime: 1.3105, eval_samples_per_second: 122.088, eval_steps_per_second: 7.63, epoch: 14.0[0m
[32m[2022-09-16 19:45:41,423] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-140[0m
[32m[2022-09-16 19:45:41,423] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:45:44,020] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-140/tokenizer_config.json[0m
[32m[2022-09-16 19:45:44,021] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-140/special_tokens_map.json[0m
[32m[2022-09-16 19:45:56,290] [    INFO][0m - loss: 0.03518793, learning_rate: 1.5e-06, global_step: 150, interval_runtime: 16.1785, interval_samples_per_second: 0.989, interval_steps_per_second: 0.618, epoch: 15.0[0m
[32m[2022-09-16 19:45:56,290] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:45:56,290] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:45:56,290] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:45:56,290] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:45:56,291] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:45:57,610] [    INFO][0m - eval_loss: 1.3917721509933472, eval_accuracy: 0.75, eval_runtime: 1.3188, eval_samples_per_second: 121.319, eval_steps_per_second: 7.582, epoch: 15.0[0m
[32m[2022-09-16 19:45:57,610] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-150[0m
[32m[2022-09-16 19:45:57,610] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:46:00,171] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-150/tokenizer_config.json[0m
[32m[2022-09-16 19:46:00,171] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-150/special_tokens_map.json[0m
[32m[2022-09-16 19:46:08,593] [    INFO][0m - loss: 0.02750246, learning_rate: 1.4000000000000001e-06, global_step: 160, interval_runtime: 12.3036, interval_samples_per_second: 1.3, interval_steps_per_second: 0.813, epoch: 16.0[0m
[32m[2022-09-16 19:46:08,963] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:46:08,963] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:46:08,963] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:46:08,963] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:46:08,963] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:46:10,272] [    INFO][0m - eval_loss: 1.446569561958313, eval_accuracy: 0.75625, eval_runtime: 1.3085, eval_samples_per_second: 122.28, eval_steps_per_second: 7.642, epoch: 16.0[0m
[32m[2022-09-16 19:46:10,272] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-160[0m
[32m[2022-09-16 19:46:10,272] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:46:12,797] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-160/tokenizer_config.json[0m
[32m[2022-09-16 19:46:12,798] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-160/special_tokens_map.json[0m
[32m[2022-09-16 19:46:21,231] [    INFO][0m - loss: 0.02590681, learning_rate: 1.3e-06, global_step: 170, interval_runtime: 12.6379, interval_samples_per_second: 1.266, interval_steps_per_second: 0.791, epoch: 17.0[0m
[32m[2022-09-16 19:46:21,232] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:46:21,232] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:46:21,232] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:46:21,232] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:46:21,232] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:46:22,548] [    INFO][0m - eval_loss: 1.4919770956039429, eval_accuracy: 0.75625, eval_runtime: 1.3154, eval_samples_per_second: 121.637, eval_steps_per_second: 7.602, epoch: 17.0[0m
[32m[2022-09-16 19:46:22,548] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-170[0m
[32m[2022-09-16 19:46:22,548] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:46:25,120] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-170/tokenizer_config.json[0m
[32m[2022-09-16 19:46:25,120] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-170/special_tokens_map.json[0m
[32m[2022-09-16 19:46:33,559] [    INFO][0m - loss: 0.02453384, learning_rate: 1.2000000000000002e-06, global_step: 180, interval_runtime: 12.3282, interval_samples_per_second: 1.298, interval_steps_per_second: 0.811, epoch: 18.0[0m
[32m[2022-09-16 19:46:33,560] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:46:33,560] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:46:33,560] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:46:33,560] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:46:33,560] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:46:34,873] [    INFO][0m - eval_loss: 1.5182182788848877, eval_accuracy: 0.75, eval_runtime: 1.3126, eval_samples_per_second: 121.894, eval_steps_per_second: 7.618, epoch: 18.0[0m
[32m[2022-09-16 19:46:34,873] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-180[0m
[32m[2022-09-16 19:46:34,873] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:46:37,479] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-180/tokenizer_config.json[0m
[32m[2022-09-16 19:46:37,479] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-180/special_tokens_map.json[0m
[32m[2022-09-16 19:46:45,877] [    INFO][0m - loss: 0.02049429, learning_rate: 1.0999999999999998e-06, global_step: 190, interval_runtime: 12.3175, interval_samples_per_second: 1.299, interval_steps_per_second: 0.812, epoch: 19.0[0m
[32m[2022-09-16 19:46:45,877] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:46:45,877] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:46:45,878] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:46:45,878] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:46:45,878] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:46:47,188] [    INFO][0m - eval_loss: 1.5372214317321777, eval_accuracy: 0.75, eval_runtime: 1.3106, eval_samples_per_second: 122.084, eval_steps_per_second: 7.63, epoch: 19.0[0m
[32m[2022-09-16 19:46:50,000] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-190[0m
[32m[2022-09-16 19:46:50,000] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:46:57,037] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-190/tokenizer_config.json[0m
[32m[2022-09-16 19:46:57,037] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-190/special_tokens_map.json[0m
[32m[2022-09-16 19:47:05,413] [    INFO][0m - loss: 0.01991219, learning_rate: 1e-06, global_step: 200, interval_runtime: 19.5359, interval_samples_per_second: 0.819, interval_steps_per_second: 0.512, epoch: 20.0[0m
[32m[2022-09-16 19:47:05,414] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:47:05,414] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:47:05,414] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:47:05,414] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:47:05,414] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:47:06,719] [    INFO][0m - eval_loss: 1.540761947631836, eval_accuracy: 0.75, eval_runtime: 1.3053, eval_samples_per_second: 122.576, eval_steps_per_second: 7.661, epoch: 20.0[0m
[32m[2022-09-16 19:47:06,720] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-200[0m
[32m[2022-09-16 19:47:06,720] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:47:09,209] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-200/tokenizer_config.json[0m
[32m[2022-09-16 19:47:09,209] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-200/special_tokens_map.json[0m
[32m[2022-09-16 19:47:17,550] [    INFO][0m - loss: 0.02315403, learning_rate: 9e-07, global_step: 210, interval_runtime: 12.1368, interval_samples_per_second: 1.318, interval_steps_per_second: 0.824, epoch: 21.0[0m
[32m[2022-09-16 19:47:17,550] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:47:17,550] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:47:17,550] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:47:17,550] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:47:17,551] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:47:18,861] [    INFO][0m - eval_loss: 1.5500361919403076, eval_accuracy: 0.75, eval_runtime: 1.3109, eval_samples_per_second: 122.058, eval_steps_per_second: 7.629, epoch: 21.0[0m
[32m[2022-09-16 19:47:19,325] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-210[0m
[32m[2022-09-16 19:47:19,325] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:47:21,937] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-210/tokenizer_config.json[0m
[32m[2022-09-16 19:47:21,937] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-210/special_tokens_map.json[0m
[32m[2022-09-16 19:47:30,368] [    INFO][0m - loss: 0.01306081, learning_rate: 8e-07, global_step: 220, interval_runtime: 12.8184, interval_samples_per_second: 1.248, interval_steps_per_second: 0.78, epoch: 22.0[0m
[32m[2022-09-16 19:47:30,369] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:47:30,369] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:47:30,369] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:47:30,369] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:47:30,369] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:47:31,689] [    INFO][0m - eval_loss: 1.5639927387237549, eval_accuracy: 0.75625, eval_runtime: 1.3194, eval_samples_per_second: 121.264, eval_steps_per_second: 7.579, epoch: 22.0[0m
[32m[2022-09-16 19:47:31,689] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-220[0m
[32m[2022-09-16 19:47:31,689] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:47:36,349] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-220/tokenizer_config.json[0m
[32m[2022-09-16 19:47:36,350] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-220/special_tokens_map.json[0m
[32m[2022-09-16 19:47:49,120] [    INFO][0m - loss: 0.0137653, learning_rate: 7.000000000000001e-07, global_step: 230, interval_runtime: 18.7515, interval_samples_per_second: 0.853, interval_steps_per_second: 0.533, epoch: 23.0[0m
[32m[2022-09-16 19:47:49,120] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:47:49,120] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:47:49,120] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:47:49,120] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:47:49,120] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:47:50,448] [    INFO][0m - eval_loss: 1.5793440341949463, eval_accuracy: 0.75625, eval_runtime: 1.3279, eval_samples_per_second: 120.496, eval_steps_per_second: 7.531, epoch: 23.0[0m
[32m[2022-09-16 19:47:50,449] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-230[0m
[32m[2022-09-16 19:47:50,449] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:47:52,970] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-230/tokenizer_config.json[0m
[32m[2022-09-16 19:47:52,971] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-230/special_tokens_map.json[0m
[32m[2022-09-16 19:48:03,178] [    INFO][0m - loss: 0.01121911, learning_rate: 6.000000000000001e-07, global_step: 240, interval_runtime: 14.0587, interval_samples_per_second: 1.138, interval_steps_per_second: 0.711, epoch: 24.0[0m
[32m[2022-09-16 19:48:03,179] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:48:03,179] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:48:03,179] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:48:03,179] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:48:03,179] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:48:04,487] [    INFO][0m - eval_loss: 1.5925270318984985, eval_accuracy: 0.75625, eval_runtime: 1.3075, eval_samples_per_second: 122.373, eval_steps_per_second: 7.648, epoch: 24.0[0m
[32m[2022-09-16 19:48:04,828] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-240[0m
[32m[2022-09-16 19:48:04,828] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:48:07,293] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-240/tokenizer_config.json[0m
[32m[2022-09-16 19:48:07,293] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-240/special_tokens_map.json[0m
[32m[2022-09-16 19:48:20,191] [    INFO][0m - loss: 0.01265756, learning_rate: 5e-07, global_step: 250, interval_runtime: 17.0131, interval_samples_per_second: 0.94, interval_steps_per_second: 0.588, epoch: 25.0[0m
[32m[2022-09-16 19:48:20,192] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:48:20,192] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:48:20,192] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:48:20,192] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:48:20,192] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:48:21,495] [    INFO][0m - eval_loss: 1.604591727256775, eval_accuracy: 0.75625, eval_runtime: 1.3027, eval_samples_per_second: 122.82, eval_steps_per_second: 7.676, epoch: 25.0[0m
[32m[2022-09-16 19:48:21,496] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-250[0m
[32m[2022-09-16 19:48:21,496] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:48:24,002] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-250/tokenizer_config.json[0m
[32m[2022-09-16 19:48:24,002] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-250/special_tokens_map.json[0m
[32m[2022-09-16 19:48:35,832] [    INFO][0m - loss: 0.00915161, learning_rate: 4e-07, global_step: 260, interval_runtime: 15.6409, interval_samples_per_second: 1.023, interval_steps_per_second: 0.639, epoch: 26.0[0m
[32m[2022-09-16 19:48:35,833] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:48:35,833] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:48:35,833] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:48:35,833] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:48:35,833] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:48:37,151] [    INFO][0m - eval_loss: 1.6160780191421509, eval_accuracy: 0.75625, eval_runtime: 1.3173, eval_samples_per_second: 121.465, eval_steps_per_second: 7.592, epoch: 26.0[0m
[32m[2022-09-16 19:48:37,151] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-260[0m
[32m[2022-09-16 19:48:37,151] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:48:39,749] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-260/tokenizer_config.json[0m
[32m[2022-09-16 19:48:39,750] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-260/special_tokens_map.json[0m
[32m[2022-09-16 19:48:49,000] [    INFO][0m - loss: 0.0047292, learning_rate: 3.0000000000000004e-07, global_step: 270, interval_runtime: 12.3707, interval_samples_per_second: 1.293, interval_steps_per_second: 0.808, epoch: 27.0[0m
[32m[2022-09-16 19:48:49,000] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:48:49,001] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:48:49,001] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:48:49,001] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:48:49,001] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:48:50,312] [    INFO][0m - eval_loss: 1.6227061748504639, eval_accuracy: 0.75625, eval_runtime: 1.3113, eval_samples_per_second: 122.017, eval_steps_per_second: 7.626, epoch: 27.0[0m
[32m[2022-09-16 19:48:50,313] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-270[0m
[32m[2022-09-16 19:48:50,313] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:48:52,845] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-270/tokenizer_config.json[0m
[32m[2022-09-16 19:48:52,845] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-270/special_tokens_map.json[0m
[32m[2022-09-16 19:49:01,021] [    INFO][0m - loss: 0.01069537, learning_rate: 2e-07, global_step: 280, interval_runtime: 12.8182, interval_samples_per_second: 1.248, interval_steps_per_second: 0.78, epoch: 28.0[0m
[32m[2022-09-16 19:49:03,673] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:49:03,674] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:49:03,674] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:49:03,674] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:49:03,674] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:49:05,005] [    INFO][0m - eval_loss: 1.6273704767227173, eval_accuracy: 0.75625, eval_runtime: 1.3073, eval_samples_per_second: 122.386, eval_steps_per_second: 7.649, epoch: 28.0[0m
[32m[2022-09-16 19:49:05,006] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-280[0m
[32m[2022-09-16 19:49:05,006] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:49:07,449] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-280/tokenizer_config.json[0m
[32m[2022-09-16 19:49:07,450] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-280/special_tokens_map.json[0m
[32m[2022-09-16 19:49:15,621] [    INFO][0m - loss: 0.00707683, learning_rate: 1e-07, global_step: 290, interval_runtime: 14.5996, interval_samples_per_second: 1.096, interval_steps_per_second: 0.685, epoch: 29.0[0m
[32m[2022-09-16 19:49:15,621] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:49:15,621] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:49:15,622] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:49:15,622] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:49:15,622] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:49:16,942] [    INFO][0m - eval_loss: 1.6306912899017334, eval_accuracy: 0.75625, eval_runtime: 1.3198, eval_samples_per_second: 121.23, eval_steps_per_second: 7.577, epoch: 29.0[0m
[32m[2022-09-16 19:49:19,590] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-290[0m
[32m[2022-09-16 19:49:19,590] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:49:22,163] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-290/tokenizer_config.json[0m
[32m[2022-09-16 19:49:22,163] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-290/special_tokens_map.json[0m
[32m[2022-09-16 19:49:30,930] [    INFO][0m - loss: 0.00828639, learning_rate: 0.0, global_step: 300, interval_runtime: 15.3094, interval_samples_per_second: 1.045, interval_steps_per_second: 0.653, epoch: 30.0[0m
[32m[2022-09-16 19:49:30,931] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:49:30,931] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:49:30,931] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:49:30,931] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:49:30,931] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:49:32,259] [    INFO][0m - eval_loss: 1.6320358514785767, eval_accuracy: 0.75625, eval_runtime: 1.3275, eval_samples_per_second: 120.53, eval_steps_per_second: 7.533, epoch: 30.0[0m
[32m[2022-09-16 19:49:32,259] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-300[0m
[32m[2022-09-16 19:49:32,260] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:49:39,451] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-300/tokenizer_config.json[0m
[32m[2022-09-16 19:49:39,451] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-300/special_tokens_map.json[0m
[32m[2022-09-16 19:49:44,237] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-16 19:49:44,238] [    INFO][0m - Loading best model from ./checkpoints_ocnli/checkpoint-30 (score: 0.76875).[0m
[32m[2022-09-16 19:49:46,070] [    INFO][0m - train_runtime: 433.4724, train_samples_per_second: 11.073, train_steps_per_second: 0.692, train_loss: 0.10662026723225912, epoch: 30.0[0m
[32m[2022-09-16 19:49:46,072] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/[0m
[32m[2022-09-16 19:49:46,072] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:49:48,523] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/tokenizer_config.json[0m
[32m[2022-09-16 19:49:48,523] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/special_tokens_map.json[0m
[32m[2022-09-16 19:49:48,525] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-16 19:49:48,525] [    INFO][0m -   epoch                    =       30.0[0m
[32m[2022-09-16 19:49:48,525] [    INFO][0m -   train_loss               =     0.1066[0m
[32m[2022-09-16 19:49:48,525] [    INFO][0m -   train_runtime            = 0:07:13.47[0m
[32m[2022-09-16 19:49:48,525] [    INFO][0m -   train_samples_per_second =     11.073[0m
[32m[2022-09-16 19:49:48,525] [    INFO][0m -   train_steps_per_second   =      0.692[0m
[32m[2022-09-16 19:49:48,528] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-16 19:49:48,528] [    INFO][0m -   Num examples = 2520[0m
[32m[2022-09-16 19:49:48,529] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:49:48,529] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:49:48,529] [    INFO][0m -   Total prediction steps = 158[0m
[32m[2022-09-16 19:50:10,208] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-16 19:50:10,208] [    INFO][0m -   test_accuracy           =     0.7782[0m
[32m[2022-09-16 19:50:10,209] [    INFO][0m -   test_loss               =     0.5258[0m
[32m[2022-09-16 19:50:10,209] [    INFO][0m -   test_runtime            = 0:00:21.67[0m
[32m[2022-09-16 19:50:10,209] [    INFO][0m -   test_samples_per_second =     116.24[0m
[32m[2022-09-16 19:50:10,209] [    INFO][0m -   test_steps_per_second   =      7.288[0m
[32m[2022-09-16 19:50:10,209] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-16 19:50:10,209] [    INFO][0m -   Num examples = 3000[0m
[32m[2022-09-16 19:50:10,209] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:50:10,210] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:50:10,210] [    INFO][0m -   Total prediction steps = 188[0m
[32m[2022-09-16 19:50:37,868] [    INFO][0m - Predictions for ocnlif saved to ./fewclue_submit_examples.[0m
{
  "labels": 2,
  "text_a": "\u4e03\u4e94\u671f\u95f4\u5f00\u59cb,\u56fd\u5bb6\u53c8\u6295\u8d44\u5c06\u6b66\u6c49\u5e02\u533a\u7684\u90e8\u5206\u571f\u5824\u6539\u5efa\u4e3a\u94a2\u7b4b\u6ce5\u51dd\u571f\u9632\u6c34\u5899",
  "text_b": "\u516b\u4e94\u671f\u95f4\u4f1a\u628a\u5269\u4e0b\u7684\u571f\u5824\u90fd\u6539\u5efa\u5b8c",
  "uid": 0
}

 
==========
bustm
==========
 
[32m[2022-09-16 19:50:59,519] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-16 19:50:59,520] [    INFO][0m - ============================================================[0m
[32m[2022-09-16 19:50:59,520] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-16 19:50:59,520] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-16 19:50:59,520] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-16 19:50:59,520] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-16 19:50:59,520] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-16 19:50:59,520] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-16 19:50:59,520] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-16 19:50:59,520] [    INFO][0m - [0m
[32m[2022-09-16 19:50:59,520] [    INFO][0m - ============================================================[0m
[32m[2022-09-16 19:50:59,520] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-16 19:50:59,521] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-16 19:50:59,521] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-16 19:50:59,521] [    INFO][0m - pretrained                    :../checkpoints_cmnli/checkpoint-22000/model_state.pdparams[0m
[32m[2022-09-16 19:50:59,521] [    INFO][0m - prompt                        :‚Äú{'text':'text_a'}‚ÄùÂíå‚Äú{'text':'text_b'}‚ÄùÊèèËø∞ÁöÑÊòØ{'mask'}{'mask'}ÁöÑ‰∫ãÊÉÖ„ÄÇ[0m
[32m[2022-09-16 19:50:59,521] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-16 19:50:59,521] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-16 19:50:59,521] [    INFO][0m - task_name                     :bustm[0m
[32m[2022-09-16 19:50:59,521] [    INFO][0m - [0m
[32m[2022-09-16 19:50:59,521] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0916 19:50:59.522912 22953 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0916 19:50:59.527012 22953 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-16 19:51:07,217] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-16 19:51:07,228] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-16 19:51:07,228] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-16 19:51:07,229] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùÂíå‚Äú'}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': '‚ÄùÊèèËø∞ÁöÑÊòØ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'ÁöÑ‰∫ãÊÉÖ„ÄÇ'}][0m
[32m[2022-09-16 19:51:09,542] [    INFO][0m - ============================================================[0m
[32m[2022-09-16 19:51:09,542] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-16 19:51:09,543] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-16 19:51:09,543] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-16 19:51:09,543] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-16 19:51:09,543] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-16 19:51:09,543] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-16 19:51:09,543] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-16 19:51:09,543] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-16 19:51:09,543] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-16 19:51:09,544] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-16 19:51:09,544] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-16 19:51:09,544] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-16 19:51:09,544] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-16 19:51:09,544] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-16 19:51:09,544] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-16 19:51:09,544] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-16 19:51:09,544] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-16 19:51:09,544] [    INFO][0m - eval_batch_size               :16[0m
[32m[2022-09-16 19:51:09,544] [    INFO][0m - eval_steps                    :200[0m
[32m[2022-09-16 19:51:09,544] [    INFO][0m - evaluation_strategy           :IntervalStrategy.EPOCH[0m
[32m[2022-09-16 19:51:09,544] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-16 19:51:09,545] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-16 19:51:09,545] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-16 19:51:09,545] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-16 19:51:09,545] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-16 19:51:09,545] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-16 19:51:09,545] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-16 19:51:09,545] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-16 19:51:09,545] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-16 19:51:09,545] [    INFO][0m - learning_rate                 :3e-06[0m
[32m[2022-09-16 19:51:09,545] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-16 19:51:09,545] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-16 19:51:09,545] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-16 19:51:09,545] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-16 19:51:09,545] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-16 19:51:09,546] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-16 19:51:09,546] [    INFO][0m - logging_dir                   :./checkpoints_bustm/runs/Sep16_19-50-59_instance-3bwob41y-01[0m
[32m[2022-09-16 19:51:09,546] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-16 19:51:09,546] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-16 19:51:09,546] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-16 19:51:09,546] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-16 19:51:09,546] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-16 19:51:09,546] [    INFO][0m - max_seq_length                :64[0m
[32m[2022-09-16 19:51:09,546] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-16 19:51:09,546] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-16 19:51:09,546] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-16 19:51:09,546] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-16 19:51:09,546] [    INFO][0m - num_train_epochs              :30.0[0m
[32m[2022-09-16 19:51:09,547] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-16 19:51:09,547] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-16 19:51:09,547] [    INFO][0m - output_dir                    :./checkpoints_bustm/[0m
[32m[2022-09-16 19:51:09,547] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-16 19:51:09,547] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-16 19:51:09,547] [    INFO][0m - per_device_eval_batch_size    :16[0m
[32m[2022-09-16 19:51:09,547] [    INFO][0m - per_device_train_batch_size   :16[0m
[32m[2022-09-16 19:51:09,547] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-16 19:51:09,547] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-16 19:51:09,547] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-16 19:51:09,547] [    INFO][0m - ppt_learning_rate             :3e-05[0m
[32m[2022-09-16 19:51:09,547] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-16 19:51:09,547] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-16 19:51:09,548] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-16 19:51:09,548] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-16 19:51:09,548] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-16 19:51:09,548] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-16 19:51:09,548] [    INFO][0m - run_name                      :./checkpoints_bustm/[0m
[32m[2022-09-16 19:51:09,548] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-16 19:51:09,548] [    INFO][0m - save_steps                    :200[0m
[32m[2022-09-16 19:51:09,548] [    INFO][0m - save_strategy                 :IntervalStrategy.EPOCH[0m
[32m[2022-09-16 19:51:09,548] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-16 19:51:09,548] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-16 19:51:09,548] [    INFO][0m - seed                          :42[0m
[32m[2022-09-16 19:51:09,548] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-16 19:51:09,548] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-16 19:51:09,548] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-16 19:51:09,549] [    INFO][0m - train_batch_size              :16[0m
[32m[2022-09-16 19:51:09,549] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-16 19:51:09,549] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-16 19:51:09,549] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-16 19:51:09,549] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-16 19:51:09,549] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-16 19:51:09,549] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-16 19:51:09,549] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-16 19:51:09,549] [    INFO][0m - [0m
[32m[2022-09-16 19:51:09,553] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-16 19:51:09,553] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:51:09,553] [    INFO][0m -   Num Epochs = 30[0m
[32m[2022-09-16 19:51:09,553] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2022-09-16 19:51:09,553] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2022-09-16 19:51:09,553] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-16 19:51:09,553] [    INFO][0m -   Total optimization steps = 300.0[0m
[32m[2022-09-16 19:51:09,554] [    INFO][0m -   Total num train samples = 4800[0m
[32m[2022-09-16 19:51:12,561] [    INFO][0m - loss: 5.77232285, learning_rate: 2.9e-06, global_step: 10, interval_runtime: 3.0066, interval_samples_per_second: 5.322, interval_steps_per_second: 3.326, epoch: 1.0[0m
[32m[2022-09-16 19:51:12,562] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:51:12,563] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:51:12,563] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:51:12,563] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:51:12,563] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:51:13,348] [    INFO][0m - eval_loss: 1.5989818572998047, eval_accuracy: 0.5125, eval_runtime: 0.7844, eval_samples_per_second: 203.98, eval_steps_per_second: 12.749, epoch: 1.0[0m
[32m[2022-09-16 19:51:13,349] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-10[0m
[32m[2022-09-16 19:51:13,349] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:51:16,614] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-10/tokenizer_config.json[0m
[32m[2022-09-16 19:51:16,614] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-10/special_tokens_map.json[0m
[32m[2022-09-16 19:51:25,113] [    INFO][0m - loss: 1.15628729, learning_rate: 2.8000000000000003e-06, global_step: 20, interval_runtime: 12.5519, interval_samples_per_second: 1.275, interval_steps_per_second: 0.797, epoch: 2.0[0m
[32m[2022-09-16 19:51:25,114] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:51:25,114] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:51:25,114] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:51:25,114] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:51:25,114] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:51:25,874] [    INFO][0m - eval_loss: 0.803570568561554, eval_accuracy: 0.5125, eval_runtime: 0.7589, eval_samples_per_second: 210.821, eval_steps_per_second: 13.176, epoch: 2.0[0m
[32m[2022-09-16 19:51:25,874] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-20[0m
[32m[2022-09-16 19:51:25,874] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:51:29,081] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-20/tokenizer_config.json[0m
[32m[2022-09-16 19:51:29,081] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-20/special_tokens_map.json[0m
[32m[2022-09-16 19:51:36,691] [    INFO][0m - loss: 0.59801702, learning_rate: 2.7e-06, global_step: 30, interval_runtime: 11.5781, interval_samples_per_second: 1.382, interval_steps_per_second: 0.864, epoch: 3.0[0m
[32m[2022-09-16 19:51:36,692] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:51:36,692] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:51:36,692] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:51:36,692] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:51:36,692] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:51:37,451] [    INFO][0m - eval_loss: 0.38096100091934204, eval_accuracy: 0.5625, eval_runtime: 0.7582, eval_samples_per_second: 211.035, eval_steps_per_second: 13.19, epoch: 3.0[0m
[32m[2022-09-16 19:51:37,451] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-30[0m
[32m[2022-09-16 19:51:37,451] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:51:40,286] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-30/tokenizer_config.json[0m
[32m[2022-09-16 19:51:40,287] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-30/special_tokens_map.json[0m
[32m[2022-09-16 19:51:47,743] [    INFO][0m - loss: 0.3555902, learning_rate: 2.6e-06, global_step: 40, interval_runtime: 11.0524, interval_samples_per_second: 1.448, interval_steps_per_second: 0.905, epoch: 4.0[0m
[32m[2022-09-16 19:51:47,744] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:51:47,744] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:51:47,744] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:51:47,744] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:51:47,744] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:51:48,492] [    INFO][0m - eval_loss: 0.27856606245040894, eval_accuracy: 0.775, eval_runtime: 0.7477, eval_samples_per_second: 214.0, eval_steps_per_second: 13.375, epoch: 4.0[0m
[32m[2022-09-16 19:51:48,492] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-40[0m
[32m[2022-09-16 19:51:48,492] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:51:51,236] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-40/tokenizer_config.json[0m
[32m[2022-09-16 19:51:51,237] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-40/special_tokens_map.json[0m
[32m[2022-09-16 19:51:58,495] [    INFO][0m - loss: 0.26782019, learning_rate: 2.5e-06, global_step: 50, interval_runtime: 10.752, interval_samples_per_second: 1.488, interval_steps_per_second: 0.93, epoch: 5.0[0m
[32m[2022-09-16 19:51:58,496] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:51:58,496] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:51:58,496] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:51:58,496] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:51:58,496] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:51:59,234] [    INFO][0m - eval_loss: 0.2667742967605591, eval_accuracy: 0.725, eval_runtime: 0.7373, eval_samples_per_second: 217.013, eval_steps_per_second: 13.563, epoch: 5.0[0m
[32m[2022-09-16 19:51:59,234] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-50[0m
[32m[2022-09-16 19:51:59,234] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:52:01,973] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-50/tokenizer_config.json[0m
[32m[2022-09-16 19:52:01,973] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-50/special_tokens_map.json[0m
[32m[2022-09-16 19:52:09,346] [    INFO][0m - loss: 0.23798928, learning_rate: 2.4000000000000003e-06, global_step: 60, interval_runtime: 10.8501, interval_samples_per_second: 1.475, interval_steps_per_second: 0.922, epoch: 6.0[0m
[32m[2022-09-16 19:52:09,346] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:52:09,347] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:52:09,347] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:52:09,347] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:52:09,347] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:52:10,086] [    INFO][0m - eval_loss: 0.23302030563354492, eval_accuracy: 0.79375, eval_runtime: 0.7394, eval_samples_per_second: 216.378, eval_steps_per_second: 13.524, epoch: 6.0[0m
[32m[2022-09-16 19:52:10,087] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-60[0m
[32m[2022-09-16 19:52:10,087] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:52:17,432] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-60/tokenizer_config.json[0m
[32m[2022-09-16 19:52:17,432] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-60/special_tokens_map.json[0m
[32m[2022-09-16 19:52:24,649] [    INFO][0m - loss: 0.19991727, learning_rate: 2.3000000000000004e-06, global_step: 70, interval_runtime: 15.3035, interval_samples_per_second: 1.046, interval_steps_per_second: 0.653, epoch: 7.0[0m
[32m[2022-09-16 19:52:24,649] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:52:24,650] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:52:24,650] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:52:24,650] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:52:24,650] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:52:25,382] [    INFO][0m - eval_loss: 0.24100759625434875, eval_accuracy: 0.81875, eval_runtime: 0.7322, eval_samples_per_second: 218.515, eval_steps_per_second: 13.657, epoch: 7.0[0m
[32m[2022-09-16 19:52:25,383] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-70[0m
[32m[2022-09-16 19:52:25,383] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:52:28,040] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-70/tokenizer_config.json[0m
[32m[2022-09-16 19:52:28,113] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-70/special_tokens_map.json[0m
[32m[2022-09-16 19:52:35,293] [    INFO][0m - loss: 0.16469409, learning_rate: 2.1999999999999997e-06, global_step: 80, interval_runtime: 10.6442, interval_samples_per_second: 1.503, interval_steps_per_second: 0.939, epoch: 8.0[0m
[32m[2022-09-16 19:52:35,294] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:52:35,294] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:52:35,294] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:52:35,294] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:52:35,294] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:52:36,033] [    INFO][0m - eval_loss: 0.25420504808425903, eval_accuracy: 0.81875, eval_runtime: 0.7393, eval_samples_per_second: 216.422, eval_steps_per_second: 13.526, epoch: 8.0[0m
[32m[2022-09-16 19:52:36,034] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-80[0m
[32m[2022-09-16 19:52:36,034] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:52:39,822] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-80/tokenizer_config.json[0m
[32m[2022-09-16 19:52:39,823] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-80/special_tokens_map.json[0m
[32m[2022-09-16 19:52:46,787] [    INFO][0m - loss: 0.14140288, learning_rate: 2.1e-06, global_step: 90, interval_runtime: 11.4937, interval_samples_per_second: 1.392, interval_steps_per_second: 0.87, epoch: 9.0[0m
[32m[2022-09-16 19:52:46,787] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:52:46,788] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:52:46,788] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:52:46,788] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:52:46,788] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:52:47,535] [    INFO][0m - eval_loss: 0.2711009085178375, eval_accuracy: 0.8125, eval_runtime: 0.7341, eval_samples_per_second: 217.94, eval_steps_per_second: 13.621, epoch: 9.0[0m
[32m[2022-09-16 19:52:47,535] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-90[0m
[32m[2022-09-16 19:52:47,536] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:52:50,097] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-90/tokenizer_config.json[0m
[32m[2022-09-16 19:52:50,097] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-90/special_tokens_map.json[0m
[32m[2022-09-16 19:52:57,188] [    INFO][0m - loss: 0.11900821, learning_rate: 2e-06, global_step: 100, interval_runtime: 10.4014, interval_samples_per_second: 1.538, interval_steps_per_second: 0.961, epoch: 10.0[0m
[32m[2022-09-16 19:52:57,189] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:52:57,189] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:52:57,189] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:52:57,189] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:52:57,189] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:52:57,925] [    INFO][0m - eval_loss: 0.2933776378631592, eval_accuracy: 0.81875, eval_runtime: 0.7355, eval_samples_per_second: 217.542, eval_steps_per_second: 13.596, epoch: 10.0[0m
[32m[2022-09-16 19:52:57,925] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-100[0m
[32m[2022-09-16 19:52:57,925] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:53:00,499] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-16 19:53:00,944] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-16 19:53:07,892] [    INFO][0m - loss: 0.08958808, learning_rate: 1.9e-06, global_step: 110, interval_runtime: 10.7037, interval_samples_per_second: 1.495, interval_steps_per_second: 0.934, epoch: 11.0[0m
[32m[2022-09-16 19:53:07,892] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:53:07,892] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:53:07,893] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:53:07,893] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:53:07,893] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:53:08,641] [    INFO][0m - eval_loss: 0.30695515871047974, eval_accuracy: 0.825, eval_runtime: 0.7485, eval_samples_per_second: 213.774, eval_steps_per_second: 13.361, epoch: 11.0[0m
[32m[2022-09-16 19:53:08,642] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-110[0m
[32m[2022-09-16 19:53:08,642] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:53:11,199] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-110/tokenizer_config.json[0m
[32m[2022-09-16 19:53:11,199] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-110/special_tokens_map.json[0m
[32m[2022-09-16 19:53:18,127] [    INFO][0m - loss: 0.08002052, learning_rate: 1.8e-06, global_step: 120, interval_runtime: 10.2348, interval_samples_per_second: 1.563, interval_steps_per_second: 0.977, epoch: 12.0[0m
[32m[2022-09-16 19:53:18,127] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:53:18,127] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:53:18,127] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:53:18,127] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:53:18,128] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:53:18,862] [    INFO][0m - eval_loss: 0.340230792760849, eval_accuracy: 0.81875, eval_runtime: 0.7342, eval_samples_per_second: 217.922, eval_steps_per_second: 13.62, epoch: 12.0[0m
[32m[2022-09-16 19:53:18,862] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-120[0m
[32m[2022-09-16 19:53:18,862] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:53:21,380] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-120/tokenizer_config.json[0m
[32m[2022-09-16 19:53:21,380] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-120/special_tokens_map.json[0m
[32m[2022-09-16 19:53:28,274] [    INFO][0m - loss: 0.06283922, learning_rate: 1.7e-06, global_step: 130, interval_runtime: 10.1468, interval_samples_per_second: 1.577, interval_steps_per_second: 0.986, epoch: 13.0[0m
[32m[2022-09-16 19:53:28,274] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:53:28,274] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:53:28,275] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:53:28,275] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:53:28,275] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:53:29,012] [    INFO][0m - eval_loss: 0.3591872751712799, eval_accuracy: 0.80625, eval_runtime: 0.7375, eval_samples_per_second: 216.953, eval_steps_per_second: 13.56, epoch: 13.0[0m
[32m[2022-09-16 19:53:29,013] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-130[0m
[32m[2022-09-16 19:53:29,013] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:53:31,499] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-130/tokenizer_config.json[0m
[32m[2022-09-16 19:53:31,499] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-130/special_tokens_map.json[0m
[32m[2022-09-16 19:53:42,154] [    INFO][0m - loss: 0.04793467, learning_rate: 1.6e-06, global_step: 140, interval_runtime: 10.088, interval_samples_per_second: 1.586, interval_steps_per_second: 0.991, epoch: 14.0[0m
[32m[2022-09-16 19:53:42,155] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:53:42,155] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:53:42,155] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:53:42,155] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:53:42,155] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:53:42,902] [    INFO][0m - eval_loss: 0.39243948459625244, eval_accuracy: 0.81875, eval_runtime: 0.7466, eval_samples_per_second: 214.292, eval_steps_per_second: 13.393, epoch: 14.0[0m
[32m[2022-09-16 19:53:42,902] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-140[0m
[32m[2022-09-16 19:53:42,902] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:53:45,792] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-140/tokenizer_config.json[0m
[32m[2022-09-16 19:53:45,793] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-140/special_tokens_map.json[0m
[32m[2022-09-16 19:53:53,324] [    INFO][0m - loss: 0.05470588, learning_rate: 1.5e-06, global_step: 150, interval_runtime: 14.9625, interval_samples_per_second: 1.069, interval_steps_per_second: 0.668, epoch: 15.0[0m
[32m[2022-09-16 19:53:53,324] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:53:53,325] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:53:53,325] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:53:53,325] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:53:53,325] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:53:54,069] [    INFO][0m - eval_loss: 0.4165276885032654, eval_accuracy: 0.81875, eval_runtime: 0.7444, eval_samples_per_second: 214.944, eval_steps_per_second: 13.434, epoch: 15.0[0m
[32m[2022-09-16 19:53:54,070] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-150[0m
[32m[2022-09-16 19:53:54,070] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:53:56,633] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-150/tokenizer_config.json[0m
[32m[2022-09-16 19:53:56,633] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-150/special_tokens_map.json[0m
[32m[2022-09-16 19:54:03,585] [    INFO][0m - loss: 0.04914331, learning_rate: 1.4000000000000001e-06, global_step: 160, interval_runtime: 10.2608, interval_samples_per_second: 1.559, interval_steps_per_second: 0.975, epoch: 16.0[0m
[32m[2022-09-16 19:54:03,585] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:54:03,585] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:54:03,585] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:54:03,586] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:54:03,586] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:54:04,317] [    INFO][0m - eval_loss: 0.46271824836730957, eval_accuracy: 0.8125, eval_runtime: 0.7312, eval_samples_per_second: 218.804, eval_steps_per_second: 13.675, epoch: 16.0[0m
[32m[2022-09-16 19:54:04,756] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-160[0m
[32m[2022-09-16 19:54:04,756] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:54:07,308] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-160/tokenizer_config.json[0m
[32m[2022-09-16 19:54:07,309] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-160/special_tokens_map.json[0m
[32m[2022-09-16 19:54:14,184] [    INFO][0m - loss: 0.02846679, learning_rate: 1.3e-06, global_step: 170, interval_runtime: 10.5985, interval_samples_per_second: 1.51, interval_steps_per_second: 0.944, epoch: 17.0[0m
[32m[2022-09-16 19:54:14,184] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:54:14,184] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:54:14,185] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:54:14,185] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:54:14,185] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:54:14,922] [    INFO][0m - eval_loss: 0.47151193022727966, eval_accuracy: 0.825, eval_runtime: 0.7368, eval_samples_per_second: 217.142, eval_steps_per_second: 13.571, epoch: 17.0[0m
[32m[2022-09-16 19:54:15,889] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-170[0m
[32m[2022-09-16 19:54:15,889] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:54:18,422] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-170/tokenizer_config.json[0m
[32m[2022-09-16 19:54:18,423] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-170/special_tokens_map.json[0m
[32m[2022-09-16 19:54:25,350] [    INFO][0m - loss: 0.02814891, learning_rate: 1.2000000000000002e-06, global_step: 180, interval_runtime: 11.1668, interval_samples_per_second: 1.433, interval_steps_per_second: 0.896, epoch: 18.0[0m
[32m[2022-09-16 19:54:25,351] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:54:25,351] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:54:25,351] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:54:25,351] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:54:25,351] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:54:26,089] [    INFO][0m - eval_loss: 0.5106849670410156, eval_accuracy: 0.8125, eval_runtime: 0.7378, eval_samples_per_second: 216.853, eval_steps_per_second: 13.553, epoch: 18.0[0m
[32m[2022-09-16 19:54:26,089] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-180[0m
[32m[2022-09-16 19:54:26,090] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:54:28,598] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-180/tokenizer_config.json[0m
[32m[2022-09-16 19:54:28,598] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-180/special_tokens_map.json[0m
[32m[2022-09-16 19:54:35,703] [    INFO][0m - loss: 0.02586718, learning_rate: 1.0999999999999998e-06, global_step: 190, interval_runtime: 10.3532, interval_samples_per_second: 1.545, interval_steps_per_second: 0.966, epoch: 19.0[0m
[32m[2022-09-16 19:54:35,704] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:54:35,704] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:54:35,704] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:54:35,704] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:54:35,704] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:54:36,441] [    INFO][0m - eval_loss: 0.5551999807357788, eval_accuracy: 0.80625, eval_runtime: 0.7365, eval_samples_per_second: 217.245, eval_steps_per_second: 13.578, epoch: 19.0[0m
[32m[2022-09-16 19:54:36,441] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-190[0m
[32m[2022-09-16 19:54:36,441] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:54:39,151] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-190/tokenizer_config.json[0m
[32m[2022-09-16 19:54:39,152] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-190/special_tokens_map.json[0m
[32m[2022-09-16 19:54:46,367] [    INFO][0m - loss: 0.02353724, learning_rate: 1e-06, global_step: 200, interval_runtime: 10.6635, interval_samples_per_second: 1.5, interval_steps_per_second: 0.938, epoch: 20.0[0m
[32m[2022-09-16 19:54:46,367] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:54:46,367] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:54:46,368] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:54:46,368] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:54:46,368] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:54:47,114] [    INFO][0m - eval_loss: 0.5615605115890503, eval_accuracy: 0.8, eval_runtime: 0.7461, eval_samples_per_second: 214.461, eval_steps_per_second: 13.404, epoch: 20.0[0m
[32m[2022-09-16 19:54:47,420] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-200[0m
[32m[2022-09-16 19:54:47,420] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:54:49,926] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-200/tokenizer_config.json[0m
[32m[2022-09-16 19:54:49,927] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-200/special_tokens_map.json[0m
[32m[2022-09-16 19:54:56,914] [    INFO][0m - loss: 0.03752085, learning_rate: 9e-07, global_step: 210, interval_runtime: 10.5474, interval_samples_per_second: 1.517, interval_steps_per_second: 0.948, epoch: 21.0[0m
[32m[2022-09-16 19:54:56,915] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:54:56,915] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:54:56,915] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:54:56,915] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:54:56,915] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:54:57,654] [    INFO][0m - eval_loss: 0.5837724804878235, eval_accuracy: 0.8125, eval_runtime: 0.7384, eval_samples_per_second: 216.675, eval_steps_per_second: 13.542, epoch: 21.0[0m
[32m[2022-09-16 19:54:57,654] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-210[0m
[32m[2022-09-16 19:54:57,654] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:55:00,170] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-210/tokenizer_config.json[0m
[32m[2022-09-16 19:55:00,170] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-210/special_tokens_map.json[0m
[32m[2022-09-16 19:55:06,965] [    INFO][0m - loss: 0.00741129, learning_rate: 8e-07, global_step: 220, interval_runtime: 10.0508, interval_samples_per_second: 1.592, interval_steps_per_second: 0.995, epoch: 22.0[0m
[32m[2022-09-16 19:55:06,966] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:55:06,966] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:55:06,966] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:55:06,966] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:55:06,966] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:55:08,315] [    INFO][0m - eval_loss: 0.6081163287162781, eval_accuracy: 0.80625, eval_runtime: 0.7395, eval_samples_per_second: 216.355, eval_steps_per_second: 13.522, epoch: 22.0[0m
[32m[2022-09-16 19:55:08,316] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-220[0m
[32m[2022-09-16 19:55:08,316] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:55:10,778] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-220/tokenizer_config.json[0m
[32m[2022-09-16 19:55:10,778] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-220/special_tokens_map.json[0m
[32m[2022-09-16 19:55:17,492] [    INFO][0m - loss: 0.01393446, learning_rate: 7.000000000000001e-07, global_step: 230, interval_runtime: 10.5272, interval_samples_per_second: 1.52, interval_steps_per_second: 0.95, epoch: 23.0[0m
[32m[2022-09-16 19:55:17,493] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:55:17,493] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:55:17,493] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:55:17,493] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:55:17,493] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:55:18,226] [    INFO][0m - eval_loss: 0.621627688407898, eval_accuracy: 0.80625, eval_runtime: 0.733, eval_samples_per_second: 218.286, eval_steps_per_second: 13.643, epoch: 23.0[0m
[32m[2022-09-16 19:55:18,227] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-230[0m
[32m[2022-09-16 19:55:18,227] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:55:22,036] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-230/tokenizer_config.json[0m
[32m[2022-09-16 19:55:22,036] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-230/special_tokens_map.json[0m
[32m[2022-09-16 19:55:28,851] [    INFO][0m - loss: 0.00316853, learning_rate: 6.000000000000001e-07, global_step: 240, interval_runtime: 11.3591, interval_samples_per_second: 1.409, interval_steps_per_second: 0.88, epoch: 24.0[0m
[32m[2022-09-16 19:55:28,852] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:55:28,852] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:55:28,852] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:55:28,852] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:55:28,852] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:55:29,588] [    INFO][0m - eval_loss: 0.6434474587440491, eval_accuracy: 0.8, eval_runtime: 0.7357, eval_samples_per_second: 217.475, eval_steps_per_second: 13.592, epoch: 24.0[0m
[32m[2022-09-16 19:55:29,588] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-240[0m
[32m[2022-09-16 19:55:29,589] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:55:32,081] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-240/tokenizer_config.json[0m
[32m[2022-09-16 19:55:32,082] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-240/special_tokens_map.json[0m
[32m[2022-09-16 19:55:38,786] [    INFO][0m - loss: 0.00151016, learning_rate: 5e-07, global_step: 250, interval_runtime: 9.935, interval_samples_per_second: 1.61, interval_steps_per_second: 1.007, epoch: 25.0[0m
[32m[2022-09-16 19:55:38,787] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:55:38,787] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:55:38,787] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:55:38,787] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:55:38,787] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:55:39,525] [    INFO][0m - eval_loss: 0.6614589095115662, eval_accuracy: 0.8, eval_runtime: 0.7374, eval_samples_per_second: 216.977, eval_steps_per_second: 13.561, epoch: 25.0[0m
[32m[2022-09-16 19:55:39,525] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-250[0m
[32m[2022-09-16 19:55:39,525] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:55:41,948] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-250/tokenizer_config.json[0m
[32m[2022-09-16 19:55:41,949] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-250/special_tokens_map.json[0m
[32m[2022-09-16 19:55:48,598] [    INFO][0m - loss: 0.01402308, learning_rate: 4e-07, global_step: 260, interval_runtime: 9.8109, interval_samples_per_second: 1.631, interval_steps_per_second: 1.019, epoch: 26.0[0m
[32m[2022-09-16 19:55:48,598] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:55:48,598] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:55:48,599] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:55:48,599] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:55:48,599] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:55:49,330] [    INFO][0m - eval_loss: 0.6739277243614197, eval_accuracy: 0.8, eval_runtime: 0.7316, eval_samples_per_second: 218.69, eval_steps_per_second: 13.668, epoch: 26.0[0m
[32m[2022-09-16 19:55:49,920] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-260[0m
[32m[2022-09-16 19:55:49,920] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:55:52,418] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-260/tokenizer_config.json[0m
[32m[2022-09-16 19:55:52,418] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-260/special_tokens_map.json[0m
[32m[2022-09-16 19:55:59,216] [    INFO][0m - loss: 0.00852655, learning_rate: 3.0000000000000004e-07, global_step: 270, interval_runtime: 10.6188, interval_samples_per_second: 1.507, interval_steps_per_second: 0.942, epoch: 27.0[0m
[32m[2022-09-16 19:55:59,217] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:55:59,217] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:55:59,217] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:55:59,217] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:55:59,217] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:55:59,953] [    INFO][0m - eval_loss: 0.686284065246582, eval_accuracy: 0.8125, eval_runtime: 0.7361, eval_samples_per_second: 217.357, eval_steps_per_second: 13.585, epoch: 27.0[0m
[32m[2022-09-16 19:55:59,954] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-270[0m
[32m[2022-09-16 19:55:59,954] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:56:03,789] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-270/tokenizer_config.json[0m
[32m[2022-09-16 19:56:03,790] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-270/special_tokens_map.json[0m
[32m[2022-09-16 19:56:10,460] [    INFO][0m - loss: 0.02416702, learning_rate: 2e-07, global_step: 280, interval_runtime: 11.2434, interval_samples_per_second: 1.423, interval_steps_per_second: 0.889, epoch: 28.0[0m
[32m[2022-09-16 19:56:10,460] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:56:10,460] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:56:10,460] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:56:10,460] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:56:10,460] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:56:11,217] [    INFO][0m - eval_loss: 0.6948781609535217, eval_accuracy: 0.8125, eval_runtime: 0.7563, eval_samples_per_second: 211.546, eval_steps_per_second: 13.222, epoch: 28.0[0m
[32m[2022-09-16 19:56:11,382] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-280[0m
[32m[2022-09-16 19:56:11,383] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:56:13,979] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-280/tokenizer_config.json[0m
[32m[2022-09-16 19:56:13,979] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-280/special_tokens_map.json[0m
[32m[2022-09-16 19:56:20,949] [    INFO][0m - loss: 0.00857016, learning_rate: 1e-07, global_step: 290, interval_runtime: 10.4897, interval_samples_per_second: 1.525, interval_steps_per_second: 0.953, epoch: 29.0[0m
[32m[2022-09-16 19:56:21,828] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:56:21,828] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:56:21,828] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:56:21,828] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:56:21,828] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:56:22,566] [    INFO][0m - eval_loss: 0.6934242248535156, eval_accuracy: 0.8125, eval_runtime: 0.7379, eval_samples_per_second: 216.832, eval_steps_per_second: 13.552, epoch: 29.0[0m
[32m[2022-09-16 19:56:22,567] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-290[0m
[32m[2022-09-16 19:56:22,567] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:56:25,175] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-290/tokenizer_config.json[0m
[32m[2022-09-16 19:56:25,175] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-290/special_tokens_map.json[0m
[32m[2022-09-16 19:56:32,089] [    INFO][0m - loss: 0.00150612, learning_rate: 0.0, global_step: 300, interval_runtime: 11.1401, interval_samples_per_second: 1.436, interval_steps_per_second: 0.898, epoch: 30.0[0m
[32m[2022-09-16 19:56:32,090] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:56:32,090] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 19:56:32,090] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:56:32,090] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:56:32,090] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 19:56:32,827] [    INFO][0m - eval_loss: 0.6934168934822083, eval_accuracy: 0.8125, eval_runtime: 0.7367, eval_samples_per_second: 217.178, eval_steps_per_second: 13.574, epoch: 30.0[0m
[32m[2022-09-16 19:56:32,827] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-300[0m
[32m[2022-09-16 19:56:32,827] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:56:35,346] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-300/tokenizer_config.json[0m
[32m[2022-09-16 19:56:35,347] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-300/special_tokens_map.json[0m
[32m[2022-09-16 19:56:40,327] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-16 19:56:40,327] [    INFO][0m - Loading best model from ./checkpoints_bustm/checkpoint-110 (score: 0.825).[0m
[32m[2022-09-16 19:56:41,779] [    INFO][0m - train_runtime: 332.2253, train_samples_per_second: 14.448, train_steps_per_second: 0.903, train_loss: 0.32078797719130914, epoch: 30.0[0m
[32m[2022-09-16 19:56:41,794] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/[0m
[32m[2022-09-16 19:56:41,795] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:56:45,466] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/tokenizer_config.json[0m
[32m[2022-09-16 19:56:45,467] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/special_tokens_map.json[0m
[32m[2022-09-16 19:56:45,468] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-16 19:56:45,468] [    INFO][0m -   epoch                    =       30.0[0m
[32m[2022-09-16 19:56:45,468] [    INFO][0m -   train_loss               =     0.3208[0m
[32m[2022-09-16 19:56:45,468] [    INFO][0m -   train_runtime            = 0:05:32.22[0m
[32m[2022-09-16 19:56:45,469] [    INFO][0m -   train_samples_per_second =     14.448[0m
[32m[2022-09-16 19:56:45,469] [    INFO][0m -   train_steps_per_second   =      0.903[0m
[32m[2022-09-16 19:56:45,471] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-16 19:56:45,472] [    INFO][0m -   Num examples = 1772[0m
[32m[2022-09-16 19:56:45,472] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:56:45,472] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:56:45,472] [    INFO][0m -   Total prediction steps = 111[0m
[32m[2022-09-16 19:56:54,550] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-16 19:56:54,550] [    INFO][0m -   test_accuracy           =     0.7754[0m
[32m[2022-09-16 19:56:54,550] [    INFO][0m -   test_loss               =     0.3266[0m
[32m[2022-09-16 19:56:54,551] [    INFO][0m -   test_runtime            = 0:00:09.07[0m
[32m[2022-09-16 19:56:54,551] [    INFO][0m -   test_samples_per_second =    195.197[0m
[32m[2022-09-16 19:56:54,551] [    INFO][0m -   test_steps_per_second   =     12.227[0m
[32m[2022-09-16 19:56:54,551] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-16 19:56:54,551] [    INFO][0m -   Num examples = 2000[0m
[32m[2022-09-16 19:56:54,551] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:56:54,551] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:56:54,552] [    INFO][0m -   Total prediction steps = 125[0m
[32m[2022-09-16 19:57:05,504] [    INFO][0m - Predictions for bustm saved to ./fewclue_submit_examples.[0m
{
  "labels": 1,
  "text_a": "\u53eb\u7238\u7238\u53eb\u4e00\u58f0\u6211\u542c\u542c",
  "text_b": "\u90a3\u4f60\u53eb\u6211\u4e00\u58f0\u7238\u7238",
  "uid": 0
}

 
==========
chid
==========
 
[32m[2022-09-16 19:57:37,356] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-16 19:57:37,357] [    INFO][0m - ============================================================[0m
[32m[2022-09-16 19:57:37,357] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-16 19:57:37,357] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-16 19:57:37,357] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-16 19:57:37,357] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-16 19:57:37,357] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-16 19:57:37,357] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-16 19:57:37,357] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-16 19:57:37,357] [    INFO][0m - [0m
[32m[2022-09-16 19:57:37,357] [    INFO][0m - ============================================================[0m
[32m[2022-09-16 19:57:37,357] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-16 19:57:37,358] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-16 19:57:37,358] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-16 19:57:37,358] [    INFO][0m - pretrained                    :../checkpoints_cmnli/checkpoint-22000/model_state.pdparams[0m
[32m[2022-09-16 19:57:37,358] [    INFO][0m - prompt                        :‚Äú{'text':'text_a'}‚ÄùËøôÂè•ËØù‰∏≠ÊàêËØ≠[{'text':'text_b'}]ÁöÑÁêÜËß£Ê≠£Á°ÆÂêóÔºü{'mask'}{'mask'}„ÄÇ[0m
[32m[2022-09-16 19:57:37,358] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-16 19:57:37,358] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-16 19:57:37,358] [    INFO][0m - task_name                     :chid[0m
[32m[2022-09-16 19:57:37,358] [    INFO][0m - [0m
[32m[2022-09-16 19:57:37,358] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0916 19:57:37.360416 30827 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0916 19:57:37.364554 30827 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-16 19:57:43,582] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-16 19:57:43,593] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-16 19:57:43,594] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-16 19:57:43,594] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùËøôÂè•ËØù‰∏≠ÊàêËØ≠['}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': ']ÁöÑÁêÜËß£Ê≠£Á°ÆÂêóÔºü'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': '„ÄÇ'}][0m
[32m[2022-09-16 19:57:45,631] [    INFO][0m - ============================================================[0m
[32m[2022-09-16 19:57:45,632] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-16 19:57:45,632] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-16 19:57:45,632] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-16 19:57:45,632] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-16 19:57:45,632] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-16 19:57:45,632] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-16 19:57:45,632] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-16 19:57:45,632] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-16 19:57:45,632] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-16 19:57:45,633] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-16 19:57:45,633] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-16 19:57:45,633] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-16 19:57:45,633] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-16 19:57:45,633] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-16 19:57:45,633] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-16 19:57:45,633] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-16 19:57:45,633] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-16 19:57:45,633] [    INFO][0m - eval_batch_size               :16[0m
[32m[2022-09-16 19:57:45,633] [    INFO][0m - eval_steps                    :200[0m
[32m[2022-09-16 19:57:45,633] [    INFO][0m - evaluation_strategy           :IntervalStrategy.EPOCH[0m
[32m[2022-09-16 19:57:45,633] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-16 19:57:45,633] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-16 19:57:45,634] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-16 19:57:45,634] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-16 19:57:45,634] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-16 19:57:45,634] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-16 19:57:45,634] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-16 19:57:45,634] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-16 19:57:45,634] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-16 19:57:45,634] [    INFO][0m - learning_rate                 :3e-06[0m
[32m[2022-09-16 19:57:45,634] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-16 19:57:45,634] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-16 19:57:45,634] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-16 19:57:45,634] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-16 19:57:45,634] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-16 19:57:45,634] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-16 19:57:45,635] [    INFO][0m - logging_dir                   :./checkpoints_chid/runs/Sep16_19-57-37_instance-3bwob41y-01[0m
[32m[2022-09-16 19:57:45,635] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-16 19:57:45,635] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-16 19:57:45,635] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-16 19:57:45,635] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-16 19:57:45,635] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-16 19:57:45,635] [    INFO][0m - max_seq_length                :256[0m
[32m[2022-09-16 19:57:45,635] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-16 19:57:45,635] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-16 19:57:45,635] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-16 19:57:45,635] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-16 19:57:45,635] [    INFO][0m - num_train_epochs              :30.0[0m
[32m[2022-09-16 19:57:45,635] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-16 19:57:45,636] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-16 19:57:45,636] [    INFO][0m - output_dir                    :./checkpoints_chid/[0m
[32m[2022-09-16 19:57:45,636] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-16 19:57:45,636] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-16 19:57:45,636] [    INFO][0m - per_device_eval_batch_size    :16[0m
[32m[2022-09-16 19:57:45,636] [    INFO][0m - per_device_train_batch_size   :16[0m
[32m[2022-09-16 19:57:45,636] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-16 19:57:45,636] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-16 19:57:45,636] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-16 19:57:45,636] [    INFO][0m - ppt_learning_rate             :3e-05[0m
[32m[2022-09-16 19:57:45,636] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-16 19:57:45,636] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-16 19:57:45,636] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-16 19:57:45,636] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-16 19:57:45,637] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-16 19:57:45,637] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-16 19:57:45,637] [    INFO][0m - run_name                      :./checkpoints_chid/[0m
[32m[2022-09-16 19:57:45,637] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-16 19:57:45,637] [    INFO][0m - save_steps                    :200[0m
[32m[2022-09-16 19:57:45,637] [    INFO][0m - save_strategy                 :IntervalStrategy.EPOCH[0m
[32m[2022-09-16 19:57:45,637] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-16 19:57:45,637] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-16 19:57:45,637] [    INFO][0m - seed                          :42[0m
[32m[2022-09-16 19:57:45,637] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-16 19:57:45,637] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-16 19:57:45,637] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-16 19:57:45,637] [    INFO][0m - train_batch_size              :16[0m
[32m[2022-09-16 19:57:45,637] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-16 19:57:45,638] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-16 19:57:45,638] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-16 19:57:45,638] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-16 19:57:45,638] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-16 19:57:45,638] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-16 19:57:45,638] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-16 19:57:45,638] [    INFO][0m - [0m
[32m[2022-09-16 19:57:45,640] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-16 19:57:45,641] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 19:57:45,641] [    INFO][0m -   Num Epochs = 30[0m
[32m[2022-09-16 19:57:45,641] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2022-09-16 19:57:45,641] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2022-09-16 19:57:45,641] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-16 19:57:45,641] [    INFO][0m -   Total optimization steps = 2670.0[0m
[32m[2022-09-16 19:57:45,641] [    INFO][0m -   Total num train samples = 42420[0m
[32m[2022-09-16 19:57:52,937] [    INFO][0m - loss: 6.60932388, learning_rate: 2.98876404494382e-06, global_step: 10, interval_runtime: 7.2949, interval_samples_per_second: 2.193, interval_steps_per_second: 1.371, epoch: 0.1124[0m
[32m[2022-09-16 19:57:59,255] [    INFO][0m - loss: 1.38816395, learning_rate: 2.9775280898876406e-06, global_step: 20, interval_runtime: 6.3183, interval_samples_per_second: 2.532, interval_steps_per_second: 1.583, epoch: 0.2247[0m
[32m[2022-09-16 19:58:05,557] [    INFO][0m - loss: 0.3493444, learning_rate: 2.9662921348314606e-06, global_step: 30, interval_runtime: 6.3012, interval_samples_per_second: 2.539, interval_steps_per_second: 1.587, epoch: 0.3371[0m
[32m[2022-09-16 19:58:11,881] [    INFO][0m - loss: 0.58599501, learning_rate: 2.955056179775281e-06, global_step: 40, interval_runtime: 6.3244, interval_samples_per_second: 2.53, interval_steps_per_second: 1.581, epoch: 0.4494[0m
[32m[2022-09-16 19:58:18,216] [    INFO][0m - loss: 0.46219568, learning_rate: 2.943820224719101e-06, global_step: 50, interval_runtime: 6.3349, interval_samples_per_second: 2.526, interval_steps_per_second: 1.579, epoch: 0.5618[0m
[32m[2022-09-16 19:58:24,549] [    INFO][0m - loss: 0.43160357, learning_rate: 2.932584269662921e-06, global_step: 60, interval_runtime: 6.3324, interval_samples_per_second: 2.527, interval_steps_per_second: 1.579, epoch: 0.6742[0m
[32m[2022-09-16 19:58:30,877] [    INFO][0m - loss: 0.4254725, learning_rate: 2.921348314606742e-06, global_step: 70, interval_runtime: 6.3286, interval_samples_per_second: 2.528, interval_steps_per_second: 1.58, epoch: 0.7865[0m
[32m[2022-09-16 19:58:37,184] [    INFO][0m - loss: 0.50352793, learning_rate: 2.910112359550562e-06, global_step: 80, interval_runtime: 6.3069, interval_samples_per_second: 2.537, interval_steps_per_second: 1.586, epoch: 0.8989[0m
[32m[2022-09-16 19:58:42,396] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 19:58:42,397] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 19:58:42,397] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 19:58:42,397] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 19:58:42,397] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 19:59:04,505] [    INFO][0m - eval_loss: 0.42089343070983887, eval_accuracy: 0.8571428571428571, eval_runtime: 22.1076, eval_samples_per_second: 63.96, eval_steps_per_second: 4.026, epoch: 1.0[0m
[32m[2022-09-16 19:59:04,536] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-89[0m
[32m[2022-09-16 19:59:04,537] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 19:59:07,373] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-89/tokenizer_config.json[0m
[32m[2022-09-16 19:59:07,373] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-89/special_tokens_map.json[0m
[32m[2022-09-16 19:59:13,699] [    INFO][0m - loss: 0.45024905, learning_rate: 2.898876404494382e-06, global_step: 90, interval_runtime: 36.5156, interval_samples_per_second: 0.438, interval_steps_per_second: 0.274, epoch: 1.0112[0m
[32m[2022-09-16 19:59:20,012] [    INFO][0m - loss: 0.49322543, learning_rate: 2.8876404494382025e-06, global_step: 100, interval_runtime: 6.313, interval_samples_per_second: 2.534, interval_steps_per_second: 1.584, epoch: 1.1236[0m
[32m[2022-09-16 19:59:26,345] [    INFO][0m - loss: 0.45063243, learning_rate: 2.8764044943820226e-06, global_step: 110, interval_runtime: 6.3329, interval_samples_per_second: 2.526, interval_steps_per_second: 1.579, epoch: 1.236[0m
[32m[2022-09-16 19:59:32,665] [    INFO][0m - loss: 0.47316108, learning_rate: 2.8651685393258426e-06, global_step: 120, interval_runtime: 6.3193, interval_samples_per_second: 2.532, interval_steps_per_second: 1.582, epoch: 1.3483[0m
[32m[2022-09-16 19:59:38,988] [    INFO][0m - loss: 0.42147236, learning_rate: 2.853932584269663e-06, global_step: 130, interval_runtime: 6.3228, interval_samples_per_second: 2.531, interval_steps_per_second: 1.582, epoch: 1.4607[0m
[32m[2022-09-16 19:59:45,313] [    INFO][0m - loss: 0.43152423, learning_rate: 2.842696629213483e-06, global_step: 140, interval_runtime: 6.3259, interval_samples_per_second: 2.529, interval_steps_per_second: 1.581, epoch: 1.573[0m
[32m[2022-09-16 19:59:51,752] [    INFO][0m - loss: 0.4507164, learning_rate: 2.8314606741573035e-06, global_step: 150, interval_runtime: 6.4389, interval_samples_per_second: 2.485, interval_steps_per_second: 1.553, epoch: 1.6854[0m
[32m[2022-09-16 19:59:58,070] [    INFO][0m - loss: 0.36512163, learning_rate: 2.8202247191011236e-06, global_step: 160, interval_runtime: 6.318, interval_samples_per_second: 2.532, interval_steps_per_second: 1.583, epoch: 1.7978[0m
[32m[2022-09-16 20:00:04,397] [    INFO][0m - loss: 0.44731216, learning_rate: 2.8089887640449436e-06, global_step: 170, interval_runtime: 6.3265, interval_samples_per_second: 2.529, interval_steps_per_second: 1.581, epoch: 1.9101[0m
[32m[2022-09-16 20:00:08,987] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 20:00:08,987] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 20:00:08,987] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:00:08,988] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:00:08,988] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 20:00:31,005] [    INFO][0m - eval_loss: 0.4303072690963745, eval_accuracy: 0.8571428571428571, eval_runtime: 22.0174, eval_samples_per_second: 64.222, eval_steps_per_second: 4.042, epoch: 2.0[0m
[32m[2022-09-16 20:00:31,030] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-178[0m
[32m[2022-09-16 20:00:31,030] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:00:33,926] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-178/tokenizer_config.json[0m
[32m[2022-09-16 20:00:33,926] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-178/special_tokens_map.json[0m
[32m[2022-09-16 20:00:41,050] [    INFO][0m - loss: 0.38300641, learning_rate: 2.797752808988764e-06, global_step: 180, interval_runtime: 36.653, interval_samples_per_second: 0.437, interval_steps_per_second: 0.273, epoch: 2.0225[0m
[32m[2022-09-16 20:00:47,352] [    INFO][0m - loss: 0.44187908, learning_rate: 2.7865168539325845e-06, global_step: 190, interval_runtime: 6.3025, interval_samples_per_second: 2.539, interval_steps_per_second: 1.587, epoch: 2.1348[0m
[32m[2022-09-16 20:00:53,690] [    INFO][0m - loss: 0.43811765, learning_rate: 2.7752808988764045e-06, global_step: 200, interval_runtime: 6.3375, interval_samples_per_second: 2.525, interval_steps_per_second: 1.578, epoch: 2.2472[0m
[32m[2022-09-16 20:01:00,036] [    INFO][0m - loss: 0.44482656, learning_rate: 2.764044943820225e-06, global_step: 210, interval_runtime: 6.3451, interval_samples_per_second: 2.522, interval_steps_per_second: 1.576, epoch: 2.3596[0m
[32m[2022-09-16 20:01:06,389] [    INFO][0m - loss: 0.48100224, learning_rate: 2.752808988764045e-06, global_step: 220, interval_runtime: 6.3543, interval_samples_per_second: 2.518, interval_steps_per_second: 1.574, epoch: 2.4719[0m
[32m[2022-09-16 20:01:12,727] [    INFO][0m - loss: 0.49490247, learning_rate: 2.7415730337078655e-06, global_step: 230, interval_runtime: 6.3369, interval_samples_per_second: 2.525, interval_steps_per_second: 1.578, epoch: 2.5843[0m
[32m[2022-09-16 20:01:19,071] [    INFO][0m - loss: 0.46876254, learning_rate: 2.7303370786516855e-06, global_step: 240, interval_runtime: 6.3443, interval_samples_per_second: 2.522, interval_steps_per_second: 1.576, epoch: 2.6966[0m
[32m[2022-09-16 20:01:25,405] [    INFO][0m - loss: 0.39089611, learning_rate: 2.7191011235955055e-06, global_step: 250, interval_runtime: 6.3347, interval_samples_per_second: 2.526, interval_steps_per_second: 1.579, epoch: 2.809[0m
[32m[2022-09-16 20:01:31,743] [    INFO][0m - loss: 0.37983618, learning_rate: 2.707865168539326e-06, global_step: 260, interval_runtime: 6.3378, interval_samples_per_second: 2.525, interval_steps_per_second: 1.578, epoch: 2.9213[0m
[32m[2022-09-16 20:01:35,704] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 20:01:35,704] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 20:01:35,704] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:01:35,704] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:01:35,704] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 20:01:57,699] [    INFO][0m - eval_loss: 0.41265058517456055, eval_accuracy: 0.8571428571428571, eval_runtime: 21.9942, eval_samples_per_second: 64.29, eval_steps_per_second: 4.047, epoch: 3.0[0m
[32m[2022-09-16 20:01:57,724] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-267[0m
[32m[2022-09-16 20:01:57,724] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:02:00,794] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-267/tokenizer_config.json[0m
[32m[2022-09-16 20:02:00,795] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-267/special_tokens_map.json[0m
[32m[2022-09-16 20:02:08,320] [    INFO][0m - loss: 0.34344289, learning_rate: 2.696629213483146e-06, global_step: 270, interval_runtime: 36.5776, interval_samples_per_second: 0.437, interval_steps_per_second: 0.273, epoch: 3.0337[0m
[32m[2022-09-16 20:02:14,652] [    INFO][0m - loss: 0.42297902, learning_rate: 2.685393258426966e-06, global_step: 280, interval_runtime: 6.3312, interval_samples_per_second: 2.527, interval_steps_per_second: 1.579, epoch: 3.1461[0m
[32m[2022-09-16 20:02:20,967] [    INFO][0m - loss: 0.41445265, learning_rate: 2.6741573033707865e-06, global_step: 290, interval_runtime: 6.3152, interval_samples_per_second: 2.534, interval_steps_per_second: 1.583, epoch: 3.2584[0m
[32m[2022-09-16 20:02:27,286] [    INFO][0m - loss: 0.4480062, learning_rate: 2.6629213483146066e-06, global_step: 300, interval_runtime: 6.3193, interval_samples_per_second: 2.532, interval_steps_per_second: 1.582, epoch: 3.3708[0m
[32m[2022-09-16 20:02:33,596] [    INFO][0m - loss: 0.44447885, learning_rate: 2.651685393258427e-06, global_step: 310, interval_runtime: 6.3102, interval_samples_per_second: 2.536, interval_steps_per_second: 1.585, epoch: 3.4831[0m
[32m[2022-09-16 20:02:39,914] [    INFO][0m - loss: 0.43538375, learning_rate: 2.6404494382022475e-06, global_step: 320, interval_runtime: 6.3181, interval_samples_per_second: 2.532, interval_steps_per_second: 1.583, epoch: 3.5955[0m
[32m[2022-09-16 20:02:46,248] [    INFO][0m - loss: 0.42206273, learning_rate: 2.6292134831460675e-06, global_step: 330, interval_runtime: 6.3335, interval_samples_per_second: 2.526, interval_steps_per_second: 1.579, epoch: 3.7079[0m
[32m[2022-09-16 20:02:52,572] [    INFO][0m - loss: 0.37562132, learning_rate: 2.617977528089888e-06, global_step: 340, interval_runtime: 6.3243, interval_samples_per_second: 2.53, interval_steps_per_second: 1.581, epoch: 3.8202[0m
[32m[2022-09-16 20:02:58,893] [    INFO][0m - loss: 0.43807602, learning_rate: 2.606741573033708e-06, global_step: 350, interval_runtime: 6.3204, interval_samples_per_second: 2.531, interval_steps_per_second: 1.582, epoch: 3.9326[0m
[32m[2022-09-16 20:03:02,218] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 20:03:02,219] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 20:03:02,219] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:03:02,219] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:03:02,219] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 20:03:24,195] [    INFO][0m - eval_loss: 0.43023526668548584, eval_accuracy: 0.8571428571428571, eval_runtime: 21.9758, eval_samples_per_second: 64.344, eval_steps_per_second: 4.05, epoch: 4.0[0m
[32m[2022-09-16 20:03:24,219] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-356[0m
[32m[2022-09-16 20:03:24,219] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:03:27,095] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-356/tokenizer_config.json[0m
[32m[2022-09-16 20:03:27,095] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-356/special_tokens_map.json[0m
[32m[2022-09-16 20:03:35,169] [    INFO][0m - loss: 0.47756066, learning_rate: 2.595505617977528e-06, global_step: 360, interval_runtime: 36.2766, interval_samples_per_second: 0.441, interval_steps_per_second: 0.276, epoch: 4.0449[0m
[32m[2022-09-16 20:03:41,469] [    INFO][0m - loss: 0.41132331, learning_rate: 2.5842696629213485e-06, global_step: 370, interval_runtime: 6.3002, interval_samples_per_second: 2.54, interval_steps_per_second: 1.587, epoch: 4.1573[0m
[32m[2022-09-16 20:03:47,791] [    INFO][0m - loss: 0.45073309, learning_rate: 2.5730337078651685e-06, global_step: 380, interval_runtime: 6.3212, interval_samples_per_second: 2.531, interval_steps_per_second: 1.582, epoch: 4.2697[0m
[32m[2022-09-16 20:03:54,103] [    INFO][0m - loss: 0.40839591, learning_rate: 2.561797752808989e-06, global_step: 390, interval_runtime: 6.312, interval_samples_per_second: 2.535, interval_steps_per_second: 1.584, epoch: 4.382[0m
[32m[2022-09-16 20:04:00,417] [    INFO][0m - loss: 0.43967829, learning_rate: 2.550561797752809e-06, global_step: 400, interval_runtime: 6.3146, interval_samples_per_second: 2.534, interval_steps_per_second: 1.584, epoch: 4.4944[0m
[32m[2022-09-16 20:04:06,741] [    INFO][0m - loss: 0.40209169, learning_rate: 2.539325842696629e-06, global_step: 410, interval_runtime: 6.3232, interval_samples_per_second: 2.53, interval_steps_per_second: 1.581, epoch: 4.6067[0m
[32m[2022-09-16 20:04:13,057] [    INFO][0m - loss: 0.46805239, learning_rate: 2.5280898876404495e-06, global_step: 420, interval_runtime: 6.3165, interval_samples_per_second: 2.533, interval_steps_per_second: 1.583, epoch: 4.7191[0m
[32m[2022-09-16 20:04:19,369] [    INFO][0m - loss: 0.46467104, learning_rate: 2.51685393258427e-06, global_step: 430, interval_runtime: 6.3114, interval_samples_per_second: 2.535, interval_steps_per_second: 1.584, epoch: 4.8315[0m
[32m[2022-09-16 20:04:25,680] [    INFO][0m - loss: 0.46636968, learning_rate: 2.50561797752809e-06, global_step: 440, interval_runtime: 6.311, interval_samples_per_second: 2.535, interval_steps_per_second: 1.585, epoch: 4.9438[0m
[32m[2022-09-16 20:04:28,388] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 20:04:28,388] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 20:04:28,388] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:04:28,388] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:04:28,388] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 20:04:50,328] [    INFO][0m - eval_loss: 0.43115878105163574, eval_accuracy: 0.8571428571428571, eval_runtime: 21.9391, eval_samples_per_second: 64.451, eval_steps_per_second: 4.057, epoch: 5.0[0m
[32m[2022-09-16 20:04:50,352] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-445[0m
[32m[2022-09-16 20:04:50,352] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:04:53,162] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-445/tokenizer_config.json[0m
[32m[2022-09-16 20:04:53,162] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-445/special_tokens_map.json[0m
[32m[2022-09-16 20:05:01,836] [    INFO][0m - loss: 0.32293296, learning_rate: 2.4943820224719104e-06, global_step: 450, interval_runtime: 36.1565, interval_samples_per_second: 0.443, interval_steps_per_second: 0.277, epoch: 5.0562[0m
[32m[2022-09-16 20:05:08,136] [    INFO][0m - loss: 0.55037484, learning_rate: 2.4831460674157305e-06, global_step: 460, interval_runtime: 6.2998, interval_samples_per_second: 2.54, interval_steps_per_second: 1.587, epoch: 5.1685[0m
[32m[2022-09-16 20:05:14,449] [    INFO][0m - loss: 0.4599638, learning_rate: 2.4719101123595505e-06, global_step: 470, interval_runtime: 6.3131, interval_samples_per_second: 2.534, interval_steps_per_second: 1.584, epoch: 5.2809[0m
[32m[2022-09-16 20:05:20,753] [    INFO][0m - loss: 0.41784844, learning_rate: 2.460674157303371e-06, global_step: 480, interval_runtime: 6.3039, interval_samples_per_second: 2.538, interval_steps_per_second: 1.586, epoch: 5.3933[0m
[32m[2022-09-16 20:05:27,072] [    INFO][0m - loss: 0.4917408, learning_rate: 2.449438202247191e-06, global_step: 490, interval_runtime: 6.3187, interval_samples_per_second: 2.532, interval_steps_per_second: 1.583, epoch: 5.5056[0m
[32m[2022-09-16 20:05:33,398] [    INFO][0m - loss: 0.38779674, learning_rate: 2.4382022471910114e-06, global_step: 500, interval_runtime: 6.3262, interval_samples_per_second: 2.529, interval_steps_per_second: 1.581, epoch: 5.618[0m
[32m[2022-09-16 20:05:39,713] [    INFO][0m - loss: 0.39996924, learning_rate: 2.4269662921348315e-06, global_step: 510, interval_runtime: 6.3154, interval_samples_per_second: 2.533, interval_steps_per_second: 1.583, epoch: 5.7303[0m
[32m[2022-09-16 20:05:46,039] [    INFO][0m - loss: 0.40104017, learning_rate: 2.4157303370786515e-06, global_step: 520, interval_runtime: 6.3253, interval_samples_per_second: 2.53, interval_steps_per_second: 1.581, epoch: 5.8427[0m
[32m[2022-09-16 20:05:53,934] [    INFO][0m - loss: 0.36147902, learning_rate: 2.404494382022472e-06, global_step: 530, interval_runtime: 6.2897, interval_samples_per_second: 2.544, interval_steps_per_second: 1.59, epoch: 5.9551[0m
[32m[2022-09-16 20:05:56,015] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 20:05:56,015] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 20:05:56,016] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:05:56,017] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:05:56,017] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 20:06:17,914] [    INFO][0m - eval_loss: 0.41330796480178833, eval_accuracy: 0.8571428571428571, eval_runtime: 21.8987, eval_samples_per_second: 64.57, eval_steps_per_second: 4.064, epoch: 6.0[0m
[32m[2022-09-16 20:06:17,939] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-534[0m
[32m[2022-09-16 20:06:17,939] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:06:20,717] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-534/tokenizer_config.json[0m
[32m[2022-09-16 20:06:20,717] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-534/special_tokens_map.json[0m
[32m[2022-09-16 20:06:29,848] [    INFO][0m - loss: 0.48501081, learning_rate: 2.393258426966292e-06, global_step: 540, interval_runtime: 37.5198, interval_samples_per_second: 0.426, interval_steps_per_second: 0.267, epoch: 6.0674[0m
[32m[2022-09-16 20:06:36,136] [    INFO][0m - loss: 0.44748731, learning_rate: 2.3820224719101125e-06, global_step: 550, interval_runtime: 6.2883, interval_samples_per_second: 2.544, interval_steps_per_second: 1.59, epoch: 6.1798[0m
[32m[2022-09-16 20:06:42,444] [    INFO][0m - loss: 0.41711469, learning_rate: 2.370786516853933e-06, global_step: 560, interval_runtime: 6.3076, interval_samples_per_second: 2.537, interval_steps_per_second: 1.585, epoch: 6.2921[0m
[32m[2022-09-16 20:06:48,761] [    INFO][0m - loss: 0.51949062, learning_rate: 2.359550561797753e-06, global_step: 570, interval_runtime: 6.3171, interval_samples_per_second: 2.533, interval_steps_per_second: 1.583, epoch: 6.4045[0m
[32m[2022-09-16 20:06:55,080] [    INFO][0m - loss: 0.32818775, learning_rate: 2.348314606741573e-06, global_step: 580, interval_runtime: 6.3185, interval_samples_per_second: 2.532, interval_steps_per_second: 1.583, epoch: 6.5169[0m
[32m[2022-09-16 20:07:01,404] [    INFO][0m - loss: 0.38119102, learning_rate: 2.3370786516853934e-06, global_step: 590, interval_runtime: 6.3246, interval_samples_per_second: 2.53, interval_steps_per_second: 1.581, epoch: 6.6292[0m
[32m[2022-09-16 20:07:07,727] [    INFO][0m - loss: 0.36544092, learning_rate: 2.3258426966292135e-06, global_step: 600, interval_runtime: 6.3229, interval_samples_per_second: 2.53, interval_steps_per_second: 1.582, epoch: 6.7416[0m
[32m[2022-09-16 20:07:14,059] [    INFO][0m - loss: 0.49818416, learning_rate: 2.314606741573034e-06, global_step: 610, interval_runtime: 6.3317, interval_samples_per_second: 2.527, interval_steps_per_second: 1.579, epoch: 6.8539[0m
[32m[2022-09-16 20:07:20,324] [    INFO][0m - loss: 0.37698958, learning_rate: 2.303370786516854e-06, global_step: 620, interval_runtime: 6.2656, interval_samples_per_second: 2.554, interval_steps_per_second: 1.596, epoch: 6.9663[0m
[32m[2022-09-16 20:07:21,804] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 20:07:21,804] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 20:07:21,804] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:07:21,804] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:07:21,804] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 20:07:43,823] [    INFO][0m - eval_loss: 0.41130945086479187, eval_accuracy: 0.8571428571428571, eval_runtime: 22.018, eval_samples_per_second: 64.22, eval_steps_per_second: 4.042, epoch: 7.0[0m
[32m[2022-09-16 20:07:43,847] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-623[0m
[32m[2022-09-16 20:07:43,847] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:07:46,968] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-623/tokenizer_config.json[0m
[32m[2022-09-16 20:07:46,969] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-623/special_tokens_map.json[0m
[32m[2022-09-16 20:07:57,300] [    INFO][0m - loss: 0.44036241, learning_rate: 2.292134831460674e-06, global_step: 630, interval_runtime: 36.9757, interval_samples_per_second: 0.433, interval_steps_per_second: 0.27, epoch: 7.0787[0m
[32m[2022-09-16 20:08:03,606] [    INFO][0m - loss: 0.49333801, learning_rate: 2.2808988764044944e-06, global_step: 640, interval_runtime: 6.3058, interval_samples_per_second: 2.537, interval_steps_per_second: 1.586, epoch: 7.191[0m
[32m[2022-09-16 20:08:09,929] [    INFO][0m - loss: 0.40971794, learning_rate: 2.2696629213483145e-06, global_step: 650, interval_runtime: 6.3227, interval_samples_per_second: 2.531, interval_steps_per_second: 1.582, epoch: 7.3034[0m
[32m[2022-09-16 20:08:16,249] [    INFO][0m - loss: 0.39091954, learning_rate: 2.258426966292135e-06, global_step: 660, interval_runtime: 6.3202, interval_samples_per_second: 2.532, interval_steps_per_second: 1.582, epoch: 7.4157[0m
[32m[2022-09-16 20:08:22,625] [    INFO][0m - loss: 0.49003158, learning_rate: 2.2471910112359554e-06, global_step: 670, interval_runtime: 6.3763, interval_samples_per_second: 2.509, interval_steps_per_second: 1.568, epoch: 7.5281[0m
[32m[2022-09-16 20:08:28,959] [    INFO][0m - loss: 0.41418705, learning_rate: 2.2359550561797754e-06, global_step: 680, interval_runtime: 6.3334, interval_samples_per_second: 2.526, interval_steps_per_second: 1.579, epoch: 7.6404[0m
[32m[2022-09-16 20:08:35,320] [    INFO][0m - loss: 0.34896772, learning_rate: 2.224719101123596e-06, global_step: 690, interval_runtime: 6.361, interval_samples_per_second: 2.515, interval_steps_per_second: 1.572, epoch: 7.7528[0m
[32m[2022-09-16 20:08:41,687] [    INFO][0m - loss: 0.39047563, learning_rate: 2.213483146067416e-06, global_step: 700, interval_runtime: 6.3669, interval_samples_per_second: 2.513, interval_steps_per_second: 1.571, epoch: 7.8652[0m
[32m[2022-09-16 20:08:47,961] [    INFO][0m - loss: 0.43119125, learning_rate: 2.202247191011236e-06, global_step: 710, interval_runtime: 6.2746, interval_samples_per_second: 2.55, interval_steps_per_second: 1.594, epoch: 7.9775[0m
[32m[2022-09-16 20:08:48,828] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 20:08:48,828] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 20:08:48,828] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:08:48,828] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:08:48,828] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 20:09:11,000] [    INFO][0m - eval_loss: 0.41129934787750244, eval_accuracy: 0.8571428571428571, eval_runtime: 22.1714, eval_samples_per_second: 63.776, eval_steps_per_second: 4.014, epoch: 8.0[0m
[32m[2022-09-16 20:09:11,024] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-712[0m
[32m[2022-09-16 20:09:11,024] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:09:13,710] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-712/tokenizer_config.json[0m
[32m[2022-09-16 20:09:13,710] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-712/special_tokens_map.json[0m
[32m[2022-09-16 20:09:24,254] [    INFO][0m - loss: 0.45705309, learning_rate: 2.1910112359550564e-06, global_step: 720, interval_runtime: 36.2931, interval_samples_per_second: 0.441, interval_steps_per_second: 0.276, epoch: 8.0899[0m
[32m[2022-09-16 20:09:30,587] [    INFO][0m - loss: 0.48105249, learning_rate: 2.1797752808988764e-06, global_step: 730, interval_runtime: 6.333, interval_samples_per_second: 2.526, interval_steps_per_second: 1.579, epoch: 8.2022[0m
[32m[2022-09-16 20:09:36,916] [    INFO][0m - loss: 0.48036017, learning_rate: 2.1685393258426965e-06, global_step: 740, interval_runtime: 6.329, interval_samples_per_second: 2.528, interval_steps_per_second: 1.58, epoch: 8.3146[0m
[32m[2022-09-16 20:09:43,243] [    INFO][0m - loss: 0.3273886, learning_rate: 2.157303370786517e-06, global_step: 750, interval_runtime: 6.3269, interval_samples_per_second: 2.529, interval_steps_per_second: 1.581, epoch: 8.427[0m
[32m[2022-09-16 20:09:50,103] [    INFO][0m - loss: 0.46689992, learning_rate: 2.146067415730337e-06, global_step: 760, interval_runtime: 6.3289, interval_samples_per_second: 2.528, interval_steps_per_second: 1.58, epoch: 8.5393[0m
[32m[2022-09-16 20:09:56,440] [    INFO][0m - loss: 0.36724586, learning_rate: 2.1348314606741574e-06, global_step: 770, interval_runtime: 6.8678, interval_samples_per_second: 2.33, interval_steps_per_second: 1.456, epoch: 8.6517[0m
[32m[2022-09-16 20:10:02,788] [    INFO][0m - loss: 0.41516795, learning_rate: 2.1235955056179774e-06, global_step: 780, interval_runtime: 6.3474, interval_samples_per_second: 2.521, interval_steps_per_second: 1.575, epoch: 8.764[0m
[32m[2022-09-16 20:10:09,128] [    INFO][0m - loss: 0.43691812, learning_rate: 2.112359550561798e-06, global_step: 790, interval_runtime: 6.3403, interval_samples_per_second: 2.524, interval_steps_per_second: 1.577, epoch: 8.8764[0m
[32m[2022-09-16 20:10:15,369] [    INFO][0m - loss: 0.36880121, learning_rate: 2.1011235955056183e-06, global_step: 800, interval_runtime: 6.2409, interval_samples_per_second: 2.564, interval_steps_per_second: 1.602, epoch: 8.9888[0m
[32m[2022-09-16 20:10:15,623] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 20:10:15,623] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 20:10:15,623] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:10:15,623] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:10:15,623] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 20:10:37,513] [    INFO][0m - eval_loss: 0.4109233617782593, eval_accuracy: 0.8571428571428571, eval_runtime: 21.8893, eval_samples_per_second: 64.598, eval_steps_per_second: 4.066, epoch: 9.0[0m
[32m[2022-09-16 20:10:37,540] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-801[0m
[32m[2022-09-16 20:10:37,540] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:10:40,157] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-801/tokenizer_config.json[0m
[32m[2022-09-16 20:10:40,157] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-801/special_tokens_map.json[0m
[32m[2022-09-16 20:10:51,011] [    INFO][0m - loss: 0.4135859, learning_rate: 2.0898876404494384e-06, global_step: 810, interval_runtime: 35.6427, interval_samples_per_second: 0.449, interval_steps_per_second: 0.281, epoch: 9.1011[0m
[32m[2022-09-16 20:10:58,060] [    INFO][0m - loss: 0.43729944, learning_rate: 2.0786516853932584e-06, global_step: 820, interval_runtime: 6.3065, interval_samples_per_second: 2.537, interval_steps_per_second: 1.586, epoch: 9.2135[0m
[32m[2022-09-16 20:11:04,378] [    INFO][0m - loss: 0.45001078, learning_rate: 2.067415730337079e-06, global_step: 830, interval_runtime: 7.0604, interval_samples_per_second: 2.266, interval_steps_per_second: 1.416, epoch: 9.3258[0m
[32m[2022-09-16 20:11:10,703] [    INFO][0m - loss: 0.36236579, learning_rate: 2.056179775280899e-06, global_step: 840, interval_runtime: 6.3249, interval_samples_per_second: 2.53, interval_steps_per_second: 1.581, epoch: 9.4382[0m
[32m[2022-09-16 20:11:17,029] [    INFO][0m - loss: 0.43091216, learning_rate: 2.0449438202247194e-06, global_step: 850, interval_runtime: 6.3257, interval_samples_per_second: 2.529, interval_steps_per_second: 1.581, epoch: 9.5506[0m
[32m[2022-09-16 20:11:23,346] [    INFO][0m - loss: 0.44021578, learning_rate: 2.0337078651685394e-06, global_step: 860, interval_runtime: 6.3175, interval_samples_per_second: 2.533, interval_steps_per_second: 1.583, epoch: 9.6629[0m
[32m[2022-09-16 20:11:29,671] [    INFO][0m - loss: 0.42847648, learning_rate: 2.0224719101123594e-06, global_step: 870, interval_runtime: 6.324, interval_samples_per_second: 2.53, interval_steps_per_second: 1.581, epoch: 9.7753[0m
[32m[2022-09-16 20:11:35,992] [    INFO][0m - loss: 0.39215031, learning_rate: 2.01123595505618e-06, global_step: 880, interval_runtime: 6.3212, interval_samples_per_second: 2.531, interval_steps_per_second: 1.582, epoch: 9.8876[0m
[32m[2022-09-16 20:11:41,862] [    INFO][0m - loss: 0.42789431, learning_rate: 2e-06, global_step: 890, interval_runtime: 5.8709, interval_samples_per_second: 2.725, interval_steps_per_second: 1.703, epoch: 10.0[0m
[32m[2022-09-16 20:11:41,863] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 20:11:41,863] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 20:11:41,863] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:11:41,863] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:11:41,864] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 20:12:03,749] [    INFO][0m - eval_loss: 0.41268977522850037, eval_accuracy: 0.8571428571428571, eval_runtime: 21.8848, eval_samples_per_second: 64.611, eval_steps_per_second: 4.067, epoch: 10.0[0m
[32m[2022-09-16 20:12:03,768] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-890[0m
[32m[2022-09-16 20:12:03,768] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:12:06,363] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-890/tokenizer_config.json[0m
[32m[2022-09-16 20:12:06,363] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-890/special_tokens_map.json[0m
[32m[2022-09-16 20:12:17,793] [    INFO][0m - loss: 0.50486684, learning_rate: 1.98876404494382e-06, global_step: 900, interval_runtime: 35.931, interval_samples_per_second: 0.445, interval_steps_per_second: 0.278, epoch: 10.1124[0m
[32m[2022-09-16 20:12:24,107] [    INFO][0m - loss: 0.34513459, learning_rate: 1.9775280898876404e-06, global_step: 910, interval_runtime: 6.3133, interval_samples_per_second: 2.534, interval_steps_per_second: 1.584, epoch: 10.2247[0m
[32m[2022-09-16 20:12:30,437] [    INFO][0m - loss: 0.37227983, learning_rate: 1.966292134831461e-06, global_step: 920, interval_runtime: 6.3301, interval_samples_per_second: 2.528, interval_steps_per_second: 1.58, epoch: 10.3371[0m
[32m[2022-09-16 20:12:36,772] [    INFO][0m - loss: 0.5226018, learning_rate: 1.955056179775281e-06, global_step: 930, interval_runtime: 6.3353, interval_samples_per_second: 2.526, interval_steps_per_second: 1.578, epoch: 10.4494[0m
[32m[2022-09-16 20:12:43,092] [    INFO][0m - loss: 0.48854856, learning_rate: 1.9438202247191013e-06, global_step: 940, interval_runtime: 6.3201, interval_samples_per_second: 2.532, interval_steps_per_second: 1.582, epoch: 10.5618[0m
[32m[2022-09-16 20:12:49,421] [    INFO][0m - loss: 0.47107949, learning_rate: 1.9325842696629214e-06, global_step: 950, interval_runtime: 6.3288, interval_samples_per_second: 2.528, interval_steps_per_second: 1.58, epoch: 10.6742[0m
[32m[2022-09-16 20:12:55,753] [    INFO][0m - loss: 0.34514318, learning_rate: 1.921348314606742e-06, global_step: 960, interval_runtime: 6.3322, interval_samples_per_second: 2.527, interval_steps_per_second: 1.579, epoch: 10.7865[0m
[32m[2022-09-16 20:13:02,070] [    INFO][0m - loss: 0.41166425, learning_rate: 1.910112359550562e-06, global_step: 970, interval_runtime: 6.3169, interval_samples_per_second: 2.533, interval_steps_per_second: 1.583, epoch: 10.8989[0m
[32m[2022-09-16 20:13:07,290] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 20:13:07,290] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 20:13:07,290] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:13:07,290] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:13:07,290] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 20:13:29,152] [    INFO][0m - eval_loss: 0.43682661652565, eval_accuracy: 0.8571428571428571, eval_runtime: 21.8611, eval_samples_per_second: 64.681, eval_steps_per_second: 4.071, epoch: 11.0[0m
[32m[2022-09-16 20:13:29,172] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-979[0m
[32m[2022-09-16 20:13:29,172] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:13:31,768] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-979/tokenizer_config.json[0m
[32m[2022-09-16 20:13:31,768] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-979/special_tokens_map.json[0m
[32m[2022-09-16 20:13:37,865] [    INFO][0m - loss: 0.3658653, learning_rate: 1.8988764044943821e-06, global_step: 980, interval_runtime: 35.4584, interval_samples_per_second: 0.451, interval_steps_per_second: 0.282, epoch: 11.0112[0m
[32m[2022-09-16 20:13:44,145] [    INFO][0m - loss: 0.44729581, learning_rate: 1.8876404494382021e-06, global_step: 990, interval_runtime: 6.6165, interval_samples_per_second: 2.418, interval_steps_per_second: 1.511, epoch: 11.1236[0m
[32m[2022-09-16 20:13:50,445] [    INFO][0m - loss: 0.35395536, learning_rate: 1.8764044943820224e-06, global_step: 1000, interval_runtime: 6.3001, interval_samples_per_second: 2.54, interval_steps_per_second: 1.587, epoch: 11.236[0m
[32m[2022-09-16 20:13:56,743] [    INFO][0m - loss: 0.49896374, learning_rate: 1.8651685393258426e-06, global_step: 1010, interval_runtime: 6.2975, interval_samples_per_second: 2.541, interval_steps_per_second: 1.588, epoch: 11.3483[0m
[32m[2022-09-16 20:14:03,049] [    INFO][0m - loss: 0.40575871, learning_rate: 1.8539325842696629e-06, global_step: 1020, interval_runtime: 6.3066, interval_samples_per_second: 2.537, interval_steps_per_second: 1.586, epoch: 11.4607[0m
[32m[2022-09-16 20:14:11,048] [    INFO][0m - loss: 0.38632076, learning_rate: 1.8426966292134831e-06, global_step: 1030, interval_runtime: 6.3161, interval_samples_per_second: 2.533, interval_steps_per_second: 1.583, epoch: 11.573[0m
[32m[2022-09-16 20:14:17,358] [    INFO][0m - loss: 0.44467635, learning_rate: 1.8314606741573036e-06, global_step: 1040, interval_runtime: 7.9918, interval_samples_per_second: 2.002, interval_steps_per_second: 1.251, epoch: 11.6854[0m
[32m[2022-09-16 20:14:23,677] [    INFO][0m - loss: 0.37085207, learning_rate: 1.8202247191011238e-06, global_step: 1050, interval_runtime: 6.3198, interval_samples_per_second: 2.532, interval_steps_per_second: 1.582, epoch: 11.7978[0m
[32m[2022-09-16 20:14:29,984] [    INFO][0m - loss: 0.45677762, learning_rate: 1.8089887640449439e-06, global_step: 1060, interval_runtime: 6.3066, interval_samples_per_second: 2.537, interval_steps_per_second: 1.586, epoch: 11.9101[0m
[32m[2022-09-16 20:14:34,566] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 20:14:34,567] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 20:14:34,567] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:14:34,567] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:14:34,567] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 20:14:56,401] [    INFO][0m - eval_loss: 0.4097752869129181, eval_accuracy: 0.8571428571428571, eval_runtime: 21.8335, eval_samples_per_second: 64.763, eval_steps_per_second: 4.076, epoch: 12.0[0m
[32m[2022-09-16 20:14:56,420] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-1068[0m
[32m[2022-09-16 20:14:56,420] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:14:59,000] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-1068/tokenizer_config.json[0m
[32m[2022-09-16 20:14:59,000] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-1068/special_tokens_map.json[0m
[32m[2022-09-16 20:15:05,347] [    INFO][0m - loss: 0.47142634, learning_rate: 1.797752808988764e-06, global_step: 1070, interval_runtime: 35.3636, interval_samples_per_second: 0.452, interval_steps_per_second: 0.283, epoch: 12.0225[0m
[32m[2022-09-16 20:15:14,933] [    INFO][0m - loss: 0.44479556, learning_rate: 1.7865168539325843e-06, global_step: 1080, interval_runtime: 6.2882, interval_samples_per_second: 2.544, interval_steps_per_second: 1.59, epoch: 12.1348[0m
[32m[2022-09-16 20:15:21,220] [    INFO][0m - loss: 0.32871983, learning_rate: 1.7752808988764046e-06, global_step: 1090, interval_runtime: 9.5844, interval_samples_per_second: 1.669, interval_steps_per_second: 1.043, epoch: 12.2472[0m
[32m[2022-09-16 20:15:27,522] [    INFO][0m - loss: 0.43520989, learning_rate: 1.7640449438202248e-06, global_step: 1100, interval_runtime: 6.3023, interval_samples_per_second: 2.539, interval_steps_per_second: 1.587, epoch: 12.3596[0m
[32m[2022-09-16 20:15:33,852] [    INFO][0m - loss: 0.35878737, learning_rate: 1.7528089887640449e-06, global_step: 1110, interval_runtime: 6.3295, interval_samples_per_second: 2.528, interval_steps_per_second: 1.58, epoch: 12.4719[0m
[32m[2022-09-16 20:15:40,196] [    INFO][0m - loss: 0.36587222, learning_rate: 1.741573033707865e-06, global_step: 1120, interval_runtime: 6.3446, interval_samples_per_second: 2.522, interval_steps_per_second: 1.576, epoch: 12.5843[0m
[32m[2022-09-16 20:15:46,527] [    INFO][0m - loss: 0.48394608, learning_rate: 1.7303370786516853e-06, global_step: 1130, interval_runtime: 6.3302, interval_samples_per_second: 2.528, interval_steps_per_second: 1.58, epoch: 12.6966[0m
[32m[2022-09-16 20:15:52,888] [    INFO][0m - loss: 0.41192856, learning_rate: 1.7191011235955056e-06, global_step: 1140, interval_runtime: 6.3609, interval_samples_per_second: 2.515, interval_steps_per_second: 1.572, epoch: 12.809[0m
[32m[2022-09-16 20:15:59,241] [    INFO][0m - loss: 0.51868606, learning_rate: 1.7078651685393256e-06, global_step: 1150, interval_runtime: 6.3533, interval_samples_per_second: 2.518, interval_steps_per_second: 1.574, epoch: 12.9213[0m
[32m[2022-09-16 20:16:03,209] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 20:16:03,209] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 20:16:03,209] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:16:03,209] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:16:03,209] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 20:16:25,297] [    INFO][0m - eval_loss: 0.4093886911869049, eval_accuracy: 0.8571428571428571, eval_runtime: 22.0874, eval_samples_per_second: 64.018, eval_steps_per_second: 4.029, epoch: 13.0[0m
[32m[2022-09-16 20:16:25,326] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-1157[0m
[32m[2022-09-16 20:16:25,326] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:16:28,197] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-1157/tokenizer_config.json[0m
[32m[2022-09-16 20:16:28,197] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-1157/special_tokens_map.json[0m
[32m[2022-09-16 20:16:35,279] [    INFO][0m - loss: 0.36398265, learning_rate: 1.6966292134831463e-06, global_step: 1160, interval_runtime: 36.038, interval_samples_per_second: 0.444, interval_steps_per_second: 0.277, epoch: 13.0337[0m
[32m[2022-09-16 20:16:41,562] [    INFO][0m - loss: 0.36028037, learning_rate: 1.6853932584269665e-06, global_step: 1170, interval_runtime: 6.2827, interval_samples_per_second: 2.547, interval_steps_per_second: 1.592, epoch: 13.1461[0m
[32m[2022-09-16 20:16:47,881] [    INFO][0m - loss: 0.4116385, learning_rate: 1.6741573033707866e-06, global_step: 1180, interval_runtime: 6.3199, interval_samples_per_second: 2.532, interval_steps_per_second: 1.582, epoch: 13.2584[0m
[32m[2022-09-16 20:16:54,199] [    INFO][0m - loss: 0.37983203, learning_rate: 1.6629213483146068e-06, global_step: 1190, interval_runtime: 6.3172, interval_samples_per_second: 2.533, interval_steps_per_second: 1.583, epoch: 13.3708[0m
[32m[2022-09-16 20:17:00,524] [    INFO][0m - loss: 0.38703594, learning_rate: 1.651685393258427e-06, global_step: 1200, interval_runtime: 6.3258, interval_samples_per_second: 2.529, interval_steps_per_second: 1.581, epoch: 13.4831[0m
[32m[2022-09-16 20:17:06,861] [    INFO][0m - loss: 0.40946727, learning_rate: 1.6404494382022473e-06, global_step: 1210, interval_runtime: 6.336, interval_samples_per_second: 2.525, interval_steps_per_second: 1.578, epoch: 13.5955[0m
[32m[2022-09-16 20:17:13,192] [    INFO][0m - loss: 0.35339437, learning_rate: 1.6292134831460673e-06, global_step: 1220, interval_runtime: 6.3315, interval_samples_per_second: 2.527, interval_steps_per_second: 1.579, epoch: 13.7079[0m
[32m[2022-09-16 20:17:19,514] [    INFO][0m - loss: 0.51343126, learning_rate: 1.6179775280898876e-06, global_step: 1230, interval_runtime: 6.3221, interval_samples_per_second: 2.531, interval_steps_per_second: 1.582, epoch: 13.8202[0m
[32m[2022-09-16 20:17:25,846] [    INFO][0m - loss: 0.48089318, learning_rate: 1.6067415730337078e-06, global_step: 1240, interval_runtime: 6.3319, interval_samples_per_second: 2.527, interval_steps_per_second: 1.579, epoch: 13.9326[0m
[32m[2022-09-16 20:17:29,176] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 20:17:29,177] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 20:17:29,177] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:17:29,177] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:17:29,177] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 20:17:51,155] [    INFO][0m - eval_loss: 0.4198977053165436, eval_accuracy: 0.8571428571428571, eval_runtime: 21.9778, eval_samples_per_second: 64.338, eval_steps_per_second: 4.05, epoch: 14.0[0m
[32m[2022-09-16 20:17:51,181] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-1246[0m
[32m[2022-09-16 20:17:51,182] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:17:53,874] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-1246/tokenizer_config.json[0m
[32m[2022-09-16 20:17:53,874] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-1246/special_tokens_map.json[0m
[32m[2022-09-16 20:18:01,603] [    INFO][0m - loss: 0.45059886, learning_rate: 1.595505617977528e-06, global_step: 1250, interval_runtime: 35.7574, interval_samples_per_second: 0.447, interval_steps_per_second: 0.28, epoch: 14.0449[0m
[32m[2022-09-16 20:18:07,906] [    INFO][0m - loss: 0.3473285, learning_rate: 1.5842696629213483e-06, global_step: 1260, interval_runtime: 6.3026, interval_samples_per_second: 2.539, interval_steps_per_second: 1.587, epoch: 14.1573[0m
[32m[2022-09-16 20:18:14,227] [    INFO][0m - loss: 0.4131546, learning_rate: 1.5730337078651683e-06, global_step: 1270, interval_runtime: 6.3214, interval_samples_per_second: 2.531, interval_steps_per_second: 1.582, epoch: 14.2697[0m
[32m[2022-09-16 20:18:20,544] [    INFO][0m - loss: 0.39568677, learning_rate: 1.561797752808989e-06, global_step: 1280, interval_runtime: 6.3163, interval_samples_per_second: 2.533, interval_steps_per_second: 1.583, epoch: 14.382[0m
[32m[2022-09-16 20:18:26,938] [    INFO][0m - loss: 0.46198359, learning_rate: 1.5505617977528093e-06, global_step: 1290, interval_runtime: 6.3244, interval_samples_per_second: 2.53, interval_steps_per_second: 1.581, epoch: 14.4944[0m
[32m[2022-09-16 20:18:33,839] [    INFO][0m - loss: 0.40814037, learning_rate: 1.5393258426966293e-06, global_step: 1300, interval_runtime: 6.391, interval_samples_per_second: 2.504, interval_steps_per_second: 1.565, epoch: 14.6067[0m
[32m[2022-09-16 20:18:40,163] [    INFO][0m - loss: 0.41556916, learning_rate: 1.5280898876404495e-06, global_step: 1310, interval_runtime: 6.9036, interval_samples_per_second: 2.318, interval_steps_per_second: 1.449, epoch: 14.7191[0m
[32m[2022-09-16 20:18:46,496] [    INFO][0m - loss: 0.47464495, learning_rate: 1.5168539325842698e-06, global_step: 1320, interval_runtime: 6.3338, interval_samples_per_second: 2.526, interval_steps_per_second: 1.579, epoch: 14.8315[0m
[32m[2022-09-16 20:18:52,815] [    INFO][0m - loss: 0.37477188, learning_rate: 1.50561797752809e-06, global_step: 1330, interval_runtime: 6.3183, interval_samples_per_second: 2.532, interval_steps_per_second: 1.583, epoch: 14.9438[0m
[32m[2022-09-16 20:18:55,519] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 20:18:55,519] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 20:18:55,520] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:18:55,520] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:18:55,520] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 20:19:17,404] [    INFO][0m - eval_loss: 0.40741774439811707, eval_accuracy: 0.8571428571428571, eval_runtime: 21.8842, eval_samples_per_second: 64.613, eval_steps_per_second: 4.067, epoch: 15.0[0m
[32m[2022-09-16 20:19:17,430] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-1335[0m
[32m[2022-09-16 20:19:17,431] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:19:20,249] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-1335/tokenizer_config.json[0m
[32m[2022-09-16 20:19:20,250] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-1335/special_tokens_map.json[0m
[32m[2022-09-16 20:19:28,679] [    INFO][0m - loss: 0.43545251, learning_rate: 1.49438202247191e-06, global_step: 1340, interval_runtime: 35.8647, interval_samples_per_second: 0.446, interval_steps_per_second: 0.279, epoch: 15.0562[0m
[32m[2022-09-16 20:19:34,974] [    INFO][0m - loss: 0.42443566, learning_rate: 1.4831460674157303e-06, global_step: 1350, interval_runtime: 6.2948, interval_samples_per_second: 2.542, interval_steps_per_second: 1.589, epoch: 15.1685[0m
[32m[2022-09-16 20:19:41,293] [    INFO][0m - loss: 0.32635791, learning_rate: 1.4719101123595505e-06, global_step: 1360, interval_runtime: 6.3187, interval_samples_per_second: 2.532, interval_steps_per_second: 1.583, epoch: 15.2809[0m
[32m[2022-09-16 20:19:47,614] [    INFO][0m - loss: 0.51858158, learning_rate: 1.460674157303371e-06, global_step: 1370, interval_runtime: 6.3214, interval_samples_per_second: 2.531, interval_steps_per_second: 1.582, epoch: 15.3933[0m
[32m[2022-09-16 20:19:53,933] [    INFO][0m - loss: 0.37431293, learning_rate: 1.449438202247191e-06, global_step: 1380, interval_runtime: 6.3184, interval_samples_per_second: 2.532, interval_steps_per_second: 1.583, epoch: 15.5056[0m
[32m[2022-09-16 20:20:00,259] [    INFO][0m - loss: 0.40731854, learning_rate: 1.4382022471910113e-06, global_step: 1390, interval_runtime: 6.3263, interval_samples_per_second: 2.529, interval_steps_per_second: 1.581, epoch: 15.618[0m
[32m[2022-09-16 20:20:06,584] [    INFO][0m - loss: 0.49022312, learning_rate: 1.4269662921348315e-06, global_step: 1400, interval_runtime: 6.3254, interval_samples_per_second: 2.529, interval_steps_per_second: 1.581, epoch: 15.7303[0m
[32m[2022-09-16 20:20:12,903] [    INFO][0m - loss: 0.3966464, learning_rate: 1.4157303370786518e-06, global_step: 1410, interval_runtime: 6.3187, interval_samples_per_second: 2.532, interval_steps_per_second: 1.583, epoch: 15.8427[0m
[32m[2022-09-16 20:20:19,194] [    INFO][0m - loss: 0.41292953, learning_rate: 1.4044943820224718e-06, global_step: 1420, interval_runtime: 6.2905, interval_samples_per_second: 2.544, interval_steps_per_second: 1.59, epoch: 15.9551[0m
[32m[2022-09-16 20:20:21,282] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 20:20:21,282] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 20:20:21,282] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:20:21,282] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:20:21,282] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 20:20:43,170] [    INFO][0m - eval_loss: 0.40327808260917664, eval_accuracy: 0.8571428571428571, eval_runtime: 21.887, eval_samples_per_second: 64.605, eval_steps_per_second: 4.066, epoch: 16.0[0m
[32m[2022-09-16 20:20:43,194] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-1424[0m
[32m[2022-09-16 20:20:43,194] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:20:45,825] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-1424/tokenizer_config.json[0m
[32m[2022-09-16 20:20:45,825] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-1424/special_tokens_map.json[0m
[32m[2022-09-16 20:20:54,735] [    INFO][0m - loss: 0.36348519, learning_rate: 1.3932584269662923e-06, global_step: 1430, interval_runtime: 35.5411, interval_samples_per_second: 0.45, interval_steps_per_second: 0.281, epoch: 16.0674[0m
[32m[2022-09-16 20:21:02,137] [    INFO][0m - loss: 0.44224243, learning_rate: 1.3820224719101125e-06, global_step: 1440, interval_runtime: 6.2901, interval_samples_per_second: 2.544, interval_steps_per_second: 1.59, epoch: 16.1798[0m
[32m[2022-09-16 20:21:08,442] [    INFO][0m - loss: 0.34543633, learning_rate: 1.3707865168539327e-06, global_step: 1450, interval_runtime: 7.4172, interval_samples_per_second: 2.157, interval_steps_per_second: 1.348, epoch: 16.2921[0m
[32m[2022-09-16 20:21:14,748] [    INFO][0m - loss: 0.4155283, learning_rate: 1.3595505617977528e-06, global_step: 1460, interval_runtime: 6.3057, interval_samples_per_second: 2.537, interval_steps_per_second: 1.586, epoch: 16.4045[0m
[32m[2022-09-16 20:21:21,069] [    INFO][0m - loss: 0.49101071, learning_rate: 1.348314606741573e-06, global_step: 1470, interval_runtime: 6.3216, interval_samples_per_second: 2.531, interval_steps_per_second: 1.582, epoch: 16.5169[0m
[32m[2022-09-16 20:21:27,387] [    INFO][0m - loss: 0.34561913, learning_rate: 1.3370786516853933e-06, global_step: 1480, interval_runtime: 6.3177, interval_samples_per_second: 2.533, interval_steps_per_second: 1.583, epoch: 16.6292[0m
[32m[2022-09-16 20:21:33,704] [    INFO][0m - loss: 0.41809945, learning_rate: 1.3258426966292135e-06, global_step: 1490, interval_runtime: 6.3169, interval_samples_per_second: 2.533, interval_steps_per_second: 1.583, epoch: 16.7416[0m
[32m[2022-09-16 20:21:40,030] [    INFO][0m - loss: 0.36074195, learning_rate: 1.3146067415730338e-06, global_step: 1500, interval_runtime: 6.3259, interval_samples_per_second: 2.529, interval_steps_per_second: 1.581, epoch: 16.8539[0m
[32m[2022-09-16 20:21:46,296] [    INFO][0m - loss: 0.4474534, learning_rate: 1.303370786516854e-06, global_step: 1510, interval_runtime: 6.266, interval_samples_per_second: 2.553, interval_steps_per_second: 1.596, epoch: 16.9663[0m
[32m[2022-09-16 20:21:47,773] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 20:21:47,773] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 20:21:47,773] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:21:47,773] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:21:47,773] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 20:22:09,718] [    INFO][0m - eval_loss: 0.38942357897758484, eval_accuracy: 0.8571428571428571, eval_runtime: 21.9441, eval_samples_per_second: 64.436, eval_steps_per_second: 4.056, epoch: 17.0[0m
[32m[2022-09-16 20:22:09,742] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-1513[0m
[32m[2022-09-16 20:22:09,742] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:22:12,373] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-1513/tokenizer_config.json[0m
[32m[2022-09-16 20:22:12,373] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-1513/special_tokens_map.json[0m
[32m[2022-09-16 20:22:21,970] [    INFO][0m - loss: 0.37095828, learning_rate: 1.2921348314606742e-06, global_step: 1520, interval_runtime: 35.6744, interval_samples_per_second: 0.449, interval_steps_per_second: 0.28, epoch: 17.0787[0m
[32m[2022-09-16 20:22:29,469] [    INFO][0m - loss: 0.40590572, learning_rate: 1.2808988764044945e-06, global_step: 1530, interval_runtime: 6.3019, interval_samples_per_second: 2.539, interval_steps_per_second: 1.587, epoch: 17.191[0m
[32m[2022-09-16 20:22:35,795] [    INFO][0m - loss: 0.51192508, learning_rate: 1.2696629213483145e-06, global_step: 1540, interval_runtime: 7.522, interval_samples_per_second: 2.127, interval_steps_per_second: 1.329, epoch: 17.3034[0m
[32m[2022-09-16 20:22:42,106] [    INFO][0m - loss: 0.42093024, learning_rate: 1.258426966292135e-06, global_step: 1550, interval_runtime: 6.3116, interval_samples_per_second: 2.535, interval_steps_per_second: 1.584, epoch: 17.4157[0m
[32m[2022-09-16 20:22:48,444] [    INFO][0m - loss: 0.38082333, learning_rate: 1.2471910112359552e-06, global_step: 1560, interval_runtime: 6.3382, interval_samples_per_second: 2.524, interval_steps_per_second: 1.578, epoch: 17.5281[0m
[32m[2022-09-16 20:22:54,764] [    INFO][0m - loss: 0.27161572, learning_rate: 1.2359550561797752e-06, global_step: 1570, interval_runtime: 6.3195, interval_samples_per_second: 2.532, interval_steps_per_second: 1.582, epoch: 17.6404[0m
[32m[2022-09-16 20:23:01,078] [    INFO][0m - loss: 0.40389004, learning_rate: 1.2247191011235955e-06, global_step: 1580, interval_runtime: 6.3145, interval_samples_per_second: 2.534, interval_steps_per_second: 1.584, epoch: 17.7528[0m
[32m[2022-09-16 20:23:07,405] [    INFO][0m - loss: 0.26136224, learning_rate: 1.2134831460674157e-06, global_step: 1590, interval_runtime: 6.3272, interval_samples_per_second: 2.529, interval_steps_per_second: 1.58, epoch: 17.8652[0m
[32m[2022-09-16 20:23:13,661] [    INFO][0m - loss: 0.33818965, learning_rate: 1.202247191011236e-06, global_step: 1600, interval_runtime: 6.2556, interval_samples_per_second: 2.558, interval_steps_per_second: 1.599, epoch: 17.9775[0m
[32m[2022-09-16 20:23:14,526] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 20:23:14,527] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 20:23:14,527] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:23:14,527] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:23:14,527] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 20:23:36,419] [    INFO][0m - eval_loss: 0.3419208526611328, eval_accuracy: 0.8571428571428571, eval_runtime: 21.8916, eval_samples_per_second: 64.591, eval_steps_per_second: 4.065, epoch: 18.0[0m
[32m[2022-09-16 20:23:36,443] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-1602[0m
[32m[2022-09-16 20:23:36,443] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:23:39,072] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-1602/tokenizer_config.json[0m
[32m[2022-09-16 20:23:39,072] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-1602/special_tokens_map.json[0m
[32m[2022-09-16 20:23:49,321] [    INFO][0m - loss: 0.38651433, learning_rate: 1.1910112359550562e-06, global_step: 1610, interval_runtime: 35.6599, interval_samples_per_second: 0.449, interval_steps_per_second: 0.28, epoch: 18.0899[0m
[32m[2022-09-16 20:23:57,251] [    INFO][0m - loss: 0.30658941, learning_rate: 1.1797752808988765e-06, global_step: 1620, interval_runtime: 6.3127, interval_samples_per_second: 2.535, interval_steps_per_second: 1.584, epoch: 18.2022[0m
[32m[2022-09-16 20:24:03,565] [    INFO][0m - loss: 0.24332914, learning_rate: 1.1685393258426967e-06, global_step: 1630, interval_runtime: 7.9312, interval_samples_per_second: 2.017, interval_steps_per_second: 1.261, epoch: 18.3146[0m
[32m[2022-09-16 20:24:09,887] [    INFO][0m - loss: 0.34588714, learning_rate: 1.157303370786517e-06, global_step: 1640, interval_runtime: 6.322, interval_samples_per_second: 2.531, interval_steps_per_second: 1.582, epoch: 18.427[0m
[32m[2022-09-16 20:24:16,204] [    INFO][0m - loss: 0.29548702, learning_rate: 1.146067415730337e-06, global_step: 1650, interval_runtime: 6.3172, interval_samples_per_second: 2.533, interval_steps_per_second: 1.583, epoch: 18.5393[0m
[32m[2022-09-16 20:24:22,532] [    INFO][0m - loss: 0.26004963, learning_rate: 1.1348314606741572e-06, global_step: 1660, interval_runtime: 6.3281, interval_samples_per_second: 2.528, interval_steps_per_second: 1.58, epoch: 18.6517[0m
[32m[2022-09-16 20:24:28,852] [    INFO][0m - loss: 0.33821187, learning_rate: 1.1235955056179777e-06, global_step: 1670, interval_runtime: 6.32, interval_samples_per_second: 2.532, interval_steps_per_second: 1.582, epoch: 18.764[0m
[32m[2022-09-16 20:24:35,169] [    INFO][0m - loss: 0.35168612, learning_rate: 1.112359550561798e-06, global_step: 1680, interval_runtime: 6.3169, interval_samples_per_second: 2.533, interval_steps_per_second: 1.583, epoch: 18.8764[0m
[32m[2022-09-16 20:24:41,408] [    INFO][0m - loss: 0.29721835, learning_rate: 1.101123595505618e-06, global_step: 1690, interval_runtime: 6.239, interval_samples_per_second: 2.565, interval_steps_per_second: 1.603, epoch: 18.9888[0m
[32m[2022-09-16 20:24:41,661] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 20:24:41,661] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 20:24:41,661] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:24:41,661] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:24:41,661] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 20:25:03,613] [    INFO][0m - eval_loss: 0.31729990243911743, eval_accuracy: 0.8521923620933521, eval_runtime: 21.9512, eval_samples_per_second: 64.416, eval_steps_per_second: 4.054, epoch: 19.0[0m
[32m[2022-09-16 20:25:03,636] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-1691[0m
[32m[2022-09-16 20:25:03,637] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:25:06,323] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-1691/tokenizer_config.json[0m
[32m[2022-09-16 20:25:06,323] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-1691/special_tokens_map.json[0m
[32m[2022-09-16 20:25:17,167] [    INFO][0m - loss: 0.25041327, learning_rate: 1.0898876404494382e-06, global_step: 1700, interval_runtime: 35.7591, interval_samples_per_second: 0.447, interval_steps_per_second: 0.28, epoch: 19.1011[0m
[32m[2022-09-16 20:25:23,468] [    INFO][0m - loss: 0.19730028, learning_rate: 1.0786516853932585e-06, global_step: 1710, interval_runtime: 6.3013, interval_samples_per_second: 2.539, interval_steps_per_second: 1.587, epoch: 19.2135[0m
[32m[2022-09-16 20:25:29,773] [    INFO][0m - loss: 0.18804172, learning_rate: 1.0674157303370787e-06, global_step: 1720, interval_runtime: 6.3043, interval_samples_per_second: 2.538, interval_steps_per_second: 1.586, epoch: 19.3258[0m
[32m[2022-09-16 20:25:36,085] [    INFO][0m - loss: 0.21564398, learning_rate: 1.056179775280899e-06, global_step: 1730, interval_runtime: 6.3122, interval_samples_per_second: 2.535, interval_steps_per_second: 1.584, epoch: 19.4382[0m
[32m[2022-09-16 20:25:42,409] [    INFO][0m - loss: 0.2604775, learning_rate: 1.0449438202247192e-06, global_step: 1740, interval_runtime: 6.324, interval_samples_per_second: 2.53, interval_steps_per_second: 1.581, epoch: 19.5506[0m
[32m[2022-09-16 20:25:48,739] [    INFO][0m - loss: 0.23878298, learning_rate: 1.0337078651685394e-06, global_step: 1750, interval_runtime: 6.3306, interval_samples_per_second: 2.527, interval_steps_per_second: 1.58, epoch: 19.6629[0m
[32m[2022-09-16 20:25:55,070] [    INFO][0m - loss: 0.17764429, learning_rate: 1.0224719101123597e-06, global_step: 1760, interval_runtime: 6.3304, interval_samples_per_second: 2.527, interval_steps_per_second: 1.58, epoch: 19.7753[0m
[32m[2022-09-16 20:26:01,403] [    INFO][0m - loss: 0.26561923, learning_rate: 1.0112359550561797e-06, global_step: 1770, interval_runtime: 6.3335, interval_samples_per_second: 2.526, interval_steps_per_second: 1.579, epoch: 19.8876[0m
[32m[2022-09-16 20:26:07,769] [    INFO][0m - loss: 0.21230371, learning_rate: 1e-06, global_step: 1780, interval_runtime: 5.8735, interval_samples_per_second: 2.724, interval_steps_per_second: 1.703, epoch: 20.0[0m
[32m[2022-09-16 20:26:07,769] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 20:26:07,770] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 20:26:07,770] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:26:07,770] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:26:07,770] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 20:26:29,673] [    INFO][0m - eval_loss: 0.3372552692890167, eval_accuracy: 0.8451202263083452, eval_runtime: 21.9024, eval_samples_per_second: 64.559, eval_steps_per_second: 4.063, epoch: 20.0[0m
[32m[2022-09-16 20:26:29,699] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-1780[0m
[32m[2022-09-16 20:26:29,700] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:26:32,351] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-1780/tokenizer_config.json[0m
[32m[2022-09-16 20:26:32,351] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-1780/special_tokens_map.json[0m
[32m[2022-09-16 20:26:43,728] [    INFO][0m - loss: 0.19069853, learning_rate: 9.887640449438202e-07, global_step: 1790, interval_runtime: 36.4514, interval_samples_per_second: 0.439, interval_steps_per_second: 0.274, epoch: 20.1124[0m
[32m[2022-09-16 20:26:53,304] [    INFO][0m - loss: 0.15948979, learning_rate: 9.775280898876404e-07, global_step: 1800, interval_runtime: 6.3158, interval_samples_per_second: 2.533, interval_steps_per_second: 1.583, epoch: 20.2247[0m
[32m[2022-09-16 20:26:59,611] [    INFO][0m - loss: 0.19533777, learning_rate: 9.662921348314607e-07, global_step: 1810, interval_runtime: 9.5666, interval_samples_per_second: 1.672, interval_steps_per_second: 1.045, epoch: 20.3371[0m
[32m[2022-09-16 20:27:05,935] [    INFO][0m - loss: 0.22825656, learning_rate: 9.55056179775281e-07, global_step: 1820, interval_runtime: 6.3241, interval_samples_per_second: 2.53, interval_steps_per_second: 1.581, epoch: 20.4494[0m
[32m[2022-09-16 20:27:12,262] [    INFO][0m - loss: 0.21183949, learning_rate: 9.438202247191011e-07, global_step: 1830, interval_runtime: 6.3274, interval_samples_per_second: 2.529, interval_steps_per_second: 1.58, epoch: 20.5618[0m
[32m[2022-09-16 20:27:18,578] [    INFO][0m - loss: 0.07501568, learning_rate: 9.325842696629213e-07, global_step: 1840, interval_runtime: 6.3157, interval_samples_per_second: 2.533, interval_steps_per_second: 1.583, epoch: 20.6742[0m
[32m[2022-09-16 20:27:24,899] [    INFO][0m - loss: 0.18844993, learning_rate: 9.213483146067416e-07, global_step: 1850, interval_runtime: 6.3211, interval_samples_per_second: 2.531, interval_steps_per_second: 1.582, epoch: 20.7865[0m
[32m[2022-09-16 20:27:31,206] [    INFO][0m - loss: 0.13793607, learning_rate: 9.101123595505619e-07, global_step: 1860, interval_runtime: 6.3071, interval_samples_per_second: 2.537, interval_steps_per_second: 1.586, epoch: 20.8989[0m
[32m[2022-09-16 20:27:36,428] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 20:27:36,429] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 20:27:36,429] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:27:36,429] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:27:36,429] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 20:27:58,474] [    INFO][0m - eval_loss: 0.4500066936016083, eval_accuracy: 0.826025459688826, eval_runtime: 22.0444, eval_samples_per_second: 64.143, eval_steps_per_second: 4.037, epoch: 21.0[0m
[32m[2022-09-16 20:27:58,498] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-1869[0m
[32m[2022-09-16 20:27:58,498] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:28:01,140] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-1869/tokenizer_config.json[0m
[32m[2022-09-16 20:28:01,140] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-1869/special_tokens_map.json[0m
[32m[2022-09-16 20:28:06,861] [    INFO][0m - loss: 0.15059869, learning_rate: 8.98876404494382e-07, global_step: 1870, interval_runtime: 35.6549, interval_samples_per_second: 0.449, interval_steps_per_second: 0.28, epoch: 21.0112[0m
[32m[2022-09-16 20:28:13,151] [    INFO][0m - loss: 0.13018101, learning_rate: 8.876404494382023e-07, global_step: 1880, interval_runtime: 6.2897, interval_samples_per_second: 2.544, interval_steps_per_second: 1.59, epoch: 21.1236[0m
[32m[2022-09-16 20:28:19,458] [    INFO][0m - loss: 0.18571218, learning_rate: 8.764044943820224e-07, global_step: 1890, interval_runtime: 6.3071, interval_samples_per_second: 2.537, interval_steps_per_second: 1.586, epoch: 21.236[0m
[32m[2022-09-16 20:28:25,784] [    INFO][0m - loss: 0.12535651, learning_rate: 8.651685393258427e-07, global_step: 1900, interval_runtime: 6.3263, interval_samples_per_second: 2.529, interval_steps_per_second: 1.581, epoch: 21.3483[0m
[32m[2022-09-16 20:28:32,102] [    INFO][0m - loss: 0.17179354, learning_rate: 8.539325842696628e-07, global_step: 1910, interval_runtime: 6.3181, interval_samples_per_second: 2.532, interval_steps_per_second: 1.583, epoch: 21.4607[0m
[32m[2022-09-16 20:28:38,433] [    INFO][0m - loss: 0.2629935, learning_rate: 8.426966292134833e-07, global_step: 1920, interval_runtime: 6.3302, interval_samples_per_second: 2.528, interval_steps_per_second: 1.58, epoch: 21.573[0m
[32m[2022-09-16 20:28:44,764] [    INFO][0m - loss: 0.17054522, learning_rate: 8.314606741573034e-07, global_step: 1930, interval_runtime: 6.3312, interval_samples_per_second: 2.527, interval_steps_per_second: 1.579, epoch: 21.6854[0m
[32m[2022-09-16 20:28:51,091] [    INFO][0m - loss: 0.10430552, learning_rate: 8.202247191011237e-07, global_step: 1940, interval_runtime: 6.3274, interval_samples_per_second: 2.529, interval_steps_per_second: 1.58, epoch: 21.7978[0m
[32m[2022-09-16 20:28:57,407] [    INFO][0m - loss: 0.1472937, learning_rate: 8.089887640449438e-07, global_step: 1950, interval_runtime: 6.3155, interval_samples_per_second: 2.533, interval_steps_per_second: 1.583, epoch: 21.9101[0m
[32m[2022-09-16 20:29:01,996] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 20:29:01,996] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 20:29:01,996] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:29:01,996] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:29:01,996] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 20:29:23,987] [    INFO][0m - eval_loss: 0.4979438781738281, eval_accuracy: 0.8422913719943423, eval_runtime: 21.9901, eval_samples_per_second: 64.302, eval_steps_per_second: 4.047, epoch: 22.0[0m
[32m[2022-09-16 20:29:24,014] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-1958[0m
[32m[2022-09-16 20:29:24,015] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:29:26,928] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-1958/tokenizer_config.json[0m
[32m[2022-09-16 20:29:26,929] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-1958/special_tokens_map.json[0m
[32m[2022-09-16 20:29:33,496] [    INFO][0m - loss: 0.06025683, learning_rate: 7.97752808988764e-07, global_step: 1960, interval_runtime: 36.0888, interval_samples_per_second: 0.443, interval_steps_per_second: 0.277, epoch: 22.0225[0m
[32m[2022-09-16 20:29:39,794] [    INFO][0m - loss: 0.07206604, learning_rate: 7.865168539325842e-07, global_step: 1970, interval_runtime: 6.2982, interval_samples_per_second: 2.54, interval_steps_per_second: 1.588, epoch: 22.1348[0m
[32m[2022-09-16 20:29:46,176] [    INFO][0m - loss: 0.10580449, learning_rate: 7.752808988764046e-07, global_step: 1980, interval_runtime: 6.3822, interval_samples_per_second: 2.507, interval_steps_per_second: 1.567, epoch: 22.2472[0m
[32m[2022-09-16 20:29:52,512] [    INFO][0m - loss: 0.10271922, learning_rate: 7.640449438202248e-07, global_step: 1990, interval_runtime: 6.3358, interval_samples_per_second: 2.525, interval_steps_per_second: 1.578, epoch: 22.3596[0m
[32m[2022-09-16 20:29:58,852] [    INFO][0m - loss: 0.13070219, learning_rate: 7.52808988764045e-07, global_step: 2000, interval_runtime: 6.3404, interval_samples_per_second: 2.523, interval_steps_per_second: 1.577, epoch: 22.4719[0m
[32m[2022-09-16 20:30:05,196] [    INFO][0m - loss: 0.1448849, learning_rate: 7.415730337078651e-07, global_step: 2010, interval_runtime: 6.3441, interval_samples_per_second: 2.522, interval_steps_per_second: 1.576, epoch: 22.5843[0m
[32m[2022-09-16 20:30:11,540] [    INFO][0m - loss: 0.14013935, learning_rate: 7.303370786516855e-07, global_step: 2020, interval_runtime: 6.3435, interval_samples_per_second: 2.522, interval_steps_per_second: 1.576, epoch: 22.6966[0m
[32m[2022-09-16 20:30:17,873] [    INFO][0m - loss: 0.16848423, learning_rate: 7.191011235955056e-07, global_step: 2030, interval_runtime: 6.3334, interval_samples_per_second: 2.526, interval_steps_per_second: 1.579, epoch: 22.809[0m
[32m[2022-09-16 20:30:24,241] [    INFO][0m - loss: 0.07314548, learning_rate: 7.078651685393259e-07, global_step: 2040, interval_runtime: 6.3678, interval_samples_per_second: 2.513, interval_steps_per_second: 1.57, epoch: 22.9213[0m
[32m[2022-09-16 20:30:28,209] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 20:30:28,251] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 20:30:28,251] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:30:28,251] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:30:28,252] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 20:30:50,195] [    INFO][0m - eval_loss: 0.6732755899429321, eval_accuracy: 0.8338048090523338, eval_runtime: 21.986, eval_samples_per_second: 64.314, eval_steps_per_second: 4.048, epoch: 23.0[0m
[32m[2022-09-16 20:30:50,220] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-2047[0m
[32m[2022-09-16 20:30:50,220] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:30:52,923] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-2047/tokenizer_config.json[0m
[32m[2022-09-16 20:30:52,923] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-2047/special_tokens_map.json[0m
[32m[2022-09-16 20:30:59,934] [    INFO][0m - loss: 0.04479423, learning_rate: 6.966292134831461e-07, global_step: 2050, interval_runtime: 35.6928, interval_samples_per_second: 0.448, interval_steps_per_second: 0.28, epoch: 23.0337[0m
[32m[2022-09-16 20:31:06,247] [    INFO][0m - loss: 0.09228508, learning_rate: 6.853932584269664e-07, global_step: 2060, interval_runtime: 6.3124, interval_samples_per_second: 2.535, interval_steps_per_second: 1.584, epoch: 23.1461[0m
[32m[2022-09-16 20:31:12,576] [    INFO][0m - loss: 0.14498904, learning_rate: 6.741573033707865e-07, global_step: 2070, interval_runtime: 6.3293, interval_samples_per_second: 2.528, interval_steps_per_second: 1.58, epoch: 23.2584[0m
[32m[2022-09-16 20:31:18,915] [    INFO][0m - loss: 0.088332, learning_rate: 6.629213483146068e-07, global_step: 2080, interval_runtime: 6.3388, interval_samples_per_second: 2.524, interval_steps_per_second: 1.578, epoch: 23.3708[0m
[32m[2022-09-16 20:31:25,241] [    INFO][0m - loss: 0.04953241, learning_rate: 6.51685393258427e-07, global_step: 2090, interval_runtime: 6.3264, interval_samples_per_second: 2.529, interval_steps_per_second: 1.581, epoch: 23.4831[0m
[32m[2022-09-16 20:31:31,572] [    INFO][0m - loss: 0.26771808, learning_rate: 6.404494382022472e-07, global_step: 2100, interval_runtime: 6.3313, interval_samples_per_second: 2.527, interval_steps_per_second: 1.579, epoch: 23.5955[0m
[32m[2022-09-16 20:31:37,897] [    INFO][0m - loss: 0.08340537, learning_rate: 6.292134831460675e-07, global_step: 2110, interval_runtime: 6.3251, interval_samples_per_second: 2.53, interval_steps_per_second: 1.581, epoch: 23.7079[0m
[32m[2022-09-16 20:31:44,229] [    INFO][0m - loss: 0.12542902, learning_rate: 6.179775280898876e-07, global_step: 2120, interval_runtime: 6.3319, interval_samples_per_second: 2.527, interval_steps_per_second: 1.579, epoch: 23.8202[0m
[32m[2022-09-16 20:31:50,575] [    INFO][0m - loss: 0.10448309, learning_rate: 6.067415730337079e-07, global_step: 2130, interval_runtime: 6.3459, interval_samples_per_second: 2.521, interval_steps_per_second: 1.576, epoch: 23.9326[0m
[32m[2022-09-16 20:31:53,906] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 20:31:53,906] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 20:31:53,906] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:31:53,906] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:31:53,906] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 20:32:16,002] [    INFO][0m - eval_loss: 0.8292134404182434, eval_accuracy: 0.8295615275813296, eval_runtime: 22.096, eval_samples_per_second: 63.993, eval_steps_per_second: 4.028, epoch: 24.0[0m
[32m[2022-09-16 20:32:16,027] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-2136[0m
[32m[2022-09-16 20:32:16,028] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:32:18,754] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-2136/tokenizer_config.json[0m
[32m[2022-09-16 20:32:18,755] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-2136/special_tokens_map.json[0m
[32m[2022-09-16 20:32:26,314] [    INFO][0m - loss: 0.09267887, learning_rate: 5.955056179775281e-07, global_step: 2140, interval_runtime: 35.7384, interval_samples_per_second: 0.448, interval_steps_per_second: 0.28, epoch: 24.0449[0m
[32m[2022-09-16 20:32:32,705] [    INFO][0m - loss: 0.07936391, learning_rate: 5.842696629213484e-07, global_step: 2150, interval_runtime: 6.3912, interval_samples_per_second: 2.503, interval_steps_per_second: 1.565, epoch: 24.1573[0m
[32m[2022-09-16 20:32:39,030] [    INFO][0m - loss: 0.06009909, learning_rate: 5.730337078651685e-07, global_step: 2160, interval_runtime: 6.3254, interval_samples_per_second: 2.529, interval_steps_per_second: 1.581, epoch: 24.2697[0m
[32m[2022-09-16 20:32:45,361] [    INFO][0m - loss: 0.0413307, learning_rate: 5.617977528089888e-07, global_step: 2170, interval_runtime: 6.3312, interval_samples_per_second: 2.527, interval_steps_per_second: 1.579, epoch: 24.382[0m
[32m[2022-09-16 20:32:51,694] [    INFO][0m - loss: 0.03921862, learning_rate: 5.50561797752809e-07, global_step: 2180, interval_runtime: 6.3317, interval_samples_per_second: 2.527, interval_steps_per_second: 1.579, epoch: 24.4944[0m
[32m[2022-09-16 20:32:58,025] [    INFO][0m - loss: 0.11109419, learning_rate: 5.393258426966292e-07, global_step: 2190, interval_runtime: 6.332, interval_samples_per_second: 2.527, interval_steps_per_second: 1.579, epoch: 24.6067[0m
[32m[2022-09-16 20:33:04,357] [    INFO][0m - loss: 0.18711933, learning_rate: 5.280898876404495e-07, global_step: 2200, interval_runtime: 6.3316, interval_samples_per_second: 2.527, interval_steps_per_second: 1.579, epoch: 24.7191[0m
[32m[2022-09-16 20:33:10,694] [    INFO][0m - loss: 0.12230355, learning_rate: 5.168539325842697e-07, global_step: 2210, interval_runtime: 6.3373, interval_samples_per_second: 2.525, interval_steps_per_second: 1.578, epoch: 24.8315[0m
[32m[2022-09-16 20:33:17,012] [    INFO][0m - loss: 0.06805321, learning_rate: 5.056179775280899e-07, global_step: 2220, interval_runtime: 6.3174, interval_samples_per_second: 2.533, interval_steps_per_second: 1.583, epoch: 24.9438[0m
[32m[2022-09-16 20:33:19,723] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 20:33:19,723] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 20:33:19,723] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:33:19,723] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:33:19,723] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 20:33:41,514] [    INFO][0m - eval_loss: 0.9567963480949402, eval_accuracy: 0.8408769448373409, eval_runtime: 21.7904, eval_samples_per_second: 64.891, eval_steps_per_second: 4.084, epoch: 25.0[0m
[32m[2022-09-16 20:33:41,530] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-2225[0m
[32m[2022-09-16 20:33:41,530] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:33:46,351] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-2225/tokenizer_config.json[0m
[32m[2022-09-16 20:33:46,351] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-2225/special_tokens_map.json[0m
[32m[2022-09-16 20:33:54,590] [    INFO][0m - loss: 0.0931289, learning_rate: 4.943820224719101e-07, global_step: 2230, interval_runtime: 37.5784, interval_samples_per_second: 0.426, interval_steps_per_second: 0.266, epoch: 25.0562[0m
[32m[2022-09-16 20:34:00,887] [    INFO][0m - loss: 0.11975739, learning_rate: 4.831460674157303e-07, global_step: 2240, interval_runtime: 6.2974, interval_samples_per_second: 2.541, interval_steps_per_second: 1.588, epoch: 25.1685[0m
[32m[2022-09-16 20:34:07,208] [    INFO][0m - loss: 0.08268047, learning_rate: 4.7191011235955054e-07, global_step: 2250, interval_runtime: 6.3207, interval_samples_per_second: 2.531, interval_steps_per_second: 1.582, epoch: 25.2809[0m
[32m[2022-09-16 20:34:13,539] [    INFO][0m - loss: 0.05864258, learning_rate: 4.606741573033708e-07, global_step: 2260, interval_runtime: 6.3311, interval_samples_per_second: 2.527, interval_steps_per_second: 1.58, epoch: 25.3933[0m
[32m[2022-09-16 20:34:19,946] [    INFO][0m - loss: 0.01858636, learning_rate: 4.49438202247191e-07, global_step: 2270, interval_runtime: 6.3311, interval_samples_per_second: 2.527, interval_steps_per_second: 1.579, epoch: 25.5056[0m
[32m[2022-09-16 20:34:26,283] [    INFO][0m - loss: 0.06761626, learning_rate: 4.382022471910112e-07, global_step: 2280, interval_runtime: 6.4129, interval_samples_per_second: 2.495, interval_steps_per_second: 1.559, epoch: 25.618[0m
[32m[2022-09-16 20:34:32,627] [    INFO][0m - loss: 0.02861085, learning_rate: 4.269662921348314e-07, global_step: 2290, interval_runtime: 6.3438, interval_samples_per_second: 2.522, interval_steps_per_second: 1.576, epoch: 25.7303[0m
[32m[2022-09-16 20:34:38,948] [    INFO][0m - loss: 0.0963988, learning_rate: 4.157303370786517e-07, global_step: 2300, interval_runtime: 6.3215, interval_samples_per_second: 2.531, interval_steps_per_second: 1.582, epoch: 25.8427[0m
[32m[2022-09-16 20:34:45,237] [    INFO][0m - loss: 0.11805967, learning_rate: 4.044943820224719e-07, global_step: 2310, interval_runtime: 6.2888, interval_samples_per_second: 2.544, interval_steps_per_second: 1.59, epoch: 25.9551[0m
[32m[2022-09-16 20:34:47,331] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 20:34:47,331] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 20:34:47,331] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:34:47,331] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:34:47,332] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 20:35:09,304] [    INFO][0m - eval_loss: 1.070768117904663, eval_accuracy: 0.8302687411598303, eval_runtime: 21.972, eval_samples_per_second: 64.355, eval_steps_per_second: 4.051, epoch: 26.0[0m
[32m[2022-09-16 20:35:09,321] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-2314[0m
[32m[2022-09-16 20:35:09,322] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:35:11,903] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-2314/tokenizer_config.json[0m
[32m[2022-09-16 20:35:11,903] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-2314/special_tokens_map.json[0m
[32m[2022-09-16 20:35:21,371] [    INFO][0m - loss: 0.07188959, learning_rate: 3.932584269662921e-07, global_step: 2320, interval_runtime: 36.1338, interval_samples_per_second: 0.443, interval_steps_per_second: 0.277, epoch: 26.0674[0m
[32m[2022-09-16 20:35:27,700] [    INFO][0m - loss: 0.02499629, learning_rate: 3.820224719101124e-07, global_step: 2330, interval_runtime: 6.3289, interval_samples_per_second: 2.528, interval_steps_per_second: 1.58, epoch: 26.1798[0m
[32m[2022-09-16 20:35:34,055] [    INFO][0m - loss: 0.09628804, learning_rate: 3.707865168539326e-07, global_step: 2340, interval_runtime: 6.3549, interval_samples_per_second: 2.518, interval_steps_per_second: 1.574, epoch: 26.2921[0m
[32m[2022-09-16 20:35:40,394] [    INFO][0m - loss: 0.08736165, learning_rate: 3.595505617977528e-07, global_step: 2350, interval_runtime: 6.3389, interval_samples_per_second: 2.524, interval_steps_per_second: 1.578, epoch: 26.4045[0m
[32m[2022-09-16 20:35:46,732] [    INFO][0m - loss: 0.19891925, learning_rate: 3.4831460674157306e-07, global_step: 2360, interval_runtime: 6.3385, interval_samples_per_second: 2.524, interval_steps_per_second: 1.578, epoch: 26.5169[0m
[32m[2022-09-16 20:35:53,076] [    INFO][0m - loss: 0.03371513, learning_rate: 3.3707865168539325e-07, global_step: 2370, interval_runtime: 6.3437, interval_samples_per_second: 2.522, interval_steps_per_second: 1.576, epoch: 26.6292[0m
[32m[2022-09-16 20:35:59,404] [    INFO][0m - loss: 0.03899823, learning_rate: 3.258426966292135e-07, global_step: 2380, interval_runtime: 6.3279, interval_samples_per_second: 2.528, interval_steps_per_second: 1.58, epoch: 26.7416[0m
[32m[2022-09-16 20:36:05,742] [    INFO][0m - loss: 0.09316935, learning_rate: 3.1460674157303374e-07, global_step: 2390, interval_runtime: 6.3386, interval_samples_per_second: 2.524, interval_steps_per_second: 1.578, epoch: 26.8539[0m
[32m[2022-09-16 20:36:12,028] [    INFO][0m - loss: 0.11175171, learning_rate: 3.0337078651685393e-07, global_step: 2400, interval_runtime: 6.2851, interval_samples_per_second: 2.546, interval_steps_per_second: 1.591, epoch: 26.9663[0m
[32m[2022-09-16 20:36:13,510] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 20:36:13,510] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 20:36:13,510] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:36:13,510] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:36:13,510] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 20:36:35,310] [    INFO][0m - eval_loss: 1.0056240558624268, eval_accuracy: 0.8507779349363508, eval_runtime: 21.7997, eval_samples_per_second: 64.863, eval_steps_per_second: 4.083, epoch: 27.0[0m
[32m[2022-09-16 20:36:35,326] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-2403[0m
[32m[2022-09-16 20:36:35,327] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:36:37,810] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-2403/tokenizer_config.json[0m
[32m[2022-09-16 20:36:37,811] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-2403/special_tokens_map.json[0m
[32m[2022-09-16 20:36:47,192] [    INFO][0m - loss: 0.13486474, learning_rate: 2.921348314606742e-07, global_step: 2410, interval_runtime: 35.1647, interval_samples_per_second: 0.455, interval_steps_per_second: 0.284, epoch: 27.0787[0m
[32m[2022-09-16 20:36:53,990] [    INFO][0m - loss: 0.04939273, learning_rate: 2.808988764044944e-07, global_step: 2420, interval_runtime: 6.3243, interval_samples_per_second: 2.53, interval_steps_per_second: 1.581, epoch: 27.191[0m
[32m[2022-09-16 20:37:00,308] [    INFO][0m - loss: 0.10631518, learning_rate: 2.696629213483146e-07, global_step: 2430, interval_runtime: 6.7911, interval_samples_per_second: 2.356, interval_steps_per_second: 1.473, epoch: 27.3034[0m
[32m[2022-09-16 20:37:06,624] [    INFO][0m - loss: 0.08154896, learning_rate: 2.5842696629213486e-07, global_step: 2440, interval_runtime: 6.3169, interval_samples_per_second: 2.533, interval_steps_per_second: 1.583, epoch: 27.4157[0m
[32m[2022-09-16 20:37:12,964] [    INFO][0m - loss: 0.03124433, learning_rate: 2.4719101123595505e-07, global_step: 2450, interval_runtime: 6.339, interval_samples_per_second: 2.524, interval_steps_per_second: 1.578, epoch: 27.5281[0m
[32m[2022-09-16 20:37:19,298] [    INFO][0m - loss: 0.11787874, learning_rate: 2.3595505617977527e-07, global_step: 2460, interval_runtime: 6.3343, interval_samples_per_second: 2.526, interval_steps_per_second: 1.579, epoch: 27.6404[0m
[32m[2022-09-16 20:37:25,622] [    INFO][0m - loss: 0.09075648, learning_rate: 2.247191011235955e-07, global_step: 2470, interval_runtime: 6.3238, interval_samples_per_second: 2.53, interval_steps_per_second: 1.581, epoch: 27.7528[0m
[32m[2022-09-16 20:37:31,947] [    INFO][0m - loss: 0.07461904, learning_rate: 2.134831460674157e-07, global_step: 2480, interval_runtime: 6.3259, interval_samples_per_second: 2.529, interval_steps_per_second: 1.581, epoch: 27.8652[0m
[32m[2022-09-16 20:37:38,206] [    INFO][0m - loss: 0.15471932, learning_rate: 2.0224719101123595e-07, global_step: 2490, interval_runtime: 6.259, interval_samples_per_second: 2.556, interval_steps_per_second: 1.598, epoch: 27.9775[0m
[32m[2022-09-16 20:37:39,074] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 20:37:39,075] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 20:37:39,075] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:37:39,075] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:37:39,075] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 20:38:00,939] [    INFO][0m - eval_loss: 1.0532019138336182, eval_accuracy: 0.847949080622348, eval_runtime: 21.864, eval_samples_per_second: 64.673, eval_steps_per_second: 4.071, epoch: 28.0[0m
[32m[2022-09-16 20:38:00,954] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-2492[0m
[32m[2022-09-16 20:38:00,954] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:38:03,526] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-2492/tokenizer_config.json[0m
[32m[2022-09-16 20:38:03,526] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-2492/special_tokens_map.json[0m
[32m[2022-09-16 20:38:13,437] [    INFO][0m - loss: 0.04282427, learning_rate: 1.910112359550562e-07, global_step: 2500, interval_runtime: 35.2307, interval_samples_per_second: 0.454, interval_steps_per_second: 0.284, epoch: 28.0899[0m
[32m[2022-09-16 20:38:19,752] [    INFO][0m - loss: 0.01600454, learning_rate: 1.797752808988764e-07, global_step: 2510, interval_runtime: 6.3149, interval_samples_per_second: 2.534, interval_steps_per_second: 1.584, epoch: 28.2022[0m
[32m[2022-09-16 20:38:26,067] [    INFO][0m - loss: 0.03163088, learning_rate: 1.6853932584269663e-07, global_step: 2520, interval_runtime: 6.3142, interval_samples_per_second: 2.534, interval_steps_per_second: 1.584, epoch: 28.3146[0m
[32m[2022-09-16 20:38:32,387] [    INFO][0m - loss: 0.14955623, learning_rate: 1.5730337078651687e-07, global_step: 2530, interval_runtime: 6.3204, interval_samples_per_second: 2.531, interval_steps_per_second: 1.582, epoch: 28.427[0m
[32m[2022-09-16 20:38:38,722] [    INFO][0m - loss: 0.05911102, learning_rate: 1.460674157303371e-07, global_step: 2540, interval_runtime: 6.3353, interval_samples_per_second: 2.526, interval_steps_per_second: 1.578, epoch: 28.5393[0m
[32m[2022-09-16 20:38:45,050] [    INFO][0m - loss: 0.01776612, learning_rate: 1.348314606741573e-07, global_step: 2550, interval_runtime: 6.3278, interval_samples_per_second: 2.529, interval_steps_per_second: 1.58, epoch: 28.6517[0m
[32m[2022-09-16 20:38:51,382] [    INFO][0m - loss: 0.12159001, learning_rate: 1.2359550561797752e-07, global_step: 2560, interval_runtime: 6.3322, interval_samples_per_second: 2.527, interval_steps_per_second: 1.579, epoch: 28.764[0m
[32m[2022-09-16 20:38:57,714] [    INFO][0m - loss: 0.15182418, learning_rate: 1.1235955056179776e-07, global_step: 2570, interval_runtime: 6.3324, interval_samples_per_second: 2.527, interval_steps_per_second: 1.579, epoch: 28.8764[0m
[32m[2022-09-16 20:39:03,976] [    INFO][0m - loss: 0.19170873, learning_rate: 1.0112359550561797e-07, global_step: 2580, interval_runtime: 6.262, interval_samples_per_second: 2.555, interval_steps_per_second: 1.597, epoch: 28.9888[0m
[32m[2022-09-16 20:39:04,227] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 20:39:04,227] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 20:39:04,227] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:39:04,227] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:39:04,227] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 20:39:26,049] [    INFO][0m - eval_loss: 1.093043327331543, eval_accuracy: 0.8444130127298444, eval_runtime: 21.821, eval_samples_per_second: 64.8, eval_steps_per_second: 4.079, epoch: 29.0[0m
[32m[2022-09-16 20:39:26,066] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-2581[0m
[32m[2022-09-16 20:39:26,066] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:39:28,542] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-2581/tokenizer_config.json[0m
[32m[2022-09-16 20:39:28,543] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-2581/special_tokens_map.json[0m
[32m[2022-09-16 20:39:39,062] [    INFO][0m - loss: 0.01327199, learning_rate: 8.98876404494382e-08, global_step: 2590, interval_runtime: 35.0849, interval_samples_per_second: 0.456, interval_steps_per_second: 0.285, epoch: 29.1011[0m
[32m[2022-09-16 20:39:45,374] [    INFO][0m - loss: 0.06549857, learning_rate: 7.865168539325844e-08, global_step: 2600, interval_runtime: 6.3125, interval_samples_per_second: 2.535, interval_steps_per_second: 1.584, epoch: 29.2135[0m
[32m[2022-09-16 20:39:51,684] [    INFO][0m - loss: 0.06197009, learning_rate: 6.741573033707865e-08, global_step: 2610, interval_runtime: 6.3104, interval_samples_per_second: 2.535, interval_steps_per_second: 1.585, epoch: 29.3258[0m
[32m[2022-09-16 20:39:58,005] [    INFO][0m - loss: 0.03558105, learning_rate: 5.617977528089888e-08, global_step: 2620, interval_runtime: 6.3203, interval_samples_per_second: 2.532, interval_steps_per_second: 1.582, epoch: 29.4382[0m
[32m[2022-09-16 20:40:04,681] [    INFO][0m - loss: 0.13052217, learning_rate: 4.49438202247191e-08, global_step: 2630, interval_runtime: 6.3322, interval_samples_per_second: 2.527, interval_steps_per_second: 1.579, epoch: 29.5506[0m
[32m[2022-09-16 20:40:11,005] [    INFO][0m - loss: 0.17579229, learning_rate: 3.370786516853933e-08, global_step: 2640, interval_runtime: 6.668, interval_samples_per_second: 2.4, interval_steps_per_second: 1.5, epoch: 29.6629[0m
[32m[2022-09-16 20:40:17,557] [    INFO][0m - loss: 0.06474515, learning_rate: 2.247191011235955e-08, global_step: 2650, interval_runtime: 6.3232, interval_samples_per_second: 2.53, interval_steps_per_second: 1.581, epoch: 29.7753[0m
[32m[2022-09-16 20:40:23,881] [    INFO][0m - loss: 0.04886622, learning_rate: 1.1235955056179776e-08, global_step: 2660, interval_runtime: 6.5528, interval_samples_per_second: 2.442, interval_steps_per_second: 1.526, epoch: 29.8876[0m
[32m[2022-09-16 20:40:29,751] [    INFO][0m - loss: 0.06060522, learning_rate: 0.0, global_step: 2670, interval_runtime: 5.8699, interval_samples_per_second: 2.726, interval_steps_per_second: 1.704, epoch: 30.0[0m
[32m[2022-09-16 20:40:29,751] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 20:40:29,751] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 20:40:29,752] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:40:29,752] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:40:29,752] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 20:40:51,542] [    INFO][0m - eval_loss: 1.107588291168213, eval_accuracy: 0.842998585572843, eval_runtime: 21.7896, eval_samples_per_second: 64.893, eval_steps_per_second: 4.085, epoch: 30.0[0m
[32m[2022-09-16 20:40:51,560] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-2670[0m
[32m[2022-09-16 20:40:51,560] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:40:54,358] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-2670/tokenizer_config.json[0m
[32m[2022-09-16 20:40:54,359] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-2670/special_tokens_map.json[0m
[32m[2022-09-16 20:41:03,279] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-16 20:41:03,279] [    INFO][0m - Loading best model from ./checkpoints_chid/checkpoint-89 (score: 0.8571428571428571).[0m
[32m[2022-09-16 20:41:04,748] [    INFO][0m - train_runtime: 2599.106, train_samples_per_second: 16.321, train_steps_per_second: 1.027, train_loss: 0.33252270973465414, epoch: 30.0[0m
[32m[2022-09-16 20:41:04,784] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/[0m
[32m[2022-09-16 20:41:04,785] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:41:07,129] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/tokenizer_config.json[0m
[32m[2022-09-16 20:41:07,133] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/special_tokens_map.json[0m
[32m[2022-09-16 20:41:07,134] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-16 20:41:07,135] [    INFO][0m -   epoch                    =       30.0[0m
[32m[2022-09-16 20:41:07,135] [    INFO][0m -   train_loss               =     0.3325[0m
[32m[2022-09-16 20:41:07,135] [    INFO][0m -   train_runtime            = 0:43:19.10[0m
[32m[2022-09-16 20:41:07,135] [    INFO][0m -   train_samples_per_second =     16.321[0m
[32m[2022-09-16 20:41:07,135] [    INFO][0m -   train_steps_per_second   =      1.027[0m
[32m[2022-09-16 20:41:07,146] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-16 20:41:07,146] [    INFO][0m -   Num examples = 14014[0m
[32m[2022-09-16 20:41:07,146] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:41:07,146] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:41:07,146] [    INFO][0m -   Total prediction steps = 876[0m
[32m[2022-09-16 20:44:54,744] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-16 20:44:54,745] [    INFO][0m -   test_accuracy           =     0.8571[0m
[32m[2022-09-16 20:44:54,745] [    INFO][0m -   test_loss               =       0.42[0m
[32m[2022-09-16 20:44:54,745] [    INFO][0m -   test_runtime            = 0:03:47.59[0m
[32m[2022-09-16 20:44:54,746] [    INFO][0m -   test_samples_per_second =     61.573[0m
[32m[2022-09-16 20:44:54,746] [    INFO][0m -   test_steps_per_second   =      3.849[0m
[32m[2022-09-16 20:44:54,746] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-16 20:44:54,746] [    INFO][0m -   Num examples = 14000[0m
[32m[2022-09-16 20:44:54,746] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:44:54,746] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:44:54,746] [    INFO][0m -   Total prediction steps = 875[0m
[32m[2022-09-16 20:48:54,729] [    INFO][0m - Predictions for chidf saved to ./fewclue_submit_examples.[0m
{
  "labels": 0,
  "text_a": "\u5927\u90e8\u5206\u7684\u5927\u529f\u544a\u6210\u90fd\u662f\u7531\u65e0\u6570\u4e2a\u9020\u8c23\u751f\u4e8b\u7d2f\u79ef\u800c\u6765,\u505a\u597d\u6bcf\u4e00\u4ef6\u5e94\u8be5\u505a\u597d\u7684\u5c0f\u4e8b\u7edd\u4e0d\u662f\u4e00\u4ef6\u5c0f\u4e8b\u3002 \u300a\u4f60\u597d\u674e\u7115\u82f1\u300b\u4e3b\u8981\u8bb2\u4e86\u8d3e\u73b2\u610f\u5916\u7a7f\u8d8a\u5230\u4e86\u6bcd\u4eb2\u5e74\u8f7b\u7684\u65f6\u5019,\u548c\u6bcd\u4eb2\u4e00\u8d77\u611f\u53d7\u4e86\u5979\u7684\u751f\u6d3b\u548c\u60c5\u611f\u3002\u8fd9\u6545\u4e8b\u662f\u6839\u636e\u8d3e\u73b2\u7684\u4eb2\u751f\u7ecf\u5386\u6539\u7f16\u800c\u6765, ...",
  "text_b": "",
  "uid": 24
}

 
==========
csl
==========
 
[32m[2022-09-16 20:49:20,585] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-16 20:49:20,586] [    INFO][0m - ============================================================[0m
[32m[2022-09-16 20:49:20,586] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-16 20:49:20,586] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-16 20:49:20,586] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-16 20:49:20,586] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-16 20:49:20,586] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-16 20:49:20,586] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-16 20:49:20,586] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-16 20:49:20,586] [    INFO][0m - [0m
[32m[2022-09-16 20:49:20,586] [    INFO][0m - ============================================================[0m
[32m[2022-09-16 20:49:20,587] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-16 20:49:20,587] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-16 20:49:20,587] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-16 20:49:20,587] [    INFO][0m - pretrained                    :../checkpoints_cmnli/checkpoint-22000/model_state.pdparams[0m
[32m[2022-09-16 20:49:20,587] [    INFO][0m - prompt                        :‚Äú{'text':'text_a'}‚ÄùÊú¨ÊñáÁöÑÂÜÖÂÆπ{'mask'}{'mask'}‚Äú{'text':'text_b'}‚Äù[0m
[32m[2022-09-16 20:49:20,587] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-16 20:49:20,587] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-16 20:49:20,587] [    INFO][0m - task_name                     :csl[0m
[32m[2022-09-16 20:49:20,587] [    INFO][0m - [0m
[32m[2022-09-16 20:49:20,587] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0916 20:49:20.589617 27309 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0916 20:49:20.593801 27309 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-16 20:49:26,557] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-16 20:49:26,570] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-16 20:49:26,570] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-16 20:49:26,571] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùÊú¨ÊñáÁöÑÂÜÖÂÆπ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': '‚Äù'}][0m
[32m[2022-09-16 20:49:28,530] [    INFO][0m - ============================================================[0m
[32m[2022-09-16 20:49:28,530] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-16 20:49:28,530] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-16 20:49:28,530] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-16 20:49:28,530] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-16 20:49:28,530] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-16 20:49:28,530] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-16 20:49:28,531] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-16 20:49:28,531] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-16 20:49:28,531] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-16 20:49:28,531] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-16 20:49:28,531] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-16 20:49:28,531] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-16 20:49:28,531] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-16 20:49:28,531] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-16 20:49:28,531] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-16 20:49:28,531] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-16 20:49:28,531] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-16 20:49:28,531] [    INFO][0m - eval_batch_size               :16[0m
[32m[2022-09-16 20:49:28,531] [    INFO][0m - eval_steps                    :200[0m
[32m[2022-09-16 20:49:28,532] [    INFO][0m - evaluation_strategy           :IntervalStrategy.EPOCH[0m
[32m[2022-09-16 20:49:28,532] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-16 20:49:28,532] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-16 20:49:28,532] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-16 20:49:28,532] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-16 20:49:28,532] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-16 20:49:28,532] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-16 20:49:28,532] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-16 20:49:28,532] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-16 20:49:28,532] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-16 20:49:28,532] [    INFO][0m - learning_rate                 :3e-06[0m
[32m[2022-09-16 20:49:28,532] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-16 20:49:28,532] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-16 20:49:28,532] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-16 20:49:28,533] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-16 20:49:28,533] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-16 20:49:28,533] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-16 20:49:28,533] [    INFO][0m - logging_dir                   :./checkpoints_csl/runs/Sep16_20-49-20_instance-3bwob41y-01[0m
[32m[2022-09-16 20:49:28,533] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-16 20:49:28,533] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-16 20:49:28,533] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-16 20:49:28,533] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-16 20:49:28,533] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-16 20:49:28,533] [    INFO][0m - max_seq_length                :320[0m
[32m[2022-09-16 20:49:28,533] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-16 20:49:28,533] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-16 20:49:28,533] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-16 20:49:28,534] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-16 20:49:28,534] [    INFO][0m - num_train_epochs              :30.0[0m
[32m[2022-09-16 20:49:28,534] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-16 20:49:28,534] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-16 20:49:28,534] [    INFO][0m - output_dir                    :./checkpoints_csl/[0m
[32m[2022-09-16 20:49:28,534] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-16 20:49:28,534] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-16 20:49:28,534] [    INFO][0m - per_device_eval_batch_size    :16[0m
[32m[2022-09-16 20:49:28,534] [    INFO][0m - per_device_train_batch_size   :16[0m
[32m[2022-09-16 20:49:28,534] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-16 20:49:28,534] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-16 20:49:28,534] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-16 20:49:28,534] [    INFO][0m - ppt_learning_rate             :3e-05[0m
[32m[2022-09-16 20:49:28,535] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-16 20:49:28,535] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-16 20:49:28,535] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-16 20:49:28,535] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-16 20:49:28,535] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-16 20:49:28,535] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-16 20:49:28,535] [    INFO][0m - run_name                      :./checkpoints_csl/[0m
[32m[2022-09-16 20:49:28,535] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-16 20:49:28,535] [    INFO][0m - save_steps                    :200[0m
[32m[2022-09-16 20:49:28,535] [    INFO][0m - save_strategy                 :IntervalStrategy.EPOCH[0m
[32m[2022-09-16 20:49:28,535] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-16 20:49:28,535] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-16 20:49:28,535] [    INFO][0m - seed                          :42[0m
[32m[2022-09-16 20:49:28,535] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-16 20:49:28,536] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-16 20:49:28,536] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-16 20:49:28,536] [    INFO][0m - train_batch_size              :16[0m
[32m[2022-09-16 20:49:28,536] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-16 20:49:28,536] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-16 20:49:28,536] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-16 20:49:28,536] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-16 20:49:28,536] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-16 20:49:28,536] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-16 20:49:28,536] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-16 20:49:28,536] [    INFO][0m - [0m
[32m[2022-09-16 20:49:28,539] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-16 20:49:28,539] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 20:49:28,539] [    INFO][0m -   Num Epochs = 30[0m
[32m[2022-09-16 20:49:28,539] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2022-09-16 20:49:28,539] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2022-09-16 20:49:28,539] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-16 20:49:28,539] [    INFO][0m -   Total optimization steps = 300.0[0m
[32m[2022-09-16 20:49:28,540] [    INFO][0m -   Total num train samples = 4800[0m
[33m[2022-09-16 20:49:28,710] [ WARNING][0m - Token indices sequence length is longer than the specified maximum sequence length for this model (608 > 512). Running this sequence through the model will result in indexing errors[0m
[32m[2022-09-16 20:49:37,154] [    INFO][0m - loss: 5.48786125, learning_rate: 2.9e-06, global_step: 10, interval_runtime: 8.6131, interval_samples_per_second: 1.858, interval_steps_per_second: 1.161, epoch: 1.0[0m
[32m[2022-09-16 20:49:37,157] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 20:49:37,157] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 20:49:37,157] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:49:37,157] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:49:37,157] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 20:49:40,323] [    INFO][0m - eval_loss: 3.3747596740722656, eval_accuracy: 0.50625, eval_runtime: 3.1655, eval_samples_per_second: 50.545, eval_steps_per_second: 3.159, epoch: 1.0[0m
[32m[2022-09-16 20:49:40,324] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-10[0m
[32m[2022-09-16 20:49:40,324] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:49:43,133] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-10/tokenizer_config.json[0m
[32m[2022-09-16 20:49:43,134] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-10/special_tokens_map.json[0m
[32m[2022-09-16 20:49:56,342] [    INFO][0m - loss: 2.66026058, learning_rate: 2.8000000000000003e-06, global_step: 20, interval_runtime: 19.1885, interval_samples_per_second: 0.834, interval_steps_per_second: 0.521, epoch: 2.0[0m
[32m[2022-09-16 20:49:56,343] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 20:49:56,344] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 20:49:56,344] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:49:56,344] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:49:56,344] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 20:49:59,489] [    INFO][0m - eval_loss: 1.4543492794036865, eval_accuracy: 0.575, eval_runtime: 3.1447, eval_samples_per_second: 50.879, eval_steps_per_second: 3.18, epoch: 2.0[0m
[32m[2022-09-16 20:49:59,490] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-20[0m
[32m[2022-09-16 20:49:59,490] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:50:02,095] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-20/tokenizer_config.json[0m
[32m[2022-09-16 20:50:02,095] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-20/special_tokens_map.json[0m
[32m[2022-09-16 20:50:15,137] [    INFO][0m - loss: 1.13012657, learning_rate: 2.7e-06, global_step: 30, interval_runtime: 18.7953, interval_samples_per_second: 0.851, interval_steps_per_second: 0.532, epoch: 3.0[0m
[32m[2022-09-16 20:50:15,139] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 20:50:15,139] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 20:50:15,139] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:50:15,139] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:50:15,139] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 20:50:18,304] [    INFO][0m - eval_loss: 0.7306946516036987, eval_accuracy: 0.5625, eval_runtime: 3.1648, eval_samples_per_second: 50.556, eval_steps_per_second: 3.16, epoch: 3.0[0m
[32m[2022-09-16 20:50:18,304] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-30[0m
[32m[2022-09-16 20:50:18,305] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:50:20,867] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-30/tokenizer_config.json[0m
[32m[2022-09-16 20:50:20,868] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-30/special_tokens_map.json[0m
[32m[2022-09-16 20:50:33,607] [    INFO][0m - loss: 0.75738521, learning_rate: 2.6e-06, global_step: 40, interval_runtime: 18.4701, interval_samples_per_second: 0.866, interval_steps_per_second: 0.541, epoch: 4.0[0m
[32m[2022-09-16 20:50:33,608] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 20:50:33,608] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 20:50:33,608] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:50:33,608] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:50:33,608] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 20:50:36,782] [    INFO][0m - eval_loss: 0.6655173897743225, eval_accuracy: 0.65, eval_runtime: 3.1732, eval_samples_per_second: 50.423, eval_steps_per_second: 3.151, epoch: 4.0[0m
[32m[2022-09-16 20:50:36,782] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-40[0m
[32m[2022-09-16 20:50:36,782] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:50:39,331] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-40/tokenizer_config.json[0m
[32m[2022-09-16 20:50:39,332] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-40/special_tokens_map.json[0m
[32m[2022-09-16 20:50:55,617] [    INFO][0m - loss: 0.67534976, learning_rate: 2.5e-06, global_step: 50, interval_runtime: 21.4233, interval_samples_per_second: 0.747, interval_steps_per_second: 0.467, epoch: 5.0[0m
[32m[2022-09-16 20:50:55,618] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 20:50:55,618] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 20:50:55,618] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:50:55,618] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:50:55,618] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 20:50:58,736] [    INFO][0m - eval_loss: 0.6384544968605042, eval_accuracy: 0.63125, eval_runtime: 3.1173, eval_samples_per_second: 51.327, eval_steps_per_second: 3.208, epoch: 5.0[0m
[32m[2022-09-16 20:50:58,736] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-50[0m
[32m[2022-09-16 20:50:58,736] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:51:01,177] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-50/tokenizer_config.json[0m
[32m[2022-09-16 20:51:01,178] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-50/special_tokens_map.json[0m
[32m[2022-09-16 20:51:14,199] [    INFO][0m - loss: 0.60207348, learning_rate: 2.4000000000000003e-06, global_step: 60, interval_runtime: 19.168, interval_samples_per_second: 0.835, interval_steps_per_second: 0.522, epoch: 6.0[0m
[32m[2022-09-16 20:51:14,200] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 20:51:14,200] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 20:51:14,200] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:51:14,200] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:51:14,200] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 20:51:17,339] [    INFO][0m - eval_loss: 0.7010308504104614, eval_accuracy: 0.56875, eval_runtime: 3.1389, eval_samples_per_second: 50.974, eval_steps_per_second: 3.186, epoch: 6.0[0m
[32m[2022-09-16 20:51:17,340] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-60[0m
[32m[2022-09-16 20:51:17,340] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:51:19,831] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-60/tokenizer_config.json[0m
[32m[2022-09-16 20:51:19,831] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-60/special_tokens_map.json[0m
[32m[2022-09-16 20:51:32,616] [    INFO][0m - loss: 0.55858784, learning_rate: 2.3000000000000004e-06, global_step: 70, interval_runtime: 18.4173, interval_samples_per_second: 0.869, interval_steps_per_second: 0.543, epoch: 7.0[0m
[32m[2022-09-16 20:51:32,616] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 20:51:32,617] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 20:51:32,617] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:51:32,617] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:51:32,617] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 20:51:35,761] [    INFO][0m - eval_loss: 0.6412855386734009, eval_accuracy: 0.625, eval_runtime: 3.1442, eval_samples_per_second: 50.887, eval_steps_per_second: 3.18, epoch: 7.0[0m
[32m[2022-09-16 20:51:35,762] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-70[0m
[32m[2022-09-16 20:51:35,762] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:51:38,164] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-70/tokenizer_config.json[0m
[32m[2022-09-16 20:51:38,164] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-70/special_tokens_map.json[0m
[32m[2022-09-16 20:51:50,824] [    INFO][0m - loss: 0.50132594, learning_rate: 2.1999999999999997e-06, global_step: 80, interval_runtime: 18.2086, interval_samples_per_second: 0.879, interval_steps_per_second: 0.549, epoch: 8.0[0m
[32m[2022-09-16 20:51:50,825] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 20:51:50,825] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 20:51:50,825] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:51:50,825] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:51:50,825] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 20:51:53,959] [    INFO][0m - eval_loss: 0.6372637748718262, eval_accuracy: 0.59375, eval_runtime: 3.1337, eval_samples_per_second: 51.058, eval_steps_per_second: 3.191, epoch: 8.0[0m
[32m[2022-09-16 20:51:53,960] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-80[0m
[32m[2022-09-16 20:51:53,960] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:51:56,465] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-80/tokenizer_config.json[0m
[32m[2022-09-16 20:51:56,465] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-80/special_tokens_map.json[0m
[32m[2022-09-16 20:52:09,227] [    INFO][0m - loss: 0.45511174, learning_rate: 2.1e-06, global_step: 90, interval_runtime: 18.4018, interval_samples_per_second: 0.869, interval_steps_per_second: 0.543, epoch: 9.0[0m
[32m[2022-09-16 20:52:09,228] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 20:52:09,228] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 20:52:09,228] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:52:09,228] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:52:09,228] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 20:52:12,371] [    INFO][0m - eval_loss: 0.6441640257835388, eval_accuracy: 0.625, eval_runtime: 3.1432, eval_samples_per_second: 50.904, eval_steps_per_second: 3.181, epoch: 9.0[0m
[32m[2022-09-16 20:52:12,372] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-90[0m
[32m[2022-09-16 20:52:12,372] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:52:14,928] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-90/tokenizer_config.json[0m
[32m[2022-09-16 20:52:14,928] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-90/special_tokens_map.json[0m
[32m[2022-09-16 20:52:28,527] [    INFO][0m - loss: 0.45395403, learning_rate: 2e-06, global_step: 100, interval_runtime: 19.301, interval_samples_per_second: 0.829, interval_steps_per_second: 0.518, epoch: 10.0[0m
[32m[2022-09-16 20:52:28,528] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 20:52:28,528] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 20:52:28,528] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:52:28,528] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:52:28,528] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 20:52:31,663] [    INFO][0m - eval_loss: 0.712651252746582, eval_accuracy: 0.625, eval_runtime: 3.1345, eval_samples_per_second: 51.045, eval_steps_per_second: 3.19, epoch: 10.0[0m
[32m[2022-09-16 20:52:31,663] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-100[0m
[32m[2022-09-16 20:52:31,663] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:52:34,340] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-16 20:52:34,340] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-16 20:52:47,414] [    INFO][0m - loss: 0.41886182, learning_rate: 1.9e-06, global_step: 110, interval_runtime: 18.8867, interval_samples_per_second: 0.847, interval_steps_per_second: 0.529, epoch: 11.0[0m
[32m[2022-09-16 20:52:47,415] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 20:52:47,415] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 20:52:47,415] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:52:47,415] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:52:47,415] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 20:52:50,553] [    INFO][0m - eval_loss: 0.669876217842102, eval_accuracy: 0.63125, eval_runtime: 3.1383, eval_samples_per_second: 50.983, eval_steps_per_second: 3.186, epoch: 11.0[0m
[32m[2022-09-16 20:52:50,554] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-110[0m
[32m[2022-09-16 20:52:50,554] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:52:53,684] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-110/tokenizer_config.json[0m
[32m[2022-09-16 20:52:53,685] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-110/special_tokens_map.json[0m
[32m[2022-09-16 20:53:06,705] [    INFO][0m - loss: 0.36210854, learning_rate: 1.8e-06, global_step: 120, interval_runtime: 19.2913, interval_samples_per_second: 0.829, interval_steps_per_second: 0.518, epoch: 12.0[0m
[32m[2022-09-16 20:53:06,706] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 20:53:06,706] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 20:53:06,706] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:53:06,706] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:53:06,706] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 20:53:09,854] [    INFO][0m - eval_loss: 0.7115546464920044, eval_accuracy: 0.61875, eval_runtime: 3.1477, eval_samples_per_second: 50.83, eval_steps_per_second: 3.177, epoch: 12.0[0m
[32m[2022-09-16 20:53:09,855] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-120[0m
[32m[2022-09-16 20:53:09,855] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:53:12,315] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-120/tokenizer_config.json[0m
[32m[2022-09-16 20:53:12,316] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-120/special_tokens_map.json[0m
[32m[2022-09-16 20:53:25,717] [    INFO][0m - loss: 0.31329145, learning_rate: 1.7e-06, global_step: 130, interval_runtime: 19.012, interval_samples_per_second: 0.842, interval_steps_per_second: 0.526, epoch: 13.0[0m
[32m[2022-09-16 20:53:25,718] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 20:53:25,718] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 20:53:25,718] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:53:25,718] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:53:25,718] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 20:53:29,303] [    INFO][0m - eval_loss: 0.7085718512535095, eval_accuracy: 0.6375, eval_runtime: 3.1373, eval_samples_per_second: 50.999, eval_steps_per_second: 3.187, epoch: 13.0[0m
[32m[2022-09-16 20:53:29,304] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-130[0m
[32m[2022-09-16 20:53:29,304] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:53:31,749] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-130/tokenizer_config.json[0m
[32m[2022-09-16 20:53:31,749] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-130/special_tokens_map.json[0m
[32m[2022-09-16 20:53:44,352] [    INFO][0m - loss: 0.28664274, learning_rate: 1.6e-06, global_step: 140, interval_runtime: 18.6343, interval_samples_per_second: 0.859, interval_steps_per_second: 0.537, epoch: 14.0[0m
[32m[2022-09-16 20:53:44,352] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 20:53:44,352] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 20:53:44,353] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:53:44,353] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:53:44,353] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 20:53:47,515] [    INFO][0m - eval_loss: 0.736000657081604, eval_accuracy: 0.65, eval_runtime: 3.1618, eval_samples_per_second: 50.605, eval_steps_per_second: 3.163, epoch: 14.0[0m
[32m[2022-09-16 20:53:47,516] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-140[0m
[32m[2022-09-16 20:53:47,516] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:53:50,085] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-140/tokenizer_config.json[0m
[32m[2022-09-16 20:53:50,086] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-140/special_tokens_map.json[0m
[32m[2022-09-16 20:54:02,832] [    INFO][0m - loss: 0.27475538, learning_rate: 1.5e-06, global_step: 150, interval_runtime: 18.4804, interval_samples_per_second: 0.866, interval_steps_per_second: 0.541, epoch: 15.0[0m
[32m[2022-09-16 20:54:02,833] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 20:54:02,833] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 20:54:02,833] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:54:02,833] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:54:02,833] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 20:54:05,977] [    INFO][0m - eval_loss: 0.7682375907897949, eval_accuracy: 0.6375, eval_runtime: 3.1439, eval_samples_per_second: 50.891, eval_steps_per_second: 3.181, epoch: 15.0[0m
[32m[2022-09-16 20:54:05,978] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-150[0m
[32m[2022-09-16 20:54:05,978] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:54:08,296] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-150/tokenizer_config.json[0m
[32m[2022-09-16 20:54:08,297] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-150/special_tokens_map.json[0m
[32m[2022-09-16 20:54:20,840] [    INFO][0m - loss: 0.20673687, learning_rate: 1.4000000000000001e-06, global_step: 160, interval_runtime: 18.0084, interval_samples_per_second: 0.888, interval_steps_per_second: 0.555, epoch: 16.0[0m
[32m[2022-09-16 20:54:20,841] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 20:54:20,841] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 20:54:20,841] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:54:20,841] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:54:20,841] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 20:54:23,984] [    INFO][0m - eval_loss: 0.8502117395401001, eval_accuracy: 0.6375, eval_runtime: 3.1426, eval_samples_per_second: 50.913, eval_steps_per_second: 3.182, epoch: 16.0[0m
[32m[2022-09-16 20:54:23,985] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-160[0m
[32m[2022-09-16 20:54:23,985] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:54:26,376] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-160/tokenizer_config.json[0m
[32m[2022-09-16 20:54:26,376] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-160/special_tokens_map.json[0m
[32m[2022-09-16 20:54:39,774] [    INFO][0m - loss: 0.19263196, learning_rate: 1.3e-06, global_step: 170, interval_runtime: 18.9335, interval_samples_per_second: 0.845, interval_steps_per_second: 0.528, epoch: 17.0[0m
[32m[2022-09-16 20:54:39,775] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 20:54:39,775] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 20:54:39,775] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:54:39,775] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:54:39,775] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 20:54:42,928] [    INFO][0m - eval_loss: 0.8013348579406738, eval_accuracy: 0.675, eval_runtime: 3.1525, eval_samples_per_second: 50.753, eval_steps_per_second: 3.172, epoch: 17.0[0m
[32m[2022-09-16 20:54:42,928] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-170[0m
[32m[2022-09-16 20:54:42,928] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:54:45,288] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-170/tokenizer_config.json[0m
[32m[2022-09-16 20:54:45,288] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-170/special_tokens_map.json[0m
[32m[2022-09-16 20:54:59,571] [    INFO][0m - loss: 0.18008637, learning_rate: 1.2000000000000002e-06, global_step: 180, interval_runtime: 19.7967, interval_samples_per_second: 0.808, interval_steps_per_second: 0.505, epoch: 18.0[0m
[32m[2022-09-16 20:54:59,572] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 20:54:59,572] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 20:54:59,572] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:54:59,572] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:54:59,572] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 20:55:02,700] [    INFO][0m - eval_loss: 0.9104498028755188, eval_accuracy: 0.65, eval_runtime: 3.1283, eval_samples_per_second: 51.147, eval_steps_per_second: 3.197, epoch: 18.0[0m
[32m[2022-09-16 20:55:02,701] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-180[0m
[32m[2022-09-16 20:55:02,701] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:55:05,539] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-180/tokenizer_config.json[0m
[32m[2022-09-16 20:55:05,539] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-180/special_tokens_map.json[0m
[32m[2022-09-16 20:55:19,932] [    INFO][0m - loss: 0.2128608, learning_rate: 1.0999999999999998e-06, global_step: 190, interval_runtime: 20.3609, interval_samples_per_second: 0.786, interval_steps_per_second: 0.491, epoch: 19.0[0m
[32m[2022-09-16 20:55:19,932] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 20:55:19,932] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 20:55:19,932] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:55:19,932] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:55:19,932] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 20:55:23,678] [    INFO][0m - eval_loss: 0.8868783712387085, eval_accuracy: 0.6875, eval_runtime: 3.15, eval_samples_per_second: 50.794, eval_steps_per_second: 3.175, epoch: 19.0[0m
[32m[2022-09-16 20:55:23,678] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-190[0m
[32m[2022-09-16 20:55:23,678] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:55:26,823] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-190/tokenizer_config.json[0m
[32m[2022-09-16 20:55:26,824] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-190/special_tokens_map.json[0m
[32m[2022-09-16 20:55:42,747] [    INFO][0m - loss: 0.16942531, learning_rate: 1e-06, global_step: 200, interval_runtime: 22.8149, interval_samples_per_second: 0.701, interval_steps_per_second: 0.438, epoch: 20.0[0m
[32m[2022-09-16 20:55:42,747] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 20:55:42,748] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 20:55:42,748] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:55:42,748] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:55:42,748] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 20:55:45,884] [    INFO][0m - eval_loss: 0.9430658221244812, eval_accuracy: 0.66875, eval_runtime: 3.1363, eval_samples_per_second: 51.015, eval_steps_per_second: 3.188, epoch: 20.0[0m
[32m[2022-09-16 20:55:45,885] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-200[0m
[32m[2022-09-16 20:55:45,885] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:55:49,074] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-200/tokenizer_config.json[0m
[32m[2022-09-16 20:55:49,074] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-200/special_tokens_map.json[0m
[32m[2022-09-16 20:56:03,742] [    INFO][0m - loss: 0.12205426, learning_rate: 9e-07, global_step: 210, interval_runtime: 20.9958, interval_samples_per_second: 0.762, interval_steps_per_second: 0.476, epoch: 21.0[0m
[32m[2022-09-16 20:56:03,743] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 20:56:03,743] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 20:56:03,743] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:56:03,743] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:56:03,743] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 20:56:06,899] [    INFO][0m - eval_loss: 0.9442802667617798, eval_accuracy: 0.68125, eval_runtime: 3.1557, eval_samples_per_second: 50.701, eval_steps_per_second: 3.169, epoch: 21.0[0m
[32m[2022-09-16 20:56:06,900] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-210[0m
[32m[2022-09-16 20:56:06,900] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:56:10,388] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-210/tokenizer_config.json[0m
[32m[2022-09-16 20:56:10,389] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-210/special_tokens_map.json[0m
[32m[2022-09-16 20:56:26,326] [    INFO][0m - loss: 0.15420494, learning_rate: 8e-07, global_step: 220, interval_runtime: 22.5832, interval_samples_per_second: 0.708, interval_steps_per_second: 0.443, epoch: 22.0[0m
[32m[2022-09-16 20:56:26,326] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 20:56:26,326] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 20:56:26,327] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:56:26,327] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:56:26,327] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 20:56:30,018] [    INFO][0m - eval_loss: 0.9594007730484009, eval_accuracy: 0.66875, eval_runtime: 3.1352, eval_samples_per_second: 51.033, eval_steps_per_second: 3.19, epoch: 22.0[0m
[32m[2022-09-16 20:56:30,019] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-220[0m
[32m[2022-09-16 20:56:30,019] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:56:33,180] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-220/tokenizer_config.json[0m
[32m[2022-09-16 20:56:33,180] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-220/special_tokens_map.json[0m
[32m[2022-09-16 20:56:47,824] [    INFO][0m - loss: 0.12012461, learning_rate: 7.000000000000001e-07, global_step: 230, interval_runtime: 21.4983, interval_samples_per_second: 0.744, interval_steps_per_second: 0.465, epoch: 23.0[0m
[32m[2022-09-16 20:56:47,825] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 20:56:47,825] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 20:56:47,825] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:56:47,825] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:56:47,826] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 20:56:50,986] [    INFO][0m - eval_loss: 1.0843355655670166, eval_accuracy: 0.6625, eval_runtime: 3.1601, eval_samples_per_second: 50.631, eval_steps_per_second: 3.164, epoch: 23.0[0m
[32m[2022-09-16 20:56:50,986] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-230[0m
[32m[2022-09-16 20:56:50,986] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:56:54,223] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-230/tokenizer_config.json[0m
[32m[2022-09-16 20:56:54,224] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-230/special_tokens_map.json[0m
[32m[2022-09-16 20:57:08,848] [    INFO][0m - loss: 0.13094499, learning_rate: 6.000000000000001e-07, global_step: 240, interval_runtime: 21.0239, interval_samples_per_second: 0.761, interval_steps_per_second: 0.476, epoch: 24.0[0m
[32m[2022-09-16 20:57:08,849] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 20:57:08,849] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 20:57:08,849] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:57:08,849] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:57:08,849] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 20:57:11,985] [    INFO][0m - eval_loss: 1.090386152267456, eval_accuracy: 0.6625, eval_runtime: 3.1355, eval_samples_per_second: 51.028, eval_steps_per_second: 3.189, epoch: 24.0[0m
[32m[2022-09-16 20:57:11,985] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-240[0m
[32m[2022-09-16 20:57:11,985] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:57:15,518] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-240/tokenizer_config.json[0m
[32m[2022-09-16 20:57:15,518] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-240/special_tokens_map.json[0m
[32m[2022-09-16 20:57:30,656] [    INFO][0m - loss: 0.17198538, learning_rate: 5e-07, global_step: 250, interval_runtime: 21.8081, interval_samples_per_second: 0.734, interval_steps_per_second: 0.459, epoch: 25.0[0m
[32m[2022-09-16 20:57:30,657] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 20:57:30,657] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 20:57:30,657] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:57:30,657] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:57:30,658] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 20:57:33,806] [    INFO][0m - eval_loss: 1.0809290409088135, eval_accuracy: 0.675, eval_runtime: 3.1482, eval_samples_per_second: 50.822, eval_steps_per_second: 3.176, epoch: 25.0[0m
[32m[2022-09-16 20:57:33,806] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-250[0m
[32m[2022-09-16 20:57:33,807] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:57:37,042] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-250/tokenizer_config.json[0m
[32m[2022-09-16 20:57:37,042] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-250/special_tokens_map.json[0m
[32m[2022-09-16 20:57:51,740] [    INFO][0m - loss: 0.12860835, learning_rate: 4e-07, global_step: 260, interval_runtime: 21.0839, interval_samples_per_second: 0.759, interval_steps_per_second: 0.474, epoch: 26.0[0m
[32m[2022-09-16 20:57:51,740] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 20:57:51,741] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 20:57:51,741] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:57:51,741] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:57:51,741] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 20:57:54,894] [    INFO][0m - eval_loss: 1.0867371559143066, eval_accuracy: 0.68125, eval_runtime: 3.153, eval_samples_per_second: 50.746, eval_steps_per_second: 3.172, epoch: 26.0[0m
[32m[2022-09-16 20:57:54,894] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-260[0m
[32m[2022-09-16 20:57:54,895] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:57:57,775] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-260/tokenizer_config.json[0m
[32m[2022-09-16 20:57:57,775] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-260/special_tokens_map.json[0m
[32m[2022-09-16 20:58:11,333] [    INFO][0m - loss: 0.12246344, learning_rate: 3.0000000000000004e-07, global_step: 270, interval_runtime: 19.5929, interval_samples_per_second: 0.817, interval_steps_per_second: 0.51, epoch: 27.0[0m
[32m[2022-09-16 20:58:11,334] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 20:58:11,334] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 20:58:11,334] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:58:11,334] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:58:11,335] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 20:58:14,479] [    INFO][0m - eval_loss: 1.0692806243896484, eval_accuracy: 0.6875, eval_runtime: 3.1442, eval_samples_per_second: 50.887, eval_steps_per_second: 3.18, epoch: 27.0[0m
[32m[2022-09-16 20:58:14,479] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-270[0m
[32m[2022-09-16 20:58:14,479] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:58:17,367] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-270/tokenizer_config.json[0m
[32m[2022-09-16 20:58:17,367] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-270/special_tokens_map.json[0m
[32m[2022-09-16 20:58:30,926] [    INFO][0m - loss: 0.12991831, learning_rate: 2e-07, global_step: 280, interval_runtime: 19.5934, interval_samples_per_second: 0.817, interval_steps_per_second: 0.51, epoch: 28.0[0m
[32m[2022-09-16 20:58:30,927] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 20:58:30,927] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 20:58:30,927] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:58:30,927] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:58:30,927] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 20:58:34,065] [    INFO][0m - eval_loss: 1.0683443546295166, eval_accuracy: 0.675, eval_runtime: 3.1366, eval_samples_per_second: 51.011, eval_steps_per_second: 3.188, epoch: 28.0[0m
[32m[2022-09-16 20:58:34,065] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-280[0m
[32m[2022-09-16 20:58:34,065] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:58:37,762] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-280/tokenizer_config.json[0m
[32m[2022-09-16 20:58:37,762] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-280/special_tokens_map.json[0m
[32m[2022-09-16 20:58:51,068] [    INFO][0m - loss: 0.08886122, learning_rate: 1e-07, global_step: 290, interval_runtime: 20.1414, interval_samples_per_second: 0.794, interval_steps_per_second: 0.496, epoch: 29.0[0m
[32m[2022-09-16 20:58:51,068] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 20:58:51,068] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 20:58:51,068] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:58:51,069] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:58:51,069] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 20:58:54,217] [    INFO][0m - eval_loss: 1.0725080966949463, eval_accuracy: 0.675, eval_runtime: 3.1484, eval_samples_per_second: 50.82, eval_steps_per_second: 3.176, epoch: 29.0[0m
[32m[2022-09-16 20:58:54,218] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-290[0m
[32m[2022-09-16 20:58:54,218] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:58:57,010] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-290/tokenizer_config.json[0m
[32m[2022-09-16 20:58:57,010] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-290/special_tokens_map.json[0m
[32m[2022-09-16 20:59:10,313] [    INFO][0m - loss: 0.15406048, learning_rate: 0.0, global_step: 300, interval_runtime: 19.2452, interval_samples_per_second: 0.831, interval_steps_per_second: 0.52, epoch: 30.0[0m
[32m[2022-09-16 20:59:10,313] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 20:59:10,313] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 20:59:10,313] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:59:10,314] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:59:10,314] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 20:59:13,457] [    INFO][0m - eval_loss: 1.0761162042617798, eval_accuracy: 0.66875, eval_runtime: 3.143, eval_samples_per_second: 50.907, eval_steps_per_second: 3.182, epoch: 30.0[0m
[32m[2022-09-16 20:59:13,457] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-300[0m
[32m[2022-09-16 20:59:13,457] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:59:19,905] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-300/tokenizer_config.json[0m
[32m[2022-09-16 20:59:19,906] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-300/special_tokens_map.json[0m
[32m[2022-09-16 20:59:25,262] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-16 20:59:25,262] [    INFO][0m - Loading best model from ./checkpoints_csl/checkpoint-190 (score: 0.6875).[0m
[32m[2022-09-16 20:59:27,002] [    INFO][0m - train_runtime: 598.4613, train_samples_per_second: 8.021, train_steps_per_second: 0.501, train_loss: 0.5740887876351675, epoch: 30.0[0m
[32m[2022-09-16 20:59:27,003] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/[0m
[32m[2022-09-16 20:59:27,004] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 20:59:29,644] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/tokenizer_config.json[0m
[32m[2022-09-16 20:59:29,644] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/special_tokens_map.json[0m
[32m[2022-09-16 20:59:29,645] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-16 20:59:29,646] [    INFO][0m -   epoch                    =       30.0[0m
[32m[2022-09-16 20:59:29,646] [    INFO][0m -   train_loss               =     0.5741[0m
[32m[2022-09-16 20:59:29,646] [    INFO][0m -   train_runtime            = 0:09:58.46[0m
[32m[2022-09-16 20:59:29,646] [    INFO][0m -   train_samples_per_second =      8.021[0m
[32m[2022-09-16 20:59:29,646] [    INFO][0m -   train_steps_per_second   =      0.501[0m
[32m[2022-09-16 20:59:29,649] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-16 20:59:29,649] [    INFO][0m -   Num examples = 2838[0m
[32m[2022-09-16 20:59:29,649] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 20:59:29,649] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 20:59:29,649] [    INFO][0m -   Total prediction steps = 178[0m
[32m[2022-09-16 21:00:26,701] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-16 21:00:26,702] [    INFO][0m -   test_accuracy           =     0.6367[0m
[32m[2022-09-16 21:00:26,702] [    INFO][0m -   test_loss               =      1.043[0m
[32m[2022-09-16 21:00:26,702] [    INFO][0m -   test_runtime            = 0:00:57.05[0m
[32m[2022-09-16 21:00:26,702] [    INFO][0m -   test_samples_per_second =     49.744[0m
[32m[2022-09-16 21:00:26,702] [    INFO][0m -   test_steps_per_second   =       3.12[0m
[32m[2022-09-16 21:00:26,703] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-16 21:00:26,703] [    INFO][0m -   Num examples = 3000[0m
[32m[2022-09-16 21:00:26,703] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 21:00:26,703] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 21:00:26,703] [    INFO][0m -   Total prediction steps = 188[0m
[32m[2022-09-16 21:01:33,772] [    INFO][0m - Predictions for cslf saved to ./fewclue_submit_examples.[0m
{
  "labels": 1,
  "text_a": "\u65e0\u7ebf\u4f20\u611f\u5668\u7f51\u7edc\u4e2d\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u901a\u7528\u8fd1\u4f3c\u67e5\u8be2\u662f\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\uff0e\u6587\u4e2d\u63d0\u51fa\u4e00\u79cd\u65e0\u7ebf\u4f20\u611f\u5668\u7f51\u7edc\u4e2d\u9690\u79c1\u4fdd\u62a4\u901a\u7528\u8fd1\u4f3c\u67e5\u8be2\u534f\u8baePGAQ\uff0ePGAQ\u5c06\u4f20\u611f\u5668\u8282\u70b9\u7f16\u53f7\u548c\u5176\u91c7\u96c6\u6570\u636e\u9690\u85cf\u4e8e\u8bbe\u8ba1\u7684\u6570\u636e\u7ed3\u6784\u4e2d\uff0c\u5728\u57fa\u7ad9\u6784\u9020\u7ebf\u6027\u65b9\u7a0b\u7ec4\u89e3\u51fa\u76f4\u65b9\u56fe\uff0c\u6839\u636e\u76f4\u65b9\u56fe\u5177\u6709\u7684\u7edf\u8ba1\u4fe1\u606f\uff0c\u4e0d\u6cc4\u9732\u9690\u79c1\u5730\u5b8c\u6210Top-k\u67e5\u8be2\u3001\u8303\u56f4\u67e5\u8be2\u3001SUM\u3001MAX/MIN\u3001Median\u3001Histogram\u7b49\u8fd1\u4f3c\u67e5\u8be2\uff0ePGAQ\u4f7f\u7528\u7f51\u5185\u6c42\u548c\u805a\u96c6\u4ee5\u51cf\u5c11\u80fd\u91cf\u6d88\u8017\uff0c\u5e76\u4e14\u80fd\u591f\u901a\u8fc7\u8c03\u8282\u76f4\u65b9\u56fe\u5212\u5206\u7c92\u5ea6\u6765\u5e73\u8861\u67e5\u8be2\u7cbe\u5ea6\u4e0e\u80fd\u91cf\u6d88\u8017\uff0ePGAQ\u534f\u8bae\u5206\u4e3aH-PGAQ\u548cF-PGAQ\u4e24\u79cd\u6a21\u5f0f\uff0eH-PGAQ\u6a21\u5f0f\u4f7f\u7528\u6570\u636e\u6270\u52a8\u6280\u672f\u52a0\u5f3a\u6570\u636e\u5b89\u5168\u6027\uff0cF-PGAQ\u4f7f\u7528\u8fc7\u6ee4\u5668\u51cf\u5c11\u8fde\u7eed\u67e5\u8be2\u901a\u4fe1\u91cf\uff0e\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u4f7f\u7528\u771f\u5b9e\u6570\u636e\u96c6\u5b9e\u9a8c\u9a8c\u8bc1\u4e86PGAQ\u7684\u5b89\u5168\u6027\u548c\u6709\u6548\u6027\uff0e",
  "text_b": "\u65e0\u7ebf\u4f20\u611f\u5668\u7f51\u7edc\u3001\u6570\u636e\u805a\u96c6\u3001\u7269\u8054\u7f51\u3001\u8fd1\u4f3c\u67e5\u8be2",
  "uid": 0
}

 
==========
cluewsc
==========
 
[32m[2022-09-16 21:01:56,248] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-16 21:01:56,249] [    INFO][0m - ============================================================[0m
[32m[2022-09-16 21:01:56,249] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-16 21:01:56,249] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-16 21:01:56,249] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-16 21:01:56,249] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-16 21:01:56,249] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-16 21:01:56,249] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-16 21:01:56,249] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-16 21:01:56,249] [    INFO][0m - [0m
[32m[2022-09-16 21:01:56,249] [    INFO][0m - ============================================================[0m
[32m[2022-09-16 21:01:56,250] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-16 21:01:56,250] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-16 21:01:56,250] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-16 21:01:56,250] [    INFO][0m - pretrained                    :../checkpoints_cmnli/checkpoint-22000/model_state.pdparams[0m
[32m[2022-09-16 21:01:56,250] [    INFO][0m - prompt                        :‚Äú{'text':'text_a'}‚Äù{'text':'text_b'}ËøôÈáå‰ª£ËØç‰ΩøÁî®Ê≠£Á°ÆÂêóÔºü{'mask'}{'mask'}[0m
[32m[2022-09-16 21:01:56,250] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-16 21:01:56,250] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-16 21:01:56,250] [    INFO][0m - task_name                     :cluewsc[0m
[32m[2022-09-16 21:01:56,250] [    INFO][0m - [0m
[32m[2022-09-16 21:01:56,250] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0916 21:01:56.251960 43150 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0916 21:01:56.255995 43150 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-16 21:02:02,316] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-16 21:02:02,327] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-16 21:02:02,328] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-16 21:02:02,328] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚Äù'}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': 'ËøôÈáå‰ª£ËØç‰ΩøÁî®Ê≠£Á°ÆÂêóÔºü'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}][0m
[32m[2022-09-16 21:02:04,067] [    INFO][0m - ============================================================[0m
[32m[2022-09-16 21:02:04,067] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-16 21:02:04,067] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-16 21:02:04,068] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-16 21:02:04,068] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-16 21:02:04,068] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-16 21:02:04,068] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-16 21:02:04,068] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-16 21:02:04,068] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-16 21:02:04,068] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-16 21:02:04,068] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-16 21:02:04,068] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-16 21:02:04,068] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-16 21:02:04,068] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-16 21:02:04,068] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-16 21:02:04,069] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-16 21:02:04,069] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-16 21:02:04,069] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-16 21:02:04,069] [    INFO][0m - eval_batch_size               :16[0m
[32m[2022-09-16 21:02:04,069] [    INFO][0m - eval_steps                    :200[0m
[32m[2022-09-16 21:02:04,069] [    INFO][0m - evaluation_strategy           :IntervalStrategy.EPOCH[0m
[32m[2022-09-16 21:02:04,069] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-16 21:02:04,069] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-16 21:02:04,069] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-16 21:02:04,069] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-16 21:02:04,069] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-16 21:02:04,069] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-16 21:02:04,069] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-16 21:02:04,070] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-16 21:02:04,070] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-16 21:02:04,070] [    INFO][0m - learning_rate                 :3e-06[0m
[32m[2022-09-16 21:02:04,070] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-16 21:02:04,070] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-16 21:02:04,070] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-16 21:02:04,070] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-16 21:02:04,070] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-16 21:02:04,070] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-16 21:02:04,070] [    INFO][0m - logging_dir                   :./checkpoints_cluewsc/runs/Sep16_21-01-56_instance-3bwob41y-01[0m
[32m[2022-09-16 21:02:04,070] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-16 21:02:04,070] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-16 21:02:04,070] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-16 21:02:04,070] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-16 21:02:04,071] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-16 21:02:04,071] [    INFO][0m - max_seq_length                :128[0m
[32m[2022-09-16 21:02:04,071] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-16 21:02:04,071] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-16 21:02:04,071] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-16 21:02:04,071] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-16 21:02:04,071] [    INFO][0m - num_train_epochs              :30.0[0m
[32m[2022-09-16 21:02:04,071] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-16 21:02:04,071] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-16 21:02:04,071] [    INFO][0m - output_dir                    :./checkpoints_cluewsc/[0m
[32m[2022-09-16 21:02:04,071] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-16 21:02:04,071] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-16 21:02:04,071] [    INFO][0m - per_device_eval_batch_size    :16[0m
[32m[2022-09-16 21:02:04,072] [    INFO][0m - per_device_train_batch_size   :16[0m
[32m[2022-09-16 21:02:04,072] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-16 21:02:04,072] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-16 21:02:04,072] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-16 21:02:04,072] [    INFO][0m - ppt_learning_rate             :3e-05[0m
[32m[2022-09-16 21:02:04,072] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-16 21:02:04,072] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-16 21:02:04,072] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-16 21:02:04,072] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-16 21:02:04,072] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-16 21:02:04,072] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-16 21:02:04,072] [    INFO][0m - run_name                      :./checkpoints_cluewsc/[0m
[32m[2022-09-16 21:02:04,072] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-16 21:02:04,072] [    INFO][0m - save_steps                    :200[0m
[32m[2022-09-16 21:02:04,073] [    INFO][0m - save_strategy                 :IntervalStrategy.EPOCH[0m
[32m[2022-09-16 21:02:04,073] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-16 21:02:04,073] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-16 21:02:04,073] [    INFO][0m - seed                          :42[0m
[32m[2022-09-16 21:02:04,073] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-16 21:02:04,073] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-16 21:02:04,073] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-16 21:02:04,073] [    INFO][0m - train_batch_size              :16[0m
[32m[2022-09-16 21:02:04,073] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-16 21:02:04,073] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-16 21:02:04,073] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-16 21:02:04,073] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-16 21:02:04,073] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-16 21:02:04,073] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-16 21:02:04,074] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-16 21:02:04,074] [    INFO][0m - [0m
[32m[2022-09-16 21:02:04,076] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-16 21:02:04,076] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 21:02:04,076] [    INFO][0m -   Num Epochs = 30[0m
[32m[2022-09-16 21:02:04,076] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2022-09-16 21:02:04,077] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2022-09-16 21:02:04,077] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-16 21:02:04,077] [    INFO][0m -   Total optimization steps = 300.0[0m
[32m[2022-09-16 21:02:04,077] [    INFO][0m -   Total num train samples = 4800[0m
[32m[2022-09-16 21:02:08,455] [    INFO][0m - loss: 4.83604965, learning_rate: 2.9e-06, global_step: 10, interval_runtime: 4.3774, interval_samples_per_second: 3.655, interval_steps_per_second: 2.284, epoch: 1.0[0m
[32m[2022-09-16 21:02:08,456] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 21:02:08,456] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 21:02:08,456] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 21:02:08,456] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 21:02:08,456] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 21:02:09,847] [    INFO][0m - eval_loss: 2.0439863204956055, eval_accuracy: 0.5031446540880503, eval_runtime: 1.3904, eval_samples_per_second: 114.358, eval_steps_per_second: 7.192, epoch: 1.0[0m
[32m[2022-09-16 21:02:09,848] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-10[0m
[32m[2022-09-16 21:02:09,848] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 21:02:12,839] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-10/tokenizer_config.json[0m
[32m[2022-09-16 21:02:12,840] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-10/special_tokens_map.json[0m
[32m[2022-09-16 21:02:21,943] [    INFO][0m - loss: 1.53464937, learning_rate: 2.8000000000000003e-06, global_step: 20, interval_runtime: 13.4878, interval_samples_per_second: 1.186, interval_steps_per_second: 0.741, epoch: 2.0[0m
[32m[2022-09-16 21:02:21,944] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 21:02:21,944] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 21:02:21,944] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 21:02:21,944] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 21:02:21,944] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 21:02:23,288] [    INFO][0m - eval_loss: 0.789290726184845, eval_accuracy: 0.5849056603773585, eval_runtime: 1.3433, eval_samples_per_second: 118.37, eval_steps_per_second: 7.445, epoch: 2.0[0m
[32m[2022-09-16 21:02:23,288] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-20[0m
[32m[2022-09-16 21:02:23,288] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 21:02:26,173] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-20/tokenizer_config.json[0m
[32m[2022-09-16 21:02:26,173] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-20/special_tokens_map.json[0m
[32m[2022-09-16 21:02:35,364] [    INFO][0m - loss: 0.85367746, learning_rate: 2.7e-06, global_step: 30, interval_runtime: 13.4208, interval_samples_per_second: 1.192, interval_steps_per_second: 0.745, epoch: 3.0[0m
[32m[2022-09-16 21:02:35,365] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 21:02:35,365] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 21:02:35,365] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 21:02:35,365] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 21:02:35,365] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 21:02:36,780] [    INFO][0m - eval_loss: 0.7293440103530884, eval_accuracy: 0.5220125786163522, eval_runtime: 1.4143, eval_samples_per_second: 112.423, eval_steps_per_second: 7.071, epoch: 3.0[0m
[32m[2022-09-16 21:02:36,781] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-30[0m
[32m[2022-09-16 21:02:36,781] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 21:02:39,807] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-30/tokenizer_config.json[0m
[32m[2022-09-16 21:02:39,807] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-30/special_tokens_map.json[0m
[32m[2022-09-16 21:02:50,193] [    INFO][0m - loss: 0.75351906, learning_rate: 2.6e-06, global_step: 40, interval_runtime: 14.8289, interval_samples_per_second: 1.079, interval_steps_per_second: 0.674, epoch: 4.0[0m
[32m[2022-09-16 21:02:50,194] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 21:02:50,194] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 21:02:50,194] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 21:02:50,194] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 21:02:50,194] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 21:02:51,590] [    INFO][0m - eval_loss: 0.7072767615318298, eval_accuracy: 0.5157232704402516, eval_runtime: 1.3954, eval_samples_per_second: 113.949, eval_steps_per_second: 7.167, epoch: 4.0[0m
[32m[2022-09-16 21:02:51,591] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-40[0m
[32m[2022-09-16 21:02:51,591] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 21:02:54,807] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-40/tokenizer_config.json[0m
[32m[2022-09-16 21:02:54,808] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-40/special_tokens_map.json[0m
[32m[2022-09-16 21:03:04,279] [    INFO][0m - loss: 0.70766802, learning_rate: 2.5e-06, global_step: 50, interval_runtime: 14.0862, interval_samples_per_second: 1.136, interval_steps_per_second: 0.71, epoch: 5.0[0m
[32m[2022-09-16 21:03:04,280] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 21:03:04,280] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 21:03:04,280] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 21:03:04,280] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 21:03:04,280] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 21:03:05,658] [    INFO][0m - eval_loss: 0.6995004415512085, eval_accuracy: 0.5283018867924528, eval_runtime: 1.378, eval_samples_per_second: 115.384, eval_steps_per_second: 7.257, epoch: 5.0[0m
[32m[2022-09-16 21:03:05,659] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-50[0m
[32m[2022-09-16 21:03:05,659] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 21:03:08,431] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-50/tokenizer_config.json[0m
[32m[2022-09-16 21:03:08,431] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-50/special_tokens_map.json[0m
[32m[2022-09-16 21:03:21,016] [    INFO][0m - loss: 0.66271763, learning_rate: 2.4000000000000003e-06, global_step: 60, interval_runtime: 16.7373, interval_samples_per_second: 0.956, interval_steps_per_second: 0.597, epoch: 6.0[0m
[32m[2022-09-16 21:03:21,017] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 21:03:21,017] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 21:03:21,017] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 21:03:21,017] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 21:03:21,017] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 21:03:22,367] [    INFO][0m - eval_loss: 0.6775528788566589, eval_accuracy: 0.559748427672956, eval_runtime: 1.3494, eval_samples_per_second: 117.828, eval_steps_per_second: 7.411, epoch: 6.0[0m
[32m[2022-09-16 21:03:22,368] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-60[0m
[32m[2022-09-16 21:03:22,368] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 21:03:25,124] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-60/tokenizer_config.json[0m
[32m[2022-09-16 21:03:25,124] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-60/special_tokens_map.json[0m
[32m[2022-09-16 21:03:33,909] [    INFO][0m - loss: 0.62013774, learning_rate: 2.3000000000000004e-06, global_step: 70, interval_runtime: 12.8932, interval_samples_per_second: 1.241, interval_steps_per_second: 0.776, epoch: 7.0[0m
[32m[2022-09-16 21:03:33,910] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 21:03:33,911] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 21:03:33,911] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 21:03:33,911] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 21:03:33,911] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 21:03:35,260] [    INFO][0m - eval_loss: 0.716836154460907, eval_accuracy: 0.5471698113207547, eval_runtime: 1.3489, eval_samples_per_second: 117.874, eval_steps_per_second: 7.413, epoch: 7.0[0m
[32m[2022-09-16 21:03:35,260] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-70[0m
[32m[2022-09-16 21:03:35,260] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 21:03:37,975] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-70/tokenizer_config.json[0m
[32m[2022-09-16 21:03:37,976] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-70/special_tokens_map.json[0m
[32m[2022-09-16 21:03:46,813] [    INFO][0m - loss: 0.48903861, learning_rate: 2.1999999999999997e-06, global_step: 80, interval_runtime: 12.9041, interval_samples_per_second: 1.24, interval_steps_per_second: 0.775, epoch: 8.0[0m
[32m[2022-09-16 21:03:46,814] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 21:03:46,814] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 21:03:46,814] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 21:03:46,814] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 21:03:46,814] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 21:03:48,191] [    INFO][0m - eval_loss: 0.7290392518043518, eval_accuracy: 0.6037735849056604, eval_runtime: 1.3759, eval_samples_per_second: 115.563, eval_steps_per_second: 7.268, epoch: 8.0[0m
[32m[2022-09-16 21:03:48,917] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-80[0m
[32m[2022-09-16 21:03:48,917] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 21:03:51,644] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-80/tokenizer_config.json[0m
[32m[2022-09-16 21:03:51,645] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-80/special_tokens_map.json[0m
[32m[2022-09-16 21:04:00,240] [    INFO][0m - loss: 0.46693797, learning_rate: 2.1e-06, global_step: 90, interval_runtime: 13.426, interval_samples_per_second: 1.192, interval_steps_per_second: 0.745, epoch: 9.0[0m
[32m[2022-09-16 21:04:00,240] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 21:04:00,241] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 21:04:00,241] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 21:04:00,241] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 21:04:00,241] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 21:04:01,599] [    INFO][0m - eval_loss: 0.8132056593894958, eval_accuracy: 0.610062893081761, eval_runtime: 1.3578, eval_samples_per_second: 117.1, eval_steps_per_second: 7.365, epoch: 9.0[0m
[32m[2022-09-16 21:04:04,359] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-90[0m
[32m[2022-09-16 21:04:04,360] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 21:04:07,362] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-90/tokenizer_config.json[0m
[32m[2022-09-16 21:04:07,363] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-90/special_tokens_map.json[0m
[32m[2022-09-16 21:04:16,975] [    INFO][0m - loss: 0.33142357, learning_rate: 2e-06, global_step: 100, interval_runtime: 16.7359, interval_samples_per_second: 0.956, interval_steps_per_second: 0.598, epoch: 10.0[0m
[32m[2022-09-16 21:04:16,976] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 21:04:16,976] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 21:04:16,977] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 21:04:16,977] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 21:04:16,977] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 21:04:18,340] [    INFO][0m - eval_loss: 0.8805636167526245, eval_accuracy: 0.610062893081761, eval_runtime: 1.3629, eval_samples_per_second: 116.662, eval_steps_per_second: 7.337, epoch: 10.0[0m
[32m[2022-09-16 21:04:18,340] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-100[0m
[32m[2022-09-16 21:04:18,340] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 21:04:21,383] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-16 21:04:21,384] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-16 21:04:34,320] [    INFO][0m - loss: 0.27907267, learning_rate: 1.9e-06, global_step: 110, interval_runtime: 17.3447, interval_samples_per_second: 0.922, interval_steps_per_second: 0.577, epoch: 11.0[0m
[32m[2022-09-16 21:04:34,321] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 21:04:34,321] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 21:04:34,321] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 21:04:34,321] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 21:04:34,321] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 21:04:35,693] [    INFO][0m - eval_loss: 0.9408758282661438, eval_accuracy: 0.6163522012578616, eval_runtime: 1.3708, eval_samples_per_second: 115.992, eval_steps_per_second: 7.295, epoch: 11.0[0m
[32m[2022-09-16 21:04:35,693] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-110[0m
[32m[2022-09-16 21:04:35,693] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 21:04:38,526] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-110/tokenizer_config.json[0m
[32m[2022-09-16 21:04:38,527] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-110/special_tokens_map.json[0m
[32m[2022-09-16 21:04:48,112] [    INFO][0m - loss: 0.24003401, learning_rate: 1.8e-06, global_step: 120, interval_runtime: 13.7923, interval_samples_per_second: 1.16, interval_steps_per_second: 0.725, epoch: 12.0[0m
[32m[2022-09-16 21:04:48,113] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 21:04:48,113] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 21:04:48,113] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 21:04:48,114] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 21:04:48,114] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 21:04:49,470] [    INFO][0m - eval_loss: 1.085922360420227, eval_accuracy: 0.6352201257861635, eval_runtime: 1.3561, eval_samples_per_second: 117.25, eval_steps_per_second: 7.374, epoch: 12.0[0m
[32m[2022-09-16 21:04:49,470] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-120[0m
[32m[2022-09-16 21:04:49,470] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 21:04:52,336] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-120/tokenizer_config.json[0m
[32m[2022-09-16 21:04:52,337] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-120/special_tokens_map.json[0m
[32m[2022-09-16 21:05:01,397] [    INFO][0m - loss: 0.18008661, learning_rate: 1.7e-06, global_step: 130, interval_runtime: 13.285, interval_samples_per_second: 1.204, interval_steps_per_second: 0.753, epoch: 13.0[0m
[32m[2022-09-16 21:05:01,398] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 21:05:01,398] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 21:05:01,399] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 21:05:01,399] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 21:05:01,399] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 21:05:02,780] [    INFO][0m - eval_loss: 1.3030277490615845, eval_accuracy: 0.6352201257861635, eval_runtime: 1.3814, eval_samples_per_second: 115.103, eval_steps_per_second: 7.239, epoch: 13.0[0m
[32m[2022-09-16 21:05:02,781] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-130[0m
[32m[2022-09-16 21:05:02,781] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 21:05:05,511] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-130/tokenizer_config.json[0m
[32m[2022-09-16 21:05:05,512] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-130/special_tokens_map.json[0m
[32m[2022-09-16 21:05:14,174] [    INFO][0m - loss: 0.16198303, learning_rate: 1.6e-06, global_step: 140, interval_runtime: 12.7056, interval_samples_per_second: 1.259, interval_steps_per_second: 0.787, epoch: 14.0[0m
[32m[2022-09-16 21:05:14,175] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 21:05:14,175] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 21:05:14,175] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 21:05:14,175] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 21:05:14,175] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 21:05:15,536] [    INFO][0m - eval_loss: 1.346008539199829, eval_accuracy: 0.6918238993710691, eval_runtime: 1.3601, eval_samples_per_second: 116.901, eval_steps_per_second: 7.352, epoch: 14.0[0m
[32m[2022-09-16 21:05:15,536] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-140[0m
[32m[2022-09-16 21:05:15,536] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 21:05:18,207] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-140/tokenizer_config.json[0m
[32m[2022-09-16 21:05:18,207] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-140/special_tokens_map.json[0m
[32m[2022-09-16 21:05:26,770] [    INFO][0m - loss: 0.10272676, learning_rate: 1.5e-06, global_step: 150, interval_runtime: 12.667, interval_samples_per_second: 1.263, interval_steps_per_second: 0.789, epoch: 15.0[0m
[32m[2022-09-16 21:05:26,771] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 21:05:26,771] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 21:05:26,771] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 21:05:26,771] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 21:05:26,771] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 21:05:28,123] [    INFO][0m - eval_loss: 1.449546217918396, eval_accuracy: 0.6729559748427673, eval_runtime: 1.3517, eval_samples_per_second: 117.633, eval_steps_per_second: 7.398, epoch: 15.0[0m
[32m[2022-09-16 21:05:28,123] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-150[0m
[32m[2022-09-16 21:05:28,123] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 21:05:35,865] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-150/tokenizer_config.json[0m
[32m[2022-09-16 21:05:35,865] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-150/special_tokens_map.json[0m
[32m[2022-09-16 21:05:44,722] [    INFO][0m - loss: 0.13322606, learning_rate: 1.4000000000000001e-06, global_step: 160, interval_runtime: 17.9519, interval_samples_per_second: 0.891, interval_steps_per_second: 0.557, epoch: 16.0[0m
[32m[2022-09-16 21:05:44,722] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 21:05:44,722] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 21:05:44,722] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 21:05:44,723] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 21:05:44,723] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 21:05:46,074] [    INFO][0m - eval_loss: 1.5204784870147705, eval_accuracy: 0.6855345911949685, eval_runtime: 1.3516, eval_samples_per_second: 117.635, eval_steps_per_second: 7.398, epoch: 16.0[0m
[32m[2022-09-16 21:05:46,075] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-160[0m
[32m[2022-09-16 21:05:46,075] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 21:05:48,786] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-160/tokenizer_config.json[0m
[32m[2022-09-16 21:05:48,787] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-160/special_tokens_map.json[0m
[32m[2022-09-16 21:05:57,826] [    INFO][0m - loss: 0.14842789, learning_rate: 1.3e-06, global_step: 170, interval_runtime: 13.1045, interval_samples_per_second: 1.221, interval_steps_per_second: 0.763, epoch: 17.0[0m
[32m[2022-09-16 21:06:00,982] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 21:06:00,983] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 21:06:00,983] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 21:06:00,983] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 21:06:00,983] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 21:06:02,342] [    INFO][0m - eval_loss: 1.5518534183502197, eval_accuracy: 0.6855345911949685, eval_runtime: 1.3589, eval_samples_per_second: 117.003, eval_steps_per_second: 7.359, epoch: 17.0[0m
[32m[2022-09-16 21:06:02,342] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-170[0m
[32m[2022-09-16 21:06:02,342] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 21:06:05,608] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-170/tokenizer_config.json[0m
[32m[2022-09-16 21:06:05,609] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-170/special_tokens_map.json[0m
[32m[2022-09-16 21:06:15,004] [    INFO][0m - loss: 0.09761272, learning_rate: 1.2000000000000002e-06, global_step: 180, interval_runtime: 17.1776, interval_samples_per_second: 0.931, interval_steps_per_second: 0.582, epoch: 18.0[0m
[32m[2022-09-16 21:06:15,005] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 21:06:15,005] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 21:06:15,005] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 21:06:15,005] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 21:06:15,005] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 21:06:16,360] [    INFO][0m - eval_loss: 1.762398600578308, eval_accuracy: 0.6729559748427673, eval_runtime: 1.3547, eval_samples_per_second: 117.368, eval_steps_per_second: 7.382, epoch: 18.0[0m
[32m[2022-09-16 21:06:16,694] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-180[0m
[32m[2022-09-16 21:06:16,695] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 21:06:19,354] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-180/tokenizer_config.json[0m
[32m[2022-09-16 21:06:19,354] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-180/special_tokens_map.json[0m
[32m[2022-09-16 21:06:28,747] [    INFO][0m - loss: 0.08687544, learning_rate: 1.0999999999999998e-06, global_step: 190, interval_runtime: 13.7429, interval_samples_per_second: 1.164, interval_steps_per_second: 0.728, epoch: 19.0[0m
[32m[2022-09-16 21:06:28,748] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 21:06:28,748] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 21:06:28,748] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 21:06:28,748] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 21:06:28,748] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 21:06:30,107] [    INFO][0m - eval_loss: 1.7920613288879395, eval_accuracy: 0.7044025157232704, eval_runtime: 1.3591, eval_samples_per_second: 116.988, eval_steps_per_second: 7.358, epoch: 19.0[0m
[32m[2022-09-16 21:06:30,108] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-190[0m
[32m[2022-09-16 21:06:30,108] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 21:06:32,974] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-190/tokenizer_config.json[0m
[32m[2022-09-16 21:06:32,974] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-190/special_tokens_map.json[0m
[32m[2022-09-16 21:06:41,481] [    INFO][0m - loss: 0.10312152, learning_rate: 1e-06, global_step: 200, interval_runtime: 12.734, interval_samples_per_second: 1.256, interval_steps_per_second: 0.785, epoch: 20.0[0m
[32m[2022-09-16 21:06:41,482] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 21:06:41,482] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 21:06:41,482] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 21:06:41,482] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 21:06:41,482] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 21:06:42,839] [    INFO][0m - eval_loss: 1.9018265008926392, eval_accuracy: 0.6981132075471698, eval_runtime: 1.3563, eval_samples_per_second: 117.232, eval_steps_per_second: 7.373, epoch: 20.0[0m
[32m[2022-09-16 21:06:42,895] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-200[0m
[32m[2022-09-16 21:06:42,896] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 21:06:45,688] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-200/tokenizer_config.json[0m
[32m[2022-09-16 21:06:45,688] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-200/special_tokens_map.json[0m
[32m[2022-09-16 21:06:54,931] [    INFO][0m - loss: 0.03164206, learning_rate: 9e-07, global_step: 210, interval_runtime: 13.4506, interval_samples_per_second: 1.19, interval_steps_per_second: 0.743, epoch: 21.0[0m
[32m[2022-09-16 21:06:54,932] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 21:06:54,932] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 21:06:54,933] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 21:06:54,933] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 21:06:54,933] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 21:06:56,292] [    INFO][0m - eval_loss: 2.08065128326416, eval_accuracy: 0.6918238993710691, eval_runtime: 1.3584, eval_samples_per_second: 117.053, eval_steps_per_second: 7.362, epoch: 21.0[0m
[32m[2022-09-16 21:06:56,292] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-210[0m
[32m[2022-09-16 21:06:56,292] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 21:06:59,190] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-210/tokenizer_config.json[0m
[32m[2022-09-16 21:06:59,191] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-210/special_tokens_map.json[0m
[32m[2022-09-16 21:07:07,876] [    INFO][0m - loss: 0.02448628, learning_rate: 8e-07, global_step: 220, interval_runtime: 12.944, interval_samples_per_second: 1.236, interval_steps_per_second: 0.773, epoch: 22.0[0m
[32m[2022-09-16 21:07:07,876] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 21:07:07,876] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 21:07:07,877] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 21:07:07,877] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 21:07:07,877] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 21:07:09,234] [    INFO][0m - eval_loss: 2.1320436000823975, eval_accuracy: 0.7169811320754716, eval_runtime: 1.3574, eval_samples_per_second: 117.135, eval_steps_per_second: 7.367, epoch: 22.0[0m
[32m[2022-09-16 21:07:09,235] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-220[0m
[32m[2022-09-16 21:07:09,235] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 21:07:11,987] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-220/tokenizer_config.json[0m
[32m[2022-09-16 21:07:11,988] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-220/special_tokens_map.json[0m
[32m[2022-09-16 21:07:22,665] [    INFO][0m - loss: 0.02732927, learning_rate: 7.000000000000001e-07, global_step: 230, interval_runtime: 12.7911, interval_samples_per_second: 1.251, interval_steps_per_second: 0.782, epoch: 23.0[0m
[32m[2022-09-16 21:07:22,666] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 21:07:22,666] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 21:07:22,666] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 21:07:22,667] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 21:07:22,667] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 21:07:24,005] [    INFO][0m - eval_loss: 2.2566051483154297, eval_accuracy: 0.710691823899371, eval_runtime: 1.3384, eval_samples_per_second: 118.796, eval_steps_per_second: 7.471, epoch: 23.0[0m
[32m[2022-09-16 21:07:24,006] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-230[0m
[32m[2022-09-16 21:07:24,006] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 21:07:26,710] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-230/tokenizer_config.json[0m
[32m[2022-09-16 21:07:26,710] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-230/special_tokens_map.json[0m
[32m[2022-09-16 21:07:35,706] [    INFO][0m - loss: 0.01397519, learning_rate: 6.000000000000001e-07, global_step: 240, interval_runtime: 15.0389, interval_samples_per_second: 1.064, interval_steps_per_second: 0.665, epoch: 24.0[0m
[32m[2022-09-16 21:07:35,706] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 21:07:35,707] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 21:07:35,707] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 21:07:35,707] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 21:07:35,707] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 21:07:37,049] [    INFO][0m - eval_loss: 2.3682541847229004, eval_accuracy: 0.710691823899371, eval_runtime: 1.3423, eval_samples_per_second: 118.454, eval_steps_per_second: 7.45, epoch: 24.0[0m
[32m[2022-09-16 21:07:37,050] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-240[0m
[32m[2022-09-16 21:07:37,050] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 21:07:39,615] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-240/tokenizer_config.json[0m
[32m[2022-09-16 21:07:39,616] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-240/special_tokens_map.json[0m
[32m[2022-09-16 21:07:48,506] [    INFO][0m - loss: 0.06414156, learning_rate: 5e-07, global_step: 250, interval_runtime: 12.8003, interval_samples_per_second: 1.25, interval_steps_per_second: 0.781, epoch: 25.0[0m
[32m[2022-09-16 21:07:48,507] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 21:07:48,507] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 21:07:48,507] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 21:07:48,507] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 21:07:48,507] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 21:07:49,868] [    INFO][0m - eval_loss: 2.3788251876831055, eval_accuracy: 0.7044025157232704, eval_runtime: 1.3605, eval_samples_per_second: 116.871, eval_steps_per_second: 7.35, epoch: 25.0[0m
[32m[2022-09-16 21:07:49,868] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-250[0m
[32m[2022-09-16 21:07:49,868] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 21:07:52,521] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-250/tokenizer_config.json[0m
[32m[2022-09-16 21:07:52,521] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-250/special_tokens_map.json[0m
[32m[2022-09-16 21:08:04,208] [    INFO][0m - loss: 0.01808413, learning_rate: 4e-07, global_step: 260, interval_runtime: 13.3647, interval_samples_per_second: 1.197, interval_steps_per_second: 0.748, epoch: 26.0[0m
[32m[2022-09-16 21:08:04,209] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 21:08:04,209] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 21:08:04,209] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 21:08:04,209] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 21:08:04,209] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 21:08:05,566] [    INFO][0m - eval_loss: 2.423177480697632, eval_accuracy: 0.7044025157232704, eval_runtime: 1.3564, eval_samples_per_second: 117.218, eval_steps_per_second: 7.372, epoch: 26.0[0m
[32m[2022-09-16 21:08:05,567] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-260[0m
[32m[2022-09-16 21:08:05,567] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 21:08:08,279] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-260/tokenizer_config.json[0m
[32m[2022-09-16 21:08:08,280] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-260/special_tokens_map.json[0m
[32m[2022-09-16 21:08:17,149] [    INFO][0m - loss: 0.04289488, learning_rate: 3.0000000000000004e-07, global_step: 270, interval_runtime: 15.278, interval_samples_per_second: 1.047, interval_steps_per_second: 0.655, epoch: 27.0[0m
[32m[2022-09-16 21:08:17,150] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 21:08:17,150] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 21:08:17,150] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 21:08:17,150] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 21:08:17,150] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 21:08:18,508] [    INFO][0m - eval_loss: 2.472243309020996, eval_accuracy: 0.7169811320754716, eval_runtime: 1.3576, eval_samples_per_second: 117.118, eval_steps_per_second: 7.366, epoch: 27.0[0m
[32m[2022-09-16 21:08:18,508] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-270[0m
[32m[2022-09-16 21:08:18,508] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 21:08:21,273] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-270/tokenizer_config.json[0m
[32m[2022-09-16 21:08:21,273] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-270/special_tokens_map.json[0m
[32m[2022-09-16 21:08:29,545] [    INFO][0m - loss: 0.00680439, learning_rate: 2e-07, global_step: 280, interval_runtime: 12.3967, interval_samples_per_second: 1.291, interval_steps_per_second: 0.807, epoch: 28.0[0m
[32m[2022-09-16 21:08:29,546] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 21:08:29,546] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 21:08:29,546] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 21:08:29,546] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 21:08:29,546] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 21:08:30,902] [    INFO][0m - eval_loss: 2.523050546646118, eval_accuracy: 0.710691823899371, eval_runtime: 1.3555, eval_samples_per_second: 117.298, eval_steps_per_second: 7.377, epoch: 28.0[0m
[32m[2022-09-16 21:08:30,903] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-280[0m
[32m[2022-09-16 21:08:30,903] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 21:08:33,463] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-280/tokenizer_config.json[0m
[32m[2022-09-16 21:08:33,463] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-280/special_tokens_map.json[0m
[32m[2022-09-16 21:08:42,635] [    INFO][0m - loss: 0.00259251, learning_rate: 1e-07, global_step: 290, interval_runtime: 13.0899, interval_samples_per_second: 1.222, interval_steps_per_second: 0.764, epoch: 29.0[0m
[32m[2022-09-16 21:08:42,636] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 21:08:42,636] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 21:08:42,636] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 21:08:42,636] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 21:08:42,637] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 21:08:43,997] [    INFO][0m - eval_loss: 2.5545599460601807, eval_accuracy: 0.710691823899371, eval_runtime: 1.3602, eval_samples_per_second: 116.891, eval_steps_per_second: 7.352, epoch: 29.0[0m
[32m[2022-09-16 21:08:43,997] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-290[0m
[32m[2022-09-16 21:08:43,997] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 21:08:46,774] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-290/tokenizer_config.json[0m
[32m[2022-09-16 21:08:46,774] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-290/special_tokens_map.json[0m
[32m[2022-09-16 21:08:55,117] [    INFO][0m - loss: 0.00454498, learning_rate: 0.0, global_step: 300, interval_runtime: 12.4752, interval_samples_per_second: 1.283, interval_steps_per_second: 0.802, epoch: 30.0[0m
[32m[2022-09-16 21:08:55,118] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 21:08:55,118] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 21:08:55,118] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 21:08:55,118] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 21:08:55,118] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 21:08:56,468] [    INFO][0m - eval_loss: 2.5632853507995605, eval_accuracy: 0.710691823899371, eval_runtime: 1.3496, eval_samples_per_second: 117.809, eval_steps_per_second: 7.409, epoch: 30.0[0m
[32m[2022-09-16 21:08:57,979] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-300[0m
[32m[2022-09-16 21:08:57,980] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 21:09:00,541] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-300/tokenizer_config.json[0m
[32m[2022-09-16 21:09:00,541] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-300/special_tokens_map.json[0m
[32m[2022-09-16 21:09:05,747] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-16 21:09:05,747] [    INFO][0m - Loading best model from ./checkpoints_cluewsc/checkpoint-220 (score: 0.7169811320754716).[0m
[32m[2022-09-16 21:09:07,275] [    INFO][0m - train_runtime: 423.1968, train_samples_per_second: 11.342, train_steps_per_second: 0.709, train_loss: 0.434182700800399, epoch: 30.0[0m
[32m[2022-09-16 21:09:07,329] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/[0m
[32m[2022-09-16 21:09:07,329] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 21:09:09,791] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/tokenizer_config.json[0m
[32m[2022-09-16 21:09:09,792] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/special_tokens_map.json[0m
[32m[2022-09-16 21:09:09,793] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-16 21:09:09,793] [    INFO][0m -   epoch                    =       30.0[0m
[32m[2022-09-16 21:09:09,793] [    INFO][0m -   train_loss               =     0.4342[0m
[32m[2022-09-16 21:09:09,794] [    INFO][0m -   train_runtime            = 0:07:03.19[0m
[32m[2022-09-16 21:09:09,794] [    INFO][0m -   train_samples_per_second =     11.342[0m
[32m[2022-09-16 21:09:09,794] [    INFO][0m -   train_steps_per_second   =      0.709[0m
[32m[2022-09-16 21:09:09,797] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-16 21:09:09,797] [    INFO][0m -   Num examples = 976[0m
[32m[2022-09-16 21:09:09,797] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 21:09:09,797] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 21:09:09,797] [    INFO][0m -   Total prediction steps = 61[0m
[32m[2022-09-16 21:09:18,386] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-16 21:09:18,386] [    INFO][0m -   test_accuracy           =     0.7213[0m
[32m[2022-09-16 21:09:18,386] [    INFO][0m -   test_loss               =     1.7352[0m
[32m[2022-09-16 21:09:18,386] [    INFO][0m -   test_runtime            = 0:00:08.58[0m
[32m[2022-09-16 21:09:18,386] [    INFO][0m -   test_samples_per_second =    113.636[0m
[32m[2022-09-16 21:09:18,386] [    INFO][0m -   test_steps_per_second   =      7.102[0m
[32m[2022-09-16 21:09:18,387] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-16 21:09:18,387] [    INFO][0m -   Num examples = 290[0m
[32m[2022-09-16 21:09:18,387] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 21:09:18,387] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 21:09:18,387] [    INFO][0m -   Total prediction steps = 19[0m
[32m[2022-09-16 21:09:21,131] [    INFO][0m - Predictions for cluewscf saved to ./fewclue_submit_examples.[0m
{
  "labels": 0,
  "text_a": "\u4e3a\u4ec0\u4e48\u8981\u51fa\u73b0\u4e00\u4e2a\u8eab\u7a7f\u519b\u88c5\u7684\u9ad8\u5927\u7537\u4eba\uff1f\u5c31\u50cf\u4e00\u7247\u6811\u53f6\u98d8\u5165\u4e86\u6811\u6797\uff0c\u4ed6\u8d70\u5230\u4e86\u6211\u7684\u5bb6\u4eba\u4e2d\u95f4\u3002",
  "text_b": "\u5176\u4e2d\u4ed6\u6307\u7684\u662f\u6811\u53f6"
}

