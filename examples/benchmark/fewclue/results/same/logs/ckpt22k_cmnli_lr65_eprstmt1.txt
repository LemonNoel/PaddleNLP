[32m[2022-09-17 17:39:56,161] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-17 17:39:56,161] [    INFO][0m - ============================================================[0m
[32m[2022-09-17 17:39:56,161] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-17 17:39:56,161] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-17 17:39:56,161] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-17 17:39:56,162] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-17 17:39:56,162] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-17 17:39:56,162] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-17 17:39:56,162] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-17 17:39:56,162] [    INFO][0m - [0m
[32m[2022-09-17 17:39:56,162] [    INFO][0m - ============================================================[0m
[32m[2022-09-17 17:39:56,162] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-17 17:39:56,162] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-17 17:39:56,162] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-17 17:39:56,162] [    INFO][0m - pretrained                    :../checkpoints_cmnli/checkpoint-22000/model_state.pdparams[0m
[32m[2022-09-17 17:39:56,162] [    INFO][0m - prompt                        :‚Äú{'text':'text_a'}‚ÄùËøôÊù°ËØÑËÆ∫ÁöÑÊÉÖÊÑüÂÄæÂêëÊòØ{'mask'}{'mask'}ÁöÑ„ÄÇ[0m
[32m[2022-09-17 17:39:56,162] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-17 17:39:56,162] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-17 17:39:56,163] [    INFO][0m - task_name                     :eprstmt[0m
[32m[2022-09-17 17:39:56,163] [    INFO][0m - [0m
[32m[2022-09-17 17:39:56,163] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0917 17:39:56.164331 24824 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0917 17:39:56.168378 24824 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-17 17:40:03,557] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-17 17:40:03,568] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-17 17:40:03,569] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-17 17:40:03,569] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùËøôÊù°ËØÑËÆ∫ÁöÑÊÉÖÊÑüÂÄæÂêëÊòØ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'ÁöÑ„ÄÇ'}][0m
[32m[2022-09-17 17:40:05,291] [    INFO][0m - ============================================================[0m
[32m[2022-09-17 17:40:05,291] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-17 17:40:05,291] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-17 17:40:05,291] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-17 17:40:05,292] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-17 17:40:05,292] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-17 17:40:05,292] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-17 17:40:05,292] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-17 17:40:05,292] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-17 17:40:05,292] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-17 17:40:05,292] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-17 17:40:05,292] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-17 17:40:05,292] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-17 17:40:05,292] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-17 17:40:05,292] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-17 17:40:05,292] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-17 17:40:05,292] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-17 17:40:05,293] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-17 17:40:05,293] [    INFO][0m - eval_batch_size               :4[0m
[32m[2022-09-17 17:40:05,293] [    INFO][0m - eval_steps                    :200[0m
[32m[2022-09-17 17:40:05,293] [    INFO][0m - evaluation_strategy           :IntervalStrategy.EPOCH[0m
[32m[2022-09-17 17:40:05,293] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-17 17:40:05,293] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-17 17:40:05,293] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-17 17:40:05,293] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-17 17:40:05,293] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-17 17:40:05,293] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-17 17:40:05,293] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-17 17:40:05,293] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-17 17:40:05,293] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-17 17:40:05,294] [    INFO][0m - learning_rate                 :3e-06[0m
[32m[2022-09-17 17:40:05,294] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-17 17:40:05,294] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-17 17:40:05,294] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-17 17:40:05,294] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-17 17:40:05,294] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-17 17:40:05,294] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-17 17:40:05,294] [    INFO][0m - logging_dir                   :./checkpoints_eprstmt/runs/Sep17_17-39-56_instance-3bwob41y-01[0m
[32m[2022-09-17 17:40:05,294] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-17 17:40:05,294] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-17 17:40:05,294] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-17 17:40:05,294] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-17 17:40:05,294] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-17 17:40:05,295] [    INFO][0m - max_seq_length                :128[0m
[32m[2022-09-17 17:40:05,295] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-17 17:40:05,295] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-17 17:40:05,295] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-17 17:40:05,295] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-17 17:40:05,295] [    INFO][0m - num_train_epochs              :10.0[0m
[32m[2022-09-17 17:40:05,295] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-17 17:40:05,295] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-17 17:40:05,295] [    INFO][0m - output_dir                    :./checkpoints_eprstmt/[0m
[32m[2022-09-17 17:40:05,295] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-17 17:40:05,295] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-17 17:40:05,295] [    INFO][0m - per_device_eval_batch_size    :4[0m
[32m[2022-09-17 17:40:05,295] [    INFO][0m - per_device_train_batch_size   :16[0m
[32m[2022-09-17 17:40:05,296] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-17 17:40:05,296] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-17 17:40:05,296] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-17 17:40:05,296] [    INFO][0m - ppt_learning_rate             :3e-05[0m
[32m[2022-09-17 17:40:05,296] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-17 17:40:05,296] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-17 17:40:05,296] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-17 17:40:05,296] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-17 17:40:05,296] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-17 17:40:05,296] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-17 17:40:05,296] [    INFO][0m - run_name                      :./checkpoints_eprstmt/[0m
[32m[2022-09-17 17:40:05,296] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-17 17:40:05,296] [    INFO][0m - save_steps                    :200[0m
[32m[2022-09-17 17:40:05,296] [    INFO][0m - save_strategy                 :IntervalStrategy.EPOCH[0m
[32m[2022-09-17 17:40:05,297] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-17 17:40:05,297] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-17 17:40:05,297] [    INFO][0m - seed                          :42[0m
[32m[2022-09-17 17:40:05,297] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-17 17:40:05,297] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-17 17:40:05,297] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-17 17:40:05,297] [    INFO][0m - train_batch_size              :16[0m
[32m[2022-09-17 17:40:05,297] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-17 17:40:05,297] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-17 17:40:05,297] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-17 17:40:05,297] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-17 17:40:05,297] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-17 17:40:05,297] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-17 17:40:05,298] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-17 17:40:05,298] [    INFO][0m - [0m
[32m[2022-09-17 17:40:05,300] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-17 17:40:05,300] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-17 17:40:05,300] [    INFO][0m -   Num Epochs = 10[0m
[32m[2022-09-17 17:40:05,300] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2022-09-17 17:40:05,301] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2022-09-17 17:40:05,301] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-17 17:40:05,301] [    INFO][0m -   Total optimization steps = 100.0[0m
[32m[2022-09-17 17:40:05,301] [    INFO][0m -   Total num train samples = 1600[0m
[32m[2022-09-17 17:40:09,473] [    INFO][0m - loss: 12.08986969, learning_rate: 2.7e-06, global_step: 10, interval_runtime: 4.1715, interval_samples_per_second: 3.836, interval_steps_per_second: 2.397, epoch: 1.0[0m
[32m[2022-09-17 17:40:09,474] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 17:40:09,474] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-17 17:40:09,474] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 17:40:09,474] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 17:40:09,474] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-17 17:40:11,216] [    INFO][0m - eval_loss: 9.336281776428223, eval_accuracy: 0.49375, eval_runtime: 1.7421, eval_samples_per_second: 91.841, eval_steps_per_second: 22.96, epoch: 1.0[0m
[32m[2022-09-17 17:40:11,217] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-10[0m
[32m[2022-09-17 17:40:11,217] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 17:40:14,078] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-10/tokenizer_config.json[0m
[32m[2022-09-17 17:40:14,078] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-10/special_tokens_map.json[0m
[32m[2022-09-17 17:40:22,960] [    INFO][0m - loss: 7.33835983, learning_rate: 2.4000000000000003e-06, global_step: 20, interval_runtime: 13.4864, interval_samples_per_second: 1.186, interval_steps_per_second: 0.741, epoch: 2.0[0m
[32m[2022-09-17 17:40:22,960] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 17:40:22,960] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-17 17:40:22,961] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 17:40:22,961] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 17:40:22,961] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-17 17:40:24,658] [    INFO][0m - eval_loss: 4.650309085845947, eval_accuracy: 0.51875, eval_runtime: 1.697, eval_samples_per_second: 94.287, eval_steps_per_second: 23.572, epoch: 2.0[0m
[32m[2022-09-17 17:40:24,658] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-20[0m
[32m[2022-09-17 17:40:24,658] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 17:40:27,440] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-20/tokenizer_config.json[0m
[32m[2022-09-17 17:40:27,440] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-20/special_tokens_map.json[0m
[32m[2022-09-17 17:40:36,350] [    INFO][0m - loss: 3.49603539, learning_rate: 2.1e-06, global_step: 30, interval_runtime: 13.3907, interval_samples_per_second: 1.195, interval_steps_per_second: 0.747, epoch: 3.0[0m
[32m[2022-09-17 17:40:36,351] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 17:40:36,351] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-17 17:40:36,351] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 17:40:36,351] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 17:40:36,352] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-17 17:40:38,034] [    INFO][0m - eval_loss: 2.032453775405884, eval_accuracy: 0.68125, eval_runtime: 1.682, eval_samples_per_second: 95.125, eval_steps_per_second: 23.781, epoch: 3.0[0m
[32m[2022-09-17 17:40:38,034] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-30[0m
[32m[2022-09-17 17:40:38,034] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 17:40:40,746] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-30/tokenizer_config.json[0m
[32m[2022-09-17 17:40:40,746] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-30/special_tokens_map.json[0m
[32m[2022-09-17 17:40:49,600] [    INFO][0m - loss: 1.58089809, learning_rate: 1.8e-06, global_step: 40, interval_runtime: 13.2504, interval_samples_per_second: 1.208, interval_steps_per_second: 0.755, epoch: 4.0[0m
[32m[2022-09-17 17:40:49,601] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 17:40:49,601] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-17 17:40:49,601] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 17:40:49,601] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 17:40:49,602] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-17 17:40:51,350] [    INFO][0m - eval_loss: 0.8565252423286438, eval_accuracy: 0.81875, eval_runtime: 1.7485, eval_samples_per_second: 91.505, eval_steps_per_second: 22.876, epoch: 4.0[0m
[32m[2022-09-17 17:40:51,351] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-40[0m
[32m[2022-09-17 17:40:51,351] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 17:40:54,141] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-40/tokenizer_config.json[0m
[32m[2022-09-17 17:40:54,142] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-40/special_tokens_map.json[0m
[32m[2022-09-17 17:41:05,762] [    INFO][0m - loss: 0.65978565, learning_rate: 1.5e-06, global_step: 50, interval_runtime: 16.1614, interval_samples_per_second: 0.99, interval_steps_per_second: 0.619, epoch: 5.0[0m
[32m[2022-09-17 17:41:05,763] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 17:41:05,763] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-17 17:41:05,763] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 17:41:05,763] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 17:41:05,763] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-17 17:41:07,287] [    INFO][0m - eval_loss: 0.38635191321372986, eval_accuracy: 0.88125, eval_runtime: 1.5228, eval_samples_per_second: 105.067, eval_steps_per_second: 26.267, epoch: 5.0[0m
[32m[2022-09-17 17:41:07,287] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-50[0m
[32m[2022-09-17 17:41:07,287] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 17:41:10,388] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-50/tokenizer_config.json[0m
[32m[2022-09-17 17:41:10,388] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-50/special_tokens_map.json[0m
[32m[2022-09-17 17:41:19,266] [    INFO][0m - loss: 0.36482115, learning_rate: 1.2000000000000002e-06, global_step: 60, interval_runtime: 13.5034, interval_samples_per_second: 1.185, interval_steps_per_second: 0.741, epoch: 6.0[0m
[32m[2022-09-17 17:41:19,266] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 17:41:19,266] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-17 17:41:19,267] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 17:41:19,267] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 17:41:19,267] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-17 17:41:20,787] [    INFO][0m - eval_loss: 0.30622929334640503, eval_accuracy: 0.88125, eval_runtime: 1.5205, eval_samples_per_second: 105.23, eval_steps_per_second: 26.308, epoch: 6.0[0m
[32m[2022-09-17 17:41:20,788] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-60[0m
[32m[2022-09-17 17:41:20,788] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 17:41:23,607] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-60/tokenizer_config.json[0m
[32m[2022-09-17 17:41:23,608] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-60/special_tokens_map.json[0m
[32m[2022-09-17 17:41:32,299] [    INFO][0m - loss: 0.25691292, learning_rate: 9e-07, global_step: 70, interval_runtime: 13.0338, interval_samples_per_second: 1.228, interval_steps_per_second: 0.767, epoch: 7.0[0m
[32m[2022-09-17 17:41:32,300] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 17:41:32,301] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-17 17:41:32,301] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 17:41:32,301] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 17:41:32,301] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-17 17:41:33,818] [    INFO][0m - eval_loss: 0.28788429498672485, eval_accuracy: 0.8875, eval_runtime: 1.5175, eval_samples_per_second: 105.434, eval_steps_per_second: 26.359, epoch: 7.0[0m
[32m[2022-09-17 17:41:33,819] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-70[0m
[32m[2022-09-17 17:41:33,819] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 17:41:36,626] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-70/tokenizer_config.json[0m
[32m[2022-09-17 17:41:36,626] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-70/special_tokens_map.json[0m
[32m[2022-09-17 17:41:45,171] [    INFO][0m - loss: 0.19861311, learning_rate: 6.000000000000001e-07, global_step: 80, interval_runtime: 12.8715, interval_samples_per_second: 1.243, interval_steps_per_second: 0.777, epoch: 8.0[0m
[32m[2022-09-17 17:41:45,171] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 17:41:45,171] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-17 17:41:45,171] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 17:41:45,171] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 17:41:45,172] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-17 17:41:46,683] [    INFO][0m - eval_loss: 0.28926029801368713, eval_accuracy: 0.89375, eval_runtime: 1.5108, eval_samples_per_second: 105.907, eval_steps_per_second: 26.477, epoch: 8.0[0m
[32m[2022-09-17 17:41:46,683] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-80[0m
[32m[2022-09-17 17:41:46,683] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 17:41:49,393] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-80/tokenizer_config.json[0m
[32m[2022-09-17 17:41:49,393] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-80/special_tokens_map.json[0m
[32m[2022-09-17 17:41:58,083] [    INFO][0m - loss: 0.19638801, learning_rate: 3.0000000000000004e-07, global_step: 90, interval_runtime: 12.9127, interval_samples_per_second: 1.239, interval_steps_per_second: 0.774, epoch: 9.0[0m
[32m[2022-09-17 17:41:58,084] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 17:41:58,084] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-17 17:41:58,084] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 17:41:58,084] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 17:41:58,084] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-17 17:41:59,595] [    INFO][0m - eval_loss: 0.28820738196372986, eval_accuracy: 0.89375, eval_runtime: 1.5107, eval_samples_per_second: 105.911, eval_steps_per_second: 26.478, epoch: 9.0[0m
[32m[2022-09-17 17:42:01,450] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-90[0m
[32m[2022-09-17 17:42:01,450] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 17:42:04,311] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-90/tokenizer_config.json[0m
[32m[2022-09-17 17:42:04,311] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-90/special_tokens_map.json[0m
[32m[2022-09-17 17:42:13,543] [    INFO][0m - loss: 0.16730989, learning_rate: 0.0, global_step: 100, interval_runtime: 15.4594, interval_samples_per_second: 1.035, interval_steps_per_second: 0.647, epoch: 10.0[0m
[32m[2022-09-17 17:42:13,544] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 17:42:13,544] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-17 17:42:13,544] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 17:42:13,544] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 17:42:13,544] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-17 17:42:15,078] [    INFO][0m - eval_loss: 0.2890450954437256, eval_accuracy: 0.89375, eval_runtime: 1.5337, eval_samples_per_second: 104.324, eval_steps_per_second: 26.081, epoch: 10.0[0m
[32m[2022-09-17 17:42:15,078] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-100[0m
[32m[2022-09-17 17:42:15,078] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 17:42:17,976] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-17 17:42:17,976] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-17 17:42:23,061] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-17 17:42:23,061] [    INFO][0m - Loading best model from ./checkpoints_eprstmt/checkpoint-80 (score: 0.89375).[0m
[32m[2022-09-17 17:42:24,644] [    INFO][0m - train_runtime: 139.342, train_samples_per_second: 11.483, train_steps_per_second: 0.718, train_loss: 2.6348993730545045, epoch: 10.0[0m
[32m[2022-09-17 17:42:24,699] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/[0m
[32m[2022-09-17 17:42:24,699] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 17:42:27,357] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/tokenizer_config.json[0m
[32m[2022-09-17 17:42:27,358] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/special_tokens_map.json[0m
[32m[2022-09-17 17:42:27,359] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-17 17:42:27,359] [    INFO][0m -   epoch                    =       10.0[0m
[32m[2022-09-17 17:42:27,359] [    INFO][0m -   train_loss               =     2.6349[0m
[32m[2022-09-17 17:42:27,359] [    INFO][0m -   train_runtime            = 0:02:19.34[0m
[32m[2022-09-17 17:42:27,360] [    INFO][0m -   train_samples_per_second =     11.483[0m
[32m[2022-09-17 17:42:27,360] [    INFO][0m -   train_steps_per_second   =      0.718[0m
[32m[2022-09-17 17:42:27,361] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-17 17:42:27,361] [    INFO][0m -   Num examples = 610[0m
[32m[2022-09-17 17:42:27,362] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 17:42:27,362] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 17:42:27,362] [    INFO][0m -   Total prediction steps = 153[0m
[32m[2022-09-17 17:42:33,369] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-17 17:42:33,370] [    INFO][0m -   test_accuracy           =     0.9115[0m
[32m[2022-09-17 17:42:33,370] [    INFO][0m -   test_loss               =     0.2568[0m
[32m[2022-09-17 17:42:33,370] [    INFO][0m -   test_runtime            = 0:00:06.00[0m
[32m[2022-09-17 17:42:33,370] [    INFO][0m -   test_samples_per_second =    101.534[0m
[32m[2022-09-17 17:42:33,370] [    INFO][0m -   test_steps_per_second   =     25.467[0m
[32m[2022-09-17 17:42:33,370] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-17 17:42:33,370] [    INFO][0m -   Num examples = 753[0m
[32m[2022-09-17 17:42:33,370] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 17:42:33,371] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 17:42:33,371] [    INFO][0m -   Total prediction steps = 189[0m
terminate called after throwing an instance of 'paddle::memory::allocation::BadAlloc'
  what():  

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   full_dygraph_function(paddle::experimental::IntArrayBase<paddle::experimental::Tensor>, paddle::experimental::ScalarBase<paddle::experimental::Tensor>, paddle::experimental::DataType, phi::Place)
1   paddle::experimental::full(paddle::experimental::IntArrayBase<paddle::experimental::Tensor> const&, paddle::experimental::ScalarBase<paddle::experimental::Tensor> const&, paddle::experimental::DataType, phi::Place const&)
2   void phi::FullKernel<float, phi::GPUContext>(phi::GPUContext const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, paddle::experimental::ScalarBase<phi::DenseTensor> const&, paddle::experimental::DataType, phi::DenseTensor*)
3   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
4   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, paddle::experimental::DataType, unsigned long, bool) const
5   phi::DenseTensor::AllocateFrom(phi::Allocator*, paddle::experimental::DataType, unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  phi::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 910.073519GB memory on GPU 0, 5.660889GB memory has been allocated and available memory is only 26.087646GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
If the above ways do not solve the out of memory problem, you can try to use CUDA managed memory. The command is `export FLAGS_use_cuda_managed_memory=false`.
 (at /paddle/paddle/fluid/memory/allocation/cuda_allocator.cc:95)

run.sh: line 65: 24824 Aborted                 CUDA_VISIBLE_DEVICES=$device python train_single.py --output_dir ./checkpoints_$task_name/ --prompt "$prompt" --max_seq_length $max_length --learning_rate 3e-6 --ppt_learning_rate 3e-5 --num_train_epochs 10 --logging_steps 10 --do_save True --do_test --eval_steps 200 --save_steps 200 --per_device_eval_batch_size 4 --per_device_train_batch_size 16 --model_name_or_path ernie-1.0-large-zh-cw --split_id few_all --task_name $task_name --metric_for_best_model accuracy --disable_tqdm True --do_train --do_predict --do_eval --load_best_model_at_end --pretrained "../checkpoints_cmnli/checkpoint-22000/model_state.pdparams" --evaluation_strategy epoch --save_strategy epoch
