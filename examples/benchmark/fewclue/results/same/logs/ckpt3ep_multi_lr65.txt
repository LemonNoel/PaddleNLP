 
==========
eprstmt
==========
 
[32m[2022-09-19 15:20:28,434] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-19 15:20:28,434] [    INFO][0m - ============================================================[0m
[32m[2022-09-19 15:20:28,434] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-19 15:20:28,435] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-19 15:20:28,435] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-19 15:20:28,435] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-19 15:20:28,435] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-19 15:20:28,435] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-19 15:20:28,435] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-19 15:20:28,435] [    INFO][0m - [0m
[32m[2022-09-19 15:20:28,435] [    INFO][0m - ============================================================[0m
[32m[2022-09-19 15:20:28,435] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-19 15:20:28,435] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-19 15:20:28,435] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-19 15:20:28,435] [    INFO][0m - pretrained                    :/ssd2/wanghuijuan03/data/zero-shot/0919_model_state.pdparams[0m
[32m[2022-09-19 15:20:28,435] [    INFO][0m - prompt                        :â€œ{'text':'text_a'}â€è¿™æ¡è¯„è®ºçš„æƒ…æ„Ÿå€¾å‘æ˜¯{'mask'}{'mask'}çš„ã€‚[0m
[32m[2022-09-19 15:20:28,436] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-19 15:20:28,436] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-19 15:20:28,436] [    INFO][0m - task_name                     :eprstmt[0m
[32m[2022-09-19 15:20:28,436] [    INFO][0m - [0m
[32m[2022-09-19 15:20:28,436] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0919 15:20:28.437604 53022 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0919 15:20:28.441994 53022 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-19 15:20:33,650] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-19 15:20:33,662] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-19 15:20:33,662] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-19 15:20:33,663] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': 'â€œ'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': 'â€è¿™æ¡è¯„è®ºçš„æƒ…æ„Ÿå€¾å‘æ˜¯'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'çš„ã€‚'}][0m
[32m[2022-09-19 15:20:35,246] [    INFO][0m - ============================================================[0m
[32m[2022-09-19 15:20:35,247] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-19 15:20:35,247] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-19 15:20:35,247] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-19 15:20:35,247] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-19 15:20:35,247] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-19 15:20:35,247] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-19 15:20:35,247] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-19 15:20:35,247] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-19 15:20:35,248] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-19 15:20:35,248] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-19 15:20:35,248] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-19 15:20:35,248] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-19 15:20:35,248] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-19 15:20:35,248] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-19 15:20:35,248] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-19 15:20:35,248] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-19 15:20:35,248] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-19 15:20:35,248] [    INFO][0m - eval_batch_size               :16[0m
[32m[2022-09-19 15:20:35,248] [    INFO][0m - eval_steps                    :200[0m
[32m[2022-09-19 15:20:35,249] [    INFO][0m - evaluation_strategy           :IntervalStrategy.EPOCH[0m
[32m[2022-09-19 15:20:35,249] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-19 15:20:35,249] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-19 15:20:35,249] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-19 15:20:35,249] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-19 15:20:35,249] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-19 15:20:35,249] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-19 15:20:35,249] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-19 15:20:35,249] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-19 15:20:35,249] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-19 15:20:35,249] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-09-19 15:20:35,249] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-19 15:20:35,249] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-19 15:20:35,249] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-19 15:20:35,250] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-19 15:20:35,250] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-19 15:20:35,250] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-19 15:20:35,250] [    INFO][0m - logging_dir                   :./checkpoints_eprstmt/runs/Sep19_15-20-28_instance-3bwob41y-01[0m
[32m[2022-09-19 15:20:35,250] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-19 15:20:35,250] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-19 15:20:35,250] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-19 15:20:35,250] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-19 15:20:35,250] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-19 15:20:35,250] [    INFO][0m - max_seq_length                :128[0m
[32m[2022-09-19 15:20:35,250] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-19 15:20:35,250] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-19 15:20:35,250] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-19 15:20:35,251] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-19 15:20:35,251] [    INFO][0m - num_train_epochs              :20.0[0m
[32m[2022-09-19 15:20:35,251] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-19 15:20:35,251] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-19 15:20:35,251] [    INFO][0m - output_dir                    :./checkpoints_eprstmt/[0m
[32m[2022-09-19 15:20:35,251] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-19 15:20:35,251] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-19 15:20:35,251] [    INFO][0m - per_device_eval_batch_size    :16[0m
[32m[2022-09-19 15:20:35,251] [    INFO][0m - per_device_train_batch_size   :16[0m
[32m[2022-09-19 15:20:35,251] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-19 15:20:35,251] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-19 15:20:35,251] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-19 15:20:35,251] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-09-19 15:20:35,252] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-19 15:20:35,252] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-19 15:20:35,252] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-19 15:20:35,252] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-19 15:20:35,252] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-19 15:20:35,252] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-19 15:20:35,252] [    INFO][0m - run_name                      :./checkpoints_eprstmt/[0m
[32m[2022-09-19 15:20:35,252] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-19 15:20:35,252] [    INFO][0m - save_steps                    :200[0m
[32m[2022-09-19 15:20:35,252] [    INFO][0m - save_strategy                 :IntervalStrategy.EPOCH[0m
[32m[2022-09-19 15:20:35,252] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-19 15:20:35,252] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-19 15:20:35,252] [    INFO][0m - seed                          :42[0m
[32m[2022-09-19 15:20:35,252] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-19 15:20:35,253] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-19 15:20:35,253] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-19 15:20:35,253] [    INFO][0m - train_batch_size              :16[0m
[32m[2022-09-19 15:20:35,253] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-19 15:20:35,253] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-19 15:20:35,253] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-19 15:20:35,253] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-19 15:20:35,253] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-19 15:20:35,253] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-19 15:20:35,253] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-19 15:20:35,253] [    INFO][0m - [0m
[32m[2022-09-19 15:20:35,257] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-19 15:20:35,257] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 15:20:35,257] [    INFO][0m -   Num Epochs = 20[0m
[32m[2022-09-19 15:20:35,257] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2022-09-19 15:20:35,257] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2022-09-19 15:20:35,257] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-19 15:20:35,257] [    INFO][0m -   Total optimization steps = 200.0[0m
[32m[2022-09-19 15:20:35,257] [    INFO][0m -   Total num train samples = 3200[0m
[32m[2022-09-19 15:20:39,811] [    INFO][0m - loss: 1.57883654, learning_rate: 2.8499999999999998e-05, global_step: 10, interval_runtime: 4.552, interval_samples_per_second: 3.515, interval_steps_per_second: 2.197, epoch: 1.0[0m
[32m[2022-09-19 15:20:39,812] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 15:20:39,812] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 15:20:39,812] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 15:20:39,812] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 15:20:39,812] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 15:20:41,174] [    INFO][0m - eval_loss: 1.101251244544983, eval_accuracy: 0.5125, eval_runtime: 1.362, eval_samples_per_second: 117.47, eval_steps_per_second: 7.342, epoch: 1.0[0m
[32m[2022-09-19 15:20:41,186] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-10[0m
[32m[2022-09-19 15:20:41,187] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 15:20:44,339] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-10/tokenizer_config.json[0m
[32m[2022-09-19 15:20:44,339] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-10/special_tokens_map.json[0m
[32m[2022-09-19 15:20:53,422] [    INFO][0m - loss: 0.53025222, learning_rate: 2.7000000000000002e-05, global_step: 20, interval_runtime: 13.6116, interval_samples_per_secondrun.sh: line 65: 51473 Killed                  CUDA_VISIBLE_DEVICES=$device python train_single.py --output_dir ./checkpoints_$task_name/ --prompt "$prompt" --max_seq_length $max_length --learning_rate 3e-5 --ppt_learning_rate 3e-4 --num_train_epochs 20 --logging_steps 10 --do_train --do_eval --do_test --disable_tqdm True --do_save True --eval_steps 200 --save_steps 200 --per_device_eval_batch_size 16 --per_device_train_batch_size 16 --model_name_or_path ernie-1.0-large-zh-cw --split_id few_all --task_name $task_name --metric_for_best_model accuracy --do_predict --load_best_model_at_end --pretrained "/ssd2/wanghuijuan03/data/zero-shot/0919_model_state.pdparams" --evaluation_strategy epoch --save_strategy epoch
rm: cannot remove â€˜./checkpoints_eprstmt/â€™: Directory not empty
 
==========
csldcp
==========
 
[32m[2022-09-19 15:23:19,051] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-19 15:23:19,052] [    INFO][0m - ============================================================[0m
[32m[2022-09-19 15:23:19,052] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-19 15:23:19,052] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-19 15:23:19,052] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-19 15:23:19,052] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-19 15:23:19,052] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-19 15:23:19,052] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-19 15:23:19,052] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-19 15:23:19,052] [    INFO][0m - [0m
[32m[2022-09-19 15:23:19,052] [    INFO][0m - ============================================================[0m
[32m[2022-09-19 15:23:19,052] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-19 15:23:19,052] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-19 15:23:19,053] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-19 15:23:19,053] [    INFO][0m - pretrained                    :/ssd2/wanghuijuan03/data/zero-shot/0919_model_state.pdparams[0m
[32m[2022-09-19 15:23:19,053] [    INFO][0m - prompt                        :â€œ{'text':'text_a'}â€è¿™ç¯‡æ–‡çŒ®çš„ç±»åˆ«æ˜¯{'mask'}{'mask'}ã€‚[0m
[32m[2022-09-19 15:23:19,053] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-19 15:23:19,053] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-19 15:23:19,053] [    INFO][0m - task_name                     :csldcp[0m
[32m[2022-09-19 15:23:19,053] [    INFO][0m - [0m
[32m[2022-09-19 15:23:19,053] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0919 15:23:19.055228 55906 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0919 15:23:19.059288 55906 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-19 15:23:25,807] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-19 15:23:25,818] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-19 15:23:25,819] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-19 15:23:25,819] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': 'â€œ'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': 'â€è¿™ç¯‡æ–‡çŒ®çš„ç±»åˆ«æ˜¯'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'ã€‚'}][0m
[32m[2022-09-19 15:23:27,448] [    INFO][0m - ============================================================[0m
[32m[2022-09-19 15:23:27,448] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-19 15:23:27,448] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-19 15:23:27,449] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-19 15:23:27,449] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-19 15:23:27,449] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-19 15:23:27,449] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-19 15:23:27,449] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-19 15:23:27,449] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-19 15:23:27,449] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-19 15:23:27,449] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-19 15:23:27,449] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-19 15:23:27,449] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-19 15:23:27,450] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-19 15:23:27,450] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-19 15:23:27,450] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-19 15:23:27,450] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-19 15:23:27,450] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-19 15:23:27,450] [    INFO][0m - eval_batch_size               :16[0m
[32m[2022-09-19 15:23:27,450] [    INFO][0m - eval_steps                    :200[0m
[32m[2022-09-19 15:23:27,450] [    INFO][0m - evaluation_strategy           :IntervalStrategy.EPOCH[0m
[32m[2022-09-19 15:23:27,450] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-19 15:23:27,450] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-19 15:23:27,450] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-19 15:23:27,450] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-19 15:23:27,450] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-19 15:23:27,450] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-19 15:23:27,451] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-19 15:23:27,451] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-19 15:23:27,451] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-19 15:23:27,451] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-09-19 15:23:27,451] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-19 15:23:27,451] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-19 15:23:27,451] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-19 15:23:27,451] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-19 15:23:27,451] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-19 15:23:27,451] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-19 15:23:27,451] [    INFO][0m - logging_dir                   :./checkpoints_csldcp/runs/Sep19_15-23-19_instance-3bwob41y-01[0m
[32m[2022-09-19 15:23:27,451] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-19 15:23:27,451] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-19 15:23:27,452] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-19 15:23:27,452] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-19 15:23:27,452] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-19 15:23:27,452] [    INFO][0m - max_seq_length                :256[0m
[32m[2022-09-19 15:23:27,452] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-19 15:23:27,452] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-19 15:23:27,452] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-19 15:23:27,452] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-19 15:23:27,452] [    INFO][0m - num_train_epochs              :20.0[0m
[32m[2022-09-19 15:23:27,452] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-19 15:23:27,452] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-19 15:23:27,452] [    INFO][0m - output_dir                    :./checkpoints_csldcp/[0m
[32m[2022-09-19 15:23:27,452] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-19 15:23:27,452] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-19 15:23:27,453] [    INFO][0m - per_device_eval_batch_size    :16[0m
[32m[2022-09-19 15:23:27,453] [    INFO][0m - per_device_train_batch_size   :16[0m
[32m[2022-09-19 15:23:27,453] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-19 15:23:27,453] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-19 15:23:27,453] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-19 15:23:27,453] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-09-19 15:23:27,453] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-19 15:23:27,453] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-19 15:23:27,453] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-19 15:23:27,453] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-19 15:23:27,453] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-19 15:23:27,453] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-19 15:23:27,453] [    INFO][0m - run_name                      :./checkpoints_csldcp/[0m
[32m[2022-09-19 15:23:27,454] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-19 15:23:27,454] [    INFO][0m - save_steps                    :200[0m
[32m[2022-09-19 15:23:27,454] [    INFO][0m - save_strategy                 :IntervalStrategy.EPOCH[0m
[32m[2022-09-19 15:23:27,454] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-19 15:23:27,454] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-19 15:23:27,454] [    INFO][0m - seed                          :42[0m
[32m[2022-09-19 15:23:27,454] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-19 15:23:27,454] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-19 15:23:27,454] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-19 15:23:27,454] [    INFO][0m - train_batch_size              :16[0m
[32m[2022-09-19 15:23:27,454] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-19 15:23:27,454] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-19 15:23:27,454] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-19 15:23:27,454] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-19 15:23:27,455] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-19 15:23:27,455] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-19 15:23:27,455] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-19 15:23:27,455] [    INFO][0m - [0m
[32m[2022-09-19 15:23:27,457] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-19 15:23:27,457] [    INFO][0m -   Num examples = 2036[0m
[32m[2022-09-19 15:23:27,458] [    INFO][0m -   Num Epochs = 20[0m
[32m[2022-09-19 15:23:27,458] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2022-09-19 15:23:27,458] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2022-09-19 15:23:27,458] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-19 15:23:27,458] [    INFO][0m -   Total optimization steps = 2560.0[0m
[32m[2022-09-19 15:23:27,458] [    INFO][0m -   Total num train samples = 40720[0m
[33m[2022-09-19 15:23:27,494] [ WARNING][0m - Token indices sequence length is longer than the specified maximum sequence length for this model (517 > 512). Running this sequence through the model will result in indexing errors[0m
[32m[2022-09-19 15:23:37,385] [    INFO][0m - loss: 2.94388885, learning_rate: 2.9882812500000002e-05, global_step: 10, interval_runtime: 9.9253, interval_samples_per_second: 1.612, interval_steps_per_second: 1.008, epoch: 0.0781[0m
[32m[2022-09-19 15:23:46,991] [    INFO][0m - loss: 2.45109749, learning_rate: 2.9765625e-05, global_step: 20, interval_runtime: 9.6069, interval_samples_per_second: 1.665, interval_steps_per_second: 1.041, epoch: 0.1562[0m
[32m[2022-09-19 15:23:55,684] [    INFO][0m - loss: 1.86716557, learning_rate: 2.96484375e-05, global_step: 30, interval_runtime: 8.6926, interval_samples_per_second: 1.841, interval_steps_per_second: 1.15, epoch: 0.2344[0m
[32m[2022-09-19 15:24:04,584] [    INFO][0m - loss: 1.89052887, learning_rate: 2.953125e-05, global_step: 40, interval_runtime: 8.8996, interval_samples_per_second: 1.798, interval_steps_per_second: 1.124, epoch: 0.3125[0m
[32m[2022-09-19 15:24:11,894] [    INFO][0m - loss: 1.83946114, learning_rate: 2.94140625e-05, global_step: 50, interval_runtime: 7.3109, interval_samples_per_second: 2.189, interval_steps_per_second: 1.368, epoch: 0.3906[0m
[32m[2022-09-19 15:24:22,416] [    INFO][0m - loss: 1.97997341, learning_rate: 2.9296875000000002e-05, global_step: 60, interval_runtime: 10.5224, interval_samples_per_second: 1.521, interval_steps_per_second: 0.95, epoch: 0.4688[0m
[32m[2022-09-19 15:24:30,339] [    INFO][0m - loss: 1.76617279, learning_rate: 2.91796875e-05, global_step: 70, interval_runtime: 7.9219, interval_samples_per_second: 2.02, interval_steps_per_second: 1.262, epoch: 0.5469[0m
[32m[2022-09-19 15:24:40,278] [    INFO][0m - loss: 1.6187748, learning_rate: 2.90625e-05, global_step: 80, interval_runtime: 9.9396, interval_samples_per_second: 1.61, interval_steps_per_second: 1.006, epoch: 0.625[0m
[32m[2022-09-19 15:24:48,989] [    INFO][0m - loss: 1.54548531, learning_rate: 2.89453125e-05, global_step: 90, interval_runtime: 8.7114, interval_samples_per_second: 1.837, interval_steps_per_second: 1.148, epoch: 0.7031[0m
[32m[2022-09-19 15:24:58,135] [    INFO][0m - loss: 1.76023674, learning_rate: 2.8828125e-05, global_step: 100, interval_runtime: 9.1456, interval_samples_per_second: 1.749, interval_steps_per_second: 1.093, epoch: 0.7812[0m
[32m[2022-09-19 15:25:07,594] [    INFO][0m - loss: 1.58892946, learning_rate: 2.87109375e-05, global_step: 110, interval_runtime: 9.3606, interval_samples_per_second: 1.709, interval_steps_per_second: 1.068, epoch: 0.8594[0m
[32m[2022-09-19 15:25:16,447] [    INFO][0m - loss: 1.72972336, learning_rate: 2.859375e-05, global_step: 120, interval_runtime: 8.9509, interval_samples_per_second: 1.788, interval_steps_per_second: 1.117, epoch: 0.9375[0m
[32m[2022-09-19 15:25:22,868] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 15:25:23,148] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-19 15:25:23,148] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 15:25:23,148] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 15:25:23,148] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-19 15:26:08,979] [    INFO][0m - eval_loss: 1.5033096075057983, eval_accuracy: 0.5043520309477756, eval_runtime: 46.1099, eval_samples_per_second: 44.849, eval_steps_per_second: 2.819, epoch: 1.0[0m
[32m[2022-09-19 15:26:09,025] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-128[0m
[32m[2022-09-19 15:26:09,025] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 15:26:12,138] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-128/tokenizer_config.json[0m
[32m[2022-09-19 15:26:12,139] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-128/special_tokens_map.json[0m
[32m[2022-09-19 15:26:19,687] [    INFO][0m - loss: 1.45177956, learning_rate: 2.8476562500000002e-05, global_step: 130, interval_runtime: 63.2403, interval_samples_per_second: 0.253, interval_steps_per_second: 0.158, epoch: 1.0156[0m
[32m[2022-09-19 15:26:26,380] [    INFO][0m - loss: 0.89233313, learning_rate: 2.8359375e-05, global_step: 140, interval_runtime: 6.6926, interval_samples_per_second: 2.391, interval_steps_per_second: 1.494, epoch: 1.0938[0m
[32m[2022-09-19 15:26:33,432] [    INFO][0m - loss: 0.90623064, learning_rate: 2.82421875e-05, global_step: 150, interval_runtime: 7.0521, interval_samples_per_second: 2.269, interval_steps_per_second: 1.418, epoch: 1.1719[0m
[32m[2022-09-19 15:26:44,842] [    INFO][0m - loss: 1.09057255, learning_rate: 2.8125e-05, global_step: 160, interval_runtime: 11.41, interval_samples_per_second: 1.402, interval_steps_per_second: 0.876, epoch: 1.25[0m
[32m[2022-09-19 15:27:00,150] [    INFO][0m - loss: 0.96141224, learning_rate: 2.80078125e-05, global_step: 170, interval_runtime: 15.3083, interval_samples_per_second: 1.045, interval_steps_per_second: 0.653, epoch: 1.3281[0m
[32m[2022-09-19 15:27:14,372] [    INFO][0m - loss: 1.16814327, learning_rate: 2.7890625000000002e-05, global_step: 180, interval_runtime: 14.2224, interval_samples_per_second: 1.125, interval_steps_per_second: 0.703, epoch: 1.4062[0m
[32m[2022-09-19 15:27:28,556] [    INFO][0m - loss: 0.83399153, learning_rate: 2.77734375e-05, global_step: 190, interval_runtime: 14.1841, interval_samples_per_second: 1.128, interval_steps_per_second: 0.705, epoch: 1.4844[0m
[32m[2022-09-19 15:27:42,800] [    INFO][0m - loss: 0.94231787, learning_rate: 2.765625e-05, global_step: 200, interval_runtime: 14.2442, interval_samples_per_second: 1.123, interval_steps_per_second: 0.702, epoch: 1.5625[0m
[32m[2022-09-19 15:27:57,059] [    INFO][0m - loss: 0.99274826, learning_rate: 2.75390625e-05, global_step: 210, interval_runtime: 14.2584, interval_samples_per_second: 1.122, interval_steps_per_second: 0.701, epoch: 1.6406[0m
[32m[2022-09-19 15:28:11,291] [    INFO][0m - loss: 0.86562443, learning_rate: 2.7421875e-05, global_step: 220, interval_runtime: 14.2321, interval_samples_per_second: 1.124, interval_steps_per_second: 0.703, epoch: 1.7188[0m
[32m[2022-09-19 15:28:25,513] [    INFO][0m - loss: 0.90648098, learning_rate: 2.7304687500000002e-05, global_step: 230, interval_runtime: 14.2227, interval_samples_per_second: 1.125, interval_steps_per_second: 0.703, epoch: 1.7969[0m
[32m[2022-09-19 15:28:39,796] [    INFO][0m - loss: 1.03647738, learning_rate: 2.71875e-05, global_step: 240, interval_runtime: 14.2825, interval_samples_per_second: 1.12, interval_steps_per_second: 0.7, epoch: 1.875[0m
[32m[2022-09-19 15:28:53,969] [    INFO][0m - loss: 1.00003033, learning_rate: 2.70703125e-05, global_step: 250, interval_runtime: 14.1725, interval_samples_per_second: 1.129, interval_steps_per_second: 0.706, epoch: 1.9531[0m
[32m[2022-09-19 15:29:01,400] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 15:29:01,400] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-19 15:29:01,401] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 15:29:01,401] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 15:29:01,401] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-19 15:30:10,728] [    INFO][0m - eval_loss: 1.388042688369751, eval_accuracy: 0.5372340425531915, eval_runtime: 69.3268, eval_samples_per_second: 29.83, eval_steps_per_second: 1.875, epoch: 2.0[0m
[32m[2022-09-19 15:30:10,755] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-256[0m
[32m[2022-09-19 15:30:10,755] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 15:30:13,453] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-256/tokenizer_config.json[0m
[32m[2022-09-19 15:30:13,454] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-256/special_tokens_map.json[0m
[32m[2022-09-19 15:30:24,004] [    INFO][0m - loss: 0.76310825, learning_rate: 2.6953125e-05, global_step: 260, interval_runtime: 90.0352, interval_samples_per_second: 0.178, interval_steps_per_second: 0.111, epoch: 2.0312[0m
[32m[2022-09-19 15:30:33,603] [    INFO][0m - loss: 0.49285269, learning_rate: 2.68359375e-05, global_step: 270, interval_runtime: 9.5987, interval_samples_per_second: 1.667, interval_steps_per_second: 1.042, epoch: 2.1094[0m
[32m[2022-09-19 15:30:40,136] [    INFO][0m - loss: 0.48902369, learning_rate: 2.6718750000000002e-05, global_step: 280, interval_runtime: 6.5334, interval_samples_per_second: 2.449, interval_steps_per_second: 1.531, epoch: 2.1875[0m
[32m[2022-09-19 15:30:46,653] [    INFO][0m - loss: 0.42868729, learning_rate: 2.66015625e-05, global_step: 290, interval_runtime: 6.5168, interval_samples_per_second: 2.455, interval_steps_per_second: 1.534, epoch: 2.2656[0m
[32m[2022-09-19 15:30:55,008] [    INFO][0m - loss: 0.51435881, learning_rate: 2.6484375000000002e-05, global_step: 300, interval_runtime: 8.3553, interval_samples_per_second: 1.915, interval_steps_per_second: 1.197, epoch: 2.3438[0m
[32m[2022-09-19 15:31:09,224] [    INFO][0m - loss: 0.55062284, learning_rate: 2.63671875e-05, global_step: 310, interval_runtime: 14.216, interval_samples_per_second: 1.125, interval_steps_per_second: 0.703, epoch: 2.4219[0m
[32m[2022-09-19 15:31:23,491] [    INFO][0m - loss: 0.54442911, learning_rate: 2.625e-05, global_step: 320, interval_runtime: 14.2669, interval_samples_per_second: 1.121, interval_steps_per_second: 0.701, epoch: 2.5[0m
[32m[2022-09-19 15:31:37,713] [    INFO][0m - loss: 0.38624451, learning_rate: 2.61328125e-05, global_step: 330, interval_runtime: 14.2212, interval_samples_per_second: 1.125, interval_steps_per_second: 0.703, epoch: 2.5781[0m
[32m[2022-09-19 15:31:51,941] [    INFO][0m - loss: 0.67242532, learning_rate: 2.6015625e-05, global_step: 340, interval_runtime: 14.2288, interval_samples_per_second: 1.124, interval_steps_per_second: 0.703, epoch: 2.6562[0m
[32m[2022-09-19 15:32:06,185] [    INFO][0m - loss: 0.56841526, learning_rate: 2.5898437500000002e-05, global_step: 350, interval_runtime: 14.2442, interval_samples_per_second: 1.123, interval_steps_per_second: 0.702, epoch: 2.7344[0m
[32m[2022-09-19 15:32:20,438] [    INFO][0m - loss: 0.67402449, learning_rate: 2.578125e-05, global_step: 360, interval_runtime: 14.2527, interval_samples_per_second: 1.123, interval_steps_per_second: 0.702, epoch: 2.8125[0m
[32m[2022-09-19 15:32:34,658] [    INFO][0m - loss: 0.5601819, learning_rate: 2.56640625e-05, global_step: 370, interval_runtime: 14.2199, interval_samples_per_second: 1.125, interval_steps_per_second: 0.703, epoch: 2.8906[0m
[32m[2022-09-19 15:32:48,855] [    INFO][0m - loss: 0.48797874, learning_rate: 2.5546875e-05, global_step: 380, interval_runtime: 14.1976, interval_samples_per_second: 1.127, interval_steps_per_second: 0.704, epoch: 2.9688[0m
[32m[2022-09-19 15:32:53,490] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 15:32:53,490] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-19 15:32:53,490] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 15:32:53,490] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 15:32:53,490] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-19 15:34:02,846] [    INFO][0m - eval_loss: 1.442694902420044, eval_accuracy: 0.5686653771760155, eval_runtime: 69.3554, eval_samples_per_second: 29.817, eval_steps_per_second: 1.874, epoch: 3.0[0m
[32m[2022-09-19 15:34:02,886] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-384[0m
[32m[2022-09-19 15:34:02,887] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 15:34:05,636] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-384/tokenizer_config.json[0m
[32m[2022-09-19 15:34:05,636] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-384/special_tokens_map.json[0m
[32m[2022-09-19 15:34:18,892] [    INFO][0m - loss: 0.36457734, learning_rate: 2.54296875e-05, global_step: 390, interval_runtime: 90.0365, interval_samples_per_second: 0.178, interval_steps_per_second: 0.111, epoch: 3.0469[0m
[32m[2022-09-19 15:34:31,974] [    INFO][0m - loss: 0.21868272, learning_rate: 2.5312500000000002e-05, global_step: 400, interval_runtime: 13.0825, interval_samples_per_second: 1.223, interval_steps_per_second: 0.764, epoch: 3.125[0m
[32m[2022-09-19 15:34:45,013] [    INFO][0m - loss: 0.28529398, learning_rate: 2.51953125e-05, global_step: 410, interval_runtime: 13.0385, interval_samples_per_second: 1.227, interval_steps_per_second: 0.767, epoch: 3.2031[0m
[32m[2022-09-19 15:34:51,667] [    INFO][0m - loss: 0.16746907, learning_rate: 2.5078125e-05, global_step: 420, interval_runtime: 6.6543, interval_samples_per_second: 2.404, interval_steps_per_second: 1.503, epoch: 3.2812[0m
[32m[2022-09-19 15:34:58,195] [    INFO][0m - loss: 0.28051229, learning_rate: 2.49609375e-05, global_step: 430, interval_runtime: 6.528, interval_samples_per_second: 2.451, interval_steps_per_second: 1.532, epoch: 3.3594[0m
[32m[2022-09-19 15:35:04,718] [    INFO][0m - loss: 0.39746008, learning_rate: 2.484375e-05, global_step: 440, interval_runtime: 6.5223, interval_samples_per_second: 2.453, interval_steps_per_second: 1.533, epoch: 3.4375[0m
[32m[2022-09-19 15:35:18,783] [    INFO][0m - loss: 0.24724975, learning_rate: 2.4726562500000002e-05, global_step: 450, interval_runtime: 14.0654, interval_samples_per_second: 1.138, interval_steps_per_second: 0.711, epoch: 3.5156[0m
[32m[2022-09-19 15:35:32,999] [    INFO][0m - loss: 0.25771976, learning_rate: 2.4609375e-05, global_step: 460, interval_runtime: 14.2158, interval_samples_per_second: 1.126, interval_steps_per_second: 0.703, epoch: 3.5938[0m
[32m[2022-09-19 15:35:47,161] [    INFO][0m - loss: 0.24241977, learning_rate: 2.44921875e-05, global_step: 470, interval_runtime: 14.1622, interval_samples_per_second: 1.13, interval_steps_per_second: 0.706, epoch: 3.6719[0m
[32m[2022-09-19 15:36:01,401] [    INFO][0m - loss: 0.25129702, learning_rate: 2.4375e-05, global_step: 480, interval_runtime: 14.2396, interval_samples_per_second: 1.124, interval_steps_per_second: 0.702, epoch: 3.75[0m
[32m[2022-09-19 15:36:15,685] [    INFO][0m - loss: 0.39174526, learning_rate: 2.42578125e-05, global_step: 490, interval_runtime: 14.2835, interval_samples_per_second: 1.12, interval_steps_per_second: 0.7, epoch: 3.8281[0m
[32m[2022-09-19 15:36:29,937] [    INFO][0m - loss: 0.2820838, learning_rate: 2.4140625e-05, global_step: 500, interval_runtime: 14.253, interval_samples_per_second: 1.123, interval_steps_per_second: 0.702, epoch: 3.9062[0m
[32m[2022-09-19 15:36:44,131] [    INFO][0m - loss: 0.24619343, learning_rate: 2.40234375e-05, global_step: 510, interval_runtime: 14.1939, interval_samples_per_second: 1.127, interval_steps_per_second: 0.705, epoch: 3.9844[0m
[32m[2022-09-19 15:36:45,943] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 15:36:45,944] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-19 15:36:45,944] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 15:36:45,944] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 15:36:45,944] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-19 15:37:56,889] [    INFO][0m - eval_loss: 2.030596971511841, eval_accuracy: 0.5764023210831721, eval_runtime: 70.9443, eval_samples_per_second: 29.15, eval_steps_per_second: 1.832, epoch: 4.0[0m
[32m[2022-09-19 15:37:56,924] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-512[0m
[32m[2022-09-19 15:37:56,924] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 15:37:59,714] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-512/tokenizer_config.json[0m
[32m[2022-09-19 15:37:59,715] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-512/special_tokens_map.json[0m
[32m[2022-09-19 15:38:15,647] [    INFO][0m - loss: 0.11496513, learning_rate: 2.3906250000000002e-05, global_step: 520, interval_runtime: 91.5161, interval_samples_per_second: 0.175, interval_steps_per_second: 0.109, epoch: 4.0625[0m
[32m[2022-09-19 15:38:28,863] [    INFO][0m - loss: 0.08546583, learning_rate: 2.37890625e-05, global_step: 530, interval_runtime: 13.2156, interval_samples_per_second: 1.211, interval_steps_per_second: 0.757, epoch: 4.1406[0m
[32m[2022-09-19 15:38:42,018] [    INFO][0m - loss: 0.11766063, learning_rate: 2.3671875e-05, global_step: 540, interval_runtime: 13.1552, interval_samples_per_second: 1.216, interval_steps_per_second: 0.76, epoch: 4.2188[0m
[32m[2022-09-19 15:38:55,078] [    INFO][0m - loss: 0.13727101, learning_rate: 2.35546875e-05, global_step: 550, interval_runtime: 13.0602, interval_samples_per_second: 1.225, interval_steps_per_second: 0.766, epoch: 4.2969[0m
[32m[2022-09-19 15:39:03,308] [    INFO][0m - loss: 0.10687484, learning_rate: 2.34375e-05, global_step: 560, interval_runtime: 8.2291, interval_samples_per_second: 1.944, interval_steps_per_second: 1.215, epoch: 4.375[0m
[32m[2022-09-19 15:39:09,845] [    INFO][0m - loss: 0.20067201, learning_rate: 2.3320312500000002e-05, global_step: 570, interval_runtime: 6.5373, interval_samples_per_second: 2.447, interval_steps_per_second: 1.53, epoch: 4.4531[0m
[32m[2022-09-19 15:39:16,385] [    INFO][0m - loss: 0.18618819, learning_rate: 2.3203125e-05, global_step: 580, interval_runtime: 6.5405, interval_samples_per_second: 2.446, interval_steps_per_second: 1.529, epoch: 4.5312[0m
[32m[2022-09-19 15:39:23,444] [    INFO][0m - loss: 0.11744117, learning_rate: 2.30859375e-05, global_step: 590, interval_runtime: 7.059, interval_samples_per_second: 2.267, interval_steps_per_second: 1.417, epoch: 4.6094[0m
[32m[2022-09-19 15:39:37,741] [    INFO][0m - loss: 0.13100945, learning_rate: 2.296875e-05, global_step: 600, interval_runtime: 14.2968, interval_samples_per_second: 1.119, interval_steps_per_second: 0.699, epoch: 4.6875[0m
[32m[2022-09-19 15:39:51,963] [    INFO][0m - loss: 0.20615768, learning_rate: 2.28515625e-05, global_step: 610, interval_runtime: 14.2213, interval_samples_per_second: 1.125, interval_steps_per_second: 0.703, epoch: 4.7656[0m
[32m[2022-09-19 15:40:06,168] [    INFO][0m - loss: 0.28611424, learning_rate: 2.2734375000000002e-05, global_step: 620, interval_runtime: 14.2059, interval_samples_per_second: 1.126, interval_steps_per_second: 0.704, epoch: 4.8438[0m
[32m[2022-09-19 15:40:20,319] [    INFO][0m - loss: 0.17727456, learning_rate: 2.26171875e-05, global_step: 630, interval_runtime: 14.151, interval_samples_per_second: 1.131, interval_steps_per_second: 0.707, epoch: 4.9219[0m
[32m[2022-09-19 15:40:33,512] [    INFO][0m - loss: 0.12288847, learning_rate: 2.25e-05, global_step: 640, interval_runtime: 13.1922, interval_samples_per_second: 1.213, interval_steps_per_second: 0.758, epoch: 5.0[0m
[32m[2022-09-19 15:40:33,513] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 15:40:33,513] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-19 15:40:33,513] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 15:40:33,513] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 15:40:33,513] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-19 15:41:44,383] [    INFO][0m - eval_loss: 2.3070085048675537, eval_accuracy: 0.5585106382978723, eval_runtime: 70.8701, eval_samples_per_second: 29.18, eval_steps_per_second: 1.834, epoch: 5.0[0m
[32m[2022-09-19 15:41:44,424] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-640[0m
[32m[2022-09-19 15:41:44,424] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 15:41:47,187] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-640/tokenizer_config.json[0m
[32m[2022-09-19 15:41:47,187] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-640/special_tokens_map.json[0m
[32m[2022-09-19 15:42:06,664] [    INFO][0m - loss: 0.06760477, learning_rate: 2.23828125e-05, global_step: 650, interval_runtime: 93.1526, interval_samples_per_second: 0.172, interval_steps_per_second: 0.107, epoch: 5.0781[0m
[32m[2022-09-19 15:42:19,802] [    INFO][0m - loss: 0.07411662, learning_rate: 2.2265625e-05, global_step: 660, interval_runtime: 13.1377, interval_samples_per_second: 1.218, interval_steps_per_second: 0.761, epoch: 5.1562[0m
[32m[2022-09-19 15:42:32,965] [    INFO][0m - loss: 0.13106213, learning_rate: 2.2148437500000002e-05, global_step: 670, interval_runtime: 13.1633, interval_samples_per_second: 1.215, interval_steps_per_second: 0.76, epoch: 5.2344[0m
[32m[2022-09-19 15:42:46,138] [    INFO][0m - loss: 0.07666066, learning_rate: 2.203125e-05, global_step: 680, interval_runtime: 13.1727, interval_samples_per_second: 1.215, interval_steps_per_second: 0.759, epoch: 5.3125[0m
[32m[2022-09-19 15:42:59,292] [    INFO][0m - loss: 0.15210227, learning_rate: 2.19140625e-05, global_step: 690, interval_runtime: 13.1545, interval_samples_per_second: 1.216, interval_steps_per_second: 0.76, epoch: 5.3906[0m
[32m[2022-09-19 15:43:12,331] [    INFO][0m - loss: 0.09563607, learning_rate: 2.1796875e-05, global_step: 700, interval_runtime: 13.0382, interval_samples_per_second: 1.227, interval_steps_per_second: 0.767, epoch: 5.4688[0m
[32m[2022-09-19 15:43:20,549] [    INFO][0m - loss: 0.12531413, learning_rate: 2.16796875e-05, global_step: 710, interval_runtime: 8.2183, interval_samples_per_second: 1.947, interval_steps_per_second: 1.217, epoch: 5.5469[0m
[32m[2022-09-19 15:43:27,059] [    INFO][0m - loss: 0.14500378, learning_rate: 2.15625e-05, global_step: 720, interval_runtime: 6.5107, interval_samples_per_second: 2.458, interval_steps_per_second: 1.536, epoch: 5.625[0m
[32m[2022-09-19 15:43:33,573] [    INFO][0m - loss: 0.06326475, learning_rate: 2.14453125e-05, global_step: 730, interval_runtime: 6.5139, interval_samples_per_second: 2.456, interval_steps_per_second: 1.535, epoch: 5.7031[0m
[32m[2022-09-19 15:43:45,831] [    INFO][0m - loss: 0.11686778, learning_rate: 2.1328125000000002e-05, global_step: 740, interval_runtime: 12.2576, interval_samples_per_second: 1.305, interval_steps_per_second: 0.816, epoch: 5.7812[0m
[32m[2022-09-19 15:44:00,052] [    INFO][0m - loss: 0.07672783, learning_rate: 2.12109375e-05, global_step: 750, interval_runtime: 14.2213, interval_samples_per_second: 1.125, interval_steps_per_second: 0.703, epoch: 5.8594[0m
[32m[2022-09-19 15:44:14,289] [    INFO][0m - loss: 0.11184324, learning_rate: 2.109375e-05, global_step: 760, interval_runtime: 14.2365, interval_samples_per_second: 1.124, interval_steps_per_second: 0.702, epoch: 5.9375[0m
[32m[2022-09-19 15:44:24,612] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 15:44:24,612] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-19 15:44:24,613] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 15:44:24,613] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 15:44:24,613] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-19 15:45:35,638] [    INFO][0m - eval_loss: 2.7761471271514893, eval_accuracy: 0.5793036750483559, eval_runtime: 71.0246, eval_samples_per_second: 29.117, eval_steps_per_second: 1.83, epoch: 6.0[0m
[32m[2022-09-19 15:45:35,674] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-768[0m
[32m[2022-09-19 15:45:35,674] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 15:45:38,430] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-768/tokenizer_config.json[0m
[32m[2022-09-19 15:45:38,431] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-768/special_tokens_map.json[0m
[32m[2022-09-19 15:45:46,575] [    INFO][0m - loss: 0.0402341, learning_rate: 2.09765625e-05, global_step: 770, interval_runtime: 92.2866, interval_samples_per_second: 0.173, interval_steps_per_second: 0.108, epoch: 6.0156[0m
[32m[2022-09-19 15:46:00,813] [    INFO][0m - loss: 0.0317267, learning_rate: 2.0859375e-05, global_step: 780, interval_runtime: 14.2377, interval_samples_per_second: 1.124, interval_steps_per_second: 0.702, epoch: 6.0938[0m
[32m[2022-09-19 15:46:15,055] [    INFO][0m - loss: 0.10975466, learning_rate: 2.0742187500000002e-05, global_step: 790, interval_runtime: 14.2414, interval_samples_per_second: 1.123, interval_steps_per_second: 0.702, epoch: 6.1719[0m
[32m[2022-09-19 15:46:27,488] [    INFO][0m - loss: 0.09186682, learning_rate: 2.0625e-05, global_step: 800, interval_runtime: 12.4336, interval_samples_per_second: 1.287, interval_steps_per_second: 0.804, epoch: 6.25[0m
[32m[2022-09-19 15:46:40,643] [    INFO][0m - loss: 0.05457765, learning_rate: 2.05078125e-05, global_step: 810, interval_runtime: 13.1551, interval_samples_per_second: 1.216, interval_steps_per_second: 0.76, epoch: 6.3281[0m
[32m[2022-09-19 15:46:53,798] [    INFO][0m - loss: 0.1544752, learning_rate: 2.0390625e-05, global_step: 820, interval_runtime: 13.1551, interval_samples_per_second: 1.216, interval_steps_per_second: 0.76, epoch: 6.4062[0m
[32m[2022-09-19 15:47:07,012] [    INFO][0m - loss: 0.02296709, learning_rate: 2.02734375e-05, global_step: 830, interval_runtime: 13.2131, interval_samples_per_second: 1.211, interval_steps_per_second: 0.757, epoch: 6.4844[0m
[32m[2022-09-19 15:47:20,015] [    INFO][0m - loss: 0.05973232, learning_rate: 2.0156250000000002e-05, global_step: 840, interval_runtime: 13.0042, interval_samples_per_second: 1.23, interval_steps_per_second: 0.769, epoch: 6.5625[0m
[32m[2022-09-19 15:47:31,375] [    INFO][0m - loss: 0.01847283, learning_rate: 2.00390625e-05, global_step: 850, interval_runtime: 11.3599, interval_samples_per_second: 1.408, interval_steps_per_second: 0.88, epoch: 6.6406[0m
[32m[2022-09-19 15:47:37,888] [    INFO][0m - loss: 0.02755415, learning_rate: 1.9921875e-05, global_step: 860, interval_runtime: 6.5129, interval_samples_per_second: 2.457, interval_steps_per_second: 1.535, epoch: 6.7188[0m
[32m[2022-09-19 15:47:44,436] [    INFO][0m - loss: 0.06036844, learning_rate: 1.98046875e-05, global_step: 870, interval_runtime: 6.5471, interval_samples_per_second: 2.444, interval_steps_per_second: 1.527, epoch: 6.7969[0m
[32m[2022-09-19 15:47:52,417] [    INFO][0m - loss: 0.08856969, learning_rate: 1.96875e-05, global_step: 880, interval_runtime: 7.9814, interval_samples_per_second: 2.005, interval_steps_per_second: 1.253, epoch: 6.875[0m
[32m[2022-09-19 15:48:06,611] [    INFO][0m - loss: 0.02994229, learning_rate: 1.95703125e-05, global_step: 890, interval_runtime: 14.1941, interval_samples_per_second: 1.127, interval_steps_per_second: 0.705, epoch: 6.9531[0m
[32m[2022-09-19 15:48:14,077] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 15:48:14,077] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-19 15:48:14,077] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 15:48:14,077] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 15:48:14,077] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-19 15:49:24,850] [    INFO][0m - eval_loss: 2.9576008319854736, eval_accuracy: 0.5826885880077369, eval_runtime: 70.7728, eval_samples_per_second: 29.22, eval_steps_per_second: 1.837, epoch: 7.0[0m
[32m[2022-09-19 15:49:24,890] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-896[0m
[32m[2022-09-19 15:49:24,891] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 15:49:27,596] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-896/tokenizer_config.json[0m
[32m[2022-09-19 15:49:27,596] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-896/special_tokens_map.json[0m
[32m[2022-09-19 15:49:38,668] [    INFO][0m - loss: 0.042905, learning_rate: 1.9453125e-05, global_step: 900, interval_runtime: 92.0572, interval_samples_per_second: 0.174, interval_steps_per_second: 0.109, epoch: 7.0312[0m
[32m[2022-09-19 15:49:52,947] [    INFO][0m - loss: 0.06579525, learning_rate: 1.93359375e-05, global_step: 910, interval_runtime: 14.2788, interval_samples_per_second: 1.121, interval_steps_per_second: 0.7, epoch: 7.1094[0m
[32m[2022-09-19 15:50:07,172] [    INFO][0m - loss: 0.01436811, learning_rate: 1.921875e-05, global_step: 920, interval_runtime: 14.225, interval_samples_per_second: 1.125, interval_steps_per_second: 0.703, epoch: 7.1875[0m
[32m[2022-09-19 15:50:21,410] [    INFO][0m - loss: 0.0053285, learning_rate: 1.91015625e-05, global_step: 930, interval_runtime: 14.238, interval_samples_per_second: 1.124, interval_steps_per_second: 0.702, epoch: 7.2656[0m
[32m[2022-09-19 15:50:35,398] [    INFO][0m - loss: 0.044197, learning_rate: 1.8984375e-05, global_step: 940, interval_runtime: 13.9876, interval_samples_per_second: 1.144, interval_steps_per_second: 0.715, epoch: 7.3438[0m
[32m[2022-09-19 15:50:48,430] [    INFO][0m - loss: 0.00292586, learning_rate: 1.88671875e-05, global_step: 950, interval_runtime: 13.0321, interval_samples_per_second: 1.228, interval_steps_per_second: 0.767, epoch: 7.4219[0m
[32m[2022-09-19 15:51:01,643] [    INFO][0m - loss: 0.02204081, learning_rate: 1.8750000000000002e-05, global_step: 960, interval_runtime: 13.213, interval_samples_per_second: 1.211, interval_steps_per_second: 0.757, epoch: 7.5[0m
[32m[2022-09-19 15:51:14,826] [    INFO][0m - loss: 0.00761292, learning_rate: 1.86328125e-05, global_step: 970, interval_runtime: 13.1833, interval_samples_per_second: 1.214, interval_steps_per_second: 0.759, epoch: 7.5781[0m
[32m[2022-09-19 15:51:27,936] [    INFO][0m - loss: 0.00556422, learning_rate: 1.8515625e-05, global_step: 980, interval_runtime: 13.1099, interval_samples_per_second: 1.22, interval_steps_per_second: 0.763, epoch: 7.6562[0m
[32m[2022-09-19 15:51:41,117] [    INFO][0m - loss: 0.04472373, learning_rate: 1.83984375e-05, global_step: 990, interval_runtime: 13.1806, interval_samples_per_second: 1.214, interval_steps_per_second: 0.759, epoch: 7.7344[0m
[32m[2022-09-19 15:51:48,553] [    INFO][0m - loss: 0.00324076, learning_rate: 1.828125e-05, global_step: 1000, interval_runtime: 7.4362, interval_samples_per_second: 2.152, interval_steps_per_second: 1.345, epoch: 7.8125[0m
[32m[2022-09-19 15:51:55,064] [    INFO][0m - loss: 0.00556776, learning_rate: 1.8164062500000002e-05, global_step: 1010, interval_runtime: 6.5116, interval_samples_per_second: 2.457, interval_steps_per_second: 1.536, epoch: 7.8906[0m
[32m[2022-09-19 15:52:01,541] [    INFO][0m - loss: 0.03565989, learning_rate: 1.8046875e-05, global_step: 1020, interval_runtime: 6.4764, interval_samples_per_second: 2.47, interval_steps_per_second: 1.544, epoch: 7.9688[0m
[32m[2022-09-19 15:52:03,712] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 15:52:03,712] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-19 15:52:03,712] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 15:52:03,712] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 15:52:03,712] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-19 15:53:14,399] [    INFO][0m - eval_loss: 3.101118564605713, eval_accuracy: 0.5788201160541586, eval_runtime: 70.6869, eval_samples_per_second: 29.256, eval_steps_per_second: 1.839, epoch: 8.0[0m
[32m[2022-09-19 15:53:14,435] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-1024[0m
[32m[2022-09-19 15:53:14,436] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 15:53:17,114] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-1024/tokenizer_config.json[0m
[32m[2022-09-19 15:53:17,115] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-1024/special_tokens_map.json[0m
[32m[2022-09-19 15:53:30,898] [    INFO][0m - loss: 0.05160122, learning_rate: 1.79296875e-05, global_step: 1030, interval_runtime: 89.3574, interval_samples_per_second: 0.179, interval_steps_per_second: 0.112, epoch: 8.0469[0m
[32m[2022-09-19 15:53:45,131] [    INFO][0m - loss: 0.01529202, learning_rate: 1.78125e-05, global_step: 1040, interval_runtime: 14.2328, interval_samples_per_second: 1.124, interval_steps_per_second: 0.703, epoch: 8.125[0m
[32m[2022-09-19 15:53:59,381] [    INFO][0m - loss: 0.01005007, learning_rate: 1.76953125e-05, global_step: 1050, interval_runtime: 14.2497, interval_samples_per_second: 1.123, interval_steps_per_second: 0.702, epoch: 8.2031[0m
[32m[2022-09-19 15:54:13,624] [    INFO][0m - loss: 0.00277975, learning_rate: 1.7578125000000002e-05, global_step: 1060, interval_runtime: 14.2437, interval_samples_per_second: 1.123, interval_steps_per_second: 0.702, epoch: 8.2812[0m
[32m[2022-09-19 15:54:27,832] [    INFO][0m - loss: 0.00414693, learning_rate: 1.74609375e-05, global_step: 1070, interval_runtime: 14.2078, interval_samples_per_second: 1.126, interval_steps_per_second: 0.704, epoch: 8.3594[0m
[32m[2022-09-19 15:54:42,035] [    INFO][0m - loss: 0.02450429, learning_rate: 1.734375e-05, global_step: 1080, interval_runtime: 14.2026, interval_samples_per_second: 1.127, interval_steps_per_second: 0.704, epoch: 8.4375[0m
[32m[2022-09-19 15:54:55,767] [    INFO][0m - loss: 0.00096986, learning_rate: 1.72265625e-05, global_step: 1090, interval_runtime: 13.7322, interval_samples_per_second: 1.165, interval_steps_per_second: 0.728, epoch: 8.5156[0m
[32m[2022-09-19 15:55:08,867] [    INFO][0m - loss: 0.01120523, learning_rate: 1.7109375e-05, global_step: 1100, interval_runtime: 13.0995, interval_samples_per_second: 1.221, interval_steps_per_second: 0.763, epoch: 8.5938[0m
[32m[2022-09-19 15:55:21,949] [    INFO][0m - loss: 0.01145917, learning_rate: 1.69921875e-05, global_step: 1110, interval_runtime: 13.0828, interval_samples_per_second: 1.223, interval_steps_per_second: 0.764, epoch: 8.6719[0m
[32m[2022-09-19 15:55:35,098] [    INFO][0m - loss: 0.00352046, learning_rate: 1.6875e-05, global_step: 1120, interval_runtime: 13.1487, interval_samples_per_second: 1.217, interval_steps_per_second: 0.761, epoch: 8.75[0m
[32m[2022-09-19 15:55:48,201] [    INFO][0m - loss: 0.06304729, learning_rate: 1.67578125e-05, global_step: 1130, interval_runtime: 13.1028, interval_samples_per_second: 1.221, interval_steps_per_second: 0.763, epoch: 8.8281[0m
[32m[2022-09-19 15:56:00,875] [    INFO][0m - loss: 0.01663317, learning_rate: 1.6640625e-05, global_step: 1140, interval_runtime: 12.674, interval_samples_per_second: 1.262, interval_steps_per_second: 0.789, epoch: 8.9062[0m
[32m[2022-09-19 15:56:07,338] [    INFO][0m - loss: 0.00765587, learning_rate: 1.65234375e-05, global_step: 1150, interval_runtime: 6.4631, interval_samples_per_second: 2.476, interval_steps_per_second: 1.547, epoch: 8.9844[0m
[32m[2022-09-19 15:56:08,150] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 15:56:08,151] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-19 15:56:08,151] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 15:56:08,151] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 15:56:08,151] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-19 15:57:04,929] [    INFO][0m - eval_loss: 3.270069122314453, eval_accuracy: 0.5831721470019342, eval_runtime: 56.7782, eval_samples_per_second: 36.422, eval_steps_per_second: 2.29, epoch: 9.0[0m
[32m[2022-09-19 15:57:04,960] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-1152[0m
[32m[2022-09-19 15:57:04,961] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 15:57:07,711] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-1152/tokenizer_config.json[0m
[32m[2022-09-19 15:57:07,711] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-1152/special_tokens_map.json[0m
[32m[2022-09-19 15:57:24,444] [    INFO][0m - loss: 0.002662, learning_rate: 1.640625e-05, global_step: 1160, interval_runtime: 77.1059, interval_samples_per_second: 0.208, interval_steps_per_second: 0.13, epoch: 9.0625[0m
[32m[2022-09-19 15:57:38,698] [    INFO][0m - loss: 0.00570608, learning_rate: 1.62890625e-05, global_step: 1170, interval_runtime: 14.2535, interval_samples_per_second: 1.123, interval_steps_per_second: 0.702, epoch: 9.1406[0m
[32m[2022-09-19 15:57:52,939] [    INFO][0m - loss: 0.00125709, learning_rate: 1.6171875000000002e-05, global_step: 1180, interval_runtime: 14.2413, interval_samples_per_second: 1.123, interval_steps_per_second: 0.702, epoch: 9.2188[0m
[32m[2022-09-19 15:58:07,159] [    INFO][0m - loss: 0.00297934, learning_rate: 1.60546875e-05, global_step: 1190, interval_runtime: 14.22, interval_samples_per_second: 1.125, interval_steps_per_second: 0.703, epoch: 9.2969[0m
[32m[2022-09-19 15:58:21,396] [    INFO][0m - loss: 0.01004014, learning_rate: 1.59375e-05, global_step: 1200, interval_runtime: 14.2371, interval_samples_per_second: 1.124, interval_steps_per_second: 0.702, epoch: 9.375[0m
[32m[2022-09-19 15:58:35,641] [    INFO][0m - loss: 0.00063655, learning_rate: 1.58203125e-05, global_step: 1210, interval_runtime: 14.2452, interval_samples_per_second: 1.123, interval_steps_per_second: 0.702, epoch: 9.4531[0m
[32m[2022-09-19 15:58:49,867] [    INFO][0m - loss: 0.00259586, learning_rate: 1.5703125e-05, global_step: 1220, interval_runtime: 14.2255, interval_samples_per_second: 1.125, interval_steps_per_second: 0.703, epoch: 9.5312[0m
[32m[2022-09-19 15:59:04,197] [    INFO][0m - loss: 0.00097458, learning_rate: 1.5585937500000002e-05, global_step: 1230, interval_runtime: 14.3303, interval_samples_per_second: 1.117, interval_steps_per_second: 0.698, epoch: 9.6094[0m
[32m[2022-09-19 15:59:17,473] [    INFO][0m - loss: 0.00683899, learning_rate: 1.546875e-05, global_step: 1240, interval_runtime: 13.276, interval_samples_per_second: 1.205, interval_steps_per_second: 0.753, epoch: 9.6875[0m
[32m[2022-09-19 15:59:30,631] [    INFO][0m - loss: 0.00165932, learning_rate: 1.53515625e-05, global_step: 1250, interval_runtime: 13.158, interval_samples_per_second: 1.216, interval_steps_per_second: 0.76, epoch: 9.7656[0m
[32m[2022-09-19 15:59:43,635] [    INFO][0m - loss: 0.00048955, learning_rate: 1.5234375000000001e-05, global_step: 1260, interval_runtime: 13.0039, interval_samples_per_second: 1.23, interval_steps_per_second: 0.769, epoch: 9.8438[0m
[32m[2022-09-19 15:59:56,874] [    INFO][0m - loss: 0.00379002, learning_rate: 1.51171875e-05, global_step: 1270, interval_runtime: 13.2396, interval_samples_per_second: 1.208, interval_steps_per_second: 0.755, epoch: 9.9219[0m
[32m[2022-09-19 16:00:08,940] [    INFO][0m - loss: 0.00148922, learning_rate: 1.5e-05, global_step: 1280, interval_runtime: 12.0655, interval_samples_per_second: 1.326, interval_steps_per_second: 0.829, epoch: 10.0[0m
[32m[2022-09-19 16:00:08,941] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 16:00:08,941] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-19 16:00:08,941] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 16:00:08,941] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 16:00:08,941] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-19 16:00:56,423] [    INFO][0m - eval_loss: 3.2375597953796387, eval_accuracy: 0.5822050290135397, eval_runtime: 47.4812, eval_samples_per_second: 43.554, eval_steps_per_second: 2.738, epoch: 10.0[0m
[32m[2022-09-19 16:00:56,454] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-1280[0m
[32m[2022-09-19 16:00:56,454] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 16:00:59,100] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-1280/tokenizer_config.json[0m
[32m[2022-09-19 16:00:59,101] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-1280/special_tokens_map.json[0m
[32m[2022-09-19 16:01:18,543] [    INFO][0m - loss: 0.00023049, learning_rate: 1.48828125e-05, global_step: 1290, interval_runtime: 69.6029, interval_samples_per_second: 0.23, interval_steps_per_second: 0.144, epoch: 10.0781[0m
[32m[2022-09-19 16:01:35,160] [    INFO][0m - loss: 0.00019841, learning_rate: 1.4765625e-05, global_step: 1300, interval_runtime: 16.6171, interval_samples_per_second: 0.963, interval_steps_per_second: 0.602, epoch: 10.1562[0m
[32m[2022-09-19 16:01:49,369] [    INFO][0m - loss: 0.00246764, learning_rate: 1.4648437500000001e-05, global_step: 1310, interval_runtime: 14.2089, interval_samples_per_second: 1.126, interval_steps_per_second: 0.704, epoch: 10.2344[0m
[32m[2022-09-19 16:02:03,592] [    INFO][0m - loss: 0.00158614, learning_rate: 1.453125e-05, global_step: 1320, interval_runtime: 14.223, interval_samples_per_second: 1.125, interval_steps_per_second: 0.703, epoch: 10.3125[0m
[32m[2022-09-19 16:02:17,822] [    INFO][0m - loss: 0.00127036, learning_rate: 1.44140625e-05, global_step: 1330, interval_runtime: 14.2296, interval_samples_per_second: 1.124, interval_steps_per_second: 0.703, epoch: 10.3906[0m
[32m[2022-09-19 16:02:32,077] [    INFO][0m - loss: 0.00034704, learning_rate: 1.4296875e-05, global_step: 1340, interval_runtime: 14.256, interval_samples_per_second: 1.122, interval_steps_per_second: 0.701, epoch: 10.4688[0m
[32m[2022-09-19 16:02:46,241] [    INFO][0m - loss: 0.0002395, learning_rate: 1.41796875e-05, global_step: 1350, interval_runtime: 14.1636, interval_samples_per_second: 1.13, interval_steps_per_second: 0.706, epoch: 10.5469[0m
[32m[2022-09-19 16:03:00,480] [    INFO][0m - loss: 0.00026854, learning_rate: 1.40625e-05, global_step: 1360, interval_runtime: 14.2383, interval_samples_per_second: 1.124, interval_steps_per_second: 0.702, epoch: 10.625[0m
[32m[2022-09-19 16:03:14,961] [    INFO][0m - loss: 0.00070337, learning_rate: 1.3945312500000001e-05, global_step: 1370, interval_runtime: 14.4815, interval_samples_per_second: 1.105, interval_steps_per_second: 0.691, epoch: 10.7031[0m
[32m[2022-09-19 16:03:28,484] [    INFO][0m - loss: 0.00026341, learning_rate: 1.3828125e-05, global_step: 1380, interval_runtime: 13.5233, interval_samples_per_second: 1.183, interval_steps_per_second: 0.739, epoch: 10.7812[0m
[32m[2022-09-19 16:03:41,559] [    INFO][0m - loss: 0.00248973, learning_rate: 1.37109375e-05, global_step: 1390, interval_runtime: 13.0747, interval_samples_per_second: 1.224, interval_steps_per_second: 0.765, epoch: 10.8594[0m
[32m[2022-09-19 16:03:54,727] [    INFO][0m - loss: 0.00549197, learning_rate: 1.359375e-05, global_step: 1400, interval_runtime: 13.1683, interval_samples_per_second: 1.215, interval_steps_per_second: 0.759, epoch: 10.9375[0m
[32m[2022-09-19 16:04:04,245] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 16:04:04,245] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-19 16:04:04,245] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 16:04:04,245] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 16:04:04,245] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-19 16:04:50,813] [    INFO][0m - eval_loss: 3.257075548171997, eval_accuracy: 0.5870406189555126, eval_runtime: 46.5669, eval_samples_per_second: 44.409, eval_steps_per_second: 2.792, epoch: 11.0[0m
[32m[2022-09-19 16:04:50,845] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-1408[0m
[32m[2022-09-19 16:04:50,845] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 16:04:53,594] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-1408/tokenizer_config.json[0m
[32m[2022-09-19 16:04:53,594] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-1408/special_tokens_map.json[0m
[32m[2022-09-19 16:05:01,714] [    INFO][0m - loss: 0.00142326, learning_rate: 1.34765625e-05, global_step: 1410, interval_runtime: 66.9867, interval_samples_per_second: 0.239, interval_steps_per_second: 0.149, epoch: 11.0156[0m
[32m[2022-09-19 16:05:15,919] [    INFO][0m - loss: 8.578e-05, learning_rate: 1.3359375000000001e-05, global_step: 1420, interval_runtime: 14.2046, interval_samples_per_second: 1.126, interval_steps_per_second: 0.704, epoch: 11.0938[0m
[32m[2022-09-19 16:05:30,181] [    INFO][0m - loss: 8.101e-05, learning_rate: 1.3242187500000001e-05, global_step: 1430, interval_runtime: 14.262, interval_samples_per_second: 1.122, interval_steps_per_second: 0.701, epoch: 11.1719[0m
[32m[2022-09-19 16:05:41,781] [    INFO][0m - loss: 0.00071652, learning_rate: 1.3125e-05, global_step: 1440, interval_runtime: 11.6003, interval_samples_per_second: 1.379, interval_steps_per_second: 0.862, epoch: 11.25[0m
[32m[2022-09-19 16:05:56,019] [    INFO][0m - loss: 0.00027692, learning_rate: 1.30078125e-05, global_step: 1450, interval_runtime: 14.2378, interval_samples_per_second: 1.124, interval_steps_per_second: 0.702, epoch: 11.3281[0m
[32m[2022-09-19 16:06:10,306] [    INFO][0m - loss: 0.00012075, learning_rate: 1.2890625e-05, global_step: 1460, interval_runtime: 14.2873, interval_samples_per_second: 1.12, interval_steps_per_second: 0.7, epoch: 11.4062[0m
[32m[2022-09-19 16:06:24,585] [    INFO][0m - loss: 0.000152, learning_rate: 1.27734375e-05, global_step: 1470, interval_runtime: 14.279, interval_samples_per_second: 1.121, interval_steps_per_second: 0.7, epoch: 11.4844[0m
[32m[2022-09-19 16:06:38,838] [    INFO][0m - loss: 6.634e-05, learning_rate: 1.2656250000000001e-05, global_step: 1480, interval_runtime: 14.2534, interval_samples_per_second: 1.123, interval_steps_per_second: 0.702, epoch: 11.5625[0m
[32m[2022-09-19 16:06:53,102] [    INFO][0m - loss: 7.338e-05, learning_rate: 1.25390625e-05, global_step: 1490, interval_runtime: 14.2638, interval_samples_per_second: 1.122, interval_steps_per_second: 0.701, epoch: 11.6406[0m
[32m[2022-09-19 16:07:07,398] [    INFO][0m - loss: 0.00010915, learning_rate: 1.2421875e-05, global_step: 1500, interval_runtime: 14.2959, interval_samples_per_second: 1.119, interval_steps_per_second: 0.699, epoch: 11.7188[0m
[32m[2022-09-19 16:07:21,578] [    INFO][0m - loss: 4.835e-05, learning_rate: 1.23046875e-05, global_step: 1510, interval_runtime: 14.1799, interval_samples_per_second: 1.128, interval_steps_per_second: 0.705, epoch: 11.7969[0m
[32m[2022-09-19 16:07:35,791] [    INFO][0m - loss: 0.00015815, learning_rate: 1.21875e-05, global_step: 1520, interval_runtime: 14.2129, interval_samples_per_second: 1.126, interval_steps_per_second: 0.704, epoch: 11.875[0m
[32m[2022-09-19 16:07:49,091] [    INFO][0m - loss: 3.503e-05, learning_rate: 1.20703125e-05, global_step: 1530, interval_runtime: 13.3, interval_samples_per_second: 1.203, interval_steps_per_second: 0.752, epoch: 11.9531[0m
[32m[2022-09-19 16:07:55,965] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 16:07:55,965] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-19 16:07:55,965] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 16:07:55,965] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 16:07:55,965] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-19 16:08:54,403] [    INFO][0m - eval_loss: 3.2922444343566895, eval_accuracy: 0.5923597678916828, eval_runtime: 58.4375, eval_samples_per_second: 35.388, eval_steps_per_second: 2.225, epoch: 12.0[0m
[32m[2022-09-19 16:08:54,438] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-1536[0m
[32m[2022-09-19 16:08:54,438] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 16:08:57,292] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-1536/tokenizer_config.json[0m
[32m[2022-09-19 16:08:57,293] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-1536/special_tokens_map.json[0m
[32m[2022-09-19 16:09:05,135] [    INFO][0m - loss: 7.083e-05, learning_rate: 1.1953125000000001e-05, global_step: 1540, interval_runtime: 76.0446, interval_samples_per_second: 0.21, interval_steps_per_second: 0.132, epoch: 12.0312[0m
[32m[2022-09-19 16:09:19,345] [    INFO][0m - loss: 3.317e-05, learning_rate: 1.18359375e-05, global_step: 1550, interval_runtime: 14.2094, interval_samples_per_second: 1.126, interval_steps_per_second: 0.704, epoch: 12.1094[0m
[32m[2022-09-19 16:09:33,567] [    INFO][0m - loss: 5.187e-05, learning_rate: 1.171875e-05, global_step: 1560, interval_runtime: 14.2219, interval_samples_per_second: 1.125, interval_steps_per_second: 0.703, epoch: 12.1875[0m
[32m[2022-09-19 16:09:47,834] [    INFO][0m - loss: 6.162e-05, learning_rate: 1.16015625e-05, global_step: 1570, interval_runtime: 14.2665, interval_samples_per_second: 1.122, interval_steps_per_second: 0.701, epoch: 12.2656[0m
[32m[2022-09-19 16:10:02,164] [    INFO][0m - loss: 4.495e-05, learning_rate: 1.1484375e-05, global_step: 1580, interval_runtime: 14.3307, interval_samples_per_second: 1.116, interval_steps_per_second: 0.698, epoch: 12.3438[0m
[32m[2022-09-19 16:10:16,428] [    INFO][0m - loss: 9.128e-05, learning_rate: 1.1367187500000001e-05, global_step: 1590, interval_runtime: 14.2635, interval_samples_per_second: 1.122, interval_steps_per_second: 0.701, epoch: 12.4219[0m
[32m[2022-09-19 16:10:30,677] [    INFO][0m - loss: 6.505e-05, learning_rate: 1.125e-05, global_step: 1600, interval_runtime: 14.2496, interval_samples_per_second: 1.123, interval_steps_per_second: 0.702, epoch: 12.5[0m
[32m[2022-09-19 16:10:44,982] [    INFO][0m - loss: 6.32e-05, learning_rate: 1.11328125e-05, global_step: 1610, interval_runtime: 14.3052, interval_samples_per_second: 1.118, interval_steps_per_second: 0.699, epoch: 12.5781[0m
[32m[2022-09-19 16:10:59,254] [    INFO][0m - loss: 0.00019589, learning_rate: 1.1015625e-05, global_step: 1620, interval_runtime: 14.272, interval_samples_per_second: 1.121, interval_steps_per_second: 0.701, epoch: 12.6562[0m
[32m[2022-09-19 16:11:13,514] [    INFO][0m - loss: 7.323e-05, learning_rate: 1.08984375e-05, global_step: 1630, interval_runtime: 14.2597, interval_samples_per_second: 1.122, interval_steps_per_second: 0.701, epoch: 12.7344[0m
[32m[2022-09-19 16:11:27,758] [    INFO][0m - loss: 4.994e-05, learning_rate: 1.078125e-05, global_step: 1640, interval_runtime: 14.2439, interval_samples_per_second: 1.123, interval_steps_per_second: 0.702, epoch: 12.8125[0m
[32m[2022-09-19 16:11:41,975] [    INFO][0m - loss: 2.578e-05, learning_rate: 1.0664062500000001e-05, global_step: 1650, interval_runtime: 14.2171, interval_samples_per_second: 1.125, interval_steps_per_second: 0.703, epoch: 12.8906[0m
[32m[2022-09-19 16:11:56,137] [    INFO][0m - loss: 6.239e-05, learning_rate: 1.0546875e-05, global_step: 1660, interval_runtime: 14.1618, interval_samples_per_second: 1.13, interval_steps_per_second: 0.706, epoch: 12.9688[0m
[32m[2022-09-19 16:12:00,732] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 16:12:00,733] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-19 16:12:00,733] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 16:12:00,733] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 16:12:00,733] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-19 16:13:09,273] [    INFO][0m - eval_loss: 3.333677053451538, eval_accuracy: 0.5918762088974855, eval_runtime: 68.5402, eval_samples_per_second: 30.172, eval_steps_per_second: 1.897, epoch: 13.0[0m
[32m[2022-09-19 16:13:09,303] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-1664[0m
[32m[2022-09-19 16:13:09,303] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 16:13:15,849] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-1664/tokenizer_config.json[0m
[32m[2022-09-19 16:13:15,849] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-1664/special_tokens_map.json[0m
[32m[2022-09-19 16:13:25,093] [    INFO][0m - loss: 2.411e-05, learning_rate: 1.04296875e-05, global_step: 1670, interval_runtime: 88.9563, interval_samples_per_second: 0.18, interval_steps_per_second: 0.112, epoch: 13.0469[0m
[32m[2022-09-19 16:13:31,582] [    INFO][0m - loss: 3.545e-05, learning_rate: 1.03125e-05, global_step: 1680, interval_runtime: 6.4888, interval_samples_per_second: 2.466, interval_steps_per_second: 1.541, epoch: 13.125[0m
[32m[2022-09-19 16:13:43,049] [    INFO][0m - loss: 7.781e-05, learning_rate: 1.01953125e-05, global_step: 1690, interval_runtime: 11.4665, interval_samples_per_second: 1.395, interval_steps_per_second: 0.872, epoch: 13.2031[0m
[32m[2022-09-19 16:13:57,283] [    INFO][0m - loss: 2.964e-05, learning_rate: 1.0078125000000001e-05, global_step: 1700, interval_runtime: 14.2348, interval_samples_per_second: 1.124, interval_steps_per_second: 0.703, epoch: 13.2812[0m
[32m[2022-09-19 16:14:11,536] [    INFO][0m - loss: 3.307e-05, learning_rate: 9.9609375e-06, global_step: 1710, interval_runtime: 14.2528, interval_samples_per_second: 1.123, interval_steps_per_second: 0.702, epoch: 13.3594[0m
[32m[2022-09-19 16:14:25,894] [    INFO][0m - loss: 3.733e-05, learning_rate: 9.84375e-06, global_step: 1720, interval_runtime: 14.3574, interval_samples_per_second: 1.114, interval_steps_per_second: 0.697, epoch: 13.4375[0m
[32m[2022-09-19 16:14:40,211] [    INFO][0m - loss: 3.738e-05, learning_rate: 9.7265625e-06, global_step: 1730, interval_runtime: 14.3175, interval_samples_per_second: 1.118, interval_steps_per_second: 0.698, epoch: 13.5156[0m
[32m[2022-09-19 16:14:54,439] [    INFO][0m - loss: 5.371e-05, learning_rate: 9.609375e-06, global_step: 1740, interval_runtime: 14.228, interval_samples_per_second: 1.125, interval_steps_per_second: 0.703, epoch: 13.5938[0m
[32m[2022-09-19 16:15:08,685] [    INFO][0m - loss: 2.346e-05, learning_rate: 9.4921875e-06, global_step: 1750, interval_runtime: 14.2462, interval_samples_per_second: 1.123, interval_steps_per_second: 0.702, epoch: 13.6719[0m
[32m[2022-09-19 16:15:22,926] [    INFO][0m - loss: 3.124e-05, learning_rate: 9.375000000000001e-06, global_step: 1760, interval_runtime: 14.2412, interval_samples_per_second: 1.124, interval_steps_per_second: 0.702, epoch: 13.75[0m
[32m[2022-09-19 16:15:37,168] [    INFO][0m - loss: 3.54e-05, learning_rate: 9.2578125e-06, global_step: 1770, interval_runtime: 14.2423, interval_samples_per_second: 1.123, interval_steps_per_second: 0.702, epoch: 13.8281[0m
[32m[2022-09-19 16:15:51,436] [    INFO][0m - loss: 4.855e-05, learning_rate: 9.140625e-06, global_step: 1780, interval_runtime: 14.2674, interval_samples_per_second: 1.121, interval_steps_per_second: 0.701, epoch: 13.9062[0m
[32m[2022-09-19 16:16:05,639] [    INFO][0m - loss: 5.131e-05, learning_rate: 9.0234375e-06, global_step: 1790, interval_runtime: 14.2023, interval_samples_per_second: 1.127, interval_steps_per_second: 0.704, epoch: 13.9844[0m
[32m[2022-09-19 16:16:07,476] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 16:16:07,477] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-19 16:16:07,477] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 16:16:07,477] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 16:16:07,477] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-19 16:17:14,240] [    INFO][0m - eval_loss: 3.3268659114837646, eval_accuracy: 0.5904255319148937, eval_runtime: 66.7632, eval_samples_per_second: 30.975, eval_steps_per_second: 1.947, epoch: 14.0[0m
[32m[2022-09-19 16:17:14,269] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-1792[0m
[32m[2022-09-19 16:17:14,270] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 16:17:16,957] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-1792/tokenizer_config.json[0m
[32m[2022-09-19 16:17:16,958] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-1792/special_tokens_map.json[0m
[32m[2022-09-19 16:17:30,244] [    INFO][0m - loss: 3.474e-05, learning_rate: 8.90625e-06, global_step: 1800, interval_runtime: 84.6058, interval_samples_per_second: 0.189, interval_steps_per_second: 0.118, epoch: 14.0625[0m
[32m[2022-09-19 16:17:36,771] [    INFO][0m - loss: 3.644e-05, learning_rate: 8.789062500000001e-06, global_step: 1810, interval_runtime: 6.5268, interval_samples_per_second: 2.451, interval_steps_per_second: 1.532, epoch: 14.1406[0m
[32m[2022-09-19 16:17:43,268] [    INFO][0m - loss: 3.62e-05, learning_rate: 8.671875e-06, global_step: 1820, interval_runtime: 6.4977, interval_samples_per_second: 2.462, interval_steps_per_second: 1.539, epoch: 14.2188[0m
[32m[2022-09-19 16:17:53,136] [    INFO][0m - loss: 3.365e-05, learning_rate: 8.5546875e-06, global_step: 1830, interval_runtime: 9.8676, interval_samples_per_second: 1.621, interval_steps_per_second: 1.013, epoch: 14.2969[0m
[32m[2022-09-19 16:18:07,345] [    INFO][0m - loss: 3.213e-05, learning_rate: 8.4375e-06, global_step: 1840, interval_runtime: 14.2087, interval_samples_per_second: 1.126, interval_steps_per_second: 0.704, epoch: 14.375[0m
[32m[2022-09-19 16:18:21,599] [    INFO][0m - loss: 2.419e-05, learning_rate: 8.3203125e-06, global_step: 1850, interval_runtime: 14.2541, interval_samples_per_second: 1.122, interval_steps_per_second: 0.702, epoch: 14.4531[0m
[32m[2022-09-19 16:18:35,791] [    INFO][0m - loss: 9.767e-05, learning_rate: 8.203125e-06, global_step: 1860, interval_runtime: 14.1919, interval_samples_per_second: 1.127, interval_steps_per_second: 0.705, epoch: 14.5312[0m
[32m[2022-09-19 16:18:50,002] [    INFO][0m - loss: 4.231e-05, learning_rate: 8.085937500000001e-06, global_step: 1870, interval_runtime: 14.2115, interval_samples_per_second: 1.126, interval_steps_per_second: 0.704, epoch: 14.6094[0m
[32m[2022-09-19 16:19:04,246] [    INFO][0m - loss: 1.887e-05, learning_rate: 7.96875e-06, global_step: 1880, interval_runtime: 14.2429, interval_samples_per_second: 1.123, interval_steps_per_second: 0.702, epoch: 14.6875[0m
[32m[2022-09-19 16:19:18,481] [    INFO][0m - loss: 2.942e-05, learning_rate: 7.8515625e-06, global_step: 1890, interval_runtime: 14.2358, interval_samples_per_second: 1.124, interval_steps_per_second: 0.702, epoch: 14.7656[0m
[32m[2022-09-19 16:19:32,602] [    INFO][0m - loss: 8.519e-05, learning_rate: 7.734375e-06, global_step: 1900, interval_runtime: 14.1207, interval_samples_per_second: 1.133, interval_steps_per_second: 0.708, epoch: 14.8438[0m
[32m[2022-09-19 16:19:46,754] [    INFO][0m - loss: 3.438e-05, learning_rate: 7.6171875000000005e-06, global_step: 1910, interval_runtime: 14.1526, interval_samples_per_second: 1.131, interval_steps_per_second: 0.707, epoch: 14.9219[0m
[32m[2022-09-19 16:19:59,983] [    INFO][0m - loss: 0.0005287, learning_rate: 7.5e-06, global_step: 1920, interval_runtime: 13.2286, interval_samples_per_second: 1.209, interval_steps_per_second: 0.756, epoch: 15.0[0m
[32m[2022-09-19 16:19:59,984] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 16:19:59,984] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-19 16:19:59,984] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 16:19:59,984] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 16:19:59,984] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-19 16:21:09,946] [    INFO][0m - eval_loss: 3.3435845375061035, eval_accuracy: 0.5952611218568665, eval_runtime: 69.961, eval_samples_per_second: 29.559, eval_steps_per_second: 1.858, epoch: 15.0[0m
[32m[2022-09-19 16:21:09,976] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-1920[0m
[32m[2022-09-19 16:21:09,976] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 16:21:12,665] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-1920/tokenizer_config.json[0m
[32m[2022-09-19 16:21:12,666] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-1920/special_tokens_map.json[0m
[32m[2022-09-19 16:21:31,029] [    INFO][0m - loss: 3.139e-05, learning_rate: 7.3828125e-06, global_step: 1930, interval_runtime: 91.0462, interval_samples_per_second: 0.176, interval_steps_per_second: 0.11, epoch: 15.0781[0m
[32m[2022-09-19 16:21:42,787] [    INFO][0m - loss: 2.688e-05, learning_rate: 7.265625e-06, global_step: 1940, interval_runtime: 11.7577, interval_samples_per_second: 1.361, interval_steps_per_second: 0.851, epoch: 15.1562[0m
[32m[2022-09-19 16:21:49,311] [    INFO][0m - loss: 8.137e-05, learning_rate: 7.1484375e-06, global_step: 1950, interval_runtime: 6.5244, interval_samples_per_second: 2.452, interval_steps_per_second: 1.533, epoch: 15.2344[0m
[32m[2022-09-19 16:21:55,854] [    INFO][0m - loss: 7.014e-05, learning_rate: 7.03125e-06, global_step: 1960, interval_runtime: 6.5428, interval_samples_per_second: 2.445, interval_steps_per_second: 1.528, epoch: 15.3125[0m
[32m[2022-09-19 16:22:03,944] [    INFO][0m - loss: 4.192e-05, learning_rate: 6.9140625e-06, global_step: 1970, interval_runtime: 8.0896, interval_samples_per_second: 1.978, interval_steps_per_second: 1.236, epoch: 15.3906[0m
[32m[2022-09-19 16:22:18,213] [    INFO][0m - loss: 4.448e-05, learning_rate: 6.796875e-06, global_step: 1980, interval_runtime: 14.269, interval_samples_per_second: 1.121, interval_steps_per_second: 0.701, epoch: 15.4688[0m
[32m[2022-09-19 16:22:32,432] [    INFO][0m - loss: 8.951e-05, learning_rate: 6.679687500000001e-06, global_step: 1990, interval_runtime: 14.2186, interval_samples_per_second: 1.125, interval_steps_per_second: 0.703, epoch: 15.5469[0m
[32m[2022-09-19 16:22:46,626] [    INFO][0m - loss: 6.029e-05, learning_rate: 6.5625e-06, global_step: 2000, interval_runtime: 14.1947, interval_samples_per_second: 1.127, interval_steps_per_second: 0.704, epoch: 15.625[0m
[32m[2022-09-19 16:23:00,902] [    INFO][0m - loss: 5.993e-05, learning_rate: 6.4453125e-06, global_step: 2010, interval_runtime: 14.2762, interval_samples_per_second: 1.121, interval_steps_per_second: 0.7, epoch: 15.7031[0m
[32m[2022-09-19 16:23:15,110] [    INFO][0m - loss: 2.027e-05, learning_rate: 6.3281250000000005e-06, global_step: 2020, interval_runtime: 14.2075, interval_samples_per_second: 1.126, interval_steps_per_second: 0.704, epoch: 15.7812[0m
[32m[2022-09-19 16:23:29,338] [    INFO][0m - loss: 3.357e-05, learning_rate: 6.2109375e-06, global_step: 2030, interval_runtime: 14.2279, interval_samples_per_second: 1.125, interval_steps_per_second: 0.703, epoch: 15.8594[0m
[32m[2022-09-19 16:23:43,603] [    INFO][0m - loss: 3.015e-05, learning_rate: 6.09375e-06, global_step: 2040, interval_runtime: 14.2653, interval_samples_per_second: 1.122, interval_steps_per_second: 0.701, epoch: 15.9375[0m
[32m[2022-09-19 16:23:53,915] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 16:23:53,915] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-19 16:23:53,915] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 16:23:53,916] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 16:23:53,916] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-19 16:25:04,282] [    INFO][0m - eval_loss: 3.349987268447876, eval_accuracy: 0.5909090909090909, eval_runtime: 70.3661, eval_samples_per_second: 29.389, eval_steps_per_second: 1.847, epoch: 16.0[0m
[32m[2022-09-19 16:25:04,311] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-2048[0m
[32m[2022-09-19 16:25:04,312] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 16:25:06,987] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-2048/tokenizer_config.json[0m
[32m[2022-09-19 16:25:06,987] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-2048/special_tokens_map.json[0m
[32m[2022-09-19 16:25:14,885] [    INFO][0m - loss: 2.629e-05, learning_rate: 5.9765625000000004e-06, global_step: 2050, interval_runtime: 91.2818, interval_samples_per_second: 0.175, interval_steps_per_second: 0.11, epoch: 16.0156[0m
[32m[2022-09-19 16:25:28,184] [    INFO][0m - loss: 3.15e-05, learning_rate: 5.859375e-06, global_step: 2060, interval_runtime: 13.2989, interval_samples_per_second: 1.203, interval_steps_per_second: 0.752, epoch: 16.0938[0m
[32m[2022-09-19 16:25:41,323] [    INFO][0m - loss: 2.26e-05, learning_rate: 5.7421875e-06, global_step: 2070, interval_runtime: 13.1398, interval_samples_per_second: 1.218, interval_steps_per_second: 0.761, epoch: 16.1719[0m
[32m[2022-09-19 16:25:54,543] [    INFO][0m - loss: 2.458e-05, learning_rate: 5.625e-06, global_step: 2080, interval_runtime: 13.2198, interval_samples_per_second: 1.21, interval_steps_per_second: 0.756, epoch: 16.25[0m
[32m[2022-09-19 16:26:01,082] [    INFO][0m - loss: 2.845e-05, learning_rate: 5.5078125e-06, global_step: 2090, interval_runtime: 6.5387, interval_samples_per_second: 2.447, interval_steps_per_second: 1.529, epoch: 16.3281[0m
[32m[2022-09-19 16:26:07,596] [    INFO][0m - loss: 2.569e-05, learning_rate: 5.390625e-06, global_step: 2100, interval_runtime: 6.5138, interval_samples_per_second: 2.456, interval_steps_per_second: 1.535, epoch: 16.4062[0m
[32m[2022-09-19 16:26:14,141] [    INFO][0m - loss: 7.216e-05, learning_rate: 5.2734375e-06, global_step: 2110, interval_runtime: 6.5446, interval_samples_per_second: 2.445, interval_steps_per_second: 1.528, epoch: 16.4844[0m
[32m[2022-09-19 16:26:28,337] [    INFO][0m - loss: 2.709e-05, learning_rate: 5.15625e-06, global_step: 2120, interval_runtime: 14.1965, interval_samples_per_second: 1.127, interval_steps_per_second: 0.704, epoch: 16.5625[0m
[32m[2022-09-19 16:26:42,545] [    INFO][0m - loss: 5.011e-05, learning_rate: 5.0390625000000005e-06, global_step: 2130, interval_runtime: 14.2085, interval_samples_per_second: 1.126, interval_steps_per_second: 0.704, epoch: 16.6406[0m
[32m[2022-09-19 16:26:56,670] [    INFO][0m - loss: 5.073e-05, learning_rate: 4.921875e-06, global_step: 2140, interval_runtime: 14.125, interval_samples_per_second: 1.133, interval_steps_per_second: 0.708, epoch: 16.7188[0m
[32m[2022-09-19 16:27:10,891] [    INFO][0m - loss: 4.347e-05, learning_rate: 4.8046875e-06, global_step: 2150, interval_runtime: 14.2208, interval_samples_per_second: 1.125, interval_steps_per_second: 0.703, epoch: 16.7969[0m
[32m[2022-09-19 16:27:25,137] [    INFO][0m - loss: 4.028e-05, learning_rate: 4.6875000000000004e-06, global_step: 2160, interval_runtime: 14.2463, interval_samples_per_second: 1.123, interval_steps_per_second: 0.702, epoch: 16.875[0m
[32m[2022-09-19 16:27:39,414] [    INFO][0m - loss: 2.823e-05, learning_rate: 4.5703125e-06, global_step: 2170, interval_runtime: 14.2762, interval_samples_per_second: 1.121, interval_steps_per_second: 0.7, epoch: 16.9531[0m
[32m[2022-09-19 16:27:46,852] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 16:27:46,852] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-19 16:27:46,852] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 16:27:46,852] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 16:27:46,852] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-19 16:28:57,582] [    INFO][0m - eval_loss: 3.3542110919952393, eval_accuracy: 0.5918762088974855, eval_runtime: 70.7297, eval_samples_per_second: 29.238, eval_steps_per_second: 1.838, epoch: 17.0[0m
[32m[2022-09-19 16:28:57,612] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-2176[0m
[32m[2022-09-19 16:28:57,612] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 16:29:00,291] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-2176/tokenizer_config.json[0m
[32m[2022-09-19 16:29:00,291] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-2176/special_tokens_map.json[0m
[32m[2022-09-19 16:29:11,046] [    INFO][0m - loss: 1.812e-05, learning_rate: 4.453125e-06, global_step: 2180, interval_runtime: 91.6323, interval_samples_per_second: 0.175, interval_steps_per_second: 0.109, epoch: 17.0312[0m
[32m[2022-09-19 16:29:24,114] [    INFO][0m - loss: 3.435e-05, learning_rate: 4.3359375e-06, global_step: 2190, interval_runtime: 13.0677, interval_samples_per_second: 1.224, interval_steps_per_second: 0.765, epoch: 17.1094[0m
[32m[2022-09-19 16:29:37,277] [    INFO][0m - loss: 2.417e-05, learning_rate: 4.21875e-06, global_step: 2200, interval_runtime: 13.1629, interval_samples_per_second: 1.216, interval_steps_per_second: 0.76, epoch: 17.1875[0m
[32m[2022-09-19 16:29:50,433] [    INFO][0m - loss: 3.224e-05, learning_rate: 4.1015625e-06, global_step: 2210, interval_runtime: 13.1562, interval_samples_per_second: 1.216, interval_steps_per_second: 0.76, epoch: 17.2656[0m
[32m[2022-09-19 16:30:03,507] [    INFO][0m - loss: 2.666e-05, learning_rate: 3.984375e-06, global_step: 2220, interval_runtime: 13.0745, interval_samples_per_second: 1.224, interval_steps_per_second: 0.765, epoch: 17.3438[0m
[32m[2022-09-19 16:30:12,107] [    INFO][0m - loss: 5.526e-05, learning_rate: 3.8671875e-06, global_step: 2230, interval_runtime: 8.5999, interval_samples_per_second: 1.86, interval_steps_per_second: 1.163, epoch: 17.4219[0m
[32m[2022-09-19 16:30:18,612] [    INFO][0m - loss: 3.016e-05, learning_rate: 3.75e-06, global_step: 2240, interval_runtime: 6.5045, interval_samples_per_second: 2.46, interval_steps_per_second: 1.537, epoch: 17.5[0m
[32m[2022-09-19 16:30:25,140] [    INFO][0m - loss: 2.174e-05, learning_rate: 3.6328125e-06, global_step: 2250, interval_runtime: 6.5278, interval_samples_per_second: 2.451, interval_steps_per_second: 1.532, epoch: 17.5781[0m
[32m[2022-09-19 16:30:37,274] [    INFO][0m - loss: 2.96e-05, learning_rate: 3.515625e-06, global_step: 2260, interval_runtime: 12.1342, interval_samples_per_second: 1.319, interval_steps_per_second: 0.824, epoch: 17.6562[0m
[32m[2022-09-19 16:30:51,475] [    INFO][0m - loss: 3.382e-05, learning_rate: 3.3984375e-06, global_step: 2270, interval_runtime: 14.201, interval_samples_per_second: 1.127, interval_steps_per_second: 0.704, epoch: 17.7344[0m
[32m[2022-09-19 16:31:05,706] [    INFO][0m - loss: 2.196e-05, learning_rate: 3.28125e-06, global_step: 2280, interval_runtime: 14.2306, interval_samples_per_second: 1.124, interval_steps_per_second: 0.703, epoch: 17.8125[0m
[32m[2022-09-19 16:31:19,838] [    INFO][0m - loss: 5.58e-05, learning_rate: 3.1640625000000003e-06, global_step: 2290, interval_runtime: 14.1323, interval_samples_per_second: 1.132, interval_steps_per_second: 0.708, epoch: 17.8906[0m
[32m[2022-09-19 16:31:33,955] [    INFO][0m - loss: 1.979e-05, learning_rate: 3.046875e-06, global_step: 2300, interval_runtime: 14.1172, interval_samples_per_second: 1.133, interval_steps_per_second: 0.708, epoch: 17.9688[0m
[32m[2022-09-19 16:31:38,500] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 16:31:38,501] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-19 16:31:38,501] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 16:31:38,501] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 16:31:38,501] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-19 16:32:49,091] [    INFO][0m - eval_loss: 3.351029634475708, eval_accuracy: 0.5899419729206963, eval_runtime: 70.5896, eval_samples_per_second: 29.296, eval_steps_per_second: 1.842, epoch: 18.0[0m
[32m[2022-09-19 16:32:49,120] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-2304[0m
[32m[2022-09-19 16:32:49,120] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 16:32:51,796] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-2304/tokenizer_config.json[0m
[32m[2022-09-19 16:32:51,796] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-2304/special_tokens_map.json[0m
[32m[2022-09-19 16:33:05,711] [    INFO][0m - loss: 2.761e-05, learning_rate: 2.9296875e-06, global_step: 2310, interval_runtime: 91.7559, interval_samples_per_second: 0.174, interval_steps_per_second: 0.109, epoch: 18.0469[0m
[32m[2022-09-19 16:33:19,243] [    INFO][0m - loss: 2.921e-05, learning_rate: 2.8125e-06, global_step: 2320, interval_runtime: 13.5317, interval_samples_per_second: 1.182, interval_steps_per_second: 0.739, epoch: 18.125[0m
[32m[2022-09-19 16:33:32,414] [    INFO][0m - loss: 1.994e-05, learning_rate: 2.6953125e-06, global_step: 2330, interval_runtime: 13.1714, interval_samples_per_second: 1.215, interval_steps_per_second: 0.759, epoch: 18.2031[0m
[32m[2022-09-19 16:33:45,575] [    INFO][0m - loss: 1.33e-05, learning_rate: 2.578125e-06, global_step: 2340, interval_runtime: 13.1604, interval_samples_per_second: 1.216, interval_steps_per_second: 0.76, epoch: 18.2812[0m
[32m[2022-09-19 16:33:58,905] [    INFO][0m - loss: 2.222e-05, learning_rate: 2.4609375e-06, global_step: 2350, interval_runtime: 13.3301, interval_samples_per_second: 1.2, interval_steps_per_second: 0.75, epoch: 18.3594[0m
[32m[2022-09-19 16:34:11,961] [    INFO][0m - loss: 7.169e-05, learning_rate: 2.3437500000000002e-06, global_step: 2360, interval_runtime: 13.0562, interval_samples_per_second: 1.225, interval_steps_per_second: 0.766, epoch: 18.4375[0m
[32m[2022-09-19 16:34:23,131] [    INFO][0m - loss: 2.256e-05, learning_rate: 2.2265625e-06, global_step: 2370, interval_runtime: 11.1705, interval_samples_per_second: 1.432, interval_steps_per_second: 0.895, epoch: 18.5156[0m
[32m[2022-09-19 16:34:29,647] [    INFO][0m - loss: 4.283e-05, learning_rate: 2.109375e-06, global_step: 2380, interval_runtime: 6.5154, interval_samples_per_second: 2.456, interval_steps_per_second: 1.535, epoch: 18.5938[0m
[32m[2022-09-19 16:34:36,166] [    INFO][0m - loss: 1.714e-05, learning_rate: 1.9921875e-06, global_step: 2390, interval_runtime: 6.5187, interval_samples_per_second: 2.454, interval_steps_per_second: 1.534, epoch: 18.6719[0m
[32m[2022-09-19 16:34:44,303] [    INFO][0m - loss: 2.555e-05, learning_rate: 1.875e-06, global_step: 2400, interval_runtime: 8.1376, interval_samples_per_second: 1.966, interval_steps_per_second: 1.229, epoch: 18.75[0m
[32m[2022-09-19 16:34:58,512] [    INFO][0m - loss: 3.177e-05, learning_rate: 1.7578125e-06, global_step: 2410, interval_runtime: 14.2088, interval_samples_per_second: 1.126, interval_steps_per_second: 0.704, epoch: 18.8281[0m
[32m[2022-09-19 16:35:12,759] [    INFO][0m - loss: 3.197e-05, learning_rate: 1.640625e-06, global_step: 2420, interval_runtime: 14.2475, interval_samples_per_second: 1.123, interval_steps_per_second: 0.702, epoch: 18.9062[0m
[32m[2022-09-19 16:35:26,919] [    INFO][0m - loss: 2.661e-05, learning_rate: 1.5234375e-06, global_step: 2430, interval_runtime: 14.1592, interval_samples_per_second: 1.13, interval_steps_per_second: 0.706, epoch: 18.9844[0m
[32m[2022-09-19 16:35:28,752] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 16:35:28,753] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-19 16:35:28,753] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 16:35:28,753] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 16:35:28,753] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-19 16:36:39,677] [    INFO][0m - eval_loss: 3.347627878189087, eval_accuracy: 0.5909090909090909, eval_runtime: 70.9239, eval_samples_per_second: 29.158, eval_steps_per_second: 1.833, epoch: 19.0[0m
[32m[2022-09-19 16:36:39,707] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-2432[0m
[32m[2022-09-19 16:36:39,707] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 16:36:42,368] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-2432/tokenizer_config.json[0m
[32m[2022-09-19 16:36:42,368] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-2432/special_tokens_map.json[0m
[32m[2022-09-19 16:36:58,994] [    INFO][0m - loss: 5.231e-05, learning_rate: 1.40625e-06, global_step: 2440, interval_runtime: 92.0745, interval_samples_per_second: 0.174, interval_steps_per_second: 0.109, epoch: 19.0625[0m
[32m[2022-09-19 16:37:13,202] [    INFO][0m - loss: 2.312e-05, learning_rate: 1.2890625e-06, global_step: 2450, interval_runtime: 14.2083, interval_samples_per_second: 1.126, interval_steps_per_second: 0.704, epoch: 19.1406[0m
[32m[2022-09-19 16:37:27,180] [    INFO][0m - loss: 2.098e-05, learning_rate: 1.1718750000000001e-06, global_step: 2460, interval_runtime: 13.9785, interval_samples_per_second: 1.145, interval_steps_per_second: 0.715, epoch: 19.2188[0m
[32m[2022-09-19 16:37:40,373] [    INFO][0m - loss: 1.93e-05, learning_rate: 1.0546875e-06, global_step: 2470, interval_runtime: 13.1931, interval_samples_per_second: 1.213, interval_steps_per_second: 0.758, epoch: 19.2969[0m
[32m[2022-09-19 16:37:53,450] [    INFO][0m - loss: 3.558e-05, learning_rate: 9.375e-07, global_step: 2480, interval_runtime: 13.0769, interval_samples_per_second: 1.224, interval_steps_per_second: 0.765, epoch: 19.375[0m
[32m[2022-09-19 16:38:06,723] [    INFO][0m - loss: 2.288e-05, learning_rate: 8.203125e-07, global_step: 2490, interval_runtime: 13.2724, interval_samples_per_second: 1.206, interval_steps_per_second: 0.753, epoch: 19.4531[0m
[32m[2022-09-19 16:38:19,921] [    INFO][0m - loss: 1.87e-05, learning_rate: 7.03125e-07, global_step: 2500, interval_runtime: 13.1991, interval_samples_per_second: 1.212, interval_steps_per_second: 0.758, epoch: 19.5312[0m
[32m[2022-09-19 16:38:33,112] [    INFO][0m - loss: 1.702e-05, learning_rate: 5.859375000000001e-07, global_step: 2510, interval_runtime: 13.1909, interval_samples_per_second: 1.213, interval_steps_per_second: 0.758, epoch: 19.6094[0m
[32m[2022-09-19 16:38:40,385] [    INFO][0m - loss: 1.509e-05, learning_rate: 4.6875e-07, global_step: 2520, interval_runtime: 7.273, interval_samples_per_second: 2.2, interval_steps_per_second: 1.375, epoch: 19.6875[0m
[32m[2022-09-19 16:38:46,897] [    INFO][0m - loss: 2.202e-05, learning_rate: 3.515625e-07, global_step: 2530, interval_runtime: 6.5118, interval_samples_per_second: 2.457, interval_steps_per_second: 1.536, epoch: 19.7656[0m
[32m[2022-09-19 16:38:53,425] [    INFO][0m - loss: 2.768e-05, learning_rate: 2.34375e-07, global_step: 2540, interval_runtime: 6.5278, interval_samples_per_second: 2.451, interval_steps_per_second: 1.532, epoch: 19.8438[0m
[32m[2022-09-19 16:39:06,916] [    INFO][0m - loss: 2.94e-05, learning_rate: 1.171875e-07, global_step: 2550, interval_runtime: 13.4914, interval_samples_per_second: 1.186, interval_steps_per_second: 0.741, epoch: 19.9219[0m
[32m[2022-09-19 16:39:20,103] [    INFO][0m - loss: 1.628e-05, learning_rate: 0.0, global_step: 2560, interval_runtime: 13.1865, interval_samples_per_second: 1.213, interval_steps_per_second: 0.758, epoch: 20.0[0m
[32m[2022-09-19 16:39:20,103] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 16:39:20,104] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-19 16:39:20,104] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 16:39:20,104] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 16:39:20,104] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-19 16:40:30,788] [    INFO][0m - eval_loss: 3.3486616611480713, eval_accuracy: 0.5904255319148937, eval_runtime: 70.6833, eval_samples_per_second: 29.257, eval_steps_per_second: 1.839, epoch: 20.0[0m
[32m[2022-09-19 16:40:30,819] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-2560[0m
[32m[2022-09-19 16:40:30,819] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 16:40:33,489] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-2560/tokenizer_config.json[0m
[32m[2022-09-19 16:40:33,489] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-2560/special_tokens_map.json[0m
[32m[2022-09-19 16:40:38,519] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-19 16:40:38,520] [    INFO][0m - Loading best model from ./checkpoints_csldcp/checkpoint-1920 (score: 0.5952611218568665).[0m
[32m[2022-09-19 16:40:41,153] [    INFO][0m - train_runtime: 4633.6948, train_samples_per_second: 8.788, train_steps_per_second: 0.552, train_loss: 0.20077536081711855, epoch: 20.0[0m
[32m[2022-09-19 16:40:41,155] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/[0m
[32m[2022-09-19 16:40:41,155] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 16:40:43,666] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/tokenizer_config.json[0m
[32m[2022-09-19 16:40:43,666] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/special_tokens_map.json[0m
[32m[2022-09-19 16:40:43,668] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-19 16:40:43,668] [    INFO][0m -   epoch                    =       20.0[0m
[32m[2022-09-19 16:40:43,668] [    INFO][0m -   train_loss               =     0.2008[0m
[32m[2022-09-19 16:40:43,668] [    INFO][0m -   train_runtime            = 1:17:13.69[0m
[32m[2022-09-19 16:40:43,668] [    INFO][0m -   train_samples_per_second =      8.788[0m
[32m[2022-09-19 16:40:43,668] [    INFO][0m -   train_steps_per_second   =      0.552[0m
[32m[2022-09-19 16:40:43,679] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-19 16:40:43,679] [    INFO][0m -   Num examples = 1784[0m
[32m[2022-09-19 16:40:43,679] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 16:40:43,679] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 16:40:43,679] [    INFO][0m -   Total prediction steps = 112[0m
[32m[2022-09-19 16:41:43,323] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-19 16:41:43,324] [    INFO][0m -   test_accuracy           =     0.5762[0m
[32m[2022-09-19 16:41:43,324] [    INFO][0m -   test_loss               =     3.4082[0m
[32m[2022-09-19 16:41:43,324] [    INFO][0m -   test_runtime            = 0:00:59.64[0m
[32m[2022-09-19 16:41:43,324] [    INFO][0m -   test_samples_per_second =      29.91[0m
[32m[2022-09-19 16:41:43,324] [    INFO][0m -   test_steps_per_second   =      1.878[0m
[32m[2022-09-19 16:41:43,325] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-19 16:41:43,325] [    INFO][0m -   Num examples = 2999[0m
[32m[2022-09-19 16:41:43,325] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 16:41:43,325] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 16:41:43,325] [    INFO][0m -   Total prediction steps = 188[0m
[32m[2022-09-19 16:43:08,200] [    INFO][0m - Predictions for csldcp saved to ./fewclue_submit_examples.[0m
{
  "labels": 26,
  "text_a": "\u672c\u6587\u7814\u7a76\u4e86\u8d35\u5dde\u4e1c\u5357\u90e8\u7684\u5929\u67f1\u3001\u9526\u5c4f\u548c\u9ece\u5e73\u5730\u533a\u7684\u5730\u8d28\u6784\u9020\u7279\u5f81\u3001\u8be5\u533a\u91d1\u77ff\u5e8a\u7279\u5f81\u53ca\u5176\u4e0e\u5730\u8d28\u6784\u9020\u7684\u5173\u7cfb.\u8ba4\u4e3a\u533a\u5185\u5317\u90e8\u548c\u5357\u90e8\u76f8\u8ddd\u6570\u5341\u516c\u91cc\u7684\u4e24\u6761\u8fd1\u4e1c\u897f\u5411\u7684\u57fa\u5e95\u526a\u5207\u65ad\u88c2\u5e26\u6784\u6210\u4e86\u672c\u533a\u5730\u5811\u5f0f\u7684\u6784\u9020\u683c\u5c40.\u7531\u4e8e\u8fd9\u4e24\u6761\u4e1c\u897f\u5411\u526a\u5207\u65ad\u88c2\u7684\u526a\u5207\u4f5c\u7528,\u5f62\u6210\u4e86\u672c\u533a\u4ee5\u5317\u4e1c\u5411\u4e3a\u4e3b\u7684\u8936\u76b1\u548c\u526a\u5207\u5e26.\u5317\u4e1c\u5411\u8936\u76b1\u548c\u526a\u5207\u5e26\u6784\u9020\u662f\u5728\u52a0\u91cc\u4e1c\u671f\u5f62\u6210\u7684,\u540c\u65f6\u4e5f\u53d1\u751f\u4e86\u7eff\u7247\u5ca9\u76f8\u7684\u53d8\u8d28\u4f5c\u7528.\u5728\u80cc\u659c\u5f62\u6210\u7684\u540c\u65f6\u6216\u7a0d\u540e\u526a\u5207\u4f5c\u7528\u5f00\u59cb\u53d1\u751f,\u526a\u5207\u4f5c\u7528\u7ee7\u627f\u7740\u5317\u4e1c\u5411,\u5f62\u6210\u4e86\u82e5\u5e72\u6761\u4e0e\u8936\u76b1\u8f74\u5e73\u884c\u6216\u76f8\u4ea4\u7684\u526a\u5207\u5e26.\u6210\u77ff\u6d41\u4f53\u6cbf\u7740\u526a\u5207\u5e26\u4e0a\u5347,\u4e00\u65b9\u9762\u5728\u526a\u5207\u5e26\u4e2d\u6c89\u6dc0\u51fa\u7a7f\u5c42\u4ea7\u51fa\u7684\u4e0d\u6574\u5408\u7834\u788e\u5e26\u578b\u91d1\u77ff\u4f53,\u5373\u900f\u955c\u72b6\u542b\u91d1\u77f3\u82f1\u8109;\u540c\u65f6\u5927\u90e8\u5206\u5145\u586b\u5230\u7531\u80cc\u659c\u548c\u526a\u5207\u4f5c\u7528\u5f62\u6210\u7684\u5c42\u95f4\u88c2\u9699\u6216\u5c42\u95f4\u7834\u788e\u5e26\u4e2d,\u4ece\u800c\u5f62\u6210\u8d4b\u5b58\u4e8e\u6d4a\u79ef\u5ca9\u7684\u987a\u5c42\u4ea7\u51fa\u7684\u5c42\u72b6\u542b\u91d1\u77f3\u82f1\u8109\u91d1\u77ff\u5e8a.\u8fd9\u79cd\u7c7b\u578b\u7684\u542b\u91d1\u77f3\u82f1\u8109,\u65e0\u8bba\u5728\u54c1\u4f4d(\u591a\u6570\u53ef\u89c1\u660e\u91d1)\u3001\u50a8\u91cf\u548c\u4ea7\u91cf\u65b9\u9762\u5747\u5f88\u6709\u524d\u666f,\u5e76\u4e14\u6709\u7740\u5341\u5206\u91cd\u8981\u7684\u5b66\u672f\u610f\u4e49,\u56e0\u4e3a\u8fd9\u5728\u56fd\u5185\u8fd8\u662f\u9996\u6b21\u53d1\u73b0.",
  "text_b": "",
  "uid": 44
}

Prediction done.
rm: cannot remove â€˜./checkpoints_csldcp/â€™: Directory not empty
 
==========
tnews
==========
 
[32m[2022-09-19 16:43:24,431] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-19 16:43:24,431] [    INFO][0m - ============================================================[0m
[32m[2022-09-19 16:43:24,432] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-19 16:43:24,432] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-19 16:43:24,432] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-19 16:43:24,432] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-19 16:43:24,432] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-19 16:43:24,432] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-19 16:43:24,432] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-19 16:43:24,432] [    INFO][0m - [0m
[32m[2022-09-19 16:43:24,432] [    INFO][0m - ============================================================[0m
[32m[2022-09-19 16:43:24,432] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-19 16:43:24,432] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-19 16:43:24,432] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-19 16:43:24,433] [    INFO][0m - pretrained                    :/ssd2/wanghuijuan03/data/zero-shot/0919_model_state.pdparams[0m
[32m[2022-09-19 16:43:24,433] [    INFO][0m - prompt                        :â€œ{'text':'text_a'}â€ä¸Šè¿°æ–°é—»é€‰è‡ª{'mask'}{'mask'}ä¸“æ ã€‚[0m
[32m[2022-09-19 16:43:24,433] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-19 16:43:24,433] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-19 16:43:24,433] [    INFO][0m - task_name                     :tnews[0m
[32m[2022-09-19 16:43:24,433] [    INFO][0m - [0m
[32m[2022-09-19 16:43:24,433] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0919 16:43:24.434762 76115 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0919 16:43:24.438632 76115 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-19 16:43:32,310] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-19 16:43:32,321] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-19 16:43:32,321] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-19 16:43:32,322] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': 'â€œ'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': 'â€ä¸Šè¿°æ–°é—»é€‰è‡ª'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'ä¸“æ ã€‚'}][0m
[32m[2022-09-19 16:43:33,986] [    INFO][0m - ============================================================[0m
[32m[2022-09-19 16:43:33,987] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-19 16:43:33,987] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-19 16:43:33,987] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-19 16:43:33,987] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-19 16:43:33,987] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-19 16:43:33,987] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-19 16:43:33,987] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-19 16:43:33,987] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-19 16:43:33,987] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-19 16:43:33,987] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-19 16:43:33,987] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-19 16:43:33,988] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-19 16:43:33,988] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-19 16:43:33,988] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-19 16:43:33,988] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-19 16:43:33,988] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-19 16:43:33,988] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-19 16:43:33,988] [    INFO][0m - eval_batch_size               :16[0m
[32m[2022-09-19 16:43:33,988] [    INFO][0m - eval_steps                    :200[0m
[32m[2022-09-19 16:43:33,988] [    INFO][0m - evaluation_strategy           :IntervalStrategy.EPOCH[0m
[32m[2022-09-19 16:43:33,988] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-19 16:43:33,988] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-19 16:43:33,988] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-19 16:43:33,988] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-19 16:43:33,988] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-19 16:43:33,989] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-19 16:43:33,989] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-19 16:43:33,989] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-19 16:43:33,989] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-19 16:43:33,989] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-09-19 16:43:33,989] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-19 16:43:33,989] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-19 16:43:33,989] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-19 16:43:33,989] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-19 16:43:33,989] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-19 16:43:33,989] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-19 16:43:33,989] [    INFO][0m - logging_dir                   :./checkpoints_tnews/runs/Sep19_16-43-24_instance-3bwob41y-01[0m
[32m[2022-09-19 16:43:33,989] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-19 16:43:33,990] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-19 16:43:33,990] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-19 16:43:33,990] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-19 16:43:33,990] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-19 16:43:33,990] [    INFO][0m - max_seq_length                :64[0m
[32m[2022-09-19 16:43:33,990] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-19 16:43:33,990] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-19 16:43:33,990] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-19 16:43:33,990] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-19 16:43:33,990] [    INFO][0m - num_train_epochs              :20.0[0m
[32m[2022-09-19 16:43:33,990] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-19 16:43:33,990] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-19 16:43:33,990] [    INFO][0m - output_dir                    :./checkpoints_tnews/[0m
[32m[2022-09-19 16:43:33,990] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-19 16:43:33,991] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-19 16:43:33,991] [    INFO][0m - per_device_eval_batch_size    :16[0m
[32m[2022-09-19 16:43:33,991] [    INFO][0m - per_device_train_batch_size   :16[0m
[32m[2022-09-19 16:43:33,991] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-19 16:43:33,991] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-19 16:43:33,991] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-19 16:43:33,991] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-09-19 16:43:33,991] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-19 16:43:33,991] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-19 16:43:33,991] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-19 16:43:33,991] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-19 16:43:33,991] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-19 16:43:33,991] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-19 16:43:33,991] [    INFO][0m - run_name                      :./checkpoints_tnews/[0m
[32m[2022-09-19 16:43:33,992] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-19 16:43:33,992] [    INFO][0m - save_steps                    :200[0m
[32m[2022-09-19 16:43:33,992] [    INFO][0m - save_strategy                 :IntervalStrategy.EPOCH[0m
[32m[2022-09-19 16:43:33,992] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-19 16:43:33,992] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-19 16:43:33,992] [    INFO][0m - seed                          :42[0m
[32m[2022-09-19 16:43:33,992] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-19 16:43:33,992] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-19 16:43:33,992] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-19 16:43:33,992] [    INFO][0m - train_batch_size              :16[0m
[32m[2022-09-19 16:43:33,992] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-19 16:43:33,992] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-19 16:43:33,992] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-19 16:43:33,992] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-19 16:43:33,993] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-19 16:43:33,993] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-19 16:43:33,993] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-19 16:43:33,993] [    INFO][0m - [0m
[32m[2022-09-19 16:43:33,995] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-19 16:43:33,995] [    INFO][0m -   Num examples = 1185[0m
[32m[2022-09-19 16:43:33,996] [    INFO][0m -   Num Epochs = 20[0m
[32m[2022-09-19 16:43:33,996] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2022-09-19 16:43:33,996] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2022-09-19 16:43:33,996] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-19 16:43:33,996] [    INFO][0m -   Total optimization steps = 1500.0[0m
[32m[2022-09-19 16:43:33,996] [    INFO][0m -   Total num train samples = 23700[0m
[32m[2022-09-19 16:43:40,618] [    INFO][0m - loss: 2.86330719, learning_rate: 2.98e-05, global_step: 10, interval_runtime: 6.6209, interval_samples_per_second: 2.417, interval_steps_per_second: 1.51, epoch: 0.1333[0m
[32m[2022-09-19 16:43:44,887] [    INFO][0m - loss: 1.74846077, learning_rate: 2.96e-05, global_step: 20, interval_runtime: 4.2689, interval_samples_per_second: 3.748, interval_steps_per_second: 2.343, epoch: 0.2667[0m
[32m[2022-09-19 16:43:49,116] [    INFO][0m - loss: 1.40177498, learning_rate: 2.94e-05, global_step: 30, interval_runtime: 4.2296, interval_samples_per_second: 3.783, interval_steps_per_second: 2.364, epoch: 0.4[0m
[32m[2022-09-19 16:43:53,370] [    INFO][0m - loss: 1.51164637, learning_rate: 2.92e-05, global_step: 40, interval_runtime: 4.2533, interval_samples_per_second: 3.762, interval_steps_per_second: 2.351, epoch: 0.5333[0m
[32m[2022-09-19 16:43:57,604] [    INFO][0m - loss: 1.50142851, learning_rate: 2.9e-05, global_step: 50, interval_runtime: 4.2339, interval_samples_per_second: 3.779, interval_steps_per_second: 2.362, epoch: 0.6667[0m
[32m[2022-09-19 16:44:01,828] [    INFO][0m - loss: 1.64748936, learning_rate: 2.88e-05, global_step: 60, interval_runtime: 4.224, interval_samples_per_second: 3.788, interval_steps_per_second: 2.367, epoch: 0.8[0m
[32m[2022-09-19 16:44:06,089] [    INFO][0m - loss: 1.56708374, learning_rate: 2.86e-05, global_step: 70, interval_runtime: 4.2612, interval_samples_per_second: 3.755, interval_steps_per_second: 2.347, epoch: 0.9333[0m
[32m[2022-09-19 16:44:07,871] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 16:44:07,871] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-19 16:44:07,871] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 16:44:07,872] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 16:44:07,872] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-19 16:44:19,771] [    INFO][0m - eval_loss: 1.4917842149734497, eval_accuracy: 0.5163934426229508, eval_runtime: 11.8996, eval_samples_per_second: 92.272, eval_steps_per_second: 5.799, epoch: 1.0[0m
[32m[2022-09-19 16:44:19,793] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-75[0m
[32m[2022-09-19 16:44:19,793] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 16:44:22,591] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-75/tokenizer_config.json[0m
[32m[2022-09-19 16:44:22,591] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-75/special_tokens_map.json[0m
[32m[2022-09-19 16:44:30,135] [    INFO][0m - loss: 1.1974618, learning_rate: 2.84e-05, global_step: 80, interval_runtime: 24.046, interval_samples_per_second: 0.665, interval_steps_per_second: 0.416, epoch: 1.0667[0m
[32m[2022-09-19 16:44:36,495] [    INFO][0m - loss: 0.94574127, learning_rate: 2.8199999999999998e-05, global_step: 90, interval_runtime: 4.2571, interval_samples_per_second: 3.758, interval_steps_per_second: 2.349, epoch: 1.2[0m
[32m[2022-09-19 16:44:40,717] [    INFO][0m - loss: 0.97234364, learning_rate: 2.8e-05, global_step: 100, interval_runtime: 6.3252, interval_samples_per_second: 2.53, interval_steps_per_second: 1.581, epoch: 1.3333[0m
[32m[2022-09-19 16:44:44,972] [    INFO][0m - loss: 1.1000309, learning_rate: 2.78e-05, global_step: 110, interval_runtime: 4.2546, interval_samples_per_second: 3.761, interval_steps_per_second: 2.35, epoch: 1.4667[0m
[32m[2022-09-19 16:44:49,209] [    INFO][0m - loss: 1.08757744, learning_rate: 2.7600000000000003e-05, global_step: 120, interval_runtime: 4.2369, interval_samples_per_second: 3.776, interval_steps_per_second: 2.36, epoch: 1.6[0m
[32m[2022-09-19 16:44:53,525] [    INFO][0m - loss: 0.96689301, learning_rate: 2.7400000000000002e-05, global_step: 130, interval_runtime: 4.3162, interval_samples_per_second: 3.707, interval_steps_per_second: 2.317, epoch: 1.7333[0m
[32m[2022-09-19 16:44:57,452] [    INFO][0m - loss: 0.96208715, learning_rate: 2.72e-05, global_step: 140, interval_runtime: 3.9278, interval_samples_per_second: 4.074, interval_steps_per_second: 2.546, epoch: 1.8667[0m
[32m[2022-09-19 16:45:01,024] [    INFO][0m - loss: 0.88963747, learning_rate: 2.7000000000000002e-05, global_step: 150, interval_runtime: 3.5715, interval_samples_per_second: 4.48, interval_steps_per_second: 2.8, epoch: 2.0[0m
[32m[2022-09-19 16:45:01,025] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 16:45:01,025] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-19 16:45:01,025] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 16:45:01,025] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 16:45:01,025] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-19 16:45:11,884] [    INFO][0m - eval_loss: 1.6193112134933472, eval_accuracy: 0.5355191256830601, eval_runtime: 10.8573, eval_samples_per_second: 101.13, eval_steps_per_second: 6.355, epoch: 2.0[0m
[32m[2022-09-19 16:45:11,903] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-150[0m
[32m[2022-09-19 16:45:11,904] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 16:45:14,738] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-150/tokenizer_config.json[0m
[32m[2022-09-19 16:45:14,738] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-150/special_tokens_map.json[0m
[32m[2022-09-19 16:45:23,881] [    INFO][0m - loss: 0.54744267, learning_rate: 2.68e-05, global_step: 160, interval_runtime: 22.8571, interval_samples_per_second: 0.7, interval_steps_per_second: 0.438, epoch: 2.1333[0m
[32m[2022-09-19 16:45:27,812] [    INFO][0m - loss: 0.58000736, learning_rate: 2.6600000000000003e-05, global_step: 170, interval_runtime: 3.9308, interval_samples_per_second: 4.07, interval_steps_per_second: 2.544, epoch: 2.2667[0m
[32m[2022-09-19 16:45:31,682] [    INFO][0m - loss: 0.45587626, learning_rate: 2.64e-05, global_step: 180, interval_runtime: 3.8699, in[32m[2022-09-19 16:45:53,239] [    INFO][0m - eval_loss: 3.3840696811676025, eval_accuracy: 0.589458413926499, eval_runtime: 57.4204, eval_samples_per_second: 36.015, eval_steps_per_second: 2.264, epoch: 19.0[0m
[32m[2022-09-19 16:45:53,275] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-2432[0m
[32m[2022-09-19 16:45:53,275] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 16:45:55,955] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-2432/tokenizer_config.json[0m
[32m[2022-09-19 16:45:55,955] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-2432/special_tokens_map.json[0m
[32m[2022-09-19 16:46:09,351] [    INFO][0m - loss: 2.477e-05, learning_rate: 1.40625e-06, global_step: 2440, interval_runtime: 75.3301, interval_samples_per_second: 0.212, interval_steps_per_second: 0.133, epoch: 19.0625[0m
[32m[2022-09-19 16:46:23,330] [    INFO][0m - loss: 1.574e-05, learning_rate: 1.2890625e-06, global_step: 2450, interval_runtime: 13.9792, interval_samples_per_second: 1.145, interval_steps_per_second: 0.715, epoch: 19.1406[0m
[32m[2022-09-19 16:46:37,058] [    INFO][0m - loss: 1.573e-05, learning_rate: 1.1718750000000001e-06, global_step: 2460, interval_runtime: 13.7276, interval_samples_per_second: 1.166, interval_steps_per_second: 0.728, epoch: 19.2188[0m
[32m[2022-09-19 16:46:47,997] [    INFO][0m - loss: 2.136e-05, learning_rate: 1.0546875e-06, global_step: 2470, interval_runtime: 10.9388, interval_samples_per_second: 1.463, interval_steps_per_second: 0.914, epoch: 19.2969[0m
[32m[2022-09-19 16:46:54,524] [    INFO][0m - loss: 1.726e-05, learning_rate: 9.375e-07, global_step: 2480, interval_runtime: 6.5279, interval_samples_per_second: 2.451, interval_steps_per_second: 1.532, epoch: 19.375[0m
[32m[2022-09-19 16:47:07,429] [    INFO][0m - loss: 1.621e-05, learning_rate: 8.203125e-07, global_step: 2490, interval_runtime: 12.9042, interval_samples_per_second: 1.24, interval_steps_per_second: 0.775, epoch: 19.4531[0m
[32m[2022-09-19 16:47:21,233] [    INFO][0m - loss: 9.472e-05, learning_rate: 7.03125e-07, global_step: 2500, interval_runtime: 13.8045, interval_samples_per_second: 1.159, interval_steps_per_second: 0.724, epoch: 19.5312[0m
[32m[2022-09-19 16:47:34,413] [    INFO][0m - loss: 2.097e-05, learning_rate: 5.859375000000001e-07, global_step: 2510, interval_runtime: 13.18, interval_samples_per_second: 1.214, interval_steps_per_second: 0.759, epoch: 19.6094[0m
[32m[2022-09-19 16:47:42,867] [    INFO][0m - loss: 4.172e-05, learning_rate: 4.6875e-07, global_step: 2520, interval_runtime: 8.4531, interval_samples_per_second: 1.893, interval_steps_per_second: 1.183, epoch: 19.6875[0m
[32m[2022-09-19 16:47:51,731] [    INFO][0m - loss: 1.871e-05, learning_rate: 3.515625e-07, global_step: 2530, interval_runtime: 8.8645, interval_samples_per_second: 1.805, interval_steps_per_second: 1.128, epoch: 19.7656[0m
[32m[2022-09-19 16:48:05,747] [    INFO][0m - loss: 1.897e-05, learning_rate: 2.34375e-07, global_step: 2540, interval_runtime: 14.0156, interval_samples_per_second: 1.142, interval_steps_per_second: 0.713, epoch: 19.8438[0m
[32m[2022-09-19 16:48:19,294] [    INFO][0m - loss: 1.134e-05, learning_rate: 1.171875e-07, global_step: 2550, interval_runtime: 13.5474, interval_samples_per_second: 1.181, interval_steps_per_second: 0.738, epoch: 19.9219[0m
[32m[2022-09-19 16:48:33,353] [    INFO][0m - loss: 2.396e-05, learning_rate: 0.0, global_step: 2560, interval_runtime: 14.0586, interval_samples_per_second: 1.138, interval_steps_per_second: 0.711, epoch: 20.0[0m
[32m[2022-09-19 16:48:33,354] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 16:48:33,354] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-19 16:48:33,354] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 16:48:33,354] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 16:48:33,354] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-19 16:49:27,092] [    INFO][0m - eval_loss: 3.3810839653015137, eval_accuracy: 0.589458413926499, eval_runtime: 53.738, eval_samples_per_second: 38.483, eval_steps_per_second: 2.419, epoch: 20.0[0m
[32m[2022-09-19 16:49:27,127] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-2560[0m
[32m[2022-09-19 16:49:27,127] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 16:49:29,901] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-2560/tokenizer_config.json[0m
[32m[2022-09-19 16:49:29,901] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-2560/special_tokens_map.json[0m
[32m[2022-09-19 16:49:34,932] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-19 16:49:34,932] [    INFO][0m - Loading best model from ./checkpoints_csldcp/checkpoint-1664 (score: 0.5904255319148937).[0m
[33m[2022-09-19 16:49:34,933] [ WARNING][0m - Could not locate the best model at ./checkpoints_csldcp/checkpoint-1664/model_state.pdparams, if you are running a distributed training on multiple nodes, you should activate `--save_on_each_node`.[0m
[32m[2022-09-19 16:49:34,933] [    INFO][0m - train_runtime: 4980.8503, train_samples_per_second: 8.175, train_steps_per_second: 0.514, train_loss: 0.19742709359055652, epoch: 20.0[0m
[32m[2022-09-19 16:49:34,934] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/[0m
[32m[2022-09-19 16:49:34,934] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 16:49:37,425] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/tokenizer_config.json[0m
[32m[2022-09-19 16:49:37,425] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/special_tokens_map.json[0m
[32m[2022-09-19 16:49:37,427] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-19 16:49:37,427] [    INFO][0m -   epoch                    =       20.0[0m
[32m[2022-09-19 16:49:37,427] [    INFO][0m -   train_loss               =     0.1974[0m
[32m[2022-09-19 16:49:37,427] [    INFO][0m -   train_runtime            = 1:23:00.85[0m
[32m[2022-09-19 16:49:37,427] [    INFO][0m -   train_samples_per_second =      8.175[0m
[32m[2022-09-19 16:49:37,427] [    INFO][0m -   train_steps_per_second   =      0.514[0m
[32m[2022-09-19 16:49:37,438] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-19 16:49:37,438] [    INFO][0m -   Num examples = 1784[0m
[32m[2022-09-19 16:49:37,438] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 16:49:37,438] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 16:49:37,438] [    INFO][0m -   Total prediction steps = 112[0m
[32m[2022-09-19 16:50:25,576] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-19 16:50:25,576] [    INFO][0m -   test_accuracy           =     0.5807[0m
[32m[2022-09-19 16:50:25,577] [    INFO][0m -   test_loss               =     3.3863[0m
[32m[2022-09-19 16:50:25,577] [    INFO][0m -   test_runtime            = 0:00:48.13[0m
[32m[2022-09-19 16:50:25,577] [    INFO][0m -   test_samples_per_second =      37.06[0m
[32m[2022-09-19 16:50:25,577] [    INFO][0m -   test_steps_per_second   =      2.327[0m
[32m[2022-09-19 16:50:25,577] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-19 16:50:25,577] [    INFO][0m -   Num examples = 2999[0m
[32m[2022-09-19 16:50:25,577] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 16:50:25,577] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 16:50:25,577] [    INFO][0m -   Total prediction steps = 188[0m
[32m[2022-09-19 16:51:50,766] [    INFO][0m - Predictions for csldcp saved to ./fewclue_submit_examples.[0m
{
  "labels": 26,
  "text_a": "\u672c\u6587\u7814\u7a76\u4e86\u8d35\u5dde\u4e1c\u5357\u90e8\u7684\u5929\u67f1\u3001\u9526\u5c4f\u548c\u9ece\u5e73\u5730\u533a\u7684\u5730\u8d28\u6784\u9020\u7279\u5f81\u3001\u8be5\u533a\u91d1\u77ff\u5e8a\u7279\u5f81\u53ca\u5176\u4e0e\u5730\u8d28\u6784\u9020\u7684\u5173\u7cfb.\u8ba4\u4e3a\u533a\u5185\u5317\u90e8\u548c\u5357\u90e8\u76f8\u8ddd\u6570\u5341\u516c\u91cc\u7684\u4e24\u6761\u8fd1\u4e1c\u897f\u5411\u7684\u57fa\u5e95\u526a\u5207\u65ad\u88c2\u5e26\u6784\u6210\u4e86\u672c\u533a\u5730\u5811\u5f0f\u7684\u6784\u9020\u683c\u5c40.\u7531\u4e8e\u8fd9\u4e24\u6761\u4e1c\u897f\u5411\u526a\u5207\u65ad\u88c2\u7684\u526a\u5207\u4f5c\u7528,\u5f62\u6210\u4e86\u672c\u533a\u4ee5\u5317\u4e1c\u5411\u4e3a\u4e3b\u7684\u8936\u76b1\u548c\u526a\u5207\u5e26.\u5317\u4e1c\u5411\u8936\u76b1\u548c\u526a\u5207\u5e26\u6784\u9020\u662f\u5728\u52a0\u91cc\u4e1c\u671f\u5f62\u6210\u7684,\u540c\u65f6\u4e5f\u53d1\u751f\u4e86\u7eff\u7247\u5ca9\u76f8\u7684\u53d8\u8d28\u4f5c\u7528.\u5728\u80cc\u659c\u5f62\u6210\u7684\u540c\u65f6\u6216\u7a0d\u540e\u526a\u5207\u4f5c\u7528\u5f00\u59cb\u53d1\u751f,\u526a\u5207\u4f5c\u7528\u7ee7\u627f\u7740\u5317\u4e1c\u5411,\u5f62\u6210\u4e86\u82e5\u5e72\u6761\u4e0e\u8936\u76b1\u8f74\u5e73\u884c\u6216\u76f8\u4ea4\u7684\u526a\u5207\u5e26.\u6210\u77ff\u6d41\u4f53\u6cbf\u7740\u526a\u5207\u5e26\u4e0a\u5347,\u4e00\u65b9\u9762\u5728\u526a\u5207\u5e26\u4e2d\u6c89\u6dc0\u51fa\u7a7f\u5c42\u4ea7\u51fa\u7684\u4e0d\u6574\u5408\u7834\u788e\u5e26\u578b\u91d1\u77ff\u4f53,\u5373\u900f\u955c\u72b6\u542b\u91d1\u77f3\u82f1\u8109;\u540c\u65f6\u5927\u90e8\u5206\u5145\u586b\u5230\u7531\u80cc\u659c\u548c\u526a\u5207\u4f5c\u7528\u5f62\u6210\u7684\u5c42\u95f4\u88c2\u9699\u6216\u5c42\u95f4\u7834\u788e\u5e26\u4e2d,\u4ece\u800c\u5f62\u6210\u8d4b\u5b58\u4e8e\u6d4a\u79ef\u5ca9\u7684\u987a\u5c42\u4ea7\u51fa\u7684\u5c42\u72b6\u542b\u91d1\u77f3\u82f1\u8109\u91d1\u77ff\u5e8a.\u8fd9\u79cd\u7c7b\u578b\u7684\u542b\u91d1\u77f3\u82f1\u8109,\u65e0\u8bba\u5728\u54c1\u4f4d(\u591a\u6570\u53ef\u89c1\u660e\u91d1)\u3001\u50a8\u91cf\u548c\u4ea7\u91cf\u65b9\u9762\u5747\u5f88\u6709\u524d\u666f,\u5e76\u4e14\u6709\u7740\u5341\u5206\u91cd\u8981\u7684\u5b66\u672f\u610f\u4e49,\u56e0\u4e3a\u8fd9\u5728\u56fd\u5185\u8fd8\u662f\u9996\u6b21\u53d1\u73b0.",
  "text_b": "",
  "uid": 44
}

Prediction done.
 
==========
tnews
==========
 
[32m[2022-09-19 16:51:57,009] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-19 16:51:57,009] [    INFO][0m - ============================================================[0m
[32m[2022-09-19 16:51:57,009] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-19 16:51:57,009] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-19 16:51:57,009] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-19 16:51:57,010] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-19 16:51:57,010] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-19 16:51:57,010] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-19 16:51:57,010] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-19 16:51:57,010] [    INFO][0m - [0m
[32m[2022-09-19 16:51:57,010] [    INFO][0m - ============================================================[0m
[32m[2022-09-19 16:51:57,010] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-19 16:51:57,010] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-19 16:51:57,010] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-19 16:51:57,010] [    INFO][0m - pretrained                    :/ssd2/wanghuijuan03/data/zero-shot/0919_model_state.pdparams[0m
[32m[2022-09-19 16:51:57,010] [    INFO][0m - prompt                        :â€œ{'text':'text_a'}â€ä¸Šè¿°æ–°é—»é€‰è‡ª{'mask'}{'mask'}ä¸“æ ã€‚[0m
[32m[2022-09-19 16:51:57,010] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-19 16:51:57,010] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-19 16:51:57,011] [    INFO][0m - task_name                     :tnews[0m
[32m[2022-09-19 16:51:57,011] [    INFO][0m - [0m
[32m[2022-09-19 16:51:57,011] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0919 16:51:57.012822  2785 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0919 16:51:57.017050  2785 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-19 16:52:04,385] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-19 16:52:04,403] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-19 16:52:04,404] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-19 16:52:04,404] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': 'â€œ'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': 'â€ä¸Šè¿°æ–°é—»é€‰è‡ª'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'ä¸“æ ã€‚'}][0m
[32m[2022-09-19 16:52:05,946] [    INFO][0m - ============================================================[0m
[32m[2022-09-19 16:52:05,947] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-19 16:52:05,947] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-19 16:52:05,947] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-19 16:52:05,947] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-19 16:52:05,947] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-19 16:52:05,947] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-19 16:52:05,947] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-19 16:52:05,947] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-19 16:52:05,947] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-19 16:52:05,948] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-19 16:52:05,948] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-19 16:52:05,948] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-19 16:52:05,948] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-19 16:52:05,948] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-19 16:52:05,948] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-19 16:52:05,948] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-19 16:52:05,948] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-19 16:52:05,948] [    INFO][0m - eval_batch_size               :16[0m
[32m[2022-09-19 16:52:05,948] [    INFO][0m - eval_steps                    :200[0m
[32m[2022-09-19 16:52:05,948] [    INFO][0m - evaluation_strategy           :IntervalStrategy.EPOCH[0m
[32m[2022-09-19 16:52:05,948] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-19 16:52:05,948] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-19 16:52:05,949] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-19 16:52:05,949] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-19 16:52:05,949] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-19 16:52:05,949] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-19 16:52:05,949] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-19 16:52:05,949] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-19 16:52:05,949] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-19 16:52:05,949] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-09-19 16:52:05,949] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-19 16:52:05,949] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-19 16:52:05,949] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-19 16:52:05,949] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-19 16:52:05,949] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-19 16:52:05,949] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-19 16:52:05,950] [    INFO][0m - logging_dir                   :./checkpoints_tnews/runs/Sep19_16-51-57_instance-3bwob41y-01[0m
[32m[2022-09-19 16:52:05,950] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-19 16:52:05,950] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-19 16:52:05,950] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-19 16:52:05,950] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-19 16:52:05,950] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-19 16:52:05,950] [    INFO][0m - max_seq_length                :64[0m
[32m[2022-09-19 16:52:05,950] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-19 16:52:05,950] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-19 16:52:05,950] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-19 16:52:05,950] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-19 16:52:05,950] [    INFO][0m - num_train_epochs              :20.0[0m
[32m[2022-09-19 16:52:05,950] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-19 16:52:05,950] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-19 16:52:05,951] [    INFO][0m - output_dir                    :./checkpoints_tnews/[0m
[32m[2022-09-19 16:52:05,951] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-19 16:52:05,951] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-19 16:52:05,951] [    INFO][0m - per_device_eval_batch_size    :16[0m
[32m[2022-09-19 16:52:05,951] [    INFO][0m - per_device_train_batch_size   :16[0m
[32m[2022-09-19 16:52:05,951] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-19 16:52:05,951] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-19 16:52:05,951] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-19 16:52:05,951] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-09-19 16:52:05,951] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-19 16:52:05,951] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-19 16:52:05,951] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-19 16:52:05,951] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-19 16:52:05,951] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-19 16:52:05,952] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-19 16:52:05,952] [    INFO][0m - run_name                      :./checkpoints_tnews/[0m
[32m[2022-09-19 16:52:05,952] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-19 16:52:05,952] [    INFO][0m - save_steps                    :200[0m
[32m[2022-09-19 16:52:05,952] [    INFO][0m - save_strategy                 :IntervalStrategy.EPOCH[0m
[32m[2022-09-19 16:52:05,952] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-19 16:52:05,952] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-19 16:52:05,952] [    INFO][0m - seed                          :42[0m
[32m[2022-09-19 16:52:05,952] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-19 16:52:05,952] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-19 16:52:05,952] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-19 16:52:05,952] [    INFO][0m - train_batch_size              :16[0m
[32m[2022-09-19 16:52:05,952] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-19 16:52:05,953] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-19 16:52:05,953] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-19 16:52:05,953] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-19 16:52:05,953] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-19 16:52:05,953] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-19 16:52:05,953] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-19 16:52:05,953] [    INFO][0m - [0m
[32m[2022-09-19 16:52:05,955] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-19 16:52:05,956] [    INFO][0m -   Num examples = 1185[0m
[32m[2022-09-19 16:52:05,956] [    INFO][0m -   Num Epochs = 20[0m
[32m[2022-09-19 16:52:05,956] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2022-09-19 16:52:05,956] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2022-09-19 16:52:05,956] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-19 16:52:05,956] [    INFO][0m -   Total optimization steps = 1500.0[0m
[32m[2022-09-19 16:52:05,956] [    INFO][0m -   Total num train samples = 23700[0m
[32m[2022-09-19 16:52:09,068] [    INFO][0m - loss: 2.87799702, learning_rate: 2.98e-05, global_step: 10, interval_runtime: 3.1109, interval_samples_per_second: 5.143, interval_steps_per_second: 3.214, epoch: 0.1333[0m
[32m[2022-09-19 16:52:11,718] [    INFO][0m - loss: 1.73155689, learning_rate: 2.96e-05, global_step: 20, interval_runtime: 1.933, interval_samples_per_second: 8.277, interval_steps_per_second: 5.173, epoch: 0.2667[0m
[32m[2022-09-19 16:52:13,645] [    INFO][0m - loss: 1.43447075, learning_rate: 2.94e-05, global_step: 30, interval_runtime: 2.6442, interval_samples_per_second: 6.051, interval_steps_per_second: 3.782, epoch: 0.4[0m
[32m[2022-09-19 16:52:17,870] [    INFO][0m - loss: 1.53277683, learning_rate: 2.92e-05, global_step: 40, interval_runtime: 3.8827, interval_samples_per_second: 4.121, interval_steps_per_second: 2.576, epoch: 0.5333[0m
[32m[2022-09-19 16:52:22,053] [    INFO][0m - loss: 1.52272377, learning_rate: 2.9e-05, global_step: 50, interval_runtime: 4.525, interval_samples_per_second: 3.536, interval_steps_per_second: 2.21, epoch: 0.6667[0m
[32m[2022-09-19 16:52:26,228] [    INFO][0m - loss: 1.6005106, learning_rate: 2.88e-05, global_step: 60, interval_runtime: 4.1747, interval_samples_per_second: 3.833, interval_steps_per_second: 2.395, epoch: 0.8[0m
[32m[2022-09-19 16:52:30,410] [    INFO][0m - loss: 1.60044594, learning_rate: 2.86e-05, global_step: 70, interval_runtime: 4.1825, interval_samples_per_second: 3.825, interval_steps_per_second: 2.391, epoch: 0.9333[0m
[32m[2022-09-19 16:52:32,182] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 16:52:32,182] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-19 16:52:32,182] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 16:52:32,182] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 16:52:32,183] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-19 16:52:43,605] [    INFO][0m - eval_loss: 1.5368688106536865, eval_accuracy: 0.5245901639344263, eval_runtime: 11.4219, eval_samples_per_second: 96.131, eval_steps_per_second: 6.041, epoch: 1.0[0m
[32m[2022-09-19 16:52:43,618] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-75[0m
[32m[2022-09-19 16:52:43,619] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 16:52:52,503] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-75/tokenizer_config.json[0m
[32m[2022-09-19 16:52:52,504] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-75/special_tokens_map.json[0m
[32m[2022-09-19 16:53:07,722] [    INFO][0m - loss: 1.18014545, learning_rate: 2.84e-05, global_step: 80, interval_runtime: 37.3111, interval_samples_per_second: 0.429, interval_steps_per_second: 0.268, epoch: 1.0667[0m
[32m[2022-09-19 16:53:11,893] [    INFO][0m - loss: 0.93993464, learning_rate: 2.8199999999999998e-05, global_step: 90, interval_runtime: 4.171, interval_samples_per_second: 3.836, interval_steps_per_second: 2.397, epoch: 1.2[0m
[32m[2022-09-19 16:53:16,062] [    INFO][0m - loss: 0.97029581, learning_rate: 2.8e-05, global_step: 100, interval_runtime: 4.17, interval_samples_per_second: 3.837, interval_steps_per_second: 2.398, epoch: 1.3333[0m
[32m[2022-09-19 16:53:20,287] [    INFO][0m - loss: 1.1112319, learning_rate: 2.78e-05, global_step: 110, interval_runtime: 4.2244, interval_samples_per_second: 3.787, interval_steps_per_second: 2.367, epoch: 1.4667[0m
[32m[2022-09-19 16:53:23,761] [    INFO][0m - loss: 1.05197926, learning_rate: 2.7600000000000003e-05, global_step: 120, interval_runtime: 3.4739, interval_samples_per_second: 4.606, interval_steps_per_second: 2.879, epoch: 1.6[0m
[32m[2022-09-19 16:53:27,297] [    INFO][0m - loss: 0.95927954, learning_rate: 2.7400000000000002e-05, global_step: 130, interval_runtime: 3.5358, interval_samples_per_second: 4.525, interval_steps_per_second: 2.828, epoch: 1.7333[0m
[32m[2022-09-19 16:53:30,904] [    INFO][0m - loss: 0.93509512, learning_rate: 2.72e-05, global_step: 140, interval_runtime: 3.6073, interval_samples_per_second: 4.435, interval_steps_per_second: 2.772, epoch: 1.8667[0m
[32m[2022-09-19 16:53:32,834] [    INFO][0m - loss: 0.8218029, learning_rate: 2.7000000000000002e-05, global_step: 150, interval_runtime: 1.9298, interval_samples_per_second: 8.291, interval_steps_per_second: 5.182, epoch: 2.0[0m
[32m[2022-09-19 16:53:32,835] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 16:53:32,835] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-19 16:53:32,835] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 16:53:32,835] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 16:53:32,835] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-19 16:53:38,443] [    INFO][0m - eval_loss: 1.6768797636032104, eval_accuracy: 0.546448087431694, eval_runtime: 5.6069, eval_samples_per_second: 195.831, eval_steps_per_second: 12.306, epoch: 2.0[0m
[32m[2022-09-19 16:53:38,462] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-150[0m
[32m[2022-09-19 16:53:38,462] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 16:53:45,249] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-150/tokenizer_config.json[0m
[32m[2022-09-19 16:53:45,249] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-150/special_tokens_map.json[0m
[32m[2022-09-19 16:54:00,763] [    INFO][0m - loss: 0.57554598, learning_rate: 2.68e-05, global_step: 160, interval_runtime: 27.9295, interval_samples_per_second: 0.573, interval_steps_per_second: 0.358, epoch: 2.1333[0m
[32m[2022-09-19 16:54:02,688] [    INFO][0m - loss: 0.50204549, learning_rate: 2.6600000000000003e-05, global_step: 170, interval_runtime: 1.9253, interval_samples_per_second: 8.311, interval_steps_per_second: 5.194, epoch: 2.2667[0m
[32m[2022-09-19 16:54:04,600] [    INFO][0m - loss: 0.52942414, learning_rate: 2.64e-05, global_step: 180, interval_runtime: 1.912, interval_samples_per_second: 8.368, interval_steps_per_second: 5.23, epoch: 2.4[0m
[32m[2022-09-19 16:54:06,504] [    INFO][0m - loss: 0.44504366, learning_rate: 2.62e-05, global_step: 190, interval_runtime: 1.9036, interval_samples_per_second: 8.405, interval_steps_per_second: 5.253, epoch: 2.5333[0m
[32m[2022-09-19 16:54:08,416] [    INFO][0m - loss: 0.63336997, learning_rate: 2.6000000000000002e-05, global_step: 200, interval_runtime: 1.9122, interval_samples_per_second: 8.367, interval_steps_per_second: 5.229, epoch: 2.6667[0m
[32m[2022-09-19 16:54:12,179] [    INFO][0m - loss: 0.52457442, learning_rate: 2.58e-05, global_step: 210, interval_runtime: 3.3734, interval_samples_per_second: 4.743, interval_steps_per_second: 2.964, epoch: 2.8[0m
[32m[2022-09-19 16:54:16,371] [    INFO][0m - loss: 0.53339133, learning_rate: 2.5600000000000002e-05, global_step: 220, interval_runtime: 4.5817, interval_samples_per_second: 3.492, interval_steps_per_second: 2.183, epoch: 2.9333[0m
[32m[2022-09-19 16:54:18,122] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 16:54:18,122] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-19 16:54:18,122] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 16:54:18,122] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 16:54:18,123] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-19 16:54:29,709] [    INFO][0m - eval_loss: 1.936758279800415, eval_accuracy: 0.5382513661202186, eval_runtime: 11.5866, eval_samples_per_second: 94.765, eval_steps_per_second: 5.955, epoch: 3.0[0m
[32m[2022-09-19 16:54:29,726] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-225[0m
[32m[2022-09-19 16:54:29,727] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 16:54:35,962] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-225/tokenizer_config.json[0m
[32m[2022-09-19 16:54:35,963] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-225/special_tokens_map.json[0m
[32m[2022-09-19 16:54:55,917] [    INFO][0m - loss: 0.37869642, learning_rate: 2.54e-05, global_step: 230, interval_runtime: 39.5452, interval_samples_per_second: 0.405, interval_steps_per_second: 0.253, epoch: 3.0667[0m
[32m[2022-09-19 16:55:00,088] [    INFO][0m - loss: 0.14114667, learning_rate: 2.52e-05, global_step: 240, interval_runtime: 4.171, interval_samples_per_second: 3.836, interval_steps_per_second: 2.397, epoch: 3.2[0m
[32m[2022-09-19 16:55:04,261] [    INFO][0m - loss: 0.14901304, learning_rate: 2.5e-05, global_step: 250, interval_runtime: 4.1729, interval_samples_per_second: 3.834, interval_steps_per_second: 2.396, epoch: 3.3333[0m
[32m[2022-09-19 16:55:08,437] [    INFO][0m - loss: 0.21801147, learning_rate: 2.48e-05, global_step: 260, interval_runtime: 4.1765, interval_samples_per_second: 3.831, interval_steps_per_second: 2.394, epoch: 3.4667[0m
[32m[2022-09-19 16:55:12,632] [    INFO][0m - loss: 0.18586345, learning_rate: 2.4599999999999998e-05, global_step: 270, interval_runtime: 4.1954, interval_samples_per_second: 3.814, interval_steps_per_second: 2.384, epoch: 3.6[0m
[32m[2022-09-19 16:55:16,830] [    INFO][0m - loss: 0.20506513, learning_rate: 2.44e-05, global_step: 280, interval_runtime: 4.1976, interval_samples_per_second: 3.812, interval_steps_per_second: 2.382, epoch: 3.7333[0m
[32m[2022-09-19 16:55:20,304] [    INFO][0m - loss: 0.37482097, learning_rate: 2.42e-05, global_step: 290, interval_runtime: 3.4745, interval_samples_per_second: 4.605, interval_steps_per_second: 2.878, epoch: 3.8667[0m
[32m[2022-09-19 16:55:23,602] [    INFO][0m - loss: 0.31647282, learning_rate: 2.4e-05, global_step: 300, interval_runtime: 3.2979, interval_samples_per_second: 4.852, interval_steps_per_second: 3.032, epoch: 4.0[0m
[32m[2022-09-19 16:55:23,603] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 16:55:23,603] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-19 16:55:23,603] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 16:55:23,603] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 16:55:23,603] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-19 16:55:31,053] [    INFO][0m - eval_loss: 2.7071006298065186, eval_accuracy: 0.5591985428051002, eval_runtime: 7.449, eval_samples_per_second: 147.403, eval_steps_per_second: 9.263, epoch: 4.0[0m
[32m[2022-09-19 16:55:31,065] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-300[0m
[32m[2022-09-19 16:55:31,065] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 16:55:37,102] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-300/tokenizer_config.json[0m
[32m[2022-09-19 16:55:37,102] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-300/special_tokens_map.json[0m
[32m[2022-09-19 16:55:53,559] [    INFO][0m - loss: 0.10553269, learning_rate: 2.38e-05, global_step: 310, interval_runtime: 29.9563, interval_samples_per_second: 0.534, interval_steps_per_second: 0.334, epoch: 4.1333[0m
[32m[2022-09-19 16:55:57,135] [    INFO][0m - loss: 0.086567, learning_rate: 2.3599999999999998e-05, global_step: 320, interval_runtime: 3.576, interval_samples_per_second: 4.474, interval_steps_per_second: 2.796, epoch: 4.2667[0m
[32m[2022-09-19 16:56:00,733] [    INFO][0m - loss: 0.05755414, learning_rate: 2.3400000000000003e-05, global_step: 330, interval_runtime: 3.5986, interval_samples_per_second: 4.446, interval_steps_per_second: 2.779, epoch: 4.4[0m
[32m[2022-09-19 16:56:02,683] [    INFO][0m - loss: 0.10766248, learning_rate: 2.32e-05, global_step: 340, interval_runtime: 1.9494, interval_samples_per_second: 8.208, interval_steps_per_second: 5.13, epoch: 4.5333[0m
[32m[2022-09-19 16:56:04,609] [    INFO][0m - loss: 0.08709183, learning_rate: 2.3000000000000003e-05, global_step: 350, interval_runtime: 1.9268, interval_samples_per_second: 8.304, interval_steps_per_second: 5.19, epoch: 4.6667[0m
[32m[2022-09-19 16:56:06,531] [    INFO][0m - loss: 0.15195153, learning_rate: 2.2800000000000002e-05, global_step: 360, interval_runtime: 1.921, interval_samples_per_second: 8.329, interval_steps_per_second: 5.206, epoch: 4.8[0m
[32m[2022-09-19 16:56:08,438] [    INFO][0m - loss: 0.22928782, learning_rate: 2.26e-05, global_step: 370, interval_runtime: 1.9076, interval_samples_per_second: 8.387, interval_steps_per_second: 5.242, epoch: 4.9333[0m
[32m[2022-09-19 16:56:09,284] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 16:56:09,284] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-19 16:56:09,284] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 16:56:09,284] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 16:56:09,284] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-19 16:56:20,958] [    INFO][0m - eval_loss: 3.86838960647583, eval_accuracy: 0.5218579234972678, eval_runtime: 11.6733, eval_samples_per_second: 94.061, eval_steps_per_second: 5.911, epoch: 5.0[0m
[32m[2022-09-19 16:56:20,977] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-375[0m
[32m[2022-09-19 16:56:20,978] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 16:56:27,468] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-375/tokenizer_config.json[0m
[32m[2022-09-19 16:56:27,469] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-375/special_tokens_map.json[0m
[32m[2022-09-19 16:56:42,366] [    INFO][0m - loss: 0.06084639, learning_rate: 2.2400000000000002e-05, global_step: 380, interval_runtime: 33.9278, interval_samples_per_second: 0.472, interval_steps_per_second: 0.295, epoch: 5.0667[0m
[32m[2022-09-19 16:56:46,242] [    INFO][0m - loss: 0.02492114, learning_rate: 2.22e-05, global_step: 390, interval_runtime: 3.8768, interval_samples_per_second: 4.127, interval_steps_per_second: 2.579, epoch: 5.2[0m
[32m[2022-09-19 16:56:50,429] [    INFO][0m - loss: 0.19867448, learning_rate: 2.2e-05, global_step: 400, interval_runtime: 4.1863, interval_samples_per_second: 3.822, interval_steps_per_second: 2.389, epoch: 5.3333[0m
[32m[2022-09-19 16:56:54,618] [    INFO][0m - loss: 0.03895625, learning_rate: 2.18e-05, global_step: 410, interval_runtime: 4.1892, interval_samples_per_second: 3.819, interval_steps_per_second: 2.387, epoch: 5.4667[0m
[32m[2022-09-19 16:56:58,814] [    INFO][0m - loss: 0.05864865, learning_rate: 2.16e-05, global_step: 420, interval_runtime: 4.1963, interval_samples_per_second: 3.813, interval_steps_per_second: 2.383, epoch: 5.6[0m
[32m[2022-09-19 16:57:03,008] [    INFO][0m - loss: 0.04880538, learning_rate: 2.1400000000000002e-05, global_step: 430, interval_runtime: 4.1937, interval_samples_per_second: 3.815, interval_steps_per_second: 2.385, epoch: 5.7333[0m
[32m[2022-09-19 16:57:07,209] [    INFO][0m - loss: 0.10584344, learning_rate: 2.12e-05, global_step: 440, interval_runtime: 4.2007, interval_samples_per_second: 3.809, interval_steps_per_second: 2.381, epoch: 5.8667[0m
[32m[2022-09-19 16:57:11,102] [    INFO][0m - loss: 0.03169483, learning_rate: 2.1e-05, global_step: 450, interval_runtime: 3.8934, interval_samples_per_second: 4.11, interval_steps_per_second: 2.568, epoch: 6.0[0m
[32m[2022-09-19 16:57:11,103] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 16:57:11,103] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-19 16:57:11,103] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 16:57:11,103] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 16:57:11,103] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-19 16:57:19,968] [    INFO][0m - eval_loss: 4.234651565551758, eval_accuracy: 0.5528233151183971, eval_runtime: 8.8646, eval_samples_per_second: 123.864, eval_steps_per_second: 7.784, epoch: 6.0[0m
[32m[2022-09-19 16:57:19,986] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-450[0m
[32m[2022-09-19 16:57:19,986] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 16:57:26,267] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-450/tokenizer_config.json[0m
[32m[2022-09-19 16:57:26,267] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-450/special_tokens_map.json[0m
[32m[2022-09-19 16:57:43,149] [    INFO][0m - loss: 0.03270416, learning_rate: 2.08e-05, global_step: 460, interval_runtime: 32.0463, interval_samples_per_second: 0.499, interval_steps_per_second: 0.312, epoch: 6.1333[0m
[32m[2022-09-19 16:57:47,344] [    INFO][0m - loss: 0.03103143, learning_rate: 2.06e-05, global_step: 470, interval_runtime: 4.1956, interval_samples_per_second: 3.814, interval_steps_per_second: 2.383, epoch: 6.2667[0m
[32m[2022-09-19 16:57:51,207] [    INFO][0m - loss: 0.13343483, learning_rate: 2.04e-05, global_step: 480, interval_runtime: 3.8623, interval_samples_per_second: 4.143, interval_steps_per_second: 2.589, epoch: 6.4[0m
[32m[2022-09-19 16:57:54,775] [    INFO][0m - loss: 0.02666903, learning_rate: 2.02e-05, global_step: 490, interval_runtime: 3.5685, interval_samples_per_second: 4.484, interval_steps_per_second: 2.802, epoch: 6.5333[0m
[32m[2022-09-19 16:58:01,327] [    INFO][0m - loss: 0.01434325, learning_rate: 1.9999999999999998e-05, global_step: 500, interval_runtime: 3.5762, interval_samples_per_second: 4.474, interval_steps_per_second: 2.796, epoch: 6.6667[0m
[32m[2022-09-19 16:58:05,535] [    INFO][0m - loss: 0.03668672, learning_rate: 1.98e-05, global_step: 510, interval_runtime: 4.9053, interval_samples_per_second: 3.262, interval_steps_per_second: 2.039, epoch: 6.8[0m
[32m[2022-09-19 16:58:07,453] [    INFO][0m - loss: 0.06602063, learning_rate: 1.96e-05, global_step: 520, interval_runtime: 4.196, interval_samples_per_second: 3.813, interval_steps_per_second: 2.383, epoch: 6.9333[0m
[32m[2022-09-19 16:58:08,310] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 16:58:08,310] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-19 16:58:08,310] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 16:58:08,311] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 16:58:08,311] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-19 16:58:14,057] [    INFO][0m - eval_loss: 4.711246967315674, eval_accuracy: 0.5300546448087432, eval_runtime: 5.7458, eval_samples_per_second: 191.097, eval_steps_per_second: 12.009, epoch: 7.0[0m
[32m[2022-09-19 16:58:14,079] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-525[0m
[32m[2022-09-19 16:58:14,079] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 16:58:20,834] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-525/tokenizer_config.json[0m
[32m[2022-09-19 16:58:20,835] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-525/special_tokens_map.json[0m
[32m[2022-09-19 16:58:34,974] [    INFO][0m - loss: 0.02299936, learning_rate: 1.9399999999999997e-05, global_step: 530, interval_runtime: 27.5215, interval_samples_per_second: 0.581, interval_steps_per_second: 0.363, epoch: 7.0667[0m
[32m[2022-09-19 16:58:37,762] [    INFO][0m - loss: 0.04452686, learning_rate: 1.9200000000000003e-05, global_step: 540, interval_runtime: 1.9279, interval_samples_per_second: 8.299, interval_steps_per_second: 5.187, epoch: 7.2[0m
[32m[2022-09-19 16:58:39,713] [    INFO][0m - loss: 0.05527792, learning_rate: 1.9e-05, global_step: 550, interval_runtime: 2.8104, interval_samples_per_second: 5.693, interval_steps_per_second: 3.558, epoch: 7.3333[0m
[32m[2022-09-19 16:58:41,653] [    INFO][0m - loss: 0.00820267, learning_rate: 1.8800000000000003e-05, global_step: 560, interval_runtime: 1.94, interval_samples_per_second: 8.247, interval_steps_per_second: 5.155, epoch: 7.4667[0m
[32m[2022-09-19 16:58:43,584] [    INFO][0m - loss: 0.00948331, learning_rate: 1.86e-05, global_step: 570, interval_runtime: 1.9312, interval_samples_per_second: 8.285, interval_steps_per_second: 5.178, epoch: 7.6[0m
[32m[2022-09-19 16:58:45,498] [    INFO][0m - loss: 0.03530395, learning_rate: 1.84e-05, global_step: 580, interval_runtime: 1.9143, interval_samples_per_second: 8.358, interval_steps_per_second: 5.224, epoch: 7.7333[0m
[32m[2022-09-19 16:58:47,408] [    INFO][0m - loss: 0.06211721, learning_rate: 1.8200000000000002e-05, global_step: 590, interval_runtime: 1.9104, interval_samples_per_second: 8.375, interval_steps_per_second: 5.235, epoch: 7.8667[0m
[32m[2022-09-19 16:58:49,223] [    INFO][0m - loss: 0.05722244, learning_rate: 1.8e-05, global_step: 600, interval_runtime: 1.8141, interval_samples_per_second: 8.82, interval_steps_per_second: 5.512, epoch: 8.0[0m
[32m[2022-09-19 16:58:49,223] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 16:58:49,224] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-19 16:58:49,224] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 16:58:49,224] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 16:58:49,224] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-19 16:58:55,204] [    INFO][0m - eval_loss: 5.177426815032959, eval_accuracy: 0.5264116575591985, eval_runtime: 5.9792, eval_samples_per_second: 183.636, eval_steps_per_second: 11.54, epoch: 8.0[0m
[32m[2022-09-19 16:58:55,225] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-600[0m
[32m[2022-09-19 16:58:55,225] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 16:58:58,302] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-600/tokenizer_config.json[0m
[32m[2022-09-19 16:58:58,302] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-600/special_tokens_map.json[0m
[32m[2022-09-19 16:59:07,737] [    INFO][0m - loss: 0.00064854, learning_rate: 1.7800000000000002e-05, global_step: 610, interval_runtime: 18.5141, interval_samples_per_second: 0.864, interval_steps_per_second: 0.54, epoch: 8.1333[0m
[32m[2022-09-19 16:59:11,972] [    INFO][0m - loss: 0.00039633, learning_rate: 1.76e-05, global_step: 620, interval_runtime: 4.2354, interval_samples_per_second: 3.778, interval_steps_per_second: 2.361, epoch: 8.2667[0m
[32m[2022-09-19 16:59:16,174] [    INFO][0m - loss: 0.00194838, learning_rate: 1.74e-05, global_step: 630, interval_runtime: 4.2024, interval_samples_per_second: 3.807, interval_steps_per_second: 2.38, epoch: 8.4[0m
[32m[2022-09-19 16:59:20,451] [    INFO][0m - loss: 0.01548498, learning_rate: 1.72e-05, global_step: 640, interval_runtime: 4.2763, interval_samples_per_second: 3.742, interval_steps_per_second: 2.338, epoch: 8.5333[0m
[32m[2022-09-19 16:59:24,717] [    INFO][0m - loss: 0.03498323, learning_rate: 1.7e-05, global_step: 650, interval_runtime: 4.2663, interval_samples_per_second: 3.75, interval_steps_per_second: 2.344, epoch: 8.6667[0m
[32m[2022-09-19 16:59:28,940] [    INFO][0m - loss: 0.07111956, learning_rate: 1.6800000000000002e-05, global_step: 660, interval_runtime: 4.2231, interval_samples_per_second: 3.789, interval_steps_per_second: 2.368, epoch: 8.8[0m
[32m[2022-09-19 16:59:33,138] [    INFO][0m - loss: 0.00428819, learning_rate: 1.66e-05, global_step: 670, interval_runtime: 4.1976, interval_samples_per_second: 3.812, interval_steps_per_second: 2.382, epoch: 8.9333[0m
[32m[2022-09-19 16:59:34,911] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 16:59:34,912] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-19 16:59:34,912] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 16:59:34,912] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 16:59:34,912] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-19 16:59:46,733] [    INFO][0m - eval_loss: 5.279946804046631, eval_accuracy: 0.5418943533697632, eval_runtime: 11.821, eval_samples_per_second: 92.886, eval_steps_per_second: 5.837, epoch: 9.0[0m
[32m[2022-09-19 16:59:46,753] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-675[0m
[32m[2022-09-19 16:59:46,754] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 16:59:49,551] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-675/tokenizer_config.json[0m
[32m[2022-09-19 16:59:49,551] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-675/special_tokens_map.json[0m
[32m[2022-09-19 16:59:57,763] [    INFO][0m - loss: 0.00148435, learning_rate: 1.64e-05, global_step: 680, interval_runtime: 24.6252, interval_samples_per_second: 0.65, interval_steps_per_second: 0.406, epoch: 9.0667[0m
[32m[2022-09-19 17:00:02,035] [    INFO][0m - loss: 0.0066595, learning_rate: 1.62e-05, global_step: 690, interval_runtime: 4.2719, interval_samples_per_second: 3.745, interval_steps_per_second: 2.341, epoch: 9.2[0m
[32m[2022-09-19 17:00:06,558] [    INFO][0m - loss: 0.00085858, learning_rate: 1.6e-05, global_step: 700, interval_runtime: 4.2682, interval_samples_per_second: 3.749, interval_steps_per_second: 2.343, epoch: 9.3333[0m
[32m[2022-09-19 17:00:10,799] [    INFO][0m - loss: 0.00259863, learning_rate: 1.5799999999999998e-05, global_step: 710, interval_runtime: 4.4956, interval_samples_per_second: 3.559, interval_steps_per_second: 2.224, epoch: 9.4667[0m
[32m[2022-09-19 17:00:15,043] [    INFO][0m - loss: 0.00192097, learning_rate: 1.56e-05, global_step: 720, interval_runtime: 4.2447, interval_samples_per_second: 3.769, interval_steps_per_second: 2.356, epoch: 9.6[0m
[32m[2022-09-19 17:00:19,335] [    INFO][0m - loss: 0.0040392, learning_rate: 1.5399999999999998e-05, global_step: 730, interval_runtime: 4.2915, interval_samples_per_second: 3.728, interval_steps_per_second: 2.33, epoch: 9.7333[0m
[32m[2022-09-19 17:00:23,569] [    INFO][0m - loss: 0.00137637, learning_rate: 1.5200000000000002e-05, global_step: 740, interval_runtime: 4.2341, interval_samples_per_second: 3.779, interval_steps_per_second: 2.362, epoch: 9.8667[0m
[32m[2022-09-19 17:00:27,470] [    INFO][0m - loss: 0.00498598, learning_rate: 1.5e-05, global_step: 750, interval_runtime: 3.901, interval_samples_per_second: 4.102, interval_steps_per_second: 2.563, epoch: 10.0[0m
[32m[2022-09-19 17:00:27,470] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 17:00:27,470] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-19 17:00:27,471] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 17:00:27,471] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 17:00:27,471] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-19 17:00:39,237] [    INFO][0m - eval_loss: 5.587587356567383, eval_accuracy: 0.5391621129326047, eval_runtime: 11.7659, eval_samples_per_second: 93.32, eval_steps_per_second: 5.864, epoch: 10.0[0m
[32m[2022-09-19 17:00:39,256] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-750[0m
[32m[2022-09-19 17:00:39,257] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 17:00:41,993] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-750/tokenizer_config.json[0m
[32m[2022-09-19 17:00:41,993] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-750/special_tokens_map.json[0m
[32m[2022-09-19 17:00:51,416] [    INFO][0m - loss: 0.00515172, learning_rate: 1.48e-05, global_step: 760, interval_runtime: 23.9467, interval_samples_per_second: 0.668, interval_steps_per_second: 0.418, epoch: 10.1333[0m
[32m[2022-09-19 17:00:55,700] [    INFO][0m - loss: 0.01671551, learning_rate: 1.46e-05, global_step: 770, interval_runtime: 4.2838, interval_samples_per_second: 3.735, interval_steps_per_second: 2.334, epoch: 10.2667[0m
[32m[2022-09-19 17:00:59,934] [    INFO][0m - loss: 0.00427023, learning_rate: 1.44e-05, global_step: 780, interval_runtime: 4.2333, interval_samples_per_second: 3.78, interval_steps_per_second: 2.362, epoch: 10.4[0m
[32m[2022-09-19 17:01:04,179] [    INFO][0m - loss: 0.01591982, learning_rate: 1.42e-05, global_step: 790, interval_runtime: 4.2447, interval_samples_per_second: 3.769, interval_steps_per_second: 2.356, epoch: 10.5333[0m
[32m[2022-09-19 17:01:08,430] [    INFO][0m - loss: 0.00077899, learning_rate: 1.4e-05, global_step: 800, interval_runtime: 4.252, interval_samples_per_second: 3.763, interval_steps_per_second: 2.352, epoch: 10.6667[0m
[32m[2022-09-19 17:01:12,671] [    INFO][0m - loss: 0.00076871, learning_rate: 1.3800000000000002e-05, global_step: 810, interval_runtime: 4.2407, interval_samples_per_second: 3.773, interval_steps_per_second: 2.358, epoch: 10.8[0m
[32m[2022-09-19 17:01:16,956] [    INFO][0m - loss: 0.00017579, learning_rate: 1.36e-05, global_step: 820, interval_runtime: 4.2849, interval_samples_per_second: 3.734, interval_steps_per_second: 2.334, epoch: 10.9333[0m
[32m[2022-09-19 17:01:18,752] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 17:01:18,752] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-19 17:01:18,752] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 17:01:18,752] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 17:01:18,752] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-19 17:01:30,537] [    INFO][0m - eval_loss: 5.416053771972656, eval_accuracy: 0.5327868852459017, eval_runtime: 11.7849, eval_samples_per_second: 93.17, eval_steps_per_second: 5.855, epoch: 11.0[0m
[32m[2022-09-19 17:01:30,557] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-825[0m
[32m[2022-09-19 17:01:30,558] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 17:01:33,271] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-825/tokenizer_config.json[0m
[32m[2022-09-19 17:01:33,271] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-825/special_tokens_map.json[0m
[32m[2022-09-19 17:01:40,631] [    INFO][0m - loss: 0.03215581, learning_rate: 1.34e-05, global_step: 830, interval_runtime: 23.6748, interval_samples_per_second: 0.676, interval_steps_per_second: 0.422, epoch: 11.0667[0m
[32m[2022-09-19 17:01:44,897] [    INFO][0m - loss: 0.00113109, learning_rate: 1.32e-05, global_step: 840, interval_runtime: 4.2661, interval_samples_per_second: 3.75, interval_steps_per_second: 2.344, epoch: 11.2[0m
[32m[2022-09-19 17:01:49,113] [    INFO][0m - loss: 0.00021651, learning_rate: 1.3000000000000001e-05, global_step: 850, interval_runtime: 4.2164, interval_samples_per_second: 3.795, interval_steps_per_second: 2.372, epoch: 11.3333[0m
[32m[2022-09-19 17:01:56,005] [    INFO][0m - loss: 9.886e-05, learning_rate: 1.2800000000000001e-05, global_step: 860, interval_runtime: 4.2812, interval_samples_per_second: 3.737, interval_steps_per_second: 2.336, epoch: 11.4667[0m
[32m[2022-09-19 17:02:00,408] [    INFO][0m - loss: 3.718e-05, learning_rate: 1.26e-05, global_step: 870, interval_runtime: 6.8893, interval_samples_per_second: 2.322, interval_steps_per_second: 1.452, epoch: 11.6[0m
[32m[2022-09-19 17:02:04,647] [    INFO][0m - loss: 0.00571166, learning_rate: 1.24e-05, global_step: 880, interval_runtime: 4.3629, interval_samples_per_second: 3.667, interval_steps_per_second: 2.292, epoch: 11.7333[0m
[32m[2022-09-19 17:02:10,462] [    INFO][0m - loss: 0.00011047, learning_rate: 1.22e-05, global_step: 890, interval_runtime: 4.2485, interval_samples_per_second: 3.766, interval_steps_per_second: 2.354, epoch: 11.8667[0m
[32m[2022-09-19 17:02:15,599] [    INFO][0m - loss: 7.737e-05, learning_rate: 1.2e-05, global_step: 900, interval_runtime: 5.5077, interval_samples_per_second: 2.905, interval_steps_per_second: 1.816, epoch: 12.0[0m
[32m[2022-09-19 17:02:15,600] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 17:02:15,600] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-19 17:02:15,600] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 17:02:15,600] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 17:02:15,600] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-19 17:02:27,469] [    INFO][0m - eval_loss: 5.537326335906982, eval_accuracy: 0.5482695810564663, eval_runtime: 11.869, eval_samples_per_second: 92.51, eval_steps_per_second: 5.813, epoch: 12.0[0m
[32m[2022-09-19 17:02:27,489] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-900[0m
[32m[2022-09-19 17:02:27,490] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 17:02:30,202] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-900/tokenizer_config.json[0m
[32m[2022-09-19 17:02:30,202] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-900/special_tokens_map.json[0m
[32m[2022-09-19 17:02:39,606] [    INFO][0m - loss: 0.00176735, learning_rate: 1.1799999999999999e-05, global_step: 910, interval_runtime: 25.2029, interval_samples_per_second: 0.635, interval_steps_per_second: 0.397, epoch: 12.1333[0m
[32m[2022-09-19 17:02:43,849] [    INFO][0m - loss: 7.124e-05, learning_rate: 1.16e-05, global_step: 920, interval_runtime: 4.2434, interval_samples_per_second: 3.771, interval_steps_per_second: 2.357, epoch: 12.2667[0m
[32m[2022-09-19 17:02:48,054] [    INFO][0m - loss: 7.57e-05, learning_rate: 1.1400000000000001e-05, global_step: 930, interval_runtime: 4.2044, interval_samples_per_second: 3.806, interval_steps_per_second: 2.378, epoch: 12.4[0m
[32m[2022-09-19 17:02:52,345] [    INFO][0m - loss: 0.00013353, learning_rate: 1.1200000000000001e-05, global_step: 940, interval_runtime: 4.2916, interval_samples_per_second: 3.728, interval_steps_per_second: 2.33, epoch: 12.5333[0m
[32m[2022-09-19 17:02:56,577] [    INFO][0m - loss: 0.00015553, learning_rate: 1.1e-05, global_step: 950, interval_runtime: 4.232, interval_samples_per_second: 3.781, interval_steps_per_second: 2.363, epoch: 12.6667[0m
[32m[2022-09-19 17:03:00,813] [    INFO][0m - loss: 7.332e-05, learning_rate: 1.08e-05, global_step: 960, interval_runtime: 4.2354, interval_samples_per_second: 3.778, interval_steps_per_second: 2.361, epoch: 12.8[0m
[32m[2022-09-19 17:03:05,113] [    INFO][0m - loss: 0.00018151, learning_rate: 1.06e-05, global_step: 970, interval_runtime: 4.2996, interval_samples_per_second: 3.721, interval_steps_per_second: 2.326, epoch: 12.9333[0m
[32m[2022-09-19 17:03:06,899] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 17:03:06,899] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-19 17:03:06,899] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 17:03:06,899] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 17:03:06,899] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-19 17:03:18,701] [    INFO][0m - eval_loss: 5.636324882507324, eval_accuracy: 0.546448087431694, eval_runtime: 11.8014, eval_samples_per_second: 93.04, eval_steps_per_second: 5.847, epoch: 13.0[0m
[32m[2022-09-19 17:03:18,720] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-975[0m
[32m[2022-09-19 17:03:18,720] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 17:03:21,517] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-975/tokenizer_config.json[0m
[32m[2022-09-19 17:03:21,517] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-975/special_tokens_map.json[0m
[32m[2022-09-19 17:03:28,601] [    INFO][0m - loss: 0.00011619, learning_rate: 1.04e-05, global_step: 980, interval_runtime: 23.4883, interval_samples_per_second: 0.681, interval_steps_per_second: 0.426, epoch: 13.0667[0m
[32m[2022-09-19 17:03:32,566] [    INFO][0m - loss: 4.838e-05, learning_rate: 1.02e-05, global_step: 990, interval_runtime: 3.9658, interval_samples_per_second: 4.035, interval_steps_per_second: 2.522, epoch: 13.2[0m
[32m[2022-09-19 17:03:36,501] [    INFO][0m - loss: 3.079e-05, learning_rate: 9.999999999999999e-06, global_step: 1000, interval_runtime: 3.9345, interval_samples_per_second: 4.067, interval_steps_per_second: 2.542, epoch: 13.3333[0m
[32m[2022-09-19 17:03:44,405] [    INFO][0m - loss: 5.272e-05, learning_rate: 9.8e-06, global_step: 1010, interval_runtime: 5.7471, interval_samples_per_second: 2.784, interval_steps_per_second: 1.74, epoch: 13.4667[0m
[32m[2022-09-19 17:03:48,341] [    INFO][0m - loss: 3.83e-05, learning_rate: 9.600000000000001e-06, global_step: 1020, interval_runtime: 6.0933, interval_samples_per_second: 2.626, interval_steps_per_second: 1.641, epoch: 13.6[0m
[32m[2022-09-19 17:03:52,234] [    INFO][0m - loss: 0.00040832, learning_rate: 9.400000000000001e-06, global_step: 1030, interval_runtime: 3.8928, interval_samples_per_second: 4.11, interval_steps_per_second: 2.569, epoch: 13.7333[0m
[32m[2022-09-19 17:03:56,157] [    INFO][0m - loss: 7.985e-05, learning_rate: 9.2e-06, global_step: 1040, interval_runtime: 3.9226, interval_samples_per_second: 4.079, interval_steps_per_second: 2.549, epoch: 13.8667[0m
[32m[2022-09-19 17:03:59,811] [    INFO][0m - loss: 0.00021559, learning_rate: 9e-06, global_step: 1050, interval_runtime: 3.6544, interval_samples_per_second: 4.378, interval_steps_per_second: 2.736, epoch: 14.0[0m
[32m[2022-09-19 17:03:59,812] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 17:03:59,812] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-19 17:03:59,812] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 17:03:59,812] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 17:03:59,812] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-19 17:04:08,921] [    INFO][0m - eval_loss: 5.72719144821167, eval_accuracy: 0.5446265938069217, eval_runtime: 9.109, eval_samples_per_second: 120.54, eval_steps_per_second: 7.575, epoch: 14.0[0m
[32m[2022-09-19 17:04:08,944] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-1050[0m
[32m[2022-09-19 17:04:08,944] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 17:04:11,733] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-1050/tokenizer_config.json[0m
[32m[2022-09-19 17:04:11,734] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-1050/special_tokens_map.json[0m
[32m[2022-09-19 17:04:21,236] [    INFO][0m - loss: 0.00010655, learning_rate: 8.8e-06, global_step: 1060, interval_runtime: 21.4252, interval_samples_per_second: 0.747, interval_steps_per_second: 0.467, epoch: 14.1333[0m
[32m[2022-09-19 17:04:25,497] [    INFO][0m - loss: 2.887e-05, learning_rate: 8.6e-06, global_step: 1070, interval_runtime: 4.261, interval_samples_per_second: 3.755, interval_steps_per_second: 2.347, epoch: 14.2667[0m
[32m[2022-09-19 17:04:29,728] [    INFO][0m - loss: 0.00014223, learning_rate: 8.400000000000001e-06, global_step: 1080, interval_runtime: 4.2303, interval_samples_per_second: 3.782, interval_steps_per_second: 2.364, epoch: 14.4[0m
[32m[2022-09-19 17:04:37,945] [    INFO][0m - loss: 3.697e-05, learning_rate: 8.2e-06, global_step: 1090, interval_runtime: 4.2613, interval_samples_per_second: 3.755, interval_steps_per_second: 2.347, epoch: 14.5333[0m
[32m[2022-09-19 17:04:42,098] [    INFO][0m - loss: 6.58e-05, learning_rate: 8e-06, global_step: 1100, interval_runtime: 8.109, interval_samples_per_second: 1.973, interval_steps_per_second: 1.233, epoch: 14.6667[0m
[32m[2022-09-19 17:04:46,352] [    INFO][0m - loss: 2.177e-05, learning_rate: 7.8e-06, global_step: 1110, interval_runtime: 4.2541, interval_samples_per_second: 3.761, interval_steps_per_second: 2.351, epoch: 14.8[0m
[32m[2022-09-19 17:04:50,593] [    INFO][0m - loss: 3.544e-05, learning_rate: 7.600000000000001e-06, global_step: 1120, interval_runtime: 4.2408, interval_samples_per_second: 3.773, interval_steps_per_second: 2.358, epoch: 14.9333[0m
[32m[2022-09-19 17:04:52,356] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 17:04:52,356] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-19 17:04:52,356] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 17:04:52,357] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 17:04:52,357] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-19 17:05:04,175] [    INFO][0m - eval_loss: 5.7727580070495605, eval_accuracy: 0.546448087431694, eval_runtime: 11.818, eval_samples_per_second: 92.909, eval_steps_per_second: 5.839, epoch: 15.0[0m
[32m[2022-09-19 17:05:04,195] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-1125[0m
[32m[2022-09-19 17:05:04,195] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 17:05:06,875] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-1125/tokenizer_config.json[0m
[32m[2022-09-19 17:05:06,875] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-1125/special_tokens_map.json[0m
[32m[2022-09-19 17:05:14,009] [    INFO][0m - loss: 0.00011479, learning_rate: 7.4e-06, global_step: 1130, interval_runtime: 23.4164, interval_samples_per_second: 0.683, interval_steps_per_second: 0.427, epoch: 15.0667[0m
[32m[2022-09-19 17:05:18,220] [    INFO][0m - loss: 9.583e-05, learning_rate: 7.2e-06, global_step: 1140, interval_runtime: 4.2111, interval_samples_per_second: 3.799, interval_steps_per_second: 2.375, epoch: 15.2[0m
[32m[2022-09-19 17:05:24,841] [    INFO][0m - loss: 2.218e-05, learning_rate: 7e-06, global_step: 1150, interval_runtime: 4.2761, interval_samples_per_second: 3.742, interval_steps_per_second: 2.339, epoch: 15.3333[0m
[32m[2022-09-19 17:05:29,121] [    INFO][0m - loss: 0.00014268, learning_rate: 6.8e-06, global_step: 1160, interval_runtime: 6.6246, interval_samples_per_second: 2.415, interval_steps_per_second: 1.51, epoch: 15.4667[0m
[32m[2022-09-19 17:05:33,326] [    INFO][0m - loss: 1.824e-05, learning_rate: 6.6e-06, global_step: 1170, interval_runtime: 4.2049, interval_samples_per_second: 3.805, interval_steps_per_second: 2.378, epoch: 15.6[0m
[32m[2022-09-19 17:05:37,622] [    INFO][0m - loss: 3.768e-05, learning_rate: 6.4000000000000006e-06, global_step: 1180, interval_runtime: 4.296, interval_samples_per_second: 3.724, interval_steps_per_second: 2.328, epoch: 15.7333[0m
[32m[2022-09-19 17:05:41,855] [    INFO][0m - loss: 0.00073146, learning_rate: 6.2e-06, global_step: 1190, interval_runtime: 4.2333, interval_samples_per_second: 3.78, interval_steps_per_second: 2.362, epoch: 15.8667[0m
[32m[2022-09-19 17:05:45,794] [    INFO][0m - loss: 2.004e-05, learning_rate: 6e-06, global_step: 1200, interval_runtime: 3.9384, interval_samples_per_second: 4.063, interval_steps_per_second: 2.539, epoch: 16.0[0m
[32m[2022-09-19 17:05:45,795] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 17:05:45,795] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-19 17:05:45,795] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 17:05:45,795] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 17:05:45,795] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-19 17:05:57,565] [    INFO][0m - eval_loss: 5.739755153656006, eval_accuracy: 0.5482695810564663, eval_runtime: 11.7698, eval_samples_per_second: 93.29, eval_steps_per_second: 5.862, epoch: 16.0[0m
[32m[2022-09-19 17:05:57,585] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-1200[0m
[32m[2022-09-19 17:05:57,585] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 17:06:00,214] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-1200/tokenizer_config.json[0m
[32m[2022-09-19 17:06:00,214] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-1200/special_tokens_map.json[0m
[32m[2022-09-19 17:06:09,415] [    INFO][0m - loss: 0.00673159, learning_rate: 5.8e-06, global_step: 1210, interval_runtime: 23.6215, interval_samples_per_second: 0.677, interval_steps_per_second: 0.423, epoch: 16.1333[0m
[32m[2022-09-19 17:06:13,646] [    INFO][0m - loss: 0.00012193, learning_rate: 5.600000000000001e-06, global_step: 1220, interval_runtime: 4.2306, interval_samples_per_second: 3.782, interval_steps_per_second: 2.364, epoch: 16.2667[0m
[32m[2022-09-19 17:06:17,897] [    INFO][0m - loss: 1.2e-05, learning_rate: 5.4e-06, global_step: 1230, interval_runtime: 4.2512, interval_samples_per_second: 3.764, interval_steps_per_second: 2.352, epoch: 16.4[0m
[32m[2022-09-19 17:06:22,176] [    INFO][0m - loss: 8.88e-06, learning_rate: 5.2e-06, global_step: 1240, interval_runtime: 4.2791, interval_samples_per_second: 3.739, interval_steps_per_second: 2.337, epoch: 16.5333[0m
[32m[2022-09-19 17:06:26,445] [    INFO][0m - loss: 7.596e-05, learning_rate: 4.9999999999999996e-06, global_step: 1250, interval_runtime: 4.269, interval_samples_per_second: 3.748, interval_steps_per_second: 2.342, epoch: 16.6667[0m
[32m[2022-09-19 17:06:30,653] [    INFO][0m - loss: 4.421e-05, learning_rate: 4.800000000000001e-06, global_step: 1260, interval_runtime: 4.2083, interval_samples_per_second: 3.802, interval_steps_per_second: 2.376, epoch: 16.8[0m
[32m[2022-09-19 17:06:34,928] [    INFO][0m - loss: 0.00121591, learning_rate: 4.6e-06, global_step: 1270, interval_runtime: 4.2744, interval_samples_per_second: 3.743, interval_steps_per_second: 2.34, epoch: 16.9333[0m
[32m[2022-09-19 17:06:36,719] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 17:06:36,719] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-19 17:06:36,720] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 17:06:36,720] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 17:06:36,720] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-19 17:06:48,472] [    INFO][0m - eval_loss: 5.691173553466797, eval_accuracy: 0.5519125683060109, eval_runtime: 11.7522, eval_samples_per_second: 93.43, eval_steps_per_second: 5.871, epoch: 17.0[0m
[32m[2022-09-19 17:06:48,491] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-1275[0m
[32m[2022-09-19 17:06:48,491] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 17:06:51,099] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-1275/tokenizer_config.json[0m
[32m[2022-09-19 17:06:51,099] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-1275/special_tokens_map.json[0m
[32m[2022-09-19 17:06:58,235] [    INFO][0m - loss: 2.601e-05, learning_rate: 4.4e-06, global_step: 1280, interval_runtime: 23.3071, interval_samples_per_second: 0.686, interval_steps_per_second: 0.429, epoch: 17.0667[0m
[32m[2022-09-19 17:07:03,997] [    INFO][0m - loss: 1.679e-05, learning_rate: 4.2000000000000004e-06, global_step: 1290, interval_runtime: 5.7617, interval_samples_per_second: 2.777, interval_steps_per_second: 1.736, epoch: 17.2[0m
[32m[2022-09-19 17:07:08,265] [    INFO][0m - loss: 2.735e-05, learning_rate: 4e-06, global_step: 1300, interval_runtime: 4.268, interval_samples_per_second: 3.749, interval_steps_per_second: 2.343, epoch: 17.3333[0m
[32m[2022-09-19 17:07:12,465] [    INFO][0m - loss: 1.506e-05, learning_rate: 3.8000000000000005e-06, global_step: 1310, interval_runtime: 4.2004, interval_samples_per_second: 3.809, interval_steps_per_second: 2.381, epoch: 17.4667[0m
[32m[2022-09-19 17:07:16,726] [    INFO][0m - loss: 0.00057381, learning_rate: 3.6e-06, global_step: 1320, interval_runtime: 4.2609, interval_samples_per_second: 3.755, interval_steps_per_second: 2.347, epoch: 17.6[0m
[32m[2022-09-19 17:07:20,967] [    INFO][0m - loss: 0.00011911, learning_rate: 3.4e-06, global_step: 1330, interval_runtime: 4.2417, interval_samples_per_second: 3.772, interval_steps_per_second: 2.358, epoch: 17.7333[0m
[32m[2022-09-19 17:07:25,228] [    INFO][0m - loss: 2.285e-05, learning_rate: 3.2000000000000003e-06, global_step: 1340, interval_runtime: 4.2601, interval_samples_per_second: 3.756, interval_steps_per_second: 2.347, epoch: 17.8667[0m
[32m[2022-09-19 17:07:31,423] [    INFO][0m - loss: 2.5e-05, learning_rate: 3e-06, global_step: 1350, interval_runtime: 3.8999, interval_samples_per_second: 4.103, interval_steps_per_second: 2.564, epoch: 18.0[0m
[32m[2022-09-19 17:07:31,424] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 17:07:31,424] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-19 17:07:31,424] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 17:07:31,424] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 17:07:31,425] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-19 17:07:43,161] [    INFO][0m - eval_loss: 5.709249496459961, eval_accuracy: 0.5482695810564663, eval_runtime: 11.7362, eval_samples_per_second: 93.557, eval_steps_per_second: 5.879, epoch: 18.0[0m
[32m[2022-09-19 17:07:43,181] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-1350[0m
[32m[2022-09-19 17:07:43,182] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 17:07:45,791] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-1350/tokenizer_config.json[0m
[32m[2022-09-19 17:07:45,791] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-1350/special_tokens_map.json[0m
[32m[2022-09-19 17:07:54,953] [    INFO][0m - loss: 1.266e-05, learning_rate: 2.8000000000000003e-06, global_step: 1360, interval_runtime: 25.8251, interval_samples_per_second: 0.62, interval_steps_per_second: 0.387, epoch: 18.1333[0m
[32m[2022-09-19 17:07:59,222] [    INFO][0m - loss: 2.027e-05, learning_rate: 2.6e-06, global_step: 1370, interval_runtime: 4.269, interval_samples_per_second: 3.748, interval_steps_per_second: 2.342, epoch: 18.2667[0m
[32m[2022-09-19 17:08:03,418] [    INFO][0m - loss: 1.938e-05, learning_rate: 2.4000000000000003e-06, global_step: 1380, interval_runtime: 4.1962, interval_samples_per_second: 3.813, interval_steps_per_second: 2.383, epoch: 18.4[0m
[32m[2022-09-19 17:08:07,675] [    INFO][0m - loss: 1.498e-05, learning_rate: 2.2e-06, global_step: 1390, interval_runtime: 4.2572, interval_samples_per_second: 3.758, interval_steps_per_second: 2.349, epoch: 18.5333[0m
[32m[2022-09-19 17:08:15,088] [    INFO][0m - loss: 2.325e-05, learning_rate: 2e-06, global_step: 1400, interval_runtime: 4.2553, interval_samples_per_second: 3.76, interval_steps_per_second: 2.35, epoch: 18.6667[0m
[32m[2022-09-19 17:08:19,322] [    INFO][0m - loss: 1.352e-05, learning_rate: 1.8e-06, global_step: 1410, interval_runtime: 7.3914, interval_samples_per_second: 2.165, interval_steps_per_second: 1.353, epoch: 18.8[0m
[32m[2022-09-19 17:08:23,570] [    INFO][0m - loss: 1.68e-05, learning_rate: 1.6000000000000001e-06, global_step: 1420, interval_runtime: 4.2483, interval_samples_per_second: 3.766, interval_steps_per_second: 2.354, epoch: 18.9333[0m
[32m[2022-09-19 17:08:25,374] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 17:08:25,374] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-19 17:08:25,374] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 17:08:25,374] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 17:08:25,374] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-19 17:08:37,120] [    INFO][0m - eval_loss: 5.710758209228516, eval_accuracy: 0.5473588342440802, eval_runtime: 11.7453, eval_samples_per_second: 93.485, eval_steps_per_second: 5.875, epoch: 19.0[0m
[32m[2022-09-19 17:08:37,138] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-1425[0m
[32m[2022-09-19 17:08:37,139] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19[32m[2022-09-19 17:08:40,488] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 17:08:40,488] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-19 17:08:40,488] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 17:08:40,48[32m[2022-09-19 17:08:46,677] [    INFO][0m - loss: 0.01275431, learning_rate: 1.4000000000000001e-06, global_step: 1430, interv[32m[2022-09-19 17:09:27,953] [    INFO][0m - eval_loss: 1.828872561454773, eval_accuracy: 0.4559359067734887, eval_runtime: 47.4642, eval_samples_per_second: 28.927, eval_steps_per_second: 1.812, epoch: 2.0[0m
[32m[2022-09-19 17:09:27,974] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-378[0m
[32m[2022-09-19 17:09:27,974] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 17:09:30,872] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-378/tokenizer_config.json[0m
[32m[2022-09-19 17:09:30,873] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-378/special_tokens_map.json[0m
[32m[2022-09-19 17:09:38,179] [    INFO][0m - loss: 1.04342632, learning_rate: 2.6984126984126984e-05, global_step: 380, interval_runtime: 65.6416, interval_samples_per_second: 0.244, interval_steps_per_second: 0.152, epoch: 2.0106[0m
[32m[2022-09-19 17:09:52,396] [    INFO][0m - loss: 0.70713334, learning_rate: 2.6904761904761905e-05, global_step: 390, interval_runtime: 14.2167, interval_samples_per_second: 1.125, interval_steps_per_second: 0.703, epoch: 2.0635[0m
[32m[2022-09-19 17:10:05,536] [    INFO][0m - loss: 0.66577725, learning_rate: 2.6825396825396827e-05, global_step: 400, interval_runtime: 13.1407, interval_samples_per_second: 1.218, interval_steps_per_second: 0.761, epoch: 2.1164[0m
[32m[2022-09-19 17:10:17,965] [    INFO][0m - loss: 0.73733282, learning_rate: 2.6746031746031745e-05, global_step: 410, interval_runtime: 12.4289, interval_samples_per_second: 1.287, interval_steps_per_second: 0.805, epoch: 2.1693[0m
[32m[2022-09-19 17:10:25,978] [    INFO][0m - loss: 0.7703516, learning_rate: 2.6666666666666667e-05, global_step: 420, interval_runtime: 8.0132, interval_samples_per_second: 1.997, interval_steps_per_second: 1.248, epoch: 2.2222[0m
[32m[2022-09-19 17:10:34,528] [    INFO][0m - loss: 0.81880121, learning_rate: 2.6587301587301588e-05, global_step: 430, interval_runtime: 8.55, interval_samples_per_second: 1.871, interval_steps_per_second: 1.17, epoch: 2.2751[0m
[32m[2022-09-19 17:10:44,268] [    INFO][0m - loss: 0.76974955, learning_rate: 2.650793650793651e-05, global_step: 440, interval_runtime: 9.7402, interval_samples_per_second: 1.643, interval_steps_per_second: 1.027, epoch: 2.328[0m
[32m[2022-09-19 17:10:52,274] [    INFO][0m - loss: 0.65256829, learning_rate: 2.6428571428571428e-05, global_step: 450, interval_runtime: 8.0059, interval_samples_per_second: 1.999, interval_steps_per_second: 1.249, epoch: 2.381[0m
[32m[2022-09-19 17:11:00,628] [    INFO][0m - loss: 0.9170846, learning_rate: 2.634920634920635e-05, global_step: 460, interval_runtime: 8.353, interval_samples_per_second: 1.915, interval_steps_per_second: 1.197, epoch: 2.4339[0m
[32m[2022-09-19 17:11:09,261] [    INFO][0m - loss: 0.83684502, learning_rate: 2.626984126984127e-05, global_step: 470, interval_runtime: 8.6333, interval_samples_per_second: 1.853, interval_steps_per_second: 1.158, epoch: 2.4868[0m
[32m[2022-09-19 17:11:22,880] [    INFO][0m - loss: 0.87368279, learning_rate: 2.6190476190476192e-05, global_step: 480, interval_runtime: 13.6189, interval_samples_per_second: 1.175, interval_steps_per_second: 0.734, epoch: 2.5397[0m
[32m[2022-09-19 17:11:31,533] [    INFO][0m - loss: 0.88113632, learning_rate: 2.611111111111111e-05, global_step: 490, interval_runtime: 8.6533, interval_samples_per_second: 1.849, interval_steps_per_second: 1.156, epoch: 2.5926[0m
[32m[2022-09-19 17:11:44,424] [    INFO][0m - loss: 0.84500217, learning_rate: 2.6031746031746032e-05, global_step: 500, interval_runtime: 12.8907, interval_samples_per_second: 1.241, interval_steps_per_second: 0.776, epoch: 2.6455[0m
[32m[2022-09-19 17:11:54,027] [    INFO][0m - loss: 0.96213942, learning_rate: 2.5952380952380953e-05, global_step: 510, interval_runtime: 9.6032, interval_samples_per_second: 1.666, interval_steps_per_second: 1.041, epoch: 2.6984[0m
[32m[2022-09-19 17:12:05,929] [    INFO][0m - loss: 0.93121576, learning_rate: 2.5873015873015875e-05, global_step: 520, interval_runtime: 11.9021, interval_samples_per_second: 1.344, interval_steps_per_second: 0.84, epoch: 2.7513[0m
[32m[2022-09-19 17:12:17,252] [    INFO][0m - loss: 0.90752544, learning_rate: 2.5793650793650793e-05, global_step: 530, interval_runtime: 11.3223, interval_samples_per_second: 1.413, interval_steps_per_second: 0.883, epoch: 2.8042[0m
[32m[2022-09-19 17:12:27,463] [    INFO][0m - loss: 0.96308937, learning_rate: 2.5714285714285714e-05, global_step: 540, interval_runtime: 10.211, interval_samples_per_second: 1.567, interval_steps_per_second: 0.979, epoch: 2.8571[0m
[32m[2022-09-19 17:12:40,563] [    INFO][0m - loss: 0.89372387, learning_rate: 2.5634920634920636e-05, global_step: 550, interval_runtime: 13.1004, interval_samples_per_second: 1.221, interval_steps_per_second: 0.763, epoch: 2.9101[0m
[32m[2022-09-19 17:12:48,984] [    INFO][0m - loss: 0.80475693, learning_rate: 2.5555555555555557e-05, global_step: 560, interval_runtime: 8.4214, interval_samples_per_second: 1.9, interval_steps_per_second: 1.187, epoch: 2.963[0m
[32m[2022-09-19 17:12:59,753] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 17:12:59,753] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-19 17:12:59,753] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 17:12:59,753] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 17:12:59,753] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-19 17:13:32,432] [    INFO][0m - eval_loss: 2.088042736053467, eval_accuracy: 0.4435542607428988, eval_runtime: 32.679, eval_samples_per_second: 42.015, eval_steps_per_second: 2.632, epoch: 3.0[0m
[32m[2022-09-19 17:13:32,456] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-567[0m
[32m[2022-09-19 17:13:32,457] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 17:13:35,053] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-567/tokenizer_config.json[0m
[32m[2022-09-19 17:13:35,054] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-567/special_tokens_map.json[0m
[32m[2022-09-19 17:13:42,767] [    INFO][0m - loss: 0.67972794, learning_rate: 2.5476190476190476e-05, global_step: 570, interval_runtime: 53.7831, interval_samples_per_second: 0.297, interval_steps_per_second: 0.186, epoch: 3.0159[0m
[32m[2022-09-19 17:13:56,279] [    INFO][0m - loss: 0.39671049, learning_rate: 2.5396825396825397e-05, global_step: 580, interval_runtime: 13.5115, interval_samples_per_second: 1.184, interval_steps_per_second: 0.74, epoch: 3.0688[0m
[32m[2022-09-19 17:14:04,914] [    INFO][0m - loss: 0.43354931, learning_rate: 2.531746031746032e-05, global_step: 590, interval_runtime: 8.6348, interval_samples_per_second: 1.853, interval_steps_per_second: 1.158, epoch: 3.1217[0m
[32m[2022-09-19 17:14:17,835] [    INFO][0m - loss: 0.44359303, learning_rate: 2.523809523809524e-05, global_step: 600, interval_runtime: 12.9211, interval_samples_per_second: 1.238, interval_steps_per_second: 0.774, epoch: 3.1746[0m
[32m[2022-09-19 17:14:30,384] [    INFO][0m - loss: 0.46989965, learning_rate: 2.5158730158730158e-05, global_step: 610, interval_runtime: 12.5495, interval_samples_per_second: 1.275, interval_steps_per_second: 0.797, epoch: 3.2275[0m
[32m[2022-09-19 17:14:39,267] [    INFO][0m - loss: 0.33525746, learning_rate: 2.507936507936508e-05, global_step: 620, interval_runtime: 8.8827, interval_samples_per_second: 1.801, interval_steps_per_second: 1.126, epoch: 3.2804[0m
[32m[2022-09-19 17:14:52,772] [    INFO][0m - loss: 0.52336097, learning_rate: 2.5e-05, global_step: 630, interval_runtime: 13.5053, interval_samples_per_second: 1.185, interval_steps_per_second: 0.74, epoch: 3.3333[0m
[32m[2022-09-19 17:15:02,036] [    INFO][0m - loss: 0.27170789, learning_rate: 2.4920634920634923e-05, global_step: 640, interval_runtime: 9.2642, interval_samples_per_second: 1.727, interval_steps_per_second: 1.079, epoch: 3.3862[0m
[32m[2022-09-19 17:15:14,250] [    INFO][0m - loss: 0.32293055, learning_rate: 2.484126984126984e-05, global_step: 650, interval_runtime: 12.2137, interval_samples_per_second: 1.31, interval_steps_per_second: 0.819, epoch: 3.4392[0m
[32m[2022-09-19 17:15:24,172] [    INFO][0m - loss: 0.3700568, learning_rate: 2.4761904761904762e-05, global_step: 660, interval_runtime: 9.9213, interval_samples_per_second: 1.613, interval_steps_per_second: 1.008, epoch: 3.4921[0m
[32m[2022-09-19 17:15:35,770] [    INFO][0m - loss: 0.47461276, learning_rate: 2.4682539682539684e-05, global_step: 670, interval_runtime: 11.5978, interval_samples_per_second: 1.38, interval_steps_per_second: 0.862, epoch: 3.545[0m
[32m[2022-09-19 17:15:49,268] [    INFO][0m - loss: 0.50641184, learning_rate: 2.4603174603174605e-05, global_step: 680, interval_runtime: 13.4983, interval_samples_per_second: 1.185, interval_steps_per_second: 0.741, epoch: 3.5979[0m
[32m[2022-09-19 17:15:58,917] [    INFO][0m - loss: 0.49444175, learning_rate: 2.4523809523809523e-05, global_step: 690, interval_runtime: 7.9962, interval_samples_per_second: 2.001, interval_steps_per_second: 1.251, epoch: 3.6508[0m
[32m[2022-09-19 17:16:12,404] [    INFO][0m - loss: 0.40083575, learning_rate: 2.4444444444444445e-05, global_step: 700, interval_runtime: 15.1395, interval_samples_per_second: 1.057, interval_steps_per_second: 0.661, epoch: 3.7037[0m
[32m[2022-09-19 17:16:24,724] [    INFO][0m - loss: 0.4183352, learning_rate: 2.4365079365079366e-05, global_step: 710, interval_runtime: 12.3209, interval_samples_per_second: 1.299, interval_steps_per_second: 0.812, epoch: 3.7566[0m
[32m[2022-09-19 17:16:34,113] [    INFO][0m - loss: 0.37050998, learning_rate: 2.4285714285714288e-05, global_step: 720, interval_runtime: 9.3886, interval_samples_per_second: 1.704, interval_steps_per_second: 1.065, epoch: 3.8095[0m
[32m[2022-09-19 17:16:47,603] [    INFO][0m - loss: 0.52452836, learning_rate: 2.4206349206349206e-05, global_step: 730, interval_runtime: 13.4906, interval_samples_per_second: 1.186, interval_steps_per_second: 0.741, epoch: 3.8624[0m
[32m[2022-09-19 17:16:59,156] [    INFO][0m - loss: 0.63295236, learning_rate: 2.4126984126984128e-05, global_step: 740, interval_runtime: 11.5524, interval_samples_per_second: 1.385, interval_steps_per_second: 0.866, epoch: 3.9153[0m
[32m[2022-09-19 17:17:09,075] [    INFO][0m - loss: 0.53656859, learning_rate: 2.404761904761905e-05, global_step: 750, interval_runtime: 9.9188, interval_samples_per_second: 1.613, interval_steps_per_second: 1.008, epoch: 3.9683[0m
[32m[2022-09-19 17:17:17,926] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 17:17:17,926] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-19 17:17:17,926] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 17:17:17,926] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 17:17:17,926] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-19 17:17:58,785] [    INFO][0m - eval_loss: 2.650620460510254, eval_accuracy: 0.43262927895120173, eval_runtime: 40.8582, eval_samples_per_second: 33.604, eval_steps_per_second: 2.105, epoch: 4.0[0m
[32m[2022-09-19 17:17:58,808] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-756[0m
[32m[2022-09-19 17:17:58,808] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 17:18:01,427] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-756/tokenizer_config.json[0m
[32m[2022-09-19 17:18:01,428] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-756/special_tokens_map.json[0m
[32m[2022-09-19 17:18:11,220] [    INFO][0m - loss: 0.41615458, learning_rate: 2.396825396825397e-05, global_step: 760, interval_runtime: 62.1456, interval_samples_per_second: 0.257, interval_steps_per_second: 0.161, epoch: 4.0212[0m
[32m[2022-09-19 17:18:27,325] [    INFO][0m - loss: 0.33445253, learning_rate: 2.388888888888889e-05, global_step: 770, interval_runtime: 16.1052, interval_samples_per_second: 0.993, interval_steps_per_second: 0.621, epoch: 4.0741[0m
[32m[2022-09-19 17:18:43,474] [    INFO][0m - loss: 0.13194153, learning_rate: 2.380952380952381e-05, global_step: 780, interval_runtime: 16.1482, interval_samples_per_second: 0.991, interval_steps_per_second: 0.619, epoch: 4.127[0m
[32m[2022-09-19 17:18:59,544] [    INFO][0m - loss: 0.29218166, learning_rate: 2.373015873015873e-05, global_step: 790, interval_runtime: 16.07, interval_samples_per_second: 0.996, interval_steps_per_second: 0.622, epoch: 4.1799[0m
[32m[2022-09-19 17:19:08,690] [    INFO][0m - loss: 0.12776893, learning_rate: 2.3650793650793653e-05, global_step: 800, interval_runtime: 9.1458, interval_samples_per_second: 1.749, interval_steps_per_second: 1.093, epoch: 4.2328[0m
[32m[2022-09-19 17:19:16,657] [    INFO][0m - loss: 0.18327395, learning_rate: 2.357142857142857e-05, global_step: 810, interval_runtime: 7.9669, interval_samples_per_second: 2.008, interval_steps_per_second: 1.255, epoch: 4.2857[0m
[32m[2022-09-19 17:19:24,922] [    INFO][0m - loss: 0.27729452, learning_rate: 2.3492063492063493e-05, global_step: 820, interval_runtime: 8.2652, interval_samples_per_second: 1.936, interval_steps_per_second: 1.21, epoch: 4.3386[0m
[32m[2022-09-19 17:19:33,545] [    INFO][0m - loss: 0.35705016, learning_rate: 2.3412698412698414e-05, global_step: 830, interval_runtime: 8.6227, interval_samples_per_second: 1.856, interval_steps_per_second: 1.16, epoch: 4.3915[0m
[32m[2022-09-19 17:19:44,651] [    INFO][0m - loss: 0.21873219, learning_rate: 2.3333333333333336e-05, global_step: 840, interval_runtime: 11.107, interval_samples_per_second: 1.441, interval_steps_per_second: 0.9, epoch: 4.4444[0m
[32m[2022-09-19 17:19:55,678] [    INFO][0m - loss: 0.24664495, learning_rate: 2.3253968253968254e-05, global_step: 850, interval_runtime: 11.0262, interval_samples_per_second: 1.451, interval_steps_per_second: 0.907, epoch: 4.4974[0m
[32m[2022-09-19 17:20:04,958] [    INFO][0m - loss: 0.19198022, learning_rate: 2.3174603174603175e-05, global_step: 860, interval_runtime: 9.2801, interval_samples_per_second: 1.724, interval_steps_per_second: 1.078, epoch: 4.5503[0m
[32m[2022-09-19 17:20:14,648] [    INFO][0m - loss: 0.23710768, learning_rate: 2.3095238095238097e-05, global_step: 870, interval_runtime: 9.6906, interval_samples_per_second: 1.651, interval_steps_per_second: 1.032, epoch: 4.6032[0m
[32m[2022-09-19 17:20:25,624] [    INFO][0m - loss: 0.23717425, learning_rate: 2.301587301587302e-05, global_step: 880, interval_runtime: 10.9755, interval_samples_per_second: 1.458, interval_steps_per_second: 0.911, epoch: 4.6561[0m
[32m[2022-09-19 17:20:36,587] [    INFO][0m - loss: 0.30320315, learning_rate: 2.2936507936507937e-05, global_step: 890, interval_runtime: 10.9632, interval_samples_per_second: 1.459, interval_steps_per_second: 0.912, epoch: 4.709[0m
[32m[2022-09-19 17:20:44,554] [    INFO][0m - loss: 0.27094452, learning_rate: 2.2857142857142858e-05, global_step: 900, interval_runtime: 7.9674, interval_samples_per_second: 2.008, interval_steps_per_second: 1.255, epoch: 4.7619[0m
[32m[2022-09-19 17:20:54,858] [    INFO][0m - loss: 0.19898133, learning_rate: 2.277777777777778e-05, global_step: 910, interval_runtime: 10.3039, interval_samples_per_second: 1.553, interval_steps_per_second: 0.971, epoch: 4.8148[0m
[32m[2022-09-19 17:21:06,045] [    INFO][0m - loss: 0.37086978, learning_rate: 2.26984126984127e-05, global_step: 920, interval_runtime: 8.696, interval_samples_per_second: 1.84, interval_steps_per_second: 1.15, epoch: 4.8677[0m
[32m[2022-09-19 17:21:17,046] [    INFO][0m - loss: 0.25036788, learning_rate: 2.261904761904762e-05, global_step: 930, interval_runtime: 13.4919, interval_samples_per_second: 1.186, interval_steps_per_second: 0.741, epoch: 4.9206[0m
[32m[2022-09-19 17:21:28,062] [    INFO][0m - loss: 0.22707877, learning_rate: 2.253968253968254e-05, global_step: 940, interval_runtime: 11.0156, interval_samples_per_second: 1.452, interval_steps_per_second: 0.908, epoch: 4.9735[0m
[32m[2022-09-19 17:21:31,879] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 17:21:31,879] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-19 17:21:31,879] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 17:21:31,879] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 17:21:31,879] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-19 17:22:05,909] [    INFO][0m - eval_loss: 3.3395676612854004, eval_accuracy: 0.4493809176984705, eval_runtime: 34.0296, eval_samples_per_second: 40.347, eval_steps_per_second: 2.527, epoch: 5.0[0m
[32m[2022-09-19 17:22:09,576] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-945[0m
[32m[2022-09-19 17:22:09,576] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 17:22:12,335] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-945/tokenizer_config.json[0m
[32m[2022-09-19 17:22:12,335] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-945/special_tokens_map.json[0m
[32m[2022-09-19 17:22:24,701] [    INFO][0m - loss: 0.21915035, learning_rate: 2.2460317460317462e-05, global_step: 950, interval_runtime: 56.6388, interval_samples_per_second: 0.282, interval_steps_per_second: 0.177, epoch: 5.0265[0m
[32m[2022-09-19 17:22:37,578] [    INFO][0m - loss: 0.17781143, learning_rate: 2.238095238095238e-05, global_step: 960, interval_runtime: 10.7133, interval_samples_per_second: 1.493, interval_steps_per_second: 0.933, epoch: 5.0794[0m
[32m[2022-09-19 17:22:46,963] [    INFO][0m - loss: 0.09728311, learning_rate: 2.2301587301587302e-05, global_step: 970, interval_runtime: 11.5486, interval_samples_per_second: 1.385, interval_steps_per_second: 0.866, epoch: 5.1323[0m
[32m[2022-09-19 17:22:57,975] [    INFO][0m - loss: 0.23350432, learning_rate: 2.222222222222222e-05, global_step: 980, interval_runtime: 11.0119, interval_samples_per_second: 1.453, interval_steps_per_second: 0.908, epoch: 5.1852[0m
[32m[2022-09-19 17:23:10,416] [    INFO][0m - loss: 0.16698011, learning_rate: 2.2142857142857145e-05, global_step: 990, interval_runtime: 12.4417, interval_samples_per_second: 1.286, interval_steps_per_second: 0.804, epoch: 5.2381[0m
[32m[2022-09-19 17:23:19,224] [    INFO][0m - loss: 0.09133163, learning_rate: 2.2063492063492063e-05, global_step: 1000, interval_runtime: 8.8078, interval_samples_per_second: 1.817, interval_steps_per_second: 1.135, epoch: 5.291[0m
[32m[2022-09-19 17:23:29,432] [    INFO][0m - loss: 0.0805913, learning_rate: 2.1984126984126984e-05, global_step: 1010, interval_runtime: 10.2075, interval_samples_per_second: 1.567, interval_steps_per_second: 0.98, epoch: 5.3439[0m
[32m[2022-09-19 17:23:38,201] [    INFO][0m - loss: 0.10666173, learning_rate: 2.1904761904761903e-05, global_step: 1020, interval_runtime: 8.7693, interval_samples_per_second: 1.825, interval_steps_per_second: 1.14, epoch: 5.3968[0m
[32m[2022-09-19 17:23:49,228] [    INFO][0m - loss: 0.15984925, learning_rate: 2.1825396825396827e-05, global_step: 1030, interval_runtime: 11.0268, interval_samples_per_second: 1.451, interval_steps_per_second: 0.907, epoch: 5.4497[0m
[32m[2022-09-19 17:23:57,779] [    INFO][0m - loss: 0.2079777, learning_rate: 2.1746031746031746e-05, global_step: 1040, interval_runtime: 8.5509, interval_samples_per_second: 1.871, interval_steps_per_second: 1.169, epoch: 5.5026[0m
[32m[2022-09-19 17:24:08,238] [    INFO][0m - loss: 0.16698579, learning_rate: 2.1666666666666667e-05, global_step: 1050, interval_runtime: 10.4596, interval_samples_per_second: 1.53, interval_steps_per_second: 0.956, epoch: 5.5556[0m
[32m[2022-09-19 17:24:19,341] [    INFO][0m - loss: 0.12905886, learning_rate: 2.1587301587301585e-05, global_step: 1060, interval_runtime: 11.1031, interval_samples_per_second: 1.441, interval_steps_per_second: 0.901, epoch: 5.6085[0m
[32m[2022-09-19 17:24:28,193] [    INFO][0m - loss: 0.11903005, learning_rate: 2.150793650793651e-05, global_step: 1070, interval_runtime: 7.9975, interval_samples_per_second: 2.001, interval_steps_per_second: 1.25, epoch: 5.6614[0m
[32m[2022-09-19 17:24:39,200] [    INFO][0m - loss: 0.20477834, learning_rate: 2.1428571428571428e-05, global_step: 1080, interval_runtime: 11.861, interval_samples_per_second: 1.349, interval_steps_per_second: 0.843, epoch: 5.7143[0m
[32m[2022-09-19 17:24:49,525] [    INFO][0m - loss: 0.15150408, learning_rate: 2.134920634920635e-05, global_step: 1090, interval_runtime: 10.3244, interval_samples_per_second: 1.55, interval_steps_per_second: 0.969, epoch: 5.7672[0m
[32m[2022-09-19 17:24:59,878] [    INFO][0m - loss: 0.29855714, learning_rate: 2.1269841269841268e-05, global_step: 1100, interval_runtime: 10.3533, interval_samples_per_second: 1.545, interval_steps_per_second: 0.966, epoch: 5.8201[0m
[32m[2022-09-19 17:25:13,812] [    INFO][0m - loss: 0.15679389, learning_rate: 2.1190476190476193e-05, global_step: 1110, interval_runtime: 13.9346, interval_samples_per_second: 1.148, interval_steps_per_second: 0.718, epoch: 5.873[0m
[32m[2022-09-19 17:25:28,471] [    INFO][0m - loss: 0.05996286, learning_rate: 2.111111111111111e-05, global_step: 1120, interval_runtime: 14.6591, interval_samples_per_second: 1.091, interval_steps_per_second: 0.682, epoch: 5.9259[0m
[32m[2022-09-19 17:25:38,759] [    INFO][0m - loss: 0.12662297, learning_rate: 2.1031746031746032e-05, global_step: 1130, interval_runtime: 10.2869, interval_samples_per_second: 1.555, interval_steps_per_second: 0.972, epoch: 5.9788[0m
[32m[2022-09-19 17:25:41,811] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 17:25:41,812] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-19 17:25:41,812] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 17:25:41,812] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 17:25:41,812] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-19 17:26:16,476] [    INFO][0m - eval_loss: 3.9171433448791504, eval_accuracy: 0.4435542607428988, eval_runtime: 34.6638, eval_samples_per_second: 39.609, eval_steps_per_second: 2.481, epoch: 6.0[0m
[32m[2022-09-19 17:26:16,501] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-1134[0m
[32m[2022-09-19 17:26:16,502] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 17:26:19,204] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-1134/tokenizer_config.json[0m
[32m[2022-09-19 17:26:19,205] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-1134/special_tokens_map.json[0m
[32m[2022-09-19 17:26:35,053] [    INFO][0m - loss: 0.14166114, learning_rate: 2.095238095238095e-05, global_step: 1140, interval_runtime: 56.2946, interval_samples_per_second: 0.284, interval_steps_per_second: 0.178, epoch: 6.0317[0m
[32m[2022-09-19 17:26:52,492] [    INFO][0m - loss: 0.04454147, learning_rate: 2.0873015873015875e-05, global_step: 1150, interval_runtime: 17.4393, interval_samples_per_second: 0.917, interval_steps_per_second: 0.573, epoch: 6.0847[0m
[32m[2022-09-19 17:27:10,041] [    INFO][0m - loss: 0.05068704, learning_rate: 2.0793650793650793e-05, global_step: 1160, interval_runtime: 17.5484, interval_samples_per_second: 0.912, interval_steps_per_second: 0.57, epoch: 6.1376[0m
[32m[2022-09-19 17:27:27,527] [    INFO][0m - loss: 0.25217485, learning_rate: 2.0714285714285715e-05, global_step: 1170, interval_runtime: 17.486, interval_samples_per_second: 0.915, interval_steps_per_second: 0.572, epoch: 6.1905[0m
[32m[2022-09-19 17:27:45,120] [    INFO][0m - loss: 0.12485328, learning_rate: 2.0634920634920633e-05, global_step: 1180, interval_runtime: 17.5933, interval_samples_per_second: 0.909, interval_steps_per_second: 0.568, epoch: 6.2434[0m
[32m[2022-09-19 17:28:02,108] [    INFO][0m - loss: 0.05224326, learning_rate: 2.0555555555555558e-05, global_step: 1190, interval_runtime: 16.9884, interval_samples_per_second: 0.942, interval_steps_per_second: 0.589, epoch: 6.2963[0m
[32m[2022-09-19 17:28:18,392] [    INFO][0m - loss: 0.05522507, learning_rate: 2.0476190476190476e-05, global_step: 1200, interval_runtime: 16.284, interval_samples_per_second: 0.983, interval_steps_per_second: 0.614, epoch: 6.3492[0m
[32m[2022-09-19 17:28:34,704] [    INFO][0m - loss: 0.06657659, learning_rate: 2.0396825396825398e-05, global_step: 1210, interval_runtime: 16.3115, interval_samples_per_second: 0.981, interval_steps_per_second: 0.613, epoch: 6.4021[0m
[32m[2022-09-19 17:28:47,333] [    INFO][0m - loss: 0.05018841, learning_rate: 2.0317460317460316e-05, global_step: 1220, interval_runtime: 12.6293, interval_samples_per_second: 1.267, interval_steps_per_second: 0.792, epoch: 6.455[0m
[32m[2022-09-19 17:28:58,115] [    INFO][0m - loss: 0.11857126, learning_rate: 2.023809523809524e-05, global_step: 1230, interval_runtime: 10.7818, interval_samples_per_second: 1.484, interval_steps_per_second: 0.927, epoch: 6.5079[0m
[32m[2022-09-19 17:29:15,633] [    INFO][0m - loss: 0.08254519, learning_rate: 2.015873015873016e-05, global_step: 1240, interval_runtime: 17.5174, interval_samples_per_second: 0.913, interval_steps_per_second: 0.571, epoch: 6.5608[0m
[32m[2022-09-19 17:29:33,141] [    INFO][0m - loss: 0.13866186, learning_rate: 2.007936507936508e-05, global_step: 1250, interval_runtime: 17.5081, interval_samples_per_second: 0.914, interval_steps_per_second: 0.571, epoch: 6.6138[0m
[32m[2022-09-19 17:29:50,578] [    INFO][0m - loss: 0.02193595, learning_rate: 1.9999999999999998e-05, global_step: 1260, interval_runtime: 17.4373, interval_samples_per_second: 0.918, interval_steps_per_second: 0.573, epoch: 6.6667[0m
[32m[2022-09-19 17:30:08,176] [    INFO][0m - loss: 0.03451762, learning_rate: 1.9920634920634923e-05, global_step: 1270, interval_runtime: 17.5983, interval_samples_per_second: 0.909, interval_steps_per_second: 0.568, epoch: 6.7196[0m
[32m[2022-09-19 17:30:25,703] [    INFO][0m - loss: 0.11829433, learning_rate: 1.984126984126984e-05, global_step: 1280, interval_runtime: 17.5269, interval_samples_per_second: 0.913, interval_steps_per_second: 0.571, epoch: 6.7725[0m
[32m[2022-09-19 17:30:43,235] [    INFO][0m - loss: 0.10181103, learning_rate: 1.9761904761904763e-05, global_step: 1290, interval_runtime: 17.5317, interval_samples_per_second: 0.913, interval_steps_per_second: 0.57, epoch: 6.8254[0m
[32m[2022-09-19 17:31:00,341] [    INFO][0m - loss: 0.10709562, learning_rate: 1.968253968253968e-05, global_step: 1300, interval_runtime: 17.1059, interval_samples_per_second: 0.935, interval_steps_per_second: 0.585, epoch: 6.8783[0m
[32m[2022-09-19 17:31:16,621] [    INFO][0m - loss: 0.10953385, learning_rate: 1.9603174603174606e-05, global_step: 1310, interval_runtime: 16.2806, interval_samples_per_second: 0.983, interval_steps_per_second: 0.614, epoch: 6.9312[0m
[32m[2022-09-19 17:31:32,847] [    INFO][0m - loss: 0.05008399, learning_rate: 1.9523809523809524e-05, global_step: 1320, interval_runtime: 16.2253, interval_samples_per_second: 0.986, interval_steps_per_second: 0.616, epoch: 6.9841[0m
[32m[2022-09-19 17:31:37,641] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 17:31:37,641] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-19 17:31:37,641] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 17:31:37,641] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 17:31:37,641] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-19 17:32:25,638] [    INFO][0m - eval_loss: 4.279397964477539, eval_accuracy: 0.437727603787327, eval_runtime: 47.9962, eval_samples_per_second: 28.606, eval_steps_per_second: 1.792, epoch: 7.0[0m
[32m[2022-09-19 17:32:25,657] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-1323[0m
[32m[2022-09-19 17:32:25,658] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 17:32:28,261] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-1323/tokenizer_config.json[0m
[32m[2022-09-19 17:32:28,262] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-1323/special_tokens_map.json[0m
[32m[2022-09-19 17:32:45,663] [    INFO][0m - loss: 0.03174299, learning_rate: 1.9444444444444445e-05, global_step: 1330, interval_runtime: 72.8157, interval_samples_per_second: 0.22, interval_steps_per_second: 0.137, epoch: 7.037[0m
[32m[2022-09-19 17:33:03,149] [    INFO][0m - loss: 0.04430717, learning_rate: 1.9365079365079363e-05, global_step: 1340, interval_runtime: 17.4863, interval_samples_per_second: 0.915, interval_steps_per_second: 0.572, epoch: 7.0899[0m
[32m[2022-09-19 17:33:20,617] [    INFO][0m - loss: 0.03559627, learning_rate: 1.928571428571429e-05, global_step: 1350, interval_runtime: 17.4678, interval_samples_per_second: 0.916, interval_steps_per_second: 0.572, epoch: 7.1429[0m
[32m[2022-09-19 17:33:38,145] [    INFO][0m - loss: 0.12439464, learning_rate: 1.9206349206349206e-05, global_step: 1360, interval_runtime: 17.5281, interval_samples_per_second: 0.913, interval_steps_per_second: 0.571, epoch: 7.1958[0m
[32m[2022-09-19 17:33:54,674] [    INFO][0m - loss: 0.0144857, learning_rate: 1.9126984126984128e-05, global_step: 1370, interval_runtime: 16.5286, interval_samples_per_second: 0.968, interval_steps_per_second: 0.605, epoch: 7.2487[0m
[32m[2022-09-19 17:34:11,001] [    INFO][0m - loss: 0.03332613, learning_rate: 1.9047619047619046e-05, global_step: 1380, interval_runtime: 16.3278, interval_samples_per_second: 0.98, interval_steps_per_second: 0.612, epoch: 7.3016[0m
[32m[2022-09-19 17:34:27,369] [    INFO][0m - loss: 0.24923234, learning_rate: 1.896825396825397e-05, global_step: 1390, interval_runtime: 16.3681, interval_samples_per_second: 0.978, interval_steps_per_second: 0.611, epoch: 7.3545[0m
[32m[2022-09-19 17:34:36,278] [    INFO][0m - loss: 0.02800065, learning_rate: 1.888888888888889e-05, global_step: 1400, interval_runtime: 8.9086, interval_samples_per_second: 1.796, interval_steps_per_second: 1.123, epoch: 7.4074[0m
[32m[2022-09-19 17:34:52,161] [    INFO][0m - loss: 0.01803634, learning_rate: 1.880952380952381e-05, global_step: 1410, interval_runtime: 15.883, interval_samples_per_second: 1.007, interval_steps_per_second: 0.63, epoch: 7.4603[0m
[32m[2022-09-19 17:35:09,721] [    INFO][0m - loss: 0.05923684, learning_rate: 1.873015873015873e-05, global_step: 1420, interval_runtime: 17.5601, interval_samples_per_second: 0.911, interval_steps_per_second: 0.569, epoch: 7.5132[0m
[32m[2022-09-19 17:35:27,208] [    INFO][0m - loss: 0.09984643, learning_rate: 1.8650793650793654e-05, global_step: 1430, interval_runtime: 17.4871, interval_samples_per_second: 0.915, interval_steps_per_second: 0.572, epoch: 7.5661[0m
[32m[2022-09-19 17:35:44,689] [    INFO][0m - loss: 0.0635087, learning_rate: 1.8571428571428572e-05, global_step: 1440, interval_runtime: 17.481, interval_samples_per_second: 0.915, interval_steps_per_second: 0.572, epoch: 7.619[0m
[32m[2022-09-19 17:36:02,213] [    INFO][0m - loss: 0.04082157, learning_rate: 1.8492063492063493e-05, global_step: 1450, interval_runtime: 17.5241, interval_samples_per_second: 0.913, interval_steps_per_second: 0.571, epoch: 7.672[0m
[32m[2022-09-19 17:36:19,732] [    INFO][0m - loss: 0.02695854, learning_rate: 1.841269841269841e-05, global_step: 1460, interval_runtime: 17.5185, interval_samples_per_second: 0.913, interval_steps_per_second: 0.571, epoch: 7.7249[0m
[32m[2022-09-19 17:36:37,266] [    INFO][0m - loss: 0.07823538, learning_rate: 1.8333333333333336e-05, global_step: 1470, interval_runtime: 17.534, interval_samples_per_second: 0.913, interval_steps_per_second: 0.57, epoch: 7.7778[0m
[32m[2022-09-19 17:36:53,831] [    INFO][0m - loss: 0.07568699, learning_rate: 1.8253968253968254e-05, global_step: 1480, interval_runtime: 16.5649, interval_samples_per_second: 0.966, interval_steps_per_second: 0.604, epoch: 7.8307[0m
[32m[2022-09-19 17:37:10,236] [    INFO][0m - loss: 0.03289191, learning_rate: 1.8174603174603176e-05, global_step: 1490, interval_runtime: 16.4054, interval_samples_per_second: 0.975, interval_steps_per_second: 0.61, epoch: 7.8836[0m
[32m[2022-09-19 17:37:26,621] [    INFO][0m - loss: 0.13157263, learning_rate: 1.8095238095238094e-05, global_step: 1500, interval_runtime: 16.3847, interval_samples_per_second: 0.977, interval_steps_per_second: 0.61, epoch: 7.9365[0m
[32m[2022-09-19 17:37:35,208] [    INFO][0m - loss: 0.12327348, learning_rate: 1.801587301587302e-05, global_step: 1510, interval_runtime: 8.5876, interval_samples_per_second: 1.863, interval_steps_per_second: 1.164, epoch: 7.9894[0m
[32m[2022-09-19 17:37:36,729] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 17:37:36,729] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-19 17:37:36,729] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 17:37:36,729] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 17:37:36,729] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-19 17:38:34,714] [    INFO][0m - eval_loss: 4.600556373596191, eval_accuracy: 0.4319009468317553, eval_runtime: 57.9845, eval_samples_per_second: 23.679, eval_steps_per_second: 1.483, epoch: 8.0[0m
[32m[2022-09-19 17:38:34,737] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-1512[0m
[32m[2022-09-19 17:38:34,738] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 17:38:37,261] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-1512/tokenizer_config.json[0m
[32m[2022-09-19 17:38:37,261] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-1512/special_tokens_map.json[0m
[32m[2022-09-19 17:38:56,305] [    INFO][0m - loss: 0.01533574, learning_rate: 1.7936507936507937e-05, global_step: 1520, interval_runtime: 81.0958, interval_samples_per_second: 0.197, interval_steps_per_second: 0.123, epoch: 8.0423[0m
[32m[2022-09-19 17:39:13,804] [    INFO][0m - loss: 0.04351688, learning_rate: 1.785714285714286e-05, global_step: 1530, interval_runtime: 17.4997, interval_samples_per_second: 0.914, interval_steps_per_second: 0.571, epoch: 8.0952[0m
[32m[2022-09-19 17:39:30,916] [    INFO][0m - loss: 0.0360703, learning_rate: 1.7777777777777777e-05, global_step: 1540, interval_runtime: 17.1114, interval_samples_per_second: 0.935, interval_steps_per_second: 0.584, epoch: 8.1481[0m
[32m[2022-09-19 17:39:47,140] [    INFO][0m - loss: 0.03509331, learning_rate: 1.76984126984127e-05, global_step: 1550, interval_runtime: 16.2247, interval_samples_per_second: 0.986, interval_steps_per_second: 0.616, epoch: 8.2011[0m
[32m[2022-09-19 17:40:03,364] [    INFO][0m - loss: 0.00288204, learning_rate: 1.761904761904762e-05, global_step: 1560, interval_runtime: 16.224, interval_samples_per_second: 0.986, interval_steps_per_second: 0.616, epoch: 8.254[0m
[32m[2022-09-19 17:40:16,367] [    INFO][0m - loss: 0.00735624, learning_rate: 1.753968253968254e-05, global_step: 1570, interval_runtime: 13.0026, interval_samples_per_second: 1.231, interval_steps_per_second: 0.769, epoch: 8.3069[0m
[32m[2022-09-19 17:40:26,968] [    INFO][0m - loss: 0.06612219, learning_rate: 1.746031746031746e-05, global_step: 1580, interval_runtime: 10.6009, interval_samples_per_second: 1.509, interval_steps_per_second: 0.943, epoch: 8.3598[0m
[32m[2022-09-19 17:40:39,905] [    INFO][0m - loss: 0.01798212, learning_rate: 1.7380952380952384e-05, global_step: 1590, interval_runtime: 12.9374, interval_samples_per_second: 1.237, interval_steps_per_second: 0.773, epoch: 8.4127[0m
[32m[2022-09-19 17:40:57,347] [    INFO][0m - loss: 0.01406789, learning_rate: 1.7301587301587302e-05, global_step: 1600, interval_runtime: 17.4415, interval_samples_per_second: 0.917, interval_steps_per_second: 0.573, epoch: 8.4656[0m
[32m[2022-09-19 17:41:14,831] [    INFO][0m - loss: 0.13062687, learning_rate: 1.7222222222222224e-05, global_step: 1610, interval_runtime: 17.4843, interval_samples_per_second: 0.915, interval_steps_per_second: 0.572, epoch: 8.5185[0m
[32m[2022-09-19 17:41:32,282] [    INFO][0m - loss: 0.08193507, learning_rate: 1.7142857142857142e-05, global_step: 1620, interval_runtime: 17.451, interval_samples_per_second: 0.917, interval_steps_per_second: 0.573, epoch: 8.5714[0m
[32m[2022-09-19 17:41:49,814] [    INFO][0m - loss: 0.01798157, learning_rate: 1.7063492063492067e-05, global_step: 1630, interval_runtime: 17.5323, interval_samples_per_second: 0.913, interval_steps_per_second: 0.57, epoch: 8.6243[0m
[32m[2022-09-19 17:42:07,332] [    INFO][0m - loss: 0.05704829, learning_rate: 1.6984126984126985e-05, global_step: 1640, interval_runtime: 17.5175, interval_samples_per_second: 0.913, interval_steps_per_second: 0.571, epoch: 8.6772[0m
[32m[2022-09-19 17:42:24,887] [    INFO][0m - loss: 0.06854565, learning_rate: 1.6904761904761906e-05, global_step: 1650, interval_runtime: 17.5553, interval_samples_per_second: 0.911, interval_steps_per_second: 0.57, epoch: 8.7302[0m
[32m[2022-09-19 17:42:41,490] [    INFO][0m - loss: 0.00455142, learning_rate: 1.6825396825396824e-05, global_step: 1660, interval_runtime: 16.6033, interval_samples_per_second: 0.964, interval_steps_per_second: 0.602, epoch: 8.7831[0m
[32m[2022-09-19 17:42:57,815] [    INFO][0m - loss: 0.0181369, learning_rate: 1.674603174603175e-05, global_step: 1670, interval_runtime: 16.3251, interval_samples_per_second: 0.98, interval_steps_per_second: 0.613, epoch: 8.836[0m
[32m[2022-09-19 17:43:14,099] [    INFO][0m - loss: 0.06055471, learning_rate: 1.6666666666666667e-05, global_step: 1680, interval_runtime: 16.2838, interval_samples_per_second: 0.983, interval_steps_per_second: 0.614, epoch: 8.8889[0m
[32m[2022-09-19 17:43:23,380] [    INFO][0m - loss: 0.03707451, learning_rate: 1.658730158730159e-05, global_step: 1690, interval_runtime: 9.2804, interval_samples_per_second: 1.724, interval_steps_per_second: 1.078, epoch: 8.9418[0m
[32m[2022-09-19 17:43:38,373] [    INFO][0m - loss: 0.01462054, learning_rate: 1.6507936507936507e-05, global_step: 1700, interval_runtime: 14.9935, interval_samples_per_second: 1.067, interval_steps_per_second: 0.667, epoch: 8.9947[0m
[32m[2022-09-19 17:43:40,080] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 17:43:40,080] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-19 17:43:40,080] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 17:43:40,080] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 17:43:40,080] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-19 17:44:38,337] [    INFO][0m - eval_loss: 4.8859968185424805, eval_accuracy: 0.44646758922068464, eval_runtime: 58.2559, eval_samples_per_second: 23.568, eval_steps_per_second: 1.476, epoch: 9.0[0m
[32m[2022-09-19 17:44:38,360] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-1701[0m
[32m[2022-09-19 17:44:38,360] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 17:44:40,898] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-1701/tokenizer_config.json[0m
[32m[2022-09-19 17:44:40,899] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-1701/special_tokens_map.json[0m
[32m[2022-09-19 17:45:01,702] [    INFO][0m - loss: 0.06166251, learning_rate: 1.6428571428571432e-05, global_step: 1710, interval_runtime: 83.3292, interval_samples_per_second: 0.192, interval_steps_per_second: 0.12, epoch: 9.0476[0m
[32m[2022-09-19 17:45:18,810] [    INFO][0m - loss: 0.02200292, learning_rate: 1.634920634920635e-05, global_step: 1720, interval_runtime: 17.1072, interval_samples_per_second: 0.935, interval_steps_per_second: 0.585, epoch: 9.1005[0m
[32m[2022-09-19 17:45:35,125] [    INFO][0m - loss: 0.02944254, learning_rate: 1.626984126984127e-05, global_step: 1730, interval_runtime: 16.3148, interval_samples_per_second: 0.981, interval_steps_per_second: 0.613, epoch: 9.1534[0m
[32m[2022-09-19 17:45:51,403] [    INFO][0m - loss: 0.00596895, learning_rate: 1.619047619047619e-05, global_step: 1740, interval_runtime: 16.2783, interval_samples_per_second: 0.983, interval_steps_per_second: 0.614, epoch: 9.2063[0m
[32m[2022-09-19 17:46:04,321] [    INFO][0m - loss: 0.01939911, learning_rate: 1.6111111111111115e-05, global_step: 1750, interval_runtime: 12.918, interval_samples_per_second: 1.239, interval_steps_per_second: 0.774, epoch: 9.2593[0m
[32m[2022-09-19 17:46:15,465] [    INFO][0m - loss: 0.03378727, learning_rate: 1.6031746031746033e-05, global_step: 1760, interval_runtime: 11.1446, interval_samples_per_second: 1.436, interval_steps_per_second: 0.897, epoch: 9.3122[0m
[32m[2022-09-19 17:46:33,054] [    INFO][0m - loss: 0.04622495, learning_rate: 1.5952380952380954e-05, global_step: 1770, interval_runtime: 17.5884, interval_samples_per_second: 0.91, interval_steps_per_second: 0.569, epoch: 9.3651[0m
[32m[2022-09-19 17:46:50,514] [    INFO][0m - loss: 0.0068844, learning_rate: 1.5873015873015872e-05, global_step: 1780, interval_runtime: 17.46, interval_samples_per_second: 0.916, interval_steps_per_second: 0.573, epoch: 9.418[0m
[32m[2022-09-19 17:47:08,090] [    INFO][0m - loss: 0.03111146, learning_rate: 1.5793650793650797e-05, global_step: 1790, interval_runtime: 17.5763, interval_samples_per_second: 0.91, interval_steps_per_second: 0.569, epoch: 9.4709[0m
[32m[2022-09-19 17:47:25,676] [    INFO][0m - loss: 0.04420629, learning_rate: 1.5714285714285715e-05, global_step: 1800, interval_runtime: 17.5863, interval_samples_per_second: 0.91, interval_steps_per_second: 0.569, epoch: 9.5238[0m
[32m[2022-09-19 17:47:43,143] [    INFO][0m - loss: 0.00527124, learning_rate: 1.5634920634920637e-05, global_step: 1810, interval_runtime: 17.4664, interval_samples_per_second: 0.916, interval_steps_per_second: 0.573, epoch: 9.5767[0m
[32m[2022-09-19 17:48:00,605] [    INFO][0m - loss: 0.02335566, learning_rate: 1.5555555555555555e-05, global_step: 1820, interval_runtime: 17.4621, interval_samples_per_second: 0.916, interval_steps_per_second: 0.573, epoch: 9.6296[0m
[32m[2022-09-19 17:48:17,745] [    INFO][0m - loss: 0.00369893, learning_rate: 1.547619047619048e-05, global_step: 1830, interval_runtime: 17.1401, interval_samples_per_second: 0.933, interval_steps_per_second: 0.583, epoch: 9.6825[0m
[32m[2022-09-19 17:48:34,104] [    INFO][0m - loss: 0.0251686, learning_rate: 1.5396825396825398e-05, global_step: 1840, interval_runtime: 16.3591, interval_samples_per_second: 0.978, interval_steps_per_second: 0.611, epoch: 9.7354[0m
[32m[2022-09-19 17:48:50,464] [    INFO][0m - loss: 0.08379075, learning_rate: 1.531746031746032e-05, global_step: 1850, interval_runtime: 16.3601, interval_samples_per_second: 0.978, interval_steps_per_second: 0.611, epoch: 9.7884[0m
[32m[2022-09-19 17:49:03,214] [    INFO][0m - loss: 0.00276592, learning_rate: 1.5238095238095238e-05, global_step: 1860, interval_runtime: 12.7499, interval_samples_per_second: 1.255, interval_steps_per_second: 0.784, epoch: 9.8413[0m
[32m[2022-09-19 17:49:14,639] [    INFO][0m - loss: 0.01749297, learning_rate: 1.515873015873016e-05, global_step: 1870, interval_runtime: 11.4252, interval_samples_per_second: 1.4, interval_steps_per_second: 0.875, epoch: 9.8942[0m
[32m[2022-09-19 17:49:32,182] [    INFO][0m - loss: 0.03400839, learning_rate: 1.507936507936508e-05, global_step: 1880, interval_runtime: 17.5423, interval_samples_per_second: 0.912, interval_steps_per_second: 0.57, epoch: 9.9471[0m
[32m[2022-09-19 17:49:49,609] [    INFO][0m - loss: 0.06116266, learning_rate: 1.5e-05, global_step: 1890, interval_runtime: 17.4277, interval_samples_per_second: 0.918, interval_steps_per_second: 0.574, epoch: 10.0[0m
[32m[2022-09-19 17:49:49,610] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 17:49:49,610] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-19 17:49:49,610] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 17:49:49,610] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 17:49:49,610] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-19 17:50:48,072] [    INFO][0m - eval_loss: 4.829209327697754, eval_accuracy: 0.4450109249817917, eval_runtime: 58.4612, eval_samples_per_second: 23.486, eval_steps_per_second: 1.471, epoch: 10.0[0m
[32m[2022-09-19 17:50:48,095] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-1890[0m
[32m[2022-09-19 17:50:48,096] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 17:50:50,713] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-1890/tokenizer_config.json[0m
[32m[2022-09-19 17:50:50,714] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-1890/special_tokens_map.json[0m
[32m[2022-09-19 17:51:12,670] [    INFO][0m - loss: 0.03339901, learning_rate: 1.492063492063492e-05, global_step: 1900, interval_runtime: 83.0605, interval_samples_per_second: 0.193, interval_steps_per_second: 0.12, epoch: 10.0529[0m
[32m[2022-09-19 17:51:28,941] [    INFO][0m - loss: 0.02325617, learning_rate: 1.4841269841269842e-05, global_step: 1910, interval_runtime: 16.2713, interval_samples_per_second: 0.983, interval_steps_per_second: 0.615, epoch: 10.1058[0m
[32m[2022-09-19 17:51:44,138] [    INFO][0m - loss: 0.00199699, learning_rate: 1.4761904761904761e-05, global_step: 1920, interval_runtime: 15.1967, interval_samples_per_second: 1.053, interval_steps_per_second: 0.658, epoch: 10.1587[0m
[32m[2022-09-19 17:51:52,429] [    INFO][0m - loss: 0.03069834, learning_rate: 1.4682539682539683e-05, global_step: 1930, interval_runtime: 8.291, interval_samples_per_second: 1.93, interval_steps_per_second: 1.206, epoch: 10.2116[0m
[32m[2022-09-19 17:52:10,094] [    INFO][0m - loss: 0.05114904, learning_rate: 1.4603174603174603e-05, global_step: 1940, interval_runtime: 17.6653, interval_samples_per_second: 0.906, interval_steps_per_second: 0.566, epoch: 10.2646[0m
[32m[2022-09-19 17:52:27,737] [    INFO][0m - loss: 0.0384446, learning_rate: 1.4523809523809524e-05, global_step: 1950, interval_runtime: 17.6428, interval_samples_per_second: 0.907, interval_steps_per_second: 0.567, epoch: 10.3175[0m
[32m[2022-09-19 17:52:45,244] [    INFO][0m - loss: 0.05172238, learning_rate: 1.4444444444444444e-05, global_step: 1960, interval_runtime: 17.507, interval_samples_per_second: 0.914, interval_steps_per_second: 0.571, epoch: 10.3704[0m
[32m[2022-09-19 17:53:02,800] [    INFO][0m - loss: 0.05466821, learning_rate: 1.4365079365079366e-05, global_step: 1970, interval_runtime: 17.5565, interval_samples_per_second: 0.911, interval_steps_per_second: 0.57, epoch: 10.4233[0m
[32m[2022-09-19 17:53:20,305] [    INFO][0m - loss: 0.00082595, learning_rate: 1.4285714285714285e-05, global_step: 1980, interval_runtime: 17.5046, interval_samples_per_second: 0.914, interval_steps_per_second: 0.571, epoch: 10.4762[0m
[32m[2022-09-19 17:53:37,853] [    INFO][0m - loss: 0.0028698, learning_rate: 1.4206349206349207e-05, global_step: 1990, interval_runtime: 17.5483, interval_samples_per_second: 0.912, interval_steps_per_second: 0.57, epoch: 10.5291[0m
[32m[2022-09-19 17:53:55,425] [    INFO][0m - loss: 0.03013335, learning_rate: 1.4126984126984127e-05, global_step: 2000, interval_runtime: 17.5716, interval_samples_per_second: 0.911, interval_steps_per_second: 0.569, epoch: 10.582[0m
[32m[2022-09-19 17:54:11,653] [    INFO][0m - loss: 0.00535789, learning_rate: 1.4047619047619048e-05, global_step: 2010, interval_runtime: 16.2282, interval_samples_per_second: 0.986, interval_steps_per_second: 0.616, epoch: 10.6349[0m
[32m[2022-09-19 17:54:27,893] [    INFO][0m - loss: 0.00169112, learning_rate: 1.3968253968253968e-05, global_step: 2020, interval_runtime: 16.2401, interval_samples_per_second: 0.985, interval_steps_per_second: 0.616, epoch: 10.6878[0m
[32m[2022-09-19 17:54:43,142] [    INFO][0m - loss: 0.04846321, learning_rate: 1.388888888888889e-05, global_step: 2030, interval_runtime: 15.2483, interval_samples_per_second: 1.049, interval_steps_per_second: 0.656, epoch: 10.7407[0m
[32m[2022-09-19 17:54:51,433] [    INFO][0m - loss: 0.02971657, learning_rate: 1.380952380952381e-05, global_step: 2040, interval_runtime: 8.2912, interval_samples_per_second: 1.93, interval_steps_per_second: 1.206, epoch: 10.7937[0m
[32m[2022-09-19 17:55:08,947] [    INFO][0m - loss: 0.03322967, learning_rate: 1.3730158730158731e-05, global_step: 2050, interval_runtime: 17.514, interval_samples_per_second: 0.914, interval_steps_per_second: 0.571, epoch: 10.8466[0m
[32m[2022-09-19 17:55:26,404] [    INFO][0m - loss: 0.01733013, learning_rate: 1.365079365079365e-05, global_step: 2060, interval_runtime: 17.4564, interval_samples_per_second: 0.917, interval_steps_per_second: 0.573, epoch: 10.8995[0m
[32m[2022-09-19 17:55:43,914] [    INFO][0m - loss: 0.00626982, learning_rate: 1.3571428571428572e-05, global_step: 2070, interval_runtime: 17.5104, interval_samples_per_second: 0.914, interval_steps_per_second: 0.571, epoch: 10.9524[0m
[32m[2022-09-19 17:55:59,502] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 17:55:59,502] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-19 17:55:59,502] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 17:55:59,502] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 17:55:59,502] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-19 17:56:56,990] [    INFO][0m - eval_loss: 4.871413230895996, eval_accuracy: 0.45010924981791695, eval_runtime: 57.4878, eval_samples_per_second: 23.883, eval_steps_per_second: 1.496, epoch: 11.0[0m
[32m[2022-09-19 17:56:57,014] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-2079[0m
[32m[2022-09-19 17:56:57,014] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 17:56:59,577] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-2079/tokenizer_config.json[0m
[32m[2022-09-19 17:56:59,578] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-2079/special_tokens_map.json[0m
[32m[2022-09-19 17:57:06,306] [    INFO][0m - loss: 0.00189181, learning_rate: 1.3492063492063492e-05, global_step: 2080, interval_runtime: 82.3925, interval_samples_per_second: 0.194, interval_steps_per_second: 0.121, epoch: 11.0053[0m
[32m[2022-09-19 17:57:22,622] [    INFO][0m - loss: 0.00082904, learning_rate: 1.3412698412698413e-05, global_step: 2090, interval_runtime: 16.3152, interval_samples_per_second: 0.981, interval_steps_per_second: 0.613, epoch: 11.0582[0m
[32m[2022-09-19 17:57:32,808] [    INFO][0m - loss: 0.00048056, learning_rate: 1.3333333333333333e-05, global_step: 2100, interval_runtime: 10.1862, interval_samples_per_second: 1.571, interval_steps_per_second: 0.982, epoch: 11.1111[0m
[32m[2022-09-19 17:57:46,975] [    INFO][0m - loss: 0.0002725, learning_rate: 1.3253968253968255e-05, global_step: 2110, interval_runtime: 14.1666, interval_samples_per_second: 1.129, interval_steps_per_second: 0.706, epoch: 11.164[0m
[32m[2022-09-19 17:58:04,521] [    INFO][0m - loss: 0.00110278, learning_rate: 1.3174603174603175e-05, global_step: 2120, interval_runtime: 17.547, interval_samples_per_second: 0.912, interval_steps_per_second: 0.57, epoch: 11.2169[0m
[32m[2022-09-19 17:58:22,029] [    INFO][0m - loss: 0.00278937, learning_rate: 1.3095238095238096e-05, global_step: 2130, interval_runtime: 17.5076, interval_samples_per_second: 0.914, interval_steps_per_second: 0.571, epoch: 11.2698[0m
[32m[2022-09-19 17:58:39,560] [    INFO][0m - loss: 0.00053824, learning_rate: 1.3015873015873016e-05, global_step: 2140, interval_runtime: 17.5313, interval_samples_per_second: 0.913, interval_steps_per_second: 0.57, epoch: 11.3228[0m
[32m[2022-09-19 17:58:57,100] [    INFO][0m - loss: 0.00106823, learning_rate: 1.2936507936507937e-05, global_step: 2150, interval_runtime: 17.5396, interval_samples_per_second: 0.912, interval_steps_per_second: 0.57, epoch: 11.3757[0m
[32m[2022-09-19 17:59:14,625] [    INFO][0m - loss: 0.01412826, learning_rate: 1.2857142857142857e-05, global_step: 2160, interval_runtime: 17.5247, interval_samples_per_second: 0.913, interval_steps_per_second: 0.571, epoch: 11.4286[0m
[32m[2022-09-19 17:59:32,140] [    INFO][0m - loss: 0.02442872, learning_rate: 1.2777777777777779e-05, global_step: 2170, interval_runtime: 17.5152, interval_samples_per_second: 0.913, interval_steps_per_second: 0.571, epoch: 11.4815[0m
[32m[2022-09-19 17:59:48,895] [    INFO][0m - loss: 0.0032875, learning_rate: 1.2698412698412699e-05, global_step: 2180, interval_runtime: 16.7551, interval_samples_per_second: 0.955, interval_steps_per_second: 0.597, epoch: 11.5344[0m
[32m[2022-09-19 18:00:05,217] [    INFO][0m - loss: 0.02637329, learning_rate: 1.261904761904762e-05, global_step: 2190, interval_runtime: 16.3219, interval_samples_per_second: 0.98, interval_steps_per_second: 0.613, epoch: 11.5873[0m
[32m[2022-09-19 18:00:21,494] [    INFO][0m - loss: 0.00157729, learning_rate: 1.253968253968254e-05, global_step: 2200, interval_runtime: 16.277, interval_samples_per_second: 0.983, interval_steps_per_second: 0.614, epoch: 11.6402[0m
[32m[2022-09-19 18:00:31,722] [    INFO][0m - loss: 0.0049483, learning_rate: 1.2460317460317461e-05, global_step: 2210, interval_runtime: 10.2278, interval_samples_per_second: 1.564, interval_steps_per_second: 0.978, epoch: 11.6931[0m
[32m[2022-09-19 18:00:45,834] [    INFO][0m - loss: 0.04901833, learning_rate: 1.2380952380952381e-05, global_step: 2220, interval_runtime: 14.1118, interval_samples_per_second: 1.134, interval_steps_per_second: 0.709, epoch: 11.746[0m
[32m[2022-09-19 18:01:04,139] [    INFO][0m - loss: 0.00367473, learning_rate: 1.2301587301587303e-05, global_step: 2230, interval_runtime: 18.3052, interval_samples_per_second: 0.874, interval_steps_per_second: 0.546, epoch: 11.7989[0m
[32m[2022-09-19 18:01:21,574] [    INFO][0m - loss: 0.06598889, learning_rate: 1.2222222222222222e-05, global_step: 2240, interval_runtime: 17.4352, interval_samples_per_second: 0.918, interval_steps_per_second: 0.574, epoch: 11.8519[0m
[32m[2022-09-19 18:01:39,083] [    INFO][0m - loss: 0.00988798, learning_rate: 1.2142857142857144e-05, global_step: 2250, interval_runtime: 17.5089, interval_samples_per_second: 0.914, interval_steps_per_second: 0.571, epoch: 11.9048[0m
[32m[2022-09-19 18:01:56,636] [    INFO][0m - loss: 0.0320372, learning_rate: 1.2063492063492064e-05, global_step: 2260, interval_runtime: 17.5535, interval_samples_per_second: 0.912, interval_steps_per_second: 0.57, epoch: 11.9577[0m
[32m[2022-09-19 18:02:10,459] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 18:02:10,460] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-19 18:02:10,460] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 18:02:10,460] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 18:02:10,460] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-19 18:03:06,324] [    INFO][0m - eval_loss: 4.888344764709473, eval_accuracy: 0.44646758922068464, eval_runtime: 55.8638, eval_samples_per_second: 24.578, eval_steps_per_second: 1.539, epoch: 12.0[0m
[32m[2022-09-19 18:03:06,349] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-2268[0m
[32m[2022-09-19 18:03:06,349] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 18:03:09,001] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-2268/tokenizer_config.json[0m
[32m[2022-09-19 18:03:09,001] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-2268/special_tokens_map.json[0m
[32m[2022-09-19 18:03:15,611] [    INFO][0m - loss: 0.07997867, learning_rate: 1.1984126984126985e-05, global_step: 2270, interval_runtime: 78.9747, interval_samples_per_second: 0.203, interval_steps_per_second: 0.127, epoch: 12.0106[0m
[32m[2022-09-19 18:03:27,393] [    INFO][0m - loss: 0.00585289, learning_rate: 1.1904761904761905e-05, global_step: 2280, interval_runtime: 11.7818, interval_samples_per_second: 1.358, interval_steps_per_second: 0.849, epoch: 12.0635[0m
[32m[2022-09-19 18:03:44,881] [    INFO][0m - loss: 0.01458502, learning_rate: 1.1825396825396827e-05, global_step: 2290, interval_runtime: 17.4874, interval_samples_per_second: 0.915, interval_steps_per_second: 0.572, epoch: 12.1164[0m
[32m[2022-09-19 18:04:02,422] [    INFO][0m - loss: 0.00050811, learning_rate: 1.1746031746031746e-05, global_step: 2300, interval_runtime: 17.5412, interval_samples_per_second: 0.912, interval_steps_per_second: 0.57, epoch: 12.1693[0m
[32m[2022-09-19 18:04:19,937] [    INFO][0m - loss: 0.00067799, learning_rate: 1.1666666666666668e-05, global_step: 2310, interval_runtime: 17.5156, interval_samples_per_second: 0.913, interval_steps_per_second: 0.571, epoch: 12.2222[0m
[32m[2022-09-19 18:04:37,452] [    INFO][0m - loss: 0.0003943, learning_rate: 1.1587301587301588e-05, global_step: 2320, interval_runtime: 17.5147, interval_samples_per_second: 0.914, interval_steps_per_second: 0.571, epoch: 12.2751[0m
[32m[2022-09-19 18:04:54,904] [    INFO][0m - loss: 0.03736078, learning_rate: 1.150793650793651e-05, global_step: 2330, interval_runtime: 17.4526, interval_samples_per_second: 0.917, interval_steps_per_second: 0.573, epoch: 12.328[0m
[32m[2022-09-19 18:05:12,439] [    INFO][0m - loss: 0.00026937, learning_rate: 1.1428571428571429e-05, global_step: 2340, interval_runtime: 17.5345, interval_samples_per_second: 0.912, interval_steps_per_second: 0.57, epoch: 12.381[0m
[32m[2022-09-19 18:05:29,441] [    INFO][0m - loss: 0.01575422, learning_rate: 1.134920634920635e-05, global_step: 2350, interval_runtime: 17.0022, interval_samples_per_second: 0.941, interval_steps_per_second: 0.588, epoch: 12.4339[0m
[32m[2022-09-19 18:05:45,723] [    INFO][0m - loss: 0.01080156, learning_rate: 1.126984126984127e-05, global_step: 2360, interval_runtime: 16.2818, interval_samples_per_second: 0.983, interval_steps_per_second: 0.614, epoch: 12.4868[0m
[32m[2022-09-19 18:06:02,013] [    INFO][0m - loss: 0.00779527, learning_rate: 1.119047619047619e-05, global_step: 2370, interval_runtime: 16.2899, interval_samples_per_second: 0.982, interval_steps_per_second: 0.614, epoch: 12.5397[0m
[32m[2022-09-19 18:06:14,515] [    INFO][0m - loss: 0.00023626, learning_rate: 1.111111111111111e-05, global_step: 2380, interval_runtime: 12.5016, interval_samples_per_second: 1.28, interval_steps_per_second: 0.8, epoch: 12.5926[0m
[32m[2022-09-19 18:06:23,326] [    INFO][0m - loss: 0.00020488, learning_rate: 1.1031746031746031e-05, global_step: 2390, interval_runtime: 8.8119, interval_samples_per_second: 1.816, interval_steps_per_second: 1.135, epoch: 12.6455[0m
[32m[2022-09-19 18:06:40,857] [    INFO][0m - loss: 0.00098357, learning_rate: 1.0952380952380951e-05, global_step: 2400, interval_runtime: 17.5305, interval_samples_per_second: 0.913, interval_steps_per_second: 0.57, epoch: 12.6984[0m
[32m[2022-09-19 18:06:58,451] [    INFO][0m - loss: 0.03321503, learning_rate: 1.0873015873015873e-05, global_step: 2410, interval_runtime: 17.5942, interval_samples_per_second: 0.909, interval_steps_per_second: 0.568, epoch: 12.7513[0m
[32m[2022-09-19 18:07:16,037] [    INFO][0m - loss: 0.00029961, learning_rate: 1.0793650793650793e-05, global_step: 2420, interval_runtime: 17.5857, interval_samples_per_second: 0.91, interval_steps_per_second: 0.569, epoch: 12.8042[0m
[32m[2022-09-19 18:07:33,526] [    INFO][0m - loss: 0.02063178, learning_rate: 1.0714285714285714e-05, global_step: 2430, interval_runtime: 17.4893, interval_samples_per_second: 0.915, interval_steps_per_second: 0.572, epoch: 12.8571[0m
[32m[2022-09-19 18:07:51,046] [    INFO][0m - loss: 0.01857721, learning_rate: 1.0634920634920634e-05, global_step: 2440, interval_runtime: 17.5201, interval_samples_per_second: 0.913, interval_steps_per_second: 0.571, epoch: 12.9101[0m
[32m[2022-09-19 18:08:08,508] [    INFO][0m - loss: 0.04472844, learning_rate: 1.0555555555555555e-05, global_step: 2450, interval_runtime: 17.4615, interval_samples_per_second: 0.916, interval_steps_per_second: 0.573, epoch: 12.963[0m
[32m[2022-09-19 18:08:20,637] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 18:08:20,638] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-19 18:08:20,638] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 18:08:20,638] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 18:08:20,638] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-19 18:09:12,138] [    INFO][0m - eval_loss: 4.985950469970703, eval_accuracy: 0.45447924253459576, eval_runtime: 51.4998, eval_samples_per_second: 26.66, eval_steps_per_second: 1.67, epoch: 13.0[0m
[32m[2022-09-19 18:09:12,162] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-2457[0m
[32m[2022-09-19 18:09:12,162] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 18:09:15,114] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-2457/tokenizer_config.json[0m
[32m[2022-09-19 18:09:15,114] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-2457/special_tokens_map.json[0m
[32m[2022-09-19 18:09:25,441] [    INFO][0m - loss: 0.00118884, learning_rate: 1.0476190476190475e-05, global_step: 2460, interval_runtime: 76.933, interval_samples_per_second: 0.208, interval_steps_per_second: 0.13, epoch: 13.0159[0m
[32m[2022-09-19 18:09:43,705] [    INFO][0m - loss: 0.00011837, learning_rate: 1.0396825396825397e-05, global_step: 2470, interval_runtime: 18.2641, interval_samples_per_second: 0.876, interval_steps_per_second: 0.548, epoch: 13.0688[0m
[32m[2022-09-19 18:10:01,269] [    INFO][0m - loss: 0.00026611, learning_rate: 1.0317460317460317e-05, global_step: 2480, interval_runtime: 17.5636, interval_samples_per_second: 0.911, interval_steps_per_second: 0.569, epoch: 13.1217[0m
[32m[2022-09-19 18:10:18,789] [    INFO][0m - loss: 0.0040293, learning_rate: 1.0238095238095238e-05, global_step: 2490, interval_runtime: 17.5203, interval_samples_per_second: 0.913, interval_steps_per_second: 0.571, epoch: 13.1746[0m
[32m[2022-09-19 18:10:36,323] [    INFO][0m - loss: 0.01166306, learning_rate: 1.0158730158730158e-05, global_step: 2500, interval_runtime: 17.5342, interval_samples_per_second: 0.913, interval_steps_per_second: 0.57, epoch: 13.2275[0m
[32m[2022-09-19 18:10:53,882] [    INFO][0m - loss: 0.00023154, learning_rate: 1.007936507936508e-05, global_step: 2510, interval_runtime: 17.5587, interval_samples_per_second: 0.911, interval_steps_per_second: 0.57, epoch: 13.2804[0m
[32m[2022-09-19 18:11:11,419] [    INFO][0m - loss: 0.00555864, learning_rate: 9.999999999999999e-06, global_step: 2520, interval_runtime: 17.5373, interval_samples_per_second: 0.912, interval_steps_per_second: 0.57, epoch: 13.3333[0m
[32m[2022-09-19 18:11:28,017] [    INFO][0m - loss: 0.00226254, learning_rate: 9.92063492063492e-06, global_step: 2530, interval_runtime: 16.5978, interval_samples_per_second: 0.964, interval_steps_per_second: 0.602, epoch: 13.3862[0m
[32m[2022-09-19 18:11:44,309] [    INFO][0m - loss: 0.00010102, learning_rate: 9.84126984126984e-06, global_step: 2540, interval_runtime: 16.292, interval_samples_per_second: 0.982, interval_steps_per_second: 0.614, epoch: 13.4392[0m
[32m[2022-09-19 18:12:00,630] [    INFO][0m - loss: 0.00369901, learning_rate: 9.761904761904762e-06, global_step: 2550, interval_runtime: 16.3215, interval_samples_per_second: 0.98, interval_steps_per_second: 0.613, epoch: 13.4921[0m
[32m[2022-09-19 18:12:09,792] [    INFO][0m - loss: 0.00732355, learning_rate: 9.682539682539682e-06, global_step: 2560, interval_runtime: 9.1614, interval_samples_per_second: 1.746, interval_steps_per_second: 1.092, epoch: 13.545[0m
[32m[2022-09-19 18:12:25,367] [    INFO][0m - loss: 0.03682399, learning_rate: 9.603174603174603e-06, global_step: 2570, interval_runtime: 15.5753, interval_samples_per_second: 1.027, interval_steps_per_second: 0.642, epoch: 13.5979[0m
[32m[2022-09-19 18:12:42,861] [    INFO][0m - loss: 0.0001452, learning_rate: 9.523809523809523e-06, global_step: 2580, interval_runtime: 17.494, interval_samples_per_second: 0.915, interval_steps_per_second: 0.572, epoch: 13.6508[0m
[32m[2022-09-19 18:13:00,410] [    INFO][0m - loss: 0.00027822, learning_rate: 9.444444444444445e-06, global_step: 2590, interval_runtime: 17.5493, interval_samples_per_second: 0.912, interval_steps_per_second: 0.57, epoch: 13.7037[0m
[32m[2022-09-19 18:13:17,906] [    INFO][0m - loss: 0.08466999, learning_rate: 9.365079365079364e-06, global_step: 2600, interval_runtime: 17.4949, interval_samples_per_second: 0.915, interval_steps_per_second: 0.572, epoch: 13.7566[0m
[32m[2022-09-19 18:13:35,391] [    INFO][0m - loss: 0.00030195, learning_rate: 9.285714285714286e-06, global_step: 2610, interval_runtime: 17.4854, interval_samples_per_second: 0.915, interval_steps_per_second: 0.572, epoch: 13.8095[0m
[32m[2022-09-19 18:13:52,904] [    INFO][0m - loss: 0.02897771, learning_rate: 9.206349206349206e-06, global_step: 2620, interval_runtime: 17.5134, interval_samples_per_second: 0.914, interval_steps_per_second: 0.571, epoch: 13.8624[0m
[32m[2022-09-19 18:14:10,465] [    INFO][0m - loss: 0.0001404, learning_rate: 9.126984126984127e-06, global_step: 2630, interval_runtime: 17.5613, interval_samples_per_second: 0.911, interval_steps_per_second: 0.569, epoch: 13.9153[0m
[32m[2022-09-19 18:14:26,951] [    INFO][0m - loss: 0.01991294, learning_rate: 9.047619047619047e-06, global_step: 2640, interval_runtime: 16.4858, interval_samples_per_second: 0.971, interval_steps_per_second: 0.607, epoch: 13.9683[0m
[32m[2022-09-19 18:14:36,560] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 18:14:36,560] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-19 18:14:36,560] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 18:14:36,560] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 18:14:36,560] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-19 18:15:23,383] [    INFO][0m - eval_loss: 5.0156989097595215, eval_accuracy: 0.4479242534595776, eval_runtime: 46.8224, eval_samples_per_second: 29.324, eval_steps_per_second: 1.837, epoch: 14.0[0m
[32m[2022-09-19 18:15:23,406] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-2646[0m
[32m[2022-09-19 18:15:23,407] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 18:15:25,954] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-2646/tokenizer_config.json[0m
[32m[2022-09-19 18:15:25,954] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-2646/special_tokens_map.json[0m
[32m[2022-09-19 18:15:37,986] [    INFO][0m - loss: 0.03036112, learning_rate: 8.968253968253968e-06, global_step: 2650, interval_runtime: 71.0352, interval_samples_per_second: 0.225, interval_steps_per_second: 0.141, epoch: 14.0212[0m
[32m[2022-09-19 18:15:55,462] [    INFO][0m - loss: 7.297e-05, learning_rate: 8.888888888888888e-06, global_step: 2660, interval_runtime: 17.4751, interval_samples_per_second: 0.916, interval_steps_per_second: 0.572, epoch: 14.0741[0m
[32m[2022-09-19 18:16:12,992] [    INFO][0m - loss: 0.01481767, learning_rate: 8.80952380952381e-06, global_step: 2670, interval_runtime: 17.5304, interval_samples_per_second: 0.913, interval_steps_per_second: 0.57, epoch: 14.127[0m
[32m[2022-09-19 18:16:30,509] [    INFO][0m - loss: 0.00012696, learning_rate: 8.73015873015873e-06, global_step: 2680, interval_runtime: 17.5173, interval_samples_per_second: 0.913, interval_steps_per_second: 0.571, epoch: 14.1799[0m
[32m[2022-09-19 18:16:48,158] [    INFO][0m - loss: 0.02429516, learning_rate: 8.650793650793651e-06, global_step: 2690, interval_runtime: 17.6487, interval_samples_per_second: 0.907, interval_steps_per_second: 0.567, epoch: 14.2328[0m
[32m[2022-09-19 18:17:05,261] [    INFO][0m - loss: 0.00023612, learning_rate: 8.571428571428571e-06, global_step: 2700, interval_runtime: 17.1027, interval_samples_per_second: 0.936, interval_steps_per_second: 0.585, epoch: 14.2857[0m
[32m[2022-09-19 18:17:21,530] [    INFO][0m - loss: 0.00210443, learning_rate: 8.492063492063492e-06, global_step: 2710, interval_runtime: 16.2692, interval_samples_per_second: 0.983, interval_steps_per_second: 0.615, epoch: 14.3386[0m
[32m[2022-09-19 18:17:37,892] [    INFO][0m - loss: 8.383e-05, learning_rate: 8.412698412698412e-06, global_step: 2720, interval_runtime: 16.3624, interval_samples_per_second: 0.978, interval_steps_per_second: 0.611, epoch: 14.3915[0m
[32m[2022-09-19 18:17:50,605] [    INFO][0m - loss: 0.02337434, learning_rate: 8.333333333333334e-06, global_step: 2730, interval_runtime: 12.7127, interval_samples_per_second: 1.259, interval_steps_per_second: 0.787, epoch: 14.4444[0m
[32m[2022-09-19 18:18:02,378] [    INFO][0m - loss: 0.00193058, learning_rate: 8.253968253968254e-06, global_step: 2740, interval_runtime: 11.7725, interval_samples_per_second: 1.359, interval_steps_per_second: 0.849, epoch: 14.4974[0m
[32m[2022-09-19 18:18:19,897] [    INFO][0m - loss: 0.02602476, learning_rate: 8.174603174603175e-06, global_step: 2750, interval_runtime: 17.5195, interval_samples_per_second: 0.913, interval_steps_per_second: 0.571, epoch: 14.5503[0m
[32m[2022-09-19 18:18:37,396] [    INFO][0m - loss: 0.00010665, learning_rate: 8.095238095238095e-06, global_step: 2760, interval_runtime: 17.4985, interval_samples_per_second: 0.914, interval_steps_per_second: 0.571, epoch: 14.6032[0m
[32m[2022-09-19 18:18:54,992] [    INFO][0m - loss: 8.317e-05, learning_rate: 8.015873015873016e-06, global_step: 2770, interval_runtime: 17.5966, interval_samples_per_second: 0.909, interval_steps_per_second: 0.568, epoch: 14.6561[0m
[32m[2022-09-19 18:19:12,536] [    INFO][0m - loss: 0.00010349, learning_rate: 7.936507936507936e-06, global_step: 2780, interval_runtime: 17.5436, interval_samples_per_second: 0.912, interval_steps_per_second: 0.57, epoch: 14.709[0m
[32m[2022-09-19 18:19:30,029] [    INFO][0m - loss: 0.00010875, learning_rate: 7.857142857142858e-06, global_step: 2790, interval_runtime: 17.4931, interval_samples_per_second: 0.915, interval_steps_per_second: 0.572, epoch: 14.7619[0m
[32m[2022-09-19 18:19:47,514] [    INFO][0m - loss: 0.005341, learning_rate: 7.777777777777777e-06, global_step: 2800, interval_runtime: 17.4855, interval_samples_per_second: 0.915, interval_steps_per_second: 0.572, epoch: 14.8148[0m
[32m[2022-09-19 18:20:04,531] [    INFO][0m - loss: 5.031e-05, learning_rate: 7.698412698412699e-06, global_step: 2810, interval_runtime: 17.0165, interval_samples_per_second: 0.94, interval_steps_per_second: 0.588, epoch: 14.8677[0m
[32m[2022-09-19 18:20:20,898] [    INFO][0m - loss: 0.05219699, learning_rate: 7.619047619047619e-06, global_step: 2820, interval_runtime: 16.3668, interval_samples_per_second: 0.978, interval_steps_per_second: 0.611, epoch: 14.9206[0m
[32m[2022-09-19 18:20:37,186] [    INFO][0m - loss: 0.00011252, learning_rate: 7.53968253968254e-06, global_step: 2830, interval_runtime: 16.2882, interval_samples_per_second: 0.982, interval_steps_per_second: 0.614, epoch: 14.9735[0m
[32m[2022-09-19 18:20:45,302] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 18:20:45,303] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-19 18:20:45,303] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 18:20:45,303] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 18:20:45,303] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-19 18:21:33,175] [    INFO][0m - eval_loss: 5.020431995391846, eval_accuracy: 0.44646758922068464, eval_runtime: 47.8721, eval_samples_per_second: 28.681, eval_steps_per_second: 1.796, epoch: 15.0[0m
[32m[2022-09-19 18:21:33,200] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-2835[0m
[32m[2022-09-19 18:21:33,201] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 18:21:35,767] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-2835/tokenizer_config.json[0m
[32m[2022-09-19 18:21:35,767] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-2835/special_tokens_map.json[0m
[32m[2022-09-19 18:21:49,504] [    INFO][0m - loss: 0.01761851, learning_rate: 7.46031746031746e-06, global_step: 2840, interval_runtime: 72.3179, interval_samples_per_second: 0.221, interval_steps_per_second: 0.138, epoch: 15.0265[0m
[32m[2022-09-19 18:22:07,032] [    INFO][0m - loss: 0.008994, learning_rate: 7.380952380952381e-06, global_step: 2850, interval_runtime: 17.5283, interval_samples_per_second: 0.913, interval_steps_per_second: 0.571, epoch: 15.0794[0m
[32m[2022-09-19 18:22:24,567] [    INFO][0m - loss: 0.006846, learning_rate: 7.301587301587301e-06, global_step: 2860, interval_runtime: 17.5349, interval_samples_per_second: 0.912, interval_steps_per_second: 0.57, epoch: 15.1323[0m
[32m[2022-09-19 18:22:42,215] [    INFO][0m - loss: 0.0092256, learning_rate: 7.222222222222222e-06, global_step: 2870, interval_runtime: 17.6475, interval_samples_per_second: 0.907, interval_steps_per_second: 0.567, epoch: 15.1852[0m
[32m[2022-09-19 18:22:58,612] [    INFO][0m - loss: 0.01301142, learning_rate: 7.142857142857143e-06, global_step: 2880, interval_runtime: 16.3973, interval_samples_per_second: 0.976, interval_steps_per_second: 0.61, epoch: 15.2381[0m
[32m[2022-09-19 18:23:14,954] [    INFO][0m - loss: 9.291e-05, learning_rate: 7.063492063492063e-06, global_step: 2890, interval_runtime: 16.3417, interval_samples_per_second: 0.979, interval_steps_per_second: 0.612, epoch: 15.291[0m
[32m[2022-09-19 18:23:31,447] [    INFO][0m - loss: 8.637e-05, learning_rate: 6.984126984126984e-06, global_step: 2900, interval_runtime: 16.4936, interval_samples_per_second: 0.97, interval_steps_per_second: 0.606, epoch: 15.3439[0m
[32m[2022-09-19 18:23:39,513] [    INFO][0m - loss: 5.312e-05, learning_rate: 6.904761904761905e-06, global_step: 2910, interval_runtime: 8.0659, interval_samples_per_second: 1.984, interval_steps_per_second: 1.24, epoch: 15.3968[0m
[32m[2022-09-19 18:23:48,634] [    INFO][0m - loss: 8.179e-05, learning_rate: 6.825396825396825e-06, global_step: 2920, interval_runtime: 9.1201, interval_samples_per_second: 1.754, interval_steps_per_second: 1.096, epoch: 15.4497[0m
[32m[2022-09-19 18:24:04,915] [    INFO][0m - loss: 0.02006189, learning_rate: 6.746031746031746e-06, global_step: 2930, interval_runtime: 16.2815, interval_samples_per_second: 0.983, interval_steps_per_second: 0.614, epoch: 15.5026[0m
[32m[2022-09-19 18:24:21,445] [    INFO][0m - loss: 6.863e-05, learning_rate: 6.666666666666667e-06, global_step: 2940, interval_runtime: 16.5304, interval_samples_per_second: 0.968, interval_steps_per_second: 0.605, epoch: 15.5556[0m
[32m[2022-09-19 18:24:37,779] [    INFO][0m - loss: 0.00635748, learning_rate: 6.587301587301587e-06, global_step: 2950, interval_runtime: 16.3336, interval_samples_per_second: 0.98, interval_steps_per_second: 0.612, epoch: 15.6085[0m
[32m[2022-09-19 18:24:54,064] [    INFO][0m - loss: 0.01864048, learning_rate: 6.507936507936508e-06, global_step: 2960, interval_runtime: 16.2853, interval_samples_per_second: 0.982, interval_steps_per_second: 0.614, epoch: 15.6614[0m
[32m[2022-09-19 18:25:09,978] [    INFO][0m - loss: 0.00749407, learning_rate: 6.428571428571429e-06, global_step: 2970, interval_runtime: 15.9142, interval_samples_per_second: 1.005, interval_steps_per_second: 0.628, epoch: 15.7143[0m
[32m[2022-09-19 18:25:26,192] [    INFO][0m - loss: 0.00010726, learning_rate: 6.349206349206349e-06, global_step: 2980, interval_runtime: 16.2138, interval_samples_per_second: 0.987, interval_steps_per_second: 0.617, epoch: 15.7672[0m
[32m[2022-09-19 18:25:42,215] [    INFO][0m - loss: 7.105e-05, learning_rate: 6.26984126984127e-06, global_step: 2990, interval_runtime: 16.023, interval_samples_per_second: 0.999, interval_steps_per_second: 0.624, epoch: 15.8201[0m
[32m[2022-09-19 18:25:58,259] [    INFO][0m - loss: 0.02352859, learning_rate: 6.190476190476191e-06, global_step: 3000, interval_runtime: 16.0437, interval_samples_per_second: 0.997, interval_steps_per_second: 0.623, epoch: 15.873[0m
[32m[2022-09-19 18:26:14,231] [    INFO][0m - loss: 0.00010378, learning_rate: 6.111111111111111e-06, global_step: 3010, interval_runtime: 15.9723, interval_samples_per_second: 1.002, interval_steps_per_second: 0.626, epoch: 15.9259[0m
[32m[2022-09-19 18:26:30,076] [    INFO][0m - loss: 9.002e-05, learning_rate: 6.031746031746032e-06, global_step: 3020, interval_runtime: 15.8447, interval_samples_per_second: 1.01, interval_steps_per_second: 0.631, epoch: 15.9788[0m
[32m[2022-09-19 18:26:36,321] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 18:26:36,321] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-19 18:26:36,322] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 18:26:36,322] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 18:26:36,322] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-19 18:27:29,389] [    INFO][0m - eval_loss: 5.012552261352539, eval_accuracy: 0.4471959213401311, eval_runtime: 53.0666, eval_samples_per_second: 25.873, eval_steps_per_second: 1.621, epoch: 16.0[0m
[32m[2022-09-19 18:27:29,405] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-3024[0m
[32m[2022-09-19 18:27:29,406] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 18:27:31,848] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-3024/tokenizer_config.json[0m
[32m[2022-09-19 18:27:31,848] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-3024/special_tokens_map.json[0m
[32m[2022-09-19 18:27:46,183] [    INFO][0m - loss: 0.01402878, learning_rate: 5.9523809523809525e-06, global_step: 3030, interval_runtime: 76.1074, interval_samples_per_second: 0.21, interval_steps_per_second: 0.131, epoch: 16.0317[0m
[32m[2022-09-19 18:28:02,049] [    INFO][0m - loss: 0.00213591, learning_rate: 5.873015873015873e-06, global_step: 3040, interval_runtime: 15.866, interval_samples_per_second: 1.008, interval_steps_per_second: 0.63, epoch: 16.0847[0m
[32m[2022-09-19 18:28:17,869] [    INFO][0m - loss: 4.551e-05, learning_rate: 5.793650793650794e-06, global_step: 3050, interval_runtime: 15.82, interval_samples_per_second: 1.011, interval_steps_per_second: 0.632, epoch: 16.1376[0m
[32m[2022-09-19 18:28:33,669] [    INFO][0m - loss: 5.898e-05, learning_rate: 5.7142857142857145e-06, global_step: 3060, interval_runtime: 15.7999, interval_samples_per_second: 1.013, interval_steps_per_second: 0.633, epoch: 16.1905[0m
[32m[2022-09-19 18:28:49,492] [    INFO][0m - loss: 4.407e-05, learning_rate: 5.634920634920635e-06, global_step: 3070, interval_runtime: 15.823, interval_samples_per_second: 1.011, interval_steps_per_second: 0.632, epoch: 16.2434[0m
[32m[2022-09-19 18:29:05,234] [    INFO][0m - loss: 0.00528582, learning_rate: 5.555555555555555e-06, global_step: 3080, interval_runtime: 15.7415, interval_samples_per_second: 1.016, interval_steps_per_second: 0.635, epoch: 16.2963[0m
[32m[2022-09-19 18:29:20,665] [    INFO][0m - loss: 3.528e-05, learning_rate: 5.476190476190476e-06, global_step: 3090, interval_runtime: 15.4316, interval_samples_per_second: 1.037, interval_steps_per_second: 0.648, epoch: 16.3492[0m
[32m[2022-09-19 18:29:36,266] [    INFO][0m - loss: 6.722e-05, learning_rate: 5.396825396825396e-06, global_step: 3100, interval_runtime: 15.6009, interval_samples_per_second: 1.026, interval_steps_per_second: 0.641, epoch: 16.4021[0m
[32m[2022-09-19 18:29:51,872] [    INFO][0m - loss: 0.02616506, learning_rate: 5.317460317460317e-06, global_step: 3110, interval_runtime: 15.6056, interval_samples_per_second: 1.025, interval_steps_per_second: 0.641, epoch: 16.455[0m
[32m[2022-09-19 18:30:07,464] [    INFO][0m - loss: 0.00011825, learning_rate: 5.238095238095238e-06, global_step: 3120, interval_runtime: 15.5927, interval_samples_per_second: 1.026, interval_steps_per_second: 0.641, epoch: 16.5079[0m
[32m[2022-09-19 18:30:22,748] [    INFO][0m - loss: 0.01826908, learning_rate: 5.158730158730158e-06, global_step: 3130, interval_runtime: 15.2832, interval_samples_per_second: 1.047, interval_steps_per_second: 0.654, epoch: 16.5608[0m
[32m[2022-09-19 18:30:38,152] [    INFO][0m - loss: 0.00311327, learning_rate: 5.079365079365079e-06, global_step: 3140, interval_runtime: 15.4043, interval_samples_per_second: 1.039, interval_steps_per_second: 0.649, epoch: 16.6138[0m
[32m[2022-09-19 18:30:53,400] [    INFO][0m - loss: 0.01820861, learning_rate: 4.9999999999999996e-06, global_step: 3150, interval_runtime: 15.2479, interval_samples_per_second: 1.049, interval_steps_per_second: 0.656, epoch: 16.6667[0m
[32m[2022-09-19 18:31:08,656] [    INFO][0m - loss: 6.224e-05, learning_rate: 4.92063492063492e-06, global_step: 3160, interval_runtime: 15.2562, interval_samples_per_second: 1.049, interval_steps_per_second: 0.655, epoch: 16.7196[0m
[32m[2022-09-19 18:31:23,803] [    INFO][0m - loss: 6.115e-05, learning_rate: 4.841269841269841e-06, global_step: 3170, interval_runtime: 15.147, interval_samples_per_second: 1.056, interval_steps_per_second: 0.66, epoch: 16.7725[0m
[32m[2022-09-19 18:31:35,674] [    INFO][0m - loss: 7.156e-05, learning_rate: 4.7619047619047615e-06, global_step: 3180, interval_runtime: 11.8709, interval_samples_per_second: 1.348, interval_steps_per_second: 0.842, epoch: 16.8254[0m
[32m[2022-09-19 18:31:43,651] [    INFO][0m - loss: 0.02072376, learning_rate: 4.682539682539682e-06, global_step: 3190, interval_runtime: 7.9774, interval_samples_per_second: 2.006, interval_steps_per_second: 1.254, epoch: 16.8783[0m
[32m[2022-09-19 18:31:58,837] [    INFO][0m - loss: 0.00443785, learning_rate: 4.603174603174603e-06, global_step: 3200, interval_runtime: 15.1856, interval_samples_per_second: 1.054, interval_steps_per_second: 0.659, epoch: 16.9312[0m
[32m[2022-09-19 18:32:15,217] [    INFO][0m - loss: 0.01498831, learning_rate: 4.5238095238095235e-06, global_step: 3210, interval_runtime: 16.38, interval_samples_per_second: 0.977, interval_steps_per_second: 0.611, epoch: 16.9841[0m
[32m[2022-09-19 18:32:20,027] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 18:32:20,027] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-19 18:32:20,028] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 18:32:20,028] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 18:32:20,028] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-19 18:33:14,839] [    INFO][0m - eval_loss: 5.043119430541992, eval_accuracy: 0.4471959213401311, eval_runtime: 54.8106, eval_samples_per_second: 25.05, eval_steps_per_second: 1.569, epoch: 17.0[0m
[32m[2022-09-19 18:33:14,856] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-3213[0m
[32m[2022-09-19 18:33:14,856] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 18:33:17,319] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-3213/tokenizer_config.json[0m
[32m[2022-09-19 18:33:17,320] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-3213/special_tokens_map.json[0m
[32m[2022-09-19 18:33:33,784] [    INFO][0m - loss: 0.00471351, learning_rate: 4.444444444444444e-06, global_step: 3220, interval_runtime: 78.567, interval_samples_per_second: 0.204, interval_steps_per_second: 0.127, epoch: 17.037[0m
[32m[2022-09-19 18:33:50,302] [    INFO][0m - loss: 5.447e-05, learning_rate: 4.365079365079365e-06, global_step: 3230, interval_runtime: 16.518, interval_samples_per_second: 0.969, interval_steps_per_second: 0.605, epoch: 17.0899[0m
[32m[2022-09-19 18:34:06,870] [    INFO][0m - loss: 0.00228301, learning_rate: 4.2857142857142855e-06, global_step: 3240, interval_runtime: 16.5677, interval_samples_per_second: 0.966, interval_steps_per_second: 0.604, epoch: 17.1429[0m
[32m[2022-09-19 18:34:23,384] [    INFO][0m - loss: 0.02205094, learning_rate: 4.206349206349206e-06, global_step: 3250, interval_runtime: 16.5139, interval_samples_per_second: 0.969, interval_steps_per_second: 0.606, epoch: 17.1958[0m
[32m[2022-09-19 18:34:39,973] [    INFO][0m - loss: 0.01706409, learning_rate: 4.126984126984127e-06, global_step: 3260, interval_runtime: 16.5889, interval_samples_per_second: 0.965, interval_steps_per_second: 0.603, epoch: 17.2487[0m
[32m[2022-09-19 18:34:56,589] [    INFO][0m - loss: 0.00989106, learning_rate: 4.0476190476190474e-06, global_step: 3270, interval_runtime: 16.616, interval_samples_per_second: 0.963, interval_steps_per_second: 0.602, epoch: 17.3016[0m
[32m[2022-09-19 18:35:13,237] [    INFO][0m - loss: 0.0024239, learning_rate: 3.968253968253968e-06, global_step: 3280, interval_runtime: 16.6484, interval_samples_per_second: 0.961, interval_steps_per_second: 0.601, epoch: 17.3545[0m
[32m[2022-09-19 18:35:29,846] [    INFO][0m - loss: 0.01176717, learning_rate: 3.888888888888889e-06, global_step: 3290, interval_runtime: 16.6086, interval_samples_per_second: 0.963, interval_steps_per_second: 0.602, epoch: 17.4074[0m
[32m[2022-09-19 18:35:46,425] [    INFO][0m - loss: 0.00013421, learning_rate: 3.8095238095238094e-06, global_step: 3300, interval_runtime: 16.5793, interval_samples_per_second: 0.965, interval_steps_per_second: 0.603, epoch: 17.4603[0m
[32m[2022-09-19 18:36:03,027] [    INFO][0m - loss: 0.00015739, learning_rate: 3.73015873015873e-06, global_step: 3310, interval_runtime: 16.6019, interval_samples_per_second: 0.964, interval_steps_per_second: 0.602, epoch: 17.5132[0m
[32m[2022-09-19 18:36:19,769] [    INFO][0m - loss: 0.00136398, learning_rate: 3.6507936507936507e-06, global_step: 3320, interval_runtime: 16.7421, interval_samples_per_second: 0.956, interval_steps_per_second: 0.597, epoch: 17.5661[0m
[32m[2022-09-19 18:36:36,381] [    INFO][0m - loss: 4.926e-05, learning_rate: 3.5714285714285714e-06, global_step: 3330, interval_runtime: 16.6124, interval_samples_per_second: 0.963, interval_steps_per_second: 0.602, epoch: 17.619[0m
[32m[2022-09-19 18:36:53,029] [    INFO][0m - loss: 0.01749362, learning_rate: 3.492063492063492e-06, global_step: 3340, interval_runtime: 16.6472, interval_samples_per_second: 0.961, interval_steps_per_second: 0.601, epoch: 17.672[0m
[32m[2022-09-19 18:37:09,724] [    INFO][0m - loss: 0.01438809, learning_rate: 3.4126984126984127e-06, global_step: 3350, interval_runtime: 16.6956, interval_samples_per_second: 0.958, interval_steps_per_second: 0.599, epoch: 17.7249[0m
[32m[2022-09-19 18:37:26,457] [    INFO][0m - loss: 0.00011358, learning_rate: 3.3333333333333333e-06, global_step: 3360, interval_runtime: 16.7327, interval_samples_per_second: 0.956, interval_steps_per_second: 0.598, epoch: 17.7778[0m
[32m[2022-09-19 18:37:43,195] [    INFO][0m - loss: 5.478e-05, learning_rate: 3.253968253968254e-06, global_step: 3370, interval_runtime: 16.7386, interval_samples_per_second: 0.956, interval_steps_per_second: 0.597, epoch: 17.8307[0m
[32m[2022-09-19 18:38:00,021] [    INFO][0m - loss: 6.707e-05, learning_rate: 3.1746031746031746e-06, global_step: 3380, interval_runtime: 16.8256, interval_samples_per_second: 0.951, interval_steps_per_second: 0.594, epoch: 17.8836[0m
[32m[2022-09-19 18:38:16,805] [    INFO][0m - loss: 4.206e-05, learning_rate: 3.0952380952380953e-06, global_step: 3390, interval_runtime: 16.7839, interval_samples_per_second: 0.953, interval_steps_per_second: 0.596, epoch: 17.9365[0m
[32m[2022-09-19 18:38:33,541] [    INFO][0m - loss: 6.219e-05, learning_rate: 3.015873015873016e-06, global_step: 3400, interval_runtime: 16.736, interval_samples_per_second: 0.956, interval_steps_per_second: 0.598, epoch: 17.9894[0m
[32m[2022-09-19 18:38:36,826] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 18:38:36,826] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-19 18:38:36,826] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 18:38:36,826] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 18:38:36,827] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-19 18:39:16,764] [    INFO][0m - eval_loss: 5.057963848114014, eval_accuracy: 0.45302257829570286, eval_runtime: 39.9375, eval_samples_per_second: 34.379, eval_steps_per_second: 2.153, epoch: 18.0[0m
[32m[2022-09-19 18:39:16,777] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-3402[0m
[32m[2022-09-19 18:39:16,777] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 18:39:19,185] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-3402/tokenizer_config.json[0m
[32m[2022-09-19 18:39:19,185] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-3402/special_tokens_map.json[0m
[32m[2022-09-19 18:39:31,323] [    INFO][0m - loss: 6.011e-05, learning_rate: 2.9365079365079366e-06, global_step: 3410, interval_runtime: 57.7817, interval_samples_per_second: 0.277, interval_steps_per_second: 0.173, epoch: 18.0423[0m
[32m[2022-09-19 18:39:39,294] [    INFO][0m - loss: 0.00223873, learning_rate: 2.8571428571428573e-06, global_step: 3420, interval_runtime: 7.9712, interval_samples_per_second: 2.007, interval_steps_per_second: 1.255, epoch: 18.0952[0m
[32m[2022-09-19 18:39:47,253] [    INFO][0m - loss: 3.678e-05, learning_rate: 2.7777777777777775e-06, global_step: 3430, interval_runtime: 7.9595, interval_samples_per_second: 2.01, interval_steps_per_second: 1.256, epoch: 18.1481[0m
[32m[2022-09-19 18:39:55,590] [    INFO][0m - loss: 5.119e-05, learning_rate: 2.698412698412698e-06, global_step: 3440, interval_runtime: 8.3366, interval_samples_per_second: 1.919, interval_steps_per_second: 1.2, epoch: 18.2011[0m
[32m[2022-09-19 18:40:04,555] [    INFO][0m - loss: 6.724e-05, learning_rate: 2.619047619047619e-06, global_step: 3450, interval_runtime: 8.9644, interval_samples_per_second: 1.785, interval_steps_per_second: 1.116, epoch: 18.254[0m
[32m[2022-09-19 18:40:13,484] [    INFO][0m - loss: 3.911e-05, learning_rate: 2.5396825396825395e-06, global_step: 3460, interval_runtime: 8.9295, interval_samples_per_second: 1.792, interval_steps_per_second: 1.12, epoch: 18.3069[0m
[32m[2022-09-19 18:40:21,776] [    INFO][0m - loss: 0.003356, learning_rate: 2.46031746031746e-06, global_step: 3470, interval_runtime: 8.2918, interval_samples_per_second: 1.93, interval_steps_per_second: 1.206, epoch: 18.3598[0m
[32m[2022-09-19 18:40:30,410] [    INFO][0m - loss: 0.01538263, learning_rate: 2.3809523809523808e-06, global_step: 3480, interval_runtime: 8.634, interval_samples_per_second: 1.853, interval_steps_per_second: 1.158, epoch: 18.4127[0m
[32m[2022-09-19 18:40:44,050] [    INFO][0m - loss: 5.954e-05, learning_rate: 2.3015873015873014e-06, global_step: 3490, interval_runtime: 13.6404, interval_samples_per_second: 1.173, interval_steps_per_second: 0.733, epoch: 18.4656[0m
[32m[2022-09-19 18:40:53,019] [    INFO][0m - loss: 4.947e-05, learning_rate: 2.222222222222222e-06, global_step: 3500, interval_runtime: 8.9689, interval_samples_per_second: 1.784, interval_steps_per_second: 1.115, epoch: 18.5185[0m
[32m[2022-09-19 18:41:05,537] [    INFO][0m - loss: 4.469e-05, learning_rate: 2.1428571428571427e-06, global_step: 3510, interval_runtime: 12.5178, interval_samples_per_second: 1.278, interval_steps_per_second: 0.799, epoch: 18.5714[0m
[32m[2022-09-19 18:41:17,806] [    INFO][0m - loss: 0.00621721, learning_rate: 2.0634920634920634e-06, global_step: 3520, interval_runtime: 12.2689, interval_samples_per_second: 1.304, interval_steps_per_second: 0.815, epoch: 18.6243[0m
[32m[2022-09-19 18:41:27,068] [    INFO][0m - loss: 0.00528513, learning_rate: 1.984126984126984e-06, global_step: 3530, interval_runtime: 9.2624, interval_samples_per_second: 1.727, interval_steps_per_second: 1.08, epoch: 18.6772[0m
[32m[2022-09-19 18:41:40,657] [    INFO][0m - loss: 9.764e-05, learning_rate: 1.9047619047619047e-06, global_step: 3540, interval_runtime: 13.5883, interval_samples_per_second: 1.177, interval_steps_per_second: 0.736, epoch: 18.7302[0m
[32m[2022-09-19 18:41:50,781] [    INFO][0m - loss: 0.01235265, learning_rate: 1.8253968253968254e-06, global_step: 3550, interval_runtime: 10.1238, interval_samples_per_second: 1.58, interval_steps_per_second: 0.988, epoch: 18.7831[0m
[32m[2022-09-19 18:42:02,212] [    INFO][0m - loss: 0.02326559, learning_rate: 1.746031746031746e-06, global_step: 3560, interval_runtime: 11.4315, interval_samples_per_second: 1.4, interval_steps_per_second: 0.875, epoch: 18.836[0m
[32m[2022-09-19 18:42:15,829] [    INFO][0m - loss: 5.142e-05, learning_rate: 1.6666666666666667e-06, global_step: 3570, interval_runtime: 13.6169, interval_samples_per_second: 1.175, interval_steps_per_second: 0.734, epoch: 18.8889[0m
[32m[2022-09-19 18:42:24,281] [    INFO][0m - loss: 0.00791683, learning_rate: 1.5873015873015873e-06, global_step: 3580, interval_runtime: 8.4524, interval_samples_per_second: 1.893, interval_steps_per_second: 1.183, epoch: 18.9418[0m
[32m[2022-09-19 18:42:40,812] [    INFO][0m - loss: 0.00824857, learning_rate: 1.507936507936508e-06, global_step: 3590, interval_runtime: 12.9737, interval_samples_per_second: 1.233, interval_steps_per_second: 0.771, epoch: 18.9947[0m
[32m[2022-09-19 18:42:41,586] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 18:42:41,586] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-19 18:42:41,586] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 18:42:41,587] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 18:42:41,587] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-19 18:43:21,340] [    INFO][0m - eval_loss: 5.068736553192139, eval_accuracy: 0.45229424617625635, eval_runtime: 39.7527, eval_samples_per_second: 34.539, eval_steps_per_second: 2.163, epoch: 19.0[0m
[32m[2022-09-19 18:43:21,366] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-3591[0m
[32m[2022-09-19 18:43:21,366] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 18:43:23,999] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-3591/tokenizer_config.json[0m
[32m[2022-09-19 18:43:23,999] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-3591/special_tokens_map.json[0m
[32m[2022-09-19 18:43:41,127] [    INFO][0m - loss: 0.00436522, learning_rate: 1.4285714285714286e-06, global_step: 3600, interval_runtime: 63.8722, interval_samples_per_second: 0.251, interval_steps_per_second: 0.157, epoch: 19.0476[0m
[32m[2022-09-19 18:43:49,802] [    INFO][0m - loss: 0.00290786, learning_rate: 1.349206349206349e-06, global_step: 3610, interval_runtime: 8.6747, interval_samples_per_second: 1.844, interval_steps_per_second: 1.153, epoch: 19.1005[0m
[32m[2022-09-19 18:44:03,354] [    INFO][0m - loss: 0.00824829, learning_rate: 1.2698412698412697e-06, global_step: 3620, interval_runtime: 13.552, interval_samples_per_second: 1.181, interval_steps_per_second: 0.738, epoch: 19.1534[0m
[32m[2022-09-19 18:44:14,769] [    INFO][0m - loss: 5.55e-05, learning_rate: 1.1904761904761904e-06, global_step: 3630, interval_runtime: 11.4153, interval_samples_per_second: 1.402, interval_steps_per_second: 0.876, epoch: 19.2063[0m
[32m[2022-09-19 18:44:24,907] [    INFO][0m - loss: 4.245e-05, learning_rate: 1.111111111111111e-06, global_step: 3640, interval_runtime: 10.1372, interval_samples_per_second: 1.578, interval_steps_per_second: 0.986, epoch: 19.2593[0m
[32m[2022-09-19 18:44:38,435] [    INFO][0m - loss: 0.00859031, learning_rate: 1.0317460317460317e-06, global_step: 3650, interval_runtime: 13.5283, interval_samples_per_second: 1.183, interval_steps_per_second: 0.739, epoch: 19.3122[0m
[32m[2022-09-19 18:44:46,410] [    INFO][0m - loss: 6.01e-05, learning_rate: 9.523809523809523e-07, global_step: 3660, interval_runtime: 7.9746, interval_samples_per_second: 2.006, interval_steps_per_second: 1.254, epoch: 19.3651[0m
[32m[2022-09-19 18:44:59,855] [    INFO][0m - loss: 4.094e-05, learning_rate: 8.73015873015873e-07, global_step: 3670, interval_runtime: 13.4458, interval_samples_per_second: 1.19, interval_steps_per_second: 0.744, epoch: 19.418[0m
[32m[2022-09-19 18:45:08,320] [    INFO][0m - loss: 6.837e-05, learning_rate: 7.936507936507937e-07, global_step: 3680, interval_runtime: 8.465, interval_samples_per_second: 1.89, interval_steps_per_second: 1.181, epoch: 19.4709[0m
[32m[2022-09-19 18:45:21,486] [    INFO][0m - loss: 4.413e-05, learning_rate: 7.142857142857143e-07, global_step: 3690, interval_runtime: 13.1659, interval_samples_per_second: 1.215, interval_steps_per_second: 0.76, epoch: 19.5238[0m
[32m[2022-09-19 18:45:33,791] [    INFO][0m - loss: 4.591e-05, learning_rate: 6.349206349206349e-07, global_step: 3700, interval_runtime: 12.1935, interval_samples_per_second: 1.312, interval_steps_per_second: 0.82, epoch: 19.5767[0m
[32m[2022-09-19 18:45:43,250] [    INFO][0m - loss: 0.004996, learning_rate: 5.555555555555555e-07, global_step: 3710, interval_runtime: 9.5704, interval_samples_per_second: 1.672, interval_steps_per_second: 1.045, epoch: 19.6296[0m
[32m[2022-09-19 18:45:56,322] [    INFO][0m - loss: 3.546e-05, learning_rate: 4.761904761904762e-07, global_step: 3720, interval_runtime: 13.0723, interval_samples_per_second: 1.224, interval_steps_per_second: 0.765, epoch: 19.6825[0m
[32m[2022-09-19 18:46:05,040] [    INFO][0m - loss: 0.01574828, learning_rate: 3.9682539682539683e-07, global_step: 3730, interval_runtime: 8.7178, interval_samples_per_second: 1.835, interval_steps_per_second: 1.147, epoch: 19.7354[0m
[32m[2022-09-19 18:46:18,347] [    INFO][0m - loss: 2.914e-05, learning_rate: 3.1746031746031743e-07, global_step: 3740, interval_runtime: 13.3062, interval_samples_per_second: 1.202, interval_steps_per_second: 0.752, epoch: 19.7884[0m
[32m[2022-09-19 18:46:28,859] [    INFO][0m - loss: 0.00905132, learning_rate: 2.380952380952381e-07, global_step: 3750, interval_runtime: 10.5122, interval_samples_per_second: 1.522, interval_steps_per_second: 0.951, epoch: 19.8413[0m
[32m[2022-09-19 18:46:39,921] [    INFO][0m - loss: 0.00389698, learning_rate: 1.5873015873015872e-07, global_step: 3760, interval_runtime: 11.0623, interval_samples_per_second: 1.446, interval_steps_per_second: 0.904, epoch: 19.8942[0m
[32m[2022-09-19 18:46:48,095] [    INFO][0m - loss: 4.688e-05, learning_rate: 7.936507936507936e-08, global_step: 3770, interval_runtime: 8.174, interval_samples_per_second: 1.957, interval_steps_per_second: 1.223, epoch: 19.9471[0m
[32m[2022-09-19 18:47:01,825] [    INFO][0m - loss: 0.01287999, learning_rate: 0.0, global_step: 3780, interval_runtime: 13.7299, interval_samples_per_second: 1.165, interval_steps_per_second: 0.728, epoch: 20.0[0m
[32m[2022-09-19 18:47:01,826] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 18:47:01,826] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-19 18:47:01,826] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 18:47:01,826] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 18:47:01,826] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-19 18:47:34,211] [    INFO][0m - eval_loss: 5.072025775909424, eval_accuracy: 0.45083758193736345, eval_runtime: 32.3845, eval_samples_per_second: 42.397, eval_steps_per_second: 2.656, epoch: 20.0[0m
[32m[2022-09-19 18:47:34,236] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-3780[0m
[32m[2022-09-19 18:47:34,236] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 18:47:36,879] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-3780/tokenizer_config.json[0m
[32m[2022-09-19 18:47:36,879] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-3780/special_tokens_map.json[0m
[32m[2022-09-19 18:47:42,857] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-19 18:47:42,858] [    INFO][0m - Loading best model from ./checkpoints_iflytek/checkpoint-378 (score: 0.4559359067734887).[0m
[33m[2022-09-19 18:47:42,858] [ WARNING][0m - Could not locate the best model at ./checkpoints_iflytek/checkpoint-378/model_state.pdparams, if you are running a distributed training on multiple nodes, you should activate `--save_on_each_node`.[0m
[32m[2022-09-19 18:47:42,858] [    INFO][0m - train_runtime: 6523.6959, train_samples_per_second: 9.271, train_steps_per_second: 0.579, train_loss: 0.2802521558246568, epoch: 20.0[0m
[32m[2022-09-19 18:47:42,859] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/[0m
[32m[2022-09-19 18:47:42,859] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 18:47:45,476] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/tokenizer_config.json[0m
[32m[2022-09-19 18:47:45,476] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/special_tokens_map.json[0m
[32m[2022-09-19 18:47:45,478] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-19 18:47:45,478] [    INFO][0m -   epoch                    =       20.0[0m
[32m[2022-09-19 18:47:45,478] [    INFO][0m -   train_loss               =     0.2803[0m
[32m[2022-09-19 18:47:45,478] [    INFO][0m -   train_runtime            = 1:48:43.69[0m
[32m[2022-09-19 18:47:45,478] [    INFO][0m -   train_samples_per_second =      9.271[0m
[32m[2022-09-19 18:47:45,478] [    INFO][0m -   train_steps_per_second   =      0.579[0m
[32m[2022-09-19 18:47:45,493] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-19 18:47:45,493] [    INFO][0m -   Num examples = 1749[0m
[32m[2022-09-19 18:47:45,493] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 18:47:45,493] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 18:47:45,493] [    INFO][0m -   Total prediction steps = 110[0m
[32m[2022-09-19 18:48:20,672] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-19 18:48:20,673] [    INFO][0m -   test_accuracy           =     0.4711[0m
[32m[2022-09-19 18:48:20,673] [    INFO][0m -   test_loss               =     4.9157[0m
[32m[2022-09-19 18:48:20,673] [    INFO][0m -   test_runtime            = 0:00:35.17[0m
[32m[2022-09-19 18:48:20,673] [    INFO][0m -   test_samples_per_second =     49.717[0m
[32m[2022-09-19 18:48:20,673] [    INFO][0m -   test_steps_per_second   =      3.127[0m
[32m[2022-09-19 18:48:20,674] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-19 18:48:20,674] [    INFO][0m -   Num examples = 2600[0m
[32m[2022-09-19 18:48:20,674] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 18:48:20,674] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 18:48:20,674] [    INFO][0m -   Total prediction steps = 163[0m
[32m[2022-09-19 18:49:19,345] [    INFO][0m - Predictions for iflytekf saved to ./fewclue_submit_examples.[0m
{
  "labels": 77,
  "text_a": "\u884c\u8baf\u901a\u662f\u5e7f\u5dde\u4ea4\u901a\u4fe1\u606f\u5316\u5efa\u8bbe\u6295\u8d44\u8425\u8fd0\u6709\u9650\u516c\u53f8\u5728\u5e7f\u5dde\u5e02\u4ea4\u901a\u8fd0\u8f93\u5c40\u7684\u6307\u5bfc\u4e0b\uff0c\u63a8\u51fa\u7684\u4e00\u6b3e\u63d0\u4f9b\u4ea4\u901a\u4fe1\u606f\u670d\u52a1\u7684\u624b\u673a\u7ec8\u7aef\u8f6f\u4ef6\uff0c\u652f\u6301Android\u548ciPhone\u7cfb\u7edf\u3002\u4e3b\u8981\u5305\u62ec\u8def\u51b5\u4fe1\u606f\u3001\u5b9e\u65f6\u516c\u4ea4\u3001\u505c\u8f66\u670d\u52a1\u3001\u7684\u58eb\u67e5\u8be2\u3001\u51fa\u884c\u89c4\u5212\u3001\u5730\u94c1\u4fe1\u606f\u3001\u822a\u7a7a\u4fe1\u606f\u3001\u94c1\u8def\u4fe1\u606f\u3001\u5ba2\u8fd0\u4fe1\u606f\u3001\u9a7e\u57f9\u4fe1\u606f\u3001\u4ea4\u901a\u8d44\u8baf\u3001WIFI\u70ed\u70b9\u7b49\u529f\u80fd\u6a21\u5757\u3002\u66f4\u65b0\u5185\u5bb91\u3001\u4f18\u5316\u5df2\u77e5bug\u3002",
  "text_b": "",
  "uid": 0
}

Prediction done.
 
==========
ocnli
==========
 
[32m[2022-09-19 18:49:34,315] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-19 18:49:34,315] [    INFO][0m - ============================================================[0m
[32m[2022-09-19 18:49:34,315] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-19 18:49:34,315] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-19 18:49:34,316] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-19 18:49:34,316] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-19 18:49:34,316] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-19 18:49:34,316] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-19 18:49:34,316] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-19 18:49:34,316] [    INFO][0m - [0m
[32m[2022-09-19 18:49:34,316] [    INFO][0m - ============================================================[0m
[32m[2022-09-19 18:49:34,316] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-19 18:49:34,316] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-19 18:49:34,316] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-19 18:49:34,316] [    INFO][0m - pretrained                    :/ssd2/wanghuijuan03/data/zero-shot/0919_model_state.pdparams[0m
[32m[2022-09-19 18:49:34,316] [    INFO][0m - prompt                        :â€œ{'text':'text_a'}â€å’Œâ€œ{'text':'text_b'}â€ä¹‹é—´çš„é€»è¾‘å…³ç³»æ˜¯{'mask'}{'mask'}ã€‚[0m
[32m[2022-09-19 18:49:34,316] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-19 18:49:34,317] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-19 18:49:34,317] [    INFO][0m - task_name                     :ocnli[0m
[32m[2022-09-19 18:49:34,317] [    INFO][0m - [0m
[32m[2022-09-19 18:49:34,317] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0919 18:49:34.318536 64618 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0919 18:49:34.322656 64618 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-19 18:49:39,229] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-19 18:49:39,241] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-19 18:49:39,241] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-19 18:49:39,242] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': 'â€œ'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': 'â€å’Œâ€œ'}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': 'â€ä¹‹é—´çš„é€»è¾‘å…³ç³»æ˜¯'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'ã€‚'}][0m
[32m[2022-09-19 18:49:40,597] [    INFO][0m - ============================================================[0m
[32m[2022-09-19 18:49:40,597] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-19 18:49:40,597] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-19 18:49:40,597] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-19 18:49:40,598] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-19 18:49:40,598] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-19 18:49:40,598] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-19 18:49:40,598] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-19 18:49:40,598] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-19 18:49:40,598] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-19 18:49:40,598] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-19 18:49:40,598] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-19 18:49:40,598] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-19 18:49:40,598] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-19 18:49:40,598] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-19 18:49:40,598] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-19 18:49:40,598] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-19 18:49:40,599] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-19 18:49:40,599] [    INFO][0m - eval_batch_size               :16[0m
[32m[2022-09-19 18:49:40,599] [    INFO][0m - eval_steps                    :200[0m
[32m[2022-09-19 18:49:40,599] [    INFO][0m - evaluation_strategy           :IntervalStrategy.EPOCH[0m
[32m[2022-09-19 18:49:40,599] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-19 18:49:40,599] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-19 18:49:40,599] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-19 18:49:40,599] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-19 18:49:40,599] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-19 18:49:40,599] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-19 18:49:40,599] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-19 18:49:40,599] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-19 18:49:40,599] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-19 18:49:40,600] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-09-19 18:49:40,600] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-19 18:49:40,600] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-19 18:49:40,600] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-19 18:49:40,600] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-19 18:49:40,600] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-19 18:49:40,600] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-19 18:49:40,600] [    INFO][0m - logging_dir                   :./checkpoints_ocnli/runs/Sep19_18-49-34_instance-3bwob41y-01[0m
[32m[2022-09-19 18:49:40,600] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-19 18:49:40,600] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-19 18:49:40,600] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-19 18:49:40,600] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-19 18:49:40,600] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-19 18:49:40,600] [    INFO][0m - max_seq_length                :128[0m
[32m[2022-09-19 18:49:40,601] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-19 18:49:40,601] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-19 18:49:40,601] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-19 18:49:40,601] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-19 18:49:40,601] [    INFO][0m - num_train_epochs              :20.0[0m
[32m[2022-09-19 18:49:40,601] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-19 18:49:40,601] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-19 18:49:40,601] [    INFO][0m - output_dir                    :./checkpoints_ocnli/[0m
[32m[2022-09-19 18:49:40,601] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-19 18:49:40,601] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-19 18:49:40,601] [    INFO][0m - per_device_eval_batch_size    :16[0m
[32m[2022-09-19 18:49:40,601] [    INFO][0m - per_device_train_batch_size   :16[0m
[32m[2022-09-19 18:49:40,601] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-19 18:49:40,601] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-19 18:49:40,602] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-19 18:49:40,602] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-09-19 18:49:40,602] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-19 18:49:40,602] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-19 18:49:40,602] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-19 18:49:40,602] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-19 18:49:40,602] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-19 18:49:40,602] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-19 18:49:40,602] [    INFO][0m - run_name                      :./checkpoints_ocnli/[0m
[32m[2022-09-19 18:49:40,602] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-19 18:49:40,602] [    INFO][0m - save_steps                    :200[0m
[32m[2022-09-19 18:49:40,602] [    INFO][0m - save_strategy                 :IntervalStrategy.EPOCH[0m
[32m[2022-09-19 18:49:40,602] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-19 18:49:40,602] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-19 18:49:40,603] [    INFO][0m - seed                          :42[0m
[32m[2022-09-19 18:49:40,603] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-19 18:49:40,603] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-19 18:49:40,603] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-19 18:49:40,603] [    INFO][0m - train_batch_size              :16[0m
[32m[2022-09-19 18:49:40,603] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-19 18:49:40,603] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-19 18:49:40,603] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-19 18:49:40,603] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-19 18:49:40,603] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-19 18:49:40,603] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-19 18:49:40,603] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-19 18:49:40,603] [    INFO][0m - [0m
[32m[2022-09-19 18:49:40,606] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-19 18:49:40,606] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 18:49:40,606] [    INFO][0m -   Num Epochs = 20[0m
[32m[2022-09-19 18:49:40,606] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2022-09-19 18:49:40,606] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2022-09-19 18:49:40,607] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-19 18:49:40,607] [    INFO][0m -   Total optimization steps = 200.0[0m
[32m[2022-09-19 18:49:40,607] [    INFO][0m -   Total num train samples = 3200[0m
[32m[2022-09-19 18:49:45,029] [    INFO][0m - loss: 3.30560493, learning_rate: 2.8499999999999998e-05, global_step: 10, interval_runtime: 4.4209, interval_samples_per_second: 3.619, interval_steps_per_second: 2.262, epoch: 1.0[0m
[32m[2022-09-19 18:49:45,030] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 18:49:45,031] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 18:49:45,031] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 18:49:45,031] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 18:49:45,031] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 18:49:46,397] [    INFO][0m - eval_loss: 1.553631067276001, eval_accuracy: 0.38125, eval_runtime: 1.3659, eval_samples_per_second: 117.142, eval_steps_per_second: 7.321, epoch: 1.0[0m
[32m[2022-09-19 18:49:46,398] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-10[0m
[32m[2022-09-19 18:49:46,398] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 18:49:49,439] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-10/tokenizer_config.json[0m
[32m[2022-09-19 18:49:49,440] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-10/special_tokens_map.json[0m
[32m[2022-09-19 18:49:58,100] [    INFO][0m - loss: 1.33502092, learning_rate: 2.7000000000000002e-05, global_step: 20, interval_runtime: 13.0709, interval_samples_per_second: 1.224, interval_steps_per_second: 0.765, epoch: 2.0[0m
[32m[2022-09-19 18:49:58,101] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 18:49:58,101] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 18:49:58,101] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 18:49:58,101] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 18:49:58,101] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 18:49:59,462] [    INFO][0m - eval_loss: 1.6233774423599243, eval_accuracy: 0.33125, eval_runtime: 1.3609, eval_samples_per_second: 117.573, eval_steps_per_second: 7.348, epoch: 2.0[0m
[32m[2022-09-19 18:49:59,463] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-20[0m
[32m[2022-09-19 18:49:59,463] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 18:50:02,298] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-20/tokenizer_config.json[0m
[32m[2022-09-19 18:50:02,298] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-20/special_tokens_map.json[0m
[32m[2022-09-19 18:50:12,596] [    INFO][0m - loss: 0.98214836, learning_rate: 2.55e-05, global_step: 30, interval_runtime: 14.4958, interval_samples_per_second: 1.104, interval_steps_per_second: 0.69, epoch: 3.0[0m
[32m[2022-09-19 18:50:12,597] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 18:50:12,597] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 18:50:12,597] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 18:50:12,598] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 18:50:12,598] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 18:50:13,957] [    INFO][0m - eval_loss: 1.335870385169983, eval_accuracy: 0.45, eval_runtime: 1.3587, eval_samples_per_second: 117.761, eval_steps_per_second: 7.36, epoch: 3.0[0m
[32m[2022-09-19 18:50:13,958] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-30[0m
[32m[2022-09-19 18:50:13,958] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 18:50:16,894] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-30/tokenizer_config.json[0m
[32m[2022-09-19 18:50:16,894] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-30/special_tokens_map.json[0m
[32m[2022-09-19 18:50:25,949] [    INFO][0m - loss: 0.54417534, learning_rate: 2.4e-05, global_step: 40, interval_runtime: 13.3535, interval_samples_per_second: 1.198, interval_steps_per_second: 0.749, epoch: 4.0[0m
[32m[2022-09-19 18:50:25,950] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 18:50:25,950] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 18:50:25,950] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 18:50:25,950] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 18:50:25,950] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 18:50:27,308] [    INFO][0m - eval_loss: 1.4811749458312988, eval_accuracy: 0.5625, eval_runtime: 1.3573, eval_samples_per_second: 117.884, eval_steps_per_second: 7.368, epoch: 4.0[0m
[32m[2022-09-19 18:50:30,636] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-40[0m
[32m[2022-09-19 18:50:30,637] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 18:50:33,635] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-40/tokenizer_config.json[0m
[32m[2022-09-19 18:50:33,636] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-40/special_tokens_map.json[0m
[32m[2022-09-19 18:50:44,192] [    INFO][0m - loss: 0.23360939, learning_rate: 2.25e-05, global_step: 50, interval_runtime: 18.2434, interval_samples_per_second: 0.877, interval_steps_per_second: 0.548, epoch: 5.0[0m
[32m[2022-09-19 18:50:44,193] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 18:50:44,193] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 18:50:44,194] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 18:50:44,194] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 18:50:44,194] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 18:50:45,564] [    INFO][0m - eval_loss: 1.7460445165634155, eval_accuracy: 0.53125, eval_runtime: 1.3702, eval_samples_per_second: 116.773, eval_steps_per_second: 7.298, epoch: 5.0[0m
[32m[2022-09-19 18:50:45,785] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-50[0m
[32m[2022-09-19 18:50:45,785] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 18:50:48,784] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-50/tokenizer_config.json[0m
[32m[2022-09-19 18:50:48,785] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-50/special_tokens_map.json[0m
[32m[2022-09-19 18:50:59,554] [    INFO][0m - loss: 0.06382051, learning_rate: 2.1e-05, global_step: 60, interval_runtime: 15.3613, interval_samples_per_second: 1.042, interval_steps_per_second: 0.651, epoch: 6.0[0m
[32m[2022-09-19 18:50:59,554] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 18:50:59,555] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 18:50:59,555] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 18:50:59,555] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 18:50:59,555] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 18:51:00,911] [    INFO][0m - eval_loss: 2.694178819656372, eval_accuracy: 0.55, eval_runtime: 1.3553, eval_samples_per_second: 118.052, eval_steps_per_second: 7.378, epoch: 6.0[0m
[32m[2022-09-19 18:51:00,911] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-60[0m
[32m[2022-09-19 18:51:00,911] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 18:51:03,930] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-60/tokenizer_config.json[0m
[32m[2022-09-19 18:51:03,931] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-60/special_tokens_map.json[0m
[32m[2022-09-19 18:51:14,661] [    INFO][0m - loss: 0.03632399, learning_rate: 1.95e-05, global_step: 70, interval_runtime: 15.1069, interval_samples_per_second: 1.059, interval_steps_per_second: 0.662, epoch: 7.0[0m
[32m[2022-09-19 18:51:14,661] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 18:51:14,662] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 18:51:14,662] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 18:51:14,662] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 18:51:14,662] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 18:51:16,024] [    INFO][0m - eval_loss: 3.956580400466919, eval_accuracy: 0.5625, eval_runtime: 1.3618, eval_samples_per_second: 117.489, eval_steps_per_second: 7.343, epoch: 7.0[0m
[32m[2022-09-19 18:51:16,158] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-70[0m
[32m[2022-09-19 18:51:16,159] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 18:51:19,453] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-70/tokenizer_config.json[0m
[32m[2022-09-19 18:51:19,454] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-70/special_tokens_map.json[0m
[32m[2022-09-19 18:51:29,815] [    INFO][0m - loss: 0.03113671, learning_rate: 1.8e-05, global_step: 80, interval_runtime: 15.1544, interval_samples_per_second: 1.056, interval_steps_per_second: 0.66, epoch: 8.0[0m
[32m[2022-09-19 18:51:29,816] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 18:51:29,816] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 18:51:29,816] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 18:51:29,816] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 18:51:29,816] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 18:51:31,174] [    INFO][0m - eval_loss: 5.740918159484863, eval_accuracy: 0.5, eval_runtime: 1.3568, eval_samples_per_second: 117.925, eval_steps_per_second: 7.37, epoch: 8.0[0m
[32m[2022-09-19 18:51:31,174] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-80[0m
[32m[2022-09-19 18:51:31,174] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 18:51:34,387] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-80/tokenizer_config.json[0m
[32m[2022-09-19 18:51:34,388] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-80/special_tokens_map.json[0m
[32m[2022-09-19 18:51:46,241] [    INFO][0m - loss: 0.05854583, learning_rate: 1.65e-05, global_step: 90, interval_runtime: 16.4259, interval_samples_per_second: 0.974, interval_steps_per_second: 0.609, epoch: 9.0[0m
[32m[2022-09-19 18:51:46,242] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 18:51:46,242] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 18:51:46,242] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 18:51:46,242] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 18:51:46,242] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 18:51:47,597] [    INFO][0m - eval_loss: 5.516229152679443, eval_accuracy: 0.53125, eval_runtime: 1.3546, eval_samples_per_second: 118.118, eval_steps_per_second: 7.382, epoch: 9.0[0m
[32m[2022-09-19 18:51:47,598] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-90[0m
[32m[2022-09-19 18:51:47,598] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 18:51:50,544] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-90/tokenizer_config.json[0m
[32m[2022-09-19 18:51:50,545] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-90/special_tokens_map.json[0m
[32m[2022-09-19 18:52:01,963] [    INFO][0m - loss: 0.00019725, learning_rate: 1.5e-05, global_step: 100, interval_runtime: 15.7221, interval_samples_per_second: 1.018, interval_steps_per_second: 0.636, epoch: 10.0[0m
[32m[2022-09-19 18:52:01,964] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 18:52:01,964] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 18:52:01,964] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 18:52:01,964] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 18:52:01,964] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 18:52:03,322] [    INFO][0m - eval_loss: 5.399621963500977, eval_accuracy: 0.525, eval_runtime: 1.3574, eval_samples_per_second: 117.872, eval_steps_per_second: 7.367, epoch: 10.0[0m
[32m[2022-09-19 18:52:03,323] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-100[0m
[32m[2022-09-19 18:52:03,323] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 18:52:06,078] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-19 18:52:06,079] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-19 18:52:16,436] [    INFO][0m - loss: 0.05502878, learning_rate: 1.3500000000000001e-05, global_step: 110, interval_runtime: 14.4734, interval_samples_per_second: 1.105, interval_steps_per_second: 0.691, epoch: 11.0[0m
[32m[2022-09-19 18:52:16,437] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 18:52:16,438] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 18:52:16,438] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 18:52:16,438] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 18:52:16,438] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 18:52:17,787] [    INFO][0m - eval_loss: 5.412727355957031, eval_accuracy: 0.53125, eval_runtime: 1.3494, eval_samples_per_second: 118.57, eval_steps_per_second: 7.411, epoch: 11.0[0m
[32m[2022-09-19 18:52:17,788] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-110[0m
[32m[2022-09-19 18:52:17,788] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 18:52:20,687] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-110/tokenizer_config.json[0m
[32m[2022-09-19 18:52:20,687] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-110/special_tokens_map.json[0m
[32m[2022-09-19 18:52:29,684] [    INFO][0m - loss: 0.00015342, learning_rate: 1.2e-05, global_step: 120, interval_runtime: 13.2474, interval_samples_per_second: 1.208, interval_steps_per_second: 0.755, epoch: 12.0[0m
[32m[2022-09-19 18:52:29,685] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 18:52:29,685] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 18:52:29,685] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 18:52:29,685] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 18:52:29,685] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 18:52:31,711] [    INFO][0m - eval_loss: 5.705589294433594, eval_accuracy: 0.525, eval_runtime: 1.3578, eval_samples_per_second: 117.835, eval_steps_per_second: 7.365, epoch: 12.0[0m
[32m[2022-09-19 18:52:31,712] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-120[0m
[32m[2022-09-19 18:52:31,712] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 18:52:34,669] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-120/tokenizer_config.json[0m
[32m[2022-09-19 18:52:34,670] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-120/special_tokens_map.json[0m
[32m[2022-09-19 18:52:43,579] [    INFO][0m - loss: 0.016914, learning_rate: 1.05e-05, global_step: 130, interval_runtime: 13.8956, interval_samples_per_second: 1.151, interval_steps_per_second: 0.72, epoch: 13.0[0m
[32m[2022-09-19 18:52:43,580] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 18:52:43,580] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 18:52:43,580] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 18:52:43,580] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 18:52:43,581] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 18:52:44,936] [    INFO][0m - eval_loss: 5.565103530883789, eval_accuracy: 0.5375, eval_runtime: 1.3552, eval_samples_per_second: 118.06, eval_steps_per_second: 7.379, epoch: 13.0[0m
[32m[2022-09-19 18:52:44,937] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-130[0m
[32m[2022-09-19 18:52:44,937] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 18:52:47,580] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-130/tokenizer_config.json[0m
[32m[2022-09-19 18:52:47,580] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-130/special_tokens_map.json[0m
[32m[2022-09-19 18:52:58,367] [    INFO][0m - loss: 0.00127509, learning_rate: 9e-06, global_step: 140, interval_runtime: 14.7873, interval_samples_per_second: 1.082, interval_steps_per_second: 0.676, epoch: 14.0[0m
[32m[2022-09-19 18:52:58,368] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 18:52:58,368] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 18:52:58,368] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 18:52:58,368] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 18:52:58,368] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 18:52:59,722] [    INFO][0m - eval_loss: 5.573521614074707, eval_accuracy: 0.51875, eval_runtime: 1.3541, eval_samples_per_second: 118.157, eval_steps_per_second: 7.385, epoch: 14.0[0m
[32m[2022-09-19 18:53:00,313] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-140[0m
[32m[2022-09-19 18:53:00,315] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 18:53:03,129] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-140/tokenizer_config.json[0m
[32m[2022-09-19 18:53:03,130] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-140/special_tokens_map.json[0m
[32m[2022-09-19 18:53:12,358] [    INFO][0m - loss: 0.00085442, learning_rate: 7.5e-06, global_step: 150, interval_runtime: 13.9915, interval_samples_per_second: 1.144, interval_steps_per_second: 0.715, epoch: 15.0[0m
[32m[2022-09-19 18:53:12,359] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 18:53:12,359] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 18:53:12,359] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 18:53:12,359] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 18:53:12,360] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 18:53:13,715] [    INFO][0m - eval_loss: 5.701867580413818, eval_accuracy: 0.5875, eval_runtime: 1.3555, eval_samples_per_second: 118.039, eval_steps_per_second: 7.377, epoch: 15.0[0m
[32m[2022-09-19 18:53:13,716] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-150[0m
[32m[2022-09-19 18:53:13,716] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 18:53:16,585] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-150/tokenizer_config.json[0m
[32m[2022-09-19 18:53:16,585] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-150/special_tokens_map.json[0m
[32m[2022-09-19 18:53:25,067] [    INFO][0m - loss: 3.356e-05, learning_rate: 6e-06, global_step: 160, interval_runtime: 12.7092, interval_samples_per_second: 1.259, interval_steps_per_second: 0.787, epoch: 16.0[0m
[32m[2022-09-19 18:53:25,068] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 18:53:25,069] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 18:53:25,069] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 18:53:25,069] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 18:53:25,069] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 18:53:26,418] [    INFO][0m - eval_loss: 5.96829080581665, eval_accuracy: 0.56875, eval_runtime: 1.3493, eval_samples_per_second: 118.581, eval_steps_per_second: 7.411, epoch: 16.0[0m
[32m[2022-09-19 18:53:26,419] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-160[0m
[32m[2022-09-19 18:53:26,419] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 18:53:29,221] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-160/tokenizer_config.json[0m
[32m[2022-09-19 18:53:29,221] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-160/special_tokens_map.json[0m
[32m[2022-09-19 18:53:37,942] [    INFO][0m - loss: 0.00015584, learning_rate: 4.5e-06, global_step: 170, interval_runtime: 12.8742, interval_samples_per_second: 1.243, interval_steps_per_second: 0.777, epoch: 17.0[0m
[32m[2022-09-19 18:53:37,942] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 18:53:37,942] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 18:53:37,943] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 18:53:37,943] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 18:53:37,943] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 18:53:39,272] [    INFO][0m - eval_loss: 5.909663200378418, eval_accuracy: 0.56875, eval_runtime: 1.3289, eval_samples_per_second: 120.399, eval_steps_per_second: 7.525, epoch: 17.0[0m
[32m[2022-09-19 18:53:39,272] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-170[0m
[32m[2022-09-19 18:53:39,272] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 18:53:41,742] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-170/tokenizer_config.json[0m
[32m[2022-09-19 18:53:41,742] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-170/special_tokens_map.json[0m
[32m[2022-09-19 18:53:49,999] [    INFO][0m - loss: 6.356e-05, learning_rate: 3e-06, global_step: 180, interval_runtime: 12.0576, interval_samples_per_second: 1.327, interval_steps_per_second: 0.829, epoch: 18.0[0m
[32m[2022-09-19 18:53:49,999] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 18:53:50,000] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 18:53:50,000] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 18:53:50,000] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 18:53:50,000] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 18:53:52,405] [    INFO][0m - eval_loss: 5.538438320159912, eval_accuracy: 0.5625, eval_runtime: 1.352, eval_samples_per_second: 118.339, eval_steps_per_second: 7.396, epoch: 18.0[0m
[32m[2022-09-19 18:53:52,405] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-180[0m
[32m[2022-09-19 18:53:52,405] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 18:53:54,943] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-180/tokenizer_config.json[0m
[32m[2022-09-19 18:53:54,943] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-180/special_tokens_map.json[0m
[32m[2022-09-19 18:54:03,121] [    INFO][0m - loss: 1.674e-05, learning_rate: 1.5e-06, global_step: 190, interval_runtime: 13.1217, interval_samples_per_second: 1.219, interval_steps_per_second: 0.762, epoch: 19.0[0m
[32m[2022-09-19 18:54:03,121] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 18:54:03,121] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 18:54:03,122] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 18:54:03,122] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 18:54:03,122] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 18:54:04,459] [    INFO][0m - eval_loss: 5.42733907699585, eval_accuracy: 0.56875, eval_runtime: 1.3367, eval_samples_per_second: 119.696, eval_steps_per_second: 7.481, epoch: 19.0[0m
[32m[2022-09-19 18:54:05,896] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-190[0m
[32m[2022-09-19 18:54:05,896] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 18:54:08,637] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-190/tokenizer_config.json[0m
[32m[2022-09-19 18:54:08,637] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-190/special_tokens_map.json[0m
[32m[2022-09-19 18:54:18,040] [    INFO][0m - loss: 9.87e-06, learning_rate: 0.0, global_step: 200, interval_runtime: 14.9193, interval_samples_per_second: 1.072, interval_steps_per_second: 0.67, epoch: 20.0[0m
[32m[2022-09-19 18:54:18,040] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 18:54:18,040] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 18:54:18,041] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 18:54:18,041] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 18:54:18,041] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 18:54:19,370] [    INFO][0m - eval_loss: 5.416449069976807, eval_accuracy: 0.5625, eval_runtime: 1.3296, eval_samples_per_second: 120.336, eval_steps_per_second: 7.521, epoch: 20.0[0m
[32m[2022-09-19 18:54:19,371] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-200[0m
[32m[2022-09-19 18:54:19,371] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 18:54:21,848] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-200/tokenizer_config.json[0m
[32m[2022-09-19 18:54:21,849] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-200/special_tokens_map.json[0m
[32m[2022-09-19 18:54:26,577] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-19 18:54:26,578] [    INFO][0m - Loading best model from ./checkpoints_ocnli/checkpoint-150 (score: 0.5875).[0m
[32m[2022-09-19 18:54:28,076] [    INFO][0m - train_runtime: 287.4681, train_samples_per_second: 11.132, train_steps_per_second: 0.696, train_loss: 0.3332544265512843, epoch: 20.0[0m
[32m[2022-09-19 18:54:28,077] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/[0m
[32m[2022-09-19 18:54:28,078] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 18:54:30,508] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/tokenizer_config.json[0m
[32m[2022-09-19 18:54:33,091] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/special_tokens_map.json[0m
[32m[2022-09-19 18:54:33,092] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-19 18:54:33,093] [    INFO][0m -   epoch                    =       20.0[0m
[32m[2022-09-19 18:54:33,093] [    INFO][0m -   train_loss               =     0.3333[0m
[32m[2022-09-19 18:54:33,093] [    INFO][0m -   train_runtime            = 0:04:47.46[0m
[32m[2022-09-19 18:54:33,093] [    INFO][0m -   train_samples_per_second =     11.132[0m
[32m[2022-09-19 18:54:33,093] [    INFO][0m -   train_steps_per_second   =      0.696[0m
[32m[2022-09-19 18:54:33,095] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-19 18:54:33,095] [    INFO][0m -   Num examples = 2520[0m
[32m[2022-09-19 18:54:33,095] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 18:54:33,095] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 18:54:33,096] [    INFO][0m -   Total prediction steps = 158[0m
[32m[2022-09-19 18:54:55,296] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-19 18:54:55,296] [    INFO][0m -   test_accuracy           =     0.5746[0m
[32m[2022-09-19 18:54:55,296] [    INFO][0m -   test_loss               =     5.2402[0m
[32m[2022-09-19 18:54:55,296] [    INFO][0m -   test_runtime            = 0:00:22.20[0m
[32m[2022-09-19 18:54:55,296] [    INFO][0m -   test_samples_per_second =    113.512[0m
[32m[2022-09-19 18:54:55,297] [    INFO][0m -   test_steps_per_second   =      7.117[0m
[32m[2022-09-19 18:54:55,297] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-19 18:54:55,297] [    INFO][0m -   Num examples = 3000[0m
[32m[2022-09-19 18:54:55,297] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 18:54:55,297] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 18:54:55,297] [    INFO][0m -   Total prediction steps = 188[0m
[32m[2022-09-19 18:55:24,750] [    INFO][0m - Predictions for ocnlif saved to ./fewclue_submit_examples.[0m
{
  "labels": 2,
  "text_a": "\u4e03\u4e94\u671f\u95f4\u5f00\u59cb,\u56fd\u5bb6\u53c8\u6295\u8d44\u5c06\u6b66\u6c49\u5e02\u533a\u7684\u90e8\u5206\u571f\u5824\u6539\u5efa\u4e3a\u94a2\u7b4b\u6ce5\u51dd\u571f\u9632\u6c34\u5899",
  "text_b": "\u516b\u4e94\u671f\u95f4\u4f1a\u628a\u5269\u4e0b\u7684\u571f\u5824\u90fd\u6539\u5efa\u5b8c",
  "uid": 0
}

Prediction done.
 
==========
bustm
==========
 
[32m[2022-09-19 18:55:40,903] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-19 18:55:40,904] [    INFO][0m - ============================================================[0m
[32m[2022-09-19 18:55:40,904] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-19 18:55:40,904] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-19 18:55:40,904] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-19 18:55:40,904] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-19 18:55:40,904] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-19 18:55:40,904] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-19 18:55:40,904] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-19 18:55:40,904] [    INFO][0m - [0m
[32m[2022-09-19 18:55:40,904] [    INFO][0m - ============================================================[0m
[32m[2022-09-19 18:55:40,904] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-19 18:55:40,905] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-19 18:55:40,905] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-19 18:55:40,905] [    INFO][0m - pretrained                    :/ssd2/wanghuijuan03/data/zero-shot/0919_model_state.pdparams[0m
[32m[2022-09-19 18:55:40,905] [    INFO][0m - prompt                        :â€œ{'text':'text_a'}â€å’Œâ€œ{'text':'text_b'}â€æè¿°çš„æ˜¯{'mask'}{'mask'}çš„äº‹æƒ…ã€‚[0m
[32m[2022-09-19 18:55:40,905] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-19 18:55:40,905] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-19 18:55:40,905] [    INFO][0m - task_name                     :bustm[0m
[32m[2022-09-19 18:55:40,905] [    INFO][0m - [0m
[32m[2022-09-19 18:55:40,905] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0919 18:55:40.907323 70287 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0919 18:55:40.911420 70287 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-19 18:55:45,931] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-19 18:55:45,942] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-19 18:55:45,942] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-19 18:55:45,943] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': 'â€œ'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': 'â€å’Œâ€œ'}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': 'â€æè¿°çš„æ˜¯'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'çš„äº‹æƒ…ã€‚'}][0m
[32m[2022-09-19 18:55:47,224] [    INFO][0m - ============================================================[0m
[32m[2022-09-19 18:55:47,224] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-19 18:55:47,224] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-19 18:55:47,224] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-19 18:55:47,224] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-19 18:55:47,225] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-19 18:55:47,225] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-19 18:55:47,225] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-19 18:55:47,225] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-19 18:55:47,225] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-19 18:55:47,225] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-19 18:55:47,225] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-19 18:55:47,225] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-19 18:55:47,225] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-19 18:55:47,225] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-19 18:55:47,225] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-19 18:55:47,225] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-19 18:55:47,225] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-19 18:55:47,226] [    INFO][0m - eval_batch_size               :16[0m
[32m[2022-09-19 18:55:47,226] [    INFO][0m - eval_steps                    :200[0m
[32m[2022-09-19 18:55:47,226] [    INFO][0m - evaluation_strategy           :IntervalStrategy.EPOCH[0m
[32m[2022-09-19 18:55:47,226] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-19 18:55:47,226] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-19 18:55:47,226] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-19 18:55:47,226] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-19 18:55:47,226] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-19 18:55:47,226] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-19 18:55:47,226] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-19 18:55:47,226] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-19 18:55:47,226] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-19 18:55:47,226] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-09-19 18:55:47,226] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-19 18:55:47,227] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-19 18:55:47,227] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-19 18:55:47,227] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-19 18:55:47,227] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-19 18:55:47,227] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-19 18:55:47,227] [    INFO][0m - logging_dir                   :./checkpoints_bustm/runs/Sep19_18-55-40_instance-3bwob41y-01[0m
[32m[2022-09-19 18:55:47,227] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-19 18:55:47,227] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-19 18:55:47,227] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-19 18:55:47,227] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-19 18:55:47,227] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-19 18:55:47,227] [    INFO][0m - max_seq_length                :64[0m
[32m[2022-09-19 18:55:47,227] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-19 18:55:47,228] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-19 18:55:47,228] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-19 18:55:47,228] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-19 18:55:47,228] [    INFO][0m - num_train_epochs              :20.0[0m
[32m[2022-09-19 18:55:47,228] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-19 18:55:47,228] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-19 18:55:47,228] [    INFO][0m - output_dir                    :./checkpoints_bustm/[0m
[32m[2022-09-19 18:55:47,228] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-19 18:55:47,228] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-19 18:55:47,228] [    INFO][0m - per_device_eval_batch_size    :16[0m
[32m[2022-09-19 18:55:47,228] [    INFO][0m - per_device_train_batch_size   :16[0m
[32m[2022-09-19 18:55:47,228] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-19 18:55:47,228] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-19 18:55:47,228] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-19 18:55:47,229] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-09-19 18:55:47,229] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-19 18:55:47,229] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-19 18:55:47,229] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-19 18:55:47,229] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-19 18:55:47,229] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-19 18:55:47,229] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-19 18:55:47,229] [    INFO][0m - run_name                      :./checkpoints_bustm/[0m
[32m[2022-09-19 18:55:47,229] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-19 18:55:47,229] [    INFO][0m - save_steps                    :200[0m
[32m[2022-09-19 18:55:47,229] [    INFO][0m - save_strategy                 :IntervalStrategy.EPOCH[0m
[32m[2022-09-19 18:55:47,229] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-19 18:55:47,229] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-19 18:55:47,230] [    INFO][0m - seed                          :42[0m
[32m[2022-09-19 18:55:47,230] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-19 18:55:47,230] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-19 18:55:47,230] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-19 18:55:47,230] [    INFO][0m - train_batch_size              :16[0m
[32m[2022-09-19 18:55:47,230] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-19 18:55:47,230] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-19 18:55:47,230] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-19 18:55:47,230] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-19 18:55:47,230] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-19 18:55:47,230] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-19 18:55:47,230] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-19 18:55:47,230] [    INFO][0m - [0m
[32m[2022-09-19 18:55:47,233] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-19 18:55:47,233] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 18:55:47,234] [    INFO][0m -   Num Epochs = 20[0m
[32m[2022-09-19 18:55:47,234] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2022-09-19 18:55:47,234] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2022-09-19 18:55:47,234] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-19 18:55:47,234] [    INFO][0m -   Total optimization steps = 200.0[0m
[32m[2022-09-19 18:55:47,234] [    INFO][0m -   Total num train samples = 3200[0m
[32m[2022-09-19 18:55:50,288] [    INFO][0m - loss: 0.59963913, learning_rate: 2.8499999999999998e-05, global_step: 10, interval_runtime: 3.0529, interval_samples_per_second: 5.241, interval_steps_per_second: 3.276, epoch: 1.0[0m
[32m[2022-09-19 18:55:50,290] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 18:55:50,290] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 18:55:50,290] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 18:55:50,290] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 18:55:50,290] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 18:55:51,063] [    INFO][0m - eval_loss: 0.3434304893016815, eval_accuracy: 0.58125, eval_runtime: 0.7727, eval_samples_per_second: 207.08, eval_steps_per_second: 12.942, epoch: 1.0[0m
[32m[2022-09-19 18:55:51,063] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-10[0m
[32m[2022-09-19 18:55:51,064] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 18:55:54,016] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-10/tokenizer_config.json[0m
[32m[2022-09-19 18:55:54,016] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-10/special_tokens_map.json[0m
[32m[2022-09-19 18:56:01,729] [    INFO][0m - loss: 0.40321326, learning_rate: 2.7000000000000002e-05, global_step: 20, interval_runtime: 11.4416, interval_samples_per_second: 1.398, interval_steps_per_second: 0.874, epoch: 2.0[0m
[32m[2022-09-19 18:56:01,730] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 18:56:01,730] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 18:56:01,730] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 18:56:01,730] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 18:56:01,731] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 18:56:02,494] [    INFO][0m - eval_loss: 0.39246129989624023, eval_accuracy: 0.51875, eval_runtime: 0.7629, eval_samples_per_second: 209.714, eval_steps_per_second: 13.107, epoch: 2.0[0m
[32m[2022-09-19 18:56:02,494] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-20[0m
[32m[2022-09-19 18:56:02,494] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 18:56:05,359] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-20/tokenizer_config.json[0m
[32m[2022-09-19 18:56:05,360] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-20/special_tokens_map.json[0m
[32m[2022-09-19 18:56:13,018] [    INFO][0m - loss: 0.34091597, learning_rate: 2.55e-05, global_step: 30, interval_runtime: 11.2889, interval_samples_per_second: 1.417, interval_steps_per_second: 0.886, epoch: 3.0[0m
[32m[2022-09-19 18:56:13,019] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 18:56:13,019] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 18:56:13,019] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 18:56:13,020] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 18:56:13,020] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 18:56:13,787] [    INFO][0m - eval_loss: 0.3328617811203003, eval_accuracy: 0.6, eval_runtime: 0.7677, eval_samples_per_second: 208.417, eval_steps_per_second: 13.026, epoch: 3.0[0m
[32m[2022-09-19 18:56:13,788] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-30[0m
[32m[2022-09-19 18:56:13,788] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 18:56:16,703] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-30/tokenizer_config.json[0m
[32m[2022-09-19 18:56:16,704] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-30/special_tokens_map.json[0m
[32m[2022-09-19 18:56:24,258] [    INFO][0m - loss: 0.22172198, learning_rate: 2.4e-05, global_step: 40, interval_runtime: 11.2395, interval_samples_per_second: 1.424, interval_steps_per_second: 0.89, epoch: 4.0[0m
[32m[2022-09-19 18:56:24,258] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 18:56:24,259] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 18:56:24,259] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 18:56:24,259] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 18:56:24,259] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 18:56:25,004] [    INFO][0m - eval_loss: 0.40543508529663086, eval_accuracy: 0.63125, eval_runtime: 0.7447, eval_samples_per_second: 214.856, eval_steps_per_second: 13.429, epoch: 4.0[0m
[32m[2022-09-19 18:56:25,004] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-40[0m
[32m[2022-09-19 18:56:25,004] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 18:56:27,797] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-40/tokenizer_config.json[0m
[32m[2022-09-19 18:56:27,797] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-40/special_tokens_map.json[0m
[32m[2022-09-19 18:56:36,168] [    INFO][0m - loss: 0.20742214, learning_rate: 2.25e-05, global_step: 50, interval_runtime: 11.9105, interval_samples_per_second: 1.343, interval_steps_per_second: 0.84, epoch: 5.0[0m
[32m[2022-09-19 18:56:36,169] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 18:56:36,169] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 18:56:36,169] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 18:56:36,169] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 18:56:36,169] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 18:56:36,926] [    INFO][0m - eval_loss: 0.42008814215660095, eval_accuracy: 0.725, eval_runtime: 0.7565, eval_samples_per_second: 211.491, eval_steps_per_second: 13.218, epoch: 5.0[0m
[32m[2022-09-19 18:56:36,926] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-50[0m
[32m[2022-09-19 18:56:36,927] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 18:56:41,499] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-50/tokenizer_config.json[0m
[32m[2022-09-19 18:56:41,499] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-50/special_tokens_map.json[0m
[32m[2022-09-19 18:56:48,337] [    INFO][0m - loss: 0.11435804, learning_rate: 2.1e-05, global_step: 60, interval_runtime: 12.1684, interval_samples_per_second: 1.315, interval_steps_per_second: 0.822, epoch: 6.0[0m
[32m[2022-09-19 18:56:52,791] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 18:56:52,792] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 18:56:52,792] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 18:56:52,792] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 18:56:52,792] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 18:56:53,560] [    INFO][0m - eval_loss: 0.4505729675292969, eval_accuracy: 0.68125, eval_runtime: 0.7684, eval_samples_per_second: 208.236, eval_steps_per_second: 13.015, epoch: 6.0[0m
[32m[2022-09-19 18:56:53,561] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-60[0m
[32m[2022-09-19 18:56:53,561] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 18:56:56,113] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-60/tokenizer_config.json[0m
[32m[2022-09-19 18:56:56,114] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-60/special_tokens_map.json[0m
[32m[2022-09-19 18:57:02,893] [    INFO][0m - loss: 0.01820072, learning_rate: 1.95e-05, global_step: 70, interval_runtime: 14.5562, interval_samples_per_second: 1.099, interval_steps_per_second: 0.687, epoch: 7.0[0m
[32m[2022-09-19 18:57:02,893] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 18:57:02,893] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 18:57:02,893] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 18:57:02,893] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 18:57:02,893] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 18:57:03,661] [    INFO][0m - eval_loss: 0.8884016871452332, eval_accuracy: 0.7, eval_runtime: 0.7671, eval_samples_per_second: 208.59, eval_steps_per_second: 13.037, epoch: 7.0[0m
[32m[2022-09-19 18:57:03,661] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-70[0m
[32m[2022-09-19 18:57:03,661] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 18:57:06,221] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-70/tokenizer_config.json[0m
[32m[2022-09-19 18:57:06,222] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-70/special_tokens_map.json[0m
[32m[2022-09-19 18:57:12,946] [    INFO][0m - loss: 0.02148875, learning_rate: 1.8e-05, global_step: 80, interval_runtime: 10.0535, interval_samples_per_second: 1.591, interval_steps_per_second: 0.995, epoch: 8.0[0m
[32m[2022-09-19 18:57:12,947] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 18:57:12,947] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 18:57:12,947] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 18:57:12,947] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 18:57:12,947] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 18:57:13,701] [    INFO][0m - eval_loss: 1.6315838098526, eval_accuracy: 0.7, eval_runtime: 0.7532, eval_samples_per_second: 212.439, eval_steps_per_second: 13.277, epoch: 8.0[0m
[32m[2022-09-19 18:57:13,701] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-80[0m
[32m[2022-09-19 18:57:13,701] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 18:57:16,140] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-80/tokenizer_config.json[0m
[32m[2022-09-19 18:57:16,140] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-80/special_tokens_map.json[0m
[32m[2022-09-19 18:57:23,928] [    INFO][0m - loss: 0.00347973, learning_rate: 1.65e-05, global_step: 90, interval_runtime: 10.9821, interval_samples_per_second: 1.457, interval_steps_per_second: 0.911, epoch: 9.0[0m
[32m[2022-09-19 18:57:23,929] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 18:57:23,929] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 18:57:23,929] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 18:57:23,929] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 18:57:23,929] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 18:57:24,679] [    INFO][0m - eval_loss: 2.257808208465576, eval_accuracy: 0.71875, eval_runtime: 0.7496, eval_samples_per_second: 213.455, eval_steps_per_second: 13.341, epoch: 9.0[0m
[32m[2022-09-19 18:57:24,679] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-90[0m
[32m[2022-09-19 18:57:24,679] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 18:57:27,218] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-90/tokenizer_config.json[0m
[32m[2022-09-19 18:57:27,219] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-90/special_tokens_map.json[0m
[32m[2022-09-19 18:57:36,557] [    INFO][0m - loss: 0.05793204, learning_rate: 1.5e-05, global_step: 100, interval_runtime: 12.6287, interval_samples_per_second: 1.267, interval_steps_per_second: 0.792, epoch: 10.0[0m
[32m[2022-09-19 18:57:36,558] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 18:57:36,558] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 18:57:36,558] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 18:57:36,558] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 18:57:36,558] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 18:57:38,136] [    INFO][0m - eval_loss: 1.8856041431427002, eval_accuracy: 0.68125, eval_runtime: 0.7475, eval_samples_per_second: 214.056, eval_steps_per_second: 13.379, epoch: 10.0[0m
[32m[2022-09-19 18:57:38,136] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-100[0m
[32m[2022-09-19 18:57:38,136] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 18:57:40,560] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-19 18:57:40,561] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-19 18:57:47,270] [    INFO][0m - loss: 0.00031037, learning_rate: 1.3500000000000001e-05, global_step: 110, interval_runtime: 10.7128, interval_samples_per_second: 1.494, interval_steps_per_second: 0.933, epoch: 11.0[0m
[32m[2022-09-19 18:57:47,270] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 18:57:47,270] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 18:57:47,270] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 18:57:47,270] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 18:57:47,271] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 18:57:48,011] [    INFO][0m - eval_loss: 1.409536361694336, eval_accuracy: 0.7, eval_runtime: 0.7408, eval_samples_per_second: 215.993, eval_steps_per_second: 13.5, epoch: 11.0[0m
[32m[2022-09-19 18:57:48,012] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-110[0m
[32m[2022-09-19 18:57:48,012] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 18:57:51,443] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-110/tokenizer_config.json[0m
[32m[2022-09-19 18:57:51,444] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-110/special_tokens_map.json[0m
[32m[2022-09-19 18:57:58,187] [    INFO][0m - loss: 0.00520284, learning_rate: 1.2e-05, global_step: 120, interval_runtime: 10.917, interval_samples_per_second: 1.466, interval_steps_per_second: 0.916, epoch: 12.0[0m
[32m[2022-09-19 18:57:58,187] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 18:57:58,187] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 18:57:58,187] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 18:57:58,187] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 18:57:58,188] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 18:57:58,946] [    INFO][0m - eval_loss: 1.3543946743011475, eval_accuracy: 0.69375, eval_runtime: 0.7582, eval_samples_per_second: 211.037, eval_steps_per_second: 13.19, epoch: 12.0[0m
[32m[2022-09-19 18:57:58,946] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-120[0m
[32m[2022-09-19 18:57:58,946] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 18:58:01,362] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-120/tokenizer_config.json[0m
[32m[2022-09-19 18:58:01,363] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-120/special_tokens_map.json[0m
[32m[2022-09-19 18:58:08,033] [    INFO][0m - loss: 1.498e-05, learning_rate: 1.05e-05, global_step: 130, interval_runtime: 9.8464, interval_samples_per_second: 1.625, interval_steps_per_second: 1.016, epoch: 13.0[0m
[32m[2022-09-19 18:58:08,034] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 18:58:08,034] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 18:58:08,034] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 18:58:08,034] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 18:58:08,034] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 18:58:08,779] [    INFO][0m - eval_loss: 1.3890390396118164, eval_accuracy: 0.725, eval_runtime: 0.7445, eval_samples_per_second: 214.911, eval_steps_per_second: 13.432, epoch: 13.0[0m
[32m[2022-09-19 18:58:08,779] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-130[0m
[32m[2022-09-19 18:58:08,779] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 18:58:11,180] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-130/tokenizer_config.json[0m
[32m[2022-09-19 18:58:11,180] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-130/special_tokens_map.json[0m
[32m[2022-09-19 18:58:17,789] [    INFO][0m - loss: 1.801e-05, learning_rate: 9e-06, global_step: 140, interval_runtime: 9.7557, interval_samples_per_second: 1.64, interval_steps_per_second: 1.025, epoch: 14.0[0m
[32m[2022-09-19 18:58:17,789] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 18:58:17,789] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 18:58:17,790] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 18:58:17,790] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 18:58:17,790] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 18:58:18,535] [    INFO][0m - eval_loss: 1.4121421575546265, eval_accuracy: 0.73125, eval_runtime: 0.745, eval_samples_per_second: 214.754, eval_steps_per_second: 13.422, epoch: 14.0[0m
[32m[2022-09-19 18:58:19,797] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-140[0m
[32m[2022-09-19 18:58:19,797] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 18:58:22,220] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-140/tokenizer_config.json[0m
[32m[2022-09-19 18:58:22,221] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-140/special_tokens_map.json[0m
[32m[2022-09-19 18:58:28,855] [    INFO][0m - loss: 4.556e-05, learning_rate: 7.5e-06, global_step: 150, interval_runtime: 11.0661, interval_samples_per_second: 1.446, interval_steps_per_second: 0.904, epoch: 15.0[0m
[32m[2022-09-19 18:58:28,855] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 18:58:28,856] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 18:58:28,856] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 18:58:28,856] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 18:58:28,856] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 18:58:29,613] [    INFO][0m - eval_loss: 1.3965238332748413, eval_accuracy: 0.70625, eval_runtime: 0.7574, eval_samples_per_second: 211.24, eval_steps_per_second: 13.202, epoch: 15.0[0m
[32m[2022-09-19 18:58:29,614] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-150[0m
[32m[2022-09-19 18:58:29,614] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 18:58:32,169] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-150/tokenizer_config.json[0m
[32m[2022-09-19 18:58:32,170] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-150/special_tokens_map.json[0m
[32m[2022-09-19 18:58:39,410] [    INFO][0m - loss: 1.46e-06, learning_rate: 6e-06, global_step: 160, interval_runtime: 10.555, interval_samples_per_second: 1.516, interval_steps_per_second: 0.947, epoch: 16.0[0m
[32m[2022-09-19 18:58:39,411] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 18:58:39,411] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 18:58:39,411] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 18:58:39,411] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 18:58:39,411] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 18:58:40,183] [    INFO][0m - eval_loss: 1.3952205181121826, eval_accuracy: 0.69375, eval_runtime: 0.7715, eval_samples_per_second: 207.396, eval_steps_per_second: 12.962, epoch: 16.0[0m
[32m[2022-09-19 18:58:40,183] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-160[0m
[32m[2022-09-19 18:58:40,183] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 18:58:42,854] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-160/tokenizer_config.json[0m
[32m[2022-09-19 18:58:42,855] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-160/special_tokens_map.json[0m
[32m[2022-09-19 18:58:50,319] [    INFO][0m - loss: 6.13e-06, learning_rate: 4.5e-06, global_step: 170, interval_runtime: 10.9087, interval_samples_per_second: 1.467, interval_steps_per_second: 0.917, epoch: 17.0[0m
[32m[2022-09-19 18:58:50,319] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 18:58:50,320] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 18:58:50,320] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 18:58:50,320] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 18:58:50,320] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 18:58:51,074] [    INFO][0m - eval_loss: 1.3995507955551147, eval_accuracy: 0.69375, eval_runtime: 0.7542, eval_samples_per_second: 212.137, eval_steps_per_second: 13.259, epoch: 17.0[0m
[32m[2022-09-19 18:58:51,075] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-170[0m
[32m[2022-09-19 18:58:51,075] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 18:58:53,605] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-170/tokenizer_config.json[0m
[32m[2022-09-19 18:58:53,606] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-170/special_tokens_map.json[0m
[32m[2022-09-19 18:59:02,063] [    INFO][0m - loss: 2.23e-06, learning_rate: 3e-06, global_step: 180, interval_runtime: 11.7442, interval_samples_per_second: 1.362, interval_steps_per_second: 0.851, epoch: 18.0[0m
[32m[2022-09-19 18:59:02,064] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 18:59:02,064] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 18:59:02,064] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 18:59:02,064] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 18:59:02,064] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 18:59:02,815] [    INFO][0m - eval_loss: 1.403732180595398, eval_accuracy: 0.69375, eval_runtime: 0.7513, eval_samples_per_second: 212.975, eval_steps_per_second: 13.311, epoch: 18.0[0m
[32m[2022-09-19 18:59:02,816] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-180[0m
[32m[2022-09-19 18:59:02,816] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 18:59:05,599] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-180/tokenizer_config.json[0m
[32m[2022-09-19 18:59:05,599] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-180/special_tokens_map.json[0m
[32m[2022-09-19 18:59:14,301] [    INFO][0m - loss: 2.71e-06, learning_rate: 1.5e-06, global_step: 190, interval_runtime: 12.2384, interval_samples_per_second: 1.307, interval_steps_per_second: 0.817, epoch: 19.0[0m
[32m[2022-09-19 18:59:14,302] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 18:59:14,302] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 18:59:14,302] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 18:59:14,302] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 18:59:14,302] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 18:59:15,064] [    INFO][0m - eval_loss: 1.405362844467163, eval_accuracy: 0.69375, eval_runtime: 0.7619, eval_samples_per_second: 209.993, eval_steps_per_second: 13.125, epoch: 19.0[0m
[32m[2022-09-19 18:59:15,065] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-190[0m
[32m[2022-09-19 18:59:15,065] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 18:59:18,389] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-190/tokenizer_config.json[0m
[32m[2022-09-19 18:59:18,390] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-190/special_tokens_map.json[0m
[32m[2022-09-19 18:59:27,121] [    INFO][0m - loss: 1.73e-06, learning_rate: 0.0, global_step: 200, interval_runtime: 12.8193, interval_samples_per_second: 1.248, interval_steps_per_second: 0.78, epoch: 20.0[0m
[32m[2022-09-19 18:59:27,121] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 18:59:27,121] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 18:59:27,121] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 18:59:27,121] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 18:59:27,121] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 18:59:27,877] [    INFO][0m - eval_loss: 1.405709981918335, eval_accuracy: 0.69375, eval_runtime: 0.7557, eval_samples_per_second: 211.713, eval_steps_per_second: 13.232, epoch: 20.0[0m
[32m[2022-09-19 18:59:27,878] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-200[0m
[32m[2022-09-19 18:59:27,878] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 18:59:31,246] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-200/tokenizer_config.json[0m
[32m[2022-09-19 18:59:31,246] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-200/special_tokens_map.json[0m
[32m[2022-09-19 18:59:38,455] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-19 18:59:38,455] [    INFO][0m - Loading best model from ./checkpoints_bustm/checkpoint-140 (score: 0.73125).[0m
[32m[2022-09-19 18:59:40,018] [    INFO][0m - train_runtime: 232.7837, train_samples_per_second: 13.747, train_steps_per_second: 0.859, train_loss: 0.09969888975786034, epoch: 20.0[0m
[32m[2022-09-19 18:59:40,072] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/[0m
[32m[2022-09-19 18:59:40,072] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 18:59:43,499] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/tokenizer_config.json[0m
[32m[2022-09-19 18:59:43,499] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/special_tokens_map.json[0m
[32m[2022-09-19 18:59:43,501] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-19 18:59:43,501] [    INFO][0m -   epoch                    =       20.0[0m
[32m[2022-09-19 18:59:43,501] [    INFO][0m -   train_loss               =     0.0997[0m
[32m[2022-09-19 18:59:43,501] [    INFO][0m -   train_runtime            = 0:03:52.78[0m
[32m[2022-09-19 18:59:43,501] [    INFO][0m -   train_samples_per_second =     13.747[0m
[32m[2022-09-19 18:59:43,501] [    INFO][0m -   train_steps_per_second   =      0.859[0m
[32m[2022-09-19 18:59:43,504] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-19 18:59:43,504] [    INFO][0m -   Num examples = 1772[0m
[32m[2022-09-19 18:59:43,504] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 18:59:43,504] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 18:59:43,504] [    INFO][0m -   Total prediction steps = 111[0m
[32m[2022-09-19 18:59:52,731] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-19 18:59:52,731] [    INFO][0m -   test_accuracy           =      0.715[0m
[32m[2022-09-19 18:59:52,732] [    INFO][0m -   test_loss               =     1.3516[0m
[32m[2022-09-19 18:59:52,732] [    INFO][0m -   test_runtime            = 0:00:09.22[0m
[32m[2022-09-19 18:59:52,732] [    INFO][0m -   test_samples_per_second =    192.054[0m
[32m[2022-09-19 18:59:52,732] [    INFO][0m -   test_steps_per_second   =      12.03[0m
[32m[2022-09-19 18:59:52,732] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-19 18:59:52,732] [    INFO][0m -   Num examples = 2000[0m
[32m[2022-09-19 18:59:52,732] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 18:59:52,733] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 18:59:52,733] [    INFO][0m -   Total prediction steps = 125[0m
[32m[2022-09-19 19:00:04,688] [    INFO][0m - Predictions for bustm saved to ./fewclue_submit_examples.[0m
{
  "labels": 1,
  "text_a": "\u53eb\u7238\u7238\u53eb\u4e00\u58f0\u6211\u542c\u542c",
  "text_b": "\u90a3\u4f60\u53eb\u6211\u4e00\u58f0\u7238\u7238",
  "uid": 0
}

Prediction done.
 
==========
chid
==========
 
[32m[2022-09-19 19:00:27,171] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-19 19:00:27,171] [    INFO][0m - ============================================================[0m
[32m[2022-09-19 19:00:27,172] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-19 19:00:27,172] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-19 19:00:27,172] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-19 19:00:27,172] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-19 19:00:27,172] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-19 19:00:27,172] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-19 19:00:27,172] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-19 19:00:27,172] [    INFO][0m - [0m
[32m[2022-09-19 19:00:27,172] [    INFO][0m - ============================================================[0m
[32m[2022-09-19 19:00:27,172] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-19 19:00:27,172] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-19 19:00:27,172] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-19 19:00:27,172] [    INFO][0m - pretrained                    :/ssd2/wanghuijuan03/data/zero-shot/0919_model_state.pdparams[0m
[32m[2022-09-19 19:00:27,173] [    INFO][0m - prompt                        :â€œ{'text':'text_a'}â€è¿™å¥è¯ä¸­æˆè¯­[{'text':'text_b'}]çš„ç†è§£æ­£ç¡®å—ï¼Ÿ{'mask'}{'mask'}ã€‚[0m
[32m[2022-09-19 19:00:27,173] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-19 19:00:27,173] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-19 19:00:27,173] [    INFO][0m - task_name                     :chid[0m
[32m[2022-09-19 19:00:27,173] [    INFO][0m - [0m
[32m[2022-09-19 19:00:27,173] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0919 19:00:27.174614 74746 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0919 19:00:27.178690 74746 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-19 19:00:31,997] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-19 19:00:32,008] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-19 19:00:32,009] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-19 19:00:32,009] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': 'â€œ'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': 'â€è¿™å¥è¯ä¸­æˆè¯­['}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': ']çš„ç†è§£æ­£ç¡®å—ï¼Ÿ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'ã€‚'}][0m
[32m[2022-09-19 19:00:33,466] [    INFO][0m - ============================================================[0m
[32m[2022-09-19 19:00:33,466] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-19 19:00:33,466] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-19 19:00:33,466] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-19 19:00:33,467] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-19 19:00:33,467] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-19 19:00:33,467] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-19 19:00:33,467] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-19 19:00:33,467] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-19 19:00:33,467] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-19 19:00:33,467] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-19 19:00:33,467] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-19 19:00:33,467] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-19 19:00:33,467] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-19 19:00:33,467] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-19 19:00:33,467] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-19 19:00:33,468] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-19 19:00:33,468] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-19 19:00:33,468] [    INFO][0m - eval_batch_size               :16[0m
[32m[2022-09-19 19:00:33,468] [    INFO][0m - eval_steps                    :200[0m
[32m[2022-09-19 19:00:33,468] [    INFO][0m - evaluation_strategy           :IntervalStrategy.EPOCH[0m
[32m[2022-09-19 19:00:33,468] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-19 19:00:33,468] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-19 19:00:33,468] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-19 19:00:33,468] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-19 19:00:33,468] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-19 19:00:33,468] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-19 19:00:33,468] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-19 19:00:33,468] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-19 19:00:33,469] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-19 19:00:33,469] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-09-19 19:00:33,469] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-19 19:00:33,469] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-19 19:00:33,469] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-19 19:00:33,469] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-19 19:00:33,469] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-19 19:00:33,469] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-19 19:00:33,469] [    INFO][0m - logging_dir                   :./checkpoints_chid/runs/Sep19_19-00-27_instance-3bwob41y-01[0m
[32m[2022-09-19 19:00:33,469] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-19 19:00:33,469] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-19 19:00:33,469] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-19 19:00:33,469] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-19 19:00:33,469] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-19 19:00:33,470] [    INFO][0m - max_seq_length                :256[0m
[32m[2022-09-19 19:00:33,470] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-19 19:00:33,470] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-19 19:00:33,470] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-19 19:00:33,470] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-19 19:00:33,470] [    INFO][0m - num_train_epochs              :20.0[0m
[32m[2022-09-19 19:00:33,470] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-19 19:00:33,470] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-19 19:00:33,470] [    INFO][0m - output_dir                    :./checkpoints_chid/[0m
[32m[2022-09-19 19:00:33,470] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-19 19:00:33,470] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-19 19:00:33,470] [    INFO][0m - per_device_eval_batch_size    :16[0m
[32m[2022-09-19 19:00:33,470] [    INFO][0m - per_device_train_batch_size   :16[0m
[32m[2022-09-19 19:00:33,471] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-19 19:00:33,471] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-19 19:00:33,471] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-19 19:00:33,471] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-09-19 19:00:33,471] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-19 19:00:33,471] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-19 19:00:33,471] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-19 19:00:33,471] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-19 19:00:33,471] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-19 19:00:33,471] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-19 19:00:33,471] [    INFO][0m - run_name                      :./checkpoints_chid/[0m
[32m[2022-09-19 19:00:33,471] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-19 19:00:33,471] [    INFO][0m - save_steps                    :200[0m
[32m[2022-09-19 19:00:33,471] [    INFO][0m - save_strategy                 :IntervalStrategy.EPOCH[0m
[32m[2022-09-19 19:00:33,472] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-19 19:00:33,472] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-19 19:00:33,472] [    INFO][0m - seed                          :42[0m
[32m[2022-09-19 19:00:33,472] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-19 19:00:33,472] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-19 19:00:33,472] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-19 19:00:33,472] [    INFO][0m - train_batch_size              :16[0m
[32m[2022-09-19 19:00:33,472] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-19 19:00:33,472] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-19 19:00:33,472] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-19 19:00:33,472] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-19 19:00:33,472] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-19 19:00:33,472] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-19 19:00:33,472] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-19 19:00:33,473] [    INFO][0m - [0m
[32m[2022-09-19 19:00:33,476] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-19 19:00:33,476] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-19 19:00:33,476] [    INFO][0m -   Num Epochs = 20[0m
[32m[2022-09-19 19:00:33,476] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2022-09-19 19:00:33,476] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2022-09-19 19:00:33,476] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-19 19:00:33,476] [    INFO][0m -   Total optimization steps = 1780.0[0m
[32m[2022-09-19 19:00:33,476] [    INFO][0m -   Total num train samples = 28280[0m
[32m[2022-09-19 19:00:40,885] [    INFO][0m - loss: 0.95180721, learning_rate: 2.9831460674157305e-05, global_step: 10, interval_runtime: 7.4076, interval_samples_per_second: 2.16, interval_steps_per_second: 1.35, epoch: 0.1124[0m
[32m[2022-09-19 19:00:47,234] [    INFO][0m - loss: 0.49224367, learning_rate: 2.9662921348314606e-05, global_step: 20, interval_runtime: 6.3485, interval_samples_per_second: 2.52, interval_steps_per_second: 1.575, epoch: 0.2247[0m
[32m[2022-09-19 19:00:53,583] [    INFO][0m - loss: 0.32133622, learning_rate: 2.949438202247191e-05, global_step: 30, interval_runtime: 6.3499, interval_samples_per_second: 2.52, interval_steps_per_second: 1.575, epoch: 0.3371[0m
[32m[2022-09-19 19:00:59,919] [    INFO][0m - loss: 0.62915659, learning_rate: 2.932584269662921e-05, global_step: 40, interval_runtime: 6.3358, interval_samples_per_second: 2.525, interval_steps_per_second: 1.578, epoch: 0.4494[0m
[32m[2022-09-19 19:01:06,276] [    INFO][0m - loss: 0.45853224, learning_rate: 2.915730337078652e-05, global_step: 50, interval_runtime: 6.3569, interval_samples_per_second: 2.517, interval_steps_per_second: 1.573, epoch: 0.5618[0m
[32m[2022-09-19 19:01:12,638] [    INFO][0m - loss: 0.40620961, learning_rate: 2.8988764044943823e-05, global_step: 60, interval_runtime: 6.3623, interval_samples_per_second: 2.515, interval_steps_per_second: 1.572, epoch: 0.6742[0m
[32m[2022-09-19 19:01:19,005] [    INFO][0m - loss: 0.46132216, learning_rate: 2.8820224719101124e-05, global_step: 70, interval_runtime: 6.3663, interval_samples_per_second: 2.513, interval_steps_per_second: 1.571, epoch: 0.7865[0m
[32m[2022-09-19 19:01:25,358] [    INFO][0m - loss: 0.49211807, learning_rate: 2.865168539325843e-05, global_step: 80, interval_runtime: 6.3536, interval_samples_per_second: 2.518, interval_steps_per_second: 1.574, epoch: 0.8989[0m
[32m[2022-09-19 19:01:30,614] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:01:30,614] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-19 19:01:30,614] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:01:30,615] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:01:30,615] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-19 19:01:52,914] [    INFO][0m - eval_loss: 0.3965844213962555, eval_accuracy: 0.504950495049505, eval_runtime: 22.2991, eval_samples_per_second: 63.411, eval_steps_per_second: 3.991, epoch: 1.0[0m
[32m[2022-09-19 19:01:52,938] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-89[0m
[32m[2022-09-19 19:01:52,939] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:01:55,946] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-89/tokenizer_config.json[0m
[32m[2022-09-19 19:01:55,947] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-89/special_tokens_map.json[0m
[32m[2022-09-19 19:02:01,880] [    INFO][0m - loss: 0.42962284, learning_rate: 2.848314606741573e-05, global_step: 90, interval_runtime: 36.5217, interval_samples_per_second: 0.438, interval_steps_per_second: 0.274, epoch: 1.0112[0m
[32m[2022-09-19 19:02:08,238] [    INFO][0m - loss: 0.43343911, learning_rate: 2.8314606741573034e-05, global_step: 100, interval_runtime: 6.3578, interval_samples_per_second: 2.517, interval_steps_per_second: 1.573, epoch: 1.1236[0m
[32m[2022-09-19 19:02:14,609] [    INFO][0m - loss: 0.41505718, learning_rate: 2.8146067415730338e-05, global_step: 110, interval_runtime: 6.3715, interval_samples_per_second: 2.511, interval_steps_per_second: 1.569, epoch: 1.236[0m
[32m[2022-09-19 19:02:21,066] [    INFO][0m - loss: 0.46642346, learning_rate: 2.797752808988764e-05, global_step: 120, interval_runtime: 6.457, interval_samples_per_second: 2.478, interval_steps_per_second: 1.549, epoch: 1.3483[0m
[32m[2022-09-19 19:02:27,460] [    INFO][0m - loss: 0.47079906, learning_rate: 2.7808988764044946e-05, global_step: 130, interval_runtime: 6.3938, interval_samples_per_second: 2.502, interval_steps_per_second: 1.564, epoch: 1.4607[0m
[32m[2022-09-19 19:02:33,826] [    INFO][0m - loss: 0.44004107, learning_rate: 2.7640449438202247e-05, global_step: 140, interval_runtime: 6.3665, interval_samples_per_second: 2.513, interval_steps_per_second: 1.571, epoch: 1.573[0m
[32m[2022-09-19 19:02:40,204] [    INFO][0m - loss: 0.40635047, learning_rate: 2.7471910112359552e-05, global_step: 150, interval_runtime: 6.3773, interval_samples_per_second: 2.509, interval_steps_per_second: 1.568, epoch: 1.6854[0m
[32m[2022-09-19 19:02:46,586] [    INFO][0m - loss: 0.35032156, learning_rate: 2.7303370786516856e-05, global_step: 160, interval_runtime: 6.3827, interval_samples_per_second: 2.507, interval_steps_per_second: 1.567, epoch: 1.7978[0m
[32m[2022-09-19 19:02:52,976] [    INFO][0m - loss: 0.4599122, learning_rate: 2.7134831460674157e-05, global_step: 170, interval_runtime: 6.3895, interval_samples_per_second: 2.504, interval_steps_per_second: 1.565, epoch: 1.9101[0m
[32m[2022-09-19 19:02:57,625] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:02:57,626] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-19 19:02:57,626] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:02:57,626] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:02:57,626] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-19 19:03:19,932] [    INFO][0m - eval_loss: 0.3442285358905792, eval_accuracy: 0.6534653465346535, eval_runtime: 22.3056, eval_samples_per_second: 63.392, eval_steps_per_second: 3.99, epoch: 2.0[0m
[32m[2022-09-19 19:03:19,957] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-178[0m
[32m[2022-09-19 19:03:19,957] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:03:22,806] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-178/tokenizer_config.json[0m
[32m[2022-09-19 19:03:22,806] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-178/special_tokens_map.json[0m
[32m[2022-09-19 19:03:29,547] [    INFO][0m - loss: 0.38434196, learning_rate: 2.696629213483146e-05, global_step: 180, interval_runtime: 36.5714, interval_samples_per_second: 0.438, interval_steps_per_second: 0.273, epoch: 2.0225[0m
[32m[2022-09-19 19:03:35,923] [    INFO][0m - loss: 0.31301997, learning_rate: 2.6797752808988762e-05, global_step: 190, interval_runtime: 6.3752, interval_samples_per_second: 2.51, interval_steps_per_second: 1.569, epoch: 2.1348[0m
[32m[2022-09-19 19:03:42,305] [    INFO][0m - loss: 0.26135192, learning_rate: 2.6629213483146066e-05, global_step: 200, interval_runtime: 6.3825, interval_samples_per_second: 2.507, interval_steps_per_second: 1.567, epoch: 2.2472[0m
[32m[2022-09-19 19:03:48,694] [    INFO][0m - loss: 0.35270121, learning_rate: 2.6460674157303374e-05, global_step: 210, interval_runtime: 6.3888, interval_samples_per_second: 2.504, interval_steps_per_second: 1.565, epoch: 2.3596[0m
[32m[2022-09-19 19:03:55,089] [    INFO][0m - loss: 0.39237127, learning_rate: 2.6292134831460675e-05, global_step: 220, interval_runtime: 6.3956, interval_samples_per_second: 2.502, interval_steps_per_second: 1.564, epoch: 2.4719[0m
[32m[2022-09-19 19:04:02,665] [    INFO][0m - loss: 0.34611957, learning_rate: 2.612359550561798e-05, global_step: 230, interval_runtime: 6.3949, interval_samples_per_second: 2.502, interval_steps_per_second: 1.564, epoch: 2.5843[0m
[32m[2022-09-19 19:04:09,055] [    INFO][0m - loss: 0.33028862, learning_rate: 2.595505617977528e-05, global_step: 240, interval_runtime: 7.5708, interval_samples_per_second: 2.113, interval_steps_per_second: 1.321, epoch: 2.6966[0m
[32m[2022-09-19 19:04:15,447] [    INFO][0m - loss: 0.25124588, learning_rate: 2.5786516853932585e-05, global_step: 250, interval_runtime: 6.3914, interval_samples_per_second: 2.503, interval_steps_per_second: 1.565, epoch: 2.809[0m
[32m[2022-09-19 19:04:21,866] [    INFO][0m - loss: 0.20431793, learning_rate: 2.561797752808989e-05, global_step: 260, interval_runtime: 6.4192, interval_samples_per_second: 2.493, interval_steps_per_second: 1.558, epoch: 2.9213[0m
[32m[2022-09-19 19:04:25,883] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:04:25,883] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-19 19:04:25,883] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:04:25,883] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:04:25,883] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-19 19:04:48,443] [    INFO][0m - eval_loss: 0.3380882441997528, eval_accuracy: 0.7425742574257426, eval_runtime: 22.5594, eval_samples_per_second: 62.679, eval_steps_per_second: 3.945, epoch: 3.0[0m
[32m[2022-09-19 19:04:48,467] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-267[0m
[32m[2022-09-19 19:04:48,468] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:04:51,171] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-267/tokenizer_config.json[0m
[32m[2022-09-19 19:04:51,172] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-267/special_tokens_map.json[0m
[32m[2022-09-19 19:04:58,305] [    INFO][0m - loss: 0.29768388, learning_rate: 2.544943820224719e-05, global_step: 270, interval_runtime: 36.439, interval_samples_per_second: 0.439, interval_steps_per_second: 0.274, epoch: 3.0337[0m
[32m[2022-09-19 19:05:04,694] [    INFO][0m - loss: 0.15781608, learning_rate: 2.5280898876404494e-05, global_step: 280, interval_runtime: 6.3895, interval_samples_per_second: 2.504, interval_steps_per_second: 1.565, epoch: 3.1461[0m
[32m[2022-09-19 19:05:11,110] [    INFO][0m - loss: 0.37029493, learning_rate: 2.51123595505618e-05, global_step: 290, interval_runtime: 6.4156, interval_samples_per_second: 2.494, interval_steps_per_second: 1.559, epoch: 3.2584[0m
[32m[2022-09-19 19:05:17,530] [    INFO][0m - loss: 0.39124022, learning_rate: 2.4943820224719103e-05, global_step: 300, interval_runtime: 6.4207, interval_samples_per_second: 2.492, interval_steps_per_second: 1.557, epoch: 3.3708[0m
[32m[2022-09-19 19:05:23,953] [    INFO][0m - loss: 0.23898749, learning_rate: 2.4775280898876407e-05, global_step: 310, interval_runtime: 6.4227, interval_samples_per_second: 2.491, interval_steps_per_second: 1.557, epoch: 3.4831[0m
[32m[2022-09-19 19:05:30,472] [    INFO][0m - loss: 0.14333421, learning_rate: 2.4606741573033708e-05, global_step: 320, interval_runtime: 6.5181, interval_samples_per_second: 2.455, interval_steps_per_second: 1.534, epoch: 3.5955[0m
[32m[2022-09-19 19:05:36,919] [    INFO][0m - loss: 0.24294887, learning_rate: 2.4438202247191012e-05, global_step: 330, interval_runtime: 6.4481, interval_samples_per_second: 2.481, interval_steps_per_second: 1.551, epoch: 3.7079[0m
[32m[2022-09-19 19:05:43,346] [    INFO][0m - loss: 0.15023671, learning_rate: 2.4269662921348313e-05, global_step: 340, interval_runtime: 6.4261, interval_samples_per_second: 2.49, interval_steps_per_second: 1.556, epoch: 3.8202[0m
[32m[2022-09-19 19:05:49,775] [    INFO][0m - loss: 0.14123989, learning_rate: 2.4101123595505617e-05, global_step: 350, interval_runtime: 6.429, interval_samples_per_second: 2.489, interval_steps_per_second: 1.555, epoch: 3.9326[0m
[32m[2022-09-19 19:05:53,158] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:05:53,158] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-19 19:05:53,158] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:05:53,158] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:05:53,158] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-19 19:06:15,493] [    INFO][0m - eval_loss: 0.7605471611022949, eval_accuracy: 0.6534653465346535, eval_runtime: 22.3344, eval_samples_per_second: 63.31, eval_steps_per_second: 3.985, epoch: 4.0[0m
[32m[2022-09-19 19:06:15,518] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-356[0m
[32m[2022-09-19 19:06:15,518] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:06:18,227] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-356/tokenizer_config.json[0m
[32m[2022-09-19 19:06:18,228] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-356/special_tokens_map.json[0m
[32m[2022-09-19 19:06:26,021] [    INFO][0m - loss: 0.26634414, learning_rate: 2.393258426966292e-05, global_step: 360, interval_runtime: 36.2468, interval_samples_per_second: 0.441, interval_steps_per_second: 0.276, epoch: 4.0449[0m
[32m[2022-09-19 19:06:32,416] [    INFO][0m - loss: 0.03427757, learning_rate: 2.3764044943820226e-05, global_step: 370, interval_runtime: 6.3948, interval_samples_per_second: 2.502, interval_steps_per_second: 1.564, epoch: 4.1573[0m
[32m[2022-09-19 19:06:38,825] [    INFO][0m - loss: 0.10341756, learning_rate: 2.359550561797753e-05, global_step: 380, interval_runtime: 6.4093, interval_samples_per_second: 2.496, interval_steps_per_second: 1.56, epoch: 4.2697[0m
[32m[2022-09-19 19:06:45,254] [    INFO][0m - loss: 0.18148096, learning_rate: 2.342696629213483e-05, global_step: 390, interval_runtime: 6.4289, interval_samples_per_second: 2.489, interval_steps_per_second: 1.555, epoch: 4.382[0m
[32m[2022-09-19 19:06:51,691] [    INFO][0m - loss: 0.08607792, learning_rate: 2.3258426966292135e-05, global_step: 400, interval_runtime: 6.4372, interval_samples_per_second: 2.486, interval_steps_per_second: 1.553, epoch: 4.4944[0m
[32m[2022-09-19 19:06:58,149] [    INFO][0m - loss: 0.0804836, learning_rate: 2.308988764044944e-05, global_step: 410, interval_runtime: 6.4574, interval_samples_per_second: 2.478, interval_steps_per_second: 1.549, epoch: 4.6067[0m
[32m[2022-09-19 19:07:04,570] [    INFO][0m - loss: 0.21787453, learning_rate: 2.292134831460674e-05, global_step: 420, interval_runtime: 6.4208, interval_samples_per_second: 2.492, interval_steps_per_second: 1.557, epoch: 4.7191[0m
[32m[2022-09-19 19:07:10,988] [    INFO][0m - loss: 0.15308661, learning_rate: 2.2752808988764045e-05, global_step: 430, interval_runtime: 6.4188, interval_samples_per_second: 2.493, interval_steps_per_second: 1.558, epoch: 4.8315[0m
[32m[2022-09-19 19:07:17,388] [    INFO][0m - loss: 0.08639907, learning_rate: 2.2584269662921346e-05, global_step: 440, interval_runtime: 6.399, interval_samples_per_second: 2.5, interval_steps_per_second: 1.563, epoch: 4.9438[0m
[32m[2022-09-19 19:07:20,141] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:07:20,141] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-19 19:07:20,141] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:07:20,141] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:07:20,141] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-19 19:07:42,501] [    INFO][0m - eval_loss: 0.5596771836280823, eval_accuracy: 0.7326732673267327, eval_runtime: 22.3592, eval_samples_per_second: 63.24, eval_steps_per_second: 3.98, epoch: 5.0[0m
[32m[2022-09-19 19:07:42,526] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-445[0m
[32m[2022-09-19 19:07:42,526] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:07:45,348] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-445/tokenizer_config.json[0m
[32m[2022-09-19 19:07:45,349] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-445/special_tokens_map.json[0m
[32m[2022-09-19 19:07:54,131] [    INFO][0m - loss: 0.00866024, learning_rate: 2.2415730337078654e-05, global_step: 450, interval_runtime: 36.7431, interval_samples_per_second: 0.435, interval_steps_per_second: 0.272, epoch: 5.0562[0m
[32m[2022-09-19 19:08:00,531] [    INFO][0m - loss: 0.06836822, learning_rate: 2.2247191011235958e-05, global_step: 460, interval_runtime: 6.4, interval_samples_per_second: 2.5, interval_steps_per_second: 1.563, epoch: 5.1685[0m
[32m[2022-09-19 19:08:06,930] [    INFO][0m - loss: 0.05704492, learning_rate: 2.207865168539326e-05, global_step: 470, interval_runtime: 6.3977, interval_samples_per_second: 2.501, interval_steps_per_second: 1.563, epoch: 5.2809[0m
[32m[2022-09-19 19:08:13,347] [    INFO][0m - loss: 0.09234877, learning_rate: 2.1910112359550563e-05, global_step: 480, interval_runtime: 6.4185, interval_samples_per_second: 2.493, interval_steps_per_second: 1.558, epoch: 5.3933[0m
[32m[2022-09-19 19:08:19,761] [    INFO][0m - loss: 0.08776794, learning_rate: 2.1741573033707864e-05, global_step: 490, interval_runtime: 6.4139, interval_samples_per_second: 2.495, interval_steps_per_second: 1.559, epoch: 5.5056[0m
[32m[2022-09-19 19:08:26,190] [    INFO][0m - loss: 0.19763508, learning_rate: 2.1573033707865168e-05, global_step: 500, interval_runtime: 6.429, interval_samples_per_second: 2.489, interval_steps_per_second: 1.555, epoch: 5.618[0m
[32m[2022-09-19 19:08:32,609] [    INFO][0m - loss: 0.05474946, learning_rate: 2.1404494382022473e-05, global_step: 510, interval_runtime: 6.4197, interval_samples_per_second: 2.492, interval_steps_per_second: 1.558, epoch: 5.7303[0m
[32m[2022-09-19 19:08:39,034] [    INFO][0m - loss: 0.11541271, learning_rate: 2.1235955056179773e-05, global_step: 520, interval_runtime: 6.4246, interval_samples_per_second: 2.49, interval_steps_per_second: 1.557, epoch: 5.8427[0m
[32m[2022-09-19 19:08:45,432] [    INFO][0m - loss: 0.06611423, learning_rate: 2.106741573033708e-05, global_step: 530, interval_runtime: 6.3979, interval_samples_per_second: 2.501, interval_steps_per_second: 1.563, epoch: 5.9551[0m
[32m[2022-09-19 19:08:47,564] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:08:47,564] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-19 19:08:47,564] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:08:47,564] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:08:47,565] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-19 19:09:09,969] [    INFO][0m - eval_loss: 0.6274575591087341, eval_accuracy: 0.6831683168316832, eval_runtime: 22.4036, eval_samples_per_second: 63.115, eval_steps_per_second: 3.973, epoch: 6.0[0m
[32m[2022-09-19 19:09:09,994] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-534[0m
[32m[2022-09-19 19:09:09,994] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:09:12,775] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-534/tokenizer_config.json[0m
[32m[2022-09-19 19:09:12,775] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-534/special_tokens_map.json[0m
[32m[2022-09-19 19:09:22,589] [    INFO][0m - loss: 0.03793779, learning_rate: 2.0898876404494382e-05, global_step: 540, interval_runtime: 37.1564, interval_samples_per_second: 0.431, interval_steps_per_second: 0.269, epoch: 6.0674[0m
[32m[2022-09-19 19:09:29,643] [    INFO][0m - loss: 0.03841417, learning_rate: 2.0730337078651686e-05, global_step: 550, interval_runtime: 6.4059, interval_samples_per_second: 2.498, interval_steps_per_second: 1.561, epoch: 6.1798[0m
[32m[2022-09-19 19:09:36,062] [    INFO][0m - loss: 0.02487746, learning_rate: 2.056179775280899e-05, global_step: 560, interval_runtime: 7.0672, interval_samples_per_second: 2.264, interval_steps_per_second: 1.415, epoch: 6.2921[0m
[32m[2022-09-19 19:09:42,483] [    INFO][0m - loss: 0.00426503, learning_rate: 2.039325842696629e-05, global_step: 570, interval_runtime: 6.4219, interval_samples_per_second: 2.491, interval_steps_per_second: 1.557, epoch: 6.4045[0m
[32m[2022-09-19 19:09:48,913] [    INFO][0m - loss: 0.02566117, learning_rate: 2.0224719101123596e-05, global_step: 580, interval_runtime: 6.4296, interval_samples_per_second: 2.488, interval_steps_per_second: 1.555, epoch: 6.5169[0m
[32m[2022-09-19 19:09:55,345] [    INFO][0m - loss: 0.00013526, learning_rate: 2.0056179775280897e-05, global_step: 590, interval_runtime: 6.4315, interval_samples_per_second: 2.488, interval_steps_per_second: 1.555, epoch: 6.6292[0m
[32m[2022-09-19 19:10:01,775] [    INFO][0m - loss: 0.09293851, learning_rate: 1.98876404494382e-05, global_step: 600, interval_runtime: 6.4303, interval_samples_per_second: 2.488, interval_steps_per_second: 1.555, epoch: 6.7416[0m
[32m[2022-09-19 19:10:08,207] [    INFO][0m - loss: 0.07205259, learning_rate: 1.971910112359551e-05, global_step: 610, interval_runtime: 6.432, interval_samples_per_second: 2.488, interval_steps_per_second: 1.555, epoch: 6.8539[0m
[32m[2022-09-19 19:10:14,578] [    INFO][0m - loss: 0.05534296, learning_rate: 1.955056179775281e-05, global_step: 620, interval_runtime: 6.371, interval_samples_per_second: 2.511, interval_steps_per_second: 1.57, epoch: 6.9663[0m
[32m[2022-09-19 19:10:16,078] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:10:16,078] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-19 19:10:16,079] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:10:16,079] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:10:16,079] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-19 19:10:38,444] [    INFO][0m - eval_loss: 0.7169905304908752, eval_accuracy: 0.6831683168316832, eval_runtime: 22.3653, eval_samples_per_second: 63.223, eval_steps_per_second: 3.979, epoch: 7.0[0m
[32m[2022-09-19 19:10:38,470] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-623[0m
[32m[2022-09-19 19:10:38,470] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:10:41,172] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-623/tokenizer_config.json[0m
[32m[2022-09-19 19:10:41,173] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-623/special_tokens_map.json[0m
[32m[2022-09-19 19:10:50,927] [    INFO][0m - loss: 0.01697746, learning_rate: 1.9382022471910114e-05, global_step: 630, interval_runtime: 36.3492, interval_samples_per_second: 0.44, interval_steps_per_second: 0.275, epoch: 7.0787[0m
[32m[2022-09-19 19:10:57,327] [    INFO][0m - loss: 0.01959302, learning_rate: 1.9213483146067415e-05, global_step: 640, interval_runtime: 6.3998, interval_samples_per_second: 2.5, interval_steps_per_second: 1.563, epoch: 7.191[0m
[32m[2022-09-19 19:11:03,732] [    INFO][0m - loss: 0.06683726, learning_rate: 1.904494382022472e-05, global_step: 650, interval_runtime: 6.4051, interval_samples_per_second: 2.498, interval_steps_per_second: 1.561, epoch: 7.3034[0m
[32m[2022-09-19 19:11:10,179] [    INFO][0m - loss: 0.00775923, learning_rate: 1.8876404494382024e-05, global_step: 660, interval_runtime: 6.4465, interval_samples_per_second: 2.482, interval_steps_per_second: 1.551, epoch: 7.4157[0m
[32m[2022-09-19 19:11:17,318] [    INFO][0m - loss: 0.06961555, learning_rate: 1.8707865168539324e-05, global_step: 670, interval_runtime: 6.4088, interval_samples_per_second: 2.497, interval_steps_per_second: 1.56, epoch: 7.5281[0m
[32m[2022-09-19 19:11:23,727] [    INFO][0m - loss: 0.00661244, learning_rate: 1.853932584269663e-05, global_step: 680, interval_runtime: 7.1397, interval_samples_per_second: 2.241, interval_steps_per_second: 1.401, epoch: 7.6404[0m
[32m[2022-09-19 19:11:30,145] [    INFO][0m - loss: 0.03189598, learning_rate: 1.8370786516853933e-05, global_step: 690, interval_runtime: 6.4182, interval_samples_per_second: 2.493, interval_steps_per_second: 1.558, epoch: 7.7528[0m
[32m[2022-09-19 19:11:36,562] [    INFO][0m - loss: 0.00027192, learning_rate: 1.8202247191011237e-05, global_step: 700, interval_runtime: 6.4171, interval_samples_per_second: 2.493, interval_steps_per_second: 1.558, epoch: 7.8652[0m
[32m[2022-09-19 19:11:42,915] [    INFO][0m - loss: 0.00022509, learning_rate: 1.803370786516854e-05, global_step: 710, interval_runtime: 6.3523, interval_samples_per_second: 2.519, interval_steps_per_second: 1.574, epoch: 7.9775[0m
[32m[2022-09-19 19:11:43,798] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:11:43,798] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-19 19:11:43,799] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:11:43,799] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:11:43,799] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-19 19:12:06,135] [    INFO][0m - eval_loss: 0.7999672293663025, eval_accuracy: 0.6881188118811881, eval_runtime: 22.3357, eval_samples_per_second: 63.307, eval_steps_per_second: 3.985, epoch: 8.0[0m
[32m[2022-09-19 19:12:06,161] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-712[0m
[32m[2022-09-19 19:12:06,161] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:12:08,848] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-712/tokenizer_config.json[0m
[32m[2022-09-19 19:12:08,849] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-712/special_tokens_map.json[0m
[32m[2022-09-19 19:12:19,198] [    INFO][0m - loss: 4.874e-05, learning_rate: 1.7865168539325843e-05, global_step: 720, interval_runtime: 36.2834, interval_samples_per_second: 0.441, interval_steps_per_second: 0.276, epoch: 8.0899[0m
[32m[2022-09-19 19:12:25,611] [    INFO][0m - loss: 0.0271915, learning_rate: 1.7696629213483147e-05, global_step: 730, interval_runtime: 6.413, interval_samples_per_second: 2.495, interval_steps_per_second: 1.559, epoch: 8.2022[0m
[32m[2022-09-19 19:12:32,008] [    INFO][0m - loss: 0.06619956, learning_rate: 1.7528089887640448e-05, global_step: 740, interval_runtime: 6.396, interval_samples_per_second: 2.502, interval_steps_per_second: 1.563, epoch: 8.3146[0m
[32m[2022-09-19 19:12:38,414] [    INFO][0m - loss: 0.0231836, learning_rate: 1.7359550561797752e-05, global_step: 750, interval_runtime: 6.4068, interval_samples_per_second: 2.497, interval_steps_per_second: 1.561, epoch: 8.427[0m
[32m[2022-09-19 19:12:44,805] [    INFO][0m - loss: 0.00020841, learning_rate: 1.7191011235955056e-05, global_step: 760, interval_runtime: 6.3906, interval_samples_per_second: 2.504, interval_steps_per_second: 1.565, epoch: 8.5393[0m
[32m[2022-09-19 19:12:51,213] [    INFO][0m - loss: 0.00015901, learning_rate: 1.702247191011236e-05, global_step: 770, interval_runtime: 6.4081, interval_samples_per_second: 2.497, interval_steps_per_second: 1.561, epoch: 8.6517[0m
[32m[2022-09-19 19:12:57,631] [    INFO][0m - loss: 0.00055698, learning_rate: 1.6853932584269665e-05, global_step: 780, interval_runtime: 6.4178, interval_samples_per_second: 2.493, interval_steps_per_second: 1.558, epoch: 8.764[0m
[32m[2022-09-19 19:13:04,047] [    INFO][0m - loss: 0.09398212, learning_rate: 1.6685393258426966e-05, global_step: 790, interval_runtime: 6.4168, interval_samples_per_second: 2.493, interval_steps_per_second: 1.558, epoch: 8.8764[0m
[32m[2022-09-19 19:13:10,390] [    INFO][0m - loss: 0.00076848, learning_rate: 1.651685393258427e-05, global_step: 800, interval_runtime: 6.3421, interval_samples_per_second: 2.523, interval_steps_per_second: 1.577, epoch: 8.9888[0m
[32m[2022-09-19 19:13:10,646] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:13:10,647] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-19 19:13:10,647] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:13:10,647] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:13:10,647] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-19 19:13:33,046] [    INFO][0m - eval_loss: 0.8326864242553711, eval_accuracy: 0.693069306930693, eval_runtime: 22.3992, eval_samples_per_second: 63.127, eval_steps_per_second: 3.973, epoch: 9.0[0m
[32m[2022-09-19 19:13:33,071] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-801[0m
[32m[2022-09-19 19:13:33,071] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:13:35,789] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-801/tokenizer_config.json[0m
[32m[2022-09-19 19:13:35,790] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-801/special_tokens_map.json[0m
[32m[2022-09-19 19:13:46,848] [    INFO][0m - loss: 0.04091526, learning_rate: 1.6348314606741574e-05, global_step: 810, interval_runtime: 36.4583, interval_samples_per_second: 0.439, interval_steps_per_second: 0.274, epoch: 9.1011[0m
[32m[2022-09-19 19:13:53,237] [    INFO][0m - loss: 0.00081995, learning_rate: 1.6179775280898875e-05, global_step: 820, interval_runtime: 6.3894, interval_samples_per_second: 2.504, interval_steps_per_second: 1.565, epoch: 9.2135[0m
[32m[2022-09-19 19:13:59,643] [    INFO][0m - loss: 0.00039086, learning_rate: 1.601123595505618e-05, global_step: 830, interval_runtime: 6.4054, interval_samples_per_second: 2.498, interval_steps_per_second: 1.561, epoch: 9.3258[0m
[32m[2022-09-19 19:14:06,038] [    INFO][0m - loss: 0.00489898, learning_rate: 1.5842696629213484e-05, global_step: 840, interval_runtime: 6.3959, interval_samples_per_second: 2.502, interval_steps_per_second: 1.564, epoch: 9.4382[0m
[32m[2022-09-19 19:14:12,446] [    INFO][0m - loss: 0.00013698, learning_rate: 1.5674157303370788e-05, global_step: 850, interval_runtime: 6.4074, interval_samples_per_second: 2.497, interval_steps_per_second: 1.561, epoch: 9.5506[0m
[32m[2022-09-19 19:14:18,851] [    INFO][0m - loss: 0.0001459, learning_rate: 1.5505617977528093e-05, global_step: 860, interval_runtime: 6.4052, interval_samples_per_second: 2.498, interval_steps_per_second: 1.561, epoch: 9.6629[0m
[32m[2022-09-19 19:14:25,264] [    INFO][0m - loss: 0.07136112, learning_rate: 1.5337078651685393e-05, global_step: 870, interval_runtime: 6.4124, interval_samples_per_second: 2.495, interval_steps_per_second: 1.559, epoch: 9.7753[0m
[32m[2022-09-19 19:14:31,675] [    INFO][0m - loss: 0.00164007, learning_rate: 1.5168539325842698e-05, global_step: 880, interval_runtime: 6.4113, interval_samples_per_second: 2.496, interval_steps_per_second: 1.56, epoch: 9.8876[0m
[32m[2022-09-19 19:14:37,629] [    INFO][0m - loss: 0.03104157, learning_rate: 1.5e-05, global_step: 890, interval_runtime: 5.9545, interval_samples_per_second: 2.687, interval_steps_per_second: 1.679, epoch: 10.0[0m
[32m[2022-09-19 19:14:37,630] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:14:37,630] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-19 19:14:37,630] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:14:37,630] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:14:37,630] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-19 19:14:59,953] [    INFO][0m - eval_loss: 0.9977447390556335, eval_accuracy: 0.7029702970297029, eval_runtime: 22.3219, eval_samples_per_second: 63.346, eval_steps_per_second: 3.987, epoch: 10.0[0m
[32m[2022-09-19 19:14:59,978] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-890[0m
[32m[2022-09-19 19:14:59,978] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:15:02,669] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-890/tokenizer_config.json[0m
[32m[2022-09-19 19:15:02,670] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-890/special_tokens_map.json[0m
[32m[2022-09-19 19:15:14,192] [    INFO][0m - loss: 0.0017292, learning_rate: 1.4831460674157303e-05, global_step: 900, interval_runtime: 36.563, interval_samples_per_second: 0.438, interval_steps_per_second: 0.274, epoch: 10.1124[0m
[32m[2022-09-19 19:15:20,893] [    INFO][0m - loss: 2.988e-05, learning_rate: 1.4662921348314606e-05, global_step: 910, interval_runtime: 6.4005, interval_samples_per_second: 2.5, interval_steps_per_second: 1.562, epoch: 10.2247[0m
[32m[2022-09-19 19:15:27,286] [    INFO][0m - loss: 0.01974347, learning_rate: 1.4494382022471912e-05, global_step: 920, interval_runtime: 6.6932, interval_samples_per_second: 2.39, interval_steps_per_second: 1.494, epoch: 10.3371[0m
[32m[2022-09-19 19:15:33,702] [    INFO][0m - loss: 0.0002476, learning_rate: 1.4325842696629214e-05, global_step: 930, interval_runtime: 6.4159, interval_samples_per_second: 2.494, interval_steps_per_second: 1.559, epoch: 10.4494[0m
[32m[2022-09-19 19:15:40,145] [    INFO][0m - loss: 0.09158302, learning_rate: 1.4157303370786517e-05, global_step: 940, interval_runtime: 6.4429, interval_samples_per_second: 2.483, interval_steps_per_second: 1.552, epoch: 10.5618[0m
[32m[2022-09-19 19:15:46,572] [    INFO][0m - loss: 7.852e-05, learning_rate: 1.398876404494382e-05, global_step: 950, interval_runtime: 6.4272, interval_samples_per_second: 2.489, interval_steps_per_second: 1.556, epoch: 10.6742[0m
[32m[2022-09-19 19:15:53,002] [    INFO][0m - loss: 0.00122213, learning_rate: 1.3820224719101124e-05, global_step: 960, interval_runtime: 6.4304, interval_samples_per_second: 2.488, interval_steps_per_second: 1.555, epoch: 10.7865[0m
[32m[2022-09-19 19:15:59,451] [    INFO][0m - loss: 2.264e-05, learning_rate: 1.3651685393258428e-05, global_step: 970, interval_runtime: 6.4489, interval_samples_per_second: 2.481, interval_steps_per_second: 1.551, epoch: 10.8989[0m
[32m[2022-09-19 19:16:04,761] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:16:04,761] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-19 19:16:04,761] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:16:04,761] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:16:04,762] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-19 19:16:27,106] [    INFO][0m - eval_loss: 1.043107509613037, eval_accuracy: 0.693069306930693, eval_runtime: 22.344, eval_samples_per_second: 63.283, eval_steps_per_second: 3.983, epoch: 11.0[0m
[32m[2022-09-19 19:16:27,131] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-979[0m
[32m[2022-09-19 19:16:27,131] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:16:29,813] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-979/tokenizer_config.json[0m
[32m[2022-09-19 19:16:29,813] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-979/special_tokens_map.json[0m
[32m[2022-09-19 19:16:35,536] [    INFO][0m - loss: 4.794e-05, learning_rate: 1.348314606741573e-05, global_step: 980, interval_runtime: 36.0848, interval_samples_per_second: 0.443, interval_steps_per_second: 0.277, epoch: 11.0112[0m
[32m[2022-09-19 19:16:43,849] [    INFO][0m - loss: 0.02180564, learning_rate: 1.3314606741573033e-05, global_step: 990, interval_runtime: 6.3847, interval_samples_per_second: 2.506, interval_steps_per_second: 1.566, epoch: 11.1236[0m
[32m[2022-09-19 19:16:50,265] [    INFO][0m - loss: 0.0006057, learning_rate: 1.3146067415730338e-05, global_step: 1000, interval_runtime: 8.3437, interval_samples_per_second: 1.918, interval_steps_per_second: 1.199, epoch: 11.236[0m
[32m[2022-09-19 19:16:56,675] [    INFO][0m - loss: 2.337e-05, learning_rate: 1.297752808988764e-05, global_step: 1010, interval_runtime: 6.4105, interval_samples_per_second: 2.496, interval_steps_per_second: 1.56, epoch: 11.3483[0m
[32m[2022-09-19 19:17:03,080] [    INFO][0m - loss: 1.689e-05, learning_rate: 1.2808988764044944e-05, global_step: 1020, interval_runtime: 6.4051, interval_samples_per_second: 2.498, interval_steps_per_second: 1.561, epoch: 11.4607[0m
[32m[2022-09-19 19:17:09,501] [    INFO][0m - loss: 1.535e-05, learning_rate: 1.2640449438202247e-05, global_step: 1030, interval_runtime: 6.4205, interval_samples_per_second: 2.492, interval_steps_per_second: 1.558, epoch: 11.573[0m
[32m[2022-09-19 19:17:15,919] [    INFO][0m - loss: 2.518e-05, learning_rate: 1.2471910112359551e-05, global_step: 1040, interval_runtime: 6.4185, interval_samples_per_second: 2.493, interval_steps_per_second: 1.558, epoch: 11.6854[0m
[32m[2022-09-19 19:17:22,330] [    INFO][0m - loss: 0.08296317, learning_rate: 1.2303370786516854e-05, global_step: 1050, interval_runtime: 6.4104, interval_samples_per_second: 2.496, interval_steps_per_second: 1.56, epoch: 11.7978[0m
[32m[2022-09-19 19:17:28,763] [    INFO][0m - loss: 9.818e-05, learning_rate: 1.2134831460674157e-05, global_step: 1060, interval_runtime: 6.4331, interval_samples_per_second: 2.487, interval_steps_per_second: 1.554, epoch: 11.9101[0m
[32m[2022-09-19 19:17:33,438] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:17:33,439] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-19 19:17:33,439] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:17:33,439] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:17:33,439] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-19 19:17:55,740] [    INFO][0m - eval_loss: 0.6432197093963623, eval_accuracy: 0.7029702970297029, eval_runtime: 22.3014, eval_samples_per_second: 63.404, eval_steps_per_second: 3.991, epoch: 12.0[0m
[32m[2022-09-19 19:17:55,767] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-1068[0m
[32m[2022-09-19 19:17:55,767] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:17:58,437] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-1068/tokenizer_config.json[0m
[32m[2022-09-19 19:17:58,437] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-1068/special_tokens_map.json[0m
[32m[2022-09-19 19:18:04,651] [    INFO][0m - loss: 0.0002314, learning_rate: 1.196629213483146e-05, global_step: 1070, interval_runtime: 35.8883, interval_samples_per_second: 0.446, interval_steps_per_second: 0.279, epoch: 12.0225[0m
[32m[2022-09-19 19:18:11,070] [    INFO][0m - loss: 0.00214757, learning_rate: 1.1797752808988765e-05, global_step: 1080, interval_runtime: 6.4189, interval_samples_per_second: 2.493, interval_steps_per_second: 1.558, epoch: 12.1348[0m
[32m[2022-09-19 19:18:17,470] [    INFO][0m - loss: 0.01047842, learning_rate: 1.1629213483146068e-05, global_step: 1090, interval_runtime: 6.4005, interval_samples_per_second: 2.5, interval_steps_per_second: 1.562, epoch: 12.2472[0m
[32m[2022-09-19 19:18:23,880] [    INFO][0m - loss: 0.00011356, learning_rate: 1.146067415730337e-05, global_step: 1100, interval_runtime: 6.4092, interval_samples_per_second: 2.496, interval_steps_per_second: 1.56, epoch: 12.3596[0m
[32m[2022-09-19 19:18:30,292] [    INFO][0m - loss: 0.00023491, learning_rate: 1.1292134831460673e-05, global_step: 1110, interval_runtime: 6.412, interval_samples_per_second: 2.495, interval_steps_per_second: 1.56, epoch: 12.4719[0m
[32m[2022-09-19 19:18:38,572] [    INFO][0m - loss: 0.00170785, learning_rate: 1.1123595505617979e-05, global_step: 1120, interval_runtime: 6.4144, interval_samples_per_second: 2.494, interval_steps_per_second: 1.559, epoch: 12.5843[0m
[32m[2022-09-19 19:18:44,982] [    INFO][0m - loss: 4.406e-05, learning_rate: 1.0955056179775282e-05, global_step: 1130, interval_runtime: 8.2753, interval_samples_per_second: 1.933, interval_steps_per_second: 1.208, epoch: 12.6966[0m
[32m[2022-09-19 19:18:51,390] [    INFO][0m - loss: 0.00116595, learning_rate: 1.0786516853932584e-05, global_step: 1140, interval_runtime: 6.4087, interval_samples_per_second: 2.497, interval_steps_per_second: 1.56, epoch: 12.809[0m
[32m[2022-09-19 19:18:57,808] [    INFO][0m - loss: 0.00010229, learning_rate: 1.0617977528089887e-05, global_step: 1150, interval_runtime: 6.4177, interval_samples_per_second: 2.493, interval_steps_per_second: 1.558, epoch: 12.9213[0m
[32m[2022-09-19 19:19:01,828] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:19:01,829] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-19 19:19:01,829] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:19:01,829] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:19:01,829] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-19 19:19:24,191] [    INFO][0m - eval_loss: 1.037065863609314, eval_accuracy: 0.698019801980198, eval_runtime: 22.3616, eval_samples_per_second: 63.233, eval_steps_per_second: 3.98, epoch: 13.0[0m
[32m[2022-09-19 19:19:24,216] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-1157[0m
[32m[2022-09-19 19:19:24,216] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:19:26,880] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-1157/tokenizer_config.json[0m
[32m[2022-09-19 19:19:26,880] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-1157/special_tokens_map.json[0m
[32m[2022-09-19 19:19:33,936] [    INFO][0m - loss: 0.00020207, learning_rate: 1.0449438202247191e-05, global_step: 1160, interval_runtime: 36.1283, interval_samples_per_second: 0.443, interval_steps_per_second: 0.277, epoch: 13.0337[0m
[32m[2022-09-19 19:19:40,333] [    INFO][0m - loss: 0.00915372, learning_rate: 1.0280898876404495e-05, global_step: 1170, interval_runtime: 6.3964, interval_samples_per_second: 2.501, interval_steps_per_second: 1.563, epoch: 13.1461[0m
[32m[2022-09-19 19:19:51,531] [    INFO][0m - loss: 1.78e-05, learning_rate: 1.0112359550561798e-05, global_step: 1180, interval_runtime: 6.4226, interval_samples_per_second: 2.491, interval_steps_per_second: 1.557, epoch: 13.2584[0m
[32m[2022-09-19 19:19:57,940] [    INFO][0m - loss: 1.089e-05, learning_rate: 9.9438202247191e-06, global_step: 1190, interval_runtime: 11.1851, interval_samples_per_second: 1.43, interval_steps_per_second: 0.894, epoch: 13.3708[0m
[32m[2022-09-19 19:20:04,366] [    INFO][0m - loss: 0.00049769, learning_rate: 9.775280898876405e-06, global_step: 1200, interval_runtime: 6.4254, interval_samples_per_second: 2.49, interval_steps_per_second: 1.556, epoch: 13.4831[0m
[32m[2022-09-19 19:20:10,790] [    INFO][0m - loss: 0.00394728, learning_rate: 9.606741573033707e-06, global_step: 1210, interval_runtime: 6.4243, interval_samples_per_second: 2.491, interval_steps_per_second: 1.557, epoch: 13.5955[0m
[32m[2022-09-19 19:20:17,229] [    INFO][0m - loss: 1.263e-05, learning_rate: 9.438202247191012e-06, global_step: 1220, interval_runtime: 6.4388, interval_samples_per_second: 2.485, interval_steps_per_second: 1.553, epoch: 13.7079[0m
[32m[2022-09-19 19:20:23,652] [    INFO][0m - loss: 1.053e-05, learning_rate: 9.269662921348314e-06, global_step: 1230, interval_runtime: 6.4233, interval_samples_per_second: 2.491, interval_steps_per_second: 1.557, epoch: 13.8202[0m
[32m[2022-09-19 19:20:30,078] [    INFO][0m - loss: 0.01655961, learning_rate: 9.101123595505619e-06, global_step: 1240, interval_runtime: 6.4253, interval_samples_per_second: 2.49, interval_steps_per_second: 1.556, epoch: 13.9326[0m
[32m[2022-09-19 19:20:33,468] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:20:33,468] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-19 19:20:33,468] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:20:33,468] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:20:33,468] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-19 19:20:55,841] [    INFO][0m - eval_loss: 1.0667738914489746, eval_accuracy: 0.7128712871287128, eval_runtime: 22.3725, eval_samples_per_second: 63.202, eval_steps_per_second: 3.978, epoch: 14.0[0m
[32m[2022-09-19 19:20:55,866] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-1246[0m
[32m[2022-09-19 19:20:55,867] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:20:58,524] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-1246/tokenizer_config.json[0m
[32m[2022-09-19 19:20:58,525] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-1246/special_tokens_map.json[0m
[32m[2022-09-19 19:21:06,196] [    INFO][0m - loss: 0.01349487, learning_rate: 8.932584269662921e-06, global_step: 1250, interval_runtime: 36.1182, interval_samples_per_second: 0.443, interval_steps_per_second: 0.277, epoch: 14.0449[0m
[32m[2022-09-19 19:21:14,465] [    INFO][0m - loss: 0.00530227, learning_rate: 8.764044943820224e-06, global_step: 1260, interval_runtime: 6.4296, interval_samples_per_second: 2.489, interval_steps_per_second: 1.555, epoch: 14.1573[0m
[32m[2022-09-19 19:21:20,876] [    INFO][0m - loss: 1.985e-05, learning_rate: 8.595505617977528e-06, global_step: 1270, interval_runtime: 8.2506, interval_samples_per_second: 1.939, interval_steps_per_second: 1.212, epoch: 14.2697[0m
[32m[2022-09-19 19:21:27,312] [    INFO][0m - loss: 0.02388138, learning_rate: 8.426966292134832e-06, global_step: 1280, interval_runtime: 6.4355, interval_samples_per_second: 2.486, interval_steps_per_second: 1.554, epoch: 14.382[0m
[32m[2022-09-19 19:21:33,740] [    INFO][0m - loss: 2.284e-05, learning_rate: 8.258426966292135e-06, global_step: 1290, interval_runtime: 6.4281, interval_samples_per_second: 2.489, interval_steps_per_second: 1.556, epoch: 14.4944[0m
[32m[2022-09-19 19:21:40,186] [    INFO][0m - loss: 1.547e-05, learning_rate: 8.089887640449438e-06, global_step: 1300, interval_runtime: 6.4461, interval_samples_per_second: 2.482, interval_steps_per_second: 1.551, epoch: 14.6067[0m
[32m[2022-09-19 19:21:46,616] [    INFO][0m - loss: 2.278e-05, learning_rate: 7.921348314606742e-06, global_step: 1310, interval_runtime: 6.4298, interval_samples_per_second: 2.488, interval_steps_per_second: 1.555, epoch: 14.7191[0m
[32m[2022-09-19 19:21:53,049] [    INFO][0m - loss: 1.773e-05, learning_rate: 7.752808988764046e-06, global_step: 1320, interval_runtime: 6.4336, interval_samples_per_second: 2.487, interval_steps_per_second: 1.554, epoch: 14.8315[0m
[32m[2022-09-19 19:21:59,470] [    INFO][0m - loss: 3.142e-05, learning_rate: 7.584269662921349e-06, global_step: 1330, interval_runtime: 6.4208, interval_samples_per_second: 2.492, interval_steps_per_second: 1.557, epoch: 14.9438[0m
[32m[2022-09-19 19:22:02,223] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:22:02,223] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-19 19:22:02,223] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:22:02,223] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:22:02,223] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-19 19:22:24,567] [    INFO][0m - eval_loss: 0.9944255352020264, eval_accuracy: 0.7079207920792079, eval_runtime: 22.3435, eval_samples_per_second: 63.284, eval_steps_per_second: 3.983, epoch: 15.0[0m
[32m[2022-09-19 19:22:24,592] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-1335[0m
[32m[2022-09-19 19:22:24,592] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:22:27,195] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-1335/tokenizer_config.json[0m
[32m[2022-09-19 19:22:27,195] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-1335/special_tokens_map.json[0m
[32m[2022-09-19 19:22:35,369] [    INFO][0m - loss: 1.524e-05, learning_rate: 7.4157303370786515e-06, global_step: 1340, interval_runtime: 35.8992, interval_samples_per_second: 0.446, interval_steps_per_second: 0.279, epoch: 15.0562[0m
[32m[2022-09-19 19:22:41,768] [    INFO][0m - loss: 0.01342316, learning_rate: 7.247191011235956e-06, global_step: 1350, interval_runtime: 6.3989, interval_samples_per_second: 2.5, interval_steps_per_second: 1.563, epoch: 15.1685[0m
[32m[2022-09-19 19:22:48,184] [    INFO][0m - loss: 1.347e-05, learning_rate: 7.078651685393258e-06, global_step: 1360, interval_runtime: 6.4153, interval_samples_per_second: 2.494, interval_steps_per_second: 1.559, epoch: 15.2809[0m
[32m[2022-09-19 19:22:54,617] [    INFO][0m - loss: 3.29e-05, learning_rate: 6.910112359550562e-06, global_step: 1370, interval_runtime: 6.4332, interval_samples_per_second: 2.487, interval_steps_per_second: 1.554, epoch: 15.3933[0m
[32m[2022-09-19 19:23:01,035] [    INFO][0m - loss: 1.491e-05, learning_rate: 6.741573033707865e-06, global_step: 1380, interval_runtime: 6.4176, interval_samples_per_second: 2.493, interval_steps_per_second: 1.558, epoch: 15.5056[0m
[32m[2022-09-19 19:23:07,495] [    INFO][0m - loss: 1.385e-05, learning_rate: 6.573033707865169e-06, global_step: 1390, interval_runtime: 6.4612, interval_samples_per_second: 2.476, interval_steps_per_second: 1.548, epoch: 15.618[0m
[32m[2022-09-19 19:23:13,915] [    INFO][0m - loss: 1.992e-05, learning_rate: 6.404494382022472e-06, global_step: 1400, interval_runtime: 6.4199, interval_samples_per_second: 2.492, interval_steps_per_second: 1.558, epoch: 15.7303[0m
[32m[2022-09-19 19:23:20,341] [    INFO][0m - loss: 0.0038988, learning_rate: 6.235955056179776e-06, global_step: 1410, interval_runtime: 6.4252, interval_samples_per_second: 2.49, interval_steps_per_second: 1.556, epoch: 15.8427[0m
[32m[2022-09-19 19:23:26,749] [    INFO][0m - loss: 2.566e-05, learning_rate: 6.067415730337078e-06, global_step: 1420, interval_runtime: 6.4084, interval_samples_per_second: 2.497, interval_steps_per_second: 1.56, epoch: 15.9551[0m
[32m[2022-09-19 19:23:28,881] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:23:28,881] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-19 19:23:28,881] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:23:28,881] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:23:28,881] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-19 19:23:51,293] [    INFO][0m - eval_loss: 0.9842814207077026, eval_accuracy: 0.7079207920792079, eval_runtime: 22.4113, eval_samples_per_second: 63.093, eval_steps_per_second: 3.971, epoch: 16.0[0m
[32m[2022-09-19 19:23:51,319] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-1424[0m
[32m[2022-09-19 19:23:51,319] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:23:54,016] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-1424/tokenizer_config.json[0m
[32m[2022-09-19 19:23:54,016] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-1424/special_tokens_map.json[0m
[32m[2022-09-19 19:24:02,792] [    INFO][0m - loss: 0.00213686, learning_rate: 5.8988764044943826e-06, global_step: 1430, interval_runtime: 36.0428, interval_samples_per_second: 0.444, interval_steps_per_second: 0.277, epoch: 16.0674[0m
[32m[2022-09-19 19:24:12,246] [    INFO][0m - loss: 0.01075712, learning_rate: 5.730337078651685e-06, global_step: 1440, interval_runtime: 6.3903, interval_samples_per_second: 2.504, interval_steps_per_second: 1.565, epoch: 16.1798[0m
[32m[2022-09-19 19:24:18,649] [    INFO][0m - loss: 1.462e-05, learning_rate: 5.5617977528089895e-06, global_step: 1450, interval_runtime: 9.4667, interval_samples_per_second: 1.69, interval_steps_per_second: 1.056, epoch: 16.2921[0m
[32m[2022-09-19 19:24:25,048] [    INFO][0m - loss: 1.442e-05, learning_rate: 5.393258426966292e-06, global_step: 1460, interval_runtime: 6.3987, interval_samples_per_second: 2.501, interval_steps_per_second: 1.563, epoch: 16.4045[0m
[32m[2022-09-19 19:24:31,457] [    INFO][0m - loss: 0.00101412, learning_rate: 5.2247191011235955e-06, global_step: 1470, interval_runtime: 6.4089, interval_samples_per_second: 2.497, interval_steps_per_second: 1.56, epoch: 16.5169[0m
[32m[2022-09-19 19:24:37,882] [    INFO][0m - loss: 1.099e-05, learning_rate: 5.056179775280899e-06, global_step: 1480, interval_runtime: 6.4253, interval_samples_per_second: 2.49, interval_steps_per_second: 1.556, epoch: 16.6292[0m
[32m[2022-09-19 19:24:44,298] [    INFO][0m - loss: 1.387e-05, learning_rate: 4.8876404494382024e-06, global_step: 1490, interval_runtime: 6.4163, interval_samples_per_second: 2.494, interval_steps_per_second: 1.559, epoch: 16.7416[0m
[32m[2022-09-19 19:24:50,716] [    INFO][0m - loss: 1.475e-05, learning_rate: 4.719101123595506e-06, global_step: 1500, interval_runtime: 6.4178, interval_samples_per_second: 2.493, interval_steps_per_second: 1.558, epoch: 16.8539[0m
[32m[2022-09-19 19:24:57,078] [    INFO][0m - loss: 2.368e-05, learning_rate: 4.550561797752809e-06, global_step: 1510, interval_runtime: 6.3619, interval_samples_per_second: 2.515, interval_steps_per_second: 1.572, epoch: 16.9663[0m
[32m[2022-09-19 19:24:58,582] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:24:58,583] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-19 19:24:58,583] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:24:58,583] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:24:58,583] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-19 19:25:20,968] [    INFO][0m - eval_loss: 1.0038058757781982, eval_accuracy: 0.7029702970297029, eval_runtime: 22.3852, eval_samples_per_second: 63.167, eval_steps_per_second: 3.976, epoch: 17.0[0m
[32m[2022-09-19 19:25:20,996] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-1513[0m
[32m[2022-09-19 19:25:20,996] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:25:23,556] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-1513/tokenizer_config.json[0m
[32m[2022-09-19 19:25:23,557] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-1513/special_tokens_map.json[0m
[32m[2022-09-19 19:25:32,970] [    INFO][0m - loss: 1.03e-05, learning_rate: 4.382022471910112e-06, global_step: 1520, interval_runtime: 35.8919, interval_samples_per_second: 0.446, interval_steps_per_second: 0.279, epoch: 17.0787[0m
[32m[2022-09-19 19:25:39,374] [    INFO][0m - loss: 0.00488518, learning_rate: 4.213483146067416e-06, global_step: 1530, interval_runtime: 6.4046, interval_samples_per_second: 2.498, interval_steps_per_second: 1.561, epoch: 17.191[0m
[32m[2022-09-19 19:25:45,777] [    INFO][0m - loss: 1.523e-05, learning_rate: 4.044943820224719e-06, global_step: 1540, interval_runtime: 6.4029, interval_samples_per_second: 2.499, interval_steps_per_second: 1.562, epoch: 17.3034[0m
[32m[2022-09-19 19:25:52,196] [    INFO][0m - loss: 8.7e-06, learning_rate: 3.876404494382023e-06, global_step: 1550, interval_runtime: 6.4189, interval_samples_per_second: 2.493, interval_steps_per_second: 1.558, epoch: 17.4157[0m
[32m[2022-09-19 19:25:58,630] [    INFO][0m - loss: 8.28e-06, learning_rate: 3.7078651685393257e-06, global_step: 1560, interval_runtime: 6.4342, interval_samples_per_second: 2.487, interval_steps_per_second: 1.554, epoch: 17.5281[0m
[32m[2022-09-19 19:26:05,050] [    INFO][0m - loss: 0.00902653, learning_rate: 3.539325842696629e-06, global_step: 1570, interval_runtime: 6.4189, interval_samples_per_second: 2.493, interval_steps_per_second: 1.558, epoch: 17.6404[0m
[32m[2022-09-19 19:26:11,462] [    INFO][0m - loss: 1.112e-05, learning_rate: 3.3707865168539327e-06, global_step: 1580, interval_runtime: 6.4126, interval_samples_per_second: 2.495, interval_steps_per_second: 1.559, epoch: 17.7528[0m
[32m[2022-09-19 19:26:17,875] [    INFO][0m - loss: 1.032e-05, learning_rate: 3.202247191011236e-06, global_step: 1590, interval_runtime: 6.4126, interval_samples_per_second: 2.495, interval_steps_per_second: 1.559, epoch: 17.8652[0m
[32m[2022-09-19 19:26:24,244] [    INFO][0m - loss: 2.229e-05, learning_rate: 3.033707865168539e-06, global_step: 1600, interval_runtime: 6.3692, interval_samples_per_second: 2.512, interval_steps_per_second: 1.57, epoch: 17.9775[0m
[32m[2022-09-19 19:26:25,123] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:26:25,123] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-19 19:26:25,124] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:26:25,124] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:26:25,124] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-19 19:26:47,462] [    INFO][0m - eval_loss: 1.0011051893234253, eval_accuracy: 0.7029702970297029, eval_runtime: 22.3375, eval_samples_per_second: 63.301, eval_steps_per_second: 3.984, epoch: 18.0[0m
[32m[2022-09-19 19:26:47,488] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-1602[0m
[32m[2022-09-19 19:26:47,488] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:26:50,062] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-1602/tokenizer_config.json[0m
[32m[2022-09-19 19:26:50,063] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-1602/special_tokens_map.json[0m
[32m[2022-09-19 19:27:00,034] [    INFO][0m - loss: 1.132e-05, learning_rate: 2.8651685393258426e-06, global_step: 1610, interval_runtime: 35.7901, interval_samples_per_second: 0.447, interval_steps_per_second: 0.279, epoch: 18.0899[0m
[32m[2022-09-19 19:27:06,426] [    INFO][0m - loss: 8.52e-06, learning_rate: 2.696629213483146e-06, global_step: 1620, interval_runtime: 6.3926, interval_samples_per_second: 2.503, interval_steps_per_second: 1.564, epoch: 18.2022[0m
[32m[2022-09-19 19:27:12,829] [    INFO][0m - loss: 7.93e-06, learning_rate: 2.5280898876404495e-06, global_step: 1630, interval_runtime: 6.4022, interval_samples_per_second: 2.499, interval_steps_per_second: 1.562, epoch: 18.3146[0m
[32m[2022-09-19 19:27:19,239] [    INFO][0m - loss: 0.00542386, learning_rate: 2.359550561797753e-06, global_step: 1640, interval_runtime: 6.4107, interval_samples_per_second: 2.496, interval_steps_per_second: 1.56, epoch: 18.427[0m
[32m[2022-09-19 19:27:28,783] [    INFO][0m - loss: 1.062e-05, learning_rate: 2.191011235955056e-06, global_step: 1650, interval_runtime: 6.417, interval_samples_per_second: 2.493, interval_steps_per_second: 1.558, epoch: 18.5393[0m
[32m[2022-09-19 19:27:35,174] [    INFO][0m - loss: 8.21e-06, learning_rate: 2.0224719101123594e-06, global_step: 1660, interval_runtime: 9.518, interval_samples_per_second: 1.681, interval_steps_per_second: 1.051, epoch: 18.6517[0m
[32m[2022-09-19 19:27:41,587] [    INFO][0m - loss: 1.078e-05, learning_rate: 1.8539325842696629e-06, global_step: 1670, interval_runtime: 6.4126, interval_samples_per_second: 2.495, interval_steps_per_second: 1.559, epoch: 18.764[0m
[32m[2022-09-19 19:27:48,023] [    INFO][0m - loss: 0.00895631, learning_rate: 1.6853932584269663e-06, global_step: 1680, interval_runtime: 6.4357, interval_samples_per_second: 2.486, interval_steps_per_second: 1.554, epoch: 18.8764[0m
[32m[2022-09-19 19:27:54,352] [    INFO][0m - loss: 1.226e-05, learning_rate: 1.5168539325842696e-06, global_step: 1690, interval_runtime: 6.329, interval_samples_per_second: 2.528, interval_steps_per_second: 1.58, epoch: 18.9888[0m
[32m[2022-09-19 19:27:54,608] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:27:54,608] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-19 19:27:54,608] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:27:54,608] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:27:54,608] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-19 19:28:16,926] [    INFO][0m - eval_loss: 1.0079522132873535, eval_accuracy: 0.698019801980198, eval_runtime: 22.3179, eval_samples_per_second: 63.357, eval_steps_per_second: 3.988, epoch: 19.0[0m
[32m[2022-09-19 19:28:16,951] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-1691[0m
[32m[2022-09-19 19:28:16,951] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:28:19,471] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-1691/tokenizer_config.json[0m
[32m[2022-09-19 19:28:19,471] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-1691/special_tokens_map.json[0m
[32m[2022-09-19 19:28:30,084] [    INFO][0m - loss: 9.25e-06, learning_rate: 1.348314606741573e-06, global_step: 1700, interval_runtime: 35.7321, interval_samples_per_second: 0.448, interval_steps_per_second: 0.28, epoch: 19.1011[0m
[32m[2022-09-19 19:28:36,475] [    INFO][0m - loss: 1.103e-05, learning_rate: 1.1797752808988765e-06, global_step: 1710, interval_runtime: 6.3914, interval_samples_per_second: 2.503, interval_steps_per_second: 1.565, epoch: 19.2135[0m
[32m[2022-09-19 19:28:42,894] [    INFO][0m - loss: 7.06e-06, learning_rate: 1.0112359550561797e-06, global_step: 1720, interval_runtime: 6.4185, interval_samples_per_second: 2.493, interval_steps_per_second: 1.558, epoch: 19.3258[0m
[32m[2022-09-19 19:28:49,297] [    INFO][0m - loss: 7.36e-06, learning_rate: 8.426966292134832e-07, global_step: 1730, interval_runtime: 6.4033, interval_samples_per_second: 2.499, interval_steps_per_second: 1.562, epoch: 19.4382[0m
[32m[2022-09-19 19:28:55,725] [    INFO][0m - loss: 1.077e-05, learning_rate: 6.741573033707865e-07, global_step: 1740, interval_runtime: 6.4281, interval_samples_per_second: 2.489, interval_steps_per_second: 1.556, epoch: 19.5506[0m
[32m[2022-09-19 19:29:02,151] [    INFO][0m - loss: 1.049e-05, learning_rate: 5.056179775280899e-07, global_step: 1750, interval_runtime: 6.4257, interval_samples_per_second: 2.49, interval_steps_per_second: 1.556, epoch: 19.6629[0m
[32m[2022-09-19 19:29:08,557] [    INFO][0m - loss: 0.00375446, learning_rate: 3.3707865168539325e-07, global_step: 1760, interval_runtime: 6.4064, interval_samples_per_second: 2.498, interval_steps_per_second: 1.561, epoch: 19.7753[0m
[32m[2022-09-19 19:29:14,977] [    INFO][0m - loss: 8.92e-06, learning_rate: 1.6853932584269663e-07, global_step: 1770, interval_runtime: 6.4199, interval_samples_per_second: 2.492, interval_steps_per_second: 1.558, epoch: 19.8876[0m
[32m[2022-09-19 19:29:20,932] [    INFO][0m - loss: 0.00553069, learning_rate: 0.0, global_step: 1780, interval_runtime: 5.9548, interval_samples_per_second: 2.687, interval_steps_per_second: 1.679, epoch: 20.0[0m
[32m[2022-09-19 19:29:20,932] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:29:20,932] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-19 19:29:20,933] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:29:20,933] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:29:20,933] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-19 19:29:43,226] [    INFO][0m - eval_loss: 1.0067788362503052, eval_accuracy: 0.7029702970297029, eval_runtime: 22.2934, eval_samples_per_second: 63.427, eval_steps_per_second: 3.992, epoch: 20.0[0m
[32m[2022-09-19 19:29:43,251] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-1780[0m
[32m[2022-09-19 19:29:43,251] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:29:45,796] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-1780/tokenizer_config.json[0m
[32m[2022-09-19 19:29:45,796] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-1780/special_tokens_map.json[0m
[32m[2022-09-19 19:29:50,437] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-19 19:29:50,437] [    INFO][0m - Loading best model from ./checkpoints_chid/checkpoint-267 (score: 0.7425742574257426).[0m
[32m[2022-09-19 19:29:51,839] [    INFO][0m - train_runtime: 1758.3622, train_samples_per_second: 16.083, train_steps_per_second: 1.012, train_loss: 0.0917179756638474, epoch: 20.0[0m
[32m[2022-09-19 19:29:51,854] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/[0m
[32m[2022-09-19 19:29:51,855] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:29:54,035] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/tokenizer_config.json[0m
[32m[2022-09-19 19:29:54,035] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/special_tokens_map.json[0m
[32m[2022-09-19 19:29:54,037] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-19 19:29:54,037] [    INFO][0m -   epoch                    =       20.0[0m
[32m[2022-09-19 19:29:54,037] [    INFO][0m -   train_loss               =     0.0917[0m
[32m[2022-09-19 19:29:54,037] [    INFO][0m -   train_runtime            = 0:29:18.36[0m
[32m[2022-09-19 19:29:54,037] [    INFO][0m -   train_samples_per_second =     16.083[0m
[32m[2022-09-19 19:29:54,037] [    INFO][0m -   train_steps_per_second   =      1.012[0m
[32m[2022-09-19 19:29:54,045] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-19 19:29:54,045] [    INFO][0m -   Num examples = 14014[0m
[32m[2022-09-19 19:29:54,045] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:29:54,045] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:29:54,045] [    INFO][0m -   Total prediction steps = 876[0m
[32m[2022-09-19 19:33:46,946] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-19 19:33:46,946] [    INFO][0m -   test_accuracy           =     0.7473[0m
[32m[2022-09-19 19:33:46,947] [    INFO][0m -   test_loss               =     0.3139[0m
[32m[2022-09-19 19:33:46,947] [    INFO][0m -   test_runtime            = 0:03:52.90[0m
[32m[2022-09-19 19:33:46,947] [    INFO][0m -   test_samples_per_second =     60.172[0m
[32m[2022-09-19 19:33:46,947] [    INFO][0m -   test_steps_per_second   =      3.761[0m
[32m[2022-09-19 19:33:46,947] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-19 19:33:46,947] [    INFO][0m -   Num examples = 14000[0m
[32m[2022-09-19 19:33:46,948] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:33:46,948] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:33:46,948] [    INFO][0m -   Total prediction steps = 875[0m
[32m[2022-09-19 19:37:52,339] [    INFO][0m - Predictions for chidf saved to ./fewclue_submit_examples.[0m
{
  "labels": 0,
  "text_a": "\u5927\u90e8\u5206\u7684\u5927\u529f\u544a\u6210\u90fd\u662f\u7531\u65e0\u6570\u4e2a\u9020\u8c23\u751f\u4e8b\u7d2f\u79ef\u800c\u6765,\u505a\u597d\u6bcf\u4e00\u4ef6\u5e94\u8be5\u505a\u597d\u7684\u5c0f\u4e8b\u7edd\u4e0d\u662f\u4e00\u4ef6\u5c0f\u4e8b\u3002 \u300a\u4f60\u597d\u674e\u7115\u82f1\u300b\u4e3b\u8981\u8bb2\u4e86\u8d3e\u73b2\u610f\u5916\u7a7f\u8d8a\u5230\u4e86\u6bcd\u4eb2\u5e74\u8f7b\u7684\u65f6\u5019,\u548c\u6bcd\u4eb2\u4e00\u8d77\u611f\u53d7\u4e86\u5979\u7684\u751f\u6d3b\u548c\u60c5\u611f\u3002\u8fd9\u6545\u4e8b\u662f\u6839\u636e\u8d3e\u73b2\u7684\u4eb2\u751f\u7ecf\u5386\u6539\u7f16\u800c\u6765, ...",
  "text_b": "",
  "uid": 24
}

Prediction done.
 
==========
csl
==========
 
[32m[2022-09-19 19:38:08,236] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-19 19:38:08,237] [    INFO][0m - ============================================================[0m
[32m[2022-09-19 19:38:08,237] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-19 19:38:08,237] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-19 19:38:08,237] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-19 19:38:08,237] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-19 19:38:08,237] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-19 19:38:08,237] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-19 19:38:08,237] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-19 19:38:08,237] [    INFO][0m - [0m
[32m[2022-09-19 19:38:08,238] [    INFO][0m - ============================================================[0m
[32m[2022-09-19 19:38:08,238] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-19 19:38:08,238] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-19 19:38:08,238] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-19 19:38:08,238] [    INFO][0m - pretrained                    :/ssd2/wanghuijuan03/data/zero-shot/0919_model_state.pdparams[0m
[32m[2022-09-19 19:38:08,238] [    INFO][0m - prompt                        :â€œ{'text':'text_a'}â€æœ¬æ–‡çš„å†…å®¹{'mask'}{'mask'}â€œ{'text':'text_b'}â€[0m
[32m[2022-09-19 19:38:08,238] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-19 19:38:08,238] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-19 19:38:08,238] [    INFO][0m - task_name                     :csl[0m
[32m[2022-09-19 19:38:08,238] [    INFO][0m - [0m
[32m[2022-09-19 19:38:08,239] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0919 19:38:08.240597 39967 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0919 19:38:08.244793 39967 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-19 19:38:13,197] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-19 19:38:13,208] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-19 19:38:13,209] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-19 19:38:13,209] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': 'â€œ'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': 'â€æœ¬æ–‡çš„å†…å®¹'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'â€œ'}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': 'â€'}][0m
[32m[2022-09-19 19:38:14,400] [    INFO][0m - ============================================================[0m
[32m[2022-09-19 19:38:14,400] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-19 19:38:14,400] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-19 19:38:14,400] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-19 19:38:14,400] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-19 19:38:14,400] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-19 19:38:14,400] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-19 19:38:14,400] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-19 19:38:14,400] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-19 19:38:14,401] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-19 19:38:14,401] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-19 19:38:14,401] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-19 19:38:14,401] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-19 19:38:14,401] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-19 19:38:14,401] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-19 19:38:14,401] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-19 19:38:14,401] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-19 19:38:14,401] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-19 19:38:14,401] [    INFO][0m - eval_batch_size               :16[0m
[32m[2022-09-19 19:38:14,401] [    INFO][0m - eval_steps                    :200[0m
[32m[2022-09-19 19:38:14,401] [    INFO][0m - evaluation_strategy           :IntervalStrategy.EPOCH[0m
[32m[2022-09-19 19:38:14,402] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-19 19:38:14,402] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-19 19:38:14,402] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-19 19:38:14,402] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-19 19:38:14,402] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-19 19:38:14,402] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-19 19:38:14,402] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-19 19:38:14,402] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-19 19:38:14,402] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-19 19:38:14,402] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-09-19 19:38:14,402] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-19 19:38:14,402] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-19 19:38:14,402] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-19 19:38:14,402] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-19 19:38:14,403] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-19 19:38:14,403] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-19 19:38:14,403] [    INFO][0m - logging_dir                   :./checkpoints_csl/runs/Sep19_19-38-08_instance-3bwob41y-01[0m
[32m[2022-09-19 19:38:14,403] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-19 19:38:14,403] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-19 19:38:14,403] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-19 19:38:14,403] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-19 19:38:14,403] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-19 19:38:14,403] [    INFO][0m - max_seq_length                :320[0m
[32m[2022-09-19 19:38:14,403] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-19 19:38:14,403] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-19 19:38:14,403] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-19 19:38:14,403] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-19 19:38:14,404] [    INFO][0m - num_train_epochs              :20.0[0m
[32m[2022-09-19 19:38:14,404] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-19 19:38:14,404] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-19 19:38:14,404] [    INFO][0m - output_dir                    :./checkpoints_csl/[0m
[32m[2022-09-19 19:38:14,404] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-19 19:38:14,404] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-19 19:38:14,404] [    INFO][0m - per_device_eval_batch_size    :16[0m
[32m[2022-09-19 19:38:14,404] [    INFO][0m - per_device_train_batch_size   :16[0m
[32m[2022-09-19 19:38:14,404] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-19 19:38:14,404] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-19 19:38:14,404] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-19 19:38:14,404] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-09-19 19:38:14,404] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-19 19:38:14,404] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-19 19:38:14,405] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-19 19:38:14,405] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-19 19:38:14,405] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-19 19:38:14,405] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-19 19:38:14,405] [    INFO][0m - run_name                      :./checkpoints_csl/[0m
[32m[2022-09-19 19:38:14,405] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-19 19:38:14,405] [    INFO][0m - save_steps                    :200[0m
[32m[2022-09-19 19:38:14,405] [    INFO][0m - save_strategy                 :IntervalStrategy.EPOCH[0m
[32m[2022-09-19 19:38:14,405] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-19 19:38:14,405] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-19 19:38:14,405] [    INFO][0m - seed                          :42[0m
[32m[2022-09-19 19:38:14,405] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-19 19:38:14,405] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-19 19:38:14,406] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-19 19:38:14,406] [    INFO][0m - train_batch_size              :16[0m
[32m[2022-09-19 19:38:14,406] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-19 19:38:14,406] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-19 19:38:14,406] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-19 19:38:14,406] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-19 19:38:14,406] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-19 19:38:14,406] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-19 19:38:14,406] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-19 19:38:14,406] [    INFO][0m - [0m
[32m[2022-09-19 19:38:14,409] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-19 19:38:14,409] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 19:38:14,409] [    INFO][0m -   Num Epochs = 20[0m
[32m[2022-09-19 19:38:14,409] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2022-09-19 19:38:14,410] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2022-09-19 19:38:14,410] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-19 19:38:14,410] [    INFO][0m -   Total optimization steps = 200.0[0m
[32m[2022-09-19 19:38:14,410] [    INFO][0m -   Total num train samples = 3200[0m
[33m[2022-09-19 19:38:14,600] [ WARNING][0m - Token indices sequence length is longer than the specified maximum sequence length for this model (608 > 512). Running this sequence through the model will result in indexing errors[0m
[32m[2022-09-19 19:38:23,220] [    INFO][0m - loss: 1.52244558, learning_rate: 2.8499999999999998e-05, global_step: 10, interval_runtime: 8.8091, interval_samples_per_second: 1.816, interval_steps_per_second: 1.135, epoch: 1.0[0m
[32m[2022-09-19 19:38:23,221] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:38:23,222] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 19:38:23,222] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:38:23,222] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:38:23,222] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 19:38:26,398] [    INFO][0m - eval_loss: 0.7706432342529297, eval_accuracy: 0.50625, eval_runtime: 3.1752, eval_samples_per_second: 50.39, eval_steps_per_second: 3.149, epoch: 1.0[0m
[32m[2022-09-19 19:38:26,399] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-10[0m
[32m[2022-09-19 19:38:26,399] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:38:30,172] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-10/tokenizer_config.json[0m
[32m[2022-09-19 19:38:30,173] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-10/special_tokens_map.json[0m
[32m[2022-09-19 19:38:44,663] [    INFO][0m - loss: 0.67821798, learning_rate: 2.7000000000000002e-05, global_step: 20, interval_runtime: 21.4428, interval_samples_per_second: 0.746, interval_steps_per_second: 0.466, epoch: 2.0[0m
[32m[2022-09-19 19:38:44,664] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:38:44,664] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 19:38:44,664] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:38:44,664] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:38:44,664] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 19:38:47,833] [    INFO][0m - eval_loss: 0.7802659273147583, eval_accuracy: 0.49375, eval_runtime: 3.1674, eval_samples_per_second: 50.514, eval_steps_per_second: 3.157, epoch: 2.0[0m
[32m[2022-09-19 19:38:47,833] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-20[0m
[32m[2022-09-19 19:38:47,833] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:38:50,753] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-20/tokenizer_config.json[0m
[32m[2022-09-19 19:38:50,753] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-20/special_tokens_map.json[0m
[32m[2022-09-19 19:39:03,837] [    INFO][0m - loss: 0.49439135, learning_rate: 2.55e-05, global_step: 30, interval_runtime: 19.1743, interval_samples_per_second: 0.834, interval_steps_per_second: 0.522, epoch: 3.0[0m
[32m[2022-09-19 19:39:03,838] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:39:03,838] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 19:39:03,838] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:39:03,838] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:39:03,838] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 19:39:07,028] [    INFO][0m - eval_loss: 0.7429043650627136, eval_accuracy: 0.5875, eval_runtime: 3.1884, eval_samples_per_second: 50.182, eval_steps_per_second: 3.136, epoch: 3.0[0m
[32m[2022-09-19 19:39:07,028] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-30[0m
[32m[2022-09-19 19:39:07,028] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:39:09,957] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-30/tokenizer_config.json[0m
[32m[2022-09-19 19:39:09,957] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-30/special_tokens_map.json[0m
[32m[2022-09-19 19:39:23,141] [    INFO][0m - loss: 0.33491306, learning_rate: 2.4e-05, global_step: 40, interval_runtime: 19.3043, interval_samples_per_second: 0.829, interval_steps_per_second: 0.518, epoch: 4.0[0m
[32m[2022-09-19 19:39:23,142] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:39:23,142] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 19:39:23,142] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:39:23,142] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:39:23,143] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 19:39:26,310] [    INFO][0m - eval_loss: 1.4879024028778076, eval_accuracy: 0.55625, eval_runtime: 3.1675, eval_samples_per_second: 50.512, eval_steps_per_second: 3.157, epoch: 4.0[0m
[32m[2022-09-19 19:39:26,311] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-40[0m
[32m[2022-09-19 19:39:26,311] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:39:28,955] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-40/tokenizer_config.json[0m
[32m[2022-09-19 19:39:28,956] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-40/special_tokens_map.json[0m
[32m[2022-09-19 19:39:41,994] [    INFO][0m - loss: 0.2540225, learning_rate: 2.25e-05, global_step: 50, interval_runtime: 18.852, interval_samples_per_second: 0.849, interval_steps_per_second: 0.53, epoch: 5.0[0m
[32m[2022-09-19 19:39:41,995] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:39:41,995] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 19:39:41,995] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:39:41,995] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:39:41,995] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 19:39:45,181] [    INFO][0m - eval_loss: 0.9445411562919617, eval_accuracy: 0.69375, eval_runtime: 3.1862, eval_samples_per_second: 50.216, eval_steps_per_second: 3.139, epoch: 5.0[0m
[32m[2022-09-19 19:39:45,182] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-50[0m
[32m[2022-09-19 19:39:45,182] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:39:48,972] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-50/tokenizer_config.json[0m
[32m[2022-09-19 19:39:48,973] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-50/special_tokens_map.json[0m
[32m[2022-09-19 19:40:02,511] [    INFO][0m - loss: 0.03940548, learning_rate: 2.1e-05, global_step: 60, interval_runtime: 20.5173, interval_samples_per_second: 0.78, interval_steps_per_second: 0.487, epoch: 6.0[0m
[32m[2022-09-19 19:40:02,512] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:40:02,512] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 19:40:02,512] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:40:02,512] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:40:02,512] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 19:40:05,687] [    INFO][0m - eval_loss: 1.436161756515503, eval_accuracy: 0.69375, eval_runtime: 3.1751, eval_samples_per_second: 50.392, eval_steps_per_second: 3.15, epoch: 6.0[0m
[32m[2022-09-19 19:40:05,688] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-60[0m
[32m[2022-09-19 19:40:05,688] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:40:08,235] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-60/tokenizer_config.json[0m
[32m[2022-09-19 19:40:08,235] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-60/special_tokens_map.json[0m
[32m[2022-09-19 19:40:21,167] [    INFO][0m - loss: 0.01012647, learning_rate: 1.95e-05, global_step: 70, interval_runtime: 18.6562, interval_samples_per_second: 0.858, interval_steps_per_second: 0.536, epoch: 7.0[0m
[32m[2022-09-19 19:40:21,168] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:40:21,168] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 19:40:21,168] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:40:21,168] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:40:21,168] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 19:40:26,366] [    INFO][0m - eval_loss: 2.975170612335205, eval_accuracy: 0.625, eval_runtime: 3.2251, eval_samples_per_second: 49.611, eval_steps_per_second: 3.101, epoch: 7.0[0m
[32m[2022-09-19 19:40:26,366] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-70[0m
[32m[2022-09-19 19:40:26,366] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:40:28,961] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-70/tokenizer_config.json[0m
[32m[2022-09-19 19:40:28,962] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-70/special_tokens_map.json[0m
[32m[2022-09-19 19:40:42,108] [    INFO][0m - loss: 0.00028006, learning_rate: 1.8e-05, global_step: 80, interval_runtime: 20.9415, interval_samples_per_second: 0.764, interval_steps_per_second: 0.478, epoch: 8.0[0m
[32m[2022-09-19 19:40:42,109] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:40:42,109] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 19:40:42,109] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:40:42,110] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:40:42,110] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 19:40:45,283] [    INFO][0m - eval_loss: 2.923732280731201, eval_accuracy: 0.66875, eval_runtime: 3.1727, eval_samples_per_second: 50.43, eval_steps_per_second: 3.152, epoch: 8.0[0m
[32m[2022-09-19 19:40:45,283] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-80[0m
[32m[2022-09-19 19:40:45,284] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:40:47,810] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-80/tokenizer_config.json[0m
[32m[2022-09-19 19:40:47,810] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-80/special_tokens_map.json[0m
[32m[2022-09-19 19:41:01,245] [    INFO][0m - loss: 0.0085555, learning_rate: 1.65e-05, global_step: 90, interval_runtime: 19.1365, interval_samples_per_second: 0.836, interval_steps_per_second: 0.523, epoch: 9.0[0m
[32m[2022-09-19 19:41:01,246] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:41:01,246] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 19:41:01,246] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:41:01,246] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:41:01,246] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 19:41:06,923] [    INFO][0m - eval_loss: 3.342874526977539, eval_accuracy: 0.65625, eval_runtime: 3.1896, eval_samples_per_second: 50.162, eval_steps_per_second: 3.135, epoch: 9.0[0m
[32m[2022-09-19 19:41:06,927] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-90[0m
[32m[2022-09-19 19:41:06,927] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:41:09,714] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-90/tokenizer_config.json[0m
[32m[2022-09-19 19:41:09,715] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-90/special_tokens_map.json[0m
[32m[2022-09-19 19:41:23,105] [    INFO][0m - loss: 0.0019704, learning_rate: 1.5e-05, global_step: 100, interval_runtime: 21.86, interval_samples_per_second: 0.732, interval_steps_per_second: 0.457, epoch: 10.0[0m
[32m[2022-09-19 19:41:23,106] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:41:23,106] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 19:41:23,106] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:41:23,106] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:41:23,106] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 19:41:26,269] [    INFO][0m - eval_loss: 3.5526065826416016, eval_accuracy: 0.65, eval_runtime: 3.1622, eval_samples_per_second: 50.598, eval_steps_per_second: 3.162, epoch: 10.0[0m
[32m[2022-09-19 19:41:26,269] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-100[0m
[32m[2022-09-19 19:41:26,269] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:41:28,913] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-19 19:41:28,914] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-19 19:41:41,691] [    INFO][0m - loss: 0.0001618, learning_rate: 1.3500000000000001e-05, global_step: 110, interval_runtime: 18.5865, interval_samples_per_second: 0.861, interval_steps_per_second: 0.538, epoch: 11.0[0m
[32m[2022-09-19 19:41:41,692] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:41:41,692] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 19:41:41,693] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:41:41,693] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:41:41,693] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 19:41:46,703] [    INFO][0m - eval_loss: 4.868542671203613, eval_accuracy: 0.63125, eval_runtime: 3.1701, eval_samples_per_second: 50.471, eval_steps_per_second: 3.154, epoch: 11.0[0m
[32m[2022-09-19 19:41:46,704] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-110[0m
[32m[2022-09-19 19:41:46,704] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:41:49,241] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-110/tokenizer_config.json[0m
[32m[2022-09-19 19:41:49,242] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-110/special_tokens_map.json[0m
[32m[2022-09-19 19:42:02,081] [    INFO][0m - loss: 0.00011686, learning_rate: 1.2e-05, global_step: 120, interval_runtime: 20.3901, interval_samples_per_second: 0.785, interval_steps_per_second: 0.49, epoch: 12.0[0m
[32m[2022-09-19 19:42:02,082] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:42:02,083] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 19:42:02,083] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:42:02,083] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:42:02,083] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 19:42:06,998] [    INFO][0m - eval_loss: 4.381241321563721, eval_accuracy: 0.66875, eval_runtime: 3.1429, eval_samples_per_second: 50.908, eval_steps_per_second: 3.182, epoch: 12.0[0m
[32m[2022-09-19 19:42:06,998] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-120[0m
[32m[2022-09-19 19:42:06,998] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:42:09,451] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-120/tokenizer_config.json[0m
[32m[2022-09-19 19:42:11,871] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-120/special_tokens_map.json[0m
[32m[2022-09-19 19:42:24,592] [    INFO][0m - loss: 0.00036395, learning_rate: 1.05e-05, global_step: 130, interval_runtime: 22.5109, interval_samples_per_second: 0.711, interval_steps_per_second: 0.444, epoch: 13.0[0m
[32m[2022-09-19 19:42:24,593] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:42:24,593] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 19:42:24,594] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:42:24,594] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:42:24,594] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 19:42:27,784] [    INFO][0m - eval_loss: 3.6610121726989746, eval_accuracy: 0.7, eval_runtime: 3.1899, eval_samples_per_second: 50.158, eval_steps_per_second: 3.135, epoch: 13.0[0m
[32m[2022-09-19 19:42:27,784] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-130[0m
[32m[2022-09-19 19:42:27,784] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:42:30,330] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-130/tokenizer_config.json[0m
[32m[2022-09-19 19:42:30,330] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-130/special_tokens_map.json[0m
[32m[2022-09-19 19:42:43,666] [    INFO][0m - loss: 0.00014485, learning_rate: 9e-06, global_step: 140, interval_runtime: 19.0739, interval_samples_per_second: 0.839, interval_steps_per_second: 0.524, epoch: 14.0[0m
[32m[2022-09-19 19:42:43,667] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:42:43,667] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 19:42:43,667] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:42:43,667] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:42:43,667] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 19:42:46,825] [    INFO][0m - eval_loss: 4.753704071044922, eval_accuracy: 0.6375, eval_runtime: 3.1582, eval_samples_per_second: 50.662, eval_steps_per_second: 3.166, epoch: 14.0[0m
[32m[2022-09-19 19:42:46,826] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-140[0m
[32m[2022-09-19 19:42:46,826] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:42:49,265] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-140/tokenizer_config.json[0m
[32m[2022-09-19 19:42:49,265] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-140/special_tokens_map.json[0m
[32m[2022-09-19 19:43:02,338] [    INFO][0m - loss: 1.54e-06, learning_rate: 7.5e-06, global_step: 150, interval_runtime: 18.672, interval_samples_per_second: 0.857, interval_steps_per_second: 0.536, epoch: 15.0[0m
[32m[2022-09-19 19:43:02,339] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:43:02,340] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 19:43:02,340] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:43:02,340] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:43:02,340] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 19:43:07,817] [    INFO][0m - eval_loss: 3.956843852996826, eval_accuracy: 0.675, eval_runtime: 3.1419, eval_samples_per_second: 50.924, eval_steps_per_second: 3.183, epoch: 15.0[0m
[32m[2022-09-19 19:43:07,817] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-150[0m
[32m[2022-09-19 19:43:07,817] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:43:10,185] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-150/tokenizer_config.json[0m
[32m[2022-09-19 19:43:10,186] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-150/special_tokens_map.json[0m
[32m[2022-09-19 19:43:24,540] [    INFO][0m - loss: 1.23e-06, learning_rate: 6e-06, global_step: 160, interval_runtime: 22.2016, interval_samples_per_second: 0.721, interval_steps_per_second: 0.45, epoch: 16.0[0m
[32m[2022-09-19 19:43:24,540] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:43:24,540] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 19:43:24,540] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:43:24,541] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:43:24,541] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 19:43:27,691] [    INFO][0m - eval_loss: 3.8729851245880127, eval_accuracy: 0.68125, eval_runtime: 3.1502, eval_samples_per_second: 50.79, eval_steps_per_second: 3.174, epoch: 16.0[0m
[32m[2022-09-19 19:43:27,691] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-160[0m
[32m[2022-09-19 19:43:27,692] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:43:30,564] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-160/tokenizer_config.json[0m
[32m[2022-09-19 19:43:30,564] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-160/special_tokens_map.json[0m
[32m[2022-09-19 19:43:46,106] [    INFO][0m - loss: 0.00166715, learning_rate: 4.5e-06, global_step: 170, interval_runtime: 21.5663, interval_samples_per_second: 0.742, interval_steps_per_second: 0.464, epoch: 17.0[0m
[32m[2022-09-19 19:43:46,107] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:43:46,108] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 19:43:46,108] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:43:46,108] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:43:46,108] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 19:43:49,272] [    INFO][0m - eval_loss: 3.8609633445739746, eval_accuracy: 0.6875, eval_runtime: 3.1639, eval_samples_per_second: 50.57, eval_steps_per_second: 3.161, epoch: 17.0[0m
[32m[2022-09-19 19:43:49,273] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-170[0m
[32m[2022-09-19 19:43:49,273] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:43:52,272] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-170/tokenizer_config.json[0m
[32m[2022-09-19 19:43:52,273] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-170/special_tokens_map.json[0m
[32m[2022-09-19 19:44:09,824] [    INFO][0m - loss: 1.35e-06, learning_rate: 3e-06, global_step: 180, interval_runtime: 21.0463, interval_samples_per_second: 0.76, interval_steps_per_second: 0.475, epoch: 18.0[0m
[32m[2022-09-19 19:44:09,825] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:44:09,825] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 19:44:09,825] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:44:09,825] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:44:09,825] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 19:44:13,071] [    INFO][0m - eval_loss: 3.859762668609619, eval_accuracy: 0.6875, eval_runtime: 3.2285, eval_samples_per_second: 49.558, eval_steps_per_second: 3.097, epoch: 18.0[0m
[32m[2022-09-19 19:44:13,072] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-180[0m
[32m[2022-09-19 19:44:13,072] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:44:16,362] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-180/tokenizer_config.json[0m
[32m[2022-09-19 19:44:16,362] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-180/special_tokens_map.json[0m
[32m[2022-09-19 19:44:31,244] [    INFO][0m - loss: 3.9e-06, learning_rate: 1.5e-06, global_step: 190, interval_runtime: 24.0916, interval_samples_per_second: 0.664, interval_steps_per_second: 0.415, epoch: 19.0[0m
[32m[2022-09-19 19:44:31,245] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:44:31,245] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 19:44:31,245] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:44:31,245] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:44:31,246] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 19:44:34,404] [    INFO][0m - eval_loss: 3.8684494495391846, eval_accuracy: 0.68125, eval_runtime: 3.1581, eval_samples_per_second: 50.664, eval_steps_per_second: 3.166, epoch: 19.0[0m
[32m[2022-09-19 19:44:34,405] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-190[0m
[32m[2022-09-19 19:44:34,405] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:44:37,881] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-190/tokenizer_config.json[0m
[32m[2022-09-19 19:44:37,881] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-190/special_tokens_map.json[0m
[32m[2022-09-19 19:44:53,241] [    INFO][0m - loss: 0.00011199, learning_rate: 0.0, global_step: 200, interval_runtime: 21.9965, interval_samples_per_second: 0.727, interval_steps_per_second: 0.455, epoch: 20.0[0m
[32m[2022-09-19 19:44:53,242] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:44:53,242] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 19:44:53,242] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:44:53,242] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:44:53,242] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 19:44:56,458] [    INFO][0m - eval_loss: 3.8588333129882812, eval_accuracy: 0.68125, eval_runtime: 3.2157, eval_samples_per_second: 49.755, eval_steps_per_second: 3.11, epoch: 20.0[0m
[32m[2022-09-19 19:44:56,459] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-200[0m
[32m[2022-09-19 19:44:56,459] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:44:59,872] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-200/tokenizer_config.json[0m
[32m[2022-09-19 19:44:59,873] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-200/special_tokens_map.json[0m
[32m[2022-09-19 19:45:06,886] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-19 19:45:06,887] [    INFO][0m - Loading best model from ./checkpoints_csl/checkpoint-130 (score: 0.7).[0m
[32m[2022-09-19 19:45:08,888] [    INFO][0m - train_runtime: 414.4772, train_samples_per_second: 7.721, train_steps_per_second: 0.483, train_loss: 0.16734514980125367, epoch: 20.0[0m
[32m[2022-09-19 19:45:14,013] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/[0m
[32m[2022-09-19 19:45:14,014] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:45:18,686] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/tokenizer_config.json[0m
[32m[2022-09-19 19:45:18,687] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/special_tokens_map.json[0m
[32m[2022-09-19 19:45:18,688] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-19 19:45:18,689] [    INFO][0m -   epoch                    =       20.0[0m
[32m[2022-09-19 19:45:18,689] [    INFO][0m -   train_loss               =     0.1673[0m
[32m[2022-09-19 19:45:18,689] [    INFO][0m -   train_runtime            = 0:06:54.47[0m
[32m[2022-09-19 19:45:18,689] [    INFO][0m -   train_samples_per_second =      7.721[0m
[32m[2022-09-19 19:45:18,689] [    INFO][0m -   train_steps_per_second   =      0.483[0m
[32m[2022-09-19 19:45:18,691] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-19 19:45:18,691] [    INFO][0m -   Num examples = 2838[0m
[32m[2022-09-19 19:45:18,692] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:45:18,692] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:45:18,692] [    INFO][0m -   Total prediction steps = 178[0m
[32m[2022-09-19 19:46:15,955] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-19 19:46:15,956] [    INFO][0m -   test_accuracy           =     0.6427[0m
[32m[2022-09-19 19:46:15,956] [    INFO][0m -   test_loss               =     4.9161[0m
[32m[2022-09-19 19:46:15,956] [    INFO][0m -   test_runtime            = 0:00:57.26[0m
[32m[2022-09-19 19:46:15,956] [    INFO][0m -   test_samples_per_second =      49.56[0m
[32m[2022-09-19 19:46:15,956] [    INFO][0m -   test_steps_per_second   =      3.108[0m
[32m[2022-09-19 19:46:15,957] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-19 19:46:15,957] [    INFO][0m -   Num examples = 3000[0m
[32m[2022-09-19 19:46:15,957] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:46:15,957] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:46:15,957] [    INFO][0m -   Total prediction steps = 188[0m
[32m[2022-09-19 19:47:25,719] [    INFO][0m - Predictions for cslf saved to ./fewclue_submit_examples.[0m
{
  "labels": 1,
  "text_a": "\u65e0\u7ebf\u4f20\u611f\u5668\u7f51\u7edc\u4e2d\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u901a\u7528\u8fd1\u4f3c\u67e5\u8be2\u662f\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\uff0e\u6587\u4e2d\u63d0\u51fa\u4e00\u79cd\u65e0\u7ebf\u4f20\u611f\u5668\u7f51\u7edc\u4e2d\u9690\u79c1\u4fdd\u62a4\u901a\u7528\u8fd1\u4f3c\u67e5\u8be2\u534f\u8baePGAQ\uff0ePGAQ\u5c06\u4f20\u611f\u5668\u8282\u70b9\u7f16\u53f7\u548c\u5176\u91c7\u96c6\u6570\u636e\u9690\u85cf\u4e8e\u8bbe\u8ba1\u7684\u6570\u636e\u7ed3\u6784\u4e2d\uff0c\u5728\u57fa\u7ad9\u6784\u9020\u7ebf\u6027\u65b9\u7a0b\u7ec4\u89e3\u51fa\u76f4\u65b9\u56fe\uff0c\u6839\u636e\u76f4\u65b9\u56fe\u5177\u6709\u7684\u7edf\u8ba1\u4fe1\u606f\uff0c\u4e0d\u6cc4\u9732\u9690\u79c1\u5730\u5b8c\u6210Top-k\u67e5\u8be2\u3001\u8303\u56f4\u67e5\u8be2\u3001SUM\u3001MAX/MIN\u3001Median\u3001Histogram\u7b49\u8fd1\u4f3c\u67e5\u8be2\uff0ePGAQ\u4f7f\u7528\u7f51\u5185\u6c42\u548c\u805a\u96c6\u4ee5\u51cf\u5c11\u80fd\u91cf\u6d88\u8017\uff0c\u5e76\u4e14\u80fd\u591f\u901a\u8fc7\u8c03\u8282\u76f4\u65b9\u56fe\u5212\u5206\u7c92\u5ea6\u6765\u5e73\u8861\u67e5\u8be2\u7cbe\u5ea6\u4e0e\u80fd\u91cf\u6d88\u8017\uff0ePGAQ\u534f\u8bae\u5206\u4e3aH-PGAQ\u548cF-PGAQ\u4e24\u79cd\u6a21\u5f0f\uff0eH-PGAQ\u6a21\u5f0f\u4f7f\u7528\u6570\u636e\u6270\u52a8\u6280\u672f\u52a0\u5f3a\u6570\u636e\u5b89\u5168\u6027\uff0cF-PGAQ\u4f7f\u7528\u8fc7\u6ee4\u5668\u51cf\u5c11\u8fde\u7eed\u67e5\u8be2\u901a\u4fe1\u91cf\uff0e\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u4f7f\u7528\u771f\u5b9e\u6570\u636e\u96c6\u5b9e\u9a8c\u9a8c\u8bc1\u4e86PGAQ\u7684\u5b89\u5168\u6027\u548c\u6709\u6548\u6027\uff0e",
  "text_b": "\u65e0\u7ebf\u4f20\u611f\u5668\u7f51\u7edc\u3001\u6570\u636e\u805a\u96c6\u3001\u7269\u8054\u7f51\u3001\u8fd1\u4f3c\u67e5\u8be2",
  "uid": 0
}

Prediction done.
 
==========
cluewsc
==========
 
[32m[2022-09-19 19:47:41,768] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-19 19:47:41,768] [    INFO][0m - ============================================================[0m
[32m[2022-09-19 19:47:41,768] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-19 19:47:41,768] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-19 19:47:41,769] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-19 19:47:41,769] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-19 19:47:41,769] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-19 19:47:41,769] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-19 19:47:41,769] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-19 19:47:41,769] [    INFO][0m - [0m
[32m[2022-09-19 19:47:41,769] [    INFO][0m - ============================================================[0m
[32m[2022-09-19 19:47:41,769] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-19 19:47:41,769] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-19 19:47:41,769] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-19 19:47:41,769] [    INFO][0m - pretrained                    :/ssd2/wanghuijuan03/data/zero-shot/0919_model_state.pdparams[0m
[32m[2022-09-19 19:47:41,770] [    INFO][0m - prompt                        :â€œ{'text':'text_a'}â€{'text':'text_b'}è¿™é‡Œä»£è¯ä½¿ç”¨æ­£ç¡®å—ï¼Ÿ{'mask'}{'mask'}[0m
[32m[2022-09-19 19:47:41,770] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-19 19:47:41,770] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-19 19:47:41,770] [    INFO][0m - task_name                     :cluewsc[0m
[32m[2022-09-19 19:47:41,770] [    INFO][0m - [0m
[32m[2022-09-19 19:47:41,770] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0919 19:47:41.771625 49929 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0919 19:47:41.775544 49929 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-19 19:47:46,453] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-19 19:47:46,464] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-19 19:47:46,464] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-19 19:47:46,465] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': 'â€œ'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': 'â€'}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': 'è¿™é‡Œä»£è¯ä½¿ç”¨æ­£ç¡®å—ï¼Ÿ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}][0m
[32m[2022-09-19 19:47:47,808] [    INFO][0m - ============================================================[0m
[32m[2022-09-19 19:47:47,808] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-19 19:47:47,809] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-19 19:47:47,809] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-19 19:47:47,809] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-19 19:47:47,809] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-19 19:47:47,809] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-19 19:47:47,809] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-19 19:47:47,809] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-19 19:47:47,809] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-19 19:47:47,809] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-19 19:47:47,809] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-19 19:47:47,809] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-19 19:47:47,810] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-19 19:47:47,810] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-19 19:47:47,810] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-19 19:47:47,810] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-19 19:47:47,810] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-19 19:47:47,810] [    INFO][0m - eval_batch_size               :16[0m
[32m[2022-09-19 19:47:47,810] [    INFO][0m - eval_steps                    :200[0m
[32m[2022-09-19 19:47:47,810] [    INFO][0m - evaluation_strategy           :IntervalStrategy.EPOCH[0m
[32m[2022-09-19 19:47:47,810] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-19 19:47:47,810] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-19 19:47:47,810] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-19 19:47:47,810] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-19 19:47:47,810] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-19 19:47:47,811] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-19 19:47:47,811] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-19 19:47:47,811] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-19 19:47:47,811] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-19 19:47:47,811] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-09-19 19:47:47,811] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-19 19:47:47,811] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-19 19:47:47,811] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-19 19:47:47,811] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-19 19:47:47,811] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-19 19:47:47,811] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-19 19:47:47,811] [    INFO][0m - logging_dir                   :./checkpoints_cluewsc/runs/Sep19_19-47-41_instance-3bwob41y-01[0m
[32m[2022-09-19 19:47:47,811] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-19 19:47:47,811] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-19 19:47:47,812] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-19 19:47:47,812] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-19 19:47:47,812] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-19 19:47:47,812] [    INFO][0m - max_seq_length                :128[0m
[32m[2022-09-19 19:47:47,812] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-19 19:47:47,812] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-19 19:47:47,812] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-19 19:47:47,812] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-19 19:47:47,812] [    INFO][0m - num_train_epochs              :20.0[0m
[32m[2022-09-19 19:47:47,812] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-19 19:47:47,812] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-19 19:47:47,812] [    INFO][0m - output_dir                    :./checkpoints_cluewsc/[0m
[32m[2022-09-19 19:47:47,813] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-19 19:47:47,813] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-19 19:47:47,813] [    INFO][0m - per_device_eval_batch_size    :16[0m
[32m[2022-09-19 19:47:47,813] [    INFO][0m - per_device_train_batch_size   :16[0m
[32m[2022-09-19 19:47:47,813] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-19 19:47:47,813] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-19 19:47:47,813] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-19 19:47:47,813] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-09-19 19:47:47,813] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-19 19:47:47,813] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-19 19:47:47,813] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-19 19:47:47,813] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-19 19:47:47,813] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-19 19:47:47,813] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-19 19:47:47,814] [    INFO][0m - run_name                      :./checkpoints_cluewsc/[0m
[32m[2022-09-19 19:47:47,814] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-19 19:47:47,814] [    INFO][0m - save_steps                    :200[0m
[32m[2022-09-19 19:47:47,814] [    INFO][0m - save_strategy                 :IntervalStrategy.EPOCH[0m
[32m[2022-09-19 19:47:47,814] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-19 19:47:47,814] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-19 19:47:47,814] [    INFO][0m - seed                          :42[0m
[32m[2022-09-19 19:47:47,814] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-19 19:47:47,814] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-19 19:47:47,814] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-19 19:47:47,814] [    INFO][0m - train_batch_size              :16[0m
[32m[2022-09-19 19:47:47,814] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-19 19:47:47,814] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-19 19:47:47,814] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-19 19:47:47,815] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-19 19:47:47,815] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-19 19:47:47,815] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-19 19:47:47,815] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-19 19:47:47,815] [    INFO][0m - [0m
[32m[2022-09-19 19:47:47,817] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-19 19:47:47,818] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 19:47:47,818] [    INFO][0m -   Num Epochs = 20[0m
[32m[2022-09-19 19:47:47,818] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2022-09-19 19:47:47,818] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2022-09-19 19:47:47,818] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-19 19:47:47,818] [    INFO][0m -   Total optimization steps = 200.0[0m
[32m[2022-09-19 19:47:47,818] [    INFO][0m -   Total num train samples = 3200[0m
[32m[2022-09-19 19:47:52,261] [    INFO][0m - loss: 1.44774551, learning_rate: 2.8499999999999998e-05, global_step: 10, interval_runtime: 4.4423, interval_samples_per_second: 3.602, interval_steps_per_second: 2.251, epoch: 1.0[0m
[32m[2022-09-19 19:47:52,262] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:47:52,262] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-19 19:47:52,262] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:47:52,262] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:47:52,262] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 19:47:53,723] [    INFO][0m - eval_loss: 0.7896623611450195, eval_accuracy: 0.5031446540880503, eval_runtime: 1.4603, eval_samples_per_second: 108.88, eval_steps_per_second: 6.848, epoch: 1.0[0m
[32m[2022-09-19 19:47:53,724] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-10[0m
[32m[2022-09-19 19:47:53,724] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:47:57,376] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-10/tokenizer_config.json[0m
[32m[2022-09-19 19:47:57,376] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-10/special_tokens_map.json[0m
[32m[2022-09-19 19:48:06,904] [    INFO][0m - loss: 0.7742383, learning_rate: 2.7000000000000002e-05, global_step: 20, interval_runtime: 14.6427, interval_samples_per_second: 1.093, interval_steps_per_second: 0.683, epoch: 2.0[0m
[32m[2022-09-19 19:48:06,905] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:48:06,905] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-19 19:48:06,905] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:48:06,905] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:48:06,905] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 19:48:08,283] [    INFO][0m - eval_loss: 0.8882116079330444, eval_accuracy: 0.5031446540880503, eval_runtime: 1.3771, eval_samples_per_second: 115.456, eval_steps_per_second: 7.261, epoch: 2.0[0m
[32m[2022-09-19 19:48:08,283] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-20[0m
[32m[2022-09-19 19:48:08,283] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:48:11,272] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-20/tokenizer_config.json[0m
[32m[2022-09-19 19:48:11,272] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-20/special_tokens_map.json[0m
[32m[2022-09-19 19:48:19,877] [    INFO][0m - loss: 0.71665258, learning_rate: 2.55e-05, global_step: 30, interval_runtime: 12.9725, interval_samples_per_second: 1.233, interval_steps_per_second: 0.771, epoch: 3.0[0m
[32m[2022-09-19 19:48:19,877] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:48:19,878] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-19 19:48:19,878] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:48:19,878] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:48:19,878] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 19:48:21,247] [    INFO][0m - eval_loss: 0.8001848459243774, eval_accuracy: 0.5345911949685535, eval_runtime: 1.3688, eval_samples_per_second: 116.161, eval_steps_per_second: 7.306, epoch: 3.0[0m
[32m[2022-09-19 19:48:21,247] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-30[0m
[32m[2022-09-19 19:48:21,247] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:48:23,976] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-30/tokenizer_config.json[0m
[32m[2022-09-19 19:48:23,976] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-30/special_tokens_map.json[0m
[32m[2022-09-19 19:48:32,654] [    INFO][0m - loss: 0.62305107, learning_rate: 2.4e-05, global_step: 40, interval_runtime: 12.7773, interval_samples_per_second: 1.252, interval_steps_per_second: 0.783, epoch: 4.0[0m
[32m[2022-09-19 19:48:32,655] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:48:32,655] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-19 19:48:32,655] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:48:32,655] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:48:32,655] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 19:48:34,007] [    INFO][0m - eval_loss: 0.7471938133239746, eval_accuracy: 0.5911949685534591, eval_runtime: 1.3512, eval_samples_per_second: 117.67, eval_steps_per_second: 7.401, epoch: 4.0[0m
[32m[2022-09-19 19:48:34,007] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-40[0m
[32m[2022-09-19 19:48:34,007] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:48:39,108] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-40/tokenizer_config.json[0m
[32m[2022-09-19 19:48:39,108] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-40/special_tokens_map.json[0m
[32m[2022-09-19 19:48:48,816] [    INFO][0m - loss: 0.50201311, learning_rate: 2.25e-05, global_step: 50, interval_runtime: 16.1621, interval_samples_per_second: 0.99, interval_steps_per_second: 0.619, epoch: 5.0[0m
[32m[2022-09-19 19:48:48,817] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:48:48,817] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-19 19:48:48,817] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:48:48,817] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:48:48,817] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 19:48:50,188] [    INFO][0m - eval_loss: 0.7041404247283936, eval_accuracy: 0.5849056603773585, eval_runtime: 1.3707, eval_samples_per_second: 115.997, eval_steps_per_second: 7.295, epoch: 5.0[0m
[32m[2022-09-19 19:48:50,188] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-50[0m
[32m[2022-09-19 19:48:50,189] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:48:52,955] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-50/tokenizer_config.json[0m
[32m[2022-09-19 19:48:52,955] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-50/special_tokens_map.json[0m
[32m[2022-09-19 19:49:02,141] [    INFO][0m - loss: 0.38715947, learning_rate: 2.1e-05, global_step: 60, interval_runtime: 13.3256, interval_samples_per_second: 1.201, interval_steps_per_second: 0.75, epoch: 6.0[0m
[32m[2022-09-19 19:49:02,142] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:49:02,142] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-19 19:49:02,143] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:49:02,143] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:49:02,143] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 19:49:03,515] [    INFO][0m - eval_loss: 1.001558542251587, eval_accuracy: 0.5786163522012578, eval_runtime: 1.3719, eval_samples_per_second: 115.896, eval_steps_per_second: 7.289, epoch: 6.0[0m
[32m[2022-09-19 19:49:07,085] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-60[0m
[32m[2022-09-19 19:49:07,085] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:49:10,559] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-60/tokenizer_config.json[0m
[32m[2022-09-19 19:49:10,559] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-60/special_tokens_map.json[0m
[32m[2022-09-19 19:49:20,226] [    INFO][0m - loss: 0.24745419, learning_rate: 1.95e-05, global_step: 70, interval_runtime: 18.0848, interval_samples_per_second: 0.885, interval_steps_per_second: 0.553, epoch: 7.0[0m
[32m[2022-09-19 19:49:20,227] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:49:20,227] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-19 19:49:20,227] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:49:20,227] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:49:20,228] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 19:49:21,622] [    INFO][0m - eval_loss: 1.0150086879730225, eval_accuracy: 0.5911949685534591, eval_runtime: 1.3944, eval_samples_per_second: 114.029, eval_steps_per_second: 7.172, epoch: 7.0[0m
[32m[2022-09-19 19:49:21,623] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-70[0m
[32m[2022-09-19 19:49:21,623] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:49:24,423] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-70/tokenizer_config.json[0m
[32m[2022-09-19 19:49:24,424] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-70/special_tokens_map.json[0m
[32m[2022-09-19 19:49:33,754] [    INFO][0m - loss: 0.17527405, learning_rate: 1.8e-05, global_step: 80, interval_runtime: 13.5272, interval_samples_per_second: 1.183, interval_steps_per_second: 0.739, epoch: 8.0[0m
[32m[2022-09-19 19:49:33,754] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:49:33,755] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-19 19:49:33,755] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:49:33,755] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:49:33,755] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 19:49:35,123] [    INFO][0m - eval_loss: 0.8202011585235596, eval_accuracy: 0.7044025157232704, eval_runtime: 1.3681, eval_samples_per_second: 116.223, eval_steps_per_second: 7.31, epoch: 8.0[0m
[32m[2022-09-19 19:49:35,124] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-80[0m
[32m[2022-09-19 19:49:35,124] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:49:37,932] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-80/tokenizer_config.json[0m
[32m[2022-09-19 19:49:37,933] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-80/special_tokens_map.json[0m
[32m[2022-09-19 19:49:46,624] [    INFO][0m - loss: 0.02258072, learning_rate: 1.65e-05, global_step: 90, interval_runtime: 12.8701, interval_samples_per_second: 1.243, interval_steps_per_second: 0.777, epoch: 9.0[0m
[32m[2022-09-19 19:49:46,624] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:49:46,625] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-19 19:49:46,625] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:49:46,625] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:49:46,625] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 19:49:48,001] [    INFO][0m - eval_loss: 1.6679069995880127, eval_accuracy: 0.6666666666666666, eval_runtime: 1.3762, eval_samples_per_second: 115.531, eval_steps_per_second: 7.266, epoch: 9.0[0m
[32m[2022-09-19 19:49:48,002] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-90[0m
[32m[2022-09-19 19:49:48,002] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:49:50,772] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-90/tokenizer_config.json[0m
[32m[2022-09-19 19:49:50,772] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-90/special_tokens_map.json[0m
[32m[2022-09-19 19:49:59,795] [    INFO][0m - loss: 0.00098535, learning_rate: 1.5e-05, global_step: 100, interval_runtime: 13.1714, interval_samples_per_second: 1.215, interval_steps_per_second: 0.759, epoch: 10.0[0m
[32m[2022-09-19 19:49:59,796] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:49:59,796] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-19 19:49:59,796] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:49:59,796] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:49:59,796] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 19:50:01,159] [    INFO][0m - eval_loss: 1.9132744073867798, eval_accuracy: 0.6918238993710691, eval_runtime: 1.3621, eval_samples_per_second: 116.729, eval_steps_per_second: 7.341, epoch: 10.0[0m
[32m[2022-09-19 19:50:01,159] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-100[0m
[32m[2022-09-19 19:50:01,159] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:50:03,787] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-19 19:50:03,788] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-19 19:50:12,297] [    INFO][0m - loss: 0.02549904, learning_rate: 1.3500000000000001e-05, global_step: 110, interval_runtime: 12.5017, interval_samples_per_second: 1.28, interval_steps_per_second: 0.8, epoch: 11.0[0m
[32m[2022-09-19 19:50:12,297] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:50:12,297] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-19 19:50:12,297] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:50:12,298] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:50:12,298] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 19:50:13,673] [    INFO][0m - eval_loss: 2.4516570568084717, eval_accuracy: 0.7044025157232704, eval_runtime: 1.3749, eval_samples_per_second: 115.649, eval_steps_per_second: 7.273, epoch: 11.0[0m
[32m[2022-09-19 19:50:13,910] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-110[0m
[32m[2022-09-19 19:50:13,910] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:50:16,595] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-110/tokenizer_config.json[0m
[32m[2022-09-19 19:50:16,596] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-110/special_tokens_map.json[0m
[32m[2022-09-19 19:50:25,956] [    INFO][0m - loss: 0.07122572, learning_rate: 1.2e-05, global_step: 120, interval_runtime: 13.6587, interval_samples_per_second: 1.171, interval_steps_per_second: 0.732, epoch: 12.0[0m
[32m[2022-09-19 19:50:25,957] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:50:25,957] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-19 19:50:25,957] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:50:25,957] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:50:25,957] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 19:50:27,342] [    INFO][0m - eval_loss: 2.6123807430267334, eval_accuracy: 0.6855345911949685, eval_runtime: 1.3841, eval_samples_per_second: 114.875, eval_steps_per_second: 7.225, epoch: 12.0[0m
[32m[2022-09-19 19:50:27,342] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-120[0m
[32m[2022-09-19 19:50:27,342] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:50:30,257] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-120/tokenizer_config.json[0m
[32m[2022-09-19 19:50:30,258] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-120/special_tokens_map.json[0m
[32m[2022-09-19 19:50:38,673] [    INFO][0m - loss: 0.00536304, learning_rate: 1.05e-05, global_step: 130, interval_runtime: 12.7174, interval_samples_per_second: 1.258, interval_steps_per_second: 0.786, epoch: 13.0[0m
[32m[2022-09-19 19:50:38,674] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:50:38,674] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-19 19:50:38,674] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:50:38,674] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:50:38,674] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 19:50:40,046] [    INFO][0m - eval_loss: 3.0401933193206787, eval_accuracy: 0.6540880503144654, eval_runtime: 1.3716, eval_samples_per_second: 115.926, eval_steps_per_second: 7.291, epoch: 13.0[0m
[32m[2022-09-19 19:50:40,046] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-130[0m
[32m[2022-09-19 19:50:40,046] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:50:42,657] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-130/tokenizer_config.json[0m
[32m[2022-09-19 19:50:42,658] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-130/special_tokens_map.json[0m
[32m[2022-09-19 19:50:51,039] [    INFO][0m - loss: 0.00013939, learning_rate: 9e-06, global_step: 140, interval_runtime: 12.3657, interval_samples_per_second: 1.294, interval_steps_per_second: 0.809, epoch: 14.0[0m
[32m[2022-09-19 19:50:52,832] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:50:52,832] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-19 19:50:52,833] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:50:52,833] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:50:52,833] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 19:50:54,204] [    INFO][0m - eval_loss: 2.454934597015381, eval_accuracy: 0.6855345911949685, eval_runtime: 1.3713, eval_samples_per_second: 115.945, eval_steps_per_second: 7.292, epoch: 14.0[0m
[32m[2022-09-19 19:50:54,205] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-140[0m
[32m[2022-09-19 19:50:54,205] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:50:56,810] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-140/tokenizer_config.json[0m
[32m[2022-09-19 19:50:56,810] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-140/special_tokens_map.json[0m
[32m[2022-09-19 19:51:05,138] [    INFO][0m - loss: 2.053e-05, learning_rate: 7.5e-06, global_step: 150, interval_runtime: 14.1, interval_samples_per_second: 1.135, interval_steps_per_second: 0.709, epoch: 15.0[0m
[32m[2022-09-19 19:51:05,139] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:51:05,139] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-19 19:51:05,139] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:51:05,139] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:51:05,139] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 19:51:06,513] [    INFO][0m - eval_loss: 2.3147406578063965, eval_accuracy: 0.6855345911949685, eval_runtime: 1.3734, eval_samples_per_second: 115.767, eval_steps_per_second: 7.281, epoch: 15.0[0m
[32m[2022-09-19 19:51:06,513] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-150[0m
[32m[2022-09-19 19:51:06,513] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:51:09,327] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-150/tokenizer_config.json[0m
[32m[2022-09-19 19:51:09,328] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-150/special_tokens_map.json[0m
[32m[2022-09-19 19:51:17,649] [    INFO][0m - loss: 3.066e-05, learning_rate: 6e-06, global_step: 160, interval_runtime: 12.5107, interval_samples_per_second: 1.279, interval_steps_per_second: 0.799, epoch: 16.0[0m
[32m[2022-09-19 19:51:17,650] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:51:17,650] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-19 19:51:17,650] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:51:17,650] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:51:17,650] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 19:51:19,010] [    INFO][0m - eval_loss: 2.317340612411499, eval_accuracy: 0.6855345911949685, eval_runtime: 1.3592, eval_samples_per_second: 116.981, eval_steps_per_second: 7.357, epoch: 16.0[0m
[32m[2022-09-19 19:51:19,010] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-160[0m
[32m[2022-09-19 19:51:19,010] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:51:22,440] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-160/tokenizer_config.json[0m
[32m[2022-09-19 19:51:22,440] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-160/special_tokens_map.json[0m
[32m[2022-09-19 19:51:31,523] [    INFO][0m - loss: 0.00010208, learning_rate: 4.5e-06, global_step: 170, interval_runtime: 13.8734, interval_samples_per_second: 1.153, interval_steps_per_second: 0.721, epoch: 17.0[0m
[32m[2022-09-19 19:51:31,523] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:51:31,524] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-19 19:51:31,524] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:51:31,524] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:51:31,524] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 19:51:32,913] [    INFO][0m - eval_loss: 2.470946788787842, eval_accuracy: 0.6855345911949685, eval_runtime: 1.3884, eval_samples_per_second: 114.52, eval_steps_per_second: 7.203, epoch: 17.0[0m
[32m[2022-09-19 19:51:32,913] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-170[0m
[32m[2022-09-19 19:51:32,914] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:51:35,513] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-170/tokenizer_config.json[0m
[32m[2022-09-19 19:51:35,514] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-170/special_tokens_map.json[0m
[32m[2022-09-19 19:51:44,358] [    INFO][0m - loss: 1.253e-05, learning_rate: 3e-06, global_step: 180, interval_runtime: 12.835, interval_samples_per_second: 1.247, interval_steps_per_second: 0.779, epoch: 18.0[0m
[32m[2022-09-19 19:51:44,359] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:51:44,359] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-19 19:51:44,359] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:51:44,359] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:51:44,359] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 19:51:45,735] [    INFO][0m - eval_loss: 2.5111050605773926, eval_accuracy: 0.6855345911949685, eval_runtime: 1.3761, eval_samples_per_second: 115.548, eval_steps_per_second: 7.267, epoch: 18.0[0m
[32m[2022-09-19 19:51:46,039] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-180[0m
[32m[2022-09-19 19:51:46,039] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:51:48,635] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-180/tokenizer_config.json[0m
[32m[2022-09-19 19:51:48,635] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-180/special_tokens_map.json[0m
[32m[2022-09-19 19:52:02,066] [    INFO][0m - loss: 2.476e-05, learning_rate: 1.5e-06, global_step: 190, interval_runtime: 17.7075, interval_samples_per_second: 0.904, interval_steps_per_second: 0.565, epoch: 19.0[0m
[32m[2022-09-19 19:52:02,067] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:52:02,067] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-19 19:52:02,067] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:52:02,067] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:52:02,067] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 19:52:03,440] [    INFO][0m - eval_loss: 2.51841402053833, eval_accuracy: 0.6855345911949685, eval_runtime: 1.3728, eval_samples_per_second: 115.822, eval_steps_per_second: 7.284, epoch: 19.0[0m
[32m[2022-09-19 19:52:03,440] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-190[0m
[32m[2022-09-19 19:52:03,441] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:52:06,053] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-190/tokenizer_config.json[0m
[32m[2022-09-19 19:52:06,054] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-190/special_tokens_map.json[0m
[32m[2022-09-19 19:52:14,447] [    INFO][0m - loss: 1.841e-05, learning_rate: 0.0, global_step: 200, interval_runtime: 12.3821, interval_samples_per_second: 1.292, interval_steps_per_second: 0.808, epoch: 20.0[0m
[32m[2022-09-19 19:52:14,448] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 19:52:14,448] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-19 19:52:14,448] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:52:14,448] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:52:14,449] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 19:52:15,822] [    INFO][0m - eval_loss: 2.51608943939209, eval_accuracy: 0.6855345911949685, eval_runtime: 1.373, eval_samples_per_second: 115.803, eval_steps_per_second: 7.283, epoch: 20.0[0m
[32m[2022-09-19 19:52:16,553] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-200[0m
[32m[2022-09-19 19:52:16,553] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:52:19,148] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-200/tokenizer_config.json[0m
[32m[2022-09-19 19:52:19,149] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-200/special_tokens_map.json[0m
[32m[2022-09-19 19:52:24,498] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-19 19:52:24,499] [    INFO][0m - Loading best model from ./checkpoints_cluewsc/checkpoint-80 (score: 0.7044025157232704).[0m
[32m[2022-09-19 19:52:26,507] [    INFO][0m - train_runtime: 278.6881, train_samples_per_second: 11.482, train_steps_per_second: 0.718, train_loss: 0.24997952512647317, epoch: 20.0[0m
[32m[2022-09-19 19:52:26,567] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/[0m
[32m[2022-09-19 19:52:26,568] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 19:52:29,374] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/tokenizer_config.json[0m
[32m[2022-09-19 19:52:29,374] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/special_tokens_map.json[0m
[32m[2022-09-19 19:52:29,376] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-19 19:52:29,376] [    INFO][0m -   epoch                    =       20.0[0m
[32m[2022-09-19 19:52:29,376] [    INFO][0m -   train_loss               =       0.25[0m
[32m[2022-09-19 19:52:29,376] [    INFO][0m -   train_runtime            = 0:04:38.68[0m
[32m[2022-09-19 19:52:29,376] [    INFO][0m -   train_samples_per_second =     11.482[0m
[32m[2022-09-19 19:52:29,376] [    INFO][0m -   train_steps_per_second   =      0.718[0m
[32m[2022-09-19 19:52:29,379] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-19 19:52:29,379] [    INFO][0m -   Num examples = 976[0m
[32m[2022-09-19 19:52:29,379] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:52:29,379] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:52:29,379] [    INFO][0m -   Total prediction steps = 61[0m
[32m[2022-09-19 19:52:38,160] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-19 19:52:38,160] [    INFO][0m -   test_accuracy           =     0.6404[0m
[32m[2022-09-19 19:52:38,161] [    INFO][0m -   test_loss               =     1.0165[0m
[32m[2022-09-19 19:52:38,161] [    INFO][0m -   test_runtime            = 0:00:08.78[0m
[32m[2022-09-19 19:52:38,161] [    INFO][0m -   test_samples_per_second =    111.151[0m
[32m[2022-09-19 19:52:38,161] [    INFO][0m -   test_steps_per_second   =      6.947[0m
[32m[2022-09-19 19:52:38,161] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-19 19:52:38,161] [    INFO][0m -   Num examples = 290[0m
[32m[2022-09-19 19:52:38,161] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 19:52:38,162] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 19:52:38,162] [    INFO][0m -   Total prediction steps = 19[0m
[32m[2022-09-19 19:52:41,074] [    INFO][0m - Predictions for cluewscf saved to ./fewclue_submit_examples.[0m
{
  "labels": 0,
  "text_a": "\u4e3a\u4ec0\u4e48\u8981\u51fa\u73b0\u4e00\u4e2a\u8eab\u7a7f\u519b\u88c5\u7684\u9ad8\u5927\u7537\u4eba\uff1f\u5c31\u50cf\u4e00\u7247\u6811\u53f6\u98d8\u5165\u4e86\u6811\u6797\uff0c\u4ed6\u8d70\u5230\u4e86\u6211\u7684\u5bb6\u4eba\u4e2d\u95f4\u3002",
  "text_b": "\u5176\u4e2d\u4ed6\u6307\u7684\u662f\u6811\u53f6"
}

Prediction done.
