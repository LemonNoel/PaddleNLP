 
==========
csldcp
==========
 
[32m[2022-09-17 17:35:20,995] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-17 17:35:20,995] [    INFO][0m - ============================================================[0m
[32m[2022-09-17 17:35:20,995] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-17 17:35:20,995] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-17 17:35:20,996] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-17 17:35:20,996] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-17 17:35:20,996] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-17 17:35:20,996] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-17 17:35:20,996] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-17 17:35:20,996] [    INFO][0m - [0m
[32m[2022-09-17 17:35:20,996] [    INFO][0m - ============================================================[0m
[32m[2022-09-17 17:35:20,996] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-17 17:35:20,996] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-17 17:35:20,996] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-17 17:35:20,996] [    INFO][0m - pretrained                    :../checkpoints_cmnli/checkpoint-22000/model_state.pdparams[0m
[32m[2022-09-17 17:35:20,996] [    INFO][0m - prompt                        :â€œ{'text':'text_a'}â€è¿™ç¯‡æ–‡çŒ®çš„ç±»åˆ«æ˜¯{'mask'}{'mask'}ã€‚[0m
[32m[2022-09-17 17:35:20,996] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-17 17:35:20,997] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-17 17:35:20,997] [    INFO][0m - task_name                     :csldcp[0m
[32m[2022-09-17 17:35:20,997] [    INFO][0m - [0m
[32m[2022-09-17 17:35:20,997] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0917 17:35:20.998487 20736 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0917 17:35:21.002548 20736 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-17 17:35:26,777] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-17 17:35:26,788] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-17 17:35:26,788] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-17 17:35:26,789] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': 'â€œ'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': 'â€è¿™ç¯‡æ–‡çŒ®çš„ç±»åˆ«æ˜¯'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'ã€‚'}][0m
[32m[2022-09-17 17:35:28,701] [    INFO][0m - ============================================================[0m
[32m[2022-09-17 17:35:28,702] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-17 17:35:28,702] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-17 17:35:28,702] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-17 17:35:28,702] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-17 17:35:28,702] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-17 17:35:28,702] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-17 17:35:28,702] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-17 17:35:28,702] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-17 17:35:28,702] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-17 17:35:28,702] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-17 17:35:28,703] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-17 17:35:28,703] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-17 17:35:28,703] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-17 17:35:28,703] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-17 17:35:28,703] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-17 17:35:28,703] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-17 17:35:28,703] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-17 17:35:28,703] [    INFO][0m - eval_batch_size               :4[0m
[32m[2022-09-17 17:35:28,703] [    INFO][0m - eval_steps                    :200[0m
[32m[2022-09-17 17:35:28,703] [    INFO][0m - evaluation_strategy           :IntervalStrategy.EPOCH[0m
[32m[2022-09-17 17:35:28,703] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-17 17:35:28,703] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-17 17:35:28,703] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-17 17:35:28,704] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-17 17:35:28,704] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-17 17:35:28,704] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-17 17:35:28,704] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-17 17:35:28,704] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-17 17:35:28,704] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-17 17:35:28,704] [    INFO][0m - learning_rate                 :3e-06[0m
[32m[2022-09-17 17:35:28,704] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-17 17:35:28,704] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-17 17:35:28,704] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-17 17:35:28,704] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-17 17:35:28,704] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-17 17:35:28,704] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-17 17:35:28,705] [    INFO][0m - logging_dir                   :./checkpoints_csldcp/runs/Sep17_17-35-20_instance-3bwob41y-01[0m
[32m[2022-09-17 17:35:28,705] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-17 17:35:28,705] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-17 17:35:28,705] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-17 17:35:28,705] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-17 17:35:28,705] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-17 17:35:28,705] [    INFO][0m - max_seq_length                :256[0m
[32m[2022-09-17 17:35:28,705] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-17 17:35:28,705] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-17 17:35:28,705] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-17 17:35:28,705] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-17 17:35:28,705] [    INFO][0m - num_train_epochs              :10.0[0m
[32m[2022-09-17 17:35:28,705] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-17 17:35:28,706] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-17 17:35:28,706] [    INFO][0m - output_dir                    :./checkpoints_csldcp/[0m
[32m[2022-09-17 17:35:28,706] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-17 17:35:28,706] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-17 17:35:28,706] [    INFO][0m - per_device_eval_batch_size    :4[0m
[32m[2022-09-17 17:35:28,706] [    INFO][0m - per_device_train_batch_size   :16[0m
[32m[2022-09-17 17:35:28,706] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-17 17:35:28,706] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-17 17:35:28,706] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-17 17:35:28,706] [    INFO][0m - ppt_learning_rate             :3e-05[0m
[32m[2022-09-17 17:35:28,706] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-17 17:35:28,706] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-17 17:35:28,706] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-17 17:35:28,706] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-17 17:35:28,707] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-17 17:35:28,707] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-17 17:35:28,707] [    INFO][0m - run_name                      :./checkpoints_csldcp/[0m
[32m[2022-09-17 17:35:28,707] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-17 17:35:28,707] [    INFO][0m - save_steps                    :200[0m
[32m[2022-09-17 17:35:28,707] [    INFO][0m - save_strategy                 :IntervalStrategy.EPOCH[0m
[32m[2022-09-17 17:35:28,707] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-17 17:35:28,707] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-17 17:35:28,707] [    INFO][0m - seed                          :42[0m
[32m[2022-09-17 17:35:28,707] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-17 17:35:28,707] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-17 17:35:28,707] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-17 17:35:28,707] [    INFO][0m - train_batch_size              :16[0m
[32m[2022-09-17 17:35:28,708] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-17 17:35:28,708] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-17 17:35:28,708] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-17 17:35:28,708] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-17 17:35:28,708] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-17 17:35:28,708] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-17 17:35:28,708] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-17 17:35:28,708] [    INFO][0m - [0m
[32m[2022-09-17 17:35:28,711] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-17 17:35:28,711] [    INFO][0m -   Num examples = 2036[0m
[32m[2022-09-17 17:35:28,711] [    INFO][0m -   Num Epochs = 10[0m
[32m[2022-09-17 17:35:28,711] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2022-09-17 17:35:28,711] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2022-09-17 17:35:28,711] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-17 17:35:28,711] [    INFO][0m -   Total optimization steps = 1280.0[0m
[32m[2022-09-17 17:35:28,711] [    INFO][0m -   Total num train samples = 20360[0m
[33m[2022-09-17 17:35:28,748] [ WARNING][0m - Token indices sequence length is longer than the specified maximum sequence length for this model (517 > 512). Running this sequence through the model will result in indexing errors[0m
[32m[2022-09-17 17:35:36,030] [    INFO][0m - loss: 7.20677338, learning_rate: 2.9765625e-06, global_step: 10, interval_runtime: 7.3178, interval_samples_per_second: 2.186, interval_steps_per_second: 1.367, epoch: 0.0781[0m
[32m[2022-09-17 17:35:42,388] [    INFO][0m - loss: 3.53501434, learning_rate: 2.953125e-06, global_step: 20, interval_runtime: 6.3579, interval_samples_per_second: 2.517, interval_steps_per_second: 1.573, epoch: 0.1562[0m
[32m[2022-09-17 17:35:48,768] [    INFO][0m - loss: 2.88119545, learning_rate: 2.9296875e-06, global_step: 30, interval_runtime: 6.38, interval_samples_per_second: 2.508, interval_steps_per_second: 1.567, epoch: 0.2344[0m
[32m[2022-09-17 17:35:55,157] [    INFO][0m - loss: 2.68164482, learning_rate: 2.90625e-06, global_step: 40, interval_runtime: 6.389, interval_samples_per_second: 2.504, interval_steps_per_second: 1.565, epoch: 0.3125[0m
[32m[2022-09-17 17:36:01,534] [    INFO][0m - loss: 2.46466446, learning_rate: 2.8828125e-06, global_step: 50, interval_runtime: 6.3766, interval_samples_per_second: 2.509, interval_steps_per_second: 1.568, epoch: 0.3906[0m
[32m[2022-09-17 17:36:07,904] [    INFO][0m - loss: 2.37659187, learning_rate: 2.859375e-06, global_step: 60, interval_runtime: 6.3698, interval_samples_per_second: 2.512, interval_steps_per_second: 1.57, epoch: 0.4688[0m
[32m[2022-09-17 17:36:14,302] [    INFO][0m - loss: 2.23544769, learning_rate: 2.8359375e-06, global_step: 70, interval_runtime: 6.3986, interval_samples_per_second: 2.501, interval_steps_per_second: 1.563, epoch: 0.5469[0m
[32m[2022-09-17 17:36:20,717] [    INFO][0m - loss: 2.02423, learning_rate: 2.8125e-06, global_step: 80, interval_runtime: 6.4153, interval_samples_per_second: 2.494, interval_steps_per_second: 1.559, epoch: 0.625[0m
[32m[2022-09-17 17:36:27,095] [    INFO][0m - loss: 2.04304466, learning_rate: 2.7890625e-06, global_step: 90, interval_runtime: 6.3778, interval_samples_per_second: 2.509, interval_steps_per_second: 1.568, epoch: 0.7031[0m
[32m[2022-09-17 17:36:33,492] [    INFO][0m - loss: 2.05232658, learning_rate: 2.765625e-06, global_step: 100, interval_runtime: 6.3969, interval_samples_per_second: 2.501, interval_steps_per_second: 1.563, epoch: 0.7812[0m
[32m[2022-09-17 17:36:39,885] [    INFO][0m - loss: 1.93775654, learning_rate: 2.7421875e-06, global_step: 110, interval_runtime: 6.3927, interval_samples_per_second: 2.503, interval_steps_per_second: 1.564, epoch: 0.8594[0m
[32m[2022-09-17 17:36:46,288] [    INFO][0m - loss: 1.93912296, learning_rate: 2.71875e-06, global_step: 120, interval_runtime: 6.4034, interval_samples_per_second: 2.499, interval_steps_per_second: 1.562, epoch: 0.9375[0m
[32m[2022-09-17 17:36:50,841] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 17:36:50,842] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-17 17:36:50,842] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 17:36:50,842] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 17:36:50,842] [    INFO][0m -   Total prediction steps = 517[0m
[32m[2022-09-17 17:37:27,825] [    INFO][0m - eval_loss: 1.6739192008972168, eval_accuracy: 0.4946808510638298, eval_runtime: 36.9826, eval_samples_per_second: 55.918, eval_steps_per_second: 13.98, epoch: 1.0[0m
[32m[2022-09-17 17:37:27,860] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-128[0m
[32m[2022-09-17 17:37:27,860] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 17:37:30,701] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-128/tokenizer_config.json[0m
[32m[2022-09-17 17:37:30,702] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-128/special_tokens_map.json[0m
[32m[2022-09-17 17:37:37,574] [    INFO][0m - loss: 2.09838619, learning_rate: 2.6953125e-06, global_step: 130, interval_runtime: 51.286, interval_samples_per_second: 0.312, interval_steps_per_second: 0.195, epoch: 1.0156[0m
[32m[2022-09-17 17:37:47,831] [    INFO][0m - loss: 1.50648441, learning_rate: 2.671875e-06, global_step: 140, interval_runtime: 6.3712, interval_samples_per_second: 2.511, interval_steps_per_second: 1.57, epoch: 1.0938[0m
[32m[2022-09-17 17:37:54,208] [    INFO][0m - loss: 1.69160385, learning_rate: 2.6484375e-06, global_step: 150, interval_runtime: 10.2626, interval_samples_per_second: 1.559, interval_steps_per_second: 0.974, epoch: 1.1719[0m
[32m[2022-09-17 17:38:00,602] [    INFO][0m - loss: 1.48252621, learning_rate: 2.6250000000000003e-06, global_step: 160, interval_runtime: 6.3941, interval_samples_per_second: 2.502, interval_steps_per_second: 1.564, epoch: 1.25[0m
[32m[2022-09-17 17:38:07,018] [    INFO][0m - loss: 1.61439629, learning_rate: 2.6015625e-06, global_step: 170, interval_runtime: 6.4162, interval_samples_per_second: 2.494, interval_steps_per_second: 1.559, epoch: 1.3281[0m
[32m[2022-09-17 17:38:13,443] [    INFO][0m - loss: 1.67620869, learning_rate: 2.578125e-06, global_step: 180, interval_runtime: 6.4244, interval_samples_per_second: 2.491, interval_steps_per_second: 1.557, epoch: 1.4062[0m
[32m[2022-09-17 17:38:19,855] [    INFO][0m - loss: 1.58533478, learning_rate: 2.5546875000000003e-06, global_step: 190, interval_runtime: 6.4118, interval_samples_per_second: 2.495, interval_steps_per_second: 1.56, epoch: 1.4844[0m
[32m[2022-09-17 17:38:26,259] [    INFO][0m - loss: 1.74753227, learning_rate: 2.53125e-06, global_step: 200, interval_runtime: 6.4051, interval_samples_per_second: 2.498, interval_steps_per_second: 1.561, epoch: 1.5625[0m
[32m[2022-09-17 17:38:32,660] [    INFO][0m - loss: 1.69987507, learning_rate: 2.5078125e-06, global_step: 210, interval_runtime: 6.4005, interval_samples_per_second: 2.5, interval_steps_per_second: 1.562, epoch: 1.6406[0m
[32m[2022-09-17 17:38:39,069] [    INFO][0m - loss: 1.63642769, learning_rate: 2.4843750000000002e-06, global_step: 220, interval_runtime: 6.4089, interval_samples_per_second: 2.497, interval_steps_per_second: 1.56, epoch: 1.7188[0m
[32m[2022-09-17 17:38:45,457] [    INFO][0m - loss: 1.602281, learning_rate: 2.4609375e-06, global_step: 230, interval_runtime: 6.3883, interval_samples_per_second: 2.505, interval_steps_per_second: 1.565, epoch: 1.7969[0m
[32m[2022-09-17 17:38:51,865] [    INFO][0m - loss: 1.61732407, learning_rate: 2.4375e-06, global_step: 240, interval_runtime: 6.4076, interval_samples_per_second: 2.497, interval_steps_per_second: 1.561, epoch: 1.875[0m
[32m[2022-09-17 17:38:58,276] [    INFO][0m - loss: 1.70696144, learning_rate: 2.4140625000000002e-06, global_step: 250, interval_runtime: 6.4109, interval_samples_per_second: 2.496, interval_steps_per_second: 1.56, epoch: 1.9531[0m
[32m[2022-09-17 17:39:01,533] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 17:39:01,533] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-17 17:39:01,533] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 17:39:01,533] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 17:39:01,533] [    INFO][0m -   Total prediction steps = 517[0m
[32m[2022-09-17 17:39:38,469] [    INFO][0m - eval_loss: 1.4366004467010498, eval_accuracy: 0.528046421663443, eval_runtime: 36.9348, eval_samples_per_second: 55.991, eval_steps_per_second: 13.998, epoch: 2.0[0m
[32m[2022-09-17 17:39:38,508] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-256[0m
[32m[2022-09-17 17:39:38,508] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 17:39:41,301] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-256/tokenizer_config.json[0m
[32m[2022-09-17 17:39:41,301] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-256/special_tokens_map.json[0m
[32m[2022-09-17 17:39:49,810] [    INFO][0m - loss: 1.39668579, learning_rate: 2.390625e-06, global_step: 260, interval_runtime: 51.5335, interval_samples_per_second: 0.31, interval_steps_per_second: 0.194, epoch: 2.0312[0m
[32m[2022-09-17 17:39:56,201] [    INFO][0m - loss: 1.46367579, learning_rate: 2.3671875e-06, global_step: 270, interval_runtime: 6.3918, interval_samples_per_second: 2.503, interval_steps_per_second: 1.565, epoch: 2.1094[0m
[32m[2022-09-17 17:40:02,624] [    INFO][0m - loss: 1.45321741, learning_rate: 2.3437500000000002e-06, global_step: 280, interval_runtime: 6.4224, interval_samples_per_second: 2.491, interval_steps_per_second: 1.557, epoch: 2.1875[0m
[32m[2022-09-17 17:40:09,051] [    INFO][0m - loss: 1.33940783, learning_rate: 2.3203125e-06, global_step: 290, interval_runtime: 6.4277, interval_samples_per_second: 2.489, interval_steps_per_second: 1.556, epoch: 2.2656[0m
[32m[2022-09-17 17:40:20,200] [    INFO][0m - loss: 1.53983231, learning_rate: 2.296875e-06, global_step: 300, interval_runtime: 6.4525, interval_samples_per_second: 2.48, interval_steps_per_second: 1.55, epoch: 2.3438[0m
[32m[2022-09-17 17:40:26,588] [    INFO][0m - loss: 1.36916323, learning_rate: 2.2734375e-06, global_step: 310, interval_runtime: 11.0845, interval_samples_per_second: 1.443, interval_steps_per_second: 0.902, epoch: 2.4219[0m
[32m[2022-09-17 17:40:33,002] [    INFO][0m - loss: 1.26997604, learning_rate: 2.25e-06, global_step: 320, interval_runtime: 6.4136, interval_samples_per_second: 2.495, interval_steps_per_second: 1.559, epoch: 2.5[0m
[32m[2022-09-17 17:40:39,400] [    INFO][0m - loss: 1.296527, learning_rate: 2.2265625e-06, global_step: 330, interval_runtime: 6.398, interval_samples_per_second: 2.501, interval_steps_per_second: 1.563, epoch: 2.5781[0m
[32m[2022-09-17 17:40:45,812] [    INFO][0m - loss: 1.41053886, learning_rate: 2.203125e-06, global_step: 340, interval_runtime: 6.4125, interval_samples_per_second: 2.495, interval_steps_per_second: 1.559, epoch: 2.6562[0m
[32m[2022-09-17 17:40:52,222] [    INFO][0m - loss: 1.35673733, learning_rate: 2.1796875e-06, global_step: 350, interval_runtime: 6.4097, interval_samples_per_second: 2.496, interval_steps_per_second: 1.56, epoch: 2.7344[0m
[32m[2022-09-17 17:40:58,637] [    INFO][0m - loss: 1.36614132, learning_rate: 2.15625e-06, global_step: 360, interval_runtime: 6.4154, interval_samples_per_second: 2.494, interval_steps_per_second: 1.559, epoch: 2.8125[0m
[32m[2022-09-17 17:41:05,046] [    INFO][0m - loss: 1.42204771, learning_rate: 2.1328125e-06, global_step: 370, interval_runtime: 6.4082, interval_samples_per_second: 2.497, interval_steps_per_second: 1.561, epoch: 2.8906[0m
[32m[2022-09-17 17:41:11,410] [    INFO][0m - loss: 1.31159096, learning_rate: 2.109375e-06, global_step: 380, interval_runtime: 6.3641, interval_samples_per_second: 2.514, interval_steps_per_second: 1.571, epoch: 2.9688[0m
[32m[2022-09-17 17:41:13,435] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 17:41:14,647] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-17 17:41:14,648] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 17:41:14,648] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 17:41:14,648] [    INFO][0m -   Total prediction steps = 517[0m
[32m[2022-09-17 17:41:51,618] [    INFO][0m - eval_loss: 1.3325481414794922, eval_accuracy: 0.5386847195357833, eval_runtime: 38.1824, eval_samples_per_second: 54.161, eval_steps_per_second: 13.54, epoch: 3.0[0m
[32m[2022-09-17 17:41:51,653] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-384[0m
[32m[2022-09-17 17:41:51,653] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 17:41:54,635] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-384/tokenizer_config.json[0m
[32m[2022-09-17 17:41:54,636] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-384/special_tokens_map.json[0m
[32m[2022-09-17 17:42:05,477] [    INFO][0m - loss: 1.24604864, learning_rate: 2.0859375e-06, global_step: 390, interval_runtime: 54.0675, interval_samples_per_second: 0.296, interval_steps_per_second: 0.185, epoch: 3.0469[0m
[32m[2022-09-17 17:42:11,861] [    INFO][0m - loss: 1.27676096, learning_rate: 2.0625e-06, global_step: 400, interval_runtime: 6.3839, interval_samples_per_second: 2.506, interval_steps_per_second: 1.566, epoch: 3.125[0m
[32m[2022-09-17 17:42:18,268] [    INFO][0m - loss: 1.16519432, learning_rate: 2.0390625e-06, global_step: 410, interval_runtime: 6.4074, interval_samples_per_second: 2.497, interval_steps_per_second: 1.561, epoch: 3.2031[0m
[32m[2022-09-17 17:42:24,683] [    INFO][0m - loss: 1.0756443, learning_rate: 2.015625e-06, global_step: 420, interval_runtime: 6.4149, interval_samples_per_second: 2.494, interval_steps_per_second: 1.559, epoch: 3.2812[0m
[32m[2022-09-17 17:42:31,567] [    INFO][0m - loss: 1.08438177, learning_rate: 1.9921875e-06, global_step: 430, interval_runtime: 6.4066, interval_samples_per_second: 2.497, interval_steps_per_second: 1.561, epoch: 3.3594[0m
[32m[2022-09-17 17:42:37,964] [    INFO][0m - loss: 1.01314707, learning_rate: 1.96875e-06, global_step: 440, interval_runtime: 6.8739, interval_samples_per_second: 2.328, interval_steps_per_second: 1.455, epoch: 3.4375[0m
[32m[2022-09-17 17:42:44,365] [    INFO][0m - loss: 0.95923901, learning_rate: 1.9453125e-06, global_step: 450, interval_runtime: 6.4011, interval_samples_per_second: 2.5, interval_steps_per_second: 1.562, epoch: 3.5156[0m
[32m[2022-09-17 17:42:50,769] [    INFO][0m - loss: 1.31490221, learning_rate: 1.921875e-06, global_step: 460, interval_runtime: 6.4041, interval_samples_per_second: 2.498, interval_steps_per_second: 1.562, epoch: 3.5938[0m
[32m[2022-09-17 17:42:57,161] [    INFO][0m - loss: 1.08385868, learning_rate: 1.8984375e-06, global_step: 470, interval_runtime: 6.3921, interval_samples_per_second: 2.503, interval_steps_per_second: 1.564, epoch: 3.6719[0m
[32m[2022-09-17 17:43:03,582] [    INFO][0m - loss: 1.25332775, learning_rate: 1.875e-06, global_step: 480, interval_runtime: 6.4213, interval_samples_per_second: 2.492, interval_steps_per_second: 1.557, epoch: 3.75[0m
[32m[2022-09-17 17:43:09,996] [    INFO][0m - loss: 1.24174948, learning_rate: 1.8515625000000001e-06, global_step: 490, interval_runtime: 6.4133, interval_samples_per_second: 2.495, interval_steps_per_second: 1.559, epoch: 3.8281[0m
[32m[2022-09-17 17:43:16,406] [    INFO][0m - loss: 1.14446735, learning_rate: 1.828125e-06, global_step: 500, interval_runtime: 6.4105, interval_samples_per_second: 2.496, interval_steps_per_second: 1.56, epoch: 3.9062[0m
[32m[2022-09-17 17:43:22,722] [    INFO][0m - loss: 1.1407198, learning_rate: 1.8046875e-06, global_step: 510, interval_runtime: 6.3157, interval_samples_per_second: 2.533, interval_steps_per_second: 1.583, epoch: 3.9844[0m
[32m[2022-09-17 17:43:23,518] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 17:43:23,519] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-17 17:43:23,519] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 17:43:23,519] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 17:43:23,519] [    INFO][0m -   Total prediction steps = 517[0m
[32m[2022-09-17 17:44:00,610] [    INFO][0m - eval_loss: 1.3143279552459717, eval_accuracy: 0.561411992263056, eval_runtime: 37.0911, eval_samples_per_second: 55.755, eval_steps_per_second: 13.939, epoch: 4.0[0m
[32m[2022-09-17 17:44:00,646] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-512[0m
[32m[2022-09-17 17:44:00,647] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 17:44:03,737] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-512/tokenizer_config.json[0m
[32m[2022-09-17 17:44:03,738] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-512/special_tokens_map.json[0m
[32m[2022-09-17 17:44:14,506] [    INFO][0m - loss: 1.2090951, learning_rate: 1.78125e-06, global_step: 520, interval_runtime: 51.7839, interval_samples_per_second: 0.309, interval_steps_per_second: 0.193, epoch: 4.0625[0m
[32m[2022-09-17 17:44:20,976] [    INFO][0m - loss: 0.95896082, learning_rate: 1.7578125e-06, global_step: 530, interval_runtime: 6.4698, interval_samples_per_second: 2.473, interval_steps_per_second: 1.546, epoch: 4.1406[0m
[32m[2022-09-17 17:44:27,391] [    INFO][0m - loss: 1.00342855, learning_rate: 1.734375e-06, global_step: 540, interval_runtime: 6.4152, interval_samples_per_second: 2.494, interval_steps_per_second: 1.559, epoch: 4.2188[0m
[32m[2022-09-17 17:44:33,801] [    INFO][0m - loss: 0.92886124, learning_rate: 1.7109375e-06, global_step: 550, interval_runtime: 6.4097, interval_samples_per_second: 2.496, interval_steps_per_second: 1.56, epoch: 4.2969[0m
[32m[2022-09-17 17:44:40,193] [    INFO][0m - loss: 0.95320187, learning_rate: 1.6875e-06, global_step: 560, interval_runtime: 6.3918, interval_samples_per_second: 2.503, interval_steps_per_second: 1.565, epoch: 4.375[0m
[32m[2022-09-17 17:44:46,606] [    INFO][0m - loss: 1.22704592, learning_rate: 1.6640625e-06, global_step: 570, interval_runtime: 6.4128, interval_samples_per_second: 2.495, interval_steps_per_second: 1.559, epoch: 4.4531[0m
[32m[2022-09-17 17:44:53,037] [    INFO][0m - loss: 1.09770355, learning_rate: 1.640625e-06, global_step: 580, interval_runtime: 6.4321, interval_samples_per_second: 2.488, interval_steps_per_second: 1.555, epoch: 4.5312[0m
[32m[2022-09-17 17:44:59,436] [    INFO][0m - loss: 1.1068531, learning_rate: 1.6171875000000001e-06, global_step: 590, interval_runtime: 6.3978, interval_samples_per_second: 2.501, interval_steps_per_second: 1.563, epoch: 4.6094[0m
[32m[2022-09-17 17:45:05,846] [    INFO][0m - loss: 1.06869602, learning_rate: 1.59375e-06, global_step: 600, interval_runtime: 6.4103, interval_samples_per_second: 2.496, interval_steps_per_second: 1.56, epoch: 4.6875[0m
[32m[2022-09-17 17:45:12,237] [    INFO][0m - loss: 0.89599209, learning_rate: 1.5703125e-06, global_step: 610, interval_runtime: 6.3912, interval_samples_per_second: 2.503, interval_steps_per_second: 1.565, epoch: 4.7656[0m
[32m[2022-09-17 17:45:18,636] [    INFO][0m - loss: 0.98737698, learning_rate: 1.5468750000000001e-06, global_step: 620, interval_runtime: 6.3995, interval_samples_per_second: 2.5, interval_steps_per_second: 1.563, epoch: 4.8438[0m
[32m[2022-09-17 17:45:25,040] [    INFO][0m - loss: 0.98473568, learning_rate: 1.5234375e-06, global_step: 630, interval_runtime: 6.4038, interval_samples_per_second: 2.499, interval_steps_per_second: 1.562, epoch: 4.9219[0m
[32m[2022-09-17 17:45:30,863] [    INFO][0m - loss: 1.09197578, learning_rate: 1.5e-06, global_step: 640, interval_runtime: 5.823, interval_samples_per_second: 2.748, interval_steps_per_second: 1.717, epoch: 5.0[0m
[32m[2022-09-17 17:45:30,864] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 17:45:30,864] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-17 17:45:30,864] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 17:45:30,864] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 17:45:30,864] [    INFO][0m -   Total prediction steps = 517[0m
[32m[2022-09-17 17:46:07,876] [    INFO][0m - eval_loss: 1.2874431610107422, eval_accuracy: 0.5560928433268859, eval_runtime: 37.0115, eval_samples_per_second: 55.874, eval_steps_per_second: 13.969, epoch: 5.0[0m
[32m[2022-09-17 17:46:07,911] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-640[0m
[32m[2022-09-17 17:46:07,911] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 17:46:10,712] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-640/tokenizer_config.json[0m
[32m[2022-09-17 17:46:10,712] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-640/special_tokens_map.json[0m
[32m[2022-09-17 17:46:22,654] [    INFO][0m - loss: 0.94741983, learning_rate: 1.4765625e-06, global_step: 650, interval_runtime: 51.7905, interval_samples_per_second: 0.309, interval_steps_per_second: 0.193, epoch: 5.0781[0m
[32m[2022-09-17 17:46:29,055] [    INFO][0m - loss: 0.95011959, learning_rate: 1.453125e-06, global_step: 660, interval_runtime: 6.4015, interval_samples_per_second: 2.499, interval_steps_per_second: 1.562, epoch: 5.1562[0m
[32m[2022-09-17 17:46:35,510] [    INFO][0m - loss: 0.9427577, learning_rate: 1.4296875e-06, global_step: 670, interval_runtime: 6.4545, interval_samples_per_second: 2.479, interval_steps_per_second: 1.549, epoch: 5.2344[0m
[32m[2022-09-17 17:46:41,918] [    INFO][0m - loss: 0.96869678, learning_rate: 1.40625e-06, global_step: 680, interval_runtime: 6.4087, interval_samples_per_second: 2.497, interval_steps_per_second: 1.56, epoch: 5.3125[0m
[32m[2022-09-17 17:46:48,328] [    INFO][0m - loss: 0.97059917, learning_rate: 1.3828125e-06, global_step: 690, interval_runtime: 6.4098, interval_samples_per_second: 2.496, interval_steps_per_second: 1.56, epoch: 5.3906[0m
[32m[2022-09-17 17:46:54,724] [    INFO][0m - loss: 0.93292084, learning_rate: 1.359375e-06, global_step: 700, interval_runtime: 6.396, interval_samples_per_second: 2.502, interval_steps_per_second: 1.563, epoch: 5.4688[0m
[32m[2022-09-17 17:47:01,139] [    INFO][0m - loss: 0.90510788, learning_rate: 1.3359375e-06, global_step: 710, interval_runtime: 6.4149, interval_samples_per_second: 2.494, interval_steps_per_second: 1.559, epoch: 5.5469[0m
[32m[2022-09-17 17:47:07,533] [    INFO][0m - loss: 0.82326393, learning_rate: 1.3125000000000001e-06, global_step: 720, interval_runtime: 6.3938, interval_samples_per_second: 2.502, interval_steps_per_second: 1.564, epoch: 5.625[0m
[32m[2022-09-17 17:47:13,936] [    INFO][0m - loss: 0.90413408, learning_rate: 1.2890625e-06, global_step: 730, interval_runtime: 6.4026, interval_samples_per_second: 2.499, interval_steps_per_second: 1.562, epoch: 5.7031[0m
[32m[2022-09-17 17:47:20,356] [    INFO][0m - loss: 0.93944836, learning_rate: 1.265625e-06, global_step: 740, interval_runtime: 6.421, interval_samples_per_second: 2.492, interval_steps_per_second: 1.557, epoch: 5.7812[0m
[32m[2022-09-17 17:47:26,765] [    INFO][0m - loss: 0.77629404, learning_rate: 1.2421875000000001e-06, global_step: 750, interval_runtime: 6.4089, interval_samples_per_second: 2.497, interval_steps_per_second: 1.56, epoch: 5.8594[0m
[32m[2022-09-17 17:47:33,192] [    INFO][0m - loss: 0.83320446, learning_rate: 1.21875e-06, global_step: 760, interval_runtime: 6.4271, interval_samples_per_second: 2.489, interval_steps_per_second: 1.556, epoch: 5.9375[0m
[32m[2022-09-17 17:47:37,748] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 17:47:37,748] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-17 17:47:37,748] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 17:47:37,749] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 17:47:37,749] [    INFO][0m -   Total prediction steps = 517[0m
[32m[2022-09-17 17:48:14,648] [    INFO][0m - eval_loss: 1.2803593873977661, eval_accuracy: 0.5647969052224371, eval_runtime: 36.8978, eval_samples_per_second: 56.047, eval_steps_per_second: 14.012, epoch: 6.0[0m
[32m[2022-09-17 17:48:14,683] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-768[0m
[32m[2022-09-17 17:48:14,683] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 17:48:17,459] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-768/tokenizer_config.json[0m
[32m[2022-09-17 17:48:17,460] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-768/special_tokens_map.json[0m
[32m[2022-09-17 17:48:24,139] [    INFO][0m - loss: 0.76782346, learning_rate: 1.1953125e-06, global_step: 770, interval_runtime: 50.9462, interval_samples_per_second: 0.314, interval_steps_per_second: 0.196, epoch: 6.0156[0m
[32m[2022-09-17 17:48:30,563] [    INFO][0m - loss: 0.66406198, learning_rate: 1.1718750000000001e-06, global_step: 780, interval_runtime: 6.4247, interval_samples_per_second: 2.49, interval_steps_per_second: 1.556, epoch: 6.0938[0m
[32m[2022-09-17 17:48:36,971] [    INFO][0m - loss: 0.76776361, learning_rate: 1.1484375e-06, global_step: 790, interval_runtime: 6.4072, interval_samples_per_second: 2.497, interval_steps_per_second: 1.561, epoch: 6.1719[0m
[32m[2022-09-17 17:48:43,398] [    INFO][0m - loss: 0.76867318, learning_rate: 1.125e-06, global_step: 800, interval_runtime: 6.4272, interval_samples_per_second: 2.489, interval_steps_per_second: 1.556, epoch: 6.25[0m
[32m[2022-09-17 17:48:49,804] [    INFO][0m - loss: 0.80916729, learning_rate: 1.1015625e-06, global_step: 810, interval_runtime: 6.406, interval_samples_per_second: 2.498, interval_steps_per_second: 1.561, epoch: 6.3281[0m
[32m[2022-09-17 17:48:56,213] [    INFO][0m - loss: 0.79109874, learning_rate: 1.078125e-06, global_step: 820, interval_runtime: 6.4095, interval_samples_per_second: 2.496, interval_steps_per_second: 1.56, epoch: 6.4062[0m
[32m[2022-09-17 17:49:02,605] [    INFO][0m - loss: 0.81829052, learning_rate: 1.0546875e-06, global_step: 830, interval_runtime: 6.3917, interval_samples_per_second: 2.503, interval_steps_per_second: 1.565, epoch: 6.4844[0m
[32m[2022-09-17 17:49:09,027] [    INFO][0m - loss: 1.0314436, learning_rate: 1.03125e-06, global_step: 840, interval_runtime: 6.4224, interval_samples_per_second: 2.491, interval_steps_per_second: 1.557, epoch: 6.5625[0m
[32m[2022-09-17 17:49:15,422] [    INFO][0m - loss: 0.73749013, learning_rate: 1.0078125e-06, global_step: 850, interval_runtime: 6.3948, interval_samples_per_second: 2.502, interval_steps_per_second: 1.564, epoch: 6.6406[0m
[32m[2022-09-17 17:49:21,804] [    INFO][0m - loss: 0.82054758, learning_rate: 9.84375e-07, global_step: 860, interval_runtime: 6.3821, interval_samples_per_second: 2.507, interval_steps_per_second: 1.567, epoch: 6.7188[0m
[32m[2022-09-17 17:49:28,216] [    INFO][0m - loss: 0.82189722, learning_rate: 9.609375e-07, global_step: 870, interval_runtime: 6.4121, interval_samples_per_second: 2.495, interval_steps_per_second: 1.56, epoch: 6.7969[0m
[32m[2022-09-17 17:49:34,644] [    INFO][0m - loss: 0.82842417, learning_rate: 9.375e-07, global_step: 880, interval_runtime: 6.4275, interval_samples_per_second: 2.489, interval_steps_per_second: 1.556, epoch: 6.875[0m
[32m[2022-09-17 17:49:41,043] [    INFO][0m - loss: 0.95559225, learning_rate: 9.140625e-07, global_step: 890, interval_runtime: 6.3988, interval_samples_per_second: 2.5, interval_steps_per_second: 1.563, epoch: 6.9531[0m
[32m[2022-09-17 17:49:44,318] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 17:49:44,318] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-17 17:49:44,318] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 17:49:44,318] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 17:49:44,318] [    INFO][0m -   Total prediction steps = 517[0m
[32m[2022-09-17 17:50:21,210] [    INFO][0m - eval_loss: 1.286360502243042, eval_accuracy: 0.561411992263056, eval_runtime: 36.891, eval_samples_per_second: 56.057, eval_steps_per_second: 14.014, epoch: 7.0[0m
[32m[2022-09-17 17:50:21,245] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-896[0m
[32m[2022-09-17 17:50:21,245] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 17:50:24,045] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-896/tokenizer_config.json[0m
[32m[2022-09-17 17:50:24,045] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-896/special_tokens_map.json[0m
[32m[2022-09-17 17:50:31,866] [    INFO][0m - loss: 0.86255999, learning_rate: 8.90625e-07, global_step: 900, interval_runtime: 50.8238, interval_samples_per_second: 0.315, interval_steps_per_second: 0.197, epoch: 7.0312[0m
[32m[2022-09-17 17:50:38,276] [    INFO][0m - loss: 0.71969328, learning_rate: 8.671875e-07, global_step: 910, interval_runtime: 6.4099, interval_samples_per_second: 2.496, interval_steps_per_second: 1.56, epoch: 7.1094[0m
[32m[2022-09-17 17:50:44,682] [    INFO][0m - loss: 0.74388924, learning_rate: 8.4375e-07, global_step: 920, interval_runtime: 6.4061, interval_samples_per_second: 2.498, interval_steps_per_second: 1.561, epoch: 7.1875[0m
[32m[2022-09-17 17:50:51,105] [    INFO][0m - loss: 0.74816952, learning_rate: 8.203125e-07, global_step: 930, interval_runtime: 6.4228, interval_samples_per_second: 2.491, interval_steps_per_second: 1.557, epoch: 7.2656[0m
[32m[2022-09-17 17:50:57,520] [    INFO][0m - loss: 0.76418667, learning_rate: 7.96875e-07, global_step: 940, interval_runtime: 6.4146, interval_samples_per_second: 2.494, interval_steps_per_second: 1.559, epoch: 7.3438[0m
[32m[2022-09-17 17:51:03,914] [    INFO][0m - loss: 0.66896935, learning_rate: 7.734375000000001e-07, global_step: 950, interval_runtime: 6.3941, interval_samples_per_second: 2.502, interval_steps_per_second: 1.564, epoch: 7.4219[0m
[32m[2022-09-17 17:51:10,329] [    INFO][0m - loss: 0.76033168, learning_rate: 7.5e-07, global_step: 960, interval_runtime: 6.4151, interval_samples_per_second: 2.494, interval_steps_per_second: 1.559, epoch: 7.5[0m
[32m[2022-09-17 17:51:16,731] [    INFO][0m - loss: 0.66783257, learning_rate: 7.265625e-07, global_step: 970, interval_runtime: 6.4016, interval_samples_per_second: 2.499, interval_steps_per_second: 1.562, epoch: 7.5781[0m
[32m[2022-09-17 17:51:23,153] [    INFO][0m - loss: 0.7055088, learning_rate: 7.03125e-07, global_step: 980, interval_runtime: 6.4222, interval_samples_per_second: 2.491, interval_steps_per_second: 1.557, epoch: 7.6562[0m
[32m[2022-09-17 17:51:29,561] [    INFO][0m - loss: 0.82910957, learning_rate: 6.796875e-07, global_step: 990, interval_runtime: 6.4084, interval_samples_per_second: 2.497, interval_steps_per_second: 1.56, epoch: 7.7344[0m
[32m[2022-09-17 17:51:35,966] [    INFO][0m - loss: 0.72392011, learning_rate: 6.562500000000001e-07, global_step: 1000, interval_runtime: 6.4042, interval_samples_per_second: 2.498, interval_steps_per_second: 1.561, epoch: 7.8125[0m
[32m[2022-09-17 17:51:42,359] [    INFO][0m - loss: 0.80588846, learning_rate: 6.328125e-07, global_step: 1010, interval_runtime: 6.3937, interval_samples_per_second: 2.502, interval_steps_per_second: 1.564, epoch: 7.8906[0m
[32m[2022-09-17 17:51:48,719] [    INFO][0m - loss: 0.92263947, learning_rate: 6.09375e-07, global_step: 1020, interval_runtime: 6.3593, interval_samples_per_second: 2.516, interval_steps_per_second: 1.573, epoch: 7.9688[0m
[32m[2022-09-17 17:51:50,745] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 17:51:50,745] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-17 17:51:50,745] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 17:51:50,745] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 17:51:50,745] [    INFO][0m -   Total prediction steps = 517[0m
[32m[2022-09-17 17:52:27,659] [    INFO][0m - eval_loss: 1.3219513893127441, eval_accuracy: 0.5623791102514507, eval_runtime: 36.9132, eval_samples_per_second: 56.023, eval_steps_per_second: 14.006, epoch: 8.0[0m
[32m[2022-09-17 17:52:27,694] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-1024[0m
[32m[2022-09-17 17:52:27,694] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 17:52:30,399] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-1024/tokenizer_config.json[0m
[32m[2022-09-17 17:52:30,400] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-1024/special_tokens_map.json[0m
[32m[2022-09-17 17:52:39,500] [    INFO][0m - loss: 0.6709012, learning_rate: 5.859375000000001e-07, global_step: 1030, interval_runtime: 50.7812, interval_samples_per_second: 0.315, interval_steps_per_second: 0.197, epoch: 8.0469[0m
[32m[2022-09-17 17:52:46,033] [    INFO][0m - loss: 0.69596786, learning_rate: 5.625e-07, global_step: 1040, interval_runtime: 6.3996, interval_samples_per_second: 2.5, interval_steps_per_second: 1.563, epoch: 8.125[0m
[32m[2022-09-17 17:52:52,429] [    INFO][0m - loss: 0.65417686, learning_rate: 5.390625e-07, global_step: 1050, interval_runtime: 6.5291, interval_samples_per_second: 2.451, interval_steps_per_second: 1.532, epoch: 8.2031[0m
[32m[2022-09-17 17:52:58,832] [    INFO][0m - loss: 0.66045232, learning_rate: 5.15625e-07, global_step: 1060, interval_runtime: 6.4031, interval_samples_per_second: 2.499, interval_steps_per_second: 1.562, epoch: 8.2812[0m
[32m[2022-09-17 17:53:05,240] [    INFO][0m - loss: 0.74782982, learning_rate: 4.921875e-07, global_step: 1070, interval_runtime: 6.4081, interval_samples_per_second: 2.497, interval_steps_per_second: 1.561, epoch: 8.3594[0m
[32m[2022-09-17 17:53:11,645] [    INFO][0m - loss: 0.76450157, learning_rate: 4.6875e-07, global_step: 1080, interval_runtime: 6.4051, interval_samples_per_second: 2.498, interval_steps_per_second: 1.561, epoch: 8.4375[0m
[32m[2022-09-17 17:53:18,038] [    INFO][0m - loss: 0.73607388, learning_rate: 4.453125e-07, global_step: 1090, interval_runtime: 6.3932, interval_samples_per_second: 2.503, interval_steps_per_second: 1.564, epoch: 8.5156[0m
[32m[2022-09-17 17:53:24,434] [    INFO][0m - loss: 0.772368, learning_rate: 4.21875e-07, global_step: 1100, interval_runtime: 6.3961, interval_samples_per_second: 2.502, interval_steps_per_second: 1.563, epoch: 8.5938[0m
[32m[2022-09-17 17:53:30,842] [    INFO][0m - loss: 0.59748859, learning_rate: 3.984375e-07, global_step: 1110, interval_runtime: 6.4075, interval_samples_per_second: 2.497, interval_steps_per_second: 1.561, epoch: 8.6719[0m
[32m[2022-09-17 17:53:37,251] [    INFO][0m - loss: 0.75714555, learning_rate: 3.75e-07, global_step: 1120, interval_runtime: 6.4088, interval_samples_per_second: 2.497, interval_steps_per_second: 1.56, epoch: 8.75[0m
[32m[2022-09-17 17:53:43,671] [    INFO][0m - loss: 0.52357535, learning_rate: 3.515625e-07, global_step: 1130, interval_runtime: 6.4205, interval_samples_per_second: 2.492, interval_steps_per_second: 1.558, epoch: 8.8281[0m
[32m[2022-09-17 17:53:50,088] [    INFO][0m - loss: 0.70050578, learning_rate: 3.2812500000000003e-07, global_step: 1140, interval_runtime: 6.4171, interval_samples_per_second: 2.493, interval_steps_per_second: 1.558, epoch: 8.9062[0m
[32m[2022-09-17 17:53:56,406] [    INFO][0m - loss: 0.63871818, learning_rate: 3.046875e-07, global_step: 1150, interval_runtime: 6.3184, interval_samples_per_second: 2.532, interval_steps_per_second: 1.583, epoch: 8.9844[0m
[32m[2022-09-17 17:53:57,205] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 17:53:57,206] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-17 17:53:57,206] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 17:53:57,206] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 17:53:57,206] [    INFO][0m -   Total prediction steps = 517[0m
[32m[2022-09-17 17:54:34,091] [    INFO][0m - eval_loss: 1.300628662109375, eval_accuracy: 0.5667311411992263, eval_runtime: 36.8846, eval_samples_per_second: 56.067, eval_steps_per_second: 14.017, epoch: 9.0[0m
[32m[2022-09-17 17:54:34,126] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-1152[0m
[32m[2022-09-17 17:54:34,126] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 17:54:36,801] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-1152/tokenizer_config.json[0m
[32m[2022-09-17 17:54:36,801] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-1152/special_tokens_map.json[0m
[32m[2022-09-17 17:54:49,696] [    INFO][0m - loss: 0.66387553, learning_rate: 2.8125e-07, global_step: 1160, interval_runtime: 50.7577, interval_samples_per_second: 0.315, interval_steps_per_second: 0.197, epoch: 9.0625[0m
[32m[2022-09-17 17:54:56,093] [    INFO][0m - loss: 0.55512247, learning_rate: 2.578125e-07, global_step: 1170, interval_runtime: 8.9289, interval_samples_per_second: 1.792, interval_steps_per_second: 1.12, epoch: 9.1406[0m
[32m[2022-09-17 17:55:02,501] [    INFO][0m - loss: 0.63094387, learning_rate: 2.34375e-07, global_step: 1180, interval_runtime: 6.4084, interval_samples_per_second: 2.497, interval_steps_per_second: 1.56, epoch: 9.2188[0m
[32m[2022-09-17 17:55:08,892] [    INFO][0m - loss: 0.56048517, learning_rate: 2.109375e-07, global_step: 1190, interval_runtime: 6.391, interval_samples_per_second: 2.504, interval_steps_per_second: 1.565, epoch: 9.2969[0m
[32m[2022-09-17 17:55:15,302] [    INFO][0m - loss: 0.64359956, learning_rate: 1.875e-07, global_step: 1200, interval_runtime: 6.4091, interval_samples_per_second: 2.496, interval_steps_per_second: 1.56, epoch: 9.375[0m
[32m[2022-09-17 17:55:21,721] [    INFO][0m - loss: 0.68929362, learning_rate: 1.6406250000000002e-07, global_step: 1210, interval_runtime: 6.4195, interval_samples_per_second: 2.492, interval_steps_per_second: 1.558, epoch: 9.4531[0m
[32m[2022-09-17 17:55:28,145] [    INFO][0m - loss: 0.6758111, learning_rate: 1.40625e-07, global_step: 1220, interval_runtime: 6.4237, interval_samples_per_second: 2.491, interval_steps_per_second: 1.557, epoch: 9.5312[0m
[32m[2022-09-17 17:55:34,563] [    INFO][0m - loss: 0.80465717, learning_rate: 1.171875e-07, global_step: 1230, interval_runtime: 6.4179, interval_samples_per_second: 2.493, interval_steps_per_second: 1.558, epoch: 9.6094[0m
[32m[2022-09-17 17:55:40,963] [    INFO][0m - loss: 0.55336285, learning_rate: 9.375e-08, global_step: 1240, interval_runtime: 6.4004, interval_samples_per_second: 2.5, interval_steps_per_second: 1.562, epoch: 9.6875[0m
[32m[2022-09-17 17:55:47,378] [    INFO][0m - loss: 0.78100071, learning_rate: 7.03125e-08, global_step: 1250, interval_runtime: 6.4148, interval_samples_per_second: 2.494, interval_steps_per_second: 1.559, epoch: 9.7656[0m
[32m[2022-09-17 17:55:53,789] [    INFO][0m - loss: 0.59956536, learning_rate: 4.6875e-08, global_step: 1260, interval_runtime: 6.4109, interval_samples_per_second: 2.496, interval_steps_per_second: 1.56, epoch: 9.8438[0m
[32m[2022-09-17 17:56:00,200] [    INFO][0m - loss: 0.75834875, learning_rate: 2.34375e-08, global_step: 1270, interval_runtime: 6.4112, interval_samples_per_second: 2.496, interval_steps_per_second: 1.56, epoch: 9.9219[0m
[32m[2022-09-17 17:56:06,046] [    INFO][0m - loss: 0.702107, learning_rate: 0.0, global_step: 1280, interval_runtime: 5.8455, interval_samples_per_second: 2.737, interval_steps_per_second: 1.711, epoch: 10.0[0m
[32m[2022-09-17 17:56:06,047] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 17:56:06,047] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-17 17:56:06,047] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 17:56:06,047] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 17:56:06,047] [    INFO][0m -   Total prediction steps = 517[0m
[32m[2022-09-17 17:56:42,930] [    INFO][0m - eval_loss: 1.3074686527252197, eval_accuracy: 0.5672147001934236, eval_runtime: 36.8823, eval_samples_per_second: 56.07, eval_steps_per_second: 14.018, epoch: 10.0[0m
[32m[2022-09-17 17:56:42,965] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/checkpoint-1280[0m
[32m[2022-09-17 17:56:42,965] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 17:56:45,616] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/checkpoint-1280/tokenizer_config.json[0m
[32m[2022-09-17 17:56:45,617] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/checkpoint-1280/special_tokens_map.json[0m
[32m[2022-09-17 17:56:50,611] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-17 17:56:50,611] [    INFO][0m - Loading best model from ./checkpoints_csldcp/checkpoint-1280 (score: 0.5672147001934236).[0m
[32m[2022-09-17 17:56:52,227] [    INFO][0m - train_runtime: 1283.515, train_samples_per_second: 15.863, train_steps_per_second: 0.997, train_loss: 1.1756943870335816, epoch: 10.0[0m
[32m[2022-09-17 17:56:52,229] [    INFO][0m - Saving model checkpoint to ./checkpoints_csldcp/[0m
[32m[2022-09-17 17:56:52,229] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 17:56:54,734] [    INFO][0m - tokenizer config file saved in ./checkpoints_csldcp/tokenizer_config.json[0m
[32m[2022-09-17 17:56:54,734] [    INFO][0m - Special tokens file saved in ./checkpoints_csldcp/special_tokens_map.json[0m
[32m[2022-09-17 17:56:54,736] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-17 17:56:54,736] [    INFO][0m -   epoch                    =       10.0[0m
[32m[2022-09-17 17:56:54,736] [    INFO][0m -   train_loss               =     1.1757[0m
[32m[2022-09-17 17:56:54,736] [    INFO][0m -   train_runtime            = 0:21:23.51[0m
[32m[2022-09-17 17:56:54,736] [    INFO][0m -   train_samples_per_second =     15.863[0m
[32m[2022-09-17 17:56:54,736] [    INFO][0m -   train_steps_per_second   =      0.997[0m
[32m[2022-09-17 17:56:54,742] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-17 17:56:54,742] [    INFO][0m -   Num examples = 1784[0m
[32m[2022-09-17 17:56:54,742] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 17:56:54,742] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 17:56:54,742] [    INFO][0m -   Total prediction steps = 446[0m
[32m[2022-09-17 17:57:26,500] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-17 17:57:26,500] [    INFO][0m -   test_accuracy           =     0.5611[0m
[32m[2022-09-17 17:57:26,500] [    INFO][0m -   test_loss               =     1.3469[0m
[32m[2022-09-17 17:57:26,501] [    INFO][0m -   test_runtime            = 0:00:31.75[0m
[32m[2022-09-17 17:57:26,501] [    INFO][0m -   test_samples_per_second =     56.175[0m
[32m[2022-09-17 17:57:26,501] [    INFO][0m -   test_steps_per_second   =     14.044[0m
[32m[2022-09-17 17:57:26,501] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-17 17:57:26,501] [    INFO][0m -   Num examples = 2999[0m
[32m[2022-09-17 17:57:26,501] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 17:57:26,501] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 17:57:26,501] [    INFO][0m -   Total prediction steps = 750[0m
[32m[2022-09-17 17:58:25,258] [    INFO][0m - Predictions for csldcp saved to ./fewclue_submit_examples.[0m
{
  "labels": 26,
  "text_a": "\u672c\u6587\u7814\u7a76\u4e86\u8d35\u5dde\u4e1c\u5357\u90e8\u7684\u5929\u67f1\u3001\u9526\u5c4f\u548c\u9ece\u5e73\u5730\u533a\u7684\u5730\u8d28\u6784\u9020\u7279\u5f81\u3001\u8be5\u533a\u91d1\u77ff\u5e8a\u7279\u5f81\u53ca\u5176\u4e0e\u5730\u8d28\u6784\u9020\u7684\u5173\u7cfb.\u8ba4\u4e3a\u533a\u5185\u5317\u90e8\u548c\u5357\u90e8\u76f8\u8ddd\u6570\u5341\u516c\u91cc\u7684\u4e24\u6761\u8fd1\u4e1c\u897f\u5411\u7684\u57fa\u5e95\u526a\u5207\u65ad\u88c2\u5e26\u6784\u6210\u4e86\u672c\u533a\u5730\u5811\u5f0f\u7684\u6784\u9020\u683c\u5c40.\u7531\u4e8e\u8fd9\u4e24\u6761\u4e1c\u897f\u5411\u526a\u5207\u65ad\u88c2\u7684\u526a\u5207\u4f5c\u7528,\u5f62\u6210\u4e86\u672c\u533a\u4ee5\u5317\u4e1c\u5411\u4e3a\u4e3b\u7684\u8936\u76b1\u548c\u526a\u5207\u5e26.\u5317\u4e1c\u5411\u8936\u76b1\u548c\u526a\u5207\u5e26\u6784\u9020\u662f\u5728\u52a0\u91cc\u4e1c\u671f\u5f62\u6210\u7684,\u540c\u65f6\u4e5f\u53d1\u751f\u4e86\u7eff\u7247\u5ca9\u76f8\u7684\u53d8\u8d28\u4f5c\u7528.\u5728\u80cc\u659c\u5f62\u6210\u7684\u540c\u65f6\u6216\u7a0d\u540e\u526a\u5207\u4f5c\u7528\u5f00\u59cb\u53d1\u751f,\u526a\u5207\u4f5c\u7528\u7ee7\u627f\u7740\u5317\u4e1c\u5411,\u5f62\u6210\u4e86\u82e5\u5e72\u6761\u4e0e\u8936\u76b1\u8f74\u5e73\u884c\u6216\u76f8\u4ea4\u7684\u526a\u5207\u5e26.\u6210\u77ff\u6d41\u4f53\u6cbf\u7740\u526a\u5207\u5e26\u4e0a\u5347,\u4e00\u65b9\u9762\u5728\u526a\u5207\u5e26\u4e2d\u6c89\u6dc0\u51fa\u7a7f\u5c42\u4ea7\u51fa\u7684\u4e0d\u6574\u5408\u7834\u788e\u5e26\u578b\u91d1\u77ff\u4f53,\u5373\u900f\u955c\u72b6\u542b\u91d1\u77f3\u82f1\u8109;\u540c\u65f6\u5927\u90e8\u5206\u5145\u586b\u5230\u7531\u80cc\u659c\u548c\u526a\u5207\u4f5c\u7528\u5f62\u6210\u7684\u5c42\u95f4\u88c2\u9699\u6216\u5c42\u95f4\u7834\u788e\u5e26\u4e2d,\u4ece\u800c\u5f62\u6210\u8d4b\u5b58\u4e8e\u6d4a\u79ef\u5ca9\u7684\u987a\u5c42\u4ea7\u51fa\u7684\u5c42\u72b6\u542b\u91d1\u77f3\u82f1\u8109\u91d1\u77ff\u5e8a.\u8fd9\u79cd\u7c7b\u578b\u7684\u542b\u91d1\u77f3\u82f1\u8109,\u65e0\u8bba\u5728\u54c1\u4f4d(\u591a\u6570\u53ef\u89c1\u660e\u91d1)\u3001\u50a8\u91cf\u548c\u4ea7\u91cf\u65b9\u9762\u5747\u5f88\u6709\u524d\u666f,\u5e76\u4e14\u6709\u7740\u5341\u5206\u91cd\u8981\u7684\u5b66\u672f\u610f\u4e49,\u56e0\u4e3a\u8fd9\u5728\u56fd\u5185\u8fd8\u662f\u9996\u6b21\u53d1\u73b0.",
  "text_b": "",
  "uid": 44
}

 
==========
tnews
==========
 
[32m[2022-09-17 17:58:36,491] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-17 17:58:36,491] [    INFO][0m - ============================================================[0m
[32m[2022-09-17 17:58:36,491] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-17 17:58:36,491] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-17 17:58:36,492] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-17 17:58:36,492] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-17 17:58:36,492] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-17 17:58:36,492] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-17 17:58:36,492] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-17 17:58:36,492] [    INFO][0m - [0m
[32m[2022-09-17 17:58:36,492] [    INFO][0m - ============================================================[0m
[32m[2022-09-17 17:58:36,492] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-17 17:58:36,492] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-17 17:58:36,492] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-17 17:58:36,492] [    INFO][0m - pretrained                    :../checkpoints_cmnli/checkpoint-22000/model_state.pdparams[0m
[32m[2022-09-17 17:58:36,492] [    INFO][0m - prompt                        :â€œ{'text':'text_a'}â€ä¸Šè¿°æ–°é—»é€‰è‡ª{'mask'}{'mask'}ä¸“æ ã€‚[0m
[32m[2022-09-17 17:58:36,492] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-17 17:58:36,493] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-17 17:58:36,493] [    INFO][0m - task_name                     :tnews[0m
[32m[2022-09-17 17:58:36,493] [    INFO][0m - [0m
[32m[2022-09-17 17:58:36,493] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0917 17:58:36.494961 45844 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0917 17:58:36.498996 45844 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-17 17:58:43,800] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-17 17:58:43,811] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-17 17:58:43,811] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-17 17:58:43,812] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': 'â€œ'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': 'â€ä¸Šè¿°æ–°é—»é€‰è‡ª'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'ä¸“æ ã€‚'}][0m
[32m[2022-09-17 17:58:45,714] [    INFO][0m - ============================================================[0m
[32m[2022-09-17 17:58:45,714] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-17 17:58:45,714] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-17 17:58:45,714] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-17 17:58:45,714] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-17 17:58:45,715] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-17 17:58:45,715] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-17 17:58:45,715] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-17 17:58:45,715] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-17 17:58:45,715] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-17 17:58:45,715] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-17 17:58:45,715] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-17 17:58:45,715] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-17 17:58:45,715] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-17 17:58:45,715] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-17 17:58:45,715] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-17 17:58:45,715] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-17 17:58:45,715] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-17 17:58:45,716] [    INFO][0m - eval_batch_size               :4[0m
[32m[2022-09-17 17:58:45,716] [    INFO][0m - eval_steps                    :200[0m
[32m[2022-09-17 17:58:45,716] [    INFO][0m - evaluation_strategy           :IntervalStrategy.EPOCH[0m
[32m[2022-09-17 17:58:45,716] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-17 17:58:45,716] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-17 17:58:45,716] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-17 17:58:45,716] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-17 17:58:45,716] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-17 17:58:45,716] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-17 17:58:45,716] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-17 17:58:45,716] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-17 17:58:45,716] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-17 17:58:45,716] [    INFO][0m - learning_rate                 :3e-06[0m
[32m[2022-09-17 17:58:45,717] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-17 17:58:45,717] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-17 17:58:45,717] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-17 17:58:45,717] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-17 17:58:45,717] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-17 17:58:45,717] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-17 17:58:45,717] [    INFO][0m - logging_dir                   :./checkpoints_tnews/runs/Sep17_17-58-36_instance-3bwob41y-01[0m
[32m[2022-09-17 17:58:45,717] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-17 17:58:45,717] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-17 17:58:45,717] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-17 17:58:45,717] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-17 17:58:45,717] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-17 17:58:45,717] [    INFO][0m - max_seq_length                :64[0m
[32m[2022-09-17 17:58:45,718] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-17 17:58:45,718] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-17 17:58:45,718] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-17 17:58:45,718] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-17 17:58:45,718] [    INFO][0m - num_train_epochs              :10.0[0m
[32m[2022-09-17 17:58:45,718] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-17 17:58:45,718] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-17 17:58:45,718] [    INFO][0m - output_dir                    :./checkpoints_tnews/[0m
[32m[2022-09-17 17:58:45,718] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-17 17:58:45,718] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-17 17:58:45,718] [    INFO][0m - per_device_eval_batch_size    :4[0m
[32m[2022-09-17 17:58:45,718] [    INFO][0m - per_device_train_batch_size   :16[0m
[32m[2022-09-17 17:58:45,718] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-17 17:58:45,719] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-17 17:58:45,719] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-17 17:58:45,719] [    INFO][0m - ppt_learning_rate             :3e-05[0m
[32m[2022-09-17 17:58:45,719] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-17 17:58:45,719] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-17 17:58:45,719] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-17 17:58:45,719] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-17 17:58:45,719] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-17 17:58:45,719] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-17 17:58:45,719] [    INFO][0m - run_name                      :./checkpoints_tnews/[0m
[32m[2022-09-17 17:58:45,719] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-17 17:58:45,719] [    INFO][0m - save_steps                    :200[0m
[32m[2022-09-17 17:58:45,719] [    INFO][0m - save_strategy                 :IntervalStrategy.EPOCH[0m
[32m[2022-09-17 17:58:45,719] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-17 17:58:45,720] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-17 17:58:45,720] [    INFO][0m - seed                          :42[0m
[32m[2022-09-17 17:58:45,720] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-17 17:58:45,720] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-17 17:58:45,720] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-17 17:58:45,720] [    INFO][0m - train_batch_size              :16[0m
[32m[2022-09-17 17:58:45,720] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-17 17:58:45,720] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-17 17:58:45,720] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-17 17:58:45,720] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-17 17:58:45,720] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-17 17:58:45,720] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-17 17:58:45,720] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-17 17:58:45,721] [    INFO][0m - [0m
[32m[2022-09-17 17:58:45,723] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-17 17:58:45,723] [    INFO][0m -   Num examples = 1185[0m
[32m[2022-09-17 17:58:45,723] [    INFO][0m -   Num Epochs = 10[0m
[32m[2022-09-17 17:58:45,723] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2022-09-17 17:58:45,723] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2022-09-17 17:58:45,723] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-17 17:58:45,724] [    INFO][0m -   Total optimization steps = 750.0[0m
[32m[2022-09-17 17:58:45,724] [    INFO][0m -   Total num train samples = 11850[0m
[32m[2022-09-17 17:58:49,056] [    INFO][0m - loss: 5.07697144, learning_rate: 2.96e-06, global_step: 10, interval_runtime: 3.3307, interval_samples_per_second: 4.804, interval_steps_per_second: 3.002, epoch: 0.1333[0m
[32m[2022-09-17 17:58:50,904] [    INFO][0m - loss: 2.94976101, learning_rate: 2.9200000000000004e-06, global_step: 20, interval_runtime: 1.8489, interval_samples_per_second: 8.654, interval_steps_per_second: 5.409, epoch: 0.2667[0m
[32m[2022-09-17 17:58:52,761] [    INFO][0m - loss: 2.37581501, learning_rate: 2.88e-06, global_step: 30, interval_runtime: 1.8566, interval_samples_per_second: 8.618, interval_steps_per_second: 5.386, epoch: 0.4[0m
[32m[2022-09-17 17:58:54,642] [    INFO][0m - loss: 2.14321308, learning_rate: 2.84e-06, global_step: 40, interval_runtime: 1.8814, interval_samples_per_second: 8.504, interval_steps_per_second: 5.315, epoch: 0.5333[0m
[32m[2022-09-17 17:58:56,551] [    INFO][0m - loss: 1.96509609, learning_rate: 2.8000000000000003e-06, global_step: 50, interval_runtime: 1.909, interval_samples_per_second: 8.381, interval_steps_per_second: 5.238, epoch: 0.6667[0m
[32m[2022-09-17 17:58:58,423] [    INFO][0m - loss: 1.87821674, learning_rate: 2.7600000000000003e-06, global_step: 60, interval_runtime: 1.8719, interval_samples_per_second: 8.548, interval_steps_per_second: 5.342, epoch: 0.8[0m
[32m[2022-09-17 17:59:00,272] [    INFO][0m - loss: 1.83092861, learning_rate: 2.72e-06, global_step: 70, interval_runtime: 1.8488, interval_samples_per_second: 8.654, interval_steps_per_second: 5.409, epoch: 0.9333[0m
[32m[2022-09-17 17:59:01,098] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 17:59:01,099] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-17 17:59:01,099] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 17:59:01,099] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 17:59:01,099] [    INFO][0m -   Total prediction steps = 275[0m
[32m[2022-09-17 17:59:10,829] [    INFO][0m - eval_loss: 1.5563831329345703, eval_accuracy: 0.5418943533697632, eval_runtime: 9.7293, eval_samples_per_second: 112.855, eval_steps_per_second: 28.265, epoch: 1.0[0m
[32m[2022-09-17 17:59:10,847] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-75[0m
[32m[2022-09-17 17:59:10,848] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 17:59:14,369] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-75/tokenizer_config.json[0m
[32m[2022-09-17 17:59:14,370] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-75/special_tokens_map.json[0m
[32m[2022-09-17 17:59:20,785] [    INFO][0m - loss: 1.45905933, learning_rate: 2.68e-06, global_step: 80, interval_runtime: 20.5132, interval_samples_per_second: 0.78, interval_steps_per_second: 0.487, epoch: 1.0667[0m
[32m[2022-09-17 17:59:22,647] [    INFO][0m - loss: 1.44963322, learning_rate: 2.64e-06, global_step: 90, interval_runtime: 1.8615, interval_samples_per_second: 8.595, interval_steps_per_second: 5.372, epoch: 1.2[0m
[32m[2022-09-17 17:59:24,494] [    INFO][0m - loss: 1.43511839, learning_rate: 2.6e-06, global_step: 100, interval_runtime: 1.8476, interval_samples_per_second: 8.66, interval_steps_per_second: 5.413, epoch: 1.3333[0m
[32m[2022-09-17 17:59:26,341] [    INFO][0m - loss: 1.65334587, learning_rate: 2.56e-06, global_step: 110, interval_runtime: 1.8474, interval_samples_per_second: 8.661, interval_steps_per_second: 5.413, epoch: 1.4667[0m
[32m[2022-09-17 17:59:28,192] [    INFO][0m - loss: 1.43847961, learning_rate: 2.52e-06, global_step: 120, interval_runtime: 1.8509, interval_samples_per_second: 8.644, interval_steps_per_second: 5.403, epoch: 1.6[0m
[32m[2022-09-17 17:59:30,048] [    INFO][0m - loss: 1.36914597, learning_rate: 2.48e-06, global_step: 130, interval_runtime: 1.8558, interval_samples_per_second: 8.622, interval_steps_per_second: 5.389, epoch: 1.7333[0m
[32m[2022-09-17 17:59:31,929] [    INFO][0m - loss: 1.42603283, learning_rate: 2.44e-06, global_step: 140, interval_runtime: 1.8804, interval_samples_per_second: 8.509, interval_steps_per_second: 5.318, epoch: 1.8667[0m
[32m[2022-09-17 17:59:33,695] [    INFO][0m - loss: 1.18769331, learning_rate: 2.4000000000000003e-06, global_step: 150, interval_runtime: 1.7664, interval_samples_per_second: 9.058, interval_steps_per_second: 5.661, epoch: 2.0[0m
[32m[2022-09-17 17:59:33,696] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 17:59:33,696] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-17 17:59:33,696] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 17:59:33,696] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 17:59:33,697] [    INFO][0m -   Total prediction steps = 275[0m
[32m[2022-09-17 17:59:43,575] [    INFO][0m - eval_loss: 1.3898109197616577, eval_accuracy: 0.5610200364298725, eval_runtime: 9.8777, eval_samples_per_second: 111.16, eval_steps_per_second: 27.841, epoch: 2.0[0m
[32m[2022-09-17 17:59:43,596] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-150[0m
[32m[2022-09-17 17:59:43,597] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 17:59:46,697] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-150/tokenizer_config.json[0m
[32m[2022-09-17 17:59:46,698] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-150/special_tokens_map.json[0m
[32m[2022-09-17 17:59:56,034] [    INFO][0m - loss: 1.30242538, learning_rate: 2.36e-06, global_step: 160, interval_runtime: 22.3389, interval_samples_per_second: 0.716, interval_steps_per_second: 0.448, epoch: 2.1333[0m
[32m[2022-09-17 17:59:57,898] [    INFO][0m - loss: 1.37872477, learning_rate: 2.32e-06, global_step: 170, interval_runtime: 1.8636, interval_samples_per_second: 8.585, interval_steps_per_second: 5.366, epoch: 2.2667[0m
[32m[2022-09-17 17:59:59,747] [    INFO][0m - loss: 1.11457281, learning_rate: 2.28e-06, global_step: 180, interval_runtime: 1.8485, interval_samples_per_second: 8.656, interval_steps_per_second: 5.41, epoch: 2.4[0m
[32m[2022-09-17 18:00:01,605] [    INFO][0m - loss: 1.11151152, learning_rate: 2.24e-06, global_step: 190, interval_runtime: 1.8574, interval_samples_per_second: 8.614, interval_steps_per_second: 5.384, epoch: 2.5333[0m
[32m[2022-09-17 18:00:03,454] [    INFO][0m - loss: 1.22455387, learning_rate: 2.1999999999999997e-06, global_step: 200, interval_runtime: 1.8507, interval_samples_per_second: 8.646, interval_steps_per_second: 5.404, epoch: 2.6667[0m
[32m[2022-09-17 18:00:05,309] [    INFO][0m - loss: 1.41413479, learning_rate: 2.16e-06, global_step: 210, interval_runtime: 1.8545, interval_samples_per_second: 8.627, interval_steps_per_second: 5.392, epoch: 2.8[0m
[32m[2022-09-17 18:00:07,156] [    INFO][0m - loss: 1.16417017, learning_rate: 2.12e-06, global_step: 220, interval_runtime: 1.8469, interval_samples_per_second: 8.663, interval_steps_per_second: 5.415, epoch: 2.9333[0m
[32m[2022-09-17 18:00:07,980] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 18:00:07,980] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-17 18:00:07,980] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:00:07,980] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:00:07,980] [    INFO][0m -   Total prediction steps = 275[0m
[32m[2022-09-17 18:00:17,599] [    INFO][0m - eval_loss: 1.3899613618850708, eval_accuracy: 0.5655737704918032, eval_runtime: 9.6182, eval_samples_per_second: 114.158, eval_steps_per_second: 28.592, epoch: 3.0[0m
[32m[2022-09-17 18:00:17,620] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-225[0m
[32m[2022-09-17 18:00:17,620] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:00:20,648] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-225/tokenizer_config.json[0m
[32m[2022-09-17 18:00:20,648] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-225/special_tokens_map.json[0m
[32m[2022-09-17 18:00:27,374] [    INFO][0m - loss: 1.10873537, learning_rate: 2.08e-06, global_step: 230, interval_runtime: 20.2181, interval_samples_per_second: 0.791, interval_steps_per_second: 0.495, epoch: 3.0667[0m
[32m[2022-09-17 18:00:29,213] [    INFO][0m - loss: 1.06961327, learning_rate: 2.0400000000000004e-06, global_step: 240, interval_runtime: 1.8393, interval_samples_per_second: 8.699, interval_steps_per_second: 5.437, epoch: 3.2[0m
[32m[2022-09-17 18:00:31,057] [    INFO][0m - loss: 1.09998007, learning_rate: 2e-06, global_step: 250, interval_runtime: 1.8435, interval_samples_per_second: 8.679, interval_steps_per_second: 5.424, epoch: 3.3333[0m
[32m[2022-09-17 18:00:32,902] [    INFO][0m - loss: 1.36164093, learning_rate: 1.96e-06, global_step: 260, interval_runtime: 1.8453, interval_samples_per_second: 8.671, interval_steps_per_second: 5.419, epoch: 3.4667[0m
[32m[2022-09-17 18:00:34,742] [    INFO][0m - loss: 1.15500937, learning_rate: 1.9200000000000003e-06, global_step: 270, interval_runtime: 1.8405, interval_samples_per_second: 8.693, interval_steps_per_second: 5.433, epoch: 3.6[0m
[32m[2022-09-17 18:00:36,582] [    INFO][0m - loss: 1.15088234, learning_rate: 1.8800000000000002e-06, global_step: 280, interval_runtime: 1.8399, interval_samples_per_second: 8.696, interval_steps_per_second: 5.435, epoch: 3.7333[0m
[32m[2022-09-17 18:00:38,419] [    INFO][0m - loss: 1.0711277, learning_rate: 1.84e-06, global_step: 290, interval_runtime: 1.8375, interval_samples_per_second: 8.708, interval_steps_per_second: 5.442, epoch: 3.8667[0m
[32m[2022-09-17 18:00:40,176] [    INFO][0m - loss: 1.110886, learning_rate: 1.8e-06, global_step: 300, interval_runtime: 1.7566, interval_samples_per_second: 9.109, interval_steps_per_second: 5.693, epoch: 4.0[0m
[32m[2022-09-17 18:00:40,177] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 18:00:40,177] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-17 18:00:40,178] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:00:40,178] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:00:40,178] [    INFO][0m -   Total prediction steps = 275[0m
[32m[2022-09-17 18:00:49,799] [    INFO][0m - eval_loss: 1.3914258480072021, eval_accuracy: 0.5610200364298725, eval_runtime: 9.6214, eval_samples_per_second: 114.121, eval_steps_per_second: 28.582, epoch: 4.0[0m
[32m[2022-09-17 18:00:49,818] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-300[0m
[32m[2022-09-17 18:00:49,818] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:00:52,731] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-300/tokenizer_config.json[0m
[32m[2022-09-17 18:00:52,731] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-300/special_tokens_map.json[0m
[32m[2022-09-17 18:01:00,191] [    INFO][0m - loss: 1.09379215, learning_rate: 1.76e-06, global_step: 310, interval_runtime: 20.0144, interval_samples_per_second: 0.799, interval_steps_per_second: 0.5, epoch: 4.1333[0m
[32m[2022-09-17 18:01:02,036] [    INFO][0m - loss: 1.04971371, learning_rate: 1.72e-06, global_step: 320, interval_runtime: 1.8449, interval_samples_per_second: 8.673, interval_steps_per_second: 5.42, epoch: 4.2667[0m
[32m[2022-09-17 18:01:03,882] [    INFO][0m - loss: 1.0675106, learning_rate: 1.6800000000000002e-06, global_step: 330, interval_runtime: 1.847, interval_samples_per_second: 8.663, interval_steps_per_second: 5.414, epoch: 4.4[0m
[32m[2022-09-17 18:01:05,728] [    INFO][0m - loss: 1.12775726, learning_rate: 1.64e-06, global_step: 340, interval_runtime: 1.8455, interval_samples_per_second: 8.67, interval_steps_per_second: 5.419, epoch: 4.5333[0m
[32m[2022-09-17 18:01:07,566] [    INFO][0m - loss: 1.21160469, learning_rate: 1.6e-06, global_step: 350, interval_runtime: 1.8379, interval_samples_per_second: 8.706, interval_steps_per_second: 5.441, epoch: 4.6667[0m
[32m[2022-09-17 18:01:09,406] [    INFO][0m - loss: 1.06220655, learning_rate: 1.56e-06, global_step: 360, interval_runtime: 1.84, interval_samples_per_second: 8.696, interval_steps_per_second: 5.435, epoch: 4.8[0m
[32m[2022-09-17 18:01:11,245] [    INFO][0m - loss: 0.98064594, learning_rate: 1.5200000000000003e-06, global_step: 370, interval_runtime: 1.8388, interval_samples_per_second: 8.702, interval_steps_per_second: 5.438, epoch: 4.9333[0m
[32m[2022-09-17 18:01:12,097] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 18:01:12,098] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-17 18:01:12,098] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:01:12,098] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:01:12,098] [    INFO][0m -   Total prediction steps = 275[0m
[32m[2022-09-17 18:01:21,757] [    INFO][0m - eval_loss: 1.370802640914917, eval_accuracy: 0.5765027322404371, eval_runtime: 9.6589, eval_samples_per_second: 113.678, eval_steps_per_second: 28.471, epoch: 5.0[0m
[32m[2022-09-17 18:01:21,775] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-375[0m
[32m[2022-09-17 18:01:21,776] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:01:24,576] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-375/tokenizer_config.json[0m
[32m[2022-09-17 18:01:24,576] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-375/special_tokens_map.json[0m
[32m[2022-09-17 18:01:31,337] [    INFO][0m - loss: 0.85325241, learning_rate: 1.48e-06, global_step: 380, interval_runtime: 20.092, interval_samples_per_second: 0.796, interval_steps_per_second: 0.498, epoch: 5.0667[0m
[32m[2022-09-17 18:01:33,171] [    INFO][0m - loss: 0.94377241, learning_rate: 1.44e-06, global_step: 390, interval_runtime: 1.8343, interval_samples_per_second: 8.723, interval_steps_per_second: 5.452, epoch: 5.2[0m
[32m[2022-09-17 18:01:36,304] [    INFO][0m - loss: 1.23733397, learning_rate: 1.4000000000000001e-06, global_step: 400, interval_runtime: 3.1329, interval_samples_per_second: 5.107, interval_steps_per_second: 3.192, epoch: 5.3333[0m
[32m[2022-09-17 18:01:38,144] [    INFO][0m - loss: 1.06670017, learning_rate: 1.36e-06, global_step: 410, interval_runtime: 1.8398, interval_samples_per_second: 8.696, interval_steps_per_second: 5.435, epoch: 5.4667[0m
[32m[2022-09-17 18:01:39,988] [    INFO][0m - loss: 0.88432045, learning_rate: 1.32e-06, global_step: 420, interval_runtime: 1.8439, interval_samples_per_second: 8.677, interval_steps_per_second: 5.423, epoch: 5.6[0m
[32m[2022-09-17 18:01:41,829] [    INFO][0m - loss: 0.97162399, learning_rate: 1.28e-06, global_step: 430, interval_runtime: 1.8411, interval_samples_per_second: 8.69, interval_steps_per_second: 5.432, epoch: 5.7333[0m
[32m[2022-09-17 18:01:43,673] [    INFO][0m - loss: 0.99489403, learning_rate: 1.24e-06, global_step: 440, interval_runtime: 1.8433, interval_samples_per_second: 8.68, interval_steps_per_second: 5.425, epoch: 5.8667[0m
[32m[2022-09-17 18:01:45,426] [    INFO][0m - loss: 0.96032333, learning_rate: 1.2000000000000002e-06, global_step: 450, interval_runtime: 1.754, interval_samples_per_second: 9.122, interval_steps_per_second: 5.701, epoch: 6.0[0m
[32m[2022-09-17 18:01:45,427] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 18:01:45,427] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-17 18:01:45,427] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:01:45,427] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:01:45,427] [    INFO][0m -   Total prediction steps = 275[0m
[32m[2022-09-17 18:01:55,015] [    INFO][0m - eval_loss: 1.3945555686950684, eval_accuracy: 0.5710382513661202, eval_runtime: 9.5876, eval_samples_per_second: 114.523, eval_steps_per_second: 28.683, epoch: 6.0[0m
[32m[2022-09-17 18:01:55,034] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-450[0m
[32m[2022-09-17 18:01:55,034] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:01:57,740] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-450/tokenizer_config.json[0m
[32m[2022-09-17 18:01:57,740] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-450/special_tokens_map.json[0m
[32m[2022-09-17 18:02:05,021] [    INFO][0m - loss: 1.04571877, learning_rate: 1.16e-06, global_step: 460, interval_runtime: 19.5914, interval_samples_per_second: 0.817, interval_steps_per_second: 0.51, epoch: 6.1333[0m
[32m[2022-09-17 18:02:06,856] [    INFO][0m - loss: 0.99268732, learning_rate: 1.12e-06, global_step: 470, interval_runtime: 1.8384, interval_samples_per_second: 8.703, interval_steps_per_second: 5.44, epoch: 6.2667[0m
[32m[2022-09-17 18:02:08,701] [    INFO][0m - loss: 1.06022177, learning_rate: 1.08e-06, global_step: 480, interval_runtime: 1.8451, interval_samples_per_second: 8.672, interval_steps_per_second: 5.42, epoch: 6.4[0m
[32m[2022-09-17 18:02:10,547] [    INFO][0m - loss: 0.9074398, learning_rate: 1.04e-06, global_step: 490, interval_runtime: 1.8462, interval_samples_per_second: 8.666, interval_steps_per_second: 5.416, epoch: 6.5333[0m
[32m[2022-09-17 18:02:12,382] [    INFO][0m - loss: 0.70727415, learning_rate: 1e-06, global_step: 500, interval_runtime: 1.8354, interval_samples_per_second: 8.717, interval_steps_per_second: 5.448, epoch: 6.6667[0m
[32m[2022-09-17 18:02:14,218] [    INFO][0m - loss: 0.79772029, learning_rate: 9.600000000000001e-07, global_step: 510, interval_runtime: 1.8358, interval_samples_per_second: 8.715, interval_steps_per_second: 5.447, epoch: 6.8[0m
[32m[2022-09-17 18:02:16,059] [    INFO][0m - loss: 1.02881641, learning_rate: 9.2e-07, global_step: 520, interval_runtime: 1.8401, interval_samples_per_second: 8.695, interval_steps_per_second: 5.434, epoch: 6.9333[0m
[32m[2022-09-17 18:02:16,892] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 18:02:16,892] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-17 18:02:16,892] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:02:16,892] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:02:16,893] [    INFO][0m -   Total prediction steps = 275[0m
[32m[2022-09-17 18:02:27,026] [    INFO][0m - eval_loss: 1.427516222000122, eval_accuracy: 0.5628415300546448, eval_runtime: 10.1328, eval_samples_per_second: 108.361, eval_steps_per_second: 27.14, epoch: 7.0[0m
[32m[2022-09-17 18:02:27,045] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-525[0m
[32m[2022-09-17 18:02:27,045] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:02:29,902] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-525/tokenizer_config.json[0m
[32m[2022-09-17 18:02:29,902] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-525/special_tokens_map.json[0m
[32m[2022-09-17 18:02:36,635] [    INFO][0m - loss: 0.79486213, learning_rate: 8.8e-07, global_step: 530, interval_runtime: 20.5762, interval_samples_per_second: 0.778, interval_steps_per_second: 0.486, epoch: 7.0667[0m
[32m[2022-09-17 18:02:38,495] [    INFO][0m - loss: 0.7952404, learning_rate: 8.400000000000001e-07, global_step: 540, interval_runtime: 1.8608, interval_samples_per_second: 8.598, interval_steps_per_second: 5.374, epoch: 7.2[0m
[32m[2022-09-17 18:02:40,347] [    INFO][0m - loss: 0.91382303, learning_rate: 8e-07, global_step: 550, interval_runtime: 1.8515, interval_samples_per_second: 8.641, interval_steps_per_second: 5.401, epoch: 7.3333[0m
[32m[2022-09-17 18:02:42,188] [    INFO][0m - loss: 0.97176371, learning_rate: 7.600000000000001e-07, global_step: 560, interval_runtime: 1.8406, interval_samples_per_second: 8.693, interval_steps_per_second: 5.433, epoch: 7.4667[0m
[32m[2022-09-17 18:02:44,036] [    INFO][0m - loss: 0.92479658, learning_rate: 7.2e-07, global_step: 570, interval_runtime: 1.8482, interval_samples_per_second: 8.657, interval_steps_per_second: 5.411, epoch: 7.6[0m
[32m[2022-09-17 18:02:45,896] [    INFO][0m - loss: 0.98052149, learning_rate: 6.8e-07, global_step: 580, interval_runtime: 1.8597, interval_samples_per_second: 8.604, interval_steps_per_second: 5.377, epoch: 7.7333[0m
[32m[2022-09-17 18:02:47,742] [    INFO][0m - loss: 0.85645657, learning_rate: 6.4e-07, global_step: 590, interval_runtime: 1.8469, interval_samples_per_second: 8.663, interval_steps_per_second: 5.414, epoch: 7.8667[0m
[32m[2022-09-17 18:02:49,492] [    INFO][0m - loss: 0.81947012, learning_rate: 6.000000000000001e-07, global_step: 600, interval_runtime: 1.7493, interval_samples_per_second: 9.146, interval_steps_per_second: 5.716, epoch: 8.0[0m
[32m[2022-09-17 18:02:49,492] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 18:02:49,492] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-17 18:02:49,492] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:02:49,493] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:02:49,493] [    INFO][0m -   Total prediction steps = 275[0m
[32m[2022-09-17 18:02:59,074] [    INFO][0m - eval_loss: 1.4384520053863525, eval_accuracy: 0.5564663023679417, eval_runtime: 9.5811, eval_samples_per_second: 114.6, eval_steps_per_second: 28.702, epoch: 8.0[0m
[32m[2022-09-17 18:02:59,093] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-600[0m
[32m[2022-09-17 18:02:59,093] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:03:01,870] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-600/tokenizer_config.json[0m
[32m[2022-09-17 18:03:01,870] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-600/special_tokens_map.json[0m
[32m[2022-09-17 18:03:09,005] [    INFO][0m - loss: 0.77909646, learning_rate: 5.6e-07, global_step: 610, interval_runtime: 19.5129, interval_samples_per_second: 0.82, interval_steps_per_second: 0.512, epoch: 8.1333[0m
[32m[2022-09-17 18:03:10,843] [    INFO][0m - loss: 0.93869705, learning_rate: 5.2e-07, global_step: 620, interval_runtime: 1.8378, interval_samples_per_second: 8.706, interval_steps_per_second: 5.441, epoch: 8.2667[0m
[32m[2022-09-17 18:03:12,699] [    INFO][0m - loss: 0.88367691, learning_rate: 4.800000000000001e-07, global_step: 630, interval_runtime: 1.8565, interval_samples_per_second: 8.618, interval_steps_per_second: 5.387, epoch: 8.4[0m
[32m[2022-09-17 18:03:14,556] [    INFO][0m - loss: 0.81945591, learning_rate: 4.4e-07, global_step: 640, interval_runtime: 1.857, interval_samples_per_second: 8.616, interval_steps_per_second: 5.385, epoch: 8.5333[0m
[32m[2022-09-17 18:03:16,408] [    INFO][0m - loss: 0.94173927, learning_rate: 4e-07, global_step: 650, interval_runtime: 1.8514, interval_samples_per_second: 8.642, interval_steps_per_second: 5.401, epoch: 8.6667[0m
[32m[2022-09-17 18:03:18,253] [    INFO][0m - loss: 0.84385443, learning_rate: 3.6e-07, global_step: 660, interval_runtime: 1.845, interval_samples_per_second: 8.672, interval_steps_per_second: 5.42, epoch: 8.8[0m
[32m[2022-09-17 18:03:20,093] [    INFO][0m - loss: 0.80051994, learning_rate: 3.2e-07, global_step: 670, interval_runtime: 1.8407, interval_samples_per_second: 8.693, interval_steps_per_second: 5.433, epoch: 8.9333[0m
[32m[2022-09-17 18:03:20,913] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 18:03:20,914] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-17 18:03:20,914] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:03:20,914] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:03:20,914] [    INFO][0m -   Total prediction steps = 275[0m
[32m[2022-09-17 18:03:30,593] [    INFO][0m - eval_loss: 1.4456486701965332, eval_accuracy: 0.5664845173041895, eval_runtime: 9.6791, eval_samples_per_second: 113.44, eval_steps_per_second: 28.412, epoch: 9.0[0m
[32m[2022-09-17 18:03:30,612] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-675[0m
[32m[2022-09-17 18:03:30,613] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:03:33,441] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-675/tokenizer_config.json[0m
[32m[2022-09-17 18:03:33,441] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-675/special_tokens_map.json[0m
[32m[2022-09-17 18:03:41,387] [    INFO][0m - loss: 0.73779507, learning_rate: 2.8e-07, global_step: 680, interval_runtime: 21.2938, interval_samples_per_second: 0.751, interval_steps_per_second: 0.47, epoch: 9.0667[0m
[32m[2022-09-17 18:03:43,233] [    INFO][0m - loss: 0.82444906, learning_rate: 2.4000000000000003e-07, global_step: 690, interval_runtime: 1.846, interval_samples_per_second: 8.668, interval_steps_per_second: 5.417, epoch: 9.2[0m
[32m[2022-09-17 18:03:45,085] [    INFO][0m - loss: 0.78557959, learning_rate: 2e-07, global_step: 700, interval_runtime: 1.8517, interval_samples_per_second: 8.641, interval_steps_per_second: 5.4, epoch: 9.3333[0m
[32m[2022-09-17 18:03:46,922] [    INFO][0m - loss: 0.82325268, learning_rate: 1.6e-07, global_step: 710, interval_runtime: 1.8375, interval_samples_per_second: 8.707, interval_steps_per_second: 5.442, epoch: 9.4667[0m
[32m[2022-09-17 18:03:48,763] [    INFO][0m - loss: 0.95261965, learning_rate: 1.2000000000000002e-07, global_step: 720, interval_runtime: 1.8412, interval_samples_per_second: 8.69, interval_steps_per_second: 5.431, epoch: 9.6[0m
[32m[2022-09-17 18:03:50,609] [    INFO][0m - loss: 0.74584441, learning_rate: 8e-08, global_step: 730, interval_runtime: 1.846, interval_samples_per_second: 8.667, interval_steps_per_second: 5.417, epoch: 9.7333[0m
[32m[2022-09-17 18:03:52,458] [    INFO][0m - loss: 0.77294946, learning_rate: 4e-08, global_step: 740, interval_runtime: 1.8484, interval_samples_per_second: 8.656, interval_steps_per_second: 5.41, epoch: 9.8667[0m
[32m[2022-09-17 18:03:54,203] [    INFO][0m - loss: 0.95486507, learning_rate: 0.0, global_step: 750, interval_runtime: 1.7449, interval_samples_per_second: 9.17, interval_steps_per_second: 5.731, epoch: 10.0[0m
[32m[2022-09-17 18:03:54,203] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 18:03:54,203] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-17 18:03:54,204] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:03:54,204] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:03:54,204] [    INFO][0m -   Total prediction steps = 275[0m
[32m[2022-09-17 18:04:03,859] [    INFO][0m - eval_loss: 1.4560989141464233, eval_accuracy: 0.563752276867031, eval_runtime: 9.6548, eval_samples_per_second: 113.726, eval_steps_per_second: 28.483, epoch: 10.0[0m
[32m[2022-09-17 18:04:03,878] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/checkpoint-750[0m
[32m[2022-09-17 18:04:03,878] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:04:06,570] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/checkpoint-750/tokenizer_config.json[0m
[32m[2022-09-17 18:04:06,570] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/checkpoint-750/special_tokens_map.json[0m
[32m[2022-09-17 18:04:11,934] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-17 18:04:11,935] [    INFO][0m - Loading best model from ./checkpoints_tnews/checkpoint-375 (score: 0.5765027322404371).[0m
[32m[2022-09-17 18:04:13,645] [    INFO][0m - train_runtime: 327.9204, train_samples_per_second: 36.137, train_steps_per_second: 2.287, train_loss: 1.189108507156372, epoch: 10.0[0m
[32m[2022-09-17 18:04:13,646] [    INFO][0m - Saving model checkpoint to ./checkpoints_tnews/[0m
[32m[2022-09-17 18:04:13,647] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:04:16,879] [    INFO][0m - tokenizer config file saved in ./checkpoints_tnews/tokenizer_config.json[0m
[32m[2022-09-17 18:04:16,880] [    INFO][0m - Special tokens file saved in ./checkpoints_tnews/special_tokens_map.json[0m
[32m[2022-09-17 18:04:16,881] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-17 18:04:16,881] [    INFO][0m -   epoch                    =       10.0[0m
[32m[2022-09-17 18:04:16,881] [    INFO][0m -   train_loss               =     1.1891[0m
[32m[2022-09-17 18:04:16,881] [    INFO][0m -   train_runtime            = 0:05:27.92[0m
[32m[2022-09-17 18:04:16,881] [    INFO][0m -   train_samples_per_second =     36.137[0m
[32m[2022-09-17 18:04:16,881] [    INFO][0m -   train_steps_per_second   =      2.287[0m
[32m[2022-09-17 18:04:16,885] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-17 18:04:16,885] [    INFO][0m -   Num examples = 2010[0m
[32m[2022-09-17 18:04:16,885] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:04:16,886] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:04:16,886] [    INFO][0m -   Total prediction steps = 503[0m
[32m[2022-09-17 18:04:34,953] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-17 18:04:34,953] [    INFO][0m -   test_accuracy           =     0.5945[0m
[32m[2022-09-17 18:04:34,954] [    INFO][0m -   test_loss               =      1.328[0m
[32m[2022-09-17 18:04:34,954] [    INFO][0m -   test_runtime            = 0:00:18.06[0m
[32m[2022-09-17 18:04:34,954] [    INFO][0m -   test_samples_per_second =     111.25[0m
[32m[2022-09-17 18:04:34,954] [    INFO][0m -   test_steps_per_second   =      27.84[0m
[32m[2022-09-17 18:04:34,954] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-17 18:04:34,954] [    INFO][0m -   Num examples = 1500[0m
[32m[2022-09-17 18:04:34,954] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:04:34,954] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:04:34,954] [    INFO][0m -   Total prediction steps = 375[0m
[32m[2022-09-17 18:04:49,081] [    INFO][0m - Predictions for tnewsf saved to ./fewclue_submit_examples.[0m
{
  "labels": 11,
  "text_a": "\u5b69\u5b50\u8ddf\u8c01\u7761\uff0c\u5c31\u662f\u8c01\u7684\u5b69\u5b50",
  "text_b": "",
  "uid": 448
}

Prediction done.
 
==========
iflytek
==========
 
[32m[2022-09-17 18:05:02,050] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-17 18:05:02,050] [    INFO][0m - ============================================================[0m
[32m[2022-09-17 18:05:02,050] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-17 18:05:02,050] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-17 18:05:02,050] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-17 18:05:02,050] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-17 18:05:02,051] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-17 18:05:02,051] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-17 18:05:02,051] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-17 18:05:02,051] [    INFO][0m - [0m
[32m[2022-09-17 18:05:02,051] [    INFO][0m - ============================================================[0m
[32m[2022-09-17 18:05:02,051] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-17 18:05:02,051] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-17 18:05:02,051] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-17 18:05:02,051] [    INFO][0m - pretrained                    :../checkpoints_cmnli/checkpoint-22000/model_state.pdparams[0m
[32m[2022-09-17 18:05:02,051] [    INFO][0m - prompt                        :â€œ{'text':'text_a'}â€å› æ­¤ï¼Œåº”ç”¨ç±»åˆ«æ˜¯{'mask'}{'mask'}ã€‚[0m
[32m[2022-09-17 18:05:02,051] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-17 18:05:02,051] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-17 18:05:02,052] [    INFO][0m - task_name                     :iflytek[0m
[32m[2022-09-17 18:05:02,052] [    INFO][0m - [0m
[32m[2022-09-17 18:05:02,052] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0917 18:05:02.053937 55959 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0917 18:05:02.058166 55959 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-17 18:05:09,418] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-17 18:05:09,429] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-17 18:05:09,429] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-17 18:05:09,430] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': 'â€œ'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': 'â€å› æ­¤ï¼Œåº”ç”¨ç±»åˆ«æ˜¯'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'ã€‚'}][0m
[32m[2022-09-17 18:05:12,220] [    INFO][0m - ============================================================[0m
[32m[2022-09-17 18:05:12,220] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-17 18:05:12,220] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-17 18:05:12,221] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-17 18:05:12,221] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-17 18:05:12,221] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-17 18:05:12,221] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-17 18:05:12,221] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-17 18:05:12,221] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-17 18:05:12,221] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-17 18:05:12,221] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-17 18:05:12,221] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-17 18:05:12,221] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-17 18:05:12,222] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-17 18:05:12,222] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-17 18:05:12,222] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-17 18:05:12,222] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-17 18:05:12,222] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-17 18:05:12,222] [    INFO][0m - eval_batch_size               :4[0m
[32m[2022-09-17 18:05:12,222] [    INFO][0m - eval_steps                    :200[0m
[32m[2022-09-17 18:05:12,222] [    INFO][0m - evaluation_strategy           :IntervalStrategy.EPOCH[0m
[32m[2022-09-17 18:05:12,222] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-17 18:05:12,222] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-17 18:05:12,222] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-17 18:05:12,222] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-17 18:05:12,222] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-17 18:05:12,223] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-17 18:05:12,223] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-17 18:05:12,223] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-17 18:05:12,223] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-17 18:05:12,223] [    INFO][0m - learning_rate                 :3e-06[0m
[32m[2022-09-17 18:05:12,223] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-17 18:05:12,223] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-17 18:05:12,223] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-17 18:05:12,223] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-17 18:05:12,223] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-17 18:05:12,223] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-17 18:05:12,223] [    INFO][0m - logging_dir                   :./checkpoints_iflytek/runs/Sep17_18-05-02_instance-3bwob41y-01[0m
[32m[2022-09-17 18:05:12,223] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-17 18:05:12,224] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-17 18:05:12,224] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-17 18:05:12,224] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-17 18:05:12,224] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-17 18:05:12,224] [    INFO][0m - max_seq_length                :320[0m
[32m[2022-09-17 18:05:12,224] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-17 18:05:12,224] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-17 18:05:12,224] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-17 18:05:12,224] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-17 18:05:12,224] [    INFO][0m - num_train_epochs              :10.0[0m
[32m[2022-09-17 18:05:12,224] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-17 18:05:12,224] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-17 18:05:12,224] [    INFO][0m - output_dir                    :./checkpoints_iflytek/[0m
[32m[2022-09-17 18:05:12,225] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-17 18:05:12,225] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-17 18:05:12,225] [    INFO][0m - per_device_eval_batch_size    :4[0m
[32m[2022-09-17 18:05:12,225] [    INFO][0m - per_device_train_batch_size   :16[0m
[32m[2022-09-17 18:05:12,225] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-17 18:05:12,225] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-17 18:05:12,225] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-17 18:05:12,225] [    INFO][0m - ppt_learning_rate             :3e-05[0m
[32m[2022-09-17 18:05:12,225] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-17 18:05:12,225] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-17 18:05:12,225] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-17 18:05:12,225] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-17 18:05:12,225] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-17 18:05:12,225] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-17 18:05:12,226] [    INFO][0m - run_name                      :./checkpoints_iflytek/[0m
[32m[2022-09-17 18:05:12,226] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-17 18:05:12,226] [    INFO][0m - save_steps                    :200[0m
[32m[2022-09-17 18:05:12,226] [    INFO][0m - save_strategy                 :IntervalStrategy.EPOCH[0m
[32m[2022-09-17 18:05:12,226] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-17 18:05:12,226] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-17 18:05:12,226] [    INFO][0m - seed                          :42[0m
[32m[2022-09-17 18:05:12,226] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-17 18:05:12,226] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-17 18:05:12,226] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-17 18:05:12,226] [    INFO][0m - train_batch_size              :16[0m
[32m[2022-09-17 18:05:12,226] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-17 18:05:12,226] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-17 18:05:12,227] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-17 18:05:12,227] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-17 18:05:12,227] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-17 18:05:12,227] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-17 18:05:12,227] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-17 18:05:12,227] [    INFO][0m - [0m
[32m[2022-09-17 18:05:12,230] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-17 18:05:12,230] [    INFO][0m -   Num examples = 3024[0m
[32m[2022-09-17 18:05:12,230] [    INFO][0m -   Num Epochs = 10[0m
[32m[2022-09-17 18:05:12,230] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2022-09-17 18:05:12,230] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2022-09-17 18:05:12,231] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-17 18:05:12,231] [    INFO][0m -   Total optimization steps = 1890.0[0m
[32m[2022-09-17 18:05:12,231] [    INFO][0m -   Total num train samples = 30240[0m
[33m[2022-09-17 18:05:12,241] [ WARNING][0m - Token indices sequence length is longer than the specified maximum sequence length for this model (637 > 512). Running this sequence through the model will result in indexing errors[0m
[32m[2022-09-17 18:05:21,217] [    INFO][0m - loss: 7.55735474, learning_rate: 2.984126984126984e-06, global_step: 10, interval_runtime: 8.9854, interval_samples_per_second: 1.781, interval_steps_per_second: 1.113, epoch: 0.0529[0m
[32m[2022-09-17 18:05:29,077] [    INFO][0m - loss: 4.43060341, learning_rate: 2.9682539682539683e-06, global_step: 20, interval_runtime: 7.8584, interval_samples_per_second: 2.036, interval_steps_per_second: 1.273, epoch: 0.1058[0m
[32m[2022-09-17 18:05:36,943] [    INFO][0m - loss: 3.91773453, learning_rate: 2.9523809523809525e-06, global_step: 30, interval_runtime: 7.8675, interval_samples_per_second: 2.034, interval_steps_per_second: 1.271, epoch: 0.1587[0m
[32m[2022-09-17 18:05:44,770] [    INFO][0m - loss: 3.58744316, learning_rate: 2.9365079365079366e-06, global_step: 40, interval_runtime: 7.8274, interval_samples_per_second: 2.044, interval_steps_per_second: 1.278, epoch: 0.2116[0m
[32m[2022-09-17 18:05:52,622] [    INFO][0m - loss: 3.21825333, learning_rate: 2.9206349206349207e-06, global_step: 50, interval_runtime: 7.8516, interval_samples_per_second: 2.038, interval_steps_per_second: 1.274, epoch: 0.2646[0m
[32m[2022-09-17 18:06:00,474] [    INFO][0m - loss: 3.10301571, learning_rate: 2.904761904761905e-06, global_step: 60, interval_runtime: 7.8521, interval_samples_per_second: 2.038, interval_steps_per_second: 1.274, epoch: 0.3175[0m
[32m[2022-09-17 18:06:08,338] [    INFO][0m - loss: 3.164538, learning_rate: 2.888888888888889e-06, global_step: 70, interval_runtime: 7.8638, interval_samples_per_second: 2.035, interval_steps_per_second: 1.272, epoch: 0.3704[0m
[32m[2022-09-17 18:06:16,206] [    INFO][0m - loss: 3.17150307, learning_rate: 2.873015873015873e-06, global_step: 80, interval_runtime: 7.8675, interval_samples_per_second: 2.034, interval_steps_per_second: 1.271, epoch: 0.4233[0m
[32m[2022-09-17 18:06:24,076] [    INFO][0m - loss: 2.99593811, learning_rate: 2.8571428571428573e-06, global_step: 90, interval_runtime: 7.8705, interval_samples_per_second: 2.033, interval_steps_per_second: 1.271, epoch: 0.4762[0m
[32m[2022-09-17 18:06:31,916] [    INFO][0m - loss: 2.73474312, learning_rate: 2.8412698412698414e-06, global_step: 100, interval_runtime: 7.8404, interval_samples_per_second: 2.041, interval_steps_per_second: 1.275, epoch: 0.5291[0m
[32m[2022-09-17 18:06:39,764] [    INFO][0m - loss: 2.84953613, learning_rate: 2.8253968253968255e-06, global_step: 110, interval_runtime: 7.848, interval_samples_per_second: 2.039, interval_steps_per_second: 1.274, epoch: 0.582[0m
[32m[2022-09-17 18:06:47,636] [    INFO][0m - loss: 2.51748524, learning_rate: 2.8095238095238096e-06, global_step: 120, interval_runtime: 7.8723, interval_samples_per_second: 2.032, interval_steps_per_second: 1.27, epoch: 0.6349[0m
[32m[2022-09-17 18:06:55,495] [    INFO][0m - loss: 2.34624119, learning_rate: 2.7936507936507934e-06, global_step: 130, interval_runtime: 7.8588, interval_samples_per_second: 2.036, interval_steps_per_second: 1.272, epoch: 0.6878[0m
[32m[2022-09-17 18:07:03,347] [    INFO][0m - loss: 2.52301769, learning_rate: 2.777777777777778e-06, global_step: 140, interval_runtime: 7.8515, interval_samples_per_second: 2.038, interval_steps_per_second: 1.274, epoch: 0.7407[0m
[32m[2022-09-17 18:07:11,179] [    INFO][0m - loss: 2.28169956, learning_rate: 2.7619047619047616e-06, global_step: 150, interval_runtime: 7.8328, interval_samples_per_second: 2.043, interval_steps_per_second: 1.277, epoch: 0.7937[0m
[32m[2022-09-17 18:07:19,045] [    INFO][0m - loss: 2.37559414, learning_rate: 2.746031746031746e-06, global_step: 160, interval_runtime: 7.8653, interval_samples_per_second: 2.034, interval_steps_per_second: 1.271, epoch: 0.8466[0m
[32m[2022-09-17 18:07:26,918] [    INFO][0m - loss: 2.34576073, learning_rate: 2.73015873015873e-06, global_step: 170, interval_runtime: 7.8732, interval_samples_per_second: 2.032, interval_steps_per_second: 1.27, epoch: 0.8995[0m
[32m[2022-09-17 18:07:34,757] [    INFO][0m - loss: 2.65064774, learning_rate: 2.7142857142857144e-06, global_step: 180, interval_runtime: 7.8388, interval_samples_per_second: 2.041, interval_steps_per_second: 1.276, epoch: 0.9524[0m
[32m[2022-09-17 18:07:41,668] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 18:07:41,668] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-17 18:07:41,668] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:07:41,669] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:07:41,669] [    INFO][0m -   Total prediction steps = 344[0m
[32m[2022-09-17 18:08:11,580] [    INFO][0m - eval_loss: 2.1051554679870605, eval_accuracy: 0.4457392571012382, eval_runtime: 29.9109, eval_samples_per_second: 45.903, eval_steps_per_second: 11.501, epoch: 1.0[0m
[32m[2022-09-17 18:08:11,603] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-189[0m
[32m[2022-09-17 18:08:11,604] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:08:14,642] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-189/tokenizer_config.json[0m
[32m[2022-09-17 18:08:14,642] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-189/special_tokens_map.json[0m
[32m[2022-09-17 18:08:21,392] [    INFO][0m - loss: 2.15524826, learning_rate: 2.698412698412698e-06, global_step: 190, interval_runtime: 46.6345, interval_samples_per_second: 0.343, interval_steps_per_second: 0.214, epoch: 1.0053[0m
[32m[2022-09-17 18:08:29,201] [    INFO][0m - loss: 1.86809483, learning_rate: 2.6825396825396827e-06, global_step: 200, interval_runtime: 7.81, interval_samples_per_second: 2.049, interval_steps_per_second: 1.28, epoch: 1.0582[0m
[32m[2022-09-17 18:08:37,065] [    INFO][0m - loss: 2.07788544, learning_rate: 2.6666666666666664e-06, global_step: 210, interval_runtime: 7.864, interval_samples_per_second: 2.035, interval_steps_per_second: 1.272, epoch: 1.1111[0m
[32m[2022-09-17 18:08:44,909] [    INFO][0m - loss: 1.86184292, learning_rate: 2.650793650793651e-06, global_step: 220, interval_runtime: 7.8441, interval_samples_per_second: 2.04, interval_steps_per_second: 1.275, epoch: 1.164[0m
[32m[2022-09-17 18:08:52,801] [    INFO][0m - loss: 2.12826405, learning_rate: 2.6349206349206347e-06, global_step: 230, interval_runtime: 7.8915, interval_samples_per_second: 2.027, interval_steps_per_second: 1.267, epoch: 1.2169[0m
[32m[2022-09-17 18:09:00,662] [    INFO][0m - loss: 1.82600212, learning_rate: 2.6190476190476192e-06, global_step: 240, interval_runtime: 7.8609, interval_samples_per_second: 2.035, interval_steps_per_second: 1.272, epoch: 1.2698[0m
[32m[2022-09-17 18:09:08,488] [    INFO][0m - loss: 2.07434731, learning_rate: 2.603174603174603e-06, global_step: 250, interval_runtime: 7.8263, interval_samples_per_second: 2.044, interval_steps_per_second: 1.278, epoch: 1.3228[0m
[32m[2022-09-17 18:09:16,334] [    INFO][0m - loss: 2.33623009, learning_rate: 2.5873015873015875e-06, global_step: 260, interval_runtime: 7.8457, interval_samples_per_second: 2.039, interval_steps_per_second: 1.275, epoch: 1.3757[0m
[32m[2022-09-17 18:09:24,248] [    INFO][0m - loss: 2.10549355, learning_rate: 2.571428571428571e-06, global_step: 270, interval_runtime: 7.9136, interval_samples_per_second: 2.022, interval_steps_per_second: 1.264, epoch: 1.4286[0m
[32m[2022-09-17 18:09:32,095] [    INFO][0m - loss: 2.0952879, learning_rate: 2.5555555555555557e-06, global_step: 280, interval_runtime: 7.8474, interval_samples_per_second: 2.039, interval_steps_per_second: 1.274, epoch: 1.4815[0m
[32m[2022-09-17 18:09:39,933] [    INFO][0m - loss: 2.0180994, learning_rate: 2.5396825396825395e-06, global_step: 290, interval_runtime: 7.8385, interval_samples_per_second: 2.041, interval_steps_per_second: 1.276, epoch: 1.5344[0m
[32m[2022-09-17 18:09:47,805] [    INFO][0m - loss: 1.97854862, learning_rate: 2.523809523809524e-06, global_step: 300, interval_runtime: 7.8721, interval_samples_per_second: 2.033, interval_steps_per_second: 1.27, epoch: 1.5873[0m
[32m[2022-09-17 18:09:55,663] [    INFO][0m - loss: 2.16391869, learning_rate: 2.5079365079365077e-06, global_step: 310, interval_runtime: 7.8571, interval_samples_per_second: 2.036, interval_steps_per_second: 1.273, epoch: 1.6402[0m
[32m[2022-09-17 18:10:03,539] [    INFO][0m - loss: 1.7984869, learning_rate: 2.4920634920634923e-06, global_step: 320, interval_runtime: 7.876, interval_samples_per_second: 2.031, interval_steps_per_second: 1.27, epoch: 1.6931[0m
[32m[2022-09-17 18:10:11,402] [    INFO][0m - loss: 2.06578712, learning_rate: 2.476190476190476e-06, global_step: 330, interval_runtime: 7.8632, interval_samples_per_second: 2.035, interval_steps_per_second: 1.272, epoch: 1.746[0m
[32m[2022-09-17 18:10:19,251] [    INFO][0m - loss: 1.9961792, learning_rate: 2.4603174603174605e-06, global_step: 340, interval_runtime: 7.8494, interval_samples_per_second: 2.038, interval_steps_per_second: 1.274, epoch: 1.7989[0m
[32m[2022-09-17 18:10:27,101] [    INFO][0m - loss: 1.85259972, learning_rate: 2.4444444444444442e-06, global_step: 350, interval_runtime: 7.8497, interval_samples_per_second: 2.038, interval_steps_per_second: 1.274, epoch: 1.8519[0m
[32m[2022-09-17 18:10:34,971] [    INFO][0m - loss: 1.91981335, learning_rate: 2.428571428571429e-06, global_step: 360, interval_runtime: 7.8704, interval_samples_per_second: 2.033, interval_steps_per_second: 1.271, epoch: 1.9048[0m
[32m[2022-09-17 18:10:42,834] [    INFO][0m - loss: 1.91299324, learning_rate: 2.4126984126984125e-06, global_step: 370, interval_runtime: 7.8621, interval_samples_per_second: 2.035, interval_steps_per_second: 1.272, epoch: 1.9577[0m
[32m[2022-09-17 18:10:48,972] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 18:10:48,973] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-17 18:10:48,973] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:10:48,973] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:10:48,973] [    INFO][0m -   Total prediction steps = 344[0m
[32m[2022-09-17 18:11:18,733] [    INFO][0m - eval_loss: 1.8991121053695679, eval_accuracy: 0.4668608885651857, eval_runtime: 29.7595, eval_samples_per_second: 46.137, eval_steps_per_second: 11.559, epoch: 2.0[0m
[32m[2022-09-17 18:11:18,757] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-378[0m
[32m[2022-09-17 18:11:18,757] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:11:21,757] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-378/tokenizer_config.json[0m
[32m[2022-09-17 18:11:21,757] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-378/special_tokens_map.json[0m
[32m[2022-09-17 18:11:29,280] [    INFO][0m - loss: 1.5655757, learning_rate: 2.396825396825397e-06, global_step: 380, interval_runtime: 46.4467, interval_samples_per_second: 0.344, interval_steps_per_second: 0.215, epoch: 2.0106[0m
[32m[2022-09-17 18:11:37,153] [    INFO][0m - loss: 1.73242435, learning_rate: 2.3809523809523808e-06, global_step: 390, interval_runtime: 7.8732, interval_samples_per_second: 2.032, interval_steps_per_second: 1.27, epoch: 2.0635[0m
[32m[2022-09-17 18:11:44,995] [    INFO][0m - loss: 1.56901054, learning_rate: 2.3650793650793653e-06, global_step: 400, interval_runtime: 7.8422, interval_samples_per_second: 2.04, interval_steps_per_second: 1.275, epoch: 2.1164[0m
[32m[2022-09-17 18:11:52,867] [    INFO][0m - loss: 1.82291737, learning_rate: 2.349206349206349e-06, global_step: 410, interval_runtime: 7.8713, interval_samples_per_second: 2.033, interval_steps_per_second: 1.27, epoch: 2.1693[0m
[32m[2022-09-17 18:12:03,669] [    INFO][0m - loss: 1.69051743, learning_rate: 2.3333333333333336e-06, global_step: 420, interval_runtime: 7.8558, interval_samples_per_second: 2.037, interval_steps_per_second: 1.273, epoch: 2.2222[0m
[32m[2022-09-17 18:12:11,512] [    INFO][0m - loss: 1.6501236, learning_rate: 2.3174603174603173e-06, global_step: 430, interval_runtime: 10.7892, interval_samples_per_second: 1.483, interval_steps_per_second: 0.927, epoch: 2.2751[0m
[32m[2022-09-17 18:12:19,372] [    INFO][0m - loss: 1.70866508, learning_rate: 2.301587301587302e-06, global_step: 440, interval_runtime: 7.8607, interval_samples_per_second: 2.035, interval_steps_per_second: 1.272, epoch: 2.328[0m
[32m[2022-09-17 18:12:27,250] [    INFO][0m - loss: 1.52251282, learning_rate: 2.2857142857142856e-06, global_step: 450, interval_runtime: 7.8775, interval_samples_per_second: 2.031, interval_steps_per_second: 1.269, epoch: 2.381[0m
[32m[2022-09-17 18:12:35,164] [    INFO][0m - loss: 1.56948509, learning_rate: 2.26984126984127e-06, global_step: 460, interval_runtime: 7.9143, interval_samples_per_second: 2.022, interval_steps_per_second: 1.264, epoch: 2.4339[0m
[32m[2022-09-17 18:12:43,052] [    INFO][0m - loss: 1.58839607, learning_rate: 2.253968253968254e-06, global_step: 470, interval_runtime: 7.8874, interval_samples_per_second: 2.029, interval_steps_per_second: 1.268, epoch: 2.4868[0m
[32m[2022-09-17 18:12:50,948] [    INFO][0m - loss: 1.72589703, learning_rate: 2.2380952380952384e-06, global_step: 480, interval_runtime: 7.8967, interval_samples_per_second: 2.026, interval_steps_per_second: 1.266, epoch: 2.5397[0m
[32m[2022-09-17 18:12:58,814] [    INFO][0m - loss: 1.75126019, learning_rate: 2.222222222222222e-06, global_step: 490, interval_runtime: 7.865, interval_samples_per_second: 2.034, interval_steps_per_second: 1.271, epoch: 2.5926[0m
[32m[2022-09-17 18:13:06,666] [    INFO][0m - loss: 1.66310863, learning_rate: 2.2063492063492066e-06, global_step: 500, interval_runtime: 7.8525, interval_samples_per_second: 2.038, interval_steps_per_second: 1.273, epoch: 2.6455[0m
[32m[2022-09-17 18:13:14,484] [    INFO][0m - loss: 1.76287022, learning_rate: 2.1904761904761903e-06, global_step: 510, interval_runtime: 7.8178, interval_samples_per_second: 2.047, interval_steps_per_second: 1.279, epoch: 2.6984[0m
[32m[2022-09-17 18:13:22,357] [    INFO][0m - loss: 1.70855904, learning_rate: 2.174603174603175e-06, global_step: 520, interval_runtime: 7.8732, interval_samples_per_second: 2.032, interval_steps_per_second: 1.27, epoch: 2.7513[0m
[32m[2022-09-17 18:13:30,231] [    INFO][0m - loss: 1.73201027, learning_rate: 2.1587301587301586e-06, global_step: 530, interval_runtime: 7.8745, interval_samples_per_second: 2.032, interval_steps_per_second: 1.27, epoch: 2.8042[0m
[32m[2022-09-17 18:13:38,108] [    INFO][0m - loss: 1.6445034, learning_rate: 2.142857142857143e-06, global_step: 540, interval_runtime: 7.8762, interval_samples_per_second: 2.031, interval_steps_per_second: 1.27, epoch: 2.8571[0m
[32m[2022-09-17 18:13:45,992] [    INFO][0m - loss: 1.53818083, learning_rate: 2.126984126984127e-06, global_step: 550, interval_runtime: 7.8843, interval_samples_per_second: 2.029, interval_steps_per_second: 1.268, epoch: 2.9101[0m
[32m[2022-09-17 18:13:53,861] [    INFO][0m - loss: 1.58057728, learning_rate: 2.1111111111111114e-06, global_step: 560, interval_runtime: 7.8695, interval_samples_per_second: 2.033, interval_steps_per_second: 1.271, epoch: 2.963[0m
[32m[2022-09-17 18:13:59,209] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 18:13:59,209] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-17 18:13:59,210] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:13:59,210] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:13:59,210] [    INFO][0m -   Total prediction steps = 344[0m
[32m[2022-09-17 18:14:29,061] [    INFO][0m - eval_loss: 1.8172192573547363, eval_accuracy: 0.46394756008739985, eval_runtime: 29.8508, eval_samples_per_second: 45.995, eval_steps_per_second: 11.524, epoch: 3.0[0m
[32m[2022-09-17 18:14:29,090] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-567[0m
[32m[2022-09-17 18:14:29,090] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:14:32,097] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-567/tokenizer_config.json[0m
[32m[2022-09-17 18:14:32,097] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-567/special_tokens_map.json[0m
[32m[2022-09-17 18:14:44,658] [    INFO][0m - loss: 1.62643871, learning_rate: 2.095238095238095e-06, global_step: 570, interval_runtime: 46.551, interval_samples_per_second: 0.344, interval_steps_per_second: 0.215, epoch: 3.0159[0m
[32m[2022-09-17 18:14:52,497] [    INFO][0m - loss: 1.49303818, learning_rate: 2.0793650793650797e-06, global_step: 580, interval_runtime: 12.0844, interval_samples_per_second: 1.324, interval_steps_per_second: 0.828, epoch: 3.0688[0m
[32m[2022-09-17 18:15:00,356] [    INFO][0m - loss: 1.67845421, learning_rate: 2.0634920634920634e-06, global_step: 590, interval_runtime: 7.8591, interval_samples_per_second: 2.036, interval_steps_per_second: 1.272, epoch: 3.1217[0m
[32m[2022-09-17 18:15:08,231] [    INFO][0m - loss: 1.5513238, learning_rate: 2.0476190476190475e-06, global_step: 600, interval_runtime: 7.8752, interval_samples_per_second: 2.032, interval_steps_per_second: 1.27, epoch: 3.1746[0m
[32m[2022-09-17 18:15:16,073] [    INFO][0m - loss: 1.49746208, learning_rate: 2.0317460317460316e-06, global_step: 610, interval_runtime: 7.8415, interval_samples_per_second: 2.04, interval_steps_per_second: 1.275, epoch: 3.2275[0m
[32m[2022-09-17 18:15:23,925] [    INFO][0m - loss: 1.30505896, learning_rate: 2.0158730158730158e-06, global_step: 620, interval_runtime: 7.8519, interval_samples_per_second: 2.038, interval_steps_per_second: 1.274, epoch: 3.2804[0m
[32m[2022-09-17 18:15:31,785] [    INFO][0m - loss: 1.49486208, learning_rate: 2e-06, global_step: 630, interval_runtime: 7.8598, interval_samples_per_second: 2.036, interval_steps_per_second: 1.272, epoch: 3.3333[0m
[32m[2022-09-17 18:15:39,674] [    INFO][0m - loss: 1.44170914, learning_rate: 1.984126984126984e-06, global_step: 640, interval_runtime: 7.889, interval_samples_per_second: 2.028, interval_steps_per_second: 1.268, epoch: 3.3862[0m
[32m[2022-09-17 18:15:47,561] [    INFO][0m - loss: 1.26061382, learning_rate: 1.968253968253968e-06, global_step: 650, interval_runtime: 7.8872, interval_samples_per_second: 2.029, interval_steps_per_second: 1.268, epoch: 3.4392[0m
[32m[2022-09-17 18:15:55,446] [    INFO][0m - loss: 1.38782959, learning_rate: 1.9523809523809523e-06, global_step: 660, interval_runtime: 7.885, interval_samples_per_second: 2.029, interval_steps_per_second: 1.268, epoch: 3.4921[0m
[32m[2022-09-17 18:16:03,304] [    INFO][0m - loss: 1.45189857, learning_rate: 1.9365079365079364e-06, global_step: 670, interval_runtime: 7.8581, interval_samples_per_second: 2.036, interval_steps_per_second: 1.273, epoch: 3.545[0m
[32m[2022-09-17 18:16:11,174] [    INFO][0m - loss: 1.34484396, learning_rate: 1.9206349206349206e-06, global_step: 680, interval_runtime: 7.8704, interval_samples_per_second: 2.033, interval_steps_per_second: 1.271, epoch: 3.5979[0m
[32m[2022-09-17 18:16:19,031] [    INFO][0m - loss: 1.65113258, learning_rate: 1.9047619047619047e-06, global_step: 690, interval_runtime: 7.8563, interval_samples_per_second: 2.037, interval_steps_per_second: 1.273, epoch: 3.6508[0m
[32m[2022-09-17 18:16:26,887] [    INFO][0m - loss: 1.67077274, learning_rate: 1.888888888888889e-06, global_step: 700, interval_runtime: 7.8568, interval_samples_per_second: 2.036, interval_steps_per_second: 1.273, epoch: 3.7037[0m
[32m[2022-09-17 18:16:34,769] [    INFO][0m - loss: 1.11771889, learning_rate: 1.873015873015873e-06, global_step: 710, interval_runtime: 7.8811, interval_samples_per_second: 2.03, interval_steps_per_second: 1.269, epoch: 3.7566[0m
[32m[2022-09-17 18:16:42,644] [    INFO][0m - loss: 1.32247963, learning_rate: 1.8571428571428573e-06, global_step: 720, interval_runtime: 7.8755, interval_samples_per_second: 2.032, interval_steps_per_second: 1.27, epoch: 3.8095[0m
[32m[2022-09-17 18:16:50,510] [    INFO][0m - loss: 1.53228159, learning_rate: 1.8412698412698412e-06, global_step: 730, interval_runtime: 7.8662, interval_samples_per_second: 2.034, interval_steps_per_second: 1.271, epoch: 3.8624[0m
[32m[2022-09-17 18:16:58,376] [    INFO][0m - loss: 1.51194773, learning_rate: 1.8253968253968256e-06, global_step: 740, interval_runtime: 7.8654, interval_samples_per_second: 2.034, interval_steps_per_second: 1.271, epoch: 3.9153[0m
[32m[2022-09-17 18:17:06,253] [    INFO][0m - loss: 1.59157829, learning_rate: 1.8095238095238095e-06, global_step: 750, interval_runtime: 7.877, interval_samples_per_second: 2.031, interval_steps_per_second: 1.27, epoch: 3.9683[0m
[32m[2022-09-17 18:17:10,808] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 18:17:10,809] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-17 18:17:10,809] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:17:10,809] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:17:10,809] [    INFO][0m -   Total prediction steps = 344[0m
[32m[2022-09-17 18:17:40,654] [    INFO][0m - eval_loss: 1.8096542358398438, eval_accuracy: 0.4690458849235251, eval_runtime: 29.8449, eval_samples_per_second: 46.004, eval_steps_per_second: 11.526, epoch: 4.0[0m
[32m[2022-09-17 18:17:40,678] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-756[0m
[32m[2022-09-17 18:17:40,679] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:17:43,699] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-756/tokenizer_config.json[0m
[32m[2022-09-17 18:17:43,700] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-756/special_tokens_map.json[0m
[32m[2022-09-17 18:17:52,803] [    INFO][0m - loss: 1.48668633, learning_rate: 1.7936507936507938e-06, global_step: 760, interval_runtime: 46.5501, interval_samples_per_second: 0.344, interval_steps_per_second: 0.215, epoch: 4.0212[0m
[32m[2022-09-17 18:18:00,706] [    INFO][0m - loss: 1.26908894, learning_rate: 1.7777777777777777e-06, global_step: 770, interval_runtime: 7.9028, interval_samples_per_second: 2.025, interval_steps_per_second: 1.265, epoch: 4.0741[0m
[32m[2022-09-17 18:18:08,597] [    INFO][0m - loss: 1.21215057, learning_rate: 1.761904761904762e-06, global_step: 780, interval_runtime: 7.8911, interval_samples_per_second: 2.028, interval_steps_per_second: 1.267, epoch: 4.127[0m
[32m[2022-09-17 18:18:16,439] [    INFO][0m - loss: 1.21293545, learning_rate: 1.746031746031746e-06, global_step: 790, interval_runtime: 7.8419, interval_samples_per_second: 2.04, interval_steps_per_second: 1.275, epoch: 4.1799[0m
[32m[2022-09-17 18:18:24,319] [    INFO][0m - loss: 1.13307447, learning_rate: 1.7301587301587303e-06, global_step: 800, interval_runtime: 7.8807, interval_samples_per_second: 2.03, interval_steps_per_second: 1.269, epoch: 4.2328[0m
[32m[2022-09-17 18:18:32,148] [    INFO][0m - loss: 1.32836208, learning_rate: 1.7142857142857143e-06, global_step: 810, interval_runtime: 7.8285, interval_samples_per_second: 2.044, interval_steps_per_second: 1.277, epoch: 4.2857[0m
[32m[2022-09-17 18:18:40,040] [    INFO][0m - loss: 1.16626997, learning_rate: 1.6984126984126986e-06, global_step: 820, interval_runtime: 7.8925, interval_samples_per_second: 2.027, interval_steps_per_second: 1.267, epoch: 4.3386[0m
[32m[2022-09-17 18:18:47,875] [    INFO][0m - loss: 1.58024883, learning_rate: 1.6825396825396825e-06, global_step: 830, interval_runtime: 7.8344, interval_samples_per_second: 2.042, interval_steps_per_second: 1.276, epoch: 4.3915[0m
[32m[2022-09-17 18:18:55,711] [    INFO][0m - loss: 1.53373594, learning_rate: 1.6666666666666669e-06, global_step: 840, interval_runtime: 7.8362, interval_samples_per_second: 2.042, interval_steps_per_second: 1.276, epoch: 4.4444[0m
[32m[2022-09-17 18:19:03,571] [    INFO][0m - loss: 1.14055386, learning_rate: 1.6507936507936508e-06, global_step: 850, interval_runtime: 7.8604, interval_samples_per_second: 2.036, interval_steps_per_second: 1.272, epoch: 4.4974[0m
[32m[2022-09-17 18:19:11,423] [    INFO][0m - loss: 1.47097816, learning_rate: 1.6349206349206351e-06, global_step: 860, interval_runtime: 7.8516, interval_samples_per_second: 2.038, interval_steps_per_second: 1.274, epoch: 4.5503[0m
[32m[2022-09-17 18:19:19,297] [    INFO][0m - loss: 1.25006485, learning_rate: 1.619047619047619e-06, global_step: 870, interval_runtime: 7.8738, interval_samples_per_second: 2.032, interval_steps_per_second: 1.27, epoch: 4.6032[0m
[32m[2022-09-17 18:19:27,130] [    INFO][0m - loss: 1.69541035, learning_rate: 1.6031746031746034e-06, global_step: 880, interval_runtime: 7.8325, interval_samples_per_second: 2.043, interval_steps_per_second: 1.277, epoch: 4.6561[0m
[32m[2022-09-17 18:19:34,992] [    INFO][0m - loss: 1.38988075, learning_rate: 1.5873015873015873e-06, global_step: 890, interval_runtime: 7.8625, interval_samples_per_second: 2.035, interval_steps_per_second: 1.272, epoch: 4.709[0m
[32m[2022-09-17 18:19:42,832] [    INFO][0m - loss: 1.28637772, learning_rate: 1.5714285714285714e-06, global_step: 900, interval_runtime: 7.8405, interval_samples_per_second: 2.041, interval_steps_per_second: 1.275, epoch: 4.7619[0m
[32m[2022-09-17 18:19:50,730] [    INFO][0m - loss: 1.47562523, learning_rate: 1.5555555555555556e-06, global_step: 910, interval_runtime: 7.8981, interval_samples_per_second: 2.026, interval_steps_per_second: 1.266, epoch: 4.8148[0m
[32m[2022-09-17 18:19:58,563] [    INFO][0m - loss: 1.29650097, learning_rate: 1.5396825396825397e-06, global_step: 920, interval_runtime: 7.8327, interval_samples_per_second: 2.043, interval_steps_per_second: 1.277, epoch: 4.8677[0m
[32m[2022-09-17 18:20:06,463] [    INFO][0m - loss: 1.32899456, learning_rate: 1.5238095238095238e-06, global_step: 930, interval_runtime: 7.9001, interval_samples_per_second: 2.025, interval_steps_per_second: 1.266, epoch: 4.9206[0m
[32m[2022-09-17 18:20:14,317] [    INFO][0m - loss: 1.23133869, learning_rate: 1.507936507936508e-06, global_step: 940, interval_runtime: 7.8534, interval_samples_per_second: 2.037, interval_steps_per_second: 1.273, epoch: 4.9735[0m
[32m[2022-09-17 18:20:18,066] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 18:20:18,066] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-17 18:20:18,066] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:20:18,066] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:20:18,066] [    INFO][0m -   Total prediction steps = 344[0m
[32m[2022-09-17 18:20:47,867] [    INFO][0m - eval_loss: 1.8352422714233398, eval_accuracy: 0.4697742170429716, eval_runtime: 29.8006, eval_samples_per_second: 46.073, eval_steps_per_second: 11.543, epoch: 5.0[0m
[32m[2022-09-17 18:20:47,891] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-945[0m
[32m[2022-09-17 18:20:47,891] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:20:50,952] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-945/tokenizer_config.json[0m
[32m[2022-09-17 18:20:50,952] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-945/special_tokens_map.json[0m
[32m[2022-09-17 18:21:00,849] [    INFO][0m - loss: 1.08398008, learning_rate: 1.492063492063492e-06, global_step: 950, interval_runtime: 46.5321, interval_samples_per_second: 0.344, interval_steps_per_second: 0.215, epoch: 5.0265[0m
[32m[2022-09-17 18:21:08,704] [    INFO][0m - loss: 1.11696129, learning_rate: 1.4761904761904762e-06, global_step: 960, interval_runtime: 7.8549, interval_samples_per_second: 2.037, interval_steps_per_second: 1.273, epoch: 5.0794[0m
[32m[2022-09-17 18:21:16,550] [    INFO][0m - loss: 1.02340641, learning_rate: 1.4603174603174604e-06, global_step: 970, interval_runtime: 7.8465, interval_samples_per_second: 2.039, interval_steps_per_second: 1.274, epoch: 5.1323[0m
[32m[2022-09-17 18:21:24,388] [    INFO][0m - loss: 1.44803143, learning_rate: 1.4444444444444445e-06, global_step: 980, interval_runtime: 7.8381, interval_samples_per_second: 2.041, interval_steps_per_second: 1.276, epoch: 5.1852[0m
[32m[2022-09-17 18:21:32,247] [    INFO][0m - loss: 1.00458755, learning_rate: 1.4285714285714286e-06, global_step: 990, interval_runtime: 7.8589, interval_samples_per_second: 2.036, interval_steps_per_second: 1.272, epoch: 5.2381[0m
[32m[2022-09-17 18:21:40,076] [    INFO][0m - loss: 1.14222279, learning_rate: 1.4126984126984128e-06, global_step: 1000, interval_runtime: 7.8285, interval_samples_per_second: 2.044, interval_steps_per_second: 1.277, epoch: 5.291[0m
[32m[2022-09-17 18:21:47,934] [    INFO][0m - loss: 1.27247524, learning_rate: 1.3968253968253967e-06, global_step: 1010, interval_runtime: 7.8587, interval_samples_per_second: 2.036, interval_steps_per_second: 1.272, epoch: 5.3439[0m
[32m[2022-09-17 18:21:55,803] [    INFO][0m - loss: 1.04381742, learning_rate: 1.3809523809523808e-06, global_step: 1020, interval_runtime: 7.8687, interval_samples_per_second: 2.033, interval_steps_per_second: 1.271, epoch: 5.3968[0m
[32m[2022-09-17 18:22:03,667] [    INFO][0m - loss: 1.19277039, learning_rate: 1.365079365079365e-06, global_step: 1030, interval_runtime: 7.864, interval_samples_per_second: 2.035, interval_steps_per_second: 1.272, epoch: 5.4497[0m
[32m[2022-09-17 18:22:11,505] [    INFO][0m - loss: 1.32311153, learning_rate: 1.349206349206349e-06, global_step: 1040, interval_runtime: 7.8375, interval_samples_per_second: 2.041, interval_steps_per_second: 1.276, epoch: 5.5026[0m
[32m[2022-09-17 18:22:19,366] [    INFO][0m - loss: 1.36824875, learning_rate: 1.3333333333333332e-06, global_step: 1050, interval_runtime: 7.8609, interval_samples_per_second: 2.035, interval_steps_per_second: 1.272, epoch: 5.5556[0m
[32m[2022-09-17 18:22:27,235] [    INFO][0m - loss: 1.55492897, learning_rate: 1.3174603174603173e-06, global_step: 1060, interval_runtime: 7.8698, interval_samples_per_second: 2.033, interval_steps_per_second: 1.271, epoch: 5.6085[0m
[32m[2022-09-17 18:22:35,077] [    INFO][0m - loss: 1.15918169, learning_rate: 1.3015873015873015e-06, global_step: 1070, interval_runtime: 7.8412, interval_samples_per_second: 2.041, interval_steps_per_second: 1.275, epoch: 5.6614[0m
[32m[2022-09-17 18:22:42,935] [    INFO][0m - loss: 1.46277666, learning_rate: 1.2857142857142856e-06, global_step: 1080, interval_runtime: 7.8581, interval_samples_per_second: 2.036, interval_steps_per_second: 1.273, epoch: 5.7143[0m
[32m[2022-09-17 18:22:50,798] [    INFO][0m - loss: 1.21339302, learning_rate: 1.2698412698412697e-06, global_step: 1090, interval_runtime: 7.8629, interval_samples_per_second: 2.035, interval_steps_per_second: 1.272, epoch: 5.7672[0m
[32m[2022-09-17 18:22:58,676] [    INFO][0m - loss: 1.12172337, learning_rate: 1.2539682539682539e-06, global_step: 1100, interval_runtime: 7.878, interval_samples_per_second: 2.031, interval_steps_per_second: 1.269, epoch: 5.8201[0m
[32m[2022-09-17 18:23:06,588] [    INFO][0m - loss: 1.16599283, learning_rate: 1.238095238095238e-06, global_step: 1110, interval_runtime: 7.9127, interval_samples_per_second: 2.022, interval_steps_per_second: 1.264, epoch: 5.873[0m
[32m[2022-09-17 18:23:14,468] [    INFO][0m - loss: 1.14869118, learning_rate: 1.2222222222222221e-06, global_step: 1120, interval_runtime: 7.8784, interval_samples_per_second: 2.031, interval_steps_per_second: 1.269, epoch: 5.9259[0m
[32m[2022-09-17 18:23:22,305] [    INFO][0m - loss: 1.13749647, learning_rate: 1.2063492063492063e-06, global_step: 1130, interval_runtime: 7.8386, interval_samples_per_second: 2.041, interval_steps_per_second: 1.276, epoch: 5.9788[0m
[32m[2022-09-17 18:23:25,302] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 18:23:25,302] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-17 18:23:25,302] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:23:25,302] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:23:25,302] [    INFO][0m -   Total prediction steps = 344[0m
[32m[2022-09-17 18:23:55,090] [    INFO][0m - eval_loss: 1.8540607690811157, eval_accuracy: 0.47050254916241807, eval_runtime: 29.7874, eval_samples_per_second: 46.093, eval_steps_per_second: 11.549, epoch: 6.0[0m
[32m[2022-09-17 18:23:55,113] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-1134[0m
[32m[2022-09-17 18:23:55,113] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:23:58,122] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-1134/tokenizer_config.json[0m
[32m[2022-09-17 18:23:58,122] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-1134/special_tokens_map.json[0m
[32m[2022-09-17 18:24:08,604] [    INFO][0m - loss: 1.09699841, learning_rate: 1.1904761904761904e-06, global_step: 1140, interval_runtime: 46.2985, interval_samples_per_second: 0.346, interval_steps_per_second: 0.216, epoch: 6.0317[0m
[32m[2022-09-17 18:24:16,465] [    INFO][0m - loss: 1.12265606, learning_rate: 1.1746031746031745e-06, global_step: 1150, interval_runtime: 7.8609, interval_samples_per_second: 2.035, interval_steps_per_second: 1.272, epoch: 6.0847[0m
[32m[2022-09-17 18:24:24,344] [    INFO][0m - loss: 0.99761686, learning_rate: 1.1587301587301586e-06, global_step: 1160, interval_runtime: 7.8789, interval_samples_per_second: 2.031, interval_steps_per_second: 1.269, epoch: 6.1376[0m
[32m[2022-09-17 18:24:32,211] [    INFO][0m - loss: 1.31188889, learning_rate: 1.1428571428571428e-06, global_step: 1170, interval_runtime: 7.8675, interval_samples_per_second: 2.034, interval_steps_per_second: 1.271, epoch: 6.1905[0m
[32m[2022-09-17 18:24:40,056] [    INFO][0m - loss: 1.01893129, learning_rate: 1.126984126984127e-06, global_step: 1180, interval_runtime: 7.8448, interval_samples_per_second: 2.04, interval_steps_per_second: 1.275, epoch: 6.2434[0m
[32m[2022-09-17 18:24:47,909] [    INFO][0m - loss: 1.0713994, learning_rate: 1.111111111111111e-06, global_step: 1190, interval_runtime: 7.8528, interval_samples_per_second: 2.037, interval_steps_per_second: 1.273, epoch: 6.2963[0m
[32m[2022-09-17 18:24:55,779] [    INFO][0m - loss: 1.02962751, learning_rate: 1.0952380952380952e-06, global_step: 1200, interval_runtime: 7.8703, interval_samples_per_second: 2.033, interval_steps_per_second: 1.271, epoch: 6.3492[0m
[32m[2022-09-17 18:25:03,624] [    INFO][0m - loss: 0.91639957, learning_rate: 1.0793650793650793e-06, global_step: 1210, interval_runtime: 7.8447, interval_samples_per_second: 2.04, interval_steps_per_second: 1.275, epoch: 6.4021[0m
[32m[2022-09-17 18:25:11,495] [    INFO][0m - loss: 1.17124157, learning_rate: 1.0634920634920634e-06, global_step: 1220, interval_runtime: 7.8711, interval_samples_per_second: 2.033, interval_steps_per_second: 1.27, epoch: 6.455[0m
[32m[2022-09-17 18:25:19,326] [    INFO][0m - loss: 1.21197624, learning_rate: 1.0476190476190476e-06, global_step: 1230, interval_runtime: 7.8312, interval_samples_per_second: 2.043, interval_steps_per_second: 1.277, epoch: 6.5079[0m
[32m[2022-09-17 18:25:27,178] [    INFO][0m - loss: 1.16815233, learning_rate: 1.0317460317460317e-06, global_step: 1240, interval_runtime: 7.8518, interval_samples_per_second: 2.038, interval_steps_per_second: 1.274, epoch: 6.5608[0m
[32m[2022-09-17 18:25:35,069] [    INFO][0m - loss: 1.28889503, learning_rate: 1.0158730158730158e-06, global_step: 1250, interval_runtime: 7.8913, interval_samples_per_second: 2.028, interval_steps_per_second: 1.267, epoch: 6.6138[0m
[32m[2022-09-17 18:25:42,916] [    INFO][0m - loss: 1.11099167, learning_rate: 1e-06, global_step: 1260, interval_runtime: 7.847, interval_samples_per_second: 2.039, interval_steps_per_second: 1.274, epoch: 6.6667[0m
[32m[2022-09-17 18:25:50,804] [    INFO][0m - loss: 1.16291332, learning_rate: 9.84126984126984e-07, global_step: 1270, interval_runtime: 7.888, interval_samples_per_second: 2.028, interval_steps_per_second: 1.268, epoch: 6.7196[0m
[32m[2022-09-17 18:25:58,710] [    INFO][0m - loss: 1.11536312, learning_rate: 9.682539682539682e-07, global_step: 1280, interval_runtime: 7.9063, interval_samples_per_second: 2.024, interval_steps_per_second: 1.265, epoch: 6.7725[0m
[32m[2022-09-17 18:26:06,586] [    INFO][0m - loss: 1.15124426, learning_rate: 9.523809523809523e-07, global_step: 1290, interval_runtime: 7.8757, interval_samples_per_second: 2.032, interval_steps_per_second: 1.27, epoch: 6.8254[0m
[32m[2022-09-17 18:26:14,447] [    INFO][0m - loss: 1.1569417, learning_rate: 9.365079365079365e-07, global_step: 1300, interval_runtime: 7.8612, interval_samples_per_second: 2.035, interval_steps_per_second: 1.272, epoch: 6.8783[0m
[32m[2022-09-17 18:26:22,300] [    INFO][0m - loss: 0.88572388, learning_rate: 9.206349206349206e-07, global_step: 1310, interval_runtime: 7.8527, interval_samples_per_second: 2.038, interval_steps_per_second: 1.273, epoch: 6.9312[0m
[32m[2022-09-17 18:26:30,095] [    INFO][0m - loss: 1.13592014, learning_rate: 9.047619047619047e-07, global_step: 1320, interval_runtime: 7.7948, interval_samples_per_second: 2.053, interval_steps_per_second: 1.283, epoch: 6.9841[0m
[32m[2022-09-17 18:26:32,344] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 18:26:32,344] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-17 18:26:32,344] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:26:32,344] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:26:32,344] [    INFO][0m -   Total prediction steps = 344[0m
[32m[2022-09-17 18:27:02,094] [    INFO][0m - eval_loss: 1.851902961730957, eval_accuracy: 0.4675892206846322, eval_runtime: 29.7497, eval_samples_per_second: 46.152, eval_steps_per_second: 11.563, epoch: 7.0[0m
[32m[2022-09-17 18:27:02,118] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-1323[0m
[32m[2022-09-17 18:27:02,118] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:27:05,108] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-1323/tokenizer_config.json[0m
[32m[2022-09-17 18:27:05,109] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-1323/special_tokens_map.json[0m
[32m[2022-09-17 18:27:16,258] [    INFO][0m - loss: 1.05076857, learning_rate: 8.888888888888889e-07, global_step: 1330, interval_runtime: 46.1626, interval_samples_per_second: 0.347, interval_steps_per_second: 0.217, epoch: 7.037[0m
[32m[2022-09-17 18:27:24,119] [    INFO][0m - loss: 0.98568573, learning_rate: 8.73015873015873e-07, global_step: 1340, interval_runtime: 7.8616, interval_samples_per_second: 2.035, interval_steps_per_second: 1.272, epoch: 7.0899[0m
[32m[2022-09-17 18:27:31,996] [    INFO][0m - loss: 0.91060591, learning_rate: 8.571428571428571e-07, global_step: 1350, interval_runtime: 7.8771, interval_samples_per_second: 2.031, interval_steps_per_second: 1.27, epoch: 7.1429[0m
[32m[2022-09-17 18:27:39,837] [    INFO][0m - loss: 1.07930689, learning_rate: 8.412698412698413e-07, global_step: 1360, interval_runtime: 7.8404, interval_samples_per_second: 2.041, interval_steps_per_second: 1.275, epoch: 7.1958[0m
[32m[2022-09-17 18:27:47,682] [    INFO][0m - loss: 1.18669186, learning_rate: 8.253968253968254e-07, global_step: 1370, interval_runtime: 7.8452, interval_samples_per_second: 2.039, interval_steps_per_second: 1.275, epoch: 7.2487[0m
[32m[2022-09-17 18:27:55,555] [    INFO][0m - loss: 1.14213362, learning_rate: 8.095238095238095e-07, global_step: 1380, interval_runtime: 7.8726, interval_samples_per_second: 2.032, interval_steps_per_second: 1.27, epoch: 7.3016[0m
[32m[2022-09-17 18:28:03,418] [    INFO][0m - loss: 1.12438116, learning_rate: 7.936507936507937e-07, global_step: 1390, interval_runtime: 7.8636, interval_samples_per_second: 2.035, interval_steps_per_second: 1.272, epoch: 7.3545[0m
[32m[2022-09-17 18:28:11,256] [    INFO][0m - loss: 1.01696634, learning_rate: 7.777777777777778e-07, global_step: 1400, interval_runtime: 7.838, interval_samples_per_second: 2.041, interval_steps_per_second: 1.276, epoch: 7.4074[0m
[32m[2022-09-17 18:28:19,126] [    INFO][0m - loss: 1.05972385, learning_rate: 7.619047619047619e-07, global_step: 1410, interval_runtime: 7.8694, interval_samples_per_second: 2.033, interval_steps_per_second: 1.271, epoch: 7.4603[0m
[32m[2022-09-17 18:28:27,015] [    INFO][0m - loss: 0.96679611, learning_rate: 7.46031746031746e-07, global_step: 1420, interval_runtime: 7.8892, interval_samples_per_second: 2.028, interval_steps_per_second: 1.268, epoch: 7.5132[0m
[32m[2022-09-17 18:28:34,875] [    INFO][0m - loss: 1.05079575, learning_rate: 7.301587301587302e-07, global_step: 1430, interval_runtime: 7.8603, interval_samples_per_second: 2.036, interval_steps_per_second: 1.272, epoch: 7.5661[0m
[32m[2022-09-17 18:28:42,707] [    INFO][0m - loss: 1.03512421, learning_rate: 7.142857142857143e-07, global_step: 1440, interval_runtime: 7.8314, interval_samples_per_second: 2.043, interval_steps_per_second: 1.277, epoch: 7.619[0m
[32m[2022-09-17 18:28:50,581] [    INFO][0m - loss: 1.08178444, learning_rate: 6.984126984126983e-07, global_step: 1450, interval_runtime: 7.8747, interval_samples_per_second: 2.032, interval_steps_per_second: 1.27, epoch: 7.672[0m
[32m[2022-09-17 18:28:58,446] [    INFO][0m - loss: 1.08689337, learning_rate: 6.825396825396825e-07, global_step: 1460, interval_runtime: 7.8652, interval_samples_per_second: 2.034, interval_steps_per_second: 1.271, epoch: 7.7249[0m
[32m[2022-09-17 18:29:06,331] [    INFO][0m - loss: 1.06901541, learning_rate: 6.666666666666666e-07, global_step: 1470, interval_runtime: 7.8846, interval_samples_per_second: 2.029, interval_steps_per_second: 1.268, epoch: 7.7778[0m
[32m[2022-09-17 18:29:14,222] [    INFO][0m - loss: 0.95207958, learning_rate: 6.507936507936507e-07, global_step: 1480, interval_runtime: 7.8909, interval_samples_per_second: 2.028, interval_steps_per_second: 1.267, epoch: 7.8307[0m
[32m[2022-09-17 18:29:22,134] [    INFO][0m - loss: 0.95806561, learning_rate: 6.349206349206349e-07, global_step: 1490, interval_runtime: 7.9119, interval_samples_per_second: 2.022, interval_steps_per_second: 1.264, epoch: 7.8836[0m
[32m[2022-09-17 18:29:29,982] [    INFO][0m - loss: 1.23218431, learning_rate: 6.19047619047619e-07, global_step: 1500, interval_runtime: 7.8478, interval_samples_per_second: 2.039, interval_steps_per_second: 1.274, epoch: 7.9365[0m
[32m[2022-09-17 18:29:37,744] [    INFO][0m - loss: 1.14259415, learning_rate: 6.031746031746031e-07, global_step: 1510, interval_runtime: 7.7615, interval_samples_per_second: 2.061, interval_steps_per_second: 1.288, epoch: 7.9894[0m
[32m[2022-09-17 18:29:39,236] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 18:29:39,236] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-17 18:29:39,236] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:29:39,236] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:29:39,237] [    INFO][0m -   Total prediction steps = 344[0m
[32m[2022-09-17 18:30:09,070] [    INFO][0m - eval_loss: 1.9081315994262695, eval_accuracy: 0.4675892206846322, eval_runtime: 29.8332, eval_samples_per_second: 46.022, eval_steps_per_second: 11.531, epoch: 8.0[0m
[32m[2022-09-17 18:30:09,094] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-1512[0m
[32m[2022-09-17 18:30:09,094] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:30:12,019] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-1512/tokenizer_config.json[0m
[32m[2022-09-17 18:30:12,019] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-1512/special_tokens_map.json[0m
[32m[2022-09-17 18:30:23,940] [    INFO][0m - loss: 0.92394991, learning_rate: 5.873015873015873e-07, global_step: 1520, interval_runtime: 46.1965, interval_samples_per_second: 0.346, interval_steps_per_second: 0.216, epoch: 8.0423[0m
[32m[2022-09-17 18:30:31,794] [    INFO][0m - loss: 1.04231758, learning_rate: 5.714285714285714e-07, global_step: 1530, interval_runtime: 7.8547, interval_samples_per_second: 2.037, interval_steps_per_second: 1.273, epoch: 8.0952[0m
[32m[2022-09-17 18:30:39,654] [    INFO][0m - loss: 1.04874878, learning_rate: 5.555555555555555e-07, global_step: 1540, interval_runtime: 7.8602, interval_samples_per_second: 2.036, interval_steps_per_second: 1.272, epoch: 8.1481[0m
[32m[2022-09-17 18:30:47,520] [    INFO][0m - loss: 0.94254475, learning_rate: 5.396825396825396e-07, global_step: 1550, interval_runtime: 7.8656, interval_samples_per_second: 2.034, interval_steps_per_second: 1.271, epoch: 8.2011[0m
[32m[2022-09-17 18:30:55,360] [    INFO][0m - loss: 0.91553173, learning_rate: 5.238095238095238e-07, global_step: 1560, interval_runtime: 7.8399, interval_samples_per_second: 2.041, interval_steps_per_second: 1.276, epoch: 8.254[0m
[32m[2022-09-17 18:31:03,217] [    INFO][0m - loss: 0.95509052, learning_rate: 5.079365079365079e-07, global_step: 1570, interval_runtime: 7.8568, interval_samples_per_second: 2.036, interval_steps_per_second: 1.273, epoch: 8.3069[0m
[32m[2022-09-17 18:31:11,104] [    INFO][0m - loss: 1.06355324, learning_rate: 4.92063492063492e-07, global_step: 1580, interval_runtime: 7.8871, interval_samples_per_second: 2.029, interval_steps_per_second: 1.268, epoch: 8.3598[0m
[32m[2022-09-17 18:31:18,985] [    INFO][0m - loss: 0.8921608, learning_rate: 4.761904761904762e-07, global_step: 1590, interval_runtime: 7.8811, interval_samples_per_second: 2.03, interval_steps_per_second: 1.269, epoch: 8.4127[0m
[32m[2022-09-17 18:31:26,823] [    INFO][0m - loss: 1.08837156, learning_rate: 4.603174603174603e-07, global_step: 1600, interval_runtime: 7.8383, interval_samples_per_second: 2.041, interval_steps_per_second: 1.276, epoch: 8.4656[0m
[32m[2022-09-17 18:31:34,704] [    INFO][0m - loss: 0.93523846, learning_rate: 4.4444444444444444e-07, global_step: 1610, interval_runtime: 7.8802, interval_samples_per_second: 2.03, interval_steps_per_second: 1.269, epoch: 8.5185[0m
[32m[2022-09-17 18:31:42,558] [    INFO][0m - loss: 1.02146711, learning_rate: 4.2857142857142857e-07, global_step: 1620, interval_runtime: 7.8542, interval_samples_per_second: 2.037, interval_steps_per_second: 1.273, epoch: 8.5714[0m
[32m[2022-09-17 18:31:50,447] [    INFO][0m - loss: 1.07812462, learning_rate: 4.126984126984127e-07, global_step: 1630, interval_runtime: 7.8893, interval_samples_per_second: 2.028, interval_steps_per_second: 1.268, epoch: 8.6243[0m
[32m[2022-09-17 18:31:58,317] [    INFO][0m - loss: 1.09730988, learning_rate: 3.9682539682539683e-07, global_step: 1640, interval_runtime: 7.8704, interval_samples_per_second: 2.033, interval_steps_per_second: 1.271, epoch: 8.6772[0m
[32m[2022-09-17 18:32:06,206] [    INFO][0m - loss: 0.98141661, learning_rate: 3.8095238095238096e-07, global_step: 1650, interval_runtime: 7.8889, interval_samples_per_second: 2.028, interval_steps_per_second: 1.268, epoch: 8.7302[0m
[32m[2022-09-17 18:32:14,071] [    INFO][0m - loss: 1.10938797, learning_rate: 3.650793650793651e-07, global_step: 1660, interval_runtime: 7.8646, interval_samples_per_second: 2.034, interval_steps_per_second: 1.272, epoch: 8.7831[0m
[32m[2022-09-17 18:32:21,972] [    INFO][0m - loss: 0.78148165, learning_rate: 3.4920634920634917e-07, global_step: 1670, interval_runtime: 7.9005, interval_samples_per_second: 2.025, interval_steps_per_second: 1.266, epoch: 8.836[0m
[32m[2022-09-17 18:32:29,849] [    INFO][0m - loss: 0.93532238, learning_rate: 3.333333333333333e-07, global_step: 1680, interval_runtime: 7.878, interval_samples_per_second: 2.031, interval_steps_per_second: 1.269, epoch: 8.8889[0m
[32m[2022-09-17 18:32:37,746] [    INFO][0m - loss: 0.91551151, learning_rate: 3.1746031746031743e-07, global_step: 1690, interval_runtime: 7.8964, interval_samples_per_second: 2.026, interval_steps_per_second: 1.266, epoch: 8.9418[0m
[32m[2022-09-17 18:32:45,489] [    INFO][0m - loss: 0.8521369, learning_rate: 3.0158730158730156e-07, global_step: 1700, interval_runtime: 7.7436, interval_samples_per_second: 2.066, interval_steps_per_second: 1.291, epoch: 8.9947[0m
[32m[2022-09-17 18:32:46,235] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 18:32:46,235] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-17 18:32:46,235] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:32:46,235] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:32:46,235] [    INFO][0m -   Total prediction steps = 344[0m
[32m[2022-09-17 18:33:16,049] [    INFO][0m - eval_loss: 1.9253562688827515, eval_accuracy: 0.471959213401311, eval_runtime: 29.8133, eval_samples_per_second: 46.053, eval_steps_per_second: 11.538, epoch: 9.0[0m
[32m[2022-09-17 18:33:16,073] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-1701[0m
[32m[2022-09-17 18:33:16,073] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:33:18,982] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-1701/tokenizer_config.json[0m
[32m[2022-09-17 18:33:18,982] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-1701/special_tokens_map.json[0m
[32m[2022-09-17 18:33:31,706] [    INFO][0m - loss: 0.94725924, learning_rate: 2.857142857142857e-07, global_step: 1710, interval_runtime: 46.2161, interval_samples_per_second: 0.346, interval_steps_per_second: 0.216, epoch: 9.0476[0m
[32m[2022-09-17 18:33:39,574] [    INFO][0m - loss: 0.84429474, learning_rate: 2.698412698412698e-07, global_step: 1720, interval_runtime: 7.8681, interval_samples_per_second: 2.034, interval_steps_per_second: 1.271, epoch: 9.1005[0m
[32m[2022-09-17 18:33:47,416] [    INFO][0m - loss: 1.28220463, learning_rate: 2.5396825396825396e-07, global_step: 1730, interval_runtime: 7.8422, interval_samples_per_second: 2.04, interval_steps_per_second: 1.275, epoch: 9.1534[0m
[32m[2022-09-17 18:33:55,287] [    INFO][0m - loss: 1.0015749, learning_rate: 2.380952380952381e-07, global_step: 1740, interval_runtime: 7.8709, interval_samples_per_second: 2.033, interval_steps_per_second: 1.271, epoch: 9.2063[0m
[32m[2022-09-17 18:34:03,152] [    INFO][0m - loss: 0.93858032, learning_rate: 2.2222222222222222e-07, global_step: 1750, interval_runtime: 7.8653, interval_samples_per_second: 2.034, interval_steps_per_second: 1.271, epoch: 9.2593[0m
[32m[2022-09-17 18:34:11,026] [    INFO][0m - loss: 0.98080864, learning_rate: 2.0634920634920635e-07, global_step: 1760, interval_runtime: 7.874, interval_samples_per_second: 2.032, interval_steps_per_second: 1.27, epoch: 9.3122[0m
[32m[2022-09-17 18:34:18,902] [    INFO][0m - loss: 0.98821955, learning_rate: 1.9047619047619048e-07, global_step: 1770, interval_runtime: 7.8755, interval_samples_per_second: 2.032, interval_steps_per_second: 1.27, epoch: 9.3651[0m
[32m[2022-09-17 18:34:26,774] [    INFO][0m - loss: 1.07932825, learning_rate: 1.7460317460317458e-07, global_step: 1780, interval_runtime: 7.8727, interval_samples_per_second: 2.032, interval_steps_per_second: 1.27, epoch: 9.418[0m
[32m[2022-09-17 18:34:34,667] [    INFO][0m - loss: 0.92650919, learning_rate: 1.5873015873015872e-07, global_step: 1790, interval_runtime: 7.8925, interval_samples_per_second: 2.027, interval_steps_per_second: 1.267, epoch: 9.4709[0m
[32m[2022-09-17 18:34:42,540] [    INFO][0m - loss: 1.04142351, learning_rate: 1.4285714285714285e-07, global_step: 1800, interval_runtime: 7.8735, interval_samples_per_second: 2.032, interval_steps_per_second: 1.27, epoch: 9.5238[0m
[32m[2022-09-17 18:34:50,399] [    INFO][0m - loss: 0.81577625, learning_rate: 1.2698412698412698e-07, global_step: 1810, interval_runtime: 7.8591, interval_samples_per_second: 2.036, interval_steps_per_second: 1.272, epoch: 9.5767[0m
[32m[2022-09-17 18:34:58,252] [    INFO][0m - loss: 1.00912256, learning_rate: 1.1111111111111111e-07, global_step: 1820, interval_runtime: 7.8523, interval_samples_per_second: 2.038, interval_steps_per_second: 1.274, epoch: 9.6296[0m
[32m[2022-09-17 18:35:06,116] [    INFO][0m - loss: 1.01735935, learning_rate: 9.523809523809524e-08, global_step: 1830, interval_runtime: 7.8639, interval_samples_per_second: 2.035, interval_steps_per_second: 1.272, epoch: 9.6825[0m
[32m[2022-09-17 18:35:13,968] [    INFO][0m - loss: 0.83788805, learning_rate: 7.936507936507936e-08, global_step: 1840, interval_runtime: 7.852, interval_samples_per_second: 2.038, interval_steps_per_second: 1.274, epoch: 9.7354[0m
[32m[2022-09-17 18:35:21,837] [    INFO][0m - loss: 1.08036757, learning_rate: 6.349206349206349e-08, global_step: 1850, interval_runtime: 7.8696, interval_samples_per_second: 2.033, interval_steps_per_second: 1.271, epoch: 9.7884[0m
[32m[2022-09-17 18:35:29,711] [    INFO][0m - loss: 0.69682598, learning_rate: 4.761904761904762e-08, global_step: 1860, interval_runtime: 7.8735, interval_samples_per_second: 2.032, interval_steps_per_second: 1.27, epoch: 9.8413[0m
[32m[2022-09-17 18:35:37,591] [    INFO][0m - loss: 0.69785709, learning_rate: 3.1746031746031744e-08, global_step: 1870, interval_runtime: 7.8802, interval_samples_per_second: 2.03, interval_steps_per_second: 1.269, epoch: 9.8942[0m
[32m[2022-09-17 18:35:45,496] [    INFO][0m - loss: 1.02872753, learning_rate: 1.5873015873015872e-08, global_step: 1880, interval_runtime: 7.9054, interval_samples_per_second: 2.024, interval_steps_per_second: 1.265, epoch: 9.9471[0m
[32m[2022-09-17 18:35:53,189] [    INFO][0m - loss: 0.94131088, learning_rate: 0.0, global_step: 1890, interval_runtime: 7.6923, interval_samples_per_second: 2.08, interval_steps_per_second: 1.3, epoch: 10.0[0m
[32m[2022-09-17 18:35:53,190] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 18:35:53,190] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-17 18:35:53,190] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:35:53,190] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:35:53,190] [    INFO][0m -   Total prediction steps = 344[0m
[32m[2022-09-17 18:36:23,080] [    INFO][0m - eval_loss: 1.9272960424423218, eval_accuracy: 0.4726875455207575, eval_runtime: 29.8891, eval_samples_per_second: 45.936, eval_steps_per_second: 11.509, epoch: 10.0[0m
[32m[2022-09-17 18:36:23,103] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-1890[0m
[32m[2022-09-17 18:36:23,103] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:36:26,021] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-1890/tokenizer_config.json[0m
[32m[2022-09-17 18:36:26,021] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-1890/special_tokens_map.json[0m
[32m[2022-09-17 18:36:31,424] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-17 18:36:31,424] [    INFO][0m - Loading best model from ./checkpoints_iflytek/checkpoint-1890 (score: 0.4726875455207575).[0m
[32m[2022-09-17 18:36:33,046] [    INFO][0m - train_runtime: 1880.8143, train_samples_per_second: 16.078, train_steps_per_second: 1.005, train_loss: 1.491682163117424, epoch: 10.0[0m
[32m[2022-09-17 18:36:33,047] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/[0m
[32m[2022-09-17 18:36:33,047] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:36:35,272] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/tokenizer_config.json[0m
[32m[2022-09-17 18:36:38,702] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/special_tokens_map.json[0m
[32m[2022-09-17 18:36:38,705] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-17 18:36:38,705] [    INFO][0m -   epoch                    =       10.0[0m
[32m[2022-09-17 18:36:38,705] [    INFO][0m -   train_loss               =     1.4917[0m
[32m[2022-09-17 18:36:38,705] [    INFO][0m -   train_runtime            = 0:31:20.81[0m
[32m[2022-09-17 18:36:38,705] [    INFO][0m -   train_samples_per_second =     16.078[0m
[32m[2022-09-17 18:36:38,705] [    INFO][0m -   train_steps_per_second   =      1.005[0m
[32m[2022-09-17 18:36:38,714] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-17 18:36:38,714] [    INFO][0m -   Num examples = 1749[0m
[32m[2022-09-17 18:36:38,714] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:36:38,714] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:36:38,714] [    INFO][0m -   Total prediction steps = 438[0m
[32m[2022-09-17 18:37:16,090] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-17 18:37:16,091] [    INFO][0m -   test_accuracy           =     0.4814[0m
[32m[2022-09-17 18:37:16,091] [    INFO][0m -   test_loss               =      1.903[0m
[32m[2022-09-17 18:37:16,091] [    INFO][0m -   test_runtime            = 0:00:37.37[0m
[32m[2022-09-17 18:37:16,091] [    INFO][0m -   test_samples_per_second =     46.794[0m
[32m[2022-09-17 18:37:16,091] [    INFO][0m -   test_steps_per_second   =     11.719[0m
[32m[2022-09-17 18:37:16,092] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-17 18:37:16,092] [    INFO][0m -   Num examples = 2600[0m
[32m[2022-09-17 18:37:16,092] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:37:16,092] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:37:16,092] [    INFO][0m -   Total prediction steps = 650[0m
[32m[2022-09-17 18:38:18,137] [    INFO][0m - Predictions for iflytekf saved to ./fewclue_submit_examples.[0m
{
  "labels": 77,
  "text_a": "\u884c\u8baf\u901a\u662f\u5e7f\u5dde\u4ea4\u901a\u4fe1\u606f\u5316\u5efa\u8bbe\u6295\u8d44\u8425\u8fd0\u6709\u9650\u516c\u53f8\u5728\u5e7f\u5dde\u5e02\u4ea4\u901a\u8fd0\u8f93\u5c40\u7684\u6307\u5bfc\u4e0b\uff0c\u63a8\u51fa\u7684\u4e00\u6b3e\u63d0\u4f9b\u4ea4\u901a\u4fe1\u606f\u670d\u52a1\u7684\u624b\u673a\u7ec8\u7aef\u8f6f\u4ef6\uff0c\u652f\u6301Android\u548ciPhone\u7cfb\u7edf\u3002\u4e3b\u8981\u5305\u62ec\u8def\u51b5\u4fe1\u606f\u3001\u5b9e\u65f6\u516c\u4ea4\u3001\u505c\u8f66\u670d\u52a1\u3001\u7684\u58eb\u67e5\u8be2\u3001\u51fa\u884c\u89c4\u5212\u3001\u5730\u94c1\u4fe1\u606f\u3001\u822a\u7a7a\u4fe1\u606f\u3001\u94c1\u8def\u4fe1\u606f\u3001\u5ba2\u8fd0\u4fe1\u606f\u3001\u9a7e\u57f9\u4fe1\u606f\u3001\u4ea4\u901a\u8d44\u8baf\u3001WIFI\u70ed\u70b9\u7b49\u529f\u80fd\u6a21\u5757\u3002\u66f4\u65b0\u5185\u5bb91\u3001\u4f18\u5316\u5df2\u77e5bug\u3002",
  "text_b": "",
  "uid": 0
}

Prediction done.
 
==========
ocnli
==========
 
[32m[2022-09-17 18:38:29,203] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-17 18:38:29,203] [    INFO][0m - ============================================================[0m
[32m[2022-09-17 18:38:29,203] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-17 18:38:29,203] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-17 18:38:29,204] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-17 18:38:29,204] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-17 18:38:29,204] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-17 18:38:29,204] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-17 18:38:29,204] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-17 18:38:29,204] [    INFO][0m - [0m
[32m[2022-09-17 18:38:29,204] [    INFO][0m - ============================================================[0m
[32m[2022-09-17 18:38:29,204] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-17 18:38:29,204] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-17 18:38:29,204] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-17 18:38:29,204] [    INFO][0m - pretrained                    :../checkpoints_cmnli/checkpoint-22000/model_state.pdparams[0m
[32m[2022-09-17 18:38:29,204] [    INFO][0m - prompt                        :â€œ{'text':'text_a'}â€å’Œâ€œ{'text':'text_b'}â€ä¹‹é—´çš„é€»è¾‘å…³ç³»æ˜¯{'mask'}{'mask'}ã€‚[0m
[32m[2022-09-17 18:38:29,205] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-17 18:38:29,205] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-17 18:38:29,205] [    INFO][0m - task_name                     :ocnli[0m
[32m[2022-09-17 18:38:29,205] [    INFO][0m - [0m
[32m[2022-09-17 18:38:29,205] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0917 18:38:29.207028 12623 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0917 18:38:29.211167 12623 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-17 18:38:36,632] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-17 18:38:36,643] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-17 18:38:36,644] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-17 18:38:36,644] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': 'â€œ'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': 'â€å’Œâ€œ'}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': 'â€ä¹‹é—´çš„é€»è¾‘å…³ç³»æ˜¯'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'ã€‚'}][0m
[32m[2022-09-17 18:38:39,623] [    INFO][0m - ============================================================[0m
[32m[2022-09-17 18:38:39,623] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-17 18:38:39,623] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-17 18:38:39,623] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-17 18:38:39,623] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-17 18:38:39,624] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-17 18:38:39,624] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-17 18:38:39,624] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-17 18:38:39,624] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-17 18:38:39,624] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-17 18:38:39,624] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-17 18:38:39,624] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-17 18:38:39,624] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-17 18:38:39,624] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-17 18:38:39,624] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-17 18:38:39,624] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-17 18:38:39,624] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-17 18:38:39,625] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-17 18:38:39,625] [    INFO][0m - eval_batch_size               :4[0m
[32m[2022-09-17 18:38:39,625] [    INFO][0m - eval_steps                    :200[0m
[32m[2022-09-17 18:38:39,625] [    INFO][0m - evaluation_strategy           :IntervalStrategy.EPOCH[0m
[32m[2022-09-17 18:38:39,625] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-17 18:38:39,625] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-17 18:38:39,625] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-17 18:38:39,625] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-17 18:38:39,625] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-17 18:38:39,625] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-17 18:38:39,625] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-17 18:38:39,625] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-17 18:38:39,625] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-17 18:38:39,626] [    INFO][0m - learning_rate                 :3e-06[0m
[32m[2022-09-17 18:38:39,626] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-17 18:38:39,626] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-17 18:38:39,626] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-17 18:38:39,626] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-17 18:38:39,626] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-17 18:38:39,626] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-17 18:38:39,626] [    INFO][0m - logging_dir                   :./checkpoints_ocnli/runs/Sep17_18-38-29_instance-3bwob41y-01[0m
[32m[2022-09-17 18:38:39,626] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-17 18:38:39,626] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-17 18:38:39,626] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-17 18:38:39,626] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-17 18:38:39,626] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-17 18:38:39,627] [    INFO][0m - max_seq_length                :128[0m
[32m[2022-09-17 18:38:39,627] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-17 18:38:39,627] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-17 18:38:39,627] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-17 18:38:39,627] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-17 18:38:39,627] [    INFO][0m - num_train_epochs              :10.0[0m
[32m[2022-09-17 18:38:39,627] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-17 18:38:39,627] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-17 18:38:39,627] [    INFO][0m - output_dir                    :./checkpoints_ocnli/[0m
[32m[2022-09-17 18:38:39,627] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-17 18:38:39,627] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-17 18:38:39,627] [    INFO][0m - per_device_eval_batch_size    :4[0m
[32m[2022-09-17 18:38:39,627] [    INFO][0m - per_device_train_batch_size   :16[0m
[32m[2022-09-17 18:38:39,628] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-17 18:38:39,628] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-17 18:38:39,628] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-17 18:38:39,628] [    INFO][0m - ppt_learning_rate             :3e-05[0m
[32m[2022-09-17 18:38:39,628] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-17 18:38:39,628] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-17 18:38:39,628] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-17 18:38:39,628] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-17 18:38:39,628] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-17 18:38:39,628] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-17 18:38:39,628] [    INFO][0m - run_name                      :./checkpoints_ocnli/[0m
[32m[2022-09-17 18:38:39,628] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-17 18:38:39,628] [    INFO][0m - save_steps                    :200[0m
[32m[2022-09-17 18:38:39,628] [    INFO][0m - save_strategy                 :IntervalStrategy.EPOCH[0m
[32m[2022-09-17 18:38:39,629] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-17 18:38:39,629] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-17 18:38:39,629] [    INFO][0m - seed                          :42[0m
[32m[2022-09-17 18:38:39,629] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-17 18:38:39,629] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-17 18:38:39,629] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-17 18:38:39,629] [    INFO][0m - train_batch_size              :16[0m
[32m[2022-09-17 18:38:39,629] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-17 18:38:39,629] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-17 18:38:39,629] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-17 18:38:39,629] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-17 18:38:39,629] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-17 18:38:39,629] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-17 18:38:39,630] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-17 18:38:39,630] [    INFO][0m - [0m
[32m[2022-09-17 18:38:39,633] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-17 18:38:39,633] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-17 18:38:39,633] [    INFO][0m -   Num Epochs = 10[0m
[32m[2022-09-17 18:38:39,633] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2022-09-17 18:38:39,633] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2022-09-17 18:38:39,633] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-17 18:38:39,633] [    INFO][0m -   Total optimization steps = 100.0[0m
[32m[2022-09-17 18:38:39,633] [    INFO][0m -   Total num train samples = 1600[0m
[32m[2022-09-17 18:38:43,976] [    INFO][0m - loss: 0.63364906, learning_rate: 2.7e-06, global_step: 10, interval_runtime: 4.3414, interval_samples_per_second: 3.685, interval_steps_per_second: 2.303, epoch: 1.0[0m
[32m[2022-09-17 18:38:43,977] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 18:38:43,977] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-17 18:38:43,977] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:38:43,977] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:38:43,978] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-17 18:38:45,572] [    INFO][0m - eval_loss: 0.6577108502388, eval_accuracy: 0.75, eval_runtime: 1.5944, eval_samples_per_second: 100.353, eval_steps_per_second: 25.088, epoch: 1.0[0m
[32m[2022-09-17 18:38:45,573] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-10[0m
[32m[2022-09-17 18:38:45,573] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:38:48,626] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-10/tokenizer_config.json[0m
[32m[2022-09-17 18:38:48,627] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-10/special_tokens_map.json[0m
[32m[2022-09-17 18:38:58,191] [    INFO][0m - loss: 0.49262848, learning_rate: 2.4000000000000003e-06, global_step: 20, interval_runtime: 14.2152, interval_samples_per_second: 1.126, interval_steps_per_second: 0.703, epoch: 2.0[0m
[32m[2022-09-17 18:38:58,192] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 18:38:58,192] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-17 18:38:58,192] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:38:58,192] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:38:58,192] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-17 18:38:59,732] [    INFO][0m - eval_loss: 0.638366162776947, eval_accuracy: 0.74375, eval_runtime: 1.5394, eval_samples_per_second: 103.939, eval_steps_per_second: 25.985, epoch: 2.0[0m
[32m[2022-09-17 18:38:59,733] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-20[0m
[32m[2022-09-17 18:38:59,733] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:39:02,526] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-20/tokenizer_config.json[0m
[32m[2022-09-17 18:39:02,526] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-20/special_tokens_map.json[0m
[32m[2022-09-17 18:39:11,628] [    INFO][0m - loss: 0.42299929, learning_rate: 2.1e-06, global_step: 30, interval_runtime: 13.4371, interval_samples_per_second: 1.191, interval_steps_per_second: 0.744, epoch: 3.0[0m
[32m[2022-09-17 18:39:11,629] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 18:39:11,629] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-17 18:39:11,629] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:39:11,629] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:39:11,630] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-17 18:39:13,154] [    INFO][0m - eval_loss: 0.6246472597122192, eval_accuracy: 0.775, eval_runtime: 1.5243, eval_samples_per_second: 104.963, eval_steps_per_second: 26.241, epoch: 3.0[0m
[32m[2022-09-17 18:39:13,155] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-30[0m
[32m[2022-09-17 18:39:13,155] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:39:15,906] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-30/tokenizer_config.json[0m
[32m[2022-09-17 18:39:15,906] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-30/special_tokens_map.json[0m
[32m[2022-09-17 18:39:24,700] [    INFO][0m - loss: 0.35392084, learning_rate: 1.8e-06, global_step: 40, interval_runtime: 13.0712, interval_samples_per_second: 1.224, interval_steps_per_second: 0.765, epoch: 4.0[0m
[32m[2022-09-17 18:39:24,701] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 18:39:24,701] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-17 18:39:24,701] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:39:24,701] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:39:24,701] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-17 18:39:26,209] [    INFO][0m - eval_loss: 0.6428707838058472, eval_accuracy: 0.76875, eval_runtime: 1.5079, eval_samples_per_second: 106.109, eval_steps_per_second: 26.527, epoch: 4.0[0m
[32m[2022-09-17 18:39:26,210] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-40[0m
[32m[2022-09-17 18:39:26,210] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:39:28,997] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-40/tokenizer_config.json[0m
[32m[2022-09-17 18:39:28,998] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-40/special_tokens_map.json[0m
[32m[2022-09-17 18:39:37,962] [    INFO][0m - loss: 0.31050026, learning_rate: 1.5e-06, global_step: 50, interval_runtime: 13.2622, interval_samples_per_second: 1.206, interval_steps_per_second: 0.754, epoch: 5.0[0m
[32m[2022-09-17 18:39:37,962] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 18:39:37,963] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-17 18:39:37,963] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:39:37,963] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:39:37,963] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-17 18:39:39,475] [    INFO][0m - eval_loss: 0.6723085641860962, eval_accuracy: 0.76875, eval_runtime: 1.5123, eval_samples_per_second: 105.796, eval_steps_per_second: 26.449, epoch: 5.0[0m
[32m[2022-09-17 18:39:39,476] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-50[0m
[32m[2022-09-17 18:39:39,476] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:39:42,167] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-50/tokenizer_config.json[0m
[32m[2022-09-17 18:39:42,249] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-50/special_tokens_map.json[0m
[32m[2022-09-17 18:39:51,019] [    INFO][0m - loss: 0.25103259, learning_rate: 1.2000000000000002e-06, global_step: 60, interval_runtime: 13.0574, interval_samples_per_second: 1.225, interval_steps_per_second: 0.766, epoch: 6.0[0m
[32m[2022-09-17 18:39:51,019] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 18:39:51,020] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-17 18:39:51,020] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:39:51,020] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:39:51,020] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-17 18:39:52,538] [    INFO][0m - eval_loss: 0.6831268668174744, eval_accuracy: 0.75625, eval_runtime: 1.518, eval_samples_per_second: 105.4, eval_steps_per_second: 26.35, epoch: 6.0[0m
[32m[2022-09-17 18:39:53,252] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-60[0m
[32m[2022-09-17 18:39:53,252] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:39:56,259] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-60/tokenizer_config.json[0m
[32m[2022-09-17 18:39:56,259] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-60/special_tokens_map.json[0m
[32m[2022-09-17 18:40:05,111] [    INFO][0m - loss: 0.23767755, learning_rate: 9e-07, global_step: 70, interval_runtime: 14.0922, interval_samples_per_second: 1.135, interval_steps_per_second: 0.71, epoch: 7.0[0m
[32m[2022-09-17 18:40:05,112] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 18:40:05,112] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-17 18:40:05,112] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:40:05,112] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:40:05,112] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-17 18:40:06,619] [    INFO][0m - eval_loss: 0.6972652673721313, eval_accuracy: 0.75625, eval_runtime: 1.5064, eval_samples_per_second: 106.211, eval_steps_per_second: 26.553, epoch: 7.0[0m
[32m[2022-09-17 18:40:06,828] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-70[0m
[32m[2022-09-17 18:40:06,829] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:40:09,637] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-70/tokenizer_config.json[0m
[32m[2022-09-17 18:40:09,638] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-70/special_tokens_map.json[0m
[32m[2022-09-17 18:40:18,323] [    INFO][0m - loss: 0.19928453, learning_rate: 6.000000000000001e-07, global_step: 80, interval_runtime: 13.212, interval_samples_per_second: 1.211, interval_steps_per_second: 0.757, epoch: 8.0[0m
[32m[2022-09-17 18:40:18,323] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 18:40:18,324] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-17 18:40:18,324] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:40:18,324] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:40:18,324] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-17 18:40:19,825] [    INFO][0m - eval_loss: 0.7102975249290466, eval_accuracy: 0.75625, eval_runtime: 1.5006, eval_samples_per_second: 106.623, eval_steps_per_second: 26.656, epoch: 8.0[0m
[32m[2022-09-17 18:40:19,825] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-80[0m
[32m[2022-09-17 18:40:19,825] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:40:25,102] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-80/tokenizer_config.json[0m
[32m[2022-09-17 18:40:25,102] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-80/special_tokens_map.json[0m
[32m[2022-09-17 18:40:36,704] [    INFO][0m - loss: 0.19016985, learning_rate: 3.0000000000000004e-07, global_step: 90, interval_runtime: 18.3808, interval_samples_per_second: 0.87, interval_steps_per_second: 0.544, epoch: 9.0[0m
[32m[2022-09-17 18:40:36,705] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 18:40:36,705] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-17 18:40:36,705] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:40:36,705] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:40:36,705] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-17 18:40:38,247] [    INFO][0m - eval_loss: 0.720185399055481, eval_accuracy: 0.7625, eval_runtime: 1.5415, eval_samples_per_second: 103.796, eval_steps_per_second: 25.949, epoch: 9.0[0m
[32m[2022-09-17 18:40:38,247] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-90[0m
[32m[2022-09-17 18:40:38,247] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:40:40,833] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-90/tokenizer_config.json[0m
[32m[2022-09-17 18:40:44,653] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-90/special_tokens_map.json[0m
[32m[2022-09-17 18:40:53,241] [    INFO][0m - loss: 0.16216074, learning_rate: 0.0, global_step: 100, interval_runtime: 16.5374, interval_samples_per_second: 0.968, interval_steps_per_second: 0.605, epoch: 10.0[0m
[32m[2022-09-17 18:40:53,242] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 18:40:53,242] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-17 18:40:53,242] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:40:53,242] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:40:53,242] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-17 18:40:55,047] [    INFO][0m - eval_loss: 0.7241129875183105, eval_accuracy: 0.7625, eval_runtime: 1.8049, eval_samples_per_second: 88.646, eval_steps_per_second: 22.162, epoch: 10.0[0m
[32m[2022-09-17 18:40:55,048] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-100[0m
[32m[2022-09-17 18:40:55,048] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:40:58,884] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-17 18:40:58,884] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-17 18:41:04,119] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-17 18:41:04,119] [    INFO][0m - Loading best model from ./checkpoints_ocnli/checkpoint-30 (score: 0.775).[0m
[32m[2022-09-17 18:41:05,955] [    INFO][0m - train_runtime: 146.3205, train_samples_per_second: 10.935, train_steps_per_second: 0.683, train_loss: 0.32540231943130493, epoch: 10.0[0m
[32m[2022-09-17 18:41:05,956] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/[0m
[32m[2022-09-17 18:41:05,957] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:41:08,529] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/tokenizer_config.json[0m
[32m[2022-09-17 18:41:08,529] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/special_tokens_map.json[0m
[32m[2022-09-17 18:41:08,530] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-17 18:41:08,531] [    INFO][0m -   epoch                    =       10.0[0m
[32m[2022-09-17 18:41:08,531] [    INFO][0m -   train_loss               =     0.3254[0m
[32m[2022-09-17 18:41:08,531] [    INFO][0m -   train_runtime            = 0:02:26.32[0m
[32m[2022-09-17 18:41:08,531] [    INFO][0m -   train_samples_per_second =     10.935[0m
[32m[2022-09-17 18:41:08,531] [    INFO][0m -   train_steps_per_second   =      0.683[0m
[32m[2022-09-17 18:41:08,533] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-17 18:41:08,533] [    INFO][0m -   Num examples = 2520[0m
[32m[2022-09-17 18:41:08,533] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:41:08,533] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:41:08,533] [    INFO][0m -   Total prediction steps = 630[0m
[32m[2022-09-17 18:41:34,896] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-17 18:41:34,897] [    INFO][0m -   test_accuracy           =     0.7766[0m
[32m[2022-09-17 18:41:34,897] [    INFO][0m -   test_loss               =     0.5269[0m
[32m[2022-09-17 18:41:34,897] [    INFO][0m -   test_runtime            = 0:00:26.36[0m
[32m[2022-09-17 18:41:34,897] [    INFO][0m -   test_samples_per_second =     95.587[0m
[32m[2022-09-17 18:41:34,897] [    INFO][0m -   test_steps_per_second   =     23.897[0m
[32m[2022-09-17 18:41:34,897] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-17 18:41:34,898] [    INFO][0m -   Num examples = 3000[0m
[32m[2022-09-17 18:41:34,898] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:41:34,898] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:41:34,898] [    INFO][0m -   Total prediction steps = 750[0m
[32m[2022-09-17 18:42:06,801] [    INFO][0m - Predictions for ocnlif saved to ./fewclue_submit_examples.[0m
{
  "labels": 2,
  "text_a": "\u4e03\u4e94\u671f\u95f4\u5f00\u59cb,\u56fd\u5bb6\u53c8\u6295\u8d44\u5c06\u6b66\u6c49\u5e02\u533a\u7684\u90e8\u5206\u571f\u5824\u6539\u5efa\u4e3a\u94a2\u7b4b\u6ce5\u51dd\u571f\u9632\u6c34\u5899",
  "text_b": "\u516b\u4e94\u671f\u95f4\u4f1a\u628a\u5269\u4e0b\u7684\u571f\u5824\u90fd\u6539\u5efa\u5b8c",
  "uid": 0
}

Prediction done.
 
==========
bustm
==========
 
[32m[2022-09-17 18:42:17,777] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-17 18:42:17,777] [    INFO][0m - ============================================================[0m
[32m[2022-09-17 18:42:17,777] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-17 18:42:17,777] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-17 18:42:17,777] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-17 18:42:17,777] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-17 18:42:17,778] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-17 18:42:17,778] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-17 18:42:17,778] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-17 18:42:17,778] [    INFO][0m - [0m
[32m[2022-09-17 18:42:17,778] [    INFO][0m - ============================================================[0m
[32m[2022-09-17 18:42:17,778] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-17 18:42:17,778] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-17 18:42:17,778] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-17 18:42:17,778] [    INFO][0m - pretrained                    :../checkpoints_cmnli/checkpoint-22000/model_state.pdparams[0m
[32m[2022-09-17 18:42:17,778] [    INFO][0m - prompt                        :â€œ{'text':'text_a'}â€å’Œâ€œ{'text':'text_b'}â€æè¿°çš„æ˜¯{'mask'}{'mask'}çš„äº‹æƒ…ã€‚[0m
[32m[2022-09-17 18:42:17,778] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-17 18:42:17,778] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-17 18:42:17,778] [    INFO][0m - task_name                     :bustm[0m
[32m[2022-09-17 18:42:17,779] [    INFO][0m - [0m
[32m[2022-09-17 18:42:17,779] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0917 18:42:17.780794 16220 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0917 18:42:17.784818 16220 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-17 18:42:25,115] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-17 18:42:25,126] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-17 18:42:25,126] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-17 18:42:25,127] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': 'â€œ'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': 'â€å’Œâ€œ'}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': 'â€æè¿°çš„æ˜¯'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'çš„äº‹æƒ…ã€‚'}][0m
[32m[2022-09-17 18:42:26,846] [    INFO][0m - ============================================================[0m
[32m[2022-09-17 18:42:26,846] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-17 18:42:26,846] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-17 18:42:26,846] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-17 18:42:26,847] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-17 18:42:26,847] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-17 18:42:26,847] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-17 18:42:26,847] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-17 18:42:26,847] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-17 18:42:26,847] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-17 18:42:26,847] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-17 18:42:26,847] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-17 18:42:26,847] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-17 18:42:26,847] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-17 18:42:26,847] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-17 18:42:26,847] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-17 18:42:26,847] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-17 18:42:26,848] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-17 18:42:26,848] [    INFO][0m - eval_batch_size               :4[0m
[32m[2022-09-17 18:42:26,848] [    INFO][0m - eval_steps                    :200[0m
[32m[2022-09-17 18:42:26,848] [    INFO][0m - evaluation_strategy           :IntervalStrategy.EPOCH[0m
[32m[2022-09-17 18:42:26,848] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-17 18:42:26,848] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-17 18:42:26,848] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-17 18:42:26,848] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-17 18:42:26,848] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-17 18:42:26,848] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-17 18:42:26,848] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-17 18:42:26,848] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-17 18:42:26,848] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-17 18:42:26,848] [    INFO][0m - learning_rate                 :3e-06[0m
[32m[2022-09-17 18:42:26,849] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-17 18:42:26,849] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-17 18:42:26,849] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-17 18:42:26,849] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-17 18:42:26,849] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-17 18:42:26,849] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-17 18:42:26,849] [    INFO][0m - logging_dir                   :./checkpoints_bustm/runs/Sep17_18-42-17_instance-3bwob41y-01[0m
[32m[2022-09-17 18:42:26,849] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-17 18:42:26,849] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-17 18:42:26,849] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-17 18:42:26,849] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-17 18:42:26,849] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-17 18:42:26,849] [    INFO][0m - max_seq_length                :64[0m
[32m[2022-09-17 18:42:26,850] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-17 18:42:26,850] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-17 18:42:26,850] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-17 18:42:26,850] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-17 18:42:26,850] [    INFO][0m - num_train_epochs              :10.0[0m
[32m[2022-09-17 18:42:26,850] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-17 18:42:26,850] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-17 18:42:26,850] [    INFO][0m - output_dir                    :./checkpoints_bustm/[0m
[32m[2022-09-17 18:42:26,850] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-17 18:42:26,850] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-17 18:42:26,850] [    INFO][0m - per_device_eval_batch_size    :4[0m
[32m[2022-09-17 18:42:26,850] [    INFO][0m - per_device_train_batch_size   :16[0m
[32m[2022-09-17 18:42:26,850] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-17 18:42:26,850] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-17 18:42:26,851] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-17 18:42:26,851] [    INFO][0m - ppt_learning_rate             :3e-05[0m
[32m[2022-09-17 18:42:26,851] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-17 18:42:26,851] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-17 18:42:26,851] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-17 18:42:26,851] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-17 18:42:26,851] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-17 18:42:26,851] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-17 18:42:26,851] [    INFO][0m - run_name                      :./checkpoints_bustm/[0m
[32m[2022-09-17 18:42:26,851] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-17 18:42:26,851] [    INFO][0m - save_steps                    :200[0m
[32m[2022-09-17 18:42:26,851] [    INFO][0m - save_strategy                 :IntervalStrategy.EPOCH[0m
[32m[2022-09-17 18:42:26,851] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-17 18:42:26,852] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-17 18:42:26,852] [    INFO][0m - seed                          :42[0m
[32m[2022-09-17 18:42:26,852] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-17 18:42:26,852] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-17 18:42:26,852] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-17 18:42:26,852] [    INFO][0m - train_batch_size              :16[0m
[32m[2022-09-17 18:42:26,852] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-17 18:42:26,852] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-17 18:42:26,852] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-17 18:42:26,852] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-17 18:42:26,852] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-17 18:42:26,852] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-17 18:42:26,852] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-17 18:42:26,852] [    INFO][0m - [0m
[32m[2022-09-17 18:42:26,855] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-17 18:42:26,855] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-17 18:42:26,855] [    INFO][0m -   Num Epochs = 10[0m
[32m[2022-09-17 18:42:26,855] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2022-09-17 18:42:26,855] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2022-09-17 18:42:26,855] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-17 18:42:26,855] [    INFO][0m -   Total optimization steps = 100.0[0m
[32m[2022-09-17 18:42:26,855] [    INFO][0m -   Total num train samples = 1600[0m
[32m[2022-09-17 18:42:29,724] [    INFO][0m - loss: 5.82767944, learning_rate: 2.7e-06, global_step: 10, interval_runtime: 2.8675, interval_samples_per_second: 5.58, interval_steps_per_second: 3.487, epoch: 1.0[0m
[32m[2022-09-17 18:42:29,724] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 18:42:29,725] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-17 18:42:29,725] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:42:29,725] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:42:29,725] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-17 18:42:31,127] [    INFO][0m - eval_loss: 1.6725190877914429, eval_accuracy: 0.5125, eval_runtime: 1.4023, eval_samples_per_second: 114.096, eval_steps_per_second: 28.524, epoch: 1.0[0m
[32m[2022-09-17 18:42:31,128] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-10[0m
[32m[2022-09-17 18:42:31,128] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:42:34,349] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-10/tokenizer_config.json[0m
[32m[2022-09-17 18:42:34,349] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-10/special_tokens_map.json[0m
[32m[2022-09-17 18:42:41,607] [    INFO][0m - loss: 1.20482121, learning_rate: 2.4000000000000003e-06, global_step: 20, interval_runtime: 11.8832, interval_samples_per_second: 1.346, interval_steps_per_second: 0.842, epoch: 2.0[0m
[32m[2022-09-17 18:42:41,608] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 18:42:41,608] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-17 18:42:41,608] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:42:41,608] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:42:41,608] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-17 18:42:42,949] [    INFO][0m - eval_loss: 0.8863123655319214, eval_accuracy: 0.5125, eval_runtime: 1.3403, eval_samples_per_second: 119.373, eval_steps_per_second: 29.843, epoch: 2.0[0m
[32m[2022-09-17 18:42:42,949] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-20[0m
[32m[2022-09-17 18:42:42,949] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:42:45,841] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-20/tokenizer_config.json[0m
[32m[2022-09-17 18:42:45,841] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-20/special_tokens_map.json[0m
[32m[2022-09-17 18:42:57,447] [    INFO][0m - loss: 0.65663905, learning_rate: 2.1e-06, global_step: 30, interval_runtime: 15.8395, interval_samples_per_second: 1.01, interval_steps_per_second: 0.631, epoch: 3.0[0m
[32m[2022-09-17 18:42:57,447] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 18:42:57,448] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-17 18:42:57,448] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:42:57,448] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:42:57,448] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-17 18:42:58,841] [    INFO][0m - eval_loss: 0.4451823830604553, eval_accuracy: 0.525, eval_runtime: 1.3927, eval_samples_per_second: 114.888, eval_steps_per_second: 28.722, epoch: 3.0[0m
[32m[2022-09-17 18:42:58,841] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-30[0m
[32m[2022-09-17 18:42:58,841] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:43:01,889] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-30/tokenizer_config.json[0m
[32m[2022-09-17 18:43:01,890] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-30/special_tokens_map.json[0m
[32m[2022-09-17 18:43:09,806] [    INFO][0m - loss: 0.40404716, learning_rate: 1.8e-06, global_step: 40, interval_runtime: 12.3594, interval_samples_per_second: 1.295, interval_steps_per_second: 0.809, epoch: 4.0[0m
[32m[2022-09-17 18:43:09,807] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 18:43:09,807] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-17 18:43:09,807] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:43:09,807] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:43:09,807] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-17 18:43:11,122] [    INFO][0m - eval_loss: 0.3071754574775696, eval_accuracy: 0.7625, eval_runtime: 1.3146, eval_samples_per_second: 121.708, eval_steps_per_second: 30.427, epoch: 4.0[0m
[32m[2022-09-17 18:43:11,122] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-40[0m
[32m[2022-09-17 18:43:11,122] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:43:13,901] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-40/tokenizer_config.json[0m
[32m[2022-09-17 18:43:13,902] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-40/special_tokens_map.json[0m
[32m[2022-09-17 18:43:21,729] [    INFO][0m - loss: 0.30057187, learning_rate: 1.5e-06, global_step: 50, interval_runtime: 11.9227, interval_samples_per_second: 1.342, interval_steps_per_second: 0.839, epoch: 5.0[0m
[32m[2022-09-17 18:43:21,730] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 18:43:21,730] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-17 18:43:21,730] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:43:21,730] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:43:21,730] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-17 18:43:23,035] [    INFO][0m - eval_loss: 0.2790135145187378, eval_accuracy: 0.75, eval_runtime: 1.3048, eval_samples_per_second: 122.621, eval_steps_per_second: 30.655, epoch: 5.0[0m
[32m[2022-09-17 18:43:23,036] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-50[0m
[32m[2022-09-17 18:43:23,036] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:43:25,714] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-50/tokenizer_config.json[0m
[32m[2022-09-17 18:43:25,715] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-50/special_tokens_map.json[0m
[32m[2022-09-17 18:43:33,432] [    INFO][0m - loss: 0.28090861, learning_rate: 1.2000000000000002e-06, global_step: 60, interval_runtime: 11.7035, interval_samples_per_second: 1.367, interval_steps_per_second: 0.854, epoch: 6.0[0m
[32m[2022-09-17 18:43:33,433] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 18:43:33,433] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-17 18:43:33,433] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:43:33,434] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:43:33,434] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-17 18:43:34,730] [    INFO][0m - eval_loss: 0.250436395406723, eval_accuracy: 0.79375, eval_runtime: 1.2963, eval_samples_per_second: 123.429, eval_steps_per_second: 30.857, epoch: 6.0[0m
[32m[2022-09-17 18:43:37,928] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-60[0m
[32m[2022-09-17 18:43:37,929] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:43:40,679] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-60/tokenizer_config.json[0m
[32m[2022-09-17 18:43:40,679] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-60/special_tokens_map.json[0m
[32m[2022-09-17 18:43:48,809] [    INFO][0m - loss: 0.24997029, learning_rate: 9e-07, global_step: 70, interval_runtime: 15.3768, interval_samples_per_second: 1.041, interval_steps_per_second: 0.65, epoch: 7.0[0m
[32m[2022-09-17 18:43:48,810] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 18:43:48,810] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-17 18:43:48,810] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:43:48,810] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:43:48,810] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-17 18:43:50,198] [    INFO][0m - eval_loss: 0.23691439628601074, eval_accuracy: 0.79375, eval_runtime: 1.387, eval_samples_per_second: 115.354, eval_steps_per_second: 28.839, epoch: 7.0[0m
[32m[2022-09-17 18:43:50,198] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-70[0m
[32m[2022-09-17 18:43:50,198] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:43:57,793] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-70/tokenizer_config.json[0m
[32m[2022-09-17 18:43:57,793] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-70/special_tokens_map.json[0m
[32m[2022-09-17 18:44:05,827] [    INFO][0m - loss: 0.22084801, learning_rate: 6.000000000000001e-07, global_step: 80, interval_runtime: 17.0175, interval_samples_per_second: 0.94, interval_steps_per_second: 0.588, epoch: 8.0[0m
[32m[2022-09-17 18:44:05,828] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 18:44:05,828] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-17 18:44:05,828] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:44:05,828] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:44:05,828] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-17 18:44:07,157] [    INFO][0m - eval_loss: 0.23424725234508514, eval_accuracy: 0.81875, eval_runtime: 1.3284, eval_samples_per_second: 120.442, eval_steps_per_second: 30.111, epoch: 8.0[0m
[32m[2022-09-17 18:44:07,157] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-80[0m
[32m[2022-09-17 18:44:07,157] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:44:10,000] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-80/tokenizer_config.json[0m
[32m[2022-09-17 18:44:10,000] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-80/special_tokens_map.json[0m
[32m[2022-09-17 18:44:17,096] [    INFO][0m - loss: 0.20946617, learning_rate: 3.0000000000000004e-07, global_step: 90, interval_runtime: 11.2697, interval_samples_per_second: 1.42, interval_steps_per_second: 0.887, epoch: 9.0[0m
[32m[2022-09-17 18:44:17,097] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 18:44:17,097] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-17 18:44:17,097] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:44:17,097] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:44:17,097] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-17 18:44:18,434] [    INFO][0m - eval_loss: 0.23870806396007538, eval_accuracy: 0.80625, eval_runtime: 1.3363, eval_samples_per_second: 119.734, eval_steps_per_second: 29.934, epoch: 9.0[0m
[32m[2022-09-17 18:44:18,434] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-90[0m
[32m[2022-09-17 18:44:18,435] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:44:21,162] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-90/tokenizer_config.json[0m
[32m[2022-09-17 18:44:21,163] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-90/special_tokens_map.json[0m
[32m[2022-09-17 18:44:28,188] [    INFO][0m - loss: 0.21158123, learning_rate: 0.0, global_step: 100, interval_runtime: 11.0914, interval_samples_per_second: 1.443, interval_steps_per_second: 0.902, epoch: 10.0[0m
[32m[2022-09-17 18:44:28,189] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 18:44:28,189] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-17 18:44:28,189] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:44:28,189] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:44:28,189] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-17 18:44:29,499] [    INFO][0m - eval_loss: 0.23710806667804718, eval_accuracy: 0.80625, eval_runtime: 1.3098, eval_samples_per_second: 122.16, eval_steps_per_second: 30.54, epoch: 10.0[0m
[32m[2022-09-17 18:44:29,499] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-100[0m
[32m[2022-09-17 18:44:29,499] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:44:32,081] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-17 18:44:32,081] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-17 18:44:37,070] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-17 18:44:37,071] [    INFO][0m - Loading best model from ./checkpoints_bustm/checkpoint-80 (score: 0.81875).[0m
[32m[2022-09-17 18:44:38,728] [    INFO][0m - train_runtime: 131.8723, train_samples_per_second: 12.133, train_steps_per_second: 0.758, train_loss: 0.9566533041000366, epoch: 10.0[0m
[32m[2022-09-17 18:44:38,750] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/[0m
[32m[2022-09-17 18:44:38,751] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:44:41,099] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/tokenizer_config.json[0m
[32m[2022-09-17 18:44:41,100] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/special_tokens_map.json[0m
[32m[2022-09-17 18:44:41,101] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-17 18:44:41,101] [    INFO][0m -   epoch                    =       10.0[0m
[32m[2022-09-17 18:44:41,101] [    INFO][0m -   train_loss               =     0.9567[0m
[32m[2022-09-17 18:44:41,101] [    INFO][0m -   train_runtime            = 0:02:11.87[0m
[32m[2022-09-17 18:44:41,101] [    INFO][0m -   train_samples_per_second =     12.133[0m
[32m[2022-09-17 18:44:41,101] [    INFO][0m -   train_steps_per_second   =      0.758[0m
[32m[2022-09-17 18:44:41,103] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-17 18:44:41,103] [    INFO][0m -   Num examples = 1772[0m
[32m[2022-09-17 18:44:41,103] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:44:41,103] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:44:41,103] [    INFO][0m -   Total prediction steps = 443[0m
[32m[2022-09-17 18:44:56,751] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-17 18:44:56,752] [    INFO][0m -   test_accuracy           =     0.7579[0m
[32m[2022-09-17 18:44:56,752] [    INFO][0m -   test_loss               =     0.2514[0m
[32m[2022-09-17 18:44:56,752] [    INFO][0m -   test_runtime            = 0:00:15.64[0m
[32m[2022-09-17 18:44:56,752] [    INFO][0m -   test_samples_per_second =    113.239[0m
[32m[2022-09-17 18:44:56,752] [    INFO][0m -   test_steps_per_second   =      28.31[0m
[32m[2022-09-17 18:44:56,752] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-17 18:44:56,753] [    INFO][0m -   Num examples = 2000[0m
[32m[2022-09-17 18:44:56,753] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:44:56,753] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:44:56,753] [    INFO][0m -   Total prediction steps = 500[0m
[32m[2022-09-17 18:45:15,598] [    INFO][0m - Predictions for bustm saved to ./fewclue_submit_examples.[0m
{
  "labels": 1,
  "text_a": "\u53eb\u7238\u7238\u53eb\u4e00\u58f0\u6211\u542c\u542c",
  "text_b": "\u90a3\u4f60\u53eb\u6211\u4e00\u58f0\u7238\u7238",
  "uid": 0
}

Prediction done.
 
==========
chid
==========
 
[32m[2022-09-17 18:45:27,259] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-17 18:45:27,259] [    INFO][0m - ============================================================[0m
[32m[2022-09-17 18:45:27,259] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-17 18:45:27,259] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-17 18:45:27,260] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-17 18:45:27,260] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-17 18:45:27,260] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-17 18:45:27,260] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-17 18:45:27,260] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-17 18:45:27,260] [    INFO][0m - [0m
[32m[2022-09-17 18:45:27,260] [    INFO][0m - ============================================================[0m
[32m[2022-09-17 18:45:27,260] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-17 18:45:27,260] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-17 18:45:27,260] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-17 18:45:27,260] [    INFO][0m - pretrained                    :../checkpoints_cmnli/checkpoint-22000/model_state.pdparams[0m
[32m[2022-09-17 18:45:27,260] [    INFO][0m - prompt                        :â€œ{'text':'text_a'}â€è¿™å¥è¯ä¸­æˆè¯­[{'text':'text_b'}]çš„ç†è§£æ­£ç¡®å—ï¼Ÿ{'mask'}{'mask'}ã€‚[0m
[32m[2022-09-17 18:45:27,260] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-17 18:45:27,261] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-17 18:45:27,261] [    INFO][0m - task_name                     :chid[0m
[32m[2022-09-17 18:45:27,261] [    INFO][0m - [0m
[32m[2022-09-17 18:45:27,261] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0917 18:45:27.262993 19168 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0917 18:45:27.267158 19168 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-17 18:45:34,961] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-17 18:45:34,973] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-17 18:45:34,973] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-17 18:45:34,974] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': 'â€œ'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': 'â€è¿™å¥è¯ä¸­æˆè¯­['}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': ']çš„ç†è§£æ­£ç¡®å—ï¼Ÿ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'ã€‚'}][0m
[32m[2022-09-17 18:45:37,062] [    INFO][0m - ============================================================[0m
[32m[2022-09-17 18:45:37,062] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-17 18:45:37,063] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-17 18:45:37,063] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-17 18:45:37,063] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-17 18:45:37,063] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-17 18:45:37,063] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-17 18:45:37,063] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-17 18:45:37,063] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-17 18:45:37,063] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-17 18:45:37,063] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-17 18:45:37,063] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-17 18:45:37,063] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-17 18:45:37,064] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-17 18:45:37,064] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-17 18:45:37,064] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-17 18:45:37,064] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-17 18:45:37,064] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-17 18:45:37,064] [    INFO][0m - eval_batch_size               :4[0m
[32m[2022-09-17 18:45:37,064] [    INFO][0m - eval_steps                    :200[0m
[32m[2022-09-17 18:45:37,064] [    INFO][0m - evaluation_strategy           :IntervalStrategy.EPOCH[0m
[32m[2022-09-17 18:45:37,064] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-17 18:45:37,064] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-17 18:45:37,064] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-17 18:45:37,064] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-17 18:45:37,064] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-17 18:45:37,065] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-17 18:45:37,065] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-17 18:45:37,065] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-17 18:45:37,065] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-17 18:45:37,065] [    INFO][0m - learning_rate                 :3e-06[0m
[32m[2022-09-17 18:45:37,065] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-17 18:45:37,065] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-17 18:45:37,065] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-17 18:45:37,065] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-17 18:45:37,065] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-17 18:45:37,065] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-17 18:45:37,065] [    INFO][0m - logging_dir                   :./checkpoints_chid/runs/Sep17_18-45-27_instance-3bwob41y-01[0m
[32m[2022-09-17 18:45:37,065] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-17 18:45:37,065] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-17 18:45:37,066] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-17 18:45:37,066] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-17 18:45:37,066] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-17 18:45:37,066] [    INFO][0m - max_seq_length                :256[0m
[32m[2022-09-17 18:45:37,066] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-17 18:45:37,066] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-17 18:45:37,066] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-17 18:45:37,066] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-17 18:45:37,066] [    INFO][0m - num_train_epochs              :10.0[0m
[32m[2022-09-17 18:45:37,066] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-17 18:45:37,066] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-17 18:45:37,066] [    INFO][0m - output_dir                    :./checkpoints_chid/[0m
[32m[2022-09-17 18:45:37,066] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-17 18:45:37,067] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-17 18:45:37,067] [    INFO][0m - per_device_eval_batch_size    :4[0m
[32m[2022-09-17 18:45:37,067] [    INFO][0m - per_device_train_batch_size   :16[0m
[32m[2022-09-17 18:45:37,067] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-17 18:45:37,067] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-17 18:45:37,067] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-17 18:45:37,067] [    INFO][0m - ppt_learning_rate             :3e-05[0m
[32m[2022-09-17 18:45:37,067] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-17 18:45:37,067] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-17 18:45:37,067] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-17 18:45:37,067] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-17 18:45:37,067] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-17 18:45:37,067] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-17 18:45:37,067] [    INFO][0m - run_name                      :./checkpoints_chid/[0m
[32m[2022-09-17 18:45:37,068] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-17 18:45:37,068] [    INFO][0m - save_steps                    :200[0m
[32m[2022-09-17 18:45:37,068] [    INFO][0m - save_strategy                 :IntervalStrategy.EPOCH[0m
[32m[2022-09-17 18:45:37,068] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-17 18:45:37,068] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-17 18:45:37,068] [    INFO][0m - seed                          :42[0m
[32m[2022-09-17 18:45:37,068] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-17 18:45:37,068] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-17 18:45:37,068] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-17 18:45:37,068] [    INFO][0m - train_batch_size              :16[0m
[32m[2022-09-17 18:45:37,068] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-17 18:45:37,068] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-17 18:45:37,068] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-17 18:45:37,069] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-17 18:45:37,069] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-17 18:45:37,069] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-17 18:45:37,069] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-17 18:45:37,069] [    INFO][0m - [0m
[32m[2022-09-17 18:45:37,072] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-17 18:45:37,072] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-17 18:45:37,072] [    INFO][0m -   Num Epochs = 10[0m
[32m[2022-09-17 18:45:37,072] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2022-09-17 18:45:37,072] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2022-09-17 18:45:37,072] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-17 18:45:37,072] [    INFO][0m -   Total optimization steps = 890.0[0m
[32m[2022-09-17 18:45:37,072] [    INFO][0m -   Total num train samples = 14140[0m
[32m[2022-09-17 18:45:44,437] [    INFO][0m - loss: 6.61474457, learning_rate: 2.9662921348314606e-06, global_step: 10, interval_runtime: 7.3636, interval_samples_per_second: 2.173, interval_steps_per_second: 1.358, epoch: 0.1124[0m
[32m[2022-09-17 18:45:50,767] [    INFO][0m - loss: 1.40047016, learning_rate: 2.932584269662921e-06, global_step: 20, interval_runtime: 6.3297, interval_samples_per_second: 2.528, interval_steps_per_second: 1.58, epoch: 0.2247[0m
[32m[2022-09-17 18:45:57,100] [    INFO][0m - loss: 0.35115511, learning_rate: 2.898876404494382e-06, global_step: 30, interval_runtime: 6.3336, interval_samples_per_second: 2.526, interval_steps_per_second: 1.579, epoch: 0.3371[0m
[32m[2022-09-17 18:46:03,429] [    INFO][0m - loss: 0.58690715, learning_rate: 2.8651685393258426e-06, global_step: 40, interval_runtime: 6.3281, interval_samples_per_second: 2.528, interval_steps_per_second: 1.58, epoch: 0.4494[0m
[32m[2022-09-17 18:46:09,761] [    INFO][0m - loss: 0.46302934, learning_rate: 2.8314606741573035e-06, global_step: 50, interval_runtime: 6.3323, interval_samples_per_second: 2.527, interval_steps_per_second: 1.579, epoch: 0.5618[0m
[32m[2022-09-17 18:46:16,089] [    INFO][0m - loss: 0.43275971, learning_rate: 2.797752808988764e-06, global_step: 60, interval_runtime: 6.3279, interval_samples_per_second: 2.528, interval_steps_per_second: 1.58, epoch: 0.6742[0m
[32m[2022-09-17 18:46:22,423] [    INFO][0m - loss: 0.42549787, learning_rate: 2.764044943820225e-06, global_step: 70, interval_runtime: 6.3338, interval_samples_per_second: 2.526, interval_steps_per_second: 1.579, epoch: 0.7865[0m
[32m[2022-09-17 18:46:28,751] [    INFO][0m - loss: 0.50885482, learning_rate: 2.7303370786516855e-06, global_step: 80, interval_runtime: 6.3286, interval_samples_per_second: 2.528, interval_steps_per_second: 1.58, epoch: 0.8989[0m
[32m[2022-09-17 18:46:33,976] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 18:46:33,976] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-17 18:46:33,976] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:46:33,976] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:46:33,976] [    INFO][0m -   Total prediction steps = 354[0m
{
  "labels": 0,
  "text_a": "\u5927\u90e8\u5206\u7684\u5927\u529f\u544a\u6210\u90fd\u662f\u7531\u65e0\u6570\u4e2a\u9020\u8c23\u751f\u4e8b\u7d2f\u79ef\u800c\u6765,\u505a\u597d\u6bcf\u4e00\u4ef6\u5e94\u8be5\u505a\u597d\u7684\u5c0f\u4e8b\u7edd\u4e0d\u662f\u4e00\u4ef6\u5c0f\u4e8b\u3002 \u300a\u4f60\u597d\u674e\u7115\u82f1\u300b\u4e3b\u8981\u8bb2\u4e86\u8d3e\u73b2\u610f\u5916\u7a7f\u8d8a\u5230\u4e86\u6bcd\u4eb2\u5e74\u8f7b\u7684\u65f6\u5019,\u548c\u6bcd\u4eb2\u4e00\u8d77\u611f\u53d7\u4e86\u5979\u7684\u751f\u6d3b\u548c\u60c5\u611f\u3002\u8fd9\u6545\u4e8b\u662f\u6839\u636e\u8d3e\u73b2\u7684\u4eb2\u751f\u7ecf\u5386\u6539\u7f16\u800c\u6765, ...",
  "text_b": "",
  "uid": 24
}

Traceback (most recent call last):
  File "train_single.py", line 216, in <module>
    main()
  File "train_single.py", line 192, in main
    train_result = trainer.train(resume_from_checkpoint=None)
  File "/ssd2/wanghuijuan03/prompt/PaddleNLP/paddlenlp/trainer/trainer_base.py", line 600, in train
    self._maybe_log_save_evaluate(tr_loss, model, epoch,
  File "/ssd2/wanghuijuan03/prompt/PaddleNLP/paddlenlp/trainer/trainer_base.py", line 710, in _maybe_log_save_evaluate
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
  File "/ssd2/wanghuijuan03/prompt/PaddleNLP/paddlenlp/trainer/trainer_base.py", line 1335, in evaluate
    output = self.evaluation_loop(
  File "/ssd2/wanghuijuan03/prompt/PaddleNLP/paddlenlp/trainer/trainer_base.py", line 1490, in evaluation_loop
    metrics = self.compute_metrics(
  File "train_single.py", line 149, in chid_compute_metrics
    acc = sum(preds == labels) / len(preds)
TypeError: 'bool' object is not iterable
 
==========
csl
==========
 
[32m[2022-09-17 18:47:03,783] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-17 18:47:03,783] [    INFO][0m - ============================================================[0m
[32m[2022-09-17 18:47:03,783] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-17 18:47:03,783] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-17 18:47:03,784] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-17 18:47:03,784] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-17 18:47:03,784] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-17 18:47:03,784] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-17 18:47:03,784] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-17 18:47:03,784] [    INFO][0m - [0m
[32m[2022-09-17 18:47:03,784] [    INFO][0m - ============================================================[0m
[32m[2022-09-17 18:47:03,784] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-17 18:47:03,784] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-17 18:47:03,784] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-17 18:47:03,784] [    INFO][0m - pretrained                    :../checkpoints_cmnli/checkpoint-22000/model_state.pdparams[0m
[32m[2022-09-17 18:47:03,784] [    INFO][0m - prompt                        :â€œ{'text':'text_a'}â€æœ¬æ–‡çš„å†…å®¹{'mask'}{'mask'}â€œ{'text':'text_b'}â€[0m
[32m[2022-09-17 18:47:03,784] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-17 18:47:03,785] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-17 18:47:03,785] [    INFO][0m - task_name                     :csl[0m
[32m[2022-09-17 18:47:03,785] [    INFO][0m - [0m
[32m[2022-09-17 18:47:03,785] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0917 18:47:03.786518 21069 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0917 18:47:03.790865 21069 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-17 18:47:11,291] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-17 18:47:11,302] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-17 18:47:11,303] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-17 18:47:11,303] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': 'â€œ'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': 'â€æœ¬æ–‡çš„å†…å®¹'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'â€œ'}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': 'â€'}][0m
[32m[2022-09-17 18:47:13,142] [    INFO][0m - ============================================================[0m
[32m[2022-09-17 18:47:13,143] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-17 18:47:13,143] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-17 18:47:13,143] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-17 18:47:13,143] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-17 18:47:13,143] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-17 18:47:13,143] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-17 18:47:13,143] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-17 18:47:13,143] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-17 18:47:13,143] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-17 18:47:13,143] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-17 18:47:13,143] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-17 18:47:13,144] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-17 18:47:13,144] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-17 18:47:13,144] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-17 18:47:13,144] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-17 18:47:13,144] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-17 18:47:13,144] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-17 18:47:13,144] [    INFO][0m - eval_batch_size               :4[0m
[32m[2022-09-17 18:47:13,144] [    INFO][0m - eval_steps                    :200[0m
[32m[2022-09-17 18:47:13,144] [    INFO][0m - evaluation_strategy           :IntervalStrategy.EPOCH[0m
[32m[2022-09-17 18:47:13,144] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-17 18:47:13,144] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-17 18:47:13,144] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-17 18:47:13,144] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-17 18:47:13,145] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-17 18:47:13,145] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-17 18:47:13,145] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-17 18:47:13,145] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-17 18:47:13,145] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-17 18:47:13,145] [    INFO][0m - learning_rate                 :3e-06[0m
[32m[2022-09-17 18:47:13,145] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-17 18:47:13,145] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-17 18:47:13,145] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-17 18:47:13,145] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-17 18:47:13,145] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-17 18:47:13,145] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-17 18:47:13,145] [    INFO][0m - logging_dir                   :./checkpoints_csl/runs/Sep17_18-47-03_instance-3bwob41y-01[0m
[32m[2022-09-17 18:47:13,145] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-17 18:47:13,146] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-17 18:47:13,146] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-17 18:47:13,146] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-17 18:47:13,146] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-17 18:47:13,146] [    INFO][0m - max_seq_length                :320[0m
[32m[2022-09-17 18:47:13,146] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-17 18:47:13,146] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-17 18:47:13,146] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-17 18:47:13,146] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-17 18:47:13,146] [    INFO][0m - num_train_epochs              :10.0[0m
[32m[2022-09-17 18:47:13,146] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-17 18:47:13,146] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-17 18:47:13,146] [    INFO][0m - output_dir                    :./checkpoints_csl/[0m
[32m[2022-09-17 18:47:13,147] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-17 18:47:13,147] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-17 18:47:13,147] [    INFO][0m - per_device_eval_batch_size    :4[0m
[32m[2022-09-17 18:47:13,147] [    INFO][0m - per_device_train_batch_size   :16[0m
[32m[2022-09-17 18:47:13,147] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-17 18:47:13,147] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-17 18:47:13,147] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-17 18:47:13,147] [    INFO][0m - ppt_learning_rate             :3e-05[0m
[32m[2022-09-17 18:47:13,147] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-17 18:47:13,147] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-17 18:47:13,147] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-17 18:47:13,147] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-17 18:47:13,147] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-17 18:47:13,147] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-17 18:47:13,148] [    INFO][0m - run_name                      :./checkpoints_csl/[0m
[32m[2022-09-17 18:47:13,148] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-17 18:47:13,148] [    INFO][0m - save_steps                    :200[0m
[32m[2022-09-17 18:47:13,148] [    INFO][0m - save_strategy                 :IntervalStrategy.EPOCH[0m
[32m[2022-09-17 18:47:13,148] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-17 18:47:13,148] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-17 18:47:13,148] [    INFO][0m - seed                          :42[0m
[32m[2022-09-17 18:47:13,148] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-17 18:47:13,148] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-17 18:47:13,148] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-17 18:47:13,148] [    INFO][0m - train_batch_size              :16[0m
[32m[2022-09-17 18:47:13,148] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-17 18:47:13,148] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-17 18:47:13,148] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-17 18:47:13,149] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-17 18:47:13,149] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-17 18:47:13,149] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-17 18:47:13,149] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-17 18:47:13,149] [    INFO][0m - [0m
[32m[2022-09-17 18:47:13,152] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-17 18:47:13,152] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-17 18:47:13,152] [    INFO][0m -   Num Epochs = 10[0m
[32m[2022-09-17 18:47:13,152] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2022-09-17 18:47:13,152] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2022-09-17 18:47:13,152] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-17 18:47:13,152] [    INFO][0m -   Total optimization steps = 100.0[0m
[32m[2022-09-17 18:47:13,153] [    INFO][0m -   Total num train samples = 1600[0m
[33m[2022-09-17 18:47:13,330] [ WARNING][0m - Token indices sequence length is longer than the specified maximum sequence length for this model (608 > 512). Running this sequence through the model will result in indexing errors[0m
[32m[2022-09-17 18:47:21,798] [    INFO][0m - loss: 5.51062126, learning_rate: 2.7e-06, global_step: 10, interval_runtime: 8.6435, interval_samples_per_second: 1.851, interval_steps_per_second: 1.157, epoch: 1.0[0m
[32m[2022-09-17 18:47:21,800] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 18:47:21,800] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-17 18:47:21,800] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:47:21,800] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:47:21,800] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-17 18:47:25,356] [    INFO][0m - eval_loss: 3.44575834274292, eval_accuracy: 0.50625, eval_runtime: 3.5556, eval_samples_per_second: 45.0, eval_steps_per_second: 11.25, epoch: 1.0[0m
[32m[2022-09-17 18:47:25,357] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-10[0m
[32m[2022-09-17 18:47:25,357] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:47:28,970] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-10/tokenizer_config.json[0m
[32m[2022-09-17 18:47:28,971] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-10/special_tokens_map.json[0m
[32m[2022-09-17 18:47:43,302] [    INFO][0m - loss: 2.75464897, learning_rate: 2.4000000000000003e-06, global_step: 20, interval_runtime: 21.5051, interval_samples_per_second: 0.744, interval_steps_per_second: 0.465, epoch: 2.0[0m
[32m[2022-09-17 18:47:43,303] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 18:47:43,303] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-17 18:47:43,303] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:47:43,303] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:47:43,303] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-17 18:47:46,752] [    INFO][0m - eval_loss: 1.5666126012802124, eval_accuracy: 0.6, eval_runtime: 3.4488, eval_samples_per_second: 46.393, eval_steps_per_second: 11.598, epoch: 2.0[0m
[32m[2022-09-17 18:47:46,753] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-20[0m
[32m[2022-09-17 18:47:46,753] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:47:49,620] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-20/tokenizer_config.json[0m
[32m[2022-09-17 18:47:49,620] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-20/special_tokens_map.json[0m
[32m[2022-09-17 18:48:03,017] [    INFO][0m - loss: 1.27283077, learning_rate: 2.1e-06, global_step: 30, interval_runtime: 19.7151, interval_samples_per_second: 0.812, interval_steps_per_second: 0.507, epoch: 3.0[0m
[32m[2022-09-17 18:48:03,018] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 18:48:03,018] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-17 18:48:03,018] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:48:03,018] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:48:03,019] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-17 18:48:06,455] [    INFO][0m - eval_loss: 0.8077991604804993, eval_accuracy: 0.5625, eval_runtime: 3.4359, eval_samples_per_second: 46.567, eval_steps_per_second: 11.642, epoch: 3.0[0m
[32m[2022-09-17 18:48:06,455] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-30[0m
[32m[2022-09-17 18:48:06,455] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:48:09,241] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-30/tokenizer_config.json[0m
[32m[2022-09-17 18:48:09,241] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-30/special_tokens_map.json[0m
[32m[2022-09-17 18:48:22,756] [    INFO][0m - loss: 0.84194851, learning_rate: 1.8e-06, global_step: 40, interval_runtime: 19.738, interval_samples_per_second: 0.811, interval_steps_per_second: 0.507, epoch: 4.0[0m
[32m[2022-09-17 18:48:22,756] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 18:48:22,756] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-17 18:48:22,757] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:48:22,757] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:48:22,757] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-17 18:48:26,197] [    INFO][0m - eval_loss: 0.686506450176239, eval_accuracy: 0.65, eval_runtime: 3.4405, eval_samples_per_second: 46.504, eval_steps_per_second: 11.626, epoch: 4.0[0m
[32m[2022-09-17 18:48:26,198] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-40[0m
[32m[2022-09-17 18:48:26,198] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:48:28,886] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-40/tokenizer_config.json[0m
[32m[2022-09-17 18:48:28,887] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-40/special_tokens_map.json[0m
[32m[2022-09-17 18:48:42,267] [    INFO][0m - loss: 0.72907224, learning_rate: 1.5e-06, global_step: 50, interval_runtime: 19.5117, interval_samples_per_second: 0.82, interval_steps_per_second: 0.513, epoch: 5.0[0m
[32m[2022-09-17 18:48:42,268] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 18:48:42,268] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-17 18:48:42,268] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:48:42,268] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:48:42,268] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-17 18:48:46,866] [    INFO][0m - eval_loss: 0.654016375541687, eval_accuracy: 0.65, eval_runtime: 3.4562, eval_samples_per_second: 46.293, eval_steps_per_second: 11.573, epoch: 5.0[0m
[32m[2022-09-17 18:48:46,866] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-50[0m
[32m[2022-09-17 18:48:46,866] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:48:49,574] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-50/tokenizer_config.json[0m
[32m[2022-09-17 18:48:49,574] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-50/special_tokens_map.json[0m
[32m[2022-09-17 18:49:02,831] [    INFO][0m - loss: 0.64518275, learning_rate: 1.2000000000000002e-06, global_step: 60, interval_runtime: 20.5642, interval_samples_per_second: 0.778, interval_steps_per_second: 0.486, epoch: 6.0[0m
[32m[2022-09-17 18:49:02,831] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 18:49:02,831] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-17 18:49:02,832] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:49:02,832] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:49:02,832] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-17 18:49:06,255] [    INFO][0m - eval_loss: 0.6576456427574158, eval_accuracy: 0.64375, eval_runtime: 3.4228, eval_samples_per_second: 46.746, eval_steps_per_second: 11.686, epoch: 6.0[0m
[32m[2022-09-17 18:49:06,255] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-60[0m
[32m[2022-09-17 18:49:06,255] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:49:13,954] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-60/tokenizer_config.json[0m
[32m[2022-09-17 18:49:13,954] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-60/special_tokens_map.json[0m
[32m[2022-09-17 18:49:27,240] [    INFO][0m - loss: 0.60848284, learning_rate: 9e-07, global_step: 70, interval_runtime: 24.4091, interval_samples_per_second: 0.655, interval_steps_per_second: 0.41, epoch: 7.0[0m
[32m[2022-09-17 18:49:27,241] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 18:49:27,241] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-17 18:49:27,241] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:49:27,241] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:49:27,241] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-17 18:49:30,677] [    INFO][0m - eval_loss: 0.6600618958473206, eval_accuracy: 0.625, eval_runtime: 3.4355, eval_samples_per_second: 46.573, eval_steps_per_second: 11.643, epoch: 7.0[0m
[32m[2022-09-17 18:49:30,678] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-70[0m
[32m[2022-09-17 18:49:30,678] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:49:33,328] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-70/tokenizer_config.json[0m
[32m[2022-09-17 18:49:33,328] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-70/special_tokens_map.json[0m
[32m[2022-09-17 18:49:46,339] [    INFO][0m - loss: 0.56810861, learning_rate: 6.000000000000001e-07, global_step: 80, interval_runtime: 19.099, interval_samples_per_second: 0.838, interval_steps_per_second: 0.524, epoch: 8.0[0m
[32m[2022-09-17 18:49:46,339] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 18:49:46,340] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-17 18:49:46,340] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:49:46,340] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:49:46,340] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-17 18:49:49,767] [    INFO][0m - eval_loss: 0.6405364274978638, eval_accuracy: 0.5875, eval_runtime: 3.4271, eval_samples_per_second: 46.687, eval_steps_per_second: 11.672, epoch: 8.0[0m
[32m[2022-09-17 18:49:49,768] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-80[0m
[32m[2022-09-17 18:49:49,768] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:49:52,393] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-80/tokenizer_config.json[0m
[32m[2022-09-17 18:49:52,393] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-80/special_tokens_map.json[0m
[32m[2022-09-17 18:50:09,731] [    INFO][0m - loss: 0.55966234, learning_rate: 3.0000000000000004e-07, global_step: 90, interval_runtime: 18.9977, interval_samples_per_second: 0.842, interval_steps_per_second: 0.526, epoch: 9.0[0m
[32m[2022-09-17 18:50:09,731] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 18:50:09,732] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-17 18:50:09,732] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:50:09,732] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:50:09,732] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-17 18:50:13,165] [    INFO][0m - eval_loss: 0.6352967619895935, eval_accuracy: 0.5875, eval_runtime: 3.4328, eval_samples_per_second: 46.609, eval_steps_per_second: 11.652, epoch: 9.0[0m
[32m[2022-09-17 18:50:13,165] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-90[0m
[32m[2022-09-17 18:50:13,165] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:50:15,830] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-90/tokenizer_config.json[0m
[32m[2022-09-17 18:50:15,831] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-90/special_tokens_map.json[0m
[32m[2022-09-17 18:50:28,754] [    INFO][0m - loss: 0.6080205, learning_rate: 0.0, global_step: 100, interval_runtime: 23.4173, interval_samples_per_second: 0.683, interval_steps_per_second: 0.427, epoch: 10.0[0m
[32m[2022-09-17 18:50:28,754] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 18:50:28,754] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-17 18:50:28,755] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:50:28,755] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:50:28,755] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-17 18:50:32,199] [    INFO][0m - eval_loss: 0.6353267431259155, eval_accuracy: 0.6125, eval_runtime: 3.4445, eval_samples_per_second: 46.451, eval_steps_per_second: 11.613, epoch: 10.0[0m
[32m[2022-09-17 18:50:32,200] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-100[0m
[32m[2022-09-17 18:50:32,200] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:50:34,719] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-17 18:50:34,719] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-17 18:50:39,714] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-17 18:50:39,714] [    INFO][0m - Loading best model from ./checkpoints_csl/checkpoint-40 (score: 0.65).[0m
[32m[2022-09-17 18:50:41,330] [    INFO][0m - train_runtime: 208.1766, train_samples_per_second: 7.686, train_steps_per_second: 0.48, train_loss: 1.4098578786849976, epoch: 10.0[0m
[32m[2022-09-17 18:50:41,332] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/[0m
[32m[2022-09-17 18:50:41,332] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:50:43,834] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/tokenizer_config.json[0m
[32m[2022-09-17 18:50:43,835] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/special_tokens_map.json[0m
[32m[2022-09-17 18:50:43,836] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-17 18:50:43,836] [    INFO][0m -   epoch                    =       10.0[0m
[32m[2022-09-17 18:50:43,836] [    INFO][0m -   train_loss               =     1.4099[0m
[32m[2022-09-17 18:50:43,836] [    INFO][0m -   train_runtime            = 0:03:28.17[0m
[32m[2022-09-17 18:50:43,836] [    INFO][0m -   train_samples_per_second =      7.686[0m
[32m[2022-09-17 18:50:43,836] [    INFO][0m -   train_steps_per_second   =       0.48[0m
[32m[2022-09-17 18:50:43,838] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-17 18:50:43,838] [    INFO][0m -   Num examples = 2838[0m
[32m[2022-09-17 18:50:43,838] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:50:43,838] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:50:43,838] [    INFO][0m -   Total prediction steps = 710[0m
[32m[2022-09-17 18:51:45,862] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-17 18:51:45,862] [    INFO][0m -   test_accuracy           =      0.586[0m
[32m[2022-09-17 18:51:45,862] [    INFO][0m -   test_loss               =     0.7244[0m
[32m[2022-09-17 18:51:45,862] [    INFO][0m -   test_runtime            = 0:01:02.02[0m
[32m[2022-09-17 18:51:45,862] [    INFO][0m -   test_samples_per_second =     45.757[0m
[32m[2022-09-17 18:51:45,863] [    INFO][0m -   test_steps_per_second   =     11.447[0m
[32m[2022-09-17 18:51:45,863] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-17 18:51:45,863] [    INFO][0m -   Num examples = 3000[0m
[32m[2022-09-17 18:51:45,863] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:51:45,863] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:51:45,863] [    INFO][0m -   Total prediction steps = 750[0m
[32m[2022-09-17 18:52:59,283] [    INFO][0m - Predictions for cslf saved to ./fewclue_submit_examples.[0m
{
  "labels": 1,
  "text_a": "\u65e0\u7ebf\u4f20\u611f\u5668\u7f51\u7edc\u4e2d\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u901a\u7528\u8fd1\u4f3c\u67e5\u8be2\u662f\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\uff0e\u6587\u4e2d\u63d0\u51fa\u4e00\u79cd\u65e0\u7ebf\u4f20\u611f\u5668\u7f51\u7edc\u4e2d\u9690\u79c1\u4fdd\u62a4\u901a\u7528\u8fd1\u4f3c\u67e5\u8be2\u534f\u8baePGAQ\uff0ePGAQ\u5c06\u4f20\u611f\u5668\u8282\u70b9\u7f16\u53f7\u548c\u5176\u91c7\u96c6\u6570\u636e\u9690\u85cf\u4e8e\u8bbe\u8ba1\u7684\u6570\u636e\u7ed3\u6784\u4e2d\uff0c\u5728\u57fa\u7ad9\u6784\u9020\u7ebf\u6027\u65b9\u7a0b\u7ec4\u89e3\u51fa\u76f4\u65b9\u56fe\uff0c\u6839\u636e\u76f4\u65b9\u56fe\u5177\u6709\u7684\u7edf\u8ba1\u4fe1\u606f\uff0c\u4e0d\u6cc4\u9732\u9690\u79c1\u5730\u5b8c\u6210Top-k\u67e5\u8be2\u3001\u8303\u56f4\u67e5\u8be2\u3001SUM\u3001MAX/MIN\u3001Median\u3001Histogram\u7b49\u8fd1\u4f3c\u67e5\u8be2\uff0ePGAQ\u4f7f\u7528\u7f51\u5185\u6c42\u548c\u805a\u96c6\u4ee5\u51cf\u5c11\u80fd\u91cf\u6d88\u8017\uff0c\u5e76\u4e14\u80fd\u591f\u901a\u8fc7\u8c03\u8282\u76f4\u65b9\u56fe\u5212\u5206\u7c92\u5ea6\u6765\u5e73\u8861\u67e5\u8be2\u7cbe\u5ea6\u4e0e\u80fd\u91cf\u6d88\u8017\uff0ePGAQ\u534f\u8bae\u5206\u4e3aH-PGAQ\u548cF-PGAQ\u4e24\u79cd\u6a21\u5f0f\uff0eH-PGAQ\u6a21\u5f0f\u4f7f\u7528\u6570\u636e\u6270\u52a8\u6280\u672f\u52a0\u5f3a\u6570\u636e\u5b89\u5168\u6027\uff0cF-PGAQ\u4f7f\u7528\u8fc7\u6ee4\u5668\u51cf\u5c11\u8fde\u7eed\u67e5\u8be2\u901a\u4fe1\u91cf\uff0e\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u4f7f\u7528\u771f\u5b9e\u6570\u636e\u96c6\u5b9e\u9a8c\u9a8c\u8bc1\u4e86PGAQ\u7684\u5b89\u5168\u6027\u548c\u6709\u6548\u6027\uff0e",
  "text_b": "\u65e0\u7ebf\u4f20\u611f\u5668\u7f51\u7edc\u3001\u6570\u636e\u805a\u96c6\u3001\u7269\u8054\u7f51\u3001\u8fd1\u4f3c\u67e5\u8be2",
  "uid": 0
}

Prediction done.
 
==========
cluewsc
==========
 
[32m[2022-09-17 18:53:10,497] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-17 18:53:10,498] [    INFO][0m - ============================================================[0m
[32m[2022-09-17 18:53:10,498] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-17 18:53:10,498] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-17 18:53:10,498] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-17 18:53:10,498] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-17 18:53:10,498] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-17 18:53:10,498] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-17 18:53:10,498] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-17 18:53:10,498] [    INFO][0m - [0m
[32m[2022-09-17 18:53:10,498] [    INFO][0m - ============================================================[0m
[32m[2022-09-17 18:53:10,498] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-17 18:53:10,498] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-17 18:53:10,499] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-17 18:53:10,499] [    INFO][0m - pretrained                    :../checkpoints_cmnli/checkpoint-22000/model_state.pdparams[0m
[32m[2022-09-17 18:53:10,499] [    INFO][0m - prompt                        :â€œ{'text':'text_a'}â€{'text':'text_b'}è¿™é‡Œä»£è¯ä½¿ç”¨æ­£ç¡®å—ï¼Ÿ{'mask'}{'mask'}[0m
[32m[2022-09-17 18:53:10,499] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-17 18:53:10,499] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-17 18:53:10,499] [    INFO][0m - task_name                     :cluewsc[0m
[32m[2022-09-17 18:53:10,499] [    INFO][0m - [0m
[32m[2022-09-17 18:53:10,499] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0917 18:53:10.500818 30137 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0917 18:53:10.504935 30137 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-17 18:53:18,162] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-17 18:53:18,173] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-17 18:53:18,173] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-17 18:53:18,174] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': 'â€œ'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': 'â€'}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': 'è¿™é‡Œä»£è¯ä½¿ç”¨æ­£ç¡®å—ï¼Ÿ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}][0m
[32m[2022-09-17 18:53:19,966] [    INFO][0m - ============================================================[0m
[32m[2022-09-17 18:53:19,966] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-17 18:53:19,966] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-17 18:53:19,966] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-17 18:53:19,966] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-17 18:53:19,966] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-17 18:53:19,966] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-17 18:53:19,966] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-17 18:53:19,967] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-17 18:53:19,967] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-17 18:53:19,967] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-17 18:53:19,967] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-17 18:53:19,967] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-17 18:53:19,967] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-17 18:53:19,967] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-17 18:53:19,967] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-17 18:53:19,967] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-17 18:53:19,967] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-17 18:53:19,967] [    INFO][0m - eval_batch_size               :4[0m
[32m[2022-09-17 18:53:19,967] [    INFO][0m - eval_steps                    :200[0m
[32m[2022-09-17 18:53:19,967] [    INFO][0m - evaluation_strategy           :IntervalStrategy.EPOCH[0m
[32m[2022-09-17 18:53:19,968] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-17 18:53:19,968] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-17 18:53:19,968] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-17 18:53:19,968] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-17 18:53:19,968] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-17 18:53:19,968] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-17 18:53:19,968] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-17 18:53:19,968] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-17 18:53:19,968] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-17 18:53:19,968] [    INFO][0m - learning_rate                 :3e-06[0m
[32m[2022-09-17 18:53:19,968] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-17 18:53:19,968] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-17 18:53:19,968] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-17 18:53:19,968] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-17 18:53:19,969] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-17 18:53:19,969] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-17 18:53:19,969] [    INFO][0m - logging_dir                   :./checkpoints_cluewsc/runs/Sep17_18-53-10_instance-3bwob41y-01[0m
[32m[2022-09-17 18:53:19,969] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-17 18:53:19,969] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-17 18:53:19,969] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-17 18:53:19,969] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-17 18:53:19,969] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-17 18:53:19,969] [    INFO][0m - max_seq_length                :128[0m
[32m[2022-09-17 18:53:19,969] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-17 18:53:19,969] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-17 18:53:19,969] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-17 18:53:19,969] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-17 18:53:19,970] [    INFO][0m - num_train_epochs              :10.0[0m
[32m[2022-09-17 18:53:19,970] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-17 18:53:19,970] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-17 18:53:19,970] [    INFO][0m - output_dir                    :./checkpoints_cluewsc/[0m
[32m[2022-09-17 18:53:19,970] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-17 18:53:19,970] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-17 18:53:19,970] [    INFO][0m - per_device_eval_batch_size    :4[0m
[32m[2022-09-17 18:53:19,970] [    INFO][0m - per_device_train_batch_size   :16[0m
[32m[2022-09-17 18:53:19,970] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-17 18:53:19,970] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-17 18:53:19,970] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-17 18:53:19,970] [    INFO][0m - ppt_learning_rate             :3e-05[0m
[32m[2022-09-17 18:53:19,970] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-17 18:53:19,971] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-17 18:53:19,971] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-17 18:53:19,971] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-17 18:53:19,971] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-17 18:53:19,971] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-17 18:53:19,971] [    INFO][0m - run_name                      :./checkpoints_cluewsc/[0m
[32m[2022-09-17 18:53:19,971] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-17 18:53:19,971] [    INFO][0m - save_steps                    :200[0m
[32m[2022-09-17 18:53:19,971] [    INFO][0m - save_strategy                 :IntervalStrategy.EPOCH[0m
[32m[2022-09-17 18:53:19,971] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-17 18:53:19,971] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-17 18:53:19,971] [    INFO][0m - seed                          :42[0m
[32m[2022-09-17 18:53:19,971] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-17 18:53:19,971] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-17 18:53:19,972] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-17 18:53:19,972] [    INFO][0m - train_batch_size              :16[0m
[32m[2022-09-17 18:53:19,972] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-17 18:53:19,972] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-17 18:53:19,972] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-17 18:53:19,972] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-17 18:53:19,972] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-17 18:53:19,972] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-17 18:53:19,972] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-17 18:53:19,972] [    INFO][0m - [0m
[32m[2022-09-17 18:53:19,975] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-17 18:53:19,975] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-17 18:53:19,975] [    INFO][0m -   Num Epochs = 10[0m
[32m[2022-09-17 18:53:19,975] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2022-09-17 18:53:19,975] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2022-09-17 18:53:19,975] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-17 18:53:19,975] [    INFO][0m -   Total optimization steps = 100.0[0m
[32m[2022-09-17 18:53:19,976] [    INFO][0m -   Total num train samples = 1600[0m
[32m[2022-09-17 18:53:24,405] [    INFO][0m - loss: 4.85750542, learning_rate: 2.7e-06, global_step: 10, interval_runtime: 4.4293, interval_samples_per_second: 3.612, interval_steps_per_second: 2.258, epoch: 1.0[0m
[32m[2022-09-17 18:53:24,406] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 18:53:24,406] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-17 18:53:24,406] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:53:24,407] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:53:24,407] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-17 18:53:25,983] [    INFO][0m - eval_loss: 2.1014950275421143, eval_accuracy: 0.5031446540880503, eval_runtime: 1.5752, eval_samples_per_second: 100.942, eval_steps_per_second: 25.394, epoch: 1.0[0m
[32m[2022-09-17 18:53:25,983] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-10[0m
[32m[2022-09-17 18:53:25,983] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:53:29,337] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-10/tokenizer_config.json[0m
[32m[2022-09-17 18:53:29,337] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-10/special_tokens_map.json[0m
[32m[2022-09-17 18:53:38,972] [    INFO][0m - loss: 1.6114706, learning_rate: 2.4000000000000003e-06, global_step: 20, interval_runtime: 14.5657, interval_samples_per_second: 1.098, interval_steps_per_second: 0.687, epoch: 2.0[0m
[32m[2022-09-17 18:53:38,973] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 18:53:38,973] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-17 18:53:38,974] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:53:38,974] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:53:38,974] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-17 18:53:40,576] [    INFO][0m - eval_loss: 0.8365536332130432, eval_accuracy: 0.5220125786163522, eval_runtime: 1.6012, eval_samples_per_second: 99.298, eval_steps_per_second: 24.981, epoch: 2.0[0m
[32m[2022-09-17 18:53:40,577] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-20[0m
[32m[2022-09-17 18:53:40,577] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:53:43,393] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-20/tokenizer_config.json[0m
[32m[2022-09-17 18:53:43,393] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-20/special_tokens_map.json[0m
[32m[2022-09-17 18:53:52,415] [    INFO][0m - loss: 0.8912446, learning_rate: 2.1e-06, global_step: 30, interval_runtime: 13.4436, interval_samples_per_second: 1.19, interval_steps_per_second: 0.744, epoch: 3.0[0m
[32m[2022-09-17 18:53:52,416] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 18:53:52,416] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-17 18:53:52,416] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:53:52,416] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:53:52,416] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-17 18:53:53,971] [    INFO][0m - eval_loss: 0.7232859134674072, eval_accuracy: 0.5220125786163522, eval_runtime: 1.5548, eval_samples_per_second: 102.263, eval_steps_per_second: 25.726, epoch: 3.0[0m
[32m[2022-09-17 18:53:53,972] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-30[0m
[32m[2022-09-17 18:53:53,972] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:53:56,824] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-30/tokenizer_config.json[0m
[32m[2022-09-17 18:53:56,825] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-30/special_tokens_map.json[0m
[32m[2022-09-17 18:54:05,705] [    INFO][0m - loss: 0.77036409, learning_rate: 1.8e-06, global_step: 40, interval_runtime: 13.2898, interval_samples_per_second: 1.204, interval_steps_per_second: 0.752, epoch: 4.0[0m
[32m[2022-09-17 18:54:05,706] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 18:54:05,706] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-17 18:54:05,706] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:54:05,706] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:54:05,706] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-17 18:54:07,245] [    INFO][0m - eval_loss: 0.7158147692680359, eval_accuracy: 0.5157232704402516, eval_runtime: 1.5388, eval_samples_per_second: 103.328, eval_steps_per_second: 25.994, epoch: 4.0[0m
[32m[2022-09-17 18:54:07,245] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-40[0m
[32m[2022-09-17 18:54:07,246] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:54:10,024] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-40/tokenizer_config.json[0m
[32m[2022-09-17 18:54:10,024] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-40/special_tokens_map.json[0m
[32m[2022-09-17 18:54:24,326] [    INFO][0m - loss: 0.71440892, learning_rate: 1.5e-06, global_step: 50, interval_runtime: 18.6212, interval_samples_per_second: 0.859, interval_steps_per_second: 0.537, epoch: 5.0[0m
[32m[2022-09-17 18:54:24,327] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 18:54:24,327] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-17 18:54:24,327] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:54:24,327] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:54:24,327] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-17 18:54:25,847] [    INFO][0m - eval_loss: 0.704332172870636, eval_accuracy: 0.5723270440251572, eval_runtime: 1.52, eval_samples_per_second: 104.602, eval_steps_per_second: 26.315, epoch: 5.0[0m
[32m[2022-09-17 18:54:25,848] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-50[0m
[32m[2022-09-17 18:54:25,848] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:54:28,520] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-50/tokenizer_config.json[0m
[32m[2022-09-17 18:54:28,520] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-50/special_tokens_map.json[0m
[32m[2022-09-17 18:54:37,325] [    INFO][0m - loss: 0.70275197, learning_rate: 1.2000000000000002e-06, global_step: 60, interval_runtime: 12.9989, interval_samples_per_second: 1.231, interval_steps_per_second: 0.769, epoch: 6.0[0m
[32m[2022-09-17 18:54:37,325] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 18:54:37,325] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-17 18:54:37,325] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:54:37,326] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:54:37,326] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-17 18:54:38,850] [    INFO][0m - eval_loss: 0.7190890312194824, eval_accuracy: 0.5283018867924528, eval_runtime: 1.5244, eval_samples_per_second: 104.301, eval_steps_per_second: 26.239, epoch: 6.0[0m
[32m[2022-09-17 18:54:38,851] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-60[0m
[32m[2022-09-17 18:54:38,851] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:54:41,633] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-60/tokenizer_config.json[0m
[32m[2022-09-17 18:54:41,633] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-60/special_tokens_map.json[0m
[32m[2022-09-17 18:54:50,340] [    INFO][0m - loss: 0.68510737, learning_rate: 9e-07, global_step: 70, interval_runtime: 13.0149, interval_samples_per_second: 1.229, interval_steps_per_second: 0.768, epoch: 7.0[0m
[32m[2022-09-17 18:54:50,340] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 18:54:50,341] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-17 18:54:50,341] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:54:50,341] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:54:50,341] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-17 18:54:51,875] [    INFO][0m - eval_loss: 0.685466468334198, eval_accuracy: 0.5534591194968553, eval_runtime: 1.5334, eval_samples_per_second: 103.69, eval_steps_per_second: 26.085, epoch: 7.0[0m
[32m[2022-09-17 18:54:51,875] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-70[0m
[32m[2022-09-17 18:54:51,875] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:54:54,556] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-70/tokenizer_config.json[0m
[32m[2022-09-17 18:54:54,556] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-70/special_tokens_map.json[0m
[32m[2022-09-17 18:55:07,214] [    INFO][0m - loss: 0.62859821, learning_rate: 6.000000000000001e-07, global_step: 80, interval_runtime: 16.8743, interval_samples_per_second: 0.948, interval_steps_per_second: 0.593, epoch: 8.0[0m
[32m[2022-09-17 18:55:07,214] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 18:55:07,215] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-17 18:55:07,215] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:55:07,215] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:55:07,215] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-17 18:55:08,726] [    INFO][0m - eval_loss: 0.6929795145988464, eval_accuracy: 0.5408805031446541, eval_runtime: 1.511, eval_samples_per_second: 105.231, eval_steps_per_second: 26.473, epoch: 8.0[0m
[32m[2022-09-17 18:55:08,726] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-80[0m
[32m[2022-09-17 18:55:08,727] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:55:11,353] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-80/tokenizer_config.json[0m
[32m[2022-09-17 18:55:11,353] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-80/special_tokens_map.json[0m
[32m[2022-09-17 18:55:19,844] [    INFO][0m - loss: 0.61872239, learning_rate: 3.0000000000000004e-07, global_step: 90, interval_runtime: 12.6303, interval_samples_per_second: 1.267, interval_steps_per_second: 0.792, epoch: 9.0[0m
[32m[2022-09-17 18:55:19,845] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 18:55:19,845] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-17 18:55:19,845] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:55:19,845] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:55:19,845] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-17 18:55:21,381] [    INFO][0m - eval_loss: 0.7048467397689819, eval_accuracy: 0.5660377358490566, eval_runtime: 1.5355, eval_samples_per_second: 103.549, eval_steps_per_second: 26.05, epoch: 9.0[0m
[32m[2022-09-17 18:55:23,037] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-90[0m
[32m[2022-09-17 18:55:23,037] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:55:26,004] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-90/tokenizer_config.json[0m
[32m[2022-09-17 18:55:26,005] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-90/special_tokens_map.json[0m
[32m[2022-09-17 18:55:34,552] [    INFO][0m - loss: 0.61989717, learning_rate: 0.0, global_step: 100, interval_runtime: 14.7078, interval_samples_per_second: 1.088, interval_steps_per_second: 0.68, epoch: 10.0[0m
[32m[2022-09-17 18:55:34,553] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-17 18:55:34,553] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-17 18:55:34,553] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:55:34,553] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:55:34,553] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-17 18:55:36,065] [    INFO][0m - eval_loss: 0.7017838358879089, eval_accuracy: 0.5660377358490566, eval_runtime: 1.5114, eval_samples_per_second: 105.203, eval_steps_per_second: 26.466, epoch: 10.0[0m
[32m[2022-09-17 18:55:36,065] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-100[0m
[32m[2022-09-17 18:55:36,065] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:55:38,596] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-17 18:55:38,597] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-17 18:55:43,517] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-17 18:55:43,518] [    INFO][0m - Loading best model from ./checkpoints_cluewsc/checkpoint-50 (score: 0.5723270440251572).[0m
[32m[2022-09-17 18:55:45,051] [    INFO][0m - train_runtime: 145.0744, train_samples_per_second: 11.029, train_steps_per_second: 0.689, train_loss: 1.2100070762634276, epoch: 10.0[0m
[32m[2022-09-17 18:55:45,108] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/[0m
[32m[2022-09-17 18:55:45,108] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-17 18:55:52,796] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/tokenizer_config.json[0m
[32m[2022-09-17 18:55:52,796] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/special_tokens_map.json[0m
[32m[2022-09-17 18:55:52,797] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-17 18:55:52,798] [    INFO][0m -   epoch                    =       10.0[0m
[32m[2022-09-17 18:55:52,798] [    INFO][0m -   train_loss               =       1.21[0m
[32m[2022-09-17 18:55:52,798] [    INFO][0m -   train_runtime            = 0:02:25.07[0m
[32m[2022-09-17 18:55:52,798] [    INFO][0m -   train_samples_per_second =     11.029[0m
[32m[2022-09-17 18:55:52,798] [    INFO][0m -   train_steps_per_second   =      0.689[0m
[32m[2022-09-17 18:55:52,799] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-17 18:55:52,800] [    INFO][0m -   Num examples = 976[0m
[32m[2022-09-17 18:55:52,800] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:55:52,800] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:55:52,800] [    INFO][0m -   Total prediction steps = 244[0m
[32m[2022-09-17 18:56:02,455] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-17 18:56:02,455] [    INFO][0m -   test_accuracy           =     0.5748[0m
[32m[2022-09-17 18:56:02,456] [    INFO][0m -   test_loss               =     0.6929[0m
[32m[2022-09-17 18:56:02,456] [    INFO][0m -   test_runtime            = 0:00:09.65[0m
[32m[2022-09-17 18:56:02,456] [    INFO][0m -   test_samples_per_second =    101.083[0m
[32m[2022-09-17 18:56:02,456] [    INFO][0m -   test_steps_per_second   =     25.271[0m
[32m[2022-09-17 18:56:02,456] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-17 18:56:02,456] [    INFO][0m -   Num examples = 290[0m
[32m[2022-09-17 18:56:02,456] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-17 18:56:02,456] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-17 18:56:02,456] [    INFO][0m -   Total prediction steps = 73[0m
[32m[2022-09-17 18:56:08,716] [    INFO][0m - Predictions for cluewscf saved to ./fewclue_submit_examples.[0m
{
  "labels": 0,
  "text_a": "\u4e3a\u4ec0\u4e48\u8981\u51fa\u73b0\u4e00\u4e2a\u8eab\u7a7f\u519b\u88c5\u7684\u9ad8\u5927\u7537\u4eba\uff1f\u5c31\u50cf\u4e00\u7247\u6811\u53f6\u98d8\u5165\u4e86\u6811\u6797\uff0c\u4ed6\u8d70\u5230\u4e86\u6211\u7684\u5bb6\u4eba\u4e2d\u95f4\u3002",
  "text_b": "\u5176\u4e2d\u4ed6\u6307\u7684\u662f\u6811\u53f6"
}

Prediction done.
