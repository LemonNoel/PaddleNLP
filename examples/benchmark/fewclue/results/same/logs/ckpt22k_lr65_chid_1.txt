[32m[2022-09-20 16:51:54,123] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-20 16:51:54,123] [    INFO][0m - ============================================================[0m
[32m[2022-09-20 16:51:54,123] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-20 16:51:54,124] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-20 16:51:54,124] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-20 16:51:54,124] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-20 16:51:54,124] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-20 16:51:54,124] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-20 16:51:54,124] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-20 16:51:54,124] [    INFO][0m - [0m
[32m[2022-09-20 16:51:54,124] [    INFO][0m - ============================================================[0m
[32m[2022-09-20 16:51:54,124] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-20 16:51:54,124] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-20 16:51:54,124] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-20 16:51:54,124] [    INFO][0m - pretrained                    :../checkpoints_cmnli/checkpoint-22000/model_state.pdparams[0m
[32m[2022-09-20 16:51:54,125] [    INFO][0m - prompt                        :‚Äú{'text':'text_a'}‚ÄùËøôÂè•ËØù‰∏≠ÊàêËØ≠[{'text':'text_b'}]ÁöÑÁêÜËß£Ê≠£Á°ÆÂêóÔºü{'mask'}{'mask'}„ÄÇ[0m
[32m[2022-09-20 16:51:54,125] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-20 16:51:54,125] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-20 16:51:54,125] [    INFO][0m - task_name                     :chid[0m
[32m[2022-09-20 16:51:54,125] [    INFO][0m - [0m
[32m[2022-09-20 16:51:54,125] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0920 16:51:54.126648 17519 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0920 16:51:54.131510 17519 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-20 16:52:02,632] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-20 16:52:02,644] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-20 16:52:02,644] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-20 16:52:02,645] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùËøôÂè•ËØù‰∏≠ÊàêËØ≠['}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': ']ÁöÑÁêÜËß£Ê≠£Á°ÆÂêóÔºü'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': '„ÄÇ'}][0m
[32m[2022-09-20 16:52:04,780] [    INFO][0m - ============================================================[0m
[32m[2022-09-20 16:52:04,780] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-20 16:52:04,780] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-20 16:52:04,780] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-20 16:52:04,780] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-20 16:52:04,780] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-20 16:52:04,780] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-20 16:52:04,781] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-20 16:52:04,781] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-20 16:52:04,781] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-20 16:52:04,781] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-20 16:52:04,781] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-20 16:52:04,781] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-20 16:52:04,781] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-20 16:52:04,781] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-20 16:52:04,781] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-20 16:52:04,781] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-20 16:52:04,781] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-20 16:52:04,781] [    INFO][0m - eval_batch_size               :16[0m
[32m[2022-09-20 16:52:04,782] [    INFO][0m - eval_steps                    :200[0m
[32m[2022-09-20 16:52:04,782] [    INFO][0m - evaluation_strategy           :IntervalStrategy.EPOCH[0m
[32m[2022-09-20 16:52:04,782] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-20 16:52:04,782] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-20 16:52:04,782] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-20 16:52:04,782] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-20 16:52:04,782] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-20 16:52:04,782] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-20 16:52:04,782] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-20 16:52:04,782] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-20 16:52:04,782] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-20 16:52:04,782] [    INFO][0m - learning_rate                 :3e-06[0m
[32m[2022-09-20 16:52:04,782] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-20 16:52:04,783] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-20 16:52:04,783] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-20 16:52:04,783] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-20 16:52:04,783] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-20 16:52:04,783] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-20 16:52:04,783] [    INFO][0m - logging_dir                   :./checkpoints_chid/runs/Sep20_16-51-54_instance-3bwob41y-01[0m
[32m[2022-09-20 16:52:04,783] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-20 16:52:04,783] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-20 16:52:04,783] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-20 16:52:04,783] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-20 16:52:04,783] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-20 16:52:04,783] [    INFO][0m - max_seq_length                :256[0m
[32m[2022-09-20 16:52:04,783] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-20 16:52:04,783] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-20 16:52:04,784] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-20 16:52:04,784] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-20 16:52:04,784] [    INFO][0m - num_train_epochs              :20.0[0m
[32m[2022-09-20 16:52:04,784] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-20 16:52:04,784] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-20 16:52:04,784] [    INFO][0m - output_dir                    :./checkpoints_chid/[0m
[32m[2022-09-20 16:52:04,784] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-20 16:52:04,784] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-20 16:52:04,784] [    INFO][0m - per_device_eval_batch_size    :16[0m
[32m[2022-09-20 16:52:04,784] [    INFO][0m - per_device_train_batch_size   :16[0m
[32m[2022-09-20 16:52:04,784] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-20 16:52:04,784] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-20 16:52:04,784] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-20 16:52:04,785] [    INFO][0m - ppt_learning_rate             :3e-05[0m
[32m[2022-09-20 16:52:04,785] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-20 16:52:04,785] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-20 16:52:04,785] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-20 16:52:04,785] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-20 16:52:04,785] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-20 16:52:04,785] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-20 16:52:04,785] [    INFO][0m - run_name                      :./checkpoints_chid/[0m
[32m[2022-09-20 16:52:04,785] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-20 16:52:04,785] [    INFO][0m - save_steps                    :200[0m
[32m[2022-09-20 16:52:04,785] [    INFO][0m - save_strategy                 :IntervalStrategy.EPOCH[0m
[32m[2022-09-20 16:52:04,785] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-20 16:52:04,785] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-20 16:52:04,786] [    INFO][0m - seed                          :42[0m
[32m[2022-09-20 16:52:04,786] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-20 16:52:04,786] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-20 16:52:04,786] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-20 16:52:04,786] [    INFO][0m - train_batch_size              :16[0m
[32m[2022-09-20 16:52:04,786] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-20 16:52:04,786] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-20 16:52:04,786] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-20 16:52:04,786] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-20 16:52:04,786] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-20 16:52:04,786] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-20 16:52:04,786] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-20 16:52:04,786] [    INFO][0m - [0m
[32m[2022-09-20 16:52:04,789] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-20 16:52:04,789] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-20 16:52:04,789] [    INFO][0m -   Num Epochs = 20[0m
[32m[2022-09-20 16:52:04,789] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2022-09-20 16:52:04,789] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2022-09-20 16:52:04,790] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-20 16:52:04,790] [    INFO][0m -   Total optimization steps = 1780.0[0m
[32m[2022-09-20 16:52:04,790] [    INFO][0m -   Total num train samples = 28280[0m
[32m[2022-09-20 16:52:12,273] [    INFO][0m - loss: 6.61069183, learning_rate: 2.9831460674157303e-06, global_step: 10, interval_runtime: 7.4821, interval_samples_per_second: 2.138, interval_steps_per_second: 1.337, epoch: 0.1124[0m
[32m[2022-09-20 16:52:18,641] [    INFO][0m - loss: 1.39083719, learning_rate: 2.9662921348314606e-06, global_step: 20, interval_runtime: 6.3675, interval_samples_per_second: 2.513, interval_steps_per_second: 1.57, epoch: 0.2247[0m
[32m[2022-09-20 16:52:25,016] [    INFO][0m - loss: 0.34996216, learning_rate: 2.949438202247191e-06, global_step: 30, interval_runtime: 6.3757, interval_samples_per_second: 2.51, interval_steps_per_second: 1.568, epoch: 0.3371[0m
[32m[2022-09-20 16:52:31,377] [    INFO][0m - loss: 0.58981624, learning_rate: 2.932584269662921e-06, global_step: 40, interval_runtime: 6.3605, interval_samples_per_second: 2.516, interval_steps_per_second: 1.572, epoch: 0.4494[0m
[32m[2022-09-20 16:52:37,749] [    INFO][0m - loss: 0.46383348, learning_rate: 2.915730337078652e-06, global_step: 50, interval_runtime: 6.3716, interval_samples_per_second: 2.511, interval_steps_per_second: 1.569, epoch: 0.5618[0m
[32m[2022-09-20 16:52:44,218] [    INFO][0m - loss: 0.43055129, learning_rate: 2.898876404494382e-06, global_step: 60, interval_runtime: 6.4699, interval_samples_per_second: 2.473, interval_steps_per_second: 1.546, epoch: 0.6742[0m
[32m[2022-09-20 16:52:50,592] [    INFO][0m - loss: 0.4246161, learning_rate: 2.8820224719101123e-06, global_step: 70, interval_runtime: 6.3742, interval_samples_per_second: 2.51, interval_steps_per_second: 1.569, epoch: 0.7865[0m
[32m[2022-09-20 16:52:56,974] [    INFO][0m - loss: 0.50360398, learning_rate: 2.8651685393258426e-06, global_step: 80, interval_runtime: 6.3817, interval_samples_per_second: 2.507, interval_steps_per_second: 1.567, epoch: 0.8989[0m
[32m[2022-09-20 16:53:02,256] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 16:53:02,257] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-20 16:53:02,257] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 16:53:02,257] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 16:53:02,257] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-20 16:53:24,546] [    INFO][0m - eval_loss: 0.42085328698158264, eval_accuracy: 0.13366336633663367, eval_runtime: 22.2892, eval_samples_per_second: 63.439, eval_steps_per_second: 3.993, epoch: 1.0[0m
[32m[2022-09-20 16:53:24,570] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-89[0m
[32m[2022-09-20 16:53:24,570] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 16:53:27,316] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-89/tokenizer_config.json[0m
[32m[2022-09-20 16:53:27,316] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-89/special_tokens_map.json[0m
[32m[2022-09-20 16:53:33,609] [    INFO][0m - loss: 0.44557614, learning_rate: 2.8483146067415733e-06, global_step: 90, interval_runtime: 36.6352, interval_samples_per_second: 0.437, interval_steps_per_second: 0.273, epoch: 1.0112[0m
[32m[2022-09-20 16:53:39,961] [    INFO][0m - loss: 0.49757724, learning_rate: 2.8314606741573035e-06, global_step: 100, interval_runtime: 6.3522, interval_samples_per_second: 2.519, interval_steps_per_second: 1.574, epoch: 1.1236[0m
[32m[2022-09-20 16:53:46,333] [    INFO][0m - loss: 0.45172691, learning_rate: 2.814606741573034e-06, global_step: 110, interval_runtime: 6.3722, interval_samples_per_second: 2.511, interval_steps_per_second: 1.569, epoch: 1.236[0m
[32m[2022-09-20 16:53:52,707] [    INFO][0m - loss: 0.47218399, learning_rate: 2.797752808988764e-06, global_step: 120, interval_runtime: 6.3736, interval_samples_per_second: 2.51, interval_steps_per_second: 1.569, epoch: 1.3483[0m
[32m[2022-09-20 16:53:59,082] [    INFO][0m - loss: 0.41710315, learning_rate: 2.7808988764044947e-06, global_step: 130, interval_runtime: 6.3752, interval_samples_per_second: 2.51, interval_steps_per_second: 1.569, epoch: 1.4607[0m
[32m[2022-09-20 16:54:05,495] [    INFO][0m - loss: 0.43525648, learning_rate: 2.764044943820225e-06, global_step: 140, interval_runtime: 6.4129, interval_samples_per_second: 2.495, interval_steps_per_second: 1.559, epoch: 1.573[0m
[32m[2022-09-20 16:54:12,613] [    INFO][0m - loss: 0.45173283, learning_rate: 2.7471910112359553e-06, global_step: 150, interval_runtime: 7.1181, interval_samples_per_second: 2.248, interval_steps_per_second: 1.405, epoch: 1.6854[0m
[32m[2022-09-20 16:54:18,993] [    INFO][0m - loss: 0.36136706, learning_rate: 2.7303370786516855e-06, global_step: 160, interval_runtime: 6.3794, interval_samples_per_second: 2.508, interval_steps_per_second: 1.568, epoch: 1.7978[0m
[32m[2022-09-20 16:54:25,393] [    INFO][0m - loss: 0.44657497, learning_rate: 2.7134831460674158e-06, global_step: 170, interval_runtime: 6.3999, interval_samples_per_second: 2.5, interval_steps_per_second: 1.563, epoch: 1.9101[0m
[32m[2022-09-20 16:54:30,041] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 16:54:30,041] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-20 16:54:30,041] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 16:54:30,041] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 16:54:30,041] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-20 16:54:52,321] [    INFO][0m - eval_loss: 0.42933329939842224, eval_accuracy: 0.21287128712871287, eval_runtime: 22.28, eval_samples_per_second: 63.465, eval_steps_per_second: 3.995, epoch: 2.0[0m
[32m[2022-09-20 16:54:52,340] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-178[0m
[32m[2022-09-20 16:54:52,340] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 16:54:54,936] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-178/tokenizer_config.json[0m
[32m[2022-09-20 16:54:54,937] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-178/special_tokens_map.json[0m
[32m[2022-09-20 16:55:01,215] [    INFO][0m - loss: 0.38519788, learning_rate: 2.696629213483146e-06, global_step: 180, interval_runtime: 35.8224, interval_samples_per_second: 0.447, interval_steps_per_second: 0.279, epoch: 2.0225[0m
[32m[2022-09-20 16:55:12,243] [    INFO][0m - loss: 0.44577179, learning_rate: 2.6797752808988763e-06, global_step: 190, interval_runtime: 6.382, interval_samples_per_second: 2.507, interval_steps_per_second: 1.567, epoch: 2.1348[0m
[32m[2022-09-20 16:55:18,765] [    INFO][0m - loss: 0.44001451, learning_rate: 2.6629213483146066e-06, global_step: 200, interval_runtime: 11.1681, interval_samples_per_second: 1.433, interval_steps_per_second: 0.895, epoch: 2.2472[0m
[32m[2022-09-20 16:55:25,147] [    INFO][0m - loss: 0.44179764, learning_rate: 2.6460674157303372e-06, global_step: 210, interval_runtime: 6.3818, interval_samples_per_second: 2.507, interval_steps_per_second: 1.567, epoch: 2.3596[0m
[32m[2022-09-20 16:55:31,539] [    INFO][0m - loss: 0.47570457, learning_rate: 2.6292134831460675e-06, global_step: 220, interval_runtime: 6.3916, interval_samples_per_second: 2.503, interval_steps_per_second: 1.565, epoch: 2.4719[0m
[32m[2022-09-20 16:55:37,927] [    INFO][0m - loss: 0.49496775, learning_rate: 2.6123595505617978e-06, global_step: 230, interval_runtime: 6.3886, interval_samples_per_second: 2.504, interval_steps_per_second: 1.565, epoch: 2.5843[0m
[32m[2022-09-20 16:55:44,318] [    INFO][0m - loss: 0.47433429, learning_rate: 2.595505617977528e-06, global_step: 240, interval_runtime: 6.3907, interval_samples_per_second: 2.504, interval_steps_per_second: 1.565, epoch: 2.6966[0m
[32m[2022-09-20 16:55:50,707] [    INFO][0m - loss: 0.395331, learning_rate: 2.5786516853932583e-06, global_step: 250, interval_runtime: 6.3895, interval_samples_per_second: 2.504, interval_steps_per_second: 1.565, epoch: 2.809[0m
[32m[2022-09-20 16:55:57,103] [    INFO][0m - loss: 0.37468448, learning_rate: 2.561797752808989e-06, global_step: 260, interval_runtime: 6.3959, interval_samples_per_second: 2.502, interval_steps_per_second: 1.564, epoch: 2.9213[0m
[32m[2022-09-20 16:56:01,116] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 16:56:01,116] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-20 16:56:01,116] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 16:56:01,116] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 16:56:01,116] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-20 16:56:23,321] [    INFO][0m - eval_loss: 0.41161590814590454, eval_accuracy: 0.3217821782178218, eval_runtime: 22.2047, eval_samples_per_second: 63.68, eval_steps_per_second: 4.008, epoch: 3.0[0m
[32m[2022-09-20 16:56:23,340] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-267[0m
[32m[2022-09-20 16:56:23,340] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 16:56:25,932] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-267/tokenizer_config.json[0m
[32m[2022-09-20 16:56:25,932] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-267/special_tokens_map.json[0m
[32m[2022-09-20 16:56:32,825] [    INFO][0m - loss: 0.34367499, learning_rate: 2.5449438202247192e-06, global_step: 270, interval_runtime: 35.7223, interval_samples_per_second: 0.448, interval_steps_per_second: 0.28, epoch: 3.0337[0m
[32m[2022-09-20 16:56:39,205] [    INFO][0m - loss: 0.42459464, learning_rate: 2.5280898876404495e-06, global_step: 280, interval_runtime: 6.3798, interval_samples_per_second: 2.508, interval_steps_per_second: 1.567, epoch: 3.1461[0m
[32m[2022-09-20 16:56:45,599] [    INFO][0m - loss: 0.40860863, learning_rate: 2.51123595505618e-06, global_step: 290, interval_runtime: 6.3935, interval_samples_per_second: 2.503, interval_steps_per_second: 1.564, epoch: 3.2584[0m
[32m[2022-09-20 16:56:51,983] [    INFO][0m - loss: 0.4430716, learning_rate: 2.4943820224719104e-06, global_step: 300, interval_runtime: 6.3844, interval_samples_per_second: 2.506, interval_steps_per_second: 1.566, epoch: 3.3708[0m
[32m[2022-09-20 16:56:58,385] [    INFO][0m - loss: 0.43887706, learning_rate: 2.4775280898876407e-06, global_step: 310, interval_runtime: 6.402, interval_samples_per_second: 2.499, interval_steps_per_second: 1.562, epoch: 3.4831[0m
[32m[2022-09-20 16:57:06,715] [    INFO][0m - loss: 0.43266449, learning_rate: 2.460674157303371e-06, global_step: 320, interval_runtime: 6.4015, interval_samples_per_second: 2.499, interval_steps_per_second: 1.562, epoch: 3.5955[0m
[32m[2022-09-20 16:57:13,112] [    INFO][0m - loss: 0.42454715, learning_rate: 2.4438202247191012e-06, global_step: 330, interval_runtime: 8.3251, interval_samples_per_second: 1.922, interval_steps_per_second: 1.201, epoch: 3.7079[0m
[32m[2022-09-20 16:57:19,509] [    INFO][0m - loss: 0.37698669, learning_rate: 2.4269662921348315e-06, global_step: 340, interval_runtime: 6.3965, interval_samples_per_second: 2.501, interval_steps_per_second: 1.563, epoch: 3.8202[0m
[32m[2022-09-20 16:57:25,895] [    INFO][0m - loss: 0.43713212, learning_rate: 2.4101123595505617e-06, global_step: 350, interval_runtime: 6.387, interval_samples_per_second: 2.505, interval_steps_per_second: 1.566, epoch: 3.9326[0m
[32m[2022-09-20 16:57:29,270] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 16:57:29,270] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-20 16:57:29,270] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 16:57:29,270] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 16:57:29,270] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-20 16:57:51,553] [    INFO][0m - eval_loss: 0.43193864822387695, eval_accuracy: 0.3217821782178218, eval_runtime: 22.2821, eval_samples_per_second: 63.459, eval_steps_per_second: 3.994, epoch: 4.0[0m
[32m[2022-09-20 16:57:51,578] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-356[0m
[32m[2022-09-20 16:57:51,578] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 16:57:54,195] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-356/tokenizer_config.json[0m
[32m[2022-09-20 16:57:54,195] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-356/special_tokens_map.json[0m
[32m[2022-09-20 16:58:01,711] [    INFO][0m - loss: 0.46715364, learning_rate: 2.393258426966292e-06, global_step: 360, interval_runtime: 35.8158, interval_samples_per_second: 0.447, interval_steps_per_second: 0.279, epoch: 4.0449[0m
[32m[2022-09-20 16:58:08,401] [    INFO][0m - loss: 0.41215334, learning_rate: 2.3764044943820227e-06, global_step: 370, interval_runtime: 6.3855, interval_samples_per_second: 2.506, interval_steps_per_second: 1.566, epoch: 4.1573[0m
[32m[2022-09-20 16:58:14,774] [    INFO][0m - loss: 0.45201735, learning_rate: 2.359550561797753e-06, global_step: 380, interval_runtime: 6.6776, interval_samples_per_second: 2.396, interval_steps_per_second: 1.498, epoch: 4.2697[0m
[32m[2022-09-20 16:58:21,163] [    INFO][0m - loss: 0.40046792, learning_rate: 2.342696629213483e-06, global_step: 390, interval_runtime: 6.3882, interval_samples_per_second: 2.505, interval_steps_per_second: 1.565, epoch: 4.382[0m
[32m[2022-09-20 16:58:27,546] [    INFO][0m - loss: 0.43816881, learning_rate: 2.3258426966292135e-06, global_step: 400, interval_runtime: 6.3839, interval_samples_per_second: 2.506, interval_steps_per_second: 1.566, epoch: 4.4944[0m
[32m[2022-09-20 16:58:33,963] [    INFO][0m - loss: 0.40049248, learning_rate: 2.3089887640449437e-06, global_step: 410, interval_runtime: 6.4164, interval_samples_per_second: 2.494, interval_steps_per_second: 1.559, epoch: 4.6067[0m
[32m[2022-09-20 16:58:40,360] [    INFO][0m - loss: 0.46484137, learning_rate: 2.292134831460674e-06, global_step: 420, interval_runtime: 6.3975, interval_samples_per_second: 2.501, interval_steps_per_second: 1.563, epoch: 4.7191[0m
[32m[2022-09-20 16:58:46,762] [    INFO][0m - loss: 0.46256766, learning_rate: 2.2752808988764042e-06, global_step: 430, interval_runtime: 6.4018, interval_samples_per_second: 2.499, interval_steps_per_second: 1.562, epoch: 4.8315[0m
[32m[2022-09-20 16:58:53,144] [    INFO][0m - loss: 0.46503201, learning_rate: 2.258426966292135e-06, global_step: 440, interval_runtime: 6.3825, interval_samples_per_second: 2.507, interval_steps_per_second: 1.567, epoch: 4.9438[0m
[32m[2022-09-20 16:58:55,881] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 16:58:55,881] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-20 16:58:55,881] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 16:58:55,881] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 16:58:55,882] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-20 16:59:18,088] [    INFO][0m - eval_loss: 0.4248376190662384, eval_accuracy: 0.3316831683168317, eval_runtime: 22.2058, eval_samples_per_second: 63.677, eval_steps_per_second: 4.008, epoch: 5.0[0m
[32m[2022-09-20 16:59:18,110] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-445[0m
[32m[2022-09-20 16:59:18,111] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 16:59:20,719] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-445/tokenizer_config.json[0m
[32m[2022-09-20 16:59:20,720] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-445/special_tokens_map.json[0m
[32m[2022-09-20 16:59:28,833] [    INFO][0m - loss: 0.32372434, learning_rate: 2.241573033707865e-06, global_step: 450, interval_runtime: 35.6883, interval_samples_per_second: 0.448, interval_steps_per_second: 0.28, epoch: 5.0562[0m
[32m[2022-09-20 16:59:35,202] [    INFO][0m - loss: 0.54342198, learning_rate: 2.224719101123596e-06, global_step: 460, interval_runtime: 6.3695, interval_samples_per_second: 2.512, interval_steps_per_second: 1.57, epoch: 5.1685[0m
[32m[2022-09-20 16:59:41,584] [    INFO][0m - loss: 0.45746756, learning_rate: 2.207865168539326e-06, global_step: 470, interval_runtime: 6.3812, interval_samples_per_second: 2.507, interval_steps_per_second: 1.567, epoch: 5.2809[0m
[32m[2022-09-20 16:59:47,977] [    INFO][0m - loss: 0.41367245, learning_rate: 2.1910112359550564e-06, global_step: 480, interval_runtime: 6.394, interval_samples_per_second: 2.502, interval_steps_per_second: 1.564, epoch: 5.3933[0m
[32m[2022-09-20 16:59:54,365] [    INFO][0m - loss: 0.4866858, learning_rate: 2.1741573033707867e-06, global_step: 490, interval_runtime: 6.3878, interval_samples_per_second: 2.505, interval_steps_per_second: 1.565, epoch: 5.5056[0m
[32m[2022-09-20 17:00:00,753] [    INFO][0m - loss: 0.39183631, learning_rate: 2.157303370786517e-06, global_step: 500, interval_runtime: 6.3877, interval_samples_per_second: 2.505, interval_steps_per_second: 1.565, epoch: 5.618[0m
[32m[2022-09-20 17:00:07,155] [    INFO][0m - loss: 0.39786041, learning_rate: 2.140449438202247e-06, global_step: 510, interval_runtime: 6.4016, interval_samples_per_second: 2.499, interval_steps_per_second: 1.562, epoch: 5.7303[0m
[32m[2022-09-20 17:00:13,579] [    INFO][0m - loss: 0.40525804, learning_rate: 2.1235955056179774e-06, global_step: 520, interval_runtime: 6.4239, interval_samples_per_second: 2.491, interval_steps_per_second: 1.557, epoch: 5.8427[0m
[32m[2022-09-20 17:00:19,946] [    INFO][0m - loss: 0.3655046, learning_rate: 2.106741573033708e-06, global_step: 530, interval_runtime: 6.3676, interval_samples_per_second: 2.513, interval_steps_per_second: 1.57, epoch: 5.9551[0m
[32m[2022-09-20 17:00:22,070] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 17:00:22,070] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-20 17:00:22,071] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 17:00:22,071] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 17:00:22,071] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-20 17:00:44,286] [    INFO][0m - eval_loss: 0.4121914803981781, eval_accuracy: 0.29207920792079206, eval_runtime: 22.2153, eval_samples_per_second: 63.65, eval_steps_per_second: 4.006, epoch: 6.0[0m
[32m[2022-09-20 17:00:44,310] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-534[0m
[32m[2022-09-20 17:00:44,310] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 17:00:46,848] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-534/tokenizer_config.json[0m
[32m[2022-09-20 17:00:46,849] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-534/special_tokens_map.json[0m
[32m[2022-09-20 17:00:55,626] [    INFO][0m - loss: 0.48596716, learning_rate: 2.0898876404494384e-06, global_step: 540, interval_runtime: 35.6793, interval_samples_per_second: 0.448, interval_steps_per_second: 0.28, epoch: 6.0674[0m
[32m[2022-09-20 17:01:02,005] [    INFO][0m - loss: 0.44313798, learning_rate: 2.0730337078651686e-06, global_step: 550, interval_runtime: 6.3799, interval_samples_per_second: 2.508, interval_steps_per_second: 1.567, epoch: 6.1798[0m
[32m[2022-09-20 17:01:08,398] [    INFO][0m - loss: 0.41965914, learning_rate: 2.056179775280899e-06, global_step: 560, interval_runtime: 6.3928, interval_samples_per_second: 2.503, interval_steps_per_second: 1.564, epoch: 6.2921[0m
[32m[2022-09-20 17:01:14,794] [    INFO][0m - loss: 0.50917783, learning_rate: 2.039325842696629e-06, global_step: 570, interval_runtime: 6.3956, interval_samples_per_second: 2.502, interval_steps_per_second: 1.564, epoch: 6.4045[0m
[32m[2022-09-20 17:01:21,199] [    INFO][0m - loss: 0.33145707, learning_rate: 2.0224719101123594e-06, global_step: 580, interval_runtime: 6.4051, interval_samples_per_second: 2.498, interval_steps_per_second: 1.561, epoch: 6.5169[0m
[32m[2022-09-20 17:01:27,721] [    INFO][0m - loss: 0.38401322, learning_rate: 2.0056179775280897e-06, global_step: 590, interval_runtime: 6.5222, interval_samples_per_second: 2.453, interval_steps_per_second: 1.533, epoch: 6.6292[0m
[32m[2022-09-20 17:01:34,117] [    INFO][0m - loss: 0.366081, learning_rate: 1.98876404494382e-06, global_step: 600, interval_runtime: 6.3954, interval_samples_per_second: 2.502, interval_steps_per_second: 1.564, epoch: 6.7416[0m
[32m[2022-09-20 17:01:40,512] [    INFO][0m - loss: 0.48958411, learning_rate: 1.9719101123595506e-06, global_step: 610, interval_runtime: 6.3956, interval_samples_per_second: 2.502, interval_steps_per_second: 1.564, epoch: 6.8539[0m
[32m[2022-09-20 17:01:46,873] [    INFO][0m - loss: 0.37138083, learning_rate: 1.955056179775281e-06, global_step: 620, interval_runtime: 6.3607, interval_samples_per_second: 2.515, interval_steps_per_second: 1.572, epoch: 6.9663[0m
[32m[2022-09-20 17:01:48,367] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 17:01:48,368] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-20 17:01:48,368] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 17:01:48,368] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 17:01:48,368] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-20 17:02:10,506] [    INFO][0m - eval_loss: 0.41215449571609497, eval_accuracy: 0.33663366336633666, eval_runtime: 22.1375, eval_samples_per_second: 63.874, eval_steps_per_second: 4.02, epoch: 7.0[0m
[32m[2022-09-20 17:02:10,520] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-623[0m
[32m[2022-09-20 17:02:10,520] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 17:02:12,980] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-623/tokenizer_config.json[0m
[32m[2022-09-20 17:02:12,980] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-623/special_tokens_map.json[0m
[32m[2022-09-20 17:02:22,283] [    INFO][0m - loss: 0.43806939, learning_rate: 1.938202247191011e-06, global_step: 630, interval_runtime: 35.4097, interval_samples_per_second: 0.452, interval_steps_per_second: 0.282, epoch: 7.0787[0m
[32m[2022-09-20 17:02:28,671] [    INFO][0m - loss: 0.49295912, learning_rate: 1.921348314606742e-06, global_step: 640, interval_runtime: 6.388, interval_samples_per_second: 2.505, interval_steps_per_second: 1.565, epoch: 7.191[0m
[32m[2022-09-20 17:02:35,048] [    INFO][0m - loss: 0.40963268, learning_rate: 1.9044943820224719e-06, global_step: 650, interval_runtime: 6.3776, interval_samples_per_second: 2.509, interval_steps_per_second: 1.568, epoch: 7.3034[0m
[32m[2022-09-20 17:02:41,447] [    INFO][0m - loss: 0.38691149, learning_rate: 1.8876404494382021e-06, global_step: 660, interval_runtime: 6.3982, interval_samples_per_second: 2.501, interval_steps_per_second: 1.563, epoch: 7.4157[0m
[32m[2022-09-20 17:02:47,849] [    INFO][0m - loss: 0.48814082, learning_rate: 1.8707865168539326e-06, global_step: 670, interval_runtime: 6.4021, interval_samples_per_second: 2.499, interval_steps_per_second: 1.562, epoch: 7.5281[0m
[32m[2022-09-20 17:02:54,254] [    INFO][0m - loss: 0.4186893, learning_rate: 1.8539325842696629e-06, global_step: 680, interval_runtime: 6.4057, interval_samples_per_second: 2.498, interval_steps_per_second: 1.561, epoch: 7.6404[0m
[32m[2022-09-20 17:03:00,649] [    INFO][0m - loss: 0.35313277, learning_rate: 1.8370786516853936e-06, global_step: 690, interval_runtime: 6.3947, interval_samples_per_second: 2.502, interval_steps_per_second: 1.564, epoch: 7.7528[0m
[32m[2022-09-20 17:03:07,048] [    INFO][0m - loss: 0.38951068, learning_rate: 1.8202247191011238e-06, global_step: 700, interval_runtime: 6.3988, interval_samples_per_second: 2.5, interval_steps_per_second: 1.563, epoch: 7.8652[0m
[32m[2022-09-20 17:03:13,379] [    INFO][0m - loss: 0.43013058, learning_rate: 1.803370786516854e-06, global_step: 710, interval_runtime: 6.3315, interval_samples_per_second: 2.527, interval_steps_per_second: 1.579, epoch: 7.9775[0m
[32m[2022-09-20 17:03:14,255] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 17:03:14,256] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-20 17:03:14,256] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 17:03:14,256] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 17:03:14,256] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-20 17:03:36,356] [    INFO][0m - eval_loss: 0.4104479253292084, eval_accuracy: 0.3613861386138614, eval_runtime: 22.0998, eval_samples_per_second: 63.982, eval_steps_per_second: 4.027, epoch: 8.0[0m
[32m[2022-09-20 17:03:36,372] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-712[0m
[32m[2022-09-20 17:03:36,373] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 17:03:38,858] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-712/tokenizer_config.json[0m
[32m[2022-09-20 17:03:38,858] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-712/special_tokens_map.json[0m
[32m[2022-09-20 17:03:48,760] [    INFO][0m - loss: 0.45881348, learning_rate: 1.7865168539325843e-06, global_step: 720, interval_runtime: 35.3807, interval_samples_per_second: 0.452, interval_steps_per_second: 0.283, epoch: 8.0899[0m
[32m[2022-09-20 17:03:55,148] [    INFO][0m - loss: 0.47745662, learning_rate: 1.7696629213483146e-06, global_step: 730, interval_runtime: 6.3877, interval_samples_per_second: 2.505, interval_steps_per_second: 1.566, epoch: 8.2022[0m
[32m[2022-09-20 17:04:01,536] [    INFO][0m - loss: 0.47904863, learning_rate: 1.7528089887640449e-06, global_step: 740, interval_runtime: 6.3886, interval_samples_per_second: 2.504, interval_steps_per_second: 1.565, epoch: 8.3146[0m
[32m[2022-09-20 17:04:07,934] [    INFO][0m - loss: 0.32159543, learning_rate: 1.7359550561797753e-06, global_step: 750, interval_runtime: 6.3978, interval_samples_per_second: 2.501, interval_steps_per_second: 1.563, epoch: 8.427[0m
[32m[2022-09-20 17:04:14,327] [    INFO][0m - loss: 0.46424303, learning_rate: 1.7191011235955056e-06, global_step: 760, interval_runtime: 6.3929, interval_samples_per_second: 2.503, interval_steps_per_second: 1.564, epoch: 8.5393[0m
[32m[2022-09-20 17:04:20,719] [    INFO][0m - loss: 0.3630336, learning_rate: 1.702247191011236e-06, global_step: 770, interval_runtime: 6.3919, interval_samples_per_second: 2.503, interval_steps_per_second: 1.564, epoch: 8.6517[0m
[32m[2022-09-20 17:04:27,115] [    INFO][0m - loss: 0.41500769, learning_rate: 1.6853932584269665e-06, global_step: 780, interval_runtime: 6.3966, interval_samples_per_second: 2.501, interval_steps_per_second: 1.563, epoch: 8.764[0m
[32m[2022-09-20 17:04:33,513] [    INFO][0m - loss: 0.43718667, learning_rate: 1.6685393258426968e-06, global_step: 790, interval_runtime: 6.397, interval_samples_per_second: 2.501, interval_steps_per_second: 1.563, epoch: 8.8764[0m
[32m[2022-09-20 17:04:39,834] [    INFO][0m - loss: 0.36925416, learning_rate: 1.651685393258427e-06, global_step: 800, interval_runtime: 6.3211, interval_samples_per_second: 2.531, interval_steps_per_second: 1.582, epoch: 8.9888[0m
[32m[2022-09-20 17:04:40,090] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 17:04:40,090] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-20 17:04:40,090] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 17:04:40,091] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 17:04:40,091] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-20 17:05:02,219] [    INFO][0m - eval_loss: 0.4115009307861328, eval_accuracy: 0.3415841584158416, eval_runtime: 22.1286, eval_samples_per_second: 63.899, eval_steps_per_second: 4.022, epoch: 9.0[0m
[32m[2022-09-20 17:05:02,236] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-801[0m
[32m[2022-09-20 17:05:02,236] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 17:05:04,698] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-801/tokenizer_config.json[0m
[32m[2022-09-20 17:05:04,698] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-801/special_tokens_map.json[0m
[32m[2022-09-20 17:05:15,231] [    INFO][0m - loss: 0.41162243, learning_rate: 1.6348314606741573e-06, global_step: 810, interval_runtime: 35.3969, interval_samples_per_second: 0.452, interval_steps_per_second: 0.283, epoch: 9.1011[0m
[32m[2022-09-20 17:05:21,625] [    INFO][0m - loss: 0.43978119, learning_rate: 1.6179775280898876e-06, global_step: 820, interval_runtime: 6.3938, interval_samples_per_second: 2.502, interval_steps_per_second: 1.564, epoch: 9.2135[0m
[32m[2022-09-20 17:05:28,024] [    INFO][0m - loss: 0.45172863, learning_rate: 1.6011235955056178e-06, global_step: 830, interval_runtime: 6.3997, interval_samples_per_second: 2.5, interval_steps_per_second: 1.563, epoch: 9.3258[0m
[32m[2022-09-20 17:05:34,411] [    INFO][0m - loss: 0.36863339, learning_rate: 1.5842696629213483e-06, global_step: 840, interval_runtime: 6.3865, interval_samples_per_second: 2.505, interval_steps_per_second: 1.566, epoch: 9.4382[0m
[32m[2022-09-20 17:05:40,802] [    INFO][0m - loss: 0.42956638, learning_rate: 1.5674157303370788e-06, global_step: 850, interval_runtime: 6.391, interval_samples_per_second: 2.504, interval_steps_per_second: 1.565, epoch: 9.5506[0m
[32m[2022-09-20 17:05:47,200] [    INFO][0m - loss: 0.43589416, learning_rate: 1.5505617977528093e-06, global_step: 860, interval_runtime: 6.3989, interval_samples_per_second: 2.5, interval_steps_per_second: 1.563, epoch: 9.6629[0m
[32m[2022-09-20 17:05:53,597] [    INFO][0m - loss: 0.43336844, learning_rate: 1.5337078651685395e-06, global_step: 870, interval_runtime: 6.3966, interval_samples_per_second: 2.501, interval_steps_per_second: 1.563, epoch: 9.7753[0m
[32m[2022-09-20 17:05:59,994] [    INFO][0m - loss: 0.38882728, learning_rate: 1.5168539325842698e-06, global_step: 880, interval_runtime: 6.3969, interval_samples_per_second: 2.501, interval_steps_per_second: 1.563, epoch: 9.8876[0m
[32m[2022-09-20 17:06:05,926] [    INFO][0m - loss: 0.42240686, learning_rate: 1.5e-06, global_step: 890, interval_runtime: 5.9325, interval_samples_per_second: 2.697, interval_steps_per_second: 1.686, epoch: 10.0[0m
[32m[2022-09-20 17:06:05,927] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 17:06:05,927] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-20 17:06:05,927] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 17:06:05,927] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 17:06:05,928] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-20 17:06:28,028] [    INFO][0m - eval_loss: 0.4136059880256653, eval_accuracy: 0.37623762376237624, eval_runtime: 22.1004, eval_samples_per_second: 63.981, eval_steps_per_second: 4.027, epoch: 10.0[0m
[32m[2022-09-20 17:06:28,044] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-890[0m
[32m[2022-09-20 17:06:28,044] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 17:06:30,537] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-890/tokenizer_config.json[0m
[32m[2022-09-20 17:06:30,537] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-890/special_tokens_map.json[0m
[32m[2022-09-20 17:06:41,761] [    INFO][0m - loss: 0.51006575, learning_rate: 1.4831460674157303e-06, global_step: 900, interval_runtime: 35.8341, interval_samples_per_second: 0.447, interval_steps_per_second: 0.279, epoch: 10.1124[0m
[32m[2022-09-20 17:06:48,155] [    INFO][0m - loss: 0.33370068, learning_rate: 1.4662921348314606e-06, global_step: 910, interval_runtime: 6.3945, interval_samples_per_second: 2.502, interval_steps_per_second: 1.564, epoch: 10.2247[0m
[32m[2022-09-20 17:06:54,548] [    INFO][0m - loss: 0.37565851, learning_rate: 1.449438202247191e-06, global_step: 920, interval_runtime: 6.3927, interval_samples_per_second: 2.503, interval_steps_per_second: 1.564, epoch: 10.3371[0m
[32m[2022-09-20 17:07:00,947] [    INFO][0m - loss: 0.51261973, learning_rate: 1.4325842696629213e-06, global_step: 930, interval_runtime: 6.3992, interval_samples_per_second: 2.5, interval_steps_per_second: 1.563, epoch: 10.4494[0m
[32m[2022-09-20 17:07:07,356] [    INFO][0m - loss: 0.48406048, learning_rate: 1.4157303370786518e-06, global_step: 940, interval_runtime: 6.4087, interval_samples_per_second: 2.497, interval_steps_per_second: 1.56, epoch: 10.5618[0m
[32m[2022-09-20 17:07:13,754] [    INFO][0m - loss: 0.47208452, learning_rate: 1.398876404494382e-06, global_step: 950, interval_runtime: 6.3986, interval_samples_per_second: 2.501, interval_steps_per_second: 1.563, epoch: 10.6742[0m
[32m[2022-09-20 17:07:20,173] [    INFO][0m - loss: 0.35279026, learning_rate: 1.3820224719101125e-06, global_step: 960, interval_runtime: 6.4187, interval_samples_per_second: 2.493, interval_steps_per_second: 1.558, epoch: 10.7865[0m
[32m[2022-09-20 17:07:26,567] [    INFO][0m - loss: 0.4011219, learning_rate: 1.3651685393258428e-06, global_step: 970, interval_runtime: 6.3939, interval_samples_per_second: 2.502, interval_steps_per_second: 1.564, epoch: 10.8989[0m
[32m[2022-09-20 17:07:31,859] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 17:07:31,860] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-20 17:07:31,860] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 17:07:31,860] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 17:07:31,860] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-20 17:07:54,003] [    INFO][0m - eval_loss: 0.43363359570503235, eval_accuracy: 0.3910891089108911, eval_runtime: 22.1424, eval_samples_per_second: 63.859, eval_steps_per_second: 4.019, epoch: 11.0[0m
[32m[2022-09-20 17:07:54,017] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-979[0m
[32m[2022-09-20 17:07:54,017] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 17:07:56,480] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-979/tokenizer_config.json[0m
[32m[2022-09-20 17:07:56,480] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-979/special_tokens_map.json[0m
[32m[2022-09-20 17:08:01,976] [    INFO][0m - loss: 0.36065631, learning_rate: 1.348314606741573e-06, global_step: 980, interval_runtime: 35.409, interval_samples_per_second: 0.452, interval_steps_per_second: 0.282, epoch: 11.0112[0m
[32m[2022-09-20 17:08:12,163] [    INFO][0m - loss: 0.44870491, learning_rate: 1.3314606741573033e-06, global_step: 990, interval_runtime: 6.3701, interval_samples_per_second: 2.512, interval_steps_per_second: 1.57, epoch: 11.1236[0m
[32m[2022-09-20 17:08:18,537] [    INFO][0m - loss: 0.34285486, learning_rate: 1.3146067415730338e-06, global_step: 1000, interval_runtime: 10.1913, interval_samples_per_second: 1.57, interval_steps_per_second: 0.981, epoch: 11.236[0m
[32m[2022-09-20 17:08:24,915] [    INFO][0m - loss: 0.49960327, learning_rate: 1.297752808988764e-06, global_step: 1010, interval_runtime: 6.3777, interval_samples_per_second: 2.509, interval_steps_per_second: 1.568, epoch: 11.3483[0m
[32m[2022-09-20 17:08:31,293] [    INFO][0m - loss: 0.40348096, learning_rate: 1.2808988764044945e-06, global_step: 1020, interval_runtime: 6.3785, interval_samples_per_second: 2.508, interval_steps_per_second: 1.568, epoch: 11.4607[0m
[32m[2022-09-20 17:08:37,686] [    INFO][0m - loss: 0.38915033, learning_rate: 1.2640449438202247e-06, global_step: 1030, interval_runtime: 6.393, interval_samples_per_second: 2.503, interval_steps_per_second: 1.564, epoch: 11.573[0m
[32m[2022-09-20 17:08:44,075] [    INFO][0m - loss: 0.44693017, learning_rate: 1.2471910112359552e-06, global_step: 1040, interval_runtime: 6.3888, interval_samples_per_second: 2.504, interval_steps_per_second: 1.565, epoch: 11.6854[0m
[32m[2022-09-20 17:08:50,462] [    INFO][0m - loss: 0.36773491, learning_rate: 1.2303370786516855e-06, global_step: 1050, interval_runtime: 6.3871, interval_samples_per_second: 2.505, interval_steps_per_second: 1.566, epoch: 11.7978[0m
[32m[2022-09-20 17:08:56,856] [    INFO][0m - loss: 0.45908985, learning_rate: 1.2134831460674157e-06, global_step: 1060, interval_runtime: 6.3941, interval_samples_per_second: 2.502, interval_steps_per_second: 1.564, epoch: 11.9101[0m
[32m[2022-09-20 17:09:01,503] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 17:09:01,503] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-20 17:09:01,504] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 17:09:01,504] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 17:09:01,504] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-20 17:09:23,582] [    INFO][0m - eval_loss: 0.4104117751121521, eval_accuracy: 0.3910891089108911, eval_runtime: 22.0776, eval_samples_per_second: 64.047, eval_steps_per_second: 4.031, epoch: 12.0[0m
[32m[2022-09-20 17:09:23,597] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-1068[0m
[32m[2022-09-20 17:09:23,597] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 17:09:26,057] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-1068/tokenizer_config.json[0m
[32m[2022-09-20 17:09:26,058] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-1068/special_tokens_map.json[0m
[32m[2022-09-20 17:09:32,183] [    INFO][0m - loss: 0.47070112, learning_rate: 1.196629213483146e-06, global_step: 1070, interval_runtime: 35.327, interval_samples_per_second: 0.453, interval_steps_per_second: 0.283, epoch: 12.0225[0m
[32m[2022-09-20 17:09:41,044] [    INFO][0m - loss: 0.44133811, learning_rate: 1.1797752808988765e-06, global_step: 1080, interval_runtime: 6.3951, interval_samples_per_second: 2.502, interval_steps_per_second: 1.564, epoch: 12.1348[0m
[32m[2022-09-20 17:09:47,416] [    INFO][0m - loss: 0.32623394, learning_rate: 1.1629213483146067e-06, global_step: 1090, interval_runtime: 8.838, interval_samples_per_second: 1.81, interval_steps_per_second: 1.131, epoch: 12.2472[0m
[32m[2022-09-20 17:09:53,810] [    INFO][0m - loss: 0.42369704, learning_rate: 1.146067415730337e-06, global_step: 1100, interval_runtime: 6.3935, interval_samples_per_second: 2.503, interval_steps_per_second: 1.564, epoch: 12.3596[0m
[32m[2022-09-20 17:10:00,208] [    INFO][0m - loss: 0.36161337, learning_rate: 1.1292134831460675e-06, global_step: 1110, interval_runtime: 6.3985, interval_samples_per_second: 2.501, interval_steps_per_second: 1.563, epoch: 12.4719[0m
[32m[2022-09-20 17:10:06,616] [    INFO][0m - loss: 0.36718688, learning_rate: 1.112359550561798e-06, global_step: 1120, interval_runtime: 6.4075, interval_samples_per_second: 2.497, interval_steps_per_second: 1.561, epoch: 12.5843[0m
[32m[2022-09-20 17:10:13,013] [    INFO][0m - loss: 0.4903801, learning_rate: 1.0955056179775282e-06, global_step: 1130, interval_runtime: 6.3971, interval_samples_per_second: 2.501, interval_steps_per_second: 1.563, epoch: 12.6966[0m
[32m[2022-09-20 17:10:19,413] [    INFO][0m - loss: 0.40938816, learning_rate: 1.0786516853932585e-06, global_step: 1140, interval_runtime: 6.3996, interval_samples_per_second: 2.5, interval_steps_per_second: 1.563, epoch: 12.809[0m
[32m[2022-09-20 17:10:25,818] [    INFO][0m - loss: 0.52014565, learning_rate: 1.0617977528089887e-06, global_step: 1150, interval_runtime: 6.4052, interval_samples_per_second: 2.498, interval_steps_per_second: 1.561, epoch: 12.9213[0m
[32m[2022-09-20 17:10:29,831] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 17:10:29,831] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-20 17:10:29,831] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 17:10:29,831] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 17:10:29,831] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-20 17:10:51,974] [    INFO][0m - eval_loss: 0.4097580909729004, eval_accuracy: 0.41089108910891087, eval_runtime: 22.1428, eval_samples_per_second: 63.858, eval_steps_per_second: 4.019, epoch: 13.0[0m
[32m[2022-09-20 17:10:51,994] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-1157[0m
[32m[2022-09-20 17:10:51,994] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 17:10:54,476] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-1157/tokenizer_config.json[0m
[32m[2022-09-20 17:10:54,477] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-1157/special_tokens_map.json[0m
[32m[2022-09-20 17:11:01,239] [    INFO][0m - loss: 0.3608109, learning_rate: 1.0449438202247192e-06, global_step: 1160, interval_runtime: 35.4216, interval_samples_per_second: 0.452, interval_steps_per_second: 0.282, epoch: 13.0337[0m
[32m[2022-09-20 17:11:07,598] [    INFO][0m - loss: 0.35983999, learning_rate: 1.0280898876404494e-06, global_step: 1170, interval_runtime: 6.3582, interval_samples_per_second: 2.516, interval_steps_per_second: 1.573, epoch: 13.1461[0m
[32m[2022-09-20 17:11:13,961] [    INFO][0m - loss: 0.40801048, learning_rate: 1.0112359550561797e-06, global_step: 1180, interval_runtime: 6.3634, interval_samples_per_second: 2.514, interval_steps_per_second: 1.571, epoch: 13.2584[0m
[32m[2022-09-20 17:11:20,355] [    INFO][0m - loss: 0.37730403, learning_rate: 9.9438202247191e-07, global_step: 1190, interval_runtime: 6.3942, interval_samples_per_second: 2.502, interval_steps_per_second: 1.564, epoch: 13.3708[0m
[32m[2022-09-20 17:11:26,753] [    INFO][0m - loss: 0.38420508, learning_rate: 9.775280898876404e-07, global_step: 1200, interval_runtime: 6.3975, interval_samples_per_second: 2.501, interval_steps_per_second: 1.563, epoch: 13.4831[0m
[32m[2022-09-20 17:11:33,147] [    INFO][0m - loss: 0.40814519, learning_rate: 9.60674157303371e-07, global_step: 1210, interval_runtime: 6.3945, interval_samples_per_second: 2.502, interval_steps_per_second: 1.564, epoch: 13.5955[0m
[32m[2022-09-20 17:11:39,551] [    INFO][0m - loss: 0.35893908, learning_rate: 9.438202247191011e-07, global_step: 1220, interval_runtime: 6.4035, interval_samples_per_second: 2.499, interval_steps_per_second: 1.562, epoch: 13.7079[0m
[32m[2022-09-20 17:11:45,947] [    INFO][0m - loss: 0.51478281, learning_rate: 9.269662921348314e-07, global_step: 1230, interval_runtime: 6.396, interval_samples_per_second: 2.502, interval_steps_per_second: 1.563, epoch: 13.8202[0m
[32m[2022-09-20 17:11:52,348] [    INFO][0m - loss: 0.48954697, learning_rate: 9.101123595505619e-07, global_step: 1240, interval_runtime: 6.4015, interval_samples_per_second: 2.499, interval_steps_per_second: 1.562, epoch: 13.9326[0m
[32m[2022-09-20 17:11:55,727] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 17:11:55,727] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-20 17:11:55,727] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 17:11:55,727] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 17:11:55,727] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-20 17:12:17,878] [    INFO][0m - eval_loss: 0.41450995206832886, eval_accuracy: 0.4603960396039604, eval_runtime: 22.1505, eval_samples_per_second: 63.836, eval_steps_per_second: 4.018, epoch: 14.0[0m
[32m[2022-09-20 17:12:17,897] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-1246[0m
[32m[2022-09-20 17:12:17,897] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 17:12:20,401] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-1246/tokenizer_config.json[0m
[32m[2022-09-20 17:12:20,402] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-1246/special_tokens_map.json[0m
[32m[2022-09-20 17:12:27,802] [    INFO][0m - loss: 0.45381823, learning_rate: 8.932584269662922e-07, global_step: 1250, interval_runtime: 35.4541, interval_samples_per_second: 0.451, interval_steps_per_second: 0.282, epoch: 14.0449[0m
[32m[2022-09-20 17:12:38,061] [    INFO][0m - loss: 0.358305, learning_rate: 8.764044943820224e-07, global_step: 1260, interval_runtime: 6.3866, interval_samples_per_second: 2.505, interval_steps_per_second: 1.566, epoch: 14.1573[0m
[32m[2022-09-20 17:12:44,442] [    INFO][0m - loss: 0.40915079, learning_rate: 8.595505617977528e-07, global_step: 1270, interval_runtime: 10.253, interval_samples_per_second: 1.561, interval_steps_per_second: 0.975, epoch: 14.2697[0m
[32m[2022-09-20 17:12:50,895] [    INFO][0m - loss: 0.39905016, learning_rate: 8.426966292134833e-07, global_step: 1280, interval_runtime: 6.4535, interval_samples_per_second: 2.479, interval_steps_per_second: 1.55, epoch: 14.382[0m
[32m[2022-09-20 17:12:57,285] [    INFO][0m - loss: 0.46411715, learning_rate: 8.258426966292135e-07, global_step: 1290, interval_runtime: 6.3896, interval_samples_per_second: 2.504, interval_steps_per_second: 1.565, epoch: 14.4944[0m
[32m[2022-09-20 17:13:03,686] [    INFO][0m - loss: 0.40805593, learning_rate: 8.089887640449438e-07, global_step: 1300, interval_runtime: 6.4009, interval_samples_per_second: 2.5, interval_steps_per_second: 1.562, epoch: 14.6067[0m
[32m[2022-09-20 17:13:10,086] [    INFO][0m - loss: 0.41482286, learning_rate: 7.921348314606742e-07, global_step: 1310, interval_runtime: 6.3996, interval_samples_per_second: 2.5, interval_steps_per_second: 1.563, epoch: 14.7191[0m
[32m[2022-09-20 17:13:16,486] [    INFO][0m - loss: 0.46959968, learning_rate: 7.752808988764046e-07, global_step: 1320, interval_runtime: 6.4007, interval_samples_per_second: 2.5, interval_steps_per_second: 1.562, epoch: 14.8315[0m
[32m[2022-09-20 17:13:22,870] [    INFO][0m - loss: 0.37424498, learning_rate: 7.584269662921349e-07, global_step: 1330, interval_runtime: 6.3838, interval_samples_per_second: 2.506, interval_steps_per_second: 1.566, epoch: 14.9438[0m
[32m[2022-09-20 17:13:25,611] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 17:13:25,611] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-20 17:13:25,611] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 17:13:25,611] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 17:13:25,611] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-20 17:13:47,699] [    INFO][0m - eval_loss: 0.409609854221344, eval_accuracy: 0.4752475247524752, eval_runtime: 22.0872, eval_samples_per_second: 64.019, eval_steps_per_second: 4.029, epoch: 15.0[0m
[32m[2022-09-20 17:13:47,717] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-1335[0m
[32m[2022-09-20 17:13:47,718] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 17:13:50,178] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-1335/tokenizer_config.json[0m
[32m[2022-09-20 17:13:50,178] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-1335/special_tokens_map.json[0m
[32m[2022-09-20 17:13:58,214] [    INFO][0m - loss: 0.4397892, learning_rate: 7.415730337078651e-07, global_step: 1340, interval_runtime: 35.3435, interval_samples_per_second: 0.453, interval_steps_per_second: 0.283, epoch: 15.0562[0m
[32m[2022-09-20 17:14:04,616] [    INFO][0m - loss: 0.43562994, learning_rate: 7.247191011235955e-07, global_step: 1350, interval_runtime: 6.4022, interval_samples_per_second: 2.499, interval_steps_per_second: 1.562, epoch: 15.1685[0m
[32m[2022-09-20 17:14:11,025] [    INFO][0m - loss: 0.33040721, learning_rate: 7.078651685393259e-07, global_step: 1360, interval_runtime: 6.4086, interval_samples_per_second: 2.497, interval_steps_per_second: 1.56, epoch: 15.2809[0m
[32m[2022-09-20 17:14:17,415] [    INFO][0m - loss: 0.52264595, learning_rate: 6.910112359550562e-07, global_step: 1370, interval_runtime: 6.3908, interval_samples_per_second: 2.504, interval_steps_per_second: 1.565, epoch: 15.3933[0m
[32m[2022-09-20 17:14:23,815] [    INFO][0m - loss: 0.37636695, learning_rate: 6.741573033707865e-07, global_step: 1380, interval_runtime: 6.4, interval_samples_per_second: 2.5, interval_steps_per_second: 1.562, epoch: 15.5056[0m
[32m[2022-09-20 17:14:30,214] [    INFO][0m - loss: 0.40427213, learning_rate: 6.573033707865169e-07, global_step: 1390, interval_runtime: 6.3988, interval_samples_per_second: 2.5, interval_steps_per_second: 1.563, epoch: 15.618[0m
[32m[2022-09-20 17:14:36,619] [    INFO][0m - loss: 0.48942342, learning_rate: 6.404494382022472e-07, global_step: 1400, interval_runtime: 6.4043, interval_samples_per_second: 2.498, interval_steps_per_second: 1.561, epoch: 15.7303[0m
[32m[2022-09-20 17:14:43,048] [    INFO][0m - loss: 0.39579635, learning_rate: 6.235955056179776e-07, global_step: 1410, interval_runtime: 6.429, interval_samples_per_second: 2.489, interval_steps_per_second: 1.555, epoch: 15.8427[0m
[32m[2022-09-20 17:14:49,438] [    INFO][0m - loss: 0.41909719, learning_rate: 6.067415730337079e-07, global_step: 1420, interval_runtime: 6.3903, interval_samples_per_second: 2.504, interval_steps_per_second: 1.565, epoch: 15.9551[0m
[32m[2022-09-20 17:14:51,559] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 17:14:51,559] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-20 17:14:51,560] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 17:14:51,560] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 17:14:51,560] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-20 17:15:13,645] [    INFO][0m - eval_loss: 0.4091755449771881, eval_accuracy: 0.4801980198019802, eval_runtime: 22.0852, eval_samples_per_second: 64.025, eval_steps_per_second: 4.03, epoch: 16.0[0m
[32m[2022-09-20 17:15:13,660] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-1424[0m
[32m[2022-09-20 17:15:13,660] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 17:15:16,153] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-1424/tokenizer_config.json[0m
[32m[2022-09-20 17:15:16,153] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-1424/special_tokens_map.json[0m
[32m[2022-09-20 17:15:24,951] [    INFO][0m - loss: 0.36856949, learning_rate: 5.898876404494382e-07, global_step: 1430, interval_runtime: 35.5124, interval_samples_per_second: 0.451, interval_steps_per_second: 0.282, epoch: 16.0674[0m
[32m[2022-09-20 17:15:31,329] [    INFO][0m - loss: 0.44297657, learning_rate: 5.730337078651685e-07, global_step: 1440, interval_runtime: 6.3785, interval_samples_per_second: 2.508, interval_steps_per_second: 1.568, epoch: 16.1798[0m
[32m[2022-09-20 17:15:37,715] [    INFO][0m - loss: 0.35481956, learning_rate: 5.56179775280899e-07, global_step: 1450, interval_runtime: 6.3861, interval_samples_per_second: 2.505, interval_steps_per_second: 1.566, epoch: 16.2921[0m
[32m[2022-09-20 17:15:44,101] [    INFO][0m - loss: 0.42398753, learning_rate: 5.393258426966292e-07, global_step: 1460, interval_runtime: 6.3859, interval_samples_per_second: 2.506, interval_steps_per_second: 1.566, epoch: 16.4045[0m
[32m[2022-09-20 17:15:50,496] [    INFO][0m - loss: 0.50874777, learning_rate: 5.224719101123596e-07, global_step: 1470, interval_runtime: 6.395, interval_samples_per_second: 2.502, interval_steps_per_second: 1.564, epoch: 16.5169[0m
[32m[2022-09-20 17:15:56,898] [    INFO][0m - loss: 0.34890501, learning_rate: 5.056179775280899e-07, global_step: 1480, interval_runtime: 6.4023, interval_samples_per_second: 2.499, interval_steps_per_second: 1.562, epoch: 16.6292[0m
[32m[2022-09-20 17:16:03,293] [    INFO][0m - loss: 0.43220196, learning_rate: 4.887640449438202e-07, global_step: 1490, interval_runtime: 6.3951, interval_samples_per_second: 2.502, interval_steps_per_second: 1.564, epoch: 16.7416[0m
[32m[2022-09-20 17:16:09,684] [    INFO][0m - loss: 0.37412436, learning_rate: 4.7191011235955054e-07, global_step: 1500, interval_runtime: 6.3907, interval_samples_per_second: 2.504, interval_steps_per_second: 1.565, epoch: 16.8539[0m
[32m[2022-09-20 17:16:16,117] [    INFO][0m - loss: 0.46710815, learning_rate: 4.5505617977528095e-07, global_step: 1510, interval_runtime: 6.4333, interval_samples_per_second: 2.487, interval_steps_per_second: 1.554, epoch: 16.9663[0m
[32m[2022-09-20 17:16:17,615] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 17:16:17,616] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-20 17:16:17,616] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 17:16:17,616] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 17:16:17,616] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-20 17:16:39,729] [    INFO][0m - eval_loss: 0.40924322605133057, eval_accuracy: 0.504950495049505, eval_runtime: 22.1123, eval_samples_per_second: 63.946, eval_steps_per_second: 4.025, epoch: 17.0[0m
[32m[2022-09-20 17:16:39,744] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-1513[0m
[32m[2022-09-20 17:16:39,745] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 17:16:42,225] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-1513/tokenizer_config.json[0m
[32m[2022-09-20 17:16:42,225] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-1513/special_tokens_map.json[0m
[32m[2022-09-20 17:16:51,419] [    INFO][0m - loss: 0.39569027, learning_rate: 4.382022471910112e-07, global_step: 1520, interval_runtime: 35.302, interval_samples_per_second: 0.453, interval_steps_per_second: 0.283, epoch: 17.0787[0m
[32m[2022-09-20 17:16:57,794] [    INFO][0m - loss: 0.42313108, learning_rate: 4.2134831460674163e-07, global_step: 1530, interval_runtime: 6.3746, interval_samples_per_second: 2.51, interval_steps_per_second: 1.569, epoch: 17.191[0m
[32m[2022-09-20 17:17:04,184] [    INFO][0m - loss: 0.55194478, learning_rate: 4.044943820224719e-07, global_step: 1540, interval_runtime: 6.39, interval_samples_per_second: 2.504, interval_steps_per_second: 1.565, epoch: 17.3034[0m
[32m[2022-09-20 17:17:10,588] [    INFO][0m - loss: 0.45507793, learning_rate: 3.876404494382023e-07, global_step: 1550, interval_runtime: 6.4046, interval_samples_per_second: 2.498, interval_steps_per_second: 1.561, epoch: 17.4157[0m
[32m[2022-09-20 17:17:17,015] [    INFO][0m - loss: 0.41374707, learning_rate: 3.707865168539326e-07, global_step: 1560, interval_runtime: 6.4262, interval_samples_per_second: 2.49, interval_steps_per_second: 1.556, epoch: 17.5281[0m
[32m[2022-09-20 17:17:23,408] [    INFO][0m - loss: 0.33363845, learning_rate: 3.5393258426966294e-07, global_step: 1570, interval_runtime: 6.3931, interval_samples_per_second: 2.503, interval_steps_per_second: 1.564, epoch: 17.6404[0m
[32m[2022-09-20 17:17:29,809] [    INFO][0m - loss: 0.42197046, learning_rate: 3.3707865168539325e-07, global_step: 1580, interval_runtime: 6.4012, interval_samples_per_second: 2.5, interval_steps_per_second: 1.562, epoch: 17.7528[0m
[32m[2022-09-20 17:17:36,239] [    INFO][0m - loss: 0.32196634, learning_rate: 3.202247191011236e-07, global_step: 1590, interval_runtime: 6.4302, interval_samples_per_second: 2.488, interval_steps_per_second: 1.555, epoch: 17.8652[0m
[32m[2022-09-20 17:17:42,579] [    INFO][0m - loss: 0.38209052, learning_rate: 3.0337078651685393e-07, global_step: 1600, interval_runtime: 6.3404, interval_samples_per_second: 2.524, interval_steps_per_second: 1.577, epoch: 17.9775[0m
[32m[2022-09-20 17:17:43,466] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 17:17:43,466] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-20 17:17:43,466] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 17:17:43,466] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 17:17:43,466] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-20 17:18:05,587] [    INFO][0m - eval_loss: 0.4128968417644501, eval_accuracy: 0.4900990099009901, eval_runtime: 22.1212, eval_samples_per_second: 63.921, eval_steps_per_second: 4.023, epoch: 18.0[0m
[32m[2022-09-20 17:18:05,601] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-1602[0m
[32m[2022-09-20 17:18:05,602] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 17:18:08,047] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-1602/tokenizer_config.json[0m
[32m[2022-09-20 17:18:08,047] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-1602/special_tokens_map.json[0m
[32m[2022-09-20 17:18:17,929] [    INFO][0m - loss: 0.50433688, learning_rate: 2.8651685393258425e-07, global_step: 1610, interval_runtime: 35.3499, interval_samples_per_second: 0.453, interval_steps_per_second: 0.283, epoch: 18.0899[0m
[32m[2022-09-20 17:18:24,302] [    INFO][0m - loss: 0.41804857, learning_rate: 2.696629213483146e-07, global_step: 1620, interval_runtime: 6.3725, interval_samples_per_second: 2.511, interval_steps_per_second: 1.569, epoch: 18.2022[0m
[32m[2022-09-20 17:18:30,718] [    INFO][0m - loss: 0.32266045, learning_rate: 2.5280898876404493e-07, global_step: 1630, interval_runtime: 6.4159, interval_samples_per_second: 2.494, interval_steps_per_second: 1.559, epoch: 18.3146[0m
[32m[2022-09-20 17:18:37,141] [    INFO][0m - loss: 0.44949188, learning_rate: 2.3595505617977527e-07, global_step: 1640, interval_runtime: 6.4235, interval_samples_per_second: 2.491, interval_steps_per_second: 1.557, epoch: 18.427[0m
[32m[2022-09-20 17:18:43,600] [    INFO][0m - loss: 0.40278087, learning_rate: 2.191011235955056e-07, global_step: 1650, interval_runtime: 6.43, interval_samples_per_second: 2.488, interval_steps_per_second: 1.555, epoch: 18.5393[0m
[32m[2022-09-20 17:18:50,005] [    INFO][0m - loss: 0.3780714, learning_rate: 2.0224719101123595e-07, global_step: 1660, interval_runtime: 6.4341, interval_samples_per_second: 2.487, interval_steps_per_second: 1.554, epoch: 18.6517[0m
[32m[2022-09-20 17:18:56,407] [    INFO][0m - loss: 0.43098011, learning_rate: 1.853932584269663e-07, global_step: 1670, interval_runtime: 6.4021, interval_samples_per_second: 2.499, interval_steps_per_second: 1.562, epoch: 18.764[0m
[32m[2022-09-20 17:19:02,803] [    INFO][0m - loss: 0.42882133, learning_rate: 1.6853932584269663e-07, global_step: 1680, interval_runtime: 6.3957, interval_samples_per_second: 2.502, interval_steps_per_second: 1.564, epoch: 18.8764[0m
[32m[2022-09-20 17:19:09,125] [    INFO][0m - loss: 0.43210344, learning_rate: 1.5168539325842697e-07, global_step: 1690, interval_runtime: 6.3214, interval_samples_per_second: 2.531, interval_steps_per_second: 1.582, epoch: 18.9888[0m
[32m[2022-09-20 17:19:09,380] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 17:19:09,381] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-20 17:19:09,381] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 17:19:09,381] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 17:19:09,381] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-20 17:19:31,495] [    INFO][0m - eval_loss: 0.4089745581150055, eval_accuracy: 0.5198019801980198, eval_runtime: 22.1135, eval_samples_per_second: 63.943, eval_steps_per_second: 4.025, epoch: 19.0[0m
[32m[2022-09-20 17:19:31,514] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-1691[0m
[32m[2022-09-20 17:19:31,515] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 17:19:33,974] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-1691/tokenizer_config.json[0m
[32m[2022-09-20 17:19:33,975] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-1691/special_tokens_map.json[0m
[32m[2022-09-20 17:19:44,552] [    INFO][0m - loss: 0.43337827, learning_rate: 1.348314606741573e-07, global_step: 1700, interval_runtime: 35.4278, interval_samples_per_second: 0.452, interval_steps_per_second: 0.282, epoch: 19.1011[0m
[32m[2022-09-20 17:19:50,986] [    INFO][0m - loss: 0.42849011, learning_rate: 1.1797752808988763e-07, global_step: 1710, interval_runtime: 6.4334, interval_samples_per_second: 2.487, interval_steps_per_second: 1.554, epoch: 19.2135[0m
[32m[2022-09-20 17:19:57,371] [    INFO][0m - loss: 0.35503027, learning_rate: 1.0112359550561797e-07, global_step: 1720, interval_runtime: 6.385, interval_samples_per_second: 2.506, interval_steps_per_second: 1.566, epoch: 19.3258[0m
[32m[2022-09-20 17:20:03,795] [    INFO][0m - loss: 0.38177593, learning_rate: 8.426966292134831e-08, global_step: 1730, interval_runtime: 6.4245, interval_samples_per_second: 2.49, interval_steps_per_second: 1.557, epoch: 19.4382[0m
[32m[2022-09-20 17:20:12,926] [    INFO][0m - loss: 0.41463461, learning_rate: 6.741573033707865e-08, global_step: 1740, interval_runtime: 6.4044, interval_samples_per_second: 2.498, interval_steps_per_second: 1.561, epoch: 19.5506[0m
[32m[2022-09-20 17:20:19,310] [    INFO][0m - loss: 0.44365721, learning_rate: 5.056179775280899e-08, global_step: 1750, interval_runtime: 9.1103, interval_samples_per_second: 1.756, interval_steps_per_second: 1.098, epoch: 19.6629[0m
[32m[2022-09-20 17:20:25,696] [    INFO][0m - loss: 0.43137774, learning_rate: 3.370786516853933e-08, global_step: 1760, interval_runtime: 6.3861, interval_samples_per_second: 2.505, interval_steps_per_second: 1.566, epoch: 19.7753[0m
[32m[2022-09-20 17:20:32,092] [    INFO][0m - loss: 0.38786292, learning_rate: 1.6853932584269663e-08, global_step: 1770, interval_runtime: 6.3961, interval_samples_per_second: 2.502, interval_steps_per_second: 1.563, epoch: 19.8876[0m
[32m[2022-09-20 17:20:38,028] [    INFO][0m - loss: 0.44108944, learning_rate: 0.0, global_step: 1780, interval_runtime: 5.9356, interval_samples_per_second: 2.696, interval_steps_per_second: 1.685, epoch: 20.0[0m
[32m[2022-09-20 17:20:38,028] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-20 17:20:38,028] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-20 17:20:38,028] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 17:20:38,029] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 17:20:38,029] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-20 17:21:00,141] [    INFO][0m - eval_loss: 0.40911903977394104, eval_accuracy: 0.5247524752475248, eval_runtime: 22.1124, eval_samples_per_second: 63.946, eval_steps_per_second: 4.025, epoch: 20.0[0m
[32m[2022-09-20 17:21:00,160] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-1780[0m
[32m[2022-09-20 17:21:00,160] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 17:21:02,620] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-1780/tokenizer_config.json[0m
[32m[2022-09-20 17:21:02,621] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-1780/special_tokens_map.json[0m
[32m[2022-09-20 17:21:07,239] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-20 17:21:07,239] [    INFO][0m - Loading best model from ./checkpoints_chid/checkpoint-1780 (score: 0.5247524752475248).[0m
[32m[2022-09-20 17:21:08,669] [    INFO][0m - train_runtime: 1743.8789, train_samples_per_second: 16.217, train_steps_per_second: 1.021, train_loss: 0.46307224597823754, epoch: 20.0[0m
[32m[2022-09-20 17:21:08,704] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/[0m
[32m[2022-09-20 17:21:08,705] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-20 17:21:11,008] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/tokenizer_config.json[0m
[32m[2022-09-20 17:21:11,008] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/special_tokens_map.json[0m
[32m[2022-09-20 17:21:11,009] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-20 17:21:11,009] [    INFO][0m -   epoch                    =       20.0[0m
[32m[2022-09-20 17:21:11,009] [    INFO][0m -   train_loss               =     0.4631[0m
[32m[2022-09-20 17:21:11,009] [    INFO][0m -   train_runtime            = 0:29:03.87[0m
[32m[2022-09-20 17:21:11,010] [    INFO][0m -   train_samples_per_second =     16.217[0m
[32m[2022-09-20 17:21:11,010] [    INFO][0m -   train_steps_per_second   =      1.021[0m
[32m[2022-09-20 17:21:11,017] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-20 17:21:11,017] [    INFO][0m -   Num examples = 14014[0m
[32m[2022-09-20 17:21:11,017] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 17:21:11,017] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 17:21:11,017] [    INFO][0m -   Total prediction steps = 876[0m
[32m[2022-09-20 17:24:59,236] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-20 17:24:59,237] [    INFO][0m -   test_accuracy           =     0.5614[0m
[32m[2022-09-20 17:24:59,237] [    INFO][0m -   test_loss               =     0.4089[0m
[32m[2022-09-20 17:24:59,237] [    INFO][0m -   test_runtime            = 0:03:48.21[0m
[32m[2022-09-20 17:24:59,237] [    INFO][0m -   test_samples_per_second =     61.406[0m
[32m[2022-09-20 17:24:59,237] [    INFO][0m -   test_steps_per_second   =      3.838[0m
[32m[2022-09-20 17:24:59,237] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-20 17:24:59,238] [    INFO][0m -   Num examples = 14000[0m
[32m[2022-09-20 17:24:59,238] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-20 17:24:59,238] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-20 17:24:59,238] [    INFO][0m -   Total prediction steps = 875[0m
[32m[2022-09-20 17:29:01,887] [    INFO][0m - Predictions for chidf saved to ./fewclue_submit_examples.[0m
{
  "labels": 0,
  "text_a": "\u5927\u90e8\u5206\u7684\u5927\u529f\u544a\u6210\u90fd\u662f\u7531\u65e0\u6570\u4e2a\u9020\u8c23\u751f\u4e8b\u7d2f\u79ef\u800c\u6765,\u505a\u597d\u6bcf\u4e00\u4ef6\u5e94\u8be5\u505a\u597d\u7684\u5c0f\u4e8b\u7edd\u4e0d\u662f\u4e00\u4ef6\u5c0f\u4e8b\u3002 \u300a\u4f60\u597d\u674e\u7115\u82f1\u300b\u4e3b\u8981\u8bb2\u4e86\u8d3e\u73b2\u610f\u5916\u7a7f\u8d8a\u5230\u4e86\u6bcd\u4eb2\u5e74\u8f7b\u7684\u65f6\u5019,\u548c\u6bcd\u4eb2\u4e00\u8d77\u611f\u53d7\u4e86\u5979\u7684\u751f\u6d3b\u548c\u60c5\u611f\u3002\u8fd9\u6545\u4e8b\u662f\u6839\u636e\u8d3e\u73b2\u7684\u4eb2\u751f\u7ecf\u5386\u6539\u7f16\u800c\u6765, ...",
  "text_b": "",
  "uid": 24
}

Prediction done.
run.sh: line 69: --pretrained: command not found
