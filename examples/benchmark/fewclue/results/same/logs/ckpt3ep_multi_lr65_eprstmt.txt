[32m[2022-09-19 20:18:41,886] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-19 20:18:41,886] [    INFO][0m - ============================================================[0m
[32m[2022-09-19 20:18:41,886] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-19 20:18:41,886] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-19 20:18:41,886] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-19 20:18:41,886] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-19 20:18:41,886] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-19 20:18:41,887] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-19 20:18:41,887] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-19 20:18:41,887] [    INFO][0m - [0m
[32m[2022-09-19 20:18:41,887] [    INFO][0m - ============================================================[0m
[32m[2022-09-19 20:18:41,887] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-19 20:18:41,887] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-19 20:18:41,887] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-19 20:18:41,887] [    INFO][0m - pretrained                    :/ssd2/wanghuijuan03/data/zero-shot/0919_model_state.pdparams[0m
[32m[2022-09-19 20:18:41,887] [    INFO][0m - prompt                        :‚Äú{'text':'text_a'}‚ÄùËøôÊù°ËØÑËÆ∫ÁöÑÊÉÖÊÑüÂÄæÂêëÊòØ{'mask'}{'mask'}ÁöÑ„ÄÇ[0m
[32m[2022-09-19 20:18:41,887] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-19 20:18:41,887] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-19 20:18:41,887] [    INFO][0m - task_name                     :eprstmt[0m
[32m[2022-09-19 20:18:41,887] [    INFO][0m - [0m
[32m[2022-09-19 20:18:41,888] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0919 20:18:41.889653  5538 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0919 20:18:41.893612  5538 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-19 20:18:46,546] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-19 20:18:46,557] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-19 20:18:46,557] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-19 20:18:46,558] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùËøôÊù°ËØÑËÆ∫ÁöÑÊÉÖÊÑüÂÄæÂêëÊòØ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'ÁöÑ„ÄÇ'}][0m
[32m[2022-09-19 20:18:47,934] [    INFO][0m - ============================================================[0m
[32m[2022-09-19 20:18:47,934] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-19 20:18:47,934] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-19 20:18:47,934] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-19 20:18:47,935] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-19 20:18:47,935] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-19 20:18:47,935] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-19 20:18:47,935] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-19 20:18:47,935] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-19 20:18:47,935] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-19 20:18:47,935] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-19 20:18:47,935] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-19 20:18:47,935] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-19 20:18:47,935] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-19 20:18:47,935] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-19 20:18:47,935] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-19 20:18:47,936] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-19 20:18:47,936] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-19 20:18:47,936] [    INFO][0m - eval_batch_size               :16[0m
[32m[2022-09-19 20:18:47,936] [    INFO][0m - eval_steps                    :200[0m
[32m[2022-09-19 20:18:47,936] [    INFO][0m - evaluation_strategy           :IntervalStrategy.EPOCH[0m
[32m[2022-09-19 20:18:47,936] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-19 20:18:47,936] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-19 20:18:47,936] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-19 20:18:47,936] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-19 20:18:47,936] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-19 20:18:47,936] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-19 20:18:47,936] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-19 20:18:47,936] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-19 20:18:47,937] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-19 20:18:47,937] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-09-19 20:18:47,937] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-19 20:18:47,937] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-19 20:18:47,937] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-19 20:18:47,937] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-19 20:18:47,937] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-19 20:18:47,937] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-19 20:18:47,937] [    INFO][0m - logging_dir                   :./checkpoints_eprstmt/runs/Sep19_20-18-41_instance-3bwob41y-01[0m
[32m[2022-09-19 20:18:47,937] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-19 20:18:47,937] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-19 20:18:47,937] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-19 20:18:47,937] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-19 20:18:47,937] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-19 20:18:47,938] [    INFO][0m - max_seq_length                :128[0m
[32m[2022-09-19 20:18:47,938] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-19 20:18:47,938] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-19 20:18:47,938] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-19 20:18:47,938] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-19 20:18:47,938] [    INFO][0m - num_train_epochs              :20.0[0m
[32m[2022-09-19 20:18:47,938] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-19 20:18:47,938] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-19 20:18:47,938] [    INFO][0m - output_dir                    :./checkpoints_eprstmt/[0m
[32m[2022-09-19 20:18:47,938] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-19 20:18:47,938] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-19 20:18:47,938] [    INFO][0m - per_device_eval_batch_size    :16[0m
[32m[2022-09-19 20:18:47,938] [    INFO][0m - per_device_train_batch_size   :16[0m
[32m[2022-09-19 20:18:47,939] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-19 20:18:47,939] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-19 20:18:47,939] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-19 20:18:47,939] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-09-19 20:18:47,939] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-19 20:18:47,939] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-19 20:18:47,939] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-19 20:18:47,939] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-19 20:18:47,939] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-19 20:18:47,939] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-19 20:18:47,939] [    INFO][0m - run_name                      :./checkpoints_eprstmt/[0m
[32m[2022-09-19 20:18:47,939] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-19 20:18:47,939] [    INFO][0m - save_steps                    :200[0m
[32m[2022-09-19 20:18:47,940] [    INFO][0m - save_strategy                 :IntervalStrategy.EPOCH[0m
[32m[2022-09-19 20:18:47,940] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-19 20:18:47,940] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-19 20:18:47,940] [    INFO][0m - seed                          :42[0m
[32m[2022-09-19 20:18:47,940] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-19 20:18:47,940] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-19 20:18:47,940] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-19 20:18:47,940] [    INFO][0m - train_batch_size              :16[0m
[32m[2022-09-19 20:18:47,940] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-19 20:18:47,940] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-19 20:18:47,940] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-19 20:18:47,940] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-19 20:18:47,940] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-19 20:18:47,940] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-19 20:18:47,941] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-19 20:18:47,941] [    INFO][0m - [0m
[32m[2022-09-19 20:18:47,944] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-19 20:18:47,944] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 20:18:47,944] [    INFO][0m -   Num Epochs = 20[0m
[32m[2022-09-19 20:18:47,944] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2022-09-19 20:18:47,944] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2022-09-19 20:18:47,944] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-19 20:18:47,944] [    INFO][0m -   Total optimization steps = 200.0[0m
[32m[2022-09-19 20:18:47,945] [    INFO][0m -   Total num train samples = 3200[0m
[32m[2022-09-19 20:18:52,306] [    INFO][0m - loss: 1.56269875, learning_rate: 2.8499999999999998e-05, global_step: 10, interval_runtime: 4.3601, interval_samples_per_second: 3.67, interval_steps_per_second: 2.294, epoch: 1.0[0m
[32m[2022-09-19 20:18:52,307] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 20:18:52,307] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 20:18:52,307] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 20:18:52,308] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 20:18:52,308] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 20:18:53,650] [    INFO][0m - eval_loss: 1.1002628803253174, eval_accuracy: 0.5625, eval_runtime: 1.3417, eval_samples_per_second: 119.252, eval_steps_per_second: 7.453, epoch: 1.0[0m
[32m[2022-09-19 20:18:53,651] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-10[0m
[32m[2022-09-19 20:18:53,651] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 20:18:56,591] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-10/tokenizer_config.json[0m
[32m[2022-09-19 20:18:56,591] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-10/special_tokens_map.json[0m
[32m[2022-09-19 20:19:09,911] [    INFO][0m - loss: 0.45179701, learning_rate: 2.7000000000000002e-05, global_step: 20, interval_runtime: 17.6055, interval_samples_per_second: 0.909, interval_steps_per_second: 0.568, epoch: 2.0[0m
[32m[2022-09-19 20:19:09,912] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 20:19:09,912] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 20:19:09,912] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 20:19:09,913] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 20:19:09,913] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 20:19:11,237] [    INFO][0m - eval_loss: 0.46906375885009766, eval_accuracy: 0.88125, eval_runtime: 1.3239, eval_samples_per_second: 120.855, eval_steps_per_second: 7.553, epoch: 2.0[0m
[32m[2022-09-19 20:19:11,238] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-20[0m
[32m[2022-09-19 20:19:11,238] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 20:19:13,851] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-20/tokenizer_config.json[0m
[32m[2022-09-19 20:19:13,851] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-20/special_tokens_map.json[0m
[32m[2022-09-19 20:19:22,123] [    INFO][0m - loss: 0.140959, learning_rate: 2.55e-05, global_step: 30, interval_runtime: 12.2117, interval_samples_per_second: 1.31, interval_steps_per_second: 0.819, epoch: 3.0[0m
[32m[2022-09-19 20:19:22,124] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 20:19:22,124] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 20:19:22,124] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 20:19:22,124] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 20:19:22,124] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 20:19:23,451] [    INFO][0m - eval_loss: 0.5188922882080078, eval_accuracy: 0.8875, eval_runtime: 1.3265, eval_samples_per_second: 120.62, eval_steps_per_second: 7.539, epoch: 3.0[0m
[32m[2022-09-19 20:19:23,451] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-30[0m
[32m[2022-09-19 20:19:23,451] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 20:19:26,012] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-30/tokenizer_config.json[0m
[32m[2022-09-19 20:19:26,012] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-30/special_tokens_map.json[0m
[32m[2022-09-19 20:19:37,943] [    INFO][0m - loss: 0.04340436, learning_rate: 2.4e-05, global_step: 40, interval_runtime: 15.8195, interval_samples_per_second: 1.011, interval_steps_per_second: 0.632, epoch: 4.0[0m
[32m[2022-09-19 20:19:37,943] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 20:19:37,943] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 20:19:37,943] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 20:19:37,944] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 20:19:37,944] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 20:19:39,259] [    INFO][0m - eval_loss: 0.6833661794662476, eval_accuracy: 0.9, eval_runtime: 1.3146, eval_samples_per_second: 121.706, eval_steps_per_second: 7.607, epoch: 4.0[0m
[32m[2022-09-19 20:19:39,259] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-40[0m
[32m[2022-09-19 20:19:39,259] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 20:19:41,654] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-40/tokenizer_config.json[0m
[32m[2022-09-19 20:19:41,654] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-40/special_tokens_map.json[0m
[32m[2022-09-19 20:19:49,891] [    INFO][0m - loss: 0.00290536, learning_rate: 2.25e-05, global_step: 50, interval_runtime: 11.9485, interval_samples_per_second: 1.339, interval_steps_per_second: 0.837, epoch: 5.0[0m
[32m[2022-09-19 20:19:49,892] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 20:19:49,892] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 20:19:49,892] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 20:19:49,892] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 20:19:49,892] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 20:19:51,218] [    INFO][0m - eval_loss: 1.0986623764038086, eval_accuracy: 0.86875, eval_runtime: 1.326, eval_samples_per_second: 120.661, eval_steps_per_second: 7.541, epoch: 5.0[0m
[32m[2022-09-19 20:19:51,513] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-50[0m
[32m[2022-09-19 20:19:51,513] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 20:19:53,988] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-50/tokenizer_config.json[0m
[32m[2022-09-19 20:19:53,988] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-50/special_tokens_map.json[0m
[32m[2022-09-19 20:20:02,199] [    INFO][0m - loss: 6.657e-05, learning_rate: 2.1e-05, global_step: 60, interval_runtime: 12.3087, interval_samples_per_second: 1.3, interval_steps_per_second: 0.812, epoch: 6.0[0m
[32m[2022-09-19 20:20:02,200] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 20:20:02,200] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 20:20:02,200] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 20:20:02,200] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 20:20:02,200] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 20:20:03,517] [    INFO][0m - eval_loss: 1.240580677986145, eval_accuracy: 0.86875, eval_runtime: 1.3171, eval_samples_per_second: 121.475, eval_steps_per_second: 7.592, epoch: 6.0[0m
[32m[2022-09-19 20:20:03,518] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-60[0m
[32m[2022-09-19 20:20:03,518] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 20:20:07,809] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-60/tokenizer_config.json[0m
[32m[2022-09-19 20:20:07,810] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-60/special_tokens_map.json[0m
[32m[2022-09-19 20:20:15,993] [    INFO][0m - loss: 1.769e-05, learning_rate: 1.95e-05, global_step: 70, interval_runtime: 13.7931, interval_samples_per_second: 1.16, interval_steps_per_second: 0.725, epoch: 7.0[0m
[32m[2022-09-19 20:20:15,993] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 20:20:15,993] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 20:20:15,993] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 20:20:15,994] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 20:20:15,994] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 20:20:17,305] [    INFO][0m - eval_loss: 1.3155882358551025, eval_accuracy: 0.86875, eval_runtime: 1.3108, eval_samples_per_second: 122.065, eval_steps_per_second: 7.629, epoch: 7.0[0m
[32m[2022-09-19 20:20:17,305] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-70[0m
[32m[2022-09-19 20:20:17,305] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 20:20:19,750] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-70/tokenizer_config.json[0m
[32m[2022-09-19 20:20:19,750] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-70/special_tokens_map.json[0m
[32m[2022-09-19 20:20:27,909] [    INFO][0m - loss: 1.1e-05, learning_rate: 1.8e-05, global_step: 80, interval_runtime: 11.9165, interval_samples_per_second: 1.343, interval_steps_per_second: 0.839, epoch: 8.0[0m
[32m[2022-09-19 20:20:27,909] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 20:20:27,909] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 20:20:27,910] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 20:20:27,910] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 20:20:27,910] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 20:20:29,222] [    INFO][0m - eval_loss: 1.3667227029800415, eval_accuracy: 0.86875, eval_runtime: 1.3125, eval_samples_per_second: 121.908, eval_steps_per_second: 7.619, epoch: 8.0[0m
[32m[2022-09-19 20:20:31,165] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-80[0m
[32m[2022-09-19 20:20:31,165] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 20:20:33,615] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-80/tokenizer_config.json[0m
[32m[2022-09-19 20:20:33,615] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-80/special_tokens_map.json[0m
[32m[2022-09-19 20:20:41,860] [    INFO][0m - loss: 6.96e-06, learning_rate: 1.65e-05, global_step: 90, interval_runtime: 13.9516, interval_samples_per_second: 1.147, interval_steps_per_second: 0.717, epoch: 9.0[0m
[32m[2022-09-19 20:20:41,861] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 20:20:41,861] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 20:20:41,861] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 20:20:41,861] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 20:20:41,861] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 20:20:43,178] [    INFO][0m - eval_loss: 1.3973243236541748, eval_accuracy: 0.86875, eval_runtime: 1.3168, eval_samples_per_second: 121.508, eval_steps_per_second: 7.594, epoch: 9.0[0m
[32m[2022-09-19 20:20:43,179] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-90[0m
[32m[2022-09-19 20:20:43,179] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 20:20:45,913] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-90/tokenizer_config.json[0m
[32m[2022-09-19 20:20:45,913] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-90/special_tokens_map.json[0m
[32m[2022-09-19 20:20:54,048] [    INFO][0m - loss: 5.77e-06, learning_rate: 1.5e-05, global_step: 100, interval_runtime: 12.1877, interval_samples_per_second: 1.313, interval_steps_per_second: 0.82, epoch: 10.0[0m
[32m[2022-09-19 20:20:54,049] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 20:20:54,049] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 20:20:54,049] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 20:20:54,049] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 20:20:54,049] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 20:20:55,359] [    INFO][0m - eval_loss: 1.4197286367416382, eval_accuracy: 0.86875, eval_runtime: 1.3096, eval_samples_per_second: 122.179, eval_steps_per_second: 7.636, epoch: 10.0[0m
[32m[2022-09-19 20:20:55,359] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-100[0m
[32m[2022-09-19 20:20:55,360] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 20:20:57,755] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-19 20:20:57,755] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-19 20:21:05,856] [    INFO][0m - loss: 5.38e-06, learning_rate: 1.3500000000000001e-05, global_step: 110, interval_runtime: 11.8076, interval_samples_per_second: 1.355, interval_steps_per_second: 0.847, epoch: 11.0[0m
[32m[2022-09-19 20:21:05,856] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 20:21:05,856] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 20:21:05,856] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 20:21:05,856] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 20:21:05,857] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 20:21:07,169] [    INFO][0m - eval_loss: 1.4397790431976318, eval_accuracy: 0.86875, eval_runtime: 1.3122, eval_samples_per_second: 121.932, eval_steps_per_second: 7.621, epoch: 11.0[0m
[32m[2022-09-19 20:21:10,292] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-110[0m
[32m[2022-09-19 20:21:10,293] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 20:21:12,812] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-110/tokenizer_config.json[0m
[32m[2022-09-19 20:21:12,812] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-110/special_tokens_map.json[0m
[32m[2022-09-19 20:21:21,635] [    INFO][0m - loss: 5.57e-06, learning_rate: 1.2e-05, global_step: 120, interval_runtime: 15.7789, interval_samples_per_second: 1.014, interval_steps_per_second: 0.634, epoch: 12.0[0m
[32m[2022-09-19 20:21:21,635] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 20:21:21,635] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 20:21:21,635] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 20:21:21,635] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 20:21:21,635] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 20:21:22,961] [    INFO][0m - eval_loss: 1.4635870456695557, eval_accuracy: 0.86875, eval_runtime: 1.3249, eval_samples_per_second: 120.762, eval_steps_per_second: 7.548, epoch: 12.0[0m
[32m[2022-09-19 20:21:22,961] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-120[0m
[32m[2022-09-19 20:21:22,961] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 20:21:25,946] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-120/tokenizer_config.json[0m
[32m[2022-09-19 20:21:25,946] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-120/special_tokens_map.json[0m
[32m[2022-09-19 20:21:34,518] [    INFO][0m - loss: 6.44e-06, learning_rate: 1.05e-05, global_step: 130, interval_runtime: 12.8836, interval_samples_per_second: 1.242, interval_steps_per_second: 0.776, epoch: 13.0[0m
[32m[2022-09-19 20:21:34,519] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 20:21:34,519] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 20:21:34,519] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 20:21:34,519] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 20:21:34,519] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 20:21:35,834] [    INFO][0m - eval_loss: 1.4881107807159424, eval_accuracy: 0.86875, eval_runtime: 1.3148, eval_samples_per_second: 121.691, eval_steps_per_second: 7.606, epoch: 13.0[0m
[32m[2022-09-19 20:21:35,834] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-130[0m
[32m[2022-09-19 20:21:35,835] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 20:21:38,456] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-130/tokenizer_config.json[0m
[32m[2022-09-19 20:21:38,456] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-130/special_tokens_map.json[0m
[32m[2022-09-19 20:21:46,865] [    INFO][0m - loss: 3.35e-06, learning_rate: 9e-06, global_step: 140, interval_runtime: 12.347, interval_samples_per_second: 1.296, interval_steps_per_second: 0.81, epoch: 14.0[0m
[32m[2022-09-19 20:21:46,866] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 20:21:46,866] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 20:21:46,866] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 20:21:46,866] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 20:21:46,866] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 20:21:48,184] [    INFO][0m - eval_loss: 1.505789875984192, eval_accuracy: 0.86875, eval_runtime: 1.3174, eval_samples_per_second: 121.455, eval_steps_per_second: 7.591, epoch: 14.0[0m
[32m[2022-09-19 20:21:48,184] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-140[0m
[32m[2022-09-19 20:21:48,185] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 20:21:50,698] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-140/tokenizer_config.json[0m
[32m[2022-09-19 20:21:50,699] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-140/special_tokens_map.json[0m
[32m[2022-09-19 20:21:59,122] [    INFO][0m - loss: 5.07e-06, learning_rate: 7.5e-06, global_step: 150, interval_runtime: 12.2572, interval_samples_per_second: 1.305, interval_steps_per_second: 0.816, epoch: 15.0[0m
[32m[2022-09-19 20:21:59,123] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 20:21:59,123] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 20:21:59,123] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 20:21:59,123] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 20:21:59,123] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 20:22:00,433] [    INFO][0m - eval_loss: 1.5198837518692017, eval_accuracy: 0.86875, eval_runtime: 1.3095, eval_samples_per_second: 122.185, eval_steps_per_second: 7.637, epoch: 15.0[0m
[32m[2022-09-19 20:22:00,433] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-150[0m
[32m[2022-09-19 20:22:00,434] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 20:22:04,095] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-150/tokenizer_config.json[0m
[32m[2022-09-19 20:22:04,095] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-150/special_tokens_map.json[0m
[32m[2022-09-19 20:22:12,515] [    INFO][0m - loss: 4.03e-06, learning_rate: 6e-06, global_step: 160, interval_runtime: 13.3928, interval_samples_per_second: 1.195, interval_steps_per_second: 0.747, epoch: 16.0[0m
[32m[2022-09-19 20:22:12,516] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 20:22:12,516] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 20:22:12,516] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 20:22:12,516] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 20:22:12,516] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 20:22:13,824] [    INFO][0m - eval_loss: 1.52975332736969, eval_accuracy: 0.86875, eval_runtime: 1.3077, eval_samples_per_second: 122.351, eval_steps_per_second: 7.647, epoch: 16.0[0m
[32m[2022-09-19 20:22:13,824] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-160[0m
[32m[2022-09-19 20:22:13,825] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 20:22:16,356] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-160/tokenizer_config.json[0m
[32m[2022-09-19 20:22:16,356] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-160/special_tokens_map.json[0m
[32m[2022-09-19 20:22:29,602] [    INFO][0m - loss: 3.3e-06, learning_rate: 4.5e-06, global_step: 170, interval_runtime: 17.0866, interval_samples_per_second: 0.936, interval_steps_per_second: 0.585, epoch: 17.0[0m
[32m[2022-09-19 20:22:29,602] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 20:22:29,603] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 20:22:29,603] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 20:22:29,603] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 20:22:29,603] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 20:22:30,928] [    INFO][0m - eval_loss: 1.5370495319366455, eval_accuracy: 0.86875, eval_runtime: 1.3246, eval_samples_per_second: 120.788, eval_steps_per_second: 7.549, epoch: 17.0[0m
[32m[2022-09-19 20:22:30,928] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-170[0m
[32m[2022-09-19 20:22:30,928] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 20:22:33,523] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-170/tokenizer_config.json[0m
[32m[2022-09-19 20:22:33,524] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-170/special_tokens_map.json[0m
[32m[2022-09-19 20:22:41,977] [    INFO][0m - loss: 2.68e-06, learning_rate: 3e-06, global_step: 180, interval_runtime: 12.3748, interval_samples_per_second: 1.293, interval_steps_per_second: 0.808, epoch: 18.0[0m
[32m[2022-09-19 20:22:41,977] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 20:22:41,977] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 20:22:41,977] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 20:22:41,977] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 20:22:41,978] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 20:22:43,290] [    INFO][0m - eval_loss: 1.541803002357483, eval_accuracy: 0.86875, eval_runtime: 1.3121, eval_samples_per_second: 121.944, eval_steps_per_second: 7.622, epoch: 18.0[0m
[32m[2022-09-19 20:22:43,290] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-180[0m
[32m[2022-09-19 20:22:43,290] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 20:22:45,869] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-180/tokenizer_config.json[0m
[32m[2022-09-19 20:22:45,869] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-180/special_tokens_map.json[0m
[32m[2022-09-19 20:22:54,757] [    INFO][0m - loss: 2.2e-06, learning_rate: 1.5e-06, global_step: 190, interval_runtime: 12.78, interval_samples_per_second: 1.252, interval_steps_per_second: 0.782, epoch: 19.0[0m
[32m[2022-09-19 20:22:54,758] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 20:22:54,758] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 20:22:54,758] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 20:22:54,758] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 20:22:54,758] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 20:22:56,098] [    INFO][0m - eval_loss: 1.5443594455718994, eval_accuracy: 0.86875, eval_runtime: 1.3397, eval_samples_per_second: 119.433, eval_steps_per_second: 7.465, epoch: 19.0[0m
[32m[2022-09-19 20:22:56,191] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-190[0m
[32m[2022-09-19 20:22:56,192] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 20:22:59,123] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-190/tokenizer_config.json[0m
[32m[2022-09-19 20:22:59,124] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-190/special_tokens_map.json[0m
[32m[2022-09-19 20:23:08,140] [    INFO][0m - loss: 0.01212243, learning_rate: 0.0, global_step: 200, interval_runtime: 13.3829, interval_samples_per_second: 1.196, interval_steps_per_second: 0.747, epoch: 20.0[0m
[32m[2022-09-19 20:23:08,140] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-19 20:23:08,140] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-19 20:23:08,141] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 20:23:08,141] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 20:23:08,141] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-19 20:23:09,926] [    INFO][0m - eval_loss: 1.545300841331482, eval_accuracy: 0.86875, eval_runtime: 1.3395, eval_samples_per_second: 119.448, eval_steps_per_second: 7.465, epoch: 20.0[0m
[32m[2022-09-19 20:23:09,926] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-200[0m
[32m[2022-09-19 20:23:09,926] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 20:23:12,520] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-200/tokenizer_config.json[0m
[32m[2022-09-19 20:23:12,520] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-200/special_tokens_map.json[0m
[32m[2022-09-19 20:23:18,045] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-19 20:23:18,045] [    INFO][0m - Loading best model from ./checkpoints_eprstmt/checkpoint-40 (score: 0.9).[0m
[32m[2022-09-19 20:23:19,384] [    INFO][0m - train_runtime: 271.438, train_samples_per_second: 11.789, train_steps_per_second: 0.737, train_loss: 0.11070164649550861, epoch: 20.0[0m
[32m[2022-09-19 20:23:19,437] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/[0m
[32m[2022-09-19 20:23:19,438] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-19 20:23:21,909] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/tokenizer_config.json[0m
[32m[2022-09-19 20:23:21,909] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/special_tokens_map.json[0m
[32m[2022-09-19 20:23:21,911] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-19 20:23:21,911] [    INFO][0m -   epoch                    =       20.0[0m
[32m[2022-09-19 20:23:21,911] [    INFO][0m -   train_loss               =     0.1107[0m
[32m[2022-09-19 20:23:21,911] [    INFO][0m -   train_runtime            = 0:04:31.43[0m
[32m[2022-09-19 20:23:21,911] [    INFO][0m -   train_samples_per_second =     11.789[0m
[32m[2022-09-19 20:23:21,911] [    INFO][0m -   train_steps_per_second   =      0.737[0m
[32m[2022-09-19 20:23:21,914] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-19 20:23:21,914] [    INFO][0m -   Num examples = 610[0m
[32m[2022-09-19 20:23:21,914] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 20:23:21,914] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 20:23:21,914] [    INFO][0m -   Total prediction steps = 39[0m
[32m[2022-09-19 20:23:27,190] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-19 20:23:27,190] [    INFO][0m -   test_accuracy           =     0.8967[0m
[32m[2022-09-19 20:23:27,191] [    INFO][0m -   test_loss               =     0.6002[0m
[32m[2022-09-19 20:23:27,191] [    INFO][0m -   test_runtime            = 0:00:05.27[0m
[32m[2022-09-19 20:23:27,191] [    INFO][0m -   test_samples_per_second =    115.609[0m
[32m[2022-09-19 20:23:27,191] [    INFO][0m -   test_steps_per_second   =      7.391[0m
[32m[2022-09-19 20:23:27,191] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-19 20:23:27,191] [    INFO][0m -   Num examples = 753[0m
[32m[2022-09-19 20:23:27,191] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-19 20:23:27,191] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-19 20:23:27,191] [    INFO][0m -   Total prediction steps = 48[0m
[32m[2022-09-19 20:23:34,403] [    INFO][0m - Predictions for eprstmt saved to ./fewclue_submit_examples.[0m
{
  "labels": 0,
  "text_a": "\u7269\u6d41\u5f88\u5feb\uff0c\u65e9\u4e0a\u4e0b\u5355\uff0c\u4e0b\u5348\u5c31\u5230\u4e86\u3002\u5305\u88c5\u4e5f\u5f88\u9ad8\u6863\u3002\u5c31\u662f\u8033\u673a\u97f3\u8d28\u5f88\u5dee\uff0c\u7172\u4e86\u4e00\u767e\u591a\u5c0f\u65f6\uff0c\u97f3\u8d28\u548c\u540c\u4e8b\u7684\u4e00\u767e\u591a\u5143\u7684\u8033\u673a\u5dee\u4e0d\u591a\uff0c1580\u5143\u4e70\u8fd9\u8033\u673a\u4e8f\u5927\u4e86\u3002",
  "text_b": "",
  "uid": 0
}

Prediction done.
