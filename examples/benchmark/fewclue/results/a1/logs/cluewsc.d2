[32m[2022-08-25 15:09:10,760] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-25 15:09:10,760] [    INFO][0m - ============================================================[0m
[32m[2022-08-25 15:09:10,761] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-25 15:09:10,761] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-25 15:09:10,761] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-25 15:09:10,761] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-25 15:09:10,761] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-25 15:09:10,761] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-08-25 15:09:10,761] [    INFO][0m - [0m
[32m[2022-08-25 15:09:10,761] [    INFO][0m - ============================================================[0m
[32m[2022-08-25 15:09:10,761] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-25 15:09:10,761] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-25 15:09:10,761] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-25 15:09:10,761] [    INFO][0m - prompt                        :{'mask'}{'soft':'ÂêàÁêÜ„ÄÇ'}{'text':'text_a'}[0m
[32m[2022-08-25 15:09:10,762] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-25 15:09:10,762] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-25 15:09:10,762] [    INFO][0m - task_name                     :cluewsc[0m
[32m[2022-08-25 15:09:10,762] [    INFO][0m - [0m
[32m[2022-08-25 15:09:10,762] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0825 15:09:10.763864 49764 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0825 15:09:10.768752 49764 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-25 15:09:13,705] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-08-25 15:09:13,729] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-08-25 15:09:13,730] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[33m[2022-08-25 15:09:13,737] [ WARNING][0m - Encoder has already set as lstm, change `prompt_encoder` will reset parameters.[0m
[32m[2022-08-25 15:09:13,745] [    INFO][0m - Using template: [{'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'soft': 'Âêà'}, {'add_prefix_space': '', 'soft': 'ÁêÜ'}, {'add_prefix_space': '', 'soft': '„ÄÇ'}, {'add_prefix_space': '', 'text': 'text_a'}][0m
2022-08-25 15:09:13,750 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-25 15:09:13,851] [    INFO][0m - ============================================================[0m
[32m[2022-08-25 15:09:13,851] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-25 15:09:13,851] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-25 15:09:13,851] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-25 15:09:13,851] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-25 15:09:13,852] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-25 15:09:13,852] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-25 15:09:13,852] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-25 15:09:13,852] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-25 15:09:13,852] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-25 15:09:13,852] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-25 15:09:13,852] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-25 15:09:13,852] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-25 15:09:13,852] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-25 15:09:13,852] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-25 15:09:13,852] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-25 15:09:13,852] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-25 15:09:13,852] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-25 15:09:13,853] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-25 15:09:13,853] [    INFO][0m - eval_steps                    :10[0m
[32m[2022-08-25 15:09:13,853] [    INFO][0m - evaluation_strategy           :IntervalStrategy.EPOCH[0m
[32m[2022-08-25 15:09:13,853] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-25 15:09:13,853] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-25 15:09:13,853] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-25 15:09:13,853] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-25 15:09:13,853] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-25 15:09:13,853] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-25 15:09:13,853] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-25 15:09:13,853] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-25 15:09:13,853] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-25 15:09:13,853] [    INFO][0m - learning_rate                 :1e-05[0m
[32m[2022-08-25 15:09:13,854] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-25 15:09:13,854] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-25 15:09:13,854] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-25 15:09:13,854] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-25 15:09:13,854] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-25 15:09:13,854] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-25 15:09:13,854] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug25_15-09-10_instance-3bwob41y-01[0m
[32m[2022-08-25 15:09:13,854] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-25 15:09:13,854] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-25 15:09:13,854] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-25 15:09:13,854] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-25 15:09:13,854] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-25 15:09:13,854] [    INFO][0m - max_seq_length                :128[0m
[32m[2022-08-25 15:09:13,854] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-25 15:09:13,855] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-25 15:09:13,855] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-25 15:09:13,855] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-25 15:09:13,855] [    INFO][0m - num_train_epochs              :20.0[0m
[32m[2022-08-25 15:09:13,855] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-25 15:09:13,855] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-25 15:09:13,855] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-25 15:09:13,855] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-25 15:09:13,855] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-25 15:09:13,855] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-25 15:09:13,855] [    INFO][0m - per_device_train_batch_size   :4[0m
[32m[2022-08-25 15:09:13,855] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-25 15:09:13,855] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-25 15:09:13,856] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-25 15:09:13,856] [    INFO][0m - ppt_learning_rate             :0.0001[0m
[32m[2022-08-25 15:09:13,856] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-25 15:09:13,856] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-25 15:09:13,856] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-25 15:09:13,856] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-25 15:09:13,856] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-25 15:09:13,856] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-25 15:09:13,856] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-25 15:09:13,856] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-25 15:09:13,856] [    INFO][0m - save_steps                    :500[0m
[32m[2022-08-25 15:09:13,856] [    INFO][0m - save_strategy                 :IntervalStrategy.EPOCH[0m
[32m[2022-08-25 15:09:13,856] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-25 15:09:13,856] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-25 15:09:13,857] [    INFO][0m - seed                          :42[0m
[32m[2022-08-25 15:09:13,857] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-25 15:09:13,857] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-25 15:09:13,857] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-25 15:09:13,857] [    INFO][0m - train_batch_size              :4[0m
[32m[2022-08-25 15:09:13,857] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-25 15:09:13,857] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-25 15:09:13,857] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-25 15:09:13,857] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-25 15:09:13,857] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-25 15:09:13,857] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-25 15:09:13,857] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-25 15:09:13,857] [    INFO][0m - [0m
[32m[2022-08-25 15:09:13,859] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-25 15:09:13,859] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-25 15:09:13,860] [    INFO][0m -   Num Epochs = 20[0m
[32m[2022-08-25 15:09:13,860] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2022-08-25 15:09:13,860] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 4[0m
[32m[2022-08-25 15:09:13,860] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-25 15:09:13,860] [    INFO][0m -   Total optimization steps = 800.0[0m
[32m[2022-08-25 15:09:13,860] [    INFO][0m -   Total num train samples = 3200[0m
[32m[2022-08-25 15:09:16,078] [    INFO][0m - loss: 0.87018137, learning_rate: 9.875000000000001e-06, global_step: 10, interval_runtime: 2.2168, interval_samples_per_second: 1.804, interval_steps_per_second: 4.511, epoch: 0.25[0m
[32m[2022-08-25 15:09:16,870] [    INFO][0m - loss: 0.74827428, learning_rate: 9.75e-06, global_step: 20, interval_runtime: 0.7919, interval_samples_per_second: 5.051, interval_steps_per_second: 12.627, epoch: 0.5[0m
[32m[2022-08-25 15:09:17,698] [    INFO][0m - loss: 0.85665579, learning_rate: 9.625e-06, global_step: 30, interval_runtime: 0.8283, interval_samples_per_second: 4.829, interval_steps_per_second: 12.073, epoch: 0.75[0m
[32m[2022-08-25 15:09:18,427] [    INFO][0m - loss: 0.73921919, learning_rate: 9.5e-06, global_step: 40, interval_runtime: 0.7285, interval_samples_per_second: 5.491, interval_steps_per_second: 13.727, epoch: 1.0[0m
[32m[2022-08-25 15:09:18,428] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 15:09:18,428] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-25 15:09:18,428] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 15:09:18,428] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 15:09:18,428] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-25 15:09:19,054] [    INFO][0m - eval_loss: 0.6950628161430359, eval_accuracy: 0.5408805031446541, eval_runtime: 0.6257, eval_samples_per_second: 254.124, eval_steps_per_second: 7.991, epoch: 1.0[0m
[32m[2022-08-25 15:09:19,054] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-40[0m
[32m[2022-08-25 15:09:19,055] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 15:09:22,825] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-40/tokenizer_config.json[0m
[32m[2022-08-25 15:09:22,826] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-40/special_tokens_map.json[0m
[32m[2022-08-25 15:09:27,732] [    INFO][0m - loss: 0.67071786, learning_rate: 9.375000000000001e-06, global_step: 50, interval_runtime: 9.3054, interval_samples_per_second: 0.43, interval_steps_per_second: 1.075, epoch: 1.25[0m
[32m[2022-08-25 15:09:28,607] [    INFO][0m - loss: 0.71578469, learning_rate: 9.250000000000001e-06, global_step: 60, interval_runtime: 0.8753, interval_samples_per_second: 4.57, interval_steps_per_second: 11.424, epoch: 1.5[0m
[32m[2022-08-25 15:09:29,438] [    INFO][0m - loss: 0.71173892, learning_rate: 9.125e-06, global_step: 70, interval_runtime: 0.8311, interval_samples_per_second: 4.813, interval_steps_per_second: 12.033, epoch: 1.75[0m
[32m[2022-08-25 15:09:30,235] [    INFO][0m - loss: 0.81129665, learning_rate: 9e-06, global_step: 80, interval_runtime: 0.7973, interval_samples_per_second: 5.017, interval_steps_per_second: 12.542, epoch: 2.0[0m
[32m[2022-08-25 15:09:30,236] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 15:09:30,236] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-25 15:09:30,236] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 15:09:30,237] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 15:09:30,237] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-25 15:09:30,952] [    INFO][0m - eval_loss: 0.7889218926429749, eval_accuracy: 0.5031446540880503, eval_runtime: 0.7155, eval_samples_per_second: 222.226, eval_steps_per_second: 6.988, epoch: 2.0[0m
[32m[2022-08-25 15:09:30,953] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-80[0m
[32m[2022-08-25 15:09:30,953] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 15:09:34,400] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-80/tokenizer_config.json[0m
[32m[2022-08-25 15:09:34,401] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-80/special_tokens_map.json[0m
[32m[2022-08-25 15:09:39,583] [    INFO][0m - loss: 0.70530853, learning_rate: 8.875e-06, global_step: 90, interval_runtime: 9.3464, interval_samples_per_second: 0.428, interval_steps_per_second: 1.07, epoch: 2.25[0m
[32m[2022-08-25 15:09:40,473] [    INFO][0m - loss: 0.64629326, learning_rate: 8.750000000000001e-06, global_step: 100, interval_runtime: 0.8907, interval_samples_per_second: 4.491, interval_steps_per_second: 11.227, epoch: 2.5[0m
[32m[2022-08-25 15:09:41,323] [    INFO][0m - loss: 0.68820271, learning_rate: 8.625000000000001e-06, global_step: 110, interval_runtime: 0.8508, interval_samples_per_second: 4.701, interval_steps_per_second: 11.753, epoch: 2.75[0m
[32m[2022-08-25 15:09:42,176] [    INFO][0m - loss: 0.71309962, learning_rate: 8.5e-06, global_step: 120, interval_runtime: 0.8525, interval_samples_per_second: 4.692, interval_steps_per_second: 11.731, epoch: 3.0[0m
[32m[2022-08-25 15:09:42,177] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 15:09:42,177] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-25 15:09:42,177] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 15:09:42,178] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 15:09:42,178] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-25 15:09:42,966] [    INFO][0m - eval_loss: 0.8186473846435547, eval_accuracy: 0.4968553459119497, eval_runtime: 0.7864, eval_samples_per_second: 202.191, eval_steps_per_second: 6.358, epoch: 3.0[0m
[32m[2022-08-25 15:09:42,966] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-120[0m
[32m[2022-08-25 15:09:42,966] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 15:09:46,486] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-120/tokenizer_config.json[0m
[32m[2022-08-25 15:09:46,486] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-120/special_tokens_map.json[0m
[32m[2022-08-25 15:09:51,577] [    INFO][0m - loss: 0.60742388, learning_rate: 8.375e-06, global_step: 130, interval_runtime: 9.4011, interval_samples_per_second: 0.425, interval_steps_per_second: 1.064, epoch: 3.25[0m
[32m[2022-08-25 15:09:52,523] [    INFO][0m - loss: 0.67129989, learning_rate: 8.25e-06, global_step: 140, interval_runtime: 0.9464, interval_samples_per_second: 4.227, interval_steps_per_second: 10.567, epoch: 3.5[0m
[32m[2022-08-25 15:09:53,486] [    INFO][0m - loss: 0.72239842, learning_rate: 8.125000000000001e-06, global_step: 150, interval_runtime: 0.9625, interval_samples_per_second: 4.156, interval_steps_per_second: 10.39, epoch: 3.75[0m
[32m[2022-08-25 15:09:54,324] [    INFO][0m - loss: 0.63464203, learning_rate: 8.000000000000001e-06, global_step: 160, interval_runtime: 0.8376, interval_samples_per_second: 4.775, interval_steps_per_second: 11.939, epoch: 4.0[0m
[32m[2022-08-25 15:09:54,324] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 15:09:54,325] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-25 15:09:54,325] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 15:09:54,325] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 15:09:54,325] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-25 15:09:55,204] [    INFO][0m - eval_loss: 0.6978967785835266, eval_accuracy: 0.5345911949685535, eval_runtime: 0.8785, eval_samples_per_second: 180.991, eval_steps_per_second: 5.692, epoch: 4.0[0m
[32m[2022-08-25 15:09:55,204] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-160[0m
[32m[2022-08-25 15:09:55,204] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 15:09:58,967] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-160/tokenizer_config.json[0m
[32m[2022-08-25 15:09:58,968] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-160/special_tokens_map.json[0m
[32m[2022-08-25 15:10:03,857] [    INFO][0m - loss: 0.62036242, learning_rate: 7.875e-06, global_step: 170, interval_runtime: 9.533, interval_samples_per_second: 0.42, interval_steps_per_second: 1.049, epoch: 4.25[0m
[32m[2022-08-25 15:10:04,910] [    INFO][0m - loss: 0.63272581, learning_rate: 7.75e-06, global_step: 180, interval_runtime: 1.054, interval_samples_per_second: 3.795, interval_steps_per_second: 9.488, epoch: 4.5[0m
[32m[2022-08-25 15:10:05,904] [    INFO][0m - loss: 0.50574193, learning_rate: 7.625e-06, global_step: 190, interval_runtime: 0.9934, interval_samples_per_second: 4.027, interval_steps_per_second: 10.067, epoch: 4.75[0m
[32m[2022-08-25 15:10:06,784] [    INFO][0m - loss: 0.64928741, learning_rate: 7.500000000000001e-06, global_step: 200, interval_runtime: 0.8798, interval_samples_per_second: 4.546, interval_steps_per_second: 11.366, epoch: 5.0[0m
[32m[2022-08-25 15:10:06,784] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 15:10:06,785] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-25 15:10:06,785] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 15:10:06,785] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 15:10:06,785] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-25 15:10:07,735] [    INFO][0m - eval_loss: 0.9678296446800232, eval_accuracy: 0.5094339622641509, eval_runtime: 0.9497, eval_samples_per_second: 167.413, eval_steps_per_second: 5.265, epoch: 5.0[0m
[32m[2022-08-25 15:10:07,735] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-25 15:10:07,735] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 15:10:11,024] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-25 15:10:11,024] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-25 15:10:15,095] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-25 15:10:15,095] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-40 (score: 0.5408805031446541).[0m
[32m[2022-08-25 15:10:16,316] [    INFO][0m - train_runtime: 62.4552, train_samples_per_second: 51.237, train_steps_per_second: 12.809, train_loss: 0.6960327339172363, epoch: 5.0[0m
[32m[2022-08-25 15:10:16,317] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-25 15:10:16,318] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 15:10:19,972] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-25 15:10:19,973] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-25 15:10:19,974] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-25 15:10:19,975] [    INFO][0m -   epoch                    =        5.0[0m
[32m[2022-08-25 15:10:19,975] [    INFO][0m -   train_loss               =      0.696[0m
[32m[2022-08-25 15:10:19,975] [    INFO][0m -   train_runtime            = 0:01:02.45[0m
[32m[2022-08-25 15:10:19,975] [    INFO][0m -   train_samples_per_second =     51.237[0m
[32m[2022-08-25 15:10:19,975] [    INFO][0m -   train_steps_per_second   =     12.809[0m
[32m[2022-08-25 15:10:19,977] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-25 15:10:19,977] [    INFO][0m -   Num examples = 976[0m
[32m[2022-08-25 15:10:19,977] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 15:10:19,977] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 15:10:19,977] [    INFO][0m -   Total prediction steps = 31[0m
[32m[2022-08-25 15:10:25,891] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-25 15:10:25,892] [    INFO][0m -   test_accuracy           =     0.4908[0m
[32m[2022-08-25 15:10:25,892] [    INFO][0m -   test_loss               =     0.7237[0m
[32m[2022-08-25 15:10:25,892] [    INFO][0m -   test_runtime            = 0:00:05.91[0m
[32m[2022-08-25 15:10:25,892] [    INFO][0m -   test_samples_per_second =    165.036[0m
[32m[2022-08-25 15:10:25,892] [    INFO][0m -   test_steps_per_second   =      5.242[0m
[32m[2022-08-25 15:10:25,892] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-25 15:10:25,893] [    INFO][0m -   Num examples = 290[0m
[32m[2022-08-25 15:10:25,893] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 15:10:25,893] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 15:10:25,893] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-08-25 15:10:27,959] [    INFO][0m - Predictions for cluewscf saved to ./fewclue_submit_examples.[0m
