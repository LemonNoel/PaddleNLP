[33m[2022-08-25 20:33:13,567] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-25 20:33:13,567] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-25 20:33:13,568] [    INFO][0m - ============================================================[0m
[32m[2022-08-25 20:33:13,568] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-25 20:33:13,568] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-25 20:33:13,568] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-25 20:33:13,568] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-25 20:33:13,568] [    INFO][0m - early_stop_patience           :6[0m
[32m[2022-08-25 20:33:13,568] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-25 20:33:13,568] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-08-25 20:33:13,568] [    INFO][0m - [0m
[32m[2022-08-25 20:33:13,568] [    INFO][0m - ============================================================[0m
[32m[2022-08-25 20:33:13,569] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-25 20:33:13,569] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-25 20:33:13,569] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-25 20:33:13,569] [    INFO][0m - prompt                        :{'text':'text_a'}{'soft':'è¿™æ¬¾åº”ç”¨å±žäºŽ'}{'mask'}{'mask'}{'soft':'ç±»åˆ«.'}[0m
[32m[2022-08-25 20:33:13,569] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-25 20:33:13,569] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-25 20:33:13,569] [    INFO][0m - task_name                     :iflytek[0m
[32m[2022-08-25 20:33:13,569] [    INFO][0m - [0m
[32m[2022-08-25 20:33:13,569] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0825 20:33:13.570883 66421 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0825 20:33:13.574767 66421 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-25 20:33:16,508] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-08-25 20:33:16,533] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-08-25 20:33:16,533] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[33m[2022-08-25 20:33:16,541] [ WARNING][0m - Encoder has already set as lstm, change `prompt_encoder` will reset parameters.[0m
[32m[2022-08-25 20:33:16,552] [    INFO][0m - Using template: [{'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'soft': 'è¿™'}, {'add_prefix_space': '', 'soft': 'æ¬¾'}, {'add_prefix_space': '', 'soft': 'åº”'}, {'add_prefix_space': '', 'soft': 'ç”¨'}, {'add_prefix_space': '', 'soft': 'å±ž'}, {'add_prefix_space': '', 'soft': 'äºŽ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'soft': 'ç±»'}, {'add_prefix_space': '', 'soft': 'åˆ«'}, {'add_prefix_space': '', 'soft': '.'}][0m
2022-08-25 20:33:16,576 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-25 20:33:16,755] [    INFO][0m - ============================================================[0m
[32m[2022-08-25 20:33:16,755] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-25 20:33:16,755] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-25 20:33:16,756] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-25 20:33:16,756] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-25 20:33:16,756] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-25 20:33:16,756] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-25 20:33:16,756] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-25 20:33:16,756] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-25 20:33:16,756] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-25 20:33:16,756] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-25 20:33:16,756] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-25 20:33:16,756] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-25 20:33:16,757] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-25 20:33:16,757] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-25 20:33:16,757] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-25 20:33:16,757] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-25 20:33:16,757] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-25 20:33:16,757] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-25 20:33:16,757] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-25 20:33:16,757] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-25 20:33:16,757] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-25 20:33:16,757] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-25 20:33:16,757] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-25 20:33:16,757] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-25 20:33:16,758] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-25 20:33:16,758] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-25 20:33:16,758] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-25 20:33:16,758] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-25 20:33:16,758] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-25 20:33:16,758] [    INFO][0m - learning_rate                 :1e-05[0m
[32m[2022-08-25 20:33:16,758] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-25 20:33:16,758] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-25 20:33:16,758] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-25 20:33:16,758] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-25 20:33:16,758] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-25 20:33:16,758] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-25 20:33:16,758] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug25_20-33-13_instance-3bwob41y-01[0m
[32m[2022-08-25 20:33:16,758] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-25 20:33:16,759] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-25 20:33:16,759] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-25 20:33:16,759] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-25 20:33:16,759] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-25 20:33:16,759] [    INFO][0m - max_seq_length                :320[0m
[32m[2022-08-25 20:33:16,759] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-25 20:33:16,759] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-25 20:33:16,759] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-25 20:33:16,759] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-25 20:33:16,759] [    INFO][0m - num_train_epochs              :20.0[0m
[32m[2022-08-25 20:33:16,759] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-25 20:33:16,759] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-25 20:33:16,760] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-25 20:33:16,760] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-25 20:33:16,760] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-25 20:33:16,760] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-25 20:33:16,760] [    INFO][0m - per_device_train_batch_size   :4[0m
[32m[2022-08-25 20:33:16,760] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-25 20:33:16,760] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-25 20:33:16,760] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-25 20:33:16,760] [    INFO][0m - ppt_learning_rate             :0.0001[0m
[32m[2022-08-25 20:33:16,760] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-25 20:33:16,760] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-25 20:33:16,760] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-25 20:33:16,760] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-25 20:33:16,760] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-25 20:33:16,761] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-25 20:33:16,761] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-25 20:33:16,761] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-25 20:33:16,761] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-25 20:33:16,761] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-25 20:33:16,761] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-25 20:33:16,761] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-25 20:33:16,761] [    INFO][0m - seed                          :42[0m
[32m[2022-08-25 20:33:16,761] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-25 20:33:16,761] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-25 20:33:16,761] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-25 20:33:16,761] [    INFO][0m - train_batch_size              :4[0m
[32m[2022-08-25 20:33:16,762] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-25 20:33:16,762] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-25 20:33:16,762] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-25 20:33:16,762] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-25 20:33:16,762] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-25 20:33:16,762] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-25 20:33:16,762] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-25 20:33:16,762] [    INFO][0m - [0m
[32m[2022-08-25 20:33:16,764] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-25 20:33:16,764] [    INFO][0m -   Num examples = 3024[0m
[32m[2022-08-25 20:33:16,764] [    INFO][0m -   Num Epochs = 20[0m
[32m[2022-08-25 20:33:16,765] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2022-08-25 20:33:16,765] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 4[0m
[32m[2022-08-25 20:33:16,765] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-25 20:33:16,765] [    INFO][0m -   Total optimization steps = 15120.0[0m
[32m[2022-08-25 20:33:16,765] [    INFO][0m -   Total num train samples = 60480[0m
[32m[2022-08-25 20:33:19,814] [    INFO][0m - loss: 4.40574875, learning_rate: 9.993386243386244e-06, global_step: 10, interval_runtime: 3.0479, interval_samples_per_second: 1.312, interval_steps_per_second: 3.281, epoch: 0.0132[0m
[32m[2022-08-25 20:33:21,153] [    INFO][0m - loss: 4.67992973, learning_rate: 9.986772486772488e-06, global_step: 20, interval_runtime: 1.339, interval_samples_per_second: 2.987, interval_steps_per_second: 7.468, epoch: 0.0265[0m
[32m[2022-08-25 20:33:22,494] [    INFO][0m - loss: 3.85124588, learning_rate: 9.980158730158731e-06, global_step: 30, interval_runtime: 1.3416, interval_samples_per_second: 2.981, interval_steps_per_second: 7.454, epoch: 0.0397[0m
[32m[2022-08-25 20:33:23,846] [    INFO][0m - loss: 3.67580605, learning_rate: 9.973544973544974e-06, global_step: 40, interval_runtime: 1.3518, interval_samples_per_second: 2.959, interval_steps_per_second: 7.398, epoch: 0.0529[0m
[32m[2022-08-25 20:33:25,226] [    INFO][0m - loss: 3.28475952, learning_rate: 9.966931216931219e-06, global_step: 50, interval_runtime: 1.3791, interval_samples_per_second: 2.9, interval_steps_per_second: 7.251, epoch: 0.0661[0m
[32m[2022-08-25 20:33:26,626] [    INFO][0m - loss: 3.55989227, learning_rate: 9.960317460317462e-06, global_step: 60, interval_runtime: 1.4004, interval_samples_per_second: 2.856, interval_steps_per_second: 7.141, epoch: 0.0794[0m
[32m[2022-08-25 20:33:28,067] [    INFO][0m - loss: 2.98831615, learning_rate: 9.953703703703704e-06, global_step: 70, interval_runtime: 1.4412, interval_samples_per_second: 2.775, interval_steps_per_second: 6.938, epoch: 0.0926[0m
[32m[2022-08-25 20:33:29,467] [    INFO][0m - loss: 3.0222496, learning_rate: 9.947089947089947e-06, global_step: 80, interval_runtime: 1.4004, interval_samples_per_second: 2.856, interval_steps_per_second: 7.141, epoch: 0.1058[0m
[32m[2022-08-25 20:33:30,919] [    INFO][0m - loss: 2.63685093, learning_rate: 9.940476190476192e-06, global_step: 90, interval_runtime: 1.4511, interval_samples_per_second: 2.757, interval_steps_per_second: 6.891, epoch: 0.119[0m
[32m[2022-08-25 20:33:32,368] [    INFO][0m - loss: 3.24616356, learning_rate: 9.933862433862435e-06, global_step: 100, interval_runtime: 1.4492, interval_samples_per_second: 2.76, interval_steps_per_second: 6.9, epoch: 0.1323[0m
[32m[2022-08-25 20:33:32,368] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 20:33:32,369] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-25 20:33:32,369] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 20:33:32,369] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 20:33:32,369] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-25 20:33:48,385] [    INFO][0m - eval_loss: 2.6350035667419434, eval_accuracy: 0.3860160233066278, eval_runtime: 16.0158, eval_samples_per_second: 85.728, eval_steps_per_second: 2.685, epoch: 0.1323[0m
[32m[2022-08-25 20:33:48,386] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-25 20:33:48,386] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 20:33:51,564] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-25 20:33:51,565] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-25 20:33:57,335] [    INFO][0m - loss: 2.61215019, learning_rate: 9.927248677248678e-06, global_step: 110, interval_runtime: 24.9667, interval_samples_per_second: 0.16, interval_steps_per_second: 0.401, epoch: 0.1455[0m
[32m[2022-08-25 20:33:58,917] [    INFO][0m - loss: 3.36532135, learning_rate: 9.920634920634922e-06, global_step: 120, interval_runtime: 1.5829, interval_samples_per_second: 2.527, interval_steps_per_second: 6.318, epoch: 0.1587[0m
[32m[2022-08-25 20:34:00,456] [    INFO][0m - loss: 3.01327095, learning_rate: 9.914021164021165e-06, global_step: 130, interval_runtime: 1.5391, interval_samples_per_second: 2.599, interval_steps_per_second: 6.497, epoch: 0.172[0m
[32m[2022-08-25 20:34:02,043] [    INFO][0m - loss: 3.25193825, learning_rate: 9.907407407407408e-06, global_step: 140, interval_runtime: 1.5865, interval_samples_per_second: 2.521, interval_steps_per_second: 6.303, epoch: 0.1852[0m
[32m[2022-08-25 20:34:03,612] [    INFO][0m - loss: 2.83918076, learning_rate: 9.900793650793653e-06, global_step: 150, interval_runtime: 1.5691, interval_samples_per_second: 2.549, interval_steps_per_second: 6.373, epoch: 0.1984[0m
[32m[2022-08-25 20:34:05,210] [    INFO][0m - loss: 2.11700439, learning_rate: 9.894179894179896e-06, global_step: 160, interval_runtime: 1.5984, interval_samples_per_second: 2.502, interval_steps_per_second: 6.256, epoch: 0.2116[0m
[32m[2022-08-25 20:34:06,805] [    INFO][0m - loss: 1.99530296, learning_rate: 9.887566137566138e-06, global_step: 170, interval_runtime: 1.595, interval_samples_per_second: 2.508, interval_steps_per_second: 6.27, epoch: 0.2249[0m
[32m[2022-08-25 20:34:08,429] [    INFO][0m - loss: 2.87757816, learning_rate: 9.880952380952381e-06, global_step: 180, interval_runtime: 1.6242, interval_samples_per_second: 2.463, interval_steps_per_second: 6.157, epoch: 0.2381[0m
[32m[2022-08-25 20:34:10,088] [    INFO][0m - loss: 2.79824104, learning_rate: 9.874338624338626e-06, global_step: 190, interval_runtime: 1.6576, interval_samples_per_second: 2.413, interval_steps_per_second: 6.033, epoch: 0.2513[0m
[32m[2022-08-25 20:34:11,816] [    INFO][0m - loss: 2.29651508, learning_rate: 9.867724867724869e-06, global_step: 200, interval_runtime: 1.7292, interval_samples_per_second: 2.313, interval_steps_per_second: 5.783, epoch: 0.2646[0m
[32m[2022-08-25 20:34:11,817] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 20:34:11,817] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-25 20:34:11,817] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 20:34:11,817] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 20:34:11,818] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-25 20:34:32,621] [    INFO][0m - eval_loss: 2.2185287475585938, eval_accuracy: 0.471959213401311, eval_runtime: 20.8029, eval_samples_per_second: 66.0, eval_steps_per_second: 2.067, epoch: 0.2646[0m
[32m[2022-08-25 20:34:32,622] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-25 20:34:32,622] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 20:34:35,910] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-25 20:34:35,910] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-25 20:34:41,442] [    INFO][0m - loss: 1.98468933, learning_rate: 9.861111111111112e-06, global_step: 210, interval_runtime: 29.6258, interval_samples_per_second: 0.135, interval_steps_per_second: 0.338, epoch: 0.2778[0m
[32m[2022-08-25 20:34:43,288] [    INFO][0m - loss: 2.5616354, learning_rate: 9.854497354497355e-06, global_step: 220, interval_runtime: 1.8461, interval_samples_per_second: 2.167, interval_steps_per_second: 5.417, epoch: 0.291[0m
[32m[2022-08-25 20:34:44,946] [    INFO][0m - loss: 2.43290367, learning_rate: 9.8478835978836e-06, global_step: 230, interval_runtime: 1.6573, interval_samples_per_second: 2.413, interval_steps_per_second: 6.034, epoch: 0.3042[0m
[32m[2022-08-25 20:34:46,652] [    INFO][0m - loss: 2.3067461, learning_rate: 9.841269841269842e-06, global_step: 240, interval_runtime: 1.7069, interval_samples_per_second: 2.343, interval_steps_per_second: 5.859, epoch: 0.3175[0m
[32m[2022-08-25 20:34:48,363] [    INFO][0m - loss: 2.22228508, learning_rate: 9.834656084656085e-06, global_step: 250, interval_runtime: 1.71, interval_samples_per_second: 2.339, interval_steps_per_second: 5.848, epoch: 0.3307[0m
[32m[2022-08-25 20:34:50,041] [    INFO][0m - loss: 2.32484474, learning_rate: 9.828042328042328e-06, global_step: 260, interval_runtime: 1.6779, interval_samples_per_second: 2.384, interval_steps_per_second: 5.96, epoch: 0.3439[0m
[32m[2022-08-25 20:34:51,759] [    INFO][0m - loss: 1.93770161, learning_rate: 9.821428571428573e-06, global_step: 270, interval_runtime: 1.7187, interval_samples_per_second: 2.327, interval_steps_per_second: 5.818, epoch: 0.3571[0m
[32m[2022-08-25 20:34:53,573] [    INFO][0m - loss: 2.18891449, learning_rate: 9.814814814814815e-06, global_step: 280, interval_runtime: 1.8139, interval_samples_per_second: 2.205, interval_steps_per_second: 5.513, epoch: 0.3704[0m
[32m[2022-08-25 20:34:55,283] [    INFO][0m - loss: 3.12132206, learning_rate: 9.808201058201058e-06, global_step: 290, interval_runtime: 1.7093, interval_samples_per_second: 2.34, interval_steps_per_second: 5.85, epoch: 0.3836[0m
[32m[2022-08-25 20:34:57,035] [    INFO][0m - loss: 1.95018387, learning_rate: 9.801587301587301e-06, global_step: 300, interval_runtime: 1.7526, interval_samples_per_second: 2.282, interval_steps_per_second: 5.706, epoch: 0.3968[0m
[32m[2022-08-25 20:34:57,035] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 20:34:57,036] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-25 20:34:57,036] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 20:34:57,036] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 20:34:57,036] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-25 20:35:22,376] [    INFO][0m - eval_loss: 2.0993034839630127, eval_accuracy: 0.4857975236707939, eval_runtime: 25.3392, eval_samples_per_second: 54.185, eval_steps_per_second: 1.697, epoch: 0.3968[0m
[32m[2022-08-25 20:35:22,376] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-25 20:35:22,376] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 20:35:24,058] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-25 20:35:24,058] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-25 20:35:28,001] [    INFO][0m - loss: 2.19761524, learning_rate: 9.794973544973546e-06, global_step: 310, interval_runtime: 30.9665, interval_samples_per_second: 0.129, interval_steps_per_second: 0.323, epoch: 0.4101[0m
[32m[2022-08-25 20:35:29,804] [    INFO][0m - loss: 2.6184864, learning_rate: 9.788359788359789e-06, global_step: 320, interval_runtime: 1.8027, interval_samples_per_second: 2.219, interval_steps_per_second: 5.547, epoch: 0.4233[0m
[32m[2022-08-25 20:35:31,642] [    INFO][0m - loss: 2.63224792, learning_rate: 9.781746031746032e-06, global_step: 330, interval_runtime: 1.838, interval_samples_per_second: 2.176, interval_steps_per_second: 5.441, epoch: 0.4365[0m
[32m[2022-08-25 20:35:33,536] [    INFO][0m - loss: 1.69509106, learning_rate: 9.775132275132276e-06, global_step: 340, interval_runtime: 1.8933, interval_samples_per_second: 2.113, interval_steps_per_second: 5.282, epoch: 0.4497[0m
[32m[2022-08-25 20:35:35,406] [    INFO][0m - loss: 3.08473549, learning_rate: 9.768518518518519e-06, global_step: 350, interval_runtime: 1.8704, interval_samples_per_second: 2.139, interval_steps_per_second: 5.346, epoch: 0.463[0m
[32m[2022-08-25 20:35:37,272] [    INFO][0m - loss: 2.18560314, learning_rate: 9.761904761904762e-06, global_step: 360, interval_runtime: 1.8656, interval_samples_per_second: 2.144, interval_steps_per_second: 5.36, epoch: 0.4762[0m
[32m[2022-08-25 20:35:39,236] [    INFO][0m - loss: 1.80363503, learning_rate: 9.755291005291007e-06, global_step: 370, interval_runtime: 1.9642, interval_samples_per_second: 2.036, interval_steps_per_second: 5.091, epoch: 0.4894[0m
[32m[2022-08-25 20:35:41,156] [    INFO][0m - loss: 2.3055994, learning_rate: 9.74867724867725e-06, global_step: 380, interval_runtime: 1.9202, interval_samples_per_second: 2.083, interval_steps_per_second: 5.208, epoch: 0.5026[0m
[32m[2022-08-25 20:35:43,121] [    INFO][0m - loss: 2.57150002, learning_rate: 9.742063492063492e-06, global_step: 390, interval_runtime: 1.9654, interval_samples_per_second: 2.035, interval_steps_per_second: 5.088, epoch: 0.5159[0m
[32m[2022-08-25 20:35:45,112] [    INFO][0m - loss: 1.96855202, learning_rate: 9.735449735449735e-06, global_step: 400, interval_runtime: 1.9902, interval_samples_per_second: 2.01, interval_steps_per_second: 5.025, epoch: 0.5291[0m
[32m[2022-08-25 20:35:45,112] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 20:35:45,112] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-25 20:35:45,112] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 20:35:45,112] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 20:35:45,112] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-25 20:36:15,289] [    INFO][0m - eval_loss: 2.0422587394714355, eval_accuracy: 0.4785142024763292, eval_runtime: 30.1761, eval_samples_per_second: 45.5, eval_steps_per_second: 1.425, epoch: 0.5291[0m
[32m[2022-08-25 20:36:15,290] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-25 20:36:15,290] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 20:36:18,722] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-25 20:36:18,723] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-25 20:36:24,703] [    INFO][0m - loss: 2.41814384, learning_rate: 9.72883597883598e-06, global_step: 410, interval_runtime: 39.5914, interval_samples_per_second: 0.101, interval_steps_per_second: 0.253, epoch: 0.5423[0m
[32m[2022-08-25 20:36:26,801] [    INFO][0m - loss: 1.98122005, learning_rate: 9.722222222222223e-06, global_step: 420, interval_runtime: 2.0974, interval_samples_per_second: 1.907, interval_steps_per_second: 4.768, epoch: 0.5556[0m
[32m[2022-08-25 20:36:28,859] [    INFO][0m - loss: 2.26456795, learning_rate: 9.715608465608466e-06, global_step: 430, interval_runtime: 2.0586, interval_samples_per_second: 1.943, interval_steps_per_second: 4.858, epoch: 0.5688[0m
[32m[2022-08-25 20:36:30,970] [    INFO][0m - loss: 1.93619709, learning_rate: 9.70899470899471e-06, global_step: 440, interval_runtime: 2.1105, interval_samples_per_second: 1.895, interval_steps_per_second: 4.738, epoch: 0.582[0m
[32m[2022-08-25 20:36:33,063] [    INFO][0m - loss: 1.99148636, learning_rate: 9.702380952380953e-06, global_step: 450, interval_runtime: 2.0933, interval_samples_per_second: 1.911, interval_steps_per_second: 4.777, epoch: 0.5952[0m
[32m[2022-08-25 20:36:35,180] [    INFO][0m - loss: 2.1017849, learning_rate: 9.695767195767196e-06, global_step: 460, interval_runtime: 2.1175, interval_samples_per_second: 1.889, interval_steps_per_second: 4.722, epoch: 0.6085[0m
[32m[2022-08-25 20:36:37,320] [    INFO][0m - loss: 2.11719627, learning_rate: 9.68915343915344e-06, global_step: 470, interval_runtime: 2.14, interval_samples_per_second: 1.869, interval_steps_per_second: 4.673, epoch: 0.6217[0m
[32m[2022-08-25 20:36:39,443] [    INFO][0m - loss: 2.16408863, learning_rate: 9.682539682539683e-06, global_step: 480, interval_runtime: 2.1225, interval_samples_per_second: 1.885, interval_steps_per_second: 4.711, epoch: 0.6349[0m
[32m[2022-08-25 20:36:41,632] [    INFO][0m - loss: 2.01469631, learning_rate: 9.675925925925926e-06, global_step: 490, interval_runtime: 2.1889, interval_samples_per_second: 1.827, interval_steps_per_second: 4.568, epoch: 0.6481[0m
[32m[2022-08-25 20:36:43,823] [    INFO][0m - loss: 1.75892982, learning_rate: 9.669312169312171e-06, global_step: 500, interval_runtime: 2.1907, interval_samples_per_second: 1.826, interval_steps_per_second: 4.565, epoch: 0.6614[0m
[32m[2022-08-25 20:36:43,823] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 20:36:43,823] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-25 20:36:43,823] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 20:36:43,823] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 20:36:43,823] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-25 20:37:18,697] [    INFO][0m - eval_loss: 2.0086398124694824, eval_accuracy: 0.4981791697013838, eval_runtime: 34.8736, eval_samples_per_second: 39.371, eval_steps_per_second: 1.233, epoch: 0.6614[0m
[32m[2022-08-25 20:37:18,698] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-08-25 20:37:18,698] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 20:37:20,360] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-08-25 20:37:20,360] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-08-25 20:37:24,783] [    INFO][0m - loss: 2.00781746, learning_rate: 9.662698412698414e-06, global_step: 510, interval_runtime: 40.9604, interval_samples_per_second: 0.098, interval_steps_per_second: 0.244, epoch: 0.6746[0m
[32m[2022-08-25 20:37:27,156] [    INFO][0m - loss: 1.77027283, learning_rate: 9.656084656084657e-06, global_step: 520, interval_runtime: 2.3729, interval_samples_per_second: 1.686, interval_steps_per_second: 4.214, epoch: 0.6878[0m
[32m[2022-08-25 20:37:29,453] [    INFO][0m - loss: 1.63088264, learning_rate: 9.649470899470901e-06, global_step: 530, interval_runtime: 2.2962, interval_samples_per_second: 1.742, interval_steps_per_second: 4.355, epoch: 0.7011[0m
[32m[2022-08-25 20:37:31,794] [    INFO][0m - loss: 1.72651997, learning_rate: 9.642857142857144e-06, global_step: 540, interval_runtime: 2.3416, interval_samples_per_second: 1.708, interval_steps_per_second: 4.271, epoch: 0.7143[0m
[32m[2022-08-25 20:37:34,133] [    INFO][0m - loss: 2.2210144, learning_rate: 9.636243386243387e-06, global_step: 550, interval_runtime: 2.3394, interval_samples_per_second: 1.71, interval_steps_per_second: 4.275, epoch: 0.7275[0m
[32m[2022-08-25 20:37:36,450] [    INFO][0m - loss: 2.42385635, learning_rate: 9.62962962962963e-06, global_step: 560, interval_runtime: 2.317, interval_samples_per_second: 1.726, interval_steps_per_second: 4.316, epoch: 0.7407[0m
[32m[2022-08-25 20:37:38,809] [    INFO][0m - loss: 2.16832848, learning_rate: 9.623015873015875e-06, global_step: 570, interval_runtime: 2.3585, interval_samples_per_second: 1.696, interval_steps_per_second: 4.24, epoch: 0.754[0m
[32m[2022-08-25 20:37:41,160] [    INFO][0m - loss: 1.80834007, learning_rate: 9.616402116402117e-06, global_step: 580, interval_runtime: 2.3508, interval_samples_per_second: 1.702, interval_steps_per_second: 4.254, epoch: 0.7672[0m
[32m[2022-08-25 20:37:43,540] [    INFO][0m - loss: 1.59739952, learning_rate: 9.60978835978836e-06, global_step: 590, interval_runtime: 2.3798, interval_samples_per_second: 1.681, interval_steps_per_second: 4.202, epoch: 0.7804[0m
[32m[2022-08-25 20:37:45,948] [    INFO][0m - loss: 1.92137012, learning_rate: 9.603174603174605e-06, global_step: 600, interval_runtime: 2.4084, interval_samples_per_second: 1.661, interval_steps_per_second: 4.152, epoch: 0.7937[0m
[32m[2022-08-25 20:37:45,949] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 20:37:45,949] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-25 20:37:45,949] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 20:37:45,949] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 20:37:45,949] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-25 20:38:25,482] [    INFO][0m - eval_loss: 2.0072758197784424, eval_accuracy: 0.49526584122359796, eval_runtime: 39.5328, eval_samples_per_second: 34.731, eval_steps_per_second: 1.088, epoch: 0.7937[0m
[32m[2022-08-25 20:38:25,483] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-08-25 20:38:25,483] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 20:38:29,539] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-08-25 20:38:29,539] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-08-25 20:38:36,477] [    INFO][0m - loss: 2.39041328, learning_rate: 9.596560846560848e-06, global_step: 610, interval_runtime: 50.529, interval_samples_per_second: 0.079, interval_steps_per_second: 0.198, epoch: 0.8069[0m
[32m[2022-08-25 20:38:39,019] [    INFO][0m - loss: 2.27561951, learning_rate: 9.58994708994709e-06, global_step: 620, interval_runtime: 2.5426, interval_samples_per_second: 1.573, interval_steps_per_second: 3.933, epoch: 0.8201[0m
[32m[2022-08-25 20:38:41,574] [    INFO][0m - loss: 1.49577818, learning_rate: 9.583333333333335e-06, global_step: 630, interval_runtime: 2.5537, interval_samples_per_second: 1.566, interval_steps_per_second: 3.916, epoch: 0.8333[0m
[32m[2022-08-25 20:38:44,234] [    INFO][0m - loss: 1.86462307, learning_rate: 9.576719576719578e-06, global_step: 640, interval_runtime: 2.6607, interval_samples_per_second: 1.503, interval_steps_per_second: 3.758, epoch: 0.8466[0m
[32m[2022-08-25 20:38:46,824] [    INFO][0m - loss: 2.37922974, learning_rate: 9.570105820105821e-06, global_step: 650, interval_runtime: 2.5896, interval_samples_per_second: 1.545, interval_steps_per_second: 3.862, epoch: 0.8598[0m
[32m[2022-08-25 20:38:49,464] [    INFO][0m - loss: 2.12208118, learning_rate: 9.563492063492064e-06, global_step: 660, interval_runtime: 2.6401, interval_samples_per_second: 1.515, interval_steps_per_second: 3.788, epoch: 0.873[0m
[32m[2022-08-25 20:38:52,040] [    INFO][0m - loss: 1.86685085, learning_rate: 9.556878306878309e-06, global_step: 670, interval_runtime: 2.576, interval_samples_per_second: 1.553, interval_steps_per_second: 3.882, epoch: 0.8862[0m
[32m[2022-08-25 20:38:54,633] [    INFO][0m - loss: 1.73159523, learning_rate: 9.550264550264551e-06, global_step: 680, interval_runtime: 2.5932, interval_samples_per_second: 1.542, interval_steps_per_second: 3.856, epoch: 0.8995[0m
[32m[2022-08-25 20:38:57,257] [    INFO][0m - loss: 2.08241081, learning_rate: 9.543650793650794e-06, global_step: 690, interval_runtime: 2.6236, interval_samples_per_second: 1.525, interval_steps_per_second: 3.812, epoch: 0.9127[0m
[32m[2022-08-25 20:38:59,967] [    INFO][0m - loss: 2.53352852, learning_rate: 9.537037037037037e-06, global_step: 700, interval_runtime: 2.7108, interval_samples_per_second: 1.476, interval_steps_per_second: 3.689, epoch: 0.9259[0m
[32m[2022-08-25 20:38:59,968] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 20:38:59,968] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-25 20:38:59,968] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 20:38:59,968] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 20:38:59,968] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-25 20:39:44,028] [    INFO][0m - eval_loss: 1.9319368600845337, eval_accuracy: 0.517115804806992, eval_runtime: 44.0596, eval_samples_per_second: 31.162, eval_steps_per_second: 0.976, epoch: 0.9259[0m
[32m[2022-08-25 20:39:44,029] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-700[0m
[32m[2022-08-25 20:39:44,029] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 20:39:45,922] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-700/tokenizer_config.json[0m
[32m[2022-08-25 20:39:45,922] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-700/special_tokens_map.json[0m
[32m[2022-08-25 20:39:51,085] [    INFO][0m - loss: 2.31024952, learning_rate: 9.530423280423282e-06, global_step: 710, interval_runtime: 51.1178, interval_samples_per_second: 0.078, interval_steps_per_second: 0.196, epoch: 0.9392[0m
[32m[2022-08-25 20:39:53,783] [    INFO][0m - loss: 2.25488892, learning_rate: 9.523809523809525e-06, global_step: 720, interval_runtime: 2.6975, interval_samples_per_second: 1.483, interval_steps_per_second: 3.707, epoch: 0.9524[0m
[32m[2022-08-25 20:39:56,574] [    INFO][0m - loss: 2.19508667, learning_rate: 9.517195767195768e-06, global_step: 730, interval_runtime: 2.7917, interval_samples_per_second: 1.433, interval_steps_per_second: 3.582, epoch: 0.9656[0m
[32m[2022-08-25 20:39:59,384] [    INFO][0m - loss: 1.72943916, learning_rate: 9.51058201058201e-06, global_step: 740, interval_runtime: 2.8092, interval_samples_per_second: 1.424, interval_steps_per_second: 3.56, epoch: 0.9788[0m
[32m[2022-08-25 20:40:02,147] [    INFO][0m - loss: 1.58459682, learning_rate: 9.503968253968255e-06, global_step: 750, interval_runtime: 2.7634, interval_samples_per_second: 1.447, interval_steps_per_second: 3.619, epoch: 0.9921[0m
[32m[2022-08-25 20:40:05,097] [    INFO][0m - loss: 1.7114254, learning_rate: 9.497354497354498e-06, global_step: 760, interval_runtime: 2.9502, interval_samples_per_second: 1.356, interval_steps_per_second: 3.39, epoch: 1.0053[0m
[32m[2022-08-25 20:40:08,025] [    INFO][0m - loss: 1.57162161, learning_rate: 9.490740740740741e-06, global_step: 770, interval_runtime: 2.9273, interval_samples_per_second: 1.366, interval_steps_per_second: 3.416, epoch: 1.0185[0m
[32m[2022-08-25 20:40:10,917] [    INFO][0m - loss: 1.8163002, learning_rate: 9.484126984126984e-06, global_step: 780, interval_runtime: 2.8921, interval_samples_per_second: 1.383, interval_steps_per_second: 3.458, epoch: 1.0317[0m
[32m[2022-08-25 20:40:13,809] [    INFO][0m - loss: 1.39833479, learning_rate: 9.477513227513228e-06, global_step: 790, interval_runtime: 2.8924, interval_samples_per_second: 1.383, interval_steps_per_second: 3.457, epoch: 1.045[0m
[32m[2022-08-25 20:40:16,705] [    INFO][0m - loss: 1.32081776, learning_rate: 9.470899470899471e-06, global_step: 800, interval_runtime: 2.8959, interval_samples_per_second: 1.381, interval_steps_per_second: 3.453, epoch: 1.0582[0m
[32m[2022-08-25 20:40:16,705] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 20:40:16,705] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-25 20:40:16,705] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 20:40:16,706] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 20:40:16,706] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-25 20:41:05,660] [    INFO][0m - eval_loss: 1.9416217803955078, eval_accuracy: 0.5185724690458849, eval_runtime: 48.9536, eval_samples_per_second: 28.047, eval_steps_per_second: 0.878, epoch: 1.0582[0m
[32m[2022-08-25 20:41:05,660] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-800[0m
[32m[2022-08-25 20:41:05,660] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 20:41:07,527] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-800/tokenizer_config.json[0m
[32m[2022-08-25 20:41:07,527] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-800/special_tokens_map.json[0m
[32m[2022-08-25 20:41:12,712] [    INFO][0m - loss: 1.52163992, learning_rate: 9.464285714285714e-06, global_step: 810, interval_runtime: 56.0067, interval_samples_per_second: 0.071, interval_steps_per_second: 0.179, epoch: 1.0714[0m
[32m[2022-08-25 20:41:15,663] [    INFO][0m - loss: 1.61135979, learning_rate: 9.457671957671959e-06, global_step: 820, interval_runtime: 2.9505, interval_samples_per_second: 1.356, interval_steps_per_second: 3.389, epoch: 1.0847[0m
[32m[2022-08-25 20:41:18,683] [    INFO][0m - loss: 1.7009634, learning_rate: 9.451058201058202e-06, global_step: 830, interval_runtime: 3.0208, interval_samples_per_second: 1.324, interval_steps_per_second: 3.31, epoch: 1.0979[0m
[32m[2022-08-25 20:41:21,723] [    INFO][0m - loss: 1.34217253, learning_rate: 9.444444444444445e-06, global_step: 840, interval_runtime: 3.04, interval_samples_per_second: 1.316, interval_steps_per_second: 3.289, epoch: 1.1111[0m
[32m[2022-08-25 20:41:24,748] [    INFO][0m - loss: 1.51362104, learning_rate: 9.437830687830689e-06, global_step: 850, interval_runtime: 3.0247, interval_samples_per_second: 1.322, interval_steps_per_second: 3.306, epoch: 1.1243[0m
[32m[2022-08-25 20:41:27,809] [    INFO][0m - loss: 1.54597073, learning_rate: 9.431216931216932e-06, global_step: 860, interval_runtime: 3.0615, interval_samples_per_second: 1.307, interval_steps_per_second: 3.266, epoch: 1.1376[0m
[32m[2022-08-25 20:41:30,917] [    INFO][0m - loss: 1.14240732, learning_rate: 9.424603174603175e-06, global_step: 870, interval_runtime: 3.1076, interval_samples_per_second: 1.287, interval_steps_per_second: 3.218, epoch: 1.1508[0m
[32m[2022-08-25 20:41:33,959] [    INFO][0m - loss: 1.62424545, learning_rate: 9.417989417989418e-06, global_step: 880, interval_runtime: 3.0415, interval_samples_per_second: 1.315, interval_steps_per_second: 3.288, epoch: 1.164[0m
[32m[2022-08-25 20:41:37,081] [    INFO][0m - loss: 2.28616295, learning_rate: 9.411375661375662e-06, global_step: 890, interval_runtime: 3.1223, interval_samples_per_second: 1.281, interval_steps_per_second: 3.203, epoch: 1.1772[0m
[32m[2022-08-25 20:41:40,257] [    INFO][0m - loss: 0.88185396, learning_rate: 9.404761904761905e-06, global_step: 900, interval_runtime: 3.1759, interval_samples_per_second: 1.259, interval_steps_per_second: 3.149, epoch: 1.1905[0m
[32m[2022-08-25 20:41:40,257] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 20:41:40,257] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-25 20:41:40,258] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 20:41:40,258] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 20:41:40,258] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-25 20:42:33,860] [    INFO][0m - eval_loss: 1.949792742729187, eval_accuracy: 0.5120174799708667, eval_runtime: 53.6014, eval_samples_per_second: 25.615, eval_steps_per_second: 0.802, epoch: 1.1905[0m
[32m[2022-08-25 20:42:33,860] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-900[0m
[32m[2022-08-25 20:42:33,860] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 20:42:35,669] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-900/tokenizer_config.json[0m
[32m[2022-08-25 20:42:35,670] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-900/special_tokens_map.json[0m
[32m[2022-08-25 20:42:41,215] [    INFO][0m - loss: 1.95541382, learning_rate: 9.398148148148148e-06, global_step: 910, interval_runtime: 60.958, interval_samples_per_second: 0.066, interval_steps_per_second: 0.164, epoch: 1.2037[0m
[32m[2022-08-25 20:42:44,357] [    INFO][0m - loss: 1.36534185, learning_rate: 9.391534391534393e-06, global_step: 920, interval_runtime: 3.143, interval_samples_per_second: 1.273, interval_steps_per_second: 3.182, epoch: 1.2169[0m
[32m[2022-08-25 20:42:47,611] [    INFO][0m - loss: 1.54242802, learning_rate: 9.384920634920636e-06, global_step: 930, interval_runtime: 3.2534, interval_samples_per_second: 1.229, interval_steps_per_second: 3.074, epoch: 1.2302[0m
[32m[2022-08-25 20:42:50,850] [    INFO][0m - loss: 1.81826973, learning_rate: 9.378306878306879e-06, global_step: 940, interval_runtime: 3.2395, interval_samples_per_second: 1.235, interval_steps_per_second: 3.087, epoch: 1.2434[0m
[32m[2022-08-25 20:42:54,091] [    INFO][0m - loss: 1.37075005, learning_rate: 9.371693121693123e-06, global_step: 950, interval_runtime: 3.2406, interval_samples_per_second: 1.234, interval_steps_per_second: 3.086, epoch: 1.2566[0m
[32m[2022-08-25 20:42:57,365] [    INFO][0m - loss: 1.37091427, learning_rate: 9.365079365079366e-06, global_step: 960, interval_runtime: 3.2732, interval_samples_per_second: 1.222, interval_steps_per_second: 3.055, epoch: 1.2698[0m
[32m[2022-08-25 20:43:00,655] [    INFO][0m - loss: 1.480966, learning_rate: 9.358465608465609e-06, global_step: 970, interval_runtime: 3.2901, interval_samples_per_second: 1.216, interval_steps_per_second: 3.039, epoch: 1.2831[0m
[32m[2022-08-25 20:43:03,974] [    INFO][0m - loss: 1.88793755, learning_rate: 9.351851851851854e-06, global_step: 980, interval_runtime: 3.3189, interval_samples_per_second: 1.205, interval_steps_per_second: 3.013, epoch: 1.2963[0m
[32m[2022-08-25 20:43:07,414] [    INFO][0m - loss: 1.7948843, learning_rate: 9.345238095238096e-06, global_step: 990, interval_runtime: 3.4409, interval_samples_per_second: 1.162, interval_steps_per_second: 2.906, epoch: 1.3095[0m
[32m[2022-08-25 20:43:10,783] [    INFO][0m - loss: 2.12598438, learning_rate: 9.33862433862434e-06, global_step: 1000, interval_runtime: 3.3692, interval_samples_per_second: 1.187, interval_steps_per_second: 2.968, epoch: 1.3228[0m
[32m[2022-08-25 20:43:10,784] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 20:43:10,784] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-25 20:43:10,784] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 20:43:10,784] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 20:43:10,784] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-25 20:44:08,983] [    INFO][0m - eval_loss: 1.8665509223937988, eval_accuracy: 0.5367807720320467, eval_runtime: 58.1984, eval_samples_per_second: 23.592, eval_steps_per_second: 0.739, epoch: 1.3228[0m
[32m[2022-08-25 20:44:08,984] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1000[0m
[32m[2022-08-25 20:44:08,984] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 20:44:10,812] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1000/tokenizer_config.json[0m
[32m[2022-08-25 20:44:10,813] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1000/special_tokens_map.json[0m
[32m[2022-08-25 20:44:16,380] [    INFO][0m - loss: 1.66638947, learning_rate: 9.332010582010584e-06, global_step: 1010, interval_runtime: 65.5962, interval_samples_per_second: 0.061, interval_steps_per_second: 0.152, epoch: 1.336[0m
[32m[2022-08-25 20:44:20,622] [    INFO][0m - loss: 2.38637447, learning_rate: 9.325396825396827e-06, global_step: 1020, interval_runtime: 3.394, interval_samples_per_second: 1.179, interval_steps_per_second: 2.946, epoch: 1.3492[0m
[32m[2022-08-25 20:44:24,065] [    INFO][0m - loss: 1.79303894, learning_rate: 9.31878306878307e-06, global_step: 1030, interval_runtime: 4.2913, interval_samples_per_second: 0.932, interval_steps_per_second: 2.33, epoch: 1.3624[0m
[32m[2022-08-25 20:44:27,510] [    INFO][0m - loss: 2.06024418, learning_rate: 9.312169312169313e-06, global_step: 1040, interval_runtime: 3.4445, interval_samples_per_second: 1.161, interval_steps_per_second: 2.903, epoch: 1.3757[0m
[32m[2022-08-25 20:44:30,980] [    INFO][0m - loss: 1.66607113, learning_rate: 9.305555555555557e-06, global_step: 1050, interval_runtime: 3.4703, interval_samples_per_second: 1.153, interval_steps_per_second: 2.882, epoch: 1.3889[0m
[32m[2022-08-25 20:44:34,488] [    INFO][0m - loss: 1.47796764, learning_rate: 9.2989417989418e-06, global_step: 1060, interval_runtime: 3.508, interval_samples_per_second: 1.14, interval_steps_per_second: 2.851, epoch: 1.4021[0m
[32m[2022-08-25 20:44:38,000] [    INFO][0m - loss: 1.35142603, learning_rate: 9.292328042328043e-06, global_step: 1070, interval_runtime: 3.5121, interval_samples_per_second: 1.139, interval_steps_per_second: 2.847, epoch: 1.4153[0m
[32m[2022-08-25 20:44:41,462] [    INFO][0m - loss: 2.04679604, learning_rate: 9.285714285714288e-06, global_step: 1080, interval_runtime: 3.4618, interval_samples_per_second: 1.155, interval_steps_per_second: 2.889, epoch: 1.4286[0m
[32m[2022-08-25 20:44:44,931] [    INFO][0m - loss: 1.4889164, learning_rate: 9.27910052910053e-06, global_step: 1090, interval_runtime: 3.4696, interval_samples_per_second: 1.153, interval_steps_per_second: 2.882, epoch: 1.4418[0m
[32m[2022-08-25 20:44:48,498] [    INFO][0m - loss: 1.36074705, learning_rate: 9.272486772486773e-06, global_step: 1100, interval_runtime: 3.5658, interval_samples_per_second: 1.122, interval_steps_per_second: 2.804, epoch: 1.455[0m
[32m[2022-08-25 20:44:48,499] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 20:44:48,499] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-25 20:44:48,499] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 20:44:48,499] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 20:44:48,500] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-25 20:45:51,395] [    INFO][0m - eval_loss: 1.8615479469299316, eval_accuracy: 0.5309541150764748, eval_runtime: 62.8949, eval_samples_per_second: 21.83, eval_steps_per_second: 0.684, epoch: 1.455[0m
[32m[2022-08-25 20:45:51,395] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1100[0m
[32m[2022-08-25 20:45:51,395] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 20:45:53,205] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1100/tokenizer_config.json[0m
[32m[2022-08-25 20:45:53,206] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1100/special_tokens_map.json[0m
[32m[2022-08-25 20:45:59,087] [    INFO][0m - loss: 1.65548782, learning_rate: 9.265873015873016e-06, global_step: 1110, interval_runtime: 70.5893, interval_samples_per_second: 0.057, interval_steps_per_second: 0.142, epoch: 1.4683[0m
[32m[2022-08-25 20:46:02,696] [    INFO][0m - loss: 1.5565609, learning_rate: 9.25925925925926e-06, global_step: 1120, interval_runtime: 3.6099, interval_samples_per_second: 1.108, interval_steps_per_second: 2.77, epoch: 1.4815[0m
[32m[2022-08-25 20:46:06,355] [    INFO][0m - loss: 1.61601429, learning_rate: 9.252645502645504e-06, global_step: 1130, interval_runtime: 3.6584, interval_samples_per_second: 1.093, interval_steps_per_second: 2.733, epoch: 1.4947[0m
[32m[2022-08-25 20:46:10,056] [    INFO][0m - loss: 1.74946308, learning_rate: 9.246031746031747e-06, global_step: 1140, interval_runtime: 3.7007, interval_samples_per_second: 1.081, interval_steps_per_second: 2.702, epoch: 1.5079[0m
[32m[2022-08-25 20:46:13,799] [    INFO][0m - loss: 1.26999607, learning_rate: 9.23941798941799e-06, global_step: 1150, interval_runtime: 3.7432, interval_samples_per_second: 1.069, interval_steps_per_second: 2.671, epoch: 1.5212[0m
[32m[2022-08-25 20:46:17,495] [    INFO][0m - loss: 1.80515594, learning_rate: 9.232804232804234e-06, global_step: 1160, interval_runtime: 3.6967, interval_samples_per_second: 1.082, interval_steps_per_second: 2.705, epoch: 1.5344[0m
[32m[2022-08-25 20:46:21,272] [    INFO][0m - loss: 2.00794086, learning_rate: 9.226190476190477e-06, global_step: 1170, interval_runtime: 3.7763, interval_samples_per_second: 1.059, interval_steps_per_second: 2.648, epoch: 1.5476[0m
[32m[2022-08-25 20:46:24,963] [    INFO][0m - loss: 1.41168413, learning_rate: 9.21957671957672e-06, global_step: 1180, interval_runtime: 3.6914, interval_samples_per_second: 1.084, interval_steps_per_second: 2.709, epoch: 1.5608[0m
[32m[2022-08-25 20:46:28,642] [    INFO][0m - loss: 1.7991333, learning_rate: 9.212962962962963e-06, global_step: 1190, interval_runtime: 3.6789, interval_samples_per_second: 1.087, interval_steps_per_second: 2.718, epoch: 1.5741[0m
[32m[2022-08-25 20:46:32,371] [    INFO][0m - loss: 1.40044565, learning_rate: 9.206349206349207e-06, global_step: 1200, interval_runtime: 3.7292, interval_samples_per_second: 1.073, interval_steps_per_second: 2.682, epoch: 1.5873[0m
[32m[2022-08-25 20:46:32,372] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 20:46:32,372] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-25 20:46:32,372] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 20:46:32,372] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 20:46:32,372] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-25 20:47:39,315] [    INFO][0m - eval_loss: 1.8405920267105103, eval_accuracy: 0.5447924253459577, eval_runtime: 66.942, eval_samples_per_second: 20.51, eval_steps_per_second: 0.642, epoch: 1.5873[0m
[32m[2022-08-25 20:47:39,315] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1200[0m
[32m[2022-08-25 20:47:39,315] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 20:47:41,191] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1200/tokenizer_config.json[0m
[32m[2022-08-25 20:47:41,192] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1200/special_tokens_map.json[0m
[32m[2022-08-25 20:47:47,205] [    INFO][0m - loss: 1.97150002, learning_rate: 9.19973544973545e-06, global_step: 1210, interval_runtime: 74.8337, interval_samples_per_second: 0.053, interval_steps_per_second: 0.134, epoch: 1.6005[0m
[32m[2022-08-25 20:47:51,035] [    INFO][0m - loss: 1.4086792, learning_rate: 9.193121693121693e-06, global_step: 1220, interval_runtime: 3.8299, interval_samples_per_second: 1.044, interval_steps_per_second: 2.611, epoch: 1.6138[0m
[32m[2022-08-25 20:47:54,854] [    INFO][0m - loss: 1.57185974, learning_rate: 9.186507936507936e-06, global_step: 1230, interval_runtime: 3.8191, interval_samples_per_second: 1.047, interval_steps_per_second: 2.618, epoch: 1.627[0m
[32m[2022-08-25 20:47:58,766] [    INFO][0m - loss: 1.92196007, learning_rate: 9.17989417989418e-06, global_step: 1240, interval_runtime: 3.9119, interval_samples_per_second: 1.023, interval_steps_per_second: 2.556, epoch: 1.6402[0m
[32m[2022-08-25 20:48:02,598] [    INFO][0m - loss: 1.41906357, learning_rate: 9.173280423280424e-06, global_step: 1250, interval_runtime: 3.8325, interval_samples_per_second: 1.044, interval_steps_per_second: 2.609, epoch: 1.6534[0m
[32m[2022-08-25 20:48:06,484] [    INFO][0m - loss: 1.12122288, learning_rate: 9.166666666666666e-06, global_step: 1260, interval_runtime: 3.8852, interval_samples_per_second: 1.03, interval_steps_per_second: 2.574, epoch: 1.6667[0m
[32m[2022-08-25 20:48:10,414] [    INFO][0m - loss: 1.53110027, learning_rate: 9.160052910052911e-06, global_step: 1270, interval_runtime: 3.9304, interval_samples_per_second: 1.018, interval_steps_per_second: 2.544, epoch: 1.6799[0m
[32m[2022-08-25 20:48:14,350] [    INFO][0m - loss: 1.68894711, learning_rate: 9.153439153439154e-06, global_step: 1280, interval_runtime: 3.9364, interval_samples_per_second: 1.016, interval_steps_per_second: 2.54, epoch: 1.6931[0m
[32m[2022-08-25 20:48:18,243] [    INFO][0m - loss: 1.75404873, learning_rate: 9.146825396825397e-06, global_step: 1290, interval_runtime: 3.8922, interval_samples_per_second: 1.028, interval_steps_per_second: 2.569, epoch: 1.7063[0m
[32m[2022-08-25 20:48:22,204] [    INFO][0m - loss: 1.85397987, learning_rate: 9.140211640211641e-06, global_step: 1300, interval_runtime: 3.9611, interval_samples_per_second: 1.01, interval_steps_per_second: 2.525, epoch: 1.7196[0m
[32m[2022-08-25 20:48:22,205] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 20:48:22,205] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-25 20:48:22,205] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 20:48:22,205] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 20:48:22,205] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-25 20:49:34,632] [    INFO][0m - eval_loss: 1.8382595777511597, eval_accuracy: 0.5520757465404225, eval_runtime: 72.4267, eval_samples_per_second: 18.957, eval_steps_per_second: 0.594, epoch: 1.7196[0m
[32m[2022-08-25 20:49:34,633] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1300[0m
[32m[2022-08-25 20:49:34,633] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 20:49:36,262] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1300/tokenizer_config.json[0m
[32m[2022-08-25 20:49:36,263] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1300/special_tokens_map.json[0m
[32m[2022-08-25 20:49:42,267] [    INFO][0m - loss: 2.00865555, learning_rate: 9.133597883597884e-06, global_step: 1310, interval_runtime: 80.0629, interval_samples_per_second: 0.05, interval_steps_per_second: 0.125, epoch: 1.7328[0m
[32m[2022-08-25 20:49:46,329] [    INFO][0m - loss: 1.8720108, learning_rate: 9.126984126984127e-06, global_step: 1320, interval_runtime: 4.0619, interval_samples_per_second: 0.985, interval_steps_per_second: 2.462, epoch: 1.746[0m
[32m[2022-08-25 20:49:50,445] [    INFO][0m - loss: 1.88581219, learning_rate: 9.120370370370372e-06, global_step: 1330, interval_runtime: 4.1161, interval_samples_per_second: 0.972, interval_steps_per_second: 2.429, epoch: 1.7593[0m
[32m[2022-08-25 20:49:54,548] [    INFO][0m - loss: 1.34467678, learning_rate: 9.113756613756615e-06, global_step: 1340, interval_runtime: 4.103, interval_samples_per_second: 0.975, interval_steps_per_second: 2.437, epoch: 1.7725[0m
[32m[2022-08-25 20:49:58,634] [    INFO][0m - loss: 1.91604023, learning_rate: 9.107142857142858e-06, global_step: 1350, interval_runtime: 4.0863, interval_samples_per_second: 0.979, interval_steps_per_second: 2.447, epoch: 1.7857[0m
[32m[2022-08-25 20:50:02,820] [    INFO][0m - loss: 1.67543221, learning_rate: 9.1005291005291e-06, global_step: 1360, interval_runtime: 4.186, interval_samples_per_second: 0.956, interval_steps_per_second: 2.389, epoch: 1.7989[0m
[32m[2022-08-25 20:50:06,944] [    INFO][0m - loss: 1.31997585, learning_rate: 9.093915343915345e-06, global_step: 1370, interval_runtime: 4.1239, interval_samples_per_second: 0.97, interval_steps_per_second: 2.425, epoch: 1.8122[0m
[32m[2022-08-25 20:50:11,160] [    INFO][0m - loss: 1.80276184, learning_rate: 9.087301587301588e-06, global_step: 1380, interval_runtime: 4.2152, interval_samples_per_second: 0.949, interval_steps_per_second: 2.372, epoch: 1.8254[0m
[32m[2022-08-25 20:50:15,389] [    INFO][0m - loss: 1.48450956, learning_rate: 9.08068783068783e-06, global_step: 1390, interval_runtime: 4.2295, interval_samples_per_second: 0.946, interval_steps_per_second: 2.364, epoch: 1.8386[0m
[32m[2022-08-25 20:50:19,625] [    INFO][0m - loss: 1.49022369, learning_rate: 9.074074074074075e-06, global_step: 1400, interval_runtime: 4.2364, interval_samples_per_second: 0.944, interval_steps_per_second: 2.361, epoch: 1.8519[0m
[32m[2022-08-25 20:50:19,626] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 20:50:19,626] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-25 20:50:19,626] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 20:50:19,626] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 20:50:19,626] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-25 20:51:36,807] [    INFO][0m - eval_loss: 1.8585346937179565, eval_accuracy: 0.5455207574654042, eval_runtime: 77.1804, eval_samples_per_second: 17.789, eval_steps_per_second: 0.557, epoch: 1.8519[0m
[32m[2022-08-25 20:51:36,808] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1400[0m
[32m[2022-08-25 20:51:36,808] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 20:51:38,491] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1400/tokenizer_config.json[0m
[32m[2022-08-25 20:51:38,492] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1400/special_tokens_map.json[0m
[32m[2022-08-25 20:51:44,744] [    INFO][0m - loss: 1.66678925, learning_rate: 9.067460317460318e-06, global_step: 1410, interval_runtime: 85.1186, interval_samples_per_second: 0.047, interval_steps_per_second: 0.117, epoch: 1.8651[0m
[32m[2022-08-25 20:51:49,040] [    INFO][0m - loss: 1.94267578, learning_rate: 9.060846560846561e-06, global_step: 1420, interval_runtime: 4.2962, interval_samples_per_second: 0.931, interval_steps_per_second: 2.328, epoch: 1.8783[0m
[32m[2022-08-25 20:51:53,378] [    INFO][0m - loss: 1.76432266, learning_rate: 9.054232804232806e-06, global_step: 1430, interval_runtime: 4.338, interval_samples_per_second: 0.922, interval_steps_per_second: 2.305, epoch: 1.8915[0m
[32m[2022-08-25 20:51:57,686] [    INFO][0m - loss: 1.59973164, learning_rate: 9.047619047619049e-06, global_step: 1440, interval_runtime: 4.3084, interval_samples_per_second: 0.928, interval_steps_per_second: 2.321, epoch: 1.9048[0m
[32m[2022-08-25 20:52:02,056] [    INFO][0m - loss: 1.51006441, learning_rate: 9.041005291005292e-06, global_step: 1450, interval_runtime: 4.3692, interval_samples_per_second: 0.915, interval_steps_per_second: 2.289, epoch: 1.918[0m
[32m[2022-08-25 20:52:06,570] [    INFO][0m - loss: 1.8251091, learning_rate: 9.034391534391536e-06, global_step: 1460, interval_runtime: 4.5146, interval_samples_per_second: 0.886, interval_steps_per_second: 2.215, epoch: 1.9312[0m
[32m[2022-08-25 20:52:11,022] [    INFO][0m - loss: 1.31281261, learning_rate: 9.027777777777779e-06, global_step: 1470, interval_runtime: 4.4515, interval_samples_per_second: 0.899, interval_steps_per_second: 2.246, epoch: 1.9444[0m
[32m[2022-08-25 20:52:15,439] [    INFO][0m - loss: 1.7014328, learning_rate: 9.021164021164022e-06, global_step: 1480, interval_runtime: 4.4169, interval_samples_per_second: 0.906, interval_steps_per_second: 2.264, epoch: 1.9577[0m
[32m[2022-08-25 20:52:19,935] [    INFO][0m - loss: 1.27843571, learning_rate: 9.014550264550267e-06, global_step: 1490, interval_runtime: 4.4961, interval_samples_per_second: 0.89, interval_steps_per_second: 2.224, epoch: 1.9709[0m
[32m[2022-08-25 20:52:24,438] [    INFO][0m - loss: 1.56970654, learning_rate: 9.00793650793651e-06, global_step: 1500, interval_runtime: 4.5033, interval_samples_per_second: 0.888, interval_steps_per_second: 2.221, epoch: 1.9841[0m
[32m[2022-08-25 20:52:24,439] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 20:52:24,439] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-25 20:52:24,439] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 20:52:24,439] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 20:52:24,439] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-25 20:53:46,549] [    INFO][0m - eval_loss: 1.8537585735321045, eval_accuracy: 0.5447924253459577, eval_runtime: 82.1091, eval_samples_per_second: 16.722, eval_steps_per_second: 0.524, epoch: 1.9841[0m
[32m[2022-08-25 20:53:46,549] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1500[0m
[32m[2022-08-25 20:53:46,550] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 20:53:48,156] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1500/tokenizer_config.json[0m
[32m[2022-08-25 20:53:48,156] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1500/special_tokens_map.json[0m
[32m[2022-08-25 20:53:53,650] [    INFO][0m - loss: 1.25141048, learning_rate: 9.001322751322752e-06, global_step: 1510, interval_runtime: 89.2118, interval_samples_per_second: 0.045, interval_steps_per_second: 0.112, epoch: 1.9974[0m
[32m[2022-08-25 20:53:59,478] [    INFO][0m - loss: 1.08805552, learning_rate: 8.994708994708995e-06, global_step: 1520, interval_runtime: 5.8274, interval_samples_per_second: 0.686, interval_steps_per_second: 1.716, epoch: 2.0106[0m
[32m[2022-08-25 20:54:04,081] [    INFO][0m - loss: 0.7115973, learning_rate: 8.98809523809524e-06, global_step: 1530, interval_runtime: 4.6031, interval_samples_per_second: 0.869, interval_steps_per_second: 2.172, epoch: 2.0238[0m
[32m[2022-08-25 20:54:08,694] [    INFO][0m - loss: 0.94554768, learning_rate: 8.981481481481483e-06, global_step: 1540, interval_runtime: 4.6135, interval_samples_per_second: 0.867, interval_steps_per_second: 2.168, epoch: 2.037[0m
[32m[2022-08-25 20:54:13,248] [    INFO][0m - loss: 1.37483034, learning_rate: 8.974867724867726e-06, global_step: 1550, interval_runtime: 4.554, interval_samples_per_second: 0.878, interval_steps_per_second: 2.196, epoch: 2.0503[0m
[32m[2022-08-25 20:54:17,879] [    INFO][0m - loss: 1.49440079, learning_rate: 8.968253968253968e-06, global_step: 1560, interval_runtime: 4.6311, interval_samples_per_second: 0.864, interval_steps_per_second: 2.159, epoch: 2.0635[0m
[32m[2022-08-25 20:54:22,533] [    INFO][0m - loss: 1.35510836, learning_rate: 8.961640211640213e-06, global_step: 1570, interval_runtime: 4.6545, interval_samples_per_second: 0.859, interval_steps_per_second: 2.148, epoch: 2.0767[0m
[32m[2022-08-25 20:54:27,181] [    INFO][0m - loss: 1.10117207, learning_rate: 8.955026455026456e-06, global_step: 1580, interval_runtime: 4.6477, interval_samples_per_second: 0.861, interval_steps_per_second: 2.152, epoch: 2.0899[0m
[32m[2022-08-25 20:54:31,798] [    INFO][0m - loss: 0.95893221, learning_rate: 8.948412698412699e-06, global_step: 1590, interval_runtime: 4.6173, interval_samples_per_second: 0.866, interval_steps_per_second: 2.166, epoch: 2.1032[0m
[32m[2022-08-25 20:54:36,548] [    INFO][0m - loss: 1.38738155, learning_rate: 8.941798941798942e-06, global_step: 1600, interval_runtime: 4.7493, interval_samples_per_second: 0.842, interval_steps_per_second: 2.106, epoch: 2.1164[0m
[32m[2022-08-25 20:54:36,549] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 20:54:36,549] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-25 20:54:36,549] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 20:54:36,549] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 20:54:36,549] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-25 20:56:03,216] [    INFO][0m - eval_loss: 1.871483564376831, eval_accuracy: 0.5542607428987618, eval_runtime: 86.667, eval_samples_per_second: 15.842, eval_steps_per_second: 0.496, epoch: 2.1164[0m
[32m[2022-08-25 20:56:03,217] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1600[0m
[32m[2022-08-25 20:56:03,217] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 20:56:04,984] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1600/tokenizer_config.json[0m
[32m[2022-08-25 20:56:04,984] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1600/special_tokens_map.json[0m
[32m[2022-08-25 20:56:11,613] [    INFO][0m - loss: 1.3098135, learning_rate: 8.935185185185186e-06, global_step: 1610, interval_runtime: 95.0655, interval_samples_per_second: 0.042, interval_steps_per_second: 0.105, epoch: 2.1296[0m
[32m[2022-08-25 20:56:16,360] [    INFO][0m - loss: 1.23016567, learning_rate: 8.92857142857143e-06, global_step: 1620, interval_runtime: 4.7465, interval_samples_per_second: 0.843, interval_steps_per_second: 2.107, epoch: 2.1429[0m
[32m[2022-08-25 20:56:21,110] [    INFO][0m - loss: 1.2941534, learning_rate: 8.921957671957672e-06, global_step: 1630, interval_runtime: 4.7499, interval_samples_per_second: 0.842, interval_steps_per_second: 2.105, epoch: 2.1561[0m
[32m[2022-08-25 20:56:25,959] [    INFO][0m - loss: 1.41650686, learning_rate: 8.915343915343915e-06, global_step: 1640, interval_runtime: 4.8496, interval_samples_per_second: 0.825, interval_steps_per_second: 2.062, epoch: 2.1693[0m
[32m[2022-08-25 20:56:30,888] [    INFO][0m - loss: 1.10919075, learning_rate: 8.90873015873016e-06, global_step: 1650, interval_runtime: 4.9291, interval_samples_per_second: 0.812, interval_steps_per_second: 2.029, epoch: 2.1825[0m
[32m[2022-08-25 20:56:35,663] [    INFO][0m - loss: 1.35814285, learning_rate: 8.902116402116403e-06, global_step: 1660, interval_runtime: 4.774, interval_samples_per_second: 0.838, interval_steps_per_second: 2.095, epoch: 2.1958[0m
[32m[2022-08-25 20:56:40,513] [    INFO][0m - loss: 1.03042355, learning_rate: 8.895502645502645e-06, global_step: 1670, interval_runtime: 4.851, interval_samples_per_second: 0.825, interval_steps_per_second: 2.061, epoch: 2.209[0m
[32m[2022-08-25 20:56:45,472] [    INFO][0m - loss: 1.18724937, learning_rate: 8.888888888888888e-06, global_step: 1680, interval_runtime: 4.9582, interval_samples_per_second: 0.807, interval_steps_per_second: 2.017, epoch: 2.2222[0m
[32m[2022-08-25 20:56:50,379] [    INFO][0m - loss: 1.46788387, learning_rate: 8.882275132275133e-06, global_step: 1690, interval_runtime: 4.9077, interval_samples_per_second: 0.815, interval_steps_per_second: 2.038, epoch: 2.2354[0m
[32m[2022-08-25 20:56:55,278] [    INFO][0m - loss: 1.02807455, learning_rate: 8.875661375661376e-06, global_step: 1700, interval_runtime: 4.8989, interval_samples_per_second: 0.817, interval_steps_per_second: 2.041, epoch: 2.2487[0m
[32m[2022-08-25 20:56:55,279] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 20:56:55,279] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-25 20:56:55,279] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 20:56:55,279] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 20:56:55,279] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-25 20:58:26,683] [    INFO][0m - eval_loss: 1.813754677772522, eval_accuracy: 0.549890750182083, eval_runtime: 91.4038, eval_samples_per_second: 15.021, eval_steps_per_second: 0.47, epoch: 2.2487[0m
[32m[2022-08-25 20:58:26,684] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1700[0m
[32m[2022-08-25 20:58:26,684] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 20:58:28,249] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1700/tokenizer_config.json[0m
[32m[2022-08-25 20:58:28,249] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1700/special_tokens_map.json[0m
[32m[2022-08-25 20:58:35,322] [    INFO][0m - loss: 1.39468327, learning_rate: 8.869047619047619e-06, global_step: 1710, interval_runtime: 100.0436, interval_samples_per_second: 0.04, interval_steps_per_second: 0.1, epoch: 2.2619[0m
[32m[2022-08-25 20:58:40,512] [    INFO][0m - loss: 1.20380125, learning_rate: 8.862433862433863e-06, global_step: 1720, interval_runtime: 5.19, interval_samples_per_second: 0.771, interval_steps_per_second: 1.927, epoch: 2.2751[0m
[32m[2022-08-25 20:58:45,563] [    INFO][0m - loss: 1.26991129, learning_rate: 8.855820105820106e-06, global_step: 1730, interval_runtime: 5.051, interval_samples_per_second: 0.792, interval_steps_per_second: 1.98, epoch: 2.2884[0m
[32m[2022-08-25 20:58:50,576] [    INFO][0m - loss: 1.65804424, learning_rate: 8.849206349206349e-06, global_step: 1740, interval_runtime: 5.0131, interval_samples_per_second: 0.798, interval_steps_per_second: 1.995, epoch: 2.3016[0m
[32m[2022-08-25 20:58:55,603] [    INFO][0m - loss: 1.19530487, learning_rate: 8.842592592592594e-06, global_step: 1750, interval_runtime: 5.0269, interval_samples_per_second: 0.796, interval_steps_per_second: 1.989, epoch: 2.3148[0m
[32m[2022-08-25 20:59:00,709] [    INFO][0m - loss: 1.23881645, learning_rate: 8.835978835978837e-06, global_step: 1760, interval_runtime: 5.1064, interval_samples_per_second: 0.783, interval_steps_per_second: 1.958, epoch: 2.328[0m
[32m[2022-08-25 20:59:05,806] [    INFO][0m - loss: 1.1985795, learning_rate: 8.82936507936508e-06, global_step: 1770, interval_runtime: 5.0966, interval_samples_per_second: 0.785, interval_steps_per_second: 1.962, epoch: 2.3413[0m
[32m[2022-08-25 20:59:10,934] [    INFO][0m - loss: 0.97752047, learning_rate: 8.822751322751324e-06, global_step: 1780, interval_runtime: 5.1273, interval_samples_per_second: 0.78, interval_steps_per_second: 1.95, epoch: 2.3545[0m
[32m[2022-08-25 20:59:16,173] [    INFO][0m - loss: 0.92240572, learning_rate: 8.816137566137567e-06, global_step: 1790, interval_runtime: 5.2399, interval_samples_per_second: 0.763, interval_steps_per_second: 1.908, epoch: 2.3677[0m
[32m[2022-08-25 20:59:21,331] [    INFO][0m - loss: 1.27674103, learning_rate: 8.80952380952381e-06, global_step: 1800, interval_runtime: 5.1573, interval_samples_per_second: 0.776, interval_steps_per_second: 1.939, epoch: 2.381[0m
[32m[2022-08-25 20:59:21,331] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 20:59:21,331] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-25 20:59:21,332] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 20:59:21,332] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 20:59:21,332] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-25 21:00:57,538] [    INFO][0m - eval_loss: 1.8633003234863281, eval_accuracy: 0.5615440640932265, eval_runtime: 96.2059, eval_samples_per_second: 14.271, eval_steps_per_second: 0.447, epoch: 2.381[0m
[32m[2022-08-25 21:00:57,539] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1800[0m
[32m[2022-08-25 21:00:57,539] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 21:00:59,238] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1800/tokenizer_config.json[0m
[32m[2022-08-25 21:00:59,238] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1800/special_tokens_map.json[0m
[32m[2022-08-25 21:01:06,319] [    INFO][0m - loss: 1.34886055, learning_rate: 8.802910052910054e-06, global_step: 1810, interval_runtime: 104.9879, interval_samples_per_second: 0.038, interval_steps_per_second: 0.095, epoch: 2.3942[0m
[32m[2022-08-25 21:01:11,590] [    INFO][0m - loss: 1.28373489, learning_rate: 8.796296296296297e-06, global_step: 1820, interval_runtime: 5.2716, interval_samples_per_second: 0.759, interval_steps_per_second: 1.897, epoch: 2.4074[0m
[32m[2022-08-25 21:01:16,786] [    INFO][0m - loss: 1.02570591, learning_rate: 8.78968253968254e-06, global_step: 1830, interval_runtime: 5.1963, interval_samples_per_second: 0.77, interval_steps_per_second: 1.924, epoch: 2.4206[0m
[32m[2022-08-25 21:01:22,121] [    INFO][0m - loss: 1.28231163, learning_rate: 8.783068783068783e-06, global_step: 1840, interval_runtime: 5.3347, interval_samples_per_second: 0.75, interval_steps_per_second: 1.875, epoch: 2.4339[0m
[32m[2022-08-25 21:01:27,399] [    INFO][0m - loss: 1.37683582, learning_rate: 8.776455026455028e-06, global_step: 1850, interval_runtime: 5.2785, interval_samples_per_second: 0.758, interval_steps_per_second: 1.894, epoch: 2.4471[0m
[32m[2022-08-25 21:01:32,701] [    INFO][0m - loss: 1.24730082, learning_rate: 8.76984126984127e-06, global_step: 1860, interval_runtime: 5.3017, interval_samples_per_second: 0.754, interval_steps_per_second: 1.886, epoch: 2.4603[0m
[32m[2022-08-25 21:01:38,043] [    INFO][0m - loss: 1.35664997, learning_rate: 8.763227513227513e-06, global_step: 1870, interval_runtime: 5.342, interval_samples_per_second: 0.749, interval_steps_per_second: 1.872, epoch: 2.4735[0m
[32m[2022-08-25 21:01:43,347] [    INFO][0m - loss: 0.84697285, learning_rate: 8.756613756613758e-06, global_step: 1880, interval_runtime: 5.3034, interval_samples_per_second: 0.754, interval_steps_per_second: 1.886, epoch: 2.4868[0m
[32m[2022-08-25 21:01:48,677] [    INFO][0m - loss: 1.19930077, learning_rate: 8.750000000000001e-06, global_step: 1890, interval_runtime: 5.3303, interval_samples_per_second: 0.75, interval_steps_per_second: 1.876, epoch: 2.5[0m
[32m[2022-08-25 21:01:54,049] [    INFO][0m - loss: 1.78966122, learning_rate: 8.743386243386244e-06, global_step: 1900, interval_runtime: 5.3714, interval_samples_per_second: 0.745, interval_steps_per_second: 1.862, epoch: 2.5132[0m
[32m[2022-08-25 21:01:54,050] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 21:01:54,050] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-25 21:01:54,050] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 21:01:54,050] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 21:01:54,050] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-25 21:03:35,622] [    INFO][0m - eval_loss: 1.860812783241272, eval_accuracy: 0.5396941005098325, eval_runtime: 101.5718, eval_samples_per_second: 13.518, eval_steps_per_second: 0.423, epoch: 2.5132[0m
[32m[2022-08-25 21:03:35,623] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1900[0m
[32m[2022-08-25 21:03:35,623] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 21:03:37,129] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1900/tokenizer_config.json[0m
[32m[2022-08-25 21:03:37,129] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1900/special_tokens_map.json[0m
[32m[2022-08-25 21:03:44,339] [    INFO][0m - loss: 1.13544693, learning_rate: 8.736772486772488e-06, global_step: 1910, interval_runtime: 110.2909, interval_samples_per_second: 0.036, interval_steps_per_second: 0.091, epoch: 2.5265[0m
[32m[2022-08-25 21:03:49,776] [    INFO][0m - loss: 1.55054531, learning_rate: 8.730158730158731e-06, global_step: 1920, interval_runtime: 5.4367, interval_samples_per_second: 0.736, interval_steps_per_second: 1.839, epoch: 2.5397[0m
[32m[2022-08-25 21:03:55,302] [    INFO][0m - loss: 1.40769444, learning_rate: 8.723544973544974e-06, global_step: 1930, interval_runtime: 5.5266, interval_samples_per_second: 0.724, interval_steps_per_second: 1.809, epoch: 2.5529[0m
[32m[2022-08-25 21:04:00,742] [    INFO][0m - loss: 1.41138182, learning_rate: 8.716931216931219e-06, global_step: 1940, interval_runtime: 5.4392, interval_samples_per_second: 0.735, interval_steps_per_second: 1.839, epoch: 2.5661[0m
[32m[2022-08-25 21:04:06,234] [    INFO][0m - loss: 1.06366758, learning_rate: 8.710317460317462e-06, global_step: 1950, interval_runtime: 5.4923, interval_samples_per_second: 0.728, interval_steps_per_second: 1.821, epoch: 2.5794[0m
[32m[2022-08-25 21:04:11,700] [    INFO][0m - loss: 1.36957817, learning_rate: 8.703703703703705e-06, global_step: 1960, interval_runtime: 5.4661, interval_samples_per_second: 0.732, interval_steps_per_second: 1.829, epoch: 2.5926[0m
[32m[2022-08-25 21:04:17,220] [    INFO][0m - loss: 1.38076363, learning_rate: 8.697089947089947e-06, global_step: 1970, interval_runtime: 5.5198, interval_samples_per_second: 0.725, interval_steps_per_second: 1.812, epoch: 2.6058[0m
[32m[2022-08-25 21:04:22,788] [    INFO][0m - loss: 1.12617788, learning_rate: 8.690476190476192e-06, global_step: 1980, interval_runtime: 5.5679, interval_samples_per_second: 0.718, interval_steps_per_second: 1.796, epoch: 2.619[0m
[32m[2022-08-25 21:04:28,303] [    INFO][0m - loss: 1.34183521, learning_rate: 8.683862433862435e-06, global_step: 1990, interval_runtime: 5.5152, interval_samples_per_second: 0.725, interval_steps_per_second: 1.813, epoch: 2.6323[0m
[32m[2022-08-25 21:04:33,824] [    INFO][0m - loss: 1.28821554, learning_rate: 8.677248677248678e-06, global_step: 2000, interval_runtime: 5.5208, interval_samples_per_second: 0.725, interval_steps_per_second: 1.811, epoch: 2.6455[0m
[32m[2022-08-25 21:04:33,824] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 21:04:33,824] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-25 21:04:33,824] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 21:04:33,825] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 21:04:33,825] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-25 21:06:19,787] [    INFO][0m - eval_loss: 1.8640671968460083, eval_accuracy: 0.5382374362709396, eval_runtime: 105.962, eval_samples_per_second: 12.957, eval_steps_per_second: 0.406, epoch: 2.6455[0m
[32m[2022-08-25 21:06:19,788] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2000[0m
[32m[2022-08-25 21:06:19,788] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 21:06:21,855] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2000/tokenizer_config.json[0m
[32m[2022-08-25 21:06:21,856] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2000/special_tokens_map.json[0m
[32m[2022-08-25 21:06:29,905] [    INFO][0m - loss: 1.46644812, learning_rate: 8.670634920634922e-06, global_step: 2010, interval_runtime: 116.0816, interval_samples_per_second: 0.034, interval_steps_per_second: 0.086, epoch: 2.6587[0m
[32m[2022-08-25 21:06:35,600] [    INFO][0m - loss: 1.02694778, learning_rate: 8.664021164021165e-06, global_step: 2020, interval_runtime: 5.695, interval_samples_per_second: 0.702, interval_steps_per_second: 1.756, epoch: 2.672[0m
[32m[2022-08-25 21:06:41,309] [    INFO][0m - loss: 1.29469509, learning_rate: 8.657407407407408e-06, global_step: 2030, interval_runtime: 5.7083, interval_samples_per_second: 0.701, interval_steps_per_second: 1.752, epoch: 2.6852[0m
[32m[2022-08-25 21:06:46,976] [    INFO][0m - loss: 1.37431221, learning_rate: 8.650793650793651e-06, global_step: 2040, interval_runtime: 5.6671, interval_samples_per_second: 0.706, interval_steps_per_second: 1.765, epoch: 2.6984[0m
[32m[2022-08-25 21:06:52,668] [    INFO][0m - loss: 1.53618107, learning_rate: 8.644179894179896e-06, global_step: 2050, interval_runtime: 5.6926, interval_samples_per_second: 0.703, interval_steps_per_second: 1.757, epoch: 2.7116[0m
[32m[2022-08-25 21:06:58,472] [    INFO][0m - loss: 1.10050707, learning_rate: 8.637566137566139e-06, global_step: 2060, interval_runtime: 5.8033, interval_samples_per_second: 0.689, interval_steps_per_second: 1.723, epoch: 2.7249[0m
[32m[2022-08-25 21:07:04,291] [    INFO][0m - loss: 1.12087193, learning_rate: 8.630952380952381e-06, global_step: 2070, interval_runtime: 5.8195, interval_samples_per_second: 0.687, interval_steps_per_second: 1.718, epoch: 2.7381[0m
[32m[2022-08-25 21:07:10,115] [    INFO][0m - loss: 1.67958775, learning_rate: 8.624338624338624e-06, global_step: 2080, interval_runtime: 5.824, interval_samples_per_second: 0.687, interval_steps_per_second: 1.717, epoch: 2.7513[0m
[32m[2022-08-25 21:07:15,990] [    INFO][0m - loss: 1.38608494, learning_rate: 8.617724867724869e-06, global_step: 2090, interval_runtime: 5.8742, interval_samples_per_second: 0.681, interval_steps_per_second: 1.702, epoch: 2.7646[0m
[32m[2022-08-25 21:07:21,855] [    INFO][0m - loss: 1.54438667, learning_rate: 8.611111111111112e-06, global_step: 2100, interval_runtime: 5.8654, interval_samples_per_second: 0.682, interval_steps_per_second: 1.705, epoch: 2.7778[0m
[32m[2022-08-25 21:07:21,856] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 21:07:21,856] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-25 21:07:21,856] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 21:07:21,856] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 21:07:21,856] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-25 21:09:12,366] [    INFO][0m - eval_loss: 1.8639460802078247, eval_accuracy: 0.5608157319737801, eval_runtime: 110.5097, eval_samples_per_second: 12.424, eval_steps_per_second: 0.389, epoch: 2.7778[0m
[32m[2022-08-25 21:09:12,367] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2100[0m
[32m[2022-08-25 21:09:12,367] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 21:09:14,377] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2100/tokenizer_config.json[0m
[32m[2022-08-25 21:09:14,377] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2100/special_tokens_map.json[0m
[32m[2022-08-25 21:09:22,820] [    INFO][0m - loss: 1.51750336, learning_rate: 8.604497354497355e-06, global_step: 2110, interval_runtime: 120.9646, interval_samples_per_second: 0.033, interval_steps_per_second: 0.083, epoch: 2.791[0m
[32m[2022-08-25 21:09:28,712] [    INFO][0m - loss: 1.13383036, learning_rate: 8.597883597883598e-06, global_step: 2120, interval_runtime: 5.8929, interval_samples_per_second: 0.679, interval_steps_per_second: 1.697, epoch: 2.8042[0m
[32m[2022-08-25 21:09:34,622] [    INFO][0m - loss: 1.29287586, learning_rate: 8.591269841269842e-06, global_step: 2130, interval_runtime: 5.9098, interval_samples_per_second: 0.677, interval_steps_per_second: 1.692, epoch: 2.8175[0m
[32m[2022-08-25 21:09:40,519] [    INFO][0m - loss: 1.34392719, learning_rate: 8.584656084656085e-06, global_step: 2140, interval_runtime: 5.897, interval_samples_per_second: 0.678, interval_steps_per_second: 1.696, epoch: 2.8307[0m
[32m[2022-08-25 21:09:46,420] [    INFO][0m - loss: 1.1442688, learning_rate: 8.578042328042328e-06, global_step: 2150, interval_runtime: 5.9006, interval_samples_per_second: 0.678, interval_steps_per_second: 1.695, epoch: 2.8439[0m
[32m[2022-08-25 21:09:52,432] [    INFO][0m - loss: 1.48052359, learning_rate: 8.571428571428571e-06, global_step: 2160, interval_runtime: 6.0126, interval_samples_per_second: 0.665, interval_steps_per_second: 1.663, epoch: 2.8571[0m
[32m[2022-08-25 21:09:58,379] [    INFO][0m - loss: 1.00966549, learning_rate: 8.564814814814816e-06, global_step: 2170, interval_runtime: 5.9467, interval_samples_per_second: 0.673, interval_steps_per_second: 1.682, epoch: 2.8704[0m
[32m[2022-08-25 21:10:04,412] [    INFO][0m - loss: 1.31209307, learning_rate: 8.558201058201058e-06, global_step: 2180, interval_runtime: 6.0325, interval_samples_per_second: 0.663, interval_steps_per_second: 1.658, epoch: 2.8836[0m
[32m[2022-08-25 21:10:10,447] [    INFO][0m - loss: 1.09105234, learning_rate: 8.551587301587301e-06, global_step: 2190, interval_runtime: 6.0354, interval_samples_per_second: 0.663, interval_steps_per_second: 1.657, epoch: 2.8968[0m
[32m[2022-08-25 21:10:16,540] [    INFO][0m - loss: 1.118787, learning_rate: 8.544973544973546e-06, global_step: 2200, interval_runtime: 6.0933, interval_samples_per_second: 0.656, interval_steps_per_second: 1.641, epoch: 2.9101[0m
[32m[2022-08-25 21:10:16,541] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 21:10:16,541] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-25 21:10:16,541] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 21:10:16,541] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 21:10:16,541] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-25 21:12:11,713] [    INFO][0m - eval_loss: 1.884164571762085, eval_accuracy: 0.5535324107793154, eval_runtime: 115.1713, eval_samples_per_second: 11.921, eval_steps_per_second: 0.373, epoch: 2.9101[0m
[32m[2022-08-25 21:12:11,714] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2200[0m
[32m[2022-08-25 21:12:11,714] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 21:12:13,903] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2200/tokenizer_config.json[0m
[32m[2022-08-25 21:12:13,903] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2200/special_tokens_map.json[0m
[32m[2022-08-25 21:12:22,497] [    INFO][0m - loss: 1.27548227, learning_rate: 8.538359788359789e-06, global_step: 2210, interval_runtime: 125.9562, interval_samples_per_second: 0.032, interval_steps_per_second: 0.079, epoch: 2.9233[0m
[32m[2022-08-25 21:12:28,628] [    INFO][0m - loss: 1.19739103, learning_rate: 8.531746031746032e-06, global_step: 2220, interval_runtime: 6.1316, interval_samples_per_second: 0.652, interval_steps_per_second: 1.631, epoch: 2.9365[0m
[32m[2022-08-25 21:12:34,854] [    INFO][0m - loss: 1.36700411, learning_rate: 8.525132275132276e-06, global_step: 2230, interval_runtime: 6.2256, interval_samples_per_second: 0.643, interval_steps_per_second: 1.606, epoch: 2.9497[0m
[32m[2022-08-25 21:12:41,049] [    INFO][0m - loss: 1.0432023, learning_rate: 8.518518518518519e-06, global_step: 2240, interval_runtime: 6.1948, interval_samples_per_second: 0.646, interval_steps_per_second: 1.614, epoch: 2.963[0m
[32m[2022-08-25 21:12:47,215] [    INFO][0m - loss: 1.24842386, learning_rate: 8.511904761904762e-06, global_step: 2250, interval_runtime: 6.1661, interval_samples_per_second: 0.649, interval_steps_per_second: 1.622, epoch: 2.9762[0m
[32m[2022-08-25 21:12:53,520] [    INFO][0m - loss: 1.31377048, learning_rate: 8.505291005291007e-06, global_step: 2260, interval_runtime: 6.3045, interval_samples_per_second: 0.634, interval_steps_per_second: 1.586, epoch: 2.9894[0m
[32m[2022-08-25 21:12:59,922] [    INFO][0m - loss: 1.29525919, learning_rate: 8.49867724867725e-06, global_step: 2270, interval_runtime: 6.402, interval_samples_per_second: 0.625, interval_steps_per_second: 1.562, epoch: 3.0026[0m
[32m[2022-08-25 21:13:06,178] [    INFO][0m - loss: 0.85491037, learning_rate: 8.492063492063492e-06, global_step: 2280, interval_runtime: 6.2569, interval_samples_per_second: 0.639, interval_steps_per_second: 1.598, epoch: 3.0159[0m
[32m[2022-08-25 21:13:12,463] [    INFO][0m - loss: 0.95204744, learning_rate: 8.485449735449735e-06, global_step: 2290, interval_runtime: 6.2849, interval_samples_per_second: 0.636, interval_steps_per_second: 1.591, epoch: 3.0291[0m
[32m[2022-08-25 21:13:18,737] [    INFO][0m - loss: 0.51799207, learning_rate: 8.47883597883598e-06, global_step: 2300, interval_runtime: 6.2736, interval_samples_per_second: 0.638, interval_steps_per_second: 1.594, epoch: 3.0423[0m
[32m[2022-08-25 21:13:18,737] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 21:13:18,737] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-25 21:13:18,738] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 21:13:18,738] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 21:13:18,738] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-25 21:15:18,672] [    INFO][0m - eval_loss: 1.984755516052246, eval_accuracy: 0.5382374362709396, eval_runtime: 119.9336, eval_samples_per_second: 11.448, eval_steps_per_second: 0.359, epoch: 3.0423[0m
[32m[2022-08-25 21:15:18,673] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2300[0m
[32m[2022-08-25 21:15:18,673] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 21:15:20,584] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2300/tokenizer_config.json[0m
[32m[2022-08-25 21:15:20,585] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2300/special_tokens_map.json[0m
[32m[2022-08-25 21:15:29,539] [    INFO][0m - loss: 1.28122272, learning_rate: 8.472222222222223e-06, global_step: 2310, interval_runtime: 130.8024, interval_samples_per_second: 0.031, interval_steps_per_second: 0.076, epoch: 3.0556[0m
[32m[2022-08-25 21:15:35,991] [    INFO][0m - loss: 0.51712112, learning_rate: 8.465608465608466e-06, global_step: 2320, interval_runtime: 6.4519, interval_samples_per_second: 0.62, interval_steps_per_second: 1.55, epoch: 3.0688[0m
[32m[2022-08-25 21:15:42,360] [    INFO][0m - loss: 0.97619095, learning_rate: 8.45899470899471e-06, global_step: 2330, interval_runtime: 6.3689, interval_samples_per_second: 0.628, interval_steps_per_second: 1.57, epoch: 3.082[0m
[32m[2022-08-25 21:15:48,741] [    INFO][0m - loss: 1.26177998, learning_rate: 8.452380952380953e-06, global_step: 2340, interval_runtime: 6.3807, interval_samples_per_second: 0.627, interval_steps_per_second: 1.567, epoch: 3.0952[0m
[32m[2022-08-25 21:15:55,843] [    INFO][0m - loss: 0.89963779, learning_rate: 8.445767195767196e-06, global_step: 2350, interval_runtime: 6.4218, interval_samples_per_second: 0.623, interval_steps_per_second: 1.557, epoch: 3.1085[0m
[32m[2022-08-25 21:16:02,249] [    INFO][0m - loss: 1.07090483, learning_rate: 8.43915343915344e-06, global_step: 2360, interval_runtime: 7.0865, interval_samples_per_second: 0.564, interval_steps_per_second: 1.411, epoch: 3.1217[0m
[32m[2022-08-25 21:16:08,673] [    INFO][0m - loss: 1.11818199, learning_rate: 8.432539682539684e-06, global_step: 2370, interval_runtime: 6.4241, interval_samples_per_second: 0.623, interval_steps_per_second: 1.557, epoch: 3.1349[0m
[32m[2022-08-25 21:16:15,107] [    INFO][0m - loss: 0.82236385, learning_rate: 8.425925925925926e-06, global_step: 2380, interval_runtime: 6.4327, interval_samples_per_second: 0.622, interval_steps_per_second: 1.555, epoch: 3.1481[0m
[32m[2022-08-25 21:16:21,651] [    INFO][0m - loss: 1.17479172, learning_rate: 8.419312169312171e-06, global_step: 2390, interval_runtime: 6.5457, interval_samples_per_second: 0.611, interval_steps_per_second: 1.528, epoch: 3.1614[0m
[32m[2022-08-25 21:16:28,088] [    INFO][0m - loss: 0.64493437, learning_rate: 8.412698412698414e-06, global_step: 2400, interval_runtime: 6.4369, interval_samples_per_second: 0.621, interval_steps_per_second: 1.554, epoch: 3.1746[0m
[32m[2022-08-25 21:16:28,089] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 21:16:28,089] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-25 21:16:28,089] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 21:16:28,089] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 21:16:28,089] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-25 21:18:32,974] [    INFO][0m - eval_loss: 1.9085466861724854, eval_accuracy: 0.5396941005098325, eval_runtime: 124.8842, eval_samples_per_second: 10.994, eval_steps_per_second: 0.344, epoch: 3.1746[0m
[32m[2022-08-25 21:18:32,974] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2400[0m
[32m[2022-08-25 21:18:32,974] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 21:18:34,864] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2400/tokenizer_config.json[0m
[32m[2022-08-25 21:18:34,864] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2400/special_tokens_map.json[0m
[32m[2022-08-25 21:18:37,087] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-25 21:18:37,087] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-1800 (score: 0.5615440640932265).[0m
[32m[2022-08-25 21:18:38,398] [    INFO][0m - train_runtime: 2721.6319, train_samples_per_second: 22.222, train_steps_per_second: 5.555, train_loss: 1.7125545620918274, epoch: 3.1746[0m
[32m[2022-08-25 21:18:38,399] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-25 21:18:38,399] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 21:18:42,053] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-25 21:18:42,054] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-25 21:18:42,056] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-25 21:18:42,056] [    INFO][0m -   epoch                    =     3.1746[0m
[32m[2022-08-25 21:18:42,056] [    INFO][0m -   train_loss               =     1.7126[0m
[32m[2022-08-25 21:18:42,056] [    INFO][0m -   train_runtime            = 0:45:21.63[0m
[32m[2022-08-25 21:18:42,056] [    INFO][0m -   train_samples_per_second =     22.222[0m
[32m[2022-08-25 21:18:42,057] [    INFO][0m -   train_steps_per_second   =      5.555[0m
[32m[2022-08-25 21:18:42,067] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-25 21:18:42,067] [    INFO][0m -   Num examples = 1749[0m
[32m[2022-08-25 21:18:42,067] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 21:18:42,067] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 21:18:42,068] [    INFO][0m -   Total prediction steps = 55[0m
[32m[2022-08-25 21:21:23,144] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-25 21:21:23,145] [    INFO][0m -   test_accuracy           =     0.5506[0m
[32m[2022-08-25 21:21:23,145] [    INFO][0m -   test_loss               =     1.8393[0m
[32m[2022-08-25 21:21:23,145] [    INFO][0m -   test_runtime            = 0:02:41.07[0m
[32m[2022-08-25 21:21:23,145] [    INFO][0m -   test_samples_per_second =     10.858[0m
[32m[2022-08-25 21:21:23,145] [    INFO][0m -   test_steps_per_second   =      0.341[0m
[32m[2022-08-25 21:21:23,146] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-25 21:21:23,146] [    INFO][0m -   Num examples = 2600[0m
[32m[2022-08-25 21:21:23,146] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 21:21:23,146] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 21:21:23,146] [    INFO][0m -   Total prediction steps = 82[0m
Traceback (most recent call last):
  File "train_single.py", line 168, in <module>
    if __name__ == '__main__':
  File "train_single.py", line 162, in main
    test_ret = trainer.predict(test_ds)
  File "/ssd2/wanghuijuan03/prompt/PaddleNLP/examples/benchmark/fewclue/postprocess.py", line 29, in postprocess
    remap = json.loads(fp.readline().strip())
  File "/ssd2/wanghuijuan03/anaconda3/envs/prompt/lib/python3.8/json/__init__.py", line 357, in loads
    return _default_decoder.decode(s)
  File "/ssd2/wanghuijuan03/anaconda3/envs/prompt/lib/python3.8/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/ssd2/wanghuijuan03/anaconda3/envs/prompt/lib/python3.8/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)
