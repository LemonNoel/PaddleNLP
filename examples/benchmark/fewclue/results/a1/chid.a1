[32m[2022-08-25 11:20:51,329] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-25 11:20:51,330] [    INFO][0m - ============================================================[0m
[32m[2022-08-25 11:20:51,330] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-25 11:20:51,330] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-25 11:20:51,330] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-25 11:20:51,330] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-25 11:20:51,330] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-25 11:20:51,330] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-08-25 11:20:51,330] [    INFO][0m - [0m
[32m[2022-08-25 11:20:51,330] [    INFO][0m - ============================================================[0m
[32m[2022-08-25 11:20:51,330] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-25 11:20:51,330] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-25 11:20:51,331] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-25 11:20:51,331] [    INFO][0m - prompt                        :{'soft':'Â∑≤Áü•ÂÄôÈÄâËØçÊúâ'}{'text':'text_b'}{'sep'}{'text':'text_a'}{'soft':'ÈóÆÔºöËøôÂè•ËØùÁöÑÁ©∫Ê†ºÂ§ÑÂ∫îËØ•Â°´Á¨¨'}{'mask'}{'soft':'‰∏™ËØç'}[0m
[32m[2022-08-25 11:20:51,331] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-25 11:20:51,331] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-25 11:20:51,331] [    INFO][0m - task_name                     :chid[0m
[32m[2022-08-25 11:20:51,331] [    INFO][0m - [0m
[32m[2022-08-25 11:20:51,331] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0825 11:20:51.332733 12357 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0825 11:20:51.336804 12357 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-25 11:20:54,184] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-08-25 11:20:54,208] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-08-25 11:20:54,208] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[33m[2022-08-25 11:20:54,215] [ WARNING][0m - Encoder has already set as lstm, change `prompt_encoder` will reset parameters.[0m
[32m[2022-08-25 11:20:54,230] [    INFO][0m - Using template: [{'add_prefix_space': '', 'soft': 'Â∑≤'}, {'add_prefix_space': '', 'soft': 'Áü•'}, {'add_prefix_space': '', 'soft': 'ÂÄô'}, {'add_prefix_space': '', 'soft': 'ÈÄâ'}, {'add_prefix_space': '', 'soft': 'ËØç'}, {'add_prefix_space': '', 'soft': 'Êúâ'}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'sep': None}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'soft': 'ÈóÆ'}, {'add_prefix_space': '', 'soft': 'Ôºö'}, {'add_prefix_space': '', 'soft': 'Ëøô'}, {'add_prefix_space': '', 'soft': 'Âè•'}, {'add_prefix_space': '', 'soft': 'ËØù'}, {'add_prefix_space': '', 'soft': 'ÁöÑ'}, {'add_prefix_space': '', 'soft': 'Á©∫'}, {'add_prefix_space': '', 'soft': 'Ê†º'}, {'add_prefix_space': '', 'soft': 'Â§Ñ'}, {'add_prefix_space': '', 'soft': 'Â∫î'}, {'add_prefix_space': '', 'soft': 'ËØ•'}, {'add_prefix_space': '', 'soft': 'Â°´'}, {'add_prefix_space': '', 'soft': 'Á¨¨'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'soft': '‰∏™'}, {'add_prefix_space': '', 'soft': 'ËØç'}][0m
2022-08-25 11:20:54,236 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-25 11:20:54,376] [    INFO][0m - ============================================================[0m
[32m[2022-08-25 11:20:54,376] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-25 11:20:54,376] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-25 11:20:54,377] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-25 11:20:54,377] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-25 11:20:54,377] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-25 11:20:54,377] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-25 11:20:54,377] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-25 11:20:54,377] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-25 11:20:54,377] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-25 11:20:54,377] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-25 11:20:54,377] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-25 11:20:54,377] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-25 11:20:54,378] [    INFO][0m - disable_tqdm                  :False[0m
[32m[2022-08-25 11:20:54,378] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-25 11:20:54,378] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-25 11:20:54,378] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-25 11:20:54,378] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-25 11:20:54,378] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-25 11:20:54,378] [    INFO][0m - eval_steps                    :10[0m
[32m[2022-08-25 11:20:54,378] [    INFO][0m - evaluation_strategy           :IntervalStrategy.EPOCH[0m
[32m[2022-08-25 11:20:54,378] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-25 11:20:54,378] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-25 11:20:54,378] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-25 11:20:54,378] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-25 11:20:54,378] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-25 11:20:54,379] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-25 11:20:54,379] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-25 11:20:54,379] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-25 11:20:54,379] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-25 11:20:54,379] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-25 11:20:54,379] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-25 11:20:54,379] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-25 11:20:54,379] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-25 11:20:54,379] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-25 11:20:54,379] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-25 11:20:54,379] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-25 11:20:54,379] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug25_11-20-51_instance-3bwob41y-01[0m
[32m[2022-08-25 11:20:54,379] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-25 11:20:54,379] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-25 11:20:54,380] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-25 11:20:54,380] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-25 11:20:54,380] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-25 11:20:54,380] [    INFO][0m - max_seq_length                :256[0m
[32m[2022-08-25 11:20:54,380] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-25 11:20:54,380] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-25 11:20:54,380] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-25 11:20:54,380] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-25 11:20:54,380] [    INFO][0m - num_train_epochs              :20.0[0m
[32m[2022-08-25 11:20:54,380] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-25 11:20:54,380] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-25 11:20:54,380] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-25 11:20:54,380] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-25 11:20:54,381] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-25 11:20:54,381] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-25 11:20:54,381] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-25 11:20:54,381] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-25 11:20:54,381] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-25 11:20:54,381] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-25 11:20:54,381] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-25 11:20:54,381] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-25 11:20:54,381] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-25 11:20:54,381] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-25 11:20:54,381] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-25 11:20:54,381] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-25 11:20:54,381] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-25 11:20:54,381] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-25 11:20:54,382] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-25 11:20:54,382] [    INFO][0m - save_steps                    :500[0m
[32m[2022-08-25 11:20:54,382] [    INFO][0m - save_strategy                 :IntervalStrategy.EPOCH[0m
[32m[2022-08-25 11:20:54,382] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-25 11:20:54,382] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-25 11:20:54,382] [    INFO][0m - seed                          :42[0m
[32m[2022-08-25 11:20:54,382] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-25 11:20:54,382] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-25 11:20:54,382] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-25 11:20:54,382] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-25 11:20:54,382] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-25 11:20:54,382] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-25 11:20:54,382] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-25 11:20:54,383] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-25 11:20:54,383] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-25 11:20:54,383] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-25 11:20:54,383] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-25 11:20:54,383] [    INFO][0m - [0m
[32m[2022-08-25 11:20:54,385] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-25 11:20:54,385] [    INFO][0m -   Num examples = 202[0m
[32m[2022-08-25 11:20:54,385] [    INFO][0m -   Num Epochs = 20[0m
[32m[2022-08-25 11:20:54,385] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-25 11:20:54,385] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-25 11:20:54,385] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-25 11:20:54,385] [    INFO][0m -   Total optimization steps = 520.0[0m
[32m[2022-08-25 11:20:54,385] [    INFO][0m -   Total num train samples = 4040[0m
  0%|          | 0/520 [00:00<?, ?it/s]  0%|          | 1/520 [00:01<13:54,  1.61s/it]  0%|          | 2/520 [00:01<06:36,  1.31it/s]  1%|          | 3/520 [00:01<04:15,  2.02it/s]  1%|          | 4/520 [00:02<03:09,  2.73it/s]  1%|          | 5/520 [00:02<02:32,  3.38it/s]  1%|          | 6/520 [00:02<02:09,  3.97it/s]  1%|‚ñè         | 7/520 [00:02<01:55,  4.44it/s]  2%|‚ñè         | 8/520 [00:02<01:46,  4.82it/s]  2%|‚ñè         | 9/520 [00:02<01:39,  5.12it/s]  2%|‚ñè         | 10/520 [00:03<01:35,  5.34it/s]                                                  2%|‚ñè         | 10/520 [00:03<01:35,  5.34it/s]  2%|‚ñè         | 11/520 [00:03<01:34,  5.40it/s]  2%|‚ñè         | 12/520 [00:03<01:31,  5.56it/s]  2%|‚ñé         | 13/520 [00:03<01:29,  5.65it/s]  3%|‚ñé         | 14/520 [00:03<01:30,  5.59it/s]  3%|‚ñé         | 15/520 [00:04<01:32,  5.46it/s]  3%|‚ñé         | 16/520 [00:04<01:30,  5.57it/s]  3%|‚ñé         | 17/520 [00:04<01:29,  5.60it/s]  3%|‚ñé         | 18/520 [00:04<01:28,  5.67it/s]  4%|‚ñé         | 19/520 [00:04<01:28,  5.68it/s]  4%|‚ñç         | 20/520 [00:04<01:28,  5.67it/s]                                                  4%|‚ñç         | 20/520 [00:04<01:28,  5.67it/s]  4%|‚ñç         | 21/520 [00:05<01:26,  5.80it/s]  4%|‚ñç         | 22/520 [00:05<01:23,  5.93it/s]  4%|‚ñç         | 23/520 [00:05<01:22,  6.03it/s]  5%|‚ñç         | 24/520 [00:05<01:21,  6.10it/s]  5%|‚ñç         | 25/520 [00:05<01:20,  6.15it/s][32m[2022-08-25 11:21:00,194] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 11:21:00,194] [    INFO][0m -   Num examples = 202[0m
[32m[2022-08-25 11:21:00,194] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 11:21:00,195] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 11:21:00,195] [    INFO][0m -   Total prediction steps = 7[0m
loss: 2.29395714, learning_rate: 2.9423076923076923e-05, global_step: 10, interval_runtime: 3.1556, interval_samples_per_second: 2.535, interval_steps_per_second: 3.169, epoch: 0.3846
loss: 1.96692829, learning_rate: 2.884615384615385e-05, global_step: 20, interval_runtime: 1.7638, interval_samples_per_second: 4.536, interval_steps_per_second: 5.669, epoch: 0.7692

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|‚ñà‚ñà‚ñä       | 2/7 [00:00<00:00,  9.63it/s][A
 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 3/7 [00:00<00:00,  7.56it/s][A
 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 4/7 [00:00<00:00,  6.78it/s][A
 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 5/7 [00:00<00:00,  6.38it/s][A
 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 6/7 [00:00<00:00,  5.91it/s][A                                                
                                             [A  5%|‚ñå         | 26/520 [00:07<01:20,  6.15it/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00<00:00,  5.91it/s][A
                                             [A[32m[2022-08-25 11:21:01,955] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-26[0m
[32m[2022-08-25 11:21:01,955] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 11:21:03,343] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-26/tokenizer_config.json[0m
[32m[2022-08-25 11:21:03,343] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-26/special_tokens_map.json[0m
  5%|‚ñå         | 27/520 [00:11<10:53,  1.33s/it]  5%|‚ñå         | 28/520 [00:11<08:32,  1.04s/it]  6%|‚ñå         | 29/520 [00:11<06:41,  1.22it/s]  6%|‚ñå         | 30/520 [00:11<05:15,  1.56it/s]                                                  6%|‚ñå         | 30/520 [00:11<05:15,  1.56it/s]  6%|‚ñå         | 31/520 [00:11<04:12,  1.94it/s]  6%|‚ñå         | 32/520 [00:11<03:24,  2.39it/s]  6%|‚ñã         | 33/520 [00:12<02:50,  2.86it/s]  7%|‚ñã         | 34/520 [00:12<02:24,  3.35it/s]  7%|‚ñã         | 35/520 [00:12<02:07,  3.81it/s]  7%|‚ñã         | 36/520 [00:12<01:55,  4.20it/s]  7%|‚ñã         | 37/520 [00:12<01:46,  4.55it/s]  7%|‚ñã         | 38/520 [00:13<01:40,  4.81it/s]  8%|‚ñä         | 39/520 [00:13<01:36,  4.96it/s]  8%|‚ñä         | 40/520 [00:13<01:34,  5.06it/s]                                                  8%|‚ñä         | 40/520 [00:13<01:34,  5.06it/s]  8%|‚ñä         | 41/520 [00:13<01:33,  5.11it/s]  8%|‚ñä         | 42/520 [00:13<01:30,  5.25it/s]  8%|‚ñä         | 43/520 [00:13<01:29,  5.35it/s]  8%|‚ñä         | 44/520 [00:14<01:27,  5.41it/s]  9%|‚ñä         | 45/520 [00:14<01:26,  5.48it/s]  9%|‚ñâ         | 46/520 [00:14<01:26,  5.47it/s]  9%|‚ñâ         | 47/520 [00:14<01:23,  5.68it/s]  9%|‚ñâ         | 48/520 [00:14<01:20,  5.87it/s]  9%|‚ñâ         | 49/520 [00:14<01:17,  6.04it/s] 10%|‚ñâ         | 50/520 [00:15<01:16,  6.15it/s]                                                 10%|‚ñâ         | 50/520 [00:15<01:16,  6.15it/s] 10%|‚ñâ         | 51/520 [00:15<01:15,  6.20it/s][32m[2022-08-25 11:21:09,777] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 11:21:09,778] [    INFO][0m -   Num examples = 202[0m
[32m[2022-08-25 11:21:09,778] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 11:21:09,778] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 11:21:09,778] [    INFO][0m -   Total prediction steps = 7[0m
eval_loss: 1.9849473237991333, eval_accuracy: 0.16336633663366337, eval_runtime: 1.7595, eval_samples_per_second: 114.803, eval_steps_per_second: 3.978, epoch: 1.0
loss: 1.96809902, learning_rate: 2.8269230769230768e-05, global_step: 30, interval_runtime: 6.7091, interval_samples_per_second: 1.192, interval_steps_per_second: 1.491, epoch: 1.1538
loss: 1.9678709, learning_rate: 2.7692307692307694e-05, global_step: 40, interval_runtime: 1.8065, interval_samples_per_second: 4.428, interval_steps_per_second: 5.535, epoch: 1.5385
loss: 2.06003571, learning_rate: 2.7115384615384616e-05, global_step: 50, interval_runtime: 1.7151, interval_samples_per_second: 4.665, interval_steps_per_second: 5.831, epoch: 1.9231

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|‚ñà‚ñà‚ñä       | 2/7 [00:00<00:00,  9.19it/s][A
 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 3/7 [00:00<00:00,  7.34it/s][A
 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 4/7 [00:00<00:00,  6.64it/s][A
 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 5/7 [00:00<00:00,  6.29it/s][A
 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 6/7 [00:00<00:00,  6.10it/s][A                                                
                                             [A 10%|‚ñà         | 52/520 [00:17<01:15,  6.20it/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00<00:00,  6.10it/s][A
                                             [A[32m[2022-08-25 11:21:11,653] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-52[0m
[32m[2022-08-25 11:21:11,653] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 11:21:13,140] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-52/tokenizer_config.json[0m
[32m[2022-08-25 11:21:13,140] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-52/special_tokens_map.json[0m
 10%|‚ñà         | 53/520 [00:20<10:35,  1.36s/it] 10%|‚ñà         | 54/520 [00:20<08:17,  1.07s/it] 11%|‚ñà         | 55/520 [00:21<06:29,  1.19it/s] 11%|‚ñà         | 56/520 [00:21<05:05,  1.52it/s] 11%|‚ñà         | 57/520 [00:21<04:03,  1.90it/s] 11%|‚ñà         | 58/520 [00:21<03:17,  2.34it/s] 11%|‚ñà‚ñè        | 59/520 [00:21<02:44,  2.80it/s] 12%|‚ñà‚ñè        | 60/520 [00:22<02:21,  3.26it/s]                                                 12%|‚ñà‚ñè        | 60/520 [00:22<02:21,  3.26it/s] 12%|‚ñà‚ñè        | 61/520 [00:22<02:06,  3.64it/s] 12%|‚ñà‚ñè        | 62/520 [00:22<01:54,  4.01it/s] 12%|‚ñà‚ñè        | 63/520 [00:22<01:45,  4.33it/s] 12%|‚ñà‚ñè        | 64/520 [00:22<01:38,  4.62it/s] 12%|‚ñà‚ñé        | 65/520 [00:23<01:33,  4.84it/s] 13%|‚ñà‚ñé        | 66/520 [00:23<01:30,  5.00it/s] 13%|‚ñà‚ñé        | 67/520 [00:23<01:28,  5.12it/s] 13%|‚ñà‚ñé        | 68/520 [00:23<01:26,  5.20it/s] 13%|‚ñà‚ñé        | 69/520 [00:23<01:25,  5.26it/s] 13%|‚ñà‚ñé        | 70/520 [00:23<01:24,  5.30it/s]                                                 13%|‚ñà‚ñé        | 70/520 [00:23<01:24,  5.30it/s] 14%|‚ñà‚ñé        | 71/520 [00:24<01:27,  5.13it/s] 14%|‚ñà‚ñç        | 72/520 [00:24<01:26,  5.18it/s] 14%|‚ñà‚ñç        | 73/520 [00:24<01:22,  5.44it/s] 14%|‚ñà‚ñç        | 74/520 [00:24<01:18,  5.69it/s] 14%|‚ñà‚ñç        | 75/520 [00:24<01:15,  5.87it/s] 15%|‚ñà‚ñç        | 76/520 [00:24<01:14,  5.98it/s] 15%|‚ñà‚ñç        | 77/520 [00:25<01:12,  6.10it/s][32m[2022-08-25 11:21:19,636] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 11:21:19,636] [    INFO][0m -   Num examples = 202[0m
[32m[2022-08-25 11:21:19,636] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 11:21:19,636] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 11:21:19,636] [    INFO][0m -   Total prediction steps = 7[0m
eval_loss: 1.9981179237365723, eval_accuracy: 0.12871287128712872, eval_runtime: 1.875, eval_samples_per_second: 107.731, eval_steps_per_second: 3.733, epoch: 2.0
loss: 1.97158508, learning_rate: 2.6538461538461538e-05, global_step: 60, interval_runtime: 6.9606, interval_samples_per_second: 1.149, interval_steps_per_second: 1.437, epoch: 2.3077
loss: 1.9964035, learning_rate: 2.5961538461538464e-05, global_step: 70, interval_runtime: 1.8655, interval_samples_per_second: 4.288, interval_steps_per_second: 5.361, epoch: 2.6923

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|‚ñà‚ñà‚ñä       | 2/7 [00:00<00:00,  9.03it/s][A
 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 3/7 [00:00<00:00,  7.30it/s][A
 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 4/7 [00:00<00:00,  6.68it/s][A
 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 5/7 [00:00<00:00,  6.35it/s][A
 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 6/7 [00:00<00:00,  6.16it/s][A                                                
                                             [A 15%|‚ñà‚ñå        | 78/520 [00:27<01:12,  6.10it/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00<00:00,  6.16it/s][A
                                             [A[32m[2022-08-25 11:21:21,646] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-78[0m
[32m[2022-08-25 11:21:21,646] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 11:21:23,040] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-78/tokenizer_config.json[0m
[32m[2022-08-25 11:21:23,040] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-78/special_tokens_map.json[0m
 15%|‚ñà‚ñå        | 79/520 [00:30<10:26,  1.42s/it] 15%|‚ñà‚ñå        | 80/520 [00:31<08:10,  1.11s/it]                                                 15%|‚ñà‚ñå        | 80/520 [00:31<08:10,  1.11s/it] 16%|‚ñà‚ñå        | 81/520 [00:31<06:24,  1.14it/s] 16%|‚ñà‚ñå        | 82/520 [00:31<05:02,  1.45it/s] 16%|‚ñà‚ñå        | 83/520 [00:31<04:00,  1.82it/s] 16%|‚ñà‚ñå        | 84/520 [00:31<03:15,  2.23it/s] 16%|‚ñà‚ñã        | 85/520 [00:32<02:42,  2.68it/s] 17%|‚ñà‚ñã        | 86/520 [00:32<02:18,  3.13it/s] 17%|‚ñà‚ñã        | 87/520 [00:32<02:01,  3.55it/s] 17%|‚ñà‚ñã        | 88/520 [00:32<01:49,  3.93it/s] 17%|‚ñà‚ñã        | 89/520 [00:32<01:41,  4.24it/s] 17%|‚ñà‚ñã        | 90/520 [00:33<01:35,  4.50it/s]                                                 17%|‚ñà‚ñã        | 90/520 [00:33<01:35,  4.50it/s] 18%|‚ñà‚ñä        | 91/520 [00:33<01:33,  4.61it/s] 18%|‚ñà‚ñä        | 92/520 [00:33<01:30,  4.72it/s] 18%|‚ñà‚ñä        | 93/520 [00:33<01:27,  4.86it/s] 18%|‚ñà‚ñä        | 94/520 [00:33<01:26,  4.93it/s] 18%|‚ñà‚ñä        | 95/520 [00:34<01:24,  5.02it/s] 18%|‚ñà‚ñä        | 96/520 [00:34<01:23,  5.08it/s] 19%|‚ñà‚ñä        | 97/520 [00:34<01:22,  5.12it/s] 19%|‚ñà‚ñâ        | 98/520 [00:34<01:22,  5.13it/s] 19%|‚ñà‚ñâ        | 99/520 [00:34<01:18,  5.36it/s] 19%|‚ñà‚ñâ        | 100/520 [00:34<01:14,  5.62it/s]                                                  19%|‚ñà‚ñâ        | 100/520 [00:34<01:14,  5.62it/s] 19%|‚ñà‚ñâ        | 101/520 [00:35<01:12,  5.79it/s] 20%|‚ñà‚ñâ        | 102/520 [00:35<01:10,  5.93it/s] 20%|‚ñà‚ñâ        | 103/520 [00:35<01:09,  6.04it/s][32m[2022-08-25 11:21:29,879] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 11:21:29,880] [    INFO][0m -   Num examples = 202[0m
[32m[2022-08-25 11:21:29,880] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 11:21:29,880] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 11:21:29,880] [    INFO][0m -   Total prediction steps = 7[0m
eval_loss: 1.9642078876495361, eval_accuracy: 0.15346534653465346, eval_runtime: 2.0087, eval_samples_per_second: 100.562, eval_steps_per_second: 3.485, epoch: 3.0
loss: 2.01379604, learning_rate: 2.5384615384615386e-05, global_step: 80, interval_runtime: 7.1482, interval_samples_per_second: 1.119, interval_steps_per_second: 1.399, epoch: 3.0769
loss: 1.98181, learning_rate: 2.4807692307692305e-05, global_step: 90, interval_runtime: 1.9203, interval_samples_per_second: 4.166, interval_steps_per_second: 5.208, epoch: 3.4615
loss: 1.96621685, learning_rate: 2.423076923076923e-05, global_step: 100, interval_runtime: 1.8844, interval_samples_per_second: 4.245, interval_steps_per_second: 5.307, epoch: 3.8462

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|‚ñà‚ñà‚ñä       | 2/7 [00:00<00:00,  9.01it/s][A
 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 3/7 [00:00<00:00,  7.28it/s][A
 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 4/7 [00:00<00:00,  6.42it/s][A
 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 5/7 [00:00<00:00,  6.17it/s][A
 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 6/7 [00:00<00:00,  6.04it/s][A                                                 
                                             [A 20%|‚ñà‚ñà        | 104/520 [00:37<01:08,  6.04it/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00<00:00,  6.04it/s][A
                                             [A[32m[2022-08-25 11:21:32,010] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-104[0m
[32m[2022-08-25 11:21:32,010] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 11:21:33,414] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-104/tokenizer_config.json[0m
[32m[2022-08-25 11:21:33,414] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-104/special_tokens_map.json[0m
 20%|‚ñà‚ñà        | 105/520 [00:41<09:46,  1.41s/it] 20%|‚ñà‚ñà        | 106/520 [00:41<07:40,  1.11s/it] 21%|‚ñà‚ñà        | 107/520 [00:41<06:00,  1.15it/s] 21%|‚ñà‚ñà        | 108/520 [00:41<04:44,  1.45it/s] 21%|‚ñà‚ñà        | 109/520 [00:41<03:46,  1.81it/s] 21%|‚ñà‚ñà        | 110/520 [00:42<03:04,  2.23it/s]                                                  21%|‚ñà‚ñà        | 110/520 [00:42<03:04,  2.23it/s] 21%|‚ñà‚ñà‚ñè       | 111/520 [00:42<02:34,  2.65it/s] 22%|‚ñà‚ñà‚ñè       | 112/520 [00:42<02:11,  3.09it/s] 22%|‚ñà‚ñà‚ñè       | 113/520 [00:42<01:57,  3.46it/s] 22%|‚ñà‚ñà‚ñè       | 114/520 [00:42<01:47,  3.79it/s] 22%|‚ñà‚ñà‚ñè       | 115/520 [00:43<01:38,  4.10it/s] 22%|‚ñà‚ñà‚ñè       | 116/520 [00:43<01:33,  4.34it/s] 22%|‚ñà‚ñà‚ñé       | 117/520 [00:43<01:28,  4.53it/s] 23%|‚ñà‚ñà‚ñé       | 118/520 [00:43<01:25,  4.67it/s] 23%|‚ñà‚ñà‚ñé       | 119/520 [00:43<01:23,  4.78it/s] 23%|‚ñà‚ñà‚ñé       | 120/520 [00:44<01:22,  4.85it/s]                                                  23%|‚ñà‚ñà‚ñé       | 120/520 [00:44<01:22,  4.85it/s] 23%|‚ñà‚ñà‚ñé       | 121/520 [00:44<01:22,  4.82it/s] 23%|‚ñà‚ñà‚ñé       | 122/520 [00:44<01:21,  4.85it/s] 24%|‚ñà‚ñà‚ñé       | 123/520 [00:44<01:21,  4.86it/s] 24%|‚ñà‚ñà‚ñç       | 124/520 [00:44<01:21,  4.88it/s] 24%|‚ñà‚ñà‚ñç       | 125/520 [00:45<01:16,  5.15it/s] 24%|‚ñà‚ñà‚ñç       | 126/520 [00:45<01:12,  5.45it/s] 24%|‚ñà‚ñà‚ñç       | 127/520 [00:45<01:08,  5.70it/s] 25%|‚ñà‚ñà‚ñç       | 128/520 [00:45<01:07,  5.83it/s] 25%|‚ñà‚ñà‚ñç       | 129/520 [00:45<01:05,  5.96it/s]                                                  25%|‚ñà‚ñà‚ñå       | 130/520 [00:45<01:05,  5.96it/s][32m[2022-08-25 11:21:40,225] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 11:21:40,225] [    INFO][0m -   Num examples = 202[0m
[32m[2022-08-25 11:21:40,225] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 11:21:40,225] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 11:21:40,225] [    INFO][0m -   Total prediction steps = 7[0m
eval_loss: 1.9727227687835693, eval_accuracy: 0.13366336633663367, eval_runtime: 2.1295, eval_samples_per_second: 94.859, eval_steps_per_second: 3.287, epoch: 4.0
loss: 1.99932137, learning_rate: 2.3653846153846153e-05, global_step: 110, interval_runtime: 7.2014, interval_samples_per_second: 1.111, interval_steps_per_second: 1.389, epoch: 4.2308
loss: 1.88371029, learning_rate: 2.307692307692308e-05, global_step: 120, interval_runtime: 1.9965, interval_samples_per_second: 4.007, interval_steps_per_second: 5.009, epoch: 4.6154
loss: 1.81656227, learning_rate: 2.25e-05, global_step: 130, interval_runtime: 1.7108, interval_samples_per_second: 4.676, interval_steps_per_second: 5.845, epoch: 5.0

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|‚ñà‚ñà‚ñä       | 2/7 [00:00<00:00,  8.88it/s][A
 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 3/7 [00:00<00:00,  7.23it/s][A
 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 4/7 [00:00<00:00,  6.60it/s][A
 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 5/7 [00:00<00:00,  6.27it/s][A
 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 6/7 [00:00<00:00,  6.09it/s][A                                                 
                                             [A 25%|‚ñà‚ñà‚ñå       | 130/520 [00:48<01:05,  5.96it/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00<00:00,  6.09it/s][A
                                             [A[32m[2022-08-25 11:21:42,502] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-130[0m
[32m[2022-08-25 11:21:42,502] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 11:21:43,886] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-130/tokenizer_config.json[0m
[32m[2022-08-25 11:21:43,886] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-130/special_tokens_map.json[0m
 25%|‚ñà‚ñà‚ñå       | 131/520 [00:51<09:24,  1.45s/it] 25%|‚ñà‚ñà‚ñå       | 132/520 [00:51<07:22,  1.14s/it] 26%|‚ñà‚ñà‚ñå       | 133/520 [00:52<05:48,  1.11it/s] 26%|‚ñà‚ñà‚ñå       | 134/520 [00:52<04:33,  1.41it/s] 26%|‚ñà‚ñà‚ñå       | 135/520 [00:52<03:37,  1.77it/s] 26%|‚ñà‚ñà‚ñå       | 136/520 [00:52<02:56,  2.18it/s] 26%|‚ñà‚ñà‚ñã       | 137/520 [00:52<02:27,  2.60it/s] 27%|‚ñà‚ñà‚ñã       | 138/520 [00:53<02:05,  3.04it/s] 27%|‚ñà‚ñà‚ñã       | 139/520 [00:53<01:50,  3.46it/s] 27%|‚ñà‚ñà‚ñã       | 140/520 [00:53<01:39,  3.83it/s]                                                  27%|‚ñà‚ñà‚ñã       | 140/520 [00:53<01:39,  3.83it/s] 27%|‚ñà‚ñà‚ñã       | 141/520 [00:53<01:32,  4.09it/s] 27%|‚ñà‚ñà‚ñã       | 142/520 [00:53<01:26,  4.35it/s] 28%|‚ñà‚ñà‚ñä       | 143/520 [00:54<01:23,  4.54it/s] 28%|‚ñà‚ñà‚ñä       | 144/520 [00:54<01:19,  4.70it/s] 28%|‚ñà‚ñà‚ñä       | 145/520 [00:54<01:17,  4.83it/s] 28%|‚ñà‚ñà‚ñä       | 146/520 [00:54<01:15,  4.93it/s] 28%|‚ñà‚ñà‚ñä       | 147/520 [00:54<01:14,  4.99it/s] 28%|‚ñà‚ñà‚ñä       | 148/520 [00:54<01:13,  5.03it/s] 29%|‚ñà‚ñà‚ñä       | 149/520 [00:55<01:13,  5.06it/s] 29%|‚ñà‚ñà‚ñâ       | 150/520 [00:55<01:12,  5.08it/s]                                                  29%|‚ñà‚ñà‚ñâ       | 150/520 [00:55<01:12,  5.08it/s] 29%|‚ñà‚ñà‚ñâ       | 151/520 [00:55<01:11,  5.18it/s] 29%|‚ñà‚ñà‚ñâ       | 152/520 [00:55<01:07,  5.48it/s] 29%|‚ñà‚ñà‚ñâ       | 153/520 [00:55<01:04,  5.71it/s] 30%|‚ñà‚ñà‚ñâ       | 154/520 [00:56<01:01,  5.91it/s] 30%|‚ñà‚ñà‚ñâ       | 155/520 [00:56<01:00,  6.01it/s][32m[2022-08-25 11:21:50,682] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 11:21:50,682] [    INFO][0m -   Num examples = 202[0m
[32m[2022-08-25 11:21:50,682] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 11:21:50,682] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 11:21:50,682] [    INFO][0m -   Total prediction steps = 7[0m
eval_loss: 2.0220744609832764, eval_accuracy: 0.16831683168316833, eval_runtime: 2.2753, eval_samples_per_second: 88.781, eval_steps_per_second: 3.077, epoch: 5.0
loss: 1.80987644, learning_rate: 2.1923076923076924e-05, global_step: 140, interval_runtime: 7.6006, interval_samples_per_second: 1.053, interval_steps_per_second: 1.316, epoch: 5.3846
loss: 1.72752838, learning_rate: 2.1346153846153846e-05, global_step: 150, interval_runtime: 1.9603, interval_samples_per_second: 4.081, interval_steps_per_second: 5.101, epoch: 5.7692

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|‚ñà‚ñà‚ñä       | 2/7 [00:00<00:00,  8.73it/s][A
 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 3/7 [00:00<00:00,  7.16it/s][A
 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 4/7 [00:00<00:00,  6.56it/s][A
 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 5/7 [00:00<00:00,  6.25it/s][A
 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 6/7 [00:00<00:00,  6.06it/s][A                                                 
                                             [A 30%|‚ñà‚ñà‚ñà       | 156/520 [00:58<01:00,  6.01it/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00<00:00,  6.06it/s][A
                                             [A[32m[2022-08-25 11:21:53,058] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-156[0m
[32m[2022-08-25 11:21:53,058] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 11:21:54,522] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-156/tokenizer_config.json[0m
[32m[2022-08-25 11:21:54,522] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-156/special_tokens_map.json[0m
 30%|‚ñà‚ñà‚ñà       | 157/520 [01:02<09:08,  1.51s/it] 30%|‚ñà‚ñà‚ñà       | 158/520 [01:02<07:10,  1.19s/it] 31%|‚ñà‚ñà‚ñà       | 159/520 [01:02<05:35,  1.08it/s] 31%|‚ñà‚ñà‚ñà       | 160/520 [01:02<04:22,  1.37it/s]                                                  31%|‚ñà‚ñà‚ñà       | 160/520 [01:02<04:22,  1.37it/s] 31%|‚ñà‚ñà‚ñà       | 161/520 [01:03<03:30,  1.71it/s] 31%|‚ñà‚ñà‚ñà       | 162/520 [01:03<02:54,  2.05it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 163/520 [01:03<02:24,  2.47it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 164/520 [01:03<02:02,  2.91it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 165/520 [01:04<01:46,  3.32it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 166/520 [01:04<01:35,  3.69it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 167/520 [01:04<01:28,  4.01it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 168/520 [01:04<01:22,  4.27it/s] 32%|‚ñà‚ñà‚ñà‚ñé      | 169/520 [01:04<01:18,  4.46it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 170/520 [01:05<01:16,  4.56it/s]                                                  33%|‚ñà‚ñà‚ñà‚ñé      | 170/520 [01:05<01:16,  4.56it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 171/520 [01:05<01:15,  4.62it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 172/520 [01:05<01:13,  4.71it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 173/520 [01:05<01:12,  4.80it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 174/520 [01:05<01:11,  4.85it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 175/520 [01:06<01:10,  4.90it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 176/520 [01:06<01:09,  4.93it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 177/520 [01:06<01:06,  5.13it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 178/520 [01:06<01:02,  5.44it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 179/520 [01:06<01:00,  5.66it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 180/520 [01:06<00:57,  5.88it/s]                                                  35%|‚ñà‚ñà‚ñà‚ñç      | 180/520 [01:06<00:57,  5.88it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 181/520 [01:07<00:56,  6.00it/s][32m[2022-08-25 11:22:01,528] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 11:22:01,528] [    INFO][0m -   Num examples = 202[0m
[32m[2022-08-25 11:22:01,529] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 11:22:01,529] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 11:22:01,529] [    INFO][0m -   Total prediction steps = 7[0m
eval_loss: 2.14009165763855, eval_accuracy: 0.12871287128712872, eval_runtime: 2.3749, eval_samples_per_second: 85.055, eval_steps_per_second: 2.947, epoch: 6.0
loss: 1.61364479, learning_rate: 2.076923076923077e-05, global_step: 160, interval_runtime: 7.5833, interval_samples_per_second: 1.055, interval_steps_per_second: 1.319, epoch: 6.1538
loss: 1.50399303, learning_rate: 2.0192307692307694e-05, global_step: 170, interval_runtime: 2.055, interval_samples_per_second: 3.893, interval_steps_per_second: 4.866, epoch: 6.5385
loss: 1.49001322, learning_rate: 1.9615384615384617e-05, global_step: 180, interval_runtime: 1.8618, interval_samples_per_second: 4.297, interval_steps_per_second: 5.371, epoch: 6.9231

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|‚ñà‚ñà‚ñä       | 2/7 [00:00<00:00,  8.43it/s][A
 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 3/7 [00:00<00:00,  7.00it/s][A
 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 4/7 [00:00<00:00,  6.49it/s][A
 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 5/7 [00:00<00:00,  6.22it/s][A
 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 6/7 [00:00<00:00,  6.08it/s][A                                                 
                                             [A 35%|‚ñà‚ñà‚ñà‚ñå      | 182/520 [01:09<00:56,  6.00it/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:01<00:00,  6.08it/s][A
                                             [A[32m[2022-08-25 11:22:04,019] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-182[0m
[32m[2022-08-25 11:22:04,019] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 11:22:05,467] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-182/tokenizer_config.json[0m
[32m[2022-08-25 11:22:05,467] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-182/special_tokens_map.json[0m
 35%|‚ñà‚ñà‚ñà‚ñå      | 183/520 [01:13<08:38,  1.54s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 184/520 [01:13<06:45,  1.21s/it] 36%|‚ñà‚ñà‚ñà‚ñå      | 185/520 [01:13<05:16,  1.06it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 186/520 [01:13<04:08,  1.35it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 187/520 [01:14<03:17,  1.69it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 188/520 [01:14<02:41,  2.05it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 189/520 [01:14<02:15,  2.44it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 190/520 [01:14<01:55,  2.86it/s]                                                  37%|‚ñà‚ñà‚ñà‚ñã      | 190/520 [01:14<01:55,  2.86it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 191/520 [01:15<01:42,  3.22it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 192/520 [01:15<01:31,  3.59it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 193/520 [01:15<01:23,  3.90it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 194/520 [01:15<01:18,  4.13it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 195/520 [01:15<01:15,  4.31it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 196/520 [01:16<01:13,  4.44it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 197/520 [01:16<01:11,  4.51it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 198/520 [01:16<01:10,  4.59it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 199/520 [01:16<01:09,  4.65it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 200/520 [01:16<01:07,  4.71it/s]                                                  38%|‚ñà‚ñà‚ñà‚ñä      | 200/520 [01:16<01:07,  4.71it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 201/520 [01:17<01:08,  4.67it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 202/520 [01:17<01:07,  4.71it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 203/520 [01:17<01:04,  4.95it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 204/520 [01:17<00:59,  5.31it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 205/520 [01:17<00:56,  5.59it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 206/520 [01:17<00:54,  5.77it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 207/520 [01:18<00:52,  5.93it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 208/520 [01:18<00:46,  6.74it/s][32m[2022-08-25 11:22:12,600] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 11:22:12,600] [    INFO][0m -   Num examples = 202[0m
[32m[2022-08-25 11:22:12,600] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 11:22:12,600] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 11:22:12,601] [    INFO][0m -   Total prediction steps = 7[0m
eval_loss: 2.246110677719116, eval_accuracy: 0.15841584158415842, eval_runtime: 2.4893, eval_samples_per_second: 81.147, eval_steps_per_second: 2.812, epoch: 7.0
loss: 1.06984062, learning_rate: 1.903846153846154e-05, global_step: 190, interval_runtime: 7.9006, interval_samples_per_second: 1.013, interval_steps_per_second: 1.266, epoch: 7.3077
loss: 1.10301619, learning_rate: 1.8461538461538465e-05, global_step: 200, interval_runtime: 2.0889, interval_samples_per_second: 3.83, interval_steps_per_second: 4.787, epoch: 7.6923

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|‚ñà‚ñà‚ñä       | 2/7 [00:00<00:00,  8.09it/s][A
 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 3/7 [00:00<00:00,  6.84it/s][A
 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 4/7 [00:00<00:00,  6.36it/s][A
 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 5/7 [00:00<00:00,  6.13it/s][A
 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 6/7 [00:00<00:00,  6.00it/s][A                                                 
                                             [A 40%|‚ñà‚ñà‚ñà‚ñà      | 208/520 [01:20<00:46,  6.74it/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:01<00:00,  6.00it/s][A
                                             [A[32m[2022-08-25 11:22:15,279] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-208[0m
[32m[2022-08-25 11:22:15,279] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 11:22:16,787] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-208/tokenizer_config.json[0m
[32m[2022-08-25 11:22:16,787] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-208/special_tokens_map.json[0m
 40%|‚ñà‚ñà‚ñà‚ñà      | 209/520 [01:25<11:16,  2.17s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 210/520 [01:25<08:11,  1.59s/it]                                                  40%|‚ñà‚ñà‚ñà‚ñà      | 210/520 [01:25<08:11,  1.59s/it] 41%|‚ñà‚ñà‚ñà‚ñà      | 211/520 [01:25<06:03,  1.18s/it] 41%|‚ñà‚ñà‚ñà‚ñà      | 212/520 [01:25<04:33,  1.13it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 213/520 [01:25<03:29,  1.46it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 214/520 [01:26<02:45,  1.85it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 215/520 [01:26<02:15,  2.26it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 216/520 [01:26<01:54,  2.64it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 217/520 [01:26<01:39,  3.04it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 218/520 [01:27<01:28,  3.40it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 219/520 [01:27<01:21,  3.70it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 220/520 [01:27<01:18,  3.82it/s]                                                  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 220/520 [01:27<01:18,  3.82it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 221/520 [01:27<01:15,  3.98it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 222/520 [01:27<01:11,  4.18it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 223/520 [01:28<01:08,  4.31it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 224/520 [01:28<01:06,  4.42it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 225/520 [01:28<01:05,  4.49it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 226/520 [01:28<01:04,  4.56it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 227/520 [01:29<01:03,  4.60it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 228/520 [01:29<01:03,  4.62it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 229/520 [01:29<01:00,  4.82it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 230/520 [01:29<00:55,  5.22it/s]                                                  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 230/520 [01:29<00:55,  5.22it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 231/520 [01:29<00:52,  5.49it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 232/520 [01:29<00:50,  5.70it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 233/520 [01:30<00:48,  5.89it/s][32m[2022-08-25 11:22:24,521] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 11:22:24,521] [    INFO][0m -   Num examples = 202[0m
[32m[2022-08-25 11:22:24,521] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 11:22:24,521] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 11:22:24,521] [    INFO][0m -   Total prediction steps = 7[0m
eval_loss: 2.608590841293335, eval_accuracy: 0.13366336633663367, eval_runtime: 2.6775, eval_samples_per_second: 75.443, eval_steps_per_second: 2.614, epoch: 8.0
loss: 1.10411568, learning_rate: 1.7884615384615384e-05, global_step: 210, interval_runtime: 8.4501, interval_samples_per_second: 0.947, interval_steps_per_second: 1.183, epoch: 8.0769
loss: 0.72867699, learning_rate: 1.7307692307692306e-05, global_step: 220, interval_runtime: 2.1763, interval_samples_per_second: 3.676, interval_steps_per_second: 4.595, epoch: 8.4615
loss: 0.65224915, learning_rate: 1.673076923076923e-05, global_step: 230, interval_runtime: 2.0592, interval_samples_per_second: 3.885, interval_steps_per_second: 4.856, epoch: 8.8462

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|‚ñà‚ñà‚ñä       | 2/7 [00:00<00:00,  8.16it/s][A
 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 3/7 [00:00<00:00,  6.97it/s][A
 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 4/7 [00:00<00:00,  6.47it/s][A
 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 5/7 [00:00<00:00,  6.22it/s][A
 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 6/7 [00:00<00:00,  6.07it/s][A                                                 
                                             [A 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 234/520 [01:32<00:48,  5.89it/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:01<00:00,  6.07it/s][A
                                             [A[32m[2022-08-25 11:22:27,324] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-234[0m
[32m[2022-08-25 11:22:27,324] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 11:22:28,918] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-234/tokenizer_config.json[0m
[32m[2022-08-25 11:22:28,918] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-234/special_tokens_map.json[0m
[32m[2022-08-25 11:22:30,970] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-25 11:22:30,970] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-130 (score: 0.16831683168316833).[0m
                                                  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 234/520 [01:37<00:48,  5.89it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 234/520 [01:37<01:59,  2.40it/s]
[32m[2022-08-25 11:22:31,972] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-25 11:22:31,972] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 11:22:35,488] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-25 11:22:35,489] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-25 11:22:35,500] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-25 11:22:35,500] [    INFO][0m -   Num examples = 2002[0m
[32m[2022-08-25 11:22:35,500] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 11:22:35,501] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 11:22:35,501] [    INFO][0m -   Total prediction steps = 63[0m
eval_loss: 2.9550576210021973, eval_accuracy: 0.12871287128712872, eval_runtime: 2.8015, eval_samples_per_second: 72.105, eval_steps_per_second: 2.499, epoch: 9.0
train_runtime: 97.5528, train_samples_per_second: 41.413, train_steps_per_second: 5.33, train_loss: 1.663910755744347, epoch: 9.0
***** train metrics *****
  epoch                    =        9.0
  train_loss               =     1.6639
  train_runtime            = 0:01:37.55
  train_samples_per_second =     41.413
  train_steps_per_second   =       5.33
  0%|          | 0/63 [00:00<?, ?it/s]  3%|‚ñé         | 2/63 [00:00<00:12,  4.78it/s]  5%|‚ñç         | 3/63 [00:00<00:17,  3.37it/s]  6%|‚ñã         | 4/63 [00:01<00:20,  2.88it/s]  8%|‚ñä         | 5/63 [00:01<00:21,  2.67it/s] 10%|‚ñâ         | 6/63 [00:02<00:22,  2.55it/s] 11%|‚ñà         | 7/63 [00:02<00:22,  2.49it/s] 13%|‚ñà‚ñé        | 8/63 [00:02<00:22,  2.45it/s] 14%|‚ñà‚ñç        | 9/63 [00:03<00:22,  2.40it/s] 16%|‚ñà‚ñå        | 10/63 [00:03<00:22,  2.37it/s] 17%|‚ñà‚ñã        | 11/63 [00:04<00:22,  2.36it/s] 19%|‚ñà‚ñâ        | 12/63 [00:04<00:21,  2.35it/s] 21%|‚ñà‚ñà        | 13/63 [00:05<00:21,  2.33it/s] 22%|‚ñà‚ñà‚ñè       | 14/63 [00:05<00:22,  2.22it/s] 24%|‚ñà‚ñà‚ñç       | 15/63 [00:06<00:21,  2.25it/s] 25%|‚ñà‚ñà‚ñå       | 16/63 [00:06<00:20,  2.28it/s] 27%|‚ñà‚ñà‚ñã       | 17/63 [00:06<00:20,  2.29it/s] 29%|‚ñà‚ñà‚ñä       | 18/63 [00:07<00:19,  2.29it/s] 30%|‚ñà‚ñà‚ñà       | 19/63 [00:07<00:19,  2.29it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 20/63 [00:08<00:18,  2.29it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 21/63 [00:08<00:18,  2.30it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 22/63 [00:09<00:17,  2.29it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 23/63 [00:09<00:17,  2.30it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 24/63 [00:09<00:16,  2.30it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 25/63 [00:10<00:16,  2.30it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 26/63 [00:10<00:16,  2.27it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 27/63 [00:11<00:15,  2.28it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 28/63 [00:11<00:15,  2.28it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 29/63 [00:12<00:14,  2.29it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 30/63 [00:12<00:14,  2.25it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 31/63 [00:13<00:14,  2.26it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 32/63 [00:13<00:13,  2.26it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 33/63 [00:13<00:13,  2.27it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 34/63 [00:14<00:12,  2.25it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 35/63 [00:14<00:12,  2.26it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 36/63 [00:15<00:11,  2.26it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 37/63 [00:15<00:11,  2.26it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 38/63 [00:16<00:11,  2.24it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 39/63 [00:16<00:10,  2.23it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 40/63 [00:17<00:10,  2.24it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 41/63 [00:17<00:09,  2.25it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 42/63 [00:17<00:09,  2.25it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 43/63 [00:18<00:08,  2.23it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 44/63 [00:18<00:08,  2.24it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 45/63 [00:19<00:08,  2.24it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 46/63 [00:19<00:07,  2.24it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 47/63 [00:20<00:07,  2.20it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 48/63 [00:20<00:06,  2.22it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 49/63 [00:21<00:06,  2.21it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 50/63 [00:21<00:05,  2.22it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 51/63 [00:22<00:05,  2.19it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 52/63 [00:22<00:05,  2.20it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 53/63 [00:22<00:04,  2.21it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 54/63 [00:23<00:04,  2.22it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 55/63 [00:23<00:03,  2.19it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 56/63 [00:24<00:03,  2.19it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 57/63 [00:24<00:02,  2.20it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 58/63 [00:25<00:02,  2.39it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 59/63 [00:25<00:01,  2.89it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 60/63 [00:25<00:00,  3.34it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 61/63 [00:25<00:00,  3.81it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 62/63 [00:25<00:00,  4.24it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:25<00:00,  5.00it/s][32m[2022-08-25 11:23:03,212] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-25 11:23:03,212] [    INFO][0m -   Num examples = 2000[0m
[32m[2022-08-25 11:23:03,212] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 11:23:03,212] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 11:23:03,212] [    INFO][0m -   Total prediction steps = 63[0m
64it [00:28,  1.28it/s]                        65it [00:28,  1.46it/s]66it [00:28,  1.61it/s]67it [00:29,  1.71it/s]68it [00:29,  1.82it/s]69it [00:30,  1.91it/s]70it [00:30,  1.99it/s]71it [00:31,  2.01it/s]72it [00:31,  2.05it/s]73it [00:32,  2.08it/s]74it [00:32,  2.10it/s]75it [00:33,  2.10it/s]76it [00:33,  2.10it/s]77it [00:34,  2.11it/s]78it [00:34,  2.11it/s]79it [00:35,  2.09it/s]80it [00:35,  2.10it/s]81it [00:36,  2.11it/s]82it [00:36,  2.12it/s]83it [00:37,  2.10it/s]84it [00:37,  2.12it/s]85it [00:37,  2.12it/s]86it [00:38,  2.12it/s]87it [00:38,  2.10it/s]88it [00:39,  2.11it/s]89it [00:39,  2.10it/s]90it [00:40,  2.11it/s]91it [00:40,  2.10it/s]92it [00:41,  2.10it/s]93it [00:41,  2.09it/s]94it [00:42,  2.08it/s]95it [00:42,  2.02it/s]96it [00:43,  2.04it/s]97it [00:43,  2.05it/s]98it [00:44,  2.04it/s]99it [00:44,  2.05it/s]100it [00:45,  2.07it/s]101it [00:45,  2.07it/s]102it [00:46,  2.04it/s]103it [00:46,  2.04it/s]104it [00:47,  2.05it/s]105it [00:47,  2.06it/s]106it [00:48,  2.03it/s]107it [00:48,  2.04it/s]108it [00:49,  2.04it/s]109it [00:49,  2.05it/s]110it [00:50,  2.05it/s]111it [00:50,  2.05it/s]112it [00:51,  2.05it/s]113it [00:51,  2.04it/s]114it [00:52,  2.04it/s]115it [00:52,  2.03it/s]116it [00:53,  2.03it/s]117it [00:53,  1.99it/s]118it [00:54,  2.00it/s]119it [00:54,  2.01it/s]120it [00:55,  2.00it/s]121it [00:55,  2.21it/s]122it [00:55,  2.72it/s]123it [00:55,  3.23it/s]124it [00:55,  3.72it/s]125it [00:56,  4.16it/s]***** test metrics *****
  test_accuracy           =     0.1444
  test_loss               =     2.0175
  test_runtime            = 0:00:27.71
  test_samples_per_second =     72.246
  test_steps_per_second   =      2.273
Traceback (most recent call last):
  File "train_single.py", line 154, in <module>
    main()
  File "train_single.py", line 150, in main
    save_to_file(test_ret, data_args.task_name)
  File "/ssd2/wanghuijuan03/prompt/PaddleNLP/examples/benchmark/fewclue/postprocess.py", line 53, in save_to_file
    fp.write(json.dumps(ret) + "\n")
  File "/ssd2/wanghuijuan03/anaconda3/envs/prompt/lib/python3.8/json/__init__.py", line 231, in dumps
    return _default_encoder.encode(obj)
  File "/ssd2/wanghuijuan03/anaconda3/envs/prompt/lib/python3.8/json/encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "/ssd2/wanghuijuan03/anaconda3/envs/prompt/lib/python3.8/json/encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "/ssd2/wanghuijuan03/anaconda3/envs/prompt/lib/python3.8/json/encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type int64 is not JSON serializable
126it [01:01,  2.03it/s]