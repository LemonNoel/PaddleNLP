[32m[2022-09-16 13:28:02,554] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-16 13:28:02,555] [    INFO][0m - ============================================================[0m
[32m[2022-09-16 13:28:02,555] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-16 13:28:02,555] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-16 13:28:02,555] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-16 13:28:02,555] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-16 13:28:02,555] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-16 13:28:02,555] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-16 13:28:02,555] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-16 13:28:02,555] [    INFO][0m - [0m
[32m[2022-09-16 13:28:02,555] [    INFO][0m - ============================================================[0m
[32m[2022-09-16 13:28:02,556] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-16 13:28:02,556] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-16 13:28:02,556] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-16 13:28:02,556] [    INFO][0m - pretrained                    :/ssd2/wanghuijuan03/data/zero-shot/checkpoints_0915/checkpoint-90000/model_state.pdparams[0m
[32m[2022-09-16 13:28:02,556] [    INFO][0m - prompt                        :‚Äú{'text':'text_a'}‚ÄùËøô‰∏™Âè•Â≠êËØ¥Êòé‰ªñÂæà{'mask'}{'mask'}„ÄÇÈÄâÈ°πÔºöÁîüÊ∞î/È´òÂÖ¥[0m
[32m[2022-09-16 13:28:02,556] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-16 13:28:02,556] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-16 13:28:02,556] [    INFO][0m - task_name                     :eprstmt[0m
[32m[2022-09-16 13:28:02,556] [    INFO][0m - [0m
[32m[2022-09-16 13:28:02,556] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0916 13:28:02.558593 57539 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0916 13:28:02.563176 57539 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-16 13:28:08,016] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-16 13:28:08,028] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-16 13:28:08,028] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-16 13:28:09,688] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùËøô‰∏™Âè•Â≠êËØ¥Êòé‰ªñÂæà'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': '„ÄÇÈÄâÈ°πÔºöÁîüÊ∞î/È´òÂÖ¥'}][0m
[32m[2022-09-16 13:28:09,795] [    INFO][0m - ============================================================[0m
[32m[2022-09-16 13:28:09,795] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-16 13:28:09,795] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-16 13:28:09,795] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-16 13:28:09,795] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-16 13:28:09,795] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-16 13:28:09,795] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-16 13:28:09,796] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-16 13:28:09,796] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-16 13:28:09,796] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-16 13:28:09,796] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-16 13:28:09,796] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-16 13:28:09,796] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-16 13:28:09,796] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-16 13:28:09,796] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-16 13:28:09,796] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-16 13:28:09,796] [    INFO][0m - do_predict                    :False[0m
[32m[2022-09-16 13:28:09,796] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-16 13:28:09,796] [    INFO][0m - eval_batch_size               :16[0m
[32m[2022-09-16 13:28:09,797] [    INFO][0m - eval_steps                    :200[0m
[32m[2022-09-16 13:28:09,797] [    INFO][0m - evaluation_strategy           :IntervalStrategy.EPOCH[0m
[32m[2022-09-16 13:28:09,797] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-16 13:28:09,797] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-16 13:28:09,797] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-16 13:28:09,797] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-16 13:28:09,797] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-16 13:28:09,797] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-16 13:28:09,797] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-16 13:28:09,797] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-16 13:28:09,797] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-16 13:28:09,797] [    INFO][0m - learning_rate                 :3e-06[0m
[32m[2022-09-16 13:28:09,797] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-16 13:28:09,798] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-16 13:28:09,798] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-16 13:28:09,798] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-16 13:28:09,798] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-16 13:28:09,798] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-16 13:28:09,798] [    INFO][0m - logging_dir                   :./checkpoints_eprstmt/runs/Sep16_13-28-02_instance-3bwob41y-01[0m
[32m[2022-09-16 13:28:09,798] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-16 13:28:09,798] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-16 13:28:09,798] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-16 13:28:09,798] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-16 13:28:09,798] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-16 13:28:09,798] [    INFO][0m - max_seq_length                :128[0m
[32m[2022-09-16 13:28:09,798] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-16 13:28:09,799] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-16 13:28:09,799] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-16 13:28:09,799] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-16 13:28:09,799] [    INFO][0m - num_train_epochs              :30.0[0m
[32m[2022-09-16 13:28:09,799] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-16 13:28:09,799] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-16 13:28:09,799] [    INFO][0m - output_dir                    :./checkpoints_eprstmt/[0m
[32m[2022-09-16 13:28:09,799] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-16 13:28:09,799] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-16 13:28:09,799] [    INFO][0m - per_device_eval_batch_size    :16[0m
[32m[2022-09-16 13:28:09,799] [    INFO][0m - per_device_train_batch_size   :16[0m
[32m[2022-09-16 13:28:09,799] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-16 13:28:09,799] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-16 13:28:09,800] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-16 13:28:09,800] [    INFO][0m - ppt_learning_rate             :3e-05[0m
[32m[2022-09-16 13:28:09,800] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-16 13:28:09,800] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-16 13:28:09,800] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-16 13:28:09,800] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-16 13:28:09,800] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-16 13:28:09,800] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-16 13:28:09,800] [    INFO][0m - run_name                      :./checkpoints_eprstmt/[0m
[32m[2022-09-16 13:28:09,800] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-16 13:28:09,800] [    INFO][0m - save_steps                    :200[0m
[32m[2022-09-16 13:28:09,800] [    INFO][0m - save_strategy                 :IntervalStrategy.EPOCH[0m
[32m[2022-09-16 13:28:09,800] [    INFO][0m - save_total_limit              :1[0m
[32m[2022-09-16 13:28:09,800] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-16 13:28:09,801] [    INFO][0m - seed                          :42[0m
[32m[2022-09-16 13:28:09,801] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-16 13:28:09,801] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-16 13:28:09,801] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-16 13:28:09,801] [    INFO][0m - train_batch_size              :16[0m
[32m[2022-09-16 13:28:09,801] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-16 13:28:09,801] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-16 13:28:09,801] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-16 13:28:09,801] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-16 13:28:09,801] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-16 13:28:09,801] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-16 13:28:09,801] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-16 13:28:09,802] [    INFO][0m - [0m
[32m[2022-09-16 13:28:09,804] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-16 13:28:09,804] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:28:09,805] [    INFO][0m -   Num Epochs = 30[0m
[32m[2022-09-16 13:28:09,805] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2022-09-16 13:28:09,805] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2022-09-16 13:28:09,805] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-16 13:28:09,805] [    INFO][0m -   Total optimization steps = 300.0[0m
[32m[2022-09-16 13:28:09,805] [    INFO][0m -   Total num train samples = 4800[0m
[32m[2022-09-16 13:28:14,224] [    INFO][0m - loss: 0.60596581, learning_rate: 2.9e-06, global_step: 10, interval_runtime: 4.4182, interval_samples_per_second: 3.621, interval_steps_per_second: 2.263, epoch: 1.0[0m
[32m[2022-09-16 13:28:14,226] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:28:14,226] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:28:14,226] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:28:14,226] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:28:14,226] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:28:15,563] [    INFO][0m - eval_loss: 0.5056675672531128, eval_accuracy: 0.85, eval_runtime: 1.3368, eval_samples_per_second: 119.691, eval_steps_per_second: 7.481, epoch: 1.0[0m
[32m[2022-09-16 13:28:15,563] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-10[0m
[32m[2022-09-16 13:28:15,563] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:28:18,395] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-10/tokenizer_config.json[0m
[32m[2022-09-16 13:28:18,395] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-10/special_tokens_map.json[0m
[32m[2022-09-16 13:28:27,177] [    INFO][0m - loss: 0.4338851, learning_rate: 2.8000000000000003e-06, global_step: 20, interval_runtime: 12.9526, interval_samples_per_second: 1.235, interval_steps_per_second: 0.772, epoch: 2.0[0m
[32m[2022-09-16 13:28:27,178] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:28:27,178] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:28:27,178] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:28:27,178] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:28:27,178] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:28:28,521] [    INFO][0m - eval_loss: 0.46904072165489197, eval_accuracy: 0.85625, eval_runtime: 1.3424, eval_samples_per_second: 119.188, eval_steps_per_second: 7.449, epoch: 2.0[0m
[32m[2022-09-16 13:28:28,521] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-20[0m
[32m[2022-09-16 13:28:28,521] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:28:31,601] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-20/tokenizer_config.json[0m
[32m[2022-09-16 13:28:31,602] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-20/special_tokens_map.json[0m
[32m[2022-09-16 13:28:41,131] [    INFO][0m - loss: 0.36736598, learning_rate: 2.7e-06, global_step: 30, interval_runtime: 13.9543, interval_samples_per_second: 1.147, interval_steps_per_second: 0.717, epoch: 3.0[0m
[32m[2022-09-16 13:28:41,132] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:28:41,132] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:28:41,132] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:28:41,132] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:28:41,132] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:28:42,458] [    INFO][0m - eval_loss: 0.4468322694301605, eval_accuracy: 0.85625, eval_runtime: 1.3251, eval_samples_per_second: 120.75, eval_steps_per_second: 7.547, epoch: 3.0[0m
[32m[2022-09-16 13:28:42,458] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-30[0m
[32m[2022-09-16 13:28:42,458] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:28:45,090] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-30/tokenizer_config.json[0m
[32m[2022-09-16 13:28:45,091] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-30/special_tokens_map.json[0m
[32m[2022-09-16 13:28:54,136] [    INFO][0m - Deleting older checkpoint [checkpoints_eprstmt/checkpoint-10] due to args.save_total_limit[0m
[32m[2022-09-16 13:29:12,697] [    INFO][0m - loss: 0.26415782, learning_rate: 2.6e-06, global_step: 40, interval_runtime: 31.5661, interval_samples_per_second: 0.507, interval_steps_per_second: 0.317, epoch: 4.0[0m
[32m[2022-09-16 13:29:12,698] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:29:12,698] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:29:12,698] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:29:12,698] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:29:12,698] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:29:14,012] [    INFO][0m - eval_loss: 0.4303835928440094, eval_accuracy: 0.85625, eval_runtime: 1.3129, eval_samples_per_second: 121.864, eval_steps_per_second: 7.617, epoch: 4.0[0m
[32m[2022-09-16 13:29:14,012] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-40[0m
[32m[2022-09-16 13:29:14,012] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:29:16,690] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-40/tokenizer_config.json[0m
[32m[2022-09-16 13:29:17,289] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-40/special_tokens_map.json[0m
[32m[2022-09-16 13:29:22,561] [    INFO][0m - Deleting older checkpoint [checkpoints_eprstmt/checkpoint-30] due to args.save_total_limit[0m
[32m[2022-09-16 13:29:26,664] [    INFO][0m - loss: 0.20194604, learning_rate: 2.5e-06, global_step: 50, interval_runtime: 13.9674, interval_samples_per_second: 1.146, interval_steps_per_second: 0.716, epoch: 5.0[0m
[32m[2022-09-16 13:29:26,665] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:29:26,665] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:29:26,666] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:29:26,666] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:29:26,666] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:29:27,981] [    INFO][0m - eval_loss: 0.4258575439453125, eval_accuracy: 0.8625, eval_runtime: 1.3149, eval_samples_per_second: 121.685, eval_steps_per_second: 7.605, epoch: 5.0[0m
[32m[2022-09-16 13:29:27,981] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-50[0m
[32m[2022-09-16 13:29:27,981] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:29:30,623] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-50/tokenizer_config.json[0m
[32m[2022-09-16 13:29:30,624] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-50/special_tokens_map.json[0m
[32m[2022-09-16 13:29:36,071] [    INFO][0m - Deleting older checkpoint [checkpoints_eprstmt/checkpoint-20] due to args.save_total_limit[0m
[32m[2022-09-16 13:29:53,329] [    INFO][0m - loss: 0.17706835, learning_rate: 2.4000000000000003e-06, global_step: 60, interval_runtime: 26.6647, interval_samples_per_second: 0.6, interval_steps_per_second: 0.375, epoch: 6.0[0m
[32m[2022-09-16 13:29:53,330] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:29:53,330] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:29:53,330] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:29:53,330] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:29:53,330] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:29:54,645] [    INFO][0m - eval_loss: 0.41998714208602905, eval_accuracy: 0.85625, eval_runtime: 1.3144, eval_samples_per_second: 121.728, eval_steps_per_second: 7.608, epoch: 6.0[0m
[32m[2022-09-16 13:29:54,645] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-60[0m
[32m[2022-09-16 13:29:54,645] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:29:57,498] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-60/tokenizer_config.json[0m
[32m[2022-09-16 13:29:57,498] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-60/special_tokens_map.json[0m
[32m[2022-09-16 13:30:02,729] [    INFO][0m - Deleting older checkpoint [checkpoints_eprstmt/checkpoint-40] due to args.save_total_limit[0m
[32m[2022-09-16 13:30:35,104] [    INFO][0m - loss: 0.12821065, learning_rate: 2.3000000000000004e-06, global_step: 70, interval_runtime: 41.7748, interval_samples_per_second: 0.383, interval_steps_per_second: 0.239, epoch: 7.0[0m
[32m[2022-09-16 13:30:35,104] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:30:35,105] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:30:35,105] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:30:35,105] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:30:35,105] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:30:36,417] [    INFO][0m - eval_loss: 0.42885494232177734, eval_accuracy: 0.8625, eval_runtime: 1.3118, eval_samples_per_second: 121.97, eval_steps_per_second: 7.623, epoch: 7.0[0m
[32m[2022-09-16 13:30:36,417] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-70[0m
[32m[2022-09-16 13:30:36,417] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:30:39,156] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-70/tokenizer_config.json[0m
[32m[2022-09-16 13:30:39,156] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-70/special_tokens_map.json[0m
[32m[2022-09-16 13:30:44,314] [    INFO][0m - Deleting older checkpoint [checkpoints_eprstmt/checkpoint-60] due to args.save_total_limit[0m
[32m[2022-09-16 13:30:48,395] [    INFO][0m - loss: 0.10264145, learning_rate: 2.1999999999999997e-06, global_step: 80, interval_runtime: 13.2914, interval_samples_per_second: 1.204, interval_steps_per_second: 0.752, epoch: 8.0[0m
[32m[2022-09-16 13:30:48,396] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:30:48,396] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:30:48,396] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:30:48,396] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:30:48,396] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:30:49,701] [    INFO][0m - eval_loss: 0.4440341889858246, eval_accuracy: 0.85625, eval_runtime: 1.305, eval_samples_per_second: 122.608, eval_steps_per_second: 7.663, epoch: 8.0[0m
[32m[2022-09-16 13:30:49,702] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-80[0m
[32m[2022-09-16 13:30:49,702] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:30:52,185] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-80/tokenizer_config.json[0m
[32m[2022-09-16 13:30:52,185] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-80/special_tokens_map.json[0m
[32m[2022-09-16 13:30:57,155] [    INFO][0m - Deleting older checkpoint [checkpoints_eprstmt/checkpoint-70] due to args.save_total_limit[0m
[32m[2022-09-16 13:31:01,198] [    INFO][0m - loss: 0.08595223, learning_rate: 2.1e-06, global_step: 90, interval_runtime: 12.803, interval_samples_per_second: 1.25, interval_steps_per_second: 0.781, epoch: 9.0[0m
[32m[2022-09-16 13:31:01,199] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:31:01,199] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:31:01,199] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:31:01,199] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:31:01,199] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:31:02,520] [    INFO][0m - eval_loss: 0.46187251806259155, eval_accuracy: 0.8625, eval_runtime: 1.3201, eval_samples_per_second: 121.201, eval_steps_per_second: 7.575, epoch: 9.0[0m
[32m[2022-09-16 13:31:02,520] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-90[0m
[32m[2022-09-16 13:31:02,520] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:31:05,639] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-90/tokenizer_config.json[0m
[32m[2022-09-16 13:31:05,640] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-90/special_tokens_map.json[0m
[32m[2022-09-16 13:31:11,518] [    INFO][0m - Deleting older checkpoint [checkpoints_eprstmt/checkpoint-80] due to args.save_total_limit[0m
[32m[2022-09-16 13:31:15,586] [    INFO][0m - loss: 0.05949429, learning_rate: 2e-06, global_step: 100, interval_runtime: 14.3871, interval_samples_per_second: 1.112, interval_steps_per_second: 0.695, epoch: 10.0[0m
[32m[2022-09-16 13:31:15,587] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:31:15,587] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:31:15,587] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:31:15,587] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:31:15,587] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:31:16,901] [    INFO][0m - eval_loss: 0.4855585992336273, eval_accuracy: 0.86875, eval_runtime: 1.3143, eval_samples_per_second: 121.74, eval_steps_per_second: 7.609, epoch: 10.0[0m
[32m[2022-09-16 13:31:16,902] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-100[0m
[32m[2022-09-16 13:31:16,902] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:31:19,837] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-16 13:31:19,837] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-16 13:31:24,952] [    INFO][0m - Deleting older checkpoint [checkpoints_eprstmt/checkpoint-50] due to args.save_total_limit[0m
[32m[2022-09-16 13:31:30,101] [    INFO][0m - loss: 0.0510269, learning_rate: 1.9e-06, global_step: 110, interval_runtime: 14.0891, interval_samples_per_second: 1.136, interval_steps_per_second: 0.71, epoch: 11.0[0m
[32m[2022-09-16 13:31:30,101] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:31:30,102] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:31:30,102] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:31:30,102] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:31:30,102] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:31:31,415] [    INFO][0m - eval_loss: 0.5171352028846741, eval_accuracy: 0.86875, eval_runtime: 1.3127, eval_samples_per_second: 121.889, eval_steps_per_second: 7.618, epoch: 11.0[0m
[32m[2022-09-16 13:31:31,415] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-110[0m
[32m[2022-09-16 13:31:31,415] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:31:34,163] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-110/tokenizer_config.json[0m
[32m[2022-09-16 13:31:34,164] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-110/special_tokens_map.json[0m
[32m[2022-09-16 13:31:40,460] [    INFO][0m - Deleting older checkpoint [checkpoints_eprstmt/checkpoint-90] due to args.save_total_limit[0m
[32m[2022-09-16 13:31:44,655] [    INFO][0m - loss: 0.05113796, learning_rate: 1.8e-06, global_step: 120, interval_runtime: 14.9808, interval_samples_per_second: 1.068, interval_steps_per_second: 0.668, epoch: 12.0[0m
[32m[2022-09-16 13:31:44,656] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:31:44,656] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:31:44,656] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:31:44,656] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:31:44,656] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:31:45,957] [    INFO][0m - eval_loss: 0.5378509759902954, eval_accuracy: 0.86875, eval_runtime: 1.3001, eval_samples_per_second: 123.07, eval_steps_per_second: 7.692, epoch: 12.0[0m
[32m[2022-09-16 13:31:45,957] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-120[0m
[32m[2022-09-16 13:31:45,957] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:31:52,577] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-120/tokenizer_config.json[0m
[32m[2022-09-16 13:31:52,577] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-120/special_tokens_map.json[0m
[32m[2022-09-16 13:31:58,665] [    INFO][0m - Deleting older checkpoint [checkpoints_eprstmt/checkpoint-110] due to args.save_total_limit[0m
[32m[2022-09-16 13:32:02,729] [    INFO][0m - loss: 0.03717141, learning_rate: 1.7e-06, global_step: 130, interval_runtime: 18.074, interval_samples_per_second: 0.885, interval_steps_per_second: 0.553, epoch: 13.0[0m
[32m[2022-09-16 13:32:02,730] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:32:02,730] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:32:02,730] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:32:02,731] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:32:02,731] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:32:04,025] [    INFO][0m - eval_loss: 0.5522477030754089, eval_accuracy: 0.86875, eval_runtime: 1.2947, eval_samples_per_second: 123.579, eval_steps_per_second: 7.724, epoch: 13.0[0m
[32m[2022-09-16 13:32:04,026] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-130[0m
[32m[2022-09-16 13:32:04,026] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:32:06,740] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-130/tokenizer_config.json[0m
[32m[2022-09-16 13:32:06,741] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-130/special_tokens_map.json[0m
[32m[2022-09-16 13:32:12,040] [    INFO][0m - Deleting older checkpoint [checkpoints_eprstmt/checkpoint-120] due to args.save_total_limit[0m
[32m[2022-09-16 13:32:16,135] [    INFO][0m - loss: 0.02437942, learning_rate: 1.6e-06, global_step: 140, interval_runtime: 13.4055, interval_samples_per_second: 1.194, interval_steps_per_second: 0.746, epoch: 14.0[0m
[32m[2022-09-16 13:32:16,136] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:32:16,136] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:32:16,136] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:32:16,137] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:32:16,137] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:32:17,460] [    INFO][0m - eval_loss: 0.5704864263534546, eval_accuracy: 0.86875, eval_runtime: 1.323, eval_samples_per_second: 120.933, eval_steps_per_second: 7.558, epoch: 14.0[0m
[32m[2022-09-16 13:32:17,460] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-140[0m
[32m[2022-09-16 13:32:17,460] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:32:20,211] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-140/tokenizer_config.json[0m
[32m[2022-09-16 13:32:20,211] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-140/special_tokens_map.json[0m
[32m[2022-09-16 13:32:26,490] [    INFO][0m - Deleting older checkpoint [checkpoints_eprstmt/checkpoint-130] due to args.save_total_limit[0m
[32m[2022-09-16 13:32:30,571] [    INFO][0m - loss: 0.02235674, learning_rate: 1.5e-06, global_step: 150, interval_runtime: 14.4358, interval_samples_per_second: 1.108, interval_steps_per_second: 0.693, epoch: 15.0[0m
[32m[2022-09-16 13:32:30,572] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:32:30,572] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:32:30,572] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:32:30,572] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:32:30,572] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:32:31,893] [    INFO][0m - eval_loss: 0.5894174575805664, eval_accuracy: 0.875, eval_runtime: 1.321, eval_samples_per_second: 121.116, eval_steps_per_second: 7.57, epoch: 15.0[0m
[32m[2022-09-16 13:32:31,894] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-150[0m
[32m[2022-09-16 13:32:31,894] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:32:35,039] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-150/tokenizer_config.json[0m
[32m[2022-09-16 13:32:35,040] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-150/special_tokens_map.json[0m
[32m[2022-09-16 13:32:40,564] [    INFO][0m - Deleting older checkpoint [checkpoints_eprstmt/checkpoint-100] due to args.save_total_limit[0m
[32m[2022-09-16 13:32:44,532] [    INFO][0m - loss: 0.01374204, learning_rate: 1.4000000000000001e-06, global_step: 160, interval_runtime: 13.9618, interval_samples_per_second: 1.146, interval_steps_per_second: 0.716, epoch: 16.0[0m
[32m[2022-09-16 13:32:44,533] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:32:44,533] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:32:44,533] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:32:44,533] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:32:44,533] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:32:45,846] [    INFO][0m - eval_loss: 0.6052002906799316, eval_accuracy: 0.875, eval_runtime: 1.3126, eval_samples_per_second: 121.898, eval_steps_per_second: 7.619, epoch: 16.0[0m
[32m[2022-09-16 13:32:45,847] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-160[0m
[32m[2022-09-16 13:32:45,847] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:32:48,517] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-160/tokenizer_config.json[0m
[32m[2022-09-16 13:32:48,517] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-160/special_tokens_map.json[0m
[32m[2022-09-16 13:32:53,581] [    INFO][0m - Deleting older checkpoint [checkpoints_eprstmt/checkpoint-140] due to args.save_total_limit[0m
[32m[2022-09-16 13:33:02,401] [    INFO][0m - loss: 0.00869603, learning_rate: 1.3e-06, global_step: 170, interval_runtime: 17.8684, interval_samples_per_second: 0.895, interval_steps_per_second: 0.56, epoch: 17.0[0m
[32m[2022-09-16 13:33:02,401] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:33:02,402] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:33:02,402] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:33:02,402] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:33:02,402] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:33:03,731] [    INFO][0m - eval_loss: 0.6204427480697632, eval_accuracy: 0.875, eval_runtime: 1.3288, eval_samples_per_second: 120.406, eval_steps_per_second: 7.525, epoch: 17.0[0m
[32m[2022-09-16 13:33:03,731] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-170[0m
[32m[2022-09-16 13:33:03,731] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:33:06,680] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-170/tokenizer_config.json[0m
[32m[2022-09-16 13:33:06,680] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-170/special_tokens_map.json[0m
[32m[2022-09-16 13:33:13,016] [    INFO][0m - Deleting older checkpoint [checkpoints_eprstmt/checkpoint-160] due to args.save_total_limit[0m
[32m[2022-09-16 13:33:17,135] [    INFO][0m - loss: 0.00570674, learning_rate: 1.2000000000000002e-06, global_step: 180, interval_runtime: 14.7338, interval_samples_per_second: 1.086, interval_steps_per_second: 0.679, epoch: 18.0[0m
[32m[2022-09-16 13:33:17,136] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:33:17,136] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:33:17,136] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:33:17,136] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:33:17,136] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:33:18,441] [    INFO][0m - eval_loss: 0.6363040208816528, eval_accuracy: 0.86875, eval_runtime: 1.3053, eval_samples_per_second: 122.58, eval_steps_per_second: 7.661, epoch: 18.0[0m
[32m[2022-09-16 13:33:18,442] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-180[0m
[32m[2022-09-16 13:33:18,442] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:33:21,328] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-180/tokenizer_config.json[0m
[32m[2022-09-16 13:33:21,328] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-180/special_tokens_map.json[0m
[32m[2022-09-16 13:33:26,824] [    INFO][0m - Deleting older checkpoint [checkpoints_eprstmt/checkpoint-170] due to args.save_total_limit[0m
[32m[2022-09-16 13:33:30,972] [    INFO][0m - loss: 0.00286028, learning_rate: 1.0999999999999998e-06, global_step: 190, interval_runtime: 13.8374, interval_samples_per_second: 1.156, interval_steps_per_second: 0.723, epoch: 19.0[0m
[32m[2022-09-16 13:33:30,972] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:33:30,973] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:33:30,973] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:33:30,973] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:33:30,973] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:33:32,290] [    INFO][0m - eval_loss: 0.648875892162323, eval_accuracy: 0.86875, eval_runtime: 1.3167, eval_samples_per_second: 121.514, eval_steps_per_second: 7.595, epoch: 19.0[0m
[32m[2022-09-16 13:33:32,290] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-190[0m
[32m[2022-09-16 13:33:32,290] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:33:34,987] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-190/tokenizer_config.json[0m
[32m[2022-09-16 13:33:34,988] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-190/special_tokens_map.json[0m
[32m[2022-09-16 13:33:40,820] [    INFO][0m - Deleting older checkpoint [checkpoints_eprstmt/checkpoint-180] due to args.save_total_limit[0m
[32m[2022-09-16 13:33:44,883] [    INFO][0m - loss: 0.00389716, learning_rate: 1e-06, global_step: 200, interval_runtime: 13.9102, interval_samples_per_second: 1.15, interval_steps_per_second: 0.719, epoch: 20.0[0m
[32m[2022-09-16 13:33:44,884] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:33:44,884] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:33:44,884] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:33:44,884] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:33:44,884] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:33:46,190] [    INFO][0m - eval_loss: 0.6609354615211487, eval_accuracy: 0.86875, eval_runtime: 1.3056, eval_samples_per_second: 122.55, eval_steps_per_second: 7.659, epoch: 20.0[0m
[32m[2022-09-16 13:33:46,190] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-200[0m
[32m[2022-09-16 13:33:46,190] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:33:48,709] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-200/tokenizer_config.json[0m
[32m[2022-09-16 13:33:48,710] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-200/special_tokens_map.json[0m
[32m[2022-09-16 13:33:53,843] [    INFO][0m - Deleting older checkpoint [checkpoints_eprstmt/checkpoint-190] due to args.save_total_limit[0m
[32m[2022-09-16 13:33:57,935] [    INFO][0m - loss: 0.00236237, learning_rate: 9e-07, global_step: 210, interval_runtime: 13.0528, interval_samples_per_second: 1.226, interval_steps_per_second: 0.766, epoch: 21.0[0m
[32m[2022-09-16 13:33:57,936] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:33:57,936] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:33:57,936] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:33:57,936] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:33:57,936] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:33:59,240] [    INFO][0m - eval_loss: 0.6692590117454529, eval_accuracy: 0.86875, eval_runtime: 1.3043, eval_samples_per_second: 122.675, eval_steps_per_second: 7.667, epoch: 21.0[0m
[32m[2022-09-16 13:33:59,241] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-210[0m
[32m[2022-09-16 13:33:59,241] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:34:02,078] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-210/tokenizer_config.json[0m
[32m[2022-09-16 13:34:03,290] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-210/special_tokens_map.json[0m
[32m[2022-09-16 13:34:08,338] [    INFO][0m - Deleting older checkpoint [checkpoints_eprstmt/checkpoint-200] due to args.save_total_limit[0m
[32m[2022-09-16 13:34:12,401] [    INFO][0m - loss: 0.00175403, learning_rate: 8e-07, global_step: 220, interval_runtime: 14.4663, interval_samples_per_second: 1.106, interval_steps_per_second: 0.691, epoch: 22.0[0m
[32m[2022-09-16 13:34:12,402] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:34:12,402] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:34:12,402] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:34:12,402] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:34:12,402] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:34:13,710] [    INFO][0m - eval_loss: 0.6756623387336731, eval_accuracy: 0.86875, eval_runtime: 1.3076, eval_samples_per_second: 122.366, eval_steps_per_second: 7.648, epoch: 22.0[0m
[32m[2022-09-16 13:34:13,710] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-220[0m
[32m[2022-09-16 13:34:13,711] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:34:16,460] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-220/tokenizer_config.json[0m
[32m[2022-09-16 13:34:16,461] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-220/special_tokens_map.json[0m
[32m[2022-09-16 13:34:21,557] [    INFO][0m - Deleting older checkpoint [checkpoints_eprstmt/checkpoint-210] due to args.save_total_limit[0m
[32m[2022-09-16 13:34:26,053] [    INFO][0m - loss: 0.00215562, learning_rate: 7.000000000000001e-07, global_step: 230, interval_runtime: 13.2048, interval_samples_per_second: 1.212, interval_steps_per_second: 0.757, epoch: 23.0[0m
[32m[2022-09-16 13:34:26,053] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:34:26,054] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:34:26,054] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:34:26,054] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:34:26,054] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:34:27,357] [    INFO][0m - eval_loss: 0.6828585267066956, eval_accuracy: 0.875, eval_runtime: 1.3031, eval_samples_per_second: 122.787, eval_steps_per_second: 7.674, epoch: 23.0[0m
[32m[2022-09-16 13:34:27,357] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-230[0m
[32m[2022-09-16 13:34:27,358] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:34:31,830] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-230/tokenizer_config.json[0m
[32m[2022-09-16 13:34:31,830] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-230/special_tokens_map.json[0m
[32m[2022-09-16 13:34:37,068] [    INFO][0m - Deleting older checkpoint [checkpoints_eprstmt/checkpoint-220] due to args.save_total_limit[0m
[32m[2022-09-16 13:34:45,733] [    INFO][0m - loss: 0.00153463, learning_rate: 6.000000000000001e-07, global_step: 240, interval_runtime: 20.1264, interval_samples_per_second: 0.795, interval_steps_per_second: 0.497, epoch: 24.0[0m
[32m[2022-09-16 13:34:46,319] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:34:46,320] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:34:46,320] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:34:46,320] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:34:46,320] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:34:47,619] [    INFO][0m - eval_loss: 0.6872542500495911, eval_accuracy: 0.875, eval_runtime: 1.2993, eval_samples_per_second: 123.142, eval_steps_per_second: 7.696, epoch: 24.0[0m
[32m[2022-09-16 13:34:47,620] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-240[0m
[32m[2022-09-16 13:34:47,620] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:34:50,285] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-240/tokenizer_config.json[0m
[32m[2022-09-16 13:34:50,285] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-240/special_tokens_map.json[0m
[32m[2022-09-16 13:34:55,359] [    INFO][0m - Deleting older checkpoint [checkpoints_eprstmt/checkpoint-230] due to args.save_total_limit[0m
[32m[2022-09-16 13:34:59,397] [    INFO][0m - loss: 0.00113625, learning_rate: 5e-07, global_step: 250, interval_runtime: 13.6647, interval_samples_per_second: 1.171, interval_steps_per_second: 0.732, epoch: 25.0[0m
[32m[2022-09-16 13:34:59,398] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:34:59,398] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:34:59,398] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:34:59,398] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:34:59,398] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:35:00,695] [    INFO][0m - eval_loss: 0.6902251243591309, eval_accuracy: 0.875, eval_runtime: 1.2969, eval_samples_per_second: 123.374, eval_steps_per_second: 7.711, epoch: 25.0[0m
[32m[2022-09-16 13:35:00,696] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-250[0m
[32m[2022-09-16 13:35:00,696] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:35:03,405] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-250/tokenizer_config.json[0m
[32m[2022-09-16 13:35:03,405] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-250/special_tokens_map.json[0m
[32m[2022-09-16 13:35:08,709] [    INFO][0m - Deleting older checkpoint [checkpoints_eprstmt/checkpoint-240] due to args.save_total_limit[0m
[32m[2022-09-16 13:35:16,064] [    INFO][0m - loss: 0.00130391, learning_rate: 4e-07, global_step: 260, interval_runtime: 13.3841, interval_samples_per_second: 1.195, interval_steps_per_second: 0.747, epoch: 26.0[0m
[32m[2022-09-16 13:35:16,065] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:35:16,065] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:35:16,065] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:35:16,065] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:35:16,065] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:35:17,370] [    INFO][0m - eval_loss: 0.6937281489372253, eval_accuracy: 0.875, eval_runtime: 1.305, eval_samples_per_second: 122.604, eval_steps_per_second: 7.663, epoch: 26.0[0m
[32m[2022-09-16 13:35:17,371] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-260[0m
[32m[2022-09-16 13:35:17,371] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:35:20,157] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-260/tokenizer_config.json[0m
[32m[2022-09-16 13:35:20,158] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-260/special_tokens_map.json[0m
[32m[2022-09-16 13:35:25,670] [    INFO][0m - Deleting older checkpoint [checkpoints_eprstmt/checkpoint-250] due to args.save_total_limit[0m
[32m[2022-09-16 13:35:31,937] [    INFO][0m - loss: 0.00107197, learning_rate: 3.0000000000000004e-07, global_step: 270, interval_runtime: 17.03, interval_samples_per_second: 0.94, interval_steps_per_second: 0.587, epoch: 27.0[0m
[32m[2022-09-16 13:35:31,938] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:35:31,938] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:35:31,938] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:35:31,938] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:35:31,938] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:35:33,240] [    INFO][0m - eval_loss: 0.6964920163154602, eval_accuracy: 0.875, eval_runtime: 1.3016, eval_samples_per_second: 122.923, eval_steps_per_second: 7.683, epoch: 27.0[0m
[32m[2022-09-16 13:35:33,240] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-270[0m
[32m[2022-09-16 13:35:33,241] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:35:35,825] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-270/tokenizer_config.json[0m
[32m[2022-09-16 13:35:35,826] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-270/special_tokens_map.json[0m
[32m[2022-09-16 13:35:41,155] [    INFO][0m - Deleting older checkpoint [checkpoints_eprstmt/checkpoint-260] due to args.save_total_limit[0m
[32m[2022-09-16 13:35:45,232] [    INFO][0m - loss: 0.0009153, learning_rate: 2e-07, global_step: 280, interval_runtime: 15.4204, interval_samples_per_second: 1.038, interval_steps_per_second: 0.648, epoch: 28.0[0m
[32m[2022-09-16 13:35:45,232] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:35:45,233] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:35:45,233] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:35:45,233] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:35:45,233] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:35:46,527] [    INFO][0m - eval_loss: 0.6981760859489441, eval_accuracy: 0.875, eval_runtime: 1.294, eval_samples_per_second: 123.65, eval_steps_per_second: 7.728, epoch: 28.0[0m
[32m[2022-09-16 13:35:47,236] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-280[0m
[32m[2022-09-16 13:35:47,236] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:35:49,968] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-280/tokenizer_config.json[0m
[32m[2022-09-16 13:35:49,968] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-280/special_tokens_map.json[0m
[32m[2022-09-16 13:35:55,310] [    INFO][0m - Deleting older checkpoint [checkpoints_eprstmt/checkpoint-270] due to args.save_total_limit[0m
[32m[2022-09-16 13:35:59,877] [    INFO][0m - loss: 0.00093224, learning_rate: 1e-07, global_step: 290, interval_runtime: 14.6446, interval_samples_per_second: 1.093, interval_steps_per_second: 0.683, epoch: 29.0[0m
[32m[2022-09-16 13:35:59,877] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:35:59,877] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:35:59,877] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:35:59,877] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:35:59,877] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:36:01,784] [    INFO][0m - eval_loss: 0.6992063522338867, eval_accuracy: 0.875, eval_runtime: 1.3061, eval_samples_per_second: 122.501, eval_steps_per_second: 7.656, epoch: 29.0[0m
[32m[2022-09-16 13:36:01,785] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-290[0m
[32m[2022-09-16 13:36:01,785] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:36:04,344] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-290/tokenizer_config.json[0m
[32m[2022-09-16 13:36:04,344] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-290/special_tokens_map.json[0m
[32m[2022-09-16 13:36:09,666] [    INFO][0m - Deleting older checkpoint [checkpoints_eprstmt/checkpoint-280] due to args.save_total_limit[0m
[32m[2022-09-16 13:36:14,716] [    INFO][0m - loss: 0.00099659, learning_rate: 0.0, global_step: 300, interval_runtime: 14.8393, interval_samples_per_second: 1.078, interval_steps_per_second: 0.674, epoch: 30.0[0m
[32m[2022-09-16 13:36:14,717] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:36:14,717] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:36:14,717] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:36:14,717] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:36:14,717] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:36:16,821] [    INFO][0m - eval_loss: 0.6995634436607361, eval_accuracy: 0.875, eval_runtime: 1.3272, eval_samples_per_second: 120.552, eval_steps_per_second: 7.534, epoch: 30.0[0m
[32m[2022-09-16 13:36:16,822] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/checkpoint-300[0m
[32m[2022-09-16 13:36:16,822] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:36:19,582] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/checkpoint-300/tokenizer_config.json[0m
[32m[2022-09-16 13:36:19,582] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/checkpoint-300/special_tokens_map.json[0m
[32m[2022-09-16 13:36:25,649] [    INFO][0m - Deleting older checkpoint [checkpoints_eprstmt/checkpoint-290] due to args.save_total_limit[0m
[32m[2022-09-16 13:36:26,397] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-16 13:36:26,398] [    INFO][0m - Loading best model from ./checkpoints_eprstmt/checkpoint-150 (score: 0.875).[0m
[32m[2022-09-16 13:36:27,807] [    INFO][0m - train_runtime: 498.0014, train_samples_per_second: 9.639, train_steps_per_second: 0.602, train_loss: 0.0887275098854055, epoch: 30.0[0m
[32m[2022-09-16 13:36:27,861] [    INFO][0m - Saving model checkpoint to ./checkpoints_eprstmt/[0m
[32m[2022-09-16 13:36:27,862] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:36:30,709] [    INFO][0m - tokenizer config file saved in ./checkpoints_eprstmt/tokenizer_config.json[0m
[32m[2022-09-16 13:36:30,710] [    INFO][0m - Special tokens file saved in ./checkpoints_eprstmt/special_tokens_map.json[0m
[32m[2022-09-16 13:36:30,711] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-16 13:36:30,711] [    INFO][0m -   epoch                    =       30.0[0m
[32m[2022-09-16 13:36:30,711] [    INFO][0m -   train_loss               =     0.0887[0m
[32m[2022-09-16 13:36:30,711] [    INFO][0m -   train_runtime            = 0:08:18.00[0m
[32m[2022-09-16 13:36:30,711] [    INFO][0m -   train_samples_per_second =      9.639[0m
[32m[2022-09-16 13:36:30,712] [    INFO][0m -   train_steps_per_second   =      0.602[0m
[32m[2022-09-16 13:36:30,714] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-16 13:36:30,715] [    INFO][0m -   Num examples = 610[0m
[32m[2022-09-16 13:36:30,715] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:36:30,715] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:36:30,715] [    INFO][0m -   Total prediction steps = 39[0m
[32m[2022-09-16 13:36:35,819] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-16 13:36:35,820] [    INFO][0m -   test_accuracy           =     0.8754[0m
[32m[2022-09-16 13:36:35,820] [    INFO][0m -   test_loss               =     0.5329[0m
[32m[2022-09-16 13:36:35,820] [    INFO][0m -   test_runtime            = 0:00:05.10[0m
[32m[2022-09-16 13:36:35,820] [    INFO][0m -   test_samples_per_second =    119.497[0m
[32m[2022-09-16 13:36:35,820] [    INFO][0m -   test_steps_per_second   =       7.64[0m
{
  "labels": 0,
  "text_a": "\u7269\u6d41\u5f88\u5feb\uff0c\u65e9\u4e0a\u4e0b\u5355\uff0c\u4e0b\u5348\u5c31\u5230\u4e86\u3002\u5305\u88c5\u4e5f\u5f88\u9ad8\u6863\u3002\u5c31\u662f\u8033\u673a\u97f3\u8d28\u5f88\u5dee\uff0c\u7172\u4e86\u4e00\u767e\u591a\u5c0f\u65f6\uff0c\u97f3\u8d28\u548c\u540c\u4e8b\u7684\u4e00\u767e\u591a\u5143\u7684\u8033\u673a\u5dee\u4e0d\u591a\uff0c1580\u5143\u4e70\u8fd9\u8033\u673a\u4e8f\u5927\u4e86\u3002",
  "text_b": "",
  "uid": 0
}

