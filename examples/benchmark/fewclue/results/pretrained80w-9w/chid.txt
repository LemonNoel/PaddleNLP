[32m[2022-09-16 13:28:02,572] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-16 13:28:02,573] [    INFO][0m - ============================================================[0m
[32m[2022-09-16 13:28:02,573] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-16 13:28:02,573] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-16 13:28:02,573] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-16 13:28:02,573] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-16 13:28:02,573] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-16 13:28:02,573] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-16 13:28:02,573] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-16 13:28:02,574] [    INFO][0m - [0m
[32m[2022-09-16 13:28:02,574] [    INFO][0m - ============================================================[0m
[32m[2022-09-16 13:28:02,574] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-16 13:28:02,574] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-16 13:28:02,574] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-16 13:28:02,574] [    INFO][0m - pretrained                    :/ssd2/wanghuijuan03/data/zero-shot/checkpoints_0915/checkpoint-90000/model_state.pdparams[0m
[32m[2022-09-16 13:28:02,574] [    INFO][0m - prompt                        :‚Äú{'text':'text_a'}‚ÄùÊàêËØ≠{'text':'text_b'}ÊîæÂú®Âè•Â≠êÊã¨Âè∑ÁöÑ‰ΩçÁΩÆÂæà{'mask'}{'mask'}„ÄÇÁ©∫Ê†ºÂ§ÑÂèØÂ°´ÔºöÊÅ∞ÂΩì/Â•áÊÄ™[0m
[32m[2022-09-16 13:28:02,574] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-16 13:28:02,574] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-16 13:28:02,574] [    INFO][0m - task_name                     :chid[0m
[32m[2022-09-16 13:28:02,574] [    INFO][0m - [0m
[32m[2022-09-16 13:28:02,575] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0916 13:28:02.576159 57557 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0916 13:28:02.580163 57557 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-16 13:28:08,224] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-16 13:28:08,239] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-16 13:28:08,239] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-16 13:28:10,019] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùÊàêËØ≠'}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': 'ÊîæÂú®Âè•Â≠êÊã¨Âè∑ÁöÑ‰ΩçÁΩÆÂæà'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': '„ÄÇÁ©∫Ê†ºÂ§ÑÂèØÂ°´ÔºöÊÅ∞ÂΩì/Â•áÊÄ™'}][0m
[32m[2022-09-16 13:28:10,199] [    INFO][0m - ============================================================[0m
[32m[2022-09-16 13:28:10,200] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-16 13:28:10,200] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-16 13:28:10,200] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-16 13:28:10,200] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-16 13:28:10,200] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-16 13:28:10,200] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-16 13:28:10,200] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-16 13:28:10,200] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-16 13:28:10,200] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-16 13:28:10,201] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-16 13:28:10,201] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-16 13:28:10,201] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-16 13:28:10,201] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-16 13:28:10,201] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-16 13:28:10,201] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-16 13:28:10,201] [    INFO][0m - do_predict                    :False[0m
[32m[2022-09-16 13:28:10,201] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-16 13:28:10,201] [    INFO][0m - eval_batch_size               :16[0m
[32m[2022-09-16 13:28:10,201] [    INFO][0m - eval_steps                    :200[0m
[32m[2022-09-16 13:28:10,201] [    INFO][0m - evaluation_strategy           :IntervalStrategy.EPOCH[0m
[32m[2022-09-16 13:28:10,201] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-16 13:28:10,201] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-16 13:28:10,202] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-16 13:28:10,202] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-16 13:28:10,202] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-16 13:28:10,202] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-16 13:28:10,202] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-16 13:28:10,202] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-16 13:28:10,202] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-16 13:28:10,202] [    INFO][0m - learning_rate                 :3e-06[0m
[32m[2022-09-16 13:28:10,202] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-16 13:28:10,202] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-16 13:28:10,202] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-16 13:28:10,202] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-16 13:28:10,202] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-16 13:28:10,202] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-16 13:28:10,203] [    INFO][0m - logging_dir                   :./checkpoints_chid/runs/Sep16_13-28-02_instance-3bwob41y-01[0m
[32m[2022-09-16 13:28:10,203] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-16 13:28:10,203] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-16 13:28:10,203] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-16 13:28:10,203] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-16 13:28:10,203] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-16 13:28:10,203] [    INFO][0m - max_seq_length                :256[0m
[32m[2022-09-16 13:28:10,203] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-16 13:28:10,203] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-16 13:28:10,203] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-16 13:28:10,203] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-16 13:28:10,203] [    INFO][0m - num_train_epochs              :30.0[0m
[32m[2022-09-16 13:28:10,203] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-16 13:28:10,204] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-16 13:28:10,204] [    INFO][0m - output_dir                    :./checkpoints_chid/[0m
[32m[2022-09-16 13:28:10,204] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-16 13:28:10,204] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-16 13:28:10,204] [    INFO][0m - per_device_eval_batch_size    :16[0m
[32m[2022-09-16 13:28:10,204] [    INFO][0m - per_device_train_batch_size   :16[0m
[32m[2022-09-16 13:28:10,204] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-16 13:28:10,204] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-16 13:28:10,204] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-16 13:28:10,204] [    INFO][0m - ppt_learning_rate             :3e-05[0m
[32m[2022-09-16 13:28:10,204] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-16 13:28:10,204] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-16 13:28:10,204] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-16 13:28:10,204] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-16 13:28:10,205] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-16 13:28:10,205] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-16 13:28:10,205] [    INFO][0m - run_name                      :./checkpoints_chid/[0m
[32m[2022-09-16 13:28:10,205] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-16 13:28:10,205] [    INFO][0m - save_steps                    :200[0m
[32m[2022-09-16 13:28:10,205] [    INFO][0m - save_strategy                 :IntervalStrategy.EPOCH[0m
[32m[2022-09-16 13:28:10,205] [    INFO][0m - save_total_limit              :1[0m
[32m[2022-09-16 13:28:10,205] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-16 13:28:10,205] [    INFO][0m - seed                          :42[0m
[32m[2022-09-16 13:28:10,205] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-16 13:28:10,205] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-16 13:28:10,205] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-16 13:28:10,205] [    INFO][0m - train_batch_size              :16[0m
[32m[2022-09-16 13:28:10,206] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-16 13:28:10,206] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-16 13:28:10,206] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-16 13:28:10,206] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-16 13:28:10,206] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-16 13:28:10,206] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-16 13:28:10,206] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-16 13:28:10,206] [    INFO][0m - [0m
[32m[2022-09-16 13:28:10,209] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-16 13:28:10,209] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 13:28:10,209] [    INFO][0m -   Num Epochs = 30[0m
[32m[2022-09-16 13:28:10,209] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2022-09-16 13:28:10,209] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2022-09-16 13:28:10,210] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-16 13:28:10,210] [    INFO][0m -   Total optimization steps = 2670.0[0m
[32m[2022-09-16 13:28:10,210] [    INFO][0m -   Total num train samples = 42420[0m
[32m[2022-09-16 13:28:17,462] [    INFO][0m - loss: 0.4926147, learning_rate: 2.98876404494382e-06, global_step: 10, interval_runtime: 7.2514, interval_samples_per_second: 2.206, interval_steps_per_second: 1.379, epoch: 0.1124[0m
[32m[2022-09-16 13:28:23,731] [    INFO][0m - loss: 0.42477446, learning_rate: 2.9775280898876406e-06, global_step: 20, interval_runtime: 6.2689, interval_samples_per_second: 2.552, interval_steps_per_second: 1.595, epoch: 0.2247[0m
[32m[2022-09-16 13:28:29,995] [    INFO][0m - loss: 0.27030118, learning_rate: 2.9662921348314606e-06, global_step: 30, interval_runtime: 6.2641, interval_samples_per_second: 2.554, interval_steps_per_second: 1.596, epoch: 0.3371[0m
[32m[2022-09-16 13:28:36,320] [    INFO][0m - loss: 0.54874005, learning_rate: 2.955056179775281e-06, global_step: 40, interval_runtime: 6.3244, interval_samples_per_second: 2.53, interval_steps_per_second: 1.581, epoch: 0.4494[0m
[32m[2022-09-16 13:28:42,610] [    INFO][0m - loss: 0.43983984, learning_rate: 2.943820224719101e-06, global_step: 50, interval_runtime: 6.2903, interval_samples_per_second: 2.544, interval_steps_per_second: 1.59, epoch: 0.5618[0m
[32m[2022-09-16 13:28:48,906] [    INFO][0m - loss: 0.40330915, learning_rate: 2.932584269662921e-06, global_step: 60, interval_runtime: 6.2958, interval_samples_per_second: 2.541, interval_steps_per_second: 1.588, epoch: 0.6742[0m
[32m[2022-09-16 13:28:55,203] [    INFO][0m - loss: 0.41650343, learning_rate: 2.921348314606742e-06, global_step: 70, interval_runtime: 6.2975, interval_samples_per_second: 2.541, interval_steps_per_second: 1.588, epoch: 0.7865[0m
[32m[2022-09-16 13:29:01,508] [    INFO][0m - loss: 0.45532174, learning_rate: 2.910112359550562e-06, global_step: 80, interval_runtime: 6.3047, interval_samples_per_second: 2.538, interval_steps_per_second: 1.586, epoch: 0.8989[0m
[32m[2022-09-16 13:29:06,698] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:29:06,699] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 13:29:06,699] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:29:06,699] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:29:06,699] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 13:29:28,193] [    INFO][0m - eval_loss: 0.38206785917282104, eval_accuracy: 0.8557284299858557, eval_runtime: 21.4943, eval_samples_per_second: 65.785, eval_steps_per_second: 4.141, epoch: 1.0[0m
[32m[2022-09-16 13:29:28,215] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-89[0m
[32m[2022-09-16 13:29:28,215] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:29:31,062] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-89/tokenizer_config.json[0m
[32m[2022-09-16 13:29:31,062] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-89/special_tokens_map.json[0m
[32m[2022-09-16 13:29:37,612] [    INFO][0m - loss: 0.38859401, learning_rate: 2.898876404494382e-06, global_step: 90, interval_runtime: 36.1038, interval_samples_per_second: 0.443, interval_steps_per_second: 0.277, epoch: 1.0112[0m
[32m[2022-09-16 13:29:43,916] [    INFO][0m - loss: 0.41232462, learning_rate: 2.8876404494382025e-06, global_step: 100, interval_runtime: 6.3039, interval_samples_per_second: 2.538, interval_steps_per_second: 1.586, epoch: 1.1236[0m
[32m[2022-09-16 13:29:50,227] [    INFO][0m - loss: 0.38839092, learning_rate: 2.8764044943820226e-06, global_step: 110, interval_runtime: 6.3112, interval_samples_per_second: 2.535, interval_steps_per_second: 1.584, epoch: 1.236[0m
[32m[2022-09-16 13:29:56,549] [    INFO][0m - loss: 0.38313174, learning_rate: 2.8651685393258426e-06, global_step: 120, interval_runtime: 6.3218, interval_samples_per_second: 2.531, interval_steps_per_second: 1.582, epoch: 1.3483[0m
[32m[2022-09-16 13:30:02,864] [    INFO][0m - loss: 0.33537669, learning_rate: 2.853932584269663e-06, global_step: 130, interval_runtime: 6.3154, interval_samples_per_second: 2.534, interval_steps_per_second: 1.583, epoch: 1.4607[0m
[32m[2022-09-16 13:30:09,171] [    INFO][0m - loss: 0.35178258, learning_rate: 2.842696629213483e-06, global_step: 140, interval_runtime: 6.3073, interval_samples_per_second: 2.537, interval_steps_per_second: 1.585, epoch: 1.573[0m
[32m[2022-09-16 13:30:15,482] [    INFO][0m - loss: 0.34657819, learning_rate: 2.8314606741573035e-06, global_step: 150, interval_runtime: 6.3104, interval_samples_per_second: 2.536, interval_steps_per_second: 1.585, epoch: 1.6854[0m
[32m[2022-09-16 13:30:21,805] [    INFO][0m - loss: 0.32655466, learning_rate: 2.8202247191011236e-06, global_step: 160, interval_runtime: 6.3237, interval_samples_per_second: 2.53, interval_steps_per_second: 1.581, epoch: 1.7978[0m
[32m[2022-09-16 13:30:28,121] [    INFO][0m - loss: 0.36804628, learning_rate: 2.8089887640449436e-06, global_step: 170, interval_runtime: 6.3161, interval_samples_per_second: 2.533, interval_steps_per_second: 1.583, epoch: 1.9101[0m
[32m[2022-09-16 13:30:32,702] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:30:32,702] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 13:30:32,703] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:30:32,703] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:30:32,703] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 13:30:54,309] [    INFO][0m - eval_loss: 0.35855457186698914, eval_accuracy: 0.8585572842998586, eval_runtime: 21.6062, eval_samples_per_second: 65.444, eval_steps_per_second: 4.119, epoch: 2.0[0m
[32m[2022-09-16 13:30:58,639] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-178[0m
[32m[2022-09-16 13:30:58,639] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:31:01,577] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-178/tokenizer_config.json[0m
[32m[2022-09-16 13:31:01,578] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-178/special_tokens_map.json[0m
[32m[2022-09-16 13:31:09,424] [    INFO][0m - loss: 0.29941237, learning_rate: 2.797752808988764e-06, global_step: 180, interval_runtime: 41.3021, interval_samples_per_second: 0.387, interval_steps_per_second: 0.242, epoch: 2.0225[0m
[32m[2022-09-16 13:31:15,722] [    INFO][0m - loss: 0.29113445, learning_rate: 2.7865168539325845e-06, global_step: 190, interval_runtime: 6.2982, interval_samples_per_second: 2.54, interval_steps_per_second: 1.588, epoch: 2.1348[0m
[32m[2022-09-16 13:31:22,032] [    INFO][0m - loss: 0.33200929, learning_rate: 2.7752808988764045e-06, global_step: 200, interval_runtime: 6.3105, interval_samples_per_second: 2.535, interval_steps_per_second: 1.585, epoch: 2.2472[0m
[32m[2022-09-16 13:31:30,101] [    INFO][0m - loss: 0.31857381, learning_rate: 2.764044943820225e-06, global_step: 210, interval_runtime: 6.3112, interval_samples_per_second: 2.535, interval_steps_per_second: 1.584, epoch: 2.3596[0m
[32m[2022-09-16 13:31:36,405] [    INFO][0m - loss: 0.34956231, learning_rate: 2.752808988764045e-06, global_step: 220, interval_runtime: 8.062, interval_samples_per_second: 1.985, interval_steps_per_second: 1.24, epoch: 2.4719[0m
[32m[2022-09-16 13:31:42,718] [    INFO][0m - loss: 0.33941028, learning_rate: 2.7415730337078655e-06, global_step: 230, interval_runtime: 6.3121, interval_samples_per_second: 2.535, interval_steps_per_second: 1.584, epoch: 2.5843[0m
[32m[2022-09-16 13:31:49,030] [    INFO][0m - loss: 0.3464967, learning_rate: 2.7303370786516855e-06, global_step: 240, interval_runtime: 6.3123, interval_samples_per_second: 2.535, interval_steps_per_second: 1.584, epoch: 2.6966[0m
[32m[2022-09-16 13:31:55,355] [    INFO][0m - loss: 0.27723086, learning_rate: 2.7191011235955055e-06, global_step: 250, interval_runtime: 6.3254, interval_samples_per_second: 2.529, interval_steps_per_second: 1.581, epoch: 2.809[0m
[32m[2022-09-16 13:32:01,680] [    INFO][0m - loss: 0.25571487, learning_rate: 2.707865168539326e-06, global_step: 260, interval_runtime: 6.3245, interval_samples_per_second: 2.53, interval_steps_per_second: 1.581, epoch: 2.9213[0m
[32m[2022-09-16 13:32:05,630] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:32:05,630] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 13:32:05,631] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:32:05,631] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:32:05,631] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 13:32:27,188] [    INFO][0m - eval_loss: 0.35019877552986145, eval_accuracy: 0.8656294200848657, eval_runtime: 21.5567, eval_samples_per_second: 65.594, eval_steps_per_second: 4.129, epoch: 3.0[0m
[32m[2022-09-16 13:32:27,210] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-267[0m
[32m[2022-09-16 13:32:27,210] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:32:29,876] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-267/tokenizer_config.json[0m
[32m[2022-09-16 13:32:29,876] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-267/special_tokens_map.json[0m
[32m[2022-09-16 13:32:35,418] [    INFO][0m - Deleting older checkpoint [checkpoints_chid/checkpoint-89] due to args.save_total_limit[0m
[32m[2022-09-16 13:32:38,770] [    INFO][0m - loss: 0.28135672, learning_rate: 2.696629213483146e-06, global_step: 270, interval_runtime: 37.0898, interval_samples_per_second: 0.431, interval_steps_per_second: 0.27, epoch: 3.0337[0m
[32m[2022-09-16 13:32:45,109] [    INFO][0m - loss: 0.27902269, learning_rate: 2.685393258426966e-06, global_step: 280, interval_runtime: 6.3392, interval_samples_per_second: 2.524, interval_steps_per_second: 1.577, epoch: 3.1461[0m
[32m[2022-09-16 13:32:51,436] [    INFO][0m - loss: 0.26560774, learning_rate: 2.6741573033707865e-06, global_step: 290, interval_runtime: 6.3266, interval_samples_per_second: 2.529, interval_steps_per_second: 1.581, epoch: 3.2584[0m
[32m[2022-09-16 13:32:58,996] [    INFO][0m - loss: 0.26766005, learning_rate: 2.6629213483146066e-06, global_step: 300, interval_runtime: 6.3341, interval_samples_per_second: 2.526, interval_steps_per_second: 1.579, epoch: 3.3708[0m
[32m[2022-09-16 13:33:05,318] [    INFO][0m - loss: 0.27328556, learning_rate: 2.651685393258427e-06, global_step: 310, interval_runtime: 7.5489, interval_samples_per_second: 2.12, interval_steps_per_second: 1.325, epoch: 3.4831[0m
[32m[2022-09-16 13:33:12,254] [    INFO][0m - loss: 0.22289672, learning_rate: 2.6404494382022475e-06, global_step: 320, interval_runtime: 6.329, interval_samples_per_second: 2.528, interval_steps_per_second: 1.58, epoch: 3.5955[0m
[32m[2022-09-16 13:33:18,588] [    INFO][0m - loss: 0.2869061, learning_rate: 2.6292134831460675e-06, global_step: 330, interval_runtime: 6.9402, interval_samples_per_second: 2.305, interval_steps_per_second: 1.441, epoch: 3.7079[0m
[32m[2022-09-16 13:33:24,934] [    INFO][0m - loss: 0.22690406, learning_rate: 2.617977528089888e-06, global_step: 340, interval_runtime: 6.3461, interval_samples_per_second: 2.521, interval_steps_per_second: 1.576, epoch: 3.8202[0m
[32m[2022-09-16 13:33:31,287] [    INFO][0m - loss: 0.22353811, learning_rate: 2.606741573033708e-06, global_step: 350, interval_runtime: 6.3532, interval_samples_per_second: 2.518, interval_steps_per_second: 1.574, epoch: 3.9326[0m
[32m[2022-09-16 13:33:34,620] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:33:34,620] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 13:33:34,620] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:33:34,621] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:33:34,621] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 13:33:56,168] [    INFO][0m - eval_loss: 0.3554017245769501, eval_accuracy: 0.8592644978783592, eval_runtime: 21.5467, eval_samples_per_second: 65.625, eval_steps_per_second: 4.131, epoch: 4.0[0m
[32m[2022-09-16 13:33:56,189] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-356[0m
[32m[2022-09-16 13:33:56,189] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:33:59,093] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-356/tokenizer_config.json[0m
[32m[2022-09-16 13:33:59,093] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-356/special_tokens_map.json[0m
[32m[2022-09-16 13:34:04,719] [    INFO][0m - Deleting older checkpoint [checkpoints_chid/checkpoint-178] due to args.save_total_limit[0m
[32m[2022-09-16 13:34:07,992] [    INFO][0m - loss: 0.28415935, learning_rate: 2.595505617977528e-06, global_step: 360, interval_runtime: 36.7052, interval_samples_per_second: 0.436, interval_steps_per_second: 0.272, epoch: 4.0449[0m
[32m[2022-09-16 13:34:15,598] [    INFO][0m - loss: 0.2097008, learning_rate: 2.5842696629213485e-06, global_step: 370, interval_runtime: 7.6067, interval_samples_per_second: 2.103, interval_steps_per_second: 1.315, epoch: 4.1573[0m
[32m[2022-09-16 13:34:21,912] [    INFO][0m - loss: 0.25164614, learning_rate: 2.5730337078651685e-06, global_step: 380, interval_runtime: 6.3136, interval_samples_per_second: 2.534, interval_steps_per_second: 1.584, epoch: 4.2697[0m
[32m[2022-09-16 13:34:28,239] [    INFO][0m - loss: 0.22319341, learning_rate: 2.561797752808989e-06, global_step: 390, interval_runtime: 6.3262, interval_samples_per_second: 2.529, interval_steps_per_second: 1.581, epoch: 4.382[0m
[32m[2022-09-16 13:34:34,568] [    INFO][0m - loss: 0.23421857, learning_rate: 2.550561797752809e-06, global_step: 400, interval_runtime: 6.33, interval_samples_per_second: 2.528, interval_steps_per_second: 1.58, epoch: 4.4944[0m
[32m[2022-09-16 13:34:40,908] [    INFO][0m - loss: 0.21030278, learning_rate: 2.539325842696629e-06, global_step: 410, interval_runtime: 6.3399, interval_samples_per_second: 2.524, interval_steps_per_second: 1.577, epoch: 4.6067[0m
[32m[2022-09-16 13:34:47,327] [    INFO][0m - loss: 0.27661529, learning_rate: 2.5280898876404495e-06, global_step: 420, interval_runtime: 6.4191, interval_samples_per_second: 2.493, interval_steps_per_second: 1.558, epoch: 4.7191[0m
[32m[2022-09-16 13:34:53,672] [    INFO][0m - loss: 0.32481096, learning_rate: 2.51685393258427e-06, global_step: 430, interval_runtime: 6.3448, interval_samples_per_second: 2.522, interval_steps_per_second: 1.576, epoch: 4.8315[0m
[32m[2022-09-16 13:34:59,988] [    INFO][0m - loss: 0.21139016, learning_rate: 2.50561797752809e-06, global_step: 440, interval_runtime: 6.3163, interval_samples_per_second: 2.533, interval_steps_per_second: 1.583, epoch: 4.9438[0m
[32m[2022-09-16 13:35:02,692] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:35:02,692] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 13:35:02,692] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:35:02,692] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:35:02,693] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 13:35:24,234] [    INFO][0m - eval_loss: 0.37277933955192566, eval_accuracy: 0.862093352192362, eval_runtime: 21.5409, eval_samples_per_second: 65.643, eval_steps_per_second: 4.132, epoch: 5.0[0m
[32m[2022-09-16 13:35:24,253] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-445[0m
[32m[2022-09-16 13:35:24,254] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:35:31,937] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-445/tokenizer_config.json[0m
[32m[2022-09-16 13:35:31,937] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-445/special_tokens_map.json[0m
[32m[2022-09-16 13:35:37,167] [    INFO][0m - Deleting older checkpoint [checkpoints_chid/checkpoint-356] due to args.save_total_limit[0m
[32m[2022-09-16 13:35:41,119] [    INFO][0m - loss: 0.1757334, learning_rate: 2.4943820224719104e-06, global_step: 450, interval_runtime: 41.1305, interval_samples_per_second: 0.389, interval_steps_per_second: 0.243, epoch: 5.0562[0m
[32m[2022-09-16 13:35:47,428] [    INFO][0m - loss: 0.23893664, learning_rate: 2.4831460674157305e-06, global_step: 460, interval_runtime: 6.3088, interval_samples_per_second: 2.536, interval_steps_per_second: 1.585, epoch: 5.1685[0m
[32m[2022-09-16 13:35:53,770] [    INFO][0m - loss: 0.17690703, learning_rate: 2.4719101123595505e-06, global_step: 470, interval_runtime: 6.342, interval_samples_per_second: 2.523, interval_steps_per_second: 1.577, epoch: 5.2809[0m
[32m[2022-09-16 13:36:00,103] [    INFO][0m - loss: 0.20998473, learning_rate: 2.460674157303371e-06, global_step: 480, interval_runtime: 6.3323, interval_samples_per_second: 2.527, interval_steps_per_second: 1.579, epoch: 5.3933[0m
[32m[2022-09-16 13:36:06,455] [    INFO][0m - loss: 0.20389011, learning_rate: 2.449438202247191e-06, global_step: 490, interval_runtime: 6.3528, interval_samples_per_second: 2.519, interval_steps_per_second: 1.574, epoch: 5.5056[0m
[32m[2022-09-16 13:36:12,793] [    INFO][0m - loss: 0.15780621, learning_rate: 2.4382022471910114e-06, global_step: 500, interval_runtime: 6.3379, interval_samples_per_second: 2.524, interval_steps_per_second: 1.578, epoch: 5.618[0m
[32m[2022-09-16 13:36:19,125] [    INFO][0m - loss: 0.15265055, learning_rate: 2.4269662921348315e-06, global_step: 510, interval_runtime: 6.3322, interval_samples_per_second: 2.527, interval_steps_per_second: 1.579, epoch: 5.7303[0m
[32m[2022-09-16 13:36:25,463] [    INFO][0m - loss: 0.2785486, learning_rate: 2.4157303370786515e-06, global_step: 520, interval_runtime: 6.3376, interval_samples_per_second: 2.525, interval_steps_per_second: 1.578, epoch: 5.8427[0m
[32m[2022-09-16 13:36:31,770] [    INFO][0m - loss: 0.14066344, learning_rate: 2.404494382022472e-06, global_step: 530, interval_runtime: 6.3068, interval_samples_per_second: 2.537, interval_steps_per_second: 1.586, epoch: 5.9551[0m
[32m[2022-09-16 13:36:33,862] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:36:33,862] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 13:36:33,862] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:36:33,862] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:36:33,862] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 13:36:55,528] [    INFO][0m - eval_loss: 0.40042540431022644, eval_accuracy: 0.8578500707213579, eval_runtime: 21.6653, eval_samples_per_second: 65.266, eval_steps_per_second: 4.108, epoch: 6.0[0m
[32m[2022-09-16 13:36:55,555] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-534[0m
[32m[2022-09-16 13:36:55,555] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:36:58,324] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-534/tokenizer_config.json[0m
[32m[2022-09-16 13:36:58,324] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-534/special_tokens_map.json[0m
[32m[2022-09-16 13:37:04,473] [    INFO][0m - Deleting older checkpoint [checkpoints_chid/checkpoint-445] due to args.save_total_limit[0m
[32m[2022-09-16 13:37:08,984] [    INFO][0m - loss: 0.22214699, learning_rate: 2.393258426966292e-06, global_step: 540, interval_runtime: 37.2145, interval_samples_per_second: 0.43, interval_steps_per_second: 0.269, epoch: 6.0674[0m
[32m[2022-09-16 13:37:15,320] [    INFO][0m - loss: 0.18311329, learning_rate: 2.3820224719101125e-06, global_step: 550, interval_runtime: 6.3352, interval_samples_per_second: 2.526, interval_steps_per_second: 1.578, epoch: 6.1798[0m
[32m[2022-09-16 13:37:21,650] [    INFO][0m - loss: 0.16516471, learning_rate: 2.370786516853933e-06, global_step: 560, interval_runtime: 6.3306, interval_samples_per_second: 2.527, interval_steps_per_second: 1.58, epoch: 6.2921[0m
[32m[2022-09-16 13:37:27,986] [    INFO][0m - loss: 0.21671476, learning_rate: 2.359550561797753e-06, global_step: 570, interval_runtime: 6.3353, interval_samples_per_second: 2.526, interval_steps_per_second: 1.578, epoch: 6.4045[0m
[32m[2022-09-16 13:37:34,308] [    INFO][0m - loss: 0.16475532, learning_rate: 2.348314606741573e-06, global_step: 580, interval_runtime: 6.3231, interval_samples_per_second: 2.53, interval_steps_per_second: 1.582, epoch: 6.5169[0m
[32m[2022-09-16 13:37:40,663] [    INFO][0m - loss: 0.14997532, learning_rate: 2.3370786516853934e-06, global_step: 590, interval_runtime: 6.3279, interval_samples_per_second: 2.528, interval_steps_per_second: 1.58, epoch: 6.6292[0m
[32m[2022-09-16 13:37:46,992] [    INFO][0m - loss: 0.10558867, learning_rate: 2.3258426966292135e-06, global_step: 600, interval_runtime: 6.3559, interval_samples_per_second: 2.517, interval_steps_per_second: 1.573, epoch: 6.7416[0m
[32m[2022-09-16 13:37:53,318] [    INFO][0m - loss: 0.31818156, learning_rate: 2.314606741573034e-06, global_step: 610, interval_runtime: 6.3255, interval_samples_per_second: 2.529, interval_steps_per_second: 1.581, epoch: 6.8539[0m
[32m[2022-09-16 13:37:59,600] [    INFO][0m - loss: 0.16463915, learning_rate: 2.303370786516854e-06, global_step: 620, interval_runtime: 6.2824, interval_samples_per_second: 2.547, interval_steps_per_second: 1.592, epoch: 6.9663[0m
[32m[2022-09-16 13:38:01,081] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:38:01,082] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 13:38:01,082] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:38:01,082] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:38:01,082] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 13:38:22,725] [    INFO][0m - eval_loss: 0.45257440209388733, eval_accuracy: 0.8472418670438473, eval_runtime: 21.6424, eval_samples_per_second: 65.335, eval_steps_per_second: 4.112, epoch: 7.0[0m
[32m[2022-09-16 13:38:22,749] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-623[0m
[32m[2022-09-16 13:38:22,750] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:38:25,760] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-623/tokenizer_config.json[0m
[32m[2022-09-16 13:38:25,849] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-623/special_tokens_map.json[0m
[32m[2022-09-16 13:38:31,271] [    INFO][0m - Deleting older checkpoint [checkpoints_chid/checkpoint-534] due to args.save_total_limit[0m
[32m[2022-09-16 13:38:36,401] [    INFO][0m - loss: 0.19986298, learning_rate: 2.292134831460674e-06, global_step: 630, interval_runtime: 36.8004, interval_samples_per_second: 0.435, interval_steps_per_second: 0.272, epoch: 7.0787[0m
[32m[2022-09-16 13:38:42,726] [    INFO][0m - loss: 0.17307651, learning_rate: 2.2808988764044944e-06, global_step: 640, interval_runtime: 6.3253, interval_samples_per_second: 2.53, interval_steps_per_second: 1.581, epoch: 7.191[0m
[32m[2022-09-16 13:38:52,760] [    INFO][0m - loss: 0.14255787, learning_rate: 2.2696629213483145e-06, global_step: 650, interval_runtime: 6.3109, interval_samples_per_second: 2.535, interval_steps_per_second: 1.585, epoch: 7.3034[0m
[32m[2022-09-16 13:39:00,131] [    INFO][0m - loss: 0.14359604, learning_rate: 2.258426966292135e-06, global_step: 660, interval_runtime: 10.044, interval_samples_per_second: 1.593, interval_steps_per_second: 0.996, epoch: 7.4157[0m
[32m[2022-09-16 13:39:06,438] [    INFO][0m - loss: 0.12426296, learning_rate: 2.2471910112359554e-06, global_step: 670, interval_runtime: 7.3577, interval_samples_per_second: 2.175, interval_steps_per_second: 1.359, epoch: 7.5281[0m
[32m[2022-09-16 13:39:12,759] [    INFO][0m - loss: 0.1127452, learning_rate: 2.2359550561797754e-06, global_step: 680, interval_runtime: 6.3209, interval_samples_per_second: 2.531, interval_steps_per_second: 1.582, epoch: 7.6404[0m
[32m[2022-09-16 13:39:19,085] [    INFO][0m - loss: 0.21105773, learning_rate: 2.224719101123596e-06, global_step: 690, interval_runtime: 6.3255, interval_samples_per_second: 2.529, interval_steps_per_second: 1.581, epoch: 7.7528[0m
[32m[2022-09-16 13:39:25,419] [    INFO][0m - loss: 0.10812249, learning_rate: 2.213483146067416e-06, global_step: 700, interval_runtime: 6.3338, interval_samples_per_second: 2.526, interval_steps_per_second: 1.579, epoch: 7.8652[0m
[32m[2022-09-16 13:39:31,683] [    INFO][0m - loss: 0.20494809, learning_rate: 2.202247191011236e-06, global_step: 710, interval_runtime: 6.2646, interval_samples_per_second: 2.554, interval_steps_per_second: 1.596, epoch: 7.9775[0m
[32m[2022-09-16 13:39:32,549] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:39:32,550] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 13:39:32,550] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:39:32,550] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:39:32,550] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 13:39:54,224] [    INFO][0m - eval_loss: 0.5034892559051514, eval_accuracy: 0.8521923620933521, eval_runtime: 21.6734, eval_samples_per_second: 65.241, eval_steps_per_second: 4.106, epoch: 8.0[0m
[32m[2022-09-16 13:39:54,251] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-712[0m
[32m[2022-09-16 13:39:54,251] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:39:56,955] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-712/tokenizer_config.json[0m
[32m[2022-09-16 13:39:56,955] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-712/special_tokens_map.json[0m
[32m[2022-09-16 13:40:02,016] [    INFO][0m - Deleting older checkpoint [checkpoints_chid/checkpoint-623] due to args.save_total_limit[0m
[32m[2022-09-16 13:40:07,781] [    INFO][0m - loss: 0.12622123, learning_rate: 2.1910112359550564e-06, global_step: 720, interval_runtime: 36.0983, interval_samples_per_second: 0.443, interval_steps_per_second: 0.277, epoch: 8.0899[0m
[32m[2022-09-16 13:40:14,129] [    INFO][0m - loss: 0.13848993, learning_rate: 2.1797752808988764e-06, global_step: 730, interval_runtime: 6.3472, interval_samples_per_second: 2.521, interval_steps_per_second: 1.575, epoch: 8.2022[0m
[32m[2022-09-16 13:40:20,475] [    INFO][0m - loss: 0.14224334, learning_rate: 2.1685393258426965e-06, global_step: 740, interval_runtime: 6.3461, interval_samples_per_second: 2.521, interval_steps_per_second: 1.576, epoch: 8.3146[0m
[32m[2022-09-16 13:40:26,853] [    INFO][0m - loss: 0.06795601, learning_rate: 2.157303370786517e-06, global_step: 750, interval_runtime: 6.3778, interval_samples_per_second: 2.509, interval_steps_per_second: 1.568, epoch: 8.427[0m
[32m[2022-09-16 13:40:33,216] [    INFO][0m - loss: 0.17385408, learning_rate: 2.146067415730337e-06, global_step: 760, interval_runtime: 6.363, interval_samples_per_second: 2.515, interval_steps_per_second: 1.572, epoch: 8.5393[0m
[32m[2022-09-16 13:40:39,552] [    INFO][0m - loss: 0.07537013, learning_rate: 2.1348314606741574e-06, global_step: 770, interval_runtime: 6.3361, interval_samples_per_second: 2.525, interval_steps_per_second: 1.578, epoch: 8.6517[0m
[32m[2022-09-16 13:40:45,896] [    INFO][0m - loss: 0.14680908, learning_rate: 2.1235955056179774e-06, global_step: 780, interval_runtime: 6.3439, interval_samples_per_second: 2.522, interval_steps_per_second: 1.576, epoch: 8.764[0m
[32m[2022-09-16 13:40:52,224] [    INFO][0m - loss: 0.26847808, learning_rate: 2.112359550561798e-06, global_step: 790, interval_runtime: 6.3283, interval_samples_per_second: 2.528, interval_steps_per_second: 1.58, epoch: 8.8764[0m
[32m[2022-09-16 13:40:58,481] [    INFO][0m - loss: 0.19617625, learning_rate: 2.1011235955056183e-06, global_step: 800, interval_runtime: 6.2568, interval_samples_per_second: 2.557, interval_steps_per_second: 1.598, epoch: 8.9888[0m
[32m[2022-09-16 13:40:58,734] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:40:58,734] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 13:40:58,734] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:40:58,734] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:40:58,735] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 13:41:20,318] [    INFO][0m - eval_loss: 0.5675128698348999, eval_accuracy: 0.8444130127298444, eval_runtime: 21.5834, eval_samples_per_second: 65.513, eval_steps_per_second: 4.124, epoch: 9.0[0m
[32m[2022-09-16 13:41:20,345] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-801[0m
[32m[2022-09-16 13:41:20,345] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:41:23,171] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-801/tokenizer_config.json[0m
[32m[2022-09-16 13:41:23,172] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-801/special_tokens_map.json[0m
[32m[2022-09-16 13:41:28,186] [    INFO][0m - Deleting older checkpoint [checkpoints_chid/checkpoint-712] due to args.save_total_limit[0m
[32m[2022-09-16 13:41:34,557] [    INFO][0m - loss: 0.09150991, learning_rate: 2.0898876404494384e-06, global_step: 810, interval_runtime: 36.0762, interval_samples_per_second: 0.444, interval_steps_per_second: 0.277, epoch: 9.1011[0m
[32m[2022-09-16 13:41:40,886] [    INFO][0m - loss: 0.13478367, learning_rate: 2.0786516853932584e-06, global_step: 820, interval_runtime: 6.3284, interval_samples_per_second: 2.528, interval_steps_per_second: 1.58, epoch: 9.2135[0m
[32m[2022-09-16 13:41:47,210] [    INFO][0m - loss: 0.04252456, learning_rate: 2.067415730337079e-06, global_step: 830, interval_runtime: 6.3249, interval_samples_per_second: 2.53, interval_steps_per_second: 1.581, epoch: 9.3258[0m
[32m[2022-09-16 13:41:53,555] [    INFO][0m - loss: 0.08153338, learning_rate: 2.056179775280899e-06, global_step: 840, interval_runtime: 6.3447, interval_samples_per_second: 2.522, interval_steps_per_second: 1.576, epoch: 9.4382[0m
[32m[2022-09-16 13:41:59,889] [    INFO][0m - loss: 0.1539206, learning_rate: 2.0449438202247194e-06, global_step: 850, interval_runtime: 6.3337, interval_samples_per_second: 2.526, interval_steps_per_second: 1.579, epoch: 9.5506[0m
[32m[2022-09-16 13:42:06,238] [    INFO][0m - loss: 0.14478145, learning_rate: 2.0337078651685394e-06, global_step: 860, interval_runtime: 6.3492, interval_samples_per_second: 2.52, interval_steps_per_second: 1.575, epoch: 9.6629[0m
[32m[2022-09-16 13:42:12,578] [    INFO][0m - loss: 0.2295933, learning_rate: 2.0224719101123594e-06, global_step: 870, interval_runtime: 6.3403, interval_samples_per_second: 2.524, interval_steps_per_second: 1.577, epoch: 9.7753[0m
[32m[2022-09-16 13:42:18,952] [    INFO][0m - loss: 0.13184344, learning_rate: 2.01123595505618e-06, global_step: 880, interval_runtime: 6.3735, interval_samples_per_second: 2.51, interval_steps_per_second: 1.569, epoch: 9.8876[0m
[32m[2022-09-16 13:42:24,830] [    INFO][0m - loss: 0.05526951, learning_rate: 2e-06, global_step: 890, interval_runtime: 5.8777, interval_samples_per_second: 2.722, interval_steps_per_second: 1.701, epoch: 10.0[0m
[32m[2022-09-16 13:42:24,830] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:42:24,830] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 13:42:24,830] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:42:24,831] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:42:24,831] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 13:42:46,512] [    INFO][0m - eval_loss: 0.6529371738433838, eval_accuracy: 0.847949080622348, eval_runtime: 21.6808, eval_samples_per_second: 65.219, eval_steps_per_second: 4.105, epoch: 10.0[0m
[32m[2022-09-16 13:42:46,537] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-890[0m
[32m[2022-09-16 13:42:46,537] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:42:49,234] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-890/tokenizer_config.json[0m
[32m[2022-09-16 13:42:49,234] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-890/special_tokens_map.json[0m
[32m[2022-09-16 13:42:54,230] [    INFO][0m - Deleting older checkpoint [checkpoints_chid/checkpoint-801] due to args.save_total_limit[0m
[32m[2022-09-16 13:43:01,228] [    INFO][0m - loss: 0.16919495, learning_rate: 1.98876404494382e-06, global_step: 900, interval_runtime: 36.3988, interval_samples_per_second: 0.44, interval_steps_per_second: 0.275, epoch: 10.1124[0m
[32m[2022-09-16 13:43:07,559] [    INFO][0m - loss: 0.04915774, learning_rate: 1.9775280898876404e-06, global_step: 910, interval_runtime: 6.3306, interval_samples_per_second: 2.527, interval_steps_per_second: 1.58, epoch: 10.2247[0m
[32m[2022-09-16 13:43:13,899] [    INFO][0m - loss: 0.1362259, learning_rate: 1.966292134831461e-06, global_step: 920, interval_runtime: 6.3401, interval_samples_per_second: 2.524, interval_steps_per_second: 1.577, epoch: 10.3371[0m
[32m[2022-09-16 13:43:20,261] [    INFO][0m - loss: 0.12580748, learning_rate: 1.955056179775281e-06, global_step: 930, interval_runtime: 6.3614, interval_samples_per_second: 2.515, interval_steps_per_second: 1.572, epoch: 10.4494[0m
[32m[2022-09-16 13:43:26,621] [    INFO][0m - loss: 0.15792592, learning_rate: 1.9438202247191013e-06, global_step: 940, interval_runtime: 6.3604, interval_samples_per_second: 2.516, interval_steps_per_second: 1.572, epoch: 10.5618[0m
[32m[2022-09-16 13:43:32,965] [    INFO][0m - loss: 0.04098777, learning_rate: 1.9325842696629214e-06, global_step: 950, interval_runtime: 6.3437, interval_samples_per_second: 2.522, interval_steps_per_second: 1.576, epoch: 10.6742[0m
[32m[2022-09-16 13:43:39,301] [    INFO][0m - loss: 0.15999095, learning_rate: 1.921348314606742e-06, global_step: 960, interval_runtime: 6.3368, interval_samples_per_second: 2.525, interval_steps_per_second: 1.578, epoch: 10.7865[0m
[32m[2022-09-16 13:43:45,670] [    INFO][0m - loss: 0.10284224, learning_rate: 1.910112359550562e-06, global_step: 970, interval_runtime: 6.3688, interval_samples_per_second: 2.512, interval_steps_per_second: 1.57, epoch: 10.8989[0m
[32m[2022-09-16 13:43:50,902] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:43:50,902] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 13:43:50,902] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:43:50,902] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:43:50,902] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 13:44:12,537] [    INFO][0m - eval_loss: 0.6827399134635925, eval_accuracy: 0.8571428571428571, eval_runtime: 21.6343, eval_samples_per_second: 65.359, eval_steps_per_second: 4.114, epoch: 11.0[0m
[32m[2022-09-16 13:44:12,561] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-979[0m
[32m[2022-09-16 13:44:12,562] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:44:15,365] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-979/tokenizer_config.json[0m
[32m[2022-09-16 13:44:15,365] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-979/special_tokens_map.json[0m
[32m[2022-09-16 13:44:21,376] [    INFO][0m - Deleting older checkpoint [checkpoints_chid/checkpoint-890] due to args.save_total_limit[0m
[32m[2022-09-16 13:44:22,739] [    INFO][0m - loss: 0.09057196, learning_rate: 1.8988764044943821e-06, global_step: 980, interval_runtime: 37.0683, interval_samples_per_second: 0.432, interval_steps_per_second: 0.27, epoch: 11.0112[0m
[32m[2022-09-16 13:44:29,057] [    INFO][0m - loss: 0.07910833, learning_rate: 1.8876404494382021e-06, global_step: 990, interval_runtime: 6.3182, interval_samples_per_second: 2.532, interval_steps_per_second: 1.583, epoch: 11.1236[0m
[32m[2022-09-16 13:44:36,559] [    INFO][0m - loss: 0.01080736, learning_rate: 1.8764044943820224e-06, global_step: 1000, interval_runtime: 6.3265, interval_samples_per_second: 2.529, interval_steps_per_second: 1.581, epoch: 11.236[0m
[32m[2022-09-16 13:44:42,907] [    INFO][0m - loss: 0.18680135, learning_rate: 1.8651685393258426e-06, global_step: 1010, interval_runtime: 7.5243, interval_samples_per_second: 2.126, interval_steps_per_second: 1.329, epoch: 11.3483[0m
[32m[2022-09-16 13:44:49,236] [    INFO][0m - loss: 0.14241486, learning_rate: 1.8539325842696629e-06, global_step: 1020, interval_runtime: 6.3278, interval_samples_per_second: 2.529, interval_steps_per_second: 1.58, epoch: 11.4607[0m
[32m[2022-09-16 13:44:55,568] [    INFO][0m - loss: 0.04588244, learning_rate: 1.8426966292134831e-06, global_step: 1030, interval_runtime: 6.3323, interval_samples_per_second: 2.527, interval_steps_per_second: 1.579, epoch: 11.573[0m
[32m[2022-09-16 13:45:01,913] [    INFO][0m - loss: 0.08907779, learning_rate: 1.8314606741573036e-06, global_step: 1040, interval_runtime: 6.3451, interval_samples_per_second: 2.522, interval_steps_per_second: 1.576, epoch: 11.6854[0m
[32m[2022-09-16 13:45:08,718] [    INFO][0m - loss: 0.14567369, learning_rate: 1.8202247191011238e-06, global_step: 1050, interval_runtime: 6.3746, interval_samples_per_second: 2.51, interval_steps_per_second: 1.569, epoch: 11.7978[0m
[32m[2022-09-16 13:45:15,073] [    INFO][0m - loss: 0.11839002, learning_rate: 1.8089887640449439e-06, global_step: 1060, interval_runtime: 6.785, interval_samples_per_second: 2.358, interval_steps_per_second: 1.474, epoch: 11.9101[0m
[32m[2022-09-16 13:45:19,665] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:45:19,666] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 13:45:19,666] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:45:19,666] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:45:19,666] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 13:45:41,235] [    INFO][0m - eval_loss: 0.7844032049179077, eval_accuracy: 0.8550212164073551, eval_runtime: 21.5692, eval_samples_per_second: 65.557, eval_steps_per_second: 4.126, epoch: 12.0[0m
[32m[2022-09-16 13:45:41,259] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-1068[0m
[32m[2022-09-16 13:45:41,259] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:45:43,978] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-1068/tokenizer_config.json[0m
[32m[2022-09-16 13:45:43,978] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-1068/special_tokens_map.json[0m
[32m[2022-09-16 13:45:49,139] [    INFO][0m - Deleting older checkpoint [checkpoints_chid/checkpoint-979] due to args.save_total_limit[0m
[32m[2022-09-16 13:45:51,138] [    INFO][0m - loss: 0.14908707, learning_rate: 1.797752808988764e-06, global_step: 1070, interval_runtime: 36.0656, interval_samples_per_second: 0.444, interval_steps_per_second: 0.277, epoch: 12.0225[0m
[32m[2022-09-16 13:45:57,447] [    INFO][0m - loss: 0.10631679, learning_rate: 1.7865168539325843e-06, global_step: 1080, interval_runtime: 6.3089, interval_samples_per_second: 2.536, interval_steps_per_second: 1.585, epoch: 12.1348[0m
[32m[2022-09-16 13:46:03,807] [    INFO][0m - loss: 0.06904432, learning_rate: 1.7752808988764046e-06, global_step: 1090, interval_runtime: 6.3602, interval_samples_per_second: 2.516, interval_steps_per_second: 1.572, epoch: 12.2472[0m
[32m[2022-09-16 13:46:10,178] [    INFO][0m - loss: 0.12167925, learning_rate: 1.7640449438202248e-06, global_step: 1100, interval_runtime: 6.3703, interval_samples_per_second: 2.512, interval_steps_per_second: 1.57, epoch: 12.3596[0m
[32m[2022-09-16 13:46:16,522] [    INFO][0m - loss: 0.04467379, learning_rate: 1.7528089887640449e-06, global_step: 1110, interval_runtime: 6.3441, interval_samples_per_second: 2.522, interval_steps_per_second: 1.576, epoch: 12.4719[0m
[32m[2022-09-16 13:46:23,021] [    INFO][0m - loss: 0.11226665, learning_rate: 1.741573033707865e-06, global_step: 1120, interval_runtime: 6.3296, interval_samples_per_second: 2.528, interval_steps_per_second: 1.58, epoch: 12.5843[0m
[32m[2022-09-16 13:46:32,564] [    INFO][0m - loss: 0.07003162, learning_rate: 1.7303370786516853e-06, global_step: 1130, interval_runtime: 6.5094, interval_samples_per_second: 2.458, interval_steps_per_second: 1.536, epoch: 12.6966[0m
[32m[2022-09-16 13:46:38,922] [    INFO][0m - loss: 0.10995952, learning_rate: 1.7191011235955056e-06, global_step: 1140, interval_runtime: 9.5609, interval_samples_per_second: 1.673, interval_steps_per_second: 1.046, epoch: 12.809[0m
[32m[2022-09-16 13:46:45,250] [    INFO][0m - loss: 0.01335581, learning_rate: 1.7078651685393256e-06, global_step: 1150, interval_runtime: 6.3289, interval_samples_per_second: 2.528, interval_steps_per_second: 1.58, epoch: 12.9213[0m
[32m[2022-09-16 13:46:49,210] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:46:49,210] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 13:46:49,210] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:46:49,210] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:46:49,210] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 13:47:10,750] [    INFO][0m - eval_loss: 0.8750590682029724, eval_accuracy: 0.8465346534653465, eval_runtime: 21.5394, eval_samples_per_second: 65.647, eval_steps_per_second: 4.132, epoch: 13.0[0m
[32m[2022-09-16 13:47:10,775] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-1157[0m
[32m[2022-09-16 13:47:10,775] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:47:13,512] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-1157/tokenizer_config.json[0m
[32m[2022-09-16 13:47:13,512] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-1157/special_tokens_map.json[0m
[32m[2022-09-16 13:47:19,748] [    INFO][0m - Deleting older checkpoint [checkpoints_chid/checkpoint-1068] due to args.save_total_limit[0m
[32m[2022-09-16 13:47:22,402] [    INFO][0m - loss: 0.1804038, learning_rate: 1.6966292134831463e-06, global_step: 1160, interval_runtime: 37.151, interval_samples_per_second: 0.431, interval_steps_per_second: 0.269, epoch: 13.0337[0m
[32m[2022-09-16 13:47:28,702] [    INFO][0m - loss: 0.05612761, learning_rate: 1.6853932584269665e-06, global_step: 1170, interval_runtime: 6.3007, interval_samples_per_second: 2.539, interval_steps_per_second: 1.587, epoch: 13.1461[0m
[32m[2022-09-16 13:47:35,012] [    INFO][0m - loss: 0.06087058, learning_rate: 1.6741573033707866e-06, global_step: 1180, interval_runtime: 6.3101, interval_samples_per_second: 2.536, interval_steps_per_second: 1.585, epoch: 13.2584[0m
[32m[2022-09-16 13:47:41,343] [    INFO][0m - loss: 0.06362985, learning_rate: 1.6629213483146068e-06, global_step: 1190, interval_runtime: 6.3306, interval_samples_per_second: 2.527, interval_steps_per_second: 1.58, epoch: 13.3708[0m
[32m[2022-09-16 13:47:47,718] [    INFO][0m - loss: 0.07996648, learning_rate: 1.651685393258427e-06, global_step: 1200, interval_runtime: 6.3753, interval_samples_per_second: 2.51, interval_steps_per_second: 1.569, epoch: 13.4831[0m
[32m[2022-09-16 13:47:54,082] [    INFO][0m - loss: 0.03554128, learning_rate: 1.6404494382022473e-06, global_step: 1210, interval_runtime: 6.3642, interval_samples_per_second: 2.514, interval_steps_per_second: 1.571, epoch: 13.5955[0m
[32m[2022-09-16 13:48:00,422] [    INFO][0m - loss: 0.01634746, learning_rate: 1.6292134831460673e-06, global_step: 1220, interval_runtime: 6.3397, interval_samples_per_second: 2.524, interval_steps_per_second: 1.577, epoch: 13.7079[0m
[32m[2022-09-16 13:48:06,762] [    INFO][0m - loss: 0.18733921, learning_rate: 1.6179775280898876e-06, global_step: 1230, interval_runtime: 6.3403, interval_samples_per_second: 2.524, interval_steps_per_second: 1.577, epoch: 13.8202[0m
[32m[2022-09-16 13:48:13,098] [    INFO][0m - loss: 0.11252053, learning_rate: 1.6067415730337078e-06, global_step: 1240, interval_runtime: 6.3354, interval_samples_per_second: 2.525, interval_steps_per_second: 1.578, epoch: 13.9326[0m
[32m[2022-09-16 13:48:16,427] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:48:16,427] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 13:48:16,427] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:48:16,427] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:48:16,427] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 13:48:38,042] [    INFO][0m - eval_loss: 0.9351561665534973, eval_accuracy: 0.8437057991513437, eval_runtime: 21.6143, eval_samples_per_second: 65.42, eval_steps_per_second: 4.118, epoch: 14.0[0m
[32m[2022-09-16 13:48:38,068] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-1246[0m
[32m[2022-09-16 13:48:38,068] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:48:40,909] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-1246/tokenizer_config.json[0m
[32m[2022-09-16 13:48:40,910] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-1246/special_tokens_map.json[0m
[32m[2022-09-16 13:48:46,202] [    INFO][0m - Deleting older checkpoint [checkpoints_chid/checkpoint-1157] due to args.save_total_limit[0m
[32m[2022-09-16 13:48:49,418] [    INFO][0m - loss: 0.04767996, learning_rate: 1.595505617977528e-06, global_step: 1250, interval_runtime: 36.3198, interval_samples_per_second: 0.441, interval_steps_per_second: 0.275, epoch: 14.0449[0m
[32m[2022-09-16 13:48:55,856] [    INFO][0m - loss: 0.08246185, learning_rate: 1.5842696629213483e-06, global_step: 1260, interval_runtime: 6.4386, interval_samples_per_second: 2.485, interval_steps_per_second: 1.553, epoch: 14.1573[0m
[32m[2022-09-16 13:49:02,255] [    INFO][0m - loss: 0.0629491, learning_rate: 1.5730337078651683e-06, global_step: 1270, interval_runtime: 6.3984, interval_samples_per_second: 2.501, interval_steps_per_second: 1.563, epoch: 14.2697[0m
[32m[2022-09-16 13:49:08,593] [    INFO][0m - loss: 0.11130815, learning_rate: 1.561797752808989e-06, global_step: 1280, interval_runtime: 6.3376, interval_samples_per_second: 2.525, interval_steps_per_second: 1.578, epoch: 14.382[0m
[32m[2022-09-16 13:49:14,936] [    INFO][0m - loss: 0.02004647, learning_rate: 1.5505617977528093e-06, global_step: 1290, interval_runtime: 6.343, interval_samples_per_second: 2.522, interval_steps_per_second: 1.577, epoch: 14.4944[0m
[32m[2022-09-16 13:49:21,276] [    INFO][0m - loss: 0.17652192, learning_rate: 1.5393258426966293e-06, global_step: 1300, interval_runtime: 6.3407, interval_samples_per_second: 2.523, interval_steps_per_second: 1.577, epoch: 14.6067[0m
[32m[2022-09-16 13:49:27,617] [    INFO][0m - loss: 0.10295279, learning_rate: 1.5280898876404495e-06, global_step: 1310, interval_runtime: 6.341, interval_samples_per_second: 2.523, interval_steps_per_second: 1.577, epoch: 14.7191[0m
[32m[2022-09-16 13:49:33,969] [    INFO][0m - loss: 0.22338667, learning_rate: 1.5168539325842698e-06, global_step: 1320, interval_runtime: 6.3521, interval_samples_per_second: 2.519, interval_steps_per_second: 1.574, epoch: 14.8315[0m
[32m[2022-09-16 13:49:40,295] [    INFO][0m - loss: 0.07595308, learning_rate: 1.50561797752809e-06, global_step: 1330, interval_runtime: 6.3262, interval_samples_per_second: 2.529, interval_steps_per_second: 1.581, epoch: 14.9438[0m
[32m[2022-09-16 13:49:43,001] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:49:43,002] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 13:49:43,002] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:49:43,002] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:49:43,002] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 13:50:04,554] [    INFO][0m - eval_loss: 0.937346875667572, eval_accuracy: 0.8451202263083452, eval_runtime: 21.552, eval_samples_per_second: 65.609, eval_steps_per_second: 4.13, epoch: 15.0[0m
[32m[2022-09-16 13:50:04,578] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-1335[0m
[32m[2022-09-16 13:50:04,578] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:50:07,304] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-1335/tokenizer_config.json[0m
[32m[2022-09-16 13:50:07,305] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-1335/special_tokens_map.json[0m
[32m[2022-09-16 13:50:12,431] [    INFO][0m - Deleting older checkpoint [checkpoints_chid/checkpoint-1246] due to args.save_total_limit[0m
[32m[2022-09-16 13:50:16,385] [    INFO][0m - loss: 0.06044568, learning_rate: 1.49438202247191e-06, global_step: 1340, interval_runtime: 36.0895, interval_samples_per_second: 0.443, interval_steps_per_second: 0.277, epoch: 15.0562[0m
[32m[2022-09-16 13:50:22,705] [    INFO][0m - loss: 0.07077494, learning_rate: 1.4831460674157303e-06, global_step: 1350, interval_runtime: 6.3204, interval_samples_per_second: 2.531, interval_steps_per_second: 1.582, epoch: 15.1685[0m
[32m[2022-09-16 13:50:29,021] [    INFO][0m - loss: 0.00811657, learning_rate: 1.4719101123595505e-06, global_step: 1360, interval_runtime: 6.3159, interval_samples_per_second: 2.533, interval_steps_per_second: 1.583, epoch: 15.2809[0m
[32m[2022-09-16 13:50:35,346] [    INFO][0m - loss: 0.08691444, learning_rate: 1.460674157303371e-06, global_step: 1370, interval_runtime: 6.3245, interval_samples_per_second: 2.53, interval_steps_per_second: 1.581, epoch: 15.3933[0m
[32m[2022-09-16 13:50:41,685] [    INFO][0m - loss: 0.04993012, learning_rate: 1.449438202247191e-06, global_step: 1380, interval_runtime: 6.3391, interval_samples_per_second: 2.524, interval_steps_per_second: 1.578, epoch: 15.5056[0m
[32m[2022-09-16 13:50:48,016] [    INFO][0m - loss: 0.03012838, learning_rate: 1.4382022471910113e-06, global_step: 1390, interval_runtime: 6.331, interval_samples_per_second: 2.527, interval_steps_per_second: 1.58, epoch: 15.618[0m
[32m[2022-09-16 13:50:54,347] [    INFO][0m - loss: 0.0658336, learning_rate: 1.4269662921348315e-06, global_step: 1400, interval_runtime: 6.3314, interval_samples_per_second: 2.527, interval_steps_per_second: 1.579, epoch: 15.7303[0m
[32m[2022-09-16 13:51:00,669] [    INFO][0m - loss: 0.07510284, learning_rate: 1.4157303370786518e-06, global_step: 1410, interval_runtime: 6.3218, interval_samples_per_second: 2.531, interval_steps_per_second: 1.582, epoch: 15.8427[0m
[32m[2022-09-16 13:51:06,988] [    INFO][0m - loss: 0.01949858, learning_rate: 1.4044943820224718e-06, global_step: 1420, interval_runtime: 6.3188, interval_samples_per_second: 2.532, interval_steps_per_second: 1.583, epoch: 15.9551[0m
[32m[2022-09-16 13:51:09,078] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:51:09,078] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 13:51:09,078] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:51:09,078] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:51:09,078] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 13:51:30,673] [    INFO][0m - eval_loss: 1.0952190160751343, eval_accuracy: 0.8380480905233381, eval_runtime: 21.5934, eval_samples_per_second: 65.483, eval_steps_per_second: 4.122, epoch: 16.0[0m
[32m[2022-09-16 13:51:30,697] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-1424[0m
[32m[2022-09-16 13:51:30,697] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:51:33,383] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-1424/tokenizer_config.json[0m
[32m[2022-09-16 13:51:33,384] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-1424/special_tokens_map.json[0m
[32m[2022-09-16 13:51:38,377] [    INFO][0m - Deleting older checkpoint [checkpoints_chid/checkpoint-1335] due to args.save_total_limit[0m
[32m[2022-09-16 13:51:42,873] [    INFO][0m - loss: 0.0426484, learning_rate: 1.3932584269662923e-06, global_step: 1430, interval_runtime: 35.8848, interval_samples_per_second: 0.446, interval_steps_per_second: 0.279, epoch: 16.0674[0m
[32m[2022-09-16 13:51:49,184] [    INFO][0m - loss: 0.01449773, learning_rate: 1.3820224719101125e-06, global_step: 1440, interval_runtime: 6.3111, interval_samples_per_second: 2.535, interval_steps_per_second: 1.585, epoch: 16.1798[0m
[32m[2022-09-16 13:51:55,537] [    INFO][0m - loss: 0.03156789, learning_rate: 1.3707865168539327e-06, global_step: 1450, interval_runtime: 6.3525, interval_samples_per_second: 2.519, interval_steps_per_second: 1.574, epoch: 16.2921[0m
[32m[2022-09-16 13:52:06,996] [    INFO][0m - loss: 0.15014, learning_rate: 1.3595505617977528e-06, global_step: 1460, interval_runtime: 6.3566, interval_samples_per_second: 2.517, interval_steps_per_second: 1.573, epoch: 16.4045[0m
[32m[2022-09-16 13:52:13,319] [    INFO][0m - loss: 0.09437091, learning_rate: 1.348314606741573e-06, global_step: 1470, interval_runtime: 11.4256, interval_samples_per_second: 1.4, interval_steps_per_second: 0.875, epoch: 16.5169[0m
[32m[2022-09-16 13:52:19,674] [    INFO][0m - loss: 0.15316734, learning_rate: 1.3370786516853933e-06, global_step: 1480, interval_runtime: 6.3546, interval_samples_per_second: 2.518, interval_steps_per_second: 1.574, epoch: 16.6292[0m
[32m[2022-09-16 13:52:26,016] [    INFO][0m - loss: 0.12781157, learning_rate: 1.3258426966292135e-06, global_step: 1490, interval_runtime: 6.343, interval_samples_per_second: 2.522, interval_steps_per_second: 1.577, epoch: 16.7416[0m
[32m[2022-09-16 13:52:32,363] [    INFO][0m - loss: 0.051605, learning_rate: 1.3146067415730338e-06, global_step: 1500, interval_runtime: 6.347, interval_samples_per_second: 2.521, interval_steps_per_second: 1.576, epoch: 16.8539[0m
[32m[2022-09-16 13:52:38,652] [    INFO][0m - loss: 0.02764443, learning_rate: 1.303370786516854e-06, global_step: 1510, interval_runtime: 6.2887, interval_samples_per_second: 2.544, interval_steps_per_second: 1.59, epoch: 16.9663[0m
[32m[2022-09-16 13:52:40,131] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:52:40,132] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 13:52:40,132] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:52:40,132] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:52:40,132] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 13:53:01,685] [    INFO][0m - eval_loss: 1.1130146980285645, eval_accuracy: 0.842998585572843, eval_runtime: 21.5523, eval_samples_per_second: 65.608, eval_steps_per_second: 4.129, epoch: 17.0[0m
[32m[2022-09-16 13:53:01,710] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-1513[0m
[32m[2022-09-16 13:53:01,710] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:53:04,484] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-1513/tokenizer_config.json[0m
[32m[2022-09-16 13:53:04,485] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-1513/special_tokens_map.json[0m
[32m[2022-09-16 13:53:09,763] [    INFO][0m - Deleting older checkpoint [checkpoints_chid/checkpoint-1424] due to args.save_total_limit[0m
[32m[2022-09-16 13:53:14,874] [    INFO][0m - loss: 0.06565458, learning_rate: 1.2921348314606742e-06, global_step: 1520, interval_runtime: 36.2215, interval_samples_per_second: 0.442, interval_steps_per_second: 0.276, epoch: 17.0787[0m
[32m[2022-09-16 13:53:21,188] [    INFO][0m - loss: 0.03222233, learning_rate: 1.2808988764044945e-06, global_step: 1530, interval_runtime: 6.3147, interval_samples_per_second: 2.534, interval_steps_per_second: 1.584, epoch: 17.191[0m
[32m[2022-09-16 13:53:27,514] [    INFO][0m - loss: 0.10624499, learning_rate: 1.2696629213483145e-06, global_step: 1540, interval_runtime: 6.3263, interval_samples_per_second: 2.529, interval_steps_per_second: 1.581, epoch: 17.3034[0m
[32m[2022-09-16 13:53:33,839] [    INFO][0m - loss: 0.0086291, learning_rate: 1.258426966292135e-06, global_step: 1550, interval_runtime: 6.3244, interval_samples_per_second: 2.53, interval_steps_per_second: 1.581, epoch: 17.4157[0m
[32m[2022-09-16 13:53:40,199] [    INFO][0m - loss: 0.00889696, learning_rate: 1.2471910112359552e-06, global_step: 1560, interval_runtime: 6.3599, interval_samples_per_second: 2.516, interval_steps_per_second: 1.572, epoch: 17.5281[0m
[32m[2022-09-16 13:53:46,579] [    INFO][0m - loss: 0.15635232, learning_rate: 1.2359550561797752e-06, global_step: 1570, interval_runtime: 6.3794, interval_samples_per_second: 2.508, interval_steps_per_second: 1.568, epoch: 17.6404[0m
[32m[2022-09-16 13:53:52,940] [    INFO][0m - loss: 0.02947827, learning_rate: 1.2247191011235955e-06, global_step: 1580, interval_runtime: 6.3611, interval_samples_per_second: 2.515, interval_steps_per_second: 1.572, epoch: 17.7528[0m
[32m[2022-09-16 13:53:59,284] [    INFO][0m - loss: 0.10388843, learning_rate: 1.2134831460674157e-06, global_step: 1590, interval_runtime: 6.3449, interval_samples_per_second: 2.522, interval_steps_per_second: 1.576, epoch: 17.8652[0m
[32m[2022-09-16 13:54:05,575] [    INFO][0m - loss: 0.09657365, learning_rate: 1.202247191011236e-06, global_step: 1600, interval_runtime: 6.2902, interval_samples_per_second: 2.544, interval_steps_per_second: 1.59, epoch: 17.9775[0m
[32m[2022-09-16 13:54:06,439] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:54:06,439] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 13:54:06,439] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:54:06,439] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:54:06,440] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 13:54:28,025] [    INFO][0m - eval_loss: 1.0827605724334717, eval_accuracy: 0.8521923620933521, eval_runtime: 21.585, eval_samples_per_second: 65.508, eval_steps_per_second: 4.123, epoch: 18.0[0m
[32m[2022-09-16 13:54:28,049] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-1602[0m
[32m[2022-09-16 13:54:28,049] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:54:30,730] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-1602/tokenizer_config.json[0m
[32m[2022-09-16 13:54:30,730] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-1602/special_tokens_map.json[0m
[32m[2022-09-16 13:54:37,143] [    INFO][0m - Deleting older checkpoint [checkpoints_chid/checkpoint-1513] due to args.save_total_limit[0m
[32m[2022-09-16 13:54:43,018] [    INFO][0m - loss: 0.13161888, learning_rate: 1.1910112359550562e-06, global_step: 1610, interval_runtime: 37.443, interval_samples_per_second: 0.427, interval_steps_per_second: 0.267, epoch: 18.0899[0m
[32m[2022-09-16 13:54:49,360] [    INFO][0m - loss: 0.0644594, learning_rate: 1.1797752808988765e-06, global_step: 1620, interval_runtime: 6.3421, interval_samples_per_second: 2.523, interval_steps_per_second: 1.577, epoch: 18.2022[0m
[32m[2022-09-16 13:54:55,707] [    INFO][0m - loss: 0.00913814, learning_rate: 1.1685393258426967e-06, global_step: 1630, interval_runtime: 6.3471, interval_samples_per_second: 2.521, interval_steps_per_second: 1.576, epoch: 18.3146[0m
[32m[2022-09-16 13:55:02,074] [    INFO][0m - loss: 0.08171588, learning_rate: 1.157303370786517e-06, global_step: 1640, interval_runtime: 6.3671, interval_samples_per_second: 2.513, interval_steps_per_second: 1.571, epoch: 18.427[0m
[32m[2022-09-16 13:55:08,432] [    INFO][0m - loss: 0.02130553, learning_rate: 1.146067415730337e-06, global_step: 1650, interval_runtime: 6.3582, interval_samples_per_second: 2.516, interval_steps_per_second: 1.573, epoch: 18.5393[0m
[32m[2022-09-16 13:55:14,800] [    INFO][0m - loss: 0.04452227, learning_rate: 1.1348314606741572e-06, global_step: 1660, interval_runtime: 6.3675, interval_samples_per_second: 2.513, interval_steps_per_second: 1.57, epoch: 18.6517[0m
[32m[2022-09-16 13:55:21,146] [    INFO][0m - loss: 0.04681694, learning_rate: 1.1235955056179777e-06, global_step: 1670, interval_runtime: 6.3468, interval_samples_per_second: 2.521, interval_steps_per_second: 1.576, epoch: 18.764[0m
[32m[2022-09-16 13:55:27,502] [    INFO][0m - loss: 0.03001204, learning_rate: 1.112359550561798e-06, global_step: 1680, interval_runtime: 6.3561, interval_samples_per_second: 2.517, interval_steps_per_second: 1.573, epoch: 18.8764[0m
[32m[2022-09-16 13:55:33,753] [    INFO][0m - loss: 0.07661771, learning_rate: 1.101123595505618e-06, global_step: 1690, interval_runtime: 6.25, interval_samples_per_second: 2.56, interval_steps_per_second: 1.6, epoch: 18.9888[0m
[32m[2022-09-16 13:55:34,005] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:55:34,005] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 13:55:34,005] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:55:34,006] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:55:34,006] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 13:55:55,598] [    INFO][0m - eval_loss: 1.1590471267700195, eval_accuracy: 0.8486562942008486, eval_runtime: 21.5925, eval_samples_per_second: 65.486, eval_steps_per_second: 4.122, epoch: 19.0[0m
[32m[2022-09-16 13:55:55,624] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-1691[0m
[32m[2022-09-16 13:55:55,624] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:55:58,639] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-1691/tokenizer_config.json[0m
[32m[2022-09-16 13:55:58,640] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-1691/special_tokens_map.json[0m
[32m[2022-09-16 13:56:04,721] [    INFO][0m - Deleting older checkpoint [checkpoints_chid/checkpoint-1602] due to args.save_total_limit[0m
[32m[2022-09-16 13:56:11,212] [    INFO][0m - loss: 0.02429106, learning_rate: 1.0898876404494382e-06, global_step: 1700, interval_runtime: 37.4596, interval_samples_per_second: 0.427, interval_steps_per_second: 0.267, epoch: 19.1011[0m
[32m[2022-09-16 13:56:17,560] [    INFO][0m - loss: 0.13422229, learning_rate: 1.0786516853932585e-06, global_step: 1710, interval_runtime: 6.3475, interval_samples_per_second: 2.521, interval_steps_per_second: 1.575, epoch: 19.2135[0m
[32m[2022-09-16 13:56:23,919] [    INFO][0m - loss: 0.05242946, learning_rate: 1.0674157303370787e-06, global_step: 1720, interval_runtime: 6.3593, interval_samples_per_second: 2.516, interval_steps_per_second: 1.572, epoch: 19.3258[0m
[32m[2022-09-16 13:56:30,251] [    INFO][0m - loss: 0.1114085, learning_rate: 1.056179775280899e-06, global_step: 1730, interval_runtime: 6.3316, interval_samples_per_second: 2.527, interval_steps_per_second: 1.579, epoch: 19.4382[0m
[32m[2022-09-16 13:56:36,594] [    INFO][0m - loss: 0.03813144, learning_rate: 1.0449438202247192e-06, global_step: 1740, interval_runtime: 6.343, interval_samples_per_second: 2.522, interval_steps_per_second: 1.577, epoch: 19.5506[0m
[32m[2022-09-16 13:56:42,924] [    INFO][0m - loss: 0.13415476, learning_rate: 1.0337078651685394e-06, global_step: 1750, interval_runtime: 6.3304, interval_samples_per_second: 2.527, interval_steps_per_second: 1.58, epoch: 19.6629[0m
[32m[2022-09-16 13:56:49,275] [    INFO][0m - loss: 0.05792876, learning_rate: 1.0224719101123597e-06, global_step: 1760, interval_runtime: 6.3512, interval_samples_per_second: 2.519, interval_steps_per_second: 1.575, epoch: 19.7753[0m
[32m[2022-09-16 13:56:55,650] [    INFO][0m - loss: 0.0649428, learning_rate: 1.0112359550561797e-06, global_step: 1770, interval_runtime: 6.3736, interval_samples_per_second: 2.51, interval_steps_per_second: 1.569, epoch: 19.8876[0m
[32m[2022-09-16 13:57:01,556] [    INFO][0m - loss: 0.05137548, learning_rate: 1e-06, global_step: 1780, interval_runtime: 5.9075, interval_samples_per_second: 2.708, interval_steps_per_second: 1.693, epoch: 20.0[0m
[32m[2022-09-16 13:57:01,557] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:57:01,558] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 13:57:01,558] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:57:01,558] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:57:01,558] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 13:57:23,269] [    INFO][0m - eval_loss: 1.1379737854003906, eval_accuracy: 0.8472418670438473, eval_runtime: 21.7101, eval_samples_per_second: 65.131, eval_steps_per_second: 4.099, epoch: 20.0[0m
[32m[2022-09-16 13:57:23,296] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-1780[0m
[32m[2022-09-16 13:57:23,297] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:57:26,243] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-1780/tokenizer_config.json[0m
[32m[2022-09-16 13:57:26,243] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-1780/special_tokens_map.json[0m
[32m[2022-09-16 13:57:31,265] [    INFO][0m - Deleting older checkpoint [checkpoints_chid/checkpoint-1691] due to args.save_total_limit[0m
[32m[2022-09-16 13:57:38,308] [    INFO][0m - loss: 0.02642883, learning_rate: 9.887640449438202e-07, global_step: 1790, interval_runtime: 36.7516, interval_samples_per_second: 0.435, interval_steps_per_second: 0.272, epoch: 20.1124[0m
[32m[2022-09-16 13:57:44,646] [    INFO][0m - loss: 0.12159429, learning_rate: 9.775280898876404e-07, global_step: 1800, interval_runtime: 6.3385, interval_samples_per_second: 2.524, interval_steps_per_second: 1.578, epoch: 20.2247[0m
[32m[2022-09-16 13:57:51,006] [    INFO][0m - loss: 0.03391125, learning_rate: 9.662921348314607e-07, global_step: 1810, interval_runtime: 6.3591, interval_samples_per_second: 2.516, interval_steps_per_second: 1.573, epoch: 20.3371[0m
[32m[2022-09-16 13:57:57,342] [    INFO][0m - loss: 0.04463273, learning_rate: 9.55056179775281e-07, global_step: 1820, interval_runtime: 6.3367, interval_samples_per_second: 2.525, interval_steps_per_second: 1.578, epoch: 20.4494[0m
[32m[2022-09-16 13:58:03,679] [    INFO][0m - loss: 0.02083261, learning_rate: 9.438202247191011e-07, global_step: 1830, interval_runtime: 6.3369, interval_samples_per_second: 2.525, interval_steps_per_second: 1.578, epoch: 20.5618[0m
[32m[2022-09-16 13:58:10,060] [    INFO][0m - loss: 0.00158042, learning_rate: 9.325842696629213e-07, global_step: 1840, interval_runtime: 6.3809, interval_samples_per_second: 2.507, interval_steps_per_second: 1.567, epoch: 20.6742[0m
[32m[2022-09-16 13:58:16,392] [    INFO][0m - loss: 0.09912716, learning_rate: 9.213483146067416e-07, global_step: 1850, interval_runtime: 6.3318, interval_samples_per_second: 2.527, interval_steps_per_second: 1.579, epoch: 20.7865[0m
[32m[2022-09-16 13:58:22,727] [    INFO][0m - loss: 0.1053978, learning_rate: 9.101123595505619e-07, global_step: 1860, interval_runtime: 6.3351, interval_samples_per_second: 2.526, interval_steps_per_second: 1.579, epoch: 20.8989[0m
[32m[2022-09-16 13:58:27,951] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:58:27,952] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 13:58:27,952] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:58:27,952] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:58:27,952] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 13:58:49,520] [    INFO][0m - eval_loss: 1.1220792531967163, eval_accuracy: 0.8472418670438473, eval_runtime: 21.5673, eval_samples_per_second: 65.562, eval_steps_per_second: 4.127, epoch: 21.0[0m
[32m[2022-09-16 13:58:49,544] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-1869[0m
[32m[2022-09-16 13:58:49,544] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:58:52,330] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-1869/tokenizer_config.json[0m
[32m[2022-09-16 13:58:52,330] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-1869/special_tokens_map.json[0m
[32m[2022-09-16 13:58:57,470] [    INFO][0m - Deleting older checkpoint [checkpoints_chid/checkpoint-1780] due to args.save_total_limit[0m
[32m[2022-09-16 13:58:58,814] [    INFO][0m - loss: 0.07873337, learning_rate: 8.98876404494382e-07, global_step: 1870, interval_runtime: 36.0872, interval_samples_per_second: 0.443, interval_steps_per_second: 0.277, epoch: 21.0112[0m
[32m[2022-09-16 13:59:05,120] [    INFO][0m - loss: 0.04975019, learning_rate: 8.876404494382023e-07, global_step: 1880, interval_runtime: 6.3054, interval_samples_per_second: 2.538, interval_steps_per_second: 1.586, epoch: 21.1236[0m
[32m[2022-09-16 13:59:11,424] [    INFO][0m - loss: 0.08075722, learning_rate: 8.764044943820224e-07, global_step: 1890, interval_runtime: 6.3044, interval_samples_per_second: 2.538, interval_steps_per_second: 1.586, epoch: 21.236[0m
[32m[2022-09-16 13:59:17,737] [    INFO][0m - loss: 0.06722451, learning_rate: 8.651685393258427e-07, global_step: 1900, interval_runtime: 6.313, interval_samples_per_second: 2.534, interval_steps_per_second: 1.584, epoch: 21.3483[0m
[32m[2022-09-16 13:59:24,058] [    INFO][0m - loss: 0.07166091, learning_rate: 8.539325842696628e-07, global_step: 1910, interval_runtime: 6.321, interval_samples_per_second: 2.531, interval_steps_per_second: 1.582, epoch: 21.4607[0m
[32m[2022-09-16 13:59:30,381] [    INFO][0m - loss: 0.01152123, learning_rate: 8.426966292134833e-07, global_step: 1920, interval_runtime: 6.3228, interval_samples_per_second: 2.531, interval_steps_per_second: 1.582, epoch: 21.573[0m
[32m[2022-09-16 13:59:36,709] [    INFO][0m - loss: 0.01332192, learning_rate: 8.314606741573034e-07, global_step: 1930, interval_runtime: 6.328, interval_samples_per_second: 2.528, interval_steps_per_second: 1.58, epoch: 21.6854[0m
[32m[2022-09-16 13:59:43,046] [    INFO][0m - loss: 0.06773939, learning_rate: 8.202247191011237e-07, global_step: 1940, interval_runtime: 6.3371, interval_samples_per_second: 2.525, interval_steps_per_second: 1.578, epoch: 21.7978[0m
[32m[2022-09-16 13:59:49,395] [    INFO][0m - loss: 0.0241347, learning_rate: 8.089887640449438e-07, global_step: 1950, interval_runtime: 6.349, interval_samples_per_second: 2.52, interval_steps_per_second: 1.575, epoch: 21.9101[0m
[32m[2022-09-16 13:59:53,987] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:59:53,987] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 13:59:53,987] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:59:53,988] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:59:53,988] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 14:00:15,598] [    INFO][0m - eval_loss: 1.1698763370513916, eval_accuracy: 0.842998585572843, eval_runtime: 21.6097, eval_samples_per_second: 65.433, eval_steps_per_second: 4.119, epoch: 22.0[0m
[32m[2022-09-16 14:00:15,622] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-1958[0m
[32m[2022-09-16 14:00:15,622] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 14:00:18,443] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-1958/tokenizer_config.json[0m
[32m[2022-09-16 14:00:18,443] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-1958/special_tokens_map.json[0m
[32m[2022-09-16 14:00:23,880] [    INFO][0m - Deleting older checkpoint [checkpoints_chid/checkpoint-1869] due to args.save_total_limit[0m
[32m[2022-09-16 14:00:25,941] [    INFO][0m - loss: 0.00300377, learning_rate: 7.97752808988764e-07, global_step: 1960, interval_runtime: 36.5461, interval_samples_per_second: 0.438, interval_steps_per_second: 0.274, epoch: 22.0225[0m
[32m[2022-09-16 14:00:32,285] [    INFO][0m - loss: 0.05083324, learning_rate: 7.865168539325842e-07, global_step: 1970, interval_runtime: 6.3438, interval_samples_per_second: 2.522, interval_steps_per_second: 1.576, epoch: 22.1348[0m
[32m[2022-09-16 14:00:38,627] [    INFO][0m - loss: 0.09335277, learning_rate: 7.752808988764046e-07, global_step: 1980, interval_runtime: 6.3417, interval_samples_per_second: 2.523, interval_steps_per_second: 1.577, epoch: 22.2472[0m
[32m[2022-09-16 14:00:44,958] [    INFO][0m - loss: 0.00034076, learning_rate: 7.640449438202248e-07, global_step: 1990, interval_runtime: 6.3319, interval_samples_per_second: 2.527, interval_steps_per_second: 1.579, epoch: 22.3596[0m
[32m[2022-09-16 14:00:51,292] [    INFO][0m - loss: 0.08297592, learning_rate: 7.52808988764045e-07, global_step: 2000, interval_runtime: 6.3336, interval_samples_per_second: 2.526, interval_steps_per_second: 1.579, epoch: 22.4719[0m
[32m[2022-09-16 14:00:57,628] [    INFO][0m - loss: 0.01833211, learning_rate: 7.415730337078651e-07, global_step: 2010, interval_runtime: 6.3366, interval_samples_per_second: 2.525, interval_steps_per_second: 1.578, epoch: 22.5843[0m
[32m[2022-09-16 14:01:03,967] [    INFO][0m - loss: 0.03765998, learning_rate: 7.303370786516855e-07, global_step: 2020, interval_runtime: 6.3384, interval_samples_per_second: 2.524, interval_steps_per_second: 1.578, epoch: 22.6966[0m
[32m[2022-09-16 14:01:10,306] [    INFO][0m - loss: 0.01626294, learning_rate: 7.191011235955056e-07, global_step: 2030, interval_runtime: 6.3389, interval_samples_per_second: 2.524, interval_steps_per_second: 1.578, epoch: 22.809[0m
[32m[2022-09-16 14:01:16,633] [    INFO][0m - loss: 0.00072569, learning_rate: 7.078651685393259e-07, global_step: 2040, interval_runtime: 6.3277, interval_samples_per_second: 2.529, interval_steps_per_second: 1.58, epoch: 22.9213[0m
[32m[2022-09-16 14:01:20,595] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 14:01:20,596] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 14:01:20,596] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 14:01:20,596] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 14:01:20,596] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 14:01:42,149] [    INFO][0m - eval_loss: 1.2147935628890991, eval_accuracy: 0.8408769448373409, eval_runtime: 21.5531, eval_samples_per_second: 65.606, eval_steps_per_second: 4.129, epoch: 23.0[0m
[32m[2022-09-16 14:01:42,176] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-2047[0m
[32m[2022-09-16 14:01:42,176] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 14:01:44,866] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-2047/tokenizer_config.json[0m
[32m[2022-09-16 14:01:44,866] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-2047/special_tokens_map.json[0m
[32m[2022-09-16 14:01:49,871] [    INFO][0m - Deleting older checkpoint [checkpoints_chid/checkpoint-1958] due to args.save_total_limit[0m
[32m[2022-09-16 14:01:52,493] [    INFO][0m - loss: 0.04868528, learning_rate: 6.966292134831461e-07, global_step: 2050, interval_runtime: 35.8595, interval_samples_per_second: 0.446, interval_steps_per_second: 0.279, epoch: 23.0337[0m
[32m[2022-09-16 14:01:58,798] [    INFO][0m - loss: 0.08117326, learning_rate: 6.853932584269664e-07, global_step: 2060, interval_runtime: 6.3054, interval_samples_per_second: 2.537, interval_steps_per_second: 1.586, epoch: 23.1461[0m
[32m[2022-09-16 14:02:07,816] [    INFO][0m - loss: 0.04231314, learning_rate: 6.741573033707865e-07, global_step: 2070, interval_runtime: 6.319, interval_samples_per_second: 2.532, interval_steps_per_second: 1.583, epoch: 23.2584[0m
[32m[2022-09-16 14:02:14,153] [    INFO][0m - loss: 0.04034556, learning_rate: 6.629213483146068e-07, global_step: 2080, interval_runtime: 9.0351, interval_samples_per_second: 1.771, interval_steps_per_second: 1.107, epoch: 23.3708[0m
[32m[2022-09-16 14:02:20,521] [    INFO][0m - loss: 0.00119723, learning_rate: 6.51685393258427e-07, global_step: 2090, interval_runtime: 6.3679, interval_samples_per_second: 2.513, interval_steps_per_second: 1.57, epoch: 23.4831[0m
[32m[2022-09-16 14:02:26,904] [    INFO][0m - loss: 0.02294968, learning_rate: 6.404494382022472e-07, global_step: 2100, interval_runtime: 6.383, interval_samples_per_second: 2.507, interval_steps_per_second: 1.567, epoch: 23.5955[0m
[32m[2022-09-16 14:02:33,284] [    INFO][0m - loss: 0.03554804, learning_rate: 6.292134831460675e-07, global_step: 2110, interval_runtime: 6.38, interval_samples_per_second: 2.508, interval_steps_per_second: 1.567, epoch: 23.7079[0m
[32m[2022-09-16 14:02:39,647] [    INFO][0m - loss: 0.10063317, learning_rate: 6.179775280898876e-07, global_step: 2120, interval_runtime: 6.3634, interval_samples_per_second: 2.514, interval_steps_per_second: 1.571, epoch: 23.8202[0m
[32m[2022-09-16 14:02:45,992] [    INFO][0m - loss: 0.02187724, learning_rate: 6.067415730337079e-07, global_step: 2130, interval_runtime: 6.3446, interval_samples_per_second: 2.522, interval_steps_per_second: 1.576, epoch: 23.9326[0m
[32m[2022-09-16 14:02:49,324] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 14:02:49,324] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 14:02:49,324] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 14:02:49,324] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 14:02:49,325] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 14:03:10,918] [    INFO][0m - eval_loss: 1.2264387607574463, eval_accuracy: 0.8444130127298444, eval_runtime: 21.5928, eval_samples_per_second: 65.485, eval_steps_per_second: 4.122, epoch: 24.0[0m
[32m[2022-09-16 14:03:10,942] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-2136[0m
[32m[2022-09-16 14:03:10,942] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 14:03:14,130] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-2136/tokenizer_config.json[0m
[32m[2022-09-16 14:03:14,130] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-2136/special_tokens_map.json[0m
[32m[2022-09-16 14:03:20,230] [    INFO][0m - Deleting older checkpoint [checkpoints_chid/checkpoint-2047] due to args.save_total_limit[0m
[32m[2022-09-16 14:03:23,521] [    INFO][0m - loss: 0.00842848, learning_rate: 5.955056179775281e-07, global_step: 2140, interval_runtime: 37.529, interval_samples_per_second: 0.426, interval_steps_per_second: 0.266, epoch: 24.0449[0m
[32m[2022-09-16 14:03:29,865] [    INFO][0m - loss: 0.00080156, learning_rate: 5.842696629213484e-07, global_step: 2150, interval_runtime: 6.3439, interval_samples_per_second: 2.522, interval_steps_per_second: 1.576, epoch: 24.1573[0m
[32m[2022-09-16 14:03:36,209] [    INFO][0m - loss: 0.00017552, learning_rate: 5.730337078651685e-07, global_step: 2160, interval_runtime: 6.3443, interval_samples_per_second: 2.522, interval_steps_per_second: 1.576, epoch: 24.2697[0m
[32m[2022-09-16 14:03:42,563] [    INFO][0m - loss: 0.00024139, learning_rate: 5.617977528089888e-07, global_step: 2170, interval_runtime: 6.3543, interval_samples_per_second: 2.518, interval_steps_per_second: 1.574, epoch: 24.382[0m
[32m[2022-09-16 14:03:48,902] [    INFO][0m - loss: 0.09363168, learning_rate: 5.50561797752809e-07, global_step: 2180, interval_runtime: 6.3387, interval_samples_per_second: 2.524, interval_steps_per_second: 1.578, epoch: 24.4944[0m
[32m[2022-09-16 14:03:55,267] [    INFO][0m - loss: 0.00023572, learning_rate: 5.393258426966292e-07, global_step: 2190, interval_runtime: 6.3655, interval_samples_per_second: 2.514, interval_steps_per_second: 1.571, epoch: 24.6067[0m
[32m[2022-09-16 14:04:01,606] [    INFO][0m - loss: 0.05425238, learning_rate: 5.280898876404495e-07, global_step: 2200, interval_runtime: 6.3384, interval_samples_per_second: 2.524, interval_steps_per_second: 1.578, epoch: 24.7191[0m
[32m[2022-09-16 14:04:07,942] [    INFO][0m - loss: 0.02755461, learning_rate: 5.168539325842697e-07, global_step: 2210, interval_runtime: 6.3367, interval_samples_per_second: 2.525, interval_steps_per_second: 1.578, epoch: 24.8315[0m
[32m[2022-09-16 14:04:14,273] [    INFO][0m - loss: 0.05311094, learning_rate: 5.056179775280899e-07, global_step: 2220, interval_runtime: 6.3306, interval_samples_per_second: 2.527, interval_steps_per_second: 1.58, epoch: 24.9438[0m
[32m[2022-09-16 14:04:16,981] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 14:04:16,981] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 14:04:16,981] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 14:04:16,981] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 14:04:16,982] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 14:04:38,557] [    INFO][0m - eval_loss: 1.260414481163025, eval_accuracy: 0.8444130127298444, eval_runtime: 21.5749, eval_samples_per_second: 65.539, eval_steps_per_second: 4.125, epoch: 25.0[0m
[32m[2022-09-16 14:04:38,581] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-2225[0m
[32m[2022-09-16 14:04:38,581] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 14:04:41,929] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-2225/tokenizer_config.json[0m
[32m[2022-09-16 14:04:41,930] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-2225/special_tokens_map.json[0m
[32m[2022-09-16 14:04:47,791] [    INFO][0m - Deleting older checkpoint [checkpoints_chid/checkpoint-2136] due to args.save_total_limit[0m
[32m[2022-09-16 14:04:51,707] [    INFO][0m - loss: 0.03728079, learning_rate: 4.943820224719101e-07, global_step: 2230, interval_runtime: 37.434, interval_samples_per_second: 0.427, interval_steps_per_second: 0.267, epoch: 25.0562[0m
[32m[2022-09-16 14:04:58,128] [    INFO][0m - loss: 0.04984066, learning_rate: 4.831460674157303e-07, global_step: 2240, interval_runtime: 6.4207, interval_samples_per_second: 2.492, interval_steps_per_second: 1.557, epoch: 25.1685[0m
[32m[2022-09-16 14:05:04,452] [    INFO][0m - loss: 0.01434577, learning_rate: 4.7191011235955054e-07, global_step: 2250, interval_runtime: 6.3242, interval_samples_per_second: 2.53, interval_steps_per_second: 1.581, epoch: 25.2809[0m
[32m[2022-09-16 14:05:11,242] [    INFO][0m - loss: 0.08247626, learning_rate: 4.606741573033708e-07, global_step: 2260, interval_runtime: 6.3282, interval_samples_per_second: 2.528, interval_steps_per_second: 1.58, epoch: 25.3933[0m
[32m[2022-09-16 14:05:17,604] [    INFO][0m - loss: 0.00429938, learning_rate: 4.49438202247191e-07, global_step: 2270, interval_runtime: 6.8232, interval_samples_per_second: 2.345, interval_steps_per_second: 1.466, epoch: 25.5056[0m
[32m[2022-09-16 14:05:23,935] [    INFO][0m - loss: 0.000633, learning_rate: 4.382022471910112e-07, global_step: 2280, interval_runtime: 6.3314, interval_samples_per_second: 2.527, interval_steps_per_second: 1.579, epoch: 25.618[0m
[32m[2022-09-16 14:05:30,284] [    INFO][0m - loss: 0.00066547, learning_rate: 4.269662921348314e-07, global_step: 2290, interval_runtime: 6.3491, interval_samples_per_second: 2.52, interval_steps_per_second: 1.575, epoch: 25.7303[0m
[32m[2022-09-16 14:05:36,610] [    INFO][0m - loss: 0.08547978, learning_rate: 4.157303370786517e-07, global_step: 2300, interval_runtime: 6.3266, interval_samples_per_second: 2.529, interval_steps_per_second: 1.581, epoch: 25.8427[0m
[32m[2022-09-16 14:05:42,916] [    INFO][0m - loss: 0.05628092, learning_rate: 4.044943820224719e-07, global_step: 2310, interval_runtime: 6.3056, interval_samples_per_second: 2.537, interval_steps_per_second: 1.586, epoch: 25.9551[0m
[32m[2022-09-16 14:05:45,010] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 14:05:45,011] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 14:05:45,011] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 14:05:45,011] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 14:05:45,011] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 14:06:06,662] [    INFO][0m - eval_loss: 1.3009707927703857, eval_accuracy: 0.8401697312588402, eval_runtime: 21.6499, eval_samples_per_second: 65.312, eval_steps_per_second: 4.111, epoch: 26.0[0m
[32m[2022-09-16 14:06:06,686] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-2314[0m
[32m[2022-09-16 14:06:06,686] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 14:06:09,357] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-2314/tokenizer_config.json[0m
[32m[2022-09-16 14:06:09,357] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-2314/special_tokens_map.json[0m
[32m[2022-09-16 14:06:14,335] [    INFO][0m - Deleting older checkpoint [checkpoints_chid/checkpoint-2225] due to args.save_total_limit[0m
[32m[2022-09-16 14:06:18,827] [    INFO][0m - loss: 0.01572255, learning_rate: 3.932584269662921e-07, global_step: 2320, interval_runtime: 35.9114, interval_samples_per_second: 0.446, interval_steps_per_second: 0.278, epoch: 26.0674[0m
[32m[2022-09-16 14:06:25,134] [    INFO][0m - loss: 0.00089039, learning_rate: 3.820224719101124e-07, global_step: 2330, interval_runtime: 6.307, interval_samples_per_second: 2.537, interval_steps_per_second: 1.586, epoch: 26.1798[0m
[32m[2022-09-16 14:06:31,469] [    INFO][0m - loss: 0.00047916, learning_rate: 3.707865168539326e-07, global_step: 2340, interval_runtime: 6.3343, interval_samples_per_second: 2.526, interval_steps_per_second: 1.579, epoch: 26.2921[0m
[32m[2022-09-16 14:06:37,817] [    INFO][0m - loss: 0.0002224, learning_rate: 3.595505617977528e-07, global_step: 2350, interval_runtime: 6.3479, interval_samples_per_second: 2.521, interval_steps_per_second: 1.575, epoch: 26.4045[0m
[32m[2022-09-16 14:06:44,170] [    INFO][0m - loss: 0.14637946, learning_rate: 3.4831460674157306e-07, global_step: 2360, interval_runtime: 6.3529, interval_samples_per_second: 2.519, interval_steps_per_second: 1.574, epoch: 26.5169[0m
[32m[2022-09-16 14:06:50,512] [    INFO][0m - loss: 0.01603638, learning_rate: 3.3707865168539325e-07, global_step: 2370, interval_runtime: 6.3425, interval_samples_per_second: 2.523, interval_steps_per_second: 1.577, epoch: 26.6292[0m
[32m[2022-09-16 14:06:56,874] [    INFO][0m - loss: 0.05695817, learning_rate: 3.258426966292135e-07, global_step: 2380, interval_runtime: 6.3618, interval_samples_per_second: 2.515, interval_steps_per_second: 1.572, epoch: 26.7416[0m
[32m[2022-09-16 14:07:03,253] [    INFO][0m - loss: 0.10160155, learning_rate: 3.1460674157303374e-07, global_step: 2390, interval_runtime: 6.3788, interval_samples_per_second: 2.508, interval_steps_per_second: 1.568, epoch: 26.8539[0m
[32m[2022-09-16 14:07:09,584] [    INFO][0m - loss: 0.02124968, learning_rate: 3.0337078651685393e-07, global_step: 2400, interval_runtime: 6.3306, interval_samples_per_second: 2.527, interval_steps_per_second: 1.58, epoch: 26.9663[0m
[32m[2022-09-16 14:07:11,068] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 14:07:11,069] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 14:07:11,069] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 14:07:11,069] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 14:07:11,069] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 14:07:32,794] [    INFO][0m - eval_loss: 1.3119782209396362, eval_accuracy: 0.8408769448373409, eval_runtime: 21.7245, eval_samples_per_second: 65.088, eval_steps_per_second: 4.097, epoch: 27.0[0m
[32m[2022-09-16 14:07:32,818] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-2403[0m
[32m[2022-09-16 14:07:32,818] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 14:07:35,923] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-2403/tokenizer_config.json[0m
[32m[2022-09-16 14:07:35,924] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-2403/special_tokens_map.json[0m
[32m[2022-09-16 14:07:42,681] [    INFO][0m - Deleting older checkpoint [checkpoints_chid/checkpoint-2314] due to args.save_total_limit[0m
[32m[2022-09-16 14:07:47,847] [    INFO][0m - loss: 0.02701012, learning_rate: 2.921348314606742e-07, global_step: 2410, interval_runtime: 38.2638, interval_samples_per_second: 0.418, interval_steps_per_second: 0.261, epoch: 27.0787[0m
[32m[2022-09-16 14:07:54,282] [    INFO][0m - loss: 0.03733094, learning_rate: 2.808988764044944e-07, global_step: 2420, interval_runtime: 6.4349, interval_samples_per_second: 2.486, interval_steps_per_second: 1.554, epoch: 27.191[0m
[32m[2022-09-16 14:08:00,666] [    INFO][0m - loss: 0.00253223, learning_rate: 2.696629213483146e-07, global_step: 2430, interval_runtime: 6.3838, interval_samples_per_second: 2.506, interval_steps_per_second: 1.566, epoch: 27.3034[0m
[32m[2022-09-16 14:08:07,015] [    INFO][0m - loss: 0.00112737, learning_rate: 2.5842696629213486e-07, global_step: 2440, interval_runtime: 6.3486, interval_samples_per_second: 2.52, interval_steps_per_second: 1.575, epoch: 27.4157[0m
[32m[2022-09-16 14:08:14,043] [    INFO][0m - loss: 0.02416414, learning_rate: 2.4719101123595505e-07, global_step: 2450, interval_runtime: 6.3414, interval_samples_per_second: 2.523, interval_steps_per_second: 1.577, epoch: 27.5281[0m
[32m[2022-09-16 14:08:20,376] [    INFO][0m - loss: 0.02585579, learning_rate: 2.3595505617977527e-07, global_step: 2460, interval_runtime: 7.0204, interval_samples_per_second: 2.279, interval_steps_per_second: 1.424, epoch: 27.6404[0m
[32m[2022-09-16 14:08:26,721] [    INFO][0m - loss: 0.02926869, learning_rate: 2.247191011235955e-07, global_step: 2470, interval_runtime: 6.3443, interval_samples_per_second: 2.522, interval_steps_per_second: 1.576, epoch: 27.7528[0m
[32m[2022-09-16 14:08:33,061] [    INFO][0m - loss: 0.04714237, learning_rate: 2.134831460674157e-07, global_step: 2480, interval_runtime: 6.34, interval_samples_per_second: 2.524, interval_steps_per_second: 1.577, epoch: 27.8652[0m
[32m[2022-09-16 14:08:39,324] [    INFO][0m - loss: 0.00173257, learning_rate: 2.0224719101123595e-07, global_step: 2490, interval_runtime: 6.2631, interval_samples_per_second: 2.555, interval_steps_per_second: 1.597, epoch: 27.9775[0m
[32m[2022-09-16 14:08:40,188] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 14:08:40,188] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 14:08:40,188] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 14:08:40,188] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 14:08:40,188] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 14:09:01,822] [    INFO][0m - eval_loss: 1.339260458946228, eval_accuracy: 0.8422913719943423, eval_runtime: 21.6333, eval_samples_per_second: 65.362, eval_steps_per_second: 4.114, epoch: 28.0[0m
[32m[2022-09-16 14:09:01,852] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-2492[0m
[32m[2022-09-16 14:09:01,852] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 14:09:04,696] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-2492/tokenizer_config.json[0m
[32m[2022-09-16 14:09:04,696] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-2492/special_tokens_map.json[0m
[32m[2022-09-16 14:09:09,682] [    INFO][0m - Deleting older checkpoint [checkpoints_chid/checkpoint-2403] due to args.save_total_limit[0m
[32m[2022-09-16 14:09:15,459] [    INFO][0m - loss: 0.09836605, learning_rate: 1.910112359550562e-07, global_step: 2500, interval_runtime: 36.1354, interval_samples_per_second: 0.443, interval_steps_per_second: 0.277, epoch: 28.0899[0m
[32m[2022-09-16 14:09:22,469] [    INFO][0m - loss: 0.01296455, learning_rate: 1.797752808988764e-07, global_step: 2510, interval_runtime: 6.3259, interval_samples_per_second: 2.529, interval_steps_per_second: 1.581, epoch: 28.2022[0m
[32m[2022-09-16 14:09:28,802] [    INFO][0m - loss: 0.06403325, learning_rate: 1.6853932584269663e-07, global_step: 2520, interval_runtime: 7.0164, interval_samples_per_second: 2.28, interval_steps_per_second: 1.425, epoch: 28.3146[0m
[32m[2022-09-16 14:09:35,125] [    INFO][0m - loss: 0.02765827, learning_rate: 1.5730337078651687e-07, global_step: 2530, interval_runtime: 6.3233, interval_samples_per_second: 2.53, interval_steps_per_second: 1.581, epoch: 28.427[0m
[32m[2022-09-16 14:09:41,440] [    INFO][0m - loss: 0.00275158, learning_rate: 1.460674157303371e-07, global_step: 2540, interval_runtime: 6.3147, interval_samples_per_second: 2.534, interval_steps_per_second: 1.584, epoch: 28.5393[0m
[32m[2022-09-16 14:09:47,756] [    INFO][0m - loss: 0.01073596, learning_rate: 1.348314606741573e-07, global_step: 2550, interval_runtime: 6.3165, interval_samples_per_second: 2.533, interval_steps_per_second: 1.583, epoch: 28.6517[0m
[32m[2022-09-16 14:09:54,093] [    INFO][0m - loss: 0.0303845, learning_rate: 1.2359550561797752e-07, global_step: 2560, interval_runtime: 6.3373, interval_samples_per_second: 2.525, interval_steps_per_second: 1.578, epoch: 28.764[0m
[32m[2022-09-16 14:10:00,398] [    INFO][0m - loss: 0.04798184, learning_rate: 1.1235955056179776e-07, global_step: 2570, interval_runtime: 6.305, interval_samples_per_second: 2.538, interval_steps_per_second: 1.586, epoch: 28.8764[0m
[32m[2022-09-16 14:10:06,618] [    INFO][0m - loss: 0.05909511, learning_rate: 1.0112359550561797e-07, global_step: 2580, interval_runtime: 6.2198, interval_samples_per_second: 2.572, interval_steps_per_second: 1.608, epoch: 28.9888[0m
[32m[2022-09-16 14:10:06,871] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 14:10:06,871] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 14:10:06,871] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 14:10:06,872] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 14:10:06,872] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 14:10:28,309] [    INFO][0m - eval_loss: 1.3409888744354248, eval_accuracy: 0.842998585572843, eval_runtime: 21.4369, eval_samples_per_second: 65.961, eval_steps_per_second: 4.152, epoch: 29.0[0m
[32m[2022-09-16 14:10:28,333] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-2581[0m
[32m[2022-09-16 14:10:28,333] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 14:10:31,017] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-2581/tokenizer_config.json[0m
[32m[2022-09-16 14:10:31,017] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-2581/special_tokens_map.json[0m
[32m[2022-09-16 14:10:36,050] [    INFO][0m - Deleting older checkpoint [checkpoints_chid/checkpoint-2492] due to args.save_total_limit[0m
[32m[2022-09-16 14:10:42,423] [    INFO][0m - loss: 3.116e-05, learning_rate: 8.98876404494382e-08, global_step: 2590, interval_runtime: 35.8053, interval_samples_per_second: 0.447, interval_steps_per_second: 0.279, epoch: 29.1011[0m
[32m[2022-09-16 14:10:48,733] [    INFO][0m - loss: 0.14502127, learning_rate: 7.865168539325844e-08, global_step: 2600, interval_runtime: 6.3096, interval_samples_per_second: 2.536, interval_steps_per_second: 1.585, epoch: 29.2135[0m
[32m[2022-09-16 14:10:55,040] [    INFO][0m - loss: 0.01341225, learning_rate: 6.741573033707865e-08, global_step: 2610, interval_runtime: 6.3068, interval_samples_per_second: 2.537, interval_steps_per_second: 1.586, epoch: 29.3258[0m
[32m[2022-09-16 14:11:01,348] [    INFO][0m - loss: 0.00087522, learning_rate: 5.617977528089888e-08, global_step: 2620, interval_runtime: 6.3084, interval_samples_per_second: 2.536, interval_steps_per_second: 1.585, epoch: 29.4382[0m
[32m[2022-09-16 14:11:07,656] [    INFO][0m - loss: 0.04408076, learning_rate: 4.49438202247191e-08, global_step: 2630, interval_runtime: 6.3075, interval_samples_per_second: 2.537, interval_steps_per_second: 1.585, epoch: 29.5506[0m
[32m[2022-09-16 14:11:13,980] [    INFO][0m - loss: 0.07028429, learning_rate: 3.370786516853933e-08, global_step: 2640, interval_runtime: 6.3239, interval_samples_per_second: 2.53, interval_steps_per_second: 1.581, epoch: 29.6629[0m
[32m[2022-09-16 14:11:20,292] [    INFO][0m - loss: 0.01524349, learning_rate: 2.247191011235955e-08, global_step: 2650, interval_runtime: 6.3116, interval_samples_per_second: 2.535, interval_steps_per_second: 1.584, epoch: 29.7753[0m
[32m[2022-09-16 14:11:26,614] [    INFO][0m - loss: 0.00354356, learning_rate: 1.1235955056179776e-08, global_step: 2660, interval_runtime: 6.3222, interval_samples_per_second: 2.531, interval_steps_per_second: 1.582, epoch: 29.8876[0m
[32m[2022-09-16 14:11:32,466] [    INFO][0m - loss: 0.02315586, learning_rate: 0.0, global_step: 2670, interval_runtime: 5.8525, interval_samples_per_second: 2.734, interval_steps_per_second: 1.709, epoch: 30.0[0m
[32m[2022-09-16 14:11:32,466] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 14:11:32,467] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-16 14:11:32,467] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 14:11:32,467] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 14:11:32,467] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-16 14:11:53,929] [    INFO][0m - eval_loss: 1.3416029214859009, eval_accuracy: 0.842998585572843, eval_runtime: 21.4616, eval_samples_per_second: 65.885, eval_steps_per_second: 4.147, epoch: 30.0[0m
[32m[2022-09-16 14:11:53,949] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-2670[0m
[32m[2022-09-16 14:11:53,949] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 14:11:56,563] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-2670/tokenizer_config.json[0m
[32m[2022-09-16 14:11:56,563] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-2670/special_tokens_map.json[0m
[32m[2022-09-16 14:12:01,697] [    INFO][0m - Deleting older checkpoint [checkpoints_chid/checkpoint-2581] due to args.save_total_limit[0m
[32m[2022-09-16 14:12:02,233] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-16 14:12:02,233] [    INFO][0m - Loading best model from ./checkpoints_chid/checkpoint-267 (score: 0.8656294200848657).[0m
[32m[2022-09-16 14:12:03,653] [    INFO][0m - train_runtime: 2633.4423, train_samples_per_second: 16.108, train_steps_per_second: 1.014, train_loss: 0.1200751349207974, epoch: 30.0[0m
[32m[2022-09-16 14:12:03,694] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/[0m
[32m[2022-09-16 14:12:03,694] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 14:12:06,377] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/tokenizer_config.json[0m
[32m[2022-09-16 14:12:06,377] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/special_tokens_map.json[0m
[32m[2022-09-16 14:12:06,378] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-16 14:12:06,378] [    INFO][0m -   epoch                    =       30.0[0m
[32m[2022-09-16 14:12:06,379] [    INFO][0m -   train_loss               =     0.1201[0m
[32m[2022-09-16 14:12:06,379] [    INFO][0m -   train_runtime            = 0:43:53.44[0m
[32m[2022-09-16 14:12:06,379] [    INFO][0m -   train_samples_per_second =     16.108[0m
[32m[2022-09-16 14:12:06,379] [    INFO][0m -   train_steps_per_second   =      1.014[0m
[32m[2022-09-16 14:12:06,390] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-16 14:12:06,390] [    INFO][0m -   Num examples = 14014[0m
[32m[2022-09-16 14:12:06,390] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 14:12:06,390] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 14:12:06,390] [    INFO][0m -   Total prediction steps = 876[0m
[32m[2022-09-16 14:15:50,500] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-16 14:15:50,501] [    INFO][0m -   test_accuracy           =     0.8645[0m
[32m[2022-09-16 14:15:50,501] [    INFO][0m -   test_loss               =     0.3267[0m
[32m[2022-09-16 14:15:50,501] [    INFO][0m -   test_runtime            = 0:03:44.11[0m
[32m[2022-09-16 14:15:50,501] [    INFO][0m -   test_samples_per_second =     62.532[0m
[32m[2022-09-16 14:15:50,501] [    INFO][0m -   test_steps_per_second   =      3.909[0m
{
  "labels": 0,
  "text_a": "\u5927\u90e8\u5206\u7684\u5927\u529f\u544a\u6210\u90fd\u662f\u7531\u65e0\u6570\u4e2a\u9020\u8c23\u751f\u4e8b\u7d2f\u79ef\u800c\u6765,\u505a\u597d\u6bcf\u4e00\u4ef6\u5e94\u8be5\u505a\u597d\u7684\u5c0f\u4e8b\u7edd\u4e0d\u662f\u4e00\u4ef6\u5c0f\u4e8b\u3002 \u300a\u4f60\u597d\u674e\u7115\u82f1\u300b\u4e3b\u8981\u8bb2\u4e86\u8d3e\u73b2\u610f\u5916\u7a7f\u8d8a\u5230\u4e86\u6bcd\u4eb2\u5e74\u8f7b\u7684\u65f6\u5019,\u548c\u6bcd\u4eb2\u4e00\u8d77\u611f\u53d7\u4e86\u5979\u7684\u751f\u6d3b\u548c\u60c5\u611f\u3002\u8fd9\u6545\u4e8b\u662f\u6839\u636e\u8d3e\u73b2\u7684\u4eb2\u751f\u7ecf\u5386\u6539\u7f16\u800c\u6765, ...",
  "text_b": "",
  "uid": 24
}

