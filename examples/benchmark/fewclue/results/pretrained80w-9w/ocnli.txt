[32m[2022-09-16 13:28:02,554] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-16 13:28:02,555] [    INFO][0m - ============================================================[0m
[32m[2022-09-16 13:28:02,555] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-16 13:28:02,555] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-16 13:28:02,555] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-16 13:28:02,555] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-16 13:28:02,555] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-16 13:28:02,555] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-16 13:28:02,555] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-16 13:28:02,555] [    INFO][0m - [0m
[32m[2022-09-16 13:28:02,556] [    INFO][0m - ============================================================[0m
[32m[2022-09-16 13:28:02,556] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-16 13:28:02,556] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-16 13:28:02,556] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-16 13:28:02,556] [    INFO][0m - pretrained                    :/ssd2/wanghuijuan03/data/zero-shot/checkpoints_0915/checkpoint-90000/model_state.pdparams[0m
[32m[2022-09-16 13:28:02,556] [    INFO][0m - prompt                        :‚Äú{'text':'text_a'}‚Äù{'mask'}{'mask'}Ôºå‚Äú{'text':'text_b'}‚ÄùÈÄâÈ°πÔºöÂõ†ËÄå/Âè¶Â§ñ/ÁÑ∂ËÄå[0m
[32m[2022-09-16 13:28:02,556] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-16 13:28:02,556] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-16 13:28:02,556] [    INFO][0m - task_name                     :ocnli[0m
[32m[2022-09-16 13:28:02,556] [    INFO][0m - [0m
[32m[2022-09-16 13:28:02,557] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0916 13:28:02.558555 57551 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0916 13:28:02.563189 57551 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-16 13:28:08,701] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-16 13:28:08,713] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-16 13:28:08,713] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-16 13:28:10,960] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚Äù'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'Ôºå‚Äú'}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': '‚ÄùÈÄâÈ°πÔºöÂõ†ËÄå/Âè¶Â§ñ/ÁÑ∂ËÄå'}][0m
[32m[2022-09-16 13:28:11,100] [    INFO][0m - ============================================================[0m
[32m[2022-09-16 13:28:11,100] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-16 13:28:11,100] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-16 13:28:11,101] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-16 13:28:11,101] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-16 13:28:11,101] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-16 13:28:11,101] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-16 13:28:11,101] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-16 13:28:11,101] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-16 13:28:11,101] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-16 13:28:11,101] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-16 13:28:11,101] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-16 13:28:11,101] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-16 13:28:11,101] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-16 13:28:11,102] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-16 13:28:11,102] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-16 13:28:11,102] [    INFO][0m - do_predict                    :False[0m
[32m[2022-09-16 13:28:11,102] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-16 13:28:11,102] [    INFO][0m - eval_batch_size               :16[0m
[32m[2022-09-16 13:28:11,102] [    INFO][0m - eval_steps                    :200[0m
[32m[2022-09-16 13:28:11,102] [    INFO][0m - evaluation_strategy           :IntervalStrategy.EPOCH[0m
[32m[2022-09-16 13:28:11,102] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-16 13:28:11,102] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-16 13:28:11,102] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-16 13:28:11,102] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-16 13:28:11,102] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-16 13:28:11,102] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-16 13:28:11,103] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-16 13:28:11,103] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-16 13:28:11,103] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-16 13:28:11,103] [    INFO][0m - learning_rate                 :3e-06[0m
[32m[2022-09-16 13:28:11,103] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-16 13:28:11,103] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-16 13:28:11,103] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-16 13:28:11,103] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-16 13:28:11,103] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-16 13:28:11,103] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-16 13:28:11,103] [    INFO][0m - logging_dir                   :./checkpoints_ocnli/runs/Sep16_13-28-02_instance-3bwob41y-01[0m
[32m[2022-09-16 13:28:11,103] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-16 13:28:11,103] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-16 13:28:11,103] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-16 13:28:11,104] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-16 13:28:11,104] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-16 13:28:11,104] [    INFO][0m - max_seq_length                :128[0m
[32m[2022-09-16 13:28:11,104] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-16 13:28:11,104] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-16 13:28:11,104] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-16 13:28:11,104] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-16 13:28:11,104] [    INFO][0m - num_train_epochs              :30.0[0m
[32m[2022-09-16 13:28:11,104] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-16 13:28:11,104] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-16 13:28:11,104] [    INFO][0m - output_dir                    :./checkpoints_ocnli/[0m
[32m[2022-09-16 13:28:11,104] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-16 13:28:11,104] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-16 13:28:11,105] [    INFO][0m - per_device_eval_batch_size    :16[0m
[32m[2022-09-16 13:28:11,105] [    INFO][0m - per_device_train_batch_size   :16[0m
[32m[2022-09-16 13:28:11,105] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-16 13:28:11,105] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-16 13:28:11,105] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-16 13:28:11,105] [    INFO][0m - ppt_learning_rate             :3e-05[0m
[32m[2022-09-16 13:28:11,105] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-16 13:28:11,105] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-16 13:28:11,105] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-16 13:28:11,105] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-16 13:28:11,105] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-16 13:28:11,105] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-16 13:28:11,105] [    INFO][0m - run_name                      :./checkpoints_ocnli/[0m
[32m[2022-09-16 13:28:11,106] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-16 13:28:11,106] [    INFO][0m - save_steps                    :200[0m
[32m[2022-09-16 13:28:11,106] [    INFO][0m - save_strategy                 :IntervalStrategy.EPOCH[0m
[32m[2022-09-16 13:28:11,106] [    INFO][0m - save_total_limit              :1[0m
[32m[2022-09-16 13:28:11,106] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-16 13:28:11,106] [    INFO][0m - seed                          :42[0m
[32m[2022-09-16 13:28:11,106] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-16 13:28:11,106] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-16 13:28:11,106] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-16 13:28:11,106] [    INFO][0m - train_batch_size              :16[0m
[32m[2022-09-16 13:28:11,106] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-16 13:28:11,106] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-16 13:28:11,106] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-16 13:28:11,106] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-16 13:28:11,107] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-16 13:28:11,107] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-16 13:28:11,107] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-16 13:28:11,107] [    INFO][0m - [0m
[32m[2022-09-16 13:28:11,110] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-16 13:28:11,110] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:28:11,110] [    INFO][0m -   Num Epochs = 30[0m
[32m[2022-09-16 13:28:11,110] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2022-09-16 13:28:11,110] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2022-09-16 13:28:11,110] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-16 13:28:11,110] [    INFO][0m -   Total optimization steps = 300.0[0m
[32m[2022-09-16 13:28:11,111] [    INFO][0m -   Total num train samples = 4800[0m
[32m[2022-09-16 13:28:16,048] [    INFO][0m - loss: 1.60613842, learning_rate: 2.9e-06, global_step: 10, interval_runtime: 4.9362, interval_samples_per_second: 3.241, interval_steps_per_second: 2.026, epoch: 1.0[0m
[32m[2022-09-16 13:28:16,049] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:28:16,049] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:28:16,049] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:28:16,049] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:28:16,049] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:28:17,424] [    INFO][0m - eval_loss: 1.5115240812301636, eval_accuracy: 0.35, eval_runtime: 1.3745, eval_samples_per_second: 116.403, eval_steps_per_second: 7.275, epoch: 1.0[0m
[32m[2022-09-16 13:28:17,425] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-10[0m
[32m[2022-09-16 13:28:17,425] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:28:20,523] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-10/tokenizer_config.json[0m
[32m[2022-09-16 13:28:20,523] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-10/special_tokens_map.json[0m
[32m[2022-09-16 13:28:29,854] [    INFO][0m - loss: 1.31297598, learning_rate: 2.8000000000000003e-06, global_step: 20, interval_runtime: 13.8057, interval_samples_per_second: 1.159, interval_steps_per_second: 0.724, epoch: 2.0[0m
[32m[2022-09-16 13:28:29,855] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:28:29,855] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:28:29,855] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:28:29,855] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:28:29,855] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:28:31,222] [    INFO][0m - eval_loss: 1.361636757850647, eval_accuracy: 0.35625, eval_runtime: 1.3669, eval_samples_per_second: 117.049, eval_steps_per_second: 7.316, epoch: 2.0[0m
[32m[2022-09-16 13:28:31,223] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-20[0m
[32m[2022-09-16 13:28:31,223] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:28:34,541] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-20/tokenizer_config.json[0m
[32m[2022-09-16 13:28:34,541] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-20/special_tokens_map.json[0m
[32m[2022-09-16 13:28:43,785] [    INFO][0m - loss: 1.14223194, learning_rate: 2.7e-06, global_step: 30, interval_runtime: 13.9314, interval_samples_per_second: 1.148, interval_steps_per_second: 0.718, epoch: 3.0[0m
[32m[2022-09-16 13:28:43,786] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:28:43,786] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:28:43,786] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:28:43,786] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:28:43,786] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:28:45,146] [    INFO][0m - eval_loss: 1.240920066833496, eval_accuracy: 0.35625, eval_runtime: 1.3592, eval_samples_per_second: 117.718, eval_steps_per_second: 7.357, epoch: 3.0[0m
[32m[2022-09-16 13:28:45,146] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-30[0m
[32m[2022-09-16 13:28:45,146] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:28:48,725] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-30/tokenizer_config.json[0m
[32m[2022-09-16 13:28:48,726] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-30/special_tokens_map.json[0m
[32m[2022-09-16 13:28:58,048] [    INFO][0m - Deleting older checkpoint [checkpoints_ocnli/checkpoint-10] due to args.save_total_limit[0m
[32m[2022-09-16 13:29:09,192] [    INFO][0m - loss: 0.96318455, learning_rate: 2.6e-06, global_step: 40, interval_runtime: 25.4074, interval_samples_per_second: 0.63, interval_steps_per_second: 0.394, epoch: 4.0[0m
[32m[2022-09-16 13:29:09,193] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:29:09,193] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:29:09,193] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:29:09,193] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:29:09,194] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:29:10,568] [    INFO][0m - eval_loss: 1.1467788219451904, eval_accuracy: 0.3625, eval_runtime: 1.3742, eval_samples_per_second: 116.434, eval_steps_per_second: 7.277, epoch: 4.0[0m
[32m[2022-09-16 13:29:10,569] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-40[0m
[32m[2022-09-16 13:29:10,569] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:29:13,643] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-40/tokenizer_config.json[0m
[32m[2022-09-16 13:29:13,644] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-40/special_tokens_map.json[0m
[32m[2022-09-16 13:29:19,676] [    INFO][0m - Deleting older checkpoint [checkpoints_ocnli/checkpoint-20] due to args.save_total_limit[0m
[32m[2022-09-16 13:29:51,216] [    INFO][0m - loss: 0.85423641, learning_rate: 2.5e-06, global_step: 50, interval_runtime: 42.0242, interval_samples_per_second: 0.381, interval_steps_per_second: 0.238, epoch: 5.0[0m
[32m[2022-09-16 13:29:51,217] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:29:51,217] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:29:51,218] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:29:51,218] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:29:51,218] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:29:52,586] [    INFO][0m - eval_loss: 1.076284408569336, eval_accuracy: 0.35625, eval_runtime: 1.3682, eval_samples_per_second: 116.943, eval_steps_per_second: 7.309, epoch: 5.0[0m
[32m[2022-09-16 13:29:52,587] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-50[0m
[32m[2022-09-16 13:29:52,587] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:29:56,182] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-50/tokenizer_config.json[0m
[32m[2022-09-16 13:29:56,183] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-50/special_tokens_map.json[0m
[32m[2022-09-16 13:30:02,662] [    INFO][0m - Deleting older checkpoint [checkpoints_ocnli/checkpoint-30] due to args.save_total_limit[0m
[32m[2022-09-16 13:30:30,350] [    INFO][0m - loss: 0.77352104, learning_rate: 2.4000000000000003e-06, global_step: 60, interval_runtime: 39.1336, interval_samples_per_second: 0.409, interval_steps_per_second: 0.256, epoch: 6.0[0m
[32m[2022-09-16 13:30:30,351] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:30:30,351] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:30:30,351] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:30:30,351] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:30:30,351] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:30:31,705] [    INFO][0m - eval_loss: 1.0123409032821655, eval_accuracy: 0.34375, eval_runtime: 1.3536, eval_samples_per_second: 118.201, eval_steps_per_second: 7.388, epoch: 6.0[0m
[32m[2022-09-16 13:30:31,706] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-60[0m
[32m[2022-09-16 13:30:31,706] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:30:35,052] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-60/tokenizer_config.json[0m
[32m[2022-09-16 13:30:35,052] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-60/special_tokens_map.json[0m
[32m[2022-09-16 13:30:40,743] [    INFO][0m - Deleting older checkpoint [checkpoints_ocnli/checkpoint-50] due to args.save_total_limit[0m
[32m[2022-09-16 13:31:02,113] [    INFO][0m - loss: 0.67742777, learning_rate: 2.3000000000000004e-06, global_step: 70, interval_runtime: 31.7627, interval_samples_per_second: 0.504, interval_steps_per_second: 0.315, epoch: 7.0[0m
[32m[2022-09-16 13:31:02,113] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:31:02,113] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:31:02,114] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:31:02,114] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:31:02,114] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:31:03,474] [    INFO][0m - eval_loss: 0.9579941630363464, eval_accuracy: 0.33125, eval_runtime: 1.3596, eval_samples_per_second: 117.677, eval_steps_per_second: 7.355, epoch: 7.0[0m
[32m[2022-09-16 13:31:03,474] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-70[0m
[32m[2022-09-16 13:31:03,474] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:31:06,690] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-70/tokenizer_config.json[0m
[32m[2022-09-16 13:31:06,691] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-70/special_tokens_map.json[0m
[32m[2022-09-16 13:31:13,042] [    INFO][0m - Deleting older checkpoint [checkpoints_ocnli/checkpoint-60] due to args.save_total_limit[0m
[32m[2022-09-16 13:31:17,162] [    INFO][0m - loss: 0.62002077, learning_rate: 2.1999999999999997e-06, global_step: 80, interval_runtime: 15.049, interval_samples_per_second: 1.063, interval_steps_per_second: 0.664, epoch: 8.0[0m
[32m[2022-09-16 13:31:17,162] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:31:17,163] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:31:17,163] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:31:17,163] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:31:17,163] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:31:18,516] [    INFO][0m - eval_loss: 0.9164583086967468, eval_accuracy: 0.35, eval_runtime: 1.3531, eval_samples_per_second: 118.25, eval_steps_per_second: 7.391, epoch: 8.0[0m
[32m[2022-09-16 13:31:18,517] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-80[0m
[32m[2022-09-16 13:31:18,517] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:31:21,546] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-80/tokenizer_config.json[0m
[32m[2022-09-16 13:31:21,547] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-80/special_tokens_map.json[0m
[32m[2022-09-16 13:31:30,103] [    INFO][0m - Deleting older checkpoint [checkpoints_ocnli/checkpoint-70] due to args.save_total_limit[0m
[32m[2022-09-16 13:31:34,245] [    INFO][0m - loss: 0.56206217, learning_rate: 2.1e-06, global_step: 90, interval_runtime: 17.0833, interval_samples_per_second: 0.937, interval_steps_per_second: 0.585, epoch: 9.0[0m
[32m[2022-09-16 13:31:34,246] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:31:34,246] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:31:34,246] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:31:34,246] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:31:34,246] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:31:35,603] [    INFO][0m - eval_loss: 0.8901235461235046, eval_accuracy: 0.36875, eval_runtime: 1.3568, eval_samples_per_second: 117.925, eval_steps_per_second: 7.37, epoch: 9.0[0m
[32m[2022-09-16 13:31:35,604] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-90[0m
[32m[2022-09-16 13:31:35,604] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:31:38,521] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-90/tokenizer_config.json[0m
[32m[2022-09-16 13:31:38,522] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-90/special_tokens_map.json[0m
[32m[2022-09-16 13:31:44,426] [    INFO][0m - Deleting older checkpoint [checkpoints_ocnli/checkpoint-40] due to args.save_total_limit[0m
[32m[2022-09-16 13:31:48,515] [    INFO][0m - loss: 0.51542511, learning_rate: 2e-06, global_step: 100, interval_runtime: 14.27, interval_samples_per_second: 1.121, interval_steps_per_second: 0.701, epoch: 10.0[0m
[32m[2022-09-16 13:31:48,516] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:31:48,516] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:31:48,516] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:31:48,516] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:31:48,516] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:31:49,871] [    INFO][0m - eval_loss: 0.872409462928772, eval_accuracy: 0.38125, eval_runtime: 1.3545, eval_samples_per_second: 118.128, eval_steps_per_second: 7.383, epoch: 10.0[0m
[32m[2022-09-16 13:31:52,576] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-100[0m
[32m[2022-09-16 13:31:52,577] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:31:55,658] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-16 13:31:55,658] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-16 13:32:01,147] [    INFO][0m - Deleting older checkpoint [checkpoints_ocnli/checkpoint-80] due to args.save_total_limit[0m
[32m[2022-09-16 13:32:05,251] [    INFO][0m - loss: 0.48114233, learning_rate: 1.9e-06, global_step: 110, interval_runtime: 16.7363, interval_samples_per_second: 0.956, interval_steps_per_second: 0.598, epoch: 11.0[0m
[32m[2022-09-16 13:32:05,252] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:32:05,253] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:32:05,253] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:32:05,253] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:32:05,253] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:32:06,606] [    INFO][0m - eval_loss: 0.8555307388305664, eval_accuracy: 0.4, eval_runtime: 1.3534, eval_samples_per_second: 118.221, eval_steps_per_second: 7.389, epoch: 11.0[0m
[32m[2022-09-16 13:32:06,607] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-110[0m
[32m[2022-09-16 13:32:06,607] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:32:09,594] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-110/tokenizer_config.json[0m
[32m[2022-09-16 13:32:09,595] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-110/special_tokens_map.json[0m
[32m[2022-09-16 13:32:15,669] [    INFO][0m - Deleting older checkpoint [checkpoints_ocnli/checkpoint-90] due to args.save_total_limit[0m
[32m[2022-09-16 13:32:20,211] [    INFO][0m - loss: 0.44789863, learning_rate: 1.8e-06, global_step: 120, interval_runtime: 14.6006, interval_samples_per_second: 1.096, interval_steps_per_second: 0.685, epoch: 12.0[0m
[32m[2022-09-16 13:32:20,211] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:32:20,212] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:32:20,212] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:32:20,212] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:32:20,212] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:32:21,566] [    INFO][0m - eval_loss: 0.8476617932319641, eval_accuracy: 0.425, eval_runtime: 1.3537, eval_samples_per_second: 118.198, eval_steps_per_second: 7.387, epoch: 12.0[0m
[32m[2022-09-16 13:32:21,566] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-120[0m
[32m[2022-09-16 13:32:21,566] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:32:24,634] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-120/tokenizer_config.json[0m
[32m[2022-09-16 13:32:24,635] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-120/special_tokens_map.json[0m
[32m[2022-09-16 13:32:31,013] [    INFO][0m - Deleting older checkpoint [checkpoints_ocnli/checkpoint-100] due to args.save_total_limit[0m
[32m[2022-09-16 13:32:36,709] [    INFO][0m - loss: 0.40848951, learning_rate: 1.7e-06, global_step: 130, interval_runtime: 16.5742, interval_samples_per_second: 0.965, interval_steps_per_second: 0.603, epoch: 13.0[0m
[32m[2022-09-16 13:32:36,710] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:32:36,710] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:32:36,710] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:32:36,710] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:32:36,711] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:32:38,097] [    INFO][0m - eval_loss: 0.8489448428153992, eval_accuracy: 0.4375, eval_runtime: 1.386, eval_samples_per_second: 115.44, eval_steps_per_second: 7.215, epoch: 13.0[0m
[32m[2022-09-16 13:32:38,097] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-130[0m
[32m[2022-09-16 13:32:38,097] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:32:41,547] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-130/tokenizer_config.json[0m
[32m[2022-09-16 13:32:41,547] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-130/special_tokens_map.json[0m
[32m[2022-09-16 13:32:47,030] [    INFO][0m - Deleting older checkpoint [checkpoints_ocnli/checkpoint-110] due to args.save_total_limit[0m
[32m[2022-09-16 13:33:07,733] [    INFO][0m - loss: 0.38185897, learning_rate: 1.6e-06, global_step: 140, interval_runtime: 31.3067, interval_samples_per_second: 0.511, interval_steps_per_second: 0.319, epoch: 14.0[0m
[32m[2022-09-16 13:33:07,734] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:33:07,734] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:33:07,734] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:33:07,734] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:33:07,734] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:33:09,100] [    INFO][0m - eval_loss: 0.8460466265678406, eval_accuracy: 0.4375, eval_runtime: 1.3661, eval_samples_per_second: 117.12, eval_steps_per_second: 7.32, epoch: 14.0[0m
[32m[2022-09-16 13:33:09,101] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-140[0m
[32m[2022-09-16 13:33:09,101] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:33:12,798] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-140/tokenizer_config.json[0m
[32m[2022-09-16 13:33:12,799] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-140/special_tokens_map.json[0m
[32m[2022-09-16 13:33:18,244] [    INFO][0m - Deleting older checkpoint [checkpoints_ocnli/checkpoint-120] due to args.save_total_limit[0m
[32m[2022-09-16 13:33:22,343] [    INFO][0m - loss: 0.34610882, learning_rate: 1.5e-06, global_step: 150, interval_runtime: 14.6102, interval_samples_per_second: 1.095, interval_steps_per_second: 0.684, epoch: 15.0[0m
[32m[2022-09-16 13:33:22,344] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:33:22,344] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:33:22,344] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:33:22,344] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:33:22,344] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:33:23,707] [    INFO][0m - eval_loss: 0.8517532348632812, eval_accuracy: 0.45, eval_runtime: 1.3629, eval_samples_per_second: 117.396, eval_steps_per_second: 7.337, epoch: 15.0[0m
[32m[2022-09-16 13:33:23,708] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-150[0m
[32m[2022-09-16 13:33:23,708] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:33:26,902] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-150/tokenizer_config.json[0m
[32m[2022-09-16 13:33:26,902] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-150/special_tokens_map.json[0m
[32m[2022-09-16 13:33:32,422] [    INFO][0m - Deleting older checkpoint [checkpoints_ocnli/checkpoint-130] due to args.save_total_limit[0m
[32m[2022-09-16 13:33:36,569] [    INFO][0m - loss: 0.32649231, learning_rate: 1.4000000000000001e-06, global_step: 160, interval_runtime: 14.226, interval_samples_per_second: 1.125, interval_steps_per_second: 0.703, epoch: 16.0[0m
[32m[2022-09-16 13:33:36,570] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:33:36,570] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:33:36,570] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:33:36,570] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:33:36,570] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:33:37,927] [    INFO][0m - eval_loss: 0.8598057627677917, eval_accuracy: 0.4375, eval_runtime: 1.3568, eval_samples_per_second: 117.928, eval_steps_per_second: 7.371, epoch: 16.0[0m
[32m[2022-09-16 13:33:37,927] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-160[0m
[32m[2022-09-16 13:33:37,928] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:33:40,901] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-160/tokenizer_config.json[0m
[32m[2022-09-16 13:33:40,902] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-160/special_tokens_map.json[0m
[32m[2022-09-16 13:33:46,397] [    INFO][0m - Deleting older checkpoint [checkpoints_ocnli/checkpoint-140] due to args.save_total_limit[0m
[32m[2022-09-16 13:33:50,581] [    INFO][0m - loss: 0.3039052, learning_rate: 1.3e-06, global_step: 170, interval_runtime: 14.0119, interval_samples_per_second: 1.142, interval_steps_per_second: 0.714, epoch: 17.0[0m
[32m[2022-09-16 13:33:50,582] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:33:50,582] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:33:50,582] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:33:50,582] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:33:50,582] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:33:52,605] [    INFO][0m - eval_loss: 0.8674575686454773, eval_accuracy: 0.44375, eval_runtime: 1.3598, eval_samples_per_second: 117.662, eval_steps_per_second: 7.354, epoch: 17.0[0m
[32m[2022-09-16 13:33:52,605] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-170[0m
[32m[2022-09-16 13:33:52,605] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:33:55,555] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-170/tokenizer_config.json[0m
[32m[2022-09-16 13:33:55,555] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-170/special_tokens_map.json[0m
[32m[2022-09-16 13:34:03,293] [    INFO][0m - Deleting older checkpoint [checkpoints_ocnli/checkpoint-160] due to args.save_total_limit[0m
[32m[2022-09-16 13:34:07,444] [    INFO][0m - loss: 0.28659251, learning_rate: 1.2000000000000002e-06, global_step: 180, interval_runtime: 16.8635, interval_samples_per_second: 0.949, interval_steps_per_second: 0.593, epoch: 18.0[0m
[32m[2022-09-16 13:34:07,445] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:34:07,445] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:34:07,445] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:34:07,445] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:34:07,445] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:34:08,799] [    INFO][0m - eval_loss: 0.8709985613822937, eval_accuracy: 0.4625, eval_runtime: 1.3532, eval_samples_per_second: 118.237, eval_steps_per_second: 7.39, epoch: 18.0[0m
[32m[2022-09-16 13:34:08,799] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-180[0m
[32m[2022-09-16 13:34:08,799] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:34:11,778] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-180/tokenizer_config.json[0m
[32m[2022-09-16 13:34:11,779] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-180/special_tokens_map.json[0m
[32m[2022-09-16 13:34:17,484] [    INFO][0m - Deleting older checkpoint [checkpoints_ocnli/checkpoint-150] due to args.save_total_limit[0m
[32m[2022-09-16 13:34:21,561] [    INFO][0m - loss: 0.2644227, learning_rate: 1.0999999999999998e-06, global_step: 190, interval_runtime: 14.1163, interval_samples_per_second: 1.133, interval_steps_per_second: 0.708, epoch: 19.0[0m
[32m[2022-09-16 13:34:21,562] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:34:21,562] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:34:21,562] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:34:21,562] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:34:21,563] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:34:22,928] [    INFO][0m - eval_loss: 0.8774846792221069, eval_accuracy: 0.46875, eval_runtime: 1.3652, eval_samples_per_second: 117.197, eval_steps_per_second: 7.325, epoch: 19.0[0m
[32m[2022-09-16 13:34:22,928] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-190[0m
[32m[2022-09-16 13:34:22,928] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:34:26,053] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-190/tokenizer_config.json[0m
[32m[2022-09-16 13:34:26,053] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-190/special_tokens_map.json[0m
[32m[2022-09-16 13:34:31,877] [    INFO][0m - Deleting older checkpoint [checkpoints_ocnli/checkpoint-170] due to args.save_total_limit[0m
[32m[2022-09-16 13:34:36,598] [    INFO][0m - loss: 0.2661485, learning_rate: 1e-06, global_step: 200, interval_runtime: 15.0368, interval_samples_per_second: 1.064, interval_steps_per_second: 0.665, epoch: 20.0[0m
[32m[2022-09-16 13:34:36,598] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:34:36,598] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:34:36,598] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:34:36,598] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:34:36,599] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:34:37,954] [    INFO][0m - eval_loss: 0.8801220059394836, eval_accuracy: 0.46875, eval_runtime: 1.3557, eval_samples_per_second: 118.018, eval_steps_per_second: 7.376, epoch: 20.0[0m
[32m[2022-09-16 13:34:42,303] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-200[0m
[32m[2022-09-16 13:34:42,304] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:34:46,319] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-200/tokenizer_config.json[0m
[32m[2022-09-16 13:34:46,319] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-200/special_tokens_map.json[0m
[32m[2022-09-16 13:34:52,211] [    INFO][0m - Deleting older checkpoint [checkpoints_ocnli/checkpoint-180] due to args.save_total_limit[0m
[32m[2022-09-16 13:35:04,893] [    INFO][0m - loss: 0.23011315, learning_rate: 9e-07, global_step: 210, interval_runtime: 28.2956, interval_samples_per_second: 0.565, interval_steps_per_second: 0.353, epoch: 21.0[0m
[32m[2022-09-16 13:35:04,894] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:35:04,894] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:35:04,894] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:35:04,894] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:35:04,894] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:35:06,251] [    INFO][0m - eval_loss: 0.8835901021957397, eval_accuracy: 0.48125, eval_runtime: 1.3572, eval_samples_per_second: 117.892, eval_steps_per_second: 7.368, epoch: 21.0[0m
[32m[2022-09-16 13:35:06,252] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-210[0m
[32m[2022-09-16 13:35:06,252] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:35:09,326] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-210/tokenizer_config.json[0m
[32m[2022-09-16 13:35:09,327] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-210/special_tokens_map.json[0m
[32m[2022-09-16 13:35:16,067] [    INFO][0m - Deleting older checkpoint [checkpoints_ocnli/checkpoint-190] due to args.save_total_limit[0m
[32m[2022-09-16 13:35:23,679] [    INFO][0m - loss: 0.22207544, learning_rate: 8e-07, global_step: 220, interval_runtime: 18.7863, interval_samples_per_second: 0.852, interval_steps_per_second: 0.532, epoch: 22.0[0m
[32m[2022-09-16 13:35:23,680] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:35:23,680] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:35:23,680] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:35:23,680] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:35:23,680] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:35:25,037] [    INFO][0m - eval_loss: 0.8890661001205444, eval_accuracy: 0.48125, eval_runtime: 1.3566, eval_samples_per_second: 117.945, eval_steps_per_second: 7.372, epoch: 22.0[0m
[32m[2022-09-16 13:35:25,038] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-220[0m
[32m[2022-09-16 13:35:25,038] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:35:31,937] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-220/tokenizer_config.json[0m
[32m[2022-09-16 13:35:31,938] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-220/special_tokens_map.json[0m
[32m[2022-09-16 13:35:37,517] [    INFO][0m - Deleting older checkpoint [checkpoints_ocnli/checkpoint-200] due to args.save_total_limit[0m
[32m[2022-09-16 13:35:41,543] [    INFO][0m - loss: 0.22392554, learning_rate: 7.000000000000001e-07, global_step: 230, interval_runtime: 17.8633, interval_samples_per_second: 0.896, interval_steps_per_second: 0.56, epoch: 23.0[0m
[32m[2022-09-16 13:35:41,544] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:35:41,544] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:35:41,544] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:35:41,544] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:35:41,544] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:35:42,904] [    INFO][0m - eval_loss: 0.8965657949447632, eval_accuracy: 0.475, eval_runtime: 1.3595, eval_samples_per_second: 117.69, eval_steps_per_second: 7.356, epoch: 23.0[0m
[32m[2022-09-16 13:35:42,904] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-230[0m
[32m[2022-09-16 13:35:42,904] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:35:47,235] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-230/tokenizer_config.json[0m
[32m[2022-09-16 13:35:47,235] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-230/special_tokens_map.json[0m
[32m[2022-09-16 13:35:53,059] [    INFO][0m - Deleting older checkpoint [checkpoints_ocnli/checkpoint-220] due to args.save_total_limit[0m
[32m[2022-09-16 13:35:57,181] [    INFO][0m - loss: 0.20803447, learning_rate: 6.000000000000001e-07, global_step: 240, interval_runtime: 15.6383, interval_samples_per_second: 1.023, interval_steps_per_second: 0.639, epoch: 24.0[0m
[32m[2022-09-16 13:35:57,182] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:35:57,182] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:35:57,182] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:35:57,182] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:35:57,182] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:35:58,541] [    INFO][0m - eval_loss: 0.9033660888671875, eval_accuracy: 0.48125, eval_runtime: 1.3592, eval_samples_per_second: 117.715, eval_steps_per_second: 7.357, epoch: 24.0[0m
[32m[2022-09-16 13:35:58,542] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-240[0m
[32m[2022-09-16 13:35:58,542] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:36:01,784] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-240/tokenizer_config.json[0m
[32m[2022-09-16 13:36:01,785] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-240/special_tokens_map.json[0m
[32m[2022-09-16 13:36:07,571] [    INFO][0m - Deleting older checkpoint [checkpoints_ocnli/checkpoint-230] due to args.save_total_limit[0m
[32m[2022-09-16 13:36:11,765] [    INFO][0m - loss: 0.20317454, learning_rate: 5e-07, global_step: 250, interval_runtime: 14.5834, interval_samples_per_second: 1.097, interval_steps_per_second: 0.686, epoch: 25.0[0m
[32m[2022-09-16 13:36:11,765] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:36:11,766] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:36:11,766] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:36:11,766] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:36:11,766] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:36:13,134] [    INFO][0m - eval_loss: 0.9058851003646851, eval_accuracy: 0.4875, eval_runtime: 1.3686, eval_samples_per_second: 116.906, eval_steps_per_second: 7.307, epoch: 25.0[0m
[32m[2022-09-16 13:36:13,135] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-250[0m
[32m[2022-09-16 13:36:13,135] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:36:16,821] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-250/tokenizer_config.json[0m
[32m[2022-09-16 13:36:16,822] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-250/special_tokens_map.json[0m
[32m[2022-09-16 13:36:22,620] [    INFO][0m - Deleting older checkpoint [checkpoints_ocnli/checkpoint-210] due to args.save_total_limit[0m
[32m[2022-09-16 13:36:26,710] [    INFO][0m - loss: 0.20157046, learning_rate: 4e-07, global_step: 260, interval_runtime: 14.9455, interval_samples_per_second: 1.071, interval_steps_per_second: 0.669, epoch: 26.0[0m
[32m[2022-09-16 13:36:26,711] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:36:26,711] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:36:26,711] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:36:26,711] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:36:26,711] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:36:28,078] [    INFO][0m - eval_loss: 0.908871054649353, eval_accuracy: 0.48125, eval_runtime: 1.3666, eval_samples_per_second: 117.079, eval_steps_per_second: 7.317, epoch: 26.0[0m
[32m[2022-09-16 13:36:28,079] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-260[0m
[32m[2022-09-16 13:36:28,079] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:36:31,294] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-260/tokenizer_config.json[0m
[32m[2022-09-16 13:36:31,294] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-260/special_tokens_map.json[0m
[32m[2022-09-16 13:36:40,596] [    INFO][0m - Deleting older checkpoint [checkpoints_ocnli/checkpoint-240] due to args.save_total_limit[0m
[32m[2022-09-16 13:36:57,024] [    INFO][0m - loss: 0.19626057, learning_rate: 3.0000000000000004e-07, global_step: 270, interval_runtime: 30.3141, interval_samples_per_second: 0.528, interval_steps_per_second: 0.33, epoch: 27.0[0m
[32m[2022-09-16 13:36:57,025] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:36:57,025] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:36:57,025] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:36:57,025] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:36:57,025] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:36:58,384] [    INFO][0m - eval_loss: 0.9128545522689819, eval_accuracy: 0.4875, eval_runtime: 1.3586, eval_samples_per_second: 117.766, eval_steps_per_second: 7.36, epoch: 27.0[0m
[32m[2022-09-16 13:36:58,384] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-270[0m
[32m[2022-09-16 13:36:58,384] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:37:01,358] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-270/tokenizer_config.json[0m
[32m[2022-09-16 13:37:01,358] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-270/special_tokens_map.json[0m
[32m[2022-09-16 13:37:07,253] [    INFO][0m - Deleting older checkpoint [checkpoints_ocnli/checkpoint-260] due to args.save_total_limit[0m
[32m[2022-09-16 13:37:18,019] [    INFO][0m - loss: 0.19168998, learning_rate: 2e-07, global_step: 280, interval_runtime: 20.9951, interval_samples_per_second: 0.762, interval_steps_per_second: 0.476, epoch: 28.0[0m
[32m[2022-09-16 13:37:18,020] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:37:18,020] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:37:18,020] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:37:18,020] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:37:18,020] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:37:19,389] [    INFO][0m - eval_loss: 0.9147836565971375, eval_accuracy: 0.4875, eval_runtime: 1.3683, eval_samples_per_second: 116.932, eval_steps_per_second: 7.308, epoch: 28.0[0m
[32m[2022-09-16 13:37:19,389] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-280[0m
[32m[2022-09-16 13:37:19,389] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:37:22,236] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-280/tokenizer_config.json[0m
[32m[2022-09-16 13:37:22,236] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-280/special_tokens_map.json[0m
[32m[2022-09-16 13:37:28,967] [    INFO][0m - Deleting older checkpoint [checkpoints_ocnli/checkpoint-270] due to args.save_total_limit[0m
[32m[2022-09-16 13:37:33,129] [    INFO][0m - loss: 0.1855433, learning_rate: 1e-07, global_step: 290, interval_runtime: 15.1102, interval_samples_per_second: 1.059, interval_steps_per_second: 0.662, epoch: 29.0[0m
[32m[2022-09-16 13:37:33,130] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:37:33,130] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:37:33,130] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:37:33,130] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:37:33,131] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:37:34,484] [    INFO][0m - eval_loss: 0.9161565899848938, eval_accuracy: 0.4875, eval_runtime: 1.353, eval_samples_per_second: 118.252, eval_steps_per_second: 7.391, epoch: 29.0[0m
[32m[2022-09-16 13:37:35,015] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-290[0m
[32m[2022-09-16 13:37:35,015] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:37:37,879] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-290/tokenizer_config.json[0m
[32m[2022-09-16 13:37:37,880] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-290/special_tokens_map.json[0m
[32m[2022-09-16 13:37:44,273] [    INFO][0m - Deleting older checkpoint [checkpoints_ocnli/checkpoint-280] due to args.save_total_limit[0m
[32m[2022-09-16 13:37:48,407] [    INFO][0m - loss: 0.20186684, learning_rate: 0.0, global_step: 300, interval_runtime: 15.2781, interval_samples_per_second: 1.047, interval_steps_per_second: 0.655, epoch: 30.0[0m
[32m[2022-09-16 13:37:48,408] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:37:48,408] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:37:48,408] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:37:48,408] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:37:48,408] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:37:49,764] [    INFO][0m - eval_loss: 0.9166373014450073, eval_accuracy: 0.4875, eval_runtime: 1.356, eval_samples_per_second: 117.99, eval_steps_per_second: 7.374, epoch: 30.0[0m
[32m[2022-09-16 13:37:51,041] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/checkpoint-300[0m
[32m[2022-09-16 13:37:51,042] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:37:54,053] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/checkpoint-300/tokenizer_config.json[0m
[32m[2022-09-16 13:37:54,054] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/checkpoint-300/special_tokens_map.json[0m
[32m[2022-09-16 13:37:59,704] [    INFO][0m - Deleting older checkpoint [checkpoints_ocnli/checkpoint-290] due to args.save_total_limit[0m
[32m[2022-09-16 13:38:00,360] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-16 13:38:00,360] [    INFO][0m - Loading best model from ./checkpoints_ocnli/checkpoint-250 (score: 0.4875).[0m
[32m[2022-09-16 13:38:02,278] [    INFO][0m - train_runtime: 591.1665, train_samples_per_second: 8.12, train_steps_per_second: 0.507, train_loss: 0.48681793093681336, epoch: 30.0[0m
[32m[2022-09-16 13:38:02,353] [    INFO][0m - Saving model checkpoint to ./checkpoints_ocnli/[0m
[32m[2022-09-16 13:38:02,354] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:38:07,390] [    INFO][0m - tokenizer config file saved in ./checkpoints_ocnli/tokenizer_config.json[0m
[32m[2022-09-16 13:38:07,391] [    INFO][0m - Special tokens file saved in ./checkpoints_ocnli/special_tokens_map.json[0m
[32m[2022-09-16 13:38:07,392] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-16 13:38:07,392] [    INFO][0m -   epoch                    =       30.0[0m
[32m[2022-09-16 13:38:07,392] [    INFO][0m -   train_loss               =     0.4868[0m
[32m[2022-09-16 13:38:07,392] [    INFO][0m -   train_runtime            = 0:09:51.16[0m
[32m[2022-09-16 13:38:07,392] [    INFO][0m -   train_samples_per_second =       8.12[0m
[32m[2022-09-16 13:38:07,392] [    INFO][0m -   train_steps_per_second   =      0.507[0m
[32m[2022-09-16 13:38:07,395] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-16 13:38:07,395] [    INFO][0m -   Num examples = 2520[0m
[32m[2022-09-16 13:38:07,395] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:38:07,395] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:38:07,395] [    INFO][0m -   Total prediction steps = 158[0m
[32m[2022-09-16 13:38:29,148] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-16 13:38:29,148] [    INFO][0m -   test_accuracy           =     0.4667[0m
[32m[2022-09-16 13:38:29,148] [    INFO][0m -   test_loss               =     0.9486[0m
[32m[2022-09-16 13:38:29,148] [    INFO][0m -   test_runtime            = 0:00:21.75[0m
[32m[2022-09-16 13:38:29,149] [    INFO][0m -   test_samples_per_second =    115.849[0m
[32m[2022-09-16 13:38:29,149] [    INFO][0m -   test_steps_per_second   =      7.264[0m
{
  "labels": 2,
  "text_a": "\u4e03\u4e94\u671f\u95f4\u5f00\u59cb,\u56fd\u5bb6\u53c8\u6295\u8d44\u5c06\u6b66\u6c49\u5e02\u533a\u7684\u90e8\u5206\u571f\u5824\u6539\u5efa\u4e3a\u94a2\u7b4b\u6ce5\u51dd\u571f\u9632\u6c34\u5899",
  "text_b": "\u516b\u4e94\u671f\u95f4\u4f1a\u628a\u5269\u4e0b\u7684\u571f\u5824\u90fd\u6539\u5efa\u5b8c",
  "uid": 0
}

