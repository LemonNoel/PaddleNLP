[32m[2022-09-16 14:11:41,818] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-16 14:11:41,818] [    INFO][0m - ============================================================[0m
[32m[2022-09-16 14:11:41,818] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-16 14:11:41,818] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-16 14:11:41,818] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-16 14:11:41,818] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-16 14:11:41,819] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-16 14:11:41,819] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-16 14:11:41,819] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-16 14:11:41,819] [    INFO][0m - [0m
[32m[2022-09-16 14:11:41,819] [    INFO][0m - ============================================================[0m
[32m[2022-09-16 14:11:41,819] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-16 14:11:41,819] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-16 14:11:41,819] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-16 14:11:41,819] [    INFO][0m - pretrained                    :/ssd2/wanghuijuan03/data/zero-shot/checkpoints_0915/checkpoint-90000/model_state.pdparams[0m
[32m[2022-09-16 14:11:41,819] [    INFO][0m - prompt                        :‚Äú{'text':'text_a'}‚ÄùÊúâ‰∫∫ËÆ§‰∏∫{'text':'text_b'}ÔºåËØ¥Êòé‰ªñ{'mask'}{'mask'}ÁêÜËß£‰∫ÜËøô‰∏™Âè•Â≠ê„ÄÇÈÄâÈ°πÔºöÂ∑≤Áªè/Ê≤°Êúâ[0m
[32m[2022-09-16 14:11:41,819] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-16 14:11:41,819] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-16 14:11:41,819] [    INFO][0m - task_name                     :cluewsc[0m
[32m[2022-09-16 14:11:41,819] [    INFO][0m - [0m
[32m[2022-09-16 14:11:41,820] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0916 14:11:41.821321 33063 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0916 14:11:41.825420 33063 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-16 14:11:46,506] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-16 14:11:46,517] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-16 14:11:46,517] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-16 14:11:48,172] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùÊúâ‰∫∫ËÆ§‰∏∫'}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': 'ÔºåËØ¥Êòé‰ªñ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'ÁêÜËß£‰∫ÜËøô‰∏™Âè•Â≠ê„ÄÇÈÄâÈ°πÔºöÂ∑≤Áªè/Ê≤°Êúâ'}][0m
[32m[2022-09-16 14:11:48,283] [    INFO][0m - ============================================================[0m
[32m[2022-09-16 14:11:48,284] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-16 14:11:48,284] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-16 14:11:48,284] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-16 14:11:48,284] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-16 14:11:48,284] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-16 14:11:48,284] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-16 14:11:48,284] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-16 14:11:48,284] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-16 14:11:48,285] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-16 14:11:48,285] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-16 14:11:48,285] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-16 14:11:48,285] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-16 14:11:48,285] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-16 14:11:48,285] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-16 14:11:48,285] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-16 14:11:48,285] [    INFO][0m - do_predict                    :False[0m
[32m[2022-09-16 14:11:48,285] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-16 14:11:48,285] [    INFO][0m - eval_batch_size               :4[0m
[32m[2022-09-16 14:11:48,285] [    INFO][0m - eval_steps                    :200[0m
[32m[2022-09-16 14:11:48,285] [    INFO][0m - evaluation_strategy           :IntervalStrategy.EPOCH[0m
[32m[2022-09-16 14:11:48,285] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-16 14:11:48,286] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-16 14:11:48,286] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-16 14:11:48,286] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-16 14:11:48,286] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-16 14:11:48,286] [    INFO][0m - gradient_accumulation_steps   :4[0m
[32m[2022-09-16 14:11:48,286] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-16 14:11:48,286] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-16 14:11:48,286] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-16 14:11:48,286] [    INFO][0m - learning_rate                 :3e-06[0m
[32m[2022-09-16 14:11:48,286] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-16 14:11:48,286] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-16 14:11:48,286] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-16 14:11:48,286] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-16 14:11:48,286] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-16 14:11:48,287] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-16 14:11:48,287] [    INFO][0m - logging_dir                   :./checkpoints_cluewsc/runs/Sep16_14-11-41_instance-3bwob41y-01[0m
[32m[2022-09-16 14:11:48,287] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-16 14:11:48,287] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-16 14:11:48,287] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-16 14:11:48,287] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-16 14:11:48,287] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-16 14:11:48,287] [    INFO][0m - max_seq_length                :256[0m
[32m[2022-09-16 14:11:48,287] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-16 14:11:48,287] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-16 14:11:48,287] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-16 14:11:48,287] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-16 14:11:48,287] [    INFO][0m - num_train_epochs              :30.0[0m
[32m[2022-09-16 14:11:48,288] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-16 14:11:48,288] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-16 14:11:48,288] [    INFO][0m - output_dir                    :./checkpoints_cluewsc/[0m
[32m[2022-09-16 14:11:48,288] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-16 14:11:48,288] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-16 14:11:48,288] [    INFO][0m - per_device_eval_batch_size    :4[0m
[32m[2022-09-16 14:11:48,288] [    INFO][0m - per_device_train_batch_size   :4[0m
[32m[2022-09-16 14:11:48,288] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-16 14:11:48,288] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-16 14:11:48,288] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-16 14:11:48,288] [    INFO][0m - ppt_learning_rate             :3e-05[0m
[32m[2022-09-16 14:11:48,288] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-16 14:11:48,288] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-16 14:11:48,289] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-16 14:11:48,289] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-16 14:11:48,289] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-16 14:11:48,289] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-16 14:11:48,289] [    INFO][0m - run_name                      :./checkpoints_cluewsc/[0m
[32m[2022-09-16 14:11:48,289] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-16 14:11:48,289] [    INFO][0m - save_steps                    :200[0m
[32m[2022-09-16 14:11:48,289] [    INFO][0m - save_strategy                 :IntervalStrategy.EPOCH[0m
[32m[2022-09-16 14:11:48,289] [    INFO][0m - save_total_limit              :1[0m
[32m[2022-09-16 14:11:48,289] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-16 14:11:48,289] [    INFO][0m - seed                          :42[0m
[32m[2022-09-16 14:11:48,289] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-16 14:11:48,289] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-16 14:11:48,289] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-16 14:11:48,290] [    INFO][0m - train_batch_size              :4[0m
[32m[2022-09-16 14:11:48,290] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-16 14:11:48,290] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-16 14:11:48,290] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-16 14:11:48,290] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-16 14:11:48,290] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-16 14:11:48,290] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-16 14:11:48,290] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-16 14:11:48,290] [    INFO][0m - [0m
[32m[2022-09-16 14:11:48,294] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-16 14:11:48,294] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 14:11:48,294] [    INFO][0m -   Num Epochs = 30[0m
[32m[2022-09-16 14:11:48,294] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2022-09-16 14:11:48,294] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2022-09-16 14:11:48,294] [    INFO][0m -   Gradient Accumulation steps = 4[0m
[32m[2022-09-16 14:11:48,295] [    INFO][0m -   Total optimization steps = 300.0[0m
[32m[2022-09-16 14:11:48,295] [    INFO][0m -   Total num train samples = 4800[0m
[32m[2022-09-16 14:11:56,543] [    INFO][0m - loss: 2.90023499, learning_rate: 2.9e-06, global_step: 10, interval_runtime: 8.2465, interval_samples_per_second: 1.94, interval_steps_per_second: 1.213, epoch: 1.0[0m
[32m[2022-09-16 14:11:56,544] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 14:11:56,544] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 14:11:56,545] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-16 14:11:56,545] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-16 14:11:56,545] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-16 14:11:59,293] [    INFO][0m - eval_loss: 2.5994646549224854, eval_accuracy: 0.5094339622641509, eval_runtime: 2.7482, eval_samples_per_second: 57.855, eval_steps_per_second: 14.555, epoch: 1.0[0m
[32m[2022-09-16 14:11:59,294] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-10[0m
[32m[2022-09-16 14:11:59,294] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 14:12:02,475] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-10/tokenizer_config.json[0m
[32m[2022-09-16 14:12:02,476] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-10/special_tokens_map.json[0m
[32m[2022-09-16 14:12:15,638] [    INFO][0m - loss: 1.43282681, learning_rate: 2.8000000000000003e-06, global_step: 20, interval_runtime: 19.0954, interval_samples_per_second: 0.838, interval_steps_per_second: 0.524, epoch: 2.0[0m
[32m[2022-09-16 14:12:15,639] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 14:12:15,639] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 14:12:15,639] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-16 14:12:15,639] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-16 14:12:15,640] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-16 14:12:18,346] [    INFO][0m - eval_loss: 1.3358162641525269, eval_accuracy: 0.5283018867924528, eval_runtime: 2.7059, eval_samples_per_second: 58.76, eval_steps_per_second: 14.782, epoch: 2.0[0m
[32m[2022-09-16 14:12:18,346] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-20[0m
[32m[2022-09-16 14:12:18,346] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 14:12:21,125] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-20/tokenizer_config.json[0m
[32m[2022-09-16 14:12:21,126] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-20/special_tokens_map.json[0m
[32m[2022-09-16 14:12:33,728] [    INFO][0m - loss: 0.84473038, learning_rate: 2.7e-06, global_step: 30, interval_runtime: 18.0898, interval_samples_per_second: 0.884, interval_steps_per_second: 0.553, epoch: 3.0[0m
[32m[2022-09-16 14:12:33,729] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 14:12:33,729] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 14:12:33,729] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-16 14:12:33,729] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-16 14:12:33,729] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-16 14:12:39,801] [    INFO][0m - eval_loss: 0.98105388879776, eval_accuracy: 0.5534591194968553, eval_runtime: 2.7228, eval_samples_per_second: 58.396, eval_steps_per_second: 14.691, epoch: 3.0[0m
[32m[2022-09-16 14:12:39,802] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-30[0m
[32m[2022-09-16 14:12:39,802] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 14:12:42,627] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-30/tokenizer_config.json[0m
[32m[2022-09-16 14:12:42,627] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-30/special_tokens_map.json[0m
[32m[2022-09-16 14:12:50,044] [    INFO][0m - Deleting older checkpoint [checkpoints_cluewsc/checkpoint-10] due to args.save_total_limit[0m
[32m[2022-09-16 14:12:57,767] [    INFO][0m - loss: 0.68414063, learning_rate: 2.6e-06, global_step: 40, interval_runtime: 24.0398, interval_samples_per_second: 0.666, interval_steps_per_second: 0.416, epoch: 4.0[0m
[32m[2022-09-16 14:12:57,768] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 14:12:57,768] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 14:12:57,768] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-16 14:12:57,768] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-16 14:12:57,768] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-16 14:13:00,488] [    INFO][0m - eval_loss: 0.8803426027297974, eval_accuracy: 0.5031446540880503, eval_runtime: 2.7195, eval_samples_per_second: 58.467, eval_steps_per_second: 14.709, epoch: 4.0[0m
[32m[2022-09-16 14:13:00,489] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-40[0m
[32m[2022-09-16 14:13:00,489] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 14:13:03,138] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-40/tokenizer_config.json[0m
[32m[2022-09-16 14:13:03,139] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-40/special_tokens_map.json[0m
[32m[2022-09-16 14:13:08,178] [    INFO][0m - Deleting older checkpoint [checkpoints_cluewsc/checkpoint-20] due to args.save_total_limit[0m
[32m[2022-09-16 14:13:15,905] [    INFO][0m - loss: 0.56909156, learning_rate: 2.5e-06, global_step: 50, interval_runtime: 18.1376, interval_samples_per_second: 0.882, interval_steps_per_second: 0.551, epoch: 5.0[0m
[32m[2022-09-16 14:13:15,905] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 14:13:15,906] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 14:13:15,906] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-16 14:13:15,906] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-16 14:13:15,906] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-16 14:13:21,533] [    INFO][0m - eval_loss: 0.8620636463165283, eval_accuracy: 0.5283018867924528, eval_runtime: 2.7114, eval_samples_per_second: 58.642, eval_steps_per_second: 14.753, epoch: 5.0[0m
[32m[2022-09-16 14:13:21,533] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-50[0m
[32m[2022-09-16 14:13:21,533] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 14:13:24,219] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-50/tokenizer_config.json[0m
[32m[2022-09-16 14:13:24,219] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-50/special_tokens_map.json[0m
[32m[2022-09-16 14:13:29,246] [    INFO][0m - Deleting older checkpoint [checkpoints_cluewsc/checkpoint-40] due to args.save_total_limit[0m
[32m[2022-09-16 14:13:37,013] [    INFO][0m - loss: 0.51951685, learning_rate: 2.4000000000000003e-06, global_step: 60, interval_runtime: 21.1078, interval_samples_per_second: 0.758, interval_steps_per_second: 0.474, epoch: 6.0[0m
[32m[2022-09-16 14:13:37,013] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 14:13:37,014] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 14:13:37,014] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-16 14:13:37,014] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-16 14:13:37,014] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-16 14:13:39,732] [    INFO][0m - eval_loss: 0.88298100233078, eval_accuracy: 0.5408805031446541, eval_runtime: 2.718, eval_samples_per_second: 58.498, eval_steps_per_second: 14.716, epoch: 6.0[0m
[32m[2022-09-16 14:13:39,732] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-60[0m
[32m[2022-09-16 14:13:39,733] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 14:13:42,384] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-60/tokenizer_config.json[0m
[32m[2022-09-16 14:13:42,384] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-60/special_tokens_map.json[0m
[32m[2022-09-16 14:13:47,397] [    INFO][0m - Deleting older checkpoint [checkpoints_cluewsc/checkpoint-50] due to args.save_total_limit[0m
[32m[2022-09-16 14:13:55,186] [    INFO][0m - loss: 0.47990355, learning_rate: 2.3000000000000004e-06, global_step: 70, interval_runtime: 18.1734, interval_samples_per_second: 0.88, interval_steps_per_second: 0.55, epoch: 7.0[0m
[32m[2022-09-16 14:13:55,186] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 14:13:55,186] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 14:13:55,186] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-16 14:13:55,187] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-16 14:13:55,187] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-16 14:13:57,905] [    INFO][0m - eval_loss: 0.8952625393867493, eval_accuracy: 0.5345911949685535, eval_runtime: 2.7179, eval_samples_per_second: 58.502, eval_steps_per_second: 14.717, epoch: 7.0[0m
[32m[2022-09-16 14:13:57,905] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-70[0m
[32m[2022-09-16 14:13:57,905] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 14:14:00,550] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-70/tokenizer_config.json[0m
[32m[2022-09-16 14:14:00,550] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-70/special_tokens_map.json[0m
[32m[2022-09-16 14:14:09,102] [    INFO][0m - Deleting older checkpoint [checkpoints_cluewsc/checkpoint-60] due to args.save_total_limit[0m
[32m[2022-09-16 14:14:18,039] [    INFO][0m - loss: 0.44286494, learning_rate: 2.1999999999999997e-06, global_step: 80, interval_runtime: 22.8534, interval_samples_per_second: 0.7, interval_steps_per_second: 0.438, epoch: 8.0[0m
[32m[2022-09-16 14:14:18,040] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 14:14:18,040] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 14:14:18,040] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-16 14:14:18,040] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-16 14:14:18,040] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-16 14:14:20,739] [    INFO][0m - eval_loss: 0.9497636556625366, eval_accuracy: 0.5408805031446541, eval_runtime: 2.6986, eval_samples_per_second: 58.92, eval_steps_per_second: 14.823, epoch: 8.0[0m
[32m[2022-09-16 14:14:20,740] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-80[0m
[32m[2022-09-16 14:14:20,740] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 14:14:23,425] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-80/tokenizer_config.json[0m
[32m[2022-09-16 14:14:23,426] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-80/special_tokens_map.json[0m
[32m[2022-09-16 14:14:28,383] [    INFO][0m - Deleting older checkpoint [checkpoints_cluewsc/checkpoint-70] due to args.save_total_limit[0m
[32m[2022-09-16 14:14:36,187] [    INFO][0m - loss: 0.3813607, learning_rate: 2.1e-06, global_step: 90, interval_runtime: 18.1474, interval_samples_per_second: 0.882, interval_steps_per_second: 0.551, epoch: 9.0[0m
[32m[2022-09-16 14:14:36,188] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 14:14:36,188] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 14:14:36,188] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-16 14:14:36,188] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-16 14:14:36,188] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-16 14:14:38,905] [    INFO][0m - eval_loss: 1.0398722887039185, eval_accuracy: 0.5534591194968553, eval_runtime: 2.7167, eval_samples_per_second: 58.527, eval_steps_per_second: 14.724, epoch: 9.0[0m
[32m[2022-09-16 14:14:38,905] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-90[0m
[32m[2022-09-16 14:14:38,906] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 14:14:41,535] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-90/tokenizer_config.json[0m
[32m[2022-09-16 14:14:41,535] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-90/special_tokens_map.json[0m
[32m[2022-09-16 14:14:46,497] [    INFO][0m - Deleting older checkpoint [checkpoints_cluewsc/checkpoint-80] due to args.save_total_limit[0m
[32m[2022-09-16 14:15:05,580] [    INFO][0m - loss: 0.31680768, learning_rate: 2e-06, global_step: 100, interval_runtime: 29.3934, interval_samples_per_second: 0.544, interval_steps_per_second: 0.34, epoch: 10.0[0m
[32m[2022-09-16 14:15:05,581] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 14:15:05,581] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 14:15:05,581] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-16 14:15:05,581] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-16 14:15:05,582] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-16 14:15:08,301] [    INFO][0m - eval_loss: 1.0799018144607544, eval_accuracy: 0.559748427672956, eval_runtime: 2.7188, eval_samples_per_second: 58.481, eval_steps_per_second: 14.712, epoch: 10.0[0m
[32m[2022-09-16 14:15:08,301] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-100[0m
[32m[2022-09-16 14:15:08,302] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 14:15:11,157] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-16 14:15:11,157] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-16 14:15:16,203] [    INFO][0m - Deleting older checkpoint [checkpoints_cluewsc/checkpoint-30] due to args.save_total_limit[0m
[32m[2022-09-16 14:15:23,924] [    INFO][0m - loss: 0.27004542, learning_rate: 1.9e-06, global_step: 110, interval_runtime: 18.3434, interval_samples_per_second: 0.872, interval_steps_per_second: 0.545, epoch: 11.0[0m
[32m[2022-09-16 14:15:23,925] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 14:15:23,925] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 14:15:23,925] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-16 14:15:23,925] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-16 14:15:23,925] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-16 14:15:26,632] [    INFO][0m - eval_loss: 1.1339740753173828, eval_accuracy: 0.5660377358490566, eval_runtime: 2.7069, eval_samples_per_second: 58.738, eval_steps_per_second: 14.777, epoch: 11.0[0m
[32m[2022-09-16 14:15:26,632] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-110[0m
[32m[2022-09-16 14:15:26,633] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 14:15:29,331] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-110/tokenizer_config.json[0m
[32m[2022-09-16 14:15:29,332] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-110/special_tokens_map.json[0m
[32m[2022-09-16 14:15:35,161] [    INFO][0m - Deleting older checkpoint [checkpoints_cluewsc/checkpoint-90] due to args.save_total_limit[0m
[32m[2022-09-16 14:15:42,856] [    INFO][0m - loss: 0.23267307, learning_rate: 1.8e-06, global_step: 120, interval_runtime: 18.9325, interval_samples_per_second: 0.845, interval_steps_per_second: 0.528, epoch: 12.0[0m
[32m[2022-09-16 14:15:42,857] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 14:15:42,857] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 14:15:42,857] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-16 14:15:42,857] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-16 14:15:42,857] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-16 14:15:45,570] [    INFO][0m - eval_loss: 1.2252440452575684, eval_accuracy: 0.5723270440251572, eval_runtime: 2.7122, eval_samples_per_second: 58.623, eval_steps_per_second: 14.748, epoch: 12.0[0m
[32m[2022-09-16 14:15:45,570] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-120[0m
[32m[2022-09-16 14:15:45,570] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 14:15:48,231] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-120/tokenizer_config.json[0m
[32m[2022-09-16 14:15:48,344] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-120/special_tokens_map.json[0m
[32m[2022-09-16 14:15:53,422] [    INFO][0m - Deleting older checkpoint [checkpoints_cluewsc/checkpoint-100] due to args.save_total_limit[0m
[32m[2022-09-16 14:16:01,124] [    INFO][0m - loss: 0.17552406, learning_rate: 1.7e-06, global_step: 130, interval_runtime: 18.2673, interval_samples_per_second: 0.876, interval_steps_per_second: 0.547, epoch: 13.0[0m
[32m[2022-09-16 14:16:01,124] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 14:16:01,125] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 14:16:01,125] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-16 14:16:01,125] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-16 14:16:01,125] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-16 14:16:03,847] [    INFO][0m - eval_loss: 1.3468143939971924, eval_accuracy: 0.5723270440251572, eval_runtime: 2.7219, eval_samples_per_second: 58.415, eval_steps_per_second: 14.696, epoch: 13.0[0m
[32m[2022-09-16 14:16:03,847] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-130[0m
[32m[2022-09-16 14:16:03,847] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 14:16:06,485] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-130/tokenizer_config.json[0m
[32m[2022-09-16 14:16:06,486] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-130/special_tokens_map.json[0m
[32m[2022-09-16 14:16:11,488] [    INFO][0m - Deleting older checkpoint [checkpoints_cluewsc/checkpoint-110] due to args.save_total_limit[0m
[32m[2022-09-16 14:16:19,723] [    INFO][0m - loss: 0.20177698, learning_rate: 1.6e-06, global_step: 140, interval_runtime: 18.5989, interval_samples_per_second: 0.86, interval_steps_per_second: 0.538, epoch: 14.0[0m
[32m[2022-09-16 14:16:19,723] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 14:16:19,723] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 14:16:19,724] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-16 14:16:19,724] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-16 14:16:19,724] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-16 14:16:22,444] [    INFO][0m - eval_loss: 1.4228147268295288, eval_accuracy: 0.5849056603773585, eval_runtime: 2.7195, eval_samples_per_second: 58.466, eval_steps_per_second: 14.708, epoch: 14.0[0m
[32m[2022-09-16 14:16:22,444] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-140[0m
[32m[2022-09-16 14:16:22,444] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 14:16:25,188] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-140/tokenizer_config.json[0m
[32m[2022-09-16 14:16:26,975] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-140/special_tokens_map.json[0m
[32m[2022-09-16 14:16:32,150] [    INFO][0m - Deleting older checkpoint [checkpoints_cluewsc/checkpoint-120] due to args.save_total_limit[0m
[32m[2022-09-16 14:16:39,829] [    INFO][0m - loss: 0.13132333, learning_rate: 1.5e-06, global_step: 150, interval_runtime: 20.1062, interval_samples_per_second: 0.796, interval_steps_per_second: 0.497, epoch: 15.0[0m
[32m[2022-09-16 14:16:39,830] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 14:16:39,830] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 14:16:39,830] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-16 14:16:39,830] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-16 14:16:39,830] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-16 14:16:42,930] [    INFO][0m - eval_loss: 1.4893723726272583, eval_accuracy: 0.5849056603773585, eval_runtime: 2.7212, eval_samples_per_second: 58.43, eval_steps_per_second: 14.699, epoch: 15.0[0m
[32m[2022-09-16 14:16:42,930] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-150[0m
[32m[2022-09-16 14:16:42,930] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 14:16:45,554] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-150/tokenizer_config.json[0m
[32m[2022-09-16 14:16:45,554] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-150/special_tokens_map.json[0m
[32m[2022-09-16 14:16:50,668] [    INFO][0m - Deleting older checkpoint [checkpoints_cluewsc/checkpoint-130] due to args.save_total_limit[0m
[32m[2022-09-16 14:16:58,336] [    INFO][0m - loss: 0.09608927, learning_rate: 1.4000000000000001e-06, global_step: 160, interval_runtime: 18.5067, interval_samples_per_second: 0.865, interval_steps_per_second: 0.54, epoch: 16.0[0m
[32m[2022-09-16 14:16:58,336] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 14:16:58,337] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 14:16:58,337] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-16 14:16:58,337] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-16 14:16:58,337] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-16 14:17:01,334] [    INFO][0m - eval_loss: 1.5645586252212524, eval_accuracy: 0.5974842767295597, eval_runtime: 2.7441, eval_samples_per_second: 57.943, eval_steps_per_second: 14.577, epoch: 16.0[0m
[32m[2022-09-16 14:17:01,335] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-160[0m
[32m[2022-09-16 14:17:01,335] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 14:17:04,109] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-160/tokenizer_config.json[0m
[32m[2022-09-16 14:17:04,110] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-160/special_tokens_map.json[0m
[32m[2022-09-16 14:17:09,237] [    INFO][0m - Deleting older checkpoint [checkpoints_cluewsc/checkpoint-140] due to args.save_total_limit[0m
[32m[2022-09-16 14:17:16,935] [    INFO][0m - loss: 0.08299471, learning_rate: 1.3e-06, global_step: 170, interval_runtime: 18.6, interval_samples_per_second: 0.86, interval_steps_per_second: 0.538, epoch: 17.0[0m
[32m[2022-09-16 14:17:16,936] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 14:17:16,936] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 14:17:16,936] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-16 14:17:16,937] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-16 14:17:16,937] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-16 14:17:19,638] [    INFO][0m - eval_loss: 1.67290198802948, eval_accuracy: 0.5660377358490566, eval_runtime: 2.701, eval_samples_per_second: 58.867, eval_steps_per_second: 14.809, epoch: 17.0[0m
[32m[2022-09-16 14:17:19,638] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-170[0m
[32m[2022-09-16 14:17:19,638] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 14:17:26,945] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-170/tokenizer_config.json[0m
[32m[2022-09-16 14:17:26,946] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-170/special_tokens_map.json[0m
[32m[2022-09-16 14:17:32,105] [    INFO][0m - Deleting older checkpoint [checkpoints_cluewsc/checkpoint-150] due to args.save_total_limit[0m
[32m[2022-09-16 14:17:39,768] [    INFO][0m - loss: 0.0730212, learning_rate: 1.2000000000000002e-06, global_step: 180, interval_runtime: 22.8325, interval_samples_per_second: 0.701, interval_steps_per_second: 0.438, epoch: 18.0[0m
[32m[2022-09-16 14:17:39,768] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 14:17:39,768] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 14:17:39,768] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-16 14:17:39,768] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-16 14:17:39,769] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-16 14:17:42,472] [    INFO][0m - eval_loss: 1.7954208850860596, eval_accuracy: 0.5786163522012578, eval_runtime: 2.7035, eval_samples_per_second: 58.812, eval_steps_per_second: 14.795, epoch: 18.0[0m
[32m[2022-09-16 14:17:46,883] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-180[0m
[32m[2022-09-16 14:17:46,884] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 14:17:49,792] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-180/tokenizer_config.json[0m
[32m[2022-09-16 14:17:49,793] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-180/special_tokens_map.json[0m
[32m[2022-09-16 14:17:55,472] [    INFO][0m - Deleting older checkpoint [checkpoints_cluewsc/checkpoint-170] due to args.save_total_limit[0m
[32m[2022-09-16 14:18:03,576] [    INFO][0m - loss: 0.07185417, learning_rate: 1.0999999999999998e-06, global_step: 190, interval_runtime: 23.8082, interval_samples_per_second: 0.672, interval_steps_per_second: 0.42, epoch: 19.0[0m
[32m[2022-09-16 14:18:03,577] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 14:18:03,577] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 14:18:03,577] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-16 14:18:03,577] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-16 14:18:03,577] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-16 14:18:06,281] [    INFO][0m - eval_loss: 1.942370891571045, eval_accuracy: 0.5786163522012578, eval_runtime: 2.7029, eval_samples_per_second: 58.825, eval_steps_per_second: 14.799, epoch: 19.0[0m
[32m[2022-09-16 14:18:07,212] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-190[0m
[32m[2022-09-16 14:18:07,212] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 14:18:09,849] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-190/tokenizer_config.json[0m
[32m[2022-09-16 14:18:09,850] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-190/special_tokens_map.json[0m
[32m[2022-09-16 14:18:15,101] [    INFO][0m - Deleting older checkpoint [checkpoints_cluewsc/checkpoint-180] due to args.save_total_limit[0m
[32m[2022-09-16 14:18:22,847] [    INFO][0m - loss: 0.04401397, learning_rate: 1e-06, global_step: 200, interval_runtime: 19.271, interval_samples_per_second: 0.83, interval_steps_per_second: 0.519, epoch: 20.0[0m
[32m[2022-09-16 14:18:22,848] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 14:18:22,848] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 14:18:22,848] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-16 14:18:22,848] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-16 14:18:22,848] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-16 14:18:25,553] [    INFO][0m - eval_loss: 2.0104143619537354, eval_accuracy: 0.5974842767295597, eval_runtime: 2.7049, eval_samples_per_second: 58.783, eval_steps_per_second: 14.788, epoch: 20.0[0m
[32m[2022-09-16 14:18:25,554] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-200[0m
[32m[2022-09-16 14:18:25,554] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 14:18:28,327] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-200/tokenizer_config.json[0m
[32m[2022-09-16 14:18:28,328] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-200/special_tokens_map.json[0m
[32m[2022-09-16 14:18:33,346] [    INFO][0m - Deleting older checkpoint [checkpoints_cluewsc/checkpoint-190] due to args.save_total_limit[0m
[32m[2022-09-16 14:18:41,101] [    INFO][0m - loss: 0.03344452, learning_rate: 9e-07, global_step: 210, interval_runtime: 18.2541, interval_samples_per_second: 0.877, interval_steps_per_second: 0.548, epoch: 21.0[0m
[32m[2022-09-16 14:18:41,102] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 14:18:41,102] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 14:18:41,102] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-16 14:18:41,103] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-16 14:18:41,103] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-16 14:18:43,800] [    INFO][0m - eval_loss: 2.0950257778167725, eval_accuracy: 0.6163522012578616, eval_runtime: 2.6972, eval_samples_per_second: 58.95, eval_steps_per_second: 14.83, epoch: 21.0[0m
[32m[2022-09-16 14:18:43,801] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-210[0m
[32m[2022-09-16 14:18:43,801] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 14:18:46,503] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-210/tokenizer_config.json[0m
[32m[2022-09-16 14:18:46,503] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-210/special_tokens_map.json[0m
[32m[2022-09-16 14:18:51,534] [    INFO][0m - Deleting older checkpoint [checkpoints_cluewsc/checkpoint-160] due to args.save_total_limit[0m
[32m[2022-09-16 14:18:59,230] [    INFO][0m - loss: 0.02471997, learning_rate: 8e-07, global_step: 220, interval_runtime: 18.1291, interval_samples_per_second: 0.883, interval_steps_per_second: 0.552, epoch: 22.0[0m
[32m[2022-09-16 14:18:59,231] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 14:18:59,231] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 14:18:59,231] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-16 14:18:59,231] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-16 14:18:59,231] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-16 14:19:01,920] [    INFO][0m - eval_loss: 2.225159168243408, eval_accuracy: 0.6037735849056604, eval_runtime: 2.6879, eval_samples_per_second: 59.154, eval_steps_per_second: 14.882, epoch: 22.0[0m
[32m[2022-09-16 14:19:01,921] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-220[0m
[32m[2022-09-16 14:19:01,921] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 14:19:04,595] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-220/tokenizer_config.json[0m
[32m[2022-09-16 14:19:04,595] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-220/special_tokens_map.json[0m
[32m[2022-09-16 14:19:09,614] [    INFO][0m - Deleting older checkpoint [checkpoints_cluewsc/checkpoint-200] due to args.save_total_limit[0m
[32m[2022-09-16 14:19:17,246] [    INFO][0m - loss: 0.03023683, learning_rate: 7.000000000000001e-07, global_step: 230, interval_runtime: 18.0152, interval_samples_per_second: 0.888, interval_steps_per_second: 0.555, epoch: 23.0[0m
[32m[2022-09-16 14:19:17,247] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 14:19:17,247] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 14:19:17,247] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-16 14:19:17,247] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-16 14:19:17,247] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-16 14:19:19,939] [    INFO][0m - eval_loss: 2.347513437271118, eval_accuracy: 0.5849056603773585, eval_runtime: 2.6921, eval_samples_per_second: 59.062, eval_steps_per_second: 14.858, epoch: 23.0[0m
[32m[2022-09-16 14:19:19,940] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-230[0m
[32m[2022-09-16 14:19:19,940] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 14:19:22,571] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-230/tokenizer_config.json[0m
[32m[2022-09-16 14:19:22,571] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-230/special_tokens_map.json[0m
[32m[2022-09-16 14:19:27,629] [    INFO][0m - Deleting older checkpoint [checkpoints_cluewsc/checkpoint-220] due to args.save_total_limit[0m
[32m[2022-09-16 14:19:35,370] [    INFO][0m - loss: 0.03583437, learning_rate: 6.000000000000001e-07, global_step: 240, interval_runtime: 18.1251, interval_samples_per_second: 0.883, interval_steps_per_second: 0.552, epoch: 24.0[0m
[32m[2022-09-16 14:19:35,371] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 14:19:35,371] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 14:19:35,371] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-16 14:19:35,371] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-16 14:19:35,371] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-16 14:19:38,065] [    INFO][0m - eval_loss: 2.4334776401519775, eval_accuracy: 0.5849056603773585, eval_runtime: 2.693, eval_samples_per_second: 59.042, eval_steps_per_second: 14.853, epoch: 24.0[0m
[32m[2022-09-16 14:19:38,065] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-240[0m
[32m[2022-09-16 14:19:38,065] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 14:19:40,652] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-240/tokenizer_config.json[0m
[32m[2022-09-16 14:19:40,652] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-240/special_tokens_map.json[0m
[32m[2022-09-16 14:19:45,732] [    INFO][0m - Deleting older checkpoint [checkpoints_cluewsc/checkpoint-230] due to args.save_total_limit[0m
[32m[2022-09-16 14:19:53,541] [    INFO][0m - loss: 0.01830749, learning_rate: 5e-07, global_step: 250, interval_runtime: 18.1705, interval_samples_per_second: 0.881, interval_steps_per_second: 0.55, epoch: 25.0[0m
[32m[2022-09-16 14:19:53,542] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 14:19:53,542] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 14:19:53,542] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-16 14:19:53,543] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-16 14:19:53,543] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-16 14:19:56,251] [    INFO][0m - eval_loss: 2.4348483085632324, eval_accuracy: 0.5974842767295597, eval_runtime: 2.7077, eval_samples_per_second: 58.721, eval_steps_per_second: 14.772, epoch: 25.0[0m
[32m[2022-09-16 14:19:56,251] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-250[0m
[32m[2022-09-16 14:19:56,252] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 14:19:59,030] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-250/tokenizer_config.json[0m
[32m[2022-09-16 14:19:59,031] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-250/special_tokens_map.json[0m
[32m[2022-09-16 14:20:04,324] [    INFO][0m - Deleting older checkpoint [checkpoints_cluewsc/checkpoint-240] due to args.save_total_limit[0m
[32m[2022-09-16 14:20:14,253] [    INFO][0m - loss: 0.02662122, learning_rate: 4e-07, global_step: 260, interval_runtime: 20.712, interval_samples_per_second: 0.773, interval_steps_per_second: 0.483, epoch: 26.0[0m
[32m[2022-09-16 14:20:14,254] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 14:20:14,254] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 14:20:14,254] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-16 14:20:14,255] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-16 14:20:14,255] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-16 14:20:16,951] [    INFO][0m - eval_loss: 2.4887895584106445, eval_accuracy: 0.6163522012578616, eval_runtime: 2.696, eval_samples_per_second: 58.976, eval_steps_per_second: 14.837, epoch: 26.0[0m
[32m[2022-09-16 14:20:16,951] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-260[0m
[32m[2022-09-16 14:20:16,951] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 14:20:20,028] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-260/tokenizer_config.json[0m
[32m[2022-09-16 14:20:20,028] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-260/special_tokens_map.json[0m
[32m[2022-09-16 14:20:26,880] [    INFO][0m - Deleting older checkpoint [checkpoints_cluewsc/checkpoint-250] due to args.save_total_limit[0m
[32m[2022-09-16 14:20:34,694] [    INFO][0m - loss: 0.01037654, learning_rate: 3.0000000000000004e-07, global_step: 270, interval_runtime: 20.4409, interval_samples_per_second: 0.783, interval_steps_per_second: 0.489, epoch: 27.0[0m
[32m[2022-09-16 14:20:34,695] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 14:20:34,695] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 14:20:34,695] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-16 14:20:34,695] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-16 14:20:34,696] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-16 14:20:37,424] [    INFO][0m - eval_loss: 2.5373194217681885, eval_accuracy: 0.610062893081761, eval_runtime: 2.7283, eval_samples_per_second: 58.277, eval_steps_per_second: 14.661, epoch: 27.0[0m
[32m[2022-09-16 14:20:37,425] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-270[0m
[32m[2022-09-16 14:20:37,425] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 14:20:40,161] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-270/tokenizer_config.json[0m
[32m[2022-09-16 14:20:40,161] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-270/special_tokens_map.json[0m
[32m[2022-09-16 14:20:48,850] [    INFO][0m - Deleting older checkpoint [checkpoints_cluewsc/checkpoint-260] due to args.save_total_limit[0m
[32m[2022-09-16 14:20:58,931] [    INFO][0m - loss: 0.00871957, learning_rate: 2e-07, global_step: 280, interval_runtime: 24.2366, interval_samples_per_second: 0.66, interval_steps_per_second: 0.413, epoch: 28.0[0m
[32m[2022-09-16 14:20:58,932] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 14:20:58,932] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 14:20:58,932] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-16 14:20:58,932] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-16 14:20:58,932] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-16 14:21:01,625] [    INFO][0m - eval_loss: 2.56429386138916, eval_accuracy: 0.6163522012578616, eval_runtime: 2.6927, eval_samples_per_second: 59.048, eval_steps_per_second: 14.855, epoch: 28.0[0m
[32m[2022-09-16 14:21:01,626] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-280[0m
[32m[2022-09-16 14:21:01,626] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 14:21:04,309] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-280/tokenizer_config.json[0m
[32m[2022-09-16 14:21:04,310] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-280/special_tokens_map.json[0m
[32m[2022-09-16 14:21:09,310] [    INFO][0m - Deleting older checkpoint [checkpoints_cluewsc/checkpoint-270] due to args.save_total_limit[0m
[32m[2022-09-16 14:21:17,035] [    INFO][0m - loss: 0.00844597, learning_rate: 1e-07, global_step: 290, interval_runtime: 18.1045, interval_samples_per_second: 0.884, interval_steps_per_second: 0.552, epoch: 29.0[0m
[32m[2022-09-16 14:21:17,036] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 14:21:17,036] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 14:21:17,036] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-16 14:21:17,036] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-16 14:21:17,036] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-16 14:21:19,737] [    INFO][0m - eval_loss: 2.5853962898254395, eval_accuracy: 0.6163522012578616, eval_runtime: 2.7001, eval_samples_per_second: 58.887, eval_steps_per_second: 14.814, epoch: 29.0[0m
[32m[2022-09-16 14:21:19,737] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-290[0m
[32m[2022-09-16 14:21:19,737] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 14:21:22,801] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-290/tokenizer_config.json[0m
[32m[2022-09-16 14:21:22,802] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-290/special_tokens_map.json[0m
[32m[2022-09-16 14:21:30,099] [    INFO][0m - Deleting older checkpoint [checkpoints_cluewsc/checkpoint-280] due to args.save_total_limit[0m
[32m[2022-09-16 14:21:41,539] [    INFO][0m - loss: 0.00498474, learning_rate: 0.0, global_step: 300, interval_runtime: 24.5035, interval_samples_per_second: 0.653, interval_steps_per_second: 0.408, epoch: 30.0[0m
[32m[2022-09-16 14:21:41,539] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 14:21:41,540] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-16 14:21:41,540] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-16 14:21:41,540] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-16 14:21:41,540] [    INFO][0m -   Total prediction steps = 40[0m
[32m[2022-09-16 14:21:44,238] [    INFO][0m - eval_loss: 2.591264486312866, eval_accuracy: 0.6163522012578616, eval_runtime: 2.6982, eval_samples_per_second: 58.929, eval_steps_per_second: 14.825, epoch: 30.0[0m
[32m[2022-09-16 14:21:44,239] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-300[0m
[32m[2022-09-16 14:21:44,239] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 14:21:47,025] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-300/tokenizer_config.json[0m
[32m[2022-09-16 14:21:47,026] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-300/special_tokens_map.json[0m
[32m[2022-09-16 14:21:52,065] [    INFO][0m - Deleting older checkpoint [checkpoints_cluewsc/checkpoint-290] due to args.save_total_limit[0m
[32m[2022-09-16 14:21:53,322] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-16 14:21:53,322] [    INFO][0m - Loading best model from ./checkpoints_cluewsc/checkpoint-210 (score: 0.6163522012578616).[0m
[32m[2022-09-16 14:21:54,907] [    INFO][0m - train_runtime: 606.6116, train_samples_per_second: 7.913, train_steps_per_second: 0.495, train_loss: 0.3384161827713251, epoch: 30.0[0m
[32m[2022-09-16 14:21:54,909] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/[0m
[32m[2022-09-16 14:21:54,910] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 14:21:57,823] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/tokenizer_config.json[0m
[32m[2022-09-16 14:21:57,823] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/special_tokens_map.json[0m
[32m[2022-09-16 14:21:57,825] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-16 14:21:57,825] [    INFO][0m -   epoch                    =       30.0[0m
[32m[2022-09-16 14:21:57,825] [    INFO][0m -   train_loss               =     0.3384[0m
[32m[2022-09-16 14:21:57,825] [    INFO][0m -   train_runtime            = 0:10:06.61[0m
[32m[2022-09-16 14:21:57,825] [    INFO][0m -   train_samples_per_second =      7.913[0m
[32m[2022-09-16 14:21:57,825] [    INFO][0m -   train_steps_per_second   =      0.495[0m
[32m[2022-09-16 14:21:57,828] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-16 14:21:57,829] [    INFO][0m -   Num examples = 976[0m
[32m[2022-09-16 14:21:57,829] [    INFO][0m -   Pre device batch size = 4[0m
[32m[2022-09-16 14:21:57,829] [    INFO][0m -   Total Batch size = 4[0m
[32m[2022-09-16 14:21:57,829] [    INFO][0m -   Total prediction steps = 244[0m
[32m[2022-09-16 14:22:14,475] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-16 14:22:14,476] [    INFO][0m -   test_accuracy           =     0.6014[0m
[32m[2022-09-16 14:22:14,476] [    INFO][0m -   test_loss               =      2.081[0m
[32m[2022-09-16 14:22:14,476] [    INFO][0m -   test_runtime            = 0:00:16.64[0m
[32m[2022-09-16 14:22:14,476] [    INFO][0m -   test_samples_per_second =     58.631[0m
[32m[2022-09-16 14:22:14,476] [    INFO][0m -   test_steps_per_second   =     14.658[0m
{
  "labels": 0,
  "text_a": "\u4e3a\u4ec0\u4e48\u8981\u51fa\u73b0\u4e00\u4e2a\u8eab\u7a7f\u519b\u88c5\u7684\u9ad8\u5927\u7537\u4eba\uff1f\u5c31\u50cf\u4e00\u7247\u6811\u53f6\u98d8\u5165\u4e86\u6811\u6797\uff0c\u4ed6\u8d70\u5230\u4e86\u6211\u7684\u5bb6\u4eba\u4e2d\u95f4\u3002",
  "text_b": "\u5176\u4e2d\u4ed6\u6307\u7684\u662f\u6811\u53f6"
}

