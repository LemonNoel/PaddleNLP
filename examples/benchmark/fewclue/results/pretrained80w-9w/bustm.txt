[32m[2022-09-16 13:28:02,479] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-16 13:28:02,480] [    INFO][0m - ============================================================[0m
[32m[2022-09-16 13:28:02,480] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-16 13:28:02,480] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-16 13:28:02,480] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-16 13:28:02,480] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-16 13:28:02,480] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-16 13:28:02,480] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-16 13:28:02,480] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-16 13:28:02,480] [    INFO][0m - [0m
[32m[2022-09-16 13:28:02,480] [    INFO][0m - ============================================================[0m
[32m[2022-09-16 13:28:02,480] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-16 13:28:02,480] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-16 13:28:02,481] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-16 13:28:02,481] [    INFO][0m - pretrained                    :/ssd2/wanghuijuan03/data/zero-shot/checkpoints_0915/checkpoint-90000/model_state.pdparams[0m
[32m[2022-09-16 13:28:02,481] [    INFO][0m - prompt                        :‚Äú{'text':'text_a'}‚ÄùÂíå‚Äú{'text':'text_b'}‚Äù‰∏§‰∏™ÈóÆÈ¢òËØ≠‰πâ{'mask'}{'mask'}Âå∫Âà´„ÄÇÈÄâÈ°πÔºöÁï•Êúâ/ÊØ´Êó†[0m
[32m[2022-09-16 13:28:02,481] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-16 13:28:02,481] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-16 13:28:02,481] [    INFO][0m - task_name                     :bustm[0m
[32m[2022-09-16 13:28:02,481] [    INFO][0m - [0m
[32m[2022-09-16 13:28:02,481] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0916 13:28:02.483213 57554 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0916 13:28:02.487082 57554 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-16 13:28:08,322] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-16 13:28:08,334] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-16 13:28:08,335] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-16 13:28:10,220] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùÂíå‚Äú'}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': '‚Äù‰∏§‰∏™ÈóÆÈ¢òËØ≠‰πâ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'Âå∫Âà´„ÄÇÈÄâÈ°πÔºöÁï•Êúâ/ÊØ´Êó†'}][0m
[32m[2022-09-16 13:28:10,330] [    INFO][0m - ============================================================[0m
[32m[2022-09-16 13:28:10,330] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-16 13:28:10,330] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-16 13:28:10,330] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-16 13:28:10,330] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-16 13:28:10,331] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-16 13:28:10,331] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-16 13:28:10,331] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-16 13:28:10,331] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-16 13:28:10,331] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-16 13:28:10,331] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-16 13:28:10,331] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-16 13:28:10,331] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-16 13:28:10,331] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-16 13:28:10,331] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-16 13:28:10,331] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-16 13:28:10,331] [    INFO][0m - do_predict                    :False[0m
[32m[2022-09-16 13:28:10,332] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-16 13:28:10,332] [    INFO][0m - eval_batch_size               :16[0m
[32m[2022-09-16 13:28:10,332] [    INFO][0m - eval_steps                    :200[0m
[32m[2022-09-16 13:28:10,332] [    INFO][0m - evaluation_strategy           :IntervalStrategy.EPOCH[0m
[32m[2022-09-16 13:28:10,332] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-16 13:28:10,332] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-16 13:28:10,332] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-16 13:28:10,332] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-16 13:28:10,332] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-16 13:28:10,332] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-16 13:28:10,332] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-16 13:28:10,332] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-16 13:28:10,332] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-16 13:28:10,333] [    INFO][0m - learning_rate                 :3e-06[0m
[32m[2022-09-16 13:28:10,333] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-16 13:28:10,333] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-16 13:28:10,333] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-16 13:28:10,333] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-16 13:28:10,333] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-16 13:28:10,333] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-16 13:28:10,333] [    INFO][0m - logging_dir                   :./checkpoints_bustm/runs/Sep16_13-28-02_instance-3bwob41y-01[0m
[32m[2022-09-16 13:28:10,333] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-16 13:28:10,333] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-16 13:28:10,333] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-16 13:28:10,333] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-16 13:28:10,333] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-16 13:28:10,334] [    INFO][0m - max_seq_length                :128[0m
[32m[2022-09-16 13:28:10,334] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-16 13:28:10,334] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-16 13:28:10,334] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-16 13:28:10,334] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-16 13:28:10,334] [    INFO][0m - num_train_epochs              :30.0[0m
[32m[2022-09-16 13:28:10,334] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-16 13:28:10,334] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-16 13:28:10,334] [    INFO][0m - output_dir                    :./checkpoints_bustm/[0m
[32m[2022-09-16 13:28:10,334] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-16 13:28:10,334] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-16 13:28:10,334] [    INFO][0m - per_device_eval_batch_size    :16[0m
[32m[2022-09-16 13:28:10,334] [    INFO][0m - per_device_train_batch_size   :16[0m
[32m[2022-09-16 13:28:10,334] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-16 13:28:10,335] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-16 13:28:10,335] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-16 13:28:10,335] [    INFO][0m - ppt_learning_rate             :3e-05[0m
[32m[2022-09-16 13:28:10,335] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-16 13:28:10,335] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-16 13:28:10,335] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-16 13:28:10,335] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-16 13:28:10,335] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-16 13:28:10,335] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-16 13:28:10,335] [    INFO][0m - run_name                      :./checkpoints_bustm/[0m
[32m[2022-09-16 13:28:10,335] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-16 13:28:10,335] [    INFO][0m - save_steps                    :200[0m
[32m[2022-09-16 13:28:10,335] [    INFO][0m - save_strategy                 :IntervalStrategy.EPOCH[0m
[32m[2022-09-16 13:28:10,336] [    INFO][0m - save_total_limit              :1[0m
[32m[2022-09-16 13:28:10,336] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-16 13:28:10,336] [    INFO][0m - seed                          :42[0m
[32m[2022-09-16 13:28:10,336] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-16 13:28:10,336] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-16 13:28:10,336] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-16 13:28:10,336] [    INFO][0m - train_batch_size              :16[0m
[32m[2022-09-16 13:28:10,336] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-16 13:28:10,336] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-16 13:28:10,336] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-16 13:28:10,336] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-16 13:28:10,336] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-16 13:28:10,336] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-16 13:28:10,337] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-16 13:28:10,337] [    INFO][0m - [0m
[32m[2022-09-16 13:28:10,339] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-16 13:28:10,339] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:28:10,339] [    INFO][0m -   Num Epochs = 30[0m
[32m[2022-09-16 13:28:10,340] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2022-09-16 13:28:10,340] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2022-09-16 13:28:10,340] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-16 13:28:10,340] [    INFO][0m -   Total optimization steps = 300.0[0m
[32m[2022-09-16 13:28:10,340] [    INFO][0m -   Total num train samples = 4800[0m
[32m[2022-09-16 13:28:15,019] [    INFO][0m - loss: 0.57981129, learning_rate: 2.9e-06, global_step: 10, interval_runtime: 4.6788, interval_samples_per_second: 3.42, interval_steps_per_second: 2.137, epoch: 1.0[0m
[32m[2022-09-16 13:28:15,020] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:28:15,020] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:28:15,020] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:28:15,020] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:28:15,021] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:28:16,333] [    INFO][0m - eval_loss: 0.6439114212989807, eval_accuracy: 0.7625, eval_runtime: 1.3116, eval_samples_per_second: 121.989, eval_steps_per_second: 7.624, epoch: 1.0[0m
[32m[2022-09-16 13:28:16,333] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-10[0m
[32m[2022-09-16 13:28:16,333] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:28:19,530] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-10/tokenizer_config.json[0m
[32m[2022-09-16 13:28:19,531] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-10/special_tokens_map.json[0m
[32m[2022-09-16 13:28:29,223] [    INFO][0m - loss: 0.40539999, learning_rate: 2.8000000000000003e-06, global_step: 20, interval_runtime: 14.2033, interval_samples_per_second: 1.126, interval_steps_per_second: 0.704, epoch: 2.0[0m
[32m[2022-09-16 13:28:29,223] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:28:29,224] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:28:29,224] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:28:29,224] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:28:29,224] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:28:30,528] [    INFO][0m - eval_loss: 0.6062324643135071, eval_accuracy: 0.775, eval_runtime: 1.3037, eval_samples_per_second: 122.725, eval_steps_per_second: 7.67, epoch: 2.0[0m
[32m[2022-09-16 13:28:30,528] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-20[0m
[32m[2022-09-16 13:28:30,528] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:28:33,949] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-20/tokenizer_config.json[0m
[32m[2022-09-16 13:28:33,949] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-20/special_tokens_map.json[0m
[32m[2022-09-16 13:28:43,470] [    INFO][0m - loss: 0.31685462, learning_rate: 2.7e-06, global_step: 30, interval_runtime: 14.2471, interval_samples_per_second: 1.123, interval_steps_per_second: 0.702, epoch: 3.0[0m
[32m[2022-09-16 13:28:43,471] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:28:43,471] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:28:43,471] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:28:43,471] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:28:43,471] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:28:44,772] [    INFO][0m - eval_loss: 0.5877221822738647, eval_accuracy: 0.775, eval_runtime: 1.3003, eval_samples_per_second: 123.046, eval_steps_per_second: 7.69, epoch: 3.0[0m
[32m[2022-09-16 13:28:44,772] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-30[0m
[32m[2022-09-16 13:28:44,772] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:28:48,207] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-30/tokenizer_config.json[0m
[32m[2022-09-16 13:28:48,207] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-30/special_tokens_map.json[0m
[32m[2022-09-16 13:28:54,136] [    INFO][0m - Deleting older checkpoint [checkpoints_bustm/checkpoint-10] due to args.save_total_limit[0m
[32m[2022-09-16 13:29:09,138] [    INFO][0m - loss: 0.27452159, learning_rate: 2.6e-06, global_step: 40, interval_runtime: 25.6683, interval_samples_per_second: 0.623, interval_steps_per_second: 0.39, epoch: 4.0[0m
[32m[2022-09-16 13:29:09,139] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:29:09,139] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:29:09,139] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:29:09,139] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:29:09,139] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:29:10,444] [    INFO][0m - eval_loss: 0.5843716859817505, eval_accuracy: 0.775, eval_runtime: 1.3048, eval_samples_per_second: 122.624, eval_steps_per_second: 7.664, epoch: 4.0[0m
[32m[2022-09-16 13:29:10,445] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-40[0m
[32m[2022-09-16 13:29:10,445] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:29:13,746] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-40/tokenizer_config.json[0m
[32m[2022-09-16 13:29:13,746] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-40/special_tokens_map.json[0m
[32m[2022-09-16 13:29:20,037] [    INFO][0m - Deleting older checkpoint [checkpoints_bustm/checkpoint-30] due to args.save_total_limit[0m
[32m[2022-09-16 13:29:24,222] [    INFO][0m - loss: 0.20127532, learning_rate: 2.5e-06, global_step: 50, interval_runtime: 15.0839, interval_samples_per_second: 1.061, interval_steps_per_second: 0.663, epoch: 5.0[0m
[32m[2022-09-16 13:29:24,223] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:29:24,223] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:29:24,223] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:29:24,223] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:29:24,223] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:29:25,533] [    INFO][0m - eval_loss: 0.591446578502655, eval_accuracy: 0.76875, eval_runtime: 1.309, eval_samples_per_second: 122.231, eval_steps_per_second: 7.639, epoch: 5.0[0m
[32m[2022-09-16 13:29:25,533] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-50[0m
[32m[2022-09-16 13:29:25,533] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:29:28,613] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-50/tokenizer_config.json[0m
[32m[2022-09-16 13:29:28,614] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-50/special_tokens_map.json[0m
[32m[2022-09-16 13:29:34,842] [    INFO][0m - Deleting older checkpoint [checkpoints_bustm/checkpoint-40] due to args.save_total_limit[0m
[32m[2022-09-16 13:29:38,995] [    INFO][0m - loss: 0.18870525, learning_rate: 2.4000000000000003e-06, global_step: 60, interval_runtime: 14.7728, interval_samples_per_second: 1.083, interval_steps_per_second: 0.677, epoch: 6.0[0m
[32m[2022-09-16 13:29:38,996] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:29:38,996] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:29:38,996] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:29:38,996] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:29:38,996] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:29:40,324] [    INFO][0m - eval_loss: 0.6046616435050964, eval_accuracy: 0.78125, eval_runtime: 1.3276, eval_samples_per_second: 120.517, eval_steps_per_second: 7.532, epoch: 6.0[0m
[32m[2022-09-16 13:29:40,324] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-60[0m
[32m[2022-09-16 13:29:40,324] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:29:43,466] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-60/tokenizer_config.json[0m
[32m[2022-09-16 13:29:43,466] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-60/special_tokens_map.json[0m
[32m[2022-09-16 13:29:49,347] [    INFO][0m - Deleting older checkpoint [checkpoints_bustm/checkpoint-20] due to args.save_total_limit[0m
[32m[2022-09-16 13:29:59,729] [    INFO][0m - loss: 0.11879016, learning_rate: 2.3000000000000004e-06, global_step: 70, interval_runtime: 20.7333, interval_samples_per_second: 0.772, interval_steps_per_second: 0.482, epoch: 7.0[0m
[32m[2022-09-16 13:29:59,730] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:29:59,730] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:29:59,730] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:29:59,730] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:29:59,730] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:30:01,056] [    INFO][0m - eval_loss: 0.6223350763320923, eval_accuracy: 0.775, eval_runtime: 1.3258, eval_samples_per_second: 120.68, eval_steps_per_second: 7.543, epoch: 7.0[0m
[32m[2022-09-16 13:30:01,057] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-70[0m
[32m[2022-09-16 13:30:01,057] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:30:04,177] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-70/tokenizer_config.json[0m
[32m[2022-09-16 13:30:04,177] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-70/special_tokens_map.json[0m
[32m[2022-09-16 13:30:09,789] [    INFO][0m - Deleting older checkpoint [checkpoints_bustm/checkpoint-50] due to args.save_total_limit[0m
[32m[2022-09-16 13:30:13,897] [    INFO][0m - loss: 0.089395, learning_rate: 2.1999999999999997e-06, global_step: 80, interval_runtime: 14.1688, interval_samples_per_second: 1.129, interval_steps_per_second: 0.706, epoch: 8.0[0m
[32m[2022-09-16 13:30:13,898] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:30:13,898] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:30:13,898] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:30:13,898] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:30:13,899] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:30:15,207] [    INFO][0m - eval_loss: 0.6529108881950378, eval_accuracy: 0.775, eval_runtime: 1.3083, eval_samples_per_second: 122.292, eval_steps_per_second: 7.643, epoch: 8.0[0m
[32m[2022-09-16 13:30:15,207] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-80[0m
[32m[2022-09-16 13:30:15,208] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:30:18,260] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-80/tokenizer_config.json[0m
[32m[2022-09-16 13:30:18,261] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-80/special_tokens_map.json[0m
[32m[2022-09-16 13:30:23,904] [    INFO][0m - Deleting older checkpoint [checkpoints_bustm/checkpoint-70] due to args.save_total_limit[0m
[32m[2022-09-16 13:30:28,037] [    INFO][0m - loss: 0.07005918, learning_rate: 2.1e-06, global_step: 90, interval_runtime: 14.1398, interval_samples_per_second: 1.132, interval_steps_per_second: 0.707, epoch: 9.0[0m
[32m[2022-09-16 13:30:28,039] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:30:28,039] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:30:28,039] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:30:28,040] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:30:28,040] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:30:29,361] [    INFO][0m - eval_loss: 0.7002183198928833, eval_accuracy: 0.79375, eval_runtime: 1.3214, eval_samples_per_second: 121.081, eval_steps_per_second: 7.568, epoch: 9.0[0m
[32m[2022-09-16 13:30:29,361] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-90[0m
[32m[2022-09-16 13:30:29,361] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:30:32,486] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-90/tokenizer_config.json[0m
[32m[2022-09-16 13:30:32,487] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-90/special_tokens_map.json[0m
[32m[2022-09-16 13:30:38,663] [    INFO][0m - Deleting older checkpoint [checkpoints_bustm/checkpoint-60] due to args.save_total_limit[0m
[32m[2022-09-16 13:31:08,110] [    INFO][0m - loss: 0.05651426, learning_rate: 2e-06, global_step: 100, interval_runtime: 40.0734, interval_samples_per_second: 0.399, interval_steps_per_second: 0.25, epoch: 10.0[0m
[32m[2022-09-16 13:31:08,111] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:31:08,111] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:31:08,111] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:31:08,111] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:31:08,111] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:31:09,430] [    INFO][0m - eval_loss: 0.7456600069999695, eval_accuracy: 0.79375, eval_runtime: 1.318, eval_samples_per_second: 121.392, eval_steps_per_second: 7.587, epoch: 10.0[0m
[32m[2022-09-16 13:31:09,431] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-100[0m
[32m[2022-09-16 13:31:09,431] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:31:12,813] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-16 13:31:12,813] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-16 13:31:18,775] [    INFO][0m - Deleting older checkpoint [checkpoints_bustm/checkpoint-80] due to args.save_total_limit[0m
[32m[2022-09-16 13:31:22,940] [    INFO][0m - loss: 0.03342224, learning_rate: 1.9e-06, global_step: 110, interval_runtime: 14.8293, interval_samples_per_second: 1.079, interval_steps_per_second: 0.674, epoch: 11.0[0m
[32m[2022-09-16 13:31:22,941] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:31:22,941] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:31:22,941] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:31:22,941] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:31:22,941] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:31:24,256] [    INFO][0m - eval_loss: 0.7903627753257751, eval_accuracy: 0.79375, eval_runtime: 1.3147, eval_samples_per_second: 121.701, eval_steps_per_second: 7.606, epoch: 11.0[0m
[32m[2022-09-16 13:31:24,949] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-110[0m
[32m[2022-09-16 13:31:24,949] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:31:30,100] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-110/tokenizer_config.json[0m
[32m[2022-09-16 13:31:30,101] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-110/special_tokens_map.json[0m
[32m[2022-09-16 13:31:36,165] [    INFO][0m - Deleting older checkpoint [checkpoints_bustm/checkpoint-100] due to args.save_total_limit[0m
[32m[2022-09-16 13:31:40,457] [    INFO][0m - loss: 0.02108916, learning_rate: 1.8e-06, global_step: 120, interval_runtime: 17.3548, interval_samples_per_second: 0.922, interval_steps_per_second: 0.576, epoch: 12.0[0m
[32m[2022-09-16 13:31:40,457] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:31:40,458] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:31:40,458] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:31:40,458] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:31:40,458] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:31:41,775] [    INFO][0m - eval_loss: 0.8368101119995117, eval_accuracy: 0.78125, eval_runtime: 1.317, eval_samples_per_second: 121.489, eval_steps_per_second: 7.593, epoch: 12.0[0m
[32m[2022-09-16 13:31:41,775] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-120[0m
[32m[2022-09-16 13:31:41,776] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:31:44,962] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-120/tokenizer_config.json[0m
[32m[2022-09-16 13:31:44,962] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-120/special_tokens_map.json[0m
[32m[2022-09-16 13:31:52,582] [    INFO][0m - Deleting older checkpoint [checkpoints_bustm/checkpoint-110] due to args.save_total_limit[0m
[32m[2022-09-16 13:31:56,713] [    INFO][0m - loss: 0.01521993, learning_rate: 1.7e-06, global_step: 130, interval_runtime: 16.4181, interval_samples_per_second: 0.975, interval_steps_per_second: 0.609, epoch: 13.0[0m
[32m[2022-09-16 13:31:56,714] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:31:56,714] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:31:56,714] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:31:56,714] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:31:56,714] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:31:58,026] [    INFO][0m - eval_loss: 0.8849980235099792, eval_accuracy: 0.78125, eval_runtime: 1.3116, eval_samples_per_second: 121.99, eval_steps_per_second: 7.624, epoch: 13.0[0m
[32m[2022-09-16 13:31:58,662] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-130[0m
[32m[2022-09-16 13:31:58,663] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:32:02,222] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-130/tokenizer_config.json[0m
[32m[2022-09-16 13:32:02,223] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-130/special_tokens_map.json[0m
[32m[2022-09-16 13:32:08,387] [    INFO][0m - Deleting older checkpoint [checkpoints_bustm/checkpoint-120] due to args.save_total_limit[0m
[32m[2022-09-16 13:32:12,495] [    INFO][0m - loss: 0.01480061, learning_rate: 1.6e-06, global_step: 140, interval_runtime: 15.7822, interval_samples_per_second: 1.014, interval_steps_per_second: 0.634, epoch: 14.0[0m
[32m[2022-09-16 13:32:12,495] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:32:12,496] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:32:12,496] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:32:12,496] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:32:12,496] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:32:13,810] [    INFO][0m - eval_loss: 0.930997371673584, eval_accuracy: 0.78125, eval_runtime: 1.3138, eval_samples_per_second: 121.78, eval_steps_per_second: 7.611, epoch: 14.0[0m
[32m[2022-09-16 13:32:13,810] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-140[0m
[32m[2022-09-16 13:32:13,810] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:32:17,806] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-140/tokenizer_config.json[0m
[32m[2022-09-16 13:32:17,807] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-140/special_tokens_map.json[0m
[32m[2022-09-16 13:32:26,490] [    INFO][0m - Deleting older checkpoint [checkpoints_bustm/checkpoint-130] due to args.save_total_limit[0m
[32m[2022-09-16 13:32:30,650] [    INFO][0m - loss: 0.00504335, learning_rate: 1.5e-06, global_step: 150, interval_runtime: 18.1552, interval_samples_per_second: 0.881, interval_steps_per_second: 0.551, epoch: 15.0[0m
[32m[2022-09-16 13:32:30,651] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:32:30,651] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:32:30,651] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:32:30,652] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:32:30,652] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:32:31,991] [    INFO][0m - eval_loss: 0.9696536064147949, eval_accuracy: 0.78125, eval_runtime: 1.3395, eval_samples_per_second: 119.451, eval_steps_per_second: 7.466, epoch: 15.0[0m
[32m[2022-09-16 13:32:31,992] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-150[0m
[32m[2022-09-16 13:32:31,992] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:32:35,477] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-150/tokenizer_config.json[0m
[32m[2022-09-16 13:32:35,478] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-150/special_tokens_map.json[0m
[32m[2022-09-16 13:32:41,687] [    INFO][0m - Deleting older checkpoint [checkpoints_bustm/checkpoint-140] due to args.save_total_limit[0m
[32m[2022-09-16 13:32:45,849] [    INFO][0m - loss: 0.00280071, learning_rate: 1.4000000000000001e-06, global_step: 160, interval_runtime: 15.1991, interval_samples_per_second: 1.053, interval_steps_per_second: 0.658, epoch: 16.0[0m
[32m[2022-09-16 13:32:45,850] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:32:45,850] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:32:45,850] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:32:45,850] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:32:45,850] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:32:47,155] [    INFO][0m - eval_loss: 1.0028984546661377, eval_accuracy: 0.78125, eval_runtime: 1.3046, eval_samples_per_second: 122.644, eval_steps_per_second: 7.665, epoch: 16.0[0m
[32m[2022-09-16 13:32:47,155] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-160[0m
[32m[2022-09-16 13:32:47,155] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:32:50,222] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-160/tokenizer_config.json[0m
[32m[2022-09-16 13:32:50,223] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-160/special_tokens_map.json[0m
[32m[2022-09-16 13:32:58,999] [    INFO][0m - Deleting older checkpoint [checkpoints_bustm/checkpoint-150] due to args.save_total_limit[0m
[32m[2022-09-16 13:33:03,090] [    INFO][0m - loss: 0.00399566, learning_rate: 1.3e-06, global_step: 170, interval_runtime: 17.2404, interval_samples_per_second: 0.928, interval_steps_per_second: 0.58, epoch: 17.0[0m
[32m[2022-09-16 13:33:03,091] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:33:03,091] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:33:03,091] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:33:03,091] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:33:03,091] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:33:04,409] [    INFO][0m - eval_loss: 1.0313451290130615, eval_accuracy: 0.78125, eval_runtime: 1.3179, eval_samples_per_second: 121.403, eval_steps_per_second: 7.588, epoch: 17.0[0m
[32m[2022-09-16 13:33:04,410] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-170[0m
[32m[2022-09-16 13:33:04,410] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:33:07,739] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-170/tokenizer_config.json[0m
[32m[2022-09-16 13:33:07,739] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-170/special_tokens_map.json[0m
[32m[2022-09-16 13:33:14,269] [    INFO][0m - Deleting older checkpoint [checkpoints_bustm/checkpoint-160] due to args.save_total_limit[0m
[32m[2022-09-16 13:33:18,420] [    INFO][0m - loss: 0.00223683, learning_rate: 1.2000000000000002e-06, global_step: 180, interval_runtime: 15.3306, interval_samples_per_second: 1.044, interval_steps_per_second: 0.652, epoch: 18.0[0m
[32m[2022-09-16 13:33:18,421] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:33:18,421] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:33:18,421] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:33:18,421] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:33:18,421] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:33:19,728] [    INFO][0m - eval_loss: 1.054935336112976, eval_accuracy: 0.78125, eval_runtime: 1.306, eval_samples_per_second: 122.511, eval_steps_per_second: 7.657, epoch: 18.0[0m
[32m[2022-09-16 13:33:19,728] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-180[0m
[32m[2022-09-16 13:33:19,728] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:33:22,796] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-180/tokenizer_config.json[0m
[32m[2022-09-16 13:33:22,796] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-180/special_tokens_map.json[0m
[32m[2022-09-16 13:33:28,897] [    INFO][0m - Deleting older checkpoint [checkpoints_bustm/checkpoint-170] due to args.save_total_limit[0m
[32m[2022-09-16 13:33:33,024] [    INFO][0m - loss: 0.00150428, learning_rate: 1.0999999999999998e-06, global_step: 190, interval_runtime: 14.6038, interval_samples_per_second: 1.096, interval_steps_per_second: 0.685, epoch: 19.0[0m
[32m[2022-09-16 13:33:33,025] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:33:33,025] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:33:33,025] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:33:33,025] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:33:33,025] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:33:34,344] [    INFO][0m - eval_loss: 1.0739696025848389, eval_accuracy: 0.78125, eval_runtime: 1.3191, eval_samples_per_second: 121.293, eval_steps_per_second: 7.581, epoch: 19.0[0m
[32m[2022-09-16 13:33:34,345] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-190[0m
[32m[2022-09-16 13:33:34,345] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:33:38,856] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-190/tokenizer_config.json[0m
[32m[2022-09-16 13:33:38,857] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-190/special_tokens_map.json[0m
[32m[2022-09-16 13:33:45,751] [    INFO][0m - Deleting older checkpoint [checkpoints_bustm/checkpoint-180] due to args.save_total_limit[0m
[32m[2022-09-16 13:33:49,915] [    INFO][0m - loss: 0.00212908, learning_rate: 1e-06, global_step: 200, interval_runtime: 16.8905, interval_samples_per_second: 0.947, interval_steps_per_second: 0.592, epoch: 20.0[0m
[32m[2022-09-16 13:33:49,916] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:33:49,916] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:33:49,916] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:33:49,916] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:33:49,916] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:33:51,237] [    INFO][0m - eval_loss: 1.0902255773544312, eval_accuracy: 0.78125, eval_runtime: 1.3209, eval_samples_per_second: 121.131, eval_steps_per_second: 7.571, epoch: 20.0[0m
[32m[2022-09-16 13:33:51,238] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-200[0m
[32m[2022-09-16 13:33:51,238] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:33:54,528] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-200/tokenizer_config.json[0m
[32m[2022-09-16 13:33:54,528] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-200/special_tokens_map.json[0m
[32m[2022-09-16 13:34:03,298] [    INFO][0m - Deleting older checkpoint [checkpoints_bustm/checkpoint-190] due to args.save_total_limit[0m
[32m[2022-09-16 13:34:07,499] [    INFO][0m - loss: 0.00725709, learning_rate: 9e-07, global_step: 210, interval_runtime: 17.5845, interval_samples_per_second: 0.91, interval_steps_per_second: 0.569, epoch: 21.0[0m
[32m[2022-09-16 13:34:07,500] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:34:07,500] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:34:07,500] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:34:07,500] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:34:07,500] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:34:08,804] [    INFO][0m - eval_loss: 1.105847954750061, eval_accuracy: 0.78125, eval_runtime: 1.3042, eval_samples_per_second: 122.677, eval_steps_per_second: 7.667, epoch: 21.0[0m
[32m[2022-09-16 13:34:08,805] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-210[0m
[32m[2022-09-16 13:34:08,805] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:34:12,033] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-210/tokenizer_config.json[0m
[32m[2022-09-16 13:34:12,034] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-210/special_tokens_map.json[0m
[32m[2022-09-16 13:34:18,144] [    INFO][0m - Deleting older checkpoint [checkpoints_bustm/checkpoint-200] due to args.save_total_limit[0m
[32m[2022-09-16 13:34:22,260] [    INFO][0m - loss: 0.0016113, learning_rate: 8e-07, global_step: 220, interval_runtime: 14.7609, interval_samples_per_second: 1.084, interval_steps_per_second: 0.677, epoch: 22.0[0m
[32m[2022-09-16 13:34:22,261] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:34:22,261] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:34:22,261] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:34:22,261] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:34:22,261] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:34:23,576] [    INFO][0m - eval_loss: 1.1186450719833374, eval_accuracy: 0.78125, eval_runtime: 1.3143, eval_samples_per_second: 121.739, eval_steps_per_second: 7.609, epoch: 22.0[0m
[32m[2022-09-16 13:34:23,576] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-220[0m
[32m[2022-09-16 13:34:23,576] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:34:26,857] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-220/tokenizer_config.json[0m
[32m[2022-09-16 13:34:26,857] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-220/special_tokens_map.json[0m
[32m[2022-09-16 13:34:32,876] [    INFO][0m - Deleting older checkpoint [checkpoints_bustm/checkpoint-210] due to args.save_total_limit[0m
[32m[2022-09-16 13:34:37,066] [    INFO][0m - loss: 0.00183606, learning_rate: 7.000000000000001e-07, global_step: 230, interval_runtime: 14.8057, interval_samples_per_second: 1.081, interval_steps_per_second: 0.675, epoch: 23.0[0m
[32m[2022-09-16 13:34:37,067] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:34:37,067] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:34:37,067] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:34:37,067] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:34:37,067] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:34:38,384] [    INFO][0m - eval_loss: 1.1311079263687134, eval_accuracy: 0.78125, eval_runtime: 1.3166, eval_samples_per_second: 121.528, eval_steps_per_second: 7.596, epoch: 23.0[0m
[32m[2022-09-16 13:34:42,303] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-230[0m
[32m[2022-09-16 13:34:42,304] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:34:46,319] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-230/tokenizer_config.json[0m
[32m[2022-09-16 13:34:46,319] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-230/special_tokens_map.json[0m
[32m[2022-09-16 13:34:53,075] [    INFO][0m - Deleting older checkpoint [checkpoints_bustm/checkpoint-220] due to args.save_total_limit[0m
[32m[2022-09-16 13:34:57,623] [    INFO][0m - loss: 0.00104181, learning_rate: 6.000000000000001e-07, global_step: 240, interval_runtime: 20.5574, interval_samples_per_second: 0.778, interval_steps_per_second: 0.486, epoch: 24.0[0m
[32m[2022-09-16 13:34:57,624] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:34:57,624] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:34:57,624] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:34:57,624] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:34:57,624] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:34:58,947] [    INFO][0m - eval_loss: 1.140529990196228, eval_accuracy: 0.78125, eval_runtime: 1.3223, eval_samples_per_second: 121.001, eval_steps_per_second: 7.563, epoch: 24.0[0m
[32m[2022-09-16 13:34:58,947] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-240[0m
[32m[2022-09-16 13:34:58,948] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:35:02,099] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-240/tokenizer_config.json[0m
[32m[2022-09-16 13:35:02,099] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-240/special_tokens_map.json[0m
[32m[2022-09-16 13:35:08,093] [    INFO][0m - Deleting older checkpoint [checkpoints_bustm/checkpoint-230] due to args.save_total_limit[0m
[32m[2022-09-16 13:35:16,064] [    INFO][0m - loss: 0.00167609, learning_rate: 5e-07, global_step: 250, interval_runtime: 14.5932, interval_samples_per_second: 1.096, interval_steps_per_second: 0.685, epoch: 25.0[0m
[32m[2022-09-16 13:35:16,065] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:35:16,065] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:35:16,065] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:35:16,065] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:35:16,066] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:35:17,370] [    INFO][0m - eval_loss: 1.14822518825531, eval_accuracy: 0.78125, eval_runtime: 1.3044, eval_samples_per_second: 122.661, eval_steps_per_second: 7.666, epoch: 25.0[0m
[32m[2022-09-16 13:35:17,370] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-250[0m
[32m[2022-09-16 13:35:17,371] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:35:20,506] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-250/tokenizer_config.json[0m
[32m[2022-09-16 13:35:20,506] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-250/special_tokens_map.json[0m
[32m[2022-09-16 13:35:26,852] [    INFO][0m - Deleting older checkpoint [checkpoints_bustm/checkpoint-240] due to args.save_total_limit[0m
[32m[2022-09-16 13:35:35,878] [    INFO][0m - loss: 0.00089722, learning_rate: 4e-07, global_step: 260, interval_runtime: 23.6615, interval_samples_per_second: 0.676, interval_steps_per_second: 0.423, epoch: 26.0[0m
[32m[2022-09-16 13:35:35,879] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:35:35,879] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:35:35,879] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:35:35,879] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:35:35,879] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:35:37,191] [    INFO][0m - eval_loss: 1.1546989679336548, eval_accuracy: 0.78125, eval_runtime: 1.3115, eval_samples_per_second: 121.997, eval_steps_per_second: 7.625, epoch: 26.0[0m
[32m[2022-09-16 13:35:37,191] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-260[0m
[32m[2022-09-16 13:35:37,191] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:35:40,514] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-260/tokenizer_config.json[0m
[32m[2022-09-16 13:35:40,514] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-260/special_tokens_map.json[0m
[32m[2022-09-16 13:35:47,239] [    INFO][0m - Deleting older checkpoint [checkpoints_bustm/checkpoint-250] due to args.save_total_limit[0m
[32m[2022-09-16 13:35:51,384] [    INFO][0m - loss: 0.00096696, learning_rate: 3.0000000000000004e-07, global_step: 270, interval_runtime: 15.5067, interval_samples_per_second: 1.032, interval_steps_per_second: 0.645, epoch: 27.0[0m
[32m[2022-09-16 13:35:51,385] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:35:51,385] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:35:51,386] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:35:51,386] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:35:51,386] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:35:52,706] [    INFO][0m - eval_loss: 1.159498929977417, eval_accuracy: 0.78125, eval_runtime: 1.3198, eval_samples_per_second: 121.234, eval_steps_per_second: 7.577, epoch: 27.0[0m
[32m[2022-09-16 13:35:52,706] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-270[0m
[32m[2022-09-16 13:35:52,706] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:35:56,475] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-270/tokenizer_config.json[0m
[32m[2022-09-16 13:35:56,476] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-270/special_tokens_map.json[0m
[32m[2022-09-16 13:36:02,422] [    INFO][0m - Deleting older checkpoint [checkpoints_bustm/checkpoint-260] due to args.save_total_limit[0m
[32m[2022-09-16 13:36:06,525] [    INFO][0m - loss: 0.00087902, learning_rate: 2e-07, global_step: 280, interval_runtime: 15.1399, interval_samples_per_second: 1.057, interval_steps_per_second: 0.661, epoch: 28.0[0m
[32m[2022-09-16 13:36:06,526] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:36:06,526] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:36:06,526] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:36:06,526] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:36:06,526] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:36:07,850] [    INFO][0m - eval_loss: 1.163104772567749, eval_accuracy: 0.78125, eval_runtime: 1.3234, eval_samples_per_second: 120.901, eval_steps_per_second: 7.556, epoch: 28.0[0m
[32m[2022-09-16 13:36:07,851] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-280[0m
[32m[2022-09-16 13:36:07,851] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:36:11,364] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-280/tokenizer_config.json[0m
[32m[2022-09-16 13:36:11,364] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-280/special_tokens_map.json[0m
[32m[2022-09-16 13:36:18,050] [    INFO][0m - Deleting older checkpoint [checkpoints_bustm/checkpoint-270] due to args.save_total_limit[0m
[32m[2022-09-16 13:36:22,189] [    INFO][0m - loss: 0.00080257, learning_rate: 1e-07, global_step: 290, interval_runtime: 15.6647, interval_samples_per_second: 1.021, interval_steps_per_second: 0.638, epoch: 29.0[0m
[32m[2022-09-16 13:36:22,190] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:36:22,190] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:36:22,190] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:36:22,190] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:36:22,190] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:36:23,504] [    INFO][0m - eval_loss: 1.1651443243026733, eval_accuracy: 0.78125, eval_runtime: 1.3138, eval_samples_per_second: 121.786, eval_steps_per_second: 7.612, epoch: 29.0[0m
[32m[2022-09-16 13:36:23,505] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-290[0m
[32m[2022-09-16 13:36:23,505] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:36:26,820] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-290/tokenizer_config.json[0m
[32m[2022-09-16 13:36:26,821] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-290/special_tokens_map.json[0m
[32m[2022-09-16 13:36:33,783] [    INFO][0m - Deleting older checkpoint [checkpoints_bustm/checkpoint-280] due to args.save_total_limit[0m
[32m[2022-09-16 13:36:37,898] [    INFO][0m - loss: 0.00103299, learning_rate: 0.0, global_step: 300, interval_runtime: 15.7088, interval_samples_per_second: 1.019, interval_steps_per_second: 0.637, epoch: 30.0[0m
[32m[2022-09-16 13:36:37,899] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-16 13:36:37,899] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-16 13:36:37,899] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:36:37,899] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:36:37,899] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-16 13:36:39,213] [    INFO][0m - eval_loss: 1.1658926010131836, eval_accuracy: 0.78125, eval_runtime: 1.3136, eval_samples_per_second: 121.802, eval_steps_per_second: 7.613, epoch: 30.0[0m
[32m[2022-09-16 13:36:39,213] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/checkpoint-300[0m
[32m[2022-09-16 13:36:39,213] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:36:42,354] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/checkpoint-300/tokenizer_config.json[0m
[32m[2022-09-16 13:36:42,354] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/checkpoint-300/special_tokens_map.json[0m
[32m[2022-09-16 13:36:48,076] [    INFO][0m - Deleting older checkpoint [checkpoints_bustm/checkpoint-290] due to args.save_total_limit[0m
[32m[2022-09-16 13:36:48,825] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-16 13:36:48,825] [    INFO][0m - Loading best model from ./checkpoints_bustm/checkpoint-90 (score: 0.79375).[0m
[32m[2022-09-16 13:36:50,525] [    INFO][0m - train_runtime: 520.1842, train_samples_per_second: 9.228, train_steps_per_second: 0.577, train_loss: 0.08071898741958042, epoch: 30.0[0m
[32m[2022-09-16 13:36:50,652] [    INFO][0m - Saving model checkpoint to ./checkpoints_bustm/[0m
[32m[2022-09-16 13:36:50,653] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-16 13:36:53,786] [    INFO][0m - tokenizer config file saved in ./checkpoints_bustm/tokenizer_config.json[0m
[32m[2022-09-16 13:36:53,786] [    INFO][0m - Special tokens file saved in ./checkpoints_bustm/special_tokens_map.json[0m
[32m[2022-09-16 13:36:53,788] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-16 13:36:53,788] [    INFO][0m -   epoch                    =       30.0[0m
[32m[2022-09-16 13:36:53,788] [    INFO][0m -   train_loss               =     0.0807[0m
[32m[2022-09-16 13:36:53,788] [    INFO][0m -   train_runtime            = 0:08:40.18[0m
[32m[2022-09-16 13:36:53,788] [    INFO][0m -   train_samples_per_second =      9.228[0m
[32m[2022-09-16 13:36:53,788] [    INFO][0m -   train_steps_per_second   =      0.577[0m
[32m[2022-09-16 13:36:53,792] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-16 13:36:53,792] [    INFO][0m -   Num examples = 1772[0m
[32m[2022-09-16 13:36:53,792] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-16 13:36:53,792] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-16 13:36:53,792] [    INFO][0m -   Total prediction steps = 111[0m
[32m[2022-09-16 13:37:08,497] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-16 13:37:08,497] [    INFO][0m -   test_accuracy           =      0.785[0m
[32m[2022-09-16 13:37:08,497] [    INFO][0m -   test_loss               =     0.6436[0m
[32m[2022-09-16 13:37:08,497] [    INFO][0m -   test_runtime            = 0:00:14.70[0m
[32m[2022-09-16 13:37:08,498] [    INFO][0m -   test_samples_per_second =    120.504[0m
[32m[2022-09-16 13:37:08,498] [    INFO][0m -   test_steps_per_second   =      7.548[0m
{
  "labels": 1,
  "text_a": "\u53eb\u7238\u7238\u53eb\u4e00\u58f0\u6211\u542c\u542c",
  "text_b": "\u90a3\u4f60\u53eb\u6211\u4e00\u58f0\u7238\u7238",
  "uid": 0
}

