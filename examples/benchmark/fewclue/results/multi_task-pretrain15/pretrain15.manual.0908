 
==========
eprstmt
==========
 
[33m[2022-09-08 11:35:29,965] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-09-08 11:35:29,966] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-08 11:35:29,966] [    INFO][0m - ============================================================[0m
[32m[2022-09-08 11:35:29,966] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-08 11:35:29,966] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-08 11:35:29,966] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-08 11:35:29,966] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-08 11:35:29,967] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-08 11:35:29,967] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-08 11:35:29,967] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-09-08 11:35:29,967] [    INFO][0m - [0m
[32m[2022-09-08 11:35:29,967] [    INFO][0m - ============================================================[0m
[32m[2022-09-08 11:35:29,967] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-08 11:35:29,967] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-08 11:35:29,967] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-08 11:35:29,967] [    INFO][0m - prompt                        :[0m
[32m[2022-09-08 11:35:29,967] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-08 11:35:29,967] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-08 11:35:29,967] [    INFO][0m - task_name                     :eprstmt[0m
[32m[2022-09-08 11:35:29,967] [    INFO][0m - [0m
[32m[2022-09-08 11:35:29,968] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0908 11:35:29.969197 46466 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0908 11:35:29.972965 46466 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-08 11:35:32,778] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-09-08 11:35:32,803] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-09-08 11:35:32,804] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[32m[2022-09-08 11:35:33,791] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùËøôÊù°ËØÑËÆ∫ÁöÑÊÉÖÊÑüÂÄæÂêëÊòØ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'ÁöÑ„ÄÇ'}][0m
[32m[2022-09-08 11:35:33,891] [    INFO][0m - ============================================================[0m
[32m[2022-09-08 11:35:33,892] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-08 11:35:33,892] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-08 11:35:33,892] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-08 11:35:33,892] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-08 11:35:33,892] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-08 11:35:33,892] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-08 11:35:33,892] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-08 11:35:33,892] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-08 11:35:33,892] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-08 11:35:33,892] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-08 11:35:33,893] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-08 11:35:33,893] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-08 11:35:33,893] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-08 11:35:33,893] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-08 11:35:33,893] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-08 11:35:33,893] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-08 11:35:33,893] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-08 11:35:33,893] [    INFO][0m - eval_batch_size               :8[0m
[32m[2022-09-08 11:35:33,893] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-09-08 11:35:33,893] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-09-08 11:35:33,893] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-08 11:35:33,893] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-08 11:35:33,893] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-08 11:35:33,894] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-08 11:35:33,894] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-08 11:35:33,894] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-08 11:35:33,894] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-08 11:35:33,894] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-08 11:35:33,894] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-08 11:35:33,894] [    INFO][0m - learning_rate                 :1e-05[0m
[32m[2022-09-08 11:35:33,894] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-08 11:35:33,894] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-08 11:35:33,894] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-08 11:35:33,894] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-08 11:35:33,894] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-08 11:35:33,894] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-08 11:35:33,894] [    INFO][0m - logging_dir                   :./checkpoints/runs/Sep08_11-35-29_instance-3bwob41y-01[0m
[32m[2022-09-08 11:35:33,895] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-08 11:35:33,895] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-08 11:35:33,895] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-08 11:35:33,895] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-08 11:35:33,895] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-08 11:35:33,895] [    INFO][0m - max_seq_length                :128[0m
[32m[2022-09-08 11:35:33,895] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-08 11:35:33,895] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-08 11:35:33,895] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-08 11:35:33,895] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-08 11:35:33,895] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-09-08 11:35:33,895] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-08 11:35:33,895] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-08 11:35:33,896] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-09-08 11:35:33,896] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-08 11:35:33,896] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-08 11:35:33,896] [    INFO][0m - per_device_eval_batch_size    :8[0m
[32m[2022-09-08 11:35:33,896] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-09-08 11:35:33,896] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-08 11:35:33,896] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-08 11:35:33,896] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-08 11:35:33,896] [    INFO][0m - ppt_learning_rate             :0.0001[0m
[32m[2022-09-08 11:35:33,896] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-08 11:35:33,896] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-08 11:35:33,896] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-08 11:35:33,896] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-08 11:35:33,897] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-08 11:35:33,897] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-08 11:35:33,897] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-09-08 11:35:33,897] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-08 11:35:33,897] [    INFO][0m - save_steps                    :100[0m
[32m[2022-09-08 11:35:33,897] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-09-08 11:35:33,897] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-08 11:35:33,897] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-08 11:35:33,897] [    INFO][0m - seed                          :42[0m
[32m[2022-09-08 11:35:33,897] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-08 11:35:33,897] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-08 11:35:33,897] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-08 11:35:33,897] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-09-08 11:35:33,897] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-08 11:35:33,898] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-08 11:35:33,898] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-08 11:35:33,898] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-08 11:35:33,898] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-08 11:35:33,898] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-08 11:35:33,898] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-08 11:35:33,898] [    INFO][0m - [0m
[32m[2022-09-08 11:35:33,899] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-08 11:35:33,900] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-08 11:35:33,900] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-09-08 11:35:33,900] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-09-08 11:35:33,900] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-09-08 11:35:33,900] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-08 11:35:33,900] [    INFO][0m -   Total optimization steps = 1000.0[0m
[32m[2022-09-08 11:35:33,900] [    INFO][0m -   Total num train samples = 8000[0m
[32m[2022-09-08 11:35:35,890] [    INFO][0m - loss: 14.86179657, learning_rate: 9.9e-06, global_step: 10, interval_runtime: 1.9882, interval_samples_per_second: 4.024, interval_steps_per_second: 5.03, epoch: 0.5[0m
[32m[2022-09-08 11:35:36,701] [    INFO][0m - loss: 1.00487232, learning_rate: 9.800000000000001e-06, global_step: 20, interval_runtime: 0.8117, interval_samples_per_second: 9.856, interval_steps_per_second: 12.32, epoch: 1.0[0m
[32m[2022-09-08 11:35:37,557] [    INFO][0m - loss: 0.17412008, learning_rate: 9.7e-06, global_step: 30, interval_runtime: 0.8565, interval_samples_per_second: 9.341, interval_steps_per_second: 11.676, epoch: 1.5[0m
[32m[2022-09-08 11:35:38,368] [    INFO][0m - loss: 0.17082008, learning_rate: 9.600000000000001e-06, global_step: 40, interval_runtime: 0.8111, interval_samples_per_second: 9.863, interval_steps_per_second: 12.329, epoch: 2.0[0m
[32m[2022-09-08 11:35:39,236] [    INFO][0m - loss: 0.06964695, learning_rate: 9.5e-06, global_step: 50, interval_runtime: 0.8669, interval_samples_per_second: 9.228, interval_steps_per_second: 11.535, epoch: 2.5[0m
[32m[2022-09-08 11:35:40,047] [    INFO][0m - loss: 0.12518601, learning_rate: 9.4e-06, global_step: 60, interval_runtime: 0.8115, interval_samples_per_second: 9.858, interval_steps_per_second: 12.323, epoch: 3.0[0m
[32m[2022-09-08 11:35:40,903] [    INFO][0m - loss: 0.09417008, learning_rate: 9.3e-06, global_step: 70, interval_runtime: 0.8565, interval_samples_per_second: 9.341, interval_steps_per_second: 11.676, epoch: 3.5[0m
[32m[2022-09-08 11:35:41,717] [    INFO][0m - loss: 0.06998265, learning_rate: 9.200000000000002e-06, global_step: 80, interval_runtime: 0.8134, interval_samples_per_second: 9.835, interval_steps_per_second: 12.294, epoch: 4.0[0m
[32m[2022-09-08 11:35:42,582] [    INFO][0m - loss: 0.03590368, learning_rate: 9.100000000000001e-06, global_step: 90, interval_runtime: 0.8651, interval_samples_per_second: 9.248, interval_steps_per_second: 11.56, epoch: 4.5[0m
[32m[2022-09-08 11:35:43,390] [    INFO][0m - loss: 0.09786077, learning_rate: 9e-06, global_step: 100, interval_runtime: 0.8082, interval_samples_per_second: 9.899, interval_steps_per_second: 12.374, epoch: 5.0[0m
[32m[2022-09-08 11:35:43,391] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 11:35:43,391] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-08 11:35:43,391] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 11:35:43,391] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 11:35:43,391] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-08 11:35:44,182] [    INFO][0m - eval_loss: 0.284269243478775, eval_accuracy: 0.893750011920929, eval_runtime: 0.7908, eval_samples_per_second: 202.336, eval_steps_per_second: 25.292, epoch: 5.0[0m
[32m[2022-09-08 11:35:44,188] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-09-08 11:35:44,188] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 11:35:47,485] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-08 11:35:47,792] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-08 11:35:54,191] [    INFO][0m - loss: 0.0425618, learning_rate: 8.900000000000001e-06, global_step: 110, interval_runtime: 10.8009, interval_samples_per_second: 0.741, interval_steps_per_second: 0.926, epoch: 5.5[0m
[32m[2022-09-08 11:35:54,998] [    INFO][0m - loss: 0.05383814, learning_rate: 8.8e-06, global_step: 120, interval_runtime: 0.8076, interval_samples_per_second: 9.906, interval_steps_per_second: 12.382, epoch: 6.0[0m
[32m[2022-09-08 11:35:55,861] [    INFO][0m - loss: 0.00265305, learning_rate: 8.700000000000001e-06, global_step: 130, interval_runtime: 0.8628, interval_samples_per_second: 9.272, interval_steps_per_second: 11.59, epoch: 6.5[0m
[32m[2022-09-08 11:35:56,674] [    INFO][0m - loss: 0.0015434, learning_rate: 8.6e-06, global_step: 140, interval_runtime: 0.813, interval_samples_per_second: 9.84, interval_steps_per_second: 12.3, epoch: 7.0[0m
[32m[2022-09-08 11:35:57,544] [    INFO][0m - loss: 0.00347948, learning_rate: 8.5e-06, global_step: 150, interval_runtime: 0.8698, interval_samples_per_second: 9.197, interval_steps_per_second: 11.497, epoch: 7.5[0m
[32m[2022-09-08 11:35:58,354] [    INFO][0m - loss: 0.0013829, learning_rate: 8.400000000000001e-06, global_step: 160, interval_runtime: 0.8096, interval_samples_per_second: 9.882, interval_steps_per_second: 12.352, epoch: 8.0[0m
[32m[2022-09-08 11:35:59,210] [    INFO][0m - loss: 0.00022444, learning_rate: 8.3e-06, global_step: 170, interval_runtime: 0.8569, interval_samples_per_second: 9.336, interval_steps_per_second: 11.67, epoch: 8.5[0m
[32m[2022-09-08 11:36:00,022] [    INFO][0m - loss: 0.00082498, learning_rate: 8.2e-06, global_step: 180, interval_runtime: 0.8113, interval_samples_per_second: 9.86, interval_steps_per_second: 12.325, epoch: 9.0[0m
[32m[2022-09-08 11:36:00,882] [    INFO][0m - loss: 0.00024985, learning_rate: 8.1e-06, global_step: 190, interval_runtime: 0.8603, interval_samples_per_second: 9.299, interval_steps_per_second: 11.624, epoch: 9.5[0m
[32m[2022-09-08 11:36:01,693] [    INFO][0m - loss: 0.00695994, learning_rate: 8.000000000000001e-06, global_step: 200, interval_runtime: 0.8106, interval_samples_per_second: 9.869, interval_steps_per_second: 12.337, epoch: 10.0[0m
[32m[2022-09-08 11:36:01,693] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 11:36:01,693] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-08 11:36:01,694] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 11:36:01,694] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 11:36:01,694] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-08 11:36:02,465] [    INFO][0m - eval_loss: 0.4419318735599518, eval_accuracy: 0.9000000357627869, eval_runtime: 0.7706, eval_samples_per_second: 207.624, eval_steps_per_second: 25.953, epoch: 10.0[0m
[32m[2022-09-08 11:36:02,470] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-09-08 11:36:02,470] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 11:36:04,037] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-09-08 11:36:04,038] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-09-08 11:36:07,406] [    INFO][0m - loss: 0.00019318, learning_rate: 7.9e-06, global_step: 210, interval_runtime: 5.7131, interval_samples_per_second: 1.4, interval_steps_per_second: 1.75, epoch: 10.5[0m
[32m[2022-09-08 11:36:08,213] [    INFO][0m - loss: 0.00023845, learning_rate: 7.800000000000002e-06, global_step: 220, interval_runtime: 0.807, interval_samples_per_second: 9.913, interval_steps_per_second: 12.391, epoch: 11.0[0m
[32m[2022-09-08 11:36:09,070] [    INFO][0m - loss: 0.00019897, learning_rate: 7.7e-06, global_step: 230, interval_runtime: 0.8571, interval_samples_per_second: 9.334, interval_steps_per_second: 11.667, epoch: 11.5[0m
[32m[2022-09-08 11:36:09,877] [    INFO][0m - loss: 0.00011103, learning_rate: 7.600000000000001e-06, global_step: 240, interval_runtime: 0.8074, interval_samples_per_second: 9.908, interval_steps_per_second: 12.386, epoch: 12.0[0m
[32m[2022-09-08 11:36:10,743] [    INFO][0m - loss: 0.00012087, learning_rate: 7.500000000000001e-06, global_step: 250, interval_runtime: 0.8652, interval_samples_per_second: 9.246, interval_steps_per_second: 11.558, epoch: 12.5[0m
[32m[2022-09-08 11:36:11,552] [    INFO][0m - loss: 0.00019123, learning_rate: 7.4e-06, global_step: 260, interval_runtime: 0.8096, interval_samples_per_second: 9.881, interval_steps_per_second: 12.351, epoch: 13.0[0m
[32m[2022-09-08 11:36:12,406] [    INFO][0m - loss: 6.982e-05, learning_rate: 7.3e-06, global_step: 270, interval_runtime: 0.8534, interval_samples_per_second: 9.374, interval_steps_per_second: 11.718, epoch: 13.5[0m
[32m[2022-09-08 11:36:13,219] [    INFO][0m - loss: 0.00012364, learning_rate: 7.2000000000000005e-06, global_step: 280, interval_runtime: 0.8134, interval_samples_per_second: 9.835, interval_steps_per_second: 12.293, epoch: 14.0[0m
[32m[2022-09-08 11:36:14,077] [    INFO][0m - loss: 8.016e-05, learning_rate: 7.100000000000001e-06, global_step: 290, interval_runtime: 0.858, interval_samples_per_second: 9.324, interval_steps_per_second: 11.655, epoch: 14.5[0m
[32m[2022-09-08 11:36:14,885] [    INFO][0m - loss: 0.0001003, learning_rate: 7e-06, global_step: 300, interval_runtime: 0.8077, interval_samples_per_second: 9.905, interval_steps_per_second: 12.381, epoch: 15.0[0m
[32m[2022-09-08 11:36:14,885] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 11:36:14,885] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-08 11:36:14,885] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 11:36:14,885] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 11:36:14,886] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-08 11:36:15,645] [    INFO][0m - eval_loss: 0.4872836470603943, eval_accuracy: 0.8812500238418579, eval_runtime: 0.7595, eval_samples_per_second: 210.676, eval_steps_per_second: 26.334, epoch: 15.0[0m
[32m[2022-09-08 11:36:15,652] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-09-08 11:36:15,652] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 11:36:17,169] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-09-08 11:36:17,169] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-09-08 11:36:20,417] [    INFO][0m - loss: 0.0002723, learning_rate: 6.9e-06, global_step: 310, interval_runtime: 5.5319, interval_samples_per_second: 1.446, interval_steps_per_second: 1.808, epoch: 15.5[0m
[32m[2022-09-08 11:36:21,236] [    INFO][0m - loss: 5.251e-05, learning_rate: 6.800000000000001e-06, global_step: 320, interval_runtime: 0.8192, interval_samples_per_second: 9.765, interval_steps_per_second: 12.207, epoch: 16.0[0m
[32m[2022-09-08 11:36:22,099] [    INFO][0m - loss: 5.901e-05, learning_rate: 6.700000000000001e-06, global_step: 330, interval_runtime: 0.8627, interval_samples_per_second: 9.273, interval_steps_per_second: 11.591, epoch: 16.5[0m
[32m[2022-09-08 11:36:22,914] [    INFO][0m - loss: 5.703e-05, learning_rate: 6.600000000000001e-06, global_step: 340, interval_runtime: 0.8149, interval_samples_per_second: 9.817, interval_steps_per_second: 12.272, epoch: 17.0[0m
[32m[2022-09-08 11:36:23,777] [    INFO][0m - loss: 0.00011229, learning_rate: 6.5000000000000004e-06, global_step: 350, interval_runtime: 0.8631, interval_samples_per_second: 9.269, interval_steps_per_second: 11.586, epoch: 17.5[0m
[32m[2022-09-08 11:36:24,588] [    INFO][0m - loss: 5.373e-05, learning_rate: 6.4000000000000006e-06, global_step: 360, interval_runtime: 0.8109, interval_samples_per_second: 9.866, interval_steps_per_second: 12.332, epoch: 18.0[0m
[32m[2022-09-08 11:36:25,453] [    INFO][0m - loss: 4.478e-05, learning_rate: 6.300000000000001e-06, global_step: 370, interval_runtime: 0.8652, interval_samples_per_second: 9.246, interval_steps_per_second: 11.558, epoch: 18.5[0m
[32m[2022-09-08 11:36:26,265] [    INFO][0m - loss: 7.668e-05, learning_rate: 6.200000000000001e-06, global_step: 380, interval_runtime: 0.8125, interval_samples_per_second: 9.846, interval_steps_per_second: 12.308, epoch: 19.0[0m
[32m[2022-09-08 11:36:27,137] [    INFO][0m - loss: 6.9e-05, learning_rate: 6.1e-06, global_step: 390, interval_runtime: 0.8718, interval_samples_per_second: 9.176, interval_steps_per_second: 11.47, epoch: 19.5[0m
[32m[2022-09-08 11:36:27,949] [    INFO][0m - loss: 5.25e-05, learning_rate: 6e-06, global_step: 400, interval_runtime: 0.8119, interval_samples_per_second: 9.853, interval_steps_per_second: 12.316, epoch: 20.0[0m
[32m[2022-09-08 11:36:32,349] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 11:36:32,349] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-08 11:36:32,349] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 11:36:32,349] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 11:36:32,349] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-08 11:36:33,125] [    INFO][0m - eval_loss: 0.48799562454223633, eval_accuracy: 0.8812500238418579, eval_runtime: 0.776, eval_samples_per_second: 206.187, eval_steps_per_second: 25.773, epoch: 20.0[0m
[32m[2022-09-08 11:36:33,131] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-09-08 11:36:33,131] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 11:36:34,635] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-09-08 11:36:34,635] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-09-08 11:36:37,958] [    INFO][0m - loss: 0.03136436, learning_rate: 5.9e-06, global_step: 410, interval_runtime: 10.0088, interval_samples_per_second: 0.799, interval_steps_per_second: 0.999, epoch: 20.5[0m
[32m[2022-09-08 11:36:38,768] [    INFO][0m - loss: 6.569e-05, learning_rate: 5.8e-06, global_step: 420, interval_runtime: 0.8102, interval_samples_per_second: 9.874, interval_steps_per_second: 12.342, epoch: 21.0[0m
[32m[2022-09-08 11:36:39,632] [    INFO][0m - loss: 7.882e-05, learning_rate: 5.7e-06, global_step: 430, interval_runtime: 0.8642, interval_samples_per_second: 9.257, interval_steps_per_second: 11.571, epoch: 21.5[0m
[32m[2022-09-08 11:36:40,443] [    INFO][0m - loss: 3.74e-05, learning_rate: 5.600000000000001e-06, global_step: 440, interval_runtime: 0.8101, interval_samples_per_second: 9.876, interval_steps_per_second: 12.344, epoch: 22.0[0m
[32m[2022-09-08 11:36:41,304] [    INFO][0m - loss: 4.512e-05, learning_rate: 5.500000000000001e-06, global_step: 450, interval_runtime: 0.8612, interval_samples_per_second: 9.289, interval_steps_per_second: 11.611, epoch: 22.5[0m
[32m[2022-09-08 11:36:42,117] [    INFO][0m - loss: 3.792e-05, learning_rate: 5.400000000000001e-06, global_step: 460, interval_runtime: 0.813, interval_samples_per_second: 9.84, interval_steps_per_second: 12.301, epoch: 23.0[0m
[32m[2022-09-08 11:36:42,980] [    INFO][0m - loss: 3.568e-05, learning_rate: 5.300000000000001e-06, global_step: 470, interval_runtime: 0.8632, interval_samples_per_second: 9.268, interval_steps_per_second: 11.584, epoch: 23.5[0m
[32m[2022-09-08 11:36:43,790] [    INFO][0m - loss: 3.661e-05, learning_rate: 5.2e-06, global_step: 480, interval_runtime: 0.8101, interval_samples_per_second: 9.876, interval_steps_per_second: 12.345, epoch: 24.0[0m
[32m[2022-09-08 11:36:44,648] [    INFO][0m - loss: 4.501e-05, learning_rate: 5.1e-06, global_step: 490, interval_runtime: 0.8574, interval_samples_per_second: 9.33, interval_steps_per_second: 11.663, epoch: 24.5[0m
[32m[2022-09-08 11:36:45,458] [    INFO][0m - loss: 0.0002378, learning_rate: 5e-06, global_step: 500, interval_runtime: 0.8105, interval_samples_per_second: 9.87, interval_steps_per_second: 12.337, epoch: 25.0[0m
[32m[2022-09-08 11:36:45,458] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 11:36:45,459] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-08 11:36:45,459] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 11:36:45,459] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 11:36:45,459] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-08 11:36:46,223] [    INFO][0m - eval_loss: 0.47878819704055786, eval_accuracy: 0.8812500238418579, eval_runtime: 0.7642, eval_samples_per_second: 209.369, eval_steps_per_second: 26.171, epoch: 25.0[0m
[32m[2022-09-08 11:36:46,230] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-09-08 11:36:46,230] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 11:36:47,737] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-09-08 11:36:47,738] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-09-08 11:36:51,104] [    INFO][0m - loss: 3.682e-05, learning_rate: 4.9000000000000005e-06, global_step: 510, interval_runtime: 5.6454, interval_samples_per_second: 1.417, interval_steps_per_second: 1.771, epoch: 25.5[0m
[32m[2022-09-08 11:36:51,917] [    INFO][0m - loss: 5.121e-05, learning_rate: 4.800000000000001e-06, global_step: 520, interval_runtime: 0.8133, interval_samples_per_second: 9.837, interval_steps_per_second: 12.296, epoch: 26.0[0m
[32m[2022-09-08 11:36:52,778] [    INFO][0m - loss: 4.036e-05, learning_rate: 4.7e-06, global_step: 530, interval_runtime: 0.8612, interval_samples_per_second: 9.289, interval_steps_per_second: 11.611, epoch: 26.5[0m
[32m[2022-09-08 11:36:53,589] [    INFO][0m - loss: 3.118e-05, learning_rate: 4.600000000000001e-06, global_step: 540, interval_runtime: 0.8111, interval_samples_per_second: 9.863, interval_steps_per_second: 12.329, epoch: 27.0[0m
[32m[2022-09-08 11:36:54,454] [    INFO][0m - loss: 4.353e-05, learning_rate: 4.5e-06, global_step: 550, interval_runtime: 0.8651, interval_samples_per_second: 9.248, interval_steps_per_second: 11.56, epoch: 27.5[0m
[32m[2022-09-08 11:36:55,265] [    INFO][0m - loss: 3.579e-05, learning_rate: 4.4e-06, global_step: 560, interval_runtime: 0.8109, interval_samples_per_second: 9.866, interval_steps_per_second: 12.332, epoch: 28.0[0m
[32m[2022-09-08 11:36:56,137] [    INFO][0m - loss: 3.892e-05, learning_rate: 4.3e-06, global_step: 570, interval_runtime: 0.8723, interval_samples_per_second: 9.171, interval_steps_per_second: 11.464, epoch: 28.5[0m
[32m[2022-09-08 11:36:56,956] [    INFO][0m - loss: 2.8e-05, learning_rate: 4.2000000000000004e-06, global_step: 580, interval_runtime: 0.8185, interval_samples_per_second: 9.774, interval_steps_per_second: 12.218, epoch: 29.0[0m
[32m[2022-09-08 11:36:57,823] [    INFO][0m - loss: 3.236e-05, learning_rate: 4.1e-06, global_step: 590, interval_runtime: 0.867, interval_samples_per_second: 9.227, interval_steps_per_second: 11.534, epoch: 29.5[0m
[32m[2022-09-08 11:36:58,637] [    INFO][0m - loss: 2.624e-05, learning_rate: 4.000000000000001e-06, global_step: 600, interval_runtime: 0.814, interval_samples_per_second: 9.828, interval_steps_per_second: 12.285, epoch: 30.0[0m
[32m[2022-09-08 11:36:58,637] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 11:36:58,637] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-08 11:36:58,638] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 11:36:58,638] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 11:36:58,638] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-08 11:36:59,416] [    INFO][0m - eval_loss: 0.5030947923660278, eval_accuracy: 0.875, eval_runtime: 0.7786, eval_samples_per_second: 205.504, eval_steps_per_second: 25.688, epoch: 30.0[0m
[32m[2022-09-08 11:36:59,422] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-09-08 11:36:59,422] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 11:37:00,890] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-09-08 11:37:00,890] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-09-08 11:37:03,189] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-08 11:37:03,190] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-200 (score: 0.9000000357627869).[0m
[32m[2022-09-08 11:37:04,304] [    INFO][0m - train_runtime: 90.4036, train_samples_per_second: 88.492, train_steps_per_second: 11.062, train_loss: 0.28087839178202556, epoch: 30.0[0m
[32m[2022-09-08 11:37:04,306] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-09-08 11:37:04,306] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 11:37:05,748] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-09-08 11:37:05,748] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-09-08 11:37:05,750] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-08 11:37:05,750] [    INFO][0m -   epoch                    =       30.0[0m
[32m[2022-09-08 11:37:05,750] [    INFO][0m -   train_loss               =     0.2809[0m
[32m[2022-09-08 11:37:05,750] [    INFO][0m -   train_runtime            = 0:01:30.40[0m
[32m[2022-09-08 11:37:05,750] [    INFO][0m -   train_samples_per_second =     88.492[0m
[32m[2022-09-08 11:37:05,750] [    INFO][0m -   train_steps_per_second   =     11.062[0m
[32m[2022-09-08 11:37:05,753] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-08 11:37:05,754] [    INFO][0m -   Num examples = 610[0m
[32m[2022-09-08 11:37:05,754] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 11:37:05,754] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 11:37:05,754] [    INFO][0m -   Total prediction steps = 77[0m
[32m[2022-09-08 11:37:08,732] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-08 11:37:08,732] [    INFO][0m -   test_accuracy           =     0.9016[0m
[32m[2022-09-08 11:37:08,733] [    INFO][0m -   test_loss               =     0.3411[0m
[32m[2022-09-08 11:37:08,733] [    INFO][0m -   test_runtime            = 0:00:02.97[0m
[32m[2022-09-08 11:37:08,733] [    INFO][0m -   test_samples_per_second =    204.802[0m
[32m[2022-09-08 11:37:08,733] [    INFO][0m -   test_steps_per_second   =     25.852[0m
[32m[2022-09-08 11:37:08,733] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-08 11:37:08,733] [    INFO][0m -   Num examples = 753[0m
[32m[2022-09-08 11:37:08,733] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 11:37:08,733] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 11:37:08,733] [    INFO][0m -   Total prediction steps = 95[0m
terminate called after throwing an instance of 'paddle::memory::allocation::BadAlloc'
  what():  

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   full_dygraph_function(paddle::experimental::IntArrayBase<paddle::experimental::Tensor>, paddle::experimental::ScalarBase<paddle::experimental::Tensor>, paddle::experimental::DataType, phi::Place)
1   paddle::experimental::full(paddle::experimental::IntArrayBase<paddle::experimental::Tensor> const&, paddle::experimental::ScalarBase<paddle::experimental::Tensor> const&, paddle::experimental::DataType, phi::Place const&)
2   void phi::FullKernel<float, phi::GPUContext>(phi::GPUContext const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, paddle::experimental::ScalarBase<phi::DenseTensor> const&, paddle::experimental::DataType, phi::DenseTensor*)
3   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
4   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, paddle::experimental::DataType, unsigned long, bool) const
5   phi::DenseTensor::AllocateFrom(phi::Allocator*, paddle::experimental::DataType, unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  phi::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 4.388858TB memory on GPU 0, 3.686279GB memory has been allocated and available memory is only 28.062256GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
If the above ways do not solve the out of memory problem, you can try to use CUDA managed memory. The command is `export FLAGS_use_cuda_managed_memory=false`.
 (at /paddle/paddle/fluid/memory/allocation/cuda_allocator.cc:95)

run.sh: line 53: 46466 Aborted                 CUDA_VISIBLE_DEVICES=$device python train_single.py --output_dir ./checkpoints/ --prompt "$prompt" --max_seq_length $max_length --per_device_eval_batch_size $batch_size --per_device_train_batch_size $batch_size --model_name_or_path ernie-3.0-base-zh --split_id few_all --task_name $task_name --metric_for_best_model accuracy --disable_tqdm True --do_test --eval_steps 100 --save_steps 100 --num_train_epochs 50 --logging_steps 10 --learning_rate 1e-5 --ppt_learning_rate 1e-4 --load_best_model_at_end $is_train --do_train $is_train --do_eval $is_train --do_save $is_train --do_predict $is_train
run.sh: line 59: --freeze_plm: command not found
 
==========
csldcp
==========
 
[33m[2022-09-08 11:37:25,542] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-09-08 11:37:25,542] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-08 11:37:25,543] [    INFO][0m - ============================================================[0m
[32m[2022-09-08 11:37:25,543] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-08 11:37:25,543] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-08 11:37:25,543] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-08 11:37:25,543] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-08 11:37:25,543] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-08 11:37:25,543] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-08 11:37:25,543] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-09-08 11:37:25,543] [    INFO][0m - [0m
[32m[2022-09-08 11:37:25,543] [    INFO][0m - ============================================================[0m
[32m[2022-09-08 11:37:25,543] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-08 11:37:25,543] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-08 11:37:25,543] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-08 11:37:25,544] [    INFO][0m - prompt                        :[0m
[32m[2022-09-08 11:37:25,544] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-08 11:37:25,544] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-08 11:37:25,544] [    INFO][0m - task_name                     :csldcp[0m
[32m[2022-09-08 11:37:25,544] [    INFO][0m - [0m
[32m[2022-09-08 11:37:25,544] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0908 11:37:25.545624 49243 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0908 11:37:25.549428 49243 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-08 11:37:28,376] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-09-08 11:37:28,401] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-09-08 11:37:28,402] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[32m[2022-09-08 11:37:29,447] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùËøôÁØáÊñáÁåÆÁöÑÁ±ªÂà´ÊòØ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': '„ÄÇ'}][0m
Traceback (most recent call last):
  File "train_single.py", line 265, in <module>
    main(0)
  File "train_single.py", line 92, in main
    train_ds, dev_ds, public_test_ds, test_ds = load_fewclue(
  File "/ssd2/wanghuijuan03/prompt/PaddleNLP/examples/benchmark/fewclue/pretrain15/data.py", line 141, in load_fewclue
    data_ds = data_ds.map(DEFAULT_CONVERT[task_name])
KeyError: 'csldcp'
run.sh: line 59: --freeze_plm: command not found
 
==========
tnews
==========
 
[33m[2022-09-08 11:37:33,035] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-09-08 11:37:33,035] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-08 11:37:33,035] [    INFO][0m - ============================================================[0m
[32m[2022-09-08 11:37:33,035] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-08 11:37:33,036] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-08 11:37:33,036] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-08 11:37:33,036] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-08 11:37:33,036] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-08 11:37:33,036] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-08 11:37:33,036] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-09-08 11:37:33,036] [    INFO][0m - [0m
[32m[2022-09-08 11:37:33,036] [    INFO][0m - ============================================================[0m
[32m[2022-09-08 11:37:33,036] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-08 11:37:33,036] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-08 11:37:33,036] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-08 11:37:33,036] [    INFO][0m - prompt                        :[0m
[32m[2022-09-08 11:37:33,036] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-08 11:37:33,037] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-08 11:37:33,037] [    INFO][0m - task_name                     :tnews[0m
[32m[2022-09-08 11:37:33,037] [    INFO][0m - [0m
[32m[2022-09-08 11:37:33,037] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0908 11:37:33.038463 49413 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0908 11:37:33.042435 49413 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-08 11:37:35,863] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-09-08 11:37:37,201] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-09-08 11:37:37,202] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[32m[2022-09-08 11:37:38,246] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚Äù‰∏äËø∞Êñ∞ÈóªÈÄâËá™'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': '‰∏ìÊ†è„ÄÇ'}][0m
[32m[2022-09-08 11:37:38,383] [    INFO][0m - ============================================================[0m
[32m[2022-09-08 11:37:38,384] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-08 11:37:38,384] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-08 11:37:38,384] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-08 11:37:38,384] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-08 11:37:38,384] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-08 11:37:38,384] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-08 11:37:38,385] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-08 11:37:38,385] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-08 11:37:38,385] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-08 11:37:38,385] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-08 11:37:38,385] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-08 11:37:38,385] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-08 11:37:38,385] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-08 11:37:38,385] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-08 11:37:38,385] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-08 11:37:38,385] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-08 11:37:38,385] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-08 11:37:38,386] [    INFO][0m - eval_batch_size               :8[0m
[32m[2022-09-08 11:37:38,386] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-09-08 11:37:38,386] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-09-08 11:37:38,386] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-08 11:37:38,386] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-08 11:37:38,386] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-08 11:37:38,386] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-08 11:37:38,386] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-08 11:37:38,386] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-08 11:37:38,386] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-08 11:37:38,386] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-08 11:37:38,386] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-08 11:37:38,386] [    INFO][0m - learning_rate                 :1e-05[0m
[32m[2022-09-08 11:37:38,387] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-08 11:37:38,387] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-08 11:37:38,387] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-08 11:37:38,387] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-08 11:37:38,387] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-08 11:37:38,387] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-08 11:37:38,387] [    INFO][0m - logging_dir                   :./checkpoints/runs/Sep08_11-37-33_instance-3bwob41y-01[0m
[32m[2022-09-08 11:37:38,387] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-08 11:37:38,387] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-08 11:37:38,387] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-08 11:37:38,387] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-08 11:37:38,387] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-08 11:37:38,387] [    INFO][0m - max_seq_length                :64[0m
[32m[2022-09-08 11:37:38,388] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-08 11:37:38,388] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-08 11:37:38,388] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-08 11:37:38,388] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-08 11:37:38,388] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-09-08 11:37:38,388] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-08 11:37:38,388] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-08 11:37:38,388] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-09-08 11:37:38,388] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-08 11:37:38,388] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-08 11:37:38,388] [    INFO][0m - per_device_eval_batch_size    :8[0m
[32m[2022-09-08 11:37:38,388] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-09-08 11:37:38,389] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-08 11:37:38,389] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-08 11:37:38,389] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-08 11:37:38,389] [    INFO][0m - ppt_learning_rate             :0.0001[0m
[32m[2022-09-08 11:37:38,389] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-08 11:37:38,389] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-08 11:37:38,389] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-08 11:37:38,389] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-08 11:37:38,389] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-08 11:37:38,389] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-08 11:37:38,389] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-09-08 11:37:38,389] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-08 11:37:38,389] [    INFO][0m - save_steps                    :100[0m
[32m[2022-09-08 11:37:38,390] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-09-08 11:37:38,390] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-08 11:37:38,390] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-08 11:37:38,390] [    INFO][0m - seed                          :42[0m
[32m[2022-09-08 11:37:38,390] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-08 11:37:38,390] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-08 11:37:38,390] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-08 11:37:38,390] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-09-08 11:37:38,390] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-08 11:37:38,390] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-08 11:37:38,390] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-08 11:37:38,390] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-08 11:37:38,390] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-08 11:37:38,391] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-08 11:37:38,391] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-08 11:37:38,391] [    INFO][0m - [0m
[32m[2022-09-08 11:37:38,393] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-08 11:37:38,393] [    INFO][0m -   Num examples = 1185[0m
[32m[2022-09-08 11:37:38,393] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-09-08 11:37:38,393] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-09-08 11:37:38,393] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-09-08 11:37:38,393] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-08 11:37:38,393] [    INFO][0m -   Total optimization steps = 7450.0[0m
[32m[2022-09-08 11:37:38,393] [    INFO][0m -   Total num train samples = 59250[0m
[32m[2022-09-08 11:37:40,051] [    INFO][0m - loss: 18.32377625, learning_rate: 9.986577181208055e-06, global_step: 10, interval_runtime: 1.6564, interval_samples_per_second: 4.83, interval_steps_per_second: 6.037, epoch: 0.0671[0m
[32m[2022-09-08 11:37:40,689] [    INFO][0m - loss: 6.68310318, learning_rate: 9.973154362416108e-06, global_step: 20, interval_runtime: 0.6384, interval_samples_per_second: 12.531, interval_steps_per_second: 15.664, epoch: 0.1342[0m
[32m[2022-09-08 11:37:41,318] [    INFO][0m - loss: 4.24784279, learning_rate: 9.959731543624162e-06, global_step: 30, interval_runtime: 0.6293, interval_samples_per_second: 12.713, interval_steps_per_second: 15.891, epoch: 0.2013[0m
[32m[2022-09-08 11:37:41,974] [    INFO][0m - loss: 3.87155609, learning_rate: 9.946308724832216e-06, global_step: 40, interval_runtime: 0.6556, interval_samples_per_second: 12.202, interval_steps_per_second: 15.252, epoch: 0.2685[0m
[32m[2022-09-08 11:37:42,609] [    INFO][0m - loss: 2.81488266, learning_rate: 9.93288590604027e-06, global_step: 50, interval_runtime: 0.6346, interval_samples_per_second: 12.606, interval_steps_per_second: 15.758, epoch: 0.3356[0m
[32m[2022-09-08 11:37:43,253] [    INFO][0m - loss: 1.89566574, learning_rate: 9.919463087248323e-06, global_step: 60, interval_runtime: 0.6445, interval_samples_per_second: 12.413, interval_steps_per_second: 15.517, epoch: 0.4027[0m
[32m[2022-09-08 11:37:43,881] [    INFO][0m - loss: 1.82279453, learning_rate: 9.906040268456377e-06, global_step: 70, interval_runtime: 0.628, interval_samples_per_second: 12.74, interval_steps_per_second: 15.925, epoch: 0.4698[0m
[32m[2022-09-08 11:37:44,517] [    INFO][0m - loss: 1.8775631, learning_rate: 9.89261744966443e-06, global_step: 80, interval_runtime: 0.6356, interval_samples_per_second: 12.587, interval_steps_per_second: 15.734, epoch: 0.5369[0m
[32m[2022-09-08 11:37:45,140] [    INFO][0m - loss: 2.08323421, learning_rate: 9.879194630872484e-06, global_step: 90, interval_runtime: 0.6234, interval_samples_per_second: 12.832, interval_steps_per_second: 16.04, epoch: 0.604[0m
[32m[2022-09-08 11:37:45,748] [    INFO][0m - loss: 1.6151989, learning_rate: 9.865771812080538e-06, global_step: 100, interval_runtime: 0.6086, interval_samples_per_second: 13.146, interval_steps_per_second: 16.432, epoch: 0.6711[0m
[32m[2022-09-08 11:37:45,749] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 11:37:45,749] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-08 11:37:45,749] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 11:37:45,749] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 11:37:45,749] [    INFO][0m -   Total prediction steps = 138[0m
[32m[2022-09-08 11:37:49,863] [    INFO][0m - eval_loss: 1.5433508157730103, eval_accuracy: 0.5346083641052246, eval_runtime: 4.1134, eval_samples_per_second: 266.932, eval_steps_per_second: 33.549, epoch: 0.6711[0m
[32m[2022-09-08 11:37:49,900] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-09-08 11:37:49,900] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 11:37:52,997] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-08 11:37:52,998] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-08 11:37:59,744] [    INFO][0m - loss: 1.72991962, learning_rate: 9.852348993288592e-06, global_step: 110, interval_runtime: 13.9954, interval_samples_per_second: 0.572, interval_steps_per_second: 0.715, epoch: 0.7383[0m
[32m[2022-09-08 11:38:00,392] [    INFO][0m - loss: 1.58904619, learning_rate: 9.838926174496645e-06, global_step: 120, interval_runtime: 0.648, interval_samples_per_second: 12.346, interval_steps_per_second: 15.432, epoch: 0.8054[0m
[32m[2022-09-08 11:38:01,041] [    INFO][0m - loss: 1.74755859, learning_rate: 9.825503355704699e-06, global_step: 130, interval_runtime: 0.6487, interval_samples_per_second: 12.333, interval_steps_per_second: 15.416, epoch: 0.8725[0m
[32m[2022-09-08 11:38:01,679] [    INFO][0m - loss: 1.97686996, learning_rate: 9.812080536912753e-06, global_step: 140, interval_runtime: 0.6385, interval_samples_per_second: 12.528, interval_steps_per_second: 15.661, epoch: 0.9396[0m
Traceback (most recent call last):
  File "train_single.py", line 265, in <module>
    main(0)
  File "train_single.py", line 219, in main
    train_result = trainer.train(resume_from_checkpoint=None)
  File "/ssd2/wanghuijuan03/prompt/PaddleNLP/paddlenlp/trainer/trainer_base.py", line 559, in train
    tr_loss_step = self.training_step(model, inputs)
  File "/ssd2/wanghuijuan03/prompt/PaddleNLP/paddlenlp/trainer/trainer_base.py", line 1074, in training_step
    loss = self.compute_loss(model, inputs)
  File "/ssd2/wanghuijuan03/prompt/PaddleNLP/paddlenlp/prompt/prompt_trainer.py", line 268, in compute_loss
    loss = self.criterion(outputs, labels)
  File "/ssd2/wanghuijuan03/anaconda3/envs/prompt/lib/python3.8/site-packages/paddle/fluid/dygraph/layers.py", line 948, in __call__
    return self.forward(*inputs, **kwargs)
  File "/ssd2/wanghuijuan03/anaconda3/envs/prompt/lib/python3.8/site-packages/paddle/nn/layer/loss.py", line 396, in forward
    ret = paddle.nn.functional.cross_entropy(input,
  File "/ssd2/wanghuijuan03/anaconda3/envs/prompt/lib/python3.8/site-packages/paddle/nn/functional/loss.py", line 2379, in cross_entropy
    raise ValueError(
ValueError: Expected nput_dims - 1 = label_dims or input_dims == label_dims             (got nput_dims3, label_dims1)
run.sh: line 59: --freeze_plm: command not found
 
==========
iflytek
==========
 
[33m[2022-09-08 11:38:06,393] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-09-08 11:38:06,393] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-08 11:38:06,394] [    INFO][0m - ============================================================[0m
[32m[2022-09-08 11:38:06,394] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-08 11:38:06,394] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-08 11:38:06,394] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-08 11:38:06,394] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-08 11:38:06,394] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-08 11:38:06,394] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-08 11:38:06,394] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-09-08 11:38:06,394] [    INFO][0m - [0m
[32m[2022-09-08 11:38:06,394] [    INFO][0m - ============================================================[0m
[32m[2022-09-08 11:38:06,394] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-08 11:38:06,394] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-08 11:38:06,395] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-08 11:38:06,395] [    INFO][0m - prompt                        :[0m
[32m[2022-09-08 11:38:06,395] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-08 11:38:06,395] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-08 11:38:06,395] [    INFO][0m - task_name                     :iflytek[0m
[32m[2022-09-08 11:38:06,395] [    INFO][0m - [0m
[32m[2022-09-08 11:38:06,395] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0908 11:38:06.396826 49989 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0908 11:38:06.401101 49989 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-08 11:38:09,230] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-09-08 11:38:09,255] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-09-08 11:38:09,255] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[32m[2022-09-08 11:38:10,302] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùËøô‰∏™Âè•Â≠êÊèèËø∞ÁöÑÂ∫îÁî®Á±ªÂà´ÊòØ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': '„ÄÇ'}][0m
[32m[2022-09-08 11:38:10,507] [    INFO][0m - ============================================================[0m
[32m[2022-09-08 11:38:10,508] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-08 11:38:10,508] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-08 11:38:10,508] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-08 11:38:10,508] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-08 11:38:10,508] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-08 11:38:10,508] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-08 11:38:10,508] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-08 11:38:10,508] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-08 11:38:10,509] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-08 11:38:10,509] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-08 11:38:10,509] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-08 11:38:10,509] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-08 11:38:10,509] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-08 11:38:10,509] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-08 11:38:10,509] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-08 11:38:10,509] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-08 11:38:10,509] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-08 11:38:10,509] [    INFO][0m - eval_batch_size               :8[0m
[32m[2022-09-08 11:38:10,509] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-09-08 11:38:10,509] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-09-08 11:38:10,509] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-08 11:38:10,510] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-08 11:38:10,510] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-08 11:38:10,510] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-08 11:38:10,510] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-08 11:38:10,510] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-08 11:38:10,510] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-08 11:38:10,510] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-08 11:38:10,510] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-08 11:38:10,510] [    INFO][0m - learning_rate                 :1e-05[0m
[32m[2022-09-08 11:38:10,510] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-08 11:38:10,510] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-08 11:38:10,510] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-08 11:38:10,510] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-08 11:38:10,511] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-08 11:38:10,511] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-08 11:38:10,511] [    INFO][0m - logging_dir                   :./checkpoints/runs/Sep08_11-38-06_instance-3bwob41y-01[0m
[32m[2022-09-08 11:38:10,511] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-08 11:38:10,511] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-08 11:38:10,511] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-08 11:38:10,511] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-08 11:38:10,511] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-08 11:38:10,511] [    INFO][0m - max_seq_length                :320[0m
[32m[2022-09-08 11:38:10,511] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-08 11:38:10,511] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-08 11:38:10,511] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-08 11:38:10,511] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-08 11:38:10,512] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-09-08 11:38:10,512] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-08 11:38:10,512] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-08 11:38:10,512] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-09-08 11:38:10,512] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-08 11:38:10,512] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-08 11:38:10,512] [    INFO][0m - per_device_eval_batch_size    :8[0m
[32m[2022-09-08 11:38:10,512] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-09-08 11:38:10,512] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-08 11:38:10,512] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-08 11:38:10,512] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-08 11:38:10,512] [    INFO][0m - ppt_learning_rate             :0.0001[0m
[32m[2022-09-08 11:38:10,512] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-08 11:38:10,512] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-08 11:38:10,513] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-08 11:38:10,513] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-08 11:38:10,513] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-08 11:38:10,513] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-08 11:38:10,513] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-09-08 11:38:10,513] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-08 11:38:10,513] [    INFO][0m - save_steps                    :100[0m
[32m[2022-09-08 11:38:10,513] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-09-08 11:38:10,513] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-08 11:38:10,513] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-08 11:38:10,513] [    INFO][0m - seed                          :42[0m
[32m[2022-09-08 11:38:10,513] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-08 11:38:10,513] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-08 11:38:10,514] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-08 11:38:10,514] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-09-08 11:38:10,514] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-08 11:38:10,514] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-08 11:38:10,514] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-08 11:38:10,514] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-08 11:38:10,514] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-08 11:38:10,514] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-08 11:38:10,514] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-08 11:38:10,514] [    INFO][0m - [0m
[32m[2022-09-08 11:38:10,516] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-08 11:38:10,516] [    INFO][0m -   Num examples = 3024[0m
[32m[2022-09-08 11:38:10,516] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-09-08 11:38:10,516] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-09-08 11:38:10,516] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-09-08 11:38:10,516] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-08 11:38:10,516] [    INFO][0m -   Total optimization steps = 18900.0[0m
[32m[2022-09-08 11:38:10,517] [    INFO][0m -   Total num train samples = 151200[0m
[32m[2022-09-08 11:38:13,385] [    INFO][0m - loss: 21.76152649, learning_rate: 9.994708994708996e-06, global_step: 10, interval_runtime: 2.8676, interval_samples_per_second: 2.79, interval_steps_per_second: 3.487, epoch: 0.0265[0m
[32m[2022-09-08 11:38:15,298] [    INFO][0m - loss: 11.22546692, learning_rate: 9.989417989417989e-06, global_step: 20, interval_runtime: 1.9128, interval_samples_per_second: 4.182, interval_steps_per_second: 5.228, epoch: 0.0529[0m
[32m[2022-09-08 11:38:17,194] [    INFO][0m - loss: 7.16753616, learning_rate: 9.984126984126986e-06, global_step: 30, interval_runtime: 1.8964, interval_samples_per_second: 4.218, interval_steps_per_second: 5.273, epoch: 0.0794[0m
[32m[2022-09-08 11:38:19,141] [    INFO][0m - loss: 6.40000153, learning_rate: 9.97883597883598e-06, global_step: 40, interval_runtime: 1.9464, interval_samples_per_second: 4.11, interval_steps_per_second: 5.138, epoch: 0.1058[0m
[32m[2022-09-08 11:38:21,080] [    INFO][0m - loss: 6.55016632, learning_rate: 9.973544973544974e-06, global_step: 50, interval_runtime: 1.9389, interval_samples_per_second: 4.126, interval_steps_per_second: 5.158, epoch: 0.1323[0m
[32m[2022-09-08 11:38:22,998] [    INFO][0m - loss: 6.14724655, learning_rate: 9.968253968253969e-06, global_step: 60, interval_runtime: 1.9185, interval_samples_per_second: 4.17, interval_steps_per_second: 5.212, epoch: 0.1587[0m
[32m[2022-09-08 11:38:24,913] [    INFO][0m - loss: 6.00889435, learning_rate: 9.962962962962964e-06, global_step: 70, interval_runtime: 1.9155, interval_samples_per_second: 4.176, interval_steps_per_second: 5.221, epoch: 0.1852[0m
[32m[2022-09-08 11:38:26,816] [    INFO][0m - loss: 5.59804535, learning_rate: 9.957671957671959e-06, global_step: 80, interval_runtime: 1.9028, interval_samples_per_second: 4.204, interval_steps_per_second: 5.255, epoch: 0.2116[0m
[32m[2022-09-08 11:38:28,729] [    INFO][0m - loss: 4.88100891, learning_rate: 9.952380952380954e-06, global_step: 90, interval_runtime: 1.913, interval_samples_per_second: 4.182, interval_steps_per_second: 5.227, epoch: 0.2381[0m
[32m[2022-09-08 11:38:30,651] [    INFO][0m - loss: 4.55688095, learning_rate: 9.947089947089947e-06, global_step: 100, interval_runtime: 1.9216, interval_samples_per_second: 4.163, interval_steps_per_second: 5.204, epoch: 0.2646[0m
[32m[2022-09-08 11:38:30,651] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 11:38:30,652] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-08 11:38:30,652] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 11:38:30,652] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 11:38:30,652] [    INFO][0m -   Total prediction steps = 172[0m
[32m[2022-09-08 11:38:46,047] [    INFO][0m - eval_loss: 3.9496917724609375, eval_accuracy: 0.2731245458126068, eval_runtime: 15.3948, eval_samples_per_second: 89.186, eval_steps_per_second: 11.173, epoch: 0.2646[0m
[32m[2022-09-08 11:38:46,100] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-09-08 11:38:46,101] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 11:38:49,015] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-08 11:38:49,016] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-08 11:38:56,373] [    INFO][0m - loss: 3.97842674, learning_rate: 9.941798941798942e-06, global_step: 110, interval_runtime: 25.7219, interval_samples_per_second: 0.311, interval_steps_per_second: 0.389, epoch: 0.291[0m
[32m[2022-09-08 11:38:58,305] [    INFO][0m - loss: 3.56519737, learning_rate: 9.936507936507937e-06, global_step: 120, interval_runtime: 1.9315, interval_samples_per_second: 4.142, interval_steps_per_second: 5.177, epoch: 0.3175[0m
[32m[2022-09-08 11:39:00,205] [    INFO][0m - loss: 3.6552887, learning_rate: 9.931216931216932e-06, global_step: 130, interval_runtime: 1.9009, interval_samples_per_second: 4.209, interval_steps_per_second: 5.261, epoch: 0.3439[0m
[32m[2022-09-08 11:39:02,132] [    INFO][0m - loss: 3.57065887, learning_rate: 9.925925925925927e-06, global_step: 140, interval_runtime: 1.9268, interval_samples_per_second: 4.152, interval_steps_per_second: 5.19, epoch: 0.3704[0m
[32m[2022-09-08 11:39:04,055] [    INFO][0m - loss: 3.79946899, learning_rate: 9.920634920634922e-06, global_step: 150, interval_runtime: 1.9231, interval_samples_per_second: 4.16, interval_steps_per_second: 5.2, epoch: 0.3968[0m
[32m[2022-09-08 11:39:05,968] [    INFO][0m - loss: 3.59819336, learning_rate: 9.915343915343916e-06, global_step: 160, interval_runtime: 1.9127, interval_samples_per_second: 4.183, interval_steps_per_second: 5.228, epoch: 0.4233[0m
[32m[2022-09-08 11:39:07,902] [    INFO][0m - loss: 3.57132797, learning_rate: 9.91005291005291e-06, global_step: 170, interval_runtime: 1.9347, interval_samples_per_second: 4.135, interval_steps_per_second: 5.169, epoch: 0.4497[0m
[32m[2022-09-08 11:39:09,824] [    INFO][0m - loss: 3.51026115, learning_rate: 9.904761904761906e-06, global_step: 180, interval_runtime: 1.9211, interval_samples_per_second: 4.164, interval_steps_per_second: 5.205, epoch: 0.4762[0m
[32m[2022-09-08 11:39:11,743] [    INFO][0m - loss: 3.14641533, learning_rate: 9.8994708994709e-06, global_step: 190, interval_runtime: 1.9195, interval_samples_per_second: 4.168, interval_steps_per_second: 5.21, epoch: 0.5026[0m
[32m[2022-09-08 11:39:13,651] [    INFO][0m - loss: 3.40737457, learning_rate: 9.894179894179896e-06, global_step: 200, interval_runtime: 1.9083, interval_samples_per_second: 4.192, interval_steps_per_second: 5.24, epoch: 0.5291[0m
[32m[2022-09-08 11:39:13,652] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 11:39:13,652] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-08 11:39:13,652] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 11:39:13,653] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 11:39:13,653] [    INFO][0m -   Total prediction steps = 172[0m
[32m[2022-09-08 11:39:28,718] [    INFO][0m - eval_loss: 2.692960739135742, eval_accuracy: 0.44282594323158264, eval_runtime: 15.0652, eval_samples_per_second: 91.137, eval_steps_per_second: 11.417, epoch: 0.5291[0m
[32m[2022-09-08 11:39:28,772] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-09-08 11:39:28,772] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 11:39:32,027] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-09-08 11:39:32,028] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-09-08 11:39:39,353] [    INFO][0m - loss: 2.99574928, learning_rate: 9.88888888888889e-06, global_step: 210, interval_runtime: 25.7007, interval_samples_per_second: 0.311, interval_steps_per_second: 0.389, epoch: 0.5556[0m
[32m[2022-09-08 11:39:41,260] [    INFO][0m - loss: 3.06165771, learning_rate: 9.883597883597884e-06, global_step: 220, interval_runtime: 1.907, interval_samples_per_second: 4.195, interval_steps_per_second: 5.244, epoch: 0.582[0m
[32m[2022-09-08 11:39:43,175] [    INFO][0m - loss: 2.79916992, learning_rate: 9.878306878306879e-06, global_step: 230, interval_runtime: 1.9161, interval_samples_per_second: 4.175, interval_steps_per_second: 5.219, epoch: 0.6085[0m
[32m[2022-09-08 11:39:45,097] [    INFO][0m - loss: 2.84986477, learning_rate: 9.873015873015874e-06, global_step: 240, interval_runtime: 1.9222, interval_samples_per_second: 4.162, interval_steps_per_second: 5.202, epoch: 0.6349[0m
[32m[2022-09-08 11:39:47,007] [    INFO][0m - loss: 2.57130127, learning_rate: 9.867724867724869e-06, global_step: 250, interval_runtime: 1.909, interval_samples_per_second: 4.191, interval_steps_per_second: 5.238, epoch: 0.6614[0m
[32m[2022-09-08 11:39:48,925] [    INFO][0m - loss: 2.62985649, learning_rate: 9.862433862433864e-06, global_step: 260, interval_runtime: 1.9183, interval_samples_per_second: 4.17, interval_steps_per_second: 5.213, epoch: 0.6878[0m
[32m[2022-09-08 11:39:50,820] [    INFO][0m - loss: 2.40393066, learning_rate: 9.857142857142859e-06, global_step: 270, interval_runtime: 1.8952, interval_samples_per_second: 4.221, interval_steps_per_second: 5.276, epoch: 0.7143[0m
[32m[2022-09-08 11:39:52,740] [    INFO][0m - loss: 2.91631031, learning_rate: 9.851851851851852e-06, global_step: 280, interval_runtime: 1.9199, interval_samples_per_second: 4.167, interval_steps_per_second: 5.209, epoch: 0.7407[0m
[32m[2022-09-08 11:39:54,654] [    INFO][0m - loss: 2.43698235, learning_rate: 9.846560846560847e-06, global_step: 290, interval_runtime: 1.9137, interval_samples_per_second: 4.18, interval_steps_per_second: 5.226, epoch: 0.7672[0m
[32m[2022-09-08 11:39:56,567] [    INFO][0m - loss: 2.60911064, learning_rate: 9.841269841269842e-06, global_step: 300, interval_runtime: 1.9131, interval_samples_per_second: 4.182, interval_steps_per_second: 5.227, epoch: 0.7937[0m
[32m[2022-09-08 11:39:56,567] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 11:39:56,567] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-08 11:39:56,567] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 11:39:56,567] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 11:39:56,568] [    INFO][0m -   Total prediction steps = 172[0m
[32m[2022-09-08 11:40:11,601] [    INFO][0m - eval_loss: 2.262831687927246, eval_accuracy: 0.4785142242908478, eval_runtime: 15.033, eval_samples_per_second: 91.332, eval_steps_per_second: 11.441, epoch: 0.7937[0m
[32m[2022-09-08 11:40:11,650] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-09-08 11:40:11,650] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 11:40:14,815] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-09-08 11:40:14,816] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-09-08 11:40:22,167] [    INFO][0m - loss: 2.82752724, learning_rate: 9.835978835978837e-06, global_step: 310, interval_runtime: 25.5997, interval_samples_per_second: 0.313, interval_steps_per_second: 0.391, epoch: 0.8201[0m
[32m[2022-09-08 11:40:24,093] [    INFO][0m - loss: 2.14835262, learning_rate: 9.830687830687832e-06, global_step: 320, interval_runtime: 1.9264, interval_samples_per_second: 4.153, interval_steps_per_second: 5.191, epoch: 0.8466[0m
[32m[2022-09-08 11:40:26,001] [    INFO][0m - loss: 2.63120785, learning_rate: 9.825396825396825e-06, global_step: 330, interval_runtime: 1.9078, interval_samples_per_second: 4.193, interval_steps_per_second: 5.242, epoch: 0.873[0m
[32m[2022-09-08 11:40:27,915] [    INFO][0m - loss: 2.45406857, learning_rate: 9.82010582010582e-06, global_step: 340, interval_runtime: 1.9144, interval_samples_per_second: 4.179, interval_steps_per_second: 5.224, epoch: 0.8995[0m
[32m[2022-09-08 11:40:29,828] [    INFO][0m - loss: 2.72249603, learning_rate: 9.814814814814815e-06, global_step: 350, interval_runtime: 1.913, interval_samples_per_second: 4.182, interval_steps_per_second: 5.227, epoch: 0.9259[0m
[32m[2022-09-08 11:40:31,738] [    INFO][0m - loss: 2.80947895, learning_rate: 9.80952380952381e-06, global_step: 360, interval_runtime: 1.9096, interval_samples_per_second: 4.189, interval_steps_per_second: 5.237, epoch: 0.9524[0m
[32m[2022-09-08 11:40:33,660] [    INFO][0m - loss: 2.38773308, learning_rate: 9.804232804232805e-06, global_step: 370, interval_runtime: 1.9227, interval_samples_per_second: 4.161, interval_steps_per_second: 5.201, epoch: 0.9788[0m
[32m[2022-09-08 11:40:35,625] [    INFO][0m - loss: 2.13143654, learning_rate: 9.7989417989418e-06, global_step: 380, interval_runtime: 1.9648, interval_samples_per_second: 4.072, interval_steps_per_second: 5.09, epoch: 1.0053[0m
[32m[2022-09-08 11:40:37,534] [    INFO][0m - loss: 2.18665047, learning_rate: 9.793650793650794e-06, global_step: 390, interval_runtime: 1.9086, interval_samples_per_second: 4.191, interval_steps_per_second: 5.239, epoch: 1.0317[0m
[32m[2022-09-08 11:40:39,441] [    INFO][0m - loss: 1.84988747, learning_rate: 9.788359788359789e-06, global_step: 400, interval_runtime: 1.907, interval_samples_per_second: 4.195, interval_steps_per_second: 5.244, epoch: 1.0582[0m
[32m[2022-09-08 11:40:39,441] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 11:40:39,441] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-08 11:40:39,442] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 11:40:39,442] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 11:40:39,442] [    INFO][0m -   Total prediction steps = 172[0m
[32m[2022-09-08 11:40:54,523] [    INFO][0m - eval_loss: 2.1476151943206787, eval_accuracy: 0.4952658712863922, eval_runtime: 15.0809, eval_samples_per_second: 91.042, eval_steps_per_second: 11.405, epoch: 1.0582[0m
[32m[2022-09-08 11:40:54,578] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-09-08 11:40:54,579] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 11:40:57,338] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-09-08 11:40:57,339] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-09-08 11:41:04,628] [    INFO][0m - loss: 2.00353756, learning_rate: 9.783068783068784e-06, global_step: 410, interval_runtime: 25.1874, interval_samples_per_second: 0.318, interval_steps_per_second: 0.397, epoch: 1.0847[0m
[32m[2022-09-08 11:41:06,551] [    INFO][0m - loss: 2.08641663, learning_rate: 9.777777777777779e-06, global_step: 420, interval_runtime: 1.9225, interval_samples_per_second: 4.161, interval_steps_per_second: 5.201, epoch: 1.1111[0m
[32m[2022-09-08 11:41:08,462] [    INFO][0m - loss: 1.87450581, learning_rate: 9.772486772486774e-06, global_step: 430, interval_runtime: 1.9109, interval_samples_per_second: 4.186, interval_steps_per_second: 5.233, epoch: 1.1376[0m
[32m[2022-09-08 11:41:10,366] [    INFO][0m - loss: 1.84530773, learning_rate: 9.767195767195769e-06, global_step: 440, interval_runtime: 1.904, interval_samples_per_second: 4.202, interval_steps_per_second: 5.252, epoch: 1.164[0m
[32m[2022-09-08 11:41:12,301] [    INFO][0m - loss: 1.99877148, learning_rate: 9.761904761904762e-06, global_step: 450, interval_runtime: 1.9347, interval_samples_per_second: 4.135, interval_steps_per_second: 5.169, epoch: 1.1905[0m
[32m[2022-09-08 11:41:14,231] [    INFO][0m - loss: 2.0949398, learning_rate: 9.756613756613757e-06, global_step: 460, interval_runtime: 1.9308, interval_samples_per_second: 4.143, interval_steps_per_second: 5.179, epoch: 1.2169[0m
[32m[2022-09-08 11:41:16,139] [    INFO][0m - loss: 2.20785961, learning_rate: 9.751322751322752e-06, global_step: 470, interval_runtime: 1.9075, interval_samples_per_second: 4.194, interval_steps_per_second: 5.243, epoch: 1.2434[0m
[32m[2022-09-08 11:41:18,058] [    INFO][0m - loss: 1.76975765, learning_rate: 9.746031746031747e-06, global_step: 480, interval_runtime: 1.9191, interval_samples_per_second: 4.169, interval_steps_per_second: 5.211, epoch: 1.2698[0m
[32m[2022-09-08 11:41:19,964] [    INFO][0m - loss: 2.00604858, learning_rate: 9.740740740740742e-06, global_step: 490, interval_runtime: 1.9057, interval_samples_per_second: 4.198, interval_steps_per_second: 5.247, epoch: 1.2963[0m
[32m[2022-09-08 11:41:21,891] [    INFO][0m - loss: 2.45833511, learning_rate: 9.735449735449735e-06, global_step: 500, interval_runtime: 1.9269, interval_samples_per_second: 4.152, interval_steps_per_second: 5.19, epoch: 1.3228[0m
[32m[2022-09-08 11:41:21,892] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 11:41:21,892] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-08 11:41:21,892] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 11:41:21,892] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 11:41:21,892] [    INFO][0m -   Total prediction steps = 172[0m
[32m[2022-09-08 11:41:37,007] [    INFO][0m - eval_loss: 2.0308172702789307, eval_accuracy: 0.49963584542274475, eval_runtime: 15.115, eval_samples_per_second: 90.837, eval_steps_per_second: 11.379, epoch: 1.3228[0m
[32m[2022-09-08 11:41:37,063] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-09-08 11:41:37,063] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 11:41:40,142] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-09-08 11:41:40,142] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-09-08 11:41:47,122] [    INFO][0m - loss: 2.26576195, learning_rate: 9.73015873015873e-06, global_step: 510, interval_runtime: 25.2311, interval_samples_per_second: 0.317, interval_steps_per_second: 0.396, epoch: 1.3492[0m
[32m[2022-09-08 11:41:49,046] [    INFO][0m - loss: 2.33344727, learning_rate: 9.724867724867725e-06, global_step: 520, interval_runtime: 1.9248, interval_samples_per_second: 4.156, interval_steps_per_second: 5.195, epoch: 1.3757[0m
[32m[2022-09-08 11:41:50,969] [    INFO][0m - loss: 1.9270813, learning_rate: 9.71957671957672e-06, global_step: 530, interval_runtime: 1.9225, interval_samples_per_second: 4.161, interval_steps_per_second: 5.201, epoch: 1.4021[0m
[32m[2022-09-08 11:41:52,909] [    INFO][0m - loss: 2.14503746, learning_rate: 9.714285714285715e-06, global_step: 540, interval_runtime: 1.9399, interval_samples_per_second: 4.124, interval_steps_per_second: 5.155, epoch: 1.4286[0m
[32m[2022-09-08 11:41:54,830] [    INFO][0m - loss: 2.01255817, learning_rate: 9.70899470899471e-06, global_step: 550, interval_runtime: 1.9209, interval_samples_per_second: 4.165, interval_steps_per_second: 5.206, epoch: 1.455[0m
[32m[2022-09-08 11:41:56,768] [    INFO][0m - loss: 2.06550503, learning_rate: 9.703703703703703e-06, global_step: 560, interval_runtime: 1.9376, interval_samples_per_second: 4.129, interval_steps_per_second: 5.161, epoch: 1.4815[0m
[32m[2022-09-08 11:41:58,686] [    INFO][0m - loss: 1.99914665, learning_rate: 9.698412698412698e-06, global_step: 570, interval_runtime: 1.9185, interval_samples_per_second: 4.17, interval_steps_per_second: 5.212, epoch: 1.5079[0m
[32m[2022-09-08 11:42:00,612] [    INFO][0m - loss: 1.91109505, learning_rate: 9.693121693121693e-06, global_step: 580, interval_runtime: 1.9259, interval_samples_per_second: 4.154, interval_steps_per_second: 5.192, epoch: 1.5344[0m
[32m[2022-09-08 11:42:02,544] [    INFO][0m - loss: 2.08381882, learning_rate: 9.687830687830688e-06, global_step: 590, interval_runtime: 1.9316, interval_samples_per_second: 4.142, interval_steps_per_second: 5.177, epoch: 1.5608[0m
[32m[2022-09-08 11:42:04,469] [    INFO][0m - loss: 1.91427269, learning_rate: 9.682539682539683e-06, global_step: 600, interval_runtime: 1.9258, interval_samples_per_second: 4.154, interval_steps_per_second: 5.193, epoch: 1.5873[0m
[32m[2022-09-08 11:42:04,470] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 11:42:04,470] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-08 11:42:04,470] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 11:42:04,470] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 11:42:04,470] [    INFO][0m -   Total prediction steps = 172[0m
[32m[2022-09-08 11:42:19,754] [    INFO][0m - eval_loss: 1.8928254842758179, eval_accuracy: 0.5178441405296326, eval_runtime: 15.2837, eval_samples_per_second: 89.834, eval_steps_per_second: 11.254, epoch: 1.5873[0m
[32m[2022-09-08 11:42:19,806] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-09-08 11:42:19,806] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 11:42:23,233] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-09-08 11:42:23,233] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-09-08 11:42:30,499] [    INFO][0m - loss: 2.05491886, learning_rate: 9.677248677248678e-06, global_step: 610, interval_runtime: 26.0295, interval_samples_per_second: 0.307, interval_steps_per_second: 0.384, epoch: 1.6138[0m
[32m[2022-09-08 11:42:32,419] [    INFO][0m - loss: 2.20031319, learning_rate: 9.671957671957672e-06, global_step: 620, interval_runtime: 1.9201, interval_samples_per_second: 4.166, interval_steps_per_second: 5.208, epoch: 1.6402[0m
[32m[2022-09-08 11:42:34,329] [    INFO][0m - loss: 1.71881618, learning_rate: 9.666666666666667e-06, global_step: 630, interval_runtime: 1.9103, interval_samples_per_second: 4.188, interval_steps_per_second: 5.235, epoch: 1.6667[0m
[32m[2022-09-08 11:42:36,256] [    INFO][0m - loss: 2.00921459, learning_rate: 9.661375661375663e-06, global_step: 640, interval_runtime: 1.9266, interval_samples_per_second: 4.152, interval_steps_per_second: 5.191, epoch: 1.6931[0m
[32m[2022-09-08 11:42:38,177] [    INFO][0m - loss: 1.89678707, learning_rate: 9.656084656084657e-06, global_step: 650, interval_runtime: 1.9218, interval_samples_per_second: 4.163, interval_steps_per_second: 5.203, epoch: 1.7196[0m
[32m[2022-09-08 11:42:40,089] [    INFO][0m - loss: 2.31013851, learning_rate: 9.650793650793652e-06, global_step: 660, interval_runtime: 1.9113, interval_samples_per_second: 4.186, interval_steps_per_second: 5.232, epoch: 1.746[0m
[32m[2022-09-08 11:42:41,992] [    INFO][0m - loss: 1.92381477, learning_rate: 9.645502645502647e-06, global_step: 670, interval_runtime: 1.9034, interval_samples_per_second: 4.203, interval_steps_per_second: 5.254, epoch: 1.7725[0m
[32m[2022-09-08 11:42:43,904] [    INFO][0m - loss: 2.17452507, learning_rate: 9.64021164021164e-06, global_step: 680, interval_runtime: 1.9117, interval_samples_per_second: 4.185, interval_steps_per_second: 5.231, epoch: 1.7989[0m
[32m[2022-09-08 11:42:45,808] [    INFO][0m - loss: 1.88179874, learning_rate: 9.634920634920637e-06, global_step: 690, interval_runtime: 1.9042, interval_samples_per_second: 4.201, interval_steps_per_second: 5.252, epoch: 1.8254[0m
[32m[2022-09-08 11:42:47,712] [    INFO][0m - loss: 1.91718712, learning_rate: 9.62962962962963e-06, global_step: 700, interval_runtime: 1.9042, interval_samples_per_second: 4.201, interval_steps_per_second: 5.251, epoch: 1.8519[0m
[32m[2022-09-08 11:42:47,713] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 11:42:47,713] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-08 11:42:47,713] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 11:42:47,713] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 11:42:47,713] [    INFO][0m -   Total prediction steps = 172[0m
[32m[2022-09-08 11:43:02,858] [    INFO][0m - eval_loss: 1.8513628244400024, eval_accuracy: 0.5280408263206482, eval_runtime: 15.1446, eval_samples_per_second: 90.66, eval_steps_per_second: 11.357, epoch: 1.8519[0m
[32m[2022-09-08 11:43:02,910] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-700[0m
[32m[2022-09-08 11:43:02,911] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 11:43:04,533] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-700/tokenizer_config.json[0m
[32m[2022-09-08 11:43:04,534] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-700/special_tokens_map.json[0m
[32m[2022-09-08 11:43:08,763] [    INFO][0m - loss: 2.18774643, learning_rate: 9.624338624338625e-06, global_step: 710, interval_runtime: 21.05, interval_samples_per_second: 0.38, interval_steps_per_second: 0.475, epoch: 1.8783[0m
[32m[2022-09-08 11:43:10,682] [    INFO][0m - loss: 1.79511604, learning_rate: 9.61904761904762e-06, global_step: 720, interval_runtime: 1.9202, interval_samples_per_second: 4.166, interval_steps_per_second: 5.208, epoch: 1.9048[0m
[32m[2022-09-08 11:43:12,611] [    INFO][0m - loss: 1.96971836, learning_rate: 9.613756613756613e-06, global_step: 730, interval_runtime: 1.9287, interval_samples_per_second: 4.148, interval_steps_per_second: 5.185, epoch: 1.9312[0m
[32m[2022-09-08 11:43:14,526] [    INFO][0m - loss: 1.847616, learning_rate: 9.60846560846561e-06, global_step: 740, interval_runtime: 1.9152, interval_samples_per_second: 4.177, interval_steps_per_second: 5.222, epoch: 1.9577[0m
[32m[2022-09-08 11:43:16,445] [    INFO][0m - loss: 1.74128036, learning_rate: 9.603174603174605e-06, global_step: 750, interval_runtime: 1.9186, interval_samples_per_second: 4.17, interval_steps_per_second: 5.212, epoch: 1.9841[0m
[32m[2022-09-08 11:43:18,391] [    INFO][0m - loss: 1.60893078, learning_rate: 9.597883597883598e-06, global_step: 760, interval_runtime: 1.9458, interval_samples_per_second: 4.111, interval_steps_per_second: 5.139, epoch: 2.0106[0m
[32m[2022-09-08 11:43:20,311] [    INFO][0m - loss: 1.47508326, learning_rate: 9.592592592592593e-06, global_step: 770, interval_runtime: 1.9198, interval_samples_per_second: 4.167, interval_steps_per_second: 5.209, epoch: 2.037[0m
[32m[2022-09-08 11:43:22,251] [    INFO][0m - loss: 1.79441967, learning_rate: 9.587301587301588e-06, global_step: 780, interval_runtime: 1.9401, interval_samples_per_second: 4.124, interval_steps_per_second: 5.154, epoch: 2.0635[0m
[32m[2022-09-08 11:43:24,186] [    INFO][0m - loss: 1.98226089, learning_rate: 9.582010582010583e-06, global_step: 790, interval_runtime: 1.9351, interval_samples_per_second: 4.134, interval_steps_per_second: 5.168, epoch: 2.0899[0m
[32m[2022-09-08 11:43:26,086] [    INFO][0m - loss: 1.50556793, learning_rate: 9.576719576719578e-06, global_step: 800, interval_runtime: 1.9004, interval_samples_per_second: 4.21, interval_steps_per_second: 5.262, epoch: 2.1164[0m
[32m[2022-09-08 11:43:26,087] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 11:43:26,087] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-08 11:43:26,087] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 11:43:26,087] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 11:43:26,087] [    INFO][0m -   Total prediction steps = 172[0m
[32m[2022-09-08 11:43:41,224] [    INFO][0m - eval_loss: 1.8750355243682861, eval_accuracy: 0.5389657616615295, eval_runtime: 15.1367, eval_samples_per_second: 90.707, eval_steps_per_second: 11.363, epoch: 2.1164[0m
[32m[2022-09-08 11:43:41,277] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-800[0m
[32m[2022-09-08 11:43:41,277] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 11:43:42,788] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-800/tokenizer_config.json[0m
[32m[2022-09-08 11:43:42,789] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-800/special_tokens_map.json[0m
[32m[2022-09-08 11:43:47,017] [    INFO][0m - loss: 1.76326847, learning_rate: 9.571428571428573e-06, global_step: 810, interval_runtime: 20.931, interval_samples_per_second: 0.382, interval_steps_per_second: 0.478, epoch: 2.1429[0m
[32m[2022-09-08 11:43:48,914] [    INFO][0m - loss: 1.72337017, learning_rate: 9.566137566137567e-06, global_step: 820, interval_runtime: 1.8965, interval_samples_per_second: 4.218, interval_steps_per_second: 5.273, epoch: 2.1693[0m
[32m[2022-09-08 11:43:50,808] [    INFO][0m - loss: 1.63829613, learning_rate: 9.560846560846561e-06, global_step: 830, interval_runtime: 1.8944, interval_samples_per_second: 4.223, interval_steps_per_second: 5.279, epoch: 2.1958[0m
[32m[2022-09-08 11:43:52,733] [    INFO][0m - loss: 1.6036911, learning_rate: 9.555555555555556e-06, global_step: 840, interval_runtime: 1.9253, interval_samples_per_second: 4.155, interval_steps_per_second: 5.194, epoch: 2.2222[0m
[32m[2022-09-08 11:43:54,658] [    INFO][0m - loss: 1.69515495, learning_rate: 9.550264550264551e-06, global_step: 850, interval_runtime: 1.9248, interval_samples_per_second: 4.156, interval_steps_per_second: 5.195, epoch: 2.2487[0m
[32m[2022-09-08 11:43:56,593] [    INFO][0m - loss: 1.6602972, learning_rate: 9.544973544973546e-06, global_step: 860, interval_runtime: 1.9344, interval_samples_per_second: 4.136, interval_steps_per_second: 5.17, epoch: 2.2751[0m
[32m[2022-09-08 11:43:58,502] [    INFO][0m - loss: 1.93265018, learning_rate: 9.539682539682541e-06, global_step: 870, interval_runtime: 1.9094, interval_samples_per_second: 4.19, interval_steps_per_second: 5.237, epoch: 2.3016[0m
[32m[2022-09-08 11:44:00,417] [    INFO][0m - loss: 1.5354888, learning_rate: 9.534391534391535e-06, global_step: 880, interval_runtime: 1.9149, interval_samples_per_second: 4.178, interval_steps_per_second: 5.222, epoch: 2.328[0m
[32m[2022-09-08 11:44:02,355] [    INFO][0m - loss: 1.59697094, learning_rate: 9.52910052910053e-06, global_step: 890, interval_runtime: 1.9377, interval_samples_per_second: 4.129, interval_steps_per_second: 5.161, epoch: 2.3545[0m
[32m[2022-09-08 11:44:04,271] [    INFO][0m - loss: 1.78218536, learning_rate: 9.523809523809525e-06, global_step: 900, interval_runtime: 1.9164, interval_samples_per_second: 4.174, interval_steps_per_second: 5.218, epoch: 2.381[0m
[32m[2022-09-08 11:44:04,272] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 11:44:04,272] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-08 11:44:04,272] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 11:44:04,272] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 11:44:04,272] [    INFO][0m -   Total prediction steps = 172[0m
[32m[2022-09-08 11:44:19,372] [    INFO][0m - eval_loss: 1.8404792547225952, eval_accuracy: 0.5316824913024902, eval_runtime: 15.0998, eval_samples_per_second: 90.928, eval_steps_per_second: 11.391, epoch: 2.381[0m
[32m[2022-09-08 11:44:19,429] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-900[0m
[32m[2022-09-08 11:44:19,429] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 11:44:21,107] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-900/tokenizer_config.json[0m
[32m[2022-09-08 11:44:21,108] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-900/special_tokens_map.json[0m
[32m[2022-09-08 11:44:25,998] [    INFO][0m - loss: 1.7129776, learning_rate: 9.51851851851852e-06, global_step: 910, interval_runtime: 21.7264, interval_samples_per_second: 0.368, interval_steps_per_second: 0.46, epoch: 2.4074[0m
[32m[2022-09-08 11:44:27,939] [    INFO][0m - loss: 1.60339413, learning_rate: 9.513227513227515e-06, global_step: 920, interval_runtime: 1.9408, interval_samples_per_second: 4.122, interval_steps_per_second: 5.153, epoch: 2.4339[0m
[32m[2022-09-08 11:44:29,873] [    INFO][0m - loss: 1.78351097, learning_rate: 9.507936507936508e-06, global_step: 930, interval_runtime: 1.9349, interval_samples_per_second: 4.135, interval_steps_per_second: 5.168, epoch: 2.4603[0m
[32m[2022-09-08 11:44:31,786] [    INFO][0m - loss: 1.4084549, learning_rate: 9.502645502645503e-06, global_step: 940, interval_runtime: 1.9121, interval_samples_per_second: 4.184, interval_steps_per_second: 5.23, epoch: 2.4868[0m
[32m[2022-09-08 11:44:33,726] [    INFO][0m - loss: 1.77052479, learning_rate: 9.497354497354498e-06, global_step: 950, interval_runtime: 1.9404, interval_samples_per_second: 4.123, interval_steps_per_second: 5.154, epoch: 2.5132[0m
[32m[2022-09-08 11:44:35,648] [    INFO][0m - loss: 1.73107624, learning_rate: 9.492063492063493e-06, global_step: 960, interval_runtime: 1.922, interval_samples_per_second: 4.162, interval_steps_per_second: 5.203, epoch: 2.5397[0m
[32m[2022-09-08 11:44:37,563] [    INFO][0m - loss: 1.89286652, learning_rate: 9.486772486772488e-06, global_step: 970, interval_runtime: 1.9154, interval_samples_per_second: 4.177, interval_steps_per_second: 5.221, epoch: 2.5661[0m
[32m[2022-09-08 11:44:39,492] [    INFO][0m - loss: 1.57074986, learning_rate: 9.481481481481483e-06, global_step: 980, interval_runtime: 1.9284, interval_samples_per_second: 4.149, interval_steps_per_second: 5.186, epoch: 2.5926[0m
[32m[2022-09-08 11:44:41,389] [    INFO][0m - loss: 1.67367115, learning_rate: 9.476190476190476e-06, global_step: 990, interval_runtime: 1.8971, interval_samples_per_second: 4.217, interval_steps_per_second: 5.271, epoch: 2.619[0m
[32m[2022-09-08 11:44:43,299] [    INFO][0m - loss: 1.78868256, learning_rate: 9.470899470899471e-06, global_step: 1000, interval_runtime: 1.9102, interval_samples_per_second: 4.188, interval_steps_per_second: 5.235, epoch: 2.6455[0m
[32m[2022-09-08 11:44:43,300] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 11:44:43,300] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-08 11:44:43,300] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 11:44:43,300] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 11:44:43,300] [    INFO][0m -   Total prediction steps = 172[0m
[32m[2022-09-08 11:44:58,678] [    INFO][0m - eval_loss: 2.162229299545288, eval_accuracy: 0.5083758234977722, eval_runtime: 15.3767, eval_samples_per_second: 89.291, eval_steps_per_second: 11.186, epoch: 2.6455[0m
[32m[2022-09-08 11:44:58,728] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1000[0m
[32m[2022-09-08 11:44:58,729] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 11:45:00,288] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1000/tokenizer_config.json[0m
[32m[2022-09-08 11:45:00,288] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1000/special_tokens_map.json[0m
[32m[2022-09-08 11:45:04,652] [    INFO][0m - loss: 1.64996433, learning_rate: 9.465608465608466e-06, global_step: 1010, interval_runtime: 21.3522, interval_samples_per_second: 0.375, interval_steps_per_second: 0.468, epoch: 2.672[0m
[32m[2022-09-08 11:45:06,550] [    INFO][0m - loss: 1.740448, learning_rate: 9.460317460317461e-06, global_step: 1020, interval_runtime: 1.899, interval_samples_per_second: 4.213, interval_steps_per_second: 5.266, epoch: 2.6984[0m
[32m[2022-09-08 11:45:08,431] [    INFO][0m - loss: 1.76221695, learning_rate: 9.455026455026456e-06, global_step: 1030, interval_runtime: 1.8807, interval_samples_per_second: 4.254, interval_steps_per_second: 5.317, epoch: 2.7249[0m
[32m[2022-09-08 11:45:10,346] [    INFO][0m - loss: 1.82318192, learning_rate: 9.449735449735451e-06, global_step: 1040, interval_runtime: 1.9157, interval_samples_per_second: 4.176, interval_steps_per_second: 5.22, epoch: 2.7513[0m
[32m[2022-09-08 11:45:12,242] [    INFO][0m - loss: 2.08723392, learning_rate: 9.444444444444445e-06, global_step: 1050, interval_runtime: 1.8956, interval_samples_per_second: 4.22, interval_steps_per_second: 5.275, epoch: 2.7778[0m
[32m[2022-09-08 11:45:14,167] [    INFO][0m - loss: 1.62542973, learning_rate: 9.43915343915344e-06, global_step: 1060, interval_runtime: 1.9247, interval_samples_per_second: 4.157, interval_steps_per_second: 5.196, epoch: 2.8042[0m
[32m[2022-09-08 11:45:16,063] [    INFO][0m - loss: 1.84194965, learning_rate: 9.433862433862435e-06, global_step: 1070, interval_runtime: 1.8964, interval_samples_per_second: 4.218, interval_steps_per_second: 5.273, epoch: 2.8307[0m
[32m[2022-09-08 11:45:17,967] [    INFO][0m - loss: 1.79040489, learning_rate: 9.42857142857143e-06, global_step: 1080, interval_runtime: 1.9039, interval_samples_per_second: 4.202, interval_steps_per_second: 5.252, epoch: 2.8571[0m
[32m[2022-09-08 11:45:19,868] [    INFO][0m - loss: 1.61664467, learning_rate: 9.423280423280425e-06, global_step: 1090, interval_runtime: 1.901, interval_samples_per_second: 4.208, interval_steps_per_second: 5.26, epoch: 2.8836[0m
[32m[2022-09-08 11:45:21,779] [    INFO][0m - loss: 1.69250011, learning_rate: 9.417989417989418e-06, global_step: 1100, interval_runtime: 1.9113, interval_samples_per_second: 4.186, interval_steps_per_second: 5.232, epoch: 2.9101[0m
[32m[2022-09-08 11:45:21,780] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 11:45:21,780] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-08 11:45:21,780] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 11:45:21,780] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 11:45:21,781] [    INFO][0m -   Total prediction steps = 172[0m
[32m[2022-09-08 11:45:36,785] [    INFO][0m - eval_loss: 2.284006118774414, eval_accuracy: 0.509832501411438, eval_runtime: 15.0039, eval_samples_per_second: 91.509, eval_steps_per_second: 11.464, epoch: 2.9101[0m
[32m[2022-09-08 11:45:36,838] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1100[0m
[32m[2022-09-08 11:45:36,838] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 11:45:38,384] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1100/tokenizer_config.json[0m
[32m[2022-09-08 11:45:38,385] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1100/special_tokens_map.json[0m
[32m[2022-09-08 11:45:42,691] [    INFO][0m - loss: 1.7607935, learning_rate: 9.412698412698413e-06, global_step: 1110, interval_runtime: 20.9112, interval_samples_per_second: 0.383, interval_steps_per_second: 0.478, epoch: 2.9365[0m
[32m[2022-09-08 11:45:44,594] [    INFO][0m - loss: 1.88300095, learning_rate: 9.407407407407408e-06, global_step: 1120, interval_runtime: 1.9033, interval_samples_per_second: 4.203, interval_steps_per_second: 5.254, epoch: 2.963[0m
[32m[2022-09-08 11:45:46,491] [    INFO][0m - loss: 1.55261059, learning_rate: 9.402116402116403e-06, global_step: 1130, interval_runtime: 1.8975, interval_samples_per_second: 4.216, interval_steps_per_second: 5.27, epoch: 2.9894[0m
[32m[2022-09-08 11:45:48,443] [    INFO][0m - loss: 1.68434525, learning_rate: 9.396825396825398e-06, global_step: 1140, interval_runtime: 1.9518, interval_samples_per_second: 4.099, interval_steps_per_second: 5.123, epoch: 3.0159[0m
[32m[2022-09-08 11:45:50,379] [    INFO][0m - loss: 1.19853325, learning_rate: 9.391534391534393e-06, global_step: 1150, interval_runtime: 1.9359, interval_samples_per_second: 4.132, interval_steps_per_second: 5.165, epoch: 3.0423[0m
[32m[2022-09-08 11:45:52,284] [    INFO][0m - loss: 1.55352783, learning_rate: 9.386243386243386e-06, global_step: 1160, interval_runtime: 1.9052, interval_samples_per_second: 4.199, interval_steps_per_second: 5.249, epoch: 3.0688[0m
[32m[2022-09-08 11:45:54,190] [    INFO][0m - loss: 1.56486883, learning_rate: 9.380952380952381e-06, global_step: 1170, interval_runtime: 1.9056, interval_samples_per_second: 4.198, interval_steps_per_second: 5.248, epoch: 3.0952[0m
[32m[2022-09-08 11:45:56,112] [    INFO][0m - loss: 1.57396345, learning_rate: 9.375661375661376e-06, global_step: 1180, interval_runtime: 1.9219, interval_samples_per_second: 4.163, interval_steps_per_second: 5.203, epoch: 3.1217[0m
[32m[2022-09-08 11:45:58,027] [    INFO][0m - loss: 1.50565224, learning_rate: 9.370370370370371e-06, global_step: 1190, interval_runtime: 1.9146, interval_samples_per_second: 4.178, interval_steps_per_second: 5.223, epoch: 3.1481[0m
[32m[2022-09-08 11:45:59,950] [    INFO][0m - loss: 1.35237036, learning_rate: 9.365079365079366e-06, global_step: 1200, interval_runtime: 1.923, interval_samples_per_second: 4.16, interval_steps_per_second: 5.2, epoch: 3.1746[0m
[32m[2022-09-08 11:45:59,950] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 11:45:59,950] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-08 11:45:59,950] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 11:45:59,950] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 11:45:59,950] [    INFO][0m -   Total prediction steps = 172[0m
[32m[2022-09-08 11:46:14,953] [    INFO][0m - eval_loss: 1.8410077095031738, eval_accuracy: 0.5251274704933167, eval_runtime: 15.0025, eval_samples_per_second: 91.518, eval_steps_per_second: 11.465, epoch: 3.1746[0m
[32m[2022-09-08 11:46:15,005] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1200[0m
[32m[2022-09-08 11:46:15,005] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 11:46:16,518] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1200/tokenizer_config.json[0m
[32m[2022-09-08 11:46:16,519] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1200/special_tokens_map.json[0m
[32m[2022-09-08 11:46:18,874] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-08 11:46:18,874] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-800 (score: 0.5389657616615295).[0m
[32m[2022-09-08 11:46:19,925] [    INFO][0m - train_runtime: 489.4076, train_samples_per_second: 308.945, train_steps_per_second: 38.618, train_loss: 2.6137294952074686, epoch: 3.1746[0m
[32m[2022-09-08 11:46:19,926] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-09-08 11:46:19,926] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 11:46:22,993] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-09-08 11:46:22,993] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-09-08 11:46:22,995] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-08 11:46:22,995] [    INFO][0m -   epoch                    =     3.1746[0m
[32m[2022-09-08 11:46:22,995] [    INFO][0m -   train_loss               =     2.6137[0m
[32m[2022-09-08 11:46:22,995] [    INFO][0m -   train_runtime            = 0:08:09.40[0m
[32m[2022-09-08 11:46:22,995] [    INFO][0m -   train_samples_per_second =    308.945[0m
[32m[2022-09-08 11:46:22,996] [    INFO][0m -   train_steps_per_second   =     38.618[0m
[32m[2022-09-08 11:46:23,001] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-08 11:46:23,001] [    INFO][0m -   Num examples = 1749[0m
[32m[2022-09-08 11:46:23,001] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 11:46:23,001] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 11:46:23,002] [    INFO][0m -   Total prediction steps = 219[0m
[32m[2022-09-08 11:46:42,791] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-08 11:46:42,792] [    INFO][0m -   test_accuracy           =     0.5283[0m
[32m[2022-09-08 11:46:42,792] [    INFO][0m -   test_loss               =     1.8504[0m
[32m[2022-09-08 11:46:42,792] [    INFO][0m -   test_runtime            = 0:00:19.78[0m
[32m[2022-09-08 11:46:42,792] [    INFO][0m -   test_samples_per_second =     88.379[0m
[32m[2022-09-08 11:46:42,792] [    INFO][0m -   test_steps_per_second   =     11.066[0m
[32m[2022-09-08 11:46:42,792] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-08 11:46:42,793] [    INFO][0m -   Num examples = 2600[0m
[32m[2022-09-08 11:46:42,793] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 11:46:42,793] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 11:46:42,793] [    INFO][0m -   Total prediction steps = 325[0m
Traceback (most recent call last):
  File "train_single.py", line 265, in <module>
    main(0)
  File "train_single.py", line 259, in main
    test_ret = postprocess(test_ret, test_ds, data_args.task_name,
  File "/ssd2/wanghuijuan03/prompt/PaddleNLP/examples/benchmark/fewclue/pretrain15/postprocess.py", line 37, in postprocess
    remap = json.load(open("label_map/iflytek_ids.json", "r"))
FileNotFoundError: [Errno 2] No such file or directory: 'label_map/iflytek_ids.json'
run.sh: line 59: --freeze_plm: command not found
 
==========
ocnli
==========
 
[33m[2022-09-08 11:47:15,434] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-09-08 11:47:15,434] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-08 11:47:15,434] [    INFO][0m - ============================================================[0m
[32m[2022-09-08 11:47:15,434] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-08 11:47:15,434] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-08 11:47:15,435] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-08 11:47:15,435] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-08 11:47:15,435] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-08 11:47:15,435] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-08 11:47:15,435] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-09-08 11:47:15,435] [    INFO][0m - [0m
[32m[2022-09-08 11:47:15,435] [    INFO][0m - ============================================================[0m
[32m[2022-09-08 11:47:15,435] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-08 11:47:15,435] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-08 11:47:15,435] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-08 11:47:15,435] [    INFO][0m - prompt                        :[0m
[32m[2022-09-08 11:47:15,435] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-08 11:47:15,435] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-08 11:47:15,436] [    INFO][0m - task_name                     :ocnli[0m
[32m[2022-09-08 11:47:15,436] [    INFO][0m - [0m
[32m[2022-09-08 11:47:15,436] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0908 11:47:15.437412 63520 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0908 11:47:15.441561 63520 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-08 11:47:18,480] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-09-08 11:47:18,505] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-09-08 11:47:18,505] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[32m[2022-09-08 11:47:19,544] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùÂíå‚Äú'}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': '‚ÄùËøô‰∏§Âè•ËØù‰πãÈó¥ÁöÑÈÄªËæëÂÖ≥Á≥ªÊòØ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}][0m
[32m[2022-09-08 11:47:19,677] [    INFO][0m - ============================================================[0m
[32m[2022-09-08 11:47:19,677] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-08 11:47:19,677] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-08 11:47:19,677] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-08 11:47:19,677] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-08 11:47:19,677] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-08 11:47:19,677] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-08 11:47:19,677] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-08 11:47:19,677] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-08 11:47:19,678] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-08 11:47:19,678] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-08 11:47:19,678] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-08 11:47:19,678] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-08 11:47:19,678] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-08 11:47:19,678] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-08 11:47:19,678] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-08 11:47:19,678] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-08 11:47:19,678] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-08 11:47:19,678] [    INFO][0m - eval_batch_size               :8[0m
[32m[2022-09-08 11:47:19,678] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-09-08 11:47:19,678] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-09-08 11:47:19,679] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-08 11:47:19,679] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-08 11:47:19,679] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-08 11:47:19,679] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-08 11:47:19,679] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-08 11:47:19,679] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-08 11:47:19,679] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-08 11:47:19,679] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-08 11:47:19,679] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-08 11:47:19,679] [    INFO][0m - learning_rate                 :1e-05[0m
[32m[2022-09-08 11:47:19,679] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-08 11:47:19,679] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-08 11:47:19,679] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-08 11:47:19,679] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-08 11:47:19,680] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-08 11:47:19,680] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-08 11:47:19,680] [    INFO][0m - logging_dir                   :./checkpoints/runs/Sep08_11-47-15_instance-3bwob41y-01[0m
[32m[2022-09-08 11:47:19,680] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-08 11:47:19,680] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-08 11:47:19,680] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-08 11:47:19,680] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-08 11:47:19,680] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-08 11:47:19,680] [    INFO][0m - max_seq_length                :128[0m
[32m[2022-09-08 11:47:19,680] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-08 11:47:19,680] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-08 11:47:19,680] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-08 11:47:19,680] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-08 11:47:19,681] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-09-08 11:47:19,681] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-08 11:47:19,681] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-08 11:47:19,681] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-09-08 11:47:19,681] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-08 11:47:19,681] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-08 11:47:19,681] [    INFO][0m - per_device_eval_batch_size    :8[0m
[32m[2022-09-08 11:47:19,681] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-09-08 11:47:19,681] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-08 11:47:19,681] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-08 11:47:19,681] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-08 11:47:19,681] [    INFO][0m - ppt_learning_rate             :0.0001[0m
[32m[2022-09-08 11:47:19,681] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-08 11:47:19,682] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-08 11:47:19,682] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-08 11:47:19,682] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-08 11:47:19,682] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-08 11:47:19,682] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-08 11:47:19,682] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-09-08 11:47:19,682] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-08 11:47:19,682] [    INFO][0m - save_steps                    :100[0m
[32m[2022-09-08 11:47:19,682] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-09-08 11:47:19,682] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-08 11:47:19,682] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-08 11:47:19,682] [    INFO][0m - seed                          :42[0m
[32m[2022-09-08 11:47:19,682] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-08 11:47:19,682] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-08 11:47:19,683] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-08 11:47:19,683] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-09-08 11:47:19,683] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-08 11:47:19,683] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-08 11:47:19,683] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-08 11:47:19,683] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-08 11:47:19,683] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-08 11:47:19,683] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-08 11:47:19,683] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-08 11:47:19,683] [    INFO][0m - [0m
[32m[2022-09-08 11:47:19,685] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-08 11:47:19,685] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-08 11:47:19,685] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-09-08 11:47:19,685] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-09-08 11:47:19,685] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-09-08 11:47:19,685] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-08 11:47:19,685] [    INFO][0m -   Total optimization steps = 1000.0[0m
[32m[2022-09-08 11:47:19,685] [    INFO][0m -   Total num train samples = 8000[0m
[32m[2022-09-08 11:47:21,449] [    INFO][0m - loss: 15.59508972, learning_rate: 9.9e-06, global_step: 10, interval_runtime: 1.7632, interval_samples_per_second: 4.537, interval_steps_per_second: 5.671, epoch: 0.5[0m
[32m[2022-09-08 11:47:22,266] [    INFO][0m - loss: 2.62115726, learning_rate: 9.800000000000001e-06, global_step: 20, interval_runtime: 0.8165, interval_samples_per_second: 9.798, interval_steps_per_second: 12.247, epoch: 1.0[0m
[32m[2022-09-08 11:47:23,134] [    INFO][0m - loss: 1.29916573, learning_rate: 9.7e-06, global_step: 30, interval_runtime: 0.8677, interval_samples_per_second: 9.22, interval_steps_per_second: 11.525, epoch: 1.5[0m
[32m[2022-09-08 11:47:23,949] [    INFO][0m - loss: 1.26751394, learning_rate: 9.600000000000001e-06, global_step: 40, interval_runtime: 0.8151, interval_samples_per_second: 9.815, interval_steps_per_second: 12.269, epoch: 2.0[0m
[32m[2022-09-08 11:47:24,819] [    INFO][0m - loss: 1.1256628, learning_rate: 9.5e-06, global_step: 50, interval_runtime: 0.8702, interval_samples_per_second: 9.193, interval_steps_per_second: 11.491, epoch: 2.5[0m
[32m[2022-09-08 11:47:25,635] [    INFO][0m - loss: 1.06491632, learning_rate: 9.4e-06, global_step: 60, interval_runtime: 0.8166, interval_samples_per_second: 9.797, interval_steps_per_second: 12.246, epoch: 3.0[0m
[32m[2022-09-08 11:47:26,507] [    INFO][0m - loss: 0.99551764, learning_rate: 9.3e-06, global_step: 70, interval_runtime: 0.8711, interval_samples_per_second: 9.184, interval_steps_per_second: 11.48, epoch: 3.5[0m
[32m[2022-09-08 11:47:27,324] [    INFO][0m - loss: 0.99738703, learning_rate: 9.200000000000002e-06, global_step: 80, interval_runtime: 0.8169, interval_samples_per_second: 9.793, interval_steps_per_second: 12.242, epoch: 4.0[0m
[32m[2022-09-08 11:47:28,197] [    INFO][0m - loss: 0.93306503, learning_rate: 9.100000000000001e-06, global_step: 90, interval_runtime: 0.8735, interval_samples_per_second: 9.159, interval_steps_per_second: 11.448, epoch: 4.5[0m
[32m[2022-09-08 11:47:29,017] [    INFO][0m - loss: 0.89612494, learning_rate: 9e-06, global_step: 100, interval_runtime: 0.8198, interval_samples_per_second: 9.759, interval_steps_per_second: 12.199, epoch: 5.0[0m
[32m[2022-09-08 11:47:29,017] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 11:47:29,017] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-08 11:47:29,018] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 11:47:29,018] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 11:47:29,018] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-08 11:47:29,823] [    INFO][0m - eval_loss: 1.012009859085083, eval_accuracy: 0.550000011920929, eval_runtime: 0.8051, eval_samples_per_second: 198.728, eval_steps_per_second: 24.841, epoch: 5.0[0m
[32m[2022-09-08 11:47:29,830] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-09-08 11:47:29,830] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 11:47:33,056] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-08 11:47:33,056] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-08 11:47:39,320] [    INFO][0m - loss: 0.80148182, learning_rate: 8.900000000000001e-06, global_step: 110, interval_runtime: 10.3025, interval_samples_per_second: 0.777, interval_steps_per_second: 0.971, epoch: 5.5[0m
[32m[2022-09-08 11:47:40,132] [    INFO][0m - loss: 0.6752121, learning_rate: 8.8e-06, global_step: 120, interval_runtime: 0.8123, interval_samples_per_second: 9.849, interval_steps_per_second: 12.311, epoch: 6.0[0m
[32m[2022-09-08 11:47:41,009] [    INFO][0m - loss: 0.60584173, learning_rate: 8.700000000000001e-06, global_step: 130, interval_runtime: 0.8775, interval_samples_per_second: 9.117, interval_steps_per_second: 11.396, epoch: 6.5[0m
[32m[2022-09-08 11:47:41,827] [    INFO][0m - loss: 0.51249504, learning_rate: 8.6e-06, global_step: 140, interval_runtime: 0.8181, interval_samples_per_second: 9.779, interval_steps_per_second: 12.223, epoch: 7.0[0m
[32m[2022-09-08 11:47:42,693] [    INFO][0m - loss: 0.44073262, learning_rate: 8.5e-06, global_step: 150, interval_runtime: 0.8654, interval_samples_per_second: 9.244, interval_steps_per_second: 11.555, epoch: 7.5[0m
[32m[2022-09-08 11:47:43,510] [    INFO][0m - loss: 0.38501492, learning_rate: 8.400000000000001e-06, global_step: 160, interval_runtime: 0.8175, interval_samples_per_second: 9.786, interval_steps_per_second: 12.233, epoch: 8.0[0m
[32m[2022-09-08 11:47:44,380] [    INFO][0m - loss: 0.22816477, learning_rate: 8.3e-06, global_step: 170, interval_runtime: 0.8701, interval_samples_per_second: 9.195, interval_steps_per_second: 11.493, epoch: 8.5[0m
[32m[2022-09-08 11:47:45,198] [    INFO][0m - loss: 0.27660205, learning_rate: 8.2e-06, global_step: 180, interval_runtime: 0.8171, interval_samples_per_second: 9.791, interval_steps_per_second: 12.239, epoch: 9.0[0m
[32m[2022-09-08 11:47:46,064] [    INFO][0m - loss: 0.19538407, learning_rate: 8.1e-06, global_step: 190, interval_runtime: 0.8665, interval_samples_per_second: 9.233, interval_steps_per_second: 11.541, epoch: 9.5[0m
[32m[2022-09-08 11:47:46,882] [    INFO][0m - loss: 0.09601246, learning_rate: 8.000000000000001e-06, global_step: 200, interval_runtime: 0.818, interval_samples_per_second: 9.78, interval_steps_per_second: 12.224, epoch: 10.0[0m
[32m[2022-09-08 11:47:46,882] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 11:47:46,882] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-08 11:47:46,882] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 11:47:46,882] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 11:47:46,883] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-08 11:47:47,687] [    INFO][0m - eval_loss: 1.9799683094024658, eval_accuracy: 0.5375000238418579, eval_runtime: 0.8043, eval_samples_per_second: 198.93, eval_steps_per_second: 24.866, epoch: 10.0[0m
[32m[2022-09-08 11:47:47,694] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-09-08 11:47:47,694] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 11:47:51,148] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-09-08 11:47:51,148] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-09-08 11:47:57,391] [    INFO][0m - loss: 0.09349144, learning_rate: 7.9e-06, global_step: 210, interval_runtime: 10.509, interval_samples_per_second: 0.761, interval_steps_per_second: 0.952, epoch: 10.5[0m
[32m[2022-09-08 11:47:58,207] [    INFO][0m - loss: 0.10372584, learning_rate: 7.800000000000002e-06, global_step: 220, interval_runtime: 0.8157, interval_samples_per_second: 9.808, interval_steps_per_second: 12.26, epoch: 11.0[0m
[32m[2022-09-08 11:47:59,072] [    INFO][0m - loss: 0.02666555, learning_rate: 7.7e-06, global_step: 230, interval_runtime: 0.8651, interval_samples_per_second: 9.248, interval_steps_per_second: 11.56, epoch: 11.5[0m
[32m[2022-09-08 11:47:59,887] [    INFO][0m - loss: 0.01851623, learning_rate: 7.600000000000001e-06, global_step: 240, interval_runtime: 0.815, interval_samples_per_second: 9.816, interval_steps_per_second: 12.27, epoch: 12.0[0m
[32m[2022-09-08 11:48:00,758] [    INFO][0m - loss: 0.10617867, learning_rate: 7.500000000000001e-06, global_step: 250, interval_runtime: 0.8715, interval_samples_per_second: 9.179, interval_steps_per_second: 11.474, epoch: 12.5[0m
[32m[2022-09-08 11:48:01,572] [    INFO][0m - loss: 0.0228399, learning_rate: 7.4e-06, global_step: 260, interval_runtime: 0.8142, interval_samples_per_second: 9.825, interval_steps_per_second: 12.281, epoch: 13.0[0m
[32m[2022-09-08 11:48:02,439] [    INFO][0m - loss: 0.00450322, learning_rate: 7.3e-06, global_step: 270, interval_runtime: 0.867, interval_samples_per_second: 9.227, interval_steps_per_second: 11.534, epoch: 13.5[0m
[32m[2022-09-08 11:48:03,264] [    INFO][0m - loss: 0.01975217, learning_rate: 7.2000000000000005e-06, global_step: 280, interval_runtime: 0.8245, interval_samples_per_second: 9.703, interval_steps_per_second: 12.128, epoch: 14.0[0m
[32m[2022-09-08 11:48:04,135] [    INFO][0m - loss: 0.00494479, learning_rate: 7.100000000000001e-06, global_step: 290, interval_runtime: 0.871, interval_samples_per_second: 9.184, interval_steps_per_second: 11.481, epoch: 14.5[0m
[32m[2022-09-08 11:48:04,952] [    INFO][0m - loss: 0.05504028, learning_rate: 7e-06, global_step: 300, interval_runtime: 0.8174, interval_samples_per_second: 9.787, interval_steps_per_second: 12.234, epoch: 15.0[0m
[32m[2022-09-08 11:48:04,953] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 11:48:04,953] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-08 11:48:04,953] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 11:48:04,953] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 11:48:04,953] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-08 11:48:05,731] [    INFO][0m - eval_loss: 3.118523359298706, eval_accuracy: 0.543749988079071, eval_runtime: 0.7781, eval_samples_per_second: 205.635, eval_steps_per_second: 25.704, epoch: 15.0[0m
[32m[2022-09-08 11:48:05,738] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-09-08 11:48:05,738] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 11:48:09,198] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-09-08 11:48:09,199] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-09-08 11:48:15,504] [    INFO][0m - loss: 0.00118236, learning_rate: 6.9e-06, global_step: 310, interval_runtime: 10.5517, interval_samples_per_second: 0.758, interval_steps_per_second: 0.948, epoch: 15.5[0m
[32m[2022-09-08 11:48:16,312] [    INFO][0m - loss: 0.00489898, learning_rate: 6.800000000000001e-06, global_step: 320, interval_runtime: 0.8081, interval_samples_per_second: 9.899, interval_steps_per_second: 12.374, epoch: 16.0[0m
[32m[2022-09-08 11:48:17,187] [    INFO][0m - loss: 0.00319394, learning_rate: 6.700000000000001e-06, global_step: 330, interval_runtime: 0.8745, interval_samples_per_second: 9.149, interval_steps_per_second: 11.436, epoch: 16.5[0m
[32m[2022-09-08 11:48:17,997] [    INFO][0m - loss: 0.00961232, learning_rate: 6.600000000000001e-06, global_step: 340, interval_runtime: 0.8108, interval_samples_per_second: 9.867, interval_steps_per_second: 12.333, epoch: 17.0[0m
[32m[2022-09-08 11:48:18,860] [    INFO][0m - loss: 0.001685, learning_rate: 6.5000000000000004e-06, global_step: 350, interval_runtime: 0.8625, interval_samples_per_second: 9.276, interval_steps_per_second: 11.595, epoch: 17.5[0m
[32m[2022-09-08 11:48:19,678] [    INFO][0m - loss: 0.00115616, learning_rate: 6.4000000000000006e-06, global_step: 360, interval_runtime: 0.818, interval_samples_per_second: 9.78, interval_steps_per_second: 12.224, epoch: 18.0[0m
[32m[2022-09-08 11:48:20,551] [    INFO][0m - loss: 0.05315652, learning_rate: 6.300000000000001e-06, global_step: 370, interval_runtime: 0.8726, interval_samples_per_second: 9.169, interval_steps_per_second: 11.461, epoch: 18.5[0m
[32m[2022-09-08 11:48:21,371] [    INFO][0m - loss: 0.00839453, learning_rate: 6.200000000000001e-06, global_step: 380, interval_runtime: 0.8205, interval_samples_per_second: 9.75, interval_steps_per_second: 12.187, epoch: 19.0[0m
[32m[2022-09-08 11:48:22,245] [    INFO][0m - loss: 0.06804699, learning_rate: 6.1e-06, global_step: 390, interval_runtime: 0.8731, interval_samples_per_second: 9.163, interval_steps_per_second: 11.453, epoch: 19.5[0m
[32m[2022-09-08 11:48:23,060] [    INFO][0m - loss: 0.00678686, learning_rate: 6e-06, global_step: 400, interval_runtime: 0.816, interval_samples_per_second: 9.804, interval_steps_per_second: 12.255, epoch: 20.0[0m
[32m[2022-09-08 11:48:23,061] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 11:48:23,061] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-08 11:48:23,061] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 11:48:23,061] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 11:48:23,061] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-08 11:48:23,873] [    INFO][0m - eval_loss: 3.583256959915161, eval_accuracy: 0.543749988079071, eval_runtime: 0.8109, eval_samples_per_second: 197.311, eval_steps_per_second: 24.664, epoch: 20.0[0m
[32m[2022-09-08 11:48:23,879] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-09-08 11:48:23,880] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 11:48:27,095] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-09-08 11:48:27,095] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-09-08 11:48:33,152] [    INFO][0m - loss: 0.01761771, learning_rate: 5.9e-06, global_step: 410, interval_runtime: 10.0908, interval_samples_per_second: 0.793, interval_steps_per_second: 0.991, epoch: 20.5[0m
[32m[2022-09-08 11:48:33,978] [    INFO][0m - loss: 0.00082478, learning_rate: 5.8e-06, global_step: 420, interval_runtime: 0.8268, interval_samples_per_second: 9.676, interval_steps_per_second: 12.095, epoch: 21.0[0m
[32m[2022-09-08 11:48:34,841] [    INFO][0m - loss: 0.03466285, learning_rate: 5.7e-06, global_step: 430, interval_runtime: 0.8628, interval_samples_per_second: 9.272, interval_steps_per_second: 11.59, epoch: 21.5[0m
[32m[2022-09-08 11:48:35,661] [    INFO][0m - loss: 0.00294812, learning_rate: 5.600000000000001e-06, global_step: 440, interval_runtime: 0.8205, interval_samples_per_second: 9.75, interval_steps_per_second: 12.188, epoch: 22.0[0m
[32m[2022-09-08 11:48:36,521] [    INFO][0m - loss: 0.00040273, learning_rate: 5.500000000000001e-06, global_step: 450, interval_runtime: 0.8598, interval_samples_per_second: 9.305, interval_steps_per_second: 11.631, epoch: 22.5[0m
[32m[2022-09-08 11:48:37,334] [    INFO][0m - loss: 0.00064667, learning_rate: 5.400000000000001e-06, global_step: 460, interval_runtime: 0.8132, interval_samples_per_second: 9.837, interval_steps_per_second: 12.297, epoch: 23.0[0m
[32m[2022-09-08 11:48:38,196] [    INFO][0m - loss: 0.01833923, learning_rate: 5.300000000000001e-06, global_step: 470, interval_runtime: 0.8619, interval_samples_per_second: 9.282, interval_steps_per_second: 11.602, epoch: 23.5[0m
[32m[2022-09-08 11:48:39,007] [    INFO][0m - loss: 0.00054971, learning_rate: 5.2e-06, global_step: 480, interval_runtime: 0.8092, interval_samples_per_second: 9.886, interval_steps_per_second: 12.357, epoch: 24.0[0m
[32m[2022-09-08 11:48:39,871] [    INFO][0m - loss: 0.00048921, learning_rate: 5.1e-06, global_step: 490, interval_runtime: 0.8661, interval_samples_per_second: 9.237, interval_steps_per_second: 11.546, epoch: 24.5[0m
[32m[2022-09-08 11:48:40,681] [    INFO][0m - loss: 0.00197149, learning_rate: 5e-06, global_step: 500, interval_runtime: 0.8101, interval_samples_per_second: 9.876, interval_steps_per_second: 12.345, epoch: 25.0[0m
[32m[2022-09-08 11:48:40,682] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 11:48:40,682] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-08 11:48:40,682] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 11:48:40,682] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 11:48:40,682] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-08 11:48:41,460] [    INFO][0m - eval_loss: 3.7294249534606934, eval_accuracy: 0.550000011920929, eval_runtime: 0.7769, eval_samples_per_second: 205.945, eval_steps_per_second: 25.743, epoch: 25.0[0m
[32m[2022-09-08 11:48:41,466] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-09-08 11:48:41,467] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 11:48:45,103] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-09-08 11:48:45,103] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-09-08 11:48:50,803] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-08 11:48:50,804] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-100 (score: 0.550000011920929).[0m
[32m[2022-09-08 11:48:51,743] [    INFO][0m - train_runtime: 92.0567, train_samples_per_second: 86.903, train_steps_per_second: 10.863, train_loss: 0.6340953244091942, epoch: 25.0[0m
[32m[2022-09-08 11:48:51,785] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-09-08 11:48:51,785] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 11:48:55,272] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-09-08 11:48:55,273] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-09-08 11:48:55,275] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-08 11:48:55,275] [    INFO][0m -   epoch                    =       25.0[0m
[32m[2022-09-08 11:48:55,275] [    INFO][0m -   train_loss               =     0.6341[0m
[32m[2022-09-08 11:48:55,275] [    INFO][0m -   train_runtime            = 0:01:32.05[0m
[32m[2022-09-08 11:48:55,275] [    INFO][0m -   train_samples_per_second =     86.903[0m
[32m[2022-09-08 11:48:55,275] [    INFO][0m -   train_steps_per_second   =     10.863[0m
[32m[2022-09-08 11:48:55,279] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-08 11:48:55,279] [    INFO][0m -   Num examples = 2520[0m
[32m[2022-09-08 11:48:55,279] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 11:48:55,279] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 11:48:55,279] [    INFO][0m -   Total prediction steps = 315[0m
[32m[2022-09-08 11:49:08,364] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-08 11:49:08,365] [    INFO][0m -   test_accuracy           =     0.4956[0m
[32m[2022-09-08 11:49:08,365] [    INFO][0m -   test_loss               =     1.0705[0m
[32m[2022-09-08 11:49:08,365] [    INFO][0m -   test_runtime            = 0:00:13.08[0m
[32m[2022-09-08 11:49:08,365] [    INFO][0m -   test_samples_per_second =    192.579[0m
[32m[2022-09-08 11:49:08,365] [    INFO][0m -   test_steps_per_second   =     24.072[0m
[32m[2022-09-08 11:49:08,366] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-08 11:49:08,366] [    INFO][0m -   Num examples = 3000[0m
[32m[2022-09-08 11:49:08,366] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 11:49:08,366] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 11:49:08,366] [    INFO][0m -   Total prediction steps = 375[0m
Traceback (most recent call last):
  File "train_single.py", line 265, in <module>
    main(0)
  File "train_single.py", line 259, in main
    test_ret = postprocess(test_ret, test_ds, data_args.task_name,
  File "/ssd2/wanghuijuan03/prompt/PaddleNLP/examples/benchmark/fewclue/pretrain15/postprocess.py", line 65, in postprocess
    ret_list.append({"id": uid, "label": id_to_label[preds[idx]]})
NameError: name 'ret_list' is not defined
run.sh: line 59: --freeze_plm: command not found
 
==========
bustm
==========
 
[33m[2022-09-08 11:49:30,144] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-09-08 11:49:30,144] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-08 11:49:30,144] [    INFO][0m - ============================================================[0m
[32m[2022-09-08 11:49:30,144] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-08 11:49:30,144] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-08 11:49:30,145] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-08 11:49:30,145] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-08 11:49:30,145] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-08 11:49:30,145] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-08 11:49:30,145] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-09-08 11:49:30,145] [    INFO][0m - [0m
[32m[2022-09-08 11:49:30,145] [    INFO][0m - ============================================================[0m
[32m[2022-09-08 11:49:30,145] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-08 11:49:30,145] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-08 11:49:30,145] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-08 11:49:30,145] [    INFO][0m - prompt                        :[0m
[32m[2022-09-08 11:49:30,145] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-08 11:49:30,145] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-08 11:49:30,146] [    INFO][0m - task_name                     :bustm[0m
[32m[2022-09-08 11:49:30,146] [    INFO][0m - [0m
[32m[2022-09-08 11:49:30,146] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0908 11:49:30.147349 66937 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0908 11:49:30.151281 66937 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-08 11:49:33,099] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-09-08 11:49:33,123] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-09-08 11:49:33,124] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[32m[2022-09-08 11:49:34,154] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùÂíå‚Äú'}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': '‚ÄùÊèèËø∞ÁöÑÊòØ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'ÁöÑ‰∫ãÊÉÖ„ÄÇ'}][0m
[32m[2022-09-08 11:49:34,264] [    INFO][0m - ============================================================[0m
[32m[2022-09-08 11:49:34,265] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-08 11:49:34,265] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-08 11:49:34,265] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-08 11:49:34,265] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-08 11:49:34,265] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-08 11:49:34,265] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-08 11:49:34,265] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-08 11:49:34,265] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-08 11:49:34,265] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-08 11:49:34,265] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-08 11:49:34,266] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-08 11:49:34,266] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-08 11:49:34,266] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-08 11:49:34,266] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-08 11:49:34,266] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-08 11:49:34,266] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-08 11:49:34,266] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-08 11:49:34,266] [    INFO][0m - eval_batch_size               :8[0m
[32m[2022-09-08 11:49:34,266] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-09-08 11:49:34,266] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-09-08 11:49:34,266] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-08 11:49:34,266] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-08 11:49:34,266] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-08 11:49:34,267] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-08 11:49:34,267] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-08 11:49:34,267] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-08 11:49:34,267] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-08 11:49:34,267] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-08 11:49:34,267] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-08 11:49:34,267] [    INFO][0m - learning_rate                 :1e-05[0m
[32m[2022-09-08 11:49:34,267] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-08 11:49:34,267] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-08 11:49:34,267] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-08 11:49:34,267] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-08 11:49:34,267] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-08 11:49:34,267] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-08 11:49:34,267] [    INFO][0m - logging_dir                   :./checkpoints/runs/Sep08_11-49-30_instance-3bwob41y-01[0m
[32m[2022-09-08 11:49:34,268] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-08 11:49:34,268] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-08 11:49:34,268] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-08 11:49:34,268] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-08 11:49:34,268] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-08 11:49:34,268] [    INFO][0m - max_seq_length                :64[0m
[32m[2022-09-08 11:49:34,268] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-08 11:49:34,268] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-08 11:49:34,268] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-08 11:49:34,268] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-08 11:49:34,268] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-09-08 11:49:34,268] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-08 11:49:34,268] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-08 11:49:34,269] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-09-08 11:49:34,269] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-08 11:49:34,269] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-08 11:49:34,269] [    INFO][0m - per_device_eval_batch_size    :8[0m
[32m[2022-09-08 11:49:34,269] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-09-08 11:49:34,269] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-08 11:49:34,269] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-08 11:49:34,269] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-08 11:49:34,269] [    INFO][0m - ppt_learning_rate             :0.0001[0m
[32m[2022-09-08 11:49:34,269] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-08 11:49:34,269] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-08 11:49:34,269] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-08 11:49:34,269] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-08 11:49:34,269] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-08 11:49:34,270] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-08 11:49:34,270] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-09-08 11:49:34,270] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-08 11:49:34,270] [    INFO][0m - save_steps                    :100[0m
[32m[2022-09-08 11:49:34,270] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-09-08 11:49:34,270] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-08 11:49:34,270] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-08 11:49:34,270] [    INFO][0m - seed                          :42[0m
[32m[2022-09-08 11:49:34,270] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-08 11:49:34,270] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-08 11:49:34,270] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-08 11:49:34,270] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-09-08 11:49:34,270] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-08 11:49:34,271] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-08 11:49:34,271] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-08 11:49:34,271] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-08 11:49:34,271] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-08 11:49:34,271] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-08 11:49:34,271] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-08 11:49:34,271] [    INFO][0m - [0m
[32m[2022-09-08 11:49:34,272] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-08 11:49:34,273] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-08 11:49:34,273] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-09-08 11:49:34,273] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-09-08 11:49:34,273] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-09-08 11:49:34,273] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-08 11:49:34,273] [    INFO][0m -   Total optimization steps = 1000.0[0m
[32m[2022-09-08 11:49:34,273] [    INFO][0m -   Total num train samples = 8000[0m
[32m[2022-09-08 11:49:35,802] [    INFO][0m - loss: 14.21831207, learning_rate: 9.9e-06, global_step: 10, interval_runtime: 1.5282, interval_samples_per_second: 5.235, interval_steps_per_second: 6.544, epoch: 0.5[0m
[32m[2022-09-08 11:49:36,376] [    INFO][0m - loss: 2.87897587, learning_rate: 9.800000000000001e-06, global_step: 20, interval_runtime: 0.5737, interval_samples_per_second: 13.945, interval_steps_per_second: 17.431, epoch: 1.0[0m
[32m[2022-09-08 11:49:37,023] [    INFO][0m - loss: 0.30855484, learning_rate: 9.7e-06, global_step: 30, interval_runtime: 0.6475, interval_samples_per_second: 12.355, interval_steps_per_second: 15.444, epoch: 1.5[0m
[32m[2022-09-08 11:49:37,601] [    INFO][0m - loss: 0.27601542, learning_rate: 9.600000000000001e-06, global_step: 40, interval_runtime: 0.5776, interval_samples_per_second: 13.85, interval_steps_per_second: 17.313, epoch: 2.0[0m
[32m[2022-09-08 11:49:38,298] [    INFO][0m - loss: 0.23976848, learning_rate: 9.5e-06, global_step: 50, interval_runtime: 0.6973, interval_samples_per_second: 11.472, interval_steps_per_second: 14.34, epoch: 2.5[0m
[32m[2022-09-08 11:49:38,998] [    INFO][0m - loss: 0.28176405, learning_rate: 9.4e-06, global_step: 60, interval_runtime: 0.6998, interval_samples_per_second: 11.432, interval_steps_per_second: 14.29, epoch: 3.0[0m
[32m[2022-09-08 11:49:39,794] [    INFO][0m - loss: 0.20289543, learning_rate: 9.3e-06, global_step: 70, interval_runtime: 0.7952, interval_samples_per_second: 10.06, interval_steps_per_second: 12.575, epoch: 3.5[0m
[32m[2022-09-08 11:49:40,452] [    INFO][0m - loss: 0.16229033, learning_rate: 9.200000000000002e-06, global_step: 80, interval_runtime: 0.6586, interval_samples_per_second: 12.147, interval_steps_per_second: 15.184, epoch: 4.0[0m
[32m[2022-09-08 11:49:41,101] [    INFO][0m - loss: 0.14246666, learning_rate: 9.100000000000001e-06, global_step: 90, interval_runtime: 0.6494, interval_samples_per_second: 12.319, interval_steps_per_second: 15.399, epoch: 4.5[0m
[32m[2022-09-08 11:49:41,677] [    INFO][0m - loss: 0.09634286, learning_rate: 9e-06, global_step: 100, interval_runtime: 0.5761, interval_samples_per_second: 13.886, interval_steps_per_second: 17.357, epoch: 5.0[0m
[32m[2022-09-08 11:49:41,678] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 11:49:41,678] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-08 11:49:41,678] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 11:49:41,678] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 11:49:41,678] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-08 11:49:42,271] [    INFO][0m - eval_loss: 0.3954005837440491, eval_accuracy: 0.7250000238418579, eval_runtime: 0.5929, eval_samples_per_second: 269.843, eval_steps_per_second: 33.73, epoch: 5.0[0m
[32m[2022-09-08 11:49:42,277] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-09-08 11:49:42,278] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 11:49:45,815] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-08 11:49:45,815] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-08 11:49:51,881] [    INFO][0m - loss: 0.10301608, learning_rate: 8.900000000000001e-06, global_step: 110, interval_runtime: 10.2038, interval_samples_per_second: 0.784, interval_steps_per_second: 0.98, epoch: 5.5[0m
[32m[2022-09-08 11:49:52,508] [    INFO][0m - loss: 0.06623882, learning_rate: 8.8e-06, global_step: 120, interval_runtime: 0.6268, interval_samples_per_second: 12.763, interval_steps_per_second: 15.953, epoch: 6.0[0m
[32m[2022-09-08 11:49:53,198] [    INFO][0m - loss: 0.09607725, learning_rate: 8.700000000000001e-06, global_step: 130, interval_runtime: 0.69, interval_samples_per_second: 11.594, interval_steps_per_second: 14.493, epoch: 6.5[0m
[32m[2022-09-08 11:49:53,780] [    INFO][0m - loss: 0.06979185, learning_rate: 8.6e-06, global_step: 140, interval_runtime: 0.5825, interval_samples_per_second: 13.735, interval_steps_per_second: 17.169, epoch: 7.0[0m
[32m[2022-09-08 11:49:54,435] [    INFO][0m - loss: 0.08466011, learning_rate: 8.5e-06, global_step: 150, interval_runtime: 0.6543, interval_samples_per_second: 12.226, interval_steps_per_second: 15.282, epoch: 7.5[0m
[32m[2022-09-08 11:49:55,015] [    INFO][0m - loss: 0.11333561, learning_rate: 8.400000000000001e-06, global_step: 160, interval_runtime: 0.5805, interval_samples_per_second: 13.781, interval_steps_per_second: 17.226, epoch: 8.0[0m
[32m[2022-09-08 11:49:55,673] [    INFO][0m - loss: 0.03370089, learning_rate: 8.3e-06, global_step: 170, interval_runtime: 0.6573, interval_samples_per_second: 12.17, interval_steps_per_second: 15.213, epoch: 8.5[0m
[32m[2022-09-08 11:49:56,263] [    INFO][0m - loss: 0.00536296, learning_rate: 8.2e-06, global_step: 180, interval_runtime: 0.5906, interval_samples_per_second: 13.545, interval_steps_per_second: 16.931, epoch: 9.0[0m
[32m[2022-09-08 11:49:56,928] [    INFO][0m - loss: 0.00239018, learning_rate: 8.1e-06, global_step: 190, interval_runtime: 0.6643, interval_samples_per_second: 12.044, interval_steps_per_second: 15.054, epoch: 9.5[0m
[32m[2022-09-08 11:49:57,506] [    INFO][0m - loss: 0.0364675, learning_rate: 8.000000000000001e-06, global_step: 200, interval_runtime: 0.5784, interval_samples_per_second: 13.832, interval_steps_per_second: 17.29, epoch: 10.0[0m
[32m[2022-09-08 11:49:57,506] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 11:49:57,506] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-08 11:49:57,506] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 11:49:57,507] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 11:49:57,507] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-08 11:49:58,086] [    INFO][0m - eval_loss: 0.875002384185791, eval_accuracy: 0.7250000238418579, eval_runtime: 0.5789, eval_samples_per_second: 276.37, eval_steps_per_second: 34.546, epoch: 10.0[0m
[32m[2022-09-08 11:49:58,093] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-09-08 11:49:58,093] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 11:50:01,306] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-09-08 11:50:01,306] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-09-08 11:50:07,518] [    INFO][0m - loss: 0.01640196, learning_rate: 7.9e-06, global_step: 210, interval_runtime: 10.0121, interval_samples_per_second: 0.799, interval_steps_per_second: 0.999, epoch: 10.5[0m
[32m[2022-09-08 11:50:08,093] [    INFO][0m - loss: 0.00093722, learning_rate: 7.800000000000002e-06, global_step: 220, interval_runtime: 0.5747, interval_samples_per_second: 13.919, interval_steps_per_second: 17.399, epoch: 11.0[0m
[32m[2022-09-08 11:50:08,751] [    INFO][0m - loss: 0.00755165, learning_rate: 7.7e-06, global_step: 230, interval_runtime: 0.6587, interval_samples_per_second: 12.146, interval_steps_per_second: 15.182, epoch: 11.5[0m
[32m[2022-09-08 11:50:09,337] [    INFO][0m - loss: 0.02396152, learning_rate: 7.600000000000001e-06, global_step: 240, interval_runtime: 0.5851, interval_samples_per_second: 13.673, interval_steps_per_second: 17.091, epoch: 12.0[0m
[32m[2022-09-08 11:50:09,985] [    INFO][0m - loss: 0.00199723, learning_rate: 7.500000000000001e-06, global_step: 250, interval_runtime: 0.6488, interval_samples_per_second: 12.33, interval_steps_per_second: 15.413, epoch: 12.5[0m
[32m[2022-09-08 11:50:10,558] [    INFO][0m - loss: 0.00148367, learning_rate: 7.4e-06, global_step: 260, interval_runtime: 0.5728, interval_samples_per_second: 13.967, interval_steps_per_second: 17.459, epoch: 13.0[0m
[32m[2022-09-08 11:50:11,202] [    INFO][0m - loss: 0.00030816, learning_rate: 7.3e-06, global_step: 270, interval_runtime: 0.6437, interval_samples_per_second: 12.429, interval_steps_per_second: 15.536, epoch: 13.5[0m
[32m[2022-09-08 11:50:11,780] [    INFO][0m - loss: 0.00025369, learning_rate: 7.2000000000000005e-06, global_step: 280, interval_runtime: 0.5783, interval_samples_per_second: 13.833, interval_steps_per_second: 17.291, epoch: 14.0[0m
[32m[2022-09-08 11:50:12,431] [    INFO][0m - loss: 0.00231502, learning_rate: 7.100000000000001e-06, global_step: 290, interval_runtime: 0.6511, interval_samples_per_second: 12.287, interval_steps_per_second: 15.358, epoch: 14.5[0m
[32m[2022-09-08 11:50:13,015] [    INFO][0m - loss: 0.00812757, learning_rate: 7e-06, global_step: 300, interval_runtime: 0.5836, interval_samples_per_second: 13.707, interval_steps_per_second: 17.134, epoch: 15.0[0m
[32m[2022-09-08 11:50:13,015] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 11:50:13,015] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-08 11:50:13,016] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 11:50:13,016] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 11:50:13,016] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-08 11:50:13,598] [    INFO][0m - eval_loss: 1.1249253749847412, eval_accuracy: 0.706250011920929, eval_runtime: 0.5817, eval_samples_per_second: 275.047, eval_steps_per_second: 34.381, epoch: 15.0[0m
[32m[2022-09-08 11:50:13,604] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-09-08 11:50:13,604] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 11:50:17,070] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-09-08 11:50:17,070] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-09-08 11:50:22,890] [    INFO][0m - loss: 0.00034659, learning_rate: 6.9e-06, global_step: 310, interval_runtime: 9.8749, interval_samples_per_second: 0.81, interval_steps_per_second: 1.013, epoch: 15.5[0m
[32m[2022-09-08 11:50:23,467] [    INFO][0m - loss: 0.00594123, learning_rate: 6.800000000000001e-06, global_step: 320, interval_runtime: 0.5771, interval_samples_per_second: 13.863, interval_steps_per_second: 17.328, epoch: 16.0[0m
[32m[2022-09-08 11:50:24,122] [    INFO][0m - loss: 0.00012838, learning_rate: 6.700000000000001e-06, global_step: 330, interval_runtime: 0.6556, interval_samples_per_second: 12.202, interval_steps_per_second: 15.252, epoch: 16.5[0m
[32m[2022-09-08 11:50:24,705] [    INFO][0m - loss: 0.01524178, learning_rate: 6.600000000000001e-06, global_step: 340, interval_runtime: 0.5821, interval_samples_per_second: 13.744, interval_steps_per_second: 17.179, epoch: 17.0[0m
[32m[2022-09-08 11:50:25,361] [    INFO][0m - loss: 0.0001019, learning_rate: 6.5000000000000004e-06, global_step: 350, interval_runtime: 0.6567, interval_samples_per_second: 12.182, interval_steps_per_second: 15.227, epoch: 17.5[0m
[32m[2022-09-08 11:50:25,933] [    INFO][0m - loss: 0.0142381, learning_rate: 6.4000000000000006e-06, global_step: 360, interval_runtime: 0.5717, interval_samples_per_second: 13.994, interval_steps_per_second: 17.493, epoch: 18.0[0m
[32m[2022-09-08 11:50:26,576] [    INFO][0m - loss: 0.00015337, learning_rate: 6.300000000000001e-06, global_step: 370, interval_runtime: 0.643, interval_samples_per_second: 12.442, interval_steps_per_second: 15.552, epoch: 18.5[0m
[32m[2022-09-08 11:50:27,151] [    INFO][0m - loss: 0.00015952, learning_rate: 6.200000000000001e-06, global_step: 380, interval_runtime: 0.5749, interval_samples_per_second: 13.914, interval_steps_per_second: 17.393, epoch: 19.0[0m
[32m[2022-09-08 11:50:27,799] [    INFO][0m - loss: 0.00256189, learning_rate: 6.1e-06, global_step: 390, interval_runtime: 0.6484, interval_samples_per_second: 12.337, interval_steps_per_second: 15.422, epoch: 19.5[0m
[32m[2022-09-08 11:50:28,380] [    INFO][0m - loss: 7.914e-05, learning_rate: 6e-06, global_step: 400, interval_runtime: 0.5808, interval_samples_per_second: 13.773, interval_steps_per_second: 17.217, epoch: 20.0[0m
[32m[2022-09-08 11:50:28,381] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 11:50:28,381] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-08 11:50:28,381] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 11:50:28,381] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 11:50:28,381] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-08 11:50:28,953] [    INFO][0m - eval_loss: 1.0813777446746826, eval_accuracy: 0.75, eval_runtime: 0.5717, eval_samples_per_second: 279.854, eval_steps_per_second: 34.982, epoch: 20.0[0m
[32m[2022-09-08 11:50:28,960] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-09-08 11:50:28,960] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 11:50:32,037] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-09-08 11:50:32,037] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-09-08 11:50:38,132] [    INFO][0m - loss: 0.00039955, learning_rate: 5.9e-06, global_step: 410, interval_runtime: 9.7521, interval_samples_per_second: 0.82, interval_steps_per_second: 1.025, epoch: 20.5[0m
[32m[2022-09-08 11:50:38,709] [    INFO][0m - loss: 0.00509489, learning_rate: 5.8e-06, global_step: 420, interval_runtime: 0.5768, interval_samples_per_second: 13.87, interval_steps_per_second: 17.338, epoch: 21.0[0m
[32m[2022-09-08 11:50:39,356] [    INFO][0m - loss: 0.00186047, learning_rate: 5.7e-06, global_step: 430, interval_runtime: 0.6467, interval_samples_per_second: 12.371, interval_steps_per_second: 15.463, epoch: 21.5[0m
[32m[2022-09-08 11:50:39,936] [    INFO][0m - loss: 7.911e-05, learning_rate: 5.600000000000001e-06, global_step: 440, interval_runtime: 0.5801, interval_samples_per_second: 13.79, interval_steps_per_second: 17.238, epoch: 22.0[0m
[32m[2022-09-08 11:50:40,581] [    INFO][0m - loss: 0.00029623, learning_rate: 5.500000000000001e-06, global_step: 450, interval_runtime: 0.6449, interval_samples_per_second: 12.406, interval_steps_per_second: 15.507, epoch: 22.5[0m
[32m[2022-09-08 11:50:41,182] [    INFO][0m - loss: 0.00131034, learning_rate: 5.400000000000001e-06, global_step: 460, interval_runtime: 0.6009, interval_samples_per_second: 13.313, interval_steps_per_second: 16.641, epoch: 23.0[0m
[32m[2022-09-08 11:50:41,828] [    INFO][0m - loss: 0.00017398, learning_rate: 5.300000000000001e-06, global_step: 470, interval_runtime: 0.6461, interval_samples_per_second: 12.382, interval_steps_per_second: 15.477, epoch: 23.5[0m
[32m[2022-09-08 11:50:42,410] [    INFO][0m - loss: 5.057e-05, learning_rate: 5.2e-06, global_step: 480, interval_runtime: 0.5822, interval_samples_per_second: 13.741, interval_steps_per_second: 17.176, epoch: 24.0[0m
[32m[2022-09-08 11:50:43,057] [    INFO][0m - loss: 5.389e-05, learning_rate: 5.1e-06, global_step: 490, interval_runtime: 0.647, interval_samples_per_second: 12.366, interval_steps_per_second: 15.457, epoch: 24.5[0m
[32m[2022-09-08 11:50:43,637] [    INFO][0m - loss: 8.833e-05, learning_rate: 5e-06, global_step: 500, interval_runtime: 0.5803, interval_samples_per_second: 13.787, interval_steps_per_second: 17.234, epoch: 25.0[0m
[32m[2022-09-08 11:50:43,638] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 11:50:43,638] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-08 11:50:43,638] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 11:50:43,638] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 11:50:43,638] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-08 11:50:44,215] [    INFO][0m - eval_loss: 1.281177282333374, eval_accuracy: 0.7125000357627869, eval_runtime: 0.5769, eval_samples_per_second: 277.348, eval_steps_per_second: 34.669, epoch: 25.0[0m
[32m[2022-09-08 11:50:44,221] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-09-08 11:50:44,222] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 11:50:47,369] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-09-08 11:50:47,370] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-09-08 11:50:53,478] [    INFO][0m - loss: 6.441e-05, learning_rate: 4.9000000000000005e-06, global_step: 510, interval_runtime: 9.8408, interval_samples_per_second: 0.813, interval_steps_per_second: 1.016, epoch: 25.5[0m
[32m[2022-09-08 11:50:54,062] [    INFO][0m - loss: 4.41e-05, learning_rate: 4.800000000000001e-06, global_step: 520, interval_runtime: 0.584, interval_samples_per_second: 13.698, interval_steps_per_second: 17.123, epoch: 26.0[0m
[32m[2022-09-08 11:50:54,727] [    INFO][0m - loss: 0.0002424, learning_rate: 4.7e-06, global_step: 530, interval_runtime: 0.6651, interval_samples_per_second: 12.028, interval_steps_per_second: 15.035, epoch: 26.5[0m
[32m[2022-09-08 11:50:55,316] [    INFO][0m - loss: 0.06879515, learning_rate: 4.600000000000001e-06, global_step: 540, interval_runtime: 0.5892, interval_samples_per_second: 13.579, interval_steps_per_second: 16.973, epoch: 27.0[0m
[32m[2022-09-08 11:50:55,971] [    INFO][0m - loss: 4.827e-05, learning_rate: 4.5e-06, global_step: 550, interval_runtime: 0.6546, interval_samples_per_second: 12.222, interval_steps_per_second: 15.277, epoch: 27.5[0m
[32m[2022-09-08 11:50:56,559] [    INFO][0m - loss: 6.792e-05, learning_rate: 4.4e-06, global_step: 560, interval_runtime: 0.5877, interval_samples_per_second: 13.611, interval_steps_per_second: 17.014, epoch: 28.0[0m
[32m[2022-09-08 11:50:57,205] [    INFO][0m - loss: 0.0001421, learning_rate: 4.3e-06, global_step: 570, interval_runtime: 0.6467, interval_samples_per_second: 12.371, interval_steps_per_second: 15.464, epoch: 28.5[0m
[32m[2022-09-08 11:50:57,790] [    INFO][0m - loss: 7.472e-05, learning_rate: 4.2000000000000004e-06, global_step: 580, interval_runtime: 0.584, interval_samples_per_second: 13.698, interval_steps_per_second: 17.122, epoch: 29.0[0m
[32m[2022-09-08 11:50:58,464] [    INFO][0m - loss: 0.00147264, learning_rate: 4.1e-06, global_step: 590, interval_runtime: 0.6749, interval_samples_per_second: 11.854, interval_steps_per_second: 14.817, epoch: 29.5[0m
[32m[2022-09-08 11:50:59,039] [    INFO][0m - loss: 3.879e-05, learning_rate: 4.000000000000001e-06, global_step: 600, interval_runtime: 0.575, interval_samples_per_second: 13.914, interval_steps_per_second: 17.393, epoch: 30.0[0m
[32m[2022-09-08 11:50:59,040] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 11:50:59,040] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-08 11:50:59,040] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 11:50:59,040] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 11:50:59,040] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-08 11:50:59,618] [    INFO][0m - eval_loss: 1.1827272176742554, eval_accuracy: 0.731249988079071, eval_runtime: 0.5782, eval_samples_per_second: 276.723, eval_steps_per_second: 34.59, epoch: 30.0[0m
[32m[2022-09-08 11:50:59,625] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-09-08 11:50:59,625] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 11:51:02,756] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-09-08 11:51:02,756] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-09-08 11:51:08,756] [    INFO][0m - loss: 6.65e-05, learning_rate: 3.900000000000001e-06, global_step: 610, interval_runtime: 9.7164, interval_samples_per_second: 0.823, interval_steps_per_second: 1.029, epoch: 30.5[0m
[32m[2022-09-08 11:51:09,401] [    INFO][0m - loss: 0.00038401, learning_rate: 3.8000000000000005e-06, global_step: 620, interval_runtime: 0.6454, interval_samples_per_second: 12.396, interval_steps_per_second: 15.495, epoch: 31.0[0m
[32m[2022-09-08 11:51:10,126] [    INFO][0m - loss: 4.328e-05, learning_rate: 3.7e-06, global_step: 630, interval_runtime: 0.7247, interval_samples_per_second: 11.039, interval_steps_per_second: 13.799, epoch: 31.5[0m
[32m[2022-09-08 11:51:10,782] [    INFO][0m - loss: 0.0140197, learning_rate: 3.6000000000000003e-06, global_step: 640, interval_runtime: 0.6563, interval_samples_per_second: 12.189, interval_steps_per_second: 15.236, epoch: 32.0[0m
[32m[2022-09-08 11:51:11,505] [    INFO][0m - loss: 0.00120919, learning_rate: 3.5e-06, global_step: 650, interval_runtime: 0.7229, interval_samples_per_second: 11.066, interval_steps_per_second: 13.832, epoch: 32.5[0m
[32m[2022-09-08 11:51:12,164] [    INFO][0m - loss: 0.00276633, learning_rate: 3.4000000000000005e-06, global_step: 660, interval_runtime: 0.6588, interval_samples_per_second: 12.142, interval_steps_per_second: 15.178, epoch: 33.0[0m
[32m[2022-09-08 11:51:12,885] [    INFO][0m - loss: 0.00015759, learning_rate: 3.3000000000000006e-06, global_step: 670, interval_runtime: 0.7212, interval_samples_per_second: 11.092, interval_steps_per_second: 13.865, epoch: 33.5[0m
[32m[2022-09-08 11:51:13,516] [    INFO][0m - loss: 0.00013621, learning_rate: 3.2000000000000003e-06, global_step: 680, interval_runtime: 0.6312, interval_samples_per_second: 12.675, interval_steps_per_second: 15.843, epoch: 34.0[0m
[32m[2022-09-08 11:51:14,173] [    INFO][0m - loss: 0.00024355, learning_rate: 3.1000000000000004e-06, global_step: 690, interval_runtime: 0.6561, interval_samples_per_second: 12.192, interval_steps_per_second: 15.241, epoch: 34.5[0m
[32m[2022-09-08 11:51:14,759] [    INFO][0m - loss: 3.148e-05, learning_rate: 3e-06, global_step: 700, interval_runtime: 0.5862, interval_samples_per_second: 13.648, interval_steps_per_second: 17.06, epoch: 35.0[0m
[32m[2022-09-08 11:51:14,759] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 11:51:14,759] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-08 11:51:14,759] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 11:51:14,759] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 11:51:14,760] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-08 11:51:15,343] [    INFO][0m - eval_loss: 1.2412660121917725, eval_accuracy: 0.7437500357627869, eval_runtime: 0.5833, eval_samples_per_second: 274.319, eval_steps_per_second: 34.29, epoch: 35.0[0m
[32m[2022-09-08 11:51:15,349] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-700[0m
[32m[2022-09-08 11:51:15,349] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 11:51:18,390] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-700/tokenizer_config.json[0m
[32m[2022-09-08 11:51:18,390] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-700/special_tokens_map.json[0m
[32m[2022-09-08 11:51:24,074] [    INFO][0m - loss: 4.4e-05, learning_rate: 2.9e-06, global_step: 710, interval_runtime: 9.315, interval_samples_per_second: 0.859, interval_steps_per_second: 1.074, epoch: 35.5[0m
[32m[2022-09-08 11:51:24,650] [    INFO][0m - loss: 4.218e-05, learning_rate: 2.8000000000000003e-06, global_step: 720, interval_runtime: 0.5762, interval_samples_per_second: 13.884, interval_steps_per_second: 17.356, epoch: 36.0[0m
[32m[2022-09-08 11:51:25,298] [    INFO][0m - loss: 0.00030815, learning_rate: 2.7000000000000004e-06, global_step: 730, interval_runtime: 0.6477, interval_samples_per_second: 12.352, interval_steps_per_second: 15.44, epoch: 36.5[0m
[32m[2022-09-08 11:51:25,877] [    INFO][0m - loss: 3.761e-05, learning_rate: 2.6e-06, global_step: 740, interval_runtime: 0.579, interval_samples_per_second: 13.817, interval_steps_per_second: 17.272, epoch: 37.0[0m
[32m[2022-09-08 11:51:26,522] [    INFO][0m - loss: 6.143e-05, learning_rate: 2.5e-06, global_step: 750, interval_runtime: 0.6455, interval_samples_per_second: 12.393, interval_steps_per_second: 15.492, epoch: 37.5[0m
[32m[2022-09-08 11:51:27,095] [    INFO][0m - loss: 0.00010016, learning_rate: 2.4000000000000003e-06, global_step: 760, interval_runtime: 0.5727, interval_samples_per_second: 13.968, interval_steps_per_second: 17.46, epoch: 38.0[0m
[32m[2022-09-08 11:51:27,741] [    INFO][0m - loss: 5.483e-05, learning_rate: 2.3000000000000004e-06, global_step: 770, interval_runtime: 0.6457, interval_samples_per_second: 12.39, interval_steps_per_second: 15.487, epoch: 38.5[0m
[32m[2022-09-08 11:51:28,317] [    INFO][0m - loss: 5.983e-05, learning_rate: 2.2e-06, global_step: 780, interval_runtime: 0.5765, interval_samples_per_second: 13.877, interval_steps_per_second: 17.346, epoch: 39.0[0m
[32m[2022-09-08 11:51:28,966] [    INFO][0m - loss: 2.252e-05, learning_rate: 2.1000000000000002e-06, global_step: 790, interval_runtime: 0.6489, interval_samples_per_second: 12.329, interval_steps_per_second: 15.411, epoch: 39.5[0m
[32m[2022-09-08 11:51:29,545] [    INFO][0m - loss: 0.00830835, learning_rate: 2.0000000000000003e-06, global_step: 800, interval_runtime: 0.579, interval_samples_per_second: 13.816, interval_steps_per_second: 17.271, epoch: 40.0[0m
[32m[2022-09-08 11:51:29,545] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 11:51:29,545] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-08 11:51:29,546] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 11:51:29,546] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 11:51:29,546] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-08 11:51:30,104] [    INFO][0m - eval_loss: 1.2877845764160156, eval_accuracy: 0.731249988079071, eval_runtime: 0.5585, eval_samples_per_second: 286.46, eval_steps_per_second: 35.807, epoch: 40.0[0m
[32m[2022-09-08 11:51:30,111] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-800[0m
[32m[2022-09-08 11:51:30,111] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 11:51:33,223] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-800/tokenizer_config.json[0m
[32m[2022-09-08 11:51:33,224] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-800/special_tokens_map.json[0m
[32m[2022-09-08 11:51:38,204] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-08 11:51:38,204] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-400 (score: 0.75).[0m
[32m[2022-09-08 11:51:39,270] [    INFO][0m - train_runtime: 124.9958, train_samples_per_second: 64.002, train_steps_per_second: 8.0, train_loss: 0.2453651449935751, epoch: 40.0[0m
[32m[2022-09-08 11:51:39,313] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-09-08 11:51:39,313] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 11:51:42,476] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-09-08 11:51:42,476] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-09-08 11:51:42,477] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-08 11:51:42,478] [    INFO][0m -   epoch                    =       40.0[0m
[32m[2022-09-08 11:51:42,478] [    INFO][0m -   train_loss               =     0.2454[0m
[32m[2022-09-08 11:51:42,478] [    INFO][0m -   train_runtime            = 0:02:04.99[0m
[32m[2022-09-08 11:51:42,478] [    INFO][0m -   train_samples_per_second =     64.002[0m
[32m[2022-09-08 11:51:42,478] [    INFO][0m -   train_steps_per_second   =        8.0[0m
[32m[2022-09-08 11:51:42,482] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-08 11:51:42,482] [    INFO][0m -   Num examples = 1772[0m
[32m[2022-09-08 11:51:42,482] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 11:51:42,482] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 11:51:42,482] [    INFO][0m -   Total prediction steps = 222[0m
[32m[2022-09-08 11:51:49,616] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-08 11:51:49,617] [    INFO][0m -   test_accuracy           =     0.7562[0m
[32m[2022-09-08 11:51:49,617] [    INFO][0m -   test_loss               =     1.0209[0m
[32m[2022-09-08 11:51:49,617] [    INFO][0m -   test_runtime            = 0:00:07.13[0m
[32m[2022-09-08 11:51:49,617] [    INFO][0m -   test_samples_per_second =    248.382[0m
[32m[2022-09-08 11:51:49,617] [    INFO][0m -   test_steps_per_second   =     31.118[0m
[32m[2022-09-08 11:51:49,618] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-08 11:51:49,618] [    INFO][0m -   Num examples = 2000[0m
[32m[2022-09-08 11:51:49,618] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 11:51:49,618] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 11:51:49,618] [    INFO][0m -   Total prediction steps = 250[0m
Traceback (most recent call last):
  File "train_single.py", line 265, in <module>
    main(0)
  File "train_single.py", line 259, in main
    test_ret = postprocess(test_ret, test_ds, data_args.task_name,
  File "/ssd2/wanghuijuan03/prompt/PaddleNLP/examples/benchmark/fewclue/pretrain15/postprocess.py", line 61, in postprocess
    ret_list.append({"id": uid, "label": str(preds[idx])})
NameError: name 'ret_list' is not defined
run.sh: line 59: --freeze_plm: command not found
 
==========
chid
==========
 
[33m[2022-09-08 11:52:02,579] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-09-08 11:52:02,580] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-08 11:52:02,580] [    INFO][0m - ============================================================[0m
[32m[2022-09-08 11:52:02,580] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-08 11:52:02,580] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-08 11:52:02,580] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-08 11:52:02,580] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-08 11:52:02,581] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-08 11:52:02,581] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-08 11:52:02,581] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-09-08 11:52:02,581] [    INFO][0m - [0m
[32m[2022-09-08 11:52:02,581] [    INFO][0m - ============================================================[0m
[32m[2022-09-08 11:52:02,581] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-08 11:52:02,581] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-08 11:52:02,581] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-08 11:52:02,581] [    INFO][0m - prompt                        :[0m
[32m[2022-09-08 11:52:02,581] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-08 11:52:02,581] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-08 11:52:02,581] [    INFO][0m - task_name                     :chid[0m
[32m[2022-09-08 11:52:02,581] [    INFO][0m - [0m
[32m[2022-09-08 11:52:02,582] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0908 11:52:02.583110 70056 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0908 11:52:02.587107 70056 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-08 11:52:05,591] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-09-08 11:52:05,615] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-09-08 11:52:05,616] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[32m[2022-09-08 11:52:06,654] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùËøôÂè•ËØù‰∏≠ÊàêËØ≠['}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': ']ÁöÑÁêÜËß£Ê≠£Á°ÆÂêóÔºü'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': '„ÄÇ'}][0m
[32m[2022-09-08 11:52:06,957] [    INFO][0m - ============================================================[0m
[32m[2022-09-08 11:52:06,957] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-08 11:52:06,957] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-08 11:52:06,957] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-08 11:52:06,957] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-08 11:52:06,958] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-08 11:52:06,958] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-08 11:52:06,958] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-08 11:52:06,958] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-08 11:52:06,958] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-08 11:52:06,958] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-08 11:52:06,958] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-08 11:52:06,958] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-08 11:52:06,958] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-08 11:52:06,958] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-08 11:52:06,958] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-08 11:52:06,958] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-08 11:52:06,958] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-08 11:52:06,959] [    INFO][0m - eval_batch_size               :8[0m
[32m[2022-09-08 11:52:06,959] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-09-08 11:52:06,959] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-09-08 11:52:06,959] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-08 11:52:06,959] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-08 11:52:06,959] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-08 11:52:06,959] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-08 11:52:06,959] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-08 11:52:06,959] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-08 11:52:06,959] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-08 11:52:06,959] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-08 11:52:06,959] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-08 11:52:06,959] [    INFO][0m - learning_rate                 :1e-05[0m
[32m[2022-09-08 11:52:06,960] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-08 11:52:06,960] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-08 11:52:06,960] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-08 11:52:06,960] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-08 11:52:06,960] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-08 11:52:06,960] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-08 11:52:06,960] [    INFO][0m - logging_dir                   :./checkpoints/runs/Sep08_11-52-02_instance-3bwob41y-01[0m
[32m[2022-09-08 11:52:06,960] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-08 11:52:06,960] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-08 11:52:06,960] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-08 11:52:06,960] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-08 11:52:06,960] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-08 11:52:06,960] [    INFO][0m - max_seq_length                :256[0m
[32m[2022-09-08 11:52:06,961] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-08 11:52:06,961] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-08 11:52:06,961] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-08 11:52:06,961] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-08 11:52:06,961] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-09-08 11:52:06,961] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-08 11:52:06,961] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-08 11:52:06,961] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-09-08 11:52:06,961] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-08 11:52:06,961] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-08 11:52:06,961] [    INFO][0m - per_device_eval_batch_size    :8[0m
[32m[2022-09-08 11:52:06,961] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-09-08 11:52:06,961] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-08 11:52:06,961] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-08 11:52:06,962] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-08 11:52:06,962] [    INFO][0m - ppt_learning_rate             :0.0001[0m
[32m[2022-09-08 11:52:06,962] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-08 11:52:06,962] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-08 11:52:06,962] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-08 11:52:06,962] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-08 11:52:06,962] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-08 11:52:06,962] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-08 11:52:06,962] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-09-08 11:52:06,962] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-08 11:52:06,962] [    INFO][0m - save_steps                    :100[0m
[32m[2022-09-08 11:52:06,962] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-09-08 11:52:06,962] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-08 11:52:06,963] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-08 11:52:06,963] [    INFO][0m - seed                          :42[0m
[32m[2022-09-08 11:52:06,963] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-08 11:52:06,963] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-08 11:52:06,963] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-08 11:52:06,963] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-09-08 11:52:06,963] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-08 11:52:06,963] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-08 11:52:06,963] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-08 11:52:06,963] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-08 11:52:06,963] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-08 11:52:06,963] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-08 11:52:06,963] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-08 11:52:06,964] [    INFO][0m - [0m
[32m[2022-09-08 11:52:06,966] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-08 11:52:06,966] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-08 11:52:06,966] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-09-08 11:52:06,966] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-09-08 11:52:06,966] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-09-08 11:52:06,966] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-08 11:52:06,966] [    INFO][0m -   Total optimization steps = 8850.0[0m
[32m[2022-09-08 11:52:06,967] [    INFO][0m -   Total num train samples = 70700[0m
[32m[2022-09-08 11:52:09,376] [    INFO][0m - loss: 20.38623047, learning_rate: 9.988700564971753e-06, global_step: 10, interval_runtime: 2.4083, interval_samples_per_second: 3.322, interval_steps_per_second: 4.152, epoch: 0.0565[0m
[32m[2022-09-08 11:52:10,888] [    INFO][0m - loss: 6.42270508, learning_rate: 9.977401129943504e-06, global_step: 20, interval_runtime: 1.5126, interval_samples_per_second: 5.289, interval_steps_per_second: 6.611, epoch: 0.113[0m
[32m[2022-09-08 11:52:12,405] [    INFO][0m - loss: 0.58249884, learning_rate: 9.966101694915256e-06, global_step: 30, interval_runtime: 1.5161, interval_samples_per_second: 5.277, interval_steps_per_second: 6.596, epoch: 0.1695[0m
[32m[2022-09-08 11:52:13,921] [    INFO][0m - loss: 0.47496929, learning_rate: 9.954802259887007e-06, global_step: 40, interval_runtime: 1.5168, interval_samples_per_second: 5.274, interval_steps_per_second: 6.593, epoch: 0.226[0m
[32m[2022-09-08 11:52:15,438] [    INFO][0m - loss: 0.36300459, learning_rate: 9.943502824858759e-06, global_step: 50, interval_runtime: 1.5169, interval_samples_per_second: 5.274, interval_steps_per_second: 6.593, epoch: 0.2825[0m
[32m[2022-09-08 11:52:16,947] [    INFO][0m - loss: 0.26816924, learning_rate: 9.93220338983051e-06, global_step: 60, interval_runtime: 1.5096, interval_samples_per_second: 5.299, interval_steps_per_second: 6.624, epoch: 0.339[0m
[32m[2022-09-08 11:52:18,464] [    INFO][0m - loss: 0.94801359, learning_rate: 9.92090395480226e-06, global_step: 70, interval_runtime: 1.5165, interval_samples_per_second: 5.275, interval_steps_per_second: 6.594, epoch: 0.3955[0m
[32m[2022-09-08 11:52:19,976] [    INFO][0m - loss: 0.64820938, learning_rate: 9.909604519774013e-06, global_step: 80, interval_runtime: 1.5118, interval_samples_per_second: 5.292, interval_steps_per_second: 6.614, epoch: 0.452[0m
[32m[2022-09-08 11:52:21,489] [    INFO][0m - loss: 0.46559181, learning_rate: 9.898305084745763e-06, global_step: 90, interval_runtime: 1.513, interval_samples_per_second: 5.288, interval_steps_per_second: 6.609, epoch: 0.5085[0m
[32m[2022-09-08 11:52:23,012] [    INFO][0m - loss: 0.42811856, learning_rate: 9.887005649717516e-06, global_step: 100, interval_runtime: 1.523, interval_samples_per_second: 5.253, interval_steps_per_second: 6.566, epoch: 0.565[0m
[32m[2022-09-08 11:52:23,013] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 11:52:23,013] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-08 11:52:23,013] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 11:52:23,013] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 11:52:23,013] [    INFO][0m -   Total prediction steps = 177[0m
[32m[2022-09-08 11:52:34,851] [    INFO][0m - eval_loss: 0.3872514069080353, eval_accuracy: 0.5594059228897095, eval_runtime: 11.8375, eval_samples_per_second: 119.451, eval_steps_per_second: 14.952, epoch: 0.565[0m
[32m[2022-09-08 11:52:34,919] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-09-08 11:52:34,919] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 11:52:38,199] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-08 11:52:38,199] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-08 11:52:45,371] [    INFO][0m - loss: 0.42841153, learning_rate: 9.875706214689266e-06, global_step: 110, interval_runtime: 22.3589, interval_samples_per_second: 0.358, interval_steps_per_second: 0.447, epoch: 0.6215[0m
[32m[2022-09-08 11:52:46,886] [    INFO][0m - loss: 0.35678897, learning_rate: 9.864406779661017e-06, global_step: 120, interval_runtime: 1.5154, interval_samples_per_second: 5.279, interval_steps_per_second: 6.599, epoch: 0.678[0m
[32m[2022-09-08 11:52:48,407] [    INFO][0m - loss: 0.4691895, learning_rate: 9.85310734463277e-06, global_step: 130, interval_runtime: 1.5201, interval_samples_per_second: 5.263, interval_steps_per_second: 6.578, epoch: 0.7345[0m
[32m[2022-09-08 11:52:49,928] [    INFO][0m - loss: 0.41251049, learning_rate: 9.84180790960452e-06, global_step: 140, interval_runtime: 1.521, interval_samples_per_second: 5.26, interval_steps_per_second: 6.575, epoch: 0.791[0m
[32m[2022-09-08 11:52:51,447] [    INFO][0m - loss: 0.47589417, learning_rate: 9.830508474576272e-06, global_step: 150, interval_runtime: 1.5188, interval_samples_per_second: 5.267, interval_steps_per_second: 6.584, epoch: 0.8475[0m
[32m[2022-09-08 11:52:52,975] [    INFO][0m - loss: 0.43925815, learning_rate: 9.819209039548023e-06, global_step: 160, interval_runtime: 1.5287, interval_samples_per_second: 5.233, interval_steps_per_second: 6.541, epoch: 0.904[0m
[32m[2022-09-08 11:52:54,512] [    INFO][0m - loss: 0.39749286, learning_rate: 9.807909604519775e-06, global_step: 170, interval_runtime: 1.5372, interval_samples_per_second: 5.204, interval_steps_per_second: 6.505, epoch: 0.9605[0m
[32m[2022-09-08 11:52:56,042] [    INFO][0m - loss: 0.36837859, learning_rate: 9.796610169491526e-06, global_step: 180, interval_runtime: 1.5297, interval_samples_per_second: 5.23, interval_steps_per_second: 6.537, epoch: 1.0169[0m
[32m[2022-09-08 11:52:57,556] [    INFO][0m - loss: 0.36068654, learning_rate: 9.785310734463278e-06, global_step: 190, interval_runtime: 1.5138, interval_samples_per_second: 5.285, interval_steps_per_second: 6.606, epoch: 1.0734[0m
[32m[2022-09-08 11:52:59,067] [    INFO][0m - loss: 0.4370204, learning_rate: 9.774011299435029e-06, global_step: 200, interval_runtime: 1.5118, interval_samples_per_second: 5.292, interval_steps_per_second: 6.614, epoch: 1.1299[0m
[32m[2022-09-08 11:52:59,068] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 11:52:59,068] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-08 11:52:59,068] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 11:52:59,068] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 11:52:59,068] [    INFO][0m -   Total prediction steps = 177[0m
[32m[2022-09-08 11:53:10,830] [    INFO][0m - eval_loss: 0.5201210379600525, eval_accuracy: 0.42574256658554077, eval_runtime: 11.7611, eval_samples_per_second: 120.227, eval_steps_per_second: 15.05, epoch: 1.1299[0m
[32m[2022-09-08 11:53:10,888] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-09-08 11:53:10,888] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 11:53:14,138] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-09-08 11:53:14,138] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-09-08 11:53:21,079] [    INFO][0m - loss: 0.35093124, learning_rate: 9.762711864406781e-06, global_step: 210, interval_runtime: 22.0108, interval_samples_per_second: 0.363, interval_steps_per_second: 0.454, epoch: 1.1864[0m
[32m[2022-09-08 11:53:22,628] [    INFO][0m - loss: 0.58314333, learning_rate: 9.751412429378532e-06, global_step: 220, interval_runtime: 1.549, interval_samples_per_second: 5.165, interval_steps_per_second: 6.456, epoch: 1.2429[0m
[32m[2022-09-08 11:53:24,159] [    INFO][0m - loss: 0.47407141, learning_rate: 9.740112994350284e-06, global_step: 230, interval_runtime: 1.5314, interval_samples_per_second: 5.224, interval_steps_per_second: 6.53, epoch: 1.2994[0m
[32m[2022-09-08 11:53:25,696] [    INFO][0m - loss: 0.45025916, learning_rate: 9.728813559322035e-06, global_step: 240, interval_runtime: 1.5369, interval_samples_per_second: 5.205, interval_steps_per_second: 6.507, epoch: 1.3559[0m
[32m[2022-09-08 11:53:27,222] [    INFO][0m - loss: 0.24655979, learning_rate: 9.717514124293787e-06, global_step: 250, interval_runtime: 1.5265, interval_samples_per_second: 5.241, interval_steps_per_second: 6.551, epoch: 1.4124[0m
[32m[2022-09-08 11:53:28,749] [    INFO][0m - loss: 0.5568223, learning_rate: 9.706214689265538e-06, global_step: 260, interval_runtime: 1.5269, interval_samples_per_second: 5.239, interval_steps_per_second: 6.549, epoch: 1.4689[0m
[32m[2022-09-08 11:53:30,271] [    INFO][0m - loss: 0.3308928, learning_rate: 9.69491525423729e-06, global_step: 270, interval_runtime: 1.5205, interval_samples_per_second: 5.262, interval_steps_per_second: 6.577, epoch: 1.5254[0m
[32m[2022-09-08 11:53:31,808] [    INFO][0m - loss: 0.36361451, learning_rate: 9.68361581920904e-06, global_step: 280, interval_runtime: 1.5382, interval_samples_per_second: 5.201, interval_steps_per_second: 6.501, epoch: 1.5819[0m
[32m[2022-09-08 11:53:33,332] [    INFO][0m - loss: 0.27444291, learning_rate: 9.672316384180791e-06, global_step: 290, interval_runtime: 1.5241, interval_samples_per_second: 5.249, interval_steps_per_second: 6.561, epoch: 1.6384[0m
[32m[2022-09-08 11:53:34,861] [    INFO][0m - loss: 0.6175961, learning_rate: 9.661016949152544e-06, global_step: 300, interval_runtime: 1.529, interval_samples_per_second: 5.232, interval_steps_per_second: 6.54, epoch: 1.6949[0m
[32m[2022-09-08 11:53:34,862] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 11:53:34,862] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-08 11:53:34,862] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 11:53:34,862] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 11:53:34,863] [    INFO][0m -   Total prediction steps = 177[0m
[32m[2022-09-08 11:53:46,734] [    INFO][0m - eval_loss: 0.35879114270210266, eval_accuracy: 0.4653465151786804, eval_runtime: 11.8706, eval_samples_per_second: 119.117, eval_steps_per_second: 14.911, epoch: 1.6949[0m
[32m[2022-09-08 11:53:46,794] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-09-08 11:53:46,794] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 11:53:50,477] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-09-08 11:53:50,477] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-09-08 11:53:57,953] [    INFO][0m - loss: 0.36510859, learning_rate: 9.649717514124294e-06, global_step: 310, interval_runtime: 23.0921, interval_samples_per_second: 0.346, interval_steps_per_second: 0.433, epoch: 1.7514[0m
[32m[2022-09-08 11:53:59,459] [    INFO][0m - loss: 0.37537005, learning_rate: 9.638418079096045e-06, global_step: 320, interval_runtime: 1.5061, interval_samples_per_second: 5.312, interval_steps_per_second: 6.64, epoch: 1.8079[0m
[32m[2022-09-08 11:54:00,967] [    INFO][0m - loss: 0.45099025, learning_rate: 9.627118644067797e-06, global_step: 330, interval_runtime: 1.5081, interval_samples_per_second: 5.305, interval_steps_per_second: 6.631, epoch: 1.8644[0m
[32m[2022-09-08 11:54:02,475] [    INFO][0m - loss: 0.32423785, learning_rate: 9.615819209039548e-06, global_step: 340, interval_runtime: 1.5082, interval_samples_per_second: 5.304, interval_steps_per_second: 6.63, epoch: 1.9209[0m
[32m[2022-09-08 11:54:03,976] [    INFO][0m - loss: 0.30899355, learning_rate: 9.6045197740113e-06, global_step: 350, interval_runtime: 1.5006, interval_samples_per_second: 5.331, interval_steps_per_second: 6.664, epoch: 1.9774[0m
[32m[2022-09-08 11:54:05,498] [    INFO][0m - loss: 0.30979967, learning_rate: 9.593220338983051e-06, global_step: 360, interval_runtime: 1.5215, interval_samples_per_second: 5.258, interval_steps_per_second: 6.572, epoch: 2.0339[0m
[32m[2022-09-08 11:54:07,008] [    INFO][0m - loss: 0.34199007, learning_rate: 9.581920903954803e-06, global_step: 370, interval_runtime: 1.5106, interval_samples_per_second: 5.296, interval_steps_per_second: 6.62, epoch: 2.0904[0m
[32m[2022-09-08 11:54:08,522] [    INFO][0m - loss: 0.43330297, learning_rate: 9.570621468926554e-06, global_step: 380, interval_runtime: 1.5137, interval_samples_per_second: 5.285, interval_steps_per_second: 6.606, epoch: 2.1469[0m
[32m[2022-09-08 11:54:10,031] [    INFO][0m - loss: 0.26783156, learning_rate: 9.559322033898306e-06, global_step: 390, interval_runtime: 1.5088, interval_samples_per_second: 5.302, interval_steps_per_second: 6.628, epoch: 2.2034[0m
[32m[2022-09-08 11:54:11,536] [    INFO][0m - loss: 0.30633605, learning_rate: 9.548022598870057e-06, global_step: 400, interval_runtime: 1.5056, interval_samples_per_second: 5.313, interval_steps_per_second: 6.642, epoch: 2.2599[0m
[32m[2022-09-08 11:54:11,537] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 11:54:11,537] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-08 11:54:11,537] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 11:54:11,537] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 11:54:11,537] [    INFO][0m -   Total prediction steps = 177[0m
[32m[2022-09-08 11:54:23,413] [    INFO][0m - eval_loss: 0.4004064202308655, eval_accuracy: 0.5049504637718201, eval_runtime: 11.8752, eval_samples_per_second: 119.071, eval_steps_per_second: 14.905, epoch: 2.2599[0m
[32m[2022-09-08 11:54:23,465] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-09-08 11:54:23,465] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 11:54:26,764] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-09-08 11:54:26,899] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-09-08 11:54:33,943] [    INFO][0m - loss: 0.33795187, learning_rate: 9.53672316384181e-06, global_step: 410, interval_runtime: 22.4066, interval_samples_per_second: 0.357, interval_steps_per_second: 0.446, epoch: 2.3164[0m
[32m[2022-09-08 11:54:35,448] [    INFO][0m - loss: 0.30401006, learning_rate: 9.52542372881356e-06, global_step: 420, interval_runtime: 1.5054, interval_samples_per_second: 5.314, interval_steps_per_second: 6.643, epoch: 2.3729[0m
[32m[2022-09-08 11:54:36,957] [    INFO][0m - loss: 0.23057516, learning_rate: 9.514124293785312e-06, global_step: 430, interval_runtime: 1.5087, interval_samples_per_second: 5.302, interval_steps_per_second: 6.628, epoch: 2.4294[0m
[32m[2022-09-08 11:54:38,469] [    INFO][0m - loss: 0.28923261, learning_rate: 9.502824858757063e-06, global_step: 440, interval_runtime: 1.5121, interval_samples_per_second: 5.291, interval_steps_per_second: 6.613, epoch: 2.4859[0m
[32m[2022-09-08 11:54:39,989] [    INFO][0m - loss: 0.38768964, learning_rate: 9.491525423728815e-06, global_step: 450, interval_runtime: 1.5201, interval_samples_per_second: 5.263, interval_steps_per_second: 6.579, epoch: 2.5424[0m
[32m[2022-09-08 11:54:41,512] [    INFO][0m - loss: 0.52246222, learning_rate: 9.480225988700566e-06, global_step: 460, interval_runtime: 1.5231, interval_samples_per_second: 5.253, interval_steps_per_second: 6.566, epoch: 2.5989[0m
[32m[2022-09-08 11:54:43,031] [    INFO][0m - loss: 0.20461273, learning_rate: 9.468926553672318e-06, global_step: 470, interval_runtime: 1.5188, interval_samples_per_second: 5.267, interval_steps_per_second: 6.584, epoch: 2.6554[0m
[32m[2022-09-08 11:54:44,553] [    INFO][0m - loss: 0.4279633, learning_rate: 9.457627118644069e-06, global_step: 480, interval_runtime: 1.5219, interval_samples_per_second: 5.257, interval_steps_per_second: 6.571, epoch: 2.7119[0m
[32m[2022-09-08 11:54:46,079] [    INFO][0m - loss: 0.3226294, learning_rate: 9.44632768361582e-06, global_step: 490, interval_runtime: 1.5259, interval_samples_per_second: 5.243, interval_steps_per_second: 6.554, epoch: 2.7684[0m
[32m[2022-09-08 11:54:47,609] [    INFO][0m - loss: 0.19900872, learning_rate: 9.435028248587572e-06, global_step: 500, interval_runtime: 1.5297, interval_samples_per_second: 5.23, interval_steps_per_second: 6.537, epoch: 2.8249[0m
[32m[2022-09-08 11:54:47,609] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 11:54:47,610] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-08 11:54:47,610] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 11:54:47,610] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 11:54:47,610] [    INFO][0m -   Total prediction steps = 177[0m
[32m[2022-09-08 11:54:59,366] [    INFO][0m - eval_loss: 0.6991438269615173, eval_accuracy: 0.4653465151786804, eval_runtime: 11.756, eval_samples_per_second: 120.279, eval_steps_per_second: 15.056, epoch: 2.8249[0m
[32m[2022-09-08 11:54:59,418] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-09-08 11:54:59,418] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 11:55:02,341] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-09-08 11:55:02,342] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-09-08 11:55:07,467] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-08 11:55:07,467] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-100 (score: 0.5594059228897095).[0m
[32m[2022-09-08 11:55:08,358] [    INFO][0m - train_runtime: 181.3913, train_samples_per_second: 389.765, train_steps_per_second: 48.79, train_loss: 0.9218314383029937, epoch: 2.8249[0m
[32m[2022-09-08 11:55:08,400] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-09-08 11:55:08,400] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 11:55:11,591] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-09-08 11:55:11,591] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-09-08 11:55:11,592] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-08 11:55:11,593] [    INFO][0m -   epoch                    =     2.8249[0m
[32m[2022-09-08 11:55:11,593] [    INFO][0m -   train_loss               =     0.9218[0m
[32m[2022-09-08 11:55:11,593] [    INFO][0m -   train_runtime            = 0:03:01.39[0m
[32m[2022-09-08 11:55:11,593] [    INFO][0m -   train_samples_per_second =    389.765[0m
[32m[2022-09-08 11:55:11,593] [    INFO][0m -   train_steps_per_second   =      48.79[0m
[32m[2022-09-08 11:55:11,596] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-08 11:55:11,596] [    INFO][0m -   Num examples = 14014[0m
[32m[2022-09-08 11:55:11,596] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 11:55:11,596] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 11:55:11,596] [    INFO][0m -   Total prediction steps = 1752[0m
[32m[2022-09-08 11:58:11,141] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-08 11:58:11,142] [    INFO][0m -   test_accuracy           =     0.5564[0m
[32m[2022-09-08 11:58:11,142] [    INFO][0m -   test_loss               =      0.388[0m
[32m[2022-09-08 11:58:11,142] [    INFO][0m -   test_runtime            = 0:02:59.54[0m
[32m[2022-09-08 11:58:11,142] [    INFO][0m -   test_samples_per_second =     78.053[0m
[32m[2022-09-08 11:58:11,142] [    INFO][0m -   test_steps_per_second   =      9.758[0m
[32m[2022-09-08 11:58:11,142] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-08 11:58:11,143] [    INFO][0m -   Num examples = 14000[0m
[32m[2022-09-08 11:58:11,143] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 11:58:11,143] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 11:58:11,143] [    INFO][0m -   Total prediction steps = 1750[0m
[32m[2022-09-08 12:01:02,974] [    INFO][0m - Predictions for chidf saved to ./fewclue_submit_examples.[0m
run.sh: line 59: --freeze_plm: command not found
 
==========
csl
==========
 
[33m[2022-09-08 12:01:06,939] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-09-08 12:01:06,939] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-08 12:01:06,940] [    INFO][0m - ============================================================[0m
[32m[2022-09-08 12:01:06,940] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-08 12:01:06,940] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-08 12:01:06,940] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-08 12:01:06,940] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-08 12:01:06,940] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-08 12:01:06,940] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-08 12:01:06,940] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-09-08 12:01:06,940] [    INFO][0m - [0m
[32m[2022-09-08 12:01:06,940] [    INFO][0m - ============================================================[0m
[32m[2022-09-08 12:01:06,940] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-08 12:01:06,940] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-08 12:01:06,941] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-08 12:01:06,941] [    INFO][0m - prompt                        :[0m
[32m[2022-09-08 12:01:06,941] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-08 12:01:06,941] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-08 12:01:06,941] [    INFO][0m - task_name                     :csl[0m
[32m[2022-09-08 12:01:06,941] [    INFO][0m - [0m
[32m[2022-09-08 12:01:06,941] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0908 12:01:06.942665 78694 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0908 12:01:06.946556 78694 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-08 12:01:09,805] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-09-08 12:01:09,830] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-09-08 12:01:09,830] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[32m[2022-09-08 12:01:10,816] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùÂÖ∂‰∏≠‚Äú'}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': '‚Äù'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'ËøôÂè•ËØùÁöÑÂÖ≥ÈîÆËØç„ÄÇ'}][0m
[32m[2022-09-08 12:01:11,339] [    INFO][0m - ============================================================[0m
[32m[2022-09-08 12:01:11,340] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-08 12:01:11,340] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-08 12:01:11,340] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-08 12:01:11,340] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-08 12:01:11,340] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-08 12:01:11,340] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-08 12:01:11,340] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-08 12:01:11,340] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-08 12:01:11,340] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-08 12:01:11,340] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-08 12:01:11,341] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-08 12:01:11,341] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-08 12:01:11,341] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-08 12:01:11,341] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-08 12:01:11,341] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-08 12:01:11,341] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-08 12:01:11,341] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-08 12:01:11,341] [    INFO][0m - eval_batch_size               :8[0m
[32m[2022-09-08 12:01:11,341] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-09-08 12:01:11,341] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-09-08 12:01:11,341] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-08 12:01:11,341] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-08 12:01:11,341] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-08 12:01:11,341] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-08 12:01:11,342] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-08 12:01:11,342] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-08 12:01:11,342] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-08 12:01:11,342] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-08 12:01:11,342] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-08 12:01:11,342] [    INFO][0m - learning_rate                 :1e-05[0m
[32m[2022-09-08 12:01:11,342] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-08 12:01:11,342] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-08 12:01:11,342] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-08 12:01:11,342] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-08 12:01:11,342] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-08 12:01:11,342] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-08 12:01:11,342] [    INFO][0m - logging_dir                   :./checkpoints/runs/Sep08_12-01-06_instance-3bwob41y-01[0m
[32m[2022-09-08 12:01:11,342] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-08 12:01:11,343] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-08 12:01:11,343] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-08 12:01:11,343] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-08 12:01:11,343] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-08 12:01:11,343] [    INFO][0m - max_seq_length                :320[0m
[32m[2022-09-08 12:01:11,343] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-08 12:01:11,343] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-08 12:01:11,343] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-08 12:01:11,343] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-08 12:01:11,343] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-09-08 12:01:11,343] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-08 12:01:11,343] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-08 12:01:11,343] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-09-08 12:01:11,344] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-08 12:01:11,344] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-08 12:01:11,344] [    INFO][0m - per_device_eval_batch_size    :8[0m
[32m[2022-09-08 12:01:11,344] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-09-08 12:01:11,344] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-08 12:01:11,344] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-08 12:01:11,344] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-08 12:01:11,344] [    INFO][0m - ppt_learning_rate             :0.0001[0m
[32m[2022-09-08 12:01:11,344] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-08 12:01:11,344] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-08 12:01:11,344] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-08 12:01:11,344] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-08 12:01:11,344] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-08 12:01:11,344] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-08 12:01:11,345] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-09-08 12:01:11,345] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-08 12:01:11,345] [    INFO][0m - save_steps                    :100[0m
[32m[2022-09-08 12:01:11,345] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-09-08 12:01:11,345] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-08 12:01:11,345] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-08 12:01:11,345] [    INFO][0m - seed                          :42[0m
[32m[2022-09-08 12:01:11,345] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-08 12:01:11,345] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-08 12:01:11,345] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-08 12:01:11,345] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-09-08 12:01:11,345] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-08 12:01:11,345] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-08 12:01:11,345] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-08 12:01:11,346] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-08 12:01:11,346] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-08 12:01:11,346] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-08 12:01:11,346] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-08 12:01:11,346] [    INFO][0m - [0m
[32m[2022-09-08 12:01:11,347] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-08 12:01:11,347] [    INFO][0m -   Num examples = 2642[0m
[32m[2022-09-08 12:01:11,348] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-09-08 12:01:11,348] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-09-08 12:01:11,348] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-09-08 12:01:11,348] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-08 12:01:11,348] [    INFO][0m -   Total optimization steps = 16550.0[0m
[32m[2022-09-08 12:01:11,348] [    INFO][0m -   Total num train samples = 132100[0m
[32m[2022-09-08 12:01:14,176] [    INFO][0m - loss: 15.2526123, learning_rate: 9.993957703927493e-06, global_step: 10, interval_runtime: 2.8266, interval_samples_per_second: 2.83, interval_steps_per_second: 3.538, epoch: 0.0302[0m
[32m[2022-09-08 12:01:16,085] [    INFO][0m - loss: 2.45476341, learning_rate: 9.987915407854986e-06, global_step: 20, interval_runtime: 1.9094, interval_samples_per_second: 4.19, interval_steps_per_second: 5.237, epoch: 0.0604[0m
[32m[2022-09-08 12:01:17,993] [    INFO][0m - loss: 0.35684485, learning_rate: 9.981873111782479e-06, global_step: 30, interval_runtime: 1.9083, interval_samples_per_second: 4.192, interval_steps_per_second: 5.24, epoch: 0.0906[0m
[32m[2022-09-08 12:01:19,914] [    INFO][0m - loss: 0.37750664, learning_rate: 9.975830815709971e-06, global_step: 40, interval_runtime: 1.9207, interval_samples_per_second: 4.165, interval_steps_per_second: 5.207, epoch: 0.1208[0m
[32m[2022-09-08 12:01:21,839] [    INFO][0m - loss: 0.36616244, learning_rate: 9.969788519637464e-06, global_step: 50, interval_runtime: 1.9256, interval_samples_per_second: 4.155, interval_steps_per_second: 5.193, epoch: 0.1511[0m
[32m[2022-09-08 12:01:23,750] [    INFO][0m - loss: 0.31126428, learning_rate: 9.963746223564955e-06, global_step: 60, interval_runtime: 1.9107, interval_samples_per_second: 4.187, interval_steps_per_second: 5.234, epoch: 0.1813[0m
[32m[2022-09-08 12:01:25,653] [    INFO][0m - loss: 0.33462279, learning_rate: 9.957703927492449e-06, global_step: 70, interval_runtime: 1.9029, interval_samples_per_second: 4.204, interval_steps_per_second: 5.255, epoch: 0.2115[0m
[32m[2022-09-08 12:01:27,570] [    INFO][0m - loss: 0.32059577, learning_rate: 9.95166163141994e-06, global_step: 80, interval_runtime: 1.9167, interval_samples_per_second: 4.174, interval_steps_per_second: 5.217, epoch: 0.2417[0m
[32m[2022-09-08 12:01:29,501] [    INFO][0m - loss: 0.3263763, learning_rate: 9.945619335347432e-06, global_step: 90, interval_runtime: 1.9313, interval_samples_per_second: 4.142, interval_steps_per_second: 5.178, epoch: 0.2719[0m
[32m[2022-09-08 12:01:31,410] [    INFO][0m - loss: 0.30213027, learning_rate: 9.939577039274926e-06, global_step: 100, interval_runtime: 1.9093, interval_samples_per_second: 4.19, interval_steps_per_second: 5.238, epoch: 0.3021[0m
[32m[2022-09-08 12:01:31,411] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:01:31,411] [    INFO][0m -   Num examples = 2712[0m
[32m[2022-09-08 12:01:31,411] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:01:31,411] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:01:31,411] [    INFO][0m -   Total prediction steps = 339[0m
(2712, 2)
2712
Traceback (most recent call last):
  File "train_single.py", line 268, in <module>
    main(0)
  File "train_single.py", line 222, in main
    train_result = trainer.train(resume_from_checkpoint=None)
  File "/ssd2/wanghuijuan03/prompt/PaddleNLP/paddlenlp/trainer/trainer_base.py", line 581, in train
    self._maybe_log_save_evaluate(tr_loss, model, epoch,
  File "/ssd2/wanghuijuan03/prompt/PaddleNLP/paddlenlp/trainer/trainer_base.py", line 710, in _maybe_log_save_evaluate
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
  File "/ssd2/wanghuijuan03/prompt/PaddleNLP/paddlenlp/trainer/trainer_base.py", line 1327, in evaluate
    output = self.evaluation_loop(
  File "/ssd2/wanghuijuan03/prompt/PaddleNLP/paddlenlp/trainer/trainer_base.py", line 1482, in evaluation_loop
    metrics = self.compute_metrics(
  File "train_single.py", line 181, in csl_metrics
    all_pred = paddle.stack(paddle.to_tensor(pred), axis=0)[:, 1]
  File "/ssd2/wanghuijuan03/anaconda3/envs/prompt/lib/python3.8/site-packages/paddle/tensor/manipulation.py", line 1715, in stack
    return _C_ops.stack(x, axis)
ValueError: (InvalidArgument) stack(): argument 'x' (position 0) must be list of Tensors, but got Tensor (at /paddle/paddle/fluid/pybind/eager_utils.cc:967)

run.sh: line 59: --freeze_plm: command not found
 
==========
cluewsc
==========
 
[33m[2022-09-08 12:02:13,287] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-09-08 12:02:13,288] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-08 12:02:13,288] [    INFO][0m - ============================================================[0m
[32m[2022-09-08 12:02:13,288] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-08 12:02:13,288] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-08 12:02:13,288] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-08 12:02:13,288] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-08 12:02:13,288] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-08 12:02:13,288] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-08 12:02:13,288] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-09-08 12:02:13,289] [    INFO][0m - [0m
[32m[2022-09-08 12:02:13,289] [    INFO][0m - ============================================================[0m
[32m[2022-09-08 12:02:13,289] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-08 12:02:13,289] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-08 12:02:13,289] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-08 12:02:13,289] [    INFO][0m - prompt                        :[0m
[32m[2022-09-08 12:02:13,289] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-08 12:02:13,289] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-08 12:02:13,289] [    INFO][0m - task_name                     :cluewsc[0m
[32m[2022-09-08 12:02:13,289] [    INFO][0m - [0m
[32m[2022-09-08 12:02:13,289] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0908 12:02:13.290944  1149 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0908 12:02:13.294955  1149 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-08 12:02:16,218] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-09-08 12:02:16,242] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-09-08 12:02:16,243] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[32m[2022-09-08 12:02:17,391] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùËøôÂè•ËØù‰∏≠‰ª£ËØç'}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': 'ÁöÑÁêÜËß£Ê≠£Á°ÆÂêóÔºü'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}][0m
[32m[2022-09-08 12:02:17,504] [    INFO][0m - ============================================================[0m
[32m[2022-09-08 12:02:17,504] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-08 12:02:17,504] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-08 12:02:17,504] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-08 12:02:17,504] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-08 12:02:17,504] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-08 12:02:17,505] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-08 12:02:17,505] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-08 12:02:17,505] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-08 12:02:17,505] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-08 12:02:17,505] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-08 12:02:17,505] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-08 12:02:17,505] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-08 12:02:17,505] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-08 12:02:17,505] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-08 12:02:17,505] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-08 12:02:17,505] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-08 12:02:17,505] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-08 12:02:17,506] [    INFO][0m - eval_batch_size               :8[0m
[32m[2022-09-08 12:02:17,506] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-09-08 12:02:17,506] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-09-08 12:02:17,506] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-08 12:02:17,506] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-08 12:02:17,506] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-08 12:02:17,506] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-08 12:02:17,506] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-08 12:02:17,506] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-08 12:02:17,506] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-08 12:02:17,506] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-08 12:02:17,506] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-08 12:02:17,506] [    INFO][0m - learning_rate                 :1e-05[0m
[32m[2022-09-08 12:02:17,507] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-08 12:02:17,507] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-08 12:02:17,507] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-08 12:02:17,507] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-08 12:02:17,507] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-08 12:02:17,507] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-08 12:02:17,507] [    INFO][0m - logging_dir                   :./checkpoints/runs/Sep08_12-02-13_instance-3bwob41y-01[0m
[32m[2022-09-08 12:02:17,507] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-08 12:02:17,507] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-08 12:02:17,507] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-08 12:02:17,507] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-08 12:02:17,507] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-08 12:02:17,507] [    INFO][0m - max_seq_length                :128[0m
[32m[2022-09-08 12:02:17,508] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-08 12:02:17,508] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-08 12:02:17,508] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-08 12:02:17,508] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-08 12:02:17,508] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-09-08 12:02:17,508] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-08 12:02:17,508] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-08 12:02:17,508] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-09-08 12:02:17,508] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-08 12:02:17,508] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-08 12:02:17,508] [    INFO][0m - per_device_eval_batch_size    :8[0m
[32m[2022-09-08 12:02:17,508] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-09-08 12:02:17,509] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-08 12:02:17,509] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-08 12:02:17,509] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-08 12:02:17,509] [    INFO][0m - ppt_learning_rate             :0.0001[0m
[32m[2022-09-08 12:02:17,509] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-08 12:02:17,509] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-08 12:02:17,509] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-08 12:02:17,509] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-08 12:02:17,509] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-08 12:02:17,509] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-08 12:02:17,509] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-09-08 12:02:17,509] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-08 12:02:17,509] [    INFO][0m - save_steps                    :100[0m
[32m[2022-09-08 12:02:17,509] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-09-08 12:02:17,510] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-08 12:02:17,510] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-08 12:02:17,510] [    INFO][0m - seed                          :42[0m
[32m[2022-09-08 12:02:17,510] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-08 12:02:17,510] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-08 12:02:17,510] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-08 12:02:17,510] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-09-08 12:02:17,510] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-08 12:02:17,510] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-08 12:02:17,510] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-08 12:02:17,510] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-08 12:02:17,510] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-08 12:02:17,510] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-08 12:02:17,511] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-08 12:02:17,511] [    INFO][0m - [0m
[32m[2022-09-08 12:02:17,512] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-08 12:02:17,512] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-08 12:02:17,512] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-09-08 12:02:17,513] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-09-08 12:02:17,513] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-09-08 12:02:17,513] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-08 12:02:17,513] [    INFO][0m -   Total optimization steps = 1000.0[0m
[32m[2022-09-08 12:02:17,513] [    INFO][0m -   Total num train samples = 8000[0m
[32m[2022-09-08 12:02:19,498] [    INFO][0m - loss: 19.05349274, learning_rate: 9.9e-06, global_step: 10, interval_runtime: 1.984, interval_samples_per_second: 4.032, interval_steps_per_second: 5.04, epoch: 0.5[0m
[32m[2022-09-08 12:02:20,334] [    INFO][0m - loss: 4.81750679, learning_rate: 9.800000000000001e-06, global_step: 20, interval_runtime: 0.8368, interval_samples_per_second: 9.56, interval_steps_per_second: 11.95, epoch: 1.0[0m
[32m[2022-09-08 12:02:21,249] [    INFO][0m - loss: 1.1231576, learning_rate: 9.7e-06, global_step: 30, interval_runtime: 0.915, interval_samples_per_second: 8.744, interval_steps_per_second: 10.929, epoch: 1.5[0m
[32m[2022-09-08 12:02:22,084] [    INFO][0m - loss: 0.77147923, learning_rate: 9.600000000000001e-06, global_step: 40, interval_runtime: 0.8351, interval_samples_per_second: 9.579, interval_steps_per_second: 11.974, epoch: 2.0[0m
[32m[2022-09-08 12:02:23,006] [    INFO][0m - loss: 0.75446186, learning_rate: 9.5e-06, global_step: 50, interval_runtime: 0.922, interval_samples_per_second: 8.677, interval_steps_per_second: 10.846, epoch: 2.5[0m
[32m[2022-09-08 12:02:23,855] [    INFO][0m - loss: 0.66509528, learning_rate: 9.4e-06, global_step: 60, interval_runtime: 0.849, interval_samples_per_second: 9.422, interval_steps_per_second: 11.778, epoch: 3.0[0m
[32m[2022-09-08 12:02:24,766] [    INFO][0m - loss: 0.63381968, learning_rate: 9.3e-06, global_step: 70, interval_runtime: 0.9108, interval_samples_per_second: 8.784, interval_steps_per_second: 10.979, epoch: 3.5[0m
[32m[2022-09-08 12:02:25,600] [    INFO][0m - loss: 0.71843252, learning_rate: 9.200000000000002e-06, global_step: 80, interval_runtime: 0.8337, interval_samples_per_second: 9.596, interval_steps_per_second: 11.995, epoch: 4.0[0m
[32m[2022-09-08 12:02:26,512] [    INFO][0m - loss: 0.61135373, learning_rate: 9.100000000000001e-06, global_step: 90, interval_runtime: 0.9122, interval_samples_per_second: 8.77, interval_steps_per_second: 10.963, epoch: 4.5[0m
[32m[2022-09-08 12:02:27,346] [    INFO][0m - loss: 0.55169659, learning_rate: 9e-06, global_step: 100, interval_runtime: 0.8332, interval_samples_per_second: 9.601, interval_steps_per_second: 12.002, epoch: 5.0[0m
[32m[2022-09-08 12:02:27,346] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:02:27,346] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-08 12:02:27,346] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:02:27,346] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:02:27,347] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-08 12:02:28,189] [    INFO][0m - eval_loss: 0.659487247467041, eval_accuracy: 0.6289308071136475, eval_runtime: 0.8422, eval_samples_per_second: 188.786, eval_steps_per_second: 23.747, epoch: 5.0[0m
[32m[2022-09-08 12:02:28,196] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-09-08 12:02:28,196] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:02:31,702] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-08 12:02:31,703] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-08 12:02:38,156] [    INFO][0m - loss: 0.49743414, learning_rate: 8.900000000000001e-06, global_step: 110, interval_runtime: 10.8106, interval_samples_per_second: 0.74, interval_steps_per_second: 0.925, epoch: 5.5[0m
[32m[2022-09-08 12:02:38,983] [    INFO][0m - loss: 0.49567471, learning_rate: 8.8e-06, global_step: 120, interval_runtime: 0.8273, interval_samples_per_second: 9.67, interval_steps_per_second: 12.087, epoch: 6.0[0m
[32m[2022-09-08 12:02:39,894] [    INFO][0m - loss: 0.38823633, learning_rate: 8.700000000000001e-06, global_step: 130, interval_runtime: 0.9107, interval_samples_per_second: 8.785, interval_steps_per_second: 10.981, epoch: 6.5[0m
[32m[2022-09-08 12:02:40,731] [    INFO][0m - loss: 0.35961707, learning_rate: 8.6e-06, global_step: 140, interval_runtime: 0.8373, interval_samples_per_second: 9.554, interval_steps_per_second: 11.943, epoch: 7.0[0m
[32m[2022-09-08 12:02:41,641] [    INFO][0m - loss: 0.26789832, learning_rate: 8.5e-06, global_step: 150, interval_runtime: 0.9099, interval_samples_per_second: 8.793, interval_steps_per_second: 10.991, epoch: 7.5[0m
[32m[2022-09-08 12:02:42,485] [    INFO][0m - loss: 0.16829368, learning_rate: 8.400000000000001e-06, global_step: 160, interval_runtime: 0.8442, interval_samples_per_second: 9.477, interval_steps_per_second: 11.846, epoch: 8.0[0m
[32m[2022-09-08 12:02:43,411] [    INFO][0m - loss: 0.19186872, learning_rate: 8.3e-06, global_step: 170, interval_runtime: 0.926, interval_samples_per_second: 8.64, interval_steps_per_second: 10.8, epoch: 8.5[0m
[32m[2022-09-08 12:02:44,250] [    INFO][0m - loss: 0.18722709, learning_rate: 8.2e-06, global_step: 180, interval_runtime: 0.8381, interval_samples_per_second: 9.545, interval_steps_per_second: 11.931, epoch: 9.0[0m
[32m[2022-09-08 12:02:45,168] [    INFO][0m - loss: 0.09403073, learning_rate: 8.1e-06, global_step: 190, interval_runtime: 0.9182, interval_samples_per_second: 8.713, interval_steps_per_second: 10.891, epoch: 9.5[0m
[32m[2022-09-08 12:02:46,009] [    INFO][0m - loss: 0.07947447, learning_rate: 8.000000000000001e-06, global_step: 200, interval_runtime: 0.8407, interval_samples_per_second: 9.516, interval_steps_per_second: 11.895, epoch: 10.0[0m
[32m[2022-09-08 12:02:46,009] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:02:46,009] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-08 12:02:46,010] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:02:46,010] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:02:46,010] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-08 12:02:46,855] [    INFO][0m - eval_loss: 1.4086335897445679, eval_accuracy: 0.654088020324707, eval_runtime: 0.8452, eval_samples_per_second: 188.111, eval_steps_per_second: 23.662, epoch: 10.0[0m
[32m[2022-09-08 12:02:46,862] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-09-08 12:02:46,862] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:02:50,247] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-09-08 12:02:50,248] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-09-08 12:02:56,549] [    INFO][0m - loss: 0.03686359, learning_rate: 7.9e-06, global_step: 210, interval_runtime: 10.5398, interval_samples_per_second: 0.759, interval_steps_per_second: 0.949, epoch: 10.5[0m
[32m[2022-09-08 12:02:57,383] [    INFO][0m - loss: 0.11092105, learning_rate: 7.800000000000002e-06, global_step: 220, interval_runtime: 0.8345, interval_samples_per_second: 9.587, interval_steps_per_second: 11.984, epoch: 11.0[0m
[32m[2022-09-08 12:02:58,296] [    INFO][0m - loss: 0.06939895, learning_rate: 7.7e-06, global_step: 230, interval_runtime: 0.9131, interval_samples_per_second: 8.762, interval_steps_per_second: 10.952, epoch: 11.5[0m
[32m[2022-09-08 12:02:59,137] [    INFO][0m - loss: 0.07227665, learning_rate: 7.600000000000001e-06, global_step: 240, interval_runtime: 0.8408, interval_samples_per_second: 9.515, interval_steps_per_second: 11.894, epoch: 12.0[0m
[32m[2022-09-08 12:03:00,066] [    INFO][0m - loss: 0.11211804, learning_rate: 7.500000000000001e-06, global_step: 250, interval_runtime: 0.9293, interval_samples_per_second: 8.609, interval_steps_per_second: 10.761, epoch: 12.5[0m
[32m[2022-09-08 12:03:00,908] [    INFO][0m - loss: 0.11922969, learning_rate: 7.4e-06, global_step: 260, interval_runtime: 0.8415, interval_samples_per_second: 9.507, interval_steps_per_second: 11.884, epoch: 13.0[0m
[32m[2022-09-08 12:03:01,838] [    INFO][0m - loss: 0.00419068, learning_rate: 7.3e-06, global_step: 270, interval_runtime: 0.9302, interval_samples_per_second: 8.6, interval_steps_per_second: 10.75, epoch: 13.5[0m
[32m[2022-09-08 12:03:02,691] [    INFO][0m - loss: 0.07820972, learning_rate: 7.2000000000000005e-06, global_step: 280, interval_runtime: 0.8533, interval_samples_per_second: 9.376, interval_steps_per_second: 11.72, epoch: 14.0[0m
[32m[2022-09-08 12:03:03,622] [    INFO][0m - loss: 0.13527269, learning_rate: 7.100000000000001e-06, global_step: 290, interval_runtime: 0.9307, interval_samples_per_second: 8.596, interval_steps_per_second: 10.745, epoch: 14.5[0m
[32m[2022-09-08 12:03:04,502] [    INFO][0m - loss: 0.07238578, learning_rate: 7e-06, global_step: 300, interval_runtime: 0.8454, interval_samples_per_second: 9.463, interval_steps_per_second: 11.829, epoch: 15.0[0m
[32m[2022-09-08 12:03:04,503] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:03:04,503] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-08 12:03:04,503] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:03:04,503] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:03:04,504] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-08 12:03:05,347] [    INFO][0m - eval_loss: 1.9470717906951904, eval_accuracy: 0.6729559302330017, eval_runtime: 0.8431, eval_samples_per_second: 188.592, eval_steps_per_second: 23.722, epoch: 15.0[0m
[32m[2022-09-08 12:03:05,353] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-09-08 12:03:05,353] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:03:08,815] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-09-08 12:03:08,816] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-09-08 12:03:14,901] [    INFO][0m - loss: 0.0013921, learning_rate: 6.9e-06, global_step: 310, interval_runtime: 10.4335, interval_samples_per_second: 0.767, interval_steps_per_second: 0.958, epoch: 15.5[0m
[32m[2022-09-08 12:03:15,739] [    INFO][0m - loss: 0.02685498, learning_rate: 6.800000000000001e-06, global_step: 320, interval_runtime: 0.8384, interval_samples_per_second: 9.542, interval_steps_per_second: 11.928, epoch: 16.0[0m
[32m[2022-09-08 12:03:16,642] [    INFO][0m - loss: 0.00527815, learning_rate: 6.700000000000001e-06, global_step: 330, interval_runtime: 0.9032, interval_samples_per_second: 8.858, interval_steps_per_second: 11.072, epoch: 16.5[0m
[32m[2022-09-08 12:03:17,479] [    INFO][0m - loss: 0.07869327, learning_rate: 6.600000000000001e-06, global_step: 340, interval_runtime: 0.8368, interval_samples_per_second: 9.56, interval_steps_per_second: 11.95, epoch: 17.0[0m
[32m[2022-09-08 12:03:18,388] [    INFO][0m - loss: 0.01319718, learning_rate: 6.5000000000000004e-06, global_step: 350, interval_runtime: 0.9094, interval_samples_per_second: 8.797, interval_steps_per_second: 10.997, epoch: 17.5[0m
[32m[2022-09-08 12:03:19,225] [    INFO][0m - loss: 0.04114324, learning_rate: 6.4000000000000006e-06, global_step: 360, interval_runtime: 0.8361, interval_samples_per_second: 9.569, interval_steps_per_second: 11.961, epoch: 18.0[0m
[32m[2022-09-08 12:03:20,131] [    INFO][0m - loss: 0.01998967, learning_rate: 6.300000000000001e-06, global_step: 370, interval_runtime: 0.9069, interval_samples_per_second: 8.821, interval_steps_per_second: 11.026, epoch: 18.5[0m
[32m[2022-09-08 12:03:20,965] [    INFO][0m - loss: 0.04561192, learning_rate: 6.200000000000001e-06, global_step: 380, interval_runtime: 0.834, interval_samples_per_second: 9.593, interval_steps_per_second: 11.991, epoch: 19.0[0m
[32m[2022-09-08 12:03:21,881] [    INFO][0m - loss: 0.03668875, learning_rate: 6.1e-06, global_step: 390, interval_runtime: 0.9156, interval_samples_per_second: 8.737, interval_steps_per_second: 10.922, epoch: 19.5[0m
[32m[2022-09-08 12:03:22,716] [    INFO][0m - loss: 0.10860999, learning_rate: 6e-06, global_step: 400, interval_runtime: 0.835, interval_samples_per_second: 9.58, interval_steps_per_second: 11.976, epoch: 20.0[0m
[32m[2022-09-08 12:03:22,716] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:03:22,716] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-08 12:03:22,717] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:03:22,717] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:03:22,717] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-08 12:03:23,564] [    INFO][0m - eval_loss: 2.2199554443359375, eval_accuracy: 0.6477987170219421, eval_runtime: 0.8472, eval_samples_per_second: 187.674, eval_steps_per_second: 23.607, epoch: 20.0[0m
[32m[2022-09-08 12:03:23,570] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-09-08 12:03:23,570] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:03:26,999] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-09-08 12:03:27,000] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-09-08 12:03:33,381] [    INFO][0m - loss: 0.05952516, learning_rate: 5.9e-06, global_step: 410, interval_runtime: 10.6653, interval_samples_per_second: 0.75, interval_steps_per_second: 0.938, epoch: 20.5[0m
[32m[2022-09-08 12:03:34,212] [    INFO][0m - loss: 0.03378142, learning_rate: 5.8e-06, global_step: 420, interval_runtime: 0.831, interval_samples_per_second: 9.627, interval_steps_per_second: 12.033, epoch: 21.0[0m
[32m[2022-09-08 12:03:35,128] [    INFO][0m - loss: 0.00057021, learning_rate: 5.7e-06, global_step: 430, interval_runtime: 0.915, interval_samples_per_second: 8.743, interval_steps_per_second: 10.929, epoch: 21.5[0m
[32m[2022-09-08 12:03:35,963] [    INFO][0m - loss: 0.00379688, learning_rate: 5.600000000000001e-06, global_step: 440, interval_runtime: 0.8362, interval_samples_per_second: 9.567, interval_steps_per_second: 11.959, epoch: 22.0[0m
[32m[2022-09-08 12:03:36,894] [    INFO][0m - loss: 0.01710977, learning_rate: 5.500000000000001e-06, global_step: 450, interval_runtime: 0.9303, interval_samples_per_second: 8.599, interval_steps_per_second: 10.749, epoch: 22.5[0m
[32m[2022-09-08 12:03:37,729] [    INFO][0m - loss: 0.00016569, learning_rate: 5.400000000000001e-06, global_step: 460, interval_runtime: 0.8354, interval_samples_per_second: 9.576, interval_steps_per_second: 11.97, epoch: 23.0[0m
[32m[2022-09-08 12:03:38,637] [    INFO][0m - loss: 0.0285875, learning_rate: 5.300000000000001e-06, global_step: 470, interval_runtime: 0.9072, interval_samples_per_second: 8.818, interval_steps_per_second: 11.023, epoch: 23.5[0m
[32m[2022-09-08 12:03:39,470] [    INFO][0m - loss: 0.02569588, learning_rate: 5.2e-06, global_step: 480, interval_runtime: 0.8334, interval_samples_per_second: 9.599, interval_steps_per_second: 11.998, epoch: 24.0[0m
[32m[2022-09-08 12:03:40,378] [    INFO][0m - loss: 0.00060633, learning_rate: 5.1e-06, global_step: 490, interval_runtime: 0.9083, interval_samples_per_second: 8.807, interval_steps_per_second: 11.009, epoch: 24.5[0m
[32m[2022-09-08 12:03:41,215] [    INFO][0m - loss: 0.05969639, learning_rate: 5e-06, global_step: 500, interval_runtime: 0.8367, interval_samples_per_second: 9.561, interval_steps_per_second: 11.951, epoch: 25.0[0m
[32m[2022-09-08 12:03:41,216] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:03:41,216] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-08 12:03:41,216] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:03:41,216] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:03:41,216] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-08 12:03:42,042] [    INFO][0m - eval_loss: 2.582927942276001, eval_accuracy: 0.6666666269302368, eval_runtime: 0.8261, eval_samples_per_second: 192.47, eval_steps_per_second: 24.21, epoch: 25.0[0m
[32m[2022-09-08 12:03:42,049] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-09-08 12:03:42,049] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:03:45,999] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-09-08 12:03:46,000] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-09-08 12:03:52,575] [    INFO][0m - loss: 0.0007301, learning_rate: 4.9000000000000005e-06, global_step: 510, interval_runtime: 11.3604, interval_samples_per_second: 0.704, interval_steps_per_second: 0.88, epoch: 25.5[0m
[32m[2022-09-08 12:03:53,411] [    INFO][0m - loss: 0.00927101, learning_rate: 4.800000000000001e-06, global_step: 520, interval_runtime: 0.8355, interval_samples_per_second: 9.575, interval_steps_per_second: 11.969, epoch: 26.0[0m
[32m[2022-09-08 12:03:54,330] [    INFO][0m - loss: 0.00034066, learning_rate: 4.7e-06, global_step: 530, interval_runtime: 0.9185, interval_samples_per_second: 8.71, interval_steps_per_second: 10.888, epoch: 26.5[0m
[32m[2022-09-08 12:03:55,161] [    INFO][0m - loss: 0.01156275, learning_rate: 4.600000000000001e-06, global_step: 540, interval_runtime: 0.8312, interval_samples_per_second: 9.625, interval_steps_per_second: 12.031, epoch: 27.0[0m
[32m[2022-09-08 12:03:56,068] [    INFO][0m - loss: 0.00015654, learning_rate: 4.5e-06, global_step: 550, interval_runtime: 0.9076, interval_samples_per_second: 8.814, interval_steps_per_second: 11.018, epoch: 27.5[0m
[32m[2022-09-08 12:03:56,909] [    INFO][0m - loss: 0.00363039, learning_rate: 4.4e-06, global_step: 560, interval_runtime: 0.8407, interval_samples_per_second: 9.516, interval_steps_per_second: 11.895, epoch: 28.0[0m
[32m[2022-09-08 12:03:57,817] [    INFO][0m - loss: 0.00073335, learning_rate: 4.3e-06, global_step: 570, interval_runtime: 0.9077, interval_samples_per_second: 8.814, interval_steps_per_second: 11.017, epoch: 28.5[0m
[32m[2022-09-08 12:03:58,651] [    INFO][0m - loss: 0.00632836, learning_rate: 4.2000000000000004e-06, global_step: 580, interval_runtime: 0.8343, interval_samples_per_second: 9.589, interval_steps_per_second: 11.986, epoch: 29.0[0m
[32m[2022-09-08 12:03:59,561] [    INFO][0m - loss: 0.00031415, learning_rate: 4.1e-06, global_step: 590, interval_runtime: 0.9099, interval_samples_per_second: 8.792, interval_steps_per_second: 10.99, epoch: 29.5[0m
[32m[2022-09-08 12:04:00,394] [    INFO][0m - loss: 9.827e-05, learning_rate: 4.000000000000001e-06, global_step: 600, interval_runtime: 0.8336, interval_samples_per_second: 9.597, interval_steps_per_second: 11.997, epoch: 30.0[0m
[32m[2022-09-08 12:04:00,395] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:04:00,395] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-08 12:04:00,395] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:04:00,395] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:04:00,396] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-08 12:04:01,220] [    INFO][0m - eval_loss: 2.6149983406066895, eval_accuracy: 0.6477987170219421, eval_runtime: 0.824, eval_samples_per_second: 192.97, eval_steps_per_second: 24.273, epoch: 30.0[0m
[32m[2022-09-08 12:04:01,227] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-09-08 12:04:01,227] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:04:04,479] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-09-08 12:04:04,479] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-09-08 12:04:10,725] [    INFO][0m - loss: 0.00011325, learning_rate: 3.900000000000001e-06, global_step: 610, interval_runtime: 10.3309, interval_samples_per_second: 0.774, interval_steps_per_second: 0.968, epoch: 30.5[0m
[32m[2022-09-08 12:04:11,564] [    INFO][0m - loss: 0.1082347, learning_rate: 3.8000000000000005e-06, global_step: 620, interval_runtime: 0.8383, interval_samples_per_second: 9.543, interval_steps_per_second: 11.929, epoch: 31.0[0m
[32m[2022-09-08 12:04:12,485] [    INFO][0m - loss: 8.222e-05, learning_rate: 3.7e-06, global_step: 630, interval_runtime: 0.9214, interval_samples_per_second: 8.682, interval_steps_per_second: 10.853, epoch: 31.5[0m
[32m[2022-09-08 12:04:13,317] [    INFO][0m - loss: 0.00010433, learning_rate: 3.6000000000000003e-06, global_step: 640, interval_runtime: 0.8323, interval_samples_per_second: 9.612, interval_steps_per_second: 12.015, epoch: 32.0[0m
[32m[2022-09-08 12:04:14,235] [    INFO][0m - loss: 0.00030883, learning_rate: 3.5e-06, global_step: 650, interval_runtime: 0.918, interval_samples_per_second: 8.714, interval_steps_per_second: 10.893, epoch: 32.5[0m
[32m[2022-09-08 12:04:15,062] [    INFO][0m - loss: 0.00012811, learning_rate: 3.4000000000000005e-06, global_step: 660, interval_runtime: 0.8265, interval_samples_per_second: 9.68, interval_steps_per_second: 12.099, epoch: 33.0[0m
[32m[2022-09-08 12:04:15,985] [    INFO][0m - loss: 0.00011063, learning_rate: 3.3000000000000006e-06, global_step: 670, interval_runtime: 0.9236, interval_samples_per_second: 8.662, interval_steps_per_second: 10.828, epoch: 33.5[0m
[32m[2022-09-08 12:04:16,818] [    INFO][0m - loss: 0.00285984, learning_rate: 3.2000000000000003e-06, global_step: 680, interval_runtime: 0.8328, interval_samples_per_second: 9.607, interval_steps_per_second: 12.008, epoch: 34.0[0m
[32m[2022-09-08 12:04:17,725] [    INFO][0m - loss: 7.067e-05, learning_rate: 3.1000000000000004e-06, global_step: 690, interval_runtime: 0.9064, interval_samples_per_second: 8.826, interval_steps_per_second: 11.033, epoch: 34.5[0m
[32m[2022-09-08 12:04:18,555] [    INFO][0m - loss: 7.892e-05, learning_rate: 3e-06, global_step: 700, interval_runtime: 0.83, interval_samples_per_second: 9.639, interval_steps_per_second: 12.049, epoch: 35.0[0m
[32m[2022-09-08 12:04:18,556] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:04:18,556] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-08 12:04:18,556] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:04:18,556] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:04:18,556] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-08 12:04:19,402] [    INFO][0m - eval_loss: 2.6286606788635254, eval_accuracy: 0.6477987170219421, eval_runtime: 0.8454, eval_samples_per_second: 188.067, eval_steps_per_second: 23.656, epoch: 35.0[0m
[32m[2022-09-08 12:04:19,408] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-700[0m
[32m[2022-09-08 12:04:19,408] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:04:22,909] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-700/tokenizer_config.json[0m
[32m[2022-09-08 12:04:22,910] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-700/special_tokens_map.json[0m
[32m[2022-09-08 12:04:28,225] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-08 12:04:28,226] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-300 (score: 0.6729559302330017).[0m
[32m[2022-09-08 12:04:29,399] [    INFO][0m - train_runtime: 131.8853, train_samples_per_second: 60.659, train_steps_per_second: 7.582, train_loss: 0.48561956651026517, epoch: 35.0[0m
[32m[2022-09-08 12:04:29,400] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-09-08 12:04:29,400] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:04:32,788] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-09-08 12:04:32,789] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-09-08 12:04:32,794] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-08 12:04:32,794] [    INFO][0m -   epoch                    =       35.0[0m
[32m[2022-09-08 12:04:32,794] [    INFO][0m -   train_loss               =     0.4856[0m
[32m[2022-09-08 12:04:32,794] [    INFO][0m -   train_runtime            = 0:02:11.88[0m
[32m[2022-09-08 12:04:32,794] [    INFO][0m -   train_samples_per_second =     60.659[0m
[32m[2022-09-08 12:04:32,795] [    INFO][0m -   train_steps_per_second   =      7.582[0m
[32m[2022-09-08 12:04:32,799] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-08 12:04:32,799] [    INFO][0m -   Num examples = 976[0m
[32m[2022-09-08 12:04:32,799] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:04:32,799] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:04:32,799] [    INFO][0m -   Total prediction steps = 122[0m
[32m[2022-09-08 12:04:37,970] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-08 12:04:37,970] [    INFO][0m -   test_accuracy           =     0.5625[0m
[32m[2022-09-08 12:04:37,971] [    INFO][0m -   test_loss               =     3.1266[0m
[32m[2022-09-08 12:04:37,971] [    INFO][0m -   test_runtime            = 0:00:05.17[0m
[32m[2022-09-08 12:04:37,971] [    INFO][0m -   test_samples_per_second =     188.74[0m
[32m[2022-09-08 12:04:37,971] [    INFO][0m -   test_steps_per_second   =     23.593[0m
[32m[2022-09-08 12:04:37,971] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-08 12:04:37,971] [    INFO][0m -   Num examples = 290[0m
[32m[2022-09-08 12:04:37,971] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:04:37,971] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:04:37,971] [    INFO][0m -   Total prediction steps = 37[0m
Traceback (most recent call last):
  File "train_single.py", line 268, in <module>
    main(0)
  File "train_single.py", line 262, in main
    test_ret = postprocess(test_ret, test_ds, data_args.task_name,
  File "/ssd2/wanghuijuan03/prompt/PaddleNLP/examples/benchmark/fewclue/pretrain15/postprocess.py", line 66, in postprocess
    ret_list.append({"id": uid, "label": id_to_label[preds[idx]]})
TypeError: unhashable type: 'numpy.ndarray'
run.sh: line 59: --freeze_plm: command not found
