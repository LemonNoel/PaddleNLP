 
==========
eprstmt
==========
 
[33m[2022-09-08 12:01:48,723] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-09-08 12:01:48,723] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-08 12:01:48,723] [    INFO][0m - ============================================================[0m
[32m[2022-09-08 12:01:48,723] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-08 12:01:48,723] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-08 12:01:48,723] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-08 12:01:48,723] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-08 12:01:48,724] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-08 12:01:48,724] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-08 12:01:48,724] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-09-08 12:01:48,724] [    INFO][0m - [0m
[32m[2022-09-08 12:01:48,724] [    INFO][0m - ============================================================[0m
[32m[2022-09-08 12:01:48,724] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-08 12:01:48,724] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-08 12:01:48,724] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-08 12:01:48,724] [    INFO][0m - prompt                        :[0m
[32m[2022-09-08 12:01:48,724] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-08 12:01:48,724] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-08 12:01:48,724] [    INFO][0m - task_name                     :eprstmt[0m
[32m[2022-09-08 12:01:48,724] [    INFO][0m - [0m
[32m[2022-09-08 12:01:48,725] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0908 12:01:48.726199 79368 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0908 12:01:48.730250 79368 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-08 12:01:51,652] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-09-08 12:01:51,677] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-09-08 12:01:51,677] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[32m[2022-09-08 12:01:52,685] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùËøôÊù°ËØÑËÆ∫ÁöÑÊÉÖÊÑüÂÄæÂêëÊòØ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'ÁöÑ„ÄÇ'}][0m
[32m[2022-09-08 12:01:52,789] [    INFO][0m - ============================================================[0m
[32m[2022-09-08 12:01:52,789] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-08 12:01:52,789] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-08 12:01:52,789] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-08 12:01:52,789] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-08 12:01:52,789] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-08 12:01:52,790] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-08 12:01:52,790] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-08 12:01:52,790] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-08 12:01:52,790] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-08 12:01:52,790] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-08 12:01:52,790] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-08 12:01:52,790] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-08 12:01:52,790] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-08 12:01:52,790] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-08 12:01:52,790] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-08 12:01:52,790] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-08 12:01:52,790] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-08 12:01:52,790] [    INFO][0m - eval_batch_size               :8[0m
[32m[2022-09-08 12:01:52,791] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-09-08 12:01:52,791] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-09-08 12:01:52,791] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-08 12:01:52,791] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-08 12:01:52,791] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-08 12:01:52,791] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-08 12:01:52,791] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-08 12:01:52,791] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-08 12:01:52,791] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-08 12:01:52,791] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-08 12:01:52,791] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-08 12:01:52,791] [    INFO][0m - learning_rate                 :1e-05[0m
[32m[2022-09-08 12:01:52,791] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-08 12:01:52,792] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-08 12:01:52,792] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-08 12:01:52,792] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-08 12:01:52,792] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-08 12:01:52,792] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-08 12:01:52,792] [    INFO][0m - logging_dir                   :./checkpoints/runs/Sep08_12-01-48_instance-3bwob41y-01[0m
[32m[2022-09-08 12:01:52,792] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-08 12:01:52,792] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-08 12:01:52,792] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-08 12:01:52,792] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-08 12:01:52,792] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-08 12:01:52,792] [    INFO][0m - max_seq_length                :128[0m
[32m[2022-09-08 12:01:52,792] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-08 12:01:52,792] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-08 12:01:52,793] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-08 12:01:52,793] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-08 12:01:52,793] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-09-08 12:01:52,793] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-08 12:01:52,793] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-08 12:01:52,793] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-09-08 12:01:52,793] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-08 12:01:52,793] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-08 12:01:52,793] [    INFO][0m - per_device_eval_batch_size    :8[0m
[32m[2022-09-08 12:01:52,793] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-09-08 12:01:52,793] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-08 12:01:52,793] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-08 12:01:52,793] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-08 12:01:52,794] [    INFO][0m - ppt_learning_rate             :0.0001[0m
[32m[2022-09-08 12:01:52,794] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-08 12:01:52,794] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-08 12:01:52,794] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-08 12:01:52,794] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-08 12:01:52,794] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-08 12:01:52,794] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-08 12:01:52,794] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-09-08 12:01:52,794] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-08 12:01:52,794] [    INFO][0m - save_steps                    :100[0m
[32m[2022-09-08 12:01:52,794] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-09-08 12:01:52,794] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-08 12:01:52,794] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-08 12:01:52,794] [    INFO][0m - seed                          :42[0m
[32m[2022-09-08 12:01:52,795] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-08 12:01:52,795] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-08 12:01:52,795] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-08 12:01:52,795] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-09-08 12:01:52,795] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-08 12:01:52,795] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-08 12:01:52,795] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-08 12:01:52,795] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-08 12:01:52,795] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-08 12:01:52,795] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-08 12:01:52,795] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-08 12:01:52,795] [    INFO][0m - [0m
[32m[2022-09-08 12:01:52,797] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-08 12:01:52,797] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-08 12:01:52,797] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-09-08 12:01:52,797] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-09-08 12:01:52,797] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-09-08 12:01:52,797] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-08 12:01:52,798] [    INFO][0m -   Total optimization steps = 1000.0[0m
[32m[2022-09-08 12:01:52,798] [    INFO][0m -   Total num train samples = 8000[0m
[32m[2022-09-08 12:01:54,840] [    INFO][0m - loss: 14.8617981, learning_rate: 9.9e-06, global_step: 10, interval_runtime: 2.0405, interval_samples_per_second: 3.921, interval_steps_per_second: 4.901, epoch: 0.5[0m
[32m[2022-09-08 12:01:55,648] [    INFO][0m - loss: 1.00487127, learning_rate: 9.800000000000001e-06, global_step: 20, interval_runtime: 0.8095, interval_samples_per_second: 9.883, interval_steps_per_second: 12.354, epoch: 1.0[0m
[32m[2022-09-08 12:01:56,501] [    INFO][0m - loss: 0.1741201, learning_rate: 9.7e-06, global_step: 30, interval_runtime: 0.8529, interval_samples_per_second: 9.38, interval_steps_per_second: 11.725, epoch: 1.5[0m
[32m[2022-09-08 12:01:57,308] [    INFO][0m - loss: 0.17081985, learning_rate: 9.600000000000001e-06, global_step: 40, interval_runtime: 0.8068, interval_samples_per_second: 9.915, interval_steps_per_second: 12.394, epoch: 2.0[0m
[32m[2022-09-08 12:01:58,167] [    INFO][0m - loss: 0.06964692, learning_rate: 9.5e-06, global_step: 50, interval_runtime: 0.8587, interval_samples_per_second: 9.316, interval_steps_per_second: 11.645, epoch: 2.5[0m
[32m[2022-09-08 12:01:58,976] [    INFO][0m - loss: 0.12518606, learning_rate: 9.4e-06, global_step: 60, interval_runtime: 0.8084, interval_samples_per_second: 9.896, interval_steps_per_second: 12.369, epoch: 3.0[0m
[32m[2022-09-08 12:01:59,838] [    INFO][0m - loss: 0.09417019, learning_rate: 9.3e-06, global_step: 70, interval_runtime: 0.8626, interval_samples_per_second: 9.275, interval_steps_per_second: 11.593, epoch: 3.5[0m
[32m[2022-09-08 12:02:00,666] [    INFO][0m - loss: 0.06998274, learning_rate: 9.200000000000002e-06, global_step: 80, interval_runtime: 0.8277, interval_samples_per_second: 9.665, interval_steps_per_second: 12.081, epoch: 4.0[0m
[32m[2022-09-08 12:02:01,523] [    INFO][0m - loss: 0.03590367, learning_rate: 9.100000000000001e-06, global_step: 90, interval_runtime: 0.857, interval_samples_per_second: 9.334, interval_steps_per_second: 11.668, epoch: 4.5[0m
[32m[2022-09-08 12:02:02,335] [    INFO][0m - loss: 0.09786071, learning_rate: 9e-06, global_step: 100, interval_runtime: 0.8121, interval_samples_per_second: 9.851, interval_steps_per_second: 12.314, epoch: 5.0[0m
[32m[2022-09-08 12:02:02,336] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:02:02,336] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-08 12:02:02,336] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:02:02,336] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:02:02,336] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-08 12:02:03,124] [    INFO][0m - eval_loss: 0.2842688262462616, eval_accuracy: 0.893750011920929, eval_runtime: 0.7878, eval_samples_per_second: 203.096, eval_steps_per_second: 25.387, epoch: 5.0[0m
[32m[2022-09-08 12:02:03,134] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-09-08 12:02:03,134] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:02:06,710] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-08 12:02:06,710] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-08 12:02:13,035] [    INFO][0m - loss: 0.04256169, learning_rate: 8.900000000000001e-06, global_step: 110, interval_runtime: 10.7, interval_samples_per_second: 0.748, interval_steps_per_second: 0.935, epoch: 5.5[0m
[32m[2022-09-08 12:02:13,838] [    INFO][0m - loss: 0.0538379, learning_rate: 8.8e-06, global_step: 120, interval_runtime: 0.8037, interval_samples_per_second: 9.954, interval_steps_per_second: 12.443, epoch: 6.0[0m
[32m[2022-09-08 12:02:14,700] [    INFO][0m - loss: 0.00265306, learning_rate: 8.700000000000001e-06, global_step: 130, interval_runtime: 0.8616, interval_samples_per_second: 9.285, interval_steps_per_second: 11.606, epoch: 6.5[0m
[32m[2022-09-08 12:02:15,515] [    INFO][0m - loss: 0.00154344, learning_rate: 8.6e-06, global_step: 140, interval_runtime: 0.8145, interval_samples_per_second: 9.822, interval_steps_per_second: 12.277, epoch: 7.0[0m
[32m[2022-09-08 12:02:16,376] [    INFO][0m - loss: 0.00347946, learning_rate: 8.5e-06, global_step: 150, interval_runtime: 0.861, interval_samples_per_second: 9.291, interval_steps_per_second: 11.614, epoch: 7.5[0m
[32m[2022-09-08 12:02:17,183] [    INFO][0m - loss: 0.00138294, learning_rate: 8.400000000000001e-06, global_step: 160, interval_runtime: 0.8077, interval_samples_per_second: 9.904, interval_steps_per_second: 12.38, epoch: 8.0[0m
[32m[2022-09-08 12:02:18,043] [    INFO][0m - loss: 0.00022444, learning_rate: 8.3e-06, global_step: 170, interval_runtime: 0.8589, interval_samples_per_second: 9.315, interval_steps_per_second: 11.643, epoch: 8.5[0m
[32m[2022-09-08 12:02:18,853] [    INFO][0m - loss: 0.00082518, learning_rate: 8.2e-06, global_step: 180, interval_runtime: 0.8103, interval_samples_per_second: 9.873, interval_steps_per_second: 12.342, epoch: 9.0[0m
[32m[2022-09-08 12:02:19,709] [    INFO][0m - loss: 0.00024985, learning_rate: 8.1e-06, global_step: 190, interval_runtime: 0.8566, interval_samples_per_second: 9.339, interval_steps_per_second: 11.674, epoch: 9.5[0m
[32m[2022-09-08 12:02:20,515] [    INFO][0m - loss: 0.00695897, learning_rate: 8.000000000000001e-06, global_step: 200, interval_runtime: 0.8057, interval_samples_per_second: 9.929, interval_steps_per_second: 12.411, epoch: 10.0[0m
[32m[2022-09-08 12:02:20,515] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:02:20,516] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-08 12:02:20,516] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:02:20,516] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:02:20,516] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-08 12:02:21,272] [    INFO][0m - eval_loss: 0.44193214178085327, eval_accuracy: 0.9000000357627869, eval_runtime: 0.7555, eval_samples_per_second: 211.79, eval_steps_per_second: 26.474, epoch: 10.0[0m
[32m[2022-09-08 12:02:21,277] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-09-08 12:02:21,277] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:02:24,640] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-09-08 12:02:24,641] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-09-08 12:02:31,158] [    INFO][0m - loss: 0.00019317, learning_rate: 7.9e-06, global_step: 210, interval_runtime: 10.643, interval_samples_per_second: 0.752, interval_steps_per_second: 0.94, epoch: 10.5[0m
[32m[2022-09-08 12:02:31,963] [    INFO][0m - loss: 0.00023843, learning_rate: 7.800000000000002e-06, global_step: 220, interval_runtime: 0.805, interval_samples_per_second: 9.937, interval_steps_per_second: 12.422, epoch: 11.0[0m
[32m[2022-09-08 12:02:32,818] [    INFO][0m - loss: 0.00019897, learning_rate: 7.7e-06, global_step: 230, interval_runtime: 0.8553, interval_samples_per_second: 9.354, interval_steps_per_second: 11.692, epoch: 11.5[0m
[32m[2022-09-08 12:02:33,624] [    INFO][0m - loss: 0.00011103, learning_rate: 7.600000000000001e-06, global_step: 240, interval_runtime: 0.806, interval_samples_per_second: 9.926, interval_steps_per_second: 12.407, epoch: 12.0[0m
[32m[2022-09-08 12:02:34,476] [    INFO][0m - loss: 0.00012087, learning_rate: 7.500000000000001e-06, global_step: 250, interval_runtime: 0.8516, interval_samples_per_second: 9.394, interval_steps_per_second: 11.743, epoch: 12.5[0m
[32m[2022-09-08 12:02:35,282] [    INFO][0m - loss: 0.00019124, learning_rate: 7.4e-06, global_step: 260, interval_runtime: 0.8059, interval_samples_per_second: 9.926, interval_steps_per_second: 12.408, epoch: 13.0[0m
[32m[2022-09-08 12:02:36,132] [    INFO][0m - loss: 6.983e-05, learning_rate: 7.3e-06, global_step: 270, interval_runtime: 0.8503, interval_samples_per_second: 9.408, interval_steps_per_second: 11.761, epoch: 13.5[0m
[32m[2022-09-08 12:02:36,940] [    INFO][0m - loss: 0.00012364, learning_rate: 7.2000000000000005e-06, global_step: 280, interval_runtime: 0.8083, interval_samples_per_second: 9.898, interval_steps_per_second: 12.372, epoch: 14.0[0m
[32m[2022-09-08 12:02:37,805] [    INFO][0m - loss: 8.016e-05, learning_rate: 7.100000000000001e-06, global_step: 290, interval_runtime: 0.8649, interval_samples_per_second: 9.25, interval_steps_per_second: 11.562, epoch: 14.5[0m
[32m[2022-09-08 12:02:38,612] [    INFO][0m - loss: 0.0001003, learning_rate: 7e-06, global_step: 300, interval_runtime: 0.8061, interval_samples_per_second: 9.925, interval_steps_per_second: 12.406, epoch: 15.0[0m
[32m[2022-09-08 12:02:38,612] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:02:38,612] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-08 12:02:38,612] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:02:38,612] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:02:38,613] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-08 12:02:39,373] [    INFO][0m - eval_loss: 0.4872850775718689, eval_accuracy: 0.8812500238418579, eval_runtime: 0.7603, eval_samples_per_second: 210.452, eval_steps_per_second: 26.306, epoch: 15.0[0m
[32m[2022-09-08 12:02:39,381] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-09-08 12:02:39,381] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:02:42,551] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-09-08 12:02:42,551] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-09-08 12:02:48,608] [    INFO][0m - loss: 0.00027231, learning_rate: 6.9e-06, global_step: 310, interval_runtime: 9.9965, interval_samples_per_second: 0.8, interval_steps_per_second: 1.0, epoch: 15.5[0m
[32m[2022-09-08 12:02:49,416] [    INFO][0m - loss: 5.251e-05, learning_rate: 6.800000000000001e-06, global_step: 320, interval_runtime: 0.8078, interval_samples_per_second: 9.903, interval_steps_per_second: 12.379, epoch: 16.0[0m
[32m[2022-09-08 12:02:50,270] [    INFO][0m - loss: 5.901e-05, learning_rate: 6.700000000000001e-06, global_step: 330, interval_runtime: 0.8539, interval_samples_per_second: 9.369, interval_steps_per_second: 11.711, epoch: 16.5[0m
[32m[2022-09-08 12:02:51,079] [    INFO][0m - loss: 5.703e-05, learning_rate: 6.600000000000001e-06, global_step: 340, interval_runtime: 0.8095, interval_samples_per_second: 9.883, interval_steps_per_second: 12.353, epoch: 17.0[0m
[32m[2022-09-08 12:02:51,933] [    INFO][0m - loss: 0.00011231, learning_rate: 6.5000000000000004e-06, global_step: 350, interval_runtime: 0.8544, interval_samples_per_second: 9.363, interval_steps_per_second: 11.704, epoch: 17.5[0m
[32m[2022-09-08 12:02:52,739] [    INFO][0m - loss: 5.373e-05, learning_rate: 6.4000000000000006e-06, global_step: 360, interval_runtime: 0.806, interval_samples_per_second: 9.926, interval_steps_per_second: 12.407, epoch: 18.0[0m
[32m[2022-09-08 12:02:53,608] [    INFO][0m - loss: 4.478e-05, learning_rate: 6.300000000000001e-06, global_step: 370, interval_runtime: 0.8681, interval_samples_per_second: 9.215, interval_steps_per_second: 11.519, epoch: 18.5[0m
[32m[2022-09-08 12:02:54,421] [    INFO][0m - loss: 7.668e-05, learning_rate: 6.200000000000001e-06, global_step: 380, interval_runtime: 0.8067, interval_samples_per_second: 9.917, interval_steps_per_second: 12.397, epoch: 19.0[0m
[32m[2022-09-08 12:02:55,280] [    INFO][0m - loss: 6.901e-05, learning_rate: 6.1e-06, global_step: 390, interval_runtime: 0.8655, interval_samples_per_second: 9.243, interval_steps_per_second: 11.554, epoch: 19.5[0m
[32m[2022-09-08 12:02:56,087] [    INFO][0m - loss: 5.25e-05, learning_rate: 6e-06, global_step: 400, interval_runtime: 0.8069, interval_samples_per_second: 9.915, interval_steps_per_second: 12.393, epoch: 20.0[0m
[32m[2022-09-08 12:02:56,087] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:02:56,087] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-08 12:02:56,087] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:02:56,087] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:02:56,087] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-08 12:02:56,845] [    INFO][0m - eval_loss: 0.48799562454223633, eval_accuracy: 0.8812500238418579, eval_runtime: 0.758, eval_samples_per_second: 211.083, eval_steps_per_second: 26.385, epoch: 20.0[0m
[32m[2022-09-08 12:02:56,852] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-09-08 12:02:56,852] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:03:00,297] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-09-08 12:03:00,298] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-09-08 12:03:06,801] [    INFO][0m - loss: 0.0313637, learning_rate: 5.9e-06, global_step: 410, interval_runtime: 10.7146, interval_samples_per_second: 0.747, interval_steps_per_second: 0.933, epoch: 20.5[0m
[32m[2022-09-08 12:03:07,605] [    INFO][0m - loss: 6.569e-05, learning_rate: 5.8e-06, global_step: 420, interval_runtime: 0.8037, interval_samples_per_second: 9.954, interval_steps_per_second: 12.442, epoch: 21.0[0m
[32m[2022-09-08 12:03:08,460] [    INFO][0m - loss: 7.881e-05, learning_rate: 5.7e-06, global_step: 430, interval_runtime: 0.8557, interval_samples_per_second: 9.349, interval_steps_per_second: 11.687, epoch: 21.5[0m
[32m[2022-09-08 12:03:09,268] [    INFO][0m - loss: 3.74e-05, learning_rate: 5.600000000000001e-06, global_step: 440, interval_runtime: 0.8076, interval_samples_per_second: 9.906, interval_steps_per_second: 12.382, epoch: 22.0[0m
[32m[2022-09-08 12:03:10,129] [    INFO][0m - loss: 4.512e-05, learning_rate: 5.500000000000001e-06, global_step: 450, interval_runtime: 0.8606, interval_samples_per_second: 9.296, interval_steps_per_second: 11.62, epoch: 22.5[0m
[32m[2022-09-08 12:03:10,934] [    INFO][0m - loss: 3.792e-05, learning_rate: 5.400000000000001e-06, global_step: 460, interval_runtime: 0.8055, interval_samples_per_second: 9.932, interval_steps_per_second: 12.415, epoch: 23.0[0m
[32m[2022-09-08 12:03:11,791] [    INFO][0m - loss: 3.568e-05, learning_rate: 5.300000000000001e-06, global_step: 470, interval_runtime: 0.8567, interval_samples_per_second: 9.338, interval_steps_per_second: 11.673, epoch: 23.5[0m
[32m[2022-09-08 12:03:12,598] [    INFO][0m - loss: 3.661e-05, learning_rate: 5.2e-06, global_step: 480, interval_runtime: 0.8068, interval_samples_per_second: 9.915, interval_steps_per_second: 12.394, epoch: 24.0[0m
[32m[2022-09-08 12:03:13,454] [    INFO][0m - loss: 4.501e-05, learning_rate: 5.1e-06, global_step: 490, interval_runtime: 0.856, interval_samples_per_second: 9.345, interval_steps_per_second: 11.682, epoch: 24.5[0m
[32m[2022-09-08 12:03:14,259] [    INFO][0m - loss: 0.00023773, learning_rate: 5e-06, global_step: 500, interval_runtime: 0.8057, interval_samples_per_second: 9.929, interval_steps_per_second: 12.411, epoch: 25.0[0m
[32m[2022-09-08 12:03:14,260] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:03:14,260] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-08 12:03:14,260] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:03:14,260] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:03:14,260] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-08 12:03:15,022] [    INFO][0m - eval_loss: 0.47878751158714294, eval_accuracy: 0.8812500238418579, eval_runtime: 0.7618, eval_samples_per_second: 210.034, eval_steps_per_second: 26.254, epoch: 25.0[0m
[32m[2022-09-08 12:03:15,029] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-09-08 12:03:15,029] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:03:18,352] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-09-08 12:03:18,352] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-09-08 12:03:24,553] [    INFO][0m - loss: 3.682e-05, learning_rate: 4.9000000000000005e-06, global_step: 510, interval_runtime: 10.2935, interval_samples_per_second: 0.777, interval_steps_per_second: 0.971, epoch: 25.5[0m
[32m[2022-09-08 12:03:25,360] [    INFO][0m - loss: 5.121e-05, learning_rate: 4.800000000000001e-06, global_step: 520, interval_runtime: 0.8067, interval_samples_per_second: 9.916, interval_steps_per_second: 12.395, epoch: 26.0[0m
[32m[2022-09-08 12:03:26,215] [    INFO][0m - loss: 4.036e-05, learning_rate: 4.7e-06, global_step: 530, interval_runtime: 0.8551, interval_samples_per_second: 9.355, interval_steps_per_second: 11.694, epoch: 26.5[0m
[32m[2022-09-08 12:03:27,030] [    INFO][0m - loss: 3.118e-05, learning_rate: 4.600000000000001e-06, global_step: 540, interval_runtime: 0.8149, interval_samples_per_second: 9.817, interval_steps_per_second: 12.272, epoch: 27.0[0m
[32m[2022-09-08 12:03:27,885] [    INFO][0m - loss: 4.353e-05, learning_rate: 4.5e-06, global_step: 550, interval_runtime: 0.8555, interval_samples_per_second: 9.352, interval_steps_per_second: 11.689, epoch: 27.5[0m
[32m[2022-09-08 12:03:28,695] [    INFO][0m - loss: 3.579e-05, learning_rate: 4.4e-06, global_step: 560, interval_runtime: 0.8095, interval_samples_per_second: 9.882, interval_steps_per_second: 12.353, epoch: 28.0[0m
[32m[2022-09-08 12:03:29,553] [    INFO][0m - loss: 3.892e-05, learning_rate: 4.3e-06, global_step: 570, interval_runtime: 0.8581, interval_samples_per_second: 9.323, interval_steps_per_second: 11.653, epoch: 28.5[0m
[32m[2022-09-08 12:03:30,363] [    INFO][0m - loss: 2.8e-05, learning_rate: 4.2000000000000004e-06, global_step: 580, interval_runtime: 0.8102, interval_samples_per_second: 9.875, interval_steps_per_second: 12.343, epoch: 29.0[0m
[32m[2022-09-08 12:03:31,216] [    INFO][0m - loss: 3.236e-05, learning_rate: 4.1e-06, global_step: 590, interval_runtime: 0.8533, interval_samples_per_second: 9.376, interval_steps_per_second: 11.72, epoch: 29.5[0m
[32m[2022-09-08 12:03:32,025] [    INFO][0m - loss: 2.624e-05, learning_rate: 4.000000000000001e-06, global_step: 600, interval_runtime: 0.8091, interval_samples_per_second: 9.888, interval_steps_per_second: 12.36, epoch: 30.0[0m
[32m[2022-09-08 12:03:32,026] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:03:32,026] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-08 12:03:32,026] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:03:32,026] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:03:32,026] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-08 12:03:32,796] [    INFO][0m - eval_loss: 0.503095269203186, eval_accuracy: 0.875, eval_runtime: 0.7695, eval_samples_per_second: 207.915, eval_steps_per_second: 25.989, epoch: 30.0[0m
[32m[2022-09-08 12:03:32,803] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-09-08 12:03:32,803] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:03:36,322] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-09-08 12:03:36,323] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-09-08 12:03:41,861] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-08 12:03:41,861] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-200 (score: 0.9000000357627869).[0m
[32m[2022-09-08 12:03:42,766] [    INFO][0m - train_runtime: 109.9681, train_samples_per_second: 72.748, train_steps_per_second: 9.094, train_loss: 0.280878368717822, epoch: 30.0[0m
[32m[2022-09-08 12:03:42,808] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-09-08 12:03:42,808] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:03:47,477] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-09-08 12:03:47,478] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-09-08 12:03:47,479] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-08 12:03:47,479] [    INFO][0m -   epoch                    =       30.0[0m
[32m[2022-09-08 12:03:47,479] [    INFO][0m -   train_loss               =     0.2809[0m
[32m[2022-09-08 12:03:47,479] [    INFO][0m -   train_runtime            = 0:01:49.96[0m
[32m[2022-09-08 12:03:47,479] [    INFO][0m -   train_samples_per_second =     72.748[0m
[32m[2022-09-08 12:03:47,479] [    INFO][0m -   train_steps_per_second   =      9.094[0m
[32m[2022-09-08 12:03:47,483] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-08 12:03:47,483] [    INFO][0m -   Num examples = 610[0m
[32m[2022-09-08 12:03:47,483] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:03:47,483] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:03:47,483] [    INFO][0m -   Total prediction steps = 77[0m
[32m[2022-09-08 12:03:50,473] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-08 12:03:50,474] [    INFO][0m -   test_accuracy           =     0.8098[0m
[32m[2022-09-08 12:03:50,474] [    INFO][0m -   test_loss               =     6.3884[0m
[32m[2022-09-08 12:03:50,474] [    INFO][0m -   test_runtime            = 0:00:02.99[0m
[32m[2022-09-08 12:03:50,474] [    INFO][0m -   test_samples_per_second =    203.964[0m
[32m[2022-09-08 12:03:50,474] [    INFO][0m -   test_steps_per_second   =     25.746[0m
[32m[2022-09-08 12:03:50,474] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-08 12:03:50,474] [    INFO][0m -   Num examples = 753[0m
[32m[2022-09-08 12:03:50,474] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:03:50,475] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:03:50,475] [    INFO][0m -   Total prediction steps = 95[0m
terminate called after throwing an instance of 'paddle::memory::allocation::BadAlloc'
  what():  

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   full_dygraph_function(paddle::experimental::IntArrayBase<paddle::experimental::Tensor>, paddle::experimental::ScalarBase<paddle::experimental::Tensor>, paddle::experimental::DataType, phi::Place)
1   paddle::experimental::full(paddle::experimental::IntArrayBase<paddle::experimental::Tensor> const&, paddle::experimental::ScalarBase<paddle::experimental::Tensor> const&, paddle::experimental::DataType, phi::Place const&)
2   void phi::FullKernel<float, phi::GPUContext>(phi::GPUContext const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, paddle::experimental::ScalarBase<phi::DenseTensor> const&, paddle::experimental::DataType, phi::DenseTensor*)
3   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
4   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, paddle::experimental::DataType, unsigned long, bool) const
5   phi::DenseTensor::AllocateFrom(phi::Allocator*, paddle::experimental::DataType, unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  phi::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 4.388858TB memory on GPU 0, 3.674561GB memory has been allocated and available memory is only 28.073975GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
If the above ways do not solve the out of memory problem, you can try to use CUDA managed memory. The command is `export FLAGS_use_cuda_managed_memory=false`.
 (at /paddle/paddle/fluid/memory/allocation/cuda_allocator.cc:95)

run.sh: line 53: 79368 Aborted                 CUDA_VISIBLE_DEVICES=$device python train_single.py --output_dir ./checkpoints/ --prompt "$prompt" --max_seq_length $max_length --per_device_eval_batch_size $batch_size --per_device_train_batch_size $batch_size --model_name_or_path ernie-3.0-base-zh --split_id few_all --task_name $task_name --metric_for_best_model accuracy --disable_tqdm True --do_test --eval_steps 100 --save_steps 100 --num_train_epochs 50 --logging_steps 10 --learning_rate 1e-5 --ppt_learning_rate 1e-4 --load_best_model_at_end $is_train --do_train $is_train --do_eval $is_train --do_save $is_train --do_predict $is_train
run.sh: line 59: --freeze_plm: command not found
 
==========
csldcp
==========
 
[33m[2022-09-08 12:04:07,152] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-09-08 12:04:07,152] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-08 12:04:07,152] [    INFO][0m - ============================================================[0m
[32m[2022-09-08 12:04:07,152] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-08 12:04:07,152] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-08 12:04:07,152] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-08 12:04:07,153] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-08 12:04:07,153] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-08 12:04:07,153] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-08 12:04:07,153] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-09-08 12:04:07,153] [    INFO][0m - [0m
[32m[2022-09-08 12:04:07,153] [    INFO][0m - ============================================================[0m
[32m[2022-09-08 12:04:07,153] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-08 12:04:07,153] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-08 12:04:07,153] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-08 12:04:07,153] [    INFO][0m - prompt                        :[0m
[32m[2022-09-08 12:04:07,153] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-08 12:04:07,153] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-08 12:04:07,153] [    INFO][0m - task_name                     :csldcp[0m
[32m[2022-09-08 12:04:07,154] [    INFO][0m - [0m
[32m[2022-09-08 12:04:07,154] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0908 12:04:07.155249  3979 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0908 12:04:07.159271  3979 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-08 12:04:09,959] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-09-08 12:04:09,984] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-09-08 12:04:09,984] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[32m[2022-09-08 12:04:10,967] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùËøôÁØáÊñáÁåÆÁöÑÁ±ªÂà´ÊòØ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': '„ÄÇ'}][0m
[32m[2022-09-08 12:04:11,145] [    INFO][0m - ============================================================[0m
[32m[2022-09-08 12:04:11,145] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-08 12:04:11,145] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-08 12:04:11,145] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-08 12:04:11,145] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-08 12:04:11,145] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-08 12:04:11,146] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-08 12:04:11,146] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-08 12:04:11,146] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-08 12:04:11,146] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-08 12:04:11,146] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-08 12:04:11,146] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-08 12:04:11,146] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-08 12:04:11,146] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-08 12:04:11,146] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-08 12:04:11,146] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-08 12:04:11,146] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-08 12:04:11,146] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-08 12:04:11,146] [    INFO][0m - eval_batch_size               :8[0m
[32m[2022-09-08 12:04:11,147] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-09-08 12:04:11,147] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-09-08 12:04:11,147] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-08 12:04:11,147] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-08 12:04:11,147] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-08 12:04:11,147] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-08 12:04:11,147] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-08 12:04:11,147] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-08 12:04:11,147] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-08 12:04:11,147] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-08 12:04:11,147] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-08 12:04:11,147] [    INFO][0m - learning_rate                 :1e-05[0m
[32m[2022-09-08 12:04:11,147] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-08 12:04:11,147] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-08 12:04:11,148] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-08 12:04:11,148] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-08 12:04:11,148] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-08 12:04:11,148] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-08 12:04:11,148] [    INFO][0m - logging_dir                   :./checkpoints/runs/Sep08_12-04-07_instance-3bwob41y-01[0m
[32m[2022-09-08 12:04:11,148] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-08 12:04:11,148] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-08 12:04:11,148] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-08 12:04:11,148] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-08 12:04:11,148] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-08 12:04:11,148] [    INFO][0m - max_seq_length                :256[0m
[32m[2022-09-08 12:04:11,148] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-08 12:04:11,148] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-08 12:04:11,149] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-08 12:04:11,149] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-08 12:04:11,149] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-09-08 12:04:11,149] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-08 12:04:11,149] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-08 12:04:11,149] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-09-08 12:04:11,149] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-08 12:04:11,149] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-08 12:04:11,149] [    INFO][0m - per_device_eval_batch_size    :8[0m
[32m[2022-09-08 12:04:11,149] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-09-08 12:04:11,149] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-08 12:04:11,149] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-08 12:04:11,149] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-08 12:04:11,149] [    INFO][0m - ppt_learning_rate             :0.0001[0m
[32m[2022-09-08 12:04:11,150] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-08 12:04:11,150] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-08 12:04:11,150] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-08 12:04:11,150] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-08 12:04:11,150] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-08 12:04:11,150] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-08 12:04:11,150] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-09-08 12:04:11,150] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-08 12:04:11,150] [    INFO][0m - save_steps                    :100[0m
[32m[2022-09-08 12:04:11,150] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-09-08 12:04:11,150] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-08 12:04:11,150] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-08 12:04:11,150] [    INFO][0m - seed                          :42[0m
[32m[2022-09-08 12:04:11,150] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-08 12:04:11,151] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-08 12:04:11,151] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-08 12:04:11,151] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-09-08 12:04:11,151] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-08 12:04:11,151] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-08 12:04:11,151] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-08 12:04:11,151] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-08 12:04:11,151] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-08 12:04:11,151] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-08 12:04:11,151] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-08 12:04:11,151] [    INFO][0m - [0m
[32m[2022-09-08 12:04:11,153] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-08 12:04:11,153] [    INFO][0m -   Num examples = 2036[0m
[32m[2022-09-08 12:04:11,153] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-09-08 12:04:11,153] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-09-08 12:04:11,153] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-09-08 12:04:11,153] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-08 12:04:11,153] [    INFO][0m -   Total optimization steps = 12750.0[0m
[32m[2022-09-08 12:04:11,153] [    INFO][0m -   Total num train samples = 101800[0m
[32m[2022-09-08 12:04:13,800] [    INFO][0m - loss: 21.79496002, learning_rate: 9.992156862745099e-06, global_step: 10, interval_runtime: 2.6456, interval_samples_per_second: 3.024, interval_steps_per_second: 3.78, epoch: 0.0392[0m
[32m[2022-09-08 12:04:15,350] [    INFO][0m - loss: 10.82360535, learning_rate: 9.984313725490197e-06, global_step: 20, interval_runtime: 1.5502, interval_samples_per_second: 5.161, interval_steps_per_second: 6.451, epoch: 0.0784[0m
[32m[2022-09-08 12:04:16,890] [    INFO][0m - loss: 6.63358536, learning_rate: 9.976470588235294e-06, global_step: 30, interval_runtime: 1.5401, interval_samples_per_second: 5.195, interval_steps_per_second: 6.493, epoch: 0.1176[0m
[32m[2022-09-08 12:04:18,427] [    INFO][0m - loss: 5.88919563, learning_rate: 9.968627450980392e-06, global_step: 40, interval_runtime: 1.5365, interval_samples_per_second: 5.207, interval_steps_per_second: 6.508, epoch: 0.1569[0m
[32m[2022-09-08 12:04:19,965] [    INFO][0m - loss: 5.42993431, learning_rate: 9.960784313725492e-06, global_step: 50, interval_runtime: 1.5383, interval_samples_per_second: 5.201, interval_steps_per_second: 6.501, epoch: 0.1961[0m
[32m[2022-09-08 12:04:21,522] [    INFO][0m - loss: 5.24470253, learning_rate: 9.952941176470588e-06, global_step: 60, interval_runtime: 1.5572, interval_samples_per_second: 5.137, interval_steps_per_second: 6.422, epoch: 0.2353[0m
[32m[2022-09-08 12:04:23,072] [    INFO][0m - loss: 4.9583271, learning_rate: 9.945098039215688e-06, global_step: 70, interval_runtime: 1.5497, interval_samples_per_second: 5.162, interval_steps_per_second: 6.453, epoch: 0.2745[0m
[32m[2022-09-08 12:04:24,617] [    INFO][0m - loss: 4.82479095, learning_rate: 9.937254901960786e-06, global_step: 80, interval_runtime: 1.5452, interval_samples_per_second: 5.177, interval_steps_per_second: 6.472, epoch: 0.3137[0m
[32m[2022-09-08 12:04:26,180] [    INFO][0m - loss: 4.5327877, learning_rate: 9.929411764705883e-06, global_step: 90, interval_runtime: 1.563, interval_samples_per_second: 5.118, interval_steps_per_second: 6.398, epoch: 0.3529[0m
[32m[2022-09-08 12:04:27,726] [    INFO][0m - loss: 3.87574463, learning_rate: 9.921568627450981e-06, global_step: 100, interval_runtime: 1.546, interval_samples_per_second: 5.175, interval_steps_per_second: 6.468, epoch: 0.3922[0m
[32m[2022-09-08 12:04:27,726] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:04:27,727] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-08 12:04:27,727] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:04:27,727] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:04:27,727] [    INFO][0m -   Total prediction steps = 259[0m
[32m[2022-09-08 12:04:46,407] [    INFO][0m - eval_loss: 3.675696611404419, eval_accuracy: 0.1663442999124527, eval_runtime: 18.6793, eval_samples_per_second: 110.711, eval_steps_per_second: 13.866, epoch: 0.3922[0m
[32m[2022-09-08 12:04:46,495] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-09-08 12:04:46,495] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:04:49,646] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-08 12:04:49,647] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-08 12:04:56,581] [    INFO][0m - loss: 4.00935936, learning_rate: 9.91372549019608e-06, global_step: 110, interval_runtime: 28.8543, interval_samples_per_second: 0.277, interval_steps_per_second: 0.347, epoch: 0.4314[0m
[32m[2022-09-08 12:04:58,119] [    INFO][0m - loss: 3.38990593, learning_rate: 9.905882352941177e-06, global_step: 120, interval_runtime: 1.539, interval_samples_per_second: 5.198, interval_steps_per_second: 6.498, epoch: 0.4706[0m
[32m[2022-09-08 12:04:59,657] [    INFO][0m - loss: 3.23498535, learning_rate: 9.898039215686275e-06, global_step: 130, interval_runtime: 1.5383, interval_samples_per_second: 5.201, interval_steps_per_second: 6.501, epoch: 0.5098[0m
[32m[2022-09-08 12:05:01,213] [    INFO][0m - loss: 3.0282856, learning_rate: 9.890196078431373e-06, global_step: 140, interval_runtime: 1.5558, interval_samples_per_second: 5.142, interval_steps_per_second: 6.428, epoch: 0.549[0m
[32m[2022-09-08 12:05:02,783] [    INFO][0m - loss: 2.81640491, learning_rate: 9.882352941176472e-06, global_step: 150, interval_runtime: 1.5697, interval_samples_per_second: 5.096, interval_steps_per_second: 6.37, epoch: 0.5882[0m
[32m[2022-09-08 12:05:04,314] [    INFO][0m - loss: 2.76012383, learning_rate: 9.874509803921569e-06, global_step: 160, interval_runtime: 1.5307, interval_samples_per_second: 5.226, interval_steps_per_second: 6.533, epoch: 0.6275[0m
[32m[2022-09-08 12:05:05,856] [    INFO][0m - loss: 2.6839098, learning_rate: 9.866666666666668e-06, global_step: 170, interval_runtime: 1.5417, interval_samples_per_second: 5.189, interval_steps_per_second: 6.486, epoch: 0.6667[0m
[32m[2022-09-08 12:05:07,390] [    INFO][0m - loss: 2.56837006, learning_rate: 9.858823529411764e-06, global_step: 180, interval_runtime: 1.5351, interval_samples_per_second: 5.211, interval_steps_per_second: 6.514, epoch: 0.7059[0m
[32m[2022-09-08 12:05:08,930] [    INFO][0m - loss: 2.40608921, learning_rate: 9.850980392156864e-06, global_step: 190, interval_runtime: 1.5398, interval_samples_per_second: 5.196, interval_steps_per_second: 6.494, epoch: 0.7451[0m
[32m[2022-09-08 12:05:10,469] [    INFO][0m - loss: 2.8354126, learning_rate: 9.843137254901962e-06, global_step: 200, interval_runtime: 1.5392, interval_samples_per_second: 5.197, interval_steps_per_second: 6.497, epoch: 0.7843[0m
[32m[2022-09-08 12:05:10,470] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:05:10,470] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-08 12:05:10,470] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:05:10,470] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:05:10,470] [    INFO][0m -   Total prediction steps = 259[0m
[32m[2022-09-08 12:05:28,733] [    INFO][0m - eval_loss: 2.1170310974121094, eval_accuracy: 0.47243714332580566, eval_runtime: 18.2623, eval_samples_per_second: 113.239, eval_steps_per_second: 14.182, epoch: 0.7843[0m
[32m[2022-09-08 12:05:28,809] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-09-08 12:05:28,809] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:05:32,182] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-09-08 12:05:32,183] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-09-08 12:05:39,226] [    INFO][0m - loss: 2.29129848, learning_rate: 9.83529411764706e-06, global_step: 210, interval_runtime: 28.7562, interval_samples_per_second: 0.278, interval_steps_per_second: 0.348, epoch: 0.8235[0m
[32m[2022-09-08 12:05:40,772] [    INFO][0m - loss: 2.14663658, learning_rate: 9.827450980392158e-06, global_step: 220, interval_runtime: 1.5464, interval_samples_per_second: 5.173, interval_steps_per_second: 6.466, epoch: 0.8627[0m
[32m[2022-09-08 12:05:42,302] [    INFO][0m - loss: 2.22534904, learning_rate: 9.819607843137255e-06, global_step: 230, interval_runtime: 1.5289, interval_samples_per_second: 5.233, interval_steps_per_second: 6.541, epoch: 0.902[0m
[32m[2022-09-08 12:05:43,852] [    INFO][0m - loss: 2.64409485, learning_rate: 9.811764705882353e-06, global_step: 240, interval_runtime: 1.5507, interval_samples_per_second: 5.159, interval_steps_per_second: 6.449, epoch: 0.9412[0m
[32m[2022-09-08 12:05:45,390] [    INFO][0m - loss: 2.26982918, learning_rate: 9.803921568627451e-06, global_step: 250, interval_runtime: 1.538, interval_samples_per_second: 5.202, interval_steps_per_second: 6.502, epoch: 0.9804[0m
[32m[2022-09-08 12:05:46,915] [    INFO][0m - loss: 2.13301907, learning_rate: 9.796078431372549e-06, global_step: 260, interval_runtime: 1.5258, interval_samples_per_second: 5.243, interval_steps_per_second: 6.554, epoch: 1.0196[0m
[32m[2022-09-08 12:05:48,463] [    INFO][0m - loss: 1.77019157, learning_rate: 9.788235294117649e-06, global_step: 270, interval_runtime: 1.5473, interval_samples_per_second: 5.17, interval_steps_per_second: 6.463, epoch: 1.0588[0m
[32m[2022-09-08 12:05:49,998] [    INFO][0m - loss: 1.78302231, learning_rate: 9.780392156862747e-06, global_step: 280, interval_runtime: 1.5352, interval_samples_per_second: 5.211, interval_steps_per_second: 6.514, epoch: 1.098[0m
[32m[2022-09-08 12:05:51,555] [    INFO][0m - loss: 2.01179008, learning_rate: 9.772549019607844e-06, global_step: 290, interval_runtime: 1.5567, interval_samples_per_second: 5.139, interval_steps_per_second: 6.424, epoch: 1.1373[0m
[32m[2022-09-08 12:05:53,113] [    INFO][0m - loss: 1.88533344, learning_rate: 9.764705882352942e-06, global_step: 300, interval_runtime: 1.558, interval_samples_per_second: 5.135, interval_steps_per_second: 6.418, epoch: 1.1765[0m
[32m[2022-09-08 12:05:53,113] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:05:53,113] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-08 12:05:53,113] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:05:53,114] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:05:53,114] [    INFO][0m -   Total prediction steps = 259[0m
[32m[2022-09-08 12:06:11,391] [    INFO][0m - eval_loss: 1.8452377319335938, eval_accuracy: 0.4883945882320404, eval_runtime: 18.2767, eval_samples_per_second: 113.15, eval_steps_per_second: 14.171, epoch: 1.1765[0m
[32m[2022-09-08 12:06:11,467] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-09-08 12:06:11,468] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:06:14,916] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-09-08 12:06:14,916] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-09-08 12:06:22,043] [    INFO][0m - loss: 1.68501587, learning_rate: 9.75686274509804e-06, global_step: 310, interval_runtime: 28.9297, interval_samples_per_second: 0.277, interval_steps_per_second: 0.346, epoch: 1.2157[0m
[32m[2022-09-08 12:06:23,591] [    INFO][0m - loss: 1.77201099, learning_rate: 9.749019607843138e-06, global_step: 320, interval_runtime: 1.5482, interval_samples_per_second: 5.167, interval_steps_per_second: 6.459, epoch: 1.2549[0m
[32m[2022-09-08 12:06:25,141] [    INFO][0m - loss: 1.75891724, learning_rate: 9.741176470588236e-06, global_step: 330, interval_runtime: 1.5496, interval_samples_per_second: 5.162, interval_steps_per_second: 6.453, epoch: 1.2941[0m
[32m[2022-09-08 12:06:26,702] [    INFO][0m - loss: 2.04864178, learning_rate: 9.733333333333334e-06, global_step: 340, interval_runtime: 1.5617, interval_samples_per_second: 5.123, interval_steps_per_second: 6.403, epoch: 1.3333[0m
[32m[2022-09-08 12:06:28,266] [    INFO][0m - loss: 2.10231781, learning_rate: 9.725490196078432e-06, global_step: 350, interval_runtime: 1.5641, interval_samples_per_second: 5.115, interval_steps_per_second: 6.393, epoch: 1.3725[0m
[32m[2022-09-08 12:06:29,824] [    INFO][0m - loss: 1.83694267, learning_rate: 9.717647058823531e-06, global_step: 360, interval_runtime: 1.5582, interval_samples_per_second: 5.134, interval_steps_per_second: 6.418, epoch: 1.4118[0m
[32m[2022-09-08 12:06:31,370] [    INFO][0m - loss: 1.83674774, learning_rate: 9.709803921568628e-06, global_step: 370, interval_runtime: 1.5452, interval_samples_per_second: 5.177, interval_steps_per_second: 6.471, epoch: 1.451[0m
[32m[2022-09-08 12:06:32,916] [    INFO][0m - loss: 1.63389301, learning_rate: 9.701960784313727e-06, global_step: 380, interval_runtime: 1.546, interval_samples_per_second: 5.174, interval_steps_per_second: 6.468, epoch: 1.4902[0m
[32m[2022-09-08 12:06:34,465] [    INFO][0m - loss: 1.82291489, learning_rate: 9.694117647058823e-06, global_step: 390, interval_runtime: 1.549, interval_samples_per_second: 5.165, interval_steps_per_second: 6.456, epoch: 1.5294[0m
[32m[2022-09-08 12:06:36,011] [    INFO][0m - loss: 1.84280281, learning_rate: 9.686274509803923e-06, global_step: 400, interval_runtime: 1.5461, interval_samples_per_second: 5.174, interval_steps_per_second: 6.468, epoch: 1.5686[0m
[32m[2022-09-08 12:06:36,011] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:06:36,011] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-08 12:06:36,012] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:06:36,012] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:06:36,012] [    INFO][0m -   Total prediction steps = 259[0m
[32m[2022-09-08 12:06:54,132] [    INFO][0m - eval_loss: 1.6733037233352661, eval_accuracy: 0.5314313769340515, eval_runtime: 18.1197, eval_samples_per_second: 114.13, eval_steps_per_second: 14.294, epoch: 1.5686[0m
[32m[2022-09-08 12:06:54,193] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-09-08 12:06:54,193] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:06:57,251] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-09-08 12:06:57,252] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-09-08 12:07:03,888] [    INFO][0m - loss: 1.67936249, learning_rate: 9.67843137254902e-06, global_step: 410, interval_runtime: 27.8769, interval_samples_per_second: 0.287, interval_steps_per_second: 0.359, epoch: 1.6078[0m
[32m[2022-09-08 12:07:05,420] [    INFO][0m - loss: 1.96010761, learning_rate: 9.670588235294119e-06, global_step: 420, interval_runtime: 1.5317, interval_samples_per_second: 5.223, interval_steps_per_second: 6.529, epoch: 1.6471[0m
[32m[2022-09-08 12:07:06,969] [    INFO][0m - loss: 1.694981, learning_rate: 9.662745098039217e-06, global_step: 430, interval_runtime: 1.5498, interval_samples_per_second: 5.162, interval_steps_per_second: 6.452, epoch: 1.6863[0m
[32m[2022-09-08 12:07:08,524] [    INFO][0m - loss: 1.68014164, learning_rate: 9.654901960784314e-06, global_step: 440, interval_runtime: 1.555, interval_samples_per_second: 5.145, interval_steps_per_second: 6.431, epoch: 1.7255[0m
[32m[2022-09-08 12:07:10,066] [    INFO][0m - loss: 1.64642067, learning_rate: 9.647058823529412e-06, global_step: 450, interval_runtime: 1.541, interval_samples_per_second: 5.192, interval_steps_per_second: 6.489, epoch: 1.7647[0m
[32m[2022-09-08 12:07:11,600] [    INFO][0m - loss: 1.84415245, learning_rate: 9.63921568627451e-06, global_step: 460, interval_runtime: 1.535, interval_samples_per_second: 5.212, interval_steps_per_second: 6.515, epoch: 1.8039[0m
[32m[2022-09-08 12:07:13,155] [    INFO][0m - loss: 1.6257513, learning_rate: 9.631372549019608e-06, global_step: 470, interval_runtime: 1.5545, interval_samples_per_second: 5.146, interval_steps_per_second: 6.433, epoch: 1.8431[0m
[32m[2022-09-08 12:07:14,699] [    INFO][0m - loss: 1.72008171, learning_rate: 9.623529411764708e-06, global_step: 480, interval_runtime: 1.544, interval_samples_per_second: 5.181, interval_steps_per_second: 6.477, epoch: 1.8824[0m
[32m[2022-09-08 12:07:16,255] [    INFO][0m - loss: 1.54325857, learning_rate: 9.615686274509804e-06, global_step: 490, interval_runtime: 1.5562, interval_samples_per_second: 5.141, interval_steps_per_second: 6.426, epoch: 1.9216[0m
[32m[2022-09-08 12:07:17,804] [    INFO][0m - loss: 1.92617512, learning_rate: 9.607843137254903e-06, global_step: 500, interval_runtime: 1.5495, interval_samples_per_second: 5.163, interval_steps_per_second: 6.454, epoch: 1.9608[0m
[32m[2022-09-08 12:07:17,805] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:07:17,805] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-08 12:07:17,805] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:07:17,805] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:07:17,805] [    INFO][0m -   Total prediction steps = 259[0m
[32m[2022-09-08 12:07:35,972] [    INFO][0m - eval_loss: 1.5519332885742188, eval_accuracy: 0.5570600032806396, eval_runtime: 18.1665, eval_samples_per_second: 113.836, eval_steps_per_second: 14.257, epoch: 1.9608[0m
[32m[2022-09-08 12:07:36,030] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-09-08 12:07:36,030] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:07:38,893] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-09-08 12:07:38,893] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-09-08 12:07:45,389] [    INFO][0m - loss: 1.39964981, learning_rate: 9.600000000000001e-06, global_step: 510, interval_runtime: 27.5843, interval_samples_per_second: 0.29, interval_steps_per_second: 0.363, epoch: 2.0[0m
[32m[2022-09-08 12:07:47,033] [    INFO][0m - loss: 1.32171621, learning_rate: 9.592156862745099e-06, global_step: 520, interval_runtime: 1.6441, interval_samples_per_second: 4.866, interval_steps_per_second: 6.082, epoch: 2.0392[0m
[32m[2022-09-08 12:07:48,577] [    INFO][0m - loss: 1.28362398, learning_rate: 9.584313725490197e-06, global_step: 530, interval_runtime: 1.5446, interval_samples_per_second: 5.179, interval_steps_per_second: 6.474, epoch: 2.0784[0m
[32m[2022-09-08 12:07:50,130] [    INFO][0m - loss: 1.33556271, learning_rate: 9.576470588235295e-06, global_step: 540, interval_runtime: 1.5524, interval_samples_per_second: 5.153, interval_steps_per_second: 6.442, epoch: 2.1176[0m
[32m[2022-09-08 12:07:51,675] [    INFO][0m - loss: 1.36867523, learning_rate: 9.568627450980393e-06, global_step: 550, interval_runtime: 1.5453, interval_samples_per_second: 5.177, interval_steps_per_second: 6.471, epoch: 2.1569[0m
[32m[2022-09-08 12:07:53,215] [    INFO][0m - loss: 1.33243361, learning_rate: 9.56078431372549e-06, global_step: 560, interval_runtime: 1.5399, interval_samples_per_second: 5.195, interval_steps_per_second: 6.494, epoch: 2.1961[0m
[32m[2022-09-08 12:07:54,762] [    INFO][0m - loss: 1.16110373, learning_rate: 9.552941176470589e-06, global_step: 570, interval_runtime: 1.5467, interval_samples_per_second: 5.172, interval_steps_per_second: 6.465, epoch: 2.2353[0m
[32m[2022-09-08 12:07:56,322] [    INFO][0m - loss: 1.36822186, learning_rate: 9.545098039215686e-06, global_step: 580, interval_runtime: 1.5602, interval_samples_per_second: 5.127, interval_steps_per_second: 6.409, epoch: 2.2745[0m
[32m[2022-09-08 12:07:57,874] [    INFO][0m - loss: 1.68020535, learning_rate: 9.537254901960786e-06, global_step: 590, interval_runtime: 1.5517, interval_samples_per_second: 5.156, interval_steps_per_second: 6.445, epoch: 2.3137[0m
[32m[2022-09-08 12:07:59,421] [    INFO][0m - loss: 1.3525486, learning_rate: 9.529411764705882e-06, global_step: 600, interval_runtime: 1.5471, interval_samples_per_second: 5.171, interval_steps_per_second: 6.464, epoch: 2.3529[0m
[32m[2022-09-08 12:07:59,421] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:07:59,421] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-08 12:07:59,422] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:07:59,422] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:07:59,422] [    INFO][0m -   Total prediction steps = 259[0m
[32m[2022-09-08 12:08:17,824] [    INFO][0m - eval_loss: 1.5037637948989868, eval_accuracy: 0.5909091234207153, eval_runtime: 18.4016, eval_samples_per_second: 112.382, eval_steps_per_second: 14.075, epoch: 2.3529[0m
[32m[2022-09-08 12:08:17,908] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-09-08 12:08:17,908] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:08:21,111] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-09-08 12:08:21,112] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-09-08 12:08:27,946] [    INFO][0m - loss: 1.43563929, learning_rate: 9.521568627450982e-06, global_step: 610, interval_runtime: 28.525, interval_samples_per_second: 0.28, interval_steps_per_second: 0.351, epoch: 2.3922[0m
[32m[2022-09-08 12:08:29,501] [    INFO][0m - loss: 1.30560207, learning_rate: 9.51372549019608e-06, global_step: 620, interval_runtime: 1.5546, interval_samples_per_second: 5.146, interval_steps_per_second: 6.433, epoch: 2.4314[0m
[32m[2022-09-08 12:08:31,047] [    INFO][0m - loss: 1.31328678, learning_rate: 9.505882352941178e-06, global_step: 630, interval_runtime: 1.5462, interval_samples_per_second: 5.174, interval_steps_per_second: 6.467, epoch: 2.4706[0m
[32m[2022-09-08 12:08:32,600] [    INFO][0m - loss: 1.12713223, learning_rate: 9.498039215686275e-06, global_step: 640, interval_runtime: 1.5528, interval_samples_per_second: 5.152, interval_steps_per_second: 6.44, epoch: 2.5098[0m
[32m[2022-09-08 12:08:34,136] [    INFO][0m - loss: 1.21085815, learning_rate: 9.490196078431373e-06, global_step: 650, interval_runtime: 1.5368, interval_samples_per_second: 5.206, interval_steps_per_second: 6.507, epoch: 2.549[0m
[32m[2022-09-08 12:08:35,686] [    INFO][0m - loss: 1.17533331, learning_rate: 9.482352941176471e-06, global_step: 660, interval_runtime: 1.55, interval_samples_per_second: 5.161, interval_steps_per_second: 6.452, epoch: 2.5882[0m
[32m[2022-09-08 12:08:37,228] [    INFO][0m - loss: 1.43368998, learning_rate: 9.47450980392157e-06, global_step: 670, interval_runtime: 1.5418, interval_samples_per_second: 5.189, interval_steps_per_second: 6.486, epoch: 2.6275[0m
[32m[2022-09-08 12:08:38,789] [    INFO][0m - loss: 1.2978632, learning_rate: 9.466666666666667e-06, global_step: 680, interval_runtime: 1.561, interval_samples_per_second: 5.125, interval_steps_per_second: 6.406, epoch: 2.6667[0m
[32m[2022-09-08 12:08:40,350] [    INFO][0m - loss: 1.20649681, learning_rate: 9.458823529411767e-06, global_step: 690, interval_runtime: 1.5604, interval_samples_per_second: 5.127, interval_steps_per_second: 6.409, epoch: 2.7059[0m
[32m[2022-09-08 12:08:41,908] [    INFO][0m - loss: 1.29337521, learning_rate: 9.450980392156863e-06, global_step: 700, interval_runtime: 1.5585, interval_samples_per_second: 5.133, interval_steps_per_second: 6.417, epoch: 2.7451[0m
[32m[2022-09-08 12:08:41,908] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:08:41,909] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-08 12:08:41,909] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:08:41,909] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:08:41,909] [    INFO][0m -   Total prediction steps = 259[0m
[32m[2022-09-08 12:09:00,103] [    INFO][0m - eval_loss: 1.4633052349090576, eval_accuracy: 0.5991296172142029, eval_runtime: 18.1937, eval_samples_per_second: 113.666, eval_steps_per_second: 14.236, epoch: 2.7451[0m
[32m[2022-09-08 12:09:00,163] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-700[0m
[32m[2022-09-08 12:09:00,163] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:09:03,286] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-700/tokenizer_config.json[0m
[32m[2022-09-08 12:09:03,287] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-700/special_tokens_map.json[0m
[32m[2022-09-08 12:09:09,862] [    INFO][0m - loss: 1.2913188, learning_rate: 9.443137254901962e-06, global_step: 710, interval_runtime: 27.9538, interval_samples_per_second: 0.286, interval_steps_per_second: 0.358, epoch: 2.7843[0m
[32m[2022-09-08 12:09:11,402] [    INFO][0m - loss: 1.42639065, learning_rate: 9.435294117647058e-06, global_step: 720, interval_runtime: 1.54, interval_samples_per_second: 5.195, interval_steps_per_second: 6.493, epoch: 2.8235[0m
[32m[2022-09-08 12:09:12,944] [    INFO][0m - loss: 1.32876539, learning_rate: 9.427450980392158e-06, global_step: 730, interval_runtime: 1.5423, interval_samples_per_second: 5.187, interval_steps_per_second: 6.484, epoch: 2.8627[0m
[32m[2022-09-08 12:09:14,491] [    INFO][0m - loss: 1.39661894, learning_rate: 9.419607843137256e-06, global_step: 740, interval_runtime: 1.5463, interval_samples_per_second: 5.174, interval_steps_per_second: 6.467, epoch: 2.902[0m
[32m[2022-09-08 12:09:16,038] [    INFO][0m - loss: 1.21806936, learning_rate: 9.411764705882354e-06, global_step: 750, interval_runtime: 1.5471, interval_samples_per_second: 5.171, interval_steps_per_second: 6.464, epoch: 2.9412[0m
[32m[2022-09-08 12:09:17,584] [    INFO][0m - loss: 1.3044239, learning_rate: 9.403921568627452e-06, global_step: 760, interval_runtime: 1.546, interval_samples_per_second: 5.175, interval_steps_per_second: 6.468, epoch: 2.9804[0m
[32m[2022-09-08 12:09:19,109] [    INFO][0m - loss: 1.15421295, learning_rate: 9.39607843137255e-06, global_step: 770, interval_runtime: 1.5249, interval_samples_per_second: 5.246, interval_steps_per_second: 6.558, epoch: 3.0196[0m
[32m[2022-09-08 12:09:20,673] [    INFO][0m - loss: 1.10435047, learning_rate: 9.388235294117647e-06, global_step: 780, interval_runtime: 1.5646, interval_samples_per_second: 5.113, interval_steps_per_second: 6.392, epoch: 3.0588[0m
[32m[2022-09-08 12:09:22,225] [    INFO][0m - loss: 1.28438702, learning_rate: 9.380392156862745e-06, global_step: 790, interval_runtime: 1.5517, interval_samples_per_second: 5.156, interval_steps_per_second: 6.445, epoch: 3.098[0m
[32m[2022-09-08 12:09:23,782] [    INFO][0m - loss: 1.06403379, learning_rate: 9.372549019607843e-06, global_step: 800, interval_runtime: 1.557, interval_samples_per_second: 5.138, interval_steps_per_second: 6.423, epoch: 3.1373[0m
[32m[2022-09-08 12:09:23,782] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:09:23,782] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-08 12:09:23,782] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:09:23,782] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:09:23,783] [    INFO][0m -   Total prediction steps = 259[0m
[32m[2022-09-08 12:09:41,964] [    INFO][0m - eval_loss: 1.452610731124878, eval_accuracy: 0.591876208782196, eval_runtime: 18.1806, eval_samples_per_second: 113.748, eval_steps_per_second: 14.246, epoch: 3.1373[0m
[32m[2022-09-08 12:09:42,026] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-800[0m
[32m[2022-09-08 12:09:42,026] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:09:45,467] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-800/tokenizer_config.json[0m
[32m[2022-09-08 12:09:45,468] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-800/special_tokens_map.json[0m
[32m[2022-09-08 12:09:52,279] [    INFO][0m - loss: 1.16044207, learning_rate: 9.364705882352943e-06, global_step: 810, interval_runtime: 28.4974, interval_samples_per_second: 0.281, interval_steps_per_second: 0.351, epoch: 3.1765[0m
[32m[2022-09-08 12:09:53,842] [    INFO][0m - loss: 0.8232585, learning_rate: 9.356862745098039e-06, global_step: 820, interval_runtime: 1.5632, interval_samples_per_second: 5.118, interval_steps_per_second: 6.397, epoch: 3.2157[0m
[32m[2022-09-08 12:09:55,387] [    INFO][0m - loss: 0.87489653, learning_rate: 9.349019607843139e-06, global_step: 830, interval_runtime: 1.5447, interval_samples_per_second: 5.179, interval_steps_per_second: 6.474, epoch: 3.2549[0m
[32m[2022-09-08 12:09:56,951] [    INFO][0m - loss: 1.08346643, learning_rate: 9.341176470588236e-06, global_step: 840, interval_runtime: 1.564, interval_samples_per_second: 5.115, interval_steps_per_second: 6.394, epoch: 3.2941[0m
[32m[2022-09-08 12:09:58,503] [    INFO][0m - loss: 0.85809746, learning_rate: 9.333333333333334e-06, global_step: 850, interval_runtime: 1.5515, interval_samples_per_second: 5.156, interval_steps_per_second: 6.446, epoch: 3.3333[0m
[32m[2022-09-08 12:10:00,057] [    INFO][0m - loss: 1.00148363, learning_rate: 9.325490196078432e-06, global_step: 860, interval_runtime: 1.5541, interval_samples_per_second: 5.148, interval_steps_per_second: 6.435, epoch: 3.3725[0m
[32m[2022-09-08 12:10:01,612] [    INFO][0m - loss: 0.87782068, learning_rate: 9.31764705882353e-06, global_step: 870, interval_runtime: 1.5551, interval_samples_per_second: 5.144, interval_steps_per_second: 6.431, epoch: 3.4118[0m
[32m[2022-09-08 12:10:03,167] [    INFO][0m - loss: 1.07543077, learning_rate: 9.309803921568628e-06, global_step: 880, interval_runtime: 1.5548, interval_samples_per_second: 5.145, interval_steps_per_second: 6.432, epoch: 3.451[0m
[32m[2022-09-08 12:10:04,720] [    INFO][0m - loss: 0.84544611, learning_rate: 9.301960784313726e-06, global_step: 890, interval_runtime: 1.5539, interval_samples_per_second: 5.148, interval_steps_per_second: 6.435, epoch: 3.4902[0m
[32m[2022-09-08 12:10:06,272] [    INFO][0m - loss: 0.75680456, learning_rate: 9.294117647058824e-06, global_step: 900, interval_runtime: 1.5513, interval_samples_per_second: 5.157, interval_steps_per_second: 6.446, epoch: 3.5294[0m
[32m[2022-09-08 12:10:06,272] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:10:06,272] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-08 12:10:06,272] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:10:06,272] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:10:06,273] [    INFO][0m -   Total prediction steps = 259[0m
[32m[2022-09-08 12:10:24,479] [    INFO][0m - eval_loss: 1.4452199935913086, eval_accuracy: 0.605899453163147, eval_runtime: 18.2062, eval_samples_per_second: 113.588, eval_steps_per_second: 14.226, epoch: 3.5294[0m
[32m[2022-09-08 12:10:24,535] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-900[0m
[32m[2022-09-08 12:10:24,535] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:10:27,647] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-900/tokenizer_config.json[0m
[32m[2022-09-08 12:10:27,647] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-900/special_tokens_map.json[0m
[32m[2022-09-08 12:10:34,605] [    INFO][0m - loss: 1.01933479, learning_rate: 9.286274509803922e-06, global_step: 910, interval_runtime: 28.3331, interval_samples_per_second: 0.282, interval_steps_per_second: 0.353, epoch: 3.5686[0m
[32m[2022-09-08 12:10:36,149] [    INFO][0m - loss: 1.27686815, learning_rate: 9.278431372549021e-06, global_step: 920, interval_runtime: 1.5445, interval_samples_per_second: 5.18, interval_steps_per_second: 6.475, epoch: 3.6078[0m
[32m[2022-09-08 12:10:37,682] [    INFO][0m - loss: 0.99537401, learning_rate: 9.270588235294117e-06, global_step: 930, interval_runtime: 1.5328, interval_samples_per_second: 5.219, interval_steps_per_second: 6.524, epoch: 3.6471[0m
[32m[2022-09-08 12:10:39,236] [    INFO][0m - loss: 0.94005985, learning_rate: 9.262745098039217e-06, global_step: 940, interval_runtime: 1.5534, interval_samples_per_second: 5.15, interval_steps_per_second: 6.437, epoch: 3.6863[0m
[32m[2022-09-08 12:10:40,789] [    INFO][0m - loss: 1.06649008, learning_rate: 9.254901960784315e-06, global_step: 950, interval_runtime: 1.5535, interval_samples_per_second: 5.15, interval_steps_per_second: 6.437, epoch: 3.7255[0m
[32m[2022-09-08 12:10:42,340] [    INFO][0m - loss: 1.2025609, learning_rate: 9.247058823529413e-06, global_step: 960, interval_runtime: 1.5513, interval_samples_per_second: 5.157, interval_steps_per_second: 6.446, epoch: 3.7647[0m
[32m[2022-09-08 12:10:43,886] [    INFO][0m - loss: 0.92440081, learning_rate: 9.23921568627451e-06, global_step: 970, interval_runtime: 1.5454, interval_samples_per_second: 5.177, interval_steps_per_second: 6.471, epoch: 3.8039[0m
[32m[2022-09-08 12:10:45,452] [    INFO][0m - loss: 0.98895216, learning_rate: 9.231372549019608e-06, global_step: 980, interval_runtime: 1.5663, interval_samples_per_second: 5.108, interval_steps_per_second: 6.385, epoch: 3.8431[0m
[32m[2022-09-08 12:10:47,003] [    INFO][0m - loss: 1.02725344, learning_rate: 9.223529411764706e-06, global_step: 990, interval_runtime: 1.5507, interval_samples_per_second: 5.159, interval_steps_per_second: 6.449, epoch: 3.8824[0m
[32m[2022-09-08 12:10:48,558] [    INFO][0m - loss: 1.03812933, learning_rate: 9.215686274509804e-06, global_step: 1000, interval_runtime: 1.5552, interval_samples_per_second: 5.144, interval_steps_per_second: 6.43, epoch: 3.9216[0m
[32m[2022-09-08 12:10:48,558] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:10:48,559] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-08 12:10:48,559] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:10:48,559] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:10:48,559] [    INFO][0m -   Total prediction steps = 259[0m
[32m[2022-09-08 12:11:06,850] [    INFO][0m - eval_loss: 1.4762392044067383, eval_accuracy: 0.6136363744735718, eval_runtime: 18.2911, eval_samples_per_second: 113.061, eval_steps_per_second: 14.16, epoch: 3.9216[0m
[32m[2022-09-08 12:11:06,928] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1000[0m
[32m[2022-09-08 12:11:06,929] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:11:10,069] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1000/tokenizer_config.json[0m
[32m[2022-09-08 12:11:10,069] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1000/special_tokens_map.json[0m
[32m[2022-09-08 12:11:16,802] [    INFO][0m - loss: 1.06134777, learning_rate: 9.207843137254902e-06, global_step: 1010, interval_runtime: 28.2439, interval_samples_per_second: 0.283, interval_steps_per_second: 0.354, epoch: 3.9608[0m
[32m[2022-09-08 12:11:18,231] [    INFO][0m - loss: 0.98831797, learning_rate: 9.200000000000002e-06, global_step: 1020, interval_runtime: 1.4291, interval_samples_per_second: 5.598, interval_steps_per_second: 6.997, epoch: 4.0[0m
[32m[2022-09-08 12:11:19,887] [    INFO][0m - loss: 0.69793601, learning_rate: 9.192156862745098e-06, global_step: 1030, interval_runtime: 1.6555, interval_samples_per_second: 4.832, interval_steps_per_second: 6.041, epoch: 4.0392[0m
[32m[2022-09-08 12:11:21,440] [    INFO][0m - loss: 0.56064954, learning_rate: 9.184313725490197e-06, global_step: 1040, interval_runtime: 1.5537, interval_samples_per_second: 5.149, interval_steps_per_second: 6.436, epoch: 4.0784[0m
[32m[2022-09-08 12:11:23,010] [    INFO][0m - loss: 0.72808285, learning_rate: 9.176470588235294e-06, global_step: 1050, interval_runtime: 1.5701, interval_samples_per_second: 5.095, interval_steps_per_second: 6.369, epoch: 4.1176[0m
[32m[2022-09-08 12:11:24,572] [    INFO][0m - loss: 0.74457197, learning_rate: 9.168627450980393e-06, global_step: 1060, interval_runtime: 1.5613, interval_samples_per_second: 5.124, interval_steps_per_second: 6.405, epoch: 4.1569[0m
[32m[2022-09-08 12:11:26,128] [    INFO][0m - loss: 0.74974089, learning_rate: 9.160784313725491e-06, global_step: 1070, interval_runtime: 1.5566, interval_samples_per_second: 5.139, interval_steps_per_second: 6.424, epoch: 4.1961[0m
[32m[2022-09-08 12:11:27,677] [    INFO][0m - loss: 0.61884384, learning_rate: 9.152941176470589e-06, global_step: 1080, interval_runtime: 1.5488, interval_samples_per_second: 5.165, interval_steps_per_second: 6.457, epoch: 4.2353[0m
[32m[2022-09-08 12:11:29,224] [    INFO][0m - loss: 0.87828474, learning_rate: 9.145098039215687e-06, global_step: 1090, interval_runtime: 1.5469, interval_samples_per_second: 5.172, interval_steps_per_second: 6.465, epoch: 4.2745[0m
[32m[2022-09-08 12:11:30,774] [    INFO][0m - loss: 0.71641283, learning_rate: 9.137254901960785e-06, global_step: 1100, interval_runtime: 1.5501, interval_samples_per_second: 5.161, interval_steps_per_second: 6.451, epoch: 4.3137[0m
[32m[2022-09-08 12:11:30,774] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:11:30,775] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-08 12:11:30,775] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:11:30,775] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:11:30,775] [    INFO][0m -   Total prediction steps = 259[0m
[32m[2022-09-08 12:11:49,175] [    INFO][0m - eval_loss: 1.450805902481079, eval_accuracy: 0.613152801990509, eval_runtime: 18.4, eval_samples_per_second: 112.391, eval_steps_per_second: 14.076, epoch: 4.3137[0m
[32m[2022-09-08 12:11:49,256] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1100[0m
[32m[2022-09-08 12:11:49,256] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:11:52,433] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1100/tokenizer_config.json[0m
[32m[2022-09-08 12:11:52,434] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1100/special_tokens_map.json[0m
[32m[2022-09-08 12:11:59,375] [    INFO][0m - loss: 0.72442231, learning_rate: 9.129411764705883e-06, global_step: 1110, interval_runtime: 28.6008, interval_samples_per_second: 0.28, interval_steps_per_second: 0.35, epoch: 4.3529[0m
[32m[2022-09-08 12:12:00,914] [    INFO][0m - loss: 0.83356152, learning_rate: 9.12156862745098e-06, global_step: 1120, interval_runtime: 1.5389, interval_samples_per_second: 5.198, interval_steps_per_second: 6.498, epoch: 4.3922[0m
[32m[2022-09-08 12:12:02,470] [    INFO][0m - loss: 0.81613874, learning_rate: 9.113725490196078e-06, global_step: 1130, interval_runtime: 1.5563, interval_samples_per_second: 5.141, interval_steps_per_second: 6.426, epoch: 4.4314[0m
[32m[2022-09-08 12:12:04,012] [    INFO][0m - loss: 0.70420232, learning_rate: 9.105882352941178e-06, global_step: 1140, interval_runtime: 1.542, interval_samples_per_second: 5.188, interval_steps_per_second: 6.485, epoch: 4.4706[0m
[32m[2022-09-08 12:12:05,572] [    INFO][0m - loss: 1.0523798, learning_rate: 9.098039215686276e-06, global_step: 1150, interval_runtime: 1.5603, interval_samples_per_second: 5.127, interval_steps_per_second: 6.409, epoch: 4.5098[0m
[32m[2022-09-08 12:12:07,138] [    INFO][0m - loss: 0.76813436, learning_rate: 9.090196078431374e-06, global_step: 1160, interval_runtime: 1.5652, interval_samples_per_second: 5.111, interval_steps_per_second: 6.389, epoch: 4.549[0m
[32m[2022-09-08 12:12:08,687] [    INFO][0m - loss: 0.71048336, learning_rate: 9.082352941176472e-06, global_step: 1170, interval_runtime: 1.5493, interval_samples_per_second: 5.164, interval_steps_per_second: 6.454, epoch: 4.5882[0m
[32m[2022-09-08 12:12:10,235] [    INFO][0m - loss: 0.88201294, learning_rate: 9.07450980392157e-06, global_step: 1180, interval_runtime: 1.5484, interval_samples_per_second: 5.167, interval_steps_per_second: 6.458, epoch: 4.6275[0m
[32m[2022-09-08 12:12:11,789] [    INFO][0m - loss: 0.85340405, learning_rate: 9.066666666666667e-06, global_step: 1190, interval_runtime: 1.5533, interval_samples_per_second: 5.15, interval_steps_per_second: 6.438, epoch: 4.6667[0m
[32m[2022-09-08 12:12:13,350] [    INFO][0m - loss: 0.71520023, learning_rate: 9.058823529411765e-06, global_step: 1200, interval_runtime: 1.5619, interval_samples_per_second: 5.122, interval_steps_per_second: 6.403, epoch: 4.7059[0m
[32m[2022-09-08 12:12:13,351] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:12:13,351] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-08 12:12:13,351] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:12:13,351] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:12:13,351] [    INFO][0m -   Total prediction steps = 259[0m
[32m[2022-09-08 12:12:31,556] [    INFO][0m - eval_loss: 1.45230233669281, eval_accuracy: 0.6208897829055786, eval_runtime: 18.2044, eval_samples_per_second: 113.599, eval_steps_per_second: 14.227, epoch: 4.7059[0m
[32m[2022-09-08 12:12:31,631] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1200[0m
[32m[2022-09-08 12:12:31,631] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:12:34,745] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1200/tokenizer_config.json[0m
[32m[2022-09-08 12:12:34,745] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1200/special_tokens_map.json[0m
[32m[2022-09-08 12:12:41,617] [    INFO][0m - loss: 0.80832338, learning_rate: 9.050980392156863e-06, global_step: 1210, interval_runtime: 28.2663, interval_samples_per_second: 0.283, interval_steps_per_second: 0.354, epoch: 4.7451[0m
[32m[2022-09-08 12:12:43,166] [    INFO][0m - loss: 0.74550624, learning_rate: 9.043137254901961e-06, global_step: 1220, interval_runtime: 1.5497, interval_samples_per_second: 5.162, interval_steps_per_second: 6.453, epoch: 4.7843[0m
[32m[2022-09-08 12:12:44,703] [    INFO][0m - loss: 0.67887878, learning_rate: 9.03529411764706e-06, global_step: 1230, interval_runtime: 1.5362, interval_samples_per_second: 5.208, interval_steps_per_second: 6.51, epoch: 4.8235[0m
[32m[2022-09-08 12:12:46,267] [    INFO][0m - loss: 0.86638594, learning_rate: 9.027450980392157e-06, global_step: 1240, interval_runtime: 1.5646, interval_samples_per_second: 5.113, interval_steps_per_second: 6.392, epoch: 4.8627[0m
[32m[2022-09-08 12:12:47,815] [    INFO][0m - loss: 0.73389688, learning_rate: 9.019607843137256e-06, global_step: 1250, interval_runtime: 1.5475, interval_samples_per_second: 5.17, interval_steps_per_second: 6.462, epoch: 4.902[0m
[32m[2022-09-08 12:12:49,365] [    INFO][0m - loss: 0.80445814, learning_rate: 9.011764705882353e-06, global_step: 1260, interval_runtime: 1.5507, interval_samples_per_second: 5.159, interval_steps_per_second: 6.449, epoch: 4.9412[0m
[32m[2022-09-08 12:12:50,901] [    INFO][0m - loss: 0.81682529, learning_rate: 9.003921568627452e-06, global_step: 1270, interval_runtime: 1.5351, interval_samples_per_second: 5.211, interval_steps_per_second: 6.514, epoch: 4.9804[0m
[32m[2022-09-08 12:12:52,418] [    INFO][0m - loss: 0.81372662, learning_rate: 8.99607843137255e-06, global_step: 1280, interval_runtime: 1.5174, interval_samples_per_second: 5.272, interval_steps_per_second: 6.59, epoch: 5.0196[0m
[32m[2022-09-08 12:12:53,977] [    INFO][0m - loss: 0.65750756, learning_rate: 8.988235294117648e-06, global_step: 1290, interval_runtime: 1.5593, interval_samples_per_second: 5.13, interval_steps_per_second: 6.413, epoch: 5.0588[0m
[32m[2022-09-08 12:12:55,532] [    INFO][0m - loss: 0.69130836, learning_rate: 8.980392156862746e-06, global_step: 1300, interval_runtime: 1.5547, interval_samples_per_second: 5.146, interval_steps_per_second: 6.432, epoch: 5.098[0m
[32m[2022-09-08 12:12:55,532] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:12:55,533] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-08 12:12:55,533] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:12:55,533] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:12:55,533] [    INFO][0m -   Total prediction steps = 259[0m
[32m[2022-09-08 12:13:13,745] [    INFO][0m - eval_loss: 1.45156729221344, eval_accuracy: 0.6257253289222717, eval_runtime: 18.2117, eval_samples_per_second: 113.553, eval_steps_per_second: 14.222, epoch: 5.098[0m
[32m[2022-09-08 12:13:13,813] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1300[0m
[32m[2022-09-08 12:13:13,813] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:13:15,295] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1300/tokenizer_config.json[0m
[32m[2022-09-08 12:13:15,296] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1300/special_tokens_map.json[0m
[32m[2022-09-08 12:13:19,117] [    INFO][0m - loss: 0.60897632, learning_rate: 8.972549019607844e-06, global_step: 1310, interval_runtime: 23.585, interval_samples_per_second: 0.339, interval_steps_per_second: 0.424, epoch: 5.1373[0m
[32m[2022-09-08 12:13:20,658] [    INFO][0m - loss: 0.52158918, learning_rate: 8.964705882352942e-06, global_step: 1320, interval_runtime: 1.541, interval_samples_per_second: 5.191, interval_steps_per_second: 6.489, epoch: 5.1765[0m
[32m[2022-09-08 12:13:22,209] [    INFO][0m - loss: 0.50237913, learning_rate: 8.95686274509804e-06, global_step: 1330, interval_runtime: 1.5509, interval_samples_per_second: 5.158, interval_steps_per_second: 6.448, epoch: 5.2157[0m
[32m[2022-09-08 12:13:23,764] [    INFO][0m - loss: 0.55988407, learning_rate: 8.949019607843137e-06, global_step: 1340, interval_runtime: 1.5548, interval_samples_per_second: 5.146, interval_steps_per_second: 6.432, epoch: 5.2549[0m
[32m[2022-09-08 12:13:25,326] [    INFO][0m - loss: 0.67055869, learning_rate: 8.941176470588237e-06, global_step: 1350, interval_runtime: 1.5617, interval_samples_per_second: 5.122, interval_steps_per_second: 6.403, epoch: 5.2941[0m
[32m[2022-09-08 12:13:26,878] [    INFO][0m - loss: 0.79135709, learning_rate: 8.933333333333333e-06, global_step: 1360, interval_runtime: 1.5523, interval_samples_per_second: 5.154, interval_steps_per_second: 6.442, epoch: 5.3333[0m
[32m[2022-09-08 12:13:28,431] [    INFO][0m - loss: 0.56988025, learning_rate: 8.925490196078433e-06, global_step: 1370, interval_runtime: 1.5527, interval_samples_per_second: 5.152, interval_steps_per_second: 6.44, epoch: 5.3725[0m
[32m[2022-09-08 12:13:29,978] [    INFO][0m - loss: 0.64418387, learning_rate: 8.91764705882353e-06, global_step: 1380, interval_runtime: 1.5475, interval_samples_per_second: 5.17, interval_steps_per_second: 6.462, epoch: 5.4118[0m
[32m[2022-09-08 12:13:31,526] [    INFO][0m - loss: 0.60455246, learning_rate: 8.909803921568628e-06, global_step: 1390, interval_runtime: 1.548, interval_samples_per_second: 5.168, interval_steps_per_second: 6.46, epoch: 5.451[0m
[32m[2022-09-08 12:13:33,082] [    INFO][0m - loss: 0.67407141, learning_rate: 8.901960784313726e-06, global_step: 1400, interval_runtime: 1.556, interval_samples_per_second: 5.141, interval_steps_per_second: 6.427, epoch: 5.4902[0m
[32m[2022-09-08 12:13:33,082] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:13:33,083] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-08 12:13:33,083] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:13:33,083] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:13:33,083] [    INFO][0m -   Total prediction steps = 259[0m
[32m[2022-09-08 12:13:51,352] [    INFO][0m - eval_loss: 1.5531803369522095, eval_accuracy: 0.6146035194396973, eval_runtime: 18.2686, eval_samples_per_second: 113.2, eval_steps_per_second: 14.177, epoch: 5.4902[0m
[32m[2022-09-08 12:13:51,409] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1400[0m
[32m[2022-09-08 12:13:51,410] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:13:52,879] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1400/tokenizer_config.json[0m
[32m[2022-09-08 12:13:52,879] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1400/special_tokens_map.json[0m
[32m[2022-09-08 12:13:56,791] [    INFO][0m - loss: 0.63795667, learning_rate: 8.894117647058824e-06, global_step: 1410, interval_runtime: 23.7084, interval_samples_per_second: 0.337, interval_steps_per_second: 0.422, epoch: 5.5294[0m
[32m[2022-09-08 12:13:58,343] [    INFO][0m - loss: 0.58166571, learning_rate: 8.886274509803922e-06, global_step: 1420, interval_runtime: 1.5524, interval_samples_per_second: 5.153, interval_steps_per_second: 6.442, epoch: 5.5686[0m
[32m[2022-09-08 12:13:59,890] [    INFO][0m - loss: 0.54031162, learning_rate: 8.87843137254902e-06, global_step: 1430, interval_runtime: 1.5468, interval_samples_per_second: 5.172, interval_steps_per_second: 6.465, epoch: 5.6078[0m
[32m[2022-09-08 12:14:01,442] [    INFO][0m - loss: 0.49967451, learning_rate: 8.870588235294118e-06, global_step: 1440, interval_runtime: 1.5517, interval_samples_per_second: 5.156, interval_steps_per_second: 6.444, epoch: 5.6471[0m
[32m[2022-09-08 12:14:02,992] [    INFO][0m - loss: 0.60184164, learning_rate: 8.862745098039216e-06, global_step: 1450, interval_runtime: 1.5505, interval_samples_per_second: 5.16, interval_steps_per_second: 6.449, epoch: 5.6863[0m
[32m[2022-09-08 12:14:04,551] [    INFO][0m - loss: 0.64074655, learning_rate: 8.854901960784315e-06, global_step: 1460, interval_runtime: 1.5595, interval_samples_per_second: 5.13, interval_steps_per_second: 6.412, epoch: 5.7255[0m
[32m[2022-09-08 12:14:06,193] [    INFO][0m - loss: 0.80977554, learning_rate: 8.847058823529413e-06, global_step: 1470, interval_runtime: 1.5552, interval_samples_per_second: 5.144, interval_steps_per_second: 6.43, epoch: 5.7647[0m
[32m[2022-09-08 12:14:07,748] [    INFO][0m - loss: 0.44290018, learning_rate: 8.839215686274511e-06, global_step: 1480, interval_runtime: 1.6414, interval_samples_per_second: 4.874, interval_steps_per_second: 6.092, epoch: 5.8039[0m
[32m[2022-09-08 12:14:09,307] [    INFO][0m - loss: 0.55411048, learning_rate: 8.831372549019609e-06, global_step: 1490, interval_runtime: 1.5585, interval_samples_per_second: 5.133, interval_steps_per_second: 6.416, epoch: 5.8431[0m
[32m[2022-09-08 12:14:10,867] [    INFO][0m - loss: 0.5525094, learning_rate: 8.823529411764707e-06, global_step: 1500, interval_runtime: 1.5605, interval_samples_per_second: 5.127, interval_steps_per_second: 6.408, epoch: 5.8824[0m
[32m[2022-09-08 12:14:10,868] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:14:10,868] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-08 12:14:10,868] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:14:10,868] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:14:10,868] [    INFO][0m -   Total prediction steps = 259[0m
[32m[2022-09-08 12:14:29,072] [    INFO][0m - eval_loss: 1.523144006729126, eval_accuracy: 0.6204062104225159, eval_runtime: 18.2035, eval_samples_per_second: 113.605, eval_steps_per_second: 14.228, epoch: 5.8824[0m
[32m[2022-09-08 12:14:29,122] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1500[0m
[32m[2022-09-08 12:14:29,122] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:14:30,576] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1500/tokenizer_config.json[0m
[32m[2022-09-08 12:14:30,577] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1500/special_tokens_map.json[0m
[32m[2022-09-08 12:14:34,407] [    INFO][0m - loss: 0.65481377, learning_rate: 8.815686274509805e-06, global_step: 1510, interval_runtime: 23.54, interval_samples_per_second: 0.34, interval_steps_per_second: 0.425, epoch: 5.9216[0m
[32m[2022-09-08 12:14:35,958] [    INFO][0m - loss: 0.36290636, learning_rate: 8.807843137254903e-06, global_step: 1520, interval_runtime: 1.5507, interval_samples_per_second: 5.159, interval_steps_per_second: 6.449, epoch: 5.9608[0m
[32m[2022-09-08 12:14:37,390] [    INFO][0m - loss: 0.6584939, learning_rate: 8.8e-06, global_step: 1530, interval_runtime: 1.4326, interval_samples_per_second: 5.584, interval_steps_per_second: 6.98, epoch: 6.0[0m
[32m[2022-09-08 12:14:39,039] [    INFO][0m - loss: 0.25349588, learning_rate: 8.7921568627451e-06, global_step: 1540, interval_runtime: 1.6483, interval_samples_per_second: 4.853, interval_steps_per_second: 6.067, epoch: 6.0392[0m
[32m[2022-09-08 12:14:40,603] [    INFO][0m - loss: 0.31206472, learning_rate: 8.784313725490196e-06, global_step: 1550, interval_runtime: 1.564, interval_samples_per_second: 5.115, interval_steps_per_second: 6.394, epoch: 6.0784[0m
[32m[2022-09-08 12:14:42,178] [    INFO][0m - loss: 0.37018502, learning_rate: 8.776470588235296e-06, global_step: 1560, interval_runtime: 1.5753, interval_samples_per_second: 5.078, interval_steps_per_second: 6.348, epoch: 6.1176[0m
[32m[2022-09-08 12:14:43,730] [    INFO][0m - loss: 0.40494123, learning_rate: 8.768627450980392e-06, global_step: 1570, interval_runtime: 1.5519, interval_samples_per_second: 5.155, interval_steps_per_second: 6.444, epoch: 6.1569[0m
[32m[2022-09-08 12:14:45,285] [    INFO][0m - loss: 0.40678635, learning_rate: 8.760784313725492e-06, global_step: 1580, interval_runtime: 1.5549, interval_samples_per_second: 5.145, interval_steps_per_second: 6.431, epoch: 6.1961[0m
[32m[2022-09-08 12:14:46,836] [    INFO][0m - loss: 0.56804295, learning_rate: 8.752941176470588e-06, global_step: 1590, interval_runtime: 1.5513, interval_samples_per_second: 5.157, interval_steps_per_second: 6.446, epoch: 6.2353[0m
[32m[2022-09-08 12:14:48,396] [    INFO][0m - loss: 0.46999264, learning_rate: 8.745098039215687e-06, global_step: 1600, interval_runtime: 1.5602, interval_samples_per_second: 5.128, interval_steps_per_second: 6.409, epoch: 6.2745[0m
[32m[2022-09-08 12:14:48,397] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:14:48,397] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-08 12:14:48,397] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:14:48,397] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:14:48,397] [    INFO][0m -   Total prediction steps = 259[0m
[32m[2022-09-08 12:15:06,616] [    INFO][0m - eval_loss: 1.5110386610031128, eval_accuracy: 0.6300773620605469, eval_runtime: 18.2184, eval_samples_per_second: 113.512, eval_steps_per_second: 14.216, epoch: 6.2745[0m
[32m[2022-09-08 12:15:06,663] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1600[0m
[32m[2022-09-08 12:15:06,663] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:15:08,115] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1600/tokenizer_config.json[0m
[32m[2022-09-08 12:15:08,115] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1600/special_tokens_map.json[0m
[32m[2022-09-08 12:15:11,957] [    INFO][0m - loss: 0.44539256, learning_rate: 8.737254901960785e-06, global_step: 1610, interval_runtime: 23.561, interval_samples_per_second: 0.34, interval_steps_per_second: 0.424, epoch: 6.3137[0m
[32m[2022-09-08 12:15:13,508] [    INFO][0m - loss: 0.57646699, learning_rate: 8.729411764705883e-06, global_step: 1620, interval_runtime: 1.5503, interval_samples_per_second: 5.16, interval_steps_per_second: 6.45, epoch: 6.3529[0m
[32m[2022-09-08 12:15:15,050] [    INFO][0m - loss: 0.43573217, learning_rate: 8.721568627450981e-06, global_step: 1630, interval_runtime: 1.5426, interval_samples_per_second: 5.186, interval_steps_per_second: 6.483, epoch: 6.3922[0m
[32m[2022-09-08 12:15:16,601] [    INFO][0m - loss: 0.47330818, learning_rate: 8.713725490196079e-06, global_step: 1640, interval_runtime: 1.5504, interval_samples_per_second: 5.16, interval_steps_per_second: 6.45, epoch: 6.4314[0m
[32m[2022-09-08 12:15:18,157] [    INFO][0m - loss: 0.33899093, learning_rate: 8.705882352941177e-06, global_step: 1650, interval_runtime: 1.5564, interval_samples_per_second: 5.14, interval_steps_per_second: 6.425, epoch: 6.4706[0m
[32m[2022-09-08 12:15:19,713] [    INFO][0m - loss: 0.46567974, learning_rate: 8.698039215686275e-06, global_step: 1660, interval_runtime: 1.5556, interval_samples_per_second: 5.143, interval_steps_per_second: 6.428, epoch: 6.5098[0m
[32m[2022-09-08 12:15:21,267] [    INFO][0m - loss: 0.68344684, learning_rate: 8.690196078431372e-06, global_step: 1670, interval_runtime: 1.554, interval_samples_per_second: 5.148, interval_steps_per_second: 6.435, epoch: 6.549[0m
[32m[2022-09-08 12:15:22,828] [    INFO][0m - loss: 0.48803115, learning_rate: 8.682352941176472e-06, global_step: 1680, interval_runtime: 1.5607, interval_samples_per_second: 5.126, interval_steps_per_second: 6.407, epoch: 6.5882[0m
[32m[2022-09-08 12:15:24,380] [    INFO][0m - loss: 0.47139382, learning_rate: 8.67450980392157e-06, global_step: 1690, interval_runtime: 1.5521, interval_samples_per_second: 5.154, interval_steps_per_second: 6.443, epoch: 6.6275[0m
[32m[2022-09-08 12:15:25,945] [    INFO][0m - loss: 0.4348011, learning_rate: 8.666666666666668e-06, global_step: 1700, interval_runtime: 1.5655, interval_samples_per_second: 5.11, interval_steps_per_second: 6.388, epoch: 6.6667[0m
[32m[2022-09-08 12:15:25,946] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:15:25,946] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-08 12:15:25,946] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:15:25,946] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:15:25,946] [    INFO][0m -   Total prediction steps = 259[0m
[32m[2022-09-08 12:15:44,289] [    INFO][0m - eval_loss: 1.5147480964660645, eval_accuracy: 0.6242746710777283, eval_runtime: 18.3426, eval_samples_per_second: 112.743, eval_steps_per_second: 14.12, epoch: 6.6667[0m
[32m[2022-09-08 12:15:44,337] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1700[0m
[32m[2022-09-08 12:15:44,337] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:15:45,784] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1700/tokenizer_config.json[0m
[32m[2022-09-08 12:15:45,785] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1700/special_tokens_map.json[0m
[32m[2022-09-08 12:15:49,608] [    INFO][0m - loss: 0.50204139, learning_rate: 8.658823529411766e-06, global_step: 1710, interval_runtime: 23.6629, interval_samples_per_second: 0.338, interval_steps_per_second: 0.423, epoch: 6.7059[0m
[32m[2022-09-08 12:15:51,168] [    INFO][0m - loss: 0.38117797, learning_rate: 8.650980392156864e-06, global_step: 1720, interval_runtime: 1.5599, interval_samples_per_second: 5.129, interval_steps_per_second: 6.411, epoch: 6.7451[0m
[32m[2022-09-08 12:15:52,731] [    INFO][0m - loss: 0.53231168, learning_rate: 8.643137254901961e-06, global_step: 1730, interval_runtime: 1.5629, interval_samples_per_second: 5.119, interval_steps_per_second: 6.398, epoch: 6.7843[0m
[32m[2022-09-08 12:15:54,295] [    INFO][0m - loss: 0.33353829, learning_rate: 8.63529411764706e-06, global_step: 1740, interval_runtime: 1.5639, interval_samples_per_second: 5.115, interval_steps_per_second: 6.394, epoch: 6.8235[0m
[32m[2022-09-08 12:15:55,873] [    INFO][0m - loss: 0.41712832, learning_rate: 8.627450980392157e-06, global_step: 1750, interval_runtime: 1.5779, interval_samples_per_second: 5.07, interval_steps_per_second: 6.338, epoch: 6.8627[0m
[32m[2022-09-08 12:15:57,436] [    INFO][0m - loss: 0.62265673, learning_rate: 8.619607843137255e-06, global_step: 1760, interval_runtime: 1.5633, interval_samples_per_second: 5.117, interval_steps_per_second: 6.397, epoch: 6.902[0m
[32m[2022-09-08 12:15:58,986] [    INFO][0m - loss: 0.60285015, learning_rate: 8.611764705882355e-06, global_step: 1770, interval_runtime: 1.55, interval_samples_per_second: 5.161, interval_steps_per_second: 6.452, epoch: 6.9412[0m
[32m[2022-09-08 12:16:00,555] [    INFO][0m - loss: 0.34999592, learning_rate: 8.603921568627451e-06, global_step: 1780, interval_runtime: 1.5686, interval_samples_per_second: 5.1, interval_steps_per_second: 6.375, epoch: 6.9804[0m
[32m[2022-09-08 12:16:02,104] [    INFO][0m - loss: 0.49948988, learning_rate: 8.59607843137255e-06, global_step: 1790, interval_runtime: 1.5492, interval_samples_per_second: 5.164, interval_steps_per_second: 6.455, epoch: 7.0196[0m
[32m[2022-09-08 12:16:03,682] [    INFO][0m - loss: 0.25058341, learning_rate: 8.588235294117647e-06, global_step: 1800, interval_runtime: 1.5777, interval_samples_per_second: 5.071, interval_steps_per_second: 6.338, epoch: 7.0588[0m
[32m[2022-09-08 12:16:03,682] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:16:03,682] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-08 12:16:03,682] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:16:03,682] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:16:03,682] [    INFO][0m -   Total prediction steps = 259[0m
[32m[2022-09-08 12:16:21,909] [    INFO][0m - eval_loss: 1.578711986541748, eval_accuracy: 0.6208897829055786, eval_runtime: 18.2266, eval_samples_per_second: 113.461, eval_steps_per_second: 14.21, epoch: 7.0588[0m
[32m[2022-09-08 12:16:21,957] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1800[0m
[32m[2022-09-08 12:16:21,957] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:16:23,406] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1800/tokenizer_config.json[0m
[32m[2022-09-08 12:16:23,406] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1800/special_tokens_map.json[0m
[32m[2022-09-08 12:16:27,226] [    INFO][0m - loss: 0.32140994, learning_rate: 8.580392156862746e-06, global_step: 1810, interval_runtime: 23.5446, interval_samples_per_second: 0.34, interval_steps_per_second: 0.425, epoch: 7.098[0m
[32m[2022-09-08 12:16:28,804] [    INFO][0m - loss: 0.35216825, learning_rate: 8.572549019607844e-06, global_step: 1820, interval_runtime: 1.5773, interval_samples_per_second: 5.072, interval_steps_per_second: 6.34, epoch: 7.1373[0m
[32m[2022-09-08 12:16:30,357] [    INFO][0m - loss: 0.31675608, learning_rate: 8.564705882352942e-06, global_step: 1830, interval_runtime: 1.5531, interval_samples_per_second: 5.151, interval_steps_per_second: 6.439, epoch: 7.1765[0m
[32m[2022-09-08 12:16:31,917] [    INFO][0m - loss: 0.22199531, learning_rate: 8.55686274509804e-06, global_step: 1840, interval_runtime: 1.5604, interval_samples_per_second: 5.127, interval_steps_per_second: 6.409, epoch: 7.2157[0m
[32m[2022-09-08 12:16:33,491] [    INFO][0m - loss: 0.37514043, learning_rate: 8.549019607843138e-06, global_step: 1850, interval_runtime: 1.5742, interval_samples_per_second: 5.082, interval_steps_per_second: 6.352, epoch: 7.2549[0m
[32m[2022-09-08 12:16:35,061] [    INFO][0m - loss: 0.43624911, learning_rate: 8.541176470588236e-06, global_step: 1860, interval_runtime: 1.5695, interval_samples_per_second: 5.097, interval_steps_per_second: 6.372, epoch: 7.2941[0m
[32m[2022-09-08 12:16:37,836] [    INFO][0m - loss: 0.40468659, learning_rate: 8.533333333333335e-06, global_step: 1870, interval_runtime: 1.5691, interval_samples_per_second: 5.098, interval_steps_per_second: 6.373, epoch: 7.3333[0m
[32m[2022-09-08 12:16:39,393] [    INFO][0m - loss: 0.29686992, learning_rate: 8.525490196078431e-06, global_step: 1880, interval_runtime: 2.7633, interval_samples_per_second: 2.895, interval_steps_per_second: 3.619, epoch: 7.3725[0m
[32m[2022-09-08 12:16:40,962] [    INFO][0m - loss: 0.31914339, learning_rate: 8.517647058823531e-06, global_step: 1890, interval_runtime: 1.569, interval_samples_per_second: 5.099, interval_steps_per_second: 6.373, epoch: 7.4118[0m
[32m[2022-09-08 12:16:42,516] [    INFO][0m - loss: 0.43673425, learning_rate: 8.509803921568627e-06, global_step: 1900, interval_runtime: 1.5538, interval_samples_per_second: 5.149, interval_steps_per_second: 6.436, epoch: 7.451[0m
[32m[2022-09-08 12:16:42,516] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:16:42,517] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-08 12:16:42,517] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:16:42,517] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:16:42,517] [    INFO][0m -   Total prediction steps = 259[0m
[32m[2022-09-08 12:17:00,706] [    INFO][0m - eval_loss: 1.5927536487579346, eval_accuracy: 0.6194390654563904, eval_runtime: 18.1888, eval_samples_per_second: 113.696, eval_steps_per_second: 14.239, epoch: 7.451[0m
[32m[2022-09-08 12:17:00,754] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1900[0m
[32m[2022-09-08 12:17:00,754] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:17:02,219] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1900/tokenizer_config.json[0m
[32m[2022-09-08 12:17:02,220] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1900/special_tokens_map.json[0m
[32m[2022-09-08 12:17:06,077] [    INFO][0m - loss: 0.28666527, learning_rate: 8.501960784313727e-06, global_step: 1910, interval_runtime: 23.5604, interval_samples_per_second: 0.34, interval_steps_per_second: 0.424, epoch: 7.4902[0m
[32m[2022-09-08 12:17:07,630] [    INFO][0m - loss: 0.30446682, learning_rate: 8.494117647058825e-06, global_step: 1920, interval_runtime: 1.5531, interval_samples_per_second: 5.151, interval_steps_per_second: 6.439, epoch: 7.5294[0m
[32m[2022-09-08 12:17:09,181] [    INFO][0m - loss: 0.28684378, learning_rate: 8.486274509803922e-06, global_step: 1930, interval_runtime: 1.551, interval_samples_per_second: 5.158, interval_steps_per_second: 6.448, epoch: 7.5686[0m
[32m[2022-09-08 12:17:10,739] [    INFO][0m - loss: 0.3087759, learning_rate: 8.47843137254902e-06, global_step: 1940, interval_runtime: 1.5587, interval_samples_per_second: 5.133, interval_steps_per_second: 6.416, epoch: 7.6078[0m
[32m[2022-09-08 12:17:12,297] [    INFO][0m - loss: 0.24282899, learning_rate: 8.470588235294118e-06, global_step: 1950, interval_runtime: 1.5581, interval_samples_per_second: 5.134, interval_steps_per_second: 6.418, epoch: 7.6471[0m
[32m[2022-09-08 12:17:13,865] [    INFO][0m - loss: 0.46883411, learning_rate: 8.462745098039216e-06, global_step: 1960, interval_runtime: 1.5676, interval_samples_per_second: 5.103, interval_steps_per_second: 6.379, epoch: 7.6863[0m
[32m[2022-09-08 12:17:15,438] [    INFO][0m - loss: 0.34871981, learning_rate: 8.454901960784314e-06, global_step: 1970, interval_runtime: 1.5725, interval_samples_per_second: 5.087, interval_steps_per_second: 6.359, epoch: 7.7255[0m
[32m[2022-09-08 12:17:16,993] [    INFO][0m - loss: 0.23718057, learning_rate: 8.447058823529412e-06, global_step: 1980, interval_runtime: 1.5552, interval_samples_per_second: 5.144, interval_steps_per_second: 6.43, epoch: 7.7647[0m
[32m[2022-09-08 12:17:18,561] [    INFO][0m - loss: 0.38709166, learning_rate: 8.43921568627451e-06, global_step: 1990, interval_runtime: 1.5685, interval_samples_per_second: 5.1, interval_steps_per_second: 6.375, epoch: 7.8039[0m
[32m[2022-09-08 12:17:20,124] [    INFO][0m - loss: 0.32588737, learning_rate: 8.43137254901961e-06, global_step: 2000, interval_runtime: 1.5623, interval_samples_per_second: 5.121, interval_steps_per_second: 6.401, epoch: 7.8431[0m
[32m[2022-09-08 12:17:20,124] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:17:20,124] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-08 12:17:20,124] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:17:20,124] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:17:20,124] [    INFO][0m -   Total prediction steps = 259[0m
[32m[2022-09-08 12:17:38,421] [    INFO][0m - eval_loss: 1.7678022384643555, eval_accuracy: 0.6295938491821289, eval_runtime: 18.2958, eval_samples_per_second: 113.031, eval_steps_per_second: 14.156, epoch: 7.8431[0m
[32m[2022-09-08 12:17:38,521] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2000[0m
[32m[2022-09-08 12:17:38,521] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:17:39,976] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2000/tokenizer_config.json[0m
[32m[2022-09-08 12:17:39,976] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2000/special_tokens_map.json[0m
[32m[2022-09-08 12:17:42,237] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-08 12:17:42,237] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-1600 (score: 0.6300773620605469).[0m
[32m[2022-09-08 12:17:43,113] [    INFO][0m - train_runtime: 811.9582, train_samples_per_second: 125.376, train_steps_per_second: 15.703, train_loss: 1.376244497179985, epoch: 7.8431[0m
[32m[2022-09-08 12:17:43,155] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-09-08 12:17:43,155] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:17:46,353] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-09-08 12:17:46,354] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-09-08 12:17:46,356] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-08 12:17:46,356] [    INFO][0m -   epoch                    =     7.8431[0m
[32m[2022-09-08 12:17:46,356] [    INFO][0m -   train_loss               =     1.3762[0m
[32m[2022-09-08 12:17:46,357] [    INFO][0m -   train_runtime            = 0:13:31.95[0m
[32m[2022-09-08 12:17:46,357] [    INFO][0m -   train_samples_per_second =    125.376[0m
[32m[2022-09-08 12:17:46,357] [    INFO][0m -   train_steps_per_second   =     15.703[0m
[32m[2022-09-08 12:17:46,366] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-08 12:17:46,366] [    INFO][0m -   Num examples = 1784[0m
[32m[2022-09-08 12:17:46,366] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:17:46,366] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:17:46,366] [    INFO][0m -   Total prediction steps = 223[0m
[32m[2022-09-08 12:18:02,072] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-08 12:18:02,072] [    INFO][0m -   test_accuracy           =     0.6396[0m
[32m[2022-09-08 12:18:02,073] [    INFO][0m -   test_loss               =     1.4496[0m
[32m[2022-09-08 12:18:02,073] [    INFO][0m -   test_runtime            = 0:00:15.70[0m
[32m[2022-09-08 12:18:02,073] [    INFO][0m -   test_samples_per_second =    113.589[0m
[32m[2022-09-08 12:18:02,073] [    INFO][0m -   test_steps_per_second   =     14.199[0m
[32m[2022-09-08 12:18:02,073] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-08 12:18:02,073] [    INFO][0m -   Num examples = 2999[0m
[32m[2022-09-08 12:18:02,073] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:18:02,073] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:18:02,074] [    INFO][0m -   Total prediction steps = 375[0m
Traceback (most recent call last):
  File "train_single.py", line 268, in <module>
    main(0)
  File "train_single.py", line 262, in main
    test_ret = postprocess(test_ret, test_ds, data_args.task_name,
  File "/ssd2/wanghuijuan03/prompt/PaddleNLP/examples/benchmark/fewclue/pretrain15/postprocess.py", line 66, in postprocess
    ret_list.append({"id": uid, "label": id_to_label[preds[idx]]})
TypeError: unhashable type: 'numpy.ndarray'
run.sh: line 59: --freeze_plm: command not found
 
==========
tnews
==========
 
[33m[2022-09-08 12:18:35,867] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-09-08 12:18:35,867] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-08 12:18:35,867] [    INFO][0m - ============================================================[0m
[32m[2022-09-08 12:18:35,867] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-08 12:18:35,868] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-08 12:18:35,868] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-08 12:18:35,868] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-08 12:18:35,868] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-08 12:18:35,868] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-08 12:18:35,868] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-09-08 12:18:35,868] [    INFO][0m - [0m
[32m[2022-09-08 12:18:35,868] [    INFO][0m - ============================================================[0m
[32m[2022-09-08 12:18:35,868] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-08 12:18:35,868] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-08 12:18:35,868] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-08 12:18:35,868] [    INFO][0m - prompt                        :[0m
[32m[2022-09-08 12:18:35,868] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-08 12:18:35,869] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-08 12:18:35,869] [    INFO][0m - task_name                     :tnews[0m
[32m[2022-09-08 12:18:35,869] [    INFO][0m - [0m
[32m[2022-09-08 12:18:35,869] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0908 12:18:35.870503 21404 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0908 12:18:35.874555 21404 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-08 12:18:38,811] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-09-08 12:18:38,836] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-09-08 12:18:38,836] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[32m[2022-09-08 12:18:39,867] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚Äù‰∏äËø∞Êñ∞ÈóªÈÄâËá™'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': '‰∏ìÊ†è„ÄÇ'}][0m
[32m[2022-09-08 12:18:39,999] [    INFO][0m - ============================================================[0m
[32m[2022-09-08 12:18:39,999] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-08 12:18:39,999] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-08 12:18:39,999] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-08 12:18:39,999] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-08 12:18:39,999] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-08 12:18:39,999] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-08 12:18:40,000] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-08 12:18:40,000] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-08 12:18:40,000] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-08 12:18:40,000] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-08 12:18:40,000] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-08 12:18:40,000] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-08 12:18:40,000] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-08 12:18:40,000] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-08 12:18:40,000] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-08 12:18:40,000] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-08 12:18:40,001] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-08 12:18:40,001] [    INFO][0m - eval_batch_size               :8[0m
[32m[2022-09-08 12:18:40,001] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-09-08 12:18:40,001] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-09-08 12:18:40,001] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-08 12:18:40,001] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-08 12:18:40,001] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-08 12:18:40,001] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-08 12:18:40,001] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-08 12:18:40,001] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-08 12:18:40,001] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-08 12:18:40,001] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-08 12:18:40,002] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-08 12:18:40,002] [    INFO][0m - learning_rate                 :1e-05[0m
[32m[2022-09-08 12:18:40,002] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-08 12:18:40,002] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-08 12:18:40,002] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-08 12:18:40,002] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-08 12:18:40,002] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-08 12:18:40,002] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-08 12:18:40,002] [    INFO][0m - logging_dir                   :./checkpoints/runs/Sep08_12-18-35_instance-3bwob41y-01[0m
[32m[2022-09-08 12:18:40,002] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-08 12:18:40,002] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-08 12:18:40,002] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-08 12:18:40,002] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-08 12:18:40,003] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-08 12:18:40,003] [    INFO][0m - max_seq_length                :64[0m
[32m[2022-09-08 12:18:40,003] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-08 12:18:40,003] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-08 12:18:40,003] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-08 12:18:40,003] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-08 12:18:40,003] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-09-08 12:18:40,003] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-08 12:18:40,003] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-08 12:18:40,003] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-09-08 12:18:40,003] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-08 12:18:40,003] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-08 12:18:40,003] [    INFO][0m - per_device_eval_batch_size    :8[0m
[32m[2022-09-08 12:18:40,003] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-09-08 12:18:40,004] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-08 12:18:40,004] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-08 12:18:40,004] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-08 12:18:40,004] [    INFO][0m - ppt_learning_rate             :0.0001[0m
[32m[2022-09-08 12:18:40,004] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-08 12:18:40,004] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-08 12:18:40,004] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-08 12:18:40,004] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-08 12:18:40,004] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-08 12:18:40,004] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-08 12:18:40,004] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-09-08 12:18:40,004] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-08 12:18:40,004] [    INFO][0m - save_steps                    :100[0m
[32m[2022-09-08 12:18:40,005] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-09-08 12:18:40,005] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-08 12:18:40,005] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-08 12:18:40,005] [    INFO][0m - seed                          :42[0m
[32m[2022-09-08 12:18:40,005] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-08 12:18:40,005] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-08 12:18:40,005] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-08 12:18:40,005] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-09-08 12:18:40,005] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-08 12:18:40,005] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-08 12:18:40,005] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-08 12:18:40,005] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-08 12:18:40,005] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-08 12:18:40,006] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-08 12:18:40,006] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-08 12:18:40,006] [    INFO][0m - [0m
[32m[2022-09-08 12:18:40,007] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-08 12:18:40,007] [    INFO][0m -   Num examples = 1185[0m
[32m[2022-09-08 12:18:40,008] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-09-08 12:18:40,008] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-09-08 12:18:40,008] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-09-08 12:18:40,008] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-08 12:18:40,008] [    INFO][0m -   Total optimization steps = 7450.0[0m
[32m[2022-09-08 12:18:40,008] [    INFO][0m -   Total num train samples = 59250[0m
[32m[2022-09-08 12:18:41,579] [    INFO][0m - loss: 18.32377625, learning_rate: 9.986577181208055e-06, global_step: 10, interval_runtime: 1.5706, interval_samples_per_second: 5.094, interval_steps_per_second: 6.367, epoch: 0.0671[0m
[32m[2022-09-08 12:18:42,211] [    INFO][0m - loss: 6.68310318, learning_rate: 9.973154362416108e-06, global_step: 20, interval_runtime: 0.632, interval_samples_per_second: 12.658, interval_steps_per_second: 15.823, epoch: 0.1342[0m
[32m[2022-09-08 12:18:42,838] [    INFO][0m - loss: 4.24784241, learning_rate: 9.959731543624162e-06, global_step: 30, interval_runtime: 0.6268, interval_samples_per_second: 12.764, interval_steps_per_second: 15.954, epoch: 0.2013[0m
[32m[2022-09-08 12:18:43,461] [    INFO][0m - loss: 3.87155609, learning_rate: 9.946308724832216e-06, global_step: 40, interval_runtime: 0.6231, interval_samples_per_second: 12.839, interval_steps_per_second: 16.049, epoch: 0.2685[0m
[32m[2022-09-08 12:18:44,092] [    INFO][0m - loss: 2.81488247, learning_rate: 9.93288590604027e-06, global_step: 50, interval_runtime: 0.6315, interval_samples_per_second: 12.669, interval_steps_per_second: 15.836, epoch: 0.3356[0m
[32m[2022-09-08 12:18:44,723] [    INFO][0m - loss: 1.89566574, learning_rate: 9.919463087248323e-06, global_step: 60, interval_runtime: 0.63, interval_samples_per_second: 12.698, interval_steps_per_second: 15.873, epoch: 0.4027[0m
[32m[2022-09-08 12:18:45,356] [    INFO][0m - loss: 1.82279472, learning_rate: 9.906040268456377e-06, global_step: 70, interval_runtime: 0.6328, interval_samples_per_second: 12.642, interval_steps_per_second: 15.803, epoch: 0.4698[0m
[32m[2022-09-08 12:18:45,975] [    INFO][0m - loss: 1.87756271, learning_rate: 9.89261744966443e-06, global_step: 80, interval_runtime: 0.6199, interval_samples_per_second: 12.904, interval_steps_per_second: 16.131, epoch: 0.5369[0m
[32m[2022-09-08 12:18:46,600] [    INFO][0m - loss: 2.08323498, learning_rate: 9.879194630872484e-06, global_step: 90, interval_runtime: 0.6239, interval_samples_per_second: 12.822, interval_steps_per_second: 16.027, epoch: 0.604[0m
[32m[2022-09-08 12:18:47,225] [    INFO][0m - loss: 1.61519909, learning_rate: 9.865771812080538e-06, global_step: 100, interval_runtime: 0.6262, interval_samples_per_second: 12.775, interval_steps_per_second: 15.969, epoch: 0.6711[0m
[32m[2022-09-08 12:18:47,226] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:18:47,226] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-08 12:18:47,226] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:18:47,226] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:18:47,226] [    INFO][0m -   Total prediction steps = 138[0m
[32m[2022-09-08 12:18:51,312] [    INFO][0m - eval_loss: 1.5433509349822998, eval_accuracy: 0.5346083641052246, eval_runtime: 4.0852, eval_samples_per_second: 268.777, eval_steps_per_second: 33.781, epoch: 0.6711[0m
[32m[2022-09-08 12:18:51,354] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-09-08 12:18:51,354] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:18:54,576] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-08 12:18:54,576] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-08 12:19:00,729] [    INFO][0m - loss: 1.72991962, learning_rate: 9.852348993288592e-06, global_step: 110, interval_runtime: 13.5034, interval_samples_per_second: 0.592, interval_steps_per_second: 0.741, epoch: 0.7383[0m
[32m[2022-09-08 12:19:01,363] [    INFO][0m - loss: 1.58904696, learning_rate: 9.838926174496645e-06, global_step: 120, interval_runtime: 0.6337, interval_samples_per_second: 12.625, interval_steps_per_second: 15.781, epoch: 0.8054[0m
[32m[2022-09-08 12:19:02,002] [    INFO][0m - loss: 1.7475584, learning_rate: 9.825503355704699e-06, global_step: 130, interval_runtime: 0.6391, interval_samples_per_second: 12.518, interval_steps_per_second: 15.648, epoch: 0.8725[0m
[32m[2022-09-08 12:19:02,642] [    INFO][0m - loss: 1.97687035, learning_rate: 9.812080536912753e-06, global_step: 140, interval_runtime: 0.6401, interval_samples_per_second: 12.499, interval_steps_per_second: 15.623, epoch: 0.9396[0m
Traceback (most recent call last):
  File "train_single.py", line 268, in <module>
    main(0)
  File "train_single.py", line 222, in main
    train_result = trainer.train(resume_from_checkpoint=None)
  File "/ssd2/wanghuijuan03/prompt/PaddleNLP/paddlenlp/trainer/trainer_base.py", line 559, in train
    tr_loss_step = self.training_step(model, inputs)
  File "/ssd2/wanghuijuan03/prompt/PaddleNLP/paddlenlp/trainer/trainer_base.py", line 1074, in training_step
    loss = self.compute_loss(model, inputs)
  File "/ssd2/wanghuijuan03/prompt/PaddleNLP/paddlenlp/prompt/prompt_trainer.py", line 268, in compute_loss
    loss = self.criterion(outputs, labels)
  File "/ssd2/wanghuijuan03/anaconda3/envs/prompt/lib/python3.8/site-packages/paddle/fluid/dygraph/layers.py", line 948, in __call__
    return self.forward(*inputs, **kwargs)
  File "/ssd2/wanghuijuan03/anaconda3/envs/prompt/lib/python3.8/site-packages/paddle/nn/layer/loss.py", line 396, in forward
    ret = paddle.nn.functional.cross_entropy(input,
  File "/ssd2/wanghuijuan03/anaconda3/envs/prompt/lib/python3.8/site-packages/paddle/nn/functional/loss.py", line 2379, in cross_entropy
    raise ValueError(
ValueError: Expected nput_dims - 1 = label_dims or input_dims == label_dims             (got nput_dims3, label_dims1)
run.sh: line 59: --freeze_plm: command not found
 
==========
iflytek
==========
 
[33m[2022-09-08 12:19:07,103] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-09-08 12:19:07,103] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-08 12:19:07,103] [    INFO][0m - ============================================================[0m
[32m[2022-09-08 12:19:07,103] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-08 12:19:07,104] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-08 12:19:07,104] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-08 12:19:07,104] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-08 12:19:07,104] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-08 12:19:07,104] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-08 12:19:07,104] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-09-08 12:19:07,104] [    INFO][0m - [0m
[32m[2022-09-08 12:19:07,104] [    INFO][0m - ============================================================[0m
[32m[2022-09-08 12:19:07,104] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-08 12:19:07,104] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-08 12:19:07,104] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-08 12:19:07,104] [    INFO][0m - prompt                        :[0m
[32m[2022-09-08 12:19:07,105] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-08 12:19:07,105] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-08 12:19:07,105] [    INFO][0m - task_name                     :iflytek[0m
[32m[2022-09-08 12:19:07,105] [    INFO][0m - [0m
[32m[2022-09-08 12:19:07,105] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0908 12:19:07.106595 21927 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0908 12:19:07.110702 21927 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-08 12:19:09,986] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-09-08 12:19:10,011] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-09-08 12:19:10,011] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[32m[2022-09-08 12:19:11,001] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùËøô‰∏™Âè•Â≠êÊèèËø∞ÁöÑÂ∫îÁî®Á±ªÂà´ÊòØ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': '„ÄÇ'}][0m
[32m[2022-09-08 12:19:11,203] [    INFO][0m - ============================================================[0m
[32m[2022-09-08 12:19:11,203] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-08 12:19:11,204] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-08 12:19:11,204] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-08 12:19:11,204] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-08 12:19:11,204] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-08 12:19:11,204] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-08 12:19:11,204] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-08 12:19:11,204] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-08 12:19:11,204] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-08 12:19:11,204] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-08 12:19:11,204] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-08 12:19:11,205] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-08 12:19:11,205] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-08 12:19:11,205] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-08 12:19:11,205] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-08 12:19:11,205] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-08 12:19:11,205] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-08 12:19:11,205] [    INFO][0m - eval_batch_size               :8[0m
[32m[2022-09-08 12:19:11,205] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-09-08 12:19:11,205] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-09-08 12:19:11,205] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-08 12:19:11,205] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-08 12:19:11,205] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-08 12:19:11,205] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-08 12:19:11,206] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-08 12:19:11,206] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-08 12:19:11,206] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-08 12:19:11,206] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-08 12:19:11,206] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-08 12:19:11,206] [    INFO][0m - learning_rate                 :1e-05[0m
[32m[2022-09-08 12:19:11,206] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-08 12:19:11,206] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-08 12:19:11,206] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-08 12:19:11,206] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-08 12:19:11,206] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-08 12:19:11,206] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-08 12:19:11,206] [    INFO][0m - logging_dir                   :./checkpoints/runs/Sep08_12-19-07_instance-3bwob41y-01[0m
[32m[2022-09-08 12:19:11,206] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-08 12:19:11,207] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-08 12:19:11,207] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-08 12:19:11,207] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-08 12:19:11,207] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-08 12:19:11,207] [    INFO][0m - max_seq_length                :320[0m
[32m[2022-09-08 12:19:11,207] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-08 12:19:11,207] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-08 12:19:11,207] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-08 12:19:11,207] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-08 12:19:11,207] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-09-08 12:19:11,207] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-08 12:19:11,207] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-08 12:19:11,207] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-09-08 12:19:11,208] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-08 12:19:11,208] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-08 12:19:11,208] [    INFO][0m - per_device_eval_batch_size    :8[0m
[32m[2022-09-08 12:19:11,208] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-09-08 12:19:11,208] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-08 12:19:11,208] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-08 12:19:11,208] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-08 12:19:11,208] [    INFO][0m - ppt_learning_rate             :0.0001[0m
[32m[2022-09-08 12:19:11,208] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-08 12:19:11,208] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-08 12:19:11,208] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-08 12:19:11,208] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-08 12:19:11,208] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-08 12:19:11,208] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-08 12:19:11,209] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-09-08 12:19:11,209] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-08 12:19:11,209] [    INFO][0m - save_steps                    :100[0m
[32m[2022-09-08 12:19:11,209] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-09-08 12:19:11,209] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-08 12:19:11,209] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-08 12:19:11,209] [    INFO][0m - seed                          :42[0m
[32m[2022-09-08 12:19:11,209] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-08 12:19:11,209] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-08 12:19:11,209] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-08 12:19:11,209] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-09-08 12:19:11,209] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-08 12:19:11,209] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-08 12:19:11,210] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-08 12:19:11,210] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-08 12:19:11,210] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-08 12:19:11,210] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-08 12:19:11,210] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-08 12:19:11,210] [    INFO][0m - [0m
[32m[2022-09-08 12:19:11,211] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-08 12:19:11,212] [    INFO][0m -   Num examples = 3024[0m
[32m[2022-09-08 12:19:11,212] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-09-08 12:19:11,212] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-09-08 12:19:11,212] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-09-08 12:19:11,212] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-08 12:19:11,212] [    INFO][0m -   Total optimization steps = 18900.0[0m
[32m[2022-09-08 12:19:11,212] [    INFO][0m -   Total num train samples = 151200[0m
[32m[2022-09-08 12:19:14,079] [    INFO][0m - loss: 21.76153107, learning_rate: 9.994708994708996e-06, global_step: 10, interval_runtime: 2.8666, interval_samples_per_second: 2.791, interval_steps_per_second: 3.488, epoch: 0.0265[0m
[32m[2022-09-08 12:19:15,995] [    INFO][0m - loss: 11.22546921, learning_rate: 9.989417989417989e-06, global_step: 20, interval_runtime: 1.9157, interval_samples_per_second: 4.176, interval_steps_per_second: 5.22, epoch: 0.0529[0m
[32m[2022-09-08 12:19:17,906] [    INFO][0m - loss: 7.16753616, learning_rate: 9.984126984126986e-06, global_step: 30, interval_runtime: 1.9104, interval_samples_per_second: 4.188, interval_steps_per_second: 5.234, epoch: 0.0794[0m
[32m[2022-09-08 12:19:19,819] [    INFO][0m - loss: 6.40000153, learning_rate: 9.97883597883598e-06, global_step: 40, interval_runtime: 1.9134, interval_samples_per_second: 4.181, interval_steps_per_second: 5.226, epoch: 0.1058[0m
[32m[2022-09-08 12:19:21,781] [    INFO][0m - loss: 6.55017319, learning_rate: 9.973544973544974e-06, global_step: 50, interval_runtime: 1.9623, interval_samples_per_second: 4.077, interval_steps_per_second: 5.096, epoch: 0.1323[0m
[32m[2022-09-08 12:19:23,704] [    INFO][0m - loss: 6.14725227, learning_rate: 9.968253968253969e-06, global_step: 60, interval_runtime: 1.9226, interval_samples_per_second: 4.161, interval_steps_per_second: 5.201, epoch: 0.1587[0m
[32m[2022-09-08 12:19:25,646] [    INFO][0m - loss: 6.00890961, learning_rate: 9.962962962962964e-06, global_step: 70, interval_runtime: 1.9418, interval_samples_per_second: 4.12, interval_steps_per_second: 5.15, epoch: 0.1852[0m
[32m[2022-09-08 12:19:27,570] [    INFO][0m - loss: 5.59807091, learning_rate: 9.957671957671959e-06, global_step: 80, interval_runtime: 1.9235, interval_samples_per_second: 4.159, interval_steps_per_second: 5.199, epoch: 0.2116[0m
[32m[2022-09-08 12:19:29,491] [    INFO][0m - loss: 4.88100853, learning_rate: 9.952380952380954e-06, global_step: 90, interval_runtime: 1.9214, interval_samples_per_second: 4.164, interval_steps_per_second: 5.205, epoch: 0.2381[0m
[32m[2022-09-08 12:19:31,421] [    INFO][0m - loss: 4.5568779, learning_rate: 9.947089947089947e-06, global_step: 100, interval_runtime: 1.9307, interval_samples_per_second: 4.144, interval_steps_per_second: 5.179, epoch: 0.2646[0m
[32m[2022-09-08 12:19:31,422] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:19:31,422] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-08 12:19:31,422] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:19:31,422] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:19:31,423] [    INFO][0m -   Total prediction steps = 172[0m
[32m[2022-09-08 12:19:46,750] [    INFO][0m - eval_loss: 3.9495508670806885, eval_accuracy: 0.2731245458126068, eval_runtime: 15.3271, eval_samples_per_second: 89.58, eval_steps_per_second: 11.222, epoch: 0.2646[0m
[32m[2022-09-08 12:19:46,804] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-09-08 12:19:46,804] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:19:50,246] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-08 12:19:50,246] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-08 12:19:57,719] [    INFO][0m - loss: 3.97745628, learning_rate: 9.941798941798942e-06, global_step: 110, interval_runtime: 26.2973, interval_samples_per_second: 0.304, interval_steps_per_second: 0.38, epoch: 0.291[0m
[32m[2022-09-08 12:19:59,650] [    INFO][0m - loss: 3.56528664, learning_rate: 9.936507936507937e-06, global_step: 120, interval_runtime: 1.9305, interval_samples_per_second: 4.144, interval_steps_per_second: 5.18, epoch: 0.3175[0m
[32m[2022-09-08 12:20:01,561] [    INFO][0m - loss: 3.6547287, learning_rate: 9.931216931216932e-06, global_step: 130, interval_runtime: 1.9122, interval_samples_per_second: 4.184, interval_steps_per_second: 5.23, epoch: 0.3439[0m
[32m[2022-09-08 12:20:03,497] [    INFO][0m - loss: 3.57043915, learning_rate: 9.925925925925927e-06, global_step: 140, interval_runtime: 1.9357, interval_samples_per_second: 4.133, interval_steps_per_second: 5.166, epoch: 0.3704[0m
[32m[2022-09-08 12:20:05,426] [    INFO][0m - loss: 3.79753685, learning_rate: 9.920634920634922e-06, global_step: 150, interval_runtime: 1.9289, interval_samples_per_second: 4.147, interval_steps_per_second: 5.184, epoch: 0.3968[0m
[32m[2022-09-08 12:20:07,332] [    INFO][0m - loss: 3.6012928, learning_rate: 9.915343915343916e-06, global_step: 160, interval_runtime: 1.9062, interval_samples_per_second: 4.197, interval_steps_per_second: 5.246, epoch: 0.4233[0m
[32m[2022-09-08 12:20:09,269] [    INFO][0m - loss: 3.55914459, learning_rate: 9.91005291005291e-06, global_step: 170, interval_runtime: 1.9373, interval_samples_per_second: 4.13, interval_steps_per_second: 5.162, epoch: 0.4497[0m
[32m[2022-09-08 12:20:11,195] [    INFO][0m - loss: 3.49490852, learning_rate: 9.904761904761906e-06, global_step: 180, interval_runtime: 1.9258, interval_samples_per_second: 4.154, interval_steps_per_second: 5.193, epoch: 0.4762[0m
[32m[2022-09-08 12:20:13,122] [    INFO][0m - loss: 3.14052963, learning_rate: 9.8994708994709e-06, global_step: 190, interval_runtime: 1.9267, interval_samples_per_second: 4.152, interval_steps_per_second: 5.19, epoch: 0.5026[0m
[32m[2022-09-08 12:20:15,036] [    INFO][0m - loss: 3.33056564, learning_rate: 9.894179894179896e-06, global_step: 200, interval_runtime: 1.9139, interval_samples_per_second: 4.18, interval_steps_per_second: 5.225, epoch: 0.5291[0m
[32m[2022-09-08 12:20:15,036] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:20:15,037] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-08 12:20:15,037] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:20:15,037] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:20:15,037] [    INFO][0m -   Total prediction steps = 172[0m
[32m[2022-09-08 12:20:30,072] [    INFO][0m - eval_loss: 2.6940383911132812, eval_accuracy: 0.44064095616340637, eval_runtime: 15.0345, eval_samples_per_second: 91.323, eval_steps_per_second: 11.44, epoch: 0.5291[0m
[32m[2022-09-08 12:20:30,120] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-09-08 12:20:30,120] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:20:33,219] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-09-08 12:20:33,219] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-09-08 12:20:40,258] [    INFO][0m - loss: 2.93764133, learning_rate: 9.88888888888889e-06, global_step: 210, interval_runtime: 25.2223, interval_samples_per_second: 0.317, interval_steps_per_second: 0.396, epoch: 0.5556[0m
[32m[2022-09-08 12:20:42,165] [    INFO][0m - loss: 3.05186462, learning_rate: 9.883597883597884e-06, global_step: 220, interval_runtime: 1.9065, interval_samples_per_second: 4.196, interval_steps_per_second: 5.245, epoch: 0.582[0m
[32m[2022-09-08 12:20:44,073] [    INFO][0m - loss: 2.77331963, learning_rate: 9.878306878306879e-06, global_step: 230, interval_runtime: 1.9083, interval_samples_per_second: 4.192, interval_steps_per_second: 5.24, epoch: 0.6085[0m
[32m[2022-09-08 12:20:45,991] [    INFO][0m - loss: 2.81587257, learning_rate: 9.873015873015874e-06, global_step: 240, interval_runtime: 1.9179, interval_samples_per_second: 4.171, interval_steps_per_second: 5.214, epoch: 0.6349[0m
[32m[2022-09-08 12:20:47,892] [    INFO][0m - loss: 2.50649223, learning_rate: 9.867724867724869e-06, global_step: 250, interval_runtime: 1.9012, interval_samples_per_second: 4.208, interval_steps_per_second: 5.26, epoch: 0.6614[0m
[32m[2022-09-08 12:20:49,807] [    INFO][0m - loss: 2.60871525, learning_rate: 9.862433862433864e-06, global_step: 260, interval_runtime: 1.9147, interval_samples_per_second: 4.178, interval_steps_per_second: 5.223, epoch: 0.6878[0m
[32m[2022-09-08 12:20:51,706] [    INFO][0m - loss: 2.29776077, learning_rate: 9.857142857142859e-06, global_step: 270, interval_runtime: 1.8988, interval_samples_per_second: 4.213, interval_steps_per_second: 5.267, epoch: 0.7143[0m
[32m[2022-09-08 12:20:53,632] [    INFO][0m - loss: 2.84964943, learning_rate: 9.851851851851852e-06, global_step: 280, interval_runtime: 1.9261, interval_samples_per_second: 4.154, interval_steps_per_second: 5.192, epoch: 0.7407[0m
[32m[2022-09-08 12:20:55,543] [    INFO][0m - loss: 2.40747337, learning_rate: 9.846560846560847e-06, global_step: 290, interval_runtime: 1.911, interval_samples_per_second: 4.186, interval_steps_per_second: 5.233, epoch: 0.7672[0m
[32m[2022-09-08 12:20:57,451] [    INFO][0m - loss: 2.60941124, learning_rate: 9.841269841269842e-06, global_step: 300, interval_runtime: 1.9082, interval_samples_per_second: 4.192, interval_steps_per_second: 5.241, epoch: 0.7937[0m
[32m[2022-09-08 12:20:57,451] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:20:57,451] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-08 12:20:57,451] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:20:57,452] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:20:57,452] [    INFO][0m -   Total prediction steps = 172[0m
[32m[2022-09-08 12:21:12,476] [    INFO][0m - eval_loss: 2.2577850818634033, eval_accuracy: 0.47414422035217285, eval_runtime: 15.0243, eval_samples_per_second: 91.385, eval_steps_per_second: 11.448, epoch: 0.7937[0m
[32m[2022-09-08 12:21:12,530] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-09-08 12:21:12,530] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:21:15,993] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-09-08 12:21:15,994] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-09-08 12:21:23,302] [    INFO][0m - loss: 2.82278633, learning_rate: 9.835978835978837e-06, global_step: 310, interval_runtime: 25.8515, interval_samples_per_second: 0.309, interval_steps_per_second: 0.387, epoch: 0.8201[0m
[32m[2022-09-08 12:21:25,223] [    INFO][0m - loss: 2.14203815, learning_rate: 9.830687830687832e-06, global_step: 320, interval_runtime: 1.9204, interval_samples_per_second: 4.166, interval_steps_per_second: 5.207, epoch: 0.8466[0m
[32m[2022-09-08 12:21:27,131] [    INFO][0m - loss: 2.63118725, learning_rate: 9.825396825396825e-06, global_step: 330, interval_runtime: 1.9087, interval_samples_per_second: 4.191, interval_steps_per_second: 5.239, epoch: 0.873[0m
[32m[2022-09-08 12:21:29,053] [    INFO][0m - loss: 2.43327122, learning_rate: 9.82010582010582e-06, global_step: 340, interval_runtime: 1.9213, interval_samples_per_second: 4.164, interval_steps_per_second: 5.205, epoch: 0.8995[0m
[32m[2022-09-08 12:21:30,960] [    INFO][0m - loss: 2.69274769, learning_rate: 9.814814814814815e-06, global_step: 350, interval_runtime: 1.9063, interval_samples_per_second: 4.197, interval_steps_per_second: 5.246, epoch: 0.9259[0m
[32m[2022-09-08 12:21:32,862] [    INFO][0m - loss: 2.79692993, learning_rate: 9.80952380952381e-06, global_step: 360, interval_runtime: 1.9026, interval_samples_per_second: 4.205, interval_steps_per_second: 5.256, epoch: 0.9524[0m
[32m[2022-09-08 12:21:34,784] [    INFO][0m - loss: 2.37282829, learning_rate: 9.804232804232805e-06, global_step: 370, interval_runtime: 1.922, interval_samples_per_second: 4.162, interval_steps_per_second: 5.203, epoch: 0.9788[0m
[32m[2022-09-08 12:21:36,766] [    INFO][0m - loss: 2.12120171, learning_rate: 9.7989417989418e-06, global_step: 380, interval_runtime: 1.9821, interval_samples_per_second: 4.036, interval_steps_per_second: 5.045, epoch: 1.0053[0m
[32m[2022-09-08 12:21:38,693] [    INFO][0m - loss: 2.18915787, learning_rate: 9.793650793650794e-06, global_step: 390, interval_runtime: 1.9268, interval_samples_per_second: 4.152, interval_steps_per_second: 5.19, epoch: 1.0317[0m
[32m[2022-09-08 12:21:40,610] [    INFO][0m - loss: 1.82749348, learning_rate: 9.788359788359789e-06, global_step: 400, interval_runtime: 1.917, interval_samples_per_second: 4.173, interval_steps_per_second: 5.216, epoch: 1.0582[0m
[32m[2022-09-08 12:21:40,611] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:21:40,611] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-08 12:21:40,611] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:21:40,611] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:21:40,611] [    INFO][0m -   Total prediction steps = 172[0m
[32m[2022-09-08 12:21:55,726] [    INFO][0m - eval_loss: 2.146574020385742, eval_accuracy: 0.5025491714477539, eval_runtime: 15.1141, eval_samples_per_second: 90.842, eval_steps_per_second: 11.38, epoch: 1.0582[0m
[32m[2022-09-08 12:21:55,788] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-09-08 12:21:55,788] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:21:59,037] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-09-08 12:21:59,037] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-09-08 12:22:06,443] [    INFO][0m - loss: 2.02958336, learning_rate: 9.783068783068784e-06, global_step: 410, interval_runtime: 25.8325, interval_samples_per_second: 0.31, interval_steps_per_second: 0.387, epoch: 1.0847[0m
[32m[2022-09-08 12:22:08,368] [    INFO][0m - loss: 2.06587696, learning_rate: 9.777777777777779e-06, global_step: 420, interval_runtime: 1.9249, interval_samples_per_second: 4.156, interval_steps_per_second: 5.195, epoch: 1.1111[0m
[32m[2022-09-08 12:22:10,278] [    INFO][0m - loss: 1.84781265, learning_rate: 9.772486772486774e-06, global_step: 430, interval_runtime: 1.9108, interval_samples_per_second: 4.187, interval_steps_per_second: 5.233, epoch: 1.1376[0m
[32m[2022-09-08 12:22:12,183] [    INFO][0m - loss: 1.79358635, learning_rate: 9.767195767195769e-06, global_step: 440, interval_runtime: 1.9051, interval_samples_per_second: 4.199, interval_steps_per_second: 5.249, epoch: 1.164[0m
[32m[2022-09-08 12:22:14,110] [    INFO][0m - loss: 1.9964262, learning_rate: 9.761904761904762e-06, global_step: 450, interval_runtime: 1.9265, interval_samples_per_second: 4.153, interval_steps_per_second: 5.191, epoch: 1.1905[0m
[32m[2022-09-08 12:22:16,028] [    INFO][0m - loss: 2.0835556, learning_rate: 9.756613756613757e-06, global_step: 460, interval_runtime: 1.9181, interval_samples_per_second: 4.171, interval_steps_per_second: 5.214, epoch: 1.2169[0m
[32m[2022-09-08 12:22:17,917] [    INFO][0m - loss: 2.19991817, learning_rate: 9.751322751322752e-06, global_step: 470, interval_runtime: 1.8897, interval_samples_per_second: 4.233, interval_steps_per_second: 5.292, epoch: 1.2434[0m
[32m[2022-09-08 12:22:19,827] [    INFO][0m - loss: 1.76570206, learning_rate: 9.746031746031747e-06, global_step: 480, interval_runtime: 1.909, interval_samples_per_second: 4.191, interval_steps_per_second: 5.238, epoch: 1.2698[0m
[32m[2022-09-08 12:22:21,715] [    INFO][0m - loss: 2.00716362, learning_rate: 9.740740740740742e-06, global_step: 490, interval_runtime: 1.8883, interval_samples_per_second: 4.237, interval_steps_per_second: 5.296, epoch: 1.2963[0m
[32m[2022-09-08 12:22:23,619] [    INFO][0m - loss: 2.44527435, learning_rate: 9.735449735449735e-06, global_step: 500, interval_runtime: 1.9048, interval_samples_per_second: 4.2, interval_steps_per_second: 5.25, epoch: 1.3228[0m
[32m[2022-09-08 12:22:23,620] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:22:23,620] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-08 12:22:23,620] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:22:23,621] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:22:23,621] [    INFO][0m -   Total prediction steps = 172[0m
[32m[2022-09-08 12:22:39,036] [    INFO][0m - eval_loss: 2.0319225788116455, eval_accuracy: 0.49890750646591187, eval_runtime: 15.4152, eval_samples_per_second: 89.068, eval_steps_per_second: 11.158, epoch: 1.3228[0m
[32m[2022-09-08 12:22:39,087] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-09-08 12:22:39,088] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:22:42,054] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-09-08 12:22:42,054] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-09-08 12:22:50,004] [    INFO][0m - loss: 2.25717182, learning_rate: 9.73015873015873e-06, global_step: 510, interval_runtime: 26.3843, interval_samples_per_second: 0.303, interval_steps_per_second: 0.379, epoch: 1.3492[0m
[32m[2022-09-08 12:22:51,921] [    INFO][0m - loss: 2.34281197, learning_rate: 9.724867724867725e-06, global_step: 520, interval_runtime: 1.9173, interval_samples_per_second: 4.173, interval_steps_per_second: 5.216, epoch: 1.3757[0m
[32m[2022-09-08 12:22:53,834] [    INFO][0m - loss: 1.92357998, learning_rate: 9.71957671957672e-06, global_step: 530, interval_runtime: 1.9129, interval_samples_per_second: 4.182, interval_steps_per_second: 5.228, epoch: 1.4021[0m
[32m[2022-09-08 12:22:55,767] [    INFO][0m - loss: 2.12872677, learning_rate: 9.714285714285715e-06, global_step: 540, interval_runtime: 1.9326, interval_samples_per_second: 4.14, interval_steps_per_second: 5.174, epoch: 1.4286[0m
[32m[2022-09-08 12:22:57,677] [    INFO][0m - loss: 2.00500336, learning_rate: 9.70899470899471e-06, global_step: 550, interval_runtime: 1.9098, interval_samples_per_second: 4.189, interval_steps_per_second: 5.236, epoch: 1.455[0m
[32m[2022-09-08 12:22:59,583] [    INFO][0m - loss: 2.05770416, learning_rate: 9.703703703703703e-06, global_step: 560, interval_runtime: 1.9068, interval_samples_per_second: 4.196, interval_steps_per_second: 5.244, epoch: 1.4815[0m
[32m[2022-09-08 12:23:01,483] [    INFO][0m - loss: 2.01829262, learning_rate: 9.698412698412698e-06, global_step: 570, interval_runtime: 1.9001, interval_samples_per_second: 4.21, interval_steps_per_second: 5.263, epoch: 1.5079[0m
[32m[2022-09-08 12:23:03,394] [    INFO][0m - loss: 1.90628948, learning_rate: 9.693121693121693e-06, global_step: 580, interval_runtime: 1.9106, interval_samples_per_second: 4.187, interval_steps_per_second: 5.234, epoch: 1.5344[0m
[32m[2022-09-08 12:23:05,299] [    INFO][0m - loss: 2.08934536, learning_rate: 9.687830687830688e-06, global_step: 590, interval_runtime: 1.9051, interval_samples_per_second: 4.199, interval_steps_per_second: 5.249, epoch: 1.5608[0m
[32m[2022-09-08 12:23:07,204] [    INFO][0m - loss: 1.90973091, learning_rate: 9.682539682539683e-06, global_step: 600, interval_runtime: 1.9054, interval_samples_per_second: 4.198, interval_steps_per_second: 5.248, epoch: 1.5873[0m
[32m[2022-09-08 12:23:07,205] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:23:07,205] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-08 12:23:07,206] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:23:07,206] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:23:07,206] [    INFO][0m -   Total prediction steps = 172[0m
[32m[2022-09-08 12:23:22,248] [    INFO][0m - eval_loss: 1.8940746784210205, eval_accuracy: 0.5156591534614563, eval_runtime: 15.042, eval_samples_per_second: 91.278, eval_steps_per_second: 11.435, epoch: 1.5873[0m
[32m[2022-09-08 12:23:22,299] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-09-08 12:23:22,300] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:23:25,429] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-09-08 12:23:25,430] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-09-08 12:23:32,583] [    INFO][0m - loss: 2.07648582, learning_rate: 9.677248677248678e-06, global_step: 610, interval_runtime: 25.3788, interval_samples_per_second: 0.315, interval_steps_per_second: 0.394, epoch: 1.6138[0m
[32m[2022-09-08 12:23:34,481] [    INFO][0m - loss: 2.19137344, learning_rate: 9.671957671957672e-06, global_step: 620, interval_runtime: 1.8972, interval_samples_per_second: 4.217, interval_steps_per_second: 5.271, epoch: 1.6402[0m
[32m[2022-09-08 12:23:36,379] [    INFO][0m - loss: 1.72193165, learning_rate: 9.666666666666667e-06, global_step: 630, interval_runtime: 1.8982, interval_samples_per_second: 4.215, interval_steps_per_second: 5.268, epoch: 1.6667[0m
[32m[2022-09-08 12:23:38,296] [    INFO][0m - loss: 1.96762924, learning_rate: 9.661375661375663e-06, global_step: 640, interval_runtime: 1.9169, interval_samples_per_second: 4.173, interval_steps_per_second: 5.217, epoch: 1.6931[0m
[32m[2022-09-08 12:23:40,205] [    INFO][0m - loss: 1.90599461, learning_rate: 9.656084656084657e-06, global_step: 650, interval_runtime: 1.9092, interval_samples_per_second: 4.19, interval_steps_per_second: 5.238, epoch: 1.7196[0m
[32m[2022-09-08 12:23:42,112] [    INFO][0m - loss: 2.29617329, learning_rate: 9.650793650793652e-06, global_step: 660, interval_runtime: 1.9068, interval_samples_per_second: 4.196, interval_steps_per_second: 5.245, epoch: 1.746[0m
[32m[2022-09-08 12:23:43,996] [    INFO][0m - loss: 1.91333733, learning_rate: 9.645502645502647e-06, global_step: 670, interval_runtime: 1.8846, interval_samples_per_second: 4.245, interval_steps_per_second: 5.306, epoch: 1.7725[0m
[32m[2022-09-08 12:23:45,897] [    INFO][0m - loss: 2.16809711, learning_rate: 9.64021164021164e-06, global_step: 680, interval_runtime: 1.9011, interval_samples_per_second: 4.208, interval_steps_per_second: 5.26, epoch: 1.7989[0m
[32m[2022-09-08 12:23:47,806] [    INFO][0m - loss: 1.88380623, learning_rate: 9.634920634920637e-06, global_step: 690, interval_runtime: 1.9092, interval_samples_per_second: 4.19, interval_steps_per_second: 5.238, epoch: 1.8254[0m
[32m[2022-09-08 12:23:49,704] [    INFO][0m - loss: 1.88828754, learning_rate: 9.62962962962963e-06, global_step: 700, interval_runtime: 1.8977, interval_samples_per_second: 4.216, interval_steps_per_second: 5.269, epoch: 1.8519[0m
[32m[2022-09-08 12:23:49,705] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:23:49,705] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-08 12:23:49,705] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:23:49,705] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:23:49,705] [    INFO][0m -   Total prediction steps = 172[0m
[32m[2022-09-08 12:24:04,738] [    INFO][0m - eval_loss: 1.8517003059387207, eval_accuracy: 0.5207574963569641, eval_runtime: 15.0325, eval_samples_per_second: 91.335, eval_steps_per_second: 11.442, epoch: 1.8519[0m
[32m[2022-09-08 12:24:04,789] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-700[0m
[32m[2022-09-08 12:24:04,789] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:24:08,253] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-700/tokenizer_config.json[0m
[32m[2022-09-08 12:24:08,253] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-700/special_tokens_map.json[0m
[32m[2022-09-08 12:24:15,627] [    INFO][0m - loss: 2.18131428, learning_rate: 9.624338624338625e-06, global_step: 710, interval_runtime: 25.9225, interval_samples_per_second: 0.309, interval_steps_per_second: 0.386, epoch: 1.8783[0m
[32m[2022-09-08 12:24:17,529] [    INFO][0m - loss: 1.79611263, learning_rate: 9.61904761904762e-06, global_step: 720, interval_runtime: 1.9027, interval_samples_per_second: 4.205, interval_steps_per_second: 5.256, epoch: 1.9048[0m
[32m[2022-09-08 12:24:19,439] [    INFO][0m - loss: 1.97748165, learning_rate: 9.613756613756613e-06, global_step: 730, interval_runtime: 1.9096, interval_samples_per_second: 4.189, interval_steps_per_second: 5.237, epoch: 1.9312[0m
[32m[2022-09-08 12:24:21,349] [    INFO][0m - loss: 1.78541756, learning_rate: 9.60846560846561e-06, global_step: 740, interval_runtime: 1.9104, interval_samples_per_second: 4.188, interval_steps_per_second: 5.235, epoch: 1.9577[0m
[32m[2022-09-08 12:24:23,264] [    INFO][0m - loss: 1.75250797, learning_rate: 9.603174603174605e-06, global_step: 750, interval_runtime: 1.9145, interval_samples_per_second: 4.179, interval_steps_per_second: 5.223, epoch: 1.9841[0m
[32m[2022-09-08 12:24:25,218] [    INFO][0m - loss: 1.53038626, learning_rate: 9.597883597883598e-06, global_step: 760, interval_runtime: 1.9543, interval_samples_per_second: 4.094, interval_steps_per_second: 5.117, epoch: 2.0106[0m
[32m[2022-09-08 12:24:27,121] [    INFO][0m - loss: 1.35708246, learning_rate: 9.592592592592593e-06, global_step: 770, interval_runtime: 1.9029, interval_samples_per_second: 4.204, interval_steps_per_second: 5.255, epoch: 2.037[0m
[32m[2022-09-08 12:24:29,041] [    INFO][0m - loss: 1.77876091, learning_rate: 9.587301587301588e-06, global_step: 780, interval_runtime: 1.9196, interval_samples_per_second: 4.167, interval_steps_per_second: 5.209, epoch: 2.0635[0m
[32m[2022-09-08 12:24:30,961] [    INFO][0m - loss: 1.82394676, learning_rate: 9.582010582010583e-06, global_step: 790, interval_runtime: 1.9203, interval_samples_per_second: 4.166, interval_steps_per_second: 5.208, epoch: 2.0899[0m
[32m[2022-09-08 12:24:32,850] [    INFO][0m - loss: 1.42094631, learning_rate: 9.576719576719578e-06, global_step: 800, interval_runtime: 1.8894, interval_samples_per_second: 4.234, interval_steps_per_second: 5.293, epoch: 2.1164[0m
[32m[2022-09-08 12:24:32,851] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:24:32,851] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-08 12:24:32,851] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:24:32,851] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:24:32,851] [    INFO][0m -   Total prediction steps = 172[0m
[32m[2022-09-08 12:24:47,875] [    INFO][0m - eval_loss: 1.861772894859314, eval_accuracy: 0.5360524654388428, eval_runtime: 15.023, eval_samples_per_second: 91.393, eval_steps_per_second: 11.449, epoch: 2.1164[0m
[32m[2022-09-08 12:24:47,926] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-800[0m
[32m[2022-09-08 12:24:47,927] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:24:51,226] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-800/tokenizer_config.json[0m
[32m[2022-09-08 12:24:51,226] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-800/special_tokens_map.json[0m
[32m[2022-09-08 12:24:58,431] [    INFO][0m - loss: 1.70917168, learning_rate: 9.571428571428573e-06, global_step: 810, interval_runtime: 25.581, interval_samples_per_second: 0.313, interval_steps_per_second: 0.391, epoch: 2.1429[0m
[32m[2022-09-08 12:25:00,319] [    INFO][0m - loss: 1.64394093, learning_rate: 9.566137566137567e-06, global_step: 820, interval_runtime: 1.8875, interval_samples_per_second: 4.238, interval_steps_per_second: 5.298, epoch: 2.1693[0m
[32m[2022-09-08 12:25:02,209] [    INFO][0m - loss: 1.61799355, learning_rate: 9.560846560846561e-06, global_step: 830, interval_runtime: 1.8898, interval_samples_per_second: 4.233, interval_steps_per_second: 5.292, epoch: 2.1958[0m
[32m[2022-09-08 12:25:04,113] [    INFO][0m - loss: 1.50658226, learning_rate: 9.555555555555556e-06, global_step: 840, interval_runtime: 1.9041, interval_samples_per_second: 4.201, interval_steps_per_second: 5.252, epoch: 2.2222[0m
[32m[2022-09-08 12:25:06,015] [    INFO][0m - loss: 1.57476149, learning_rate: 9.550264550264551e-06, global_step: 850, interval_runtime: 1.902, interval_samples_per_second: 4.206, interval_steps_per_second: 5.258, epoch: 2.2487[0m
[32m[2022-09-08 12:25:07,936] [    INFO][0m - loss: 1.59787235, learning_rate: 9.544973544973546e-06, global_step: 860, interval_runtime: 1.9209, interval_samples_per_second: 4.165, interval_steps_per_second: 5.206, epoch: 2.2751[0m
[32m[2022-09-08 12:25:09,831] [    INFO][0m - loss: 1.7965456, learning_rate: 9.539682539682541e-06, global_step: 870, interval_runtime: 1.8953, interval_samples_per_second: 4.221, interval_steps_per_second: 5.276, epoch: 2.3016[0m
[32m[2022-09-08 12:25:11,736] [    INFO][0m - loss: 1.39805126, learning_rate: 9.534391534391535e-06, global_step: 880, interval_runtime: 1.9045, interval_samples_per_second: 4.201, interval_steps_per_second: 5.251, epoch: 2.328[0m
[32m[2022-09-08 12:25:13,664] [    INFO][0m - loss: 1.46297369, learning_rate: 9.52910052910053e-06, global_step: 890, interval_runtime: 1.9285, interval_samples_per_second: 4.148, interval_steps_per_second: 5.185, epoch: 2.3545[0m
[32m[2022-09-08 12:25:15,574] [    INFO][0m - loss: 1.64184818, learning_rate: 9.523809523809525e-06, global_step: 900, interval_runtime: 1.9101, interval_samples_per_second: 4.188, interval_steps_per_second: 5.235, epoch: 2.381[0m
[32m[2022-09-08 12:25:15,575] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:25:15,575] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-08 12:25:15,575] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:25:15,575] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:25:15,575] [    INFO][0m -   Total prediction steps = 172[0m
[32m[2022-09-08 12:25:30,616] [    INFO][0m - eval_loss: 1.8203773498535156, eval_accuracy: 0.5302258133888245, eval_runtime: 15.0404, eval_samples_per_second: 91.288, eval_steps_per_second: 11.436, epoch: 2.381[0m
[32m[2022-09-08 12:25:30,667] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-900[0m
[32m[2022-09-08 12:25:30,667] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:25:34,111] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-900/tokenizer_config.json[0m
[32m[2022-09-08 12:25:34,111] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-900/special_tokens_map.json[0m
[32m[2022-09-08 12:25:41,481] [    INFO][0m - loss: 1.61579056, learning_rate: 9.51851851851852e-06, global_step: 910, interval_runtime: 25.9066, interval_samples_per_second: 0.309, interval_steps_per_second: 0.386, epoch: 2.4074[0m
[32m[2022-09-08 12:25:43,402] [    INFO][0m - loss: 1.49164391, learning_rate: 9.513227513227515e-06, global_step: 920, interval_runtime: 1.9213, interval_samples_per_second: 4.164, interval_steps_per_second: 5.205, epoch: 2.4339[0m
[32m[2022-09-08 12:25:45,325] [    INFO][0m - loss: 1.68035011, learning_rate: 9.507936507936508e-06, global_step: 930, interval_runtime: 1.9225, interval_samples_per_second: 4.161, interval_steps_per_second: 5.201, epoch: 2.4603[0m
[32m[2022-09-08 12:25:47,224] [    INFO][0m - loss: 1.37821808, learning_rate: 9.502645502645503e-06, global_step: 940, interval_runtime: 1.8997, interval_samples_per_second: 4.211, interval_steps_per_second: 5.264, epoch: 2.4868[0m
[32m[2022-09-08 12:25:49,165] [    INFO][0m - loss: 1.75918713, learning_rate: 9.497354497354498e-06, global_step: 950, interval_runtime: 1.9401, interval_samples_per_second: 4.123, interval_steps_per_second: 5.154, epoch: 2.5132[0m
[32m[2022-09-08 12:25:51,076] [    INFO][0m - loss: 1.59079018, learning_rate: 9.492063492063493e-06, global_step: 960, interval_runtime: 1.9118, interval_samples_per_second: 4.185, interval_steps_per_second: 5.231, epoch: 2.5397[0m
[32m[2022-09-08 12:25:52,985] [    INFO][0m - loss: 1.76317139, learning_rate: 9.486772486772488e-06, global_step: 970, interval_runtime: 1.9082, interval_samples_per_second: 4.192, interval_steps_per_second: 5.24, epoch: 2.5661[0m
[32m[2022-09-08 12:25:54,917] [    INFO][0m - loss: 1.51635723, learning_rate: 9.481481481481483e-06, global_step: 980, interval_runtime: 1.9324, interval_samples_per_second: 4.14, interval_steps_per_second: 5.175, epoch: 2.5926[0m
[32m[2022-09-08 12:25:56,815] [    INFO][0m - loss: 1.56871738, learning_rate: 9.476190476190476e-06, global_step: 990, interval_runtime: 1.8978, interval_samples_per_second: 4.215, interval_steps_per_second: 5.269, epoch: 2.619[0m
[32m[2022-09-08 12:25:58,726] [    INFO][0m - loss: 1.6607132, learning_rate: 9.470899470899471e-06, global_step: 1000, interval_runtime: 1.9117, interval_samples_per_second: 4.185, interval_steps_per_second: 5.231, epoch: 2.6455[0m
[32m[2022-09-08 12:25:58,727] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:25:58,727] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-08 12:25:58,727] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:25:58,727] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:25:58,727] [    INFO][0m -   Total prediction steps = 172[0m
[32m[2022-09-08 12:26:13,721] [    INFO][0m - eval_loss: 1.8226255178451538, eval_accuracy: 0.5243991613388062, eval_runtime: 14.9933, eval_samples_per_second: 91.575, eval_steps_per_second: 11.472, epoch: 2.6455[0m
[32m[2022-09-08 12:26:13,772] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1000[0m
[32m[2022-09-08 12:26:13,772] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:26:16,907] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1000/tokenizer_config.json[0m
[32m[2022-09-08 12:26:16,907] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1000/special_tokens_map.json[0m
[32m[2022-09-08 12:26:24,218] [    INFO][0m - loss: 1.59943104, learning_rate: 9.465608465608466e-06, global_step: 1010, interval_runtime: 25.4917, interval_samples_per_second: 0.314, interval_steps_per_second: 0.392, epoch: 2.672[0m
[32m[2022-09-08 12:26:26,121] [    INFO][0m - loss: 1.60716209, learning_rate: 9.460317460317461e-06, global_step: 1020, interval_runtime: 1.9026, interval_samples_per_second: 4.205, interval_steps_per_second: 5.256, epoch: 2.6984[0m
[32m[2022-09-08 12:26:27,994] [    INFO][0m - loss: 1.76607513, learning_rate: 9.455026455026456e-06, global_step: 1030, interval_runtime: 1.8732, interval_samples_per_second: 4.271, interval_steps_per_second: 5.338, epoch: 2.7249[0m
[32m[2022-09-08 12:26:29,913] [    INFO][0m - loss: 1.7626976, learning_rate: 9.449735449735451e-06, global_step: 1040, interval_runtime: 1.919, interval_samples_per_second: 4.169, interval_steps_per_second: 5.211, epoch: 2.7513[0m
[32m[2022-09-08 12:26:31,809] [    INFO][0m - loss: 1.98573494, learning_rate: 9.444444444444445e-06, global_step: 1050, interval_runtime: 1.8958, interval_samples_per_second: 4.22, interval_steps_per_second: 5.275, epoch: 2.7778[0m
[32m[2022-09-08 12:26:33,734] [    INFO][0m - loss: 1.45000715, learning_rate: 9.43915343915344e-06, global_step: 1060, interval_runtime: 1.9251, interval_samples_per_second: 4.156, interval_steps_per_second: 5.195, epoch: 2.8042[0m
[32m[2022-09-08 12:26:35,635] [    INFO][0m - loss: 1.63542118, learning_rate: 9.433862433862435e-06, global_step: 1070, interval_runtime: 1.9007, interval_samples_per_second: 4.209, interval_steps_per_second: 5.261, epoch: 2.8307[0m
[32m[2022-09-08 12:26:37,539] [    INFO][0m - loss: 1.54185362, learning_rate: 9.42857142857143e-06, global_step: 1080, interval_runtime: 1.9043, interval_samples_per_second: 4.201, interval_steps_per_second: 5.251, epoch: 2.8571[0m
[32m[2022-09-08 12:26:39,434] [    INFO][0m - loss: 1.38976068, learning_rate: 9.423280423280425e-06, global_step: 1090, interval_runtime: 1.8948, interval_samples_per_second: 4.222, interval_steps_per_second: 5.278, epoch: 2.8836[0m
[32m[2022-09-08 12:26:41,345] [    INFO][0m - loss: 1.25565729, learning_rate: 9.417989417989418e-06, global_step: 1100, interval_runtime: 1.9109, interval_samples_per_second: 4.186, interval_steps_per_second: 5.233, epoch: 2.9101[0m
[32m[2022-09-08 12:26:41,345] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:26:41,345] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-08 12:26:41,345] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:26:41,345] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:26:41,345] [    INFO][0m -   Total prediction steps = 172[0m
[32m[2022-09-08 12:26:56,345] [    INFO][0m - eval_loss: 1.8787580728530884, eval_accuracy: 0.5273124575614929, eval_runtime: 14.9993, eval_samples_per_second: 91.537, eval_steps_per_second: 11.467, epoch: 2.9101[0m
[32m[2022-09-08 12:26:56,396] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1100[0m
[32m[2022-09-08 12:26:56,396] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:26:59,530] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1100/tokenizer_config.json[0m
[32m[2022-09-08 12:26:59,530] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1100/special_tokens_map.json[0m
[32m[2022-09-08 12:27:06,770] [    INFO][0m - loss: 1.55011444, learning_rate: 9.412698412698413e-06, global_step: 1110, interval_runtime: 25.4254, interval_samples_per_second: 0.315, interval_steps_per_second: 0.393, epoch: 2.9365[0m
[32m[2022-09-08 12:27:08,668] [    INFO][0m - loss: 1.66138344, learning_rate: 9.407407407407408e-06, global_step: 1120, interval_runtime: 1.8981, interval_samples_per_second: 4.215, interval_steps_per_second: 5.269, epoch: 2.963[0m
[32m[2022-09-08 12:27:10,565] [    INFO][0m - loss: 1.4090456, learning_rate: 9.402116402116403e-06, global_step: 1130, interval_runtime: 1.8969, interval_samples_per_second: 4.217, interval_steps_per_second: 5.272, epoch: 2.9894[0m
[32m[2022-09-08 12:27:12,513] [    INFO][0m - loss: 1.50712519, learning_rate: 9.396825396825398e-06, global_step: 1140, interval_runtime: 1.9478, interval_samples_per_second: 4.107, interval_steps_per_second: 5.134, epoch: 3.0159[0m
[32m[2022-09-08 12:27:14,440] [    INFO][0m - loss: 1.13706865, learning_rate: 9.391534391534393e-06, global_step: 1150, interval_runtime: 1.9277, interval_samples_per_second: 4.15, interval_steps_per_second: 5.188, epoch: 3.0423[0m
[32m[2022-09-08 12:27:16,338] [    INFO][0m - loss: 1.33843307, learning_rate: 9.386243386243386e-06, global_step: 1160, interval_runtime: 1.8977, interval_samples_per_second: 4.216, interval_steps_per_second: 5.269, epoch: 3.0688[0m
[32m[2022-09-08 12:27:18,243] [    INFO][0m - loss: 1.43319511, learning_rate: 9.380952380952381e-06, global_step: 1170, interval_runtime: 1.9049, interval_samples_per_second: 4.2, interval_steps_per_second: 5.249, epoch: 3.0952[0m
[32m[2022-09-08 12:27:20,156] [    INFO][0m - loss: 1.38846588, learning_rate: 9.375661375661376e-06, global_step: 1180, interval_runtime: 1.9131, interval_samples_per_second: 4.182, interval_steps_per_second: 5.227, epoch: 3.1217[0m
[32m[2022-09-08 12:27:22,059] [    INFO][0m - loss: 1.42943668, learning_rate: 9.370370370370371e-06, global_step: 1190, interval_runtime: 1.9027, interval_samples_per_second: 4.205, interval_steps_per_second: 5.256, epoch: 3.1481[0m
[32m[2022-09-08 12:27:23,978] [    INFO][0m - loss: 1.24835701, learning_rate: 9.365079365079366e-06, global_step: 1200, interval_runtime: 1.9192, interval_samples_per_second: 4.168, interval_steps_per_second: 5.21, epoch: 3.1746[0m
[32m[2022-09-08 12:27:23,979] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:27:23,979] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-08 12:27:23,979] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:27:23,979] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:27:23,979] [    INFO][0m -   Total prediction steps = 172[0m
[32m[2022-09-08 12:27:39,000] [    INFO][0m - eval_loss: 1.856068730354309, eval_accuracy: 0.5185725092887878, eval_runtime: 15.0206, eval_samples_per_second: 91.408, eval_steps_per_second: 11.451, epoch: 3.1746[0m
[32m[2022-09-08 12:27:39,066] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1200[0m
[32m[2022-09-08 12:27:39,067] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:27:42,225] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1200/tokenizer_config.json[0m
[32m[2022-09-08 12:27:42,226] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1200/special_tokens_map.json[0m
[32m[2022-09-08 12:27:47,683] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-08 12:27:47,683] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-800 (score: 0.5360524654388428).[0m
[32m[2022-09-08 12:27:48,603] [    INFO][0m - train_runtime: 517.3905, train_samples_per_second: 292.236, train_steps_per_second: 36.529, train_loss: 2.560319135983785, epoch: 3.1746[0m
[32m[2022-09-08 12:27:48,645] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-09-08 12:27:48,645] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:27:52,042] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-09-08 12:27:52,043] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-09-08 12:27:52,045] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-08 12:27:52,045] [    INFO][0m -   epoch                    =     3.1746[0m
[32m[2022-09-08 12:27:52,045] [    INFO][0m -   train_loss               =     2.5603[0m
[32m[2022-09-08 12:27:52,045] [    INFO][0m -   train_runtime            = 0:08:37.39[0m
[32m[2022-09-08 12:27:52,045] [    INFO][0m -   train_samples_per_second =    292.236[0m
[32m[2022-09-08 12:27:52,045] [    INFO][0m -   train_steps_per_second   =     36.529[0m
[32m[2022-09-08 12:27:52,051] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-08 12:27:52,051] [    INFO][0m -   Num examples = 1749[0m
[32m[2022-09-08 12:27:52,051] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:27:52,051] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:27:52,051] [    INFO][0m -   Total prediction steps = 219[0m
[32m[2022-09-08 12:28:11,570] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-08 12:28:11,571] [    INFO][0m -   test_accuracy           =     0.5306[0m
[32m[2022-09-08 12:28:11,571] [    INFO][0m -   test_loss               =     1.8437[0m
[32m[2022-09-08 12:28:11,571] [    INFO][0m -   test_runtime            = 0:00:19.51[0m
[32m[2022-09-08 12:28:11,571] [    INFO][0m -   test_samples_per_second =     89.602[0m
[32m[2022-09-08 12:28:11,571] [    INFO][0m -   test_steps_per_second   =      11.22[0m
[32m[2022-09-08 12:28:11,572] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-08 12:28:11,572] [    INFO][0m -   Num examples = 2600[0m
[32m[2022-09-08 12:28:11,572] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:28:11,572] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:28:11,572] [    INFO][0m -   Total prediction steps = 325[0m
Traceback (most recent call last):
  File "train_single.py", line 268, in <module>
    main(0)
  File "train_single.py", line 262, in main
    test_ret = postprocess(test_ret, test_ds, data_args.task_name,
  File "/ssd2/wanghuijuan03/prompt/PaddleNLP/examples/benchmark/fewclue/pretrain15/postprocess.py", line 75, in postprocess
    "label": str(remap[id_to_label[preds[idx]]])
TypeError: unhashable type: 'numpy.ndarray'
run.sh: line 59: --freeze_plm: command not found
 
==========
ocnli
==========
 
[33m[2022-09-08 12:28:47,321] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-09-08 12:28:47,321] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-08 12:28:47,322] [    INFO][0m - ============================================================[0m
[32m[2022-09-08 12:28:47,322] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-08 12:28:47,322] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-08 12:28:47,322] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-08 12:28:47,322] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-08 12:28:47,322] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-08 12:28:47,322] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-08 12:28:47,322] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-09-08 12:28:47,322] [    INFO][0m - [0m
[32m[2022-09-08 12:28:47,322] [    INFO][0m - ============================================================[0m
[32m[2022-09-08 12:28:47,323] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-08 12:28:47,323] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-08 12:28:47,323] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-08 12:28:47,323] [    INFO][0m - prompt                        :[0m
[32m[2022-09-08 12:28:47,323] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-08 12:28:47,323] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-08 12:28:47,323] [    INFO][0m - task_name                     :ocnli[0m
[32m[2022-09-08 12:28:47,323] [    INFO][0m - [0m
[32m[2022-09-08 12:28:47,323] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0908 12:28:47.324884 30536 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0908 12:28:47.328781 30536 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-08 12:28:50,231] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-09-08 12:28:50,255] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-09-08 12:28:50,255] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[32m[2022-09-08 12:28:51,389] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùÂíå‚Äú'}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': '‚ÄùËøô‰∏§Âè•ËØù‰πãÈó¥ÁöÑÈÄªËæëÂÖ≥Á≥ªÊòØ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}][0m
[32m[2022-09-08 12:28:51,520] [    INFO][0m - ============================================================[0m
[32m[2022-09-08 12:28:51,520] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-08 12:28:51,520] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-08 12:28:51,520] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-08 12:28:51,520] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-08 12:28:51,520] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-08 12:28:51,521] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-08 12:28:51,521] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-08 12:28:51,521] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-08 12:28:51,521] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-08 12:28:51,521] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-08 12:28:51,521] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-08 12:28:51,521] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-08 12:28:51,521] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-08 12:28:51,521] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-08 12:28:51,521] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-08 12:28:51,521] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-08 12:28:51,521] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-08 12:28:51,522] [    INFO][0m - eval_batch_size               :8[0m
[32m[2022-09-08 12:28:51,522] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-09-08 12:28:51,522] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-09-08 12:28:51,522] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-08 12:28:51,522] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-08 12:28:51,522] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-08 12:28:51,522] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-08 12:28:51,522] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-08 12:28:51,522] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-08 12:28:51,522] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-08 12:28:51,522] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-08 12:28:51,522] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-08 12:28:51,522] [    INFO][0m - learning_rate                 :1e-05[0m
[32m[2022-09-08 12:28:51,523] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-08 12:28:51,523] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-08 12:28:51,523] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-08 12:28:51,523] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-08 12:28:51,523] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-08 12:28:51,523] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-08 12:28:51,523] [    INFO][0m - logging_dir                   :./checkpoints/runs/Sep08_12-28-47_instance-3bwob41y-01[0m
[32m[2022-09-08 12:28:51,523] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-08 12:28:51,523] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-08 12:28:51,523] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-08 12:28:51,523] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-08 12:28:51,523] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-08 12:28:51,523] [    INFO][0m - max_seq_length                :128[0m
[32m[2022-09-08 12:28:51,523] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-08 12:28:51,524] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-08 12:28:51,524] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-08 12:28:51,524] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-08 12:28:51,524] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-09-08 12:28:51,524] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-08 12:28:51,524] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-08 12:28:51,524] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-09-08 12:28:51,524] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-08 12:28:51,524] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-08 12:28:51,524] [    INFO][0m - per_device_eval_batch_size    :8[0m
[32m[2022-09-08 12:28:51,524] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-09-08 12:28:51,524] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-08 12:28:51,524] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-08 12:28:51,525] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-08 12:28:51,525] [    INFO][0m - ppt_learning_rate             :0.0001[0m
[32m[2022-09-08 12:28:51,525] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-08 12:28:51,525] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-08 12:28:51,525] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-08 12:28:51,525] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-08 12:28:51,525] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-08 12:28:51,525] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-08 12:28:51,525] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-09-08 12:28:51,525] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-08 12:28:51,525] [    INFO][0m - save_steps                    :100[0m
[32m[2022-09-08 12:28:51,525] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-09-08 12:28:51,525] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-08 12:28:51,525] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-08 12:28:51,526] [    INFO][0m - seed                          :42[0m
[32m[2022-09-08 12:28:51,526] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-08 12:28:51,526] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-08 12:28:51,526] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-08 12:28:51,526] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-09-08 12:28:51,526] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-08 12:28:51,526] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-08 12:28:51,526] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-08 12:28:51,526] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-08 12:28:51,526] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-08 12:28:51,526] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-08 12:28:51,526] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-08 12:28:51,526] [    INFO][0m - [0m
[32m[2022-09-08 12:28:51,528] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-08 12:28:51,528] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-08 12:28:51,528] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-09-08 12:28:51,528] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-09-08 12:28:51,528] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-09-08 12:28:51,528] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-08 12:28:51,529] [    INFO][0m -   Total optimization steps = 1000.0[0m
[32m[2022-09-08 12:28:51,529] [    INFO][0m -   Total num train samples = 8000[0m
[32m[2022-09-08 12:28:53,410] [    INFO][0m - loss: 15.59509125, learning_rate: 9.9e-06, global_step: 10, interval_runtime: 1.8812, interval_samples_per_second: 4.253, interval_steps_per_second: 5.316, epoch: 0.5[0m
[32m[2022-09-08 12:28:54,221] [    INFO][0m - loss: 2.62115765, learning_rate: 9.800000000000001e-06, global_step: 20, interval_runtime: 0.8107, interval_samples_per_second: 9.868, interval_steps_per_second: 12.335, epoch: 1.0[0m
[32m[2022-09-08 12:28:55,085] [    INFO][0m - loss: 1.29916601, learning_rate: 9.7e-06, global_step: 30, interval_runtime: 0.8637, interval_samples_per_second: 9.262, interval_steps_per_second: 11.578, epoch: 1.5[0m
[32m[2022-09-08 12:28:55,899] [    INFO][0m - loss: 1.26751413, learning_rate: 9.600000000000001e-06, global_step: 40, interval_runtime: 0.8137, interval_samples_per_second: 9.831, interval_steps_per_second: 12.289, epoch: 2.0[0m
[32m[2022-09-08 12:28:56,763] [    INFO][0m - loss: 1.12566299, learning_rate: 9.5e-06, global_step: 50, interval_runtime: 0.864, interval_samples_per_second: 9.26, interval_steps_per_second: 11.574, epoch: 2.5[0m
[32m[2022-09-08 12:28:57,575] [    INFO][0m - loss: 1.06491652, learning_rate: 9.4e-06, global_step: 60, interval_runtime: 0.8126, interval_samples_per_second: 9.845, interval_steps_per_second: 12.306, epoch: 3.0[0m
[32m[2022-09-08 12:28:58,438] [    INFO][0m - loss: 0.99551754, learning_rate: 9.3e-06, global_step: 70, interval_runtime: 0.8623, interval_samples_per_second: 9.278, interval_steps_per_second: 11.597, epoch: 3.5[0m
[32m[2022-09-08 12:28:59,250] [    INFO][0m - loss: 0.99738731, learning_rate: 9.200000000000002e-06, global_step: 80, interval_runtime: 0.8119, interval_samples_per_second: 9.853, interval_steps_per_second: 12.316, epoch: 4.0[0m
[32m[2022-09-08 12:29:00,118] [    INFO][0m - loss: 0.93306522, learning_rate: 9.100000000000001e-06, global_step: 90, interval_runtime: 0.8687, interval_samples_per_second: 9.209, interval_steps_per_second: 11.511, epoch: 4.5[0m
[32m[2022-09-08 12:29:00,937] [    INFO][0m - loss: 0.89612446, learning_rate: 9e-06, global_step: 100, interval_runtime: 0.818, interval_samples_per_second: 9.78, interval_steps_per_second: 12.225, epoch: 5.0[0m
[32m[2022-09-08 12:29:00,937] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:29:00,937] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-08 12:29:00,937] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:29:00,937] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:29:00,938] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-08 12:29:01,723] [    INFO][0m - eval_loss: 1.0120095014572144, eval_accuracy: 0.550000011920929, eval_runtime: 0.785, eval_samples_per_second: 203.834, eval_steps_per_second: 25.479, epoch: 5.0[0m
[32m[2022-09-08 12:29:01,729] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-09-08 12:29:01,729] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:29:04,941] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-08 12:29:04,941] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-08 12:29:11,018] [    INFO][0m - loss: 0.80148172, learning_rate: 8.900000000000001e-06, global_step: 110, interval_runtime: 10.0808, interval_samples_per_second: 0.794, interval_steps_per_second: 0.992, epoch: 5.5[0m
[32m[2022-09-08 12:29:11,839] [    INFO][0m - loss: 0.67521152, learning_rate: 8.8e-06, global_step: 120, interval_runtime: 0.8214, interval_samples_per_second: 9.74, interval_steps_per_second: 12.175, epoch: 6.0[0m
[32m[2022-09-08 12:29:12,706] [    INFO][0m - loss: 0.60584235, learning_rate: 8.700000000000001e-06, global_step: 130, interval_runtime: 0.8675, interval_samples_per_second: 9.222, interval_steps_per_second: 11.528, epoch: 6.5[0m
[32m[2022-09-08 12:29:13,521] [    INFO][0m - loss: 0.51249466, learning_rate: 8.6e-06, global_step: 140, interval_runtime: 0.8147, interval_samples_per_second: 9.82, interval_steps_per_second: 12.275, epoch: 7.0[0m
[32m[2022-09-08 12:29:14,388] [    INFO][0m - loss: 0.44073267, learning_rate: 8.5e-06, global_step: 150, interval_runtime: 0.8675, interval_samples_per_second: 9.222, interval_steps_per_second: 11.527, epoch: 7.5[0m
[32m[2022-09-08 12:29:15,202] [    INFO][0m - loss: 0.38501513, learning_rate: 8.400000000000001e-06, global_step: 160, interval_runtime: 0.8137, interval_samples_per_second: 9.831, interval_steps_per_second: 12.289, epoch: 8.0[0m
[32m[2022-09-08 12:29:16,064] [    INFO][0m - loss: 0.22816501, learning_rate: 8.3e-06, global_step: 170, interval_runtime: 0.8619, interval_samples_per_second: 9.282, interval_steps_per_second: 11.602, epoch: 8.5[0m
[32m[2022-09-08 12:29:16,877] [    INFO][0m - loss: 0.27660234, learning_rate: 8.2e-06, global_step: 180, interval_runtime: 0.813, interval_samples_per_second: 9.84, interval_steps_per_second: 12.3, epoch: 9.0[0m
[32m[2022-09-08 12:29:17,737] [    INFO][0m - loss: 0.19538403, learning_rate: 8.1e-06, global_step: 190, interval_runtime: 0.8603, interval_samples_per_second: 9.299, interval_steps_per_second: 11.624, epoch: 9.5[0m
[32m[2022-09-08 12:29:18,550] [    INFO][0m - loss: 0.09601278, learning_rate: 8.000000000000001e-06, global_step: 200, interval_runtime: 0.8129, interval_samples_per_second: 9.841, interval_steps_per_second: 12.302, epoch: 10.0[0m
[32m[2022-09-08 12:29:18,551] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:29:18,551] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-08 12:29:18,551] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:29:18,551] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:29:18,551] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-08 12:29:19,329] [    INFO][0m - eval_loss: 1.9799654483795166, eval_accuracy: 0.5375000238418579, eval_runtime: 0.7779, eval_samples_per_second: 205.681, eval_steps_per_second: 25.71, epoch: 10.0[0m
[32m[2022-09-08 12:29:19,335] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-09-08 12:29:19,336] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:29:22,926] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-09-08 12:29:22,926] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-09-08 12:29:29,632] [    INFO][0m - loss: 0.09349056, learning_rate: 7.9e-06, global_step: 210, interval_runtime: 11.0818, interval_samples_per_second: 0.722, interval_steps_per_second: 0.902, epoch: 10.5[0m
[32m[2022-09-08 12:29:30,446] [    INFO][0m - loss: 0.1037262, learning_rate: 7.800000000000002e-06, global_step: 220, interval_runtime: 0.8137, interval_samples_per_second: 9.831, interval_steps_per_second: 12.289, epoch: 11.0[0m
[32m[2022-09-08 12:29:31,311] [    INFO][0m - loss: 0.02666553, learning_rate: 7.7e-06, global_step: 230, interval_runtime: 0.8657, interval_samples_per_second: 9.241, interval_steps_per_second: 11.552, epoch: 11.5[0m
[32m[2022-09-08 12:29:32,123] [    INFO][0m - loss: 0.01851638, learning_rate: 7.600000000000001e-06, global_step: 240, interval_runtime: 0.8114, interval_samples_per_second: 9.859, interval_steps_per_second: 12.324, epoch: 12.0[0m
[32m[2022-09-08 12:29:32,984] [    INFO][0m - loss: 0.1061788, learning_rate: 7.500000000000001e-06, global_step: 250, interval_runtime: 0.8608, interval_samples_per_second: 9.293, interval_steps_per_second: 11.617, epoch: 12.5[0m
[32m[2022-09-08 12:29:33,800] [    INFO][0m - loss: 0.02283975, learning_rate: 7.4e-06, global_step: 260, interval_runtime: 0.8162, interval_samples_per_second: 9.802, interval_steps_per_second: 12.252, epoch: 13.0[0m
[32m[2022-09-08 12:29:34,669] [    INFO][0m - loss: 0.0045032, learning_rate: 7.3e-06, global_step: 270, interval_runtime: 0.8681, interval_samples_per_second: 9.215, interval_steps_per_second: 11.519, epoch: 13.5[0m
[32m[2022-09-08 12:29:35,480] [    INFO][0m - loss: 0.01975243, learning_rate: 7.2000000000000005e-06, global_step: 280, interval_runtime: 0.8122, interval_samples_per_second: 9.85, interval_steps_per_second: 12.312, epoch: 14.0[0m
[32m[2022-09-08 12:29:36,350] [    INFO][0m - loss: 0.00494473, learning_rate: 7.100000000000001e-06, global_step: 290, interval_runtime: 0.8697, interval_samples_per_second: 9.199, interval_steps_per_second: 11.499, epoch: 14.5[0m
[32m[2022-09-08 12:29:37,163] [    INFO][0m - loss: 0.05504018, learning_rate: 7e-06, global_step: 300, interval_runtime: 0.8133, interval_samples_per_second: 9.836, interval_steps_per_second: 12.295, epoch: 15.0[0m
[32m[2022-09-08 12:29:37,164] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:29:37,164] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-08 12:29:37,164] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:29:37,164] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:29:37,164] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-08 12:29:37,986] [    INFO][0m - eval_loss: 3.118520736694336, eval_accuracy: 0.543749988079071, eval_runtime: 0.8214, eval_samples_per_second: 194.784, eval_steps_per_second: 24.348, epoch: 15.0[0m
[32m[2022-09-08 12:29:37,992] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-09-08 12:29:37,993] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:29:41,156] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-09-08 12:29:41,157] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-09-08 12:29:47,583] [    INFO][0m - loss: 0.00118238, learning_rate: 6.9e-06, global_step: 310, interval_runtime: 10.4178, interval_samples_per_second: 0.768, interval_steps_per_second: 0.96, epoch: 15.5[0m
[32m[2022-09-08 12:29:48,399] [    INFO][0m - loss: 0.00489893, learning_rate: 6.800000000000001e-06, global_step: 320, interval_runtime: 0.8181, interval_samples_per_second: 9.779, interval_steps_per_second: 12.224, epoch: 16.0[0m
[32m[2022-09-08 12:29:49,268] [    INFO][0m - loss: 0.00319377, learning_rate: 6.700000000000001e-06, global_step: 330, interval_runtime: 0.8689, interval_samples_per_second: 9.207, interval_steps_per_second: 11.509, epoch: 16.5[0m
[32m[2022-09-08 12:29:50,081] [    INFO][0m - loss: 0.00961226, learning_rate: 6.600000000000001e-06, global_step: 340, interval_runtime: 0.8128, interval_samples_per_second: 9.842, interval_steps_per_second: 12.303, epoch: 17.0[0m
[32m[2022-09-08 12:29:50,945] [    INFO][0m - loss: 0.00168502, learning_rate: 6.5000000000000004e-06, global_step: 350, interval_runtime: 0.8642, interval_samples_per_second: 9.257, interval_steps_per_second: 11.571, epoch: 17.5[0m
[32m[2022-09-08 12:29:51,766] [    INFO][0m - loss: 0.00115616, learning_rate: 6.4000000000000006e-06, global_step: 360, interval_runtime: 0.8214, interval_samples_per_second: 9.739, interval_steps_per_second: 12.174, epoch: 18.0[0m
[32m[2022-09-08 12:29:52,635] [    INFO][0m - loss: 0.05315636, learning_rate: 6.300000000000001e-06, global_step: 370, interval_runtime: 0.8687, interval_samples_per_second: 9.209, interval_steps_per_second: 11.512, epoch: 18.5[0m
[32m[2022-09-08 12:29:53,453] [    INFO][0m - loss: 0.00839455, learning_rate: 6.200000000000001e-06, global_step: 380, interval_runtime: 0.8183, interval_samples_per_second: 9.776, interval_steps_per_second: 12.22, epoch: 19.0[0m
[32m[2022-09-08 12:29:54,321] [    INFO][0m - loss: 0.06804714, learning_rate: 6.1e-06, global_step: 390, interval_runtime: 0.8678, interval_samples_per_second: 9.218, interval_steps_per_second: 11.523, epoch: 19.5[0m
[32m[2022-09-08 12:29:55,142] [    INFO][0m - loss: 0.00678699, learning_rate: 6e-06, global_step: 400, interval_runtime: 0.8204, interval_samples_per_second: 9.751, interval_steps_per_second: 12.189, epoch: 20.0[0m
[32m[2022-09-08 12:29:55,142] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:29:55,142] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-08 12:29:55,142] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:29:55,143] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:29:55,143] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-08 12:29:55,919] [    INFO][0m - eval_loss: 3.583256244659424, eval_accuracy: 0.543749988079071, eval_runtime: 0.7763, eval_samples_per_second: 206.109, eval_steps_per_second: 25.764, epoch: 20.0[0m
[32m[2022-09-08 12:29:55,926] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-09-08 12:29:55,926] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:29:59,413] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-09-08 12:29:59,414] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-09-08 12:30:06,003] [    INFO][0m - loss: 0.01761708, learning_rate: 5.9e-06, global_step: 410, interval_runtime: 10.8615, interval_samples_per_second: 0.737, interval_steps_per_second: 0.921, epoch: 20.5[0m
[32m[2022-09-08 12:30:06,824] [    INFO][0m - loss: 0.00082476, learning_rate: 5.8e-06, global_step: 420, interval_runtime: 0.821, interval_samples_per_second: 9.744, interval_steps_per_second: 12.18, epoch: 21.0[0m
[32m[2022-09-08 12:30:07,695] [    INFO][0m - loss: 0.03466258, learning_rate: 5.7e-06, global_step: 430, interval_runtime: 0.8712, interval_samples_per_second: 9.182, interval_steps_per_second: 11.478, epoch: 21.5[0m
[32m[2022-09-08 12:30:08,510] [    INFO][0m - loss: 0.00294808, learning_rate: 5.600000000000001e-06, global_step: 440, interval_runtime: 0.8148, interval_samples_per_second: 9.818, interval_steps_per_second: 12.272, epoch: 22.0[0m
[32m[2022-09-08 12:30:09,393] [    INFO][0m - loss: 0.00040273, learning_rate: 5.500000000000001e-06, global_step: 450, interval_runtime: 0.8827, interval_samples_per_second: 9.063, interval_steps_per_second: 11.328, epoch: 22.5[0m
[32m[2022-09-08 12:30:10,216] [    INFO][0m - loss: 0.00064667, learning_rate: 5.400000000000001e-06, global_step: 460, interval_runtime: 0.8234, interval_samples_per_second: 9.716, interval_steps_per_second: 12.145, epoch: 23.0[0m
[32m[2022-09-08 12:30:11,086] [    INFO][0m - loss: 0.01833931, learning_rate: 5.300000000000001e-06, global_step: 470, interval_runtime: 0.8701, interval_samples_per_second: 9.195, interval_steps_per_second: 11.493, epoch: 23.5[0m
[32m[2022-09-08 12:30:11,899] [    INFO][0m - loss: 0.00054972, learning_rate: 5.2e-06, global_step: 480, interval_runtime: 0.8124, interval_samples_per_second: 9.848, interval_steps_per_second: 12.31, epoch: 24.0[0m
[32m[2022-09-08 12:30:12,763] [    INFO][0m - loss: 0.00048921, learning_rate: 5.1e-06, global_step: 490, interval_runtime: 0.8645, interval_samples_per_second: 9.254, interval_steps_per_second: 11.568, epoch: 24.5[0m
[32m[2022-09-08 12:30:13,578] [    INFO][0m - loss: 0.00197146, learning_rate: 5e-06, global_step: 500, interval_runtime: 0.8144, interval_samples_per_second: 9.823, interval_steps_per_second: 12.279, epoch: 25.0[0m
[32m[2022-09-08 12:30:13,579] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:30:13,579] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-08 12:30:13,579] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:30:13,579] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:30:13,579] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-08 12:30:14,355] [    INFO][0m - eval_loss: 3.7294259071350098, eval_accuracy: 0.550000011920929, eval_runtime: 0.7758, eval_samples_per_second: 206.226, eval_steps_per_second: 25.778, epoch: 25.0[0m
[32m[2022-09-08 12:30:14,362] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-09-08 12:30:14,362] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:30:17,612] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-09-08 12:30:17,612] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-09-08 12:30:23,109] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-08 12:30:23,110] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-100 (score: 0.550000011920929).[0m
[32m[2022-09-08 12:30:24,090] [    INFO][0m - train_runtime: 92.5606, train_samples_per_second: 86.43, train_steps_per_second: 10.804, train_loss: 0.6340953642446547, epoch: 25.0[0m
[32m[2022-09-08 12:30:24,136] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-09-08 12:30:24,136] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:30:27,590] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-09-08 12:30:27,590] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-09-08 12:30:27,591] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-08 12:30:27,592] [    INFO][0m -   epoch                    =       25.0[0m
[32m[2022-09-08 12:30:27,592] [    INFO][0m -   train_loss               =     0.6341[0m
[32m[2022-09-08 12:30:27,592] [    INFO][0m -   train_runtime            = 0:01:32.56[0m
[32m[2022-09-08 12:30:27,592] [    INFO][0m -   train_samples_per_second =      86.43[0m
[32m[2022-09-08 12:30:27,592] [    INFO][0m -   train_steps_per_second   =     10.804[0m
[32m[2022-09-08 12:30:27,595] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-08 12:30:27,595] [    INFO][0m -   Num examples = 2520[0m
[32m[2022-09-08 12:30:27,595] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:30:27,595] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:30:27,596] [    INFO][0m -   Total prediction steps = 315[0m
[32m[2022-09-08 12:30:41,001] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-08 12:30:41,002] [    INFO][0m -   test_accuracy           =     0.4956[0m
[32m[2022-09-08 12:30:41,002] [    INFO][0m -   test_loss               =     1.0705[0m
[32m[2022-09-08 12:30:41,002] [    INFO][0m -   test_runtime            = 0:00:13.40[0m
[32m[2022-09-08 12:30:41,002] [    INFO][0m -   test_samples_per_second =    187.975[0m
[32m[2022-09-08 12:30:41,002] [    INFO][0m -   test_steps_per_second   =     23.497[0m
[32m[2022-09-08 12:30:41,003] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-08 12:30:41,003] [    INFO][0m -   Num examples = 3000[0m
[32m[2022-09-08 12:30:41,003] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:30:41,003] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:30:41,003] [    INFO][0m -   Total prediction steps = 375[0m
Traceback (most recent call last):
  File "train_single.py", line 268, in <module>
    main(0)
  File "train_single.py", line 262, in main
    test_ret = postprocess(test_ret, test_ds, data_args.task_name,
  File "/ssd2/wanghuijuan03/prompt/PaddleNLP/examples/benchmark/fewclue/pretrain15/postprocess.py", line 66, in postprocess
    ret_list.append({"id": uid, "label": id_to_label[preds[idx]]})
TypeError: unhashable type: 'numpy.ndarray'
run.sh: line 59: --freeze_plm: command not found
 
==========
bustm
==========
 
[33m[2022-09-08 12:31:02,830] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-09-08 12:31:02,830] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-08 12:31:02,830] [    INFO][0m - ============================================================[0m
[32m[2022-09-08 12:31:02,830] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-08 12:31:02,830] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-08 12:31:02,831] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-08 12:31:02,831] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-08 12:31:02,831] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-08 12:31:02,831] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-08 12:31:02,831] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-09-08 12:31:02,831] [    INFO][0m - [0m
[32m[2022-09-08 12:31:02,831] [    INFO][0m - ============================================================[0m
[32m[2022-09-08 12:31:02,831] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-08 12:31:02,831] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-08 12:31:02,831] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-08 12:31:02,831] [    INFO][0m - prompt                        :[0m
[32m[2022-09-08 12:31:02,831] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-08 12:31:02,831] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-08 12:31:02,832] [    INFO][0m - task_name                     :bustm[0m
[32m[2022-09-08 12:31:02,832] [    INFO][0m - [0m
[32m[2022-09-08 12:31:02,832] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0908 12:31:02.833400 32715 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0908 12:31:02.837302 32715 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-08 12:31:05,731] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-09-08 12:31:05,757] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-09-08 12:31:05,757] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[32m[2022-09-08 12:31:06,729] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùÂíå‚Äú'}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': '‚ÄùÊèèËø∞ÁöÑÊòØ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'ÁöÑ‰∫ãÊÉÖ„ÄÇ'}][0m
[32m[2022-09-08 12:31:06,839] [    INFO][0m - ============================================================[0m
[32m[2022-09-08 12:31:06,839] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-08 12:31:06,839] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-08 12:31:06,840] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-08 12:31:06,840] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-08 12:31:06,840] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-08 12:31:06,840] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-08 12:31:06,840] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-08 12:31:06,840] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-08 12:31:06,840] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-08 12:31:06,840] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-08 12:31:06,840] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-08 12:31:06,840] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-08 12:31:06,840] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-08 12:31:06,840] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-08 12:31:06,841] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-08 12:31:06,841] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-08 12:31:06,841] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-08 12:31:06,841] [    INFO][0m - eval_batch_size               :8[0m
[32m[2022-09-08 12:31:06,841] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-09-08 12:31:06,841] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-09-08 12:31:06,841] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-08 12:31:06,841] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-08 12:31:06,841] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-08 12:31:06,841] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-08 12:31:06,841] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-08 12:31:06,841] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-08 12:31:06,841] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-08 12:31:06,842] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-08 12:31:06,842] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-08 12:31:06,842] [    INFO][0m - learning_rate                 :1e-05[0m
[32m[2022-09-08 12:31:06,842] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-08 12:31:06,842] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-08 12:31:06,842] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-08 12:31:06,842] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-08 12:31:06,842] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-08 12:31:06,842] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-08 12:31:06,842] [    INFO][0m - logging_dir                   :./checkpoints/runs/Sep08_12-31-02_instance-3bwob41y-01[0m
[32m[2022-09-08 12:31:06,842] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-08 12:31:06,842] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-08 12:31:06,842] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-08 12:31:06,843] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-08 12:31:06,843] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-08 12:31:06,843] [    INFO][0m - max_seq_length                :64[0m
[32m[2022-09-08 12:31:06,843] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-08 12:31:06,843] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-08 12:31:06,843] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-08 12:31:06,843] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-08 12:31:06,843] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-09-08 12:31:06,843] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-08 12:31:06,843] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-08 12:31:06,843] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-09-08 12:31:06,843] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-08 12:31:06,843] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-08 12:31:06,843] [    INFO][0m - per_device_eval_batch_size    :8[0m
[32m[2022-09-08 12:31:06,844] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-09-08 12:31:06,844] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-08 12:31:06,844] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-08 12:31:06,844] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-08 12:31:06,844] [    INFO][0m - ppt_learning_rate             :0.0001[0m
[32m[2022-09-08 12:31:06,844] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-08 12:31:06,844] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-08 12:31:06,844] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-08 12:31:06,844] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-08 12:31:06,844] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-08 12:31:06,844] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-08 12:31:06,844] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-09-08 12:31:06,844] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-08 12:31:06,845] [    INFO][0m - save_steps                    :100[0m
[32m[2022-09-08 12:31:06,845] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-09-08 12:31:06,845] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-08 12:31:06,845] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-08 12:31:06,845] [    INFO][0m - seed                          :42[0m
[32m[2022-09-08 12:31:06,845] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-08 12:31:06,845] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-08 12:31:06,845] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-08 12:31:06,845] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-09-08 12:31:06,845] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-08 12:31:06,845] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-08 12:31:06,845] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-08 12:31:06,845] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-08 12:31:06,845] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-08 12:31:06,846] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-08 12:31:06,846] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-08 12:31:06,846] [    INFO][0m - [0m
[32m[2022-09-08 12:31:06,847] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-08 12:31:06,847] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-08 12:31:06,847] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-09-08 12:31:06,848] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-09-08 12:31:06,848] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-09-08 12:31:06,848] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-08 12:31:06,848] [    INFO][0m -   Total optimization steps = 1000.0[0m
[32m[2022-09-08 12:31:06,848] [    INFO][0m -   Total num train samples = 8000[0m
[32m[2022-09-08 12:31:08,392] [    INFO][0m - loss: 14.2183136, learning_rate: 9.9e-06, global_step: 10, interval_runtime: 1.543, interval_samples_per_second: 5.185, interval_steps_per_second: 6.481, epoch: 0.5[0m
[32m[2022-09-08 12:31:08,976] [    INFO][0m - loss: 2.87897549, learning_rate: 9.800000000000001e-06, global_step: 20, interval_runtime: 0.5841, interval_samples_per_second: 13.697, interval_steps_per_second: 17.121, epoch: 1.0[0m
[32m[2022-09-08 12:31:09,635] [    INFO][0m - loss: 0.30855489, learning_rate: 9.7e-06, global_step: 30, interval_runtime: 0.659, interval_samples_per_second: 12.14, interval_steps_per_second: 15.175, epoch: 1.5[0m
[32m[2022-09-08 12:31:10,217] [    INFO][0m - loss: 0.27601547, learning_rate: 9.600000000000001e-06, global_step: 40, interval_runtime: 0.5827, interval_samples_per_second: 13.73, interval_steps_per_second: 17.163, epoch: 2.0[0m
[32m[2022-09-08 12:31:10,873] [    INFO][0m - loss: 0.23976841, learning_rate: 9.5e-06, global_step: 50, interval_runtime: 0.6553, interval_samples_per_second: 12.207, interval_steps_per_second: 15.259, epoch: 2.5[0m
[32m[2022-09-08 12:31:11,461] [    INFO][0m - loss: 0.28176427, learning_rate: 9.4e-06, global_step: 60, interval_runtime: 0.5881, interval_samples_per_second: 13.604, interval_steps_per_second: 17.004, epoch: 3.0[0m
[32m[2022-09-08 12:31:12,131] [    INFO][0m - loss: 0.20289555, learning_rate: 9.3e-06, global_step: 70, interval_runtime: 0.6706, interval_samples_per_second: 11.93, interval_steps_per_second: 14.913, epoch: 3.5[0m
[32m[2022-09-08 12:31:12,726] [    INFO][0m - loss: 0.16229024, learning_rate: 9.200000000000002e-06, global_step: 80, interval_runtime: 0.5947, interval_samples_per_second: 13.452, interval_steps_per_second: 16.815, epoch: 4.0[0m
[32m[2022-09-08 12:31:13,389] [    INFO][0m - loss: 0.14246659, learning_rate: 9.100000000000001e-06, global_step: 90, interval_runtime: 0.6627, interval_samples_per_second: 12.072, interval_steps_per_second: 15.09, epoch: 4.5[0m
[32m[2022-09-08 12:31:13,968] [    INFO][0m - loss: 0.0963427, learning_rate: 9e-06, global_step: 100, interval_runtime: 0.5795, interval_samples_per_second: 13.806, interval_steps_per_second: 17.257, epoch: 5.0[0m
[32m[2022-09-08 12:31:13,969] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:31:13,969] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-08 12:31:13,969] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:31:13,969] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:31:13,969] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-08 12:31:14,564] [    INFO][0m - eval_loss: 0.3954007625579834, eval_accuracy: 0.7250000238418579, eval_runtime: 0.5946, eval_samples_per_second: 269.104, eval_steps_per_second: 33.638, epoch: 5.0[0m
[32m[2022-09-08 12:31:14,569] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-09-08 12:31:14,570] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:31:17,775] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-08 12:31:17,776] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-08 12:31:23,773] [    INFO][0m - loss: 0.10301602, learning_rate: 8.900000000000001e-06, global_step: 110, interval_runtime: 9.8043, interval_samples_per_second: 0.816, interval_steps_per_second: 1.02, epoch: 5.5[0m
[32m[2022-09-08 12:31:24,368] [    INFO][0m - loss: 0.06623884, learning_rate: 8.8e-06, global_step: 120, interval_runtime: 0.5956, interval_samples_per_second: 13.431, interval_steps_per_second: 16.789, epoch: 6.0[0m
[32m[2022-09-08 12:31:25,037] [    INFO][0m - loss: 0.09607703, learning_rate: 8.700000000000001e-06, global_step: 130, interval_runtime: 0.6684, interval_samples_per_second: 11.968, interval_steps_per_second: 14.96, epoch: 6.5[0m
[32m[2022-09-08 12:31:25,635] [    INFO][0m - loss: 0.06979172, learning_rate: 8.6e-06, global_step: 140, interval_runtime: 0.5982, interval_samples_per_second: 13.374, interval_steps_per_second: 16.717, epoch: 7.0[0m
[32m[2022-09-08 12:31:26,289] [    INFO][0m - loss: 0.08466038, learning_rate: 8.5e-06, global_step: 150, interval_runtime: 0.6542, interval_samples_per_second: 12.228, interval_steps_per_second: 15.286, epoch: 7.5[0m
[32m[2022-09-08 12:31:26,868] [    INFO][0m - loss: 0.11333576, learning_rate: 8.400000000000001e-06, global_step: 160, interval_runtime: 0.5789, interval_samples_per_second: 13.82, interval_steps_per_second: 17.276, epoch: 8.0[0m
[32m[2022-09-08 12:31:27,516] [    INFO][0m - loss: 0.0337008, learning_rate: 8.3e-06, global_step: 170, interval_runtime: 0.6478, interval_samples_per_second: 12.35, interval_steps_per_second: 15.437, epoch: 8.5[0m
[32m[2022-09-08 12:31:28,091] [    INFO][0m - loss: 0.00536295, learning_rate: 8.2e-06, global_step: 180, interval_runtime: 0.5757, interval_samples_per_second: 13.895, interval_steps_per_second: 17.369, epoch: 9.0[0m
[32m[2022-09-08 12:31:28,739] [    INFO][0m - loss: 0.00239019, learning_rate: 8.1e-06, global_step: 190, interval_runtime: 0.6479, interval_samples_per_second: 12.348, interval_steps_per_second: 15.434, epoch: 9.5[0m
[32m[2022-09-08 12:31:29,314] [    INFO][0m - loss: 0.03646738, learning_rate: 8.000000000000001e-06, global_step: 200, interval_runtime: 0.5752, interval_samples_per_second: 13.909, interval_steps_per_second: 17.386, epoch: 10.0[0m
[32m[2022-09-08 12:31:29,315] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:31:29,315] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-08 12:31:29,315] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:31:29,315] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:31:29,315] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-08 12:31:29,895] [    INFO][0m - eval_loss: 0.8750006556510925, eval_accuracy: 0.7250000238418579, eval_runtime: 0.5795, eval_samples_per_second: 276.123, eval_steps_per_second: 34.515, epoch: 10.0[0m
[32m[2022-09-08 12:31:29,902] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-09-08 12:31:29,902] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:31:33,214] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-09-08 12:31:33,214] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-09-08 12:31:39,214] [    INFO][0m - loss: 0.01640187, learning_rate: 7.9e-06, global_step: 210, interval_runtime: 9.8995, interval_samples_per_second: 0.808, interval_steps_per_second: 1.01, epoch: 10.5[0m
[32m[2022-09-08 12:31:39,921] [    INFO][0m - loss: 0.00093724, learning_rate: 7.800000000000002e-06, global_step: 220, interval_runtime: 0.7073, interval_samples_per_second: 11.311, interval_steps_per_second: 14.139, epoch: 11.0[0m
[32m[2022-09-08 12:31:40,695] [    INFO][0m - loss: 0.00755211, learning_rate: 7.7e-06, global_step: 230, interval_runtime: 0.774, interval_samples_per_second: 10.335, interval_steps_per_second: 12.919, epoch: 11.5[0m
[32m[2022-09-08 12:31:41,403] [    INFO][0m - loss: 0.0239606, learning_rate: 7.600000000000001e-06, global_step: 240, interval_runtime: 0.7076, interval_samples_per_second: 11.306, interval_steps_per_second: 14.133, epoch: 12.0[0m
[32m[2022-09-08 12:31:42,202] [    INFO][0m - loss: 0.00199727, learning_rate: 7.500000000000001e-06, global_step: 250, interval_runtime: 0.7995, interval_samples_per_second: 10.006, interval_steps_per_second: 12.507, epoch: 12.5[0m
[32m[2022-09-08 12:31:42,919] [    INFO][0m - loss: 0.00148359, learning_rate: 7.4e-06, global_step: 260, interval_runtime: 0.7168, interval_samples_per_second: 11.16, interval_steps_per_second: 13.951, epoch: 13.0[0m
[32m[2022-09-08 12:31:43,715] [    INFO][0m - loss: 0.00030816, learning_rate: 7.3e-06, global_step: 270, interval_runtime: 0.7961, interval_samples_per_second: 10.049, interval_steps_per_second: 12.561, epoch: 13.5[0m
[32m[2022-09-08 12:31:44,427] [    INFO][0m - loss: 0.0002537, learning_rate: 7.2000000000000005e-06, global_step: 280, interval_runtime: 0.7119, interval_samples_per_second: 11.238, interval_steps_per_second: 14.047, epoch: 14.0[0m
[32m[2022-09-08 12:31:45,209] [    INFO][0m - loss: 0.00231503, learning_rate: 7.100000000000001e-06, global_step: 290, interval_runtime: 0.7817, interval_samples_per_second: 10.234, interval_steps_per_second: 12.793, epoch: 14.5[0m
[32m[2022-09-08 12:31:45,918] [    INFO][0m - loss: 0.0081275, learning_rate: 7e-06, global_step: 300, interval_runtime: 0.7089, interval_samples_per_second: 11.285, interval_steps_per_second: 14.106, epoch: 15.0[0m
[32m[2022-09-08 12:31:45,918] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:31:45,918] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-08 12:31:45,919] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:31:45,919] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:31:45,919] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-08 12:31:46,552] [    INFO][0m - eval_loss: 1.1249239444732666, eval_accuracy: 0.706250011920929, eval_runtime: 0.6329, eval_samples_per_second: 252.787, eval_steps_per_second: 31.598, epoch: 15.0[0m
[32m[2022-09-08 12:31:46,558] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-09-08 12:31:46,558] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:31:50,149] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-09-08 12:31:50,150] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-09-08 12:31:56,365] [    INFO][0m - loss: 0.00034658, learning_rate: 6.9e-06, global_step: 310, interval_runtime: 10.4472, interval_samples_per_second: 0.766, interval_steps_per_second: 0.957, epoch: 15.5[0m
[32m[2022-09-08 12:31:57,091] [    INFO][0m - loss: 0.00594116, learning_rate: 6.800000000000001e-06, global_step: 320, interval_runtime: 0.7257, interval_samples_per_second: 11.024, interval_steps_per_second: 13.781, epoch: 16.0[0m
[32m[2022-09-08 12:31:57,892] [    INFO][0m - loss: 0.00012838, learning_rate: 6.700000000000001e-06, global_step: 330, interval_runtime: 0.8009, interval_samples_per_second: 9.989, interval_steps_per_second: 12.486, epoch: 16.5[0m
[32m[2022-09-08 12:31:58,630] [    INFO][0m - loss: 0.01524219, learning_rate: 6.600000000000001e-06, global_step: 340, interval_runtime: 0.7378, interval_samples_per_second: 10.843, interval_steps_per_second: 13.554, epoch: 17.0[0m
[32m[2022-09-08 12:31:59,430] [    INFO][0m - loss: 0.0001019, learning_rate: 6.5000000000000004e-06, global_step: 350, interval_runtime: 0.8002, interval_samples_per_second: 9.998, interval_steps_per_second: 12.498, epoch: 17.5[0m
[32m[2022-09-08 12:32:00,156] [    INFO][0m - loss: 0.01423832, learning_rate: 6.4000000000000006e-06, global_step: 360, interval_runtime: 0.7268, interval_samples_per_second: 11.007, interval_steps_per_second: 13.759, epoch: 18.0[0m
[32m[2022-09-08 12:32:00,957] [    INFO][0m - loss: 0.00015338, learning_rate: 6.300000000000001e-06, global_step: 370, interval_runtime: 0.8008, interval_samples_per_second: 9.99, interval_steps_per_second: 12.487, epoch: 18.5[0m
[32m[2022-09-08 12:32:01,682] [    INFO][0m - loss: 0.00015952, learning_rate: 6.200000000000001e-06, global_step: 380, interval_runtime: 0.7243, interval_samples_per_second: 11.046, interval_steps_per_second: 13.807, epoch: 19.0[0m
[32m[2022-09-08 12:32:02,490] [    INFO][0m - loss: 0.00256157, learning_rate: 6.1e-06, global_step: 390, interval_runtime: 0.8083, interval_samples_per_second: 9.897, interval_steps_per_second: 12.371, epoch: 19.5[0m
[32m[2022-09-08 12:32:03,214] [    INFO][0m - loss: 7.914e-05, learning_rate: 6e-06, global_step: 400, interval_runtime: 0.7244, interval_samples_per_second: 11.044, interval_steps_per_second: 13.805, epoch: 20.0[0m
[32m[2022-09-08 12:32:03,215] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:32:03,215] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-08 12:32:03,215] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:32:03,215] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:32:03,215] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-08 12:32:03,868] [    INFO][0m - eval_loss: 1.0813777446746826, eval_accuracy: 0.75, eval_runtime: 0.6529, eval_samples_per_second: 245.044, eval_steps_per_second: 30.63, epoch: 20.0[0m
[32m[2022-09-08 12:32:03,874] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-09-08 12:32:03,875] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:32:07,058] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-09-08 12:32:07,059] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-09-08 12:32:13,384] [    INFO][0m - loss: 0.00039952, learning_rate: 5.9e-06, global_step: 410, interval_runtime: 10.17, interval_samples_per_second: 0.787, interval_steps_per_second: 0.983, epoch: 20.5[0m
[32m[2022-09-08 12:32:14,113] [    INFO][0m - loss: 0.00509449, learning_rate: 5.8e-06, global_step: 420, interval_runtime: 0.7292, interval_samples_per_second: 10.971, interval_steps_per_second: 13.713, epoch: 21.0[0m
[32m[2022-09-08 12:32:14,798] [    INFO][0m - loss: 0.00186062, learning_rate: 5.7e-06, global_step: 430, interval_runtime: 0.6849, interval_samples_per_second: 11.681, interval_steps_per_second: 14.601, epoch: 21.5[0m
[32m[2022-09-08 12:32:15,394] [    INFO][0m - loss: 7.91e-05, learning_rate: 5.600000000000001e-06, global_step: 440, interval_runtime: 0.5952, interval_samples_per_second: 13.441, interval_steps_per_second: 16.801, epoch: 22.0[0m
[32m[2022-09-08 12:32:16,062] [    INFO][0m - loss: 0.00029621, learning_rate: 5.500000000000001e-06, global_step: 450, interval_runtime: 0.6687, interval_samples_per_second: 11.963, interval_steps_per_second: 14.954, epoch: 22.5[0m
[32m[2022-09-08 12:32:16,659] [    INFO][0m - loss: 0.0013102, learning_rate: 5.400000000000001e-06, global_step: 460, interval_runtime: 0.5963, interval_samples_per_second: 13.415, interval_steps_per_second: 16.769, epoch: 23.0[0m
[32m[2022-09-08 12:32:17,330] [    INFO][0m - loss: 0.00017397, learning_rate: 5.300000000000001e-06, global_step: 470, interval_runtime: 0.6718, interval_samples_per_second: 11.909, interval_steps_per_second: 14.886, epoch: 23.5[0m
[32m[2022-09-08 12:32:17,926] [    INFO][0m - loss: 5.057e-05, learning_rate: 5.2e-06, global_step: 480, interval_runtime: 0.5955, interval_samples_per_second: 13.434, interval_steps_per_second: 16.792, epoch: 24.0[0m
[32m[2022-09-08 12:32:18,599] [    INFO][0m - loss: 5.388e-05, learning_rate: 5.1e-06, global_step: 490, interval_runtime: 0.6729, interval_samples_per_second: 11.889, interval_steps_per_second: 14.862, epoch: 24.5[0m
[32m[2022-09-08 12:32:19,198] [    INFO][0m - loss: 8.833e-05, learning_rate: 5e-06, global_step: 500, interval_runtime: 0.5992, interval_samples_per_second: 13.352, interval_steps_per_second: 16.69, epoch: 25.0[0m
[32m[2022-09-08 12:32:19,198] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:32:19,199] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-08 12:32:19,199] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:32:19,199] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:32:19,199] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-08 12:32:19,766] [    INFO][0m - eval_loss: 1.2811614274978638, eval_accuracy: 0.7125000357627869, eval_runtime: 0.5669, eval_samples_per_second: 282.237, eval_steps_per_second: 35.28, epoch: 25.0[0m
[32m[2022-09-08 12:32:19,772] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-09-08 12:32:19,773] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:32:23,087] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-09-08 12:32:23,088] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-09-08 12:32:28,983] [    INFO][0m - loss: 6.44e-05, learning_rate: 4.9000000000000005e-06, global_step: 510, interval_runtime: 9.7852, interval_samples_per_second: 0.818, interval_steps_per_second: 1.022, epoch: 25.5[0m
[32m[2022-09-08 12:32:29,585] [    INFO][0m - loss: 4.41e-05, learning_rate: 4.800000000000001e-06, global_step: 520, interval_runtime: 0.6017, interval_samples_per_second: 13.295, interval_steps_per_second: 16.619, epoch: 26.0[0m
[32m[2022-09-08 12:32:30,257] [    INFO][0m - loss: 0.00024235, learning_rate: 4.7e-06, global_step: 530, interval_runtime: 0.6718, interval_samples_per_second: 11.909, interval_steps_per_second: 14.886, epoch: 26.5[0m
[32m[2022-09-08 12:32:30,857] [    INFO][0m - loss: 0.06879547, learning_rate: 4.600000000000001e-06, global_step: 540, interval_runtime: 0.6, interval_samples_per_second: 13.333, interval_steps_per_second: 16.667, epoch: 27.0[0m
[32m[2022-09-08 12:32:31,533] [    INFO][0m - loss: 4.827e-05, learning_rate: 4.5e-06, global_step: 550, interval_runtime: 0.6762, interval_samples_per_second: 11.831, interval_steps_per_second: 14.788, epoch: 27.5[0m
[32m[2022-09-08 12:32:32,131] [    INFO][0m - loss: 6.793e-05, learning_rate: 4.4e-06, global_step: 560, interval_runtime: 0.5981, interval_samples_per_second: 13.375, interval_steps_per_second: 16.719, epoch: 28.0[0m
[32m[2022-09-08 12:32:32,795] [    INFO][0m - loss: 0.00014211, learning_rate: 4.3e-06, global_step: 570, interval_runtime: 0.6644, interval_samples_per_second: 12.041, interval_steps_per_second: 15.051, epoch: 28.5[0m
[32m[2022-09-08 12:32:33,383] [    INFO][0m - loss: 7.472e-05, learning_rate: 4.2000000000000004e-06, global_step: 580, interval_runtime: 0.5879, interval_samples_per_second: 13.608, interval_steps_per_second: 17.01, epoch: 29.0[0m
[32m[2022-09-08 12:32:34,033] [    INFO][0m - loss: 0.00147311, learning_rate: 4.1e-06, global_step: 590, interval_runtime: 0.6495, interval_samples_per_second: 12.317, interval_steps_per_second: 15.396, epoch: 29.5[0m
[32m[2022-09-08 12:32:34,626] [    INFO][0m - loss: 3.879e-05, learning_rate: 4.000000000000001e-06, global_step: 600, interval_runtime: 0.5927, interval_samples_per_second: 13.496, interval_steps_per_second: 16.871, epoch: 30.0[0m
[32m[2022-09-08 12:32:34,626] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:32:34,626] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-08 12:32:34,626] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:32:34,626] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:32:34,627] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-08 12:32:35,211] [    INFO][0m - eval_loss: 1.182724952697754, eval_accuracy: 0.731249988079071, eval_runtime: 0.5838, eval_samples_per_second: 274.059, eval_steps_per_second: 34.257, epoch: 30.0[0m
[32m[2022-09-08 12:32:35,221] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-09-08 12:32:35,221] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:32:38,556] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-09-08 12:32:38,557] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-09-08 12:32:44,616] [    INFO][0m - loss: 6.65e-05, learning_rate: 3.900000000000001e-06, global_step: 610, interval_runtime: 9.9898, interval_samples_per_second: 0.801, interval_steps_per_second: 1.001, epoch: 30.5[0m
[32m[2022-09-08 12:32:45,202] [    INFO][0m - loss: 0.00038402, learning_rate: 3.8000000000000005e-06, global_step: 620, interval_runtime: 0.5861, interval_samples_per_second: 13.648, interval_steps_per_second: 17.061, epoch: 31.0[0m
[32m[2022-09-08 12:32:45,849] [    INFO][0m - loss: 4.328e-05, learning_rate: 3.7e-06, global_step: 630, interval_runtime: 0.6472, interval_samples_per_second: 12.362, interval_steps_per_second: 15.452, epoch: 31.5[0m
[32m[2022-09-08 12:32:46,489] [    INFO][0m - loss: 0.01401831, learning_rate: 3.6000000000000003e-06, global_step: 640, interval_runtime: 0.6395, interval_samples_per_second: 12.51, interval_steps_per_second: 15.637, epoch: 32.0[0m
[32m[2022-09-08 12:32:47,177] [    INFO][0m - loss: 0.00120925, learning_rate: 3.5e-06, global_step: 650, interval_runtime: 0.6891, interval_samples_per_second: 11.61, interval_steps_per_second: 14.512, epoch: 32.5[0m
[32m[2022-09-08 12:32:47,771] [    INFO][0m - loss: 0.00276579, learning_rate: 3.4000000000000005e-06, global_step: 660, interval_runtime: 0.5932, interval_samples_per_second: 13.486, interval_steps_per_second: 16.858, epoch: 33.0[0m
[32m[2022-09-08 12:32:48,428] [    INFO][0m - loss: 0.00015764, learning_rate: 3.3000000000000006e-06, global_step: 670, interval_runtime: 0.657, interval_samples_per_second: 12.177, interval_steps_per_second: 15.222, epoch: 33.5[0m
[32m[2022-09-08 12:32:49,010] [    INFO][0m - loss: 0.00013622, learning_rate: 3.2000000000000003e-06, global_step: 680, interval_runtime: 0.5825, interval_samples_per_second: 13.733, interval_steps_per_second: 17.166, epoch: 34.0[0m
[32m[2022-09-08 12:32:49,662] [    INFO][0m - loss: 0.00024355, learning_rate: 3.1000000000000004e-06, global_step: 690, interval_runtime: 0.6515, interval_samples_per_second: 12.28, interval_steps_per_second: 15.35, epoch: 34.5[0m
[32m[2022-09-08 12:32:50,238] [    INFO][0m - loss: 3.148e-05, learning_rate: 3e-06, global_step: 700, interval_runtime: 0.5763, interval_samples_per_second: 13.881, interval_steps_per_second: 17.352, epoch: 35.0[0m
[32m[2022-09-08 12:32:50,238] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:32:50,238] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-08 12:32:50,239] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:32:50,239] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:32:50,239] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-08 12:32:50,815] [    INFO][0m - eval_loss: 1.2412660121917725, eval_accuracy: 0.7437500357627869, eval_runtime: 0.5765, eval_samples_per_second: 277.551, eval_steps_per_second: 34.694, epoch: 35.0[0m
[32m[2022-09-08 12:32:50,822] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-700[0m
[32m[2022-09-08 12:32:50,822] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:32:53,922] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-700/tokenizer_config.json[0m
[32m[2022-09-08 12:32:53,922] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-700/special_tokens_map.json[0m
[32m[2022-09-08 12:32:59,832] [    INFO][0m - loss: 4.4e-05, learning_rate: 2.9e-06, global_step: 710, interval_runtime: 9.5938, interval_samples_per_second: 0.834, interval_steps_per_second: 1.042, epoch: 35.5[0m
[32m[2022-09-08 12:33:00,414] [    INFO][0m - loss: 4.218e-05, learning_rate: 2.8000000000000003e-06, global_step: 720, interval_runtime: 0.5817, interval_samples_per_second: 13.753, interval_steps_per_second: 17.191, epoch: 36.0[0m
[32m[2022-09-08 12:33:01,056] [    INFO][0m - loss: 0.00030794, learning_rate: 2.7000000000000004e-06, global_step: 730, interval_runtime: 0.6429, interval_samples_per_second: 12.444, interval_steps_per_second: 15.555, epoch: 36.5[0m
[32m[2022-09-08 12:33:01,639] [    INFO][0m - loss: 3.761e-05, learning_rate: 2.6e-06, global_step: 740, interval_runtime: 0.5827, interval_samples_per_second: 13.73, interval_steps_per_second: 17.162, epoch: 37.0[0m
[32m[2022-09-08 12:33:02,298] [    INFO][0m - loss: 6.143e-05, learning_rate: 2.5e-06, global_step: 750, interval_runtime: 0.6594, interval_samples_per_second: 12.132, interval_steps_per_second: 15.165, epoch: 37.5[0m
[32m[2022-09-08 12:33:02,889] [    INFO][0m - loss: 0.00010016, learning_rate: 2.4000000000000003e-06, global_step: 760, interval_runtime: 0.591, interval_samples_per_second: 13.537, interval_steps_per_second: 16.921, epoch: 38.0[0m
[32m[2022-09-08 12:33:03,549] [    INFO][0m - loss: 5.483e-05, learning_rate: 2.3000000000000004e-06, global_step: 770, interval_runtime: 0.6594, interval_samples_per_second: 12.133, interval_steps_per_second: 15.166, epoch: 38.5[0m
[32m[2022-09-08 12:33:04,127] [    INFO][0m - loss: 5.982e-05, learning_rate: 2.2e-06, global_step: 780, interval_runtime: 0.5784, interval_samples_per_second: 13.831, interval_steps_per_second: 17.288, epoch: 39.0[0m
[32m[2022-09-08 12:33:04,778] [    INFO][0m - loss: 2.252e-05, learning_rate: 2.1000000000000002e-06, global_step: 790, interval_runtime: 0.6505, interval_samples_per_second: 12.299, interval_steps_per_second: 15.374, epoch: 39.5[0m
[32m[2022-09-08 12:33:05,362] [    INFO][0m - loss: 0.00830709, learning_rate: 2.0000000000000003e-06, global_step: 800, interval_runtime: 0.5847, interval_samples_per_second: 13.683, interval_steps_per_second: 17.103, epoch: 40.0[0m
[32m[2022-09-08 12:33:05,363] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:33:05,363] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-08 12:33:05,363] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:33:05,363] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:33:05,363] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-08 12:33:05,942] [    INFO][0m - eval_loss: 1.2877800464630127, eval_accuracy: 0.731249988079071, eval_runtime: 0.5783, eval_samples_per_second: 276.676, eval_steps_per_second: 34.584, epoch: 40.0[0m
[32m[2022-09-08 12:33:05,948] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-800[0m
[32m[2022-09-08 12:33:05,948] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:33:09,001] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-800/tokenizer_config.json[0m
[32m[2022-09-08 12:33:09,002] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-800/special_tokens_map.json[0m
[32m[2022-09-08 12:33:14,049] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-08 12:33:14,049] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-400 (score: 0.75).[0m
[32m[2022-09-08 12:33:15,069] [    INFO][0m - train_runtime: 128.2207, train_samples_per_second: 62.392, train_steps_per_second: 7.799, train_loss: 0.2453651154276122, epoch: 40.0[0m
[32m[2022-09-08 12:33:15,111] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-09-08 12:33:15,112] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:33:18,192] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-09-08 12:33:18,192] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-09-08 12:33:18,194] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-08 12:33:18,194] [    INFO][0m -   epoch                    =       40.0[0m
[32m[2022-09-08 12:33:18,194] [    INFO][0m -   train_loss               =     0.2454[0m
[32m[2022-09-08 12:33:18,194] [    INFO][0m -   train_runtime            = 0:02:08.22[0m
[32m[2022-09-08 12:33:18,194] [    INFO][0m -   train_samples_per_second =     62.392[0m
[32m[2022-09-08 12:33:18,194] [    INFO][0m -   train_steps_per_second   =      7.799[0m
[32m[2022-09-08 12:33:18,198] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-08 12:33:18,199] [    INFO][0m -   Num examples = 1772[0m
[32m[2022-09-08 12:33:18,199] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:33:18,199] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:33:18,199] [    INFO][0m -   Total prediction steps = 222[0m
[32m[2022-09-08 12:33:25,488] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-08 12:33:25,489] [    INFO][0m -   test_accuracy           =     0.7562[0m
[32m[2022-09-08 12:33:25,489] [    INFO][0m -   test_loss               =     1.0209[0m
[32m[2022-09-08 12:33:25,489] [    INFO][0m -   test_runtime            = 0:00:07.28[0m
[32m[2022-09-08 12:33:25,489] [    INFO][0m -   test_samples_per_second =    243.092[0m
[32m[2022-09-08 12:33:25,489] [    INFO][0m -   test_steps_per_second   =     30.455[0m
[32m[2022-09-08 12:33:25,489] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-08 12:33:25,490] [    INFO][0m -   Num examples = 2000[0m
[32m[2022-09-08 12:33:25,490] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:33:25,490] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:33:25,490] [    INFO][0m -   Total prediction steps = 250[0m
[32m[2022-09-08 12:33:36,389] [    INFO][0m - Predictions for bustm saved to ./fewclue_submit_examples.[0m
run.sh: line 59: --freeze_plm: command not found
 
==========
chid
==========
 
[33m[2022-09-08 12:33:40,249] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-09-08 12:33:40,249] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-08 12:33:40,249] [    INFO][0m - ============================================================[0m
[32m[2022-09-08 12:33:40,249] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-08 12:33:40,249] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-08 12:33:40,249] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-08 12:33:40,249] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-08 12:33:40,249] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-08 12:33:40,250] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-08 12:33:40,250] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-09-08 12:33:40,250] [    INFO][0m - [0m
[32m[2022-09-08 12:33:40,250] [    INFO][0m - ============================================================[0m
[32m[2022-09-08 12:33:40,250] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-08 12:33:40,250] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-08 12:33:40,250] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-08 12:33:40,250] [    INFO][0m - prompt                        :[0m
[32m[2022-09-08 12:33:40,250] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-08 12:33:40,250] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-08 12:33:40,250] [    INFO][0m - task_name                     :chid[0m
[32m[2022-09-08 12:33:40,250] [    INFO][0m - [0m
[32m[2022-09-08 12:33:40,251] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0908 12:33:40.252200 35209 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0908 12:33:40.256048 35209 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-08 12:33:43,070] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-09-08 12:33:43,094] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-09-08 12:33:43,094] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[32m[2022-09-08 12:33:44,103] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùËøôÂè•ËØù‰∏≠ÊàêËØ≠['}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': ']ÁöÑÁêÜËß£Ê≠£Á°ÆÂêóÔºü'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': '„ÄÇ'}][0m
[32m[2022-09-08 12:33:44,400] [    INFO][0m - ============================================================[0m
[32m[2022-09-08 12:33:44,400] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-08 12:33:44,400] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-08 12:33:44,401] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-08 12:33:44,401] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-08 12:33:44,401] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-08 12:33:44,401] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-08 12:33:44,401] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-08 12:33:44,401] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-08 12:33:44,401] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-08 12:33:44,401] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-08 12:33:44,401] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-08 12:33:44,401] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-08 12:33:44,401] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-08 12:33:44,401] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-08 12:33:44,402] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-08 12:33:44,402] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-08 12:33:44,402] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-08 12:33:44,402] [    INFO][0m - eval_batch_size               :8[0m
[32m[2022-09-08 12:33:44,402] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-09-08 12:33:44,402] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-09-08 12:33:44,402] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-08 12:33:44,402] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-08 12:33:44,402] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-08 12:33:44,402] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-08 12:33:44,402] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-08 12:33:44,402] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-08 12:33:44,402] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-08 12:33:44,402] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-08 12:33:44,403] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-08 12:33:44,403] [    INFO][0m - learning_rate                 :1e-05[0m
[32m[2022-09-08 12:33:44,403] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-08 12:33:44,403] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-08 12:33:44,403] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-08 12:33:44,403] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-08 12:33:44,403] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-08 12:33:44,403] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-08 12:33:44,403] [    INFO][0m - logging_dir                   :./checkpoints/runs/Sep08_12-33-40_instance-3bwob41y-01[0m
[32m[2022-09-08 12:33:44,403] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-08 12:33:44,403] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-08 12:33:44,403] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-08 12:33:44,403] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-08 12:33:44,404] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-08 12:33:44,404] [    INFO][0m - max_seq_length                :256[0m
[32m[2022-09-08 12:33:44,404] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-08 12:33:44,404] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-08 12:33:44,404] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-08 12:33:44,404] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-08 12:33:44,404] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-09-08 12:33:44,404] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-08 12:33:44,404] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-08 12:33:44,404] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-09-08 12:33:44,404] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-08 12:33:44,404] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-08 12:33:44,404] [    INFO][0m - per_device_eval_batch_size    :8[0m
[32m[2022-09-08 12:33:44,405] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-09-08 12:33:44,405] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-08 12:33:44,405] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-08 12:33:44,405] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-08 12:33:44,405] [    INFO][0m - ppt_learning_rate             :0.0001[0m
[32m[2022-09-08 12:33:44,405] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-08 12:33:44,405] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-08 12:33:44,405] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-08 12:33:44,405] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-08 12:33:44,405] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-08 12:33:44,405] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-08 12:33:44,405] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-09-08 12:33:44,405] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-08 12:33:44,405] [    INFO][0m - save_steps                    :100[0m
[32m[2022-09-08 12:33:44,406] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-09-08 12:33:44,406] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-08 12:33:44,406] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-08 12:33:44,406] [    INFO][0m - seed                          :42[0m
[32m[2022-09-08 12:33:44,406] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-08 12:33:44,406] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-08 12:33:44,406] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-08 12:33:44,406] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-09-08 12:33:44,406] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-08 12:33:44,406] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-08 12:33:44,406] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-08 12:33:44,406] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-08 12:33:44,406] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-08 12:33:44,406] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-08 12:33:44,407] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-08 12:33:44,407] [    INFO][0m - [0m
[32m[2022-09-08 12:33:44,409] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-08 12:33:44,409] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-08 12:33:44,409] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-09-08 12:33:44,409] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-09-08 12:33:44,409] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-09-08 12:33:44,409] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-08 12:33:44,409] [    INFO][0m -   Total optimization steps = 8850.0[0m
[32m[2022-09-08 12:33:44,409] [    INFO][0m -   Total num train samples = 70700[0m
[32m[2022-09-08 12:33:46,875] [    INFO][0m - loss: 20.38623047, learning_rate: 9.988700564971753e-06, global_step: 10, interval_runtime: 2.4645, interval_samples_per_second: 3.246, interval_steps_per_second: 4.058, epoch: 0.0565[0m
[32m[2022-09-08 12:33:48,383] [    INFO][0m - loss: 6.42270508, learning_rate: 9.977401129943504e-06, global_step: 20, interval_runtime: 1.5085, interval_samples_per_second: 5.303, interval_steps_per_second: 6.629, epoch: 0.113[0m
[32m[2022-09-08 12:33:49,897] [    INFO][0m - loss: 0.58249903, learning_rate: 9.966101694915256e-06, global_step: 30, interval_runtime: 1.5136, interval_samples_per_second: 5.285, interval_steps_per_second: 6.607, epoch: 0.1695[0m
[32m[2022-09-08 12:33:51,410] [    INFO][0m - loss: 0.47496881, learning_rate: 9.954802259887007e-06, global_step: 40, interval_runtime: 1.5131, interval_samples_per_second: 5.287, interval_steps_per_second: 6.609, epoch: 0.226[0m
[32m[2022-09-08 12:33:52,927] [    INFO][0m - loss: 0.36300476, learning_rate: 9.943502824858759e-06, global_step: 50, interval_runtime: 1.5164, interval_samples_per_second: 5.276, interval_steps_per_second: 6.595, epoch: 0.2825[0m
[32m[2022-09-08 12:33:54,437] [    INFO][0m - loss: 0.26816926, learning_rate: 9.93220338983051e-06, global_step: 60, interval_runtime: 1.511, interval_samples_per_second: 5.295, interval_steps_per_second: 6.618, epoch: 0.339[0m
[32m[2022-09-08 12:33:55,951] [    INFO][0m - loss: 0.94801397, learning_rate: 9.92090395480226e-06, global_step: 70, interval_runtime: 1.5136, interval_samples_per_second: 5.285, interval_steps_per_second: 6.607, epoch: 0.3955[0m
[32m[2022-09-08 12:33:57,461] [    INFO][0m - loss: 0.64820986, learning_rate: 9.909604519774013e-06, global_step: 80, interval_runtime: 1.5095, interval_samples_per_second: 5.3, interval_steps_per_second: 6.625, epoch: 0.452[0m
[32m[2022-09-08 12:33:58,972] [    INFO][0m - loss: 0.46559186, learning_rate: 9.898305084745763e-06, global_step: 90, interval_runtime: 1.5115, interval_samples_per_second: 5.293, interval_steps_per_second: 6.616, epoch: 0.5085[0m
[32m[2022-09-08 12:34:00,480] [    INFO][0m - loss: 0.42811871, learning_rate: 9.887005649717516e-06, global_step: 100, interval_runtime: 1.5081, interval_samples_per_second: 5.305, interval_steps_per_second: 6.631, epoch: 0.565[0m
[32m[2022-09-08 12:34:00,481] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:34:00,481] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-08 12:34:00,481] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:34:00,481] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:34:00,481] [    INFO][0m -   Total prediction steps = 177[0m
[32m[2022-09-08 12:34:12,602] [    INFO][0m - eval_loss: 0.3872513473033905, eval_accuracy: 0.5594059228897095, eval_runtime: 12.12, eval_samples_per_second: 116.666, eval_steps_per_second: 14.604, epoch: 0.565[0m
[32m[2022-09-08 12:34:12,667] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-09-08 12:34:12,667] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:34:16,179] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-08 12:34:16,179] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-08 12:34:23,255] [    INFO][0m - loss: 0.42841144, learning_rate: 9.875706214689266e-06, global_step: 110, interval_runtime: 22.7743, interval_samples_per_second: 0.351, interval_steps_per_second: 0.439, epoch: 0.6215[0m
[32m[2022-09-08 12:34:24,763] [    INFO][0m - loss: 0.35678868, learning_rate: 9.864406779661017e-06, global_step: 120, interval_runtime: 1.5085, interval_samples_per_second: 5.303, interval_steps_per_second: 6.629, epoch: 0.678[0m
[32m[2022-09-08 12:34:26,274] [    INFO][0m - loss: 0.46918945, learning_rate: 9.85310734463277e-06, global_step: 130, interval_runtime: 1.5107, interval_samples_per_second: 5.296, interval_steps_per_second: 6.619, epoch: 0.7345[0m
[32m[2022-09-08 12:34:27,781] [    INFO][0m - loss: 0.41251116, learning_rate: 9.84180790960452e-06, global_step: 140, interval_runtime: 1.5071, interval_samples_per_second: 5.308, interval_steps_per_second: 6.635, epoch: 0.791[0m
[32m[2022-09-08 12:34:29,294] [    INFO][0m - loss: 0.47589402, learning_rate: 9.830508474576272e-06, global_step: 150, interval_runtime: 1.5132, interval_samples_per_second: 5.287, interval_steps_per_second: 6.608, epoch: 0.8475[0m
[32m[2022-09-08 12:34:30,801] [    INFO][0m - loss: 0.43925838, learning_rate: 9.819209039548023e-06, global_step: 160, interval_runtime: 1.5066, interval_samples_per_second: 5.31, interval_steps_per_second: 6.637, epoch: 0.904[0m
[32m[2022-09-08 12:34:32,311] [    INFO][0m - loss: 0.39749279, learning_rate: 9.807909604519775e-06, global_step: 170, interval_runtime: 1.5105, interval_samples_per_second: 5.296, interval_steps_per_second: 6.621, epoch: 0.9605[0m
[32m[2022-09-08 12:34:33,825] [    INFO][0m - loss: 0.36837873, learning_rate: 9.796610169491526e-06, global_step: 180, interval_runtime: 1.5144, interval_samples_per_second: 5.283, interval_steps_per_second: 6.603, epoch: 1.0169[0m
[32m[2022-09-08 12:34:35,342] [    INFO][0m - loss: 0.36068683, learning_rate: 9.785310734463278e-06, global_step: 190, interval_runtime: 1.5165, interval_samples_per_second: 5.275, interval_steps_per_second: 6.594, epoch: 1.0734[0m
[32m[2022-09-08 12:34:36,863] [    INFO][0m - loss: 0.43702035, learning_rate: 9.774011299435029e-06, global_step: 200, interval_runtime: 1.5208, interval_samples_per_second: 5.26, interval_steps_per_second: 6.575, epoch: 1.1299[0m
[32m[2022-09-08 12:34:36,863] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:34:36,863] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-08 12:34:36,863] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:34:36,863] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:34:36,863] [    INFO][0m -   Total prediction steps = 177[0m
[32m[2022-09-08 12:34:48,520] [    INFO][0m - eval_loss: 0.5201207399368286, eval_accuracy: 0.42574256658554077, eval_runtime: 11.6565, eval_samples_per_second: 121.306, eval_steps_per_second: 15.185, epoch: 1.1299[0m
[32m[2022-09-08 12:34:48,573] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-09-08 12:34:48,573] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:34:51,762] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-09-08 12:34:51,762] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-09-08 12:34:58,822] [    INFO][0m - loss: 0.35093143, learning_rate: 9.762711864406781e-06, global_step: 210, interval_runtime: 21.9596, interval_samples_per_second: 0.364, interval_steps_per_second: 0.455, epoch: 1.1864[0m
[32m[2022-09-08 12:35:00,333] [    INFO][0m - loss: 0.58314333, learning_rate: 9.751412429378532e-06, global_step: 220, interval_runtime: 1.5111, interval_samples_per_second: 5.294, interval_steps_per_second: 6.618, epoch: 1.2429[0m
[32m[2022-09-08 12:35:01,838] [    INFO][0m - loss: 0.47407107, learning_rate: 9.740112994350284e-06, global_step: 230, interval_runtime: 1.5052, interval_samples_per_second: 5.315, interval_steps_per_second: 6.644, epoch: 1.2994[0m
[32m[2022-09-08 12:35:03,349] [    INFO][0m - loss: 0.45025978, learning_rate: 9.728813559322035e-06, global_step: 240, interval_runtime: 1.5102, interval_samples_per_second: 5.297, interval_steps_per_second: 6.622, epoch: 1.3559[0m
[32m[2022-09-08 12:35:04,857] [    INFO][0m - loss: 0.24655986, learning_rate: 9.717514124293787e-06, global_step: 250, interval_runtime: 1.5087, interval_samples_per_second: 5.303, interval_steps_per_second: 6.628, epoch: 1.4124[0m
[32m[2022-09-08 12:35:06,364] [    INFO][0m - loss: 0.55682316, learning_rate: 9.706214689265538e-06, global_step: 260, interval_runtime: 1.5072, interval_samples_per_second: 5.308, interval_steps_per_second: 6.635, epoch: 1.4689[0m
[32m[2022-09-08 12:35:07,871] [    INFO][0m - loss: 0.33089333, learning_rate: 9.69491525423729e-06, global_step: 270, interval_runtime: 1.5069, interval_samples_per_second: 5.309, interval_steps_per_second: 6.636, epoch: 1.5254[0m
[32m[2022-09-08 12:35:09,380] [    INFO][0m - loss: 0.36361451, learning_rate: 9.68361581920904e-06, global_step: 280, interval_runtime: 1.5088, interval_samples_per_second: 5.302, interval_steps_per_second: 6.628, epoch: 1.5819[0m
[32m[2022-09-08 12:35:10,891] [    INFO][0m - loss: 0.27444274, learning_rate: 9.672316384180791e-06, global_step: 290, interval_runtime: 1.5105, interval_samples_per_second: 5.296, interval_steps_per_second: 6.62, epoch: 1.6384[0m
[32m[2022-09-08 12:35:12,398] [    INFO][0m - loss: 0.61759162, learning_rate: 9.661016949152544e-06, global_step: 300, interval_runtime: 1.5074, interval_samples_per_second: 5.307, interval_steps_per_second: 6.634, epoch: 1.6949[0m
[32m[2022-09-08 12:35:12,398] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:35:12,399] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-08 12:35:12,399] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:35:12,399] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:35:12,399] [    INFO][0m -   Total prediction steps = 177[0m
[32m[2022-09-08 12:35:24,070] [    INFO][0m - eval_loss: 0.3587925434112549, eval_accuracy: 0.4653465151786804, eval_runtime: 11.6705, eval_samples_per_second: 121.16, eval_steps_per_second: 15.166, epoch: 1.6949[0m
[32m[2022-09-08 12:35:24,123] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-09-08 12:35:24,124] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:35:27,605] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-09-08 12:35:27,605] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-09-08 12:35:34,485] [    INFO][0m - loss: 0.36510656, learning_rate: 9.649717514124294e-06, global_step: 310, interval_runtime: 22.0866, interval_samples_per_second: 0.362, interval_steps_per_second: 0.453, epoch: 1.7514[0m
[32m[2022-09-08 12:35:35,992] [    INFO][0m - loss: 0.37536943, learning_rate: 9.638418079096045e-06, global_step: 320, interval_runtime: 1.5073, interval_samples_per_second: 5.308, interval_steps_per_second: 6.635, epoch: 1.8079[0m
[32m[2022-09-08 12:35:37,499] [    INFO][0m - loss: 0.45099068, learning_rate: 9.627118644067797e-06, global_step: 330, interval_runtime: 1.5066, interval_samples_per_second: 5.31, interval_steps_per_second: 6.637, epoch: 1.8644[0m
[32m[2022-09-08 12:35:39,005] [    INFO][0m - loss: 0.32423804, learning_rate: 9.615819209039548e-06, global_step: 340, interval_runtime: 1.5067, interval_samples_per_second: 5.31, interval_steps_per_second: 6.637, epoch: 1.9209[0m
[32m[2022-09-08 12:35:40,507] [    INFO][0m - loss: 0.30899348, learning_rate: 9.6045197740113e-06, global_step: 350, interval_runtime: 1.502, interval_samples_per_second: 5.326, interval_steps_per_second: 6.658, epoch: 1.9774[0m
[32m[2022-09-08 12:35:42,025] [    INFO][0m - loss: 0.30980048, learning_rate: 9.593220338983051e-06, global_step: 360, interval_runtime: 1.5178, interval_samples_per_second: 5.271, interval_steps_per_second: 6.589, epoch: 2.0339[0m
[32m[2022-09-08 12:35:43,536] [    INFO][0m - loss: 0.3419904, learning_rate: 9.581920903954803e-06, global_step: 370, interval_runtime: 1.5111, interval_samples_per_second: 5.294, interval_steps_per_second: 6.617, epoch: 2.0904[0m
[32m[2022-09-08 12:35:45,043] [    INFO][0m - loss: 0.43330202, learning_rate: 9.570621468926554e-06, global_step: 380, interval_runtime: 1.5072, interval_samples_per_second: 5.308, interval_steps_per_second: 6.635, epoch: 2.1469[0m
[32m[2022-09-08 12:35:46,553] [    INFO][0m - loss: 0.26783211, learning_rate: 9.559322033898306e-06, global_step: 390, interval_runtime: 1.5101, interval_samples_per_second: 5.298, interval_steps_per_second: 6.622, epoch: 2.2034[0m
[32m[2022-09-08 12:35:48,067] [    INFO][0m - loss: 0.30633574, learning_rate: 9.548022598870057e-06, global_step: 400, interval_runtime: 1.5139, interval_samples_per_second: 5.284, interval_steps_per_second: 6.606, epoch: 2.2599[0m
[32m[2022-09-08 12:35:48,068] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:35:48,068] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-08 12:35:48,068] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:35:48,068] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:35:48,068] [    INFO][0m -   Total prediction steps = 177[0m
[32m[2022-09-08 12:35:59,672] [    INFO][0m - eval_loss: 0.4004059433937073, eval_accuracy: 0.5049504637718201, eval_runtime: 11.603, eval_samples_per_second: 121.865, eval_steps_per_second: 15.255, epoch: 2.2599[0m
[32m[2022-09-08 12:35:59,724] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-09-08 12:35:59,724] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:36:02,704] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-09-08 12:36:02,704] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-09-08 12:36:09,313] [    INFO][0m - loss: 0.33795135, learning_rate: 9.53672316384181e-06, global_step: 410, interval_runtime: 21.2458, interval_samples_per_second: 0.377, interval_steps_per_second: 0.471, epoch: 2.3164[0m
[32m[2022-09-08 12:36:10,819] [    INFO][0m - loss: 0.30400977, learning_rate: 9.52542372881356e-06, global_step: 420, interval_runtime: 1.5062, interval_samples_per_second: 5.311, interval_steps_per_second: 6.639, epoch: 2.3729[0m
[32m[2022-09-08 12:36:12,331] [    INFO][0m - loss: 0.23057499, learning_rate: 9.514124293785312e-06, global_step: 430, interval_runtime: 1.5112, interval_samples_per_second: 5.294, interval_steps_per_second: 6.617, epoch: 2.4294[0m
[32m[2022-09-08 12:36:13,843] [    INFO][0m - loss: 0.28923364, learning_rate: 9.502824858757063e-06, global_step: 440, interval_runtime: 1.5122, interval_samples_per_second: 5.29, interval_steps_per_second: 6.613, epoch: 2.4859[0m
[32m[2022-09-08 12:36:15,353] [    INFO][0m - loss: 0.38768952, learning_rate: 9.491525423728815e-06, global_step: 450, interval_runtime: 1.5102, interval_samples_per_second: 5.297, interval_steps_per_second: 6.622, epoch: 2.5424[0m
[32m[2022-09-08 12:36:16,862] [    INFO][0m - loss: 0.52246213, learning_rate: 9.480225988700566e-06, global_step: 460, interval_runtime: 1.5088, interval_samples_per_second: 5.302, interval_steps_per_second: 6.628, epoch: 2.5989[0m
[32m[2022-09-08 12:36:18,372] [    INFO][0m - loss: 0.20461245, learning_rate: 9.468926553672318e-06, global_step: 470, interval_runtime: 1.5106, interval_samples_per_second: 5.296, interval_steps_per_second: 6.62, epoch: 2.6554[0m
[32m[2022-09-08 12:36:19,887] [    INFO][0m - loss: 0.42796245, learning_rate: 9.457627118644069e-06, global_step: 480, interval_runtime: 1.5142, interval_samples_per_second: 5.283, interval_steps_per_second: 6.604, epoch: 2.7119[0m
[32m[2022-09-08 12:36:21,397] [    INFO][0m - loss: 0.3226285, learning_rate: 9.44632768361582e-06, global_step: 490, interval_runtime: 1.51, interval_samples_per_second: 5.298, interval_steps_per_second: 6.623, epoch: 2.7684[0m
[32m[2022-09-08 12:36:22,908] [    INFO][0m - loss: 0.19900923, learning_rate: 9.435028248587572e-06, global_step: 500, interval_runtime: 1.5118, interval_samples_per_second: 5.292, interval_steps_per_second: 6.615, epoch: 2.8249[0m
[32m[2022-09-08 12:36:22,909] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:36:22,909] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-08 12:36:22,909] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:36:22,909] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:36:22,909] [    INFO][0m -   Total prediction steps = 177[0m
[32m[2022-09-08 12:36:34,532] [    INFO][0m - eval_loss: 0.6991459727287292, eval_accuracy: 0.4653465151786804, eval_runtime: 11.6225, eval_samples_per_second: 121.66, eval_steps_per_second: 15.229, epoch: 2.8249[0m
[32m[2022-09-08 12:36:34,584] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-09-08 12:36:34,585] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:36:37,794] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-09-08 12:36:38,052] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-09-08 12:36:43,171] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-08 12:36:43,171] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-100 (score: 0.5594059228897095).[0m
[32m[2022-09-08 12:36:44,130] [    INFO][0m - train_runtime: 179.7201, train_samples_per_second: 393.389, train_steps_per_second: 49.243, train_loss: 0.9218313484191895, epoch: 2.8249[0m
[32m[2022-09-08 12:36:44,184] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-09-08 12:36:44,185] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:36:47,309] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-09-08 12:36:47,309] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-09-08 12:36:47,310] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-08 12:36:47,310] [    INFO][0m -   epoch                    =     2.8249[0m
[32m[2022-09-08 12:36:47,310] [    INFO][0m -   train_loss               =     0.9218[0m
[32m[2022-09-08 12:36:47,310] [    INFO][0m -   train_runtime            = 0:02:59.72[0m
[32m[2022-09-08 12:36:47,311] [    INFO][0m -   train_samples_per_second =    393.389[0m
[32m[2022-09-08 12:36:47,311] [    INFO][0m -   train_steps_per_second   =     49.243[0m
[32m[2022-09-08 12:36:47,314] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-08 12:36:47,314] [    INFO][0m -   Num examples = 14014[0m
[32m[2022-09-08 12:36:47,314] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:36:47,314] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:36:47,314] [    INFO][0m -   Total prediction steps = 1752[0m
[32m[2022-09-08 12:39:47,278] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-08 12:39:47,279] [    INFO][0m -   test_accuracy           =     0.5564[0m
[32m[2022-09-08 12:39:47,279] [    INFO][0m -   test_loss               =      0.388[0m
[32m[2022-09-08 12:39:47,279] [    INFO][0m -   test_runtime            = 0:02:59.96[0m
[32m[2022-09-08 12:39:47,279] [    INFO][0m -   test_samples_per_second =     77.871[0m
[32m[2022-09-08 12:39:47,279] [    INFO][0m -   test_steps_per_second   =      9.735[0m
[32m[2022-09-08 12:39:47,279] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-08 12:39:47,279] [    INFO][0m -   Num examples = 14000[0m
[32m[2022-09-08 12:39:47,279] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:39:47,280] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:39:47,280] [    INFO][0m -   Total prediction steps = 1750[0m
[32m[2022-09-08 12:42:34,377] [    INFO][0m - Predictions for chidf saved to ./fewclue_submit_examples.[0m
run.sh: line 59: --freeze_plm: command not found
 
==========
csl
==========
 
[33m[2022-09-08 12:42:38,417] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-09-08 12:42:38,418] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-08 12:42:38,418] [    INFO][0m - ============================================================[0m
[32m[2022-09-08 12:42:38,418] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-08 12:42:38,418] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-08 12:42:38,418] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-08 12:42:38,418] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-08 12:42:38,418] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-08 12:42:38,418] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-08 12:42:38,419] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-09-08 12:42:38,419] [    INFO][0m - [0m
[32m[2022-09-08 12:42:38,419] [    INFO][0m - ============================================================[0m
[32m[2022-09-08 12:42:38,419] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-08 12:42:38,419] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-08 12:42:38,419] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-08 12:42:38,419] [    INFO][0m - prompt                        :[0m
[32m[2022-09-08 12:42:38,419] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-08 12:42:38,419] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-08 12:42:38,419] [    INFO][0m - task_name                     :csl[0m
[32m[2022-09-08 12:42:38,419] [    INFO][0m - [0m
[32m[2022-09-08 12:42:38,420] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0908 12:42:38.421005 47922 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0908 12:42:38.424782 47922 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-08 12:42:41,311] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-09-08 12:42:41,336] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-09-08 12:42:41,336] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[32m[2022-09-08 12:42:42,324] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùÂÖ∂‰∏≠‚Äú'}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': '‚Äù'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'ËøôÂè•ËØùÁöÑÂÖ≥ÈîÆËØç„ÄÇ'}][0m
[32m[2022-09-08 12:42:42,841] [    INFO][0m - ============================================================[0m
[32m[2022-09-08 12:42:42,841] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-08 12:42:42,841] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-08 12:42:42,841] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-08 12:42:42,841] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-08 12:42:42,841] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-08 12:42:42,841] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-08 12:42:42,841] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-08 12:42:42,841] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-08 12:42:42,842] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-08 12:42:42,842] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-08 12:42:42,842] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-08 12:42:42,842] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-08 12:42:42,842] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-08 12:42:42,842] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-08 12:42:42,842] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-08 12:42:42,842] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-08 12:42:42,842] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-08 12:42:42,842] [    INFO][0m - eval_batch_size               :8[0m
[32m[2022-09-08 12:42:42,842] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-09-08 12:42:42,842] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-09-08 12:42:42,843] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-08 12:42:42,843] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-08 12:42:42,843] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-08 12:42:42,843] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-08 12:42:42,843] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-08 12:42:42,843] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-08 12:42:42,843] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-08 12:42:42,843] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-08 12:42:42,843] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-08 12:42:42,843] [    INFO][0m - learning_rate                 :1e-05[0m
[32m[2022-09-08 12:42:42,843] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-08 12:42:42,843] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-08 12:42:42,843] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-08 12:42:42,843] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-08 12:42:42,844] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-08 12:42:42,844] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-08 12:42:42,844] [    INFO][0m - logging_dir                   :./checkpoints/runs/Sep08_12-42-38_instance-3bwob41y-01[0m
[32m[2022-09-08 12:42:42,844] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-08 12:42:42,844] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-08 12:42:42,844] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-08 12:42:42,844] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-08 12:42:42,844] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-08 12:42:42,844] [    INFO][0m - max_seq_length                :320[0m
[32m[2022-09-08 12:42:42,844] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-08 12:42:42,844] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-08 12:42:42,844] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-08 12:42:42,844] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-08 12:42:42,845] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-09-08 12:42:42,845] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-08 12:42:42,845] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-08 12:42:42,845] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-09-08 12:42:42,845] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-08 12:42:42,845] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-08 12:42:42,845] [    INFO][0m - per_device_eval_batch_size    :8[0m
[32m[2022-09-08 12:42:42,845] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-09-08 12:42:42,845] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-08 12:42:42,845] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-08 12:42:42,845] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-08 12:42:42,845] [    INFO][0m - ppt_learning_rate             :0.0001[0m
[32m[2022-09-08 12:42:42,845] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-08 12:42:42,845] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-08 12:42:42,846] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-08 12:42:42,846] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-08 12:42:42,846] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-08 12:42:42,846] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-08 12:42:42,846] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-09-08 12:42:42,846] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-08 12:42:42,846] [    INFO][0m - save_steps                    :100[0m
[32m[2022-09-08 12:42:42,846] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-09-08 12:42:42,846] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-08 12:42:42,846] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-08 12:42:42,846] [    INFO][0m - seed                          :42[0m
[32m[2022-09-08 12:42:42,846] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-08 12:42:42,846] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-08 12:42:42,847] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-08 12:42:42,847] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-09-08 12:42:42,847] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-08 12:42:42,847] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-08 12:42:42,847] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-08 12:42:42,847] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-08 12:42:42,847] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-08 12:42:42,847] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-08 12:42:42,847] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-08 12:42:42,847] [    INFO][0m - [0m
[32m[2022-09-08 12:42:42,849] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-08 12:42:42,849] [    INFO][0m -   Num examples = 2642[0m
[32m[2022-09-08 12:42:42,849] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-09-08 12:42:42,849] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-09-08 12:42:42,849] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-09-08 12:42:42,849] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-08 12:42:42,849] [    INFO][0m -   Total optimization steps = 16550.0[0m
[32m[2022-09-08 12:42:42,849] [    INFO][0m -   Total num train samples = 132100[0m
[32m[2022-09-08 12:42:45,653] [    INFO][0m - loss: 15.2526123, learning_rate: 9.993957703927493e-06, global_step: 10, interval_runtime: 2.8034, interval_samples_per_second: 2.854, interval_steps_per_second: 3.567, epoch: 0.0302[0m
[32m[2022-09-08 12:42:47,554] [    INFO][0m - loss: 2.4547636, learning_rate: 9.987915407854986e-06, global_step: 20, interval_runtime: 1.9011, interval_samples_per_second: 4.208, interval_steps_per_second: 5.26, epoch: 0.0604[0m
[32m[2022-09-08 12:42:49,458] [    INFO][0m - loss: 0.35684483, learning_rate: 9.981873111782479e-06, global_step: 30, interval_runtime: 1.903, interval_samples_per_second: 4.204, interval_steps_per_second: 5.255, epoch: 0.0906[0m
[32m[2022-09-08 12:42:51,368] [    INFO][0m - loss: 0.37750609, learning_rate: 9.975830815709971e-06, global_step: 40, interval_runtime: 1.9106, interval_samples_per_second: 4.187, interval_steps_per_second: 5.234, epoch: 0.1208[0m
[32m[2022-09-08 12:42:53,289] [    INFO][0m - loss: 0.36616225, learning_rate: 9.969788519637464e-06, global_step: 50, interval_runtime: 1.9206, interval_samples_per_second: 4.165, interval_steps_per_second: 5.207, epoch: 0.1511[0m
[32m[2022-09-08 12:42:55,198] [    INFO][0m - loss: 0.31126511, learning_rate: 9.963746223564955e-06, global_step: 60, interval_runtime: 1.9096, interval_samples_per_second: 4.189, interval_steps_per_second: 5.237, epoch: 0.1813[0m
[32m[2022-09-08 12:42:57,097] [    INFO][0m - loss: 0.33462372, learning_rate: 9.957703927492449e-06, global_step: 70, interval_runtime: 1.8985, interval_samples_per_second: 4.214, interval_steps_per_second: 5.267, epoch: 0.2115[0m
[32m[2022-09-08 12:42:59,008] [    INFO][0m - loss: 0.32059581, learning_rate: 9.95166163141994e-06, global_step: 80, interval_runtime: 1.9118, interval_samples_per_second: 4.184, interval_steps_per_second: 5.231, epoch: 0.2417[0m
[32m[2022-09-08 12:43:00,915] [    INFO][0m - loss: 0.32638652, learning_rate: 9.945619335347432e-06, global_step: 90, interval_runtime: 1.9062, interval_samples_per_second: 4.197, interval_steps_per_second: 5.246, epoch: 0.2719[0m
[32m[2022-09-08 12:43:02,827] [    INFO][0m - loss: 0.30213146, learning_rate: 9.939577039274926e-06, global_step: 100, interval_runtime: 1.9126, interval_samples_per_second: 4.183, interval_steps_per_second: 5.228, epoch: 0.3021[0m
[32m[2022-09-08 12:43:02,828] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:43:02,828] [    INFO][0m -   Num examples = 2712[0m
[32m[2022-09-08 12:43:02,828] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:43:02,828] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:43:02,828] [    INFO][0m -   Total prediction steps = 339[0m
(2712, 2)
2712
Traceback (most recent call last):
  File "train_single.py", line 268, in <module>
    main(0)
  File "train_single.py", line 222, in main
    train_result = trainer.train(resume_from_checkpoint=None)
  File "/ssd2/wanghuijuan03/prompt/PaddleNLP/paddlenlp/trainer/trainer_base.py", line 581, in train
    self._maybe_log_save_evaluate(tr_loss, model, epoch,
  File "/ssd2/wanghuijuan03/prompt/PaddleNLP/paddlenlp/trainer/trainer_base.py", line 710, in _maybe_log_save_evaluate
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
  File "/ssd2/wanghuijuan03/prompt/PaddleNLP/paddlenlp/trainer/trainer_base.py", line 1327, in evaluate
    output = self.evaluation_loop(
  File "/ssd2/wanghuijuan03/prompt/PaddleNLP/paddlenlp/trainer/trainer_base.py", line 1482, in evaluation_loop
    metrics = self.compute_metrics(
  File "train_single.py", line 181, in csl_metrics
    all_pred = paddle.stack(paddle.to_tensor(pred), axis=0)[:, 1]
  File "/ssd2/wanghuijuan03/anaconda3/envs/prompt/lib/python3.8/site-packages/paddle/tensor/manipulation.py", line 1715, in stack
    return _C_ops.stack(x, axis)
ValueError: (InvalidArgument) stack(): argument 'x' (position 0) must be list of Tensors, but got Tensor (at /paddle/paddle/fluid/pybind/eager_utils.cc:967)

run.sh: line 59: --freeze_plm: command not found
 
==========
cluewsc
==========
 
[33m[2022-09-08 12:43:44,720] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-09-08 12:43:44,721] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-08 12:43:44,721] [    INFO][0m - ============================================================[0m
[32m[2022-09-08 12:43:44,721] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-08 12:43:44,721] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-08 12:43:44,721] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-08 12:43:44,721] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-08 12:43:44,721] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-08 12:43:44,722] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-08 12:43:44,722] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-09-08 12:43:44,722] [    INFO][0m - [0m
[32m[2022-09-08 12:43:44,722] [    INFO][0m - ============================================================[0m
[32m[2022-09-08 12:43:44,722] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-08 12:43:44,722] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-08 12:43:44,722] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-08 12:43:44,722] [    INFO][0m - prompt                        :[0m
[32m[2022-09-08 12:43:44,722] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-08 12:43:44,722] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-08 12:43:44,722] [    INFO][0m - task_name                     :cluewsc[0m
[32m[2022-09-08 12:43:44,722] [    INFO][0m - [0m
[32m[2022-09-08 12:43:44,723] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0908 12:43:44.724318 49130 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0908 12:43:44.728235 49130 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-08 12:43:47,693] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-09-08 12:43:47,718] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-09-08 12:43:47,718] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[32m[2022-09-08 12:43:48,751] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùËøôÂè•ËØù‰∏≠‰ª£ËØç'}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': 'ÁöÑÁêÜËß£Ê≠£Á°ÆÂêóÔºü'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}][0m
[32m[2022-09-08 12:43:48,858] [    INFO][0m - ============================================================[0m
[32m[2022-09-08 12:43:48,859] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-08 12:43:48,859] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-08 12:43:48,859] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-08 12:43:48,859] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-08 12:43:48,859] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-08 12:43:48,859] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-08 12:43:48,859] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-08 12:43:48,859] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-08 12:43:48,859] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-08 12:43:48,859] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-08 12:43:48,860] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-08 12:43:48,860] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-08 12:43:48,860] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-08 12:43:48,860] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-08 12:43:48,860] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-08 12:43:48,860] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-08 12:43:48,860] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-08 12:43:48,860] [    INFO][0m - eval_batch_size               :8[0m
[32m[2022-09-08 12:43:48,860] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-09-08 12:43:48,860] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-09-08 12:43:48,860] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-08 12:43:48,860] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-08 12:43:48,860] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-08 12:43:48,861] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-08 12:43:48,861] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-08 12:43:48,861] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-08 12:43:48,861] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-08 12:43:48,861] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-08 12:43:48,861] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-08 12:43:48,861] [    INFO][0m - learning_rate                 :1e-05[0m
[32m[2022-09-08 12:43:48,861] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-08 12:43:48,861] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-08 12:43:48,861] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-08 12:43:48,861] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-08 12:43:48,861] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-08 12:43:48,861] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-08 12:43:48,861] [    INFO][0m - logging_dir                   :./checkpoints/runs/Sep08_12-43-44_instance-3bwob41y-01[0m
[32m[2022-09-08 12:43:48,862] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-08 12:43:48,862] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-08 12:43:48,862] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-08 12:43:48,862] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-08 12:43:48,862] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-08 12:43:48,862] [    INFO][0m - max_seq_length                :128[0m
[32m[2022-09-08 12:43:48,862] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-08 12:43:48,862] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-08 12:43:48,862] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-08 12:43:48,862] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-08 12:43:48,862] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-09-08 12:43:48,862] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-08 12:43:48,862] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-08 12:43:48,863] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-09-08 12:43:48,863] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-08 12:43:48,863] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-08 12:43:48,863] [    INFO][0m - per_device_eval_batch_size    :8[0m
[32m[2022-09-08 12:43:48,863] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-09-08 12:43:48,863] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-08 12:43:48,863] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-08 12:43:48,863] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-08 12:43:48,863] [    INFO][0m - ppt_learning_rate             :0.0001[0m
[32m[2022-09-08 12:43:48,863] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-08 12:43:48,863] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-08 12:43:48,863] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-08 12:43:48,863] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-08 12:43:48,863] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-08 12:43:48,864] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-08 12:43:48,864] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-09-08 12:43:48,864] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-08 12:43:48,864] [    INFO][0m - save_steps                    :100[0m
[32m[2022-09-08 12:43:48,864] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-09-08 12:43:48,864] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-08 12:43:48,864] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-08 12:43:48,864] [    INFO][0m - seed                          :42[0m
[32m[2022-09-08 12:43:48,864] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-08 12:43:48,864] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-08 12:43:48,864] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-08 12:43:48,864] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-09-08 12:43:48,864] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-08 12:43:48,865] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-08 12:43:48,865] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-08 12:43:48,865] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-08 12:43:48,865] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-08 12:43:48,865] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-08 12:43:48,865] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-08 12:43:48,865] [    INFO][0m - [0m
[32m[2022-09-08 12:43:48,866] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-08 12:43:48,867] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-08 12:43:48,867] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-09-08 12:43:48,867] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-09-08 12:43:48,867] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-09-08 12:43:48,867] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-08 12:43:48,867] [    INFO][0m -   Total optimization steps = 1000.0[0m
[32m[2022-09-08 12:43:48,867] [    INFO][0m -   Total num train samples = 8000[0m
[32m[2022-09-08 12:43:50,681] [    INFO][0m - loss: 19.05349121, learning_rate: 9.9e-06, global_step: 10, interval_runtime: 1.813, interval_samples_per_second: 4.413, interval_steps_per_second: 5.516, epoch: 0.5[0m
[32m[2022-09-08 12:43:51,511] [    INFO][0m - loss: 4.81750832, learning_rate: 9.800000000000001e-06, global_step: 20, interval_runtime: 0.8306, interval_samples_per_second: 9.632, interval_steps_per_second: 12.04, epoch: 1.0[0m
[32m[2022-09-08 12:43:52,428] [    INFO][0m - loss: 1.12315779, learning_rate: 9.7e-06, global_step: 30, interval_runtime: 0.9165, interval_samples_per_second: 8.729, interval_steps_per_second: 10.911, epoch: 1.5[0m
[32m[2022-09-08 12:43:53,261] [    INFO][0m - loss: 0.77147923, learning_rate: 9.600000000000001e-06, global_step: 40, interval_runtime: 0.8326, interval_samples_per_second: 9.608, interval_steps_per_second: 12.01, epoch: 2.0[0m
[32m[2022-09-08 12:43:54,186] [    INFO][0m - loss: 0.75446134, learning_rate: 9.5e-06, global_step: 50, interval_runtime: 0.9254, interval_samples_per_second: 8.645, interval_steps_per_second: 10.806, epoch: 2.5[0m
[32m[2022-09-08 12:43:55,029] [    INFO][0m - loss: 0.66509647, learning_rate: 9.4e-06, global_step: 60, interval_runtime: 0.8423, interval_samples_per_second: 9.498, interval_steps_per_second: 11.872, epoch: 3.0[0m
[32m[2022-09-08 12:43:55,938] [    INFO][0m - loss: 0.63381958, learning_rate: 9.3e-06, global_step: 70, interval_runtime: 0.9092, interval_samples_per_second: 8.798, interval_steps_per_second: 10.998, epoch: 3.5[0m
[32m[2022-09-08 12:43:56,771] [    INFO][0m - loss: 0.71843314, learning_rate: 9.200000000000002e-06, global_step: 80, interval_runtime: 0.8334, interval_samples_per_second: 9.599, interval_steps_per_second: 11.999, epoch: 4.0[0m
[32m[2022-09-08 12:43:57,676] [    INFO][0m - loss: 0.61135383, learning_rate: 9.100000000000001e-06, global_step: 90, interval_runtime: 0.9048, interval_samples_per_second: 8.841, interval_steps_per_second: 11.052, epoch: 4.5[0m
[32m[2022-09-08 12:43:58,507] [    INFO][0m - loss: 0.55169649, learning_rate: 9e-06, global_step: 100, interval_runtime: 0.8315, interval_samples_per_second: 9.621, interval_steps_per_second: 12.026, epoch: 5.0[0m
[32m[2022-09-08 12:43:58,508] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:43:58,508] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-08 12:43:58,508] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:43:58,508] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:43:58,508] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-08 12:43:59,350] [    INFO][0m - eval_loss: 0.6594870686531067, eval_accuracy: 0.6289308071136475, eval_runtime: 0.8414, eval_samples_per_second: 188.965, eval_steps_per_second: 23.769, epoch: 5.0[0m
[32m[2022-09-08 12:43:59,356] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-09-08 12:43:59,356] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:44:01,984] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-08 12:44:01,984] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-08 12:44:08,515] [    INFO][0m - loss: 0.49743433, learning_rate: 8.900000000000001e-06, global_step: 110, interval_runtime: 10.0072, interval_samples_per_second: 0.799, interval_steps_per_second: 0.999, epoch: 5.5[0m
[32m[2022-09-08 12:44:09,345] [    INFO][0m - loss: 0.49567528, learning_rate: 8.8e-06, global_step: 120, interval_runtime: 0.8302, interval_samples_per_second: 9.637, interval_steps_per_second: 12.046, epoch: 6.0[0m
[32m[2022-09-08 12:44:10,249] [    INFO][0m - loss: 0.38823729, learning_rate: 8.700000000000001e-06, global_step: 130, interval_runtime: 0.9042, interval_samples_per_second: 8.847, interval_steps_per_second: 11.059, epoch: 6.5[0m
[32m[2022-09-08 12:44:11,080] [    INFO][0m - loss: 0.3596184, learning_rate: 8.6e-06, global_step: 140, interval_runtime: 0.8313, interval_samples_per_second: 9.624, interval_steps_per_second: 12.03, epoch: 7.0[0m
[32m[2022-09-08 12:44:11,998] [    INFO][0m - loss: 0.26789868, learning_rate: 8.5e-06, global_step: 150, interval_runtime: 0.9178, interval_samples_per_second: 8.717, interval_steps_per_second: 10.896, epoch: 7.5[0m
[32m[2022-09-08 12:44:12,831] [    INFO][0m - loss: 0.16829373, learning_rate: 8.400000000000001e-06, global_step: 160, interval_runtime: 0.8334, interval_samples_per_second: 9.599, interval_steps_per_second: 11.999, epoch: 8.0[0m
[32m[2022-09-08 12:44:13,744] [    INFO][0m - loss: 0.19186865, learning_rate: 8.3e-06, global_step: 170, interval_runtime: 0.9125, interval_samples_per_second: 8.767, interval_steps_per_second: 10.959, epoch: 8.5[0m
[32m[2022-09-08 12:44:14,575] [    INFO][0m - loss: 0.18722756, learning_rate: 8.2e-06, global_step: 180, interval_runtime: 0.8313, interval_samples_per_second: 9.623, interval_steps_per_second: 12.029, epoch: 9.0[0m
[32m[2022-09-08 12:44:15,480] [    INFO][0m - loss: 0.09403107, learning_rate: 8.1e-06, global_step: 190, interval_runtime: 0.9051, interval_samples_per_second: 8.839, interval_steps_per_second: 11.049, epoch: 9.5[0m
[32m[2022-09-08 12:44:16,312] [    INFO][0m - loss: 0.07947445, learning_rate: 8.000000000000001e-06, global_step: 200, interval_runtime: 0.8313, interval_samples_per_second: 9.623, interval_steps_per_second: 12.029, epoch: 10.0[0m
[32m[2022-09-08 12:44:16,313] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:44:16,313] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-08 12:44:16,313] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:44:16,313] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:44:16,313] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-08 12:44:17,137] [    INFO][0m - eval_loss: 1.4086345434188843, eval_accuracy: 0.654088020324707, eval_runtime: 0.8233, eval_samples_per_second: 193.117, eval_steps_per_second: 24.291, epoch: 10.0[0m
[32m[2022-09-08 12:44:17,144] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-09-08 12:44:17,144] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:44:20,351] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-09-08 12:44:20,352] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-09-08 12:44:26,660] [    INFO][0m - loss: 0.03686401, learning_rate: 7.9e-06, global_step: 210, interval_runtime: 10.3487, interval_samples_per_second: 0.773, interval_steps_per_second: 0.966, epoch: 10.5[0m
[32m[2022-09-08 12:44:27,495] [    INFO][0m - loss: 0.11092187, learning_rate: 7.800000000000002e-06, global_step: 220, interval_runtime: 0.8341, interval_samples_per_second: 9.591, interval_steps_per_second: 11.989, epoch: 11.0[0m
[32m[2022-09-08 12:44:28,394] [    INFO][0m - loss: 0.06939887, learning_rate: 7.7e-06, global_step: 230, interval_runtime: 0.9, interval_samples_per_second: 8.889, interval_steps_per_second: 11.111, epoch: 11.5[0m
[32m[2022-09-08 12:44:29,225] [    INFO][0m - loss: 0.07227685, learning_rate: 7.600000000000001e-06, global_step: 240, interval_runtime: 0.8308, interval_samples_per_second: 9.629, interval_steps_per_second: 12.036, epoch: 12.0[0m
[32m[2022-09-08 12:44:30,129] [    INFO][0m - loss: 0.11211735, learning_rate: 7.500000000000001e-06, global_step: 250, interval_runtime: 0.9041, interval_samples_per_second: 8.849, interval_steps_per_second: 11.061, epoch: 12.5[0m
[32m[2022-09-08 12:44:30,962] [    INFO][0m - loss: 0.11923018, learning_rate: 7.4e-06, global_step: 260, interval_runtime: 0.8327, interval_samples_per_second: 9.607, interval_steps_per_second: 12.009, epoch: 13.0[0m
[32m[2022-09-08 12:44:31,876] [    INFO][0m - loss: 0.00419093, learning_rate: 7.3e-06, global_step: 270, interval_runtime: 0.9143, interval_samples_per_second: 8.75, interval_steps_per_second: 10.938, epoch: 13.5[0m
[32m[2022-09-08 12:44:32,709] [    INFO][0m - loss: 0.07820295, learning_rate: 7.2000000000000005e-06, global_step: 280, interval_runtime: 0.8322, interval_samples_per_second: 9.613, interval_steps_per_second: 12.016, epoch: 14.0[0m
[32m[2022-09-08 12:44:33,613] [    INFO][0m - loss: 0.13527248, learning_rate: 7.100000000000001e-06, global_step: 290, interval_runtime: 0.9047, interval_samples_per_second: 8.843, interval_steps_per_second: 11.054, epoch: 14.5[0m
[32m[2022-09-08 12:44:34,444] [    INFO][0m - loss: 0.07238474, learning_rate: 7e-06, global_step: 300, interval_runtime: 0.8305, interval_samples_per_second: 9.632, interval_steps_per_second: 12.04, epoch: 15.0[0m
[32m[2022-09-08 12:44:34,444] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:44:34,444] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-08 12:44:34,444] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:44:34,445] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:44:34,445] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-08 12:44:35,263] [    INFO][0m - eval_loss: 1.9470773935317993, eval_accuracy: 0.6729559302330017, eval_runtime: 0.8177, eval_samples_per_second: 194.439, eval_steps_per_second: 24.458, epoch: 15.0[0m
[32m[2022-09-08 12:44:35,269] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-09-08 12:44:35,269] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:44:38,428] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-09-08 12:44:38,428] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-09-08 12:44:44,849] [    INFO][0m - loss: 0.00139206, learning_rate: 6.9e-06, global_step: 310, interval_runtime: 10.4053, interval_samples_per_second: 0.769, interval_steps_per_second: 0.961, epoch: 15.5[0m
[32m[2022-09-08 12:44:45,678] [    INFO][0m - loss: 0.02685223, learning_rate: 6.800000000000001e-06, global_step: 320, interval_runtime: 0.8291, interval_samples_per_second: 9.649, interval_steps_per_second: 12.062, epoch: 16.0[0m
[32m[2022-09-08 12:44:46,590] [    INFO][0m - loss: 0.00527715, learning_rate: 6.700000000000001e-06, global_step: 330, interval_runtime: 0.9118, interval_samples_per_second: 8.774, interval_steps_per_second: 10.968, epoch: 16.5[0m
[32m[2022-09-08 12:44:47,423] [    INFO][0m - loss: 0.07868731, learning_rate: 6.600000000000001e-06, global_step: 340, interval_runtime: 0.8335, interval_samples_per_second: 9.598, interval_steps_per_second: 11.998, epoch: 17.0[0m
[32m[2022-09-08 12:44:48,326] [    INFO][0m - loss: 0.01319671, learning_rate: 6.5000000000000004e-06, global_step: 350, interval_runtime: 0.9028, interval_samples_per_second: 8.862, interval_steps_per_second: 11.077, epoch: 17.5[0m
[32m[2022-09-08 12:44:49,157] [    INFO][0m - loss: 0.04114558, learning_rate: 6.4000000000000006e-06, global_step: 360, interval_runtime: 0.8304, interval_samples_per_second: 9.633, interval_steps_per_second: 12.042, epoch: 18.0[0m
[32m[2022-09-08 12:44:50,070] [    INFO][0m - loss: 0.01999157, learning_rate: 6.300000000000001e-06, global_step: 370, interval_runtime: 0.9132, interval_samples_per_second: 8.761, interval_steps_per_second: 10.951, epoch: 18.5[0m
[32m[2022-09-08 12:44:50,900] [    INFO][0m - loss: 0.04561737, learning_rate: 6.200000000000001e-06, global_step: 380, interval_runtime: 0.8298, interval_samples_per_second: 9.641, interval_steps_per_second: 12.051, epoch: 19.0[0m
[32m[2022-09-08 12:44:51,807] [    INFO][0m - loss: 0.0366881, learning_rate: 6.1e-06, global_step: 390, interval_runtime: 0.9067, interval_samples_per_second: 8.823, interval_steps_per_second: 11.029, epoch: 19.5[0m
[32m[2022-09-08 12:44:52,640] [    INFO][0m - loss: 0.10860392, learning_rate: 6e-06, global_step: 400, interval_runtime: 0.8338, interval_samples_per_second: 9.595, interval_steps_per_second: 11.994, epoch: 20.0[0m
[32m[2022-09-08 12:44:52,640] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:44:52,641] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-08 12:44:52,641] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:44:52,641] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:44:52,641] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-08 12:44:53,460] [    INFO][0m - eval_loss: 2.219977378845215, eval_accuracy: 0.6477987170219421, eval_runtime: 0.8188, eval_samples_per_second: 194.183, eval_steps_per_second: 24.426, epoch: 20.0[0m
[32m[2022-09-08 12:44:53,466] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-09-08 12:44:53,466] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:44:56,603] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-09-08 12:44:56,604] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-09-08 12:45:02,600] [    INFO][0m - loss: 0.05952834, learning_rate: 5.9e-06, global_step: 410, interval_runtime: 9.96, interval_samples_per_second: 0.803, interval_steps_per_second: 1.004, epoch: 20.5[0m
[32m[2022-09-08 12:45:03,437] [    INFO][0m - loss: 0.03377681, learning_rate: 5.8e-06, global_step: 420, interval_runtime: 0.8366, interval_samples_per_second: 9.563, interval_steps_per_second: 11.953, epoch: 21.0[0m
[32m[2022-09-08 12:45:04,338] [    INFO][0m - loss: 0.0005702, learning_rate: 5.7e-06, global_step: 430, interval_runtime: 0.9009, interval_samples_per_second: 8.88, interval_steps_per_second: 11.1, epoch: 21.5[0m
[32m[2022-09-08 12:45:05,170] [    INFO][0m - loss: 0.0037967, learning_rate: 5.600000000000001e-06, global_step: 440, interval_runtime: 0.8324, interval_samples_per_second: 9.61, interval_steps_per_second: 12.013, epoch: 22.0[0m
[32m[2022-09-08 12:45:06,073] [    INFO][0m - loss: 0.01711031, learning_rate: 5.500000000000001e-06, global_step: 450, interval_runtime: 0.9034, interval_samples_per_second: 8.856, interval_steps_per_second: 11.069, epoch: 22.5[0m
[32m[2022-09-08 12:45:06,901] [    INFO][0m - loss: 0.00016569, learning_rate: 5.400000000000001e-06, global_step: 460, interval_runtime: 0.8275, interval_samples_per_second: 9.668, interval_steps_per_second: 12.085, epoch: 23.0[0m
[32m[2022-09-08 12:45:07,805] [    INFO][0m - loss: 0.02858829, learning_rate: 5.300000000000001e-06, global_step: 470, interval_runtime: 0.9039, interval_samples_per_second: 8.85, interval_steps_per_second: 11.063, epoch: 23.5[0m
[32m[2022-09-08 12:45:08,635] [    INFO][0m - loss: 0.02569544, learning_rate: 5.2e-06, global_step: 480, interval_runtime: 0.8297, interval_samples_per_second: 9.642, interval_steps_per_second: 12.052, epoch: 24.0[0m
[32m[2022-09-08 12:45:09,538] [    INFO][0m - loss: 0.00060605, learning_rate: 5.1e-06, global_step: 490, interval_runtime: 0.9035, interval_samples_per_second: 8.854, interval_steps_per_second: 11.068, epoch: 24.5[0m
[32m[2022-09-08 12:45:10,382] [    INFO][0m - loss: 0.05967309, learning_rate: 5e-06, global_step: 500, interval_runtime: 0.8443, interval_samples_per_second: 9.475, interval_steps_per_second: 11.844, epoch: 25.0[0m
[32m[2022-09-08 12:45:10,383] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:45:10,383] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-08 12:45:10,383] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:45:10,383] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:45:10,383] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-08 12:45:11,202] [    INFO][0m - eval_loss: 2.5827667713165283, eval_accuracy: 0.6666666269302368, eval_runtime: 0.8183, eval_samples_per_second: 194.316, eval_steps_per_second: 24.442, epoch: 25.0[0m
[32m[2022-09-08 12:45:11,208] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-09-08 12:45:11,208] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:45:14,345] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-09-08 12:45:14,345] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-09-08 12:45:20,640] [    INFO][0m - loss: 0.00073399, learning_rate: 4.9000000000000005e-06, global_step: 510, interval_runtime: 10.2574, interval_samples_per_second: 0.78, interval_steps_per_second: 0.975, epoch: 25.5[0m
[32m[2022-09-08 12:45:21,467] [    INFO][0m - loss: 0.00925938, learning_rate: 4.800000000000001e-06, global_step: 520, interval_runtime: 0.8274, interval_samples_per_second: 9.669, interval_steps_per_second: 12.086, epoch: 26.0[0m
[32m[2022-09-08 12:45:22,381] [    INFO][0m - loss: 0.00034078, learning_rate: 4.7e-06, global_step: 530, interval_runtime: 0.9137, interval_samples_per_second: 8.755, interval_steps_per_second: 10.944, epoch: 26.5[0m
[32m[2022-09-08 12:45:23,213] [    INFO][0m - loss: 0.01152925, learning_rate: 4.600000000000001e-06, global_step: 540, interval_runtime: 0.8315, interval_samples_per_second: 9.621, interval_steps_per_second: 12.026, epoch: 27.0[0m
[32m[2022-09-08 12:45:24,124] [    INFO][0m - loss: 0.0001566, learning_rate: 4.5e-06, global_step: 550, interval_runtime: 0.9111, interval_samples_per_second: 8.78, interval_steps_per_second: 10.976, epoch: 27.5[0m
[32m[2022-09-08 12:45:24,954] [    INFO][0m - loss: 0.00363196, learning_rate: 4.4e-06, global_step: 560, interval_runtime: 0.8303, interval_samples_per_second: 9.635, interval_steps_per_second: 12.044, epoch: 28.0[0m
[32m[2022-09-08 12:45:25,853] [    INFO][0m - loss: 0.00073107, learning_rate: 4.3e-06, global_step: 570, interval_runtime: 0.8986, interval_samples_per_second: 8.903, interval_steps_per_second: 11.129, epoch: 28.5[0m
[32m[2022-09-08 12:45:26,683] [    INFO][0m - loss: 0.00627895, learning_rate: 4.2000000000000004e-06, global_step: 580, interval_runtime: 0.8303, interval_samples_per_second: 9.635, interval_steps_per_second: 12.043, epoch: 29.0[0m
[32m[2022-09-08 12:45:27,587] [    INFO][0m - loss: 0.00031491, learning_rate: 4.1e-06, global_step: 590, interval_runtime: 0.9042, interval_samples_per_second: 8.847, interval_steps_per_second: 11.059, epoch: 29.5[0m
[32m[2022-09-08 12:45:28,422] [    INFO][0m - loss: 9.826e-05, learning_rate: 4.000000000000001e-06, global_step: 600, interval_runtime: 0.8337, interval_samples_per_second: 9.595, interval_steps_per_second: 11.994, epoch: 30.0[0m
[32m[2022-09-08 12:45:28,423] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:45:28,423] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-08 12:45:28,423] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:45:28,423] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:45:28,423] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-08 12:45:29,242] [    INFO][0m - eval_loss: 2.6148505210876465, eval_accuracy: 0.6477987170219421, eval_runtime: 0.8192, eval_samples_per_second: 194.088, eval_steps_per_second: 24.414, epoch: 30.0[0m
[32m[2022-09-08 12:45:29,248] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-09-08 12:45:29,249] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:45:32,417] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-09-08 12:45:32,417] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-09-08 12:45:38,623] [    INFO][0m - loss: 0.00011331, learning_rate: 3.900000000000001e-06, global_step: 610, interval_runtime: 10.2022, interval_samples_per_second: 0.784, interval_steps_per_second: 0.98, epoch: 30.5[0m
[32m[2022-09-08 12:45:39,452] [    INFO][0m - loss: 0.10807854, learning_rate: 3.8000000000000005e-06, global_step: 620, interval_runtime: 0.8286, interval_samples_per_second: 9.655, interval_steps_per_second: 12.068, epoch: 31.0[0m
[32m[2022-09-08 12:45:40,358] [    INFO][0m - loss: 8.22e-05, learning_rate: 3.7e-06, global_step: 630, interval_runtime: 0.9057, interval_samples_per_second: 8.833, interval_steps_per_second: 11.042, epoch: 31.5[0m
[32m[2022-09-08 12:45:41,188] [    INFO][0m - loss: 0.00010424, learning_rate: 3.6000000000000003e-06, global_step: 640, interval_runtime: 0.8306, interval_samples_per_second: 9.632, interval_steps_per_second: 12.04, epoch: 32.0[0m
[32m[2022-09-08 12:45:42,104] [    INFO][0m - loss: 0.00030769, learning_rate: 3.5e-06, global_step: 650, interval_runtime: 0.9163, interval_samples_per_second: 8.731, interval_steps_per_second: 10.913, epoch: 32.5[0m
[32m[2022-09-08 12:45:42,936] [    INFO][0m - loss: 0.00012814, learning_rate: 3.4000000000000005e-06, global_step: 660, interval_runtime: 0.8321, interval_samples_per_second: 9.614, interval_steps_per_second: 12.018, epoch: 33.0[0m
[32m[2022-09-08 12:45:43,838] [    INFO][0m - loss: 0.00011073, learning_rate: 3.3000000000000006e-06, global_step: 670, interval_runtime: 0.9018, interval_samples_per_second: 8.871, interval_steps_per_second: 11.089, epoch: 33.5[0m
[32m[2022-09-08 12:45:44,669] [    INFO][0m - loss: 0.00285854, learning_rate: 3.2000000000000003e-06, global_step: 680, interval_runtime: 0.831, interval_samples_per_second: 9.627, interval_steps_per_second: 12.034, epoch: 34.0[0m
[32m[2022-09-08 12:45:45,576] [    INFO][0m - loss: 7.065e-05, learning_rate: 3.1000000000000004e-06, global_step: 690, interval_runtime: 0.9066, interval_samples_per_second: 8.824, interval_steps_per_second: 11.03, epoch: 34.5[0m
[32m[2022-09-08 12:45:46,408] [    INFO][0m - loss: 7.891e-05, learning_rate: 3e-06, global_step: 700, interval_runtime: 0.8326, interval_samples_per_second: 9.609, interval_steps_per_second: 12.011, epoch: 35.0[0m
[32m[2022-09-08 12:45:46,409] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-08 12:45:46,409] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-08 12:45:46,409] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:45:46,409] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:45:46,409] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-08 12:45:47,231] [    INFO][0m - eval_loss: 2.628679037094116, eval_accuracy: 0.6477987170219421, eval_runtime: 0.8214, eval_samples_per_second: 193.569, eval_steps_per_second: 24.348, epoch: 35.0[0m
[32m[2022-09-08 12:45:47,237] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-700[0m
[32m[2022-09-08 12:45:47,237] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:45:50,298] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-700/tokenizer_config.json[0m
[32m[2022-09-08 12:45:50,301] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-700/special_tokens_map.json[0m
[32m[2022-09-08 12:45:55,587] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-08 12:45:55,587] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-300 (score: 0.6729559302330017).[0m
[32m[2022-09-08 12:45:56,619] [    INFO][0m - train_runtime: 127.7508, train_samples_per_second: 62.622, train_steps_per_second: 7.828, train_loss: 0.48561554414561087, epoch: 35.0[0m
[32m[2022-09-08 12:45:56,659] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-09-08 12:45:56,660] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-08 12:46:00,068] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-09-08 12:46:00,068] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-09-08 12:46:00,070] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-08 12:46:00,070] [    INFO][0m -   epoch                    =       35.0[0m
[32m[2022-09-08 12:46:00,070] [    INFO][0m -   train_loss               =     0.4856[0m
[32m[2022-09-08 12:46:00,070] [    INFO][0m -   train_runtime            = 0:02:07.75[0m
[32m[2022-09-08 12:46:00,070] [    INFO][0m -   train_samples_per_second =     62.622[0m
[32m[2022-09-08 12:46:00,070] [    INFO][0m -   train_steps_per_second   =      7.828[0m
[32m[2022-09-08 12:46:00,074] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-08 12:46:00,074] [    INFO][0m -   Num examples = 976[0m
[32m[2022-09-08 12:46:00,074] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:46:00,074] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:46:00,074] [    INFO][0m -   Total prediction steps = 122[0m
[32m[2022-09-08 12:46:05,172] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-08 12:46:05,173] [    INFO][0m -   test_accuracy           =     0.5625[0m
[32m[2022-09-08 12:46:05,173] [    INFO][0m -   test_loss               =     3.1266[0m
[32m[2022-09-08 12:46:05,173] [    INFO][0m -   test_runtime            = 0:00:05.09[0m
[32m[2022-09-08 12:46:05,173] [    INFO][0m -   test_samples_per_second =    191.437[0m
[32m[2022-09-08 12:46:05,173] [    INFO][0m -   test_steps_per_second   =      23.93[0m
[32m[2022-09-08 12:46:05,174] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-08 12:46:05,174] [    INFO][0m -   Num examples = 290[0m
[32m[2022-09-08 12:46:05,174] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-08 12:46:05,174] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-08 12:46:05,174] [    INFO][0m -   Total prediction steps = 37[0m
Traceback (most recent call last):
  File "train_single.py", line 268, in <module>
    main(0)
  File "train_single.py", line 262, in main
    test_ret = postprocess(test_ret, test_ds, data_args.task_name,
  File "/ssd2/wanghuijuan03/prompt/PaddleNLP/examples/benchmark/fewclue/pretrain15/postprocess.py", line 66, in postprocess
    ret_list.append({"id": uid, "label": id_to_label[preds[idx]]})
TypeError: unhashable type: 'numpy.ndarray'
run.sh: line 59: --freeze_plm: command not found
