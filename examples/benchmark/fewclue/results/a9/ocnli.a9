[33m[2022-08-26 14:22:29,103] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-26 14:22:29,104] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-26 14:22:29,104] [    INFO][0m - ============================================================[0m
[32m[2022-08-26 14:22:29,104] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-26 14:22:29,104] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-26 14:22:29,104] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-26 14:22:29,104] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-26 14:22:29,104] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-08-26 14:22:29,104] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-26 14:22:29,104] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-08-26 14:22:29,104] [    INFO][0m - [0m
[32m[2022-08-26 14:22:29,105] [    INFO][0m - ============================================================[0m
[32m[2022-08-26 14:22:29,105] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-26 14:22:29,105] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-26 14:22:29,105] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-26 14:22:29,105] [    INFO][0m - prompt                        :{'hard':'ËØ∑Áî®Ê≠£Á°ÆÁöÑËøûÊé•ËØçÂ°´Á©∫Ôºö'}{'text':'text_a'}{'mask'}{'mask'}{'text':'text_b'}[0m
[32m[2022-08-26 14:22:29,105] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-26 14:22:29,105] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-26 14:22:29,105] [    INFO][0m - task_name                     :ocnli[0m
[32m[2022-08-26 14:22:29,105] [    INFO][0m - [0m
[32m[2022-08-26 14:22:29,105] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0826 14:22:29.106935 80977 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0826 14:22:29.112016 80977 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-26 14:22:31,674] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-08-26 14:22:31,698] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-08-26 14:22:31,699] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[32m[2022-08-26 14:22:31,700] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': 'ËØ∑Áî®Ê≠£Á°ÆÁöÑËøûÊé•ËØçÂ°´Á©∫Ôºö'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'text': 'text_b'}][0m
2022-08-26 14:22:31,704 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-26 14:22:31,829] [    INFO][0m - ============================================================[0m
[32m[2022-08-26 14:22:31,829] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-26 14:22:31,829] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-26 14:22:31,829] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-26 14:22:31,830] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-26 14:22:31,830] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-26 14:22:31,830] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-26 14:22:31,830] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-26 14:22:31,830] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-26 14:22:31,830] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-26 14:22:31,830] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-26 14:22:31,830] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-26 14:22:31,830] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-26 14:22:31,830] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-26 14:22:31,830] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-26 14:22:31,831] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-26 14:22:31,831] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-26 14:22:31,831] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-26 14:22:31,831] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-26 14:22:31,831] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-26 14:22:31,831] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-26 14:22:31,831] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-26 14:22:31,831] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-26 14:22:31,831] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-26 14:22:31,831] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-26 14:22:31,831] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-26 14:22:31,831] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-26 14:22:31,831] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-26 14:22:31,831] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-26 14:22:31,832] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-26 14:22:31,832] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-26 14:22:31,832] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-26 14:22:31,832] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-26 14:22:31,832] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-26 14:22:31,832] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-26 14:22:31,832] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-26 14:22:31,832] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-26 14:22:31,832] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug26_14-22-29_instance-3bwob41y-01[0m
[32m[2022-08-26 14:22:31,832] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-26 14:22:31,832] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-26 14:22:31,832] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-26 14:22:31,832] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-26 14:22:31,833] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-26 14:22:31,833] [    INFO][0m - max_seq_length                :64[0m
[32m[2022-08-26 14:22:31,833] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-26 14:22:31,833] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-26 14:22:31,833] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-26 14:22:31,833] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-26 14:22:31,833] [    INFO][0m - num_train_epochs              :20.0[0m
[32m[2022-08-26 14:22:31,833] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-26 14:22:31,833] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-26 14:22:31,833] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-26 14:22:31,833] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-26 14:22:31,833] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-26 14:22:31,833] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-26 14:22:31,833] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-26 14:22:31,834] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-26 14:22:31,834] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-26 14:22:31,834] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-26 14:22:31,834] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-26 14:22:31,834] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-26 14:22:31,834] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-26 14:22:31,834] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-26 14:22:31,834] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-26 14:22:31,834] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-26 14:22:31,834] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-26 14:22:31,834] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-26 14:22:31,834] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-26 14:22:31,834] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-26 14:22:31,835] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-26 14:22:31,835] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-26 14:22:31,835] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-26 14:22:31,835] [    INFO][0m - seed                          :42[0m
[32m[2022-08-26 14:22:31,835] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-26 14:22:31,835] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-26 14:22:31,835] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-26 14:22:31,835] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-26 14:22:31,835] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-26 14:22:31,835] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-26 14:22:31,835] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-26 14:22:31,835] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-26 14:22:31,835] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-26 14:22:31,835] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-26 14:22:31,836] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-26 14:22:31,836] [    INFO][0m - [0m
[32m[2022-08-26 14:22:31,837] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-26 14:22:31,837] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 14:22:31,837] [    INFO][0m -   Num Epochs = 20[0m
[32m[2022-08-26 14:22:31,837] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-26 14:22:31,838] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-26 14:22:31,838] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-26 14:22:31,838] [    INFO][0m -   Total optimization steps = 400.0[0m
[32m[2022-08-26 14:22:31,838] [    INFO][0m -   Total num train samples = 3200[0m
[32m[2022-08-26 14:22:33,384] [    INFO][0m - loss: 1.28649311, learning_rate: 2.925e-05, global_step: 10, interval_runtime: 1.5446, interval_samples_per_second: 5.179, interval_steps_per_second: 6.474, epoch: 0.5[0m
[32m[2022-08-26 14:22:33,941] [    INFO][0m - loss: 1.20911655, learning_rate: 2.8499999999999998e-05, global_step: 20, interval_runtime: 0.5581, interval_samples_per_second: 14.335, interval_steps_per_second: 17.919, epoch: 1.0[0m
[32m[2022-08-26 14:22:34,563] [    INFO][0m - loss: 1.11587477, learning_rate: 2.7750000000000004e-05, global_step: 30, interval_runtime: 0.6221, interval_samples_per_second: 12.859, interval_steps_per_second: 16.074, epoch: 1.5[0m
[32m[2022-08-26 14:22:35,125] [    INFO][0m - loss: 1.09805641, learning_rate: 2.7000000000000002e-05, global_step: 40, interval_runtime: 0.5616, interval_samples_per_second: 14.244, interval_steps_per_second: 17.806, epoch: 2.0[0m
[32m[2022-08-26 14:22:35,741] [    INFO][0m - loss: 0.93875399, learning_rate: 2.625e-05, global_step: 50, interval_runtime: 0.6157, interval_samples_per_second: 12.993, interval_steps_per_second: 16.241, epoch: 2.5[0m
[32m[2022-08-26 14:22:36,297] [    INFO][0m - loss: 0.90050135, learning_rate: 2.55e-05, global_step: 60, interval_runtime: 0.5561, interval_samples_per_second: 14.386, interval_steps_per_second: 17.982, epoch: 3.0[0m
[32m[2022-08-26 14:22:36,912] [    INFO][0m - loss: 0.68949404, learning_rate: 2.475e-05, global_step: 70, interval_runtime: 0.6152, interval_samples_per_second: 13.003, interval_steps_per_second: 16.254, epoch: 3.5[0m
[32m[2022-08-26 14:22:37,459] [    INFO][0m - loss: 0.74376426, learning_rate: 2.4e-05, global_step: 80, interval_runtime: 0.547, interval_samples_per_second: 14.624, interval_steps_per_second: 18.28, epoch: 4.0[0m
[32m[2022-08-26 14:22:38,068] [    INFO][0m - loss: 0.5177145, learning_rate: 2.3250000000000003e-05, global_step: 90, interval_runtime: 0.6095, interval_samples_per_second: 13.125, interval_steps_per_second: 16.407, epoch: 4.5[0m
[32m[2022-08-26 14:22:38,620] [    INFO][0m - loss: 0.50249124, learning_rate: 2.25e-05, global_step: 100, interval_runtime: 0.5511, interval_samples_per_second: 14.516, interval_steps_per_second: 18.145, epoch: 5.0[0m
[32m[2022-08-26 14:22:38,620] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 14:22:38,620] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 14:22:38,620] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 14:22:38,621] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 14:22:38,621] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 14:22:39,013] [    INFO][0m - eval_loss: 1.4888839721679688, eval_accuracy: 0.4375, eval_runtime: 0.3926, eval_samples_per_second: 407.577, eval_steps_per_second: 12.737, epoch: 5.0[0m
[32m[2022-08-26 14:22:39,014] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-26 14:22:39,014] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 14:22:42,225] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-26 14:22:42,225] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-26 14:22:48,224] [    INFO][0m - loss: 0.38102548, learning_rate: 2.175e-05, global_step: 110, interval_runtime: 9.6044, interval_samples_per_second: 0.833, interval_steps_per_second: 1.041, epoch: 5.5[0m
[32m[2022-08-26 14:22:48,773] [    INFO][0m - loss: 0.29651716, learning_rate: 2.1e-05, global_step: 120, interval_runtime: 0.5486, interval_samples_per_second: 14.582, interval_steps_per_second: 18.227, epoch: 6.0[0m
[32m[2022-08-26 14:22:49,384] [    INFO][0m - loss: 0.2298142, learning_rate: 2.025e-05, global_step: 130, interval_runtime: 0.6111, interval_samples_per_second: 13.091, interval_steps_per_second: 16.364, epoch: 6.5[0m
[32m[2022-08-26 14:22:49,938] [    INFO][0m - loss: 0.19517717, learning_rate: 1.95e-05, global_step: 140, interval_runtime: 0.5538, interval_samples_per_second: 14.446, interval_steps_per_second: 18.058, epoch: 7.0[0m
[32m[2022-08-26 14:22:50,551] [    INFO][0m - loss: 0.11257952, learning_rate: 1.8750000000000002e-05, global_step: 150, interval_runtime: 0.6135, interval_samples_per_second: 13.04, interval_steps_per_second: 16.3, epoch: 7.5[0m
[32m[2022-08-26 14:22:51,099] [    INFO][0m - loss: 0.11139786, learning_rate: 1.8e-05, global_step: 160, interval_runtime: 0.5484, interval_samples_per_second: 14.589, interval_steps_per_second: 18.236, epoch: 8.0[0m
[32m[2022-08-26 14:22:51,713] [    INFO][0m - loss: 0.09447997, learning_rate: 1.725e-05, global_step: 170, interval_runtime: 0.6134, interval_samples_per_second: 13.042, interval_steps_per_second: 16.303, epoch: 8.5[0m
[32m[2022-08-26 14:22:52,270] [    INFO][0m - loss: 0.0427792, learning_rate: 1.65e-05, global_step: 180, interval_runtime: 0.5573, interval_samples_per_second: 14.356, interval_steps_per_second: 17.945, epoch: 9.0[0m
[32m[2022-08-26 14:22:52,890] [    INFO][0m - loss: 0.13375099, learning_rate: 1.575e-05, global_step: 190, interval_runtime: 0.6197, interval_samples_per_second: 12.909, interval_steps_per_second: 16.137, epoch: 9.5[0m
[32m[2022-08-26 14:22:53,447] [    INFO][0m - loss: 0.02495731, learning_rate: 1.5e-05, global_step: 200, interval_runtime: 0.5566, interval_samples_per_second: 14.374, interval_steps_per_second: 17.968, epoch: 10.0[0m
[32m[2022-08-26 14:22:53,447] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 14:22:53,447] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 14:22:53,448] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 14:22:53,448] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 14:22:53,448] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 14:22:53,844] [    INFO][0m - eval_loss: 2.633897304534912, eval_accuracy: 0.48125, eval_runtime: 0.3962, eval_samples_per_second: 403.881, eval_steps_per_second: 12.621, epoch: 10.0[0m
[32m[2022-08-26 14:22:53,844] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-26 14:22:53,845] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 14:22:56,855] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-26 14:22:56,855] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-26 14:23:02,942] [    INFO][0m - loss: 0.05076677, learning_rate: 1.4249999999999999e-05, global_step: 210, interval_runtime: 9.495, interval_samples_per_second: 0.843, interval_steps_per_second: 1.053, epoch: 10.5[0m
[32m[2022-08-26 14:23:03,493] [    INFO][0m - loss: 0.01672703, learning_rate: 1.3500000000000001e-05, global_step: 220, interval_runtime: 0.5513, interval_samples_per_second: 14.512, interval_steps_per_second: 18.139, epoch: 11.0[0m
[32m[2022-08-26 14:23:04,120] [    INFO][0m - loss: 0.03283118, learning_rate: 1.275e-05, global_step: 230, interval_runtime: 0.6275, interval_samples_per_second: 12.749, interval_steps_per_second: 15.936, epoch: 11.5[0m
[32m[2022-08-26 14:23:04,701] [    INFO][0m - loss: 0.01361423, learning_rate: 1.2e-05, global_step: 240, interval_runtime: 0.581, interval_samples_per_second: 13.77, interval_steps_per_second: 17.213, epoch: 12.0[0m
[32m[2022-08-26 14:23:05,347] [    INFO][0m - loss: 0.00239527, learning_rate: 1.125e-05, global_step: 250, interval_runtime: 0.6453, interval_samples_per_second: 12.397, interval_steps_per_second: 15.496, epoch: 12.5[0m
[32m[2022-08-26 14:23:05,927] [    INFO][0m - loss: 0.00116542, learning_rate: 1.05e-05, global_step: 260, interval_runtime: 0.5796, interval_samples_per_second: 13.803, interval_steps_per_second: 17.254, epoch: 13.0[0m
[32m[2022-08-26 14:23:06,565] [    INFO][0m - loss: 0.0028287, learning_rate: 9.75e-06, global_step: 270, interval_runtime: 0.6386, interval_samples_per_second: 12.528, interval_steps_per_second: 15.66, epoch: 13.5[0m
[32m[2022-08-26 14:23:07,127] [    INFO][0m - loss: 0.07242276, learning_rate: 9e-06, global_step: 280, interval_runtime: 0.5623, interval_samples_per_second: 14.227, interval_steps_per_second: 17.783, epoch: 14.0[0m
[32m[2022-08-26 14:23:07,781] [    INFO][0m - loss: 0.00343318, learning_rate: 8.25e-06, global_step: 290, interval_runtime: 0.6539, interval_samples_per_second: 12.234, interval_steps_per_second: 15.292, epoch: 14.5[0m
[32m[2022-08-26 14:23:08,348] [    INFO][0m - loss: 0.02047596, learning_rate: 7.5e-06, global_step: 300, interval_runtime: 0.5671, interval_samples_per_second: 14.108, interval_steps_per_second: 17.635, epoch: 15.0[0m
[32m[2022-08-26 14:23:08,349] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 14:23:08,349] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 14:23:08,349] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 14:23:08,349] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 14:23:08,349] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 14:23:08,750] [    INFO][0m - eval_loss: 3.3583436012268066, eval_accuracy: 0.49375, eval_runtime: 0.4007, eval_samples_per_second: 399.331, eval_steps_per_second: 12.479, epoch: 15.0[0m
[32m[2022-08-26 14:23:08,750] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-26 14:23:08,751] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 14:23:12,027] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-26 14:23:12,028] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-26 14:23:17,818] [    INFO][0m - loss: 0.00417517, learning_rate: 6.750000000000001e-06, global_step: 310, interval_runtime: 9.4698, interval_samples_per_second: 0.845, interval_steps_per_second: 1.056, epoch: 15.5[0m
[32m[2022-08-26 14:23:18,373] [    INFO][0m - loss: 0.00094752, learning_rate: 6e-06, global_step: 320, interval_runtime: 0.5548, interval_samples_per_second: 14.418, interval_steps_per_second: 18.023, epoch: 16.0[0m
[32m[2022-08-26 14:23:18,995] [    INFO][0m - loss: 0.01067988, learning_rate: 5.25e-06, global_step: 330, interval_runtime: 0.6219, interval_samples_per_second: 12.863, interval_steps_per_second: 16.079, epoch: 16.5[0m
[32m[2022-08-26 14:23:19,562] [    INFO][0m - loss: 0.00031372, learning_rate: 4.5e-06, global_step: 340, interval_runtime: 0.5673, interval_samples_per_second: 14.102, interval_steps_per_second: 17.627, epoch: 17.0[0m
[32m[2022-08-26 14:23:20,188] [    INFO][0m - loss: 0.00345402, learning_rate: 3.75e-06, global_step: 350, interval_runtime: 0.6257, interval_samples_per_second: 12.785, interval_steps_per_second: 15.981, epoch: 17.5[0m
[32m[2022-08-26 14:23:20,753] [    INFO][0m - loss: 0.00012768, learning_rate: 3e-06, global_step: 360, interval_runtime: 0.5657, interval_samples_per_second: 14.142, interval_steps_per_second: 17.677, epoch: 18.0[0m
[32m[2022-08-26 14:23:21,376] [    INFO][0m - loss: 0.09174603, learning_rate: 2.25e-06, global_step: 370, interval_runtime: 0.6226, interval_samples_per_second: 12.85, interval_steps_per_second: 16.063, epoch: 18.5[0m
[32m[2022-08-26 14:23:21,942] [    INFO][0m - loss: 0.00593911, learning_rate: 1.5e-06, global_step: 380, interval_runtime: 0.5656, interval_samples_per_second: 14.143, interval_steps_per_second: 17.679, epoch: 19.0[0m
[32m[2022-08-26 14:23:22,574] [    INFO][0m - loss: 0.00015376, learning_rate: 7.5e-07, global_step: 390, interval_runtime: 0.632, interval_samples_per_second: 12.657, interval_steps_per_second: 15.822, epoch: 19.5[0m
[32m[2022-08-26 14:23:23,141] [    INFO][0m - loss: 0.00106102, learning_rate: 0.0, global_step: 400, interval_runtime: 0.5668, interval_samples_per_second: 14.115, interval_steps_per_second: 17.643, epoch: 20.0[0m
[32m[2022-08-26 14:23:23,141] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 14:23:23,142] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 14:23:23,142] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 14:23:23,142] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 14:23:23,142] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 14:23:23,541] [    INFO][0m - eval_loss: 3.4892685413360596, eval_accuracy: 0.5, eval_runtime: 0.399, eval_samples_per_second: 401.045, eval_steps_per_second: 12.533, epoch: 20.0[0m
[32m[2022-08-26 14:23:23,541] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-26 14:23:23,542] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 14:23:26,910] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-26 14:23:26,911] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-26 14:23:32,584] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-26 14:23:32,584] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-400 (score: 0.5).[0m
[32m[2022-08-26 14:23:33,367] [    INFO][0m - train_runtime: 61.5284, train_samples_per_second: 52.008, train_steps_per_second: 6.501, train_loss: 0.27399993637460285, epoch: 20.0[0m
[32m[2022-08-26 14:23:33,392] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-26 14:23:33,392] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 14:23:36,290] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-26 14:23:36,291] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-26 14:23:36,293] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-26 14:23:36,293] [    INFO][0m -   epoch                    =       20.0[0m
[32m[2022-08-26 14:23:36,293] [    INFO][0m -   train_loss               =      0.274[0m
[32m[2022-08-26 14:23:36,293] [    INFO][0m -   train_runtime            = 0:01:01.52[0m
[32m[2022-08-26 14:23:36,293] [    INFO][0m -   train_samples_per_second =     52.008[0m
[32m[2022-08-26 14:23:36,293] [    INFO][0m -   train_steps_per_second   =      6.501[0m
[32m[2022-08-26 14:23:36,296] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-26 14:23:36,296] [    INFO][0m -   Num examples = 2520[0m
[32m[2022-08-26 14:23:36,296] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 14:23:36,296] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 14:23:36,296] [    INFO][0m -   Total prediction steps = 79[0m
[32m[2022-08-26 14:23:42,465] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-26 14:23:42,466] [    INFO][0m -   test_accuracy           =     0.4817[0m
[32m[2022-08-26 14:23:42,466] [    INFO][0m -   test_loss               =     3.8092[0m
[32m[2022-08-26 14:23:42,466] [    INFO][0m -   test_runtime            = 0:00:06.16[0m
[32m[2022-08-26 14:23:42,466] [    INFO][0m -   test_samples_per_second =    408.465[0m
[32m[2022-08-26 14:23:42,466] [    INFO][0m -   test_steps_per_second   =     12.805[0m
[32m[2022-08-26 14:23:42,467] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-26 14:23:42,467] [    INFO][0m -   Num examples = 3000[0m
[32m[2022-08-26 14:23:42,467] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 14:23:42,467] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 14:23:42,467] [    INFO][0m -   Total prediction steps = 94[0m
[32m[2022-08-26 14:23:51,854] [    INFO][0m - Predictions for ocnlif saved to ./fewclue_submit_examples.[0m
