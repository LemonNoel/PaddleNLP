[33m[2022-08-31 15:44:30,739] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-31 15:44:30,739] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-31 15:44:30,739] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 15:44:30,740] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-31 15:44:30,740] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 15:44:30,740] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-31 15:44:30,740] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-31 15:44:30,740] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-08-31 15:44:30,740] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-31 15:44:30,740] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-08-31 15:44:30,740] [    INFO][0m - [0m
[32m[2022-08-31 15:44:30,740] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 15:44:30,740] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-31 15:44:30,740] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 15:44:30,740] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-31 15:44:30,740] [    INFO][0m - prompt                        :‚Äú{'text':'text_a'}‚ÄùÂíå‚Äú{'text':'text_b'}‚Äù‰πãÈó¥ÁöÑÈÄªËæëÂÖ≥Á≥ªÊòØ{'mask'}{'mask'}„ÄÇ[0m
[32m[2022-08-31 15:44:30,741] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-31 15:44:30,741] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-31 15:44:30,741] [    INFO][0m - task_name                     :bustm[0m
[32m[2022-08-31 15:44:30,741] [    INFO][0m - [0m
[32m[2022-08-31 15:44:30,741] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0831 15:44:30.742673 38953 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0831 15:44:30.746858 38953 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-31 15:44:33,798] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-08-31 15:44:33,824] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-08-31 15:44:33,824] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[32m[2022-08-31 15:44:33,826] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùÂíå‚Äú'}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': '‚Äù‰πãÈó¥ÁöÑÈÄªËæëÂÖ≥Á≥ªÊòØ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': '„ÄÇ'}][0m
[32m[2022-08-31 15:44:33,828] [    INFO][0m - {'0': 0, '1': 1}[0m
2022-08-31 15:44:33,830 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-31 15:44:35,010] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 15:44:35,011] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-31 15:44:35,011] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 15:44:35,011] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-31 15:44:35,011] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-31 15:44:35,011] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-31 15:44:35,011] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-31 15:44:35,011] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-31 15:44:35,011] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-31 15:44:35,011] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-31 15:44:35,012] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-31 15:44:35,012] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-31 15:44:35,012] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-31 15:44:35,012] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-31 15:44:35,012] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-31 15:44:35,012] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-31 15:44:35,012] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-31 15:44:35,012] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-31 15:44:35,012] [    INFO][0m - eval_batch_size               :64[0m
[32m[2022-08-31 15:44:35,012] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-31 15:44:35,012] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-31 15:44:35,012] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-31 15:44:35,012] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-31 15:44:35,013] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-31 15:44:35,013] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-31 15:44:35,013] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-31 15:44:35,013] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-31 15:44:35,013] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-31 15:44:35,013] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-31 15:44:35,013] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-31 15:44:35,013] [    INFO][0m - learning_rate                 :3e-06[0m
[32m[2022-08-31 15:44:35,013] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-31 15:44:35,013] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-31 15:44:35,013] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-31 15:44:35,013] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-31 15:44:35,013] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-31 15:44:35,013] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-31 15:44:35,014] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug31_15-44-30_instance-3bwob41y-01[0m
[32m[2022-08-31 15:44:35,014] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-31 15:44:35,014] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-31 15:44:35,014] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-31 15:44:35,014] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-31 15:44:35,014] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-31 15:44:35,014] [    INFO][0m - max_seq_length                :64[0m
[32m[2022-08-31 15:44:35,014] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-31 15:44:35,014] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-31 15:44:35,014] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-31 15:44:35,014] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-31 15:44:35,014] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-08-31 15:44:35,014] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-31 15:44:35,015] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-31 15:44:35,015] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-31 15:44:35,015] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-31 15:44:35,015] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-31 15:44:35,015] [    INFO][0m - per_device_eval_batch_size    :64[0m
[32m[2022-08-31 15:44:35,015] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-31 15:44:35,015] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-31 15:44:35,015] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-31 15:44:35,015] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-31 15:44:35,015] [    INFO][0m - ppt_learning_rate             :3e-05[0m
[32m[2022-08-31 15:44:35,015] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-31 15:44:35,015] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-31 15:44:35,015] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-31 15:44:35,016] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-31 15:44:35,016] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-31 15:44:35,016] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-31 15:44:35,016] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-31 15:44:35,016] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-31 15:44:35,016] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-31 15:44:35,016] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-31 15:44:35,016] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-31 15:44:35,016] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-31 15:44:35,016] [    INFO][0m - seed                          :42[0m
[32m[2022-08-31 15:44:35,016] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-31 15:44:35,016] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-31 15:44:35,016] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-31 15:44:35,016] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-31 15:44:35,017] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-31 15:44:35,017] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-31 15:44:35,017] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-31 15:44:35,017] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-31 15:44:35,017] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-31 15:44:35,017] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-31 15:44:35,017] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-31 15:44:35,017] [    INFO][0m - [0m
[32m[2022-08-31 15:44:35,019] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-31 15:44:35,019] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 15:44:35,019] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-08-31 15:44:35,019] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-31 15:44:35,019] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-31 15:44:35,019] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-31 15:44:35,019] [    INFO][0m -   Total optimization steps = 1000.0[0m
[32m[2022-08-31 15:44:35,019] [    INFO][0m -   Total num train samples = 8000[0m
[32m[2022-08-31 15:44:36,649] [    INFO][0m - loss: 0.88896751, learning_rate: 2.97e-06, global_step: 10, interval_runtime: 1.6289, interval_samples_per_second: 4.911, interval_steps_per_second: 6.139, epoch: 0.5[0m
[32m[2022-08-31 15:44:37,262] [    INFO][0m - loss: 0.64667411, learning_rate: 2.9400000000000002e-06, global_step: 20, interval_runtime: 0.6129, interval_samples_per_second: 13.052, interval_steps_per_second: 16.315, epoch: 1.0[0m
[32m[2022-08-31 15:44:37,993] [    INFO][0m - loss: 0.5511353, learning_rate: 2.91e-06, global_step: 30, interval_runtime: 0.7302, interval_samples_per_second: 10.956, interval_steps_per_second: 13.695, epoch: 1.5[0m
[32m[2022-08-31 15:44:38,596] [    INFO][0m - loss: 0.5265954, learning_rate: 2.88e-06, global_step: 40, interval_runtime: 0.6043, interval_samples_per_second: 13.237, interval_steps_per_second: 16.547, epoch: 2.0[0m
[32m[2022-08-31 15:44:39,295] [    INFO][0m - loss: 0.5057292, learning_rate: 2.85e-06, global_step: 50, interval_runtime: 0.698, interval_samples_per_second: 11.461, interval_steps_per_second: 14.327, epoch: 2.5[0m
[32m[2022-08-31 15:44:39,919] [    INFO][0m - loss: 0.4847744, learning_rate: 2.82e-06, global_step: 60, interval_runtime: 0.6243, interval_samples_per_second: 12.813, interval_steps_per_second: 16.017, epoch: 3.0[0m
[32m[2022-08-31 15:44:40,613] [    INFO][0m - loss: 0.45595593, learning_rate: 2.7900000000000004e-06, global_step: 70, interval_runtime: 0.6941, interval_samples_per_second: 11.526, interval_steps_per_second: 14.408, epoch: 3.5[0m
[32m[2022-08-31 15:44:41,221] [    INFO][0m - loss: 0.46130877, learning_rate: 2.7600000000000003e-06, global_step: 80, interval_runtime: 0.6034, interval_samples_per_second: 13.258, interval_steps_per_second: 16.573, epoch: 4.0[0m
[32m[2022-08-31 15:44:41,964] [    INFO][0m - loss: 0.45316024, learning_rate: 2.73e-06, global_step: 90, interval_runtime: 0.7476, interval_samples_per_second: 10.7, interval_steps_per_second: 13.375, epoch: 4.5[0m
[32m[2022-08-31 15:44:42,584] [    INFO][0m - loss: 0.44171243, learning_rate: 2.7e-06, global_step: 100, interval_runtime: 0.6197, interval_samples_per_second: 12.91, interval_steps_per_second: 16.137, epoch: 5.0[0m
[32m[2022-08-31 15:44:42,584] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 15:44:42,584] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 15:44:42,584] [    INFO][0m -   Pre device batch size = 64[0m
[32m[2022-08-31 15:44:42,584] [    INFO][0m -   Total Batch size = 64[0m
[32m[2022-08-31 15:44:42,584] [    INFO][0m -   Total prediction steps = 3[0m
[32m[2022-08-31 15:44:42,991] [    INFO][0m - eval_loss: 0.5289353132247925, eval_accuracy: 0.71875, eval_runtime: 0.4065, eval_samples_per_second: 393.566, eval_steps_per_second: 7.379, epoch: 5.0[0m
[32m[2022-08-31 15:44:42,992] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-31 15:44:42,992] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 15:44:46,481] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-31 15:44:46,481] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-31 15:44:52,740] [    INFO][0m - loss: 0.44530439, learning_rate: 2.6700000000000003e-06, global_step: 110, interval_runtime: 10.1563, interval_samples_per_second: 0.788, interval_steps_per_second: 0.985, epoch: 5.5[0m
[32m[2022-08-31 15:44:53,369] [    INFO][0m - loss: 0.33146527, learning_rate: 2.64e-06, global_step: 120, interval_runtime: 0.6292, interval_samples_per_second: 12.715, interval_steps_per_second: 15.894, epoch: 6.0[0m
[32m[2022-08-31 15:44:54,066] [    INFO][0m - loss: 0.40676155, learning_rate: 2.61e-06, global_step: 130, interval_runtime: 0.6971, interval_samples_per_second: 11.476, interval_steps_per_second: 14.345, epoch: 6.5[0m
[32m[2022-08-31 15:44:54,670] [    INFO][0m - loss: 0.35050581, learning_rate: 2.58e-06, global_step: 140, interval_runtime: 0.6036, interval_samples_per_second: 13.253, interval_steps_per_second: 16.566, epoch: 7.0[0m
[32m[2022-08-31 15:44:55,386] [    INFO][0m - loss: 0.37288456, learning_rate: 2.55e-06, global_step: 150, interval_runtime: 0.7166, interval_samples_per_second: 11.164, interval_steps_per_second: 13.955, epoch: 7.5[0m
[32m[2022-08-31 15:44:56,001] [    INFO][0m - loss: 0.32167215, learning_rate: 2.52e-06, global_step: 160, interval_runtime: 0.6144, interval_samples_per_second: 13.022, interval_steps_per_second: 16.277, epoch: 8.0[0m
[32m[2022-08-31 15:44:56,699] [    INFO][0m - loss: 0.33962078, learning_rate: 2.49e-06, global_step: 170, interval_runtime: 0.6977, interval_samples_per_second: 11.466, interval_steps_per_second: 14.332, epoch: 8.5[0m
[32m[2022-08-31 15:44:57,316] [    INFO][0m - loss: 0.28936708, learning_rate: 2.4599999999999997e-06, global_step: 180, interval_runtime: 0.6172, interval_samples_per_second: 12.962, interval_steps_per_second: 16.202, epoch: 9.0[0m
[32m[2022-08-31 15:44:58,020] [    INFO][0m - loss: 0.25753407, learning_rate: 2.43e-06, global_step: 190, interval_runtime: 0.7037, interval_samples_per_second: 11.369, interval_steps_per_second: 14.211, epoch: 9.5[0m
[32m[2022-08-31 15:44:58,619] [    INFO][0m - loss: 0.26170125, learning_rate: 2.4000000000000003e-06, global_step: 200, interval_runtime: 0.599, interval_samples_per_second: 13.355, interval_steps_per_second: 16.694, epoch: 10.0[0m
[32m[2022-08-31 15:44:58,619] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 15:44:58,619] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 15:44:58,619] [    INFO][0m -   Pre device batch size = 64[0m
[32m[2022-08-31 15:44:58,619] [    INFO][0m -   Total Batch size = 64[0m
[32m[2022-08-31 15:44:58,619] [    INFO][0m -   Total prediction steps = 3[0m
[32m[2022-08-31 15:44:59,032] [    INFO][0m - eval_loss: 0.5767113566398621, eval_accuracy: 0.75625, eval_runtime: 0.4122, eval_samples_per_second: 388.14, eval_steps_per_second: 7.278, epoch: 10.0[0m
[32m[2022-08-31 15:44:59,032] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-31 15:44:59,032] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 15:45:02,421] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-31 15:45:02,422] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-31 15:45:08,441] [    INFO][0m - loss: 0.19871724, learning_rate: 2.37e-06, global_step: 210, interval_runtime: 9.8223, interval_samples_per_second: 0.814, interval_steps_per_second: 1.018, epoch: 10.5[0m
[32m[2022-08-31 15:45:09,057] [    INFO][0m - loss: 0.31564803, learning_rate: 2.34e-06, global_step: 220, interval_runtime: 0.6164, interval_samples_per_second: 12.978, interval_steps_per_second: 16.223, epoch: 11.0[0m
[32m[2022-08-31 15:45:09,762] [    INFO][0m - loss: 0.22873111, learning_rate: 2.31e-06, global_step: 230, interval_runtime: 0.7043, interval_samples_per_second: 11.359, interval_steps_per_second: 14.199, epoch: 11.5[0m
[32m[2022-08-31 15:45:10,365] [    INFO][0m - loss: 0.24474502, learning_rate: 2.28e-06, global_step: 240, interval_runtime: 0.6034, interval_samples_per_second: 13.258, interval_steps_per_second: 16.572, epoch: 12.0[0m
[32m[2022-08-31 15:45:11,051] [    INFO][0m - loss: 0.24647665, learning_rate: 2.25e-06, global_step: 250, interval_runtime: 0.6859, interval_samples_per_second: 11.663, interval_steps_per_second: 14.579, epoch: 12.5[0m
[32m[2022-08-31 15:45:11,677] [    INFO][0m - loss: 0.21027927, learning_rate: 2.22e-06, global_step: 260, interval_runtime: 0.6252, interval_samples_per_second: 12.795, interval_steps_per_second: 15.994, epoch: 13.0[0m
[32m[2022-08-31 15:45:12,373] [    INFO][0m - loss: 0.21873636, learning_rate: 2.19e-06, global_step: 270, interval_runtime: 0.6967, interval_samples_per_second: 11.483, interval_steps_per_second: 14.354, epoch: 13.5[0m
[32m[2022-08-31 15:45:12,991] [    INFO][0m - loss: 0.15704542, learning_rate: 2.16e-06, global_step: 280, interval_runtime: 0.6184, interval_samples_per_second: 12.937, interval_steps_per_second: 16.171, epoch: 14.0[0m
[32m[2022-08-31 15:45:13,705] [    INFO][0m - loss: 0.21445637, learning_rate: 2.13e-06, global_step: 290, interval_runtime: 0.7136, interval_samples_per_second: 11.211, interval_steps_per_second: 14.014, epoch: 14.5[0m
[32m[2022-08-31 15:45:14,311] [    INFO][0m - loss: 0.16901183, learning_rate: 2.1e-06, global_step: 300, interval_runtime: 0.6059, interval_samples_per_second: 13.204, interval_steps_per_second: 16.505, epoch: 15.0[0m
[32m[2022-08-31 15:45:14,312] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 15:45:14,312] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 15:45:14,312] [    INFO][0m -   Pre device batch size = 64[0m
[32m[2022-08-31 15:45:14,312] [    INFO][0m -   Total Batch size = 64[0m
[32m[2022-08-31 15:45:14,312] [    INFO][0m -   Total prediction steps = 3[0m
[32m[2022-08-31 15:45:14,724] [    INFO][0m - eval_loss: 0.669242262840271, eval_accuracy: 0.75625, eval_runtime: 0.4119, eval_samples_per_second: 388.431, eval_steps_per_second: 7.283, epoch: 15.0[0m
[32m[2022-08-31 15:45:14,725] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-31 15:45:14,725] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 15:45:17,770] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-31 15:45:17,770] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-31 15:45:23,842] [    INFO][0m - loss: 0.16996475, learning_rate: 2.07e-06, global_step: 310, interval_runtime: 9.5312, interval_samples_per_second: 0.839, interval_steps_per_second: 1.049, epoch: 15.5[0m
[32m[2022-08-31 15:45:24,460] [    INFO][0m - loss: 0.14105431, learning_rate: 2.0400000000000004e-06, global_step: 320, interval_runtime: 0.6175, interval_samples_per_second: 12.955, interval_steps_per_second: 16.193, epoch: 16.0[0m
[32m[2022-08-31 15:45:25,154] [    INFO][0m - loss: 0.2429615, learning_rate: 2.0100000000000002e-06, global_step: 330, interval_runtime: 0.694, interval_samples_per_second: 11.528, interval_steps_per_second: 14.41, epoch: 16.5[0m
[32m[2022-08-31 15:45:25,753] [    INFO][0m - loss: 0.11440705, learning_rate: 1.98e-06, global_step: 340, interval_runtime: 0.5991, interval_samples_per_second: 13.353, interval_steps_per_second: 16.691, epoch: 17.0[0m
[32m[2022-08-31 15:45:26,448] [    INFO][0m - loss: 0.15048392, learning_rate: 1.95e-06, global_step: 350, interval_runtime: 0.6954, interval_samples_per_second: 11.505, interval_steps_per_second: 14.381, epoch: 17.5[0m
[32m[2022-08-31 15:45:27,072] [    INFO][0m - loss: 0.08273788, learning_rate: 1.9200000000000003e-06, global_step: 360, interval_runtime: 0.6236, interval_samples_per_second: 12.829, interval_steps_per_second: 16.037, epoch: 18.0[0m
[32m[2022-08-31 15:45:27,747] [    INFO][0m - loss: 0.14205502, learning_rate: 1.8900000000000001e-06, global_step: 370, interval_runtime: 0.6747, interval_samples_per_second: 11.857, interval_steps_per_second: 14.821, epoch: 18.5[0m
[32m[2022-08-31 15:45:28,361] [    INFO][0m - loss: 0.10258003, learning_rate: 1.86e-06, global_step: 380, interval_runtime: 0.6144, interval_samples_per_second: 13.021, interval_steps_per_second: 16.276, epoch: 19.0[0m
[32m[2022-08-31 15:45:29,048] [    INFO][0m - loss: 0.10135612, learning_rate: 1.83e-06, global_step: 390, interval_runtime: 0.6873, interval_samples_per_second: 11.639, interval_steps_per_second: 14.549, epoch: 19.5[0m
[32m[2022-08-31 15:45:29,656] [    INFO][0m - loss: 0.10840607, learning_rate: 1.8e-06, global_step: 400, interval_runtime: 0.6077, interval_samples_per_second: 13.165, interval_steps_per_second: 16.456, epoch: 20.0[0m
[32m[2022-08-31 15:45:29,656] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 15:45:29,656] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 15:45:29,656] [    INFO][0m -   Pre device batch size = 64[0m
[32m[2022-08-31 15:45:29,657] [    INFO][0m -   Total Batch size = 64[0m
[32m[2022-08-31 15:45:29,657] [    INFO][0m -   Total prediction steps = 3[0m
[32m[2022-08-31 15:45:30,077] [    INFO][0m - eval_loss: 0.8693636655807495, eval_accuracy: 0.7625, eval_runtime: 0.4197, eval_samples_per_second: 381.197, eval_steps_per_second: 7.147, epoch: 20.0[0m
[32m[2022-08-31 15:45:30,077] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-31 15:45:30,077] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 15:45:33,114] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-31 15:45:33,114] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-31 15:45:39,283] [    INFO][0m - loss: 0.05505176, learning_rate: 1.77e-06, global_step: 410, interval_runtime: 9.6267, interval_samples_per_second: 0.831, interval_steps_per_second: 1.039, epoch: 20.5[0m
[32m[2022-08-31 15:45:39,891] [    INFO][0m - loss: 0.10294433, learning_rate: 1.7399999999999999e-06, global_step: 420, interval_runtime: 0.6084, interval_samples_per_second: 13.15, interval_steps_per_second: 16.437, epoch: 21.0[0m
[32m[2022-08-31 15:45:40,593] [    INFO][0m - loss: 0.11535712, learning_rate: 1.71e-06, global_step: 430, interval_runtime: 0.702, interval_samples_per_second: 11.396, interval_steps_per_second: 14.245, epoch: 21.5[0m
[32m[2022-08-31 15:45:41,196] [    INFO][0m - loss: 0.13845012, learning_rate: 1.6800000000000002e-06, global_step: 440, interval_runtime: 0.6036, interval_samples_per_second: 13.253, interval_steps_per_second: 16.567, epoch: 22.0[0m
[32m[2022-08-31 15:45:41,892] [    INFO][0m - loss: 0.04359584, learning_rate: 1.65e-06, global_step: 450, interval_runtime: 0.6955, interval_samples_per_second: 11.502, interval_steps_per_second: 14.378, epoch: 22.5[0m
[32m[2022-08-31 15:45:42,532] [    INFO][0m - loss: 0.17925252, learning_rate: 1.6200000000000002e-06, global_step: 460, interval_runtime: 0.6402, interval_samples_per_second: 12.497, interval_steps_per_second: 15.621, epoch: 23.0[0m
[32m[2022-08-31 15:45:43,239] [    INFO][0m - loss: 0.16461763, learning_rate: 1.59e-06, global_step: 470, interval_runtime: 0.7071, interval_samples_per_second: 11.315, interval_steps_per_second: 14.143, epoch: 23.5[0m
[32m[2022-08-31 15:45:43,837] [    INFO][0m - loss: 0.02479789, learning_rate: 1.56e-06, global_step: 480, interval_runtime: 0.5977, interval_samples_per_second: 13.385, interval_steps_per_second: 16.732, epoch: 24.0[0m
[32m[2022-08-31 15:45:44,523] [    INFO][0m - loss: 0.09632662, learning_rate: 1.53e-06, global_step: 490, interval_runtime: 0.6863, interval_samples_per_second: 11.657, interval_steps_per_second: 14.571, epoch: 24.5[0m
[32m[2022-08-31 15:45:45,206] [    INFO][0m - loss: 0.04264288, learning_rate: 1.5e-06, global_step: 500, interval_runtime: 0.6827, interval_samples_per_second: 11.717, interval_steps_per_second: 14.647, epoch: 25.0[0m
[32m[2022-08-31 15:45:45,206] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 15:45:45,206] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 15:45:45,206] [    INFO][0m -   Pre device batch size = 64[0m
[32m[2022-08-31 15:45:45,207] [    INFO][0m -   Total Batch size = 64[0m
[32m[2022-08-31 15:45:45,207] [    INFO][0m -   Total prediction steps = 3[0m
[32m[2022-08-31 15:45:45,652] [    INFO][0m - eval_loss: 1.1624122858047485, eval_accuracy: 0.775, eval_runtime: 0.4454, eval_samples_per_second: 359.267, eval_steps_per_second: 6.736, epoch: 25.0[0m
[32m[2022-08-31 15:45:45,653] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-08-31 15:45:45,653] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 15:45:48,872] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-08-31 15:45:48,872] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-08-31 15:45:55,283] [    INFO][0m - loss: 0.08477014, learning_rate: 1.4700000000000001e-06, global_step: 510, interval_runtime: 10.077, interval_samples_per_second: 0.794, interval_steps_per_second: 0.992, epoch: 25.5[0m
[32m[2022-08-31 15:45:55,920] [    INFO][0m - loss: 0.02523921, learning_rate: 1.44e-06, global_step: 520, interval_runtime: 0.6366, interval_samples_per_second: 12.566, interval_steps_per_second: 15.708, epoch: 26.0[0m
[32m[2022-08-31 15:45:56,629] [    INFO][0m - loss: 0.01774342, learning_rate: 1.41e-06, global_step: 530, interval_runtime: 0.7094, interval_samples_per_second: 11.277, interval_steps_per_second: 14.096, epoch: 26.5[0m
[32m[2022-08-31 15:45:57,244] [    INFO][0m - loss: 0.07241099, learning_rate: 1.3800000000000001e-06, global_step: 540, interval_runtime: 0.6151, interval_samples_per_second: 13.006, interval_steps_per_second: 16.258, epoch: 27.0[0m
[32m[2022-08-31 15:45:57,971] [    INFO][0m - loss: 0.01290939, learning_rate: 1.35e-06, global_step: 550, interval_runtime: 0.7266, interval_samples_per_second: 11.011, interval_steps_per_second: 13.764, epoch: 27.5[0m
[32m[2022-08-31 15:45:58,616] [    INFO][0m - loss: 0.05976906, learning_rate: 1.32e-06, global_step: 560, interval_runtime: 0.6451, interval_samples_per_second: 12.401, interval_steps_per_second: 15.501, epoch: 28.0[0m
[32m[2022-08-31 15:45:59,311] [    INFO][0m - loss: 0.00465042, learning_rate: 1.29e-06, global_step: 570, interval_runtime: 0.6952, interval_samples_per_second: 11.508, interval_steps_per_second: 14.385, epoch: 28.5[0m
[32m[2022-08-31 15:45:59,913] [    INFO][0m - loss: 0.02225488, learning_rate: 1.26e-06, global_step: 580, interval_runtime: 0.6016, interval_samples_per_second: 13.299, interval_steps_per_second: 16.624, epoch: 29.0[0m
[32m[2022-08-31 15:46:00,690] [    INFO][0m - loss: 0.0708407, learning_rate: 1.2299999999999999e-06, global_step: 590, interval_runtime: 0.7773, interval_samples_per_second: 10.292, interval_steps_per_second: 12.865, epoch: 29.5[0m
[32m[2022-08-31 15:46:01,290] [    INFO][0m - loss: 0.07362646, learning_rate: 1.2000000000000002e-06, global_step: 600, interval_runtime: 0.5998, interval_samples_per_second: 13.337, interval_steps_per_second: 16.671, epoch: 30.0[0m
[32m[2022-08-31 15:46:01,290] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 15:46:01,291] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 15:46:01,291] [    INFO][0m -   Pre device batch size = 64[0m
[32m[2022-08-31 15:46:01,291] [    INFO][0m -   Total Batch size = 64[0m
[32m[2022-08-31 15:46:01,291] [    INFO][0m -   Total prediction steps = 3[0m
[32m[2022-08-31 15:46:01,698] [    INFO][0m - eval_loss: 1.43393075466156, eval_accuracy: 0.775, eval_runtime: 0.4056, eval_samples_per_second: 394.512, eval_steps_per_second: 7.397, epoch: 30.0[0m
[32m[2022-08-31 15:46:01,698] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-08-31 15:46:01,698] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 15:46:04,956] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-08-31 15:46:04,956] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-08-31 15:46:11,728] [    INFO][0m - loss: 0.05456835, learning_rate: 1.17e-06, global_step: 610, interval_runtime: 10.4374, interval_samples_per_second: 0.766, interval_steps_per_second: 0.958, epoch: 30.5[0m
[32m[2022-08-31 15:46:12,354] [    INFO][0m - loss: 0.07588097, learning_rate: 1.14e-06, global_step: 620, interval_runtime: 0.6265, interval_samples_per_second: 12.77, interval_steps_per_second: 15.962, epoch: 31.0[0m
[32m[2022-08-31 15:46:13,057] [    INFO][0m - loss: 0.00125404, learning_rate: 1.11e-06, global_step: 630, interval_runtime: 0.7035, interval_samples_per_second: 11.371, interval_steps_per_second: 14.214, epoch: 31.5[0m
[32m[2022-08-31 15:46:13,688] [    INFO][0m - loss: 0.064906, learning_rate: 1.08e-06, global_step: 640, interval_runtime: 0.6308, interval_samples_per_second: 12.682, interval_steps_per_second: 15.853, epoch: 32.0[0m
[32m[2022-08-31 15:46:14,436] [    INFO][0m - loss: 0.07826684, learning_rate: 1.05e-06, global_step: 650, interval_runtime: 0.7482, interval_samples_per_second: 10.692, interval_steps_per_second: 13.365, epoch: 32.5[0m
[32m[2022-08-31 15:46:15,055] [    INFO][0m - loss: 0.05486078, learning_rate: 1.0200000000000002e-06, global_step: 660, interval_runtime: 0.619, interval_samples_per_second: 12.924, interval_steps_per_second: 16.155, epoch: 33.0[0m
[32m[2022-08-31 15:46:15,730] [    INFO][0m - loss: 0.01095172, learning_rate: 9.9e-07, global_step: 670, interval_runtime: 0.6743, interval_samples_per_second: 11.864, interval_steps_per_second: 14.829, epoch: 33.5[0m
[32m[2022-08-31 15:46:16,366] [    INFO][0m - loss: 0.00254967, learning_rate: 9.600000000000001e-07, global_step: 680, interval_runtime: 0.6359, interval_samples_per_second: 12.581, interval_steps_per_second: 15.726, epoch: 34.0[0m
[32m[2022-08-31 15:46:17,037] [    INFO][0m - loss: 0.00299182, learning_rate: 9.3e-07, global_step: 690, interval_runtime: 0.6714, interval_samples_per_second: 11.915, interval_steps_per_second: 14.893, epoch: 34.5[0m
[32m[2022-08-31 15:46:17,600] [    INFO][0m - loss: 0.03986676, learning_rate: 9e-07, global_step: 700, interval_runtime: 0.5627, interval_samples_per_second: 14.217, interval_steps_per_second: 17.772, epoch: 35.0[0m
[32m[2022-08-31 15:46:17,600] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 15:46:17,600] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 15:46:17,601] [    INFO][0m -   Pre device batch size = 64[0m
[32m[2022-08-31 15:46:17,601] [    INFO][0m -   Total Batch size = 64[0m
[32m[2022-08-31 15:46:17,601] [    INFO][0m -   Total prediction steps = 3[0m
[32m[2022-08-31 15:46:18,006] [    INFO][0m - eval_loss: 1.5270344018936157, eval_accuracy: 0.775, eval_runtime: 0.4048, eval_samples_per_second: 395.242, eval_steps_per_second: 7.411, epoch: 35.0[0m
[32m[2022-08-31 15:46:18,006] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-700[0m
[32m[2022-08-31 15:46:18,006] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 15:46:21,321] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-700/tokenizer_config.json[0m
[32m[2022-08-31 15:46:21,321] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-700/special_tokens_map.json[0m
[32m[2022-08-31 15:46:27,825] [    INFO][0m - loss: 0.03608787, learning_rate: 8.699999999999999e-07, global_step: 710, interval_runtime: 10.2253, interval_samples_per_second: 0.782, interval_steps_per_second: 0.978, epoch: 35.5[0m
[32m[2022-08-31 15:46:28,420] [    INFO][0m - loss: 0.03444436, learning_rate: 8.400000000000001e-07, global_step: 720, interval_runtime: 0.5952, interval_samples_per_second: 13.441, interval_steps_per_second: 16.802, epoch: 36.0[0m
[32m[2022-08-31 15:46:29,066] [    INFO][0m - loss: 0.01430326, learning_rate: 8.100000000000001e-07, global_step: 730, interval_runtime: 0.6454, interval_samples_per_second: 12.396, interval_steps_per_second: 15.494, epoch: 36.5[0m
[32m[2022-08-31 15:46:29,673] [    INFO][0m - loss: 0.00608699, learning_rate: 7.8e-07, global_step: 740, interval_runtime: 0.6068, interval_samples_per_second: 13.185, interval_steps_per_second: 16.481, epoch: 37.0[0m
[32m[2022-08-31 15:46:30,353] [    INFO][0m - loss: 0.00465085, learning_rate: 7.5e-07, global_step: 750, interval_runtime: 0.6811, interval_samples_per_second: 11.745, interval_steps_per_second: 14.682, epoch: 37.5[0m
[32m[2022-08-31 15:46:30,961] [    INFO][0m - loss: 0.00695676, learning_rate: 7.2e-07, global_step: 760, interval_runtime: 0.6075, interval_samples_per_second: 13.169, interval_steps_per_second: 16.461, epoch: 38.0[0m
[32m[2022-08-31 15:46:31,653] [    INFO][0m - loss: 0.00083126, learning_rate: 6.900000000000001e-07, global_step: 770, interval_runtime: 0.6916, interval_samples_per_second: 11.567, interval_steps_per_second: 14.459, epoch: 38.5[0m
[32m[2022-08-31 15:46:32,277] [    INFO][0m - loss: 0.00786778, learning_rate: 6.6e-07, global_step: 780, interval_runtime: 0.6245, interval_samples_per_second: 12.811, interval_steps_per_second: 16.014, epoch: 39.0[0m
[32m[2022-08-31 15:46:33,019] [    INFO][0m - loss: 0.01463142, learning_rate: 6.3e-07, global_step: 790, interval_runtime: 0.7412, interval_samples_per_second: 10.793, interval_steps_per_second: 13.491, epoch: 39.5[0m
[32m[2022-08-31 15:46:33,654] [    INFO][0m - loss: 0.00441706, learning_rate: 6.000000000000001e-07, global_step: 800, interval_runtime: 0.6361, interval_samples_per_second: 12.576, interval_steps_per_second: 15.72, epoch: 40.0[0m
[32m[2022-08-31 15:46:33,655] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 15:46:33,655] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 15:46:33,655] [    INFO][0m -   Pre device batch size = 64[0m
[32m[2022-08-31 15:46:33,655] [    INFO][0m -   Total Batch size = 64[0m
[32m[2022-08-31 15:46:33,655] [    INFO][0m -   Total prediction steps = 3[0m
[32m[2022-08-31 15:46:34,058] [    INFO][0m - eval_loss: 1.6268466711044312, eval_accuracy: 0.775, eval_runtime: 0.4027, eval_samples_per_second: 397.328, eval_steps_per_second: 7.45, epoch: 40.0[0m
[32m[2022-08-31 15:46:34,059] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-800[0m
[32m[2022-08-31 15:46:34,059] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 15:46:37,536] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-800/tokenizer_config.json[0m
[32m[2022-08-31 15:46:37,537] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-800/special_tokens_map.json[0m
[32m[2022-08-31 15:46:43,824] [    INFO][0m - loss: 0.08458266, learning_rate: 5.7e-07, global_step: 810, interval_runtime: 10.1697, interval_samples_per_second: 0.787, interval_steps_per_second: 0.983, epoch: 40.5[0m
[32m[2022-08-31 15:46:44,420] [    INFO][0m - loss: 0.04554208, learning_rate: 5.4e-07, global_step: 820, interval_runtime: 0.5964, interval_samples_per_second: 13.413, interval_steps_per_second: 16.767, epoch: 41.0[0m
[32m[2022-08-31 15:46:45,092] [    INFO][0m - loss: 0.00376775, learning_rate: 5.100000000000001e-07, global_step: 830, interval_runtime: 0.672, interval_samples_per_second: 11.904, interval_steps_per_second: 14.88, epoch: 41.5[0m
[32m[2022-08-31 15:46:45,698] [    INFO][0m - loss: 0.00483408, learning_rate: 4.800000000000001e-07, global_step: 840, interval_runtime: 0.6053, interval_samples_per_second: 13.216, interval_steps_per_second: 16.52, epoch: 42.0[0m
[32m[2022-08-31 15:46:46,390] [    INFO][0m - loss: 0.00158474, learning_rate: 4.5e-07, global_step: 850, interval_runtime: 0.6925, interval_samples_per_second: 11.552, interval_steps_per_second: 14.44, epoch: 42.5[0m
[32m[2022-08-31 15:46:47,011] [    INFO][0m - loss: 0.00107581, learning_rate: 4.2000000000000006e-07, global_step: 860, interval_runtime: 0.6207, interval_samples_per_second: 12.888, interval_steps_per_second: 16.11, epoch: 43.0[0m
[32m[2022-08-31 15:46:47,738] [    INFO][0m - loss: 0.00094408, learning_rate: 3.9e-07, global_step: 870, interval_runtime: 0.7274, interval_samples_per_second: 10.998, interval_steps_per_second: 13.747, epoch: 43.5[0m
[32m[2022-08-31 15:46:48,438] [    INFO][0m - loss: 0.09632738, learning_rate: 3.6e-07, global_step: 880, interval_runtime: 0.6992, interval_samples_per_second: 11.442, interval_steps_per_second: 14.302, epoch: 44.0[0m
[32m[2022-08-31 15:46:49,185] [    INFO][0m - loss: 0.09200005, learning_rate: 3.3e-07, global_step: 890, interval_runtime: 0.7475, interval_samples_per_second: 10.702, interval_steps_per_second: 13.378, epoch: 44.5[0m
[32m[2022-08-31 15:46:49,823] [    INFO][0m - loss: 0.00129864, learning_rate: 3.0000000000000004e-07, global_step: 900, interval_runtime: 0.6369, interval_samples_per_second: 12.56, interval_steps_per_second: 15.7, epoch: 45.0[0m
[32m[2022-08-31 15:46:49,823] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 15:46:49,823] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 15:46:49,824] [    INFO][0m -   Pre device batch size = 64[0m
[32m[2022-08-31 15:46:49,824] [    INFO][0m -   Total Batch size = 64[0m
[32m[2022-08-31 15:46:49,824] [    INFO][0m -   Total prediction steps = 3[0m
[32m[2022-08-31 15:46:50,239] [    INFO][0m - eval_loss: 1.6650623083114624, eval_accuracy: 0.775, eval_runtime: 0.4152, eval_samples_per_second: 385.338, eval_steps_per_second: 7.225, epoch: 45.0[0m
[32m[2022-08-31 15:46:50,240] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-900[0m
[32m[2022-08-31 15:46:50,240] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 15:46:53,796] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-900/tokenizer_config.json[0m
[32m[2022-08-31 15:46:53,797] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-900/special_tokens_map.json[0m
[32m[2022-08-31 15:46:59,430] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-31 15:46:59,430] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-500 (score: 0.775).[0m
[32m[2022-08-31 15:47:00,444] [    INFO][0m - train_runtime: 145.4235, train_samples_per_second: 55.012, train_steps_per_second: 6.876, train_loss: 0.16231408211195633, epoch: 45.0[0m
[32m[2022-08-31 15:47:00,486] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-31 15:47:00,486] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 15:47:03,949] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-31 15:47:03,950] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-31 15:47:03,953] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-31 15:47:03,953] [    INFO][0m -   epoch                    =       45.0[0m
[32m[2022-08-31 15:47:03,953] [    INFO][0m -   train_loss               =     0.1623[0m
[32m[2022-08-31 15:47:03,954] [    INFO][0m -   train_runtime            = 0:02:25.42[0m
[32m[2022-08-31 15:47:03,954] [    INFO][0m -   train_samples_per_second =     55.012[0m
[32m[2022-08-31 15:47:03,954] [    INFO][0m -   train_steps_per_second   =      6.876[0m
[32m[2022-08-31 15:47:03,962] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-31 15:47:03,962] [    INFO][0m -   Num examples = 1772[0m
[32m[2022-08-31 15:47:03,962] [    INFO][0m -   Pre device batch size = 64[0m
[32m[2022-08-31 15:47:03,963] [    INFO][0m -   Total Batch size = 64[0m
[32m[2022-08-31 15:47:03,963] [    INFO][0m -   Total prediction steps = 28[0m
[32m[2022-08-31 15:47:08,437] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-31 15:47:08,438] [    INFO][0m -   test_accuracy           =     0.7489[0m
[32m[2022-08-31 15:47:08,438] [    INFO][0m -   test_loss               =     1.4322[0m
[32m[2022-08-31 15:47:08,439] [    INFO][0m -   test_runtime            = 0:00:04.47[0m
[32m[2022-08-31 15:47:08,439] [    INFO][0m -   test_samples_per_second =    395.961[0m
[32m[2022-08-31 15:47:08,439] [    INFO][0m -   test_steps_per_second   =      6.257[0m
[32m[2022-08-31 15:47:08,439] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-31 15:47:08,440] [    INFO][0m -   Num examples = 2000[0m
[32m[2022-08-31 15:47:08,440] [    INFO][0m -   Pre device batch size = 64[0m
[32m[2022-08-31 15:47:08,440] [    INFO][0m -   Total Batch size = 64[0m
[32m[2022-08-31 15:47:08,440] [    INFO][0m -   Total prediction steps = 32[0m
[32m[2022-08-31 15:47:15,198] [    INFO][0m - Predictions for bustm saved to ./fewclue_submit_examples.[0m
[]
run.sh: line 72: --freeze_plm: command not found
