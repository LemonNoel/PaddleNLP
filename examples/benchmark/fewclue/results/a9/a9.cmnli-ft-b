 
==========
eprstmt
==========
 
[33m[2022-08-31 18:02:40,798] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-31 18:02:40,798] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-31 18:02:40,799] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 18:02:40,799] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-31 18:02:40,799] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 18:02:40,799] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-31 18:02:40,799] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-31 18:02:40,799] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-08-31 18:02:40,799] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-31 18:02:40,799] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-08-31 18:02:40,799] [    INFO][0m - [0m
[32m[2022-08-31 18:02:40,799] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 18:02:40,799] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-31 18:02:40,799] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 18:02:40,800] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-31 18:02:40,800] [    INFO][0m - prompt                        :{'text':'text_a'}{'hard':'Ëøô‰∏™Âè•ËØùË°®Á§∫Êàë'}{'mask'}{'hard':'ÂñúÊ¨¢Ëøô‰∏™‰∏úË•ø'}[0m
[32m[2022-08-31 18:02:40,800] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-31 18:02:40,800] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-31 18:02:40,800] [    INFO][0m - task_name                     :eprstmt[0m
[32m[2022-08-31 18:02:40,800] [    INFO][0m - [0m
[32m[2022-08-31 18:02:40,800] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0831 18:02:40.801797 26081 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0831 18:02:40.806100 26081 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-31 18:02:43,794] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-08-31 18:02:43,818] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-08-31 18:02:43,819] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[32m[2022-08-31 18:02:43,820] [    INFO][0m - Using template: [{'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': 'Ëøô‰∏™Âè•ËØùË°®Á§∫Êàë'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'ÂñúÊ¨¢Ëøô‰∏™‰∏úË•ø'}][0m
[32m[2022-08-31 18:02:43,822] [    INFO][0m - {'Negative': 0, 'Positive': 1}[0m
2022-08-31 18:02:43,823 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-31 18:02:44,891] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 18:02:44,891] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-31 18:02:44,891] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 18:02:44,891] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-31 18:02:44,892] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-31 18:02:44,892] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-31 18:02:44,892] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-31 18:02:44,892] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-31 18:02:44,892] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-31 18:02:44,892] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-31 18:02:44,892] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-31 18:02:44,892] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-31 18:02:44,892] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-31 18:02:44,892] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-31 18:02:44,892] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-31 18:02:44,892] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-31 18:02:44,892] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-31 18:02:44,893] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-31 18:02:44,893] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-31 18:02:44,893] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-31 18:02:44,893] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-31 18:02:44,893] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-31 18:02:44,893] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-31 18:02:44,893] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-31 18:02:44,893] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-31 18:02:44,893] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-31 18:02:44,893] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-31 18:02:44,893] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-31 18:02:44,893] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-31 18:02:44,893] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-31 18:02:44,894] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-31 18:02:44,894] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-31 18:02:44,894] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-31 18:02:44,894] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-31 18:02:44,894] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-31 18:02:44,894] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-31 18:02:44,894] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-31 18:02:44,894] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug31_18-02-40_instance-3bwob41y-01[0m
[32m[2022-08-31 18:02:44,894] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-31 18:02:44,894] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-31 18:02:44,894] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-31 18:02:44,894] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-31 18:02:44,894] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-31 18:02:44,894] [    INFO][0m - max_seq_length                :128[0m
[32m[2022-08-31 18:02:44,895] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-31 18:02:44,895] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-31 18:02:44,895] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-31 18:02:44,895] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-31 18:02:44,895] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-08-31 18:02:44,895] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-31 18:02:44,895] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-31 18:02:44,895] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-31 18:02:44,895] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-31 18:02:44,895] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-31 18:02:44,895] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-31 18:02:44,895] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-31 18:02:44,895] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-31 18:02:44,895] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-31 18:02:44,896] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-31 18:02:44,896] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-31 18:02:44,896] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-31 18:02:44,896] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-31 18:02:44,896] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-31 18:02:44,896] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-31 18:02:44,896] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-31 18:02:44,896] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-31 18:02:44,896] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-31 18:02:44,896] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-31 18:02:44,896] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-31 18:02:44,896] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-31 18:02:44,896] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-31 18:02:44,897] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-31 18:02:44,897] [    INFO][0m - seed                          :42[0m
[32m[2022-08-31 18:02:44,897] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-31 18:02:44,897] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-31 18:02:44,897] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-31 18:02:44,897] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-31 18:02:44,897] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-31 18:02:44,897] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-31 18:02:44,897] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-31 18:02:44,897] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-31 18:02:44,897] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-31 18:02:44,897] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-31 18:02:44,897] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-31 18:02:44,897] [    INFO][0m - [0m
[32m[2022-08-31 18:02:44,899] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-31 18:02:44,899] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:02:44,899] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-08-31 18:02:44,899] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-31 18:02:44,899] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-31 18:02:44,900] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-31 18:02:44,900] [    INFO][0m -   Total optimization steps = 1000.0[0m
[32m[2022-08-31 18:02:44,900] [    INFO][0m -   Total num train samples = 8000[0m
[32m[2022-08-31 18:02:47,096] [    INFO][0m - loss: 0.46588955, learning_rate: 2.97e-05, global_step: 10, interval_runtime: 2.1949, interval_samples_per_second: 3.645, interval_steps_per_second: 4.556, epoch: 0.5[0m
[32m[2022-08-31 18:02:47,979] [    INFO][0m - loss: 0.46476421, learning_rate: 2.94e-05, global_step: 20, interval_runtime: 0.8836, interval_samples_per_second: 9.054, interval_steps_per_second: 11.317, epoch: 1.0[0m
[32m[2022-08-31 18:02:48,915] [    INFO][0m - loss: 0.27447679, learning_rate: 2.91e-05, global_step: 30, interval_runtime: 0.9355, interval_samples_per_second: 8.552, interval_steps_per_second: 10.69, epoch: 1.5[0m
[32m[2022-08-31 18:02:49,820] [    INFO][0m - loss: 0.2008806, learning_rate: 2.88e-05, global_step: 40, interval_runtime: 0.9053, interval_samples_per_second: 8.837, interval_steps_per_second: 11.046, epoch: 2.0[0m
[32m[2022-08-31 18:02:50,755] [    INFO][0m - loss: 0.12641231, learning_rate: 2.8499999999999998e-05, global_step: 50, interval_runtime: 0.9352, interval_samples_per_second: 8.554, interval_steps_per_second: 10.693, epoch: 2.5[0m
[32m[2022-08-31 18:02:51,640] [    INFO][0m - loss: 0.08487588, learning_rate: 2.8199999999999998e-05, global_step: 60, interval_runtime: 0.8848, interval_samples_per_second: 9.042, interval_steps_per_second: 11.302, epoch: 3.0[0m
[32m[2022-08-31 18:02:52,593] [    INFO][0m - loss: 0.0229289, learning_rate: 2.79e-05, global_step: 70, interval_runtime: 0.9528, interval_samples_per_second: 8.396, interval_steps_per_second: 10.495, epoch: 3.5[0m
[32m[2022-08-31 18:02:53,474] [    INFO][0m - loss: 0.05750499, learning_rate: 2.7600000000000003e-05, global_step: 80, interval_runtime: 0.8818, interval_samples_per_second: 9.072, interval_steps_per_second: 11.34, epoch: 4.0[0m
[32m[2022-08-31 18:02:54,423] [    INFO][0m - loss: 0.06044604, learning_rate: 2.7300000000000003e-05, global_step: 90, interval_runtime: 0.9484, interval_samples_per_second: 8.436, interval_steps_per_second: 10.545, epoch: 4.5[0m
[32m[2022-08-31 18:02:55,327] [    INFO][0m - loss: 0.00125095, learning_rate: 2.7000000000000002e-05, global_step: 100, interval_runtime: 0.9047, interval_samples_per_second: 8.843, interval_steps_per_second: 11.054, epoch: 5.0[0m
[32m[2022-08-31 18:02:55,328] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:02:55,328] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:02:55,328] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:02:55,328] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:02:55,328] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:02:56,011] [    INFO][0m - eval_loss: 1.3916871547698975, eval_accuracy: 0.8625, eval_runtime: 0.6826, eval_samples_per_second: 234.401, eval_steps_per_second: 7.325, epoch: 5.0[0m
[32m[2022-08-31 18:02:56,012] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-31 18:02:56,012] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:02:59,198] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-31 18:02:59,198] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-31 18:03:09,007] [    INFO][0m - loss: 5.097e-05, learning_rate: 2.6700000000000002e-05, global_step: 110, interval_runtime: 13.6787, interval_samples_per_second: 0.585, interval_steps_per_second: 0.731, epoch: 5.5[0m
[32m[2022-08-31 18:03:09,893] [    INFO][0m - loss: 0.00444183, learning_rate: 2.64e-05, global_step: 120, interval_runtime: 0.8867, interval_samples_per_second: 9.023, interval_steps_per_second: 11.278, epoch: 6.0[0m
[32m[2022-08-31 18:03:10,825] [    INFO][0m - loss: 6.532e-05, learning_rate: 2.61e-05, global_step: 130, interval_runtime: 0.9303, interval_samples_per_second: 8.599, interval_steps_per_second: 10.749, epoch: 6.5[0m
[32m[2022-08-31 18:03:11,712] [    INFO][0m - loss: 0.00040809, learning_rate: 2.58e-05, global_step: 140, interval_runtime: 0.8883, interval_samples_per_second: 9.006, interval_steps_per_second: 11.258, epoch: 7.0[0m
[32m[2022-08-31 18:03:12,653] [    INFO][0m - loss: 4.51e-06, learning_rate: 2.55e-05, global_step: 150, interval_runtime: 0.9419, interval_samples_per_second: 8.494, interval_steps_per_second: 10.617, epoch: 7.5[0m
[32m[2022-08-31 18:03:13,540] [    INFO][0m - loss: 1.258e-05, learning_rate: 2.52e-05, global_step: 160, interval_runtime: 0.8864, interval_samples_per_second: 9.025, interval_steps_per_second: 11.281, epoch: 8.0[0m
[32m[2022-08-31 18:03:14,482] [    INFO][0m - loss: 1.07e-05, learning_rate: 2.49e-05, global_step: 170, interval_runtime: 0.9417, interval_samples_per_second: 8.495, interval_steps_per_second: 10.619, epoch: 8.5[0m
[32m[2022-08-31 18:03:15,367] [    INFO][0m - loss: 2.28e-06, learning_rate: 2.4599999999999998e-05, global_step: 180, interval_runtime: 0.8859, interval_samples_per_second: 9.031, interval_steps_per_second: 11.288, epoch: 9.0[0m
[32m[2022-08-31 18:03:16,300] [    INFO][0m - loss: 1.71e-06, learning_rate: 2.43e-05, global_step: 190, interval_runtime: 0.9312, interval_samples_per_second: 8.591, interval_steps_per_second: 10.739, epoch: 9.5[0m
[32m[2022-08-31 18:03:17,193] [    INFO][0m - loss: 1.26e-06, learning_rate: 2.4e-05, global_step: 200, interval_runtime: 0.8943, interval_samples_per_second: 8.946, interval_steps_per_second: 11.182, epoch: 10.0[0m
[32m[2022-08-31 18:03:17,194] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:03:17,194] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:03:17,194] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:03:17,194] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:03:17,194] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:03:17,871] [    INFO][0m - eval_loss: 1.853280782699585, eval_accuracy: 0.85625, eval_runtime: 0.6767, eval_samples_per_second: 236.437, eval_steps_per_second: 7.389, epoch: 10.0[0m
[32m[2022-08-31 18:03:17,872] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-31 18:03:17,872] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:03:21,160] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-31 18:03:21,160] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-31 18:03:28,352] [    INFO][0m - loss: 6.5e-07, learning_rate: 2.37e-05, global_step: 210, interval_runtime: 11.1594, interval_samples_per_second: 0.717, interval_steps_per_second: 0.896, epoch: 10.5[0m
[32m[2022-08-31 18:03:29,236] [    INFO][0m - loss: 7.452e-05, learning_rate: 2.3400000000000003e-05, global_step: 220, interval_runtime: 0.8838, interval_samples_per_second: 9.052, interval_steps_per_second: 11.315, epoch: 11.0[0m
[32m[2022-08-31 18:03:30,169] [    INFO][0m - loss: 3.71e-06, learning_rate: 2.3100000000000002e-05, global_step: 230, interval_runtime: 0.9326, interval_samples_per_second: 8.578, interval_steps_per_second: 10.723, epoch: 11.5[0m
[32m[2022-08-31 18:03:31,053] [    INFO][0m - loss: 0.00027689, learning_rate: 2.2800000000000002e-05, global_step: 240, interval_runtime: 0.884, interval_samples_per_second: 9.05, interval_steps_per_second: 11.313, epoch: 12.0[0m
[32m[2022-08-31 18:03:31,983] [    INFO][0m - loss: 7.6e-07, learning_rate: 2.25e-05, global_step: 250, interval_runtime: 0.9308, interval_samples_per_second: 8.594, interval_steps_per_second: 10.743, epoch: 12.5[0m
[32m[2022-08-31 18:03:32,875] [    INFO][0m - loss: 2e-07, learning_rate: 2.22e-05, global_step: 260, interval_runtime: 0.8914, interval_samples_per_second: 8.975, interval_steps_per_second: 11.218, epoch: 13.0[0m
[32m[2022-08-31 18:03:33,821] [    INFO][0m - loss: 2.61e-06, learning_rate: 2.19e-05, global_step: 270, interval_runtime: 0.9456, interval_samples_per_second: 8.46, interval_steps_per_second: 10.575, epoch: 13.5[0m
[32m[2022-08-31 18:03:34,707] [    INFO][0m - loss: 2.4e-07, learning_rate: 2.16e-05, global_step: 280, interval_runtime: 0.8868, interval_samples_per_second: 9.021, interval_steps_per_second: 11.276, epoch: 14.0[0m
[32m[2022-08-31 18:03:35,655] [    INFO][0m - loss: 0.00053723, learning_rate: 2.13e-05, global_step: 290, interval_runtime: 0.9473, interval_samples_per_second: 8.445, interval_steps_per_second: 10.557, epoch: 14.5[0m
[32m[2022-08-31 18:03:36,547] [    INFO][0m - loss: 2.1e-07, learning_rate: 2.1e-05, global_step: 300, interval_runtime: 0.8921, interval_samples_per_second: 8.968, interval_steps_per_second: 11.21, epoch: 15.0[0m
[32m[2022-08-31 18:03:36,547] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:03:36,547] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:03:36,547] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:03:36,547] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:03:36,548] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:03:37,232] [    INFO][0m - eval_loss: 2.1619346141815186, eval_accuracy: 0.85, eval_runtime: 0.6844, eval_samples_per_second: 233.773, eval_steps_per_second: 7.305, epoch: 15.0[0m
[32m[2022-08-31 18:03:37,233] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-31 18:03:37,233] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:03:39,078] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-31 18:03:39,078] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-31 18:03:43,155] [    INFO][0m - loss: 8e-08, learning_rate: 2.07e-05, global_step: 310, interval_runtime: 6.6088, interval_samples_per_second: 1.211, interval_steps_per_second: 1.513, epoch: 15.5[0m
[32m[2022-08-31 18:03:44,183] [    INFO][0m - loss: 0.00022828, learning_rate: 2.04e-05, global_step: 320, interval_runtime: 1.0271, interval_samples_per_second: 7.789, interval_steps_per_second: 9.736, epoch: 16.0[0m
[32m[2022-08-31 18:03:45,246] [    INFO][0m - loss: 2e-07, learning_rate: 2.01e-05, global_step: 330, interval_runtime: 1.0626, interval_samples_per_second: 7.529, interval_steps_per_second: 9.411, epoch: 16.5[0m
[32m[2022-08-31 18:03:46,311] [    INFO][0m - loss: 1.5e-07, learning_rate: 1.98e-05, global_step: 340, interval_runtime: 1.0652, interval_samples_per_second: 7.51, interval_steps_per_second: 9.388, epoch: 17.0[0m
[32m[2022-08-31 18:03:47,409] [    INFO][0m - loss: 2.14e-06, learning_rate: 1.95e-05, global_step: 350, interval_runtime: 1.0982, interval_samples_per_second: 7.284, interval_steps_per_second: 9.106, epoch: 17.5[0m
[32m[2022-08-31 18:03:48,468] [    INFO][0m - loss: 1.5e-07, learning_rate: 1.9200000000000003e-05, global_step: 360, interval_runtime: 1.0593, interval_samples_per_second: 7.552, interval_steps_per_second: 9.44, epoch: 18.0[0m
[32m[2022-08-31 18:03:49,571] [    INFO][0m - loss: 9.865e-05, learning_rate: 1.8900000000000002e-05, global_step: 370, interval_runtime: 1.1032, interval_samples_per_second: 7.252, interval_steps_per_second: 9.065, epoch: 18.5[0m
[32m[2022-08-31 18:03:50,629] [    INFO][0m - loss: 1.1e-07, learning_rate: 1.86e-05, global_step: 380, interval_runtime: 1.0579, interval_samples_per_second: 7.562, interval_steps_per_second: 9.453, epoch: 19.0[0m
[32m[2022-08-31 18:03:51,739] [    INFO][0m - loss: 1.548e-05, learning_rate: 1.83e-05, global_step: 390, interval_runtime: 1.1097, interval_samples_per_second: 7.209, interval_steps_per_second: 9.012, epoch: 19.5[0m
[32m[2022-08-31 18:03:52,797] [    INFO][0m - loss: 8e-08, learning_rate: 1.8e-05, global_step: 400, interval_runtime: 1.0578, interval_samples_per_second: 7.563, interval_steps_per_second: 9.453, epoch: 20.0[0m
[32m[2022-08-31 18:03:52,797] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:03:52,797] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:03:52,797] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:03:52,797] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:03:52,798] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:03:53,596] [    INFO][0m - eval_loss: 2.3057312965393066, eval_accuracy: 0.85, eval_runtime: 0.7982, eval_samples_per_second: 200.443, eval_steps_per_second: 6.264, epoch: 20.0[0m
[32m[2022-08-31 18:03:53,596] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-31 18:03:53,597] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:03:55,357] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-31 18:03:55,357] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-31 18:03:59,719] [    INFO][0m - loss: 0.11214963, learning_rate: 1.77e-05, global_step: 410, interval_runtime: 6.9226, interval_samples_per_second: 1.156, interval_steps_per_second: 1.445, epoch: 20.5[0m
[32m[2022-08-31 18:04:00,898] [    INFO][0m - loss: 9e-08, learning_rate: 1.74e-05, global_step: 420, interval_runtime: 1.1788, interval_samples_per_second: 6.787, interval_steps_per_second: 8.483, epoch: 21.0[0m
[32m[2022-08-31 18:04:01,834] [    INFO][0m - loss: 0.15240507, learning_rate: 1.71e-05, global_step: 430, interval_runtime: 0.9356, interval_samples_per_second: 8.55, interval_steps_per_second: 10.688, epoch: 21.5[0m
[32m[2022-08-31 18:04:02,714] [    INFO][0m - loss: 1.6e-07, learning_rate: 1.6800000000000002e-05, global_step: 440, interval_runtime: 0.8805, interval_samples_per_second: 9.086, interval_steps_per_second: 11.357, epoch: 22.0[0m
[32m[2022-08-31 18:04:03,645] [    INFO][0m - loss: 4.5e-07, learning_rate: 1.65e-05, global_step: 450, interval_runtime: 0.9309, interval_samples_per_second: 8.594, interval_steps_per_second: 10.742, epoch: 22.5[0m
[32m[2022-08-31 18:04:04,578] [    INFO][0m - loss: 9e-08, learning_rate: 1.62e-05, global_step: 460, interval_runtime: 0.9321, interval_samples_per_second: 8.583, interval_steps_per_second: 10.728, epoch: 23.0[0m
[32m[2022-08-31 18:04:05,508] [    INFO][0m - loss: 2.1e-07, learning_rate: 1.59e-05, global_step: 470, interval_runtime: 0.9308, interval_samples_per_second: 8.595, interval_steps_per_second: 10.743, epoch: 23.5[0m
[32m[2022-08-31 18:04:06,400] [    INFO][0m - loss: 5.68e-06, learning_rate: 1.56e-05, global_step: 480, interval_runtime: 0.8924, interval_samples_per_second: 8.964, interval_steps_per_second: 11.205, epoch: 24.0[0m
[32m[2022-08-31 18:04:07,353] [    INFO][0m - loss: 1.439e-05, learning_rate: 1.53e-05, global_step: 490, interval_runtime: 0.9529, interval_samples_per_second: 8.395, interval_steps_per_second: 10.494, epoch: 24.5[0m
[32m[2022-08-31 18:04:08,244] [    INFO][0m - loss: 1.1e-07, learning_rate: 1.5e-05, global_step: 500, interval_runtime: 0.8902, interval_samples_per_second: 8.987, interval_steps_per_second: 11.233, epoch: 25.0[0m
[32m[2022-08-31 18:04:08,244] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:04:08,244] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:04:08,244] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:04:08,245] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:04:08,245] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:04:08,927] [    INFO][0m - eval_loss: 2.2009212970733643, eval_accuracy: 0.85, eval_runtime: 0.6822, eval_samples_per_second: 234.526, eval_steps_per_second: 7.329, epoch: 25.0[0m
[32m[2022-08-31 18:04:08,928] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-08-31 18:04:08,928] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:04:11,093] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-08-31 18:04:13,525] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-08-31 18:04:16,959] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-31 18:04:16,960] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-100 (score: 0.8625).[0m
[32m[2022-08-31 18:04:18,208] [    INFO][0m - train_runtime: 93.3074, train_samples_per_second: 85.738, train_steps_per_second: 10.717, train_loss: 0.04060495414618072, epoch: 25.0[0m
[32m[2022-08-31 18:04:18,209] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-31 18:04:18,209] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:04:20,284] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-31 18:04:20,284] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-31 18:04:20,285] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-31 18:04:20,286] [    INFO][0m -   epoch                    =       25.0[0m
[32m[2022-08-31 18:04:20,286] [    INFO][0m -   train_loss               =     0.0406[0m
[32m[2022-08-31 18:04:20,286] [    INFO][0m -   train_runtime            = 0:01:33.30[0m
[32m[2022-08-31 18:04:20,286] [    INFO][0m -   train_samples_per_second =     85.738[0m
[32m[2022-08-31 18:04:20,286] [    INFO][0m -   train_steps_per_second   =     10.717[0m
[32m[2022-08-31 18:04:20,289] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-31 18:04:20,289] [    INFO][0m -   Num examples = 610[0m
[32m[2022-08-31 18:04:20,289] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:04:20,289] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:04:20,289] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-08-31 18:04:23,078] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-31 18:04:23,079] [    INFO][0m -   test_accuracy           =     0.8689[0m
[32m[2022-08-31 18:04:23,079] [    INFO][0m -   test_loss               =       1.24[0m
[32m[2022-08-31 18:04:23,079] [    INFO][0m -   test_runtime            = 0:00:02.78[0m
[32m[2022-08-31 18:04:23,079] [    INFO][0m -   test_samples_per_second =    218.703[0m
[32m[2022-08-31 18:04:23,079] [    INFO][0m -   test_steps_per_second   =      7.171[0m
[32m[2022-08-31 18:04:23,079] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-31 18:04:23,079] [    INFO][0m -   Num examples = 753[0m
[32m[2022-08-31 18:04:23,079] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:04:23,079] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:04:23,080] [    INFO][0m -   Total prediction steps = 24[0m
[32m[2022-08-31 18:04:27,713] [    INFO][0m - Predictions for eprstmt saved to ./fewclue_submit_examples.[0m
[]
run.sh: line 75: --freeze_plm: command not found
 
==========
csldcp
==========
 
[33m[2022-08-31 18:04:32,150] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-31 18:04:34,989] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-31 18:04:34,990] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 18:04:34,990] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-31 18:04:34,990] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 18:04:34,990] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-31 18:04:34,990] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-31 18:04:34,991] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-08-31 18:04:34,991] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-31 18:04:34,991] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-08-31 18:04:34,991] [    INFO][0m - [0m
[32m[2022-08-31 18:04:34,991] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 18:04:34,991] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-31 18:04:34,991] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 18:04:34,991] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-31 18:04:34,991] [    INFO][0m - prompt                        :{'hard':'ÈòÖËØª‰∏ãËæπÊúâÂÖ≥'}{'mask'}{'mask'}{'hard':'ÁöÑÊùêÊñô'}{'text':'text_a'}[0m
[32m[2022-08-31 18:04:34,991] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-31 18:04:34,991] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-31 18:04:34,991] [    INFO][0m - task_name                     :csldcp[0m
[32m[2022-08-31 18:04:34,991] [    INFO][0m - [0m
[32m[2022-08-31 18:04:34,992] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0831 18:04:34.994027 42821 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0831 18:04:34.998355 42821 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-31 18:04:38,730] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-08-31 18:04:38,755] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-08-31 18:04:38,755] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[32m[2022-08-31 18:04:38,756] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': 'ÈòÖËØª‰∏ãËæπÊúâÂÖ≥'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'ÁöÑÊùêÊñô'}, {'add_prefix_space': '', 'text': 'text_a'}][0m
[32m[2022-08-31 18:04:38,775] [    INFO][0m - {'‰∏≠ÂåªÂ≠¶/‰∏≠ËçØÂ≠¶': 0, '‰∏≠ÂõΩËØ≠Ë®ÄÊñáÂ≠¶': 1, '‰∫§ÈÄöËøêËæìÂ∑•Á®ã': 2, '‰ΩìËÇ≤Â≠¶': 3, '‰ΩúÁâ©Â≠¶': 4, '‰ø°ÊÅØ‰∏éÈÄö‰ø°Â∑•Á®ã': 5, 'ÂÖâÂ≠¶Â∑•Á®ã': 6, 'ÂÖ¨ÂÖ±Âç´Áîü‰∏éÈ¢ÑÈò≤ÂåªÂ≠¶': 7, 'ÂÖ¨ÂÖ±ÁÆ°ÁêÜ': 8, 'ÂÖµÂô®ÁßëÂ≠¶‰∏éÊäÄÊúØ': 9, 'ÂÜõ‰∫ãÂ≠¶': 10, 'ÂÜú‰∏öÂ∑•Á®ã': 11, 'ÂÜú‰∏öËµÑÊ∫êÂà©Áî®': 12, 'ÂÜúÊûóÁªèÊµéÁÆ°ÁêÜ': 13, 'ÂÜ∂ÈáëÂ∑•Á®ã': 14, 'ÂäõÂ≠¶': 15, 'Âä®ÂäõÂ∑•Á®ãÂèäÂ∑•Á®ãÁÉ≠Áâ©ÁêÜ': 16, 'ÂåñÂ≠¶/ÂåñÂ≠¶Â∑•Á®ã‰∏éÊäÄÊúØ': 17, 'ÂéÜÂè≤Â≠¶': 18, 'Âè£ËÖîÂåªÂ≠¶': 19, 'Âì≤Â≠¶': 20, 'Âõ≠Ëâ∫Â≠¶': 21, 'Âõæ‰π¶È¶Ü„ÄÅÊÉÖÊä•‰∏éÊ°£Ê°àÁÆ°ÁêÜ': 22, 'ÂúüÊú®Â∑•Á®ã': 23, 'Âú∞ÁêÉÁâ©ÁêÜÂ≠¶': 24, 'Âú∞ÁêÜÂ≠¶': 25, 'Âú∞Ë¥®Â≠¶/Âú∞Ë¥®ËµÑÊ∫ê‰∏éÂú∞Ë¥®Â∑•Á®ã': 26, 'Âü∫Á°ÄÂåªÂ≠¶/‰∏¥Â∫äÂåªÂ≠¶': 27, 'Â§ßÊ∞îÁßëÂ≠¶': 28, 'Â§©ÊñáÂ≠¶': 29, 'Â∑•ÂïÜÁÆ°ÁêÜ': 30, 'Â∫îÁî®ÁªèÊµéÂ≠¶': 31, 'Âª∫Á≠ëÂ≠¶': 32, 'ÂøÉÁêÜÂ≠¶': 33, 'ÊéßÂà∂ÁßëÂ≠¶‰∏éÂ∑•Á®ã': 34, 'ÊîøÊ≤ªÂ≠¶': 35, 'ÊïôËÇ≤Â≠¶': 36, 'Êï∞Â≠¶': 37, 'Êñ∞Èóª‰º†Êí≠Â≠¶': 38, 'Êú∫Ê¢∞Â∑•Á®ã': 39, 'ÊùêÊñôÁßëÂ≠¶‰∏éÂ∑•Á®ã': 40, 'ÊûóÂ≠¶/Êûó‰∏öÂ∑•Á®ã': 41, 'Ê†∏ÁßëÂ≠¶‰∏éÊäÄÊúØ': 42, 'Ê§çÁâ©‰øùÊä§': 43, 'Ê∞ëÊóèÂ≠¶': 44, 'Ê∞¥‰∫ß': 45, 'Ê∞¥Âà©Â∑•Á®ã': 46, 'Ê≥ïÂ≠¶': 47, 'ÊµãÁªòÁßëÂ≠¶‰∏éÊäÄÊúØ': 48, 'Êµ∑Ê¥ãÁßëÂ≠¶': 49, 'Áâ©ÁêÜÂ≠¶': 50, 'ÁéØÂ¢ÉÁßëÂ≠¶‰∏éÂ∑•Á®ã': 51, 'ÁêÜËÆ∫ÁªèÊµéÂ≠¶': 52, 'ÁîüÁâ©Â≠¶/ÁîüÁâ©ÁßëÂ≠¶‰∏éÂ∑•Á®ã': 53, 'ÁîµÂ≠êÁßëÂ≠¶‰∏éÊäÄÊúØ': 54, 'ÁîµÊ∞îÂ∑•Á®ã': 55, 'ÁïúÁâßÂ≠¶/ÂÖΩÂåªÂ≠¶': 56, 'Áü≥Ê≤π‰∏éÂ§©ÁÑ∂Ê∞îÂ∑•Á®ã': 57, 'Áüø‰∏öÂ∑•Á®ã': 58, 'Á§æ‰ºöÂ≠¶': 59, 'Á∫∫ÁªáÁßëÂ≠¶‰∏éÂ∑•Á®ã': 60, 'Ëà™Á©∫ÂÆáËà™ÁßëÂ≠¶‰∏éÊäÄÊúØ': 61, 'ËàπËà∂‰∏éÊµ∑Ê¥ãÂ∑•Á®ã': 62, 'Ëâ∫ÊúØÂ≠¶': 63, 'ËçØÂ≠¶': 64, 'ËÆ°ÁÆóÊú∫ÁßëÂ≠¶‰∏éÊäÄÊúØ': 65, 'È£üÂìÅÁßëÂ≠¶‰∏éÂ∑•Á®ã': 66}[0m
2022-08-31 18:04:38,777 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-31 18:04:40,043] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 18:04:40,043] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-31 18:04:40,043] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 18:04:40,044] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-31 18:04:40,044] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-31 18:04:40,044] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-31 18:04:40,044] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-31 18:04:40,044] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-31 18:04:40,044] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-31 18:04:40,044] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-31 18:04:40,044] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-31 18:04:40,044] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-31 18:04:40,044] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-31 18:04:40,044] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-31 18:04:40,044] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-31 18:04:40,045] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-31 18:04:40,045] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-31 18:04:40,045] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-31 18:04:40,045] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-31 18:04:40,045] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-31 18:04:40,045] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-31 18:04:40,045] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-31 18:04:40,045] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-31 18:04:40,045] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-31 18:04:40,045] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-31 18:04:40,045] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-31 18:04:40,045] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-31 18:04:40,045] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-31 18:04:40,045] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-31 18:04:40,046] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-31 18:04:40,046] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-31 18:04:40,046] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-31 18:04:40,046] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-31 18:04:40,046] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-31 18:04:40,046] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-31 18:04:40,046] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-31 18:04:40,046] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-31 18:04:40,046] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug31_18-04-32_instance-3bwob41y-01[0m
[32m[2022-08-31 18:04:40,046] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-31 18:04:40,046] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-31 18:04:40,046] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-31 18:04:40,046] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-31 18:04:40,047] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-31 18:04:40,047] [    INFO][0m - max_seq_length                :256[0m
[32m[2022-08-31 18:04:40,047] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-31 18:04:40,047] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-31 18:04:40,047] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-31 18:04:40,047] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-31 18:04:40,047] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-08-31 18:04:40,047] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-31 18:04:40,047] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-31 18:04:40,047] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-31 18:04:40,047] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-31 18:04:40,047] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-31 18:04:40,047] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-31 18:04:40,047] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-31 18:04:40,048] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-31 18:04:40,048] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-31 18:04:40,048] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-31 18:04:40,048] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-31 18:04:40,048] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-31 18:04:40,048] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-31 18:04:40,048] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-31 18:04:40,048] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-31 18:04:40,048] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-31 18:04:40,048] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-31 18:04:40,048] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-31 18:04:40,048] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-31 18:04:40,048] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-31 18:04:40,049] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-31 18:04:40,049] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-31 18:04:40,049] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-31 18:04:40,049] [    INFO][0m - seed                          :42[0m
[32m[2022-08-31 18:04:40,049] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-31 18:04:40,049] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-31 18:04:40,049] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-31 18:04:40,049] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-31 18:04:40,049] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-31 18:04:40,049] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-31 18:04:40,049] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-31 18:04:40,049] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-31 18:04:40,049] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-31 18:04:40,049] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-31 18:04:40,050] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-31 18:04:40,050] [    INFO][0m - [0m
[32m[2022-08-31 18:04:40,051] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-31 18:04:40,051] [    INFO][0m -   Num examples = 2036[0m
[32m[2022-08-31 18:04:40,051] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-08-31 18:04:40,052] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-31 18:04:40,052] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-31 18:04:40,052] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-31 18:04:40,052] [    INFO][0m -   Total optimization steps = 12750.0[0m
[32m[2022-08-31 18:04:40,052] [    INFO][0m -   Total num train samples = 101800[0m
[32m[2022-08-31 18:04:43,314] [    INFO][0m - loss: 4.29268799, learning_rate: 2.9976470588235296e-05, global_step: 10, interval_runtime: 3.2601, interval_samples_per_second: 2.454, interval_steps_per_second: 3.067, epoch: 0.0392[0m
[32m[2022-08-31 18:04:45,223] [    INFO][0m - loss: 3.0183136, learning_rate: 2.9952941176470588e-05, global_step: 20, interval_runtime: 1.9099, interval_samples_per_second: 4.189, interval_steps_per_second: 5.236, epoch: 0.0784[0m
[32m[2022-08-31 18:04:47,156] [    INFO][0m - loss: 2.64802704, learning_rate: 2.9929411764705883e-05, global_step: 30, interval_runtime: 1.9324, interval_samples_per_second: 4.14, interval_steps_per_second: 5.175, epoch: 0.1176[0m
[32m[2022-08-31 18:04:49,091] [    INFO][0m - loss: 2.30685081, learning_rate: 2.9905882352941175e-05, global_step: 40, interval_runtime: 1.9351, interval_samples_per_second: 4.134, interval_steps_per_second: 5.168, epoch: 0.1569[0m
[32m[2022-08-31 18:04:51,043] [    INFO][0m - loss: 2.15134506, learning_rate: 2.988235294117647e-05, global_step: 50, interval_runtime: 1.9528, interval_samples_per_second: 4.097, interval_steps_per_second: 5.121, epoch: 0.1961[0m
[32m[2022-08-31 18:04:52,958] [    INFO][0m - loss: 2.43043594, learning_rate: 2.9858823529411763e-05, global_step: 60, interval_runtime: 1.9148, interval_samples_per_second: 4.178, interval_steps_per_second: 5.222, epoch: 0.2353[0m
[32m[2022-08-31 18:04:54,871] [    INFO][0m - loss: 2.28196621, learning_rate: 2.9835294117647058e-05, global_step: 70, interval_runtime: 1.9127, interval_samples_per_second: 4.183, interval_steps_per_second: 5.228, epoch: 0.2745[0m
[32m[2022-08-31 18:04:56,749] [    INFO][0m - loss: 1.82489395, learning_rate: 2.9811764705882357e-05, global_step: 80, interval_runtime: 1.878, interval_samples_per_second: 4.26, interval_steps_per_second: 5.325, epoch: 0.3137[0m
[32m[2022-08-31 18:04:58,636] [    INFO][0m - loss: 2.22058334, learning_rate: 2.978823529411765e-05, global_step: 90, interval_runtime: 1.8871, interval_samples_per_second: 4.239, interval_steps_per_second: 5.299, epoch: 0.3529[0m
[32m[2022-08-31 18:05:00,484] [    INFO][0m - loss: 1.81770535, learning_rate: 2.9764705882352944e-05, global_step: 100, interval_runtime: 1.8472, interval_samples_per_second: 4.331, interval_steps_per_second: 5.414, epoch: 0.3922[0m
[32m[2022-08-31 18:05:00,485] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:05:00,485] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 18:05:00,485] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:05:00,485] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:05:00,486] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 18:05:19,140] [    INFO][0m - eval_loss: 1.8239384889602661, eval_accuracy: 0.5188588007736944, eval_runtime: 18.6545, eval_samples_per_second: 110.858, eval_steps_per_second: 3.484, epoch: 0.3922[0m
[32m[2022-08-31 18:05:19,141] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-31 18:05:19,141] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:05:22,264] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-31 18:05:22,265] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-31 18:05:29,645] [    INFO][0m - loss: 2.53931084, learning_rate: 2.9741176470588236e-05, global_step: 110, interval_runtime: 29.162, interval_samples_per_second: 0.274, interval_steps_per_second: 0.343, epoch: 0.4314[0m
[32m[2022-08-31 18:05:31,254] [    INFO][0m - loss: 1.62861538, learning_rate: 2.971764705882353e-05, global_step: 120, interval_runtime: 1.6086, interval_samples_per_second: 4.973, interval_steps_per_second: 6.217, epoch: 0.4706[0m
[32m[2022-08-31 18:05:32,863] [    INFO][0m - loss: 2.07700367, learning_rate: 2.9694117647058823e-05, global_step: 130, interval_runtime: 1.6091, interval_samples_per_second: 4.972, interval_steps_per_second: 6.215, epoch: 0.5098[0m
[32m[2022-08-31 18:05:34,479] [    INFO][0m - loss: 2.06900845, learning_rate: 2.967058823529412e-05, global_step: 140, interval_runtime: 1.616, interval_samples_per_second: 4.951, interval_steps_per_second: 6.188, epoch: 0.549[0m
[32m[2022-08-31 18:05:36,097] [    INFO][0m - loss: 1.78053169, learning_rate: 2.9647058823529414e-05, global_step: 150, interval_runtime: 1.6182, interval_samples_per_second: 4.944, interval_steps_per_second: 6.18, epoch: 0.5882[0m
[32m[2022-08-31 18:05:37,712] [    INFO][0m - loss: 1.93918648, learning_rate: 2.9623529411764706e-05, global_step: 160, interval_runtime: 1.6152, interval_samples_per_second: 4.953, interval_steps_per_second: 6.191, epoch: 0.6275[0m
[32m[2022-08-31 18:05:39,335] [    INFO][0m - loss: 1.96838341, learning_rate: 2.96e-05, global_step: 170, interval_runtime: 1.6228, interval_samples_per_second: 4.93, interval_steps_per_second: 6.162, epoch: 0.6667[0m
[32m[2022-08-31 18:05:40,945] [    INFO][0m - loss: 1.74262295, learning_rate: 2.9576470588235293e-05, global_step: 180, interval_runtime: 1.6102, interval_samples_per_second: 4.968, interval_steps_per_second: 6.211, epoch: 0.7059[0m
[32m[2022-08-31 18:05:42,565] [    INFO][0m - loss: 1.66548252, learning_rate: 2.955294117647059e-05, global_step: 190, interval_runtime: 1.6193, interval_samples_per_second: 4.94, interval_steps_per_second: 6.176, epoch: 0.7451[0m
[32m[2022-08-31 18:05:44,185] [    INFO][0m - loss: 2.28549538, learning_rate: 2.952941176470588e-05, global_step: 200, interval_runtime: 1.6208, interval_samples_per_second: 4.936, interval_steps_per_second: 6.17, epoch: 0.7843[0m
[32m[2022-08-31 18:05:44,186] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:05:44,186] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 18:05:44,186] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:05:44,186] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:05:44,186] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 18:06:03,065] [    INFO][0m - eval_loss: 1.7101821899414062, eval_accuracy: 0.5140232108317214, eval_runtime: 18.8782, eval_samples_per_second: 109.545, eval_steps_per_second: 3.443, epoch: 0.7843[0m
[32m[2022-08-31 18:06:03,066] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-31 18:06:03,067] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:06:06,665] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-31 18:06:06,666] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-31 18:06:14,494] [    INFO][0m - loss: 1.98762474, learning_rate: 2.9505882352941176e-05, global_step: 210, interval_runtime: 30.3089, interval_samples_per_second: 0.264, interval_steps_per_second: 0.33, epoch: 0.8235[0m
[32m[2022-08-31 18:06:16,466] [    INFO][0m - loss: 1.55591736, learning_rate: 2.948235294117647e-05, global_step: 220, interval_runtime: 1.9712, interval_samples_per_second: 4.058, interval_steps_per_second: 5.073, epoch: 0.8627[0m
[32m[2022-08-31 18:06:18,494] [    INFO][0m - loss: 1.56383801, learning_rate: 2.9458823529411763e-05, global_step: 230, interval_runtime: 2.028, interval_samples_per_second: 3.945, interval_steps_per_second: 4.931, epoch: 0.902[0m
[32m[2022-08-31 18:06:20,479] [    INFO][0m - loss: 2.38441582, learning_rate: 2.9435294117647062e-05, global_step: 240, interval_runtime: 1.9848, interval_samples_per_second: 4.031, interval_steps_per_second: 5.038, epoch: 0.9412[0m
[32m[2022-08-31 18:06:22,431] [    INFO][0m - loss: 1.91971893, learning_rate: 2.9411764705882354e-05, global_step: 250, interval_runtime: 1.953, interval_samples_per_second: 4.096, interval_steps_per_second: 5.12, epoch: 0.9804[0m
[32m[2022-08-31 18:06:24,286] [    INFO][0m - loss: 1.61256161, learning_rate: 2.938823529411765e-05, global_step: 260, interval_runtime: 1.8547, interval_samples_per_second: 4.313, interval_steps_per_second: 5.392, epoch: 1.0196[0m
[32m[2022-08-31 18:06:26,227] [    INFO][0m - loss: 1.12540245, learning_rate: 2.9364705882352944e-05, global_step: 270, interval_runtime: 1.9414, interval_samples_per_second: 4.121, interval_steps_per_second: 5.151, epoch: 1.0588[0m
[32m[2022-08-31 18:06:28,227] [    INFO][0m - loss: 1.15039845, learning_rate: 2.9341176470588236e-05, global_step: 280, interval_runtime: 1.9984, interval_samples_per_second: 4.003, interval_steps_per_second: 5.004, epoch: 1.098[0m
[32m[2022-08-31 18:06:30,145] [    INFO][0m - loss: 1.14463558, learning_rate: 2.9317647058823532e-05, global_step: 290, interval_runtime: 1.9193, interval_samples_per_second: 4.168, interval_steps_per_second: 5.21, epoch: 1.1373[0m
[32m[2022-08-31 18:06:32,091] [    INFO][0m - loss: 1.36518116, learning_rate: 2.9294117647058824e-05, global_step: 300, interval_runtime: 1.9456, interval_samples_per_second: 4.112, interval_steps_per_second: 5.14, epoch: 1.1765[0m
[32m[2022-08-31 18:06:32,091] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:06:32,091] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 18:06:32,091] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:06:32,091] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:06:32,092] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 18:06:51,660] [    INFO][0m - eval_loss: 1.6425042152404785, eval_accuracy: 0.5309477756286267, eval_runtime: 19.5681, eval_samples_per_second: 105.682, eval_steps_per_second: 3.322, epoch: 1.1765[0m
[32m[2022-08-31 18:06:51,661] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-31 18:06:51,661] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:06:55,303] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-31 18:06:55,303] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-31 18:07:02,534] [    INFO][0m - loss: 1.25582304, learning_rate: 2.927058823529412e-05, global_step: 310, interval_runtime: 30.4435, interval_samples_per_second: 0.263, interval_steps_per_second: 0.328, epoch: 1.2157[0m
[32m[2022-08-31 18:07:04,158] [    INFO][0m - loss: 1.37823238, learning_rate: 2.924705882352941e-05, global_step: 320, interval_runtime: 1.624, interval_samples_per_second: 4.926, interval_steps_per_second: 6.158, epoch: 1.2549[0m
[32m[2022-08-31 18:07:05,792] [    INFO][0m - loss: 1.09010458, learning_rate: 2.9223529411764706e-05, global_step: 330, interval_runtime: 1.6302, interval_samples_per_second: 4.907, interval_steps_per_second: 6.134, epoch: 1.2941[0m
[32m[2022-08-31 18:07:07,418] [    INFO][0m - loss: 1.45509806, learning_rate: 2.92e-05, global_step: 340, interval_runtime: 1.6289, interval_samples_per_second: 4.911, interval_steps_per_second: 6.139, epoch: 1.3333[0m
[32m[2022-08-31 18:07:09,265] [    INFO][0m - loss: 1.88124332, learning_rate: 2.9176470588235294e-05, global_step: 350, interval_runtime: 1.6209, interval_samples_per_second: 4.935, interval_steps_per_second: 6.169, epoch: 1.3725[0m
[32m[2022-08-31 18:07:10,884] [    INFO][0m - loss: 1.29903603, learning_rate: 2.915294117647059e-05, global_step: 360, interval_runtime: 1.8454, interval_samples_per_second: 4.335, interval_steps_per_second: 5.419, epoch: 1.4118[0m
[32m[2022-08-31 18:07:12,511] [    INFO][0m - loss: 1.42542286, learning_rate: 2.912941176470588e-05, global_step: 370, interval_runtime: 1.6273, interval_samples_per_second: 4.916, interval_steps_per_second: 6.145, epoch: 1.451[0m
[32m[2022-08-31 18:07:14,137] [    INFO][0m - loss: 1.17307358, learning_rate: 2.9105882352941176e-05, global_step: 380, interval_runtime: 1.6254, interval_samples_per_second: 4.922, interval_steps_per_second: 6.152, epoch: 1.4902[0m
[32m[2022-08-31 18:07:15,757] [    INFO][0m - loss: 1.50555601, learning_rate: 2.908235294117647e-05, global_step: 390, interval_runtime: 1.6205, interval_samples_per_second: 4.937, interval_steps_per_second: 6.171, epoch: 1.5294[0m
[32m[2022-08-31 18:07:17,390] [    INFO][0m - loss: 1.28287916, learning_rate: 2.9058823529411767e-05, global_step: 400, interval_runtime: 1.6329, interval_samples_per_second: 4.899, interval_steps_per_second: 6.124, epoch: 1.5686[0m
[32m[2022-08-31 18:07:17,391] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:07:17,391] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 18:07:17,391] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:07:17,391] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:07:17,391] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 18:07:35,771] [    INFO][0m - eval_loss: 1.6337885856628418, eval_accuracy: 0.5488394584139265, eval_runtime: 18.3791, eval_samples_per_second: 112.519, eval_steps_per_second: 3.537, epoch: 1.5686[0m
[32m[2022-08-31 18:07:35,771] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-31 18:07:35,772] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:07:38,874] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-31 18:07:38,874] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-31 18:07:46,155] [    INFO][0m - loss: 1.29280825, learning_rate: 2.9035294117647062e-05, global_step: 410, interval_runtime: 28.7653, interval_samples_per_second: 0.278, interval_steps_per_second: 0.348, epoch: 1.6078[0m
[32m[2022-08-31 18:07:47,766] [    INFO][0m - loss: 1.44494352, learning_rate: 2.9011764705882354e-05, global_step: 420, interval_runtime: 1.611, interval_samples_per_second: 4.966, interval_steps_per_second: 6.207, epoch: 1.6471[0m
[32m[2022-08-31 18:07:49,380] [    INFO][0m - loss: 1.23276167, learning_rate: 2.898823529411765e-05, global_step: 430, interval_runtime: 1.6141, interval_samples_per_second: 4.956, interval_steps_per_second: 6.195, epoch: 1.6863[0m
[32m[2022-08-31 18:07:50,995] [    INFO][0m - loss: 1.29260044, learning_rate: 2.896470588235294e-05, global_step: 440, interval_runtime: 1.615, interval_samples_per_second: 4.954, interval_steps_per_second: 6.192, epoch: 1.7255[0m
[32m[2022-08-31 18:07:52,611] [    INFO][0m - loss: 1.26367168, learning_rate: 2.8941176470588237e-05, global_step: 450, interval_runtime: 1.6158, interval_samples_per_second: 4.951, interval_steps_per_second: 6.189, epoch: 1.7647[0m
[32m[2022-08-31 18:07:54,221] [    INFO][0m - loss: 1.42815704, learning_rate: 2.891764705882353e-05, global_step: 460, interval_runtime: 1.6096, interval_samples_per_second: 4.97, interval_steps_per_second: 6.213, epoch: 1.8039[0m
[32m[2022-08-31 18:07:55,844] [    INFO][0m - loss: 1.43618288, learning_rate: 2.8894117647058824e-05, global_step: 470, interval_runtime: 1.6232, interval_samples_per_second: 4.928, interval_steps_per_second: 6.161, epoch: 1.8431[0m
[32m[2022-08-31 18:07:57,459] [    INFO][0m - loss: 1.48888741, learning_rate: 2.887058823529412e-05, global_step: 480, interval_runtime: 1.6146, interval_samples_per_second: 4.955, interval_steps_per_second: 6.194, epoch: 1.8824[0m
[32m[2022-08-31 18:07:59,075] [    INFO][0m - loss: 1.31447029, learning_rate: 2.884705882352941e-05, global_step: 490, interval_runtime: 1.6168, interval_samples_per_second: 4.948, interval_steps_per_second: 6.185, epoch: 1.9216[0m
[32m[2022-08-31 18:08:00,698] [    INFO][0m - loss: 1.71222382, learning_rate: 2.8823529411764707e-05, global_step: 500, interval_runtime: 1.6225, interval_samples_per_second: 4.931, interval_steps_per_second: 6.163, epoch: 1.9608[0m
[32m[2022-08-31 18:08:00,698] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:08:00,699] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 18:08:00,699] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:08:00,699] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:08:00,699] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 18:08:19,133] [    INFO][0m - eval_loss: 1.5294945240020752, eval_accuracy: 0.5802707930367504, eval_runtime: 18.4335, eval_samples_per_second: 112.187, eval_steps_per_second: 3.526, epoch: 1.9608[0m
[32m[2022-08-31 18:08:19,133] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-08-31 18:08:19,134] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:08:22,198] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-08-31 18:08:22,198] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-08-31 18:08:29,320] [    INFO][0m - loss: 1.0287425, learning_rate: 2.88e-05, global_step: 510, interval_runtime: 28.6211, interval_samples_per_second: 0.28, interval_steps_per_second: 0.349, epoch: 2.0[0m
[32m[2022-08-31 18:08:31,030] [    INFO][0m - loss: 0.83685265, learning_rate: 2.8776470588235294e-05, global_step: 520, interval_runtime: 1.7108, interval_samples_per_second: 4.676, interval_steps_per_second: 5.845, epoch: 2.0392[0m
[32m[2022-08-31 18:08:32,646] [    INFO][0m - loss: 0.89511166, learning_rate: 2.8752941176470586e-05, global_step: 530, interval_runtime: 1.6161, interval_samples_per_second: 4.95, interval_steps_per_second: 6.188, epoch: 2.0784[0m
[32m[2022-08-31 18:08:34,260] [    INFO][0m - loss: 0.90385246, learning_rate: 2.872941176470588e-05, global_step: 540, interval_runtime: 1.6145, interval_samples_per_second: 4.955, interval_steps_per_second: 6.194, epoch: 2.1176[0m
[32m[2022-08-31 18:08:35,877] [    INFO][0m - loss: 0.88279333, learning_rate: 2.870588235294118e-05, global_step: 550, interval_runtime: 1.6169, interval_samples_per_second: 4.948, interval_steps_per_second: 6.185, epoch: 2.1569[0m
[32m[2022-08-31 18:08:37,485] [    INFO][0m - loss: 0.81096096, learning_rate: 2.8682352941176472e-05, global_step: 560, interval_runtime: 1.6082, interval_samples_per_second: 4.974, interval_steps_per_second: 6.218, epoch: 2.1961[0m
[32m[2022-08-31 18:08:39,106] [    INFO][0m - loss: 0.71391783, learning_rate: 2.8658823529411767e-05, global_step: 570, interval_runtime: 1.6197, interval_samples_per_second: 4.939, interval_steps_per_second: 6.174, epoch: 2.2353[0m
[32m[2022-08-31 18:08:40,729] [    INFO][0m - loss: 0.93318691, learning_rate: 2.863529411764706e-05, global_step: 580, interval_runtime: 1.6236, interval_samples_per_second: 4.927, interval_steps_per_second: 6.159, epoch: 2.2745[0m
[32m[2022-08-31 18:08:42,375] [    INFO][0m - loss: 1.00710936, learning_rate: 2.8611764705882355e-05, global_step: 590, interval_runtime: 1.646, interval_samples_per_second: 4.86, interval_steps_per_second: 6.075, epoch: 2.3137[0m
[32m[2022-08-31 18:08:44,003] [    INFO][0m - loss: 0.82292309, learning_rate: 2.8588235294117647e-05, global_step: 600, interval_runtime: 1.6272, interval_samples_per_second: 4.916, interval_steps_per_second: 6.145, epoch: 2.3529[0m
[32m[2022-08-31 18:08:44,003] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:08:44,004] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 18:08:44,004] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:08:44,004] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:08:44,004] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 18:09:02,495] [    INFO][0m - eval_loss: 1.7155249118804932, eval_accuracy: 0.5730174081237911, eval_runtime: 18.4904, eval_samples_per_second: 111.842, eval_steps_per_second: 3.515, epoch: 2.3529[0m
[32m[2022-08-31 18:09:02,496] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-08-31 18:09:02,496] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:09:04,258] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-08-31 18:09:04,258] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-08-31 18:09:08,733] [    INFO][0m - loss: 0.92842579, learning_rate: 2.8564705882352942e-05, global_step: 610, interval_runtime: 24.7308, interval_samples_per_second: 0.323, interval_steps_per_second: 0.404, epoch: 2.3922[0m
[32m[2022-08-31 18:09:10,362] [    INFO][0m - loss: 0.97564926, learning_rate: 2.8541176470588237e-05, global_step: 620, interval_runtime: 1.6286, interval_samples_per_second: 4.912, interval_steps_per_second: 6.14, epoch: 2.4314[0m
[32m[2022-08-31 18:09:11,976] [    INFO][0m - loss: 0.87157526, learning_rate: 2.851764705882353e-05, global_step: 630, interval_runtime: 1.6145, interval_samples_per_second: 4.955, interval_steps_per_second: 6.194, epoch: 2.4706[0m
[32m[2022-08-31 18:09:13,589] [    INFO][0m - loss: 0.86988287, learning_rate: 2.8494117647058825e-05, global_step: 640, interval_runtime: 1.6131, interval_samples_per_second: 4.959, interval_steps_per_second: 6.199, epoch: 2.5098[0m
[32m[2022-08-31 18:09:15,216] [    INFO][0m - loss: 0.71158628, learning_rate: 2.8470588235294117e-05, global_step: 650, interval_runtime: 1.6262, interval_samples_per_second: 4.92, interval_steps_per_second: 6.149, epoch: 2.549[0m
[32m[2022-08-31 18:09:16,833] [    INFO][0m - loss: 0.80443459, learning_rate: 2.8447058823529412e-05, global_step: 660, interval_runtime: 1.6172, interval_samples_per_second: 4.947, interval_steps_per_second: 6.183, epoch: 2.5882[0m
[32m[2022-08-31 18:09:18,448] [    INFO][0m - loss: 1.05030022, learning_rate: 2.8423529411764707e-05, global_step: 670, interval_runtime: 1.6155, interval_samples_per_second: 4.952, interval_steps_per_second: 6.19, epoch: 2.6275[0m
[32m[2022-08-31 18:09:20,066] [    INFO][0m - loss: 0.96591425, learning_rate: 2.84e-05, global_step: 680, interval_runtime: 1.617, interval_samples_per_second: 4.947, interval_steps_per_second: 6.184, epoch: 2.6667[0m
[32m[2022-08-31 18:09:21,692] [    INFO][0m - loss: 0.83220911, learning_rate: 2.8376470588235294e-05, global_step: 690, interval_runtime: 1.6264, interval_samples_per_second: 4.919, interval_steps_per_second: 6.148, epoch: 2.7059[0m
[32m[2022-08-31 18:09:23,315] [    INFO][0m - loss: 0.75583553, learning_rate: 2.835294117647059e-05, global_step: 700, interval_runtime: 1.6235, interval_samples_per_second: 4.928, interval_steps_per_second: 6.16, epoch: 2.7451[0m
[32m[2022-08-31 18:09:23,316] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:09:23,316] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 18:09:23,316] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:09:23,316] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:09:23,316] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 18:09:41,847] [    INFO][0m - eval_loss: 1.585652470588684, eval_accuracy: 0.5855899419729207, eval_runtime: 18.5298, eval_samples_per_second: 111.604, eval_steps_per_second: 3.508, epoch: 2.7451[0m
[32m[2022-08-31 18:09:42,574] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-700[0m
[32m[2022-08-31 18:09:42,574] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:09:44,363] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-700/tokenizer_config.json[0m
[32m[2022-08-31 18:09:44,364] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-700/special_tokens_map.json[0m
[32m[2022-08-31 18:09:49,036] [    INFO][0m - loss: 1.02761841, learning_rate: 2.8329411764705885e-05, global_step: 710, interval_runtime: 25.7203, interval_samples_per_second: 0.311, interval_steps_per_second: 0.389, epoch: 2.7843[0m
[32m[2022-08-31 18:09:50,659] [    INFO][0m - loss: 0.97932205, learning_rate: 2.8305882352941177e-05, global_step: 720, interval_runtime: 1.6239, interval_samples_per_second: 4.926, interval_steps_per_second: 6.158, epoch: 2.8235[0m
[32m[2022-08-31 18:09:52,277] [    INFO][0m - loss: 1.07807255, learning_rate: 2.8282352941176472e-05, global_step: 730, interval_runtime: 1.6173, interval_samples_per_second: 4.946, interval_steps_per_second: 6.183, epoch: 2.8627[0m
[32m[2022-08-31 18:09:53,904] [    INFO][0m - loss: 1.16104536, learning_rate: 2.8258823529411768e-05, global_step: 740, interval_runtime: 1.627, interval_samples_per_second: 4.917, interval_steps_per_second: 6.146, epoch: 2.902[0m
[32m[2022-08-31 18:09:55,524] [    INFO][0m - loss: 0.92030296, learning_rate: 2.823529411764706e-05, global_step: 750, interval_runtime: 1.6201, interval_samples_per_second: 4.938, interval_steps_per_second: 6.173, epoch: 2.9412[0m
[32m[2022-08-31 18:09:57,143] [    INFO][0m - loss: 0.8233489, learning_rate: 2.8211764705882355e-05, global_step: 760, interval_runtime: 1.6198, interval_samples_per_second: 4.939, interval_steps_per_second: 6.174, epoch: 2.9804[0m
[32m[2022-08-31 18:09:58,748] [    INFO][0m - loss: 0.65555468, learning_rate: 2.8188235294117647e-05, global_step: 770, interval_runtime: 1.6043, interval_samples_per_second: 4.987, interval_steps_per_second: 6.233, epoch: 3.0196[0m
[32m[2022-08-31 18:10:00,361] [    INFO][0m - loss: 0.62188749, learning_rate: 2.8164705882352942e-05, global_step: 780, interval_runtime: 1.6137, interval_samples_per_second: 4.957, interval_steps_per_second: 6.197, epoch: 3.0588[0m
[32m[2022-08-31 18:10:01,992] [    INFO][0m - loss: 0.77324495, learning_rate: 2.8141176470588234e-05, global_step: 790, interval_runtime: 1.6298, interval_samples_per_second: 4.909, interval_steps_per_second: 6.136, epoch: 3.098[0m
[32m[2022-08-31 18:10:03,607] [    INFO][0m - loss: 0.46661825, learning_rate: 2.811764705882353e-05, global_step: 800, interval_runtime: 1.6158, interval_samples_per_second: 4.951, interval_steps_per_second: 6.189, epoch: 3.1373[0m
[32m[2022-08-31 18:10:03,608] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:10:03,608] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 18:10:03,608] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:10:03,608] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:10:03,608] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 18:10:22,345] [    INFO][0m - eval_loss: 1.621410608291626, eval_accuracy: 0.5986460348162476, eval_runtime: 18.7359, eval_samples_per_second: 110.376, eval_steps_per_second: 3.469, epoch: 3.1373[0m
[32m[2022-08-31 18:10:22,345] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-800[0m
[32m[2022-08-31 18:10:22,345] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:10:24,082] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-800/tokenizer_config.json[0m
[32m[2022-08-31 18:10:24,083] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-800/special_tokens_map.json[0m
[32m[2022-08-31 18:10:28,811] [    INFO][0m - loss: 0.50172882, learning_rate: 2.8094117647058825e-05, global_step: 810, interval_runtime: 25.2032, interval_samples_per_second: 0.317, interval_steps_per_second: 0.397, epoch: 3.1765[0m
[32m[2022-08-31 18:10:30,423] [    INFO][0m - loss: 0.4555964, learning_rate: 2.8070588235294117e-05, global_step: 820, interval_runtime: 1.6131, interval_samples_per_second: 4.959, interval_steps_per_second: 6.199, epoch: 3.2157[0m
[32m[2022-08-31 18:10:32,048] [    INFO][0m - loss: 0.38704982, learning_rate: 2.8047058823529412e-05, global_step: 830, interval_runtime: 1.6239, interval_samples_per_second: 4.926, interval_steps_per_second: 6.158, epoch: 3.2549[0m
[32m[2022-08-31 18:10:33,675] [    INFO][0m - loss: 0.6238626, learning_rate: 2.8023529411764704e-05, global_step: 840, interval_runtime: 1.6278, interval_samples_per_second: 4.915, interval_steps_per_second: 6.143, epoch: 3.2941[0m
[32m[2022-08-31 18:10:35,302] [    INFO][0m - loss: 0.48345251, learning_rate: 2.8e-05, global_step: 850, interval_runtime: 1.626, interval_samples_per_second: 4.92, interval_steps_per_second: 6.15, epoch: 3.3333[0m
[32m[2022-08-31 18:10:36,928] [    INFO][0m - loss: 0.55616055, learning_rate: 2.7976470588235295e-05, global_step: 860, interval_runtime: 1.6258, interval_samples_per_second: 4.921, interval_steps_per_second: 6.151, epoch: 3.3725[0m
[32m[2022-08-31 18:10:38,559] [    INFO][0m - loss: 0.45203624, learning_rate: 2.795294117647059e-05, global_step: 870, interval_runtime: 1.6319, interval_samples_per_second: 4.902, interval_steps_per_second: 6.128, epoch: 3.4118[0m
[32m[2022-08-31 18:10:40,184] [    INFO][0m - loss: 0.83355751, learning_rate: 2.7929411764705886e-05, global_step: 880, interval_runtime: 1.625, interval_samples_per_second: 4.923, interval_steps_per_second: 6.154, epoch: 3.451[0m
[32m[2022-08-31 18:10:41,806] [    INFO][0m - loss: 0.4496376, learning_rate: 2.7905882352941178e-05, global_step: 890, interval_runtime: 1.6217, interval_samples_per_second: 4.933, interval_steps_per_second: 6.166, epoch: 3.4902[0m
[32m[2022-08-31 18:10:43,431] [    INFO][0m - loss: 0.51861801, learning_rate: 2.7882352941176473e-05, global_step: 900, interval_runtime: 1.6256, interval_samples_per_second: 4.921, interval_steps_per_second: 6.152, epoch: 3.5294[0m
[32m[2022-08-31 18:10:43,432] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:10:43,432] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 18:10:43,432] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:10:43,432] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:10:43,432] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 18:11:02,015] [    INFO][0m - eval_loss: 1.6216373443603516, eval_accuracy: 0.6107350096711799, eval_runtime: 18.5824, eval_samples_per_second: 111.288, eval_steps_per_second: 3.498, epoch: 3.5294[0m
[32m[2022-08-31 18:11:02,016] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-900[0m
[32m[2022-08-31 18:11:02,016] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:11:03,906] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-900/tokenizer_config.json[0m
[32m[2022-08-31 18:11:03,907] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-900/special_tokens_map.json[0m
[32m[2022-08-31 18:11:08,784] [    INFO][0m - loss: 0.63528051, learning_rate: 2.7858823529411765e-05, global_step: 910, interval_runtime: 25.3526, interval_samples_per_second: 0.316, interval_steps_per_second: 0.394, epoch: 3.5686[0m
[32m[2022-08-31 18:11:10,400] [    INFO][0m - loss: 0.55823555, learning_rate: 2.783529411764706e-05, global_step: 920, interval_runtime: 1.6154, interval_samples_per_second: 4.952, interval_steps_per_second: 6.19, epoch: 3.6078[0m
[32m[2022-08-31 18:11:12,013] [    INFO][0m - loss: 0.76437645, learning_rate: 2.7811764705882352e-05, global_step: 930, interval_runtime: 1.6134, interval_samples_per_second: 4.959, interval_steps_per_second: 6.198, epoch: 3.6471[0m
[32m[2022-08-31 18:11:13,631] [    INFO][0m - loss: 0.54610338, learning_rate: 2.7788235294117647e-05, global_step: 940, interval_runtime: 1.618, interval_samples_per_second: 4.944, interval_steps_per_second: 6.181, epoch: 3.6863[0m
[32m[2022-08-31 18:11:15,256] [    INFO][0m - loss: 0.76589799, learning_rate: 2.7764705882352943e-05, global_step: 950, interval_runtime: 1.6248, interval_samples_per_second: 4.924, interval_steps_per_second: 6.155, epoch: 3.7255[0m
[32m[2022-08-31 18:11:16,880] [    INFO][0m - loss: 0.73398314, learning_rate: 2.7741176470588235e-05, global_step: 960, interval_runtime: 1.624, interval_samples_per_second: 4.926, interval_steps_per_second: 6.158, epoch: 3.7647[0m
[32m[2022-08-31 18:11:18,509] [    INFO][0m - loss: 0.52916627, learning_rate: 2.771764705882353e-05, global_step: 970, interval_runtime: 1.6295, interval_samples_per_second: 4.909, interval_steps_per_second: 6.137, epoch: 3.8039[0m
[32m[2022-08-31 18:11:20,152] [    INFO][0m - loss: 0.47675295, learning_rate: 2.7694117647058822e-05, global_step: 980, interval_runtime: 1.6427, interval_samples_per_second: 4.87, interval_steps_per_second: 6.087, epoch: 3.8431[0m
[32m[2022-08-31 18:11:21,773] [    INFO][0m - loss: 0.56634498, learning_rate: 2.7670588235294117e-05, global_step: 990, interval_runtime: 1.6213, interval_samples_per_second: 4.934, interval_steps_per_second: 6.168, epoch: 3.8824[0m
[32m[2022-08-31 18:11:23,416] [    INFO][0m - loss: 0.59453778, learning_rate: 2.764705882352941e-05, global_step: 1000, interval_runtime: 1.6433, interval_samples_per_second: 4.868, interval_steps_per_second: 6.085, epoch: 3.9216[0m
[32m[2022-08-31 18:11:23,417] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:11:23,417] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 18:11:23,417] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:11:23,417] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:11:23,417] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 18:11:41,871] [    INFO][0m - eval_loss: 1.6563588380813599, eval_accuracy: 0.5962282398452611, eval_runtime: 18.4533, eval_samples_per_second: 112.067, eval_steps_per_second: 3.522, epoch: 3.9216[0m
[32m[2022-08-31 18:11:41,872] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1000[0m
[32m[2022-08-31 18:11:41,872] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:11:43,706] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1000/tokenizer_config.json[0m
[32m[2022-08-31 18:11:43,706] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1000/special_tokens_map.json[0m
[32m[2022-08-31 18:11:48,181] [    INFO][0m - loss: 0.57394252, learning_rate: 2.7623529411764705e-05, global_step: 1010, interval_runtime: 24.764, interval_samples_per_second: 0.323, interval_steps_per_second: 0.404, epoch: 3.9608[0m
[32m[2022-08-31 18:11:49,728] [    INFO][0m - loss: 0.5483676, learning_rate: 2.7600000000000003e-05, global_step: 1020, interval_runtime: 1.5471, interval_samples_per_second: 5.171, interval_steps_per_second: 6.464, epoch: 4.0[0m
[32m[2022-08-31 18:11:51,448] [    INFO][0m - loss: 0.45813632, learning_rate: 2.7576470588235295e-05, global_step: 1030, interval_runtime: 1.72, interval_samples_per_second: 4.651, interval_steps_per_second: 5.814, epoch: 4.0392[0m
[32m[2022-08-31 18:11:53,064] [    INFO][0m - loss: 0.25766544, learning_rate: 2.755294117647059e-05, global_step: 1040, interval_runtime: 1.6162, interval_samples_per_second: 4.95, interval_steps_per_second: 6.187, epoch: 4.0784[0m
[32m[2022-08-31 18:11:54,691] [    INFO][0m - loss: 0.34646292, learning_rate: 2.7529411764705883e-05, global_step: 1050, interval_runtime: 1.627, interval_samples_per_second: 4.917, interval_steps_per_second: 6.146, epoch: 4.1176[0m
[32m[2022-08-31 18:11:56,317] [    INFO][0m - loss: 0.41486526, learning_rate: 2.7505882352941178e-05, global_step: 1060, interval_runtime: 1.6257, interval_samples_per_second: 4.921, interval_steps_per_second: 6.151, epoch: 4.1569[0m
[32m[2022-08-31 18:11:57,937] [    INFO][0m - loss: 0.29454746, learning_rate: 2.7482352941176473e-05, global_step: 1070, interval_runtime: 1.6203, interval_samples_per_second: 4.937, interval_steps_per_second: 6.172, epoch: 4.1961[0m
[32m[2022-08-31 18:11:59,582] [    INFO][0m - loss: 0.38372738, learning_rate: 2.7458823529411765e-05, global_step: 1080, interval_runtime: 1.6445, interval_samples_per_second: 4.865, interval_steps_per_second: 6.081, epoch: 4.2353[0m
[32m[2022-08-31 18:12:01,213] [    INFO][0m - loss: 0.37818108, learning_rate: 2.743529411764706e-05, global_step: 1090, interval_runtime: 1.6307, interval_samples_per_second: 4.906, interval_steps_per_second: 6.132, epoch: 4.2745[0m
[32m[2022-08-31 18:12:02,831] [    INFO][0m - loss: 0.32927537, learning_rate: 2.7411764705882353e-05, global_step: 1100, interval_runtime: 1.6187, interval_samples_per_second: 4.942, interval_steps_per_second: 6.178, epoch: 4.3137[0m
[32m[2022-08-31 18:12:02,831] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:12:02,831] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 18:12:02,832] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:12:02,832] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:12:02,832] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 18:12:23,529] [    INFO][0m - eval_loss: 1.751434564590454, eval_accuracy: 0.6039651837524178, eval_runtime: 18.7669, eval_samples_per_second: 110.194, eval_steps_per_second: 3.464, epoch: 4.3137[0m
[32m[2022-08-31 18:12:23,530] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1100[0m
[32m[2022-08-31 18:12:23,530] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:12:25,264] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1100/tokenizer_config.json[0m
[32m[2022-08-31 18:12:25,264] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1100/special_tokens_map.json[0m
[32m[2022-08-31 18:12:29,986] [    INFO][0m - loss: 0.2739641, learning_rate: 2.7388235294117648e-05, global_step: 1110, interval_runtime: 27.1545, interval_samples_per_second: 0.295, interval_steps_per_second: 0.368, epoch: 4.3529[0m
[32m[2022-08-31 18:12:31,630] [    INFO][0m - loss: 0.43453059, learning_rate: 2.736470588235294e-05, global_step: 1120, interval_runtime: 1.6447, interval_samples_per_second: 4.864, interval_steps_per_second: 6.08, epoch: 4.3922[0m
[32m[2022-08-31 18:12:33,250] [    INFO][0m - loss: 0.50195708, learning_rate: 2.7341176470588235e-05, global_step: 1130, interval_runtime: 1.6201, interval_samples_per_second: 4.938, interval_steps_per_second: 6.173, epoch: 4.4314[0m
[32m[2022-08-31 18:12:34,905] [    INFO][0m - loss: 0.43556175, learning_rate: 2.731764705882353e-05, global_step: 1140, interval_runtime: 1.6554, interval_samples_per_second: 4.833, interval_steps_per_second: 6.041, epoch: 4.4706[0m
[32m[2022-08-31 18:12:36,545] [    INFO][0m - loss: 0.63599586, learning_rate: 2.7294117647058822e-05, global_step: 1150, interval_runtime: 1.6387, interval_samples_per_second: 4.882, interval_steps_per_second: 6.102, epoch: 4.5098[0m
[32m[2022-08-31 18:12:38,170] [    INFO][0m - loss: 0.36832404, learning_rate: 2.7270588235294118e-05, global_step: 1160, interval_runtime: 1.6257, interval_samples_per_second: 4.921, interval_steps_per_second: 6.151, epoch: 4.549[0m
[32m[2022-08-31 18:12:39,804] [    INFO][0m - loss: 0.33814778, learning_rate: 2.7247058823529413e-05, global_step: 1170, interval_runtime: 1.6347, interval_samples_per_second: 4.894, interval_steps_per_second: 6.117, epoch: 4.5882[0m
[32m[2022-08-31 18:12:41,424] [    INFO][0m - loss: 0.49256992, learning_rate: 2.722352941176471e-05, global_step: 1180, interval_runtime: 1.6198, interval_samples_per_second: 4.939, interval_steps_per_second: 6.174, epoch: 4.6275[0m
[32m[2022-08-31 18:12:43,066] [    INFO][0m - loss: 0.34735913, learning_rate: 2.72e-05, global_step: 1190, interval_runtime: 1.6417, interval_samples_per_second: 4.873, interval_steps_per_second: 6.091, epoch: 4.6667[0m
[32m[2022-08-31 18:12:44,700] [    INFO][0m - loss: 0.33804996, learning_rate: 2.7176470588235296e-05, global_step: 1200, interval_runtime: 1.6341, interval_samples_per_second: 4.896, interval_steps_per_second: 6.12, epoch: 4.7059[0m
[32m[2022-08-31 18:12:44,701] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:12:44,701] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 18:12:44,702] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:12:44,702] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:12:44,702] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 18:13:03,260] [    INFO][0m - eval_loss: 1.7770086526870728, eval_accuracy: 0.6088007736943907, eval_runtime: 18.5575, eval_samples_per_second: 111.437, eval_steps_per_second: 3.503, epoch: 4.7059[0m
[32m[2022-08-31 18:13:03,260] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1200[0m
[32m[2022-08-31 18:13:03,260] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:13:04,966] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1200/tokenizer_config.json[0m
[32m[2022-08-31 18:13:04,966] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1200/special_tokens_map.json[0m
[32m[2022-08-31 18:13:09,308] [    INFO][0m - loss: 0.41216445, learning_rate: 2.715294117647059e-05, global_step: 1210, interval_runtime: 24.6072, interval_samples_per_second: 0.325, interval_steps_per_second: 0.406, epoch: 4.7451[0m
[32m[2022-08-31 18:13:10,922] [    INFO][0m - loss: 0.27612, learning_rate: 2.7129411764705883e-05, global_step: 1220, interval_runtime: 1.6146, interval_samples_per_second: 4.955, interval_steps_per_second: 6.194, epoch: 4.7843[0m
[32m[2022-08-31 18:13:12,533] [    INFO][0m - loss: 0.2596055, learning_rate: 2.710588235294118e-05, global_step: 1230, interval_runtime: 1.6106, interval_samples_per_second: 4.967, interval_steps_per_second: 6.209, epoch: 4.8235[0m
[32m[2022-08-31 18:13:14,153] [    INFO][0m - loss: 0.41895094, learning_rate: 2.708235294117647e-05, global_step: 1240, interval_runtime: 1.6201, interval_samples_per_second: 4.938, interval_steps_per_second: 6.172, epoch: 4.8627[0m
[32m[2022-08-31 18:13:15,770] [    INFO][0m - loss: 0.36265574, learning_rate: 2.7058823529411766e-05, global_step: 1250, interval_runtime: 1.6173, interval_samples_per_second: 4.947, interval_steps_per_second: 6.183, epoch: 4.902[0m
[32m[2022-08-31 18:13:19,327] [    INFO][0m - loss: 0.45530539, learning_rate: 2.7035294117647058e-05, global_step: 1260, interval_runtime: 1.6239, interval_samples_per_second: 4.926, interval_steps_per_second: 6.158, epoch: 4.9412[0m
[32m[2022-08-31 18:13:20,955] [    INFO][0m - loss: 0.40874758, learning_rate: 2.7011764705882353e-05, global_step: 1270, interval_runtime: 3.5617, interval_samples_per_second: 2.246, interval_steps_per_second: 2.808, epoch: 4.9804[0m
[32m[2022-08-31 18:13:22,567] [    INFO][0m - loss: 0.44995871, learning_rate: 2.698823529411765e-05, global_step: 1280, interval_runtime: 1.6112, interval_samples_per_second: 4.965, interval_steps_per_second: 6.207, epoch: 5.0196[0m
[32m[2022-08-31 18:13:24,193] [    INFO][0m - loss: 0.21566892, learning_rate: 2.696470588235294e-05, global_step: 1290, interval_runtime: 1.6259, interval_samples_per_second: 4.92, interval_steps_per_second: 6.15, epoch: 5.0588[0m
[32m[2022-08-31 18:13:25,830] [    INFO][0m - loss: 0.17176942, learning_rate: 2.6941176470588236e-05, global_step: 1300, interval_runtime: 1.6374, interval_samples_per_second: 4.886, interval_steps_per_second: 6.107, epoch: 5.098[0m
[32m[2022-08-31 18:13:25,831] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:13:25,831] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 18:13:25,831] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:13:25,831] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:13:25,831] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-31 18:13:44,512] [    INFO][0m - eval_loss: 1.8385810852050781, eval_accuracy: 0.5991295938104448, eval_runtime: 18.6805, eval_samples_per_second: 110.704, eval_steps_per_second: 3.48, epoch: 5.098[0m
[32m[2022-08-31 18:13:44,577] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1300[0m
[32m[2022-08-31 18:13:44,577] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:13:46,367] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1300/tokenizer_config.json[0m
[32m[2022-08-31 18:13:46,367] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1300/special_tokens_map.json[0m
[32m[2022-08-31 18:13:49,400] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-31 18:13:49,400] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-900 (score: 0.6107350096711799).[0m
[32m[2022-08-31 18:13:50,315] [    INFO][0m - train_runtime: 550.2622, train_samples_per_second: 185.003, train_steps_per_second: 23.171, train_loss: 1.0559581302679502, epoch: 5.098[0m
[32m[2022-08-31 18:13:50,353] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-31 18:13:50,354] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:13:53,717] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-31 18:13:53,717] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-31 18:13:53,720] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-31 18:13:53,720] [    INFO][0m -   epoch                    =      5.098[0m
[32m[2022-08-31 18:13:53,720] [    INFO][0m -   train_loss               =      1.056[0m
[32m[2022-08-31 18:13:53,720] [    INFO][0m -   train_runtime            = 0:09:10.26[0m
[32m[2022-08-31 18:13:53,720] [    INFO][0m -   train_samples_per_second =    185.003[0m
[32m[2022-08-31 18:13:53,721] [    INFO][0m -   train_steps_per_second   =     23.171[0m
[32m[2022-08-31 18:13:53,727] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-31 18:13:53,727] [    INFO][0m -   Num examples = 1784[0m
[32m[2022-08-31 18:13:53,727] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:13:53,727] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:13:53,727] [    INFO][0m -   Total prediction steps = 56[0m
[32m[2022-08-31 18:14:09,797] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-31 18:14:09,798] [    INFO][0m -   test_accuracy           =     0.6138[0m
[32m[2022-08-31 18:14:09,798] [    INFO][0m -   test_loss               =     1.5661[0m
[32m[2022-08-31 18:14:09,798] [    INFO][0m -   test_runtime            = 0:00:16.07[0m
[32m[2022-08-31 18:14:09,798] [    INFO][0m -   test_samples_per_second =    111.011[0m
[32m[2022-08-31 18:14:09,799] [    INFO][0m -   test_steps_per_second   =      3.485[0m
[32m[2022-08-31 18:14:09,799] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-31 18:14:09,799] [    INFO][0m -   Num examples = 2999[0m
[32m[2022-08-31 18:14:09,799] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:14:09,799] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:14:09,799] [    INFO][0m -   Total prediction steps = 94[0m
[32m[2022-08-31 18:14:42,592] [    INFO][0m - Predictions for csldcp saved to ./fewclue_submit_examples.[0m
[]
run.sh: line 75: --freeze_plm: command not found
 
==========
tnews
==========
 
[33m[2022-08-31 18:14:46,792] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-31 18:14:46,792] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-31 18:14:46,793] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 18:14:46,793] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-31 18:14:46,793] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 18:14:46,793] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-31 18:14:46,793] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-31 18:14:46,793] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-08-31 18:14:46,793] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-31 18:14:46,793] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-08-31 18:14:46,793] [    INFO][0m - [0m
[32m[2022-08-31 18:14:46,793] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 18:14:46,794] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-31 18:14:46,794] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 18:14:46,794] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-31 18:14:46,794] [    INFO][0m - prompt                        :{'hard':'‰∏ãËæπÊí≠Êä•‰∏ÄÂàô'}{'mask'}{'mask'}{'hard':'Êñ∞ÈóªÔºö'}{'text':'text_a'}[0m
[32m[2022-08-31 18:14:46,794] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-31 18:14:46,794] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-31 18:14:46,794] [    INFO][0m - task_name                     :tnews[0m
[32m[2022-08-31 18:14:46,794] [    INFO][0m - [0m
[32m[2022-08-31 18:14:46,794] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0831 18:14:46.796334 22434 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0831 18:14:46.800557 22434 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-31 18:14:49,940] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-08-31 18:14:49,967] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-08-31 18:14:49,967] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[32m[2022-08-31 18:14:49,968] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‰∏ãËæπÊí≠Êä•‰∏ÄÂàô'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'Êñ∞ÈóªÔºö'}, {'add_prefix_space': '', 'text': 'text_a'}][0m
[32m[2022-08-31 18:14:49,974] [    INFO][0m - {'news_agriculture': 0, 'news_car': 1, 'news_culture': 2, 'news_edu': 3, 'news_entertainment': 4, 'news_finance': 5, 'news_game': 6, 'news_house': 7, 'news_military': 8, 'news_sports': 9, 'news_stock': 10, 'news_story': 11, 'news_tech': 12, 'news_travel': 13, 'news_world': 14}[0m
2022-08-31 18:14:49,976 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-31 18:14:51,077] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 18:14:53,078] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-31 18:14:53,078] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 18:14:53,079] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-31 18:14:53,079] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-31 18:14:53,079] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-31 18:14:53,079] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-31 18:14:53,079] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-31 18:14:53,080] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-31 18:14:53,080] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-31 18:14:53,080] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-31 18:14:53,080] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-31 18:14:53,080] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-31 18:14:53,080] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-31 18:14:53,080] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-31 18:14:53,080] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-31 18:14:53,081] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-31 18:14:53,081] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-31 18:14:53,081] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-31 18:14:53,081] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-31 18:14:53,081] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-31 18:14:53,081] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-31 18:14:53,081] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-31 18:14:53,082] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-31 18:14:53,082] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-31 18:14:53,082] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-31 18:14:53,082] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-31 18:14:53,082] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-31 18:14:53,082] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-31 18:14:53,082] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-31 18:14:53,083] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-31 18:14:53,083] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-31 18:14:53,083] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-31 18:14:53,083] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-31 18:14:53,083] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-31 18:14:53,083] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-31 18:14:53,083] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-31 18:14:53,083] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug31_18-14-46_instance-3bwob41y-01[0m
[32m[2022-08-31 18:14:53,083] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-31 18:14:53,083] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-31 18:14:53,083] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-31 18:14:53,083] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-31 18:14:53,083] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-31 18:14:53,084] [    INFO][0m - max_seq_length                :64[0m
[32m[2022-08-31 18:14:53,084] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-31 18:14:53,084] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-31 18:14:53,084] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-31 18:14:53,084] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-31 18:14:53,084] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-08-31 18:14:53,084] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-31 18:14:53,084] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-31 18:14:53,084] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-31 18:14:53,084] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-31 18:14:53,084] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-31 18:14:53,084] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-31 18:14:53,084] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-31 18:14:53,085] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-31 18:14:53,085] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-31 18:14:53,085] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-31 18:14:53,085] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-31 18:14:53,085] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-31 18:14:53,085] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-31 18:14:53,085] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-31 18:14:53,085] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-31 18:14:53,085] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-31 18:14:53,085] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-31 18:14:53,085] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-31 18:14:53,085] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-31 18:14:53,085] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-31 18:14:53,085] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-31 18:14:53,086] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-31 18:14:53,086] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-31 18:14:53,086] [    INFO][0m - seed                          :42[0m
[32m[2022-08-31 18:14:53,086] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-31 18:14:53,086] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-31 18:14:53,086] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-31 18:14:53,086] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-31 18:14:53,086] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-31 18:14:53,086] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-31 18:14:53,086] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-31 18:14:53,086] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-31 18:14:53,086] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-31 18:14:53,086] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-31 18:14:53,086] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-31 18:14:53,087] [    INFO][0m - [0m
[32m[2022-08-31 18:14:53,088] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-31 18:14:53,088] [    INFO][0m -   Num examples = 1185[0m
[32m[2022-08-31 18:14:53,089] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-08-31 18:14:53,089] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-31 18:14:53,089] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-31 18:14:53,089] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-31 18:14:53,089] [    INFO][0m -   Total optimization steps = 7450.0[0m
[32m[2022-08-31 18:14:53,089] [    INFO][0m -   Total num train samples = 59250[0m
[32m[2022-08-31 18:14:54,756] [    INFO][0m - loss: 2.61659317, learning_rate: 2.9959731543624162e-05, global_step: 10, interval_runtime: 1.6662, interval_samples_per_second: 4.801, interval_steps_per_second: 6.002, epoch: 0.0671[0m
[32m[2022-08-31 18:14:55,386] [    INFO][0m - loss: 2.1084547, learning_rate: 2.9919463087248323e-05, global_step: 20, interval_runtime: 0.6298, interval_samples_per_second: 12.703, interval_steps_per_second: 15.879, epoch: 0.1342[0m
[32m[2022-08-31 18:14:56,023] [    INFO][0m - loss: 1.7481636, learning_rate: 2.9879194630872484e-05, global_step: 30, interval_runtime: 0.6364, interval_samples_per_second: 12.57, interval_steps_per_second: 15.713, epoch: 0.2013[0m
[32m[2022-08-31 18:14:56,637] [    INFO][0m - loss: 1.82070885, learning_rate: 2.9838926174496645e-05, global_step: 40, interval_runtime: 0.6152, interval_samples_per_second: 13.003, interval_steps_per_second: 16.254, epoch: 0.2685[0m
[32m[2022-08-31 18:14:57,280] [    INFO][0m - loss: 1.65231819, learning_rate: 2.9798657718120806e-05, global_step: 50, interval_runtime: 0.643, interval_samples_per_second: 12.441, interval_steps_per_second: 15.551, epoch: 0.3356[0m
[32m[2022-08-31 18:14:57,979] [    INFO][0m - loss: 1.24393978, learning_rate: 2.9758389261744967e-05, global_step: 60, interval_runtime: 0.698, interval_samples_per_second: 11.461, interval_steps_per_second: 14.326, epoch: 0.4027[0m
[32m[2022-08-31 18:14:58,592] [    INFO][0m - loss: 1.47449369, learning_rate: 2.9718120805369125e-05, global_step: 70, interval_runtime: 0.6138, interval_samples_per_second: 13.033, interval_steps_per_second: 16.291, epoch: 0.4698[0m
[32m[2022-08-31 18:14:59,243] [    INFO][0m - loss: 1.58359718, learning_rate: 2.967785234899329e-05, global_step: 80, interval_runtime: 0.6504, interval_samples_per_second: 12.299, interval_steps_per_second: 15.374, epoch: 0.5369[0m
[32m[2022-08-31 18:14:59,859] [    INFO][0m - loss: 1.82011147, learning_rate: 2.963758389261745e-05, global_step: 90, interval_runtime: 0.6162, interval_samples_per_second: 12.983, interval_steps_per_second: 16.228, epoch: 0.604[0m
[32m[2022-08-31 18:15:00,465] [    INFO][0m - loss: 1.40423746, learning_rate: 2.9597315436241612e-05, global_step: 100, interval_runtime: 0.6055, interval_samples_per_second: 13.213, interval_steps_per_second: 16.516, epoch: 0.6711[0m
[32m[2022-08-31 18:15:00,466] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:15:00,466] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-08-31 18:15:00,466] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:15:00,466] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:15:00,466] [    INFO][0m -   Total prediction steps = 35[0m
[32m[2022-08-31 18:15:03,165] [    INFO][0m - eval_loss: 1.412941813468933, eval_accuracy: 0.5646630236794171, eval_runtime: 2.6985, eval_samples_per_second: 406.893, eval_steps_per_second: 12.97, epoch: 0.6711[0m
[32m[2022-08-31 18:15:03,166] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-31 18:15:03,166] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:15:06,944] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-31 18:15:06,945] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-31 18:15:13,579] [    INFO][0m - loss: 1.57439871, learning_rate: 2.9557046979865773e-05, global_step: 110, interval_runtime: 13.1139, interval_samples_per_second: 0.61, interval_steps_per_second: 0.763, epoch: 0.7383[0m
[32m[2022-08-31 18:15:14,193] [    INFO][0m - loss: 1.43729649, learning_rate: 2.951677852348993e-05, global_step: 120, interval_runtime: 0.6143, interval_samples_per_second: 13.024, interval_steps_per_second: 16.28, epoch: 0.8054[0m
[32m[2022-08-31 18:15:14,808] [    INFO][0m - loss: 1.66100388, learning_rate: 2.9476510067114095e-05, global_step: 130, interval_runtime: 0.615, interval_samples_per_second: 13.008, interval_steps_per_second: 16.26, epoch: 0.8725[0m
[32m[2022-08-31 18:15:15,524] [    INFO][0m - loss: 1.86075325, learning_rate: 2.9436241610738256e-05, global_step: 140, interval_runtime: 0.7162, interval_samples_per_second: 11.17, interval_steps_per_second: 13.963, epoch: 0.9396[0m
[32m[2022-08-31 18:15:16,210] [    INFO][0m - loss: 1.43051033, learning_rate: 2.9395973154362418e-05, global_step: 150, interval_runtime: 0.6857, interval_samples_per_second: 11.666, interval_steps_per_second: 14.583, epoch: 1.0067[0m
[32m[2022-08-31 18:15:16,823] [    INFO][0m - loss: 1.13347187, learning_rate: 2.935570469798658e-05, global_step: 160, interval_runtime: 0.6131, interval_samples_per_second: 13.048, interval_steps_per_second: 16.311, epoch: 1.0738[0m
[32m[2022-08-31 18:15:17,524] [    INFO][0m - loss: 1.05009823, learning_rate: 2.9315436241610736e-05, global_step: 170, interval_runtime: 0.7011, interval_samples_per_second: 11.41, interval_steps_per_second: 14.262, epoch: 1.1409[0m
[32m[2022-08-31 18:15:18,133] [    INFO][0m - loss: 1.05951118, learning_rate: 2.92751677852349e-05, global_step: 180, interval_runtime: 0.6089, interval_samples_per_second: 13.138, interval_steps_per_second: 16.423, epoch: 1.2081[0m
[32m[2022-08-31 18:15:18,767] [    INFO][0m - loss: 1.35783777, learning_rate: 2.9234899328859062e-05, global_step: 190, interval_runtime: 0.6347, interval_samples_per_second: 12.605, interval_steps_per_second: 15.756, epoch: 1.2752[0m
[32m[2022-08-31 18:15:19,387] [    INFO][0m - loss: 0.92444487, learning_rate: 2.9194630872483223e-05, global_step: 200, interval_runtime: 0.6197, interval_samples_per_second: 12.91, interval_steps_per_second: 16.138, epoch: 1.3423[0m
[32m[2022-08-31 18:15:19,388] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:15:19,388] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-08-31 18:15:19,388] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:15:19,388] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:15:19,388] [    INFO][0m -   Total prediction steps = 35[0m
[32m[2022-08-31 18:15:22,081] [    INFO][0m - eval_loss: 1.60214102268219, eval_accuracy: 0.5473588342440802, eval_runtime: 2.692, eval_samples_per_second: 407.872, eval_steps_per_second: 13.001, epoch: 1.3423[0m
[32m[2022-08-31 18:15:22,081] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-31 18:15:22,082] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:15:27,919] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-31 18:15:27,919] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-31 18:15:34,112] [    INFO][0m - loss: 1.43063154, learning_rate: 2.915436241610738e-05, global_step: 210, interval_runtime: 14.7248, interval_samples_per_second: 0.543, interval_steps_per_second: 0.679, epoch: 1.4094[0m
[32m[2022-08-31 18:15:34,714] [    INFO][0m - loss: 1.1433568, learning_rate: 2.9114093959731542e-05, global_step: 220, interval_runtime: 0.602, interval_samples_per_second: 13.29, interval_steps_per_second: 16.613, epoch: 1.4765[0m
[32m[2022-08-31 18:15:35,314] [    INFO][0m - loss: 1.08994465, learning_rate: 2.9073825503355706e-05, global_step: 230, interval_runtime: 0.6004, interval_samples_per_second: 13.324, interval_steps_per_second: 16.655, epoch: 1.5436[0m
[32m[2022-08-31 18:15:35,976] [    INFO][0m - loss: 1.27302666, learning_rate: 2.9033557046979868e-05, global_step: 240, interval_runtime: 0.6621, interval_samples_per_second: 12.083, interval_steps_per_second: 15.104, epoch: 1.6107[0m
[32m[2022-08-31 18:15:36,586] [    INFO][0m - loss: 1.0912714, learning_rate: 2.899328859060403e-05, global_step: 250, interval_runtime: 0.6097, interval_samples_per_second: 13.12, interval_steps_per_second: 16.401, epoch: 1.6779[0m
[32m[2022-08-31 18:15:37,186] [    INFO][0m - loss: 1.22866631, learning_rate: 2.8953020134228186e-05, global_step: 260, interval_runtime: 0.5995, interval_samples_per_second: 13.345, interval_steps_per_second: 16.681, epoch: 1.745[0m
[32m[2022-08-31 18:15:37,818] [    INFO][0m - loss: 1.08874083, learning_rate: 2.891275167785235e-05, global_step: 270, interval_runtime: 0.6322, interval_samples_per_second: 12.654, interval_steps_per_second: 15.818, epoch: 1.8121[0m
[32m[2022-08-31 18:15:38,468] [    INFO][0m - loss: 1.05141659, learning_rate: 2.8872483221476512e-05, global_step: 280, interval_runtime: 0.6505, interval_samples_per_second: 12.299, interval_steps_per_second: 15.374, epoch: 1.8792[0m
[32m[2022-08-31 18:15:39,078] [    INFO][0m - loss: 1.00838051, learning_rate: 2.8832214765100673e-05, global_step: 290, interval_runtime: 0.5999, interval_samples_per_second: 13.336, interval_steps_per_second: 16.67, epoch: 1.9463[0m
[32m[2022-08-31 18:15:39,702] [    INFO][0m - loss: 1.04156551, learning_rate: 2.879194630872483e-05, global_step: 300, interval_runtime: 0.6335, interval_samples_per_second: 12.627, interval_steps_per_second: 15.784, epoch: 2.0134[0m
[32m[2022-08-31 18:15:39,702] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:15:39,702] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-08-31 18:15:39,702] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:15:39,702] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:15:39,702] [    INFO][0m -   Total prediction steps = 35[0m
[32m[2022-08-31 18:15:42,394] [    INFO][0m - eval_loss: 1.5352082252502441, eval_accuracy: 0.5628415300546448, eval_runtime: 2.6912, eval_samples_per_second: 407.997, eval_steps_per_second: 13.005, epoch: 2.0134[0m
[32m[2022-08-31 18:15:42,395] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-31 18:15:42,395] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:15:45,510] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-31 18:15:45,510] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-31 18:15:51,401] [    INFO][0m - loss: 0.63973846, learning_rate: 2.8751677852348992e-05, global_step: 310, interval_runtime: 11.6992, interval_samples_per_second: 0.684, interval_steps_per_second: 0.855, epoch: 2.0805[0m
[32m[2022-08-31 18:15:52,045] [    INFO][0m - loss: 0.80069427, learning_rate: 2.8711409395973157e-05, global_step: 320, interval_runtime: 0.6445, interval_samples_per_second: 12.413, interval_steps_per_second: 15.516, epoch: 2.1477[0m
[32m[2022-08-31 18:15:52,730] [    INFO][0m - loss: 0.62775831, learning_rate: 2.8671140939597318e-05, global_step: 330, interval_runtime: 0.6847, interval_samples_per_second: 11.684, interval_steps_per_second: 14.605, epoch: 2.2148[0m
[32m[2022-08-31 18:15:53,375] [    INFO][0m - loss: 0.94150152, learning_rate: 2.863087248322148e-05, global_step: 340, interval_runtime: 0.6451, interval_samples_per_second: 12.4, interval_steps_per_second: 15.501, epoch: 2.2819[0m
[32m[2022-08-31 18:15:54,000] [    INFO][0m - loss: 0.75268254, learning_rate: 2.8590604026845637e-05, global_step: 350, interval_runtime: 0.6251, interval_samples_per_second: 12.798, interval_steps_per_second: 15.997, epoch: 2.349[0m
[32m[2022-08-31 18:15:54,680] [    INFO][0m - loss: 0.62247286, learning_rate: 2.8550335570469798e-05, global_step: 360, interval_runtime: 0.6795, interval_samples_per_second: 11.773, interval_steps_per_second: 14.716, epoch: 2.4161[0m
[32m[2022-08-31 18:15:55,336] [    INFO][0m - loss: 0.65126348, learning_rate: 2.8510067114093962e-05, global_step: 370, interval_runtime: 0.6561, interval_samples_per_second: 12.193, interval_steps_per_second: 15.241, epoch: 2.4832[0m
[32m[2022-08-31 18:15:55,961] [    INFO][0m - loss: 0.80412035, learning_rate: 2.8469798657718123e-05, global_step: 380, interval_runtime: 0.6252, interval_samples_per_second: 12.796, interval_steps_per_second: 15.995, epoch: 2.5503[0m
[32m[2022-08-31 18:15:56,619] [    INFO][0m - loss: 0.73329601, learning_rate: 2.8429530201342284e-05, global_step: 390, interval_runtime: 0.6576, interval_samples_per_second: 12.166, interval_steps_per_second: 15.207, epoch: 2.6174[0m
[32m[2022-08-31 18:15:57,268] [    INFO][0m - loss: 0.8434742, learning_rate: 2.8389261744966442e-05, global_step: 400, interval_runtime: 0.6487, interval_samples_per_second: 12.333, interval_steps_per_second: 15.416, epoch: 2.6846[0m
[32m[2022-08-31 18:15:57,268] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:15:57,268] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-08-31 18:15:57,268] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:15:57,268] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:15:57,269] [    INFO][0m -   Total prediction steps = 35[0m
[32m[2022-08-31 18:15:59,979] [    INFO][0m - eval_loss: 1.8080772161483765, eval_accuracy: 0.5255009107468124, eval_runtime: 2.71, eval_samples_per_second: 405.165, eval_steps_per_second: 12.915, epoch: 2.6846[0m
[32m[2022-08-31 18:15:59,980] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-31 18:15:59,980] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:16:03,174] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-31 18:16:03,175] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-31 18:16:09,426] [    INFO][0m - loss: 0.68694186, learning_rate: 2.8348993288590603e-05, global_step: 410, interval_runtime: 12.1585, interval_samples_per_second: 0.658, interval_steps_per_second: 0.822, epoch: 2.7517[0m
[32m[2022-08-31 18:16:10,065] [    INFO][0m - loss: 0.76685081, learning_rate: 2.8308724832214768e-05, global_step: 420, interval_runtime: 0.6387, interval_samples_per_second: 12.526, interval_steps_per_second: 15.658, epoch: 2.8188[0m
[32m[2022-08-31 18:16:10,712] [    INFO][0m - loss: 0.98560591, learning_rate: 2.826845637583893e-05, global_step: 430, interval_runtime: 0.6476, interval_samples_per_second: 12.353, interval_steps_per_second: 15.442, epoch: 2.8859[0m
[32m[2022-08-31 18:16:11,335] [    INFO][0m - loss: 0.83666935, learning_rate: 2.8228187919463087e-05, global_step: 440, interval_runtime: 0.6226, interval_samples_per_second: 12.85, interval_steps_per_second: 16.063, epoch: 2.953[0m
[32m[2022-08-31 18:16:11,997] [    INFO][0m - loss: 0.66528077, learning_rate: 2.8187919463087248e-05, global_step: 450, interval_runtime: 0.6618, interval_samples_per_second: 12.088, interval_steps_per_second: 15.11, epoch: 3.0201[0m
[32m[2022-08-31 18:16:12,607] [    INFO][0m - loss: 0.34242191, learning_rate: 2.814765100671141e-05, global_step: 460, interval_runtime: 0.61, interval_samples_per_second: 13.116, interval_steps_per_second: 16.395, epoch: 3.0872[0m
[32m[2022-08-31 18:16:13,221] [    INFO][0m - loss: 0.32147427, learning_rate: 2.8107382550335573e-05, global_step: 470, interval_runtime: 0.6148, interval_samples_per_second: 13.012, interval_steps_per_second: 16.264, epoch: 3.1544[0m
[32m[2022-08-31 18:16:13,893] [    INFO][0m - loss: 0.25956862, learning_rate: 2.8067114093959734e-05, global_step: 480, interval_runtime: 0.6714, interval_samples_per_second: 11.915, interval_steps_per_second: 14.893, epoch: 3.2215[0m
[32m[2022-08-31 18:16:14,571] [    INFO][0m - loss: 0.44135866, learning_rate: 2.8026845637583892e-05, global_step: 490, interval_runtime: 0.6783, interval_samples_per_second: 11.794, interval_steps_per_second: 14.742, epoch: 3.2886[0m
[32m[2022-08-31 18:16:15,175] [    INFO][0m - loss: 0.43095632, learning_rate: 2.7986577181208053e-05, global_step: 500, interval_runtime: 0.6034, interval_samples_per_second: 13.259, interval_steps_per_second: 16.573, epoch: 3.3557[0m
[32m[2022-08-31 18:16:15,175] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:16:15,175] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-08-31 18:16:15,175] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:16:15,175] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:16:15,175] [    INFO][0m -   Total prediction steps = 35[0m
[32m[2022-08-31 18:16:17,877] [    INFO][0m - eval_loss: 2.1055824756622314, eval_accuracy: 0.5437158469945356, eval_runtime: 2.7016, eval_samples_per_second: 406.431, eval_steps_per_second: 12.955, epoch: 3.3557[0m
[32m[2022-08-31 18:16:17,877] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-08-31 18:16:17,878] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:16:21,251] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-08-31 18:16:21,252] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-08-31 18:16:27,515] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-31 18:16:27,516] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-100 (score: 0.5646630236794171).[0m
[32m[2022-08-31 18:16:28,583] [    INFO][0m - train_runtime: 95.4934, train_samples_per_second: 620.462, train_steps_per_second: 78.016, train_loss: 1.1112615189552306, epoch: 3.3557[0m
[32m[2022-08-31 18:16:28,623] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-31 18:16:28,623] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:16:31,762] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-31 18:16:31,762] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-31 18:16:31,763] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-31 18:16:31,763] [    INFO][0m -   epoch                    =     3.3557[0m
[32m[2022-08-31 18:16:31,763] [    INFO][0m -   train_loss               =     1.1113[0m
[32m[2022-08-31 18:16:31,763] [    INFO][0m -   train_runtime            = 0:01:35.49[0m
[32m[2022-08-31 18:16:31,764] [    INFO][0m -   train_samples_per_second =    620.462[0m
[32m[2022-08-31 18:16:31,764] [    INFO][0m -   train_steps_per_second   =     78.016[0m
[32m[2022-08-31 18:16:31,767] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-31 18:16:31,767] [    INFO][0m -   Num examples = 2010[0m
[32m[2022-08-31 18:16:31,767] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:16:31,767] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:16:31,767] [    INFO][0m -   Total prediction steps = 63[0m
[32m[2022-08-31 18:16:36,735] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-31 18:16:36,736] [    INFO][0m -   test_accuracy           =     0.5736[0m
[32m[2022-08-31 18:16:36,736] [    INFO][0m -   test_loss               =     1.3745[0m
[32m[2022-08-31 18:16:36,736] [    INFO][0m -   test_runtime            = 0:00:04.96[0m
[32m[2022-08-31 18:16:36,736] [    INFO][0m -   test_samples_per_second =    404.555[0m
[32m[2022-08-31 18:16:36,736] [    INFO][0m -   test_steps_per_second   =      12.68[0m
[32m[2022-08-31 18:16:36,737] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-31 18:16:36,737] [    INFO][0m -   Num examples = 1500[0m
[32m[2022-08-31 18:16:36,737] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:16:36,737] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:16:36,737] [    INFO][0m -   Total prediction steps = 47[0m
[32m[2022-08-31 18:16:41,385] [    INFO][0m - Predictions for tnewsf saved to ./fewclue_submit_examples.[0m
[]
run.sh: line 75: --freeze_plm: command not found
 
==========
iflytek
==========
 
[33m[2022-08-31 18:16:45,917] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-31 18:16:45,917] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-31 18:16:45,918] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 18:16:45,918] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-31 18:16:45,918] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 18:16:45,918] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-31 18:16:45,918] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-31 18:16:45,918] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-08-31 18:16:45,918] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-31 18:16:45,918] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-08-31 18:16:45,918] [    INFO][0m - [0m
[32m[2022-08-31 18:16:45,918] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 18:16:45,918] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-31 18:16:45,918] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 18:16:45,919] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-31 18:16:45,919] [    INFO][0m - prompt                        :{'text':'text_a'}{'hard':'ËøôÊ¨æÂ∫îÁî®ÊòØ'}{'mask'}{'mask'}{'hard':'Á±ªÂûãÁöÑ„ÄÇ'}[0m
[32m[2022-08-31 18:16:45,919] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-31 18:16:45,919] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-31 18:16:45,919] [    INFO][0m - task_name                     :iflytek[0m
[32m[2022-08-31 18:16:45,919] [    INFO][0m - [0m
[32m[2022-08-31 18:16:45,919] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0831 18:16:45.920925 30108 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0831 18:16:45.925112 30108 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-31 18:16:48,891] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-08-31 18:16:48,920] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-08-31 18:16:48,921] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[32m[2022-08-31 18:16:48,922] [    INFO][0m - Using template: [{'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': 'ËøôÊ¨æÂ∫îÁî®ÊòØ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'Á±ªÂûãÁöÑ„ÄÇ'}][0m
[32m[2022-08-31 18:16:48,958] [    INFO][0m - {'KÊ≠å': 0, 'MOBA': 1, '‰∏≠Â∞èÂ≠¶': 2, '‰π∞Êàø': 3, '‰∫åÊâã': 4, '‰∫≤Â≠êÂÑøÁ´•': 5, '‰ªô‰æ†': 6, '‰ºëÈó≤ÁõäÊô∫': 7, '‰ΩìËÇ≤Âí®ËÆØ': 8, '‰ΩìËÇ≤Á´ûÊäÄ': 9, '‰øùÈô©': 10, 'ÂÄüË¥∑': 11, 'ÂÖçË¥πWIFI': 12, 'ÂÖ¨ÂÖ±‰∫§ÈÄö': 13, 'ÂÖ¨Âä°Âëò': 14, 'ÂÖ∂‰ªñ': 15, 'ÂÖªÁîü‰øùÂÅ•': 16, 'ÂÖºËÅå': 17, 'ÂáèËÇ•Áò¶Ë∫´': 18, 'Âá∫ÂõΩ': 19, 'ÂäûÂÖ¨': 20, 'Âä®‰ΩúÁ±ª': 21, 'ÂåªÁñóÊúçÂä°': 22, 'Âç°Áâå': 23, 'Âç≥Êó∂ÈÄöËÆØ': 24, 'ÂêåÂüéÊúçÂä°': 25, 'Âõ¢Ë¥≠': 26, 'Âú∞ÂõæÂØºËà™': 27, 'Â§ñÂçñ': 28, 'Â•≥ÊÄß': 29, 'Â©öÂ∫Ü': 30, 'Â©öÊÅãÁ§æ‰∫§': 31, 'ÂÆ∂Êîø': 32, 'Â∞ÑÂáªÊ∏∏Êàè': 33, 'Â∞èËØ¥': 34, 'Â∑•‰ΩúÁ§æ‰∫§': 35, 'Â∑•ÂÖ∑': 36, 'ÂΩ©Á•®': 37, 'ÂΩ±ÂÉèÂâ™Ëæë': 38, 'ÂΩ±ËßÜÂ®±‰πê': 39, 'ÂæÆÂçöÂçöÂÆ¢': 40, 'Âø´ÈÄíÁâ©ÊµÅ': 41, 'ÊÉÖ‰æ£Á§æ‰∫§': 42, 'Êàê‰∫∫': 43, 'Êàê‰∫∫ÊïôËÇ≤': 44, 'ÊâìËΩ¶': 45, 'ÊäÄÊúØ': 46, 'ÊêûÁ¨ë': 47, 'ÊëÑÂΩ±‰øÆÂõæ': 48, 'ÊîØ‰ªò': 49, 'Êî∂Ê¨æ': 50, 'ÊîøÂä°': 51, 'ÊïôËæÖ': 52, 'Êñ∞Èóª': 53, 'ÊóÖÊ∏∏ËµÑËÆØ': 54, 'Êó•Â∏∏ÂÖªËΩ¶': 55, 'Êó•Á®ãÁÆ°ÁêÜ': 56, 'ÊùÇÂøó': 57, 'Ê£ãÁâå‰∏≠ÂøÉ': 58, 'ÊØçÂ©¥': 59, 'Ê∞ëÂÆøÁü≠Áßü': 60, 'Ê∞ëËà™': 61, 'Ê±ÇËÅå': 62, 'Ê±ΩËΩ¶‰∫§Êòì': 63, 'Ê±ΩËΩ¶Âí®ËØ¢': 64, 'Êº´Áîª': 65, 'ÁêÜË¥¢': 66, 'ÁîüÊ¥ªÁ§æ‰∫§': 67, 'ÁîµÂè∞': 68, 'ÁîµÂïÜ': 69, 'ÁîµÂ≠ê‰∫ßÂìÅ': 70, 'ÁîµÂΩ±Á•®Âä°': 71, 'ÁôæÁßë': 72, 'Áõ¥Êí≠': 73, 'Áõ∏Êú∫': 74, 'Áü≠ËßÜÈ¢ë': 75, 'Á§æ‰∫§Â∑•ÂÖ∑': 76, 'Á§æÂå∫ÊúçÂä°': 77, 'Á§æÂå∫Ë∂ÖÂ∏Ç': 78, 'ÁßüÊàø': 79, 'ÁßüËΩ¶': 80, 'Á¨îËÆ∞': 81, 'Á≠ñÁï•': 82, 'Á∫¶‰ºöÁ§æ‰∫§': 83, 'ÁªèËê•': 84, 'ÁªèËê•ÂÖªÊàê': 85, 'ÁªòÁîª': 86, 'ÁªºÂêàÈ¢ÑÂÆö': 87, 'ÁæéÂ¶ÜÁæé‰∏ö': 88, 'ÁæéÈ¢ú': 89, 'ËÅåËÄÉ': 90, 'ËÇ°Á•®': 91, 'Ëâ∫ÊúØ': 92, 'Ëã±ËØ≠': 93, 'ËèúË∞±': 94, 'ËñÖÁæäÊØõ': 95, 'Ë°åÁ®ãÁÆ°ÁêÜ': 96, 'Ë°åËΩ¶ËæÖÂä©': 97, 'Ë£Ö‰øÆÂÆ∂Â±Ö': 98, 'ËßÜÈ¢ë': 99, 'ËßÜÈ¢ëÊïôËÇ≤': 100, 'ËÆ∞Ë¥¶': 101, 'ËÆ∫ÂùõÂúàÂ≠ê': 102, 'ËØ≠Ë®Ä(ÈùûËã±ËØ≠)': 103, 'Ë¥≠Áâ©Âí®ËØ¢': 104, 'ËæÖÂä©Â∑•ÂÖ∑': 105, 'ËøêÂä®ÂÅ•Ë∫´': 106, 'ËøùÁ´†': 107, 'ÈÖíÂ∫ó': 108, 'ÈìÅË∑Ø': 109, 'Èì∂Ë°å': 110, 'ÈóÆÁ≠î‰∫§ÊµÅ': 111, 'ÈóÆËØäÊåÇÂè∑': 112, 'Èü≥‰πê': 113, 'È£ûË°åÁ©∫Êàò': 114, 'È§êÈ•ÆÂ∫ó': 115, 'È©æÊ†°': 116, 'È´òÁ≠âÊïôËÇ≤': 117, 'È≠îÂπª': 118}[0m
2022-08-31 18:16:48,960 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-31 18:16:50,052] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 18:16:50,053] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-31 18:16:50,053] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 18:16:50,053] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-31 18:16:50,053] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-31 18:16:50,053] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-31 18:16:50,053] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-31 18:16:50,053] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-31 18:16:50,053] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-31 18:16:50,053] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-31 18:16:50,054] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-31 18:16:50,054] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-31 18:16:50,054] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-31 18:16:50,054] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-31 18:16:50,054] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-31 18:16:50,054] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-31 18:16:50,054] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-31 18:16:50,054] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-31 18:16:50,054] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-31 18:16:50,054] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-31 18:16:50,054] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-31 18:16:50,054] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-31 18:16:50,054] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-31 18:16:50,055] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-31 18:16:50,055] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-31 18:16:50,055] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-31 18:16:50,055] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-31 18:16:50,055] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-31 18:16:50,055] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-31 18:16:50,055] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-31 18:16:50,055] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-31 18:16:50,055] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-31 18:16:50,055] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-31 18:16:50,055] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-31 18:16:50,055] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-31 18:16:50,055] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-31 18:16:50,055] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-31 18:16:50,056] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug31_18-16-45_instance-3bwob41y-01[0m
[32m[2022-08-31 18:16:50,056] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-31 18:16:50,056] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-31 18:16:50,056] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-31 18:16:50,056] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-31 18:16:50,056] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-31 18:16:50,056] [    INFO][0m - max_seq_length                :320[0m
[32m[2022-08-31 18:16:50,056] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-31 18:16:50,056] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-31 18:16:50,056] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-31 18:16:50,056] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-31 18:16:50,056] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-08-31 18:16:50,056] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-31 18:16:50,057] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-31 18:16:50,057] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-31 18:16:50,057] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-31 18:16:50,057] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-31 18:16:50,057] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-31 18:16:50,057] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-31 18:16:50,057] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-31 18:16:50,057] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-31 18:16:50,057] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-31 18:16:50,057] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-31 18:16:50,057] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-31 18:16:50,057] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-31 18:16:50,057] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-31 18:16:50,058] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-31 18:16:50,058] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-31 18:16:50,058] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-31 18:16:50,058] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-31 18:16:50,058] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-31 18:16:50,058] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-31 18:16:50,058] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-31 18:16:50,058] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-31 18:16:50,058] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-31 18:16:50,058] [    INFO][0m - seed                          :42[0m
[32m[2022-08-31 18:16:50,058] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-31 18:16:50,059] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-31 18:16:50,059] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-31 18:16:50,059] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-31 18:16:50,059] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-31 18:16:50,059] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-31 18:16:50,059] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-31 18:16:50,059] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-31 18:16:50,060] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-31 18:16:50,060] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-31 18:16:50,060] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-31 18:16:50,060] [    INFO][0m - [0m
[32m[2022-08-31 18:16:50,063] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-31 18:16:50,063] [    INFO][0m -   Num examples = 3024[0m
[32m[2022-08-31 18:16:50,063] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-08-31 18:16:50,063] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-31 18:16:50,064] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-31 18:16:50,064] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-31 18:16:50,064] [    INFO][0m -   Total optimization steps = 18900.0[0m
[32m[2022-08-31 18:16:50,064] [    INFO][0m -   Total num train samples = 151200[0m
[32m[2022-08-31 18:16:53,211] [    INFO][0m - loss: 5.18136711, learning_rate: 2.9984126984126986e-05, global_step: 10, interval_runtime: 3.1458, interval_samples_per_second: 2.543, interval_steps_per_second: 3.179, epoch: 0.0265[0m
[32m[2022-08-31 18:16:55,156] [    INFO][0m - loss: 3.94135437, learning_rate: 2.9968253968253967e-05, global_step: 20, interval_runtime: 1.9452, interval_samples_per_second: 4.113, interval_steps_per_second: 5.141, epoch: 0.0529[0m
[32m[2022-08-31 18:16:57,116] [    INFO][0m - loss: 3.54083099, learning_rate: 2.9952380952380952e-05, global_step: 30, interval_runtime: 1.9604, interval_samples_per_second: 4.081, interval_steps_per_second: 5.101, epoch: 0.0794[0m
[32m[2022-08-31 18:16:59,071] [    INFO][0m - loss: 2.70018425, learning_rate: 2.9936507936507937e-05, global_step: 40, interval_runtime: 1.9553, interval_samples_per_second: 4.091, interval_steps_per_second: 5.114, epoch: 0.1058[0m
[32m[2022-08-31 18:17:01,052] [    INFO][0m - loss: 2.7392025, learning_rate: 2.992063492063492e-05, global_step: 50, interval_runtime: 1.9796, interval_samples_per_second: 4.041, interval_steps_per_second: 5.052, epoch: 0.1323[0m
[32m[2022-08-31 18:17:03,008] [    INFO][0m - loss: 2.79857616, learning_rate: 2.9904761904761907e-05, global_step: 60, interval_runtime: 1.9571, interval_samples_per_second: 4.088, interval_steps_per_second: 5.11, epoch: 0.1587[0m
[32m[2022-08-31 18:17:04,971] [    INFO][0m - loss: 2.96104279, learning_rate: 2.9888888888888892e-05, global_step: 70, interval_runtime: 1.963, interval_samples_per_second: 4.075, interval_steps_per_second: 5.094, epoch: 0.1852[0m
[32m[2022-08-31 18:17:06,921] [    INFO][0m - loss: 2.48807621, learning_rate: 2.9873015873015874e-05, global_step: 80, interval_runtime: 1.9493, interval_samples_per_second: 4.104, interval_steps_per_second: 5.13, epoch: 0.2116[0m
[32m[2022-08-31 18:17:08,870] [    INFO][0m - loss: 2.26288891, learning_rate: 2.985714285714286e-05, global_step: 90, interval_runtime: 1.9492, interval_samples_per_second: 4.104, interval_steps_per_second: 5.13, epoch: 0.2381[0m
[32m[2022-08-31 18:17:10,823] [    INFO][0m - loss: 2.28507004, learning_rate: 2.984126984126984e-05, global_step: 100, interval_runtime: 1.9528, interval_samples_per_second: 4.097, interval_steps_per_second: 5.121, epoch: 0.2646[0m
[32m[2022-08-31 18:17:10,823] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:17:10,823] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 18:17:10,824] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:17:10,824] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:17:10,824] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 18:17:26,400] [    INFO][0m - eval_loss: 2.2499265670776367, eval_accuracy: 0.45229424617625635, eval_runtime: 15.5061, eval_samples_per_second: 88.546, eval_steps_per_second: 2.773, epoch: 0.2646[0m
[32m[2022-08-31 18:17:26,401] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-31 18:17:26,401] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:17:29,719] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-31 18:17:29,720] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-31 18:17:37,667] [    INFO][0m - loss: 2.1718874, learning_rate: 2.9825396825396825e-05, global_step: 110, interval_runtime: 26.8442, interval_samples_per_second: 0.298, interval_steps_per_second: 0.373, epoch: 0.291[0m
[32m[2022-08-31 18:17:39,626] [    INFO][0m - loss: 2.21932983, learning_rate: 2.980952380952381e-05, global_step: 120, interval_runtime: 1.9594, interval_samples_per_second: 4.083, interval_steps_per_second: 5.104, epoch: 0.3175[0m
[32m[2022-08-31 18:17:41,571] [    INFO][0m - loss: 2.34291077, learning_rate: 2.9793650793650792e-05, global_step: 130, interval_runtime: 1.9454, interval_samples_per_second: 4.112, interval_steps_per_second: 5.14, epoch: 0.3439[0m
[32m[2022-08-31 18:17:43,529] [    INFO][0m - loss: 2.23509331, learning_rate: 2.9777777777777777e-05, global_step: 140, interval_runtime: 1.9575, interval_samples_per_second: 4.087, interval_steps_per_second: 5.109, epoch: 0.3704[0m
[32m[2022-08-31 18:17:45,478] [    INFO][0m - loss: 2.48036041, learning_rate: 2.9761904761904762e-05, global_step: 150, interval_runtime: 1.9492, interval_samples_per_second: 4.104, interval_steps_per_second: 5.13, epoch: 0.3968[0m
[32m[2022-08-31 18:17:47,427] [    INFO][0m - loss: 2.39364834, learning_rate: 2.9746031746031747e-05, global_step: 160, interval_runtime: 1.9487, interval_samples_per_second: 4.105, interval_steps_per_second: 5.132, epoch: 0.4233[0m
[32m[2022-08-31 18:17:49,380] [    INFO][0m - loss: 2.23843632, learning_rate: 2.9730158730158732e-05, global_step: 170, interval_runtime: 1.9536, interval_samples_per_second: 4.095, interval_steps_per_second: 5.119, epoch: 0.4497[0m
[32m[2022-08-31 18:17:51,341] [    INFO][0m - loss: 2.55195312, learning_rate: 2.9714285714285717e-05, global_step: 180, interval_runtime: 1.9603, interval_samples_per_second: 4.081, interval_steps_per_second: 5.101, epoch: 0.4762[0m
[32m[2022-08-31 18:17:53,303] [    INFO][0m - loss: 2.0915472, learning_rate: 2.96984126984127e-05, global_step: 190, interval_runtime: 1.9624, interval_samples_per_second: 4.077, interval_steps_per_second: 5.096, epoch: 0.5026[0m
[32m[2022-08-31 18:17:55,262] [    INFO][0m - loss: 2.10553207, learning_rate: 2.9682539682539683e-05, global_step: 200, interval_runtime: 1.9582, interval_samples_per_second: 4.085, interval_steps_per_second: 5.107, epoch: 0.5291[0m
[32m[2022-08-31 18:17:55,262] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:17:55,262] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 18:17:55,262] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:17:55,263] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:17:55,263] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 18:18:10,698] [    INFO][0m - eval_loss: 2.072263479232788, eval_accuracy: 0.4690458849235251, eval_runtime: 15.4347, eval_samples_per_second: 88.956, eval_steps_per_second: 2.786, epoch: 0.5291[0m
[32m[2022-08-31 18:18:10,698] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-31 18:18:10,698] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:18:13,998] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-31 18:18:13,999] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-31 18:18:21,833] [    INFO][0m - loss: 2.15310173, learning_rate: 2.966666666666667e-05, global_step: 210, interval_runtime: 26.5717, interval_samples_per_second: 0.301, interval_steps_per_second: 0.376, epoch: 0.5556[0m
[32m[2022-08-31 18:18:23,791] [    INFO][0m - loss: 2.17940025, learning_rate: 2.965079365079365e-05, global_step: 220, interval_runtime: 1.9579, interval_samples_per_second: 4.086, interval_steps_per_second: 5.107, epoch: 0.582[0m
[32m[2022-08-31 18:18:25,751] [    INFO][0m - loss: 2.04140015, learning_rate: 2.9634920634920635e-05, global_step: 230, interval_runtime: 1.9598, interval_samples_per_second: 4.082, interval_steps_per_second: 5.103, epoch: 0.6085[0m
[32m[2022-08-31 18:18:27,713] [    INFO][0m - loss: 2.09325294, learning_rate: 2.961904761904762e-05, global_step: 240, interval_runtime: 1.9621, interval_samples_per_second: 4.077, interval_steps_per_second: 5.097, epoch: 0.6349[0m
[32m[2022-08-31 18:18:29,689] [    INFO][0m - loss: 1.78277206, learning_rate: 2.96031746031746e-05, global_step: 250, interval_runtime: 1.9764, interval_samples_per_second: 4.048, interval_steps_per_second: 5.06, epoch: 0.6614[0m
[32m[2022-08-31 18:18:31,649] [    INFO][0m - loss: 2.03645668, learning_rate: 2.958730158730159e-05, global_step: 260, interval_runtime: 1.9597, interval_samples_per_second: 4.082, interval_steps_per_second: 5.103, epoch: 0.6878[0m
[32m[2022-08-31 18:18:33,617] [    INFO][0m - loss: 1.64938087, learning_rate: 2.9571428571428575e-05, global_step: 270, interval_runtime: 1.9676, interval_samples_per_second: 4.066, interval_steps_per_second: 5.082, epoch: 0.7143[0m
[32m[2022-08-31 18:18:35,586] [    INFO][0m - loss: 2.30791988, learning_rate: 2.9555555555555556e-05, global_step: 280, interval_runtime: 1.969, interval_samples_per_second: 4.063, interval_steps_per_second: 5.079, epoch: 0.7407[0m
[32m[2022-08-31 18:18:37,550] [    INFO][0m - loss: 1.9841465, learning_rate: 2.953968253968254e-05, global_step: 290, interval_runtime: 1.9648, interval_samples_per_second: 4.072, interval_steps_per_second: 5.09, epoch: 0.7672[0m
[32m[2022-08-31 18:18:39,512] [    INFO][0m - loss: 1.8460638, learning_rate: 2.9523809523809523e-05, global_step: 300, interval_runtime: 1.9616, interval_samples_per_second: 4.078, interval_steps_per_second: 5.098, epoch: 0.7937[0m
[32m[2022-08-31 18:18:39,512] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:18:39,513] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 18:18:39,513] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:18:39,513] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:18:39,513] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 18:18:54,813] [    INFO][0m - eval_loss: 1.983701467514038, eval_accuracy: 0.4989075018208303, eval_runtime: 15.2994, eval_samples_per_second: 89.742, eval_steps_per_second: 2.811, epoch: 0.7937[0m
[32m[2022-08-31 18:18:54,813] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-31 18:18:54,813] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:18:58,358] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-31 18:18:58,358] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-31 18:19:06,565] [    INFO][0m - loss: 2.3668087, learning_rate: 2.9507936507936508e-05, global_step: 310, interval_runtime: 27.0527, interval_samples_per_second: 0.296, interval_steps_per_second: 0.37, epoch: 0.8201[0m
[32m[2022-08-31 18:19:08,536] [    INFO][0m - loss: 1.72734871, learning_rate: 2.9492063492063493e-05, global_step: 320, interval_runtime: 1.9715, interval_samples_per_second: 4.058, interval_steps_per_second: 5.072, epoch: 0.8466[0m
[32m[2022-08-31 18:19:10,494] [    INFO][0m - loss: 2.17353287, learning_rate: 2.9476190476190475e-05, global_step: 330, interval_runtime: 1.957, interval_samples_per_second: 4.088, interval_steps_per_second: 5.11, epoch: 0.873[0m
[32m[2022-08-31 18:19:12,457] [    INFO][0m - loss: 2.09777908, learning_rate: 2.946031746031746e-05, global_step: 340, interval_runtime: 1.9622, interval_samples_per_second: 4.077, interval_steps_per_second: 5.096, epoch: 0.8995[0m
[32m[2022-08-31 18:19:14,433] [    INFO][0m - loss: 2.16687851, learning_rate: 2.9444444444444445e-05, global_step: 350, interval_runtime: 1.9769, interval_samples_per_second: 4.047, interval_steps_per_second: 5.058, epoch: 0.9259[0m
[32m[2022-08-31 18:19:16,399] [    INFO][0m - loss: 2.32947807, learning_rate: 2.942857142857143e-05, global_step: 360, interval_runtime: 1.9653, interval_samples_per_second: 4.071, interval_steps_per_second: 5.088, epoch: 0.9524[0m
[32m[2022-08-31 18:19:18,367] [    INFO][0m - loss: 2.01471252, learning_rate: 2.9412698412698414e-05, global_step: 370, interval_runtime: 1.9684, interval_samples_per_second: 4.064, interval_steps_per_second: 5.08, epoch: 0.9788[0m
[32m[2022-08-31 18:19:20,444] [    INFO][0m - loss: 1.70975761, learning_rate: 2.93968253968254e-05, global_step: 380, interval_runtime: 2.0776, interval_samples_per_second: 3.851, interval_steps_per_second: 4.813, epoch: 1.0053[0m
[32m[2022-08-31 18:19:22,409] [    INFO][0m - loss: 1.57907019, learning_rate: 2.938095238095238e-05, global_step: 390, interval_runtime: 1.9652, interval_samples_per_second: 4.071, interval_steps_per_second: 5.089, epoch: 1.0317[0m
[32m[2022-08-31 18:19:24,366] [    INFO][0m - loss: 1.30217323, learning_rate: 2.9365079365079366e-05, global_step: 400, interval_runtime: 1.9567, interval_samples_per_second: 4.088, interval_steps_per_second: 5.111, epoch: 1.0582[0m
[32m[2022-08-31 18:19:24,367] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:19:24,367] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 18:19:24,367] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:19:24,367] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:19:24,367] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 18:19:39,791] [    INFO][0m - eval_loss: 2.0387954711914062, eval_accuracy: 0.4989075018208303, eval_runtime: 15.4232, eval_samples_per_second: 89.022, eval_steps_per_second: 2.788, epoch: 1.0582[0m
[32m[2022-08-31 18:19:39,792] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-31 18:19:39,792] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:19:43,227] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-31 18:19:43,227] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-31 18:19:50,885] [    INFO][0m - loss: 1.40787182, learning_rate: 2.934920634920635e-05, global_step: 410, interval_runtime: 26.5195, interval_samples_per_second: 0.302, interval_steps_per_second: 0.377, epoch: 1.0847[0m
[32m[2022-08-31 18:19:52,836] [    INFO][0m - loss: 1.5112999, learning_rate: 2.9333333333333333e-05, global_step: 420, interval_runtime: 1.9509, interval_samples_per_second: 4.101, interval_steps_per_second: 5.126, epoch: 1.1111[0m
[32m[2022-08-31 18:19:54,800] [    INFO][0m - loss: 1.36169262, learning_rate: 2.9317460317460318e-05, global_step: 430, interval_runtime: 1.9639, interval_samples_per_second: 4.073, interval_steps_per_second: 5.092, epoch: 1.1376[0m
[32m[2022-08-31 18:19:56,761] [    INFO][0m - loss: 1.4761404, learning_rate: 2.9301587301587303e-05, global_step: 440, interval_runtime: 1.9605, interval_samples_per_second: 4.081, interval_steps_per_second: 5.101, epoch: 1.164[0m
[32m[2022-08-31 18:19:58,728] [    INFO][0m - loss: 1.5088995, learning_rate: 2.9285714285714284e-05, global_step: 450, interval_runtime: 1.9674, interval_samples_per_second: 4.066, interval_steps_per_second: 5.083, epoch: 1.1905[0m
[32m[2022-08-31 18:20:01,707] [    INFO][0m - loss: 1.57008553, learning_rate: 2.9269841269841272e-05, global_step: 460, interval_runtime: 1.9628, interval_samples_per_second: 4.076, interval_steps_per_second: 5.095, epoch: 1.2169[0m
[32m[2022-08-31 18:20:03,667] [    INFO][0m - loss: 1.65247498, learning_rate: 2.9253968253968257e-05, global_step: 470, interval_runtime: 2.976, interval_samples_per_second: 2.688, interval_steps_per_second: 3.36, epoch: 1.2434[0m
[32m[2022-08-31 18:20:05,634] [    INFO][0m - loss: 1.5004549, learning_rate: 2.923809523809524e-05, global_step: 480, interval_runtime: 1.9671, interval_samples_per_second: 4.067, interval_steps_per_second: 5.084, epoch: 1.2698[0m
[32m[2022-08-31 18:20:07,594] [    INFO][0m - loss: 1.67486248, learning_rate: 2.9222222222222224e-05, global_step: 490, interval_runtime: 1.9596, interval_samples_per_second: 4.082, interval_steps_per_second: 5.103, epoch: 1.2963[0m
[32m[2022-08-31 18:20:09,562] [    INFO][0m - loss: 1.96402225, learning_rate: 2.9206349206349206e-05, global_step: 500, interval_runtime: 1.9683, interval_samples_per_second: 4.065, interval_steps_per_second: 5.081, epoch: 1.3228[0m
[32m[2022-08-31 18:20:09,562] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:20:09,562] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 18:20:09,563] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:20:09,563] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:20:09,563] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 18:20:25,105] [    INFO][0m - eval_loss: 2.0107924938201904, eval_accuracy: 0.493809176984705, eval_runtime: 15.5418, eval_samples_per_second: 88.343, eval_steps_per_second: 2.767, epoch: 1.3228[0m
[32m[2022-08-31 18:20:25,105] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-08-31 18:20:25,106] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:20:30,551] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-08-31 18:20:30,552] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-08-31 18:20:38,976] [    INFO][0m - loss: 1.89411259, learning_rate: 2.919047619047619e-05, global_step: 510, interval_runtime: 29.4135, interval_samples_per_second: 0.272, interval_steps_per_second: 0.34, epoch: 1.3492[0m
[32m[2022-08-31 18:20:40,929] [    INFO][0m - loss: 1.69528599, learning_rate: 2.9174603174603176e-05, global_step: 520, interval_runtime: 1.9538, interval_samples_per_second: 4.095, interval_steps_per_second: 5.118, epoch: 1.3757[0m
[32m[2022-08-31 18:20:42,892] [    INFO][0m - loss: 1.35031261, learning_rate: 2.9158730158730157e-05, global_step: 530, interval_runtime: 1.9627, interval_samples_per_second: 4.076, interval_steps_per_second: 5.095, epoch: 1.4021[0m
[32m[2022-08-31 18:20:44,860] [    INFO][0m - loss: 1.57504425, learning_rate: 2.9142857142857142e-05, global_step: 540, interval_runtime: 1.9684, interval_samples_per_second: 4.064, interval_steps_per_second: 5.08, epoch: 1.4286[0m
[32m[2022-08-31 18:20:46,848] [    INFO][0m - loss: 1.42833776, learning_rate: 2.9126984126984127e-05, global_step: 550, interval_runtime: 1.9876, interval_samples_per_second: 4.025, interval_steps_per_second: 5.031, epoch: 1.455[0m
[32m[2022-08-31 18:20:48,804] [    INFO][0m - loss: 1.55286169, learning_rate: 2.9111111111111112e-05, global_step: 560, interval_runtime: 1.9563, interval_samples_per_second: 4.089, interval_steps_per_second: 5.112, epoch: 1.4815[0m
[32m[2022-08-31 18:20:50,759] [    INFO][0m - loss: 1.58622818, learning_rate: 2.9095238095238097e-05, global_step: 570, interval_runtime: 1.9542, interval_samples_per_second: 4.094, interval_steps_per_second: 5.117, epoch: 1.5079[0m
[32m[2022-08-31 18:20:52,761] [    INFO][0m - loss: 1.66113529, learning_rate: 2.9079365079365082e-05, global_step: 580, interval_runtime: 2.0027, interval_samples_per_second: 3.995, interval_steps_per_second: 4.993, epoch: 1.5344[0m
[32m[2022-08-31 18:20:54,723] [    INFO][0m - loss: 1.6517519, learning_rate: 2.9063492063492064e-05, global_step: 590, interval_runtime: 1.9614, interval_samples_per_second: 4.079, interval_steps_per_second: 5.099, epoch: 1.5608[0m
[32m[2022-08-31 18:20:56,690] [    INFO][0m - loss: 1.45144386, learning_rate: 2.904761904761905e-05, global_step: 600, interval_runtime: 1.9675, interval_samples_per_second: 4.066, interval_steps_per_second: 5.082, epoch: 1.5873[0m
[32m[2022-08-31 18:20:56,691] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:20:56,691] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 18:20:56,691] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:20:56,691] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:20:56,691] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 18:21:12,015] [    INFO][0m - eval_loss: 1.9892802238464355, eval_accuracy: 0.4959941733430444, eval_runtime: 15.3231, eval_samples_per_second: 89.604, eval_steps_per_second: 2.806, epoch: 1.5873[0m
[32m[2022-08-31 18:21:12,015] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-08-31 18:21:12,015] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:21:15,198] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-08-31 18:21:15,198] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-08-31 18:21:22,539] [    INFO][0m - loss: 1.66625671, learning_rate: 2.9031746031746034e-05, global_step: 610, interval_runtime: 25.8489, interval_samples_per_second: 0.309, interval_steps_per_second: 0.387, epoch: 1.6138[0m
[32m[2022-08-31 18:21:24,516] [    INFO][0m - loss: 1.67719059, learning_rate: 2.9015873015873015e-05, global_step: 620, interval_runtime: 1.977, interval_samples_per_second: 4.046, interval_steps_per_second: 5.058, epoch: 1.6402[0m
[32m[2022-08-31 18:21:26,467] [    INFO][0m - loss: 1.25172539, learning_rate: 2.9e-05, global_step: 630, interval_runtime: 1.951, interval_samples_per_second: 4.101, interval_steps_per_second: 5.126, epoch: 1.6667[0m
[32m[2022-08-31 18:21:28,429] [    INFO][0m - loss: 1.58796196, learning_rate: 2.8984126984126985e-05, global_step: 640, interval_runtime: 1.962, interval_samples_per_second: 4.077, interval_steps_per_second: 5.097, epoch: 1.6931[0m
[32m[2022-08-31 18:21:30,382] [    INFO][0m - loss: 1.64943371, learning_rate: 2.8968253968253967e-05, global_step: 650, interval_runtime: 1.9533, interval_samples_per_second: 4.096, interval_steps_per_second: 5.12, epoch: 1.7196[0m
[32m[2022-08-31 18:21:32,346] [    INFO][0m - loss: 1.92694016, learning_rate: 2.8952380952380955e-05, global_step: 660, interval_runtime: 1.9637, interval_samples_per_second: 4.074, interval_steps_per_second: 5.093, epoch: 1.746[0m
[32m[2022-08-31 18:21:34,302] [    INFO][0m - loss: 1.71218452, learning_rate: 2.893650793650794e-05, global_step: 670, interval_runtime: 1.9559, interval_samples_per_second: 4.09, interval_steps_per_second: 5.113, epoch: 1.7725[0m
[32m[2022-08-31 18:21:36,260] [    INFO][0m - loss: 1.79167938, learning_rate: 2.892063492063492e-05, global_step: 680, interval_runtime: 1.9576, interval_samples_per_second: 4.087, interval_steps_per_second: 5.108, epoch: 1.7989[0m
[32m[2022-08-31 18:21:38,220] [    INFO][0m - loss: 1.49153137, learning_rate: 2.8904761904761907e-05, global_step: 690, interval_runtime: 1.961, interval_samples_per_second: 4.079, interval_steps_per_second: 5.099, epoch: 1.8254[0m
[32m[2022-08-31 18:21:40,198] [    INFO][0m - loss: 1.32044392, learning_rate: 2.8888888888888888e-05, global_step: 700, interval_runtime: 1.9709, interval_samples_per_second: 4.059, interval_steps_per_second: 5.074, epoch: 1.8519[0m
[32m[2022-08-31 18:21:40,199] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:21:40,199] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 18:21:40,199] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:21:40,199] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:21:40,199] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 18:21:55,658] [    INFO][0m - eval_loss: 2.014725923538208, eval_accuracy: 0.5149308084486526, eval_runtime: 15.4581, eval_samples_per_second: 88.821, eval_steps_per_second: 2.782, epoch: 1.8519[0m
[32m[2022-08-31 18:21:55,658] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-700[0m
[32m[2022-08-31 18:21:55,658] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:21:58,887] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-700/tokenizer_config.json[0m
[32m[2022-08-31 18:21:58,888] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-700/special_tokens_map.json[0m
[32m[2022-08-31 18:22:06,511] [    INFO][0m - loss: 1.62613506, learning_rate: 2.8873015873015873e-05, global_step: 710, interval_runtime: 26.3199, interval_samples_per_second: 0.304, interval_steps_per_second: 0.38, epoch: 1.8783[0m
[32m[2022-08-31 18:22:08,465] [    INFO][0m - loss: 1.44171972, learning_rate: 2.8857142857142858e-05, global_step: 720, interval_runtime: 1.9538, interval_samples_per_second: 4.094, interval_steps_per_second: 5.118, epoch: 1.9048[0m
[32m[2022-08-31 18:22:10,423] [    INFO][0m - loss: 1.7513485, learning_rate: 2.884126984126984e-05, global_step: 730, interval_runtime: 1.9577, interval_samples_per_second: 4.086, interval_steps_per_second: 5.108, epoch: 1.9312[0m
[32m[2022-08-31 18:22:12,383] [    INFO][0m - loss: 1.49067879, learning_rate: 2.8825396825396825e-05, global_step: 740, interval_runtime: 1.9606, interval_samples_per_second: 4.08, interval_steps_per_second: 5.1, epoch: 1.9577[0m
[32m[2022-08-31 18:22:14,349] [    INFO][0m - loss: 1.43194818, learning_rate: 2.880952380952381e-05, global_step: 750, interval_runtime: 1.9657, interval_samples_per_second: 4.07, interval_steps_per_second: 5.087, epoch: 1.9841[0m
[32m[2022-08-31 18:22:16,407] [    INFO][0m - loss: 1.21471577, learning_rate: 2.8793650793650795e-05, global_step: 760, interval_runtime: 2.058, interval_samples_per_second: 3.887, interval_steps_per_second: 4.859, epoch: 2.0106[0m
[32m[2022-08-31 18:22:18,371] [    INFO][0m - loss: 0.91253157, learning_rate: 2.877777777777778e-05, global_step: 770, interval_runtime: 1.9638, interval_samples_per_second: 4.074, interval_steps_per_second: 5.092, epoch: 2.037[0m
[32m[2022-08-31 18:22:20,337] [    INFO][0m - loss: 1.25691624, learning_rate: 2.8761904761904765e-05, global_step: 780, interval_runtime: 1.9662, interval_samples_per_second: 4.069, interval_steps_per_second: 5.086, epoch: 2.0635[0m
[32m[2022-08-31 18:22:22,305] [    INFO][0m - loss: 0.97538109, learning_rate: 2.8746031746031746e-05, global_step: 790, interval_runtime: 1.9683, interval_samples_per_second: 4.064, interval_steps_per_second: 5.081, epoch: 2.0899[0m
[32m[2022-08-31 18:22:24,258] [    INFO][0m - loss: 0.8670373, learning_rate: 2.873015873015873e-05, global_step: 800, interval_runtime: 1.9529, interval_samples_per_second: 4.097, interval_steps_per_second: 5.121, epoch: 2.1164[0m
[32m[2022-08-31 18:22:24,259] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:22:24,259] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 18:22:24,259] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:22:24,259] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:22:24,259] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 18:22:39,578] [    INFO][0m - eval_loss: 2.033665180206299, eval_accuracy: 0.504734158776402, eval_runtime: 15.318, eval_samples_per_second: 89.633, eval_steps_per_second: 2.807, epoch: 2.1164[0m
[32m[2022-08-31 18:22:39,578] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-800[0m
[32m[2022-08-31 18:22:39,578] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:22:43,018] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-800/tokenizer_config.json[0m
[32m[2022-08-31 18:22:43,018] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-800/special_tokens_map.json[0m
[32m[2022-08-31 18:22:50,597] [    INFO][0m - loss: 1.03988714, learning_rate: 2.8714285714285716e-05, global_step: 810, interval_runtime: 26.3384, interval_samples_per_second: 0.304, interval_steps_per_second: 0.38, epoch: 2.1429[0m
[32m[2022-08-31 18:22:52,544] [    INFO][0m - loss: 1.17634306, learning_rate: 2.8698412698412698e-05, global_step: 820, interval_runtime: 1.9467, interval_samples_per_second: 4.109, interval_steps_per_second: 5.137, epoch: 2.1693[0m
[32m[2022-08-31 18:22:54,511] [    INFO][0m - loss: 1.03594675, learning_rate: 2.8682539682539683e-05, global_step: 830, interval_runtime: 1.9671, interval_samples_per_second: 4.067, interval_steps_per_second: 5.084, epoch: 2.1958[0m
[32m[2022-08-31 18:22:56,490] [    INFO][0m - loss: 1.05637331, learning_rate: 2.8666666666666668e-05, global_step: 840, interval_runtime: 1.9789, interval_samples_per_second: 4.043, interval_steps_per_second: 5.053, epoch: 2.2222[0m
[32m[2022-08-31 18:22:58,444] [    INFO][0m - loss: 1.21666441, learning_rate: 2.865079365079365e-05, global_step: 850, interval_runtime: 1.9545, interval_samples_per_second: 4.093, interval_steps_per_second: 5.116, epoch: 2.2487[0m
[32m[2022-08-31 18:23:00,396] [    INFO][0m - loss: 0.97110491, learning_rate: 2.8634920634920638e-05, global_step: 860, interval_runtime: 1.9523, interval_samples_per_second: 4.098, interval_steps_per_second: 5.122, epoch: 2.2751[0m
[32m[2022-08-31 18:23:02,356] [    INFO][0m - loss: 1.36455221, learning_rate: 2.8619047619047623e-05, global_step: 870, interval_runtime: 1.9597, interval_samples_per_second: 4.082, interval_steps_per_second: 5.103, epoch: 2.3016[0m
[32m[2022-08-31 18:23:04,314] [    INFO][0m - loss: 1.05108557, learning_rate: 2.8603174603174604e-05, global_step: 880, interval_runtime: 1.9579, interval_samples_per_second: 4.086, interval_steps_per_second: 5.107, epoch: 2.328[0m
[32m[2022-08-31 18:23:06,280] [    INFO][0m - loss: 1.03922749, learning_rate: 2.858730158730159e-05, global_step: 890, interval_runtime: 1.9657, interval_samples_per_second: 4.07, interval_steps_per_second: 5.087, epoch: 2.3545[0m
[32m[2022-08-31 18:23:08,244] [    INFO][0m - loss: 0.93439322, learning_rate: 2.857142857142857e-05, global_step: 900, interval_runtime: 1.9639, interval_samples_per_second: 4.073, interval_steps_per_second: 5.092, epoch: 2.381[0m
[32m[2022-08-31 18:23:08,245] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:23:08,245] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 18:23:08,245] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:23:08,245] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:23:08,245] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 18:23:23,775] [    INFO][0m - eval_loss: 2.062000274658203, eval_accuracy: 0.5105608157319738, eval_runtime: 15.5295, eval_samples_per_second: 88.412, eval_steps_per_second: 2.769, epoch: 2.381[0m
[32m[2022-08-31 18:23:23,775] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-900[0m
[32m[2022-08-31 18:23:23,776] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:23:26,941] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-900/tokenizer_config.json[0m
[32m[2022-08-31 18:23:26,941] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-900/special_tokens_map.json[0m
[32m[2022-08-31 18:23:34,340] [    INFO][0m - loss: 1.07003403, learning_rate: 2.8555555555555556e-05, global_step: 910, interval_runtime: 26.0961, interval_samples_per_second: 0.307, interval_steps_per_second: 0.383, epoch: 2.4074[0m
[32m[2022-08-31 18:23:36,296] [    INFO][0m - loss: 1.12995129, learning_rate: 2.853968253968254e-05, global_step: 920, interval_runtime: 1.9562, interval_samples_per_second: 4.09, interval_steps_per_second: 5.112, epoch: 2.4339[0m
[32m[2022-08-31 18:23:38,260] [    INFO][0m - loss: 1.12234049, learning_rate: 2.8523809523809522e-05, global_step: 930, interval_runtime: 1.9641, interval_samples_per_second: 4.073, interval_steps_per_second: 5.091, epoch: 2.4603[0m
[32m[2022-08-31 18:23:40,232] [    INFO][0m - loss: 0.92270832, learning_rate: 2.8507936507936507e-05, global_step: 940, interval_runtime: 1.9716, interval_samples_per_second: 4.058, interval_steps_per_second: 5.072, epoch: 2.4868[0m
[32m[2022-08-31 18:23:42,194] [    INFO][0m - loss: 1.17641363, learning_rate: 2.8492063492063492e-05, global_step: 950, interval_runtime: 1.9627, interval_samples_per_second: 4.076, interval_steps_per_second: 5.095, epoch: 2.5132[0m
[32m[2022-08-31 18:23:44,165] [    INFO][0m - loss: 1.07951813, learning_rate: 2.8476190476190477e-05, global_step: 960, interval_runtime: 1.971, interval_samples_per_second: 4.059, interval_steps_per_second: 5.074, epoch: 2.5397[0m
[32m[2022-08-31 18:23:46,143] [    INFO][0m - loss: 1.23205099, learning_rate: 2.8460317460317462e-05, global_step: 970, interval_runtime: 1.9773, interval_samples_per_second: 4.046, interval_steps_per_second: 5.057, epoch: 2.5661[0m
[32m[2022-08-31 18:23:48,117] [    INFO][0m - loss: 1.16421566, learning_rate: 2.8444444444444447e-05, global_step: 980, interval_runtime: 1.9742, interval_samples_per_second: 4.052, interval_steps_per_second: 5.065, epoch: 2.5926[0m
[32m[2022-08-31 18:23:50,093] [    INFO][0m - loss: 1.12226954, learning_rate: 2.842857142857143e-05, global_step: 990, interval_runtime: 1.9761, interval_samples_per_second: 4.048, interval_steps_per_second: 5.06, epoch: 2.619[0m
[32m[2022-08-31 18:23:52,067] [    INFO][0m - loss: 1.17039642, learning_rate: 2.8412698412698414e-05, global_step: 1000, interval_runtime: 1.9733, interval_samples_per_second: 4.054, interval_steps_per_second: 5.068, epoch: 2.6455[0m
[32m[2022-08-31 18:23:52,067] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:23:52,068] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 18:23:52,068] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:23:52,068] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:23:52,068] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 18:24:07,689] [    INFO][0m - eval_loss: 2.0554580688476562, eval_accuracy: 0.5061908230152949, eval_runtime: 15.6211, eval_samples_per_second: 87.894, eval_steps_per_second: 2.753, epoch: 2.6455[0m
[32m[2022-08-31 18:24:07,690] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1000[0m
[32m[2022-08-31 18:24:07,690] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:24:12,649] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1000/tokenizer_config.json[0m
[32m[2022-08-31 18:24:12,650] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1000/special_tokens_map.json[0m
[32m[2022-08-31 18:24:20,010] [    INFO][0m - loss: 1.08246918, learning_rate: 2.83968253968254e-05, global_step: 1010, interval_runtime: 27.9431, interval_samples_per_second: 0.286, interval_steps_per_second: 0.358, epoch: 2.672[0m
[32m[2022-08-31 18:24:21,964] [    INFO][0m - loss: 1.15612364, learning_rate: 2.838095238095238e-05, global_step: 1020, interval_runtime: 1.9549, interval_samples_per_second: 4.092, interval_steps_per_second: 5.115, epoch: 2.6984[0m
[32m[2022-08-31 18:24:23,924] [    INFO][0m - loss: 1.18322992, learning_rate: 2.8365079365079365e-05, global_step: 1030, interval_runtime: 1.9594, interval_samples_per_second: 4.083, interval_steps_per_second: 5.103, epoch: 2.7249[0m
[32m[2022-08-31 18:24:25,897] [    INFO][0m - loss: 1.2241291, learning_rate: 2.834920634920635e-05, global_step: 1040, interval_runtime: 1.9732, interval_samples_per_second: 4.054, interval_steps_per_second: 5.068, epoch: 2.7513[0m
[32m[2022-08-31 18:24:27,858] [    INFO][0m - loss: 1.37769566, learning_rate: 2.8333333333333332e-05, global_step: 1050, interval_runtime: 1.9605, interval_samples_per_second: 4.081, interval_steps_per_second: 5.101, epoch: 2.7778[0m
[32m[2022-08-31 18:24:29,824] [    INFO][0m - loss: 1.14368811, learning_rate: 2.8317460317460317e-05, global_step: 1060, interval_runtime: 1.9661, interval_samples_per_second: 4.069, interval_steps_per_second: 5.086, epoch: 2.8042[0m
[32m[2022-08-31 18:24:31,801] [    INFO][0m - loss: 1.20829945, learning_rate: 2.8301587301587305e-05, global_step: 1070, interval_runtime: 1.9769, interval_samples_per_second: 4.047, interval_steps_per_second: 5.058, epoch: 2.8307[0m
[32m[2022-08-31 18:24:33,765] [    INFO][0m - loss: 1.2931076, learning_rate: 2.8285714285714287e-05, global_step: 1080, interval_runtime: 1.964, interval_samples_per_second: 4.073, interval_steps_per_second: 5.092, epoch: 2.8571[0m
[32m[2022-08-31 18:24:35,711] [    INFO][0m - loss: 0.97679071, learning_rate: 2.8269841269841272e-05, global_step: 1090, interval_runtime: 1.9461, interval_samples_per_second: 4.111, interval_steps_per_second: 5.139, epoch: 2.8836[0m
[32m[2022-08-31 18:24:37,692] [    INFO][0m - loss: 0.96975183, learning_rate: 2.8253968253968253e-05, global_step: 1100, interval_runtime: 1.9801, interval_samples_per_second: 4.04, interval_steps_per_second: 5.05, epoch: 2.9101[0m
[32m[2022-08-31 18:24:37,693] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:24:37,693] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 18:24:37,693] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:24:37,694] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:24:37,694] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 18:24:53,148] [    INFO][0m - eval_loss: 2.0581188201904297, eval_accuracy: 0.5287691187181355, eval_runtime: 15.4543, eval_samples_per_second: 88.842, eval_steps_per_second: 2.782, epoch: 2.9101[0m
[32m[2022-08-31 18:24:53,149] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1100[0m
[32m[2022-08-31 18:24:53,149] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:24:57,350] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1100/tokenizer_config.json[0m
[32m[2022-08-31 18:24:57,350] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1100/special_tokens_map.json[0m
[32m[2022-08-31 18:25:04,580] [    INFO][0m - loss: 1.14411888, learning_rate: 2.823809523809524e-05, global_step: 1110, interval_runtime: 26.8889, interval_samples_per_second: 0.298, interval_steps_per_second: 0.372, epoch: 2.9365[0m
[32m[2022-08-31 18:25:06,550] [    INFO][0m - loss: 1.03708439, learning_rate: 2.8222222222222223e-05, global_step: 1120, interval_runtime: 1.9703, interval_samples_per_second: 4.06, interval_steps_per_second: 5.075, epoch: 2.963[0m
[32m[2022-08-31 18:25:08,512] [    INFO][0m - loss: 1.13659105, learning_rate: 2.8206349206349205e-05, global_step: 1130, interval_runtime: 1.962, interval_samples_per_second: 4.077, interval_steps_per_second: 5.097, epoch: 2.9894[0m
[32m[2022-08-31 18:25:10,570] [    INFO][0m - loss: 0.89124327, learning_rate: 2.819047619047619e-05, global_step: 1140, interval_runtime: 2.0578, interval_samples_per_second: 3.888, interval_steps_per_second: 4.86, epoch: 3.0159[0m
[32m[2022-08-31 18:25:12,519] [    INFO][0m - loss: 0.57536774, learning_rate: 2.8174603174603175e-05, global_step: 1150, interval_runtime: 1.9487, interval_samples_per_second: 4.105, interval_steps_per_second: 5.132, epoch: 3.0423[0m
[32m[2022-08-31 18:25:14,479] [    INFO][0m - loss: 0.74953823, learning_rate: 2.8158730158730157e-05, global_step: 1160, interval_runtime: 1.9601, interval_samples_per_second: 4.081, interval_steps_per_second: 5.102, epoch: 3.0688[0m
[32m[2022-08-31 18:25:16,436] [    INFO][0m - loss: 0.780478, learning_rate: 2.8142857142857145e-05, global_step: 1170, interval_runtime: 1.9569, interval_samples_per_second: 4.088, interval_steps_per_second: 5.11, epoch: 3.0952[0m
[32m[2022-08-31 18:25:18,401] [    INFO][0m - loss: 0.81062145, learning_rate: 2.812698412698413e-05, global_step: 1180, interval_runtime: 1.9652, interval_samples_per_second: 4.071, interval_steps_per_second: 5.088, epoch: 3.1217[0m
[32m[2022-08-31 18:25:20,367] [    INFO][0m - loss: 0.85882978, learning_rate: 2.811111111111111e-05, global_step: 1190, interval_runtime: 1.9663, interval_samples_per_second: 4.069, interval_steps_per_second: 5.086, epoch: 3.1481[0m
[32m[2022-08-31 18:25:22,333] [    INFO][0m - loss: 0.82765055, learning_rate: 2.8095238095238096e-05, global_step: 1200, interval_runtime: 1.9655, interval_samples_per_second: 4.07, interval_steps_per_second: 5.088, epoch: 3.1746[0m
[32m[2022-08-31 18:25:22,333] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:25:22,333] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 18:25:22,333] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:25:22,333] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:25:22,334] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 18:25:37,680] [    INFO][0m - eval_loss: 2.1054294109344482, eval_accuracy: 0.5149308084486526, eval_runtime: 15.3464, eval_samples_per_second: 89.467, eval_steps_per_second: 2.802, epoch: 3.1746[0m
[32m[2022-08-31 18:25:37,681] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1200[0m
[32m[2022-08-31 18:25:37,681] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:25:41,826] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1200/tokenizer_config.json[0m
[32m[2022-08-31 18:25:42,104] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1200/special_tokens_map.json[0m
[32m[2022-08-31 18:25:49,758] [    INFO][0m - loss: 0.86276188, learning_rate: 2.807936507936508e-05, global_step: 1210, interval_runtime: 27.425, interval_samples_per_second: 0.292, interval_steps_per_second: 0.365, epoch: 3.2011[0m
[32m[2022-08-31 18:25:51,723] [    INFO][0m - loss: 0.87350101, learning_rate: 2.8063492063492063e-05, global_step: 1220, interval_runtime: 1.9646, interval_samples_per_second: 4.072, interval_steps_per_second: 5.09, epoch: 3.2275[0m
[32m[2022-08-31 18:25:53,687] [    INFO][0m - loss: 0.83181086, learning_rate: 2.8047619047619048e-05, global_step: 1230, interval_runtime: 1.9644, interval_samples_per_second: 4.072, interval_steps_per_second: 5.091, epoch: 3.254[0m
[32m[2022-08-31 18:25:55,670] [    INFO][0m - loss: 0.66389303, learning_rate: 2.8031746031746033e-05, global_step: 1240, interval_runtime: 1.983, interval_samples_per_second: 4.034, interval_steps_per_second: 5.043, epoch: 3.2804[0m
[32m[2022-08-31 18:25:57,658] [    INFO][0m - loss: 0.78171186, learning_rate: 2.8015873015873015e-05, global_step: 1250, interval_runtime: 1.9878, interval_samples_per_second: 4.025, interval_steps_per_second: 5.031, epoch: 3.3069[0m
[32m[2022-08-31 18:25:59,635] [    INFO][0m - loss: 0.67949586, learning_rate: 2.8e-05, global_step: 1260, interval_runtime: 1.976, interval_samples_per_second: 4.049, interval_steps_per_second: 5.061, epoch: 3.3333[0m
[32m[2022-08-31 18:26:01,600] [    INFO][0m - loss: 0.61863503, learning_rate: 2.7984126984126988e-05, global_step: 1270, interval_runtime: 1.9659, interval_samples_per_second: 4.069, interval_steps_per_second: 5.087, epoch: 3.3598[0m
[32m[2022-08-31 18:26:03,565] [    INFO][0m - loss: 0.88239002, learning_rate: 2.796825396825397e-05, global_step: 1280, interval_runtime: 1.9652, interval_samples_per_second: 4.071, interval_steps_per_second: 5.088, epoch: 3.3862[0m
[32m[2022-08-31 18:26:05,520] [    INFO][0m - loss: 0.60115652, learning_rate: 2.7952380952380955e-05, global_step: 1290, interval_runtime: 1.9545, interval_samples_per_second: 4.093, interval_steps_per_second: 5.116, epoch: 3.4127[0m
[32m[2022-08-31 18:26:07,479] [    INFO][0m - loss: 0.40067744, learning_rate: 2.7936507936507936e-05, global_step: 1300, interval_runtime: 1.9596, interval_samples_per_second: 4.082, interval_steps_per_second: 5.103, epoch: 3.4392[0m
[32m[2022-08-31 18:26:07,480] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:26:07,480] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 18:26:07,480] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:26:07,480] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:26:07,480] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 18:26:22,991] [    INFO][0m - eval_loss: 2.1536319255828857, eval_accuracy: 0.5185724690458849, eval_runtime: 15.5104, eval_samples_per_second: 88.521, eval_steps_per_second: 2.772, epoch: 3.4392[0m
[32m[2022-08-31 18:26:22,991] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1300[0m
[32m[2022-08-31 18:26:22,992] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:26:26,434] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1300/tokenizer_config.json[0m
[32m[2022-08-31 18:26:26,434] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1300/special_tokens_map.json[0m
[32m[2022-08-31 18:26:34,261] [    INFO][0m - loss: 0.5385829, learning_rate: 2.792063492063492e-05, global_step: 1310, interval_runtime: 26.7816, interval_samples_per_second: 0.299, interval_steps_per_second: 0.373, epoch: 3.4656[0m
[32m[2022-08-31 18:26:36,230] [    INFO][0m - loss: 0.84679174, learning_rate: 2.7904761904761906e-05, global_step: 1320, interval_runtime: 1.9689, interval_samples_per_second: 4.063, interval_steps_per_second: 5.079, epoch: 3.4921[0m
[32m[2022-08-31 18:26:38,188] [    INFO][0m - loss: 0.69613123, learning_rate: 2.7888888888888888e-05, global_step: 1330, interval_runtime: 1.9583, interval_samples_per_second: 4.085, interval_steps_per_second: 5.106, epoch: 3.5185[0m
[32m[2022-08-31 18:26:40,161] [    INFO][0m - loss: 0.81996002, learning_rate: 2.7873015873015873e-05, global_step: 1340, interval_runtime: 1.9734, interval_samples_per_second: 4.054, interval_steps_per_second: 5.067, epoch: 3.545[0m
[32m[2022-08-31 18:26:42,140] [    INFO][0m - loss: 0.81242704, learning_rate: 2.7857142857142858e-05, global_step: 1350, interval_runtime: 1.9782, interval_samples_per_second: 4.044, interval_steps_per_second: 5.055, epoch: 3.5714[0m
[32m[2022-08-31 18:26:44,337] [    INFO][0m - loss: 0.7736599, learning_rate: 2.784126984126984e-05, global_step: 1360, interval_runtime: 1.9672, interval_samples_per_second: 4.067, interval_steps_per_second: 5.083, epoch: 3.5979[0m
[32m[2022-08-31 18:26:46,307] [    INFO][0m - loss: 0.81081352, learning_rate: 2.7825396825396828e-05, global_step: 1370, interval_runtime: 2.2002, interval_samples_per_second: 3.636, interval_steps_per_second: 4.545, epoch: 3.6243[0m
[32m[2022-08-31 18:26:48,266] [    INFO][0m - loss: 0.97261639, learning_rate: 2.7809523809523813e-05, global_step: 1380, interval_runtime: 1.9593, interval_samples_per_second: 4.083, interval_steps_per_second: 5.104, epoch: 3.6508[0m
[32m[2022-08-31 18:26:50,227] [    INFO][0m - loss: 0.70435166, learning_rate: 2.7793650793650794e-05, global_step: 1390, interval_runtime: 1.9604, interval_samples_per_second: 4.081, interval_steps_per_second: 5.101, epoch: 3.6772[0m
[32m[2022-08-31 18:26:52,191] [    INFO][0m - loss: 0.8894268, learning_rate: 2.777777777777778e-05, global_step: 1400, interval_runtime: 1.9639, interval_samples_per_second: 4.073, interval_steps_per_second: 5.092, epoch: 3.7037[0m
[32m[2022-08-31 18:26:52,192] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:26:52,192] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 18:26:52,192] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:26:52,192] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:26:52,192] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 18:27:07,639] [    INFO][0m - eval_loss: 2.1773083209991455, eval_accuracy: 0.49526584122359796, eval_runtime: 15.4468, eval_samples_per_second: 88.886, eval_steps_per_second: 2.784, epoch: 3.7037[0m
[32m[2022-08-31 18:27:07,640] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1400[0m
[32m[2022-08-31 18:27:07,640] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:27:09,437] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1400/tokenizer_config.json[0m
[32m[2022-08-31 18:27:09,437] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1400/special_tokens_map.json[0m
[32m[2022-08-31 18:27:14,256] [    INFO][0m - loss: 0.48010082, learning_rate: 2.7761904761904764e-05, global_step: 1410, interval_runtime: 22.0647, interval_samples_per_second: 0.363, interval_steps_per_second: 0.453, epoch: 3.7302[0m
[32m[2022-08-31 18:27:16,224] [    INFO][0m - loss: 0.71102099, learning_rate: 2.7746031746031746e-05, global_step: 1420, interval_runtime: 1.9681, interval_samples_per_second: 4.065, interval_steps_per_second: 5.081, epoch: 3.7566[0m
[32m[2022-08-31 18:27:18,190] [    INFO][0m - loss: 0.73440104, learning_rate: 2.773015873015873e-05, global_step: 1430, interval_runtime: 1.9659, interval_samples_per_second: 4.069, interval_steps_per_second: 5.087, epoch: 3.7831[0m
[32m[2022-08-31 18:27:20,160] [    INFO][0m - loss: 0.69677591, learning_rate: 2.7714285714285716e-05, global_step: 1440, interval_runtime: 1.9701, interval_samples_per_second: 4.061, interval_steps_per_second: 5.076, epoch: 3.8095[0m
[32m[2022-08-31 18:27:22,133] [    INFO][0m - loss: 1.19915762, learning_rate: 2.7698412698412697e-05, global_step: 1450, interval_runtime: 1.9734, interval_samples_per_second: 4.054, interval_steps_per_second: 5.068, epoch: 3.836[0m
[32m[2022-08-31 18:27:24,093] [    INFO][0m - loss: 0.83587818, learning_rate: 2.7682539682539682e-05, global_step: 1460, interval_runtime: 1.9599, interval_samples_per_second: 4.082, interval_steps_per_second: 5.102, epoch: 3.8624[0m
[32m[2022-08-31 18:27:26,067] [    INFO][0m - loss: 1.0226469, learning_rate: 2.766666666666667e-05, global_step: 1470, interval_runtime: 1.9739, interval_samples_per_second: 4.053, interval_steps_per_second: 5.066, epoch: 3.8889[0m
[32m[2022-08-31 18:27:28,020] [    INFO][0m - loss: 1.0858284, learning_rate: 2.7650793650793652e-05, global_step: 1480, interval_runtime: 1.9537, interval_samples_per_second: 4.095, interval_steps_per_second: 5.119, epoch: 3.9153[0m
[32m[2022-08-31 18:27:29,979] [    INFO][0m - loss: 0.79714956, learning_rate: 2.7634920634920637e-05, global_step: 1490, interval_runtime: 1.959, interval_samples_per_second: 4.084, interval_steps_per_second: 5.105, epoch: 3.9418[0m
[32m[2022-08-31 18:27:31,951] [    INFO][0m - loss: 0.88348131, learning_rate: 2.761904761904762e-05, global_step: 1500, interval_runtime: 1.9714, interval_samples_per_second: 4.058, interval_steps_per_second: 5.073, epoch: 3.9683[0m
[32m[2022-08-31 18:27:31,951] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:27:31,951] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 18:27:31,952] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:27:31,952] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:27:31,952] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-31 18:27:47,305] [    INFO][0m - eval_loss: 2.1878066062927246, eval_accuracy: 0.5010924981791697, eval_runtime: 15.3526, eval_samples_per_second: 89.431, eval_steps_per_second: 2.801, epoch: 3.9683[0m
[32m[2022-08-31 18:27:47,305] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1500[0m
[32m[2022-08-31 18:27:47,306] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:27:48,914] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1500/tokenizer_config.json[0m
[32m[2022-08-31 18:27:48,914] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1500/special_tokens_map.json[0m
[32m[2022-08-31 18:27:51,553] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-31 18:27:51,553] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-1100 (score: 0.5287691187181355).[0m
[32m[2022-08-31 18:27:52,412] [    INFO][0m - train_runtime: 662.3475, train_samples_per_second: 228.279, train_steps_per_second: 28.535, train_loss: 1.465322169939677, epoch: 3.9683[0m
[32m[2022-08-31 18:27:52,457] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-31 18:27:52,457] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:27:55,618] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-31 18:27:55,618] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-31 18:27:55,621] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-31 18:27:55,621] [    INFO][0m -   epoch                    =     3.9683[0m
[32m[2022-08-31 18:27:55,621] [    INFO][0m -   train_loss               =     1.4653[0m
[32m[2022-08-31 18:27:55,621] [    INFO][0m -   train_runtime            = 0:11:02.34[0m
[32m[2022-08-31 18:27:55,621] [    INFO][0m -   train_samples_per_second =    228.279[0m
[32m[2022-08-31 18:27:55,621] [    INFO][0m -   train_steps_per_second   =     28.535[0m
[32m[2022-08-31 18:27:55,631] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-31 18:27:55,631] [    INFO][0m -   Num examples = 1749[0m
[32m[2022-08-31 18:27:55,631] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:27:55,631] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:27:55,632] [    INFO][0m -   Total prediction steps = 55[0m
[32m[2022-08-31 18:28:15,438] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-31 18:28:15,439] [    INFO][0m -   test_accuracy           =     0.5232[0m
[32m[2022-08-31 18:28:15,439] [    INFO][0m -   test_loss               =     1.9409[0m
[32m[2022-08-31 18:28:15,439] [    INFO][0m -   test_runtime            = 0:00:19.80[0m
[32m[2022-08-31 18:28:15,439] [    INFO][0m -   test_samples_per_second =     88.303[0m
[32m[2022-08-31 18:28:15,439] [    INFO][0m -   test_steps_per_second   =      2.777[0m
[32m[2022-08-31 18:28:15,440] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-31 18:28:15,440] [    INFO][0m -   Num examples = 2600[0m
[32m[2022-08-31 18:28:15,440] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:28:15,440] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:28:15,440] [    INFO][0m -   Total prediction steps = 82[0m
[]
Traceback (most recent call last):
  File "train_single.py", line 174, in <module>
    main()
  File "train_single.py", line 168, in main
    test_ret = postprocess(test_ret, test_ds, data_args.task_name,
  File "/ssd2/wanghuijuan03/prompt/PaddleNLP/examples/benchmark/fewclue/a9/postprocess.py", line 29, in postprocess
    remap = json.loads(fp.readline().strip())
  File "/ssd2/wanghuijuan03/anaconda3/envs/prompt/lib/python3.8/json/__init__.py", line 357, in loads
    return _default_decoder.decode(s)
  File "/ssd2/wanghuijuan03/anaconda3/envs/prompt/lib/python3.8/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/ssd2/wanghuijuan03/anaconda3/envs/prompt/lib/python3.8/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)
run.sh: line 75: --freeze_plm: command not found
 
==========
ocnli
==========
 
[33m[2022-08-31 18:28:48,761] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-31 18:28:48,762] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-31 18:28:48,762] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 18:28:48,762] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-31 18:28:48,762] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 18:28:48,762] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-31 18:28:48,762] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-31 18:28:48,762] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-08-31 18:28:48,762] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-31 18:28:48,762] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-08-31 18:28:48,762] [    INFO][0m - [0m
[32m[2022-08-31 18:28:48,763] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 18:28:48,763] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-31 18:28:48,763] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 18:28:48,763] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-31 18:28:48,763] [    INFO][0m - prompt                        :‚Äú{'text':'text_a'}‚ÄùÂíå‚Äú{'text':'text_b'}‚Äù‰πãÈó¥ÁöÑÈÄªËæëÂÖ≥Á≥ªÊòØ{'mask'}{'mask'}„ÄÇ[0m
[32m[2022-08-31 18:28:48,763] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-31 18:28:48,763] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-31 18:28:48,763] [    INFO][0m - task_name                     :ocnli[0m
[32m[2022-08-31 18:28:48,763] [    INFO][0m - [0m
[32m[2022-08-31 18:28:48,763] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0831 18:28:48.765586 24783 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0831 18:28:48.769791 24783 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-31 18:28:51,883] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-08-31 18:28:51,908] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-08-31 18:28:51,909] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[32m[2022-08-31 18:28:51,910] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùÂíå‚Äú'}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': '‚Äù‰πãÈó¥ÁöÑÈÄªËæëÂÖ≥Á≥ªÊòØ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': '„ÄÇ'}][0m
[32m[2022-08-31 18:28:51,913] [    INFO][0m - {'contradiction': 0, 'entailment': 1, 'neutral': 2}[0m
2022-08-31 18:28:51,914 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-31 18:28:53,118] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 18:28:53,118] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-31 18:28:53,118] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 18:28:53,118] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-31 18:28:53,118] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-31 18:28:53,118] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-31 18:28:53,118] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-31 18:28:53,118] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-31 18:28:53,118] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-31 18:28:53,119] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-31 18:28:53,119] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-31 18:28:53,119] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-31 18:28:53,119] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-31 18:28:53,119] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-31 18:28:53,119] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-31 18:28:53,119] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-31 18:28:53,119] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-31 18:28:53,119] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-31 18:28:53,119] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-31 18:28:53,119] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-31 18:28:53,119] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-31 18:28:53,120] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-31 18:28:53,120] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-31 18:28:53,120] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-31 18:28:53,120] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-31 18:28:53,120] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-31 18:28:53,120] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-31 18:28:53,120] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-31 18:28:53,120] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-31 18:28:53,120] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-31 18:28:53,120] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-31 18:28:53,120] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-31 18:28:53,120] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-31 18:28:53,120] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-31 18:28:53,121] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-31 18:28:53,121] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-31 18:28:53,121] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-31 18:28:53,121] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug31_18-28-48_instance-3bwob41y-01[0m
[32m[2022-08-31 18:28:53,121] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-31 18:28:53,121] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-31 18:28:53,121] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-31 18:28:53,121] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-31 18:28:53,121] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-31 18:28:53,121] [    INFO][0m - max_seq_length                :128[0m
[32m[2022-08-31 18:28:53,121] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-31 18:28:53,121] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-31 18:28:53,121] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-31 18:28:53,122] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-31 18:28:53,122] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-08-31 18:28:53,122] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-31 18:28:53,122] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-31 18:28:53,122] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-31 18:28:53,122] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-31 18:28:53,122] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-31 18:28:53,122] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-31 18:28:53,122] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-31 18:28:53,122] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-31 18:28:53,122] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-31 18:28:53,122] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-31 18:28:53,122] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-31 18:28:53,123] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-31 18:28:53,123] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-31 18:28:53,123] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-31 18:28:53,123] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-31 18:28:53,123] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-31 18:28:53,123] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-31 18:28:53,123] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-31 18:28:53,123] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-31 18:28:53,123] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-31 18:28:53,123] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-31 18:28:53,123] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-31 18:28:53,123] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-31 18:28:53,123] [    INFO][0m - seed                          :42[0m
[32m[2022-08-31 18:28:53,124] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-31 18:28:53,124] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-31 18:28:53,124] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-31 18:28:53,124] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-31 18:28:53,124] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-31 18:28:53,124] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-31 18:28:53,124] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-31 18:28:53,124] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-31 18:28:53,124] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-31 18:28:53,124] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-31 18:28:53,124] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-31 18:28:53,124] [    INFO][0m - [0m
[32m[2022-08-31 18:28:53,126] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-31 18:28:53,126] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:28:53,126] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-08-31 18:28:53,126] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-31 18:28:53,126] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-31 18:28:53,126] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-31 18:28:53,127] [    INFO][0m -   Total optimization steps = 1000.0[0m
[32m[2022-08-31 18:28:53,127] [    INFO][0m -   Total num train samples = 8000[0m
[32m[2022-08-31 18:28:54,963] [    INFO][0m - loss: 0.82073584, learning_rate: 2.97e-05, global_step: 10, interval_runtime: 1.8362, interval_samples_per_second: 4.357, interval_steps_per_second: 5.446, epoch: 0.5[0m
[32m[2022-08-31 18:28:55,858] [    INFO][0m - loss: 0.7758687, learning_rate: 2.94e-05, global_step: 20, interval_runtime: 0.8944, interval_samples_per_second: 8.944, interval_steps_per_second: 11.181, epoch: 1.0[0m
[32m[2022-08-31 18:28:56,809] [    INFO][0m - loss: 0.45332828, learning_rate: 2.91e-05, global_step: 30, interval_runtime: 0.9507, interval_samples_per_second: 8.415, interval_steps_per_second: 10.519, epoch: 1.5[0m
[32m[2022-08-31 18:28:57,699] [    INFO][0m - loss: 0.36945708, learning_rate: 2.88e-05, global_step: 40, interval_runtime: 0.89, interval_samples_per_second: 8.989, interval_steps_per_second: 11.236, epoch: 2.0[0m
[32m[2022-08-31 18:28:58,649] [    INFO][0m - loss: 0.24664249, learning_rate: 2.8499999999999998e-05, global_step: 50, interval_runtime: 0.9502, interval_samples_per_second: 8.42, interval_steps_per_second: 10.524, epoch: 2.5[0m
[32m[2022-08-31 18:28:59,553] [    INFO][0m - loss: 0.0514859, learning_rate: 2.8199999999999998e-05, global_step: 60, interval_runtime: 0.903, interval_samples_per_second: 8.859, interval_steps_per_second: 11.074, epoch: 3.0[0m
[32m[2022-08-31 18:29:00,505] [    INFO][0m - loss: 0.04337934, learning_rate: 2.79e-05, global_step: 70, interval_runtime: 0.9529, interval_samples_per_second: 8.395, interval_steps_per_second: 10.494, epoch: 3.5[0m
[32m[2022-08-31 18:29:01,391] [    INFO][0m - loss: 0.06169096, learning_rate: 2.7600000000000003e-05, global_step: 80, interval_runtime: 0.8855, interval_samples_per_second: 9.034, interval_steps_per_second: 11.293, epoch: 4.0[0m
[32m[2022-08-31 18:29:02,339] [    INFO][0m - loss: 0.05587057, learning_rate: 2.7300000000000003e-05, global_step: 90, interval_runtime: 0.9489, interval_samples_per_second: 8.431, interval_steps_per_second: 10.538, epoch: 4.5[0m
[32m[2022-08-31 18:29:03,229] [    INFO][0m - loss: 0.00338748, learning_rate: 2.7000000000000002e-05, global_step: 100, interval_runtime: 0.8894, interval_samples_per_second: 8.995, interval_steps_per_second: 11.243, epoch: 5.0[0m
[32m[2022-08-31 18:29:03,230] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:29:03,230] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:29:03,230] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:29:03,230] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:29:03,230] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:29:03,955] [    INFO][0m - eval_loss: 2.7664809226989746, eval_accuracy: 0.74375, eval_runtime: 0.7248, eval_samples_per_second: 220.759, eval_steps_per_second: 6.899, epoch: 5.0[0m
[32m[2022-08-31 18:29:03,955] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-31 18:29:03,956] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:29:07,564] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-31 18:29:07,565] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-31 18:29:15,034] [    INFO][0m - loss: 0.03326714, learning_rate: 2.6700000000000002e-05, global_step: 110, interval_runtime: 11.8048, interval_samples_per_second: 0.678, interval_steps_per_second: 0.847, epoch: 5.5[0m
[32m[2022-08-31 18:29:15,923] [    INFO][0m - loss: 6.36e-05, learning_rate: 2.64e-05, global_step: 120, interval_runtime: 0.8887, interval_samples_per_second: 9.002, interval_steps_per_second: 11.252, epoch: 6.0[0m
[32m[2022-08-31 18:29:16,870] [    INFO][0m - loss: 0.05470418, learning_rate: 2.61e-05, global_step: 130, interval_runtime: 0.9473, interval_samples_per_second: 8.445, interval_steps_per_second: 10.557, epoch: 6.5[0m
[32m[2022-08-31 18:29:17,762] [    INFO][0m - loss: 0.00013306, learning_rate: 2.58e-05, global_step: 140, interval_runtime: 0.8922, interval_samples_per_second: 8.966, interval_steps_per_second: 11.208, epoch: 7.0[0m
[32m[2022-08-31 18:29:18,714] [    INFO][0m - loss: 6.13e-06, learning_rate: 2.55e-05, global_step: 150, interval_runtime: 0.9521, interval_samples_per_second: 8.402, interval_steps_per_second: 10.503, epoch: 7.5[0m
[32m[2022-08-31 18:29:19,611] [    INFO][0m - loss: 0.00021148, learning_rate: 2.52e-05, global_step: 160, interval_runtime: 0.8967, interval_samples_per_second: 8.921, interval_steps_per_second: 11.152, epoch: 8.0[0m
[32m[2022-08-31 18:29:20,597] [    INFO][0m - loss: 0.00014895, learning_rate: 2.49e-05, global_step: 170, interval_runtime: 0.9853, interval_samples_per_second: 8.119, interval_steps_per_second: 10.149, epoch: 8.5[0m
[32m[2022-08-31 18:29:21,491] [    INFO][0m - loss: 0.00311369, learning_rate: 2.4599999999999998e-05, global_step: 180, interval_runtime: 0.8948, interval_samples_per_second: 8.941, interval_steps_per_second: 11.176, epoch: 9.0[0m
[32m[2022-08-31 18:29:22,443] [    INFO][0m - loss: 0.07186024, learning_rate: 2.43e-05, global_step: 190, interval_runtime: 0.9515, interval_samples_per_second: 8.408, interval_steps_per_second: 10.509, epoch: 9.5[0m
[32m[2022-08-31 18:29:23,337] [    INFO][0m - loss: 0.00208529, learning_rate: 2.4e-05, global_step: 200, interval_runtime: 0.8945, interval_samples_per_second: 8.944, interval_steps_per_second: 11.18, epoch: 10.0[0m
[32m[2022-08-31 18:29:23,338] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:29:23,338] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:29:23,338] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:29:23,338] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:29:23,338] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:29:24,045] [    INFO][0m - eval_loss: 3.3940861225128174, eval_accuracy: 0.7375, eval_runtime: 0.7064, eval_samples_per_second: 226.489, eval_steps_per_second: 7.078, epoch: 10.0[0m
[32m[2022-08-31 18:29:24,045] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-31 18:29:24,045] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:29:27,235] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-31 18:29:27,236] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-31 18:29:33,655] [    INFO][0m - loss: 1.219e-05, learning_rate: 2.37e-05, global_step: 210, interval_runtime: 10.3181, interval_samples_per_second: 0.775, interval_steps_per_second: 0.969, epoch: 10.5[0m
[32m[2022-08-31 18:29:34,550] [    INFO][0m - loss: 0.00021969, learning_rate: 2.3400000000000003e-05, global_step: 220, interval_runtime: 0.8955, interval_samples_per_second: 8.933, interval_steps_per_second: 11.167, epoch: 11.0[0m
[32m[2022-08-31 18:29:35,507] [    INFO][0m - loss: 8.32e-06, learning_rate: 2.3100000000000002e-05, global_step: 230, interval_runtime: 0.9559, interval_samples_per_second: 8.369, interval_steps_per_second: 10.461, epoch: 11.5[0m
[32m[2022-08-31 18:29:36,394] [    INFO][0m - loss: 6.836e-05, learning_rate: 2.2800000000000002e-05, global_step: 240, interval_runtime: 0.8874, interval_samples_per_second: 9.015, interval_steps_per_second: 11.268, epoch: 12.0[0m
[32m[2022-08-31 18:29:37,341] [    INFO][0m - loss: 3.97e-06, learning_rate: 2.25e-05, global_step: 250, interval_runtime: 0.9472, interval_samples_per_second: 8.446, interval_steps_per_second: 10.557, epoch: 12.5[0m
[32m[2022-08-31 18:29:38,227] [    INFO][0m - loss: 1.635e-05, learning_rate: 2.22e-05, global_step: 260, interval_runtime: 0.886, interval_samples_per_second: 9.029, interval_steps_per_second: 11.287, epoch: 13.0[0m
[32m[2022-08-31 18:29:39,163] [    INFO][0m - loss: 0.13155071, learning_rate: 2.19e-05, global_step: 270, interval_runtime: 0.936, interval_samples_per_second: 8.547, interval_steps_per_second: 10.683, epoch: 13.5[0m
[32m[2022-08-31 18:29:40,051] [    INFO][0m - loss: 9.48e-06, learning_rate: 2.16e-05, global_step: 280, interval_runtime: 0.8874, interval_samples_per_second: 9.016, interval_steps_per_second: 11.269, epoch: 14.0[0m
[32m[2022-08-31 18:29:41,000] [    INFO][0m - loss: 1.91e-06, learning_rate: 2.13e-05, global_step: 290, interval_runtime: 0.9492, interval_samples_per_second: 8.428, interval_steps_per_second: 10.535, epoch: 14.5[0m
[32m[2022-08-31 18:29:41,902] [    INFO][0m - loss: 0.00012674, learning_rate: 2.1e-05, global_step: 300, interval_runtime: 0.902, interval_samples_per_second: 8.87, interval_steps_per_second: 11.087, epoch: 15.0[0m
[32m[2022-08-31 18:29:41,903] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:29:41,903] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:29:41,903] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:29:41,903] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:29:41,903] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:29:42,684] [    INFO][0m - eval_loss: 3.3041749000549316, eval_accuracy: 0.7375, eval_runtime: 0.7804, eval_samples_per_second: 205.028, eval_steps_per_second: 6.407, epoch: 15.0[0m
[32m[2022-08-31 18:29:42,684] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-31 18:29:42,684] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:29:46,270] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-31 18:29:46,270] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-31 18:29:55,509] [    INFO][0m - loss: 3.57e-06, learning_rate: 2.07e-05, global_step: 310, interval_runtime: 13.6071, interval_samples_per_second: 0.588, interval_steps_per_second: 0.735, epoch: 15.5[0m
[32m[2022-08-31 18:29:56,419] [    INFO][0m - loss: 5e-07, learning_rate: 2.04e-05, global_step: 320, interval_runtime: 0.91, interval_samples_per_second: 8.791, interval_steps_per_second: 10.989, epoch: 16.0[0m
[32m[2022-08-31 18:29:57,363] [    INFO][0m - loss: 8.3e-07, learning_rate: 2.01e-05, global_step: 330, interval_runtime: 0.9445, interval_samples_per_second: 8.47, interval_steps_per_second: 10.588, epoch: 16.5[0m
[32m[2022-08-31 18:29:58,253] [    INFO][0m - loss: 2.63e-06, learning_rate: 1.98e-05, global_step: 340, interval_runtime: 0.8891, interval_samples_per_second: 8.998, interval_steps_per_second: 11.247, epoch: 17.0[0m
[32m[2022-08-31 18:29:59,199] [    INFO][0m - loss: 1.48e-06, learning_rate: 1.95e-05, global_step: 350, interval_runtime: 0.9465, interval_samples_per_second: 8.452, interval_steps_per_second: 10.565, epoch: 17.5[0m
[32m[2022-08-31 18:30:00,091] [    INFO][0m - loss: 1.17e-06, learning_rate: 1.9200000000000003e-05, global_step: 360, interval_runtime: 0.8922, interval_samples_per_second: 8.966, interval_steps_per_second: 11.208, epoch: 18.0[0m
[32m[2022-08-31 18:30:01,032] [    INFO][0m - loss: 0.00019211, learning_rate: 1.8900000000000002e-05, global_step: 370, interval_runtime: 0.9404, interval_samples_per_second: 8.507, interval_steps_per_second: 10.633, epoch: 18.5[0m
[32m[2022-08-31 18:30:01,921] [    INFO][0m - loss: 0.00018763, learning_rate: 1.86e-05, global_step: 380, interval_runtime: 0.8898, interval_samples_per_second: 8.99, interval_steps_per_second: 11.238, epoch: 19.0[0m
[32m[2022-08-31 18:30:02,869] [    INFO][0m - loss: 4.14e-06, learning_rate: 1.83e-05, global_step: 390, interval_runtime: 0.9476, interval_samples_per_second: 8.443, interval_steps_per_second: 10.553, epoch: 19.5[0m
[32m[2022-08-31 18:30:03,760] [    INFO][0m - loss: 0.0002659, learning_rate: 1.8e-05, global_step: 400, interval_runtime: 0.8909, interval_samples_per_second: 8.98, interval_steps_per_second: 11.225, epoch: 20.0[0m
[32m[2022-08-31 18:30:03,761] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:30:03,761] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:30:03,761] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:30:03,761] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:30:03,761] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:30:04,474] [    INFO][0m - eval_loss: 3.4036865234375, eval_accuracy: 0.75625, eval_runtime: 0.7129, eval_samples_per_second: 224.425, eval_steps_per_second: 7.013, epoch: 20.0[0m
[32m[2022-08-31 18:30:04,475] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-31 18:30:04,475] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:30:07,672] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-31 18:30:07,672] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-31 18:30:14,466] [    INFO][0m - loss: 1.65e-06, learning_rate: 1.77e-05, global_step: 410, interval_runtime: 10.7057, interval_samples_per_second: 0.747, interval_steps_per_second: 0.934, epoch: 20.5[0m
[32m[2022-08-31 18:30:15,348] [    INFO][0m - loss: 6.8e-07, learning_rate: 1.74e-05, global_step: 420, interval_runtime: 0.8825, interval_samples_per_second: 9.066, interval_steps_per_second: 11.332, epoch: 21.0[0m
[32m[2022-08-31 18:30:16,286] [    INFO][0m - loss: 3.1e-07, learning_rate: 1.71e-05, global_step: 430, interval_runtime: 0.9373, interval_samples_per_second: 8.535, interval_steps_per_second: 10.668, epoch: 21.5[0m
[32m[2022-08-31 18:30:17,171] [    INFO][0m - loss: 0.15146048, learning_rate: 1.6800000000000002e-05, global_step: 440, interval_runtime: 0.8859, interval_samples_per_second: 9.03, interval_steps_per_second: 11.287, epoch: 22.0[0m
[32m[2022-08-31 18:30:18,109] [    INFO][0m - loss: 1.57e-06, learning_rate: 1.65e-05, global_step: 450, interval_runtime: 0.9379, interval_samples_per_second: 8.53, interval_steps_per_second: 10.662, epoch: 22.5[0m
[32m[2022-08-31 18:30:18,997] [    INFO][0m - loss: 3.61e-06, learning_rate: 1.62e-05, global_step: 460, interval_runtime: 0.888, interval_samples_per_second: 9.009, interval_steps_per_second: 11.261, epoch: 23.0[0m
[32m[2022-08-31 18:30:19,939] [    INFO][0m - loss: 1.08e-06, learning_rate: 1.59e-05, global_step: 470, interval_runtime: 0.9413, interval_samples_per_second: 8.499, interval_steps_per_second: 10.624, epoch: 23.5[0m
[32m[2022-08-31 18:30:20,827] [    INFO][0m - loss: 7.6e-07, learning_rate: 1.56e-05, global_step: 480, interval_runtime: 0.888, interval_samples_per_second: 9.009, interval_steps_per_second: 11.261, epoch: 24.0[0m
[32m[2022-08-31 18:30:21,785] [    INFO][0m - loss: 1.77e-06, learning_rate: 1.53e-05, global_step: 490, interval_runtime: 0.9586, interval_samples_per_second: 8.346, interval_steps_per_second: 10.432, epoch: 24.5[0m
[32m[2022-08-31 18:30:22,674] [    INFO][0m - loss: 1.317e-05, learning_rate: 1.5e-05, global_step: 500, interval_runtime: 0.8883, interval_samples_per_second: 9.006, interval_steps_per_second: 11.257, epoch: 25.0[0m
[32m[2022-08-31 18:30:22,674] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:30:22,674] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:30:22,674] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:30:22,674] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:30:22,674] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:30:23,390] [    INFO][0m - eval_loss: 3.430889129638672, eval_accuracy: 0.75, eval_runtime: 0.7153, eval_samples_per_second: 223.681, eval_steps_per_second: 6.99, epoch: 25.0[0m
[32m[2022-08-31 18:30:23,390] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-08-31 18:30:23,390] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:30:29,169] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-08-31 18:30:29,169] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-08-31 18:30:35,505] [    INFO][0m - loss: 2.2e-07, learning_rate: 1.47e-05, global_step: 510, interval_runtime: 12.8317, interval_samples_per_second: 0.623, interval_steps_per_second: 0.779, epoch: 25.5[0m
[32m[2022-08-31 18:30:36,415] [    INFO][0m - loss: 1.28e-06, learning_rate: 1.44e-05, global_step: 520, interval_runtime: 0.9097, interval_samples_per_second: 8.794, interval_steps_per_second: 10.993, epoch: 26.0[0m
[32m[2022-08-31 18:30:37,360] [    INFO][0m - loss: 5.7e-07, learning_rate: 1.4099999999999999e-05, global_step: 530, interval_runtime: 0.9452, interval_samples_per_second: 8.464, interval_steps_per_second: 10.58, epoch: 26.5[0m
[32m[2022-08-31 18:30:38,251] [    INFO][0m - loss: 1.85e-06, learning_rate: 1.3800000000000002e-05, global_step: 540, interval_runtime: 0.8904, interval_samples_per_second: 8.985, interval_steps_per_second: 11.231, epoch: 27.0[0m
[32m[2022-08-31 18:30:39,192] [    INFO][0m - loss: 9.5e-07, learning_rate: 1.3500000000000001e-05, global_step: 550, interval_runtime: 0.9414, interval_samples_per_second: 8.498, interval_steps_per_second: 10.623, epoch: 27.5[0m
[32m[2022-08-31 18:30:40,101] [    INFO][0m - loss: 1.661e-05, learning_rate: 1.32e-05, global_step: 560, interval_runtime: 0.9086, interval_samples_per_second: 8.805, interval_steps_per_second: 11.006, epoch: 28.0[0m
[32m[2022-08-31 18:30:41,046] [    INFO][0m - loss: 1.1e-07, learning_rate: 1.29e-05, global_step: 570, interval_runtime: 0.9452, interval_samples_per_second: 8.464, interval_steps_per_second: 10.58, epoch: 28.5[0m
[32m[2022-08-31 18:30:41,949] [    INFO][0m - loss: 1.04e-06, learning_rate: 1.26e-05, global_step: 580, interval_runtime: 0.9036, interval_samples_per_second: 8.853, interval_steps_per_second: 11.067, epoch: 29.0[0m
[32m[2022-08-31 18:30:42,893] [    INFO][0m - loss: 1e-07, learning_rate: 1.2299999999999999e-05, global_step: 590, interval_runtime: 0.9436, interval_samples_per_second: 8.478, interval_steps_per_second: 10.597, epoch: 29.5[0m
[32m[2022-08-31 18:30:43,783] [    INFO][0m - loss: 3e-07, learning_rate: 1.2e-05, global_step: 600, interval_runtime: 0.8903, interval_samples_per_second: 8.986, interval_steps_per_second: 11.232, epoch: 30.0[0m
[32m[2022-08-31 18:30:43,784] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:30:43,784] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:30:43,784] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:30:43,784] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:30:43,784] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:30:44,516] [    INFO][0m - eval_loss: 3.424053907394409, eval_accuracy: 0.75625, eval_runtime: 0.7322, eval_samples_per_second: 218.528, eval_steps_per_second: 6.829, epoch: 30.0[0m
[32m[2022-08-31 18:30:44,517] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-08-31 18:30:44,517] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:30:48,016] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-08-31 18:30:48,016] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-08-31 18:30:55,146] [    INFO][0m - loss: 1.03e-06, learning_rate: 1.1700000000000001e-05, global_step: 610, interval_runtime: 11.3622, interval_samples_per_second: 0.704, interval_steps_per_second: 0.88, epoch: 30.5[0m
[32m[2022-08-31 18:30:56,024] [    INFO][0m - loss: 2.2e-07, learning_rate: 1.1400000000000001e-05, global_step: 620, interval_runtime: 0.879, interval_samples_per_second: 9.102, interval_steps_per_second: 11.377, epoch: 31.0[0m
[32m[2022-08-31 18:30:56,962] [    INFO][0m - loss: 0.0849127, learning_rate: 1.11e-05, global_step: 630, interval_runtime: 0.9371, interval_samples_per_second: 8.537, interval_steps_per_second: 10.671, epoch: 31.5[0m
[32m[2022-08-31 18:30:57,853] [    INFO][0m - loss: 5.66e-06, learning_rate: 1.08e-05, global_step: 640, interval_runtime: 0.8908, interval_samples_per_second: 8.981, interval_steps_per_second: 11.226, epoch: 32.0[0m
[32m[2022-08-31 18:30:58,793] [    INFO][0m - loss: 3.9e-07, learning_rate: 1.05e-05, global_step: 650, interval_runtime: 0.9406, interval_samples_per_second: 8.505, interval_steps_per_second: 10.632, epoch: 32.5[0m
[32m[2022-08-31 18:30:59,684] [    INFO][0m - loss: 1.27e-06, learning_rate: 1.02e-05, global_step: 660, interval_runtime: 0.8904, interval_samples_per_second: 8.985, interval_steps_per_second: 11.231, epoch: 33.0[0m
[32m[2022-08-31 18:31:00,651] [    INFO][0m - loss: 8.3e-07, learning_rate: 9.9e-06, global_step: 670, interval_runtime: 0.9676, interval_samples_per_second: 8.268, interval_steps_per_second: 10.334, epoch: 33.5[0m
[32m[2022-08-31 18:31:01,535] [    INFO][0m - loss: 4.133e-05, learning_rate: 9.600000000000001e-06, global_step: 680, interval_runtime: 0.8837, interval_samples_per_second: 9.053, interval_steps_per_second: 11.316, epoch: 34.0[0m
[32m[2022-08-31 18:31:02,780] [    INFO][0m - loss: 0.00102488, learning_rate: 9.3e-06, global_step: 690, interval_runtime: 0.9654, interval_samples_per_second: 8.287, interval_steps_per_second: 10.358, epoch: 34.5[0m
[32m[2022-08-31 18:31:03,674] [    INFO][0m - loss: 5.1e-07, learning_rate: 9e-06, global_step: 700, interval_runtime: 1.1736, interval_samples_per_second: 6.817, interval_steps_per_second: 8.521, epoch: 35.0[0m
[32m[2022-08-31 18:31:03,674] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:31:03,674] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:31:03,675] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:31:03,675] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:31:03,675] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:31:04,385] [    INFO][0m - eval_loss: 3.555405378341675, eval_accuracy: 0.7375, eval_runtime: 0.7096, eval_samples_per_second: 225.482, eval_steps_per_second: 7.046, epoch: 35.0[0m
[32m[2022-08-31 18:31:04,386] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-700[0m
[32m[2022-08-31 18:31:04,386] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:31:07,952] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-700/tokenizer_config.json[0m
[32m[2022-08-31 18:31:07,953] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-700/special_tokens_map.json[0m
[32m[2022-08-31 18:31:14,691] [    INFO][0m - loss: 2.61e-06, learning_rate: 8.7e-06, global_step: 710, interval_runtime: 11.0171, interval_samples_per_second: 0.726, interval_steps_per_second: 0.908, epoch: 35.5[0m
[32m[2022-08-31 18:31:15,591] [    INFO][0m - loss: 2.14e-05, learning_rate: 8.400000000000001e-06, global_step: 720, interval_runtime: 0.8997, interval_samples_per_second: 8.892, interval_steps_per_second: 11.115, epoch: 36.0[0m
[32m[2022-08-31 18:31:16,536] [    INFO][0m - loss: 3.5e-07, learning_rate: 8.1e-06, global_step: 730, interval_runtime: 0.9458, interval_samples_per_second: 8.458, interval_steps_per_second: 10.573, epoch: 36.5[0m
[32m[2022-08-31 18:31:17,430] [    INFO][0m - loss: 1.69e-06, learning_rate: 7.8e-06, global_step: 740, interval_runtime: 0.8936, interval_samples_per_second: 8.952, interval_steps_per_second: 11.19, epoch: 37.0[0m
[32m[2022-08-31 18:31:18,409] [    INFO][0m - loss: 3.5e-07, learning_rate: 7.5e-06, global_step: 750, interval_runtime: 0.9792, interval_samples_per_second: 8.17, interval_steps_per_second: 10.212, epoch: 37.5[0m
[32m[2022-08-31 18:31:19,292] [    INFO][0m - loss: 3.69e-06, learning_rate: 7.2e-06, global_step: 760, interval_runtime: 0.8826, interval_samples_per_second: 9.064, interval_steps_per_second: 11.33, epoch: 38.0[0m
[32m[2022-08-31 18:31:20,287] [    INFO][0m - loss: 1.6e-07, learning_rate: 6.900000000000001e-06, global_step: 770, interval_runtime: 0.9953, interval_samples_per_second: 8.038, interval_steps_per_second: 10.047, epoch: 38.5[0m
[32m[2022-08-31 18:31:21,178] [    INFO][0m - loss: 4.7e-07, learning_rate: 6.6e-06, global_step: 780, interval_runtime: 0.8913, interval_samples_per_second: 8.976, interval_steps_per_second: 11.22, epoch: 39.0[0m
[32m[2022-08-31 18:31:22,120] [    INFO][0m - loss: 1.843e-05, learning_rate: 6.3e-06, global_step: 790, interval_runtime: 0.9417, interval_samples_per_second: 8.495, interval_steps_per_second: 10.619, epoch: 39.5[0m
[32m[2022-08-31 18:31:23,015] [    INFO][0m - loss: 4.586e-05, learning_rate: 6e-06, global_step: 800, interval_runtime: 0.8945, interval_samples_per_second: 8.944, interval_steps_per_second: 11.18, epoch: 40.0[0m
[32m[2022-08-31 18:31:23,015] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:31:23,016] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:31:23,016] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:31:23,016] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:31:23,016] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:31:23,711] [    INFO][0m - eval_loss: 3.5315375328063965, eval_accuracy: 0.7375, eval_runtime: 0.6946, eval_samples_per_second: 230.35, eval_steps_per_second: 7.198, epoch: 40.0[0m
[32m[2022-08-31 18:31:23,711] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-800[0m
[32m[2022-08-31 18:31:23,711] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:31:27,197] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-800/tokenizer_config.json[0m
[32m[2022-08-31 18:31:27,198] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-800/special_tokens_map.json[0m
[32m[2022-08-31 18:31:34,443] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-31 18:31:34,698] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-400 (score: 0.75625).[0m
[32m[2022-08-31 18:31:36,009] [    INFO][0m - train_runtime: 162.8812, train_samples_per_second: 49.116, train_steps_per_second: 6.139, train_loss: 0.042721375740297135, epoch: 40.0[0m
[32m[2022-08-31 18:31:36,011] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-31 18:31:36,011] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:31:40,208] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-31 18:31:40,210] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-31 18:31:40,213] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-31 18:31:40,213] [    INFO][0m -   epoch                    =       40.0[0m
[32m[2022-08-31 18:31:40,213] [    INFO][0m -   train_loss               =     0.0427[0m
[32m[2022-08-31 18:31:40,213] [    INFO][0m -   train_runtime            = 0:02:42.88[0m
[32m[2022-08-31 18:31:40,213] [    INFO][0m -   train_samples_per_second =     49.116[0m
[32m[2022-08-31 18:31:40,214] [    INFO][0m -   train_steps_per_second   =      6.139[0m
[32m[2022-08-31 18:31:40,231] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-31 18:31:40,232] [    INFO][0m -   Num examples = 2520[0m
[32m[2022-08-31 18:31:40,232] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:31:40,232] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:31:40,232] [    INFO][0m -   Total prediction steps = 79[0m
[32m[2022-08-31 18:31:51,489] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-31 18:31:51,490] [    INFO][0m -   test_accuracy           =     0.7115[0m
[32m[2022-08-31 18:31:51,490] [    INFO][0m -   test_loss               =     3.7295[0m
[32m[2022-08-31 18:31:51,490] [    INFO][0m -   test_runtime            = 0:00:11.25[0m
[32m[2022-08-31 18:31:51,490] [    INFO][0m -   test_samples_per_second =    223.844[0m
[32m[2022-08-31 18:31:51,490] [    INFO][0m -   test_steps_per_second   =      7.017[0m
[32m[2022-08-31 18:31:51,491] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-31 18:31:51,491] [    INFO][0m -   Num examples = 3000[0m
[32m[2022-08-31 18:31:51,491] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:31:51,491] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:31:51,491] [    INFO][0m -   Total prediction steps = 94[0m
[32m[2022-08-31 18:32:08,097] [    INFO][0m - Predictions for ocnlif saved to ./fewclue_submit_examples.[0m
[]
run.sh: line 75: --freeze_plm: command not found
 
==========
bustm
==========
 
[33m[2022-08-31 18:32:12,412] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-31 18:32:12,412] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-31 18:32:12,412] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 18:32:12,412] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-31 18:32:12,412] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 18:32:12,412] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-31 18:32:12,412] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-31 18:32:12,413] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-08-31 18:32:12,413] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-31 18:32:12,413] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-08-31 18:32:12,413] [    INFO][0m - [0m
[32m[2022-08-31 18:32:12,413] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 18:32:12,413] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-31 18:32:12,413] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 18:32:12,413] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-31 18:32:12,413] [    INFO][0m - prompt                        :‚Äú{'text':'text_a'}‚ÄùÂíå‚Äú{'text':'text_b'}‚Äù‰πãÈó¥ÁöÑÈÄªËæëÂÖ≥Á≥ªÊòØ{'mask'}{'mask'}„ÄÇ[0m
[32m[2022-08-31 18:32:12,413] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-31 18:32:12,413] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-31 18:32:12,413] [    INFO][0m - task_name                     :bustm[0m
[32m[2022-08-31 18:32:12,413] [    INFO][0m - [0m
[32m[2022-08-31 18:32:12,414] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0831 18:32:12.415762 41208 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0831 18:32:12.419826 41208 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-31 18:32:15,486] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-08-31 18:32:15,510] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-08-31 18:32:15,511] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[32m[2022-08-31 18:32:15,512] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùÂíå‚Äú'}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': '‚Äù‰πãÈó¥ÁöÑÈÄªËæëÂÖ≥Á≥ªÊòØ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': '„ÄÇ'}][0m
[32m[2022-08-31 18:32:15,514] [    INFO][0m - {'0': 0, '1': 1}[0m
2022-08-31 18:32:15,516 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-31 18:32:16,704] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 18:32:16,704] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-31 18:32:16,704] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 18:32:16,704] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-31 18:32:16,704] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-31 18:32:16,704] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-31 18:32:16,704] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-31 18:32:16,705] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-31 18:32:16,705] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-31 18:32:16,705] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-31 18:32:16,705] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-31 18:32:16,705] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-31 18:32:16,705] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-31 18:32:16,705] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-31 18:32:16,705] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-31 18:32:16,705] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-31 18:32:16,705] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-31 18:32:16,705] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-31 18:32:16,705] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-31 18:32:16,705] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-31 18:32:16,706] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-31 18:32:16,706] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-31 18:32:16,706] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-31 18:32:16,706] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-31 18:32:16,706] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-31 18:32:16,706] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-31 18:32:16,706] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-31 18:32:16,706] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-31 18:32:16,706] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-31 18:32:16,706] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-31 18:32:16,706] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-31 18:32:16,706] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-31 18:32:16,706] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-31 18:32:16,707] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-31 18:32:16,707] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-31 18:32:16,707] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-31 18:32:16,707] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-31 18:32:16,707] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug31_18-32-12_instance-3bwob41y-01[0m
[32m[2022-08-31 18:32:16,707] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-31 18:32:16,707] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-31 18:32:16,707] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-31 18:32:16,707] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-31 18:32:16,707] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-31 18:32:16,707] [    INFO][0m - max_seq_length                :64[0m
[32m[2022-08-31 18:32:16,707] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-31 18:32:16,707] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-31 18:32:16,707] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-31 18:32:16,708] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-31 18:32:16,708] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-08-31 18:32:16,708] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-31 18:32:16,708] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-31 18:32:16,708] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-31 18:32:16,708] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-31 18:32:16,708] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-31 18:32:16,708] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-31 18:32:16,708] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-31 18:32:16,708] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-31 18:32:16,708] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-31 18:32:16,708] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-31 18:32:16,708] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-31 18:32:16,709] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-31 18:32:16,709] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-31 18:32:16,709] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-31 18:32:16,709] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-31 18:32:16,709] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-31 18:32:16,709] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-31 18:32:16,709] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-31 18:32:16,709] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-31 18:32:16,709] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-31 18:32:16,709] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-31 18:32:16,709] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-31 18:32:16,709] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-31 18:32:16,709] [    INFO][0m - seed                          :42[0m
[32m[2022-08-31 18:32:16,709] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-31 18:32:16,710] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-31 18:32:16,710] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-31 18:32:16,710] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-31 18:32:16,710] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-31 18:32:16,710] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-31 18:32:16,710] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-31 18:32:16,710] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-31 18:32:16,710] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-31 18:32:16,710] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-31 18:32:16,710] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-31 18:32:16,710] [    INFO][0m - [0m
[32m[2022-08-31 18:32:16,712] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-31 18:32:16,712] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:32:16,712] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-08-31 18:32:16,712] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-31 18:32:16,712] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-31 18:32:16,712] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-31 18:32:16,712] [    INFO][0m -   Total optimization steps = 1000.0[0m
[32m[2022-08-31 18:32:16,712] [    INFO][0m -   Total num train samples = 8000[0m
[32m[2022-08-31 18:32:18,612] [    INFO][0m - loss: 0.53009381, learning_rate: 2.97e-05, global_step: 10, interval_runtime: 1.8989, interval_samples_per_second: 4.213, interval_steps_per_second: 5.266, epoch: 0.5[0m
[32m[2022-08-31 18:32:19,264] [    INFO][0m - loss: 0.85495119, learning_rate: 2.94e-05, global_step: 20, interval_runtime: 0.6514, interval_samples_per_second: 12.28, interval_steps_per_second: 15.35, epoch: 1.0[0m
[32m[2022-08-31 18:32:19,975] [    INFO][0m - loss: 0.47228088, learning_rate: 2.91e-05, global_step: 30, interval_runtime: 0.711, interval_samples_per_second: 11.251, interval_steps_per_second: 14.064, epoch: 1.5[0m
[32m[2022-08-31 18:32:20,602] [    INFO][0m - loss: 0.25270383, learning_rate: 2.88e-05, global_step: 40, interval_runtime: 0.6274, interval_samples_per_second: 12.75, interval_steps_per_second: 15.938, epoch: 2.0[0m
[32m[2022-08-31 18:32:21,300] [    INFO][0m - loss: 0.13325735, learning_rate: 2.8499999999999998e-05, global_step: 50, interval_runtime: 0.6974, interval_samples_per_second: 11.472, interval_steps_per_second: 14.34, epoch: 2.5[0m
[32m[2022-08-31 18:32:21,971] [    INFO][0m - loss: 0.2382087, learning_rate: 2.8199999999999998e-05, global_step: 60, interval_runtime: 0.6719, interval_samples_per_second: 11.907, interval_steps_per_second: 14.883, epoch: 3.0[0m
[32m[2022-08-31 18:32:22,709] [    INFO][0m - loss: 0.10115914, learning_rate: 2.79e-05, global_step: 70, interval_runtime: 0.7375, interval_samples_per_second: 10.848, interval_steps_per_second: 13.56, epoch: 3.5[0m
[32m[2022-08-31 18:32:23,528] [    INFO][0m - loss: 0.00937707, learning_rate: 2.7600000000000003e-05, global_step: 80, interval_runtime: 0.8191, interval_samples_per_second: 9.767, interval_steps_per_second: 12.209, epoch: 4.0[0m
[32m[2022-08-31 18:32:24,257] [    INFO][0m - loss: 0.00058857, learning_rate: 2.7300000000000003e-05, global_step: 90, interval_runtime: 0.7294, interval_samples_per_second: 10.967, interval_steps_per_second: 13.709, epoch: 4.5[0m
[32m[2022-08-31 18:32:24,859] [    INFO][0m - loss: 0.22239389, learning_rate: 2.7000000000000002e-05, global_step: 100, interval_runtime: 0.602, interval_samples_per_second: 13.289, interval_steps_per_second: 16.611, epoch: 5.0[0m
[32m[2022-08-31 18:32:24,860] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:32:24,860] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:32:24,860] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:32:24,860] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:32:24,860] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:32:25,282] [    INFO][0m - eval_loss: 1.685847520828247, eval_accuracy: 0.7875, eval_runtime: 0.4213, eval_samples_per_second: 379.759, eval_steps_per_second: 11.867, epoch: 5.0[0m
[32m[2022-08-31 18:32:25,282] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-31 18:32:25,282] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:32:28,403] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-31 18:32:28,404] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-31 18:32:35,156] [    INFO][0m - loss: 0.0003503, learning_rate: 2.6700000000000002e-05, global_step: 110, interval_runtime: 10.296, interval_samples_per_second: 0.777, interval_steps_per_second: 0.971, epoch: 5.5[0m
[32m[2022-08-31 18:32:35,802] [    INFO][0m - loss: 2.917e-05, learning_rate: 2.64e-05, global_step: 120, interval_runtime: 0.6467, interval_samples_per_second: 12.37, interval_steps_per_second: 15.463, epoch: 6.0[0m
[32m[2022-08-31 18:32:36,492] [    INFO][0m - loss: 0.12170155, learning_rate: 2.61e-05, global_step: 130, interval_runtime: 0.6894, interval_samples_per_second: 11.604, interval_steps_per_second: 14.504, epoch: 6.5[0m
[32m[2022-08-31 18:32:37,116] [    INFO][0m - loss: 0.00031326, learning_rate: 2.58e-05, global_step: 140, interval_runtime: 0.6241, interval_samples_per_second: 12.818, interval_steps_per_second: 16.023, epoch: 7.0[0m
[32m[2022-08-31 18:32:37,836] [    INFO][0m - loss: 0.01981974, learning_rate: 2.55e-05, global_step: 150, interval_runtime: 0.7202, interval_samples_per_second: 11.108, interval_steps_per_second: 13.885, epoch: 7.5[0m
[32m[2022-08-31 18:32:38,471] [    INFO][0m - loss: 0.00253536, learning_rate: 2.52e-05, global_step: 160, interval_runtime: 0.6347, interval_samples_per_second: 12.605, interval_steps_per_second: 15.756, epoch: 8.0[0m
[32m[2022-08-31 18:32:39,168] [    INFO][0m - loss: 0.00246607, learning_rate: 2.49e-05, global_step: 170, interval_runtime: 0.6969, interval_samples_per_second: 11.479, interval_steps_per_second: 14.349, epoch: 8.5[0m
[32m[2022-08-31 18:32:39,832] [    INFO][0m - loss: 0.08445424, learning_rate: 2.4599999999999998e-05, global_step: 180, interval_runtime: 0.6648, interval_samples_per_second: 12.035, interval_steps_per_second: 15.043, epoch: 9.0[0m
[32m[2022-08-31 18:32:40,548] [    INFO][0m - loss: 0.00136367, learning_rate: 2.43e-05, global_step: 190, interval_runtime: 0.7152, interval_samples_per_second: 11.186, interval_steps_per_second: 13.982, epoch: 9.5[0m
[32m[2022-08-31 18:32:41,163] [    INFO][0m - loss: 1.322e-05, learning_rate: 2.4e-05, global_step: 200, interval_runtime: 0.6158, interval_samples_per_second: 12.992, interval_steps_per_second: 16.239, epoch: 10.0[0m
[32m[2022-08-31 18:32:41,164] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:32:41,164] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:32:41,164] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:32:41,164] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:32:41,164] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:32:41,604] [    INFO][0m - eval_loss: 2.3284897804260254, eval_accuracy: 0.7625, eval_runtime: 0.4401, eval_samples_per_second: 363.562, eval_steps_per_second: 11.361, epoch: 10.0[0m
[32m[2022-08-31 18:32:41,605] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-31 18:32:41,605] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:32:47,560] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-31 18:32:47,560] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-31 18:32:53,938] [    INFO][0m - loss: 7.2e-06, learning_rate: 2.37e-05, global_step: 210, interval_runtime: 12.7744, interval_samples_per_second: 0.626, interval_steps_per_second: 0.783, epoch: 10.5[0m
[32m[2022-08-31 18:32:54,723] [    INFO][0m - loss: 0.03672312, learning_rate: 2.3400000000000003e-05, global_step: 220, interval_runtime: 0.7846, interval_samples_per_second: 10.196, interval_steps_per_second: 12.746, epoch: 11.0[0m
[32m[2022-08-31 18:32:55,440] [    INFO][0m - loss: 1.918e-05, learning_rate: 2.3100000000000002e-05, global_step: 230, interval_runtime: 0.7179, interval_samples_per_second: 11.143, interval_steps_per_second: 13.929, epoch: 11.5[0m
[32m[2022-08-31 18:32:56,082] [    INFO][0m - loss: 0.00159422, learning_rate: 2.2800000000000002e-05, global_step: 240, interval_runtime: 0.6419, interval_samples_per_second: 12.463, interval_steps_per_second: 15.579, epoch: 12.0[0m
[32m[2022-08-31 18:32:56,844] [    INFO][0m - loss: 0.00248198, learning_rate: 2.25e-05, global_step: 250, interval_runtime: 0.7615, interval_samples_per_second: 10.506, interval_steps_per_second: 13.132, epoch: 12.5[0m
[32m[2022-08-31 18:32:57,481] [    INFO][0m - loss: 7.17e-06, learning_rate: 2.22e-05, global_step: 260, interval_runtime: 0.6376, interval_samples_per_second: 12.548, interval_steps_per_second: 15.684, epoch: 13.0[0m
[32m[2022-08-31 18:32:58,195] [    INFO][0m - loss: 0.00532479, learning_rate: 2.19e-05, global_step: 270, interval_runtime: 0.7139, interval_samples_per_second: 11.206, interval_steps_per_second: 14.007, epoch: 13.5[0m
[32m[2022-08-31 18:32:58,838] [    INFO][0m - loss: 1.858e-05, learning_rate: 2.16e-05, global_step: 280, interval_runtime: 0.6427, interval_samples_per_second: 12.447, interval_steps_per_second: 15.559, epoch: 14.0[0m
[32m[2022-08-31 18:32:59,516] [    INFO][0m - loss: 9e-07, learning_rate: 2.13e-05, global_step: 290, interval_runtime: 0.6777, interval_samples_per_second: 11.805, interval_steps_per_second: 14.756, epoch: 14.5[0m
[32m[2022-08-31 18:33:00,121] [    INFO][0m - loss: 0.12724158, learning_rate: 2.1e-05, global_step: 300, interval_runtime: 0.6054, interval_samples_per_second: 13.214, interval_steps_per_second: 16.517, epoch: 15.0[0m
[32m[2022-08-31 18:33:00,121] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:33:00,121] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:33:00,122] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:33:00,122] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:33:00,122] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:33:00,561] [    INFO][0m - eval_loss: 2.4734177589416504, eval_accuracy: 0.7875, eval_runtime: 0.4384, eval_samples_per_second: 364.972, eval_steps_per_second: 11.405, epoch: 15.0[0m
[32m[2022-08-31 18:33:00,561] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-31 18:33:00,562] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:33:03,778] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-31 18:33:03,778] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-31 18:33:10,060] [    INFO][0m - loss: 4.36e-06, learning_rate: 2.07e-05, global_step: 310, interval_runtime: 9.9394, interval_samples_per_second: 0.805, interval_steps_per_second: 1.006, epoch: 15.5[0m
[32m[2022-08-31 18:33:10,667] [    INFO][0m - loss: 0.00015322, learning_rate: 2.04e-05, global_step: 320, interval_runtime: 0.607, interval_samples_per_second: 13.179, interval_steps_per_second: 16.474, epoch: 16.0[0m
[32m[2022-08-31 18:33:11,358] [    INFO][0m - loss: 2.97e-06, learning_rate: 2.01e-05, global_step: 330, interval_runtime: 0.6899, interval_samples_per_second: 11.595, interval_steps_per_second: 14.494, epoch: 16.5[0m
[32m[2022-08-31 18:33:11,980] [    INFO][0m - loss: 0.00049116, learning_rate: 1.98e-05, global_step: 340, interval_runtime: 0.6223, interval_samples_per_second: 12.856, interval_steps_per_second: 16.069, epoch: 17.0[0m
[32m[2022-08-31 18:33:12,680] [    INFO][0m - loss: 1.6e-06, learning_rate: 1.95e-05, global_step: 350, interval_runtime: 0.7007, interval_samples_per_second: 11.418, interval_steps_per_second: 14.272, epoch: 17.5[0m
[32m[2022-08-31 18:33:13,279] [    INFO][0m - loss: 0.00140168, learning_rate: 1.9200000000000003e-05, global_step: 360, interval_runtime: 0.5986, interval_samples_per_second: 13.364, interval_steps_per_second: 16.705, epoch: 18.0[0m
[32m[2022-08-31 18:33:14,017] [    INFO][0m - loss: 4.4e-07, learning_rate: 1.8900000000000002e-05, global_step: 370, interval_runtime: 0.7376, interval_samples_per_second: 10.847, interval_steps_per_second: 13.558, epoch: 18.5[0m
[32m[2022-08-31 18:33:14,609] [    INFO][0m - loss: 5.01e-06, learning_rate: 1.86e-05, global_step: 380, interval_runtime: 0.5923, interval_samples_per_second: 13.507, interval_steps_per_second: 16.884, epoch: 19.0[0m
[32m[2022-08-31 18:33:15,342] [    INFO][0m - loss: 2.675e-05, learning_rate: 1.83e-05, global_step: 390, interval_runtime: 0.7329, interval_samples_per_second: 10.915, interval_steps_per_second: 13.644, epoch: 19.5[0m
[32m[2022-08-31 18:33:15,944] [    INFO][0m - loss: 2.95e-06, learning_rate: 1.8e-05, global_step: 400, interval_runtime: 0.602, interval_samples_per_second: 13.289, interval_steps_per_second: 16.611, epoch: 20.0[0m
[32m[2022-08-31 18:33:15,945] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:33:15,945] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:33:15,945] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:33:15,945] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:33:15,945] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:33:16,435] [    INFO][0m - eval_loss: 2.6615912914276123, eval_accuracy: 0.7875, eval_runtime: 0.4896, eval_samples_per_second: 326.774, eval_steps_per_second: 10.212, epoch: 20.0[0m
[32m[2022-08-31 18:33:16,435] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-31 18:33:16,436] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:33:20,217] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-31 18:33:20,217] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-31 18:33:26,857] [    INFO][0m - loss: 4.5e-07, learning_rate: 1.77e-05, global_step: 410, interval_runtime: 10.9131, interval_samples_per_second: 0.733, interval_steps_per_second: 0.916, epoch: 20.5[0m
[32m[2022-08-31 18:33:27,452] [    INFO][0m - loss: 2.49e-06, learning_rate: 1.74e-05, global_step: 420, interval_runtime: 0.5956, interval_samples_per_second: 13.433, interval_steps_per_second: 16.791, epoch: 21.0[0m
[32m[2022-08-31 18:33:28,153] [    INFO][0m - loss: 6.8e-07, learning_rate: 1.71e-05, global_step: 430, interval_runtime: 0.7001, interval_samples_per_second: 11.427, interval_steps_per_second: 14.284, epoch: 21.5[0m
[32m[2022-08-31 18:33:28,793] [    INFO][0m - loss: 3.76e-06, learning_rate: 1.6800000000000002e-05, global_step: 440, interval_runtime: 0.6399, interval_samples_per_second: 12.501, interval_steps_per_second: 15.627, epoch: 22.0[0m
[32m[2022-08-31 18:33:29,500] [    INFO][0m - loss: 1.6e-06, learning_rate: 1.65e-05, global_step: 450, interval_runtime: 0.7076, interval_samples_per_second: 11.306, interval_steps_per_second: 14.133, epoch: 22.5[0m
[32m[2022-08-31 18:33:30,096] [    INFO][0m - loss: 3.9e-07, learning_rate: 1.62e-05, global_step: 460, interval_runtime: 0.596, interval_samples_per_second: 13.424, interval_steps_per_second: 16.78, epoch: 23.0[0m
[32m[2022-08-31 18:33:30,794] [    INFO][0m - loss: 0.06512278, learning_rate: 1.59e-05, global_step: 470, interval_runtime: 0.6981, interval_samples_per_second: 11.46, interval_steps_per_second: 14.325, epoch: 23.5[0m
[32m[2022-08-31 18:33:31,380] [    INFO][0m - loss: 0.00021963, learning_rate: 1.56e-05, global_step: 480, interval_runtime: 0.5857, interval_samples_per_second: 13.659, interval_steps_per_second: 17.073, epoch: 24.0[0m
[32m[2022-08-31 18:33:32,083] [    INFO][0m - loss: 2.1e-07, learning_rate: 1.53e-05, global_step: 490, interval_runtime: 0.7036, interval_samples_per_second: 11.371, interval_steps_per_second: 14.213, epoch: 24.5[0m
[32m[2022-08-31 18:33:32,745] [    INFO][0m - loss: 0.00440067, learning_rate: 1.5e-05, global_step: 500, interval_runtime: 0.6615, interval_samples_per_second: 12.093, interval_steps_per_second: 15.116, epoch: 25.0[0m
[32m[2022-08-31 18:33:32,745] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:33:32,746] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:33:32,746] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:33:32,746] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:33:32,746] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:33:33,162] [    INFO][0m - eval_loss: 2.740253448486328, eval_accuracy: 0.76875, eval_runtime: 0.4161, eval_samples_per_second: 384.526, eval_steps_per_second: 12.016, epoch: 25.0[0m
[32m[2022-08-31 18:33:33,162] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-08-31 18:33:33,163] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:33:36,949] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-08-31 18:33:36,949] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-08-31 18:33:42,989] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-31 18:33:42,990] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-100 (score: 0.7875).[0m
[32m[2022-08-31 18:33:44,015] [    INFO][0m - train_runtime: 87.3018, train_samples_per_second: 91.636, train_steps_per_second: 11.455, train_loss: 0.06586645095495805, epoch: 25.0[0m
[32m[2022-08-31 18:33:44,057] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-31 18:33:44,058] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:33:47,452] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-31 18:33:47,452] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-31 18:33:47,453] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-31 18:33:47,453] [    INFO][0m -   epoch                    =       25.0[0m
[32m[2022-08-31 18:33:47,453] [    INFO][0m -   train_loss               =     0.0659[0m
[32m[2022-08-31 18:33:47,453] [    INFO][0m -   train_runtime            = 0:01:27.30[0m
[32m[2022-08-31 18:33:47,454] [    INFO][0m -   train_samples_per_second =     91.636[0m
[32m[2022-08-31 18:33:47,454] [    INFO][0m -   train_steps_per_second   =     11.455[0m
[32m[2022-08-31 18:33:47,456] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-31 18:33:47,457] [    INFO][0m -   Num examples = 1772[0m
[32m[2022-08-31 18:33:47,457] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:33:47,457] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:33:47,457] [    INFO][0m -   Total prediction steps = 56[0m
[32m[2022-08-31 18:33:52,089] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-31 18:33:52,090] [    INFO][0m -   test_accuracy           =     0.7455[0m
[32m[2022-08-31 18:33:52,090] [    INFO][0m -   test_loss               =     2.3851[0m
[32m[2022-08-31 18:33:52,090] [    INFO][0m -   test_runtime            = 0:00:04.63[0m
[32m[2022-08-31 18:33:52,090] [    INFO][0m -   test_samples_per_second =    382.512[0m
[32m[2022-08-31 18:33:52,090] [    INFO][0m -   test_steps_per_second   =     12.088[0m
[32m[2022-08-31 18:33:52,090] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-31 18:33:52,091] [    INFO][0m -   Num examples = 2000[0m
[32m[2022-08-31 18:33:52,091] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:33:52,091] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:33:52,091] [    INFO][0m -   Total prediction steps = 63[0m
[32m[2022-08-31 18:33:59,009] [    INFO][0m - Predictions for bustm saved to ./fewclue_submit_examples.[0m
[]
run.sh: line 75: --freeze_plm: command not found
 
==========
chid
==========
 
[33m[2022-08-31 18:34:03,299] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-31 18:34:03,299] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-31 18:34:03,299] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 18:34:03,299] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-31 18:34:03,299] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 18:34:03,299] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-31 18:34:03,300] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-31 18:34:03,300] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-08-31 18:34:03,300] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-31 18:34:03,300] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-08-31 18:34:03,300] [    INFO][0m - [0m
[32m[2022-08-31 18:34:03,300] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 18:34:03,300] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-31 18:34:03,300] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 18:34:03,300] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-31 18:34:03,300] [    INFO][0m - prompt                        :{'text':'text_a'}{'sep'}{'hard':'ËøôÂè•ËØù‰∏≠ÁöÑÊàêËØ≠‰ΩøÁî®'}{'mask'}{'mask'}[0m
[32m[2022-08-31 18:34:03,300] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-31 18:34:03,300] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-31 18:34:03,300] [    INFO][0m - task_name                     :chid[0m
[32m[2022-08-31 18:34:03,301] [    INFO][0m - [0m
[32m[2022-08-31 18:34:03,301] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0831 18:34:03.302389 54605 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0831 18:34:03.306550 54605 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-31 18:34:06,289] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-08-31 18:34:06,315] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-08-31 18:34:06,315] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[32m[2022-08-31 18:34:06,317] [    INFO][0m - Using template: [{'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'sep': None}, {'add_prefix_space': '', 'hard': 'ËøôÂè•ËØù‰∏≠ÁöÑÊàêËØ≠‰ΩøÁî®'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}][0m
[32m[2022-08-31 18:34:06,319] [    INFO][0m - {0: 0, 1: 1}[0m
2022-08-31 18:34:06,321 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-31 18:34:07,574] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 18:34:07,574] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-31 18:34:07,574] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 18:34:07,574] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-31 18:34:07,574] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-31 18:34:07,574] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-31 18:34:07,575] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-31 18:34:07,575] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-31 18:34:07,575] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-31 18:34:07,575] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-31 18:34:07,575] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-31 18:34:07,575] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-31 18:34:07,575] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-31 18:34:07,575] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-31 18:34:07,575] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-31 18:34:07,575] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-31 18:34:07,575] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-31 18:34:07,575] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-31 18:34:07,575] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-31 18:34:07,576] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-31 18:34:07,576] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-31 18:34:07,576] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-31 18:34:07,576] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-31 18:34:07,576] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-31 18:34:07,576] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-31 18:34:07,576] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-31 18:34:07,576] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-31 18:34:07,576] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-31 18:34:07,576] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-31 18:34:07,576] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-31 18:34:07,576] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-31 18:34:07,576] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-31 18:34:07,577] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-31 18:34:07,577] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-31 18:34:07,577] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-31 18:34:07,577] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-31 18:34:07,577] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-31 18:34:07,577] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug31_18-34-03_instance-3bwob41y-01[0m
[32m[2022-08-31 18:34:07,577] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-31 18:34:07,577] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-31 18:34:07,577] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-31 18:34:07,577] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-31 18:34:07,577] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-31 18:34:07,577] [    INFO][0m - max_seq_length                :256[0m
[32m[2022-08-31 18:34:07,577] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-31 18:34:07,578] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-31 18:34:07,578] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-31 18:34:07,578] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-31 18:34:07,578] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-08-31 18:34:07,578] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-31 18:34:07,578] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-31 18:34:07,578] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-31 18:34:07,578] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-31 18:34:07,578] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-31 18:34:07,578] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-31 18:34:07,578] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-31 18:34:07,578] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-31 18:34:07,578] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-31 18:34:07,578] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-31 18:34:07,579] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-31 18:34:07,579] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-31 18:34:07,579] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-31 18:34:07,579] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-31 18:34:07,579] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-31 18:34:07,579] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-31 18:34:07,579] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-31 18:34:07,579] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-31 18:34:07,579] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-31 18:34:07,579] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-31 18:34:07,579] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-31 18:34:07,579] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-31 18:34:07,579] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-31 18:34:07,580] [    INFO][0m - seed                          :42[0m
[32m[2022-08-31 18:34:07,580] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-31 18:34:07,580] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-31 18:34:07,580] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-31 18:34:07,580] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-31 18:34:07,580] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-31 18:34:07,580] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-31 18:34:07,580] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-31 18:34:07,580] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-31 18:34:07,580] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-31 18:34:07,580] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-31 18:34:07,580] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-31 18:34:07,581] [    INFO][0m - [0m
[32m[2022-08-31 18:34:07,582] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-31 18:34:07,582] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-08-31 18:34:07,583] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-08-31 18:34:07,583] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-31 18:34:07,583] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-31 18:34:07,583] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-31 18:34:07,583] [    INFO][0m -   Total optimization steps = 8850.0[0m
[32m[2022-08-31 18:34:07,583] [    INFO][0m -   Total num train samples = 70700[0m
[32m[2022-08-31 18:34:10,450] [    INFO][0m - loss: 0.73506947, learning_rate: 2.9966101694915256e-05, global_step: 10, interval_runtime: 2.8649, interval_samples_per_second: 2.792, interval_steps_per_second: 3.49, epoch: 0.0565[0m
[32m[2022-08-31 18:34:12,066] [    INFO][0m - loss: 0.42065277, learning_rate: 2.9932203389830508e-05, global_step: 20, interval_runtime: 1.6178, interval_samples_per_second: 4.945, interval_steps_per_second: 6.181, epoch: 0.113[0m
[32m[2022-08-31 18:34:13,660] [    INFO][0m - loss: 0.37895668, learning_rate: 2.9898305084745767e-05, global_step: 30, interval_runtime: 1.5933, interval_samples_per_second: 5.021, interval_steps_per_second: 6.276, epoch: 0.1695[0m
[32m[2022-08-31 18:34:15,253] [    INFO][0m - loss: 0.46497903, learning_rate: 2.986440677966102e-05, global_step: 40, interval_runtime: 1.5924, interval_samples_per_second: 5.024, interval_steps_per_second: 6.28, epoch: 0.226[0m
[32m[2022-08-31 18:34:16,848] [    INFO][0m - loss: 0.34177194, learning_rate: 2.9830508474576274e-05, global_step: 50, interval_runtime: 1.5962, interval_samples_per_second: 5.012, interval_steps_per_second: 6.265, epoch: 0.2825[0m
[32m[2022-08-31 18:34:18,448] [    INFO][0m - loss: 0.21687756, learning_rate: 2.9796610169491526e-05, global_step: 60, interval_runtime: 1.5994, interval_samples_per_second: 5.002, interval_steps_per_second: 6.253, epoch: 0.339[0m
[32m[2022-08-31 18:34:20,046] [    INFO][0m - loss: 0.70728559, learning_rate: 2.976271186440678e-05, global_step: 70, interval_runtime: 1.5979, interval_samples_per_second: 5.007, interval_steps_per_second: 6.258, epoch: 0.3955[0m
[32m[2022-08-31 18:34:21,664] [    INFO][0m - loss: 0.62296782, learning_rate: 2.9728813559322033e-05, global_step: 80, interval_runtime: 1.6167, interval_samples_per_second: 4.948, interval_steps_per_second: 6.185, epoch: 0.452[0m
[32m[2022-08-31 18:34:23,268] [    INFO][0m - loss: 0.4479887, learning_rate: 2.9694915254237292e-05, global_step: 90, interval_runtime: 1.6047, interval_samples_per_second: 4.985, interval_steps_per_second: 6.232, epoch: 0.5085[0m
[32m[2022-08-31 18:34:24,866] [    INFO][0m - loss: 0.41291008, learning_rate: 2.9661016949152544e-05, global_step: 100, interval_runtime: 1.5987, interval_samples_per_second: 5.004, interval_steps_per_second: 6.255, epoch: 0.565[0m
[32m[2022-08-31 18:34:24,867] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:34:24,867] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-08-31 18:34:24,867] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:34:24,867] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:34:24,868] [    INFO][0m -   Total prediction steps = 45[0m
[32m[2022-08-31 18:34:36,592] [    INFO][0m - eval_loss: 0.4116353690624237, eval_accuracy: 0.28217821782178215, eval_runtime: 11.7239, eval_samples_per_second: 120.608, eval_steps_per_second: 3.838, epoch: 0.565[0m
[32m[2022-08-31 18:34:36,593] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-31 18:34:36,593] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:34:39,764] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-31 18:34:39,765] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-31 18:34:46,662] [    INFO][0m - loss: 0.42409253, learning_rate: 2.96271186440678e-05, global_step: 110, interval_runtime: 21.7952, interval_samples_per_second: 0.367, interval_steps_per_second: 0.459, epoch: 0.6215[0m
[32m[2022-08-31 18:34:48,260] [    INFO][0m - loss: 0.36642075, learning_rate: 2.959322033898305e-05, global_step: 120, interval_runtime: 1.5986, interval_samples_per_second: 5.004, interval_steps_per_second: 6.256, epoch: 0.678[0m
[32m[2022-08-31 18:34:49,857] [    INFO][0m - loss: 0.43935137, learning_rate: 2.9559322033898306e-05, global_step: 130, interval_runtime: 1.5969, interval_samples_per_second: 5.01, interval_steps_per_second: 6.262, epoch: 0.7345[0m
[32m[2022-08-31 18:34:51,458] [    INFO][0m - loss: 0.41467309, learning_rate: 2.9525423728813558e-05, global_step: 140, interval_runtime: 1.6014, interval_samples_per_second: 4.996, interval_steps_per_second: 6.245, epoch: 0.791[0m
[32m[2022-08-31 18:34:53,051] [    INFO][0m - loss: 0.48501191, learning_rate: 2.9491525423728817e-05, global_step: 150, interval_runtime: 1.5927, interval_samples_per_second: 5.023, interval_steps_per_second: 6.279, epoch: 0.8475[0m
[32m[2022-08-31 18:34:54,656] [    INFO][0m - loss: 0.45384889, learning_rate: 2.945762711864407e-05, global_step: 160, interval_runtime: 1.6057, interval_samples_per_second: 4.982, interval_steps_per_second: 6.228, epoch: 0.904[0m
[32m[2022-08-31 18:34:56,255] [    INFO][0m - loss: 0.39271982, learning_rate: 2.9423728813559324e-05, global_step: 170, interval_runtime: 1.593, interval_samples_per_second: 5.022, interval_steps_per_second: 6.277, epoch: 0.9605[0m
[32m[2022-08-31 18:34:57,878] [    INFO][0m - loss: 0.39610198, learning_rate: 2.9389830508474576e-05, global_step: 180, interval_runtime: 1.6282, interval_samples_per_second: 4.913, interval_steps_per_second: 6.142, epoch: 1.0169[0m
[32m[2022-08-31 18:34:59,476] [    INFO][0m - loss: 0.5127171, learning_rate: 2.935593220338983e-05, global_step: 190, interval_runtime: 1.5988, interval_samples_per_second: 5.004, interval_steps_per_second: 6.255, epoch: 1.0734[0m
[32m[2022-08-31 18:35:01,073] [    INFO][0m - loss: 0.44384923, learning_rate: 2.9322033898305087e-05, global_step: 200, interval_runtime: 1.5962, interval_samples_per_second: 5.012, interval_steps_per_second: 6.265, epoch: 1.1299[0m
[32m[2022-08-31 18:35:01,073] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:35:01,073] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-08-31 18:35:01,073] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:35:01,073] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:35:01,073] [    INFO][0m -   Total prediction steps = 45[0m
[32m[2022-08-31 18:35:12,706] [    INFO][0m - eval_loss: 0.4144256114959717, eval_accuracy: 0.18811881188118812, eval_runtime: 11.6321, eval_samples_per_second: 121.56, eval_steps_per_second: 3.869, epoch: 1.1299[0m
[32m[2022-08-31 18:35:12,706] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-31 18:35:12,707] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:35:16,024] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-31 18:35:16,024] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-31 18:35:23,425] [    INFO][0m - loss: 0.36848986, learning_rate: 2.9288135593220342e-05, global_step: 210, interval_runtime: 22.3524, interval_samples_per_second: 0.358, interval_steps_per_second: 0.447, epoch: 1.1864[0m
[32m[2022-08-31 18:35:25,025] [    INFO][0m - loss: 0.6843914, learning_rate: 2.9254237288135594e-05, global_step: 220, interval_runtime: 1.5997, interval_samples_per_second: 5.001, interval_steps_per_second: 6.251, epoch: 1.2429[0m
[32m[2022-08-31 18:35:26,619] [    INFO][0m - loss: 0.45458398, learning_rate: 2.922033898305085e-05, global_step: 230, interval_runtime: 1.5941, interval_samples_per_second: 5.019, interval_steps_per_second: 6.273, epoch: 1.2994[0m
[32m[2022-08-31 18:35:28,219] [    INFO][0m - loss: 0.45439448, learning_rate: 2.91864406779661e-05, global_step: 240, interval_runtime: 1.6005, interval_samples_per_second: 4.999, interval_steps_per_second: 6.248, epoch: 1.3559[0m
[32m[2022-08-31 18:35:29,811] [    INFO][0m - loss: 0.33332925, learning_rate: 2.9152542372881356e-05, global_step: 250, interval_runtime: 1.5913, interval_samples_per_second: 5.027, interval_steps_per_second: 6.284, epoch: 1.4124[0m
[32m[2022-08-31 18:35:31,410] [    INFO][0m - loss: 0.46146507, learning_rate: 2.911864406779661e-05, global_step: 260, interval_runtime: 1.5993, interval_samples_per_second: 5.002, interval_steps_per_second: 6.253, epoch: 1.4689[0m
[32m[2022-08-31 18:35:33,010] [    INFO][0m - loss: 0.36587927, learning_rate: 2.9084745762711867e-05, global_step: 270, interval_runtime: 1.6001, interval_samples_per_second: 5.0, interval_steps_per_second: 6.249, epoch: 1.5254[0m
[32m[2022-08-31 18:35:34,610] [    INFO][0m - loss: 0.4780725, learning_rate: 2.905084745762712e-05, global_step: 280, interval_runtime: 1.6, interval_samples_per_second: 5.0, interval_steps_per_second: 6.25, epoch: 1.5819[0m
[32m[2022-08-31 18:35:36,212] [    INFO][0m - loss: 0.30681388, learning_rate: 2.9016949152542374e-05, global_step: 290, interval_runtime: 1.6013, interval_samples_per_second: 4.996, interval_steps_per_second: 6.245, epoch: 1.6384[0m
[32m[2022-08-31 18:35:37,829] [    INFO][0m - loss: 0.51044068, learning_rate: 2.8983050847457626e-05, global_step: 300, interval_runtime: 1.6176, interval_samples_per_second: 4.946, interval_steps_per_second: 6.182, epoch: 1.6949[0m
[32m[2022-08-31 18:35:37,829] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:35:37,830] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-08-31 18:35:37,830] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:35:37,830] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:35:37,830] [    INFO][0m -   Total prediction steps = 45[0m
[32m[2022-08-31 18:35:49,426] [    INFO][0m - eval_loss: 0.41137388348579407, eval_accuracy: 0.22772277227722773, eval_runtime: 11.5958, eval_samples_per_second: 121.941, eval_steps_per_second: 3.881, epoch: 1.6949[0m
[32m[2022-08-31 18:35:49,427] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-31 18:35:49,427] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:35:52,901] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-31 18:35:52,901] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-31 18:36:00,246] [    INFO][0m - loss: 0.38173015, learning_rate: 2.894915254237288e-05, global_step: 310, interval_runtime: 22.4174, interval_samples_per_second: 0.357, interval_steps_per_second: 0.446, epoch: 1.7514[0m
[32m[2022-08-31 18:36:01,844] [    INFO][0m - loss: 0.3786943, learning_rate: 2.8915254237288137e-05, global_step: 320, interval_runtime: 1.5967, interval_samples_per_second: 5.01, interval_steps_per_second: 6.263, epoch: 1.8079[0m
[32m[2022-08-31 18:36:03,454] [    INFO][0m - loss: 0.5285923, learning_rate: 2.8881355932203392e-05, global_step: 330, interval_runtime: 1.6104, interval_samples_per_second: 4.968, interval_steps_per_second: 6.21, epoch: 1.8644[0m
[32m[2022-08-31 18:36:05,056] [    INFO][0m - loss: 0.31749558, learning_rate: 2.8847457627118644e-05, global_step: 340, interval_runtime: 1.6026, interval_samples_per_second: 4.992, interval_steps_per_second: 6.24, epoch: 1.9209[0m
[32m[2022-08-31 18:36:06,654] [    INFO][0m - loss: 0.32344604, learning_rate: 2.88135593220339e-05, global_step: 350, interval_runtime: 1.5983, interval_samples_per_second: 5.005, interval_steps_per_second: 6.257, epoch: 1.9774[0m
[32m[2022-08-31 18:36:08,270] [    INFO][0m - loss: 0.3712647, learning_rate: 2.877966101694915e-05, global_step: 360, interval_runtime: 1.6157, interval_samples_per_second: 4.951, interval_steps_per_second: 6.189, epoch: 2.0339[0m
[32m[2022-08-31 18:36:09,870] [    INFO][0m - loss: 0.33126838, learning_rate: 2.874576271186441e-05, global_step: 370, interval_runtime: 1.6002, interval_samples_per_second: 4.999, interval_steps_per_second: 6.249, epoch: 2.0904[0m
[32m[2022-08-31 18:36:11,484] [    INFO][0m - loss: 0.46172333, learning_rate: 2.8711864406779662e-05, global_step: 380, interval_runtime: 1.6134, interval_samples_per_second: 4.959, interval_steps_per_second: 6.198, epoch: 2.1469[0m
[32m[2022-08-31 18:36:13,088] [    INFO][0m - loss: 0.49048176, learning_rate: 2.8677966101694917e-05, global_step: 390, interval_runtime: 1.6044, interval_samples_per_second: 4.986, interval_steps_per_second: 6.233, epoch: 2.2034[0m
[32m[2022-08-31 18:36:14,692] [    INFO][0m - loss: 0.41640358, learning_rate: 2.864406779661017e-05, global_step: 400, interval_runtime: 1.6037, interval_samples_per_second: 4.988, interval_steps_per_second: 6.235, epoch: 2.2599[0m
[32m[2022-08-31 18:36:14,693] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:36:14,693] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-08-31 18:36:14,693] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:36:14,693] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:36:14,693] [    INFO][0m -   Total prediction steps = 45[0m
[32m[2022-08-31 18:36:26,362] [    INFO][0m - eval_loss: 0.4125131368637085, eval_accuracy: 0.2722772277227723, eval_runtime: 11.6689, eval_samples_per_second: 121.177, eval_steps_per_second: 3.856, epoch: 2.2599[0m
[32m[2022-08-31 18:36:26,363] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-31 18:36:26,363] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:36:29,880] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-31 18:36:29,881] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-31 18:36:37,357] [    INFO][0m - loss: 0.50410476, learning_rate: 2.8610169491525424e-05, global_step: 410, interval_runtime: 22.6655, interval_samples_per_second: 0.353, interval_steps_per_second: 0.441, epoch: 2.3164[0m
[32m[2022-08-31 18:36:38,948] [    INFO][0m - loss: 0.34482622, learning_rate: 2.8576271186440676e-05, global_step: 420, interval_runtime: 1.5906, interval_samples_per_second: 5.03, interval_steps_per_second: 6.287, epoch: 2.3729[0m
[32m[2022-08-31 18:36:40,552] [    INFO][0m - loss: 0.40884919, learning_rate: 2.8542372881355935e-05, global_step: 430, interval_runtime: 1.6035, interval_samples_per_second: 4.989, interval_steps_per_second: 6.236, epoch: 2.4294[0m
[32m[2022-08-31 18:36:42,154] [    INFO][0m - loss: 0.50905566, learning_rate: 2.8508474576271187e-05, global_step: 440, interval_runtime: 1.6024, interval_samples_per_second: 4.993, interval_steps_per_second: 6.241, epoch: 2.4859[0m
[32m[2022-08-31 18:36:43,762] [    INFO][0m - loss: 0.45234308, learning_rate: 2.8474576271186442e-05, global_step: 450, interval_runtime: 1.6085, interval_samples_per_second: 4.974, interval_steps_per_second: 6.217, epoch: 2.5424[0m
[32m[2022-08-31 18:36:45,366] [    INFO][0m - loss: 0.56078029, learning_rate: 2.8440677966101694e-05, global_step: 460, interval_runtime: 1.6034, interval_samples_per_second: 4.989, interval_steps_per_second: 6.237, epoch: 2.5989[0m
[32m[2022-08-31 18:36:46,985] [    INFO][0m - loss: 0.48406687, learning_rate: 2.840677966101695e-05, global_step: 470, interval_runtime: 1.6194, interval_samples_per_second: 4.94, interval_steps_per_second: 6.175, epoch: 2.6554[0m
[32m[2022-08-31 18:36:48,592] [    INFO][0m - loss: 0.42664514, learning_rate: 2.8372881355932205e-05, global_step: 480, interval_runtime: 1.6069, interval_samples_per_second: 4.979, interval_steps_per_second: 6.223, epoch: 2.7119[0m
[32m[2022-08-31 18:36:50,203] [    INFO][0m - loss: 0.41150393, learning_rate: 2.833898305084746e-05, global_step: 490, interval_runtime: 1.6104, interval_samples_per_second: 4.968, interval_steps_per_second: 6.21, epoch: 2.7684[0m
[32m[2022-08-31 18:36:51,819] [    INFO][0m - loss: 0.35407209, learning_rate: 2.8305084745762712e-05, global_step: 500, interval_runtime: 1.6164, interval_samples_per_second: 4.949, interval_steps_per_second: 6.187, epoch: 2.8249[0m
[32m[2022-08-31 18:36:51,819] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:36:51,820] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-08-31 18:36:51,820] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:36:51,820] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:36:51,820] [    INFO][0m -   Total prediction steps = 45[0m
[32m[2022-08-31 18:37:03,473] [    INFO][0m - eval_loss: 0.41468626260757446, eval_accuracy: 0.3465346534653465, eval_runtime: 11.6529, eval_samples_per_second: 121.343, eval_steps_per_second: 3.862, epoch: 2.8249[0m
[32m[2022-08-31 18:37:03,474] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-08-31 18:37:03,474] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:37:06,786] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-08-31 18:37:06,786] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-08-31 18:37:14,337] [    INFO][0m - loss: 0.33351066, learning_rate: 2.8271186440677967e-05, global_step: 510, interval_runtime: 22.5174, interval_samples_per_second: 0.355, interval_steps_per_second: 0.444, epoch: 2.8814[0m
[32m[2022-08-31 18:37:15,929] [    INFO][0m - loss: 0.35152721, learning_rate: 2.823728813559322e-05, global_step: 520, interval_runtime: 1.5924, interval_samples_per_second: 5.024, interval_steps_per_second: 6.28, epoch: 2.9379[0m
[32m[2022-08-31 18:37:17,515] [    INFO][0m - loss: 0.30944147, learning_rate: 2.8203389830508475e-05, global_step: 530, interval_runtime: 1.5862, interval_samples_per_second: 5.043, interval_steps_per_second: 6.304, epoch: 2.9944[0m
[32m[2022-08-31 18:37:19,127] [    INFO][0m - loss: 0.33817415, learning_rate: 2.816949152542373e-05, global_step: 540, interval_runtime: 1.6119, interval_samples_per_second: 4.963, interval_steps_per_second: 6.204, epoch: 3.0508[0m
[32m[2022-08-31 18:37:20,737] [    INFO][0m - loss: 0.37743165, learning_rate: 2.8135593220338985e-05, global_step: 550, interval_runtime: 1.6097, interval_samples_per_second: 4.97, interval_steps_per_second: 6.212, epoch: 3.1073[0m
[32m[2022-08-31 18:37:22,342] [    INFO][0m - loss: 0.44336786, learning_rate: 2.8101694915254237e-05, global_step: 560, interval_runtime: 1.6058, interval_samples_per_second: 4.982, interval_steps_per_second: 6.228, epoch: 3.1638[0m
[32m[2022-08-31 18:37:23,943] [    INFO][0m - loss: 0.43021545, learning_rate: 2.8067796610169492e-05, global_step: 570, interval_runtime: 1.6012, interval_samples_per_second: 4.996, interval_steps_per_second: 6.245, epoch: 3.2203[0m
[32m[2022-08-31 18:37:25,548] [    INFO][0m - loss: 0.40552683, learning_rate: 2.8033898305084744e-05, global_step: 580, interval_runtime: 1.6038, interval_samples_per_second: 4.988, interval_steps_per_second: 6.235, epoch: 3.2768[0m
[32m[2022-08-31 18:37:27,156] [    INFO][0m - loss: 0.45405488, learning_rate: 2.8e-05, global_step: 590, interval_runtime: 1.6088, interval_samples_per_second: 4.973, interval_steps_per_second: 6.216, epoch: 3.3333[0m
[32m[2022-08-31 18:37:28,753] [    INFO][0m - loss: 0.47624316, learning_rate: 2.7966101694915255e-05, global_step: 600, interval_runtime: 1.5971, interval_samples_per_second: 5.009, interval_steps_per_second: 6.261, epoch: 3.3898[0m
[32m[2022-08-31 18:37:28,754] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:37:28,754] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-08-31 18:37:28,754] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:37:28,754] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:37:28,754] [    INFO][0m -   Total prediction steps = 45[0m
[32m[2022-08-31 18:37:40,369] [    INFO][0m - eval_loss: 0.41092294454574585, eval_accuracy: 0.3118811881188119, eval_runtime: 11.6146, eval_samples_per_second: 121.743, eval_steps_per_second: 3.874, epoch: 3.3898[0m
[32m[2022-08-31 18:37:40,369] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-08-31 18:37:40,370] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:37:43,624] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-08-31 18:37:43,628] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-08-31 18:37:52,466] [    INFO][0m - loss: 0.38672266, learning_rate: 2.793220338983051e-05, global_step: 610, interval_runtime: 23.7124, interval_samples_per_second: 0.337, interval_steps_per_second: 0.422, epoch: 3.4463[0m
[32m[2022-08-31 18:37:54,065] [    INFO][0m - loss: 0.4330039, learning_rate: 2.7898305084745762e-05, global_step: 620, interval_runtime: 1.5993, interval_samples_per_second: 5.002, interval_steps_per_second: 6.253, epoch: 3.5028[0m
[32m[2022-08-31 18:37:55,670] [    INFO][0m - loss: 0.39687536, learning_rate: 2.7864406779661017e-05, global_step: 630, interval_runtime: 1.6047, interval_samples_per_second: 4.985, interval_steps_per_second: 6.232, epoch: 3.5593[0m
[32m[2022-08-31 18:37:57,269] [    INFO][0m - loss: 0.42443881, learning_rate: 2.783050847457627e-05, global_step: 640, interval_runtime: 1.5987, interval_samples_per_second: 5.004, interval_steps_per_second: 6.255, epoch: 3.6158[0m
[32m[2022-08-31 18:37:58,868] [    INFO][0m - loss: 0.38241129, learning_rate: 2.7796610169491528e-05, global_step: 650, interval_runtime: 1.5987, interval_samples_per_second: 5.004, interval_steps_per_second: 6.255, epoch: 3.6723[0m
[32m[2022-08-31 18:38:00,474] [    INFO][0m - loss: 0.41456461, learning_rate: 2.776271186440678e-05, global_step: 660, interval_runtime: 1.6068, interval_samples_per_second: 4.979, interval_steps_per_second: 6.224, epoch: 3.7288[0m
[32m[2022-08-31 18:38:02,083] [    INFO][0m - loss: 0.33849382, learning_rate: 2.7728813559322035e-05, global_step: 670, interval_runtime: 1.609, interval_samples_per_second: 4.972, interval_steps_per_second: 6.215, epoch: 3.7853[0m
[32m[2022-08-31 18:38:03,687] [    INFO][0m - loss: 0.37200747, learning_rate: 2.7694915254237287e-05, global_step: 680, interval_runtime: 1.6044, interval_samples_per_second: 4.986, interval_steps_per_second: 6.233, epoch: 3.8418[0m
[32m[2022-08-31 18:38:05,298] [    INFO][0m - loss: 0.40417433, learning_rate: 2.7661016949152542e-05, global_step: 690, interval_runtime: 1.6101, interval_samples_per_second: 4.968, interval_steps_per_second: 6.211, epoch: 3.8983[0m
[32m[2022-08-31 18:38:06,900] [    INFO][0m - loss: 0.45197096, learning_rate: 2.7627118644067794e-05, global_step: 700, interval_runtime: 1.6018, interval_samples_per_second: 4.994, interval_steps_per_second: 6.243, epoch: 3.9548[0m
[32m[2022-08-31 18:38:06,900] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:38:06,901] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-08-31 18:38:06,901] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:38:06,901] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:38:06,901] [    INFO][0m -   Total prediction steps = 45[0m
[32m[2022-08-31 18:38:18,580] [    INFO][0m - eval_loss: 0.410128116607666, eval_accuracy: 0.14356435643564355, eval_runtime: 11.6788, eval_samples_per_second: 121.074, eval_steps_per_second: 3.853, epoch: 3.9548[0m
[32m[2022-08-31 18:38:18,581] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-700[0m
[32m[2022-08-31 18:38:18,581] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:38:22,134] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-700/tokenizer_config.json[0m
[32m[2022-08-31 18:38:22,135] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-700/special_tokens_map.json[0m
[32m[2022-08-31 18:38:31,964] [    INFO][0m - loss: 0.49185362, learning_rate: 2.7593220338983053e-05, global_step: 710, interval_runtime: 25.0641, interval_samples_per_second: 0.319, interval_steps_per_second: 0.399, epoch: 4.0113[0m
[32m[2022-08-31 18:38:33,561] [    INFO][0m - loss: 0.3766978, learning_rate: 2.7559322033898305e-05, global_step: 720, interval_runtime: 1.5977, interval_samples_per_second: 5.007, interval_steps_per_second: 6.259, epoch: 4.0678[0m
[32m[2022-08-31 18:38:35,162] [    INFO][0m - loss: 0.44901199, learning_rate: 2.752542372881356e-05, global_step: 730, interval_runtime: 1.6005, interval_samples_per_second: 4.999, interval_steps_per_second: 6.248, epoch: 4.1243[0m
[32m[2022-08-31 18:38:36,772] [    INFO][0m - loss: 0.37422099, learning_rate: 2.7491525423728812e-05, global_step: 740, interval_runtime: 1.6103, interval_samples_per_second: 4.968, interval_steps_per_second: 6.21, epoch: 4.1808[0m
[32m[2022-08-31 18:38:38,375] [    INFO][0m - loss: 0.36257353, learning_rate: 2.7457627118644068e-05, global_step: 750, interval_runtime: 1.6029, interval_samples_per_second: 4.991, interval_steps_per_second: 6.239, epoch: 4.2373[0m
[32m[2022-08-31 18:38:39,985] [    INFO][0m - loss: 0.45752568, learning_rate: 2.742372881355932e-05, global_step: 760, interval_runtime: 1.61, interval_samples_per_second: 4.969, interval_steps_per_second: 6.211, epoch: 4.2938[0m
[32m[2022-08-31 18:38:41,609] [    INFO][0m - loss: 0.39691896, learning_rate: 2.7389830508474578e-05, global_step: 770, interval_runtime: 1.624, interval_samples_per_second: 4.926, interval_steps_per_second: 6.158, epoch: 4.3503[0m
[32m[2022-08-31 18:38:43,210] [    INFO][0m - loss: 0.46056852, learning_rate: 2.735593220338983e-05, global_step: 780, interval_runtime: 1.6014, interval_samples_per_second: 4.996, interval_steps_per_second: 6.245, epoch: 4.4068[0m
[32m[2022-08-31 18:38:44,816] [    INFO][0m - loss: 0.45599775, learning_rate: 2.7322033898305085e-05, global_step: 790, interval_runtime: 1.6056, interval_samples_per_second: 4.982, interval_steps_per_second: 6.228, epoch: 4.4633[0m
[32m[2022-08-31 18:38:46,418] [    INFO][0m - loss: 0.4114996, learning_rate: 2.7288135593220337e-05, global_step: 800, interval_runtime: 1.6018, interval_samples_per_second: 4.994, interval_steps_per_second: 6.243, epoch: 4.5198[0m
[32m[2022-08-31 18:38:46,419] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:38:46,419] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-08-31 18:38:46,419] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:38:46,419] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:38:46,419] [    INFO][0m -   Total prediction steps = 45[0m
[32m[2022-08-31 18:38:58,075] [    INFO][0m - eval_loss: 0.41196441650390625, eval_accuracy: 0.23267326732673269, eval_runtime: 11.6549, eval_samples_per_second: 121.322, eval_steps_per_second: 3.861, epoch: 4.5198[0m
[32m[2022-08-31 18:38:58,075] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-800[0m
[32m[2022-08-31 18:38:58,075] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:39:01,906] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-800/tokenizer_config.json[0m
[32m[2022-08-31 18:39:01,906] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-800/special_tokens_map.json[0m
[32m[2022-08-31 18:39:09,220] [    INFO][0m - loss: 0.38162184, learning_rate: 2.7254237288135593e-05, global_step: 810, interval_runtime: 22.8017, interval_samples_per_second: 0.351, interval_steps_per_second: 0.439, epoch: 4.5763[0m
[32m[2022-08-31 18:39:10,832] [    INFO][0m - loss: 0.38077993, learning_rate: 2.722033898305085e-05, global_step: 820, interval_runtime: 1.6126, interval_samples_per_second: 4.961, interval_steps_per_second: 6.201, epoch: 4.6328[0m
[32m[2022-08-31 18:39:12,445] [    INFO][0m - loss: 0.42355309, learning_rate: 2.7186440677966103e-05, global_step: 830, interval_runtime: 1.6131, interval_samples_per_second: 4.959, interval_steps_per_second: 6.199, epoch: 4.6893[0m
[32m[2022-08-31 18:39:14,054] [    INFO][0m - loss: 0.51617384, learning_rate: 2.715254237288136e-05, global_step: 840, interval_runtime: 1.6082, interval_samples_per_second: 4.975, interval_steps_per_second: 6.218, epoch: 4.7458[0m
[32m[2022-08-31 18:39:15,659] [    INFO][0m - loss: 0.50297413, learning_rate: 2.711864406779661e-05, global_step: 850, interval_runtime: 1.6062, interval_samples_per_second: 4.981, interval_steps_per_second: 6.226, epoch: 4.8023[0m
[32m[2022-08-31 18:39:17,277] [    INFO][0m - loss: 0.43483834, learning_rate: 2.7084745762711866e-05, global_step: 860, interval_runtime: 1.6175, interval_samples_per_second: 4.946, interval_steps_per_second: 6.182, epoch: 4.8588[0m
[32m[2022-08-31 18:39:18,887] [    INFO][0m - loss: 0.35695686, learning_rate: 2.7050847457627118e-05, global_step: 870, interval_runtime: 1.6102, interval_samples_per_second: 4.968, interval_steps_per_second: 6.21, epoch: 4.9153[0m
[32m[2022-08-31 18:39:20,495] [    INFO][0m - loss: 0.46141901, learning_rate: 2.7016949152542376e-05, global_step: 880, interval_runtime: 1.6078, interval_samples_per_second: 4.976, interval_steps_per_second: 6.22, epoch: 4.9718[0m
[32m[2022-08-31 18:39:22,101] [    INFO][0m - loss: 0.32118349, learning_rate: 2.698305084745763e-05, global_step: 890, interval_runtime: 1.6057, interval_samples_per_second: 4.982, interval_steps_per_second: 6.228, epoch: 5.0282[0m
[32m[2022-08-31 18:39:23,703] [    INFO][0m - loss: 0.41443844, learning_rate: 2.6949152542372884e-05, global_step: 900, interval_runtime: 1.6022, interval_samples_per_second: 4.993, interval_steps_per_second: 6.242, epoch: 5.0847[0m
[32m[2022-08-31 18:39:23,704] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:39:23,704] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-08-31 18:39:23,704] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:39:23,704] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:39:23,704] [    INFO][0m -   Total prediction steps = 45[0m
[32m[2022-08-31 18:39:35,344] [    INFO][0m - eval_loss: 0.4223526120185852, eval_accuracy: 0.3118811881188119, eval_runtime: 11.6398, eval_samples_per_second: 121.479, eval_steps_per_second: 3.866, epoch: 5.0847[0m
[32m[2022-08-31 18:39:35,345] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-900[0m
[32m[2022-08-31 18:39:35,345] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:39:39,531] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-900/tokenizer_config.json[0m
[32m[2022-08-31 18:39:39,840] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-900/special_tokens_map.json[0m
[32m[2022-08-31 18:39:45,561] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-31 18:39:45,561] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-500 (score: 0.3465346534653465).[0m
[32m[2022-08-31 18:39:46,508] [    INFO][0m - train_runtime: 338.9245, train_samples_per_second: 208.601, train_steps_per_second: 26.112, train_loss: 0.4256490887535943, epoch: 5.0847[0m
[32m[2022-08-31 18:39:46,510] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-31 18:39:46,510] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:39:49,454] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-31 18:39:49,455] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-31 18:39:49,457] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-31 18:39:49,458] [    INFO][0m -   epoch                    =     5.0847[0m
[32m[2022-08-31 18:39:49,458] [    INFO][0m -   train_loss               =     0.4256[0m
[32m[2022-08-31 18:39:49,458] [    INFO][0m -   train_runtime            = 0:05:38.92[0m
[32m[2022-08-31 18:39:49,458] [    INFO][0m -   train_samples_per_second =    208.601[0m
[32m[2022-08-31 18:39:49,459] [    INFO][0m -   train_steps_per_second   =     26.112[0m
[32m[2022-08-31 18:39:49,465] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-31 18:39:49,465] [    INFO][0m -   Num examples = 14014[0m
[32m[2022-08-31 18:39:49,466] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:39:49,466] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:39:49,466] [    INFO][0m -   Total prediction steps = 438[0m
[32m[2022-08-31 18:41:45,051] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-31 18:41:45,052] [    INFO][0m -   test_accuracy           =     0.3362[0m
[32m[2022-08-31 18:41:45,052] [    INFO][0m -   test_loss               =     0.4146[0m
[32m[2022-08-31 18:41:45,052] [    INFO][0m -   test_runtime            = 0:01:55.58[0m
[32m[2022-08-31 18:41:45,052] [    INFO][0m -   test_samples_per_second =    121.243[0m
[32m[2022-08-31 18:41:45,052] [    INFO][0m -   test_steps_per_second   =      3.789[0m
[32m[2022-08-31 18:41:45,053] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-31 18:41:45,053] [    INFO][0m -   Num examples = 14000[0m
[32m[2022-08-31 18:41:45,053] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:41:45,053] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:41:45,053] [    INFO][0m -   Total prediction steps = 438[0m
[32m[2022-08-31 18:44:00,127] [    INFO][0m - Predictions for chidf saved to ./fewclue_submit_examples.[0m
[]
run.sh: line 75: --freeze_plm: command not found
 
==========
csl
==========
 
[33m[2022-08-31 18:44:04,532] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-31 18:44:04,533] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-31 18:44:04,533] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 18:44:04,533] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-31 18:44:04,533] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 18:44:04,533] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-31 18:44:04,534] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-31 18:44:04,534] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-08-31 18:44:04,534] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-31 18:44:04,534] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-08-31 18:44:04,534] [    INFO][0m - [0m
[32m[2022-08-31 18:44:04,534] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 18:44:04,535] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-31 18:44:04,535] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 18:44:04,535] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-31 18:44:04,535] [    INFO][0m - prompt                        :{'text': 'text_a'}{'hard':'‰∏äÊñá‰∏≠'}{'mask'}{'hard': 'Ëøô‰∫õÂÖ≥ÈîÆËØçÔºö'}{'text':'text_b'}[0m
[32m[2022-08-31 18:44:04,536] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-31 18:44:04,536] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-31 18:44:04,536] [    INFO][0m - task_name                     :csl[0m
[32m[2022-08-31 18:44:04,536] [    INFO][0m - [0m
[32m[2022-08-31 18:44:04,536] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0831 18:44:04.538877 34990 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0831 18:44:04.544903 34990 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-31 18:44:07,786] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-08-31 18:44:07,814] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-08-31 18:44:07,815] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[32m[2022-08-31 18:44:07,816] [    INFO][0m - Using template: [{'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‰∏äÊñá‰∏≠'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'Ëøô‰∫õÂÖ≥ÈîÆËØçÔºö'}, {'add_prefix_space': '', 'text': 'text_b'}][0m
[32m[2022-08-31 18:44:07,818] [    INFO][0m - {'0': 0, '1': 1}[0m
2022-08-31 18:44:07,820 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-31 18:44:09,017] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 18:44:09,018] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-31 18:44:09,018] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 18:44:09,018] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-31 18:44:09,018] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-31 18:44:09,018] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-31 18:44:09,018] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-31 18:44:09,018] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-31 18:44:09,018] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-31 18:44:09,018] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-31 18:44:09,018] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-31 18:44:09,019] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-31 18:44:09,019] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-31 18:44:09,019] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-31 18:44:09,019] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-31 18:44:09,019] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-31 18:44:09,019] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-31 18:44:09,019] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-31 18:44:09,019] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-31 18:44:09,019] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-31 18:44:09,019] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-31 18:44:09,019] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-31 18:44:09,019] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-31 18:44:09,019] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-31 18:44:09,020] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-31 18:44:09,020] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-31 18:44:09,020] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-31 18:44:09,020] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-31 18:44:09,020] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-31 18:44:09,020] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-31 18:44:09,020] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-31 18:44:09,020] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-31 18:44:09,020] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-31 18:44:09,020] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-31 18:44:09,020] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-31 18:44:09,020] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-31 18:44:09,020] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-31 18:44:09,021] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug31_18-44-04_instance-3bwob41y-01[0m
[32m[2022-08-31 18:44:09,021] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-31 18:44:09,021] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-31 18:44:09,021] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-31 18:44:09,021] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-31 18:44:09,021] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-31 18:44:09,021] [    INFO][0m - max_seq_length                :320[0m
[32m[2022-08-31 18:44:09,021] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-31 18:44:09,021] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-31 18:44:09,021] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-31 18:44:09,021] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-31 18:44:09,021] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-08-31 18:44:09,021] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-31 18:44:09,021] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-31 18:44:09,022] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-31 18:44:09,022] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-31 18:44:09,022] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-31 18:44:09,022] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-31 18:44:09,022] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-31 18:44:09,022] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-31 18:44:09,022] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-31 18:44:09,022] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-31 18:44:09,022] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-31 18:44:09,022] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-31 18:44:09,022] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-31 18:44:09,022] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-31 18:44:09,022] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-31 18:44:09,023] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-31 18:44:09,023] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-31 18:44:09,023] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-31 18:44:09,023] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-31 18:44:09,023] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-31 18:44:09,023] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-31 18:44:09,023] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-31 18:44:09,023] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-31 18:44:09,023] [    INFO][0m - seed                          :42[0m
[32m[2022-08-31 18:44:09,023] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-31 18:44:09,023] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-31 18:44:09,023] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-31 18:44:09,023] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-31 18:44:09,023] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-31 18:44:09,024] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-31 18:44:09,024] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-31 18:44:09,024] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-31 18:44:09,024] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-31 18:44:09,024] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-31 18:44:09,024] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-31 18:44:09,024] [    INFO][0m - [0m
[32m[2022-08-31 18:44:09,026] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-31 18:44:09,026] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:44:09,026] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-08-31 18:44:09,026] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-31 18:44:09,026] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-31 18:44:09,026] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-31 18:44:09,026] [    INFO][0m -   Total optimization steps = 1000.0[0m
[32m[2022-08-31 18:44:09,026] [    INFO][0m -   Total num train samples = 8000[0m
[32m[2022-08-31 18:44:12,003] [    INFO][0m - loss: 0.75810184, learning_rate: 2.97e-05, global_step: 10, interval_runtime: 2.9762, interval_samples_per_second: 2.688, interval_steps_per_second: 3.36, epoch: 0.5[0m
[32m[2022-08-31 18:44:13,954] [    INFO][0m - loss: 0.69854498, learning_rate: 2.94e-05, global_step: 20, interval_runtime: 1.9509, interval_samples_per_second: 4.101, interval_steps_per_second: 5.126, epoch: 1.0[0m
[32m[2022-08-31 18:44:16,019] [    INFO][0m - loss: 0.65995283, learning_rate: 2.91e-05, global_step: 30, interval_runtime: 2.064, interval_samples_per_second: 3.876, interval_steps_per_second: 4.845, epoch: 1.5[0m
[32m[2022-08-31 18:44:17,984] [    INFO][0m - loss: 0.66983986, learning_rate: 2.88e-05, global_step: 40, interval_runtime: 1.9653, interval_samples_per_second: 4.071, interval_steps_per_second: 5.088, epoch: 2.0[0m
[32m[2022-08-31 18:44:20,064] [    INFO][0m - loss: 0.57840462, learning_rate: 2.8499999999999998e-05, global_step: 50, interval_runtime: 2.0804, interval_samples_per_second: 3.845, interval_steps_per_second: 4.807, epoch: 2.5[0m
[32m[2022-08-31 18:44:22,027] [    INFO][0m - loss: 0.59338059, learning_rate: 2.8199999999999998e-05, global_step: 60, interval_runtime: 1.9628, interval_samples_per_second: 4.076, interval_steps_per_second: 5.095, epoch: 3.0[0m
[32m[2022-08-31 18:44:24,108] [    INFO][0m - loss: 0.3653204, learning_rate: 2.79e-05, global_step: 70, interval_runtime: 2.0808, interval_samples_per_second: 3.845, interval_steps_per_second: 4.806, epoch: 3.5[0m
[32m[2022-08-31 18:44:26,068] [    INFO][0m - loss: 0.41133595, learning_rate: 2.7600000000000003e-05, global_step: 80, interval_runtime: 1.9602, interval_samples_per_second: 4.081, interval_steps_per_second: 5.102, epoch: 4.0[0m
[32m[2022-08-31 18:44:28,143] [    INFO][0m - loss: 0.36328001, learning_rate: 2.7300000000000003e-05, global_step: 90, interval_runtime: 2.0751, interval_samples_per_second: 3.855, interval_steps_per_second: 4.819, epoch: 4.5[0m
[32m[2022-08-31 18:44:30,099] [    INFO][0m - loss: 0.08592048, learning_rate: 2.7000000000000002e-05, global_step: 100, interval_runtime: 1.9554, interval_samples_per_second: 4.091, interval_steps_per_second: 5.114, epoch: 5.0[0m
[32m[2022-08-31 18:44:30,099] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:44:30,100] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:44:30,100] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:44:30,100] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:44:30,100] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:44:31,901] [    INFO][0m - eval_loss: 2.3984477519989014, eval_accuracy: 0.5875, eval_runtime: 1.8012, eval_samples_per_second: 88.829, eval_steps_per_second: 2.776, epoch: 5.0[0m
[32m[2022-08-31 18:44:31,902] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-31 18:44:31,902] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:44:35,779] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-31 18:44:35,780] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-31 18:44:44,035] [    INFO][0m - loss: 0.18405993, learning_rate: 2.6700000000000002e-05, global_step: 110, interval_runtime: 13.9364, interval_samples_per_second: 0.574, interval_steps_per_second: 0.718, epoch: 5.5[0m
[32m[2022-08-31 18:44:45,989] [    INFO][0m - loss: 0.40155535, learning_rate: 2.64e-05, global_step: 120, interval_runtime: 1.9539, interval_samples_per_second: 4.094, interval_steps_per_second: 5.118, epoch: 6.0[0m
[32m[2022-08-31 18:44:48,062] [    INFO][0m - loss: 0.25767438, learning_rate: 2.61e-05, global_step: 130, interval_runtime: 2.0732, interval_samples_per_second: 3.859, interval_steps_per_second: 4.824, epoch: 6.5[0m
[32m[2022-08-31 18:44:50,028] [    INFO][0m - loss: 0.18071847, learning_rate: 2.58e-05, global_step: 140, interval_runtime: 1.9663, interval_samples_per_second: 4.069, interval_steps_per_second: 5.086, epoch: 7.0[0m
[32m[2022-08-31 18:44:52,108] [    INFO][0m - loss: 0.17497184, learning_rate: 2.55e-05, global_step: 150, interval_runtime: 2.0797, interval_samples_per_second: 3.847, interval_steps_per_second: 4.808, epoch: 7.5[0m
[32m[2022-08-31 18:44:54,070] [    INFO][0m - loss: 0.02216291, learning_rate: 2.52e-05, global_step: 160, interval_runtime: 1.9618, interval_samples_per_second: 4.078, interval_steps_per_second: 5.097, epoch: 8.0[0m
[32m[2022-08-31 18:44:56,157] [    INFO][0m - loss: 0.044032, learning_rate: 2.49e-05, global_step: 170, interval_runtime: 2.0872, interval_samples_per_second: 3.833, interval_steps_per_second: 4.791, epoch: 8.5[0m
[32m[2022-08-31 18:44:58,111] [    INFO][0m - loss: 0.104283, learning_rate: 2.4599999999999998e-05, global_step: 180, interval_runtime: 1.9539, interval_samples_per_second: 4.094, interval_steps_per_second: 5.118, epoch: 9.0[0m
[32m[2022-08-31 18:45:00,184] [    INFO][0m - loss: 0.09479325, learning_rate: 2.43e-05, global_step: 190, interval_runtime: 2.0728, interval_samples_per_second: 3.86, interval_steps_per_second: 4.824, epoch: 9.5[0m
[32m[2022-08-31 18:45:02,143] [    INFO][0m - loss: 0.1248167, learning_rate: 2.4e-05, global_step: 200, interval_runtime: 1.9591, interval_samples_per_second: 4.084, interval_steps_per_second: 5.104, epoch: 10.0[0m
[32m[2022-08-31 18:45:02,143] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:45:02,143] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:45:02,143] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:45:02,144] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:45:02,144] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:45:03,960] [    INFO][0m - eval_loss: 5.639901638031006, eval_accuracy: 0.60625, eval_runtime: 1.8158, eval_samples_per_second: 88.117, eval_steps_per_second: 2.754, epoch: 10.0[0m
[32m[2022-08-31 18:45:03,960] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-31 18:45:03,960] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:45:08,771] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-31 18:45:08,771] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-31 18:45:16,199] [    INFO][0m - loss: 0.08342866, learning_rate: 2.37e-05, global_step: 210, interval_runtime: 14.056, interval_samples_per_second: 0.569, interval_steps_per_second: 0.711, epoch: 10.5[0m
[32m[2022-08-31 18:45:18,154] [    INFO][0m - loss: 0.04126431, learning_rate: 2.3400000000000003e-05, global_step: 220, interval_runtime: 1.9549, interval_samples_per_second: 4.092, interval_steps_per_second: 5.115, epoch: 11.0[0m
[32m[2022-08-31 18:45:20,225] [    INFO][0m - loss: 0.02889044, learning_rate: 2.3100000000000002e-05, global_step: 230, interval_runtime: 2.0711, interval_samples_per_second: 3.863, interval_steps_per_second: 4.828, epoch: 11.5[0m
[32m[2022-08-31 18:45:22,176] [    INFO][0m - loss: 0.00027981, learning_rate: 2.2800000000000002e-05, global_step: 240, interval_runtime: 1.9512, interval_samples_per_second: 4.1, interval_steps_per_second: 5.125, epoch: 12.0[0m
[32m[2022-08-31 18:45:24,258] [    INFO][0m - loss: 0.05938932, learning_rate: 2.25e-05, global_step: 250, interval_runtime: 2.0811, interval_samples_per_second: 3.844, interval_steps_per_second: 4.805, epoch: 12.5[0m
[32m[2022-08-31 18:45:26,247] [    INFO][0m - loss: 0.01742443, learning_rate: 2.22e-05, global_step: 260, interval_runtime: 1.99, interval_samples_per_second: 4.02, interval_steps_per_second: 5.025, epoch: 13.0[0m
[32m[2022-08-31 18:45:28,340] [    INFO][0m - loss: 0.0004264, learning_rate: 2.19e-05, global_step: 270, interval_runtime: 2.0919, interval_samples_per_second: 3.824, interval_steps_per_second: 4.78, epoch: 13.5[0m
[32m[2022-08-31 18:45:30,310] [    INFO][0m - loss: 0.13105872, learning_rate: 2.16e-05, global_step: 280, interval_runtime: 1.9704, interval_samples_per_second: 4.06, interval_steps_per_second: 5.075, epoch: 14.0[0m
[32m[2022-08-31 18:45:32,401] [    INFO][0m - loss: 3.743e-05, learning_rate: 2.13e-05, global_step: 290, interval_runtime: 2.0913, interval_samples_per_second: 3.825, interval_steps_per_second: 4.782, epoch: 14.5[0m
[32m[2022-08-31 18:45:34,369] [    INFO][0m - loss: 0.00432376, learning_rate: 2.1e-05, global_step: 300, interval_runtime: 1.9677, interval_samples_per_second: 4.066, interval_steps_per_second: 5.082, epoch: 15.0[0m
[32m[2022-08-31 18:45:34,370] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:45:34,370] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:45:34,370] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:45:34,370] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:45:34,370] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:45:36,186] [    INFO][0m - eval_loss: 5.46368408203125, eval_accuracy: 0.575, eval_runtime: 1.815, eval_samples_per_second: 88.156, eval_steps_per_second: 2.755, epoch: 15.0[0m
[32m[2022-08-31 18:45:36,186] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-31 18:45:36,187] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:45:39,671] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-31 18:45:39,671] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-31 18:45:48,272] [    INFO][0m - loss: 0.01208965, learning_rate: 2.07e-05, global_step: 310, interval_runtime: 13.9031, interval_samples_per_second: 0.575, interval_steps_per_second: 0.719, epoch: 15.5[0m
[32m[2022-08-31 18:45:50,215] [    INFO][0m - loss: 1.394e-05, learning_rate: 2.04e-05, global_step: 320, interval_runtime: 1.944, interval_samples_per_second: 4.115, interval_steps_per_second: 5.144, epoch: 16.0[0m
[32m[2022-08-31 18:45:52,312] [    INFO][0m - loss: 0.04111186, learning_rate: 2.01e-05, global_step: 330, interval_runtime: 2.0958, interval_samples_per_second: 3.817, interval_steps_per_second: 4.771, epoch: 16.5[0m
[32m[2022-08-31 18:45:54,268] [    INFO][0m - loss: 0.0034791, learning_rate: 1.98e-05, global_step: 340, interval_runtime: 1.9561, interval_samples_per_second: 4.09, interval_steps_per_second: 5.112, epoch: 17.0[0m
[32m[2022-08-31 18:45:56,334] [    INFO][0m - loss: 0.07760232, learning_rate: 1.95e-05, global_step: 350, interval_runtime: 2.0663, interval_samples_per_second: 3.872, interval_steps_per_second: 4.84, epoch: 17.5[0m
[32m[2022-08-31 18:45:58,294] [    INFO][0m - loss: 0.11652658, learning_rate: 1.9200000000000003e-05, global_step: 360, interval_runtime: 1.9601, interval_samples_per_second: 4.082, interval_steps_per_second: 5.102, epoch: 18.0[0m
[32m[2022-08-31 18:46:00,388] [    INFO][0m - loss: 0.00080995, learning_rate: 1.8900000000000002e-05, global_step: 370, interval_runtime: 2.0934, interval_samples_per_second: 3.821, interval_steps_per_second: 4.777, epoch: 18.5[0m
[32m[2022-08-31 18:46:02,341] [    INFO][0m - loss: 0.01489812, learning_rate: 1.86e-05, global_step: 380, interval_runtime: 1.9534, interval_samples_per_second: 4.095, interval_steps_per_second: 5.119, epoch: 19.0[0m
[32m[2022-08-31 18:46:04,427] [    INFO][0m - loss: 0.0062231, learning_rate: 1.83e-05, global_step: 390, interval_runtime: 2.0672, interval_samples_per_second: 3.87, interval_steps_per_second: 4.837, epoch: 19.5[0m
[32m[2022-08-31 18:46:06,381] [    INFO][0m - loss: 0.01938867, learning_rate: 1.8e-05, global_step: 400, interval_runtime: 1.9731, interval_samples_per_second: 4.054, interval_steps_per_second: 5.068, epoch: 20.0[0m
[32m[2022-08-31 18:46:06,382] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:46:06,382] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:46:06,382] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:46:06,382] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:46:06,382] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:46:08,191] [    INFO][0m - eval_loss: 5.00430154800415, eval_accuracy: 0.575, eval_runtime: 1.808, eval_samples_per_second: 88.493, eval_steps_per_second: 2.765, epoch: 20.0[0m
[32m[2022-08-31 18:46:08,192] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-31 18:46:08,193] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:46:11,611] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-31 18:46:11,612] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-31 18:46:20,371] [    INFO][0m - loss: 7.72e-06, learning_rate: 1.77e-05, global_step: 410, interval_runtime: 13.9902, interval_samples_per_second: 0.572, interval_steps_per_second: 0.715, epoch: 20.5[0m
[32m[2022-08-31 18:46:22,327] [    INFO][0m - loss: 7.7e-06, learning_rate: 1.74e-05, global_step: 420, interval_runtime: 1.9561, interval_samples_per_second: 4.09, interval_steps_per_second: 5.112, epoch: 21.0[0m
[32m[2022-08-31 18:46:24,404] [    INFO][0m - loss: 0.01242496, learning_rate: 1.71e-05, global_step: 430, interval_runtime: 2.0767, interval_samples_per_second: 3.852, interval_steps_per_second: 4.815, epoch: 21.5[0m
[32m[2022-08-31 18:46:26,377] [    INFO][0m - loss: 2.378e-05, learning_rate: 1.6800000000000002e-05, global_step: 440, interval_runtime: 1.9723, interval_samples_per_second: 4.056, interval_steps_per_second: 5.07, epoch: 22.0[0m
[32m[2022-08-31 18:46:28,458] [    INFO][0m - loss: 0.00387159, learning_rate: 1.65e-05, global_step: 450, interval_runtime: 2.0816, interval_samples_per_second: 3.843, interval_steps_per_second: 4.804, epoch: 22.5[0m
[32m[2022-08-31 18:46:30,430] [    INFO][0m - loss: 7.38e-06, learning_rate: 1.62e-05, global_step: 460, interval_runtime: 1.9716, interval_samples_per_second: 4.058, interval_steps_per_second: 5.072, epoch: 23.0[0m
[32m[2022-08-31 18:46:32,505] [    INFO][0m - loss: 0.00236035, learning_rate: 1.59e-05, global_step: 470, interval_runtime: 2.0749, interval_samples_per_second: 3.856, interval_steps_per_second: 4.82, epoch: 23.5[0m
[32m[2022-08-31 18:46:34,459] [    INFO][0m - loss: 0.07348896, learning_rate: 1.56e-05, global_step: 480, interval_runtime: 1.9545, interval_samples_per_second: 4.093, interval_steps_per_second: 5.116, epoch: 24.0[0m
[32m[2022-08-31 18:46:36,568] [    INFO][0m - loss: 0.11594906, learning_rate: 1.53e-05, global_step: 490, interval_runtime: 2.1092, interval_samples_per_second: 3.793, interval_steps_per_second: 4.741, epoch: 24.5[0m
[32m[2022-08-31 18:46:38,539] [    INFO][0m - loss: 0.0333353, learning_rate: 1.5e-05, global_step: 500, interval_runtime: 1.9702, interval_samples_per_second: 4.06, interval_steps_per_second: 5.076, epoch: 25.0[0m
[32m[2022-08-31 18:46:38,540] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:46:38,540] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:46:38,540] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:46:38,540] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:46:38,540] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:46:40,338] [    INFO][0m - eval_loss: 5.163125991821289, eval_accuracy: 0.6125, eval_runtime: 1.7983, eval_samples_per_second: 88.975, eval_steps_per_second: 2.78, epoch: 25.0[0m
[32m[2022-08-31 18:46:40,339] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-08-31 18:46:40,339] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:46:45,196] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-08-31 18:46:45,511] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-08-31 18:46:54,383] [    INFO][0m - loss: 0.01732189, learning_rate: 1.47e-05, global_step: 510, interval_runtime: 15.8448, interval_samples_per_second: 0.505, interval_steps_per_second: 0.631, epoch: 25.5[0m
[32m[2022-08-31 18:46:56,327] [    INFO][0m - loss: 0.00010093, learning_rate: 1.44e-05, global_step: 520, interval_runtime: 1.9438, interval_samples_per_second: 4.116, interval_steps_per_second: 5.145, epoch: 26.0[0m
[32m[2022-08-31 18:46:58,400] [    INFO][0m - loss: 6.53e-06, learning_rate: 1.4099999999999999e-05, global_step: 530, interval_runtime: 2.0729, interval_samples_per_second: 3.859, interval_steps_per_second: 4.824, epoch: 26.5[0m
[32m[2022-08-31 18:47:00,357] [    INFO][0m - loss: 0.12095721, learning_rate: 1.3800000000000002e-05, global_step: 540, interval_runtime: 1.9573, interval_samples_per_second: 4.087, interval_steps_per_second: 5.109, epoch: 27.0[0m
[32m[2022-08-31 18:47:02,412] [    INFO][0m - loss: 5.051e-05, learning_rate: 1.3500000000000001e-05, global_step: 550, interval_runtime: 2.0543, interval_samples_per_second: 3.894, interval_steps_per_second: 4.868, epoch: 27.5[0m
[32m[2022-08-31 18:47:04,361] [    INFO][0m - loss: 0.11686832, learning_rate: 1.32e-05, global_step: 560, interval_runtime: 1.9494, interval_samples_per_second: 4.104, interval_steps_per_second: 5.13, epoch: 28.0[0m
[32m[2022-08-31 18:47:06,424] [    INFO][0m - loss: 0.00065863, learning_rate: 1.29e-05, global_step: 570, interval_runtime: 2.0632, interval_samples_per_second: 3.878, interval_steps_per_second: 4.847, epoch: 28.5[0m
[32m[2022-08-31 18:47:08,383] [    INFO][0m - loss: 0.00013567, learning_rate: 1.26e-05, global_step: 580, interval_runtime: 1.9591, interval_samples_per_second: 4.084, interval_steps_per_second: 5.104, epoch: 29.0[0m
[32m[2022-08-31 18:47:10,471] [    INFO][0m - loss: 1.096e-05, learning_rate: 1.2299999999999999e-05, global_step: 590, interval_runtime: 2.0876, interval_samples_per_second: 3.832, interval_steps_per_second: 4.79, epoch: 29.5[0m
[32m[2022-08-31 18:47:12,426] [    INFO][0m - loss: 1.131e-05, learning_rate: 1.2e-05, global_step: 600, interval_runtime: 1.9558, interval_samples_per_second: 4.09, interval_steps_per_second: 5.113, epoch: 30.0[0m
[32m[2022-08-31 18:47:12,427] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:47:12,427] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:47:12,427] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:47:12,427] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:47:12,427] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:47:14,227] [    INFO][0m - eval_loss: 4.850930213928223, eval_accuracy: 0.60625, eval_runtime: 1.7997, eval_samples_per_second: 88.903, eval_steps_per_second: 2.778, epoch: 30.0[0m
[32m[2022-08-31 18:47:14,228] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-08-31 18:47:14,228] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:47:19,130] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-08-31 18:47:19,131] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-08-31 18:47:26,826] [    INFO][0m - loss: 1.272e-05, learning_rate: 1.1700000000000001e-05, global_step: 610, interval_runtime: 14.3986, interval_samples_per_second: 0.556, interval_steps_per_second: 0.695, epoch: 30.5[0m
[32m[2022-08-31 18:47:28,780] [    INFO][0m - loss: 5.97e-06, learning_rate: 1.1400000000000001e-05, global_step: 620, interval_runtime: 1.955, interval_samples_per_second: 4.092, interval_steps_per_second: 5.115, epoch: 31.0[0m
[32m[2022-08-31 18:47:31,227] [    INFO][0m - loss: 2.682e-05, learning_rate: 1.11e-05, global_step: 630, interval_runtime: 2.0939, interval_samples_per_second: 3.821, interval_steps_per_second: 4.776, epoch: 31.5[0m
[32m[2022-08-31 18:47:33,188] [    INFO][0m - loss: 3.894e-05, learning_rate: 1.08e-05, global_step: 640, interval_runtime: 2.3138, interval_samples_per_second: 3.457, interval_steps_per_second: 4.322, epoch: 32.0[0m
[32m[2022-08-31 18:47:35,257] [    INFO][0m - loss: 0.00019234, learning_rate: 1.05e-05, global_step: 650, interval_runtime: 2.0691, interval_samples_per_second: 3.866, interval_steps_per_second: 4.833, epoch: 32.5[0m
[32m[2022-08-31 18:47:37,232] [    INFO][0m - loss: 6.53e-06, learning_rate: 1.02e-05, global_step: 660, interval_runtime: 1.9743, interval_samples_per_second: 4.052, interval_steps_per_second: 5.065, epoch: 33.0[0m
[32m[2022-08-31 18:47:39,321] [    INFO][0m - loss: 0.03322736, learning_rate: 9.9e-06, global_step: 670, interval_runtime: 2.0889, interval_samples_per_second: 3.83, interval_steps_per_second: 4.787, epoch: 33.5[0m
[32m[2022-08-31 18:47:41,281] [    INFO][0m - loss: 7.21e-06, learning_rate: 9.600000000000001e-06, global_step: 680, interval_runtime: 1.9601, interval_samples_per_second: 4.081, interval_steps_per_second: 5.102, epoch: 34.0[0m
[32m[2022-08-31 18:47:43,353] [    INFO][0m - loss: 0.15876729, learning_rate: 9.3e-06, global_step: 690, interval_runtime: 2.0728, interval_samples_per_second: 3.86, interval_steps_per_second: 4.824, epoch: 34.5[0m
[32m[2022-08-31 18:47:45,321] [    INFO][0m - loss: 4.458e-05, learning_rate: 9e-06, global_step: 700, interval_runtime: 1.9571, interval_samples_per_second: 4.088, interval_steps_per_second: 5.11, epoch: 35.0[0m
[32m[2022-08-31 18:47:45,322] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:47:45,322] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:47:45,322] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:47:45,322] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:47:45,322] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:47:47,167] [    INFO][0m - eval_loss: 5.140313148498535, eval_accuracy: 0.6375, eval_runtime: 1.8448, eval_samples_per_second: 86.729, eval_steps_per_second: 2.71, epoch: 35.0[0m
[32m[2022-08-31 18:47:47,168] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-700[0m
[32m[2022-08-31 18:47:47,168] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:47:50,269] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-700/tokenizer_config.json[0m
[32m[2022-08-31 18:47:50,269] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-700/special_tokens_map.json[0m
[32m[2022-08-31 18:47:57,627] [    INFO][0m - loss: 2.916e-05, learning_rate: 8.7e-06, global_step: 710, interval_runtime: 12.3156, interval_samples_per_second: 0.65, interval_steps_per_second: 0.812, epoch: 35.5[0m
[32m[2022-08-31 18:47:59,596] [    INFO][0m - loss: 3.668e-05, learning_rate: 8.400000000000001e-06, global_step: 720, interval_runtime: 1.9691, interval_samples_per_second: 4.063, interval_steps_per_second: 5.079, epoch: 36.0[0m
[32m[2022-08-31 18:48:01,675] [    INFO][0m - loss: 2.229e-05, learning_rate: 8.1e-06, global_step: 730, interval_runtime: 2.0796, interval_samples_per_second: 3.847, interval_steps_per_second: 4.809, epoch: 36.5[0m
[32m[2022-08-31 18:48:03,631] [    INFO][0m - loss: 1.634e-05, learning_rate: 7.8e-06, global_step: 740, interval_runtime: 1.9563, interval_samples_per_second: 4.089, interval_steps_per_second: 5.112, epoch: 37.0[0m
[32m[2022-08-31 18:48:05,714] [    INFO][0m - loss: 7.124e-05, learning_rate: 7.5e-06, global_step: 750, interval_runtime: 2.083, interval_samples_per_second: 3.841, interval_steps_per_second: 4.801, epoch: 37.5[0m
[32m[2022-08-31 18:48:07,675] [    INFO][0m - loss: 0.00050376, learning_rate: 7.2e-06, global_step: 760, interval_runtime: 1.9614, interval_samples_per_second: 4.079, interval_steps_per_second: 5.099, epoch: 38.0[0m
[32m[2022-08-31 18:48:09,748] [    INFO][0m - loss: 1.111e-05, learning_rate: 6.900000000000001e-06, global_step: 770, interval_runtime: 2.0729, interval_samples_per_second: 3.859, interval_steps_per_second: 4.824, epoch: 38.5[0m
[32m[2022-08-31 18:48:11,725] [    INFO][0m - loss: 8.48e-06, learning_rate: 6.6e-06, global_step: 780, interval_runtime: 1.9724, interval_samples_per_second: 4.056, interval_steps_per_second: 5.07, epoch: 39.0[0m
[32m[2022-08-31 18:48:13,843] [    INFO][0m - loss: 0.0001934, learning_rate: 6.3e-06, global_step: 790, interval_runtime: 2.1215, interval_samples_per_second: 3.771, interval_steps_per_second: 4.714, epoch: 39.5[0m
[32m[2022-08-31 18:48:15,822] [    INFO][0m - loss: 6.83e-06, learning_rate: 6e-06, global_step: 800, interval_runtime: 1.9792, interval_samples_per_second: 4.042, interval_steps_per_second: 5.053, epoch: 40.0[0m
[32m[2022-08-31 18:48:15,822] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:48:15,823] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:48:15,823] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:48:15,823] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:48:15,823] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:48:17,638] [    INFO][0m - eval_loss: 4.911840915679932, eval_accuracy: 0.6125, eval_runtime: 1.8146, eval_samples_per_second: 88.173, eval_steps_per_second: 2.755, epoch: 40.0[0m
[32m[2022-08-31 18:48:17,638] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-800[0m
[32m[2022-08-31 18:48:17,639] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:48:21,593] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-800/tokenizer_config.json[0m
[32m[2022-08-31 18:48:21,593] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-800/special_tokens_map.json[0m
[32m[2022-08-31 18:48:29,063] [    INFO][0m - loss: 7.69e-06, learning_rate: 5.7000000000000005e-06, global_step: 810, interval_runtime: 13.2411, interval_samples_per_second: 0.604, interval_steps_per_second: 0.755, epoch: 40.5[0m
[32m[2022-08-31 18:48:31,014] [    INFO][0m - loss: 1.001e-05, learning_rate: 5.4e-06, global_step: 820, interval_runtime: 1.9512, interval_samples_per_second: 4.1, interval_steps_per_second: 5.125, epoch: 41.0[0m
[32m[2022-08-31 18:48:33,100] [    INFO][0m - loss: 5.49e-06, learning_rate: 5.1e-06, global_step: 830, interval_runtime: 2.0862, interval_samples_per_second: 3.835, interval_steps_per_second: 4.793, epoch: 41.5[0m
[32m[2022-08-31 18:48:35,060] [    INFO][0m - loss: 2.072e-05, learning_rate: 4.800000000000001e-06, global_step: 840, interval_runtime: 1.9594, interval_samples_per_second: 4.083, interval_steps_per_second: 5.104, epoch: 42.0[0m
[32m[2022-08-31 18:48:37,132] [    INFO][0m - loss: 5.13e-06, learning_rate: 4.5e-06, global_step: 850, interval_runtime: 2.0721, interval_samples_per_second: 3.861, interval_steps_per_second: 4.826, epoch: 42.5[0m
[32m[2022-08-31 18:48:39,121] [    INFO][0m - loss: 7.68e-06, learning_rate: 4.2000000000000004e-06, global_step: 860, interval_runtime: 1.9886, interval_samples_per_second: 4.023, interval_steps_per_second: 5.029, epoch: 43.0[0m
[32m[2022-08-31 18:48:41,214] [    INFO][0m - loss: 9.94e-06, learning_rate: 3.9e-06, global_step: 870, interval_runtime: 2.0936, interval_samples_per_second: 3.821, interval_steps_per_second: 4.777, epoch: 43.5[0m
[32m[2022-08-31 18:48:43,171] [    INFO][0m - loss: 6.31e-06, learning_rate: 3.6e-06, global_step: 880, interval_runtime: 1.9569, interval_samples_per_second: 4.088, interval_steps_per_second: 5.11, epoch: 44.0[0m
[32m[2022-08-31 18:48:45,275] [    INFO][0m - loss: 9.23e-06, learning_rate: 3.3e-06, global_step: 890, interval_runtime: 2.1034, interval_samples_per_second: 3.803, interval_steps_per_second: 4.754, epoch: 44.5[0m
[32m[2022-08-31 18:48:47,237] [    INFO][0m - loss: 5.81e-06, learning_rate: 3e-06, global_step: 900, interval_runtime: 1.963, interval_samples_per_second: 4.075, interval_steps_per_second: 5.094, epoch: 45.0[0m
[32m[2022-08-31 18:48:47,238] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:48:47,238] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:48:47,238] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:48:47,238] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:48:47,239] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:48:49,962] [    INFO][0m - eval_loss: 5.053745269775391, eval_accuracy: 0.625, eval_runtime: 1.7599, eval_samples_per_second: 90.914, eval_steps_per_second: 2.841, epoch: 45.0[0m
[32m[2022-08-31 18:48:49,962] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-900[0m
[32m[2022-08-31 18:48:49,962] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:48:53,169] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-900/tokenizer_config.json[0m
[32m[2022-08-31 18:48:53,169] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-900/special_tokens_map.json[0m
[32m[2022-08-31 18:49:00,702] [    INFO][0m - loss: 4.63e-06, learning_rate: 2.7e-06, global_step: 910, interval_runtime: 13.4649, interval_samples_per_second: 0.594, interval_steps_per_second: 0.743, epoch: 45.5[0m
[32m[2022-08-31 18:49:02,647] [    INFO][0m - loss: 6.77e-06, learning_rate: 2.4000000000000003e-06, global_step: 920, interval_runtime: 1.9448, interval_samples_per_second: 4.113, interval_steps_per_second: 5.142, epoch: 46.0[0m
[32m[2022-08-31 18:49:04,746] [    INFO][0m - loss: 8.03e-06, learning_rate: 2.1000000000000002e-06, global_step: 930, interval_runtime: 2.0984, interval_samples_per_second: 3.812, interval_steps_per_second: 4.765, epoch: 46.5[0m
[32m[2022-08-31 18:49:06,703] [    INFO][0m - loss: 7.43e-06, learning_rate: 1.8e-06, global_step: 940, interval_runtime: 1.9578, interval_samples_per_second: 4.086, interval_steps_per_second: 5.108, epoch: 47.0[0m
[32m[2022-08-31 18:49:08,778] [    INFO][0m - loss: 6.01e-06, learning_rate: 1.5e-06, global_step: 950, interval_runtime: 2.0743, interval_samples_per_second: 3.857, interval_steps_per_second: 4.821, epoch: 47.5[0m
[32m[2022-08-31 18:49:10,732] [    INFO][0m - loss: 4.02e-06, learning_rate: 1.2000000000000002e-06, global_step: 960, interval_runtime: 1.9533, interval_samples_per_second: 4.096, interval_steps_per_second: 5.12, epoch: 48.0[0m
[32m[2022-08-31 18:49:12,857] [    INFO][0m - loss: 4.01e-06, learning_rate: 9e-07, global_step: 970, interval_runtime: 2.1253, interval_samples_per_second: 3.764, interval_steps_per_second: 4.705, epoch: 48.5[0m
[32m[2022-08-31 18:49:14,826] [    INFO][0m - loss: 7.38e-06, learning_rate: 6.000000000000001e-07, global_step: 980, interval_runtime: 1.9691, interval_samples_per_second: 4.063, interval_steps_per_second: 5.078, epoch: 49.0[0m
[32m[2022-08-31 18:49:16,904] [    INFO][0m - loss: 6.58e-06, learning_rate: 3.0000000000000004e-07, global_step: 990, interval_runtime: 2.079, interval_samples_per_second: 3.848, interval_steps_per_second: 4.81, epoch: 49.5[0m
[32m[2022-08-31 18:49:18,939] [    INFO][0m - loss: 7.85e-05, learning_rate: 0.0, global_step: 1000, interval_runtime: 1.9713, interval_samples_per_second: 4.058, interval_steps_per_second: 5.073, epoch: 50.0[0m
[32m[2022-08-31 18:49:18,940] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:49:18,941] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:49:18,941] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:49:18,941] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:49:18,941] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:49:20,735] [    INFO][0m - eval_loss: 5.074812412261963, eval_accuracy: 0.625, eval_runtime: 1.7936, eval_samples_per_second: 89.206, eval_steps_per_second: 2.788, epoch: 50.0[0m
[32m[2022-08-31 18:49:20,735] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1000[0m
[32m[2022-08-31 18:49:20,736] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:49:24,410] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1000/tokenizer_config.json[0m
[32m[2022-08-31 18:49:24,412] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1000/special_tokens_map.json[0m
[32m[2022-08-31 18:49:31,690] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-31 18:49:31,691] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-700 (score: 0.6375).[0m
[32m[2022-08-31 18:49:32,766] [    INFO][0m - train_runtime: 323.7384, train_samples_per_second: 24.711, train_steps_per_second: 3.089, train_loss: 0.08122865127877958, epoch: 50.0[0m
[32m[2022-08-31 18:49:32,822] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-31 18:49:32,822] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:49:36,456] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-31 18:49:36,456] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-31 18:49:36,459] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-31 18:49:36,459] [    INFO][0m -   epoch                    =       50.0[0m
[32m[2022-08-31 18:49:36,459] [    INFO][0m -   train_loss               =     0.0812[0m
[32m[2022-08-31 18:49:36,459] [    INFO][0m -   train_runtime            = 0:05:23.73[0m
[32m[2022-08-31 18:49:36,459] [    INFO][0m -   train_samples_per_second =     24.711[0m
[32m[2022-08-31 18:49:36,459] [    INFO][0m -   train_steps_per_second   =      3.089[0m
[32m[2022-08-31 18:49:36,465] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-31 18:49:36,465] [    INFO][0m -   Num examples = 2838[0m
[32m[2022-08-31 18:49:36,465] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:49:36,465] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:49:36,465] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-08-31 18:50:08,625] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-31 18:50:08,626] [    INFO][0m -   test_accuracy           =     0.5962[0m
[32m[2022-08-31 18:50:08,626] [    INFO][0m -   test_loss               =     5.2356[0m
[32m[2022-08-31 18:50:08,626] [    INFO][0m -   test_runtime            = 0:00:32.16[0m
[32m[2022-08-31 18:50:08,626] [    INFO][0m -   test_samples_per_second =     88.246[0m
[32m[2022-08-31 18:50:08,626] [    INFO][0m -   test_steps_per_second   =      2.767[0m
[32m[2022-08-31 18:50:08,627] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-31 18:50:08,627] [    INFO][0m -   Num examples = 3000[0m
[32m[2022-08-31 18:50:08,627] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:50:08,627] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:50:08,627] [    INFO][0m -   Total prediction steps = 94[0m
[32m[2022-08-31 18:50:50,473] [    INFO][0m - Predictions for cslf saved to ./fewclue_submit_examples.[0m
[]
run.sh: line 75: --freeze_plm: command not found
 
==========
cluewsc
==========
 
[33m[2022-08-31 18:50:55,273] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-31 18:50:55,273] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-31 18:50:55,274] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 18:50:55,274] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-31 18:50:55,274] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 18:50:55,274] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-31 18:50:55,274] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-31 18:50:55,274] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-08-31 18:50:55,274] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-31 18:50:55,274] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-08-31 18:50:55,274] [    INFO][0m - [0m
[32m[2022-08-31 18:50:55,274] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 18:50:55,274] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-31 18:50:55,274] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 18:50:55,275] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-31 18:50:55,275] [    INFO][0m - prompt                        :{'text':'text_a'}{'hard':'ÂÖ∂‰∏≠‰ª£ËØç‰ΩøÁî®'}{'mask'}{'mask'}[0m
[32m[2022-08-31 18:50:55,275] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-31 18:50:55,275] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-31 18:50:55,275] [    INFO][0m - task_name                     :cluewsc[0m
[32m[2022-08-31 18:50:55,275] [    INFO][0m - [0m
[32m[2022-08-31 18:50:55,275] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0831 18:50:55.277221 76252 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0831 18:50:55.281574 76252 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-31 18:50:58,352] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-08-31 18:50:58,376] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-08-31 18:50:58,376] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[32m[2022-08-31 18:50:58,377] [    INFO][0m - Using template: [{'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': 'ÂÖ∂‰∏≠‰ª£ËØç‰ΩøÁî®'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}][0m
[32m[2022-08-31 18:50:58,379] [    INFO][0m - {'false': 0, 'true': 1}[0m
2022-08-31 18:50:58,381 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-31 18:50:59,433] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 18:50:59,433] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-31 18:50:59,433] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 18:50:59,433] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-31 18:50:59,433] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-31 18:50:59,433] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-31 18:50:59,433] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-31 18:50:59,433] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-31 18:50:59,433] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-31 18:50:59,434] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-31 18:50:59,434] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-31 18:50:59,434] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-31 18:50:59,434] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-31 18:50:59,434] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-31 18:50:59,434] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-31 18:50:59,434] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-31 18:50:59,434] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-31 18:50:59,434] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-31 18:50:59,434] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-31 18:50:59,434] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-31 18:50:59,434] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-31 18:50:59,434] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-31 18:50:59,435] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-31 18:50:59,435] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-31 18:50:59,435] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-31 18:50:59,435] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-31 18:50:59,435] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-31 18:50:59,435] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-31 18:50:59,435] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-31 18:50:59,435] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-31 18:50:59,435] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-31 18:50:59,435] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-31 18:50:59,435] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-31 18:50:59,435] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-31 18:50:59,435] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-31 18:50:59,435] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-31 18:50:59,436] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-31 18:50:59,436] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug31_18-50-55_instance-3bwob41y-01[0m
[32m[2022-08-31 18:50:59,436] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-31 18:50:59,436] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-31 18:50:59,436] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-31 18:50:59,436] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-31 18:50:59,436] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-31 18:50:59,436] [    INFO][0m - max_seq_length                :128[0m
[32m[2022-08-31 18:50:59,436] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-31 18:50:59,436] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-31 18:50:59,436] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-31 18:50:59,436] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-31 18:50:59,436] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-08-31 18:50:59,437] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-31 18:50:59,437] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-31 18:50:59,437] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-31 18:50:59,437] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-31 18:50:59,437] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-31 18:50:59,437] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-31 18:50:59,437] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-31 18:50:59,437] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-31 18:50:59,437] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-31 18:50:59,437] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-31 18:50:59,437] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-31 18:50:59,437] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-31 18:50:59,437] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-31 18:50:59,438] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-31 18:50:59,438] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-31 18:50:59,438] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-31 18:50:59,438] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-31 18:50:59,438] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-31 18:50:59,438] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-31 18:50:59,438] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-31 18:50:59,438] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-31 18:50:59,438] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-31 18:50:59,438] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-31 18:50:59,438] [    INFO][0m - seed                          :42[0m
[32m[2022-08-31 18:50:59,438] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-31 18:50:59,438] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-31 18:50:59,438] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-31 18:50:59,439] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-31 18:50:59,439] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-31 18:50:59,439] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-31 18:50:59,439] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-31 18:50:59,439] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-31 18:50:59,439] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-31 18:50:59,439] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-31 18:50:59,439] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-31 18:50:59,439] [    INFO][0m - [0m
[32m[2022-08-31 18:50:59,441] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-31 18:50:59,441] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 18:50:59,441] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-08-31 18:50:59,441] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-31 18:50:59,441] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-31 18:50:59,441] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-31 18:50:59,441] [    INFO][0m -   Total optimization steps = 1000.0[0m
[32m[2022-08-31 18:50:59,441] [    INFO][0m -   Total num train samples = 8000[0m
[32m[2022-08-31 18:51:01,344] [    INFO][0m - loss: 0.69999514, learning_rate: 2.97e-05, global_step: 10, interval_runtime: 1.9022, interval_samples_per_second: 4.206, interval_steps_per_second: 5.257, epoch: 0.5[0m
[32m[2022-08-31 18:51:02,235] [    INFO][0m - loss: 0.76244335, learning_rate: 2.94e-05, global_step: 20, interval_runtime: 0.8902, interval_samples_per_second: 8.987, interval_steps_per_second: 11.234, epoch: 1.0[0m
[32m[2022-08-31 18:51:03,177] [    INFO][0m - loss: 0.7152638, learning_rate: 2.91e-05, global_step: 30, interval_runtime: 0.9414, interval_samples_per_second: 8.498, interval_steps_per_second: 10.623, epoch: 1.5[0m
[32m[2022-08-31 18:51:04,072] [    INFO][0m - loss: 0.60849009, learning_rate: 2.88e-05, global_step: 40, interval_runtime: 0.8959, interval_samples_per_second: 8.93, interval_steps_per_second: 11.162, epoch: 2.0[0m
[32m[2022-08-31 18:51:05,019] [    INFO][0m - loss: 0.56594157, learning_rate: 2.8499999999999998e-05, global_step: 50, interval_runtime: 0.9474, interval_samples_per_second: 8.445, interval_steps_per_second: 10.556, epoch: 2.5[0m
[32m[2022-08-31 18:51:05,909] [    INFO][0m - loss: 0.42973285, learning_rate: 2.8199999999999998e-05, global_step: 60, interval_runtime: 0.8898, interval_samples_per_second: 8.991, interval_steps_per_second: 11.238, epoch: 3.0[0m
[32m[2022-08-31 18:51:06,877] [    INFO][0m - loss: 0.30315237, learning_rate: 2.79e-05, global_step: 70, interval_runtime: 0.9682, interval_samples_per_second: 8.262, interval_steps_per_second: 10.328, epoch: 3.5[0m
[32m[2022-08-31 18:51:07,780] [    INFO][0m - loss: 0.40406218, learning_rate: 2.7600000000000003e-05, global_step: 80, interval_runtime: 0.9025, interval_samples_per_second: 8.864, interval_steps_per_second: 11.08, epoch: 4.0[0m
[32m[2022-08-31 18:51:08,748] [    INFO][0m - loss: 0.25378532, learning_rate: 2.7300000000000003e-05, global_step: 90, interval_runtime: 0.9686, interval_samples_per_second: 8.259, interval_steps_per_second: 10.324, epoch: 4.5[0m
[32m[2022-08-31 18:51:09,639] [    INFO][0m - loss: 0.25192547, learning_rate: 2.7000000000000002e-05, global_step: 100, interval_runtime: 0.8908, interval_samples_per_second: 8.981, interval_steps_per_second: 11.226, epoch: 5.0[0m
[32m[2022-08-31 18:51:09,640] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:51:09,640] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-31 18:51:09,640] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:51:09,640] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:51:09,641] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:51:10,445] [    INFO][0m - eval_loss: 1.75977623462677, eval_accuracy: 0.5471698113207547, eval_runtime: 0.8045, eval_samples_per_second: 197.63, eval_steps_per_second: 6.215, epoch: 5.0[0m
[32m[2022-08-31 18:51:10,446] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-31 18:51:10,446] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:51:13,591] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-31 18:51:13,591] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-31 18:51:20,831] [    INFO][0m - loss: 0.31933727, learning_rate: 2.6700000000000002e-05, global_step: 110, interval_runtime: 11.1912, interval_samples_per_second: 0.715, interval_steps_per_second: 0.894, epoch: 5.5[0m
[32m[2022-08-31 18:51:21,725] [    INFO][0m - loss: 0.15948551, learning_rate: 2.64e-05, global_step: 120, interval_runtime: 0.8942, interval_samples_per_second: 8.946, interval_steps_per_second: 11.183, epoch: 6.0[0m
[32m[2022-08-31 18:51:22,672] [    INFO][0m - loss: 0.22003379, learning_rate: 2.61e-05, global_step: 130, interval_runtime: 0.9472, interval_samples_per_second: 8.446, interval_steps_per_second: 10.557, epoch: 6.5[0m
[32m[2022-08-31 18:51:23,569] [    INFO][0m - loss: 0.33015018, learning_rate: 2.58e-05, global_step: 140, interval_runtime: 0.8974, interval_samples_per_second: 8.914, interval_steps_per_second: 11.143, epoch: 7.0[0m
[32m[2022-08-31 18:51:24,566] [    INFO][0m - loss: 0.19703496, learning_rate: 2.55e-05, global_step: 150, interval_runtime: 0.9969, interval_samples_per_second: 8.025, interval_steps_per_second: 10.031, epoch: 7.5[0m
[32m[2022-08-31 18:51:25,461] [    INFO][0m - loss: 0.15101049, learning_rate: 2.52e-05, global_step: 160, interval_runtime: 0.8944, interval_samples_per_second: 8.944, interval_steps_per_second: 11.181, epoch: 8.0[0m
[32m[2022-08-31 18:51:26,404] [    INFO][0m - loss: 0.16055123, learning_rate: 2.49e-05, global_step: 170, interval_runtime: 0.9438, interval_samples_per_second: 8.476, interval_steps_per_second: 10.596, epoch: 8.5[0m
[32m[2022-08-31 18:51:27,299] [    INFO][0m - loss: 0.24746401, learning_rate: 2.4599999999999998e-05, global_step: 180, interval_runtime: 0.8942, interval_samples_per_second: 8.947, interval_steps_per_second: 11.183, epoch: 9.0[0m
[32m[2022-08-31 18:51:28,303] [    INFO][0m - loss: 0.17254958, learning_rate: 2.43e-05, global_step: 190, interval_runtime: 1.0041, interval_samples_per_second: 7.967, interval_steps_per_second: 9.959, epoch: 9.5[0m
[32m[2022-08-31 18:51:29,195] [    INFO][0m - loss: 0.22302911, learning_rate: 2.4e-05, global_step: 200, interval_runtime: 0.8921, interval_samples_per_second: 8.967, interval_steps_per_second: 11.209, epoch: 10.0[0m
[32m[2022-08-31 18:51:29,195] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:51:29,195] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-31 18:51:29,196] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:51:29,196] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:51:29,196] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:51:29,887] [    INFO][0m - eval_loss: 2.5301105976104736, eval_accuracy: 0.5031446540880503, eval_runtime: 0.6913, eval_samples_per_second: 230.006, eval_steps_per_second: 7.233, epoch: 10.0[0m
[32m[2022-08-31 18:51:29,888] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-31 18:51:29,888] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:51:34,069] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-31 18:51:34,070] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-31 18:51:42,685] [    INFO][0m - loss: 0.1445245, learning_rate: 2.37e-05, global_step: 210, interval_runtime: 13.49, interval_samples_per_second: 0.593, interval_steps_per_second: 0.741, epoch: 10.5[0m
[32m[2022-08-31 18:51:43,587] [    INFO][0m - loss: 0.12833631, learning_rate: 2.3400000000000003e-05, global_step: 220, interval_runtime: 0.9024, interval_samples_per_second: 8.865, interval_steps_per_second: 11.082, epoch: 11.0[0m
[32m[2022-08-31 18:51:44,548] [    INFO][0m - loss: 0.02135051, learning_rate: 2.3100000000000002e-05, global_step: 230, interval_runtime: 0.9604, interval_samples_per_second: 8.33, interval_steps_per_second: 10.412, epoch: 11.5[0m
[32m[2022-08-31 18:51:45,442] [    INFO][0m - loss: 0.09701591, learning_rate: 2.2800000000000002e-05, global_step: 240, interval_runtime: 0.8942, interval_samples_per_second: 8.947, interval_steps_per_second: 11.183, epoch: 12.0[0m
[32m[2022-08-31 18:51:46,388] [    INFO][0m - loss: 0.35025036, learning_rate: 2.25e-05, global_step: 250, interval_runtime: 0.9457, interval_samples_per_second: 8.459, interval_steps_per_second: 10.574, epoch: 12.5[0m
[32m[2022-08-31 18:51:47,283] [    INFO][0m - loss: 0.27734632, learning_rate: 2.22e-05, global_step: 260, interval_runtime: 0.8951, interval_samples_per_second: 8.937, interval_steps_per_second: 11.171, epoch: 13.0[0m
[32m[2022-08-31 18:51:48,229] [    INFO][0m - loss: 0.10105042, learning_rate: 2.19e-05, global_step: 270, interval_runtime: 0.9459, interval_samples_per_second: 8.458, interval_steps_per_second: 10.572, epoch: 13.5[0m
[32m[2022-08-31 18:51:49,122] [    INFO][0m - loss: 0.1208755, learning_rate: 2.16e-05, global_step: 280, interval_runtime: 0.8931, interval_samples_per_second: 8.957, interval_steps_per_second: 11.196, epoch: 14.0[0m
[32m[2022-08-31 18:51:50,065] [    INFO][0m - loss: 0.05941354, learning_rate: 2.13e-05, global_step: 290, interval_runtime: 0.9429, interval_samples_per_second: 8.484, interval_steps_per_second: 10.606, epoch: 14.5[0m
[32m[2022-08-31 18:51:50,957] [    INFO][0m - loss: 0.03783453, learning_rate: 2.1e-05, global_step: 300, interval_runtime: 0.8919, interval_samples_per_second: 8.97, interval_steps_per_second: 11.212, epoch: 15.0[0m
[32m[2022-08-31 18:51:50,957] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:51:50,958] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-31 18:51:50,958] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:51:50,958] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:51:50,958] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:51:51,662] [    INFO][0m - eval_loss: 3.160464286804199, eval_accuracy: 0.5786163522012578, eval_runtime: 0.7039, eval_samples_per_second: 225.893, eval_steps_per_second: 7.104, epoch: 15.0[0m
[32m[2022-08-31 18:51:51,662] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-31 18:51:51,663] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:51:55,455] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-31 18:51:55,456] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-31 18:52:02,247] [    INFO][0m - loss: 0.00402651, learning_rate: 2.07e-05, global_step: 310, interval_runtime: 11.2903, interval_samples_per_second: 0.709, interval_steps_per_second: 0.886, epoch: 15.5[0m
[32m[2022-08-31 18:52:03,136] [    INFO][0m - loss: 0.05990899, learning_rate: 2.04e-05, global_step: 320, interval_runtime: 0.8894, interval_samples_per_second: 8.995, interval_steps_per_second: 11.244, epoch: 16.0[0m
[32m[2022-08-31 18:52:04,097] [    INFO][0m - loss: 1.606e-05, learning_rate: 2.01e-05, global_step: 330, interval_runtime: 0.9599, interval_samples_per_second: 8.334, interval_steps_per_second: 10.418, epoch: 16.5[0m
[32m[2022-08-31 18:52:04,993] [    INFO][0m - loss: 0.00093422, learning_rate: 1.98e-05, global_step: 340, interval_runtime: 0.8969, interval_samples_per_second: 8.919, interval_steps_per_second: 11.149, epoch: 17.0[0m
[32m[2022-08-31 18:52:05,949] [    INFO][0m - loss: 0.05474999, learning_rate: 1.95e-05, global_step: 350, interval_runtime: 0.9555, interval_samples_per_second: 8.373, interval_steps_per_second: 10.466, epoch: 17.5[0m
[32m[2022-08-31 18:52:07,107] [    INFO][0m - loss: 0.00497239, learning_rate: 1.9200000000000003e-05, global_step: 360, interval_runtime: 0.8923, interval_samples_per_second: 8.965, interval_steps_per_second: 11.207, epoch: 18.0[0m
[32m[2022-08-31 18:52:08,054] [    INFO][0m - loss: 7.134e-05, learning_rate: 1.8900000000000002e-05, global_step: 370, interval_runtime: 1.2125, interval_samples_per_second: 6.598, interval_steps_per_second: 8.247, epoch: 18.5[0m
[32m[2022-08-31 18:52:08,943] [    INFO][0m - loss: 0.03426317, learning_rate: 1.86e-05, global_step: 380, interval_runtime: 0.8888, interval_samples_per_second: 9.001, interval_steps_per_second: 11.251, epoch: 19.0[0m
[32m[2022-08-31 18:52:09,883] [    INFO][0m - loss: 0.06547521, learning_rate: 1.83e-05, global_step: 390, interval_runtime: 0.9403, interval_samples_per_second: 8.508, interval_steps_per_second: 10.635, epoch: 19.5[0m
[32m[2022-08-31 18:52:10,772] [    INFO][0m - loss: 3.6e-07, learning_rate: 1.8e-05, global_step: 400, interval_runtime: 0.8885, interval_samples_per_second: 9.004, interval_steps_per_second: 11.255, epoch: 20.0[0m
[32m[2022-08-31 18:52:10,772] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:52:10,772] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-31 18:52:10,772] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:52:10,772] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:52:10,772] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:52:11,475] [    INFO][0m - eval_loss: 4.577356338500977, eval_accuracy: 0.5408805031446541, eval_runtime: 0.7025, eval_samples_per_second: 226.322, eval_steps_per_second: 7.117, epoch: 20.0[0m
[32m[2022-08-31 18:52:11,476] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-31 18:52:11,476] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:52:14,573] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-31 18:52:14,574] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-31 18:52:22,102] [    INFO][0m - loss: 4.585e-05, learning_rate: 1.77e-05, global_step: 410, interval_runtime: 11.3304, interval_samples_per_second: 0.706, interval_steps_per_second: 0.883, epoch: 20.5[0m
[32m[2022-08-31 18:52:22,995] [    INFO][0m - loss: 0.10981491, learning_rate: 1.74e-05, global_step: 420, interval_runtime: 0.8934, interval_samples_per_second: 8.954, interval_steps_per_second: 11.193, epoch: 21.0[0m
[32m[2022-08-31 18:52:23,949] [    INFO][0m - loss: 0.00015144, learning_rate: 1.71e-05, global_step: 430, interval_runtime: 0.9535, interval_samples_per_second: 8.39, interval_steps_per_second: 10.487, epoch: 21.5[0m
[32m[2022-08-31 18:52:24,869] [    INFO][0m - loss: 7.05e-06, learning_rate: 1.6800000000000002e-05, global_step: 440, interval_runtime: 0.92, interval_samples_per_second: 8.696, interval_steps_per_second: 10.87, epoch: 22.0[0m
[32m[2022-08-31 18:52:25,825] [    INFO][0m - loss: 2.87e-06, learning_rate: 1.65e-05, global_step: 450, interval_runtime: 0.9561, interval_samples_per_second: 8.367, interval_steps_per_second: 10.459, epoch: 22.5[0m
[32m[2022-08-31 18:52:26,711] [    INFO][0m - loss: 0.00623313, learning_rate: 1.62e-05, global_step: 460, interval_runtime: 0.8862, interval_samples_per_second: 9.028, interval_steps_per_second: 11.285, epoch: 23.0[0m
[32m[2022-08-31 18:52:27,649] [    INFO][0m - loss: 5.81e-06, learning_rate: 1.59e-05, global_step: 470, interval_runtime: 0.9381, interval_samples_per_second: 8.528, interval_steps_per_second: 10.66, epoch: 23.5[0m
[32m[2022-08-31 18:52:28,549] [    INFO][0m - loss: 0.00046219, learning_rate: 1.56e-05, global_step: 480, interval_runtime: 0.8999, interval_samples_per_second: 8.89, interval_steps_per_second: 11.112, epoch: 24.0[0m
[32m[2022-08-31 18:52:29,507] [    INFO][0m - loss: 9.61e-05, learning_rate: 1.53e-05, global_step: 490, interval_runtime: 0.9579, interval_samples_per_second: 8.352, interval_steps_per_second: 10.44, epoch: 24.5[0m
[32m[2022-08-31 18:52:30,409] [    INFO][0m - loss: 3.691e-05, learning_rate: 1.5e-05, global_step: 500, interval_runtime: 0.9018, interval_samples_per_second: 8.871, interval_steps_per_second: 11.089, epoch: 25.0[0m
[32m[2022-08-31 18:52:30,410] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:52:30,410] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-31 18:52:30,410] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:52:30,410] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:52:30,410] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:52:31,108] [    INFO][0m - eval_loss: 4.866957187652588, eval_accuracy: 0.559748427672956, eval_runtime: 0.6977, eval_samples_per_second: 227.887, eval_steps_per_second: 7.166, epoch: 25.0[0m
[32m[2022-08-31 18:52:31,108] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-08-31 18:52:31,108] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:52:35,756] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-08-31 18:52:35,756] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-08-31 18:52:43,819] [    INFO][0m - loss: 0.08745863, learning_rate: 1.47e-05, global_step: 510, interval_runtime: 13.4103, interval_samples_per_second: 0.597, interval_steps_per_second: 0.746, epoch: 25.5[0m
[32m[2022-08-31 18:52:44,715] [    INFO][0m - loss: 0.12251722, learning_rate: 1.44e-05, global_step: 520, interval_runtime: 0.8956, interval_samples_per_second: 8.932, interval_steps_per_second: 11.165, epoch: 26.0[0m
[32m[2022-08-31 18:52:45,662] [    INFO][0m - loss: 1.076e-05, learning_rate: 1.4099999999999999e-05, global_step: 530, interval_runtime: 0.947, interval_samples_per_second: 8.448, interval_steps_per_second: 10.56, epoch: 26.5[0m
[32m[2022-08-31 18:52:46,553] [    INFO][0m - loss: 1.867e-05, learning_rate: 1.3800000000000002e-05, global_step: 540, interval_runtime: 0.8914, interval_samples_per_second: 8.975, interval_steps_per_second: 11.219, epoch: 27.0[0m
[32m[2022-08-31 18:52:47,523] [    INFO][0m - loss: 0.08112174, learning_rate: 1.3500000000000001e-05, global_step: 550, interval_runtime: 0.97, interval_samples_per_second: 8.247, interval_steps_per_second: 10.309, epoch: 27.5[0m
[32m[2022-08-31 18:52:48,415] [    INFO][0m - loss: 5.92e-06, learning_rate: 1.32e-05, global_step: 560, interval_runtime: 0.892, interval_samples_per_second: 8.969, interval_steps_per_second: 11.211, epoch: 28.0[0m
[32m[2022-08-31 18:52:49,408] [    INFO][0m - loss: 6.83e-06, learning_rate: 1.29e-05, global_step: 570, interval_runtime: 0.9924, interval_samples_per_second: 8.061, interval_steps_per_second: 10.076, epoch: 28.5[0m
[32m[2022-08-31 18:52:50,315] [    INFO][0m - loss: 2.21e-06, learning_rate: 1.26e-05, global_step: 580, interval_runtime: 0.9071, interval_samples_per_second: 8.82, interval_steps_per_second: 11.024, epoch: 29.0[0m
[32m[2022-08-31 18:52:51,263] [    INFO][0m - loss: 1.54e-06, learning_rate: 1.2299999999999999e-05, global_step: 590, interval_runtime: 0.9488, interval_samples_per_second: 8.432, interval_steps_per_second: 10.54, epoch: 29.5[0m
[32m[2022-08-31 18:52:52,165] [    INFO][0m - loss: 4.49e-06, learning_rate: 1.2e-05, global_step: 600, interval_runtime: 0.9018, interval_samples_per_second: 8.871, interval_steps_per_second: 11.089, epoch: 30.0[0m
[32m[2022-08-31 18:52:52,166] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:52:52,166] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-31 18:52:52,166] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:52:52,166] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:52:52,166] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:52:52,864] [    INFO][0m - eval_loss: 4.702962875366211, eval_accuracy: 0.5786163522012578, eval_runtime: 0.6975, eval_samples_per_second: 227.963, eval_steps_per_second: 7.169, epoch: 30.0[0m
[32m[2022-08-31 18:52:52,864] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-08-31 18:52:52,864] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:52:56,342] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-08-31 18:52:56,343] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-08-31 18:53:03,014] [    INFO][0m - loss: 2.25e-06, learning_rate: 1.1700000000000001e-05, global_step: 610, interval_runtime: 10.8482, interval_samples_per_second: 0.737, interval_steps_per_second: 0.922, epoch: 30.5[0m
[32m[2022-08-31 18:53:03,908] [    INFO][0m - loss: 2.16e-05, learning_rate: 1.1400000000000001e-05, global_step: 620, interval_runtime: 0.895, interval_samples_per_second: 8.939, interval_steps_per_second: 11.174, epoch: 31.0[0m
[32m[2022-08-31 18:53:04,859] [    INFO][0m - loss: 2.222e-05, learning_rate: 1.11e-05, global_step: 630, interval_runtime: 0.9505, interval_samples_per_second: 8.416, interval_steps_per_second: 10.52, epoch: 31.5[0m
[32m[2022-08-31 18:53:05,752] [    INFO][0m - loss: 9.831e-05, learning_rate: 1.08e-05, global_step: 640, interval_runtime: 0.8927, interval_samples_per_second: 8.962, interval_steps_per_second: 11.203, epoch: 32.0[0m
[32m[2022-08-31 18:53:06,714] [    INFO][0m - loss: 3.55e-06, learning_rate: 1.05e-05, global_step: 650, interval_runtime: 0.9619, interval_samples_per_second: 8.317, interval_steps_per_second: 10.396, epoch: 32.5[0m
[32m[2022-08-31 18:53:07,651] [    INFO][0m - loss: 1.7e-06, learning_rate: 1.02e-05, global_step: 660, interval_runtime: 0.9377, interval_samples_per_second: 8.532, interval_steps_per_second: 10.664, epoch: 33.0[0m
[32m[2022-08-31 18:53:08,616] [    INFO][0m - loss: 1.58e-06, learning_rate: 9.9e-06, global_step: 670, interval_runtime: 0.9649, interval_samples_per_second: 8.291, interval_steps_per_second: 10.363, epoch: 33.5[0m
[32m[2022-08-31 18:53:09,543] [    INFO][0m - loss: 8.977e-05, learning_rate: 9.600000000000001e-06, global_step: 680, interval_runtime: 0.9268, interval_samples_per_second: 8.632, interval_steps_per_second: 10.79, epoch: 34.0[0m
[32m[2022-08-31 18:53:10,516] [    INFO][0m - loss: 2.81e-06, learning_rate: 9.3e-06, global_step: 690, interval_runtime: 0.9734, interval_samples_per_second: 8.218, interval_steps_per_second: 10.273, epoch: 34.5[0m
[32m[2022-08-31 18:53:11,418] [    INFO][0m - loss: 5.24e-06, learning_rate: 9e-06, global_step: 700, interval_runtime: 0.9016, interval_samples_per_second: 8.873, interval_steps_per_second: 11.092, epoch: 35.0[0m
[32m[2022-08-31 18:53:11,419] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 18:53:11,419] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-31 18:53:11,419] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:53:11,419] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:53:11,419] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 18:53:12,124] [    INFO][0m - eval_loss: 5.579157829284668, eval_accuracy: 0.5471698113207547, eval_runtime: 0.7043, eval_samples_per_second: 225.768, eval_steps_per_second: 7.1, epoch: 35.0[0m
[32m[2022-08-31 18:53:12,124] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-700[0m
[32m[2022-08-31 18:53:12,124] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:53:17,633] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-700/tokenizer_config.json[0m
[32m[2022-08-31 18:53:17,634] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-700/special_tokens_map.json[0m
[32m[2022-08-31 18:53:28,048] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-31 18:53:30,616] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-300 (score: 0.5786163522012578).[0m
[32m[2022-08-31 18:53:31,810] [    INFO][0m - train_runtime: 152.3669, train_samples_per_second: 52.505, train_steps_per_second: 6.563, train_loss: 0.13065868131796085, epoch: 35.0[0m
[32m[2022-08-31 18:53:31,812] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-31 18:53:31,812] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 18:53:37,569] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-31 18:53:37,830] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-31 18:53:37,833] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-31 18:53:37,833] [    INFO][0m -   epoch                    =       35.0[0m
[32m[2022-08-31 18:53:37,833] [    INFO][0m -   train_loss               =     0.1307[0m
[32m[2022-08-31 18:53:37,833] [    INFO][0m -   train_runtime            = 0:02:32.36[0m
[32m[2022-08-31 18:53:37,834] [    INFO][0m -   train_samples_per_second =     52.505[0m
[32m[2022-08-31 18:53:37,834] [    INFO][0m -   train_steps_per_second   =      6.563[0m
[32m[2022-08-31 18:53:37,840] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-31 18:53:37,840] [    INFO][0m -   Num examples = 976[0m
[32m[2022-08-31 18:53:37,840] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:53:37,840] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:53:37,840] [    INFO][0m -   Total prediction steps = 31[0m
[32m[2022-08-31 18:53:42,219] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-31 18:53:42,220] [    INFO][0m -   test_accuracy           =      0.498[0m
[32m[2022-08-31 18:53:42,220] [    INFO][0m -   test_loss               =     4.7517[0m
[32m[2022-08-31 18:53:42,220] [    INFO][0m -   test_runtime            = 0:00:04.37[0m
[32m[2022-08-31 18:53:42,220] [    INFO][0m -   test_samples_per_second =    222.879[0m
[32m[2022-08-31 18:53:42,220] [    INFO][0m -   test_steps_per_second   =      7.079[0m
[32m[2022-08-31 18:53:42,221] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-31 18:53:42,221] [    INFO][0m -   Num examples = 290[0m
[32m[2022-08-31 18:53:42,221] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 18:53:42,221] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 18:53:42,221] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-08-31 18:53:43,775] [    INFO][0m - Predictions for cluewscf saved to ./fewclue_submit_examples.[0m
[]
run.sh: line 75: --freeze_plm: command not found
