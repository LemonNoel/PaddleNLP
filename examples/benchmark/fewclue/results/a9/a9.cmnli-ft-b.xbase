 
==========
ocnli
==========
 
[33m[2022-09-01 14:26:55,252] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-09-01 14:26:55,252] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-01 14:26:55,252] [    INFO][0m - ============================================================[0m
[32m[2022-09-01 14:26:55,253] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-01 14:26:55,253] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-01 14:26:55,253] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-01 14:26:55,253] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-01 14:26:55,253] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-01 14:26:55,253] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-01 14:26:55,253] [    INFO][0m - model_name_or_path            :ernie-3.0-xbase-zh[0m
[32m[2022-09-01 14:26:55,253] [    INFO][0m - [0m
[32m[2022-09-01 14:26:55,253] [    INFO][0m - ============================================================[0m
[32m[2022-09-01 14:26:55,253] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-01 14:26:55,253] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-01 14:26:55,253] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-01 14:26:55,254] [    INFO][0m - prompt                        :‚Äú{'text':'text_a'}‚ÄùÂíå‚Äú{'text':'text_b'}‚Äù‰πãÈó¥ÁöÑÈÄªËæëÂÖ≥Á≥ªÊòØ{'mask'}{'mask'}„ÄÇ[0m
[32m[2022-09-01 14:26:55,254] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-01 14:26:55,254] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-01 14:26:55,254] [    INFO][0m - task_name                     :ocnli[0m
[32m[2022-09-01 14:26:55,254] [    INFO][0m - [0m
[32m[2022-09-01 14:26:55,254] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/ernie_3.0_xbase_zh.pdparams[0m
W0901 14:26:55.255636 81675 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0901 14:26:55.259733 81675 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-01 14:27:01,147] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/ernie_3.0_xbase_zh_vocab.txt[0m
[32m[2022-09-01 14:27:01,175] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/tokenizer_config.json[0m
[32m[2022-09-01 14:27:01,176] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/special_tokens_map.json[0m
[32m[2022-09-01 14:27:01,177] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùÂíå‚Äú'}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': '‚Äù‰πãÈó¥ÁöÑÈÄªËæëÂÖ≥Á≥ªÊòØ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': '„ÄÇ'}][0m
[32m[2022-09-01 14:27:01,180] [    INFO][0m - {'contradiction': 0, 'entailment': 1, 'neutral': 2}[0m
2022-09-01 14:27:01,181 INFO [download.py:119] unique_endpoints {''}
[32m[2022-09-01 14:27:03,424] [    INFO][0m - ============================================================[0m
[32m[2022-09-01 14:27:03,425] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-01 14:27:03,425] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-01 14:27:03,425] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-01 14:27:03,425] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-01 14:27:03,425] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-01 14:27:03,425] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-01 14:27:03,425] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-01 14:27:03,426] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-01 14:27:03,426] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-01 14:27:03,426] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-01 14:27:03,426] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-01 14:27:03,426] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-01 14:27:03,426] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-01 14:27:03,426] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-01 14:27:03,426] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-01 14:27:03,426] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-01 14:27:03,426] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-01 14:27:03,426] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-09-01 14:27:03,426] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-09-01 14:27:03,426] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-09-01 14:27:03,427] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-01 14:27:03,427] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-01 14:27:03,427] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-01 14:27:03,427] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-01 14:27:03,427] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-01 14:27:03,427] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-01 14:27:03,427] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-01 14:27:03,427] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-01 14:27:03,427] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-01 14:27:03,427] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-09-01 14:27:03,427] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-01 14:27:03,427] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-01 14:27:03,427] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-01 14:27:03,427] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-01 14:27:03,428] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-01 14:27:03,428] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-01 14:27:03,428] [    INFO][0m - logging_dir                   :./checkpoints_iflytek/runs/Sep01_14-26-55_instance-3bwob41y-01[0m
[32m[2022-09-01 14:27:03,428] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-01 14:27:03,428] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-01 14:27:03,428] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-01 14:27:03,428] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-01 14:27:03,428] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-01 14:27:03,428] [    INFO][0m - max_seq_length                :128[0m
[32m[2022-09-01 14:27:03,428] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-01 14:27:03,428] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-01 14:27:03,428] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-01 14:27:03,428] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-01 14:27:03,429] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-09-01 14:27:03,429] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-01 14:27:03,429] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-01 14:27:03,429] [    INFO][0m - output_dir                    :./checkpoints_iflytek/[0m
[32m[2022-09-01 14:27:03,429] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-01 14:27:03,429] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-01 14:27:03,429] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-09-01 14:27:03,429] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-09-01 14:27:03,429] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-01 14:27:03,429] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-01 14:27:03,429] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-01 14:27:03,429] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-09-01 14:27:03,429] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-01 14:27:03,429] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-01 14:27:03,430] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-01 14:27:03,430] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-01 14:27:03,430] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-01 14:27:03,430] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-01 14:27:03,430] [    INFO][0m - run_name                      :./checkpoints_iflytek/[0m
[32m[2022-09-01 14:27:03,430] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-01 14:27:03,430] [    INFO][0m - save_steps                    :100[0m
[32m[2022-09-01 14:27:03,430] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-09-01 14:27:03,430] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-01 14:27:03,430] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-01 14:27:03,430] [    INFO][0m - seed                          :42[0m
[32m[2022-09-01 14:27:03,430] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-01 14:27:03,430] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-01 14:27:03,431] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-01 14:27:03,431] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-09-01 14:27:03,431] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-01 14:27:03,431] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-01 14:27:03,431] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-01 14:27:03,431] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-01 14:27:03,431] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-01 14:27:03,431] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-01 14:27:03,431] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-01 14:27:03,431] [    INFO][0m - [0m
[32m[2022-09-01 14:27:03,433] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-01 14:27:03,434] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-01 14:27:03,434] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-09-01 14:27:03,434] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-09-01 14:27:03,434] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-09-01 14:27:03,434] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-01 14:27:03,434] [    INFO][0m -   Total optimization steps = 1000.0[0m
[32m[2022-09-01 14:27:03,434] [    INFO][0m -   Total num train samples = 8000[0m
[32m[2022-09-01 14:27:06,735] [    INFO][0m - loss: 0.83209295, learning_rate: 2.97e-05, global_step: 10, interval_runtime: 3.2995, interval_samples_per_second: 2.425, interval_steps_per_second: 3.031, epoch: 0.5[0m
[32m[2022-09-01 14:27:08,714] [    INFO][0m - loss: 0.7374114, learning_rate: 2.94e-05, global_step: 20, interval_runtime: 1.9793, interval_samples_per_second: 4.042, interval_steps_per_second: 5.052, epoch: 1.0[0m
[32m[2022-09-01 14:27:10,746] [    INFO][0m - loss: 0.466961, learning_rate: 2.91e-05, global_step: 30, interval_runtime: 2.0322, interval_samples_per_second: 3.937, interval_steps_per_second: 4.921, epoch: 1.5[0m
[32m[2022-09-01 14:27:12,730] [    INFO][0m - loss: 0.24297466, learning_rate: 2.88e-05, global_step: 40, interval_runtime: 1.9841, interval_samples_per_second: 4.032, interval_steps_per_second: 5.04, epoch: 2.0[0m
[32m[2022-09-01 14:27:14,765] [    INFO][0m - loss: 0.14823012, learning_rate: 2.8499999999999998e-05, global_step: 50, interval_runtime: 2.0347, interval_samples_per_second: 3.932, interval_steps_per_second: 4.915, epoch: 2.5[0m
[32m[2022-09-01 14:27:16,760] [    INFO][0m - loss: 0.08428458, learning_rate: 2.8199999999999998e-05, global_step: 60, interval_runtime: 1.9944, interval_samples_per_second: 4.011, interval_steps_per_second: 5.014, epoch: 3.0[0m
[32m[2022-09-01 14:27:18,804] [    INFO][0m - loss: 0.07463234, learning_rate: 2.79e-05, global_step: 70, interval_runtime: 2.0446, interval_samples_per_second: 3.913, interval_steps_per_second: 4.891, epoch: 3.5[0m
[32m[2022-09-01 14:27:20,799] [    INFO][0m - loss: 0.06761032, learning_rate: 2.7600000000000003e-05, global_step: 80, interval_runtime: 1.994, interval_samples_per_second: 4.012, interval_steps_per_second: 5.015, epoch: 4.0[0m
[32m[2022-09-01 14:27:22,842] [    INFO][0m - loss: 0.10613631, learning_rate: 2.7300000000000003e-05, global_step: 90, interval_runtime: 2.0442, interval_samples_per_second: 3.914, interval_steps_per_second: 4.892, epoch: 4.5[0m
[32m[2022-09-01 14:27:24,928] [    INFO][0m - loss: 0.00247672, learning_rate: 2.7000000000000002e-05, global_step: 100, interval_runtime: 1.9877, interval_samples_per_second: 4.025, interval_steps_per_second: 5.031, epoch: 5.0[0m
[32m[2022-09-01 14:27:24,929] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 14:27:24,929] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-01 14:27:24,929] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 14:27:24,929] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 14:27:24,929] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-09-01 14:27:26,266] [    INFO][0m - eval_loss: 4.789215087890625, eval_accuracy: 0.7125, eval_runtime: 1.3361, eval_samples_per_second: 119.748, eval_steps_per_second: 3.742, epoch: 5.0[0m
[32m[2022-09-01 14:27:26,266] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-100[0m
[32m[2022-09-01 14:27:26,267] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 14:27:34,847] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-01 14:27:34,848] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-01 14:27:52,148] [    INFO][0m - loss: 0.01409877, learning_rate: 2.6700000000000002e-05, global_step: 110, interval_runtime: 27.3181, interval_samples_per_second: 0.293, interval_steps_per_second: 0.366, epoch: 5.5[0m
[32m[2022-09-01 14:27:54,137] [    INFO][0m - loss: 0.30620317, learning_rate: 2.64e-05, global_step: 120, interval_runtime: 1.9889, interval_samples_per_second: 4.022, interval_steps_per_second: 5.028, epoch: 6.0[0m
[32m[2022-09-01 14:27:56,177] [    INFO][0m - loss: 0.00119882, learning_rate: 2.61e-05, global_step: 130, interval_runtime: 2.0405, interval_samples_per_second: 3.921, interval_steps_per_second: 4.901, epoch: 6.5[0m
[32m[2022-09-01 14:27:58,168] [    INFO][0m - loss: 4.143e-05, learning_rate: 2.58e-05, global_step: 140, interval_runtime: 1.9912, interval_samples_per_second: 4.018, interval_steps_per_second: 5.022, epoch: 7.0[0m
[32m[2022-09-01 14:28:00,222] [    INFO][0m - loss: 7.43e-06, learning_rate: 2.55e-05, global_step: 150, interval_runtime: 2.0538, interval_samples_per_second: 3.895, interval_steps_per_second: 4.869, epoch: 7.5[0m
[32m[2022-09-01 14:28:02,231] [    INFO][0m - loss: 2.92e-06, learning_rate: 2.52e-05, global_step: 160, interval_runtime: 2.0087, interval_samples_per_second: 3.983, interval_steps_per_second: 4.978, epoch: 8.0[0m
[32m[2022-09-01 14:28:04,272] [    INFO][0m - loss: 0.00152276, learning_rate: 2.49e-05, global_step: 170, interval_runtime: 2.041, interval_samples_per_second: 3.92, interval_steps_per_second: 4.899, epoch: 8.5[0m
[32m[2022-09-01 14:28:06,255] [    INFO][0m - loss: 2.6e-07, learning_rate: 2.4599999999999998e-05, global_step: 180, interval_runtime: 1.9827, interval_samples_per_second: 4.035, interval_steps_per_second: 5.044, epoch: 9.0[0m
[32m[2022-09-01 14:28:08,292] [    INFO][0m - loss: 1.96e-06, learning_rate: 2.43e-05, global_step: 190, interval_runtime: 2.0372, interval_samples_per_second: 3.927, interval_steps_per_second: 4.909, epoch: 9.5[0m
[32m[2022-09-01 14:28:10,276] [    INFO][0m - loss: 1.26e-06, learning_rate: 2.4e-05, global_step: 200, interval_runtime: 1.9837, interval_samples_per_second: 4.033, interval_steps_per_second: 5.041, epoch: 10.0[0m
[32m[2022-09-01 14:28:10,277] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 14:28:10,277] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-01 14:28:10,277] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 14:28:10,277] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 14:28:10,277] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-09-01 14:28:11,607] [    INFO][0m - eval_loss: 4.515881061553955, eval_accuracy: 0.71875, eval_runtime: 1.33, eval_samples_per_second: 120.302, eval_steps_per_second: 3.759, epoch: 10.0[0m
[32m[2022-09-01 14:28:11,607] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-200[0m
[32m[2022-09-01 14:28:11,608] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 14:28:20,726] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-200/tokenizer_config.json[0m
[32m[2022-09-01 14:28:22,919] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-200/special_tokens_map.json[0m
[32m[2022-09-01 14:28:39,762] [    INFO][0m - loss: 2.48e-06, learning_rate: 2.37e-05, global_step: 210, interval_runtime: 29.4857, interval_samples_per_second: 0.271, interval_steps_per_second: 0.339, epoch: 10.5[0m
[32m[2022-09-01 14:28:41,739] [    INFO][0m - loss: 4.45e-06, learning_rate: 2.3400000000000003e-05, global_step: 220, interval_runtime: 1.9775, interval_samples_per_second: 4.046, interval_steps_per_second: 5.057, epoch: 11.0[0m
[32m[2022-09-01 14:28:43,781] [    INFO][0m - loss: 0.00014029, learning_rate: 2.3100000000000002e-05, global_step: 230, interval_runtime: 2.0421, interval_samples_per_second: 3.918, interval_steps_per_second: 4.897, epoch: 11.5[0m
[32m[2022-09-01 14:28:45,760] [    INFO][0m - loss: 7.16e-06, learning_rate: 2.2800000000000002e-05, global_step: 240, interval_runtime: 1.9789, interval_samples_per_second: 4.043, interval_steps_per_second: 5.053, epoch: 12.0[0m
[32m[2022-09-01 14:28:47,793] [    INFO][0m - loss: 1.18e-05, learning_rate: 2.25e-05, global_step: 250, interval_runtime: 2.0334, interval_samples_per_second: 3.934, interval_steps_per_second: 4.918, epoch: 12.5[0m
[32m[2022-09-01 14:28:49,871] [    INFO][0m - loss: 0.06882751, learning_rate: 2.22e-05, global_step: 260, interval_runtime: 1.9894, interval_samples_per_second: 4.021, interval_steps_per_second: 5.027, epoch: 13.0[0m
[32m[2022-09-01 14:28:51,910] [    INFO][0m - loss: 7.48e-06, learning_rate: 2.19e-05, global_step: 270, interval_runtime: 2.1276, interval_samples_per_second: 3.76, interval_steps_per_second: 4.7, epoch: 13.5[0m
[32m[2022-09-01 14:28:53,903] [    INFO][0m - loss: 0.19557358, learning_rate: 2.16e-05, global_step: 280, interval_runtime: 1.9924, interval_samples_per_second: 4.015, interval_steps_per_second: 5.019, epoch: 14.0[0m
[32m[2022-09-01 14:28:55,943] [    INFO][0m - loss: 0.00018372, learning_rate: 2.13e-05, global_step: 290, interval_runtime: 2.04, interval_samples_per_second: 3.922, interval_steps_per_second: 4.902, epoch: 14.5[0m
[32m[2022-09-01 14:28:57,939] [    INFO][0m - loss: 5.15e-06, learning_rate: 2.1e-05, global_step: 300, interval_runtime: 1.9965, interval_samples_per_second: 4.007, interval_steps_per_second: 5.009, epoch: 15.0[0m
[32m[2022-09-01 14:28:57,940] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 14:28:57,940] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-01 14:28:57,940] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 14:28:57,940] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 14:28:57,940] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-09-01 14:28:59,274] [    INFO][0m - eval_loss: 4.5718793869018555, eval_accuracy: 0.70625, eval_runtime: 1.3335, eval_samples_per_second: 119.982, eval_steps_per_second: 3.749, epoch: 15.0[0m
[32m[2022-09-01 14:28:59,274] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-300[0m
[32m[2022-09-01 14:28:59,275] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 14:29:06,848] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-300/tokenizer_config.json[0m
[32m[2022-09-01 14:29:07,130] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-300/special_tokens_map.json[0m
[32m[2022-09-01 14:29:29,511] [    INFO][0m - loss: 0.00069795, learning_rate: 2.07e-05, global_step: 310, interval_runtime: 31.5715, interval_samples_per_second: 0.253, interval_steps_per_second: 0.317, epoch: 15.5[0m
[32m[2022-09-01 14:29:31,490] [    INFO][0m - loss: 0.03527205, learning_rate: 2.04e-05, global_step: 320, interval_runtime: 1.9786, interval_samples_per_second: 4.043, interval_steps_per_second: 5.054, epoch: 16.0[0m
[32m[2022-09-01 14:29:33,521] [    INFO][0m - loss: 8.461e-05, learning_rate: 2.01e-05, global_step: 330, interval_runtime: 2.0314, interval_samples_per_second: 3.938, interval_steps_per_second: 4.923, epoch: 16.5[0m
[32m[2022-09-01 14:29:35,503] [    INFO][0m - loss: 0.09525796, learning_rate: 1.98e-05, global_step: 340, interval_runtime: 1.9823, interval_samples_per_second: 4.036, interval_steps_per_second: 5.045, epoch: 17.0[0m
[32m[2022-09-01 14:29:37,534] [    INFO][0m - loss: 6.48e-06, learning_rate: 1.95e-05, global_step: 350, interval_runtime: 2.0307, interval_samples_per_second: 3.94, interval_steps_per_second: 4.924, epoch: 17.5[0m
[32m[2022-09-01 14:29:39,520] [    INFO][0m - loss: 0.00011605, learning_rate: 1.9200000000000003e-05, global_step: 360, interval_runtime: 1.9861, interval_samples_per_second: 4.028, interval_steps_per_second: 5.035, epoch: 18.0[0m
[32m[2022-09-01 14:29:41,555] [    INFO][0m - loss: 7.8e-07, learning_rate: 1.8900000000000002e-05, global_step: 370, interval_runtime: 2.0353, interval_samples_per_second: 3.931, interval_steps_per_second: 4.913, epoch: 18.5[0m
[32m[2022-09-01 14:29:43,546] [    INFO][0m - loss: 5.3e-07, learning_rate: 1.86e-05, global_step: 380, interval_runtime: 1.9906, interval_samples_per_second: 4.019, interval_steps_per_second: 5.024, epoch: 19.0[0m
[32m[2022-09-01 14:29:45,578] [    INFO][0m - loss: 0.00169992, learning_rate: 1.83e-05, global_step: 390, interval_runtime: 2.0325, interval_samples_per_second: 3.936, interval_steps_per_second: 4.92, epoch: 19.5[0m
[32m[2022-09-01 14:29:47,566] [    INFO][0m - loss: 0.00089689, learning_rate: 1.8e-05, global_step: 400, interval_runtime: 1.9873, interval_samples_per_second: 4.026, interval_steps_per_second: 5.032, epoch: 20.0[0m
[32m[2022-09-01 14:29:47,566] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 14:29:47,566] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-01 14:29:47,566] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 14:29:47,566] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 14:29:47,566] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-09-01 14:29:48,899] [    INFO][0m - eval_loss: 4.0503249168396, eval_accuracy: 0.7375, eval_runtime: 1.3323, eval_samples_per_second: 120.094, eval_steps_per_second: 3.753, epoch: 20.0[0m
[32m[2022-09-01 14:29:48,899] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-400[0m
[32m[2022-09-01 14:29:48,899] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 14:29:56,964] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-400/tokenizer_config.json[0m
[32m[2022-09-01 14:29:56,965] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-400/special_tokens_map.json[0m
[32m[2022-09-01 14:30:13,968] [    INFO][0m - loss: 1.05e-06, learning_rate: 1.77e-05, global_step: 410, interval_runtime: 26.4006, interval_samples_per_second: 0.303, interval_steps_per_second: 0.379, epoch: 20.5[0m
[32m[2022-09-01 14:30:15,943] [    INFO][0m - loss: 1.83e-06, learning_rate: 1.74e-05, global_step: 420, interval_runtime: 1.9774, interval_samples_per_second: 4.046, interval_steps_per_second: 5.057, epoch: 21.0[0m
[32m[2022-09-01 14:30:17,971] [    INFO][0m - loss: 1.907e-05, learning_rate: 1.71e-05, global_step: 430, interval_runtime: 2.0272, interval_samples_per_second: 3.946, interval_steps_per_second: 4.933, epoch: 21.5[0m
[32m[2022-09-01 14:30:19,950] [    INFO][0m - loss: 3.5e-07, learning_rate: 1.6800000000000002e-05, global_step: 440, interval_runtime: 1.9791, interval_samples_per_second: 4.042, interval_steps_per_second: 5.053, epoch: 22.0[0m
[32m[2022-09-01 14:30:21,982] [    INFO][0m - loss: 2.59e-06, learning_rate: 1.65e-05, global_step: 450, interval_runtime: 2.0321, interval_samples_per_second: 3.937, interval_steps_per_second: 4.921, epoch: 22.5[0m
[32m[2022-09-01 14:30:23,962] [    INFO][0m - loss: 7.62e-06, learning_rate: 1.62e-05, global_step: 460, interval_runtime: 1.9803, interval_samples_per_second: 4.04, interval_steps_per_second: 5.05, epoch: 23.0[0m
[32m[2022-09-01 14:30:25,999] [    INFO][0m - loss: 0.00978884, learning_rate: 1.59e-05, global_step: 470, interval_runtime: 2.0363, interval_samples_per_second: 3.929, interval_steps_per_second: 4.911, epoch: 23.5[0m
[32m[2022-09-01 14:30:27,989] [    INFO][0m - loss: 7.5e-07, learning_rate: 1.56e-05, global_step: 480, interval_runtime: 1.9903, interval_samples_per_second: 4.02, interval_steps_per_second: 5.024, epoch: 24.0[0m
[32m[2022-09-01 14:30:30,020] [    INFO][0m - loss: 1.32e-06, learning_rate: 1.53e-05, global_step: 490, interval_runtime: 2.0307, interval_samples_per_second: 3.94, interval_steps_per_second: 4.924, epoch: 24.5[0m
[32m[2022-09-01 14:30:32,005] [    INFO][0m - loss: 5.8e-07, learning_rate: 1.5e-05, global_step: 500, interval_runtime: 1.9856, interval_samples_per_second: 4.029, interval_steps_per_second: 5.036, epoch: 25.0[0m
[32m[2022-09-01 14:30:32,006] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 14:30:32,006] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-01 14:30:32,006] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 14:30:32,006] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 14:30:32,006] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-09-01 14:30:33,332] [    INFO][0m - eval_loss: 4.516998291015625, eval_accuracy: 0.74375, eval_runtime: 1.3264, eval_samples_per_second: 120.632, eval_steps_per_second: 3.77, epoch: 25.0[0m
[32m[2022-09-01 14:30:33,333] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-500[0m
[32m[2022-09-01 14:30:33,333] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 14:30:40,998] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-500/tokenizer_config.json[0m
[32m[2022-09-01 14:30:40,999] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-500/special_tokens_map.json[0m
[32m[2022-09-01 14:30:57,554] [    INFO][0m - loss: 1.23e-06, learning_rate: 1.47e-05, global_step: 510, interval_runtime: 25.5485, interval_samples_per_second: 0.313, interval_steps_per_second: 0.391, epoch: 25.5[0m
[32m[2022-09-01 14:30:59,529] [    INFO][0m - loss: 1.893e-05, learning_rate: 1.44e-05, global_step: 520, interval_runtime: 1.975, interval_samples_per_second: 4.051, interval_steps_per_second: 5.063, epoch: 26.0[0m
[32m[2022-09-01 14:31:01,557] [    INFO][0m - loss: 3.3e-07, learning_rate: 1.4099999999999999e-05, global_step: 530, interval_runtime: 2.0279, interval_samples_per_second: 3.945, interval_steps_per_second: 4.931, epoch: 26.5[0m
[32m[2022-09-01 14:31:03,536] [    INFO][0m - loss: 4.4e-07, learning_rate: 1.3800000000000002e-05, global_step: 540, interval_runtime: 1.9798, interval_samples_per_second: 4.041, interval_steps_per_second: 5.051, epoch: 27.0[0m
[32m[2022-09-01 14:31:05,568] [    INFO][0m - loss: 2.66e-06, learning_rate: 1.3500000000000001e-05, global_step: 550, interval_runtime: 2.0315, interval_samples_per_second: 3.938, interval_steps_per_second: 4.922, epoch: 27.5[0m
[32m[2022-09-01 14:31:07,548] [    INFO][0m - loss: 0.05280114, learning_rate: 1.32e-05, global_step: 560, interval_runtime: 1.9797, interval_samples_per_second: 4.041, interval_steps_per_second: 5.051, epoch: 28.0[0m
[32m[2022-09-01 14:31:09,580] [    INFO][0m - loss: 0.00950993, learning_rate: 1.29e-05, global_step: 570, interval_runtime: 2.0326, interval_samples_per_second: 3.936, interval_steps_per_second: 4.92, epoch: 28.5[0m
[32m[2022-09-01 14:31:11,561] [    INFO][0m - loss: 2.7e-07, learning_rate: 1.26e-05, global_step: 580, interval_runtime: 1.9811, interval_samples_per_second: 4.038, interval_steps_per_second: 5.048, epoch: 29.0[0m
[32m[2022-09-01 14:31:13,593] [    INFO][0m - loss: 6e-07, learning_rate: 1.2299999999999999e-05, global_step: 590, interval_runtime: 2.0315, interval_samples_per_second: 3.938, interval_steps_per_second: 4.922, epoch: 29.5[0m
[32m[2022-09-01 14:31:15,575] [    INFO][0m - loss: 0.04435861, learning_rate: 1.2e-05, global_step: 600, interval_runtime: 1.9816, interval_samples_per_second: 4.037, interval_steps_per_second: 5.046, epoch: 30.0[0m
[32m[2022-09-01 14:31:15,575] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 14:31:15,575] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-01 14:31:15,575] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 14:31:15,575] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 14:31:15,575] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-09-01 14:31:16,903] [    INFO][0m - eval_loss: 4.725959777832031, eval_accuracy: 0.69375, eval_runtime: 1.3271, eval_samples_per_second: 120.568, eval_steps_per_second: 3.768, epoch: 30.0[0m
[32m[2022-09-01 14:31:16,903] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-600[0m
[32m[2022-09-01 14:31:16,903] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 14:31:24,752] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-600/tokenizer_config.json[0m
[32m[2022-09-01 14:31:24,753] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-600/special_tokens_map.json[0m
[32m[2022-09-01 14:31:41,664] [    INFO][0m - loss: 4.2e-07, learning_rate: 1.1700000000000001e-05, global_step: 610, interval_runtime: 26.089, interval_samples_per_second: 0.307, interval_steps_per_second: 0.383, epoch: 30.5[0m
[32m[2022-09-01 14:31:43,648] [    INFO][0m - loss: 0.00117342, learning_rate: 1.1400000000000001e-05, global_step: 620, interval_runtime: 1.9835, interval_samples_per_second: 4.033, interval_steps_per_second: 5.042, epoch: 31.0[0m
[32m[2022-09-01 14:31:45,690] [    INFO][0m - loss: 3.9e-06, learning_rate: 1.11e-05, global_step: 630, interval_runtime: 2.0422, interval_samples_per_second: 3.917, interval_steps_per_second: 4.897, epoch: 31.5[0m
[32m[2022-09-01 14:31:47,669] [    INFO][0m - loss: 0.01598526, learning_rate: 1.08e-05, global_step: 640, interval_runtime: 1.9797, interval_samples_per_second: 4.041, interval_steps_per_second: 5.051, epoch: 32.0[0m
[32m[2022-09-01 14:31:49,702] [    INFO][0m - loss: 7.8e-07, learning_rate: 1.05e-05, global_step: 650, interval_runtime: 2.0317, interval_samples_per_second: 3.938, interval_steps_per_second: 4.922, epoch: 32.5[0m
[32m[2022-09-01 14:31:51,688] [    INFO][0m - loss: 0.00190149, learning_rate: 1.02e-05, global_step: 660, interval_runtime: 1.987, interval_samples_per_second: 4.026, interval_steps_per_second: 5.033, epoch: 33.0[0m
[32m[2022-09-01 14:31:53,718] [    INFO][0m - loss: 1.15e-06, learning_rate: 9.9e-06, global_step: 670, interval_runtime: 2.0304, interval_samples_per_second: 3.94, interval_steps_per_second: 4.925, epoch: 33.5[0m
[32m[2022-09-01 14:31:55,698] [    INFO][0m - loss: 1.48e-06, learning_rate: 9.600000000000001e-06, global_step: 680, interval_runtime: 1.9793, interval_samples_per_second: 4.042, interval_steps_per_second: 5.052, epoch: 34.0[0m
[32m[2022-09-01 14:31:57,729] [    INFO][0m - loss: 2.1e-07, learning_rate: 9.3e-06, global_step: 690, interval_runtime: 2.0304, interval_samples_per_second: 3.94, interval_steps_per_second: 4.925, epoch: 34.5[0m
[32m[2022-09-01 14:31:59,714] [    INFO][0m - loss: 2.5e-07, learning_rate: 9e-06, global_step: 700, interval_runtime: 1.9864, interval_samples_per_second: 4.027, interval_steps_per_second: 5.034, epoch: 35.0[0m
[32m[2022-09-01 14:31:59,715] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 14:31:59,715] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-01 14:31:59,715] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 14:31:59,715] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 14:31:59,715] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-09-01 14:32:01,044] [    INFO][0m - eval_loss: 4.055443286895752, eval_accuracy: 0.75, eval_runtime: 1.3287, eval_samples_per_second: 120.418, eval_steps_per_second: 3.763, epoch: 35.0[0m
[32m[2022-09-01 14:32:01,044] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-700[0m
[32m[2022-09-01 14:32:01,044] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 14:32:09,064] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-700/tokenizer_config.json[0m
[32m[2022-09-01 14:32:09,065] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-700/special_tokens_map.json[0m
[32m[2022-09-01 14:32:25,366] [    INFO][0m - loss: 8e-07, learning_rate: 8.7e-06, global_step: 710, interval_runtime: 25.6518, interval_samples_per_second: 0.312, interval_steps_per_second: 0.39, epoch: 35.5[0m
[32m[2022-09-01 14:32:27,339] [    INFO][0m - loss: 0.11303176, learning_rate: 8.400000000000001e-06, global_step: 720, interval_runtime: 1.9731, interval_samples_per_second: 4.055, interval_steps_per_second: 5.068, epoch: 36.0[0m
[32m[2022-09-01 14:32:29,365] [    INFO][0m - loss: 5.7e-07, learning_rate: 8.1e-06, global_step: 730, interval_runtime: 2.0255, interval_samples_per_second: 3.95, interval_steps_per_second: 4.937, epoch: 36.5[0m
[32m[2022-09-01 14:32:31,338] [    INFO][0m - loss: 4.2e-07, learning_rate: 7.8e-06, global_step: 740, interval_runtime: 1.9738, interval_samples_per_second: 4.053, interval_steps_per_second: 5.066, epoch: 37.0[0m
[32m[2022-09-01 14:32:33,366] [    INFO][0m - loss: 5.6e-07, learning_rate: 7.5e-06, global_step: 750, interval_runtime: 2.0278, interval_samples_per_second: 3.945, interval_steps_per_second: 4.932, epoch: 37.5[0m
[32m[2022-09-01 14:32:35,342] [    INFO][0m - loss: 3.9e-07, learning_rate: 7.2e-06, global_step: 760, interval_runtime: 1.9759, interval_samples_per_second: 4.049, interval_steps_per_second: 5.061, epoch: 38.0[0m
[32m[2022-09-01 14:32:37,368] [    INFO][0m - loss: 1.22e-06, learning_rate: 6.900000000000001e-06, global_step: 770, interval_runtime: 2.0264, interval_samples_per_second: 3.948, interval_steps_per_second: 4.935, epoch: 38.5[0m
[32m[2022-09-01 14:32:39,348] [    INFO][0m - loss: 1.7e-07, learning_rate: 6.6e-06, global_step: 780, interval_runtime: 1.9795, interval_samples_per_second: 4.041, interval_steps_per_second: 5.052, epoch: 39.0[0m
[32m[2022-09-01 14:32:41,378] [    INFO][0m - loss: 5.2e-07, learning_rate: 6.3e-06, global_step: 790, interval_runtime: 2.0298, interval_samples_per_second: 3.941, interval_steps_per_second: 4.927, epoch: 39.5[0m
[32m[2022-09-01 14:32:43,353] [    INFO][0m - loss: 0.05861114, learning_rate: 6e-06, global_step: 800, interval_runtime: 1.975, interval_samples_per_second: 4.051, interval_steps_per_second: 5.063, epoch: 40.0[0m
[32m[2022-09-01 14:32:43,353] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 14:32:43,353] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-01 14:32:43,353] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 14:32:43,353] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 14:32:43,353] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-09-01 14:32:44,679] [    INFO][0m - eval_loss: 4.100071430206299, eval_accuracy: 0.75, eval_runtime: 1.3253, eval_samples_per_second: 120.725, eval_steps_per_second: 3.773, epoch: 40.0[0m
[32m[2022-09-01 14:32:44,679] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-800[0m
[32m[2022-09-01 14:32:44,679] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 14:32:52,673] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-800/tokenizer_config.json[0m
[32m[2022-09-01 14:32:52,674] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-800/special_tokens_map.json[0m
[32m[2022-09-01 14:33:08,782] [    INFO][0m - loss: 4.1e-07, learning_rate: 5.7000000000000005e-06, global_step: 810, interval_runtime: 25.4293, interval_samples_per_second: 0.315, interval_steps_per_second: 0.393, epoch: 40.5[0m
[32m[2022-09-01 14:33:10,754] [    INFO][0m - loss: 1.28e-06, learning_rate: 5.4e-06, global_step: 820, interval_runtime: 1.9719, interval_samples_per_second: 4.057, interval_steps_per_second: 5.071, epoch: 41.0[0m
[32m[2022-09-01 14:33:12,771] [    INFO][0m - loss: 0.21735432, learning_rate: 5.1e-06, global_step: 830, interval_runtime: 2.0167, interval_samples_per_second: 3.967, interval_steps_per_second: 4.959, epoch: 41.5[0m
[32m[2022-09-01 14:33:14,745] [    INFO][0m - loss: 1.8e-07, learning_rate: 4.800000000000001e-06, global_step: 840, interval_runtime: 1.9744, interval_samples_per_second: 4.052, interval_steps_per_second: 5.065, epoch: 42.0[0m
[32m[2022-09-01 14:33:16,767] [    INFO][0m - loss: 1.05e-06, learning_rate: 4.5e-06, global_step: 850, interval_runtime: 2.0225, interval_samples_per_second: 3.956, interval_steps_per_second: 4.944, epoch: 42.5[0m
[32m[2022-09-01 14:33:18,742] [    INFO][0m - loss: 4.861e-05, learning_rate: 4.2000000000000004e-06, global_step: 860, interval_runtime: 1.9741, interval_samples_per_second: 4.052, interval_steps_per_second: 5.065, epoch: 43.0[0m
[32m[2022-09-01 14:33:20,764] [    INFO][0m - loss: 8.2e-07, learning_rate: 3.9e-06, global_step: 870, interval_runtime: 2.0226, interval_samples_per_second: 3.955, interval_steps_per_second: 4.944, epoch: 43.5[0m
[32m[2022-09-01 14:33:22,740] [    INFO][0m - loss: 7e-08, learning_rate: 3.6e-06, global_step: 880, interval_runtime: 1.9754, interval_samples_per_second: 4.05, interval_steps_per_second: 5.062, epoch: 44.0[0m
[32m[2022-09-01 14:33:24,768] [    INFO][0m - loss: 9e-07, learning_rate: 3.3e-06, global_step: 890, interval_runtime: 2.0287, interval_samples_per_second: 3.943, interval_steps_per_second: 4.929, epoch: 44.5[0m
[32m[2022-09-01 14:33:26,745] [    INFO][0m - loss: 7.2e-07, learning_rate: 3e-06, global_step: 900, interval_runtime: 1.9764, interval_samples_per_second: 4.048, interval_steps_per_second: 5.06, epoch: 45.0[0m
[32m[2022-09-01 14:33:26,745] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 14:33:26,745] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-01 14:33:26,745] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 14:33:26,745] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 14:33:26,746] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-09-01 14:33:28,077] [    INFO][0m - eval_loss: 3.9590506553649902, eval_accuracy: 0.7625, eval_runtime: 1.3311, eval_samples_per_second: 120.203, eval_steps_per_second: 3.756, epoch: 45.0[0m
[32m[2022-09-01 14:33:28,077] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-900[0m
[32m[2022-09-01 14:33:28,077] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 14:33:35,538] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-900/tokenizer_config.json[0m
[32m[2022-09-01 14:33:35,539] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-900/special_tokens_map.json[0m
[32m[2022-09-01 14:33:52,448] [    INFO][0m - loss: 4e-08, learning_rate: 2.7e-06, global_step: 910, interval_runtime: 25.703, interval_samples_per_second: 0.311, interval_steps_per_second: 0.389, epoch: 45.5[0m
[32m[2022-09-01 14:33:54,416] [    INFO][0m - loss: 6.56e-05, learning_rate: 2.4000000000000003e-06, global_step: 920, interval_runtime: 1.9679, interval_samples_per_second: 4.065, interval_steps_per_second: 5.082, epoch: 46.0[0m
[32m[2022-09-01 14:33:56,438] [    INFO][0m - loss: 8.4e-07, learning_rate: 2.1000000000000002e-06, global_step: 930, interval_runtime: 2.0219, interval_samples_per_second: 3.957, interval_steps_per_second: 4.946, epoch: 46.5[0m
[32m[2022-09-01 14:33:58,408] [    INFO][0m - loss: 0.00689993, learning_rate: 1.8e-06, global_step: 940, interval_runtime: 1.97, interval_samples_per_second: 4.061, interval_steps_per_second: 5.076, epoch: 47.0[0m
[32m[2022-09-01 14:34:00,436] [    INFO][0m - loss: 8e-08, learning_rate: 1.5e-06, global_step: 950, interval_runtime: 2.0277, interval_samples_per_second: 3.945, interval_steps_per_second: 4.932, epoch: 47.5[0m
[32m[2022-09-01 14:34:02,408] [    INFO][0m - loss: 0.0036723, learning_rate: 1.2000000000000002e-06, global_step: 960, interval_runtime: 1.9726, interval_samples_per_second: 4.056, interval_steps_per_second: 5.069, epoch: 48.0[0m
[32m[2022-09-01 14:34:04,430] [    INFO][0m - loss: 1.6e-07, learning_rate: 9e-07, global_step: 970, interval_runtime: 2.0217, interval_samples_per_second: 3.957, interval_steps_per_second: 4.946, epoch: 48.5[0m
[32m[2022-09-01 14:34:06,407] [    INFO][0m - loss: 8e-08, learning_rate: 6.000000000000001e-07, global_step: 980, interval_runtime: 1.9772, interval_samples_per_second: 4.046, interval_steps_per_second: 5.058, epoch: 49.0[0m
[32m[2022-09-01 14:34:08,434] [    INFO][0m - loss: 3e-08, learning_rate: 3.0000000000000004e-07, global_step: 990, interval_runtime: 2.0274, interval_samples_per_second: 3.946, interval_steps_per_second: 4.932, epoch: 49.5[0m
[32m[2022-09-01 14:34:10,410] [    INFO][0m - loss: 8.2e-07, learning_rate: 0.0, global_step: 1000, interval_runtime: 1.9753, interval_samples_per_second: 4.05, interval_steps_per_second: 5.063, epoch: 50.0[0m
[32m[2022-09-01 14:34:10,410] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 14:34:10,410] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-01 14:34:10,410] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 14:34:10,410] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 14:34:10,410] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-09-01 14:34:11,737] [    INFO][0m - eval_loss: 3.9634597301483154, eval_accuracy: 0.74375, eval_runtime: 1.3266, eval_samples_per_second: 120.608, eval_steps_per_second: 3.769, epoch: 50.0[0m
[32m[2022-09-01 14:34:11,737] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-1000[0m
[32m[2022-09-01 14:34:11,738] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 14:34:19,103] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-1000/tokenizer_config.json[0m
[32m[2022-09-01 14:34:19,104] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-1000/special_tokens_map.json[0m
[32m[2022-09-01 14:34:33,403] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-01 14:34:33,403] [    INFO][0m - Loading best model from ./checkpoints_iflytek/checkpoint-900 (score: 0.7625).[0m
[32m[2022-09-01 14:34:35,633] [    INFO][0m - train_runtime: 452.1975, train_samples_per_second: 17.691, train_steps_per_second: 2.211, train_loss: 0.04019968327120526, epoch: 50.0[0m
[32m[2022-09-01 14:34:35,635] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/[0m
[32m[2022-09-01 14:34:35,635] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 14:34:43,305] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/tokenizer_config.json[0m
[32m[2022-09-01 14:34:43,306] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/special_tokens_map.json[0m
[32m[2022-09-01 14:34:43,308] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-01 14:34:43,308] [    INFO][0m -   epoch                    =       50.0[0m
[32m[2022-09-01 14:34:43,308] [    INFO][0m -   train_loss               =     0.0402[0m
[32m[2022-09-01 14:34:43,308] [    INFO][0m -   train_runtime            = 0:07:32.19[0m
[32m[2022-09-01 14:34:43,308] [    INFO][0m -   train_samples_per_second =     17.691[0m
[32m[2022-09-01 14:34:43,308] [    INFO][0m -   train_steps_per_second   =      2.211[0m
[32m[2022-09-01 14:34:43,313] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-01 14:34:43,313] [    INFO][0m -   Num examples = 2520[0m
[32m[2022-09-01 14:34:43,314] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 14:34:43,314] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 14:34:43,314] [    INFO][0m -   Total prediction steps = 79[0m
[32m[2022-09-01 14:35:04,087] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-01 14:35:04,088] [    INFO][0m -   test_accuracy           =     0.7159[0m
[32m[2022-09-01 14:35:04,088] [    INFO][0m -   test_loss               =     4.4662[0m
[32m[2022-09-01 14:35:04,088] [    INFO][0m -   test_runtime            = 0:00:20.77[0m
[32m[2022-09-01 14:35:04,088] [    INFO][0m -   test_samples_per_second =    121.307[0m
[32m[2022-09-01 14:35:04,088] [    INFO][0m -   test_steps_per_second   =      3.803[0m
[32m[2022-09-01 14:35:04,089] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-01 14:35:04,089] [    INFO][0m -   Num examples = 3000[0m
[32m[2022-09-01 14:35:04,089] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 14:35:04,089] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 14:35:04,089] [    INFO][0m -   Total prediction steps = 94[0m
[32m[2022-09-01 14:35:31,712] [    INFO][0m - Predictions for ocnlif saved to ./fewclue_submit_examples.[0m
[]
run.sh: line 78: --freeze_plm: command not found
 
==========
bustm
==========
 
[33m[2022-09-01 14:35:35,775] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-09-01 14:35:35,776] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-01 14:35:35,776] [    INFO][0m - ============================================================[0m
[32m[2022-09-01 14:35:35,776] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-01 14:35:35,776] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-01 14:35:35,776] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-01 14:35:35,776] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-01 14:35:35,776] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-01 14:35:35,776] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-01 14:35:35,777] [    INFO][0m - model_name_or_path            :ernie-3.0-xbase-zh[0m
[32m[2022-09-01 14:35:35,777] [    INFO][0m - [0m
[32m[2022-09-01 14:35:35,777] [    INFO][0m - ============================================================[0m
[32m[2022-09-01 14:35:35,777] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-01 14:35:35,777] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-01 14:35:35,777] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-01 14:35:35,777] [    INFO][0m - prompt                        :‚Äú{'text':'text_a'}‚ÄùÂíå‚Äú{'text':'text_b'}‚Äù‰πãÈó¥ÁöÑÈÄªËæëÂÖ≥Á≥ªÊòØ{'mask'}{'mask'}„ÄÇ[0m
[32m[2022-09-01 14:35:35,777] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-01 14:35:35,777] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-01 14:35:35,777] [    INFO][0m - task_name                     :bustm[0m
[32m[2022-09-01 14:35:35,777] [    INFO][0m - [0m
[32m[2022-09-01 14:35:35,778] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/ernie_3.0_xbase_zh.pdparams[0m
W0901 14:35:35.779052  9742 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0901 14:35:35.782949  9742 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-01 14:35:41,762] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/ernie_3.0_xbase_zh_vocab.txt[0m
[32m[2022-09-01 14:35:41,787] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/tokenizer_config.json[0m
[32m[2022-09-01 14:35:41,787] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/special_tokens_map.json[0m
[32m[2022-09-01 14:35:41,789] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùÂíå‚Äú'}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': '‚Äù‰πãÈó¥ÁöÑÈÄªËæëÂÖ≥Á≥ªÊòØ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': '„ÄÇ'}][0m
[32m[2022-09-01 14:35:41,791] [    INFO][0m - {'0': 0, '1': 1}[0m
2022-09-01 14:35:41,792 INFO [download.py:119] unique_endpoints {''}
[32m[2022-09-01 14:35:44,022] [    INFO][0m - ============================================================[0m
[32m[2022-09-01 14:35:44,022] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-01 14:35:44,022] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-01 14:35:44,022] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-01 14:35:44,022] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-01 14:35:44,023] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-01 14:35:44,023] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-01 14:35:44,023] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-01 14:35:44,023] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-01 14:35:44,023] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-01 14:35:44,023] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-01 14:35:44,023] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-01 14:35:44,023] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-01 14:35:44,023] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-01 14:35:44,023] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-01 14:35:44,023] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-01 14:35:44,023] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-01 14:35:44,023] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-01 14:35:44,024] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-09-01 14:35:44,024] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-09-01 14:35:44,024] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-09-01 14:35:44,024] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-01 14:35:44,024] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-01 14:35:44,024] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-01 14:35:44,024] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-01 14:35:44,024] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-01 14:35:44,024] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-01 14:35:44,024] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-01 14:35:44,024] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-01 14:35:44,024] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-01 14:35:44,024] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-09-01 14:35:44,025] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-01 14:35:44,025] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-01 14:35:44,025] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-01 14:35:44,025] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-01 14:35:44,025] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-01 14:35:44,025] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-01 14:35:44,025] [    INFO][0m - logging_dir                   :./checkpoints_iflytek/runs/Sep01_14-35-35_instance-3bwob41y-01[0m
[32m[2022-09-01 14:35:44,025] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-01 14:35:44,025] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-01 14:35:44,025] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-01 14:35:44,025] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-01 14:35:44,025] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-01 14:35:44,025] [    INFO][0m - max_seq_length                :64[0m
[32m[2022-09-01 14:35:44,026] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-01 14:35:44,026] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-01 14:35:44,026] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-01 14:35:44,026] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-01 14:35:44,026] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-09-01 14:35:44,026] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-01 14:35:44,026] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-01 14:35:44,026] [    INFO][0m - output_dir                    :./checkpoints_iflytek/[0m
[32m[2022-09-01 14:35:44,026] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-01 14:35:44,026] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-01 14:35:44,026] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-09-01 14:35:44,026] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-09-01 14:35:44,026] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-01 14:35:44,026] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-01 14:35:44,027] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-01 14:35:44,027] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-09-01 14:35:44,027] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-01 14:35:44,027] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-01 14:35:44,027] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-01 14:35:44,027] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-01 14:35:44,027] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-01 14:35:44,027] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-01 14:35:44,027] [    INFO][0m - run_name                      :./checkpoints_iflytek/[0m
[32m[2022-09-01 14:35:44,027] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-01 14:35:44,027] [    INFO][0m - save_steps                    :100[0m
[32m[2022-09-01 14:35:44,027] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-09-01 14:35:44,027] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-01 14:35:44,027] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-01 14:35:44,028] [    INFO][0m - seed                          :42[0m
[32m[2022-09-01 14:35:44,028] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-01 14:35:44,028] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-01 14:35:44,028] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-01 14:35:44,028] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-09-01 14:35:44,028] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-01 14:35:44,028] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-01 14:35:44,028] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-01 14:35:44,028] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-01 14:35:44,028] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-01 14:35:44,028] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-01 14:35:44,028] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-01 14:35:44,028] [    INFO][0m - [0m
[32m[2022-09-01 14:35:44,031] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-01 14:35:44,031] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-01 14:35:44,031] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-09-01 14:35:44,031] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-09-01 14:35:44,031] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-09-01 14:35:44,031] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-01 14:35:44,031] [    INFO][0m -   Total optimization steps = 1000.0[0m
[32m[2022-09-01 14:35:44,031] [    INFO][0m -   Total num train samples = 8000[0m
[32m[2022-09-01 14:35:46,280] [    INFO][0m - loss: 0.46586699, learning_rate: 2.97e-05, global_step: 10, interval_runtime: 2.2476, interval_samples_per_second: 3.559, interval_steps_per_second: 4.449, epoch: 0.5[0m
[32m[2022-09-01 14:35:47,449] [    INFO][0m - loss: 0.67728329, learning_rate: 2.94e-05, global_step: 20, interval_runtime: 1.168, interval_samples_per_second: 6.849, interval_steps_per_second: 8.561, epoch: 1.0[0m
[32m[2022-09-01 14:35:48,660] [    INFO][0m - loss: 0.30534337, learning_rate: 2.91e-05, global_step: 30, interval_runtime: 1.2128, interval_samples_per_second: 6.596, interval_steps_per_second: 8.245, epoch: 1.5[0m
[32m[2022-09-01 14:35:49,827] [    INFO][0m - loss: 0.20567565, learning_rate: 2.88e-05, global_step: 40, interval_runtime: 1.1665, interval_samples_per_second: 6.858, interval_steps_per_second: 8.572, epoch: 2.0[0m
[32m[2022-09-01 14:35:51,039] [    INFO][0m - loss: 0.09156911, learning_rate: 2.8499999999999998e-05, global_step: 50, interval_runtime: 1.2118, interval_samples_per_second: 6.602, interval_steps_per_second: 8.252, epoch: 2.5[0m
[32m[2022-09-01 14:35:52,204] [    INFO][0m - loss: 0.02627118, learning_rate: 2.8199999999999998e-05, global_step: 60, interval_runtime: 1.1649, interval_samples_per_second: 6.867, interval_steps_per_second: 8.584, epoch: 3.0[0m
[32m[2022-09-01 14:35:53,418] [    INFO][0m - loss: 0.01409289, learning_rate: 2.79e-05, global_step: 70, interval_runtime: 1.2145, interval_samples_per_second: 6.587, interval_steps_per_second: 8.234, epoch: 3.5[0m
[32m[2022-09-01 14:35:54,590] [    INFO][0m - loss: 8.5e-07, learning_rate: 2.7600000000000003e-05, global_step: 80, interval_runtime: 1.1702, interval_samples_per_second: 6.836, interval_steps_per_second: 8.545, epoch: 4.0[0m
[32m[2022-09-01 14:35:55,818] [    INFO][0m - loss: 1.112e-05, learning_rate: 2.7300000000000003e-05, global_step: 90, interval_runtime: 1.2298, interval_samples_per_second: 6.505, interval_steps_per_second: 8.132, epoch: 4.5[0m
[32m[2022-09-01 14:35:56,985] [    INFO][0m - loss: 0.06294038, learning_rate: 2.7000000000000002e-05, global_step: 100, interval_runtime: 1.1661, interval_samples_per_second: 6.861, interval_steps_per_second: 8.576, epoch: 5.0[0m
[32m[2022-09-01 14:35:56,985] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 14:35:56,986] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-01 14:35:56,986] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 14:35:56,986] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 14:35:56,986] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-09-01 14:35:57,729] [    INFO][0m - eval_loss: 3.3958115577697754, eval_accuracy: 0.79375, eval_runtime: 0.7434, eval_samples_per_second: 215.234, eval_steps_per_second: 6.726, epoch: 5.0[0m
[32m[2022-09-01 14:35:57,730] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-100[0m
[32m[2022-09-01 14:35:57,730] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 14:36:05,792] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-01 14:36:05,793] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-01 14:36:21,782] [    INFO][0m - loss: 9e-08, learning_rate: 2.6700000000000002e-05, global_step: 110, interval_runtime: 24.7975, interval_samples_per_second: 0.323, interval_steps_per_second: 0.403, epoch: 5.5[0m
[32m[2022-09-01 14:36:22,943] [    INFO][0m - loss: 1.5e-07, learning_rate: 2.64e-05, global_step: 120, interval_runtime: 1.1608, interval_samples_per_second: 6.892, interval_steps_per_second: 8.615, epoch: 6.0[0m
[32m[2022-09-01 14:36:24,150] [    INFO][0m - loss: 0.04627004, learning_rate: 2.61e-05, global_step: 130, interval_runtime: 1.2071, interval_samples_per_second: 6.628, interval_steps_per_second: 8.284, epoch: 6.5[0m
[32m[2022-09-01 14:36:25,313] [    INFO][0m - loss: 7.228e-05, learning_rate: 2.58e-05, global_step: 140, interval_runtime: 1.1637, interval_samples_per_second: 6.875, interval_steps_per_second: 8.593, epoch: 7.0[0m
[32m[2022-09-01 14:36:26,521] [    INFO][0m - loss: 0.01720027, learning_rate: 2.55e-05, global_step: 150, interval_runtime: 1.2077, interval_samples_per_second: 6.624, interval_steps_per_second: 8.28, epoch: 7.5[0m
[32m[2022-09-01 14:36:27,685] [    INFO][0m - loss: 0.07094864, learning_rate: 2.52e-05, global_step: 160, interval_runtime: 1.1639, interval_samples_per_second: 6.874, interval_steps_per_second: 8.592, epoch: 8.0[0m
[32m[2022-09-01 14:36:28,897] [    INFO][0m - loss: 0.2088783, learning_rate: 2.49e-05, global_step: 170, interval_runtime: 1.2118, interval_samples_per_second: 6.602, interval_steps_per_second: 8.252, epoch: 8.5[0m
[32m[2022-09-01 14:36:30,063] [    INFO][0m - loss: 0.18264579, learning_rate: 2.4599999999999998e-05, global_step: 180, interval_runtime: 1.1662, interval_samples_per_second: 6.86, interval_steps_per_second: 8.575, epoch: 9.0[0m
[32m[2022-09-01 14:36:31,272] [    INFO][0m - loss: 1.177e-05, learning_rate: 2.43e-05, global_step: 190, interval_runtime: 1.2094, interval_samples_per_second: 6.615, interval_steps_per_second: 8.269, epoch: 9.5[0m
[32m[2022-09-01 14:36:32,439] [    INFO][0m - loss: 0.00375826, learning_rate: 2.4e-05, global_step: 200, interval_runtime: 1.1664, interval_samples_per_second: 6.859, interval_steps_per_second: 8.573, epoch: 10.0[0m
[32m[2022-09-01 14:36:32,439] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 14:36:32,439] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-01 14:36:32,439] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 14:36:32,439] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 14:36:32,440] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-09-01 14:36:33,180] [    INFO][0m - eval_loss: 2.8925323486328125, eval_accuracy: 0.81875, eval_runtime: 0.7402, eval_samples_per_second: 216.151, eval_steps_per_second: 6.755, epoch: 10.0[0m
[32m[2022-09-01 14:36:33,180] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-200[0m
[32m[2022-09-01 14:36:33,181] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 14:36:40,376] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-200/tokenizer_config.json[0m
[32m[2022-09-01 14:36:40,376] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-200/special_tokens_map.json[0m
[32m[2022-09-01 14:36:56,170] [    INFO][0m - loss: 4.28e-06, learning_rate: 2.37e-05, global_step: 210, interval_runtime: 23.7313, interval_samples_per_second: 0.337, interval_steps_per_second: 0.421, epoch: 10.5[0m
[32m[2022-09-01 14:36:57,357] [    INFO][0m - loss: 0.09551146, learning_rate: 2.3400000000000003e-05, global_step: 220, interval_runtime: 1.1872, interval_samples_per_second: 6.738, interval_steps_per_second: 8.423, epoch: 11.0[0m
[32m[2022-09-01 14:36:58,565] [    INFO][0m - loss: 0.04647715, learning_rate: 2.3100000000000002e-05, global_step: 230, interval_runtime: 1.2083, interval_samples_per_second: 6.621, interval_steps_per_second: 8.276, epoch: 11.5[0m
[32m[2022-09-01 14:36:59,731] [    INFO][0m - loss: 9.02e-06, learning_rate: 2.2800000000000002e-05, global_step: 240, interval_runtime: 1.1652, interval_samples_per_second: 6.866, interval_steps_per_second: 8.582, epoch: 12.0[0m
[32m[2022-09-01 14:37:00,939] [    INFO][0m - loss: 4.39e-06, learning_rate: 2.25e-05, global_step: 250, interval_runtime: 1.2079, interval_samples_per_second: 6.623, interval_steps_per_second: 8.279, epoch: 12.5[0m
[32m[2022-09-01 14:37:02,102] [    INFO][0m - loss: 7.753e-05, learning_rate: 2.22e-05, global_step: 260, interval_runtime: 1.1635, interval_samples_per_second: 6.876, interval_steps_per_second: 8.595, epoch: 13.0[0m
[32m[2022-09-01 14:37:03,311] [    INFO][0m - loss: 6.1e-07, learning_rate: 2.19e-05, global_step: 270, interval_runtime: 1.2091, interval_samples_per_second: 6.616, interval_steps_per_second: 8.27, epoch: 13.5[0m
[32m[2022-09-01 14:37:04,475] [    INFO][0m - loss: 4.32e-06, learning_rate: 2.16e-05, global_step: 280, interval_runtime: 1.1634, interval_samples_per_second: 6.876, interval_steps_per_second: 8.596, epoch: 14.0[0m
[32m[2022-09-01 14:37:05,682] [    INFO][0m - loss: 7.4e-06, learning_rate: 2.13e-05, global_step: 290, interval_runtime: 1.2067, interval_samples_per_second: 6.63, interval_steps_per_second: 8.287, epoch: 14.5[0m
[32m[2022-09-01 14:37:06,846] [    INFO][0m - loss: 1.75e-06, learning_rate: 2.1e-05, global_step: 300, interval_runtime: 1.1647, interval_samples_per_second: 6.869, interval_steps_per_second: 8.586, epoch: 15.0[0m
[32m[2022-09-01 14:37:06,847] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 14:37:06,847] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-01 14:37:06,847] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 14:37:06,847] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 14:37:06,847] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-09-01 14:37:07,590] [    INFO][0m - eval_loss: 2.9676060676574707, eval_accuracy: 0.8125, eval_runtime: 0.7425, eval_samples_per_second: 215.484, eval_steps_per_second: 6.734, epoch: 15.0[0m
[32m[2022-09-01 14:37:07,590] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-300[0m
[32m[2022-09-01 14:37:07,590] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 14:37:14,981] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-300/tokenizer_config.json[0m
[32m[2022-09-01 14:37:14,981] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-300/special_tokens_map.json[0m
[32m[2022-09-01 14:37:30,939] [    INFO][0m - loss: 8.4e-07, learning_rate: 2.07e-05, global_step: 310, interval_runtime: 24.0934, interval_samples_per_second: 0.332, interval_steps_per_second: 0.415, epoch: 15.5[0m
[32m[2022-09-01 14:37:32,099] [    INFO][0m - loss: 2.27e-06, learning_rate: 2.04e-05, global_step: 320, interval_runtime: 1.1599, interval_samples_per_second: 6.897, interval_steps_per_second: 8.622, epoch: 16.0[0m
[32m[2022-09-01 14:37:33,306] [    INFO][0m - loss: 3.769e-05, learning_rate: 2.01e-05, global_step: 330, interval_runtime: 1.2066, interval_samples_per_second: 6.63, interval_steps_per_second: 8.288, epoch: 16.5[0m
[32m[2022-09-01 14:37:34,467] [    INFO][0m - loss: 6.8e-07, learning_rate: 1.98e-05, global_step: 340, interval_runtime: 1.1614, interval_samples_per_second: 6.888, interval_steps_per_second: 8.61, epoch: 17.0[0m
[32m[2022-09-01 14:37:35,675] [    INFO][0m - loss: 9.1e-07, learning_rate: 1.95e-05, global_step: 350, interval_runtime: 1.2068, interval_samples_per_second: 6.629, interval_steps_per_second: 8.286, epoch: 17.5[0m
[32m[2022-09-01 14:37:36,842] [    INFO][0m - loss: 8.5e-07, learning_rate: 1.9200000000000003e-05, global_step: 360, interval_runtime: 1.1676, interval_samples_per_second: 6.852, interval_steps_per_second: 8.565, epoch: 18.0[0m
[32m[2022-09-01 14:37:38,052] [    INFO][0m - loss: 6e-08, learning_rate: 1.8900000000000002e-05, global_step: 370, interval_runtime: 1.2103, interval_samples_per_second: 6.61, interval_steps_per_second: 8.262, epoch: 18.5[0m
[32m[2022-09-01 14:37:39,215] [    INFO][0m - loss: 1.2e-07, learning_rate: 1.86e-05, global_step: 380, interval_runtime: 1.1626, interval_samples_per_second: 6.881, interval_steps_per_second: 8.601, epoch: 19.0[0m
[32m[2022-09-01 14:37:40,421] [    INFO][0m - loss: 1.3e-07, learning_rate: 1.83e-05, global_step: 390, interval_runtime: 1.2065, interval_samples_per_second: 6.631, interval_steps_per_second: 8.288, epoch: 19.5[0m
[32m[2022-09-01 14:37:41,585] [    INFO][0m - loss: 4e-08, learning_rate: 1.8e-05, global_step: 400, interval_runtime: 1.164, interval_samples_per_second: 6.873, interval_steps_per_second: 8.591, epoch: 20.0[0m
[32m[2022-09-01 14:37:41,586] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 14:37:41,586] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-01 14:37:41,586] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 14:37:41,586] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 14:37:41,586] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-09-01 14:37:42,328] [    INFO][0m - eval_loss: 2.9515156745910645, eval_accuracy: 0.825, eval_runtime: 0.7415, eval_samples_per_second: 215.783, eval_steps_per_second: 6.743, epoch: 20.0[0m
[32m[2022-09-01 14:37:42,328] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-400[0m
[32m[2022-09-01 14:37:42,328] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 14:37:49,987] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-400/tokenizer_config.json[0m
[32m[2022-09-01 14:37:49,988] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-400/special_tokens_map.json[0m
[32m[2022-09-01 14:38:06,564] [    INFO][0m - loss: 6e-08, learning_rate: 1.77e-05, global_step: 410, interval_runtime: 24.9789, interval_samples_per_second: 0.32, interval_steps_per_second: 0.4, epoch: 20.5[0m
[32m[2022-09-01 14:38:07,725] [    INFO][0m - loss: 7e-08, learning_rate: 1.74e-05, global_step: 420, interval_runtime: 1.1605, interval_samples_per_second: 6.894, interval_steps_per_second: 8.617, epoch: 21.0[0m
[32m[2022-09-01 14:38:08,935] [    INFO][0m - loss: 0.00010496, learning_rate: 1.71e-05, global_step: 430, interval_runtime: 1.21, interval_samples_per_second: 6.612, interval_steps_per_second: 8.265, epoch: 21.5[0m
[32m[2022-09-01 14:38:10,098] [    INFO][0m - loss: 1.5e-07, learning_rate: 1.6800000000000002e-05, global_step: 440, interval_runtime: 1.1633, interval_samples_per_second: 6.877, interval_steps_per_second: 8.596, epoch: 22.0[0m
[32m[2022-09-01 14:38:11,306] [    INFO][0m - loss: 2.34e-06, learning_rate: 1.65e-05, global_step: 450, interval_runtime: 1.2078, interval_samples_per_second: 6.623, interval_steps_per_second: 8.279, epoch: 22.5[0m
[32m[2022-09-01 14:38:12,470] [    INFO][0m - loss: 0.0028231, learning_rate: 1.62e-05, global_step: 460, interval_runtime: 1.1639, interval_samples_per_second: 6.874, interval_steps_per_second: 8.592, epoch: 23.0[0m
[32m[2022-09-01 14:38:13,676] [    INFO][0m - loss: 1.3e-07, learning_rate: 1.59e-05, global_step: 470, interval_runtime: 1.2065, interval_samples_per_second: 6.631, interval_steps_per_second: 8.289, epoch: 23.5[0m
[32m[2022-09-01 14:38:14,842] [    INFO][0m - loss: 1.8e-07, learning_rate: 1.56e-05, global_step: 480, interval_runtime: 1.1657, interval_samples_per_second: 6.863, interval_steps_per_second: 8.579, epoch: 24.0[0m
[32m[2022-09-01 14:38:16,052] [    INFO][0m - loss: 3e-08, learning_rate: 1.53e-05, global_step: 490, interval_runtime: 1.2098, interval_samples_per_second: 6.613, interval_steps_per_second: 8.266, epoch: 24.5[0m
[32m[2022-09-01 14:38:17,214] [    INFO][0m - loss: 6.8e-07, learning_rate: 1.5e-05, global_step: 500, interval_runtime: 1.1627, interval_samples_per_second: 6.88, interval_steps_per_second: 8.601, epoch: 25.0[0m
[32m[2022-09-01 14:38:17,215] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 14:38:17,215] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-01 14:38:17,215] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 14:38:17,215] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 14:38:17,215] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-09-01 14:38:17,958] [    INFO][0m - eval_loss: 3.2018096446990967, eval_accuracy: 0.81875, eval_runtime: 0.7421, eval_samples_per_second: 215.616, eval_steps_per_second: 6.738, epoch: 25.0[0m
[32m[2022-09-01 14:38:17,958] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-500[0m
[32m[2022-09-01 14:38:17,958] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 14:38:25,895] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-500/tokenizer_config.json[0m
[32m[2022-09-01 14:38:25,895] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-500/special_tokens_map.json[0m
[32m[2022-09-01 14:38:42,296] [    INFO][0m - loss: 0.01054167, learning_rate: 1.47e-05, global_step: 510, interval_runtime: 25.0812, interval_samples_per_second: 0.319, interval_steps_per_second: 0.399, epoch: 25.5[0m
[32m[2022-09-01 14:38:43,456] [    INFO][0m - loss: 2.6e-07, learning_rate: 1.44e-05, global_step: 520, interval_runtime: 1.1604, interval_samples_per_second: 6.894, interval_steps_per_second: 8.618, epoch: 26.0[0m
[32m[2022-09-01 14:38:44,665] [    INFO][0m - loss: 0.06746191, learning_rate: 1.4099999999999999e-05, global_step: 530, interval_runtime: 1.2085, interval_samples_per_second: 6.62, interval_steps_per_second: 8.275, epoch: 26.5[0m
[32m[2022-09-01 14:38:45,828] [    INFO][0m - loss: 2.48e-06, learning_rate: 1.3800000000000002e-05, global_step: 540, interval_runtime: 1.1636, interval_samples_per_second: 6.875, interval_steps_per_second: 8.594, epoch: 27.0[0m
[32m[2022-09-01 14:38:47,051] [    INFO][0m - loss: 5.12e-06, learning_rate: 1.3500000000000001e-05, global_step: 550, interval_runtime: 1.2227, interval_samples_per_second: 6.543, interval_steps_per_second: 8.179, epoch: 27.5[0m
[32m[2022-09-01 14:38:48,214] [    INFO][0m - loss: 8.6e-07, learning_rate: 1.32e-05, global_step: 560, interval_runtime: 1.1631, interval_samples_per_second: 6.878, interval_steps_per_second: 8.598, epoch: 28.0[0m
[32m[2022-09-01 14:38:49,421] [    INFO][0m - loss: 0.00159798, learning_rate: 1.29e-05, global_step: 570, interval_runtime: 1.2067, interval_samples_per_second: 6.63, interval_steps_per_second: 8.287, epoch: 28.5[0m
[32m[2022-09-01 14:38:50,583] [    INFO][0m - loss: 0.08040231, learning_rate: 1.26e-05, global_step: 580, interval_runtime: 1.1627, interval_samples_per_second: 6.881, interval_steps_per_second: 8.601, epoch: 29.0[0m
[32m[2022-09-01 14:38:51,789] [    INFO][0m - loss: 0.00334447, learning_rate: 1.2299999999999999e-05, global_step: 590, interval_runtime: 1.2058, interval_samples_per_second: 6.635, interval_steps_per_second: 8.293, epoch: 29.5[0m
[32m[2022-09-01 14:38:52,951] [    INFO][0m - loss: 1.9e-06, learning_rate: 1.2e-05, global_step: 600, interval_runtime: 1.1623, interval_samples_per_second: 6.883, interval_steps_per_second: 8.604, epoch: 30.0[0m
[32m[2022-09-01 14:38:52,952] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 14:38:52,952] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-01 14:38:52,952] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 14:38:52,952] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 14:38:52,952] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-09-01 14:38:53,694] [    INFO][0m - eval_loss: 3.1699624061584473, eval_accuracy: 0.8125, eval_runtime: 0.7415, eval_samples_per_second: 215.781, eval_steps_per_second: 6.743, epoch: 30.0[0m
[32m[2022-09-01 14:38:53,695] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-600[0m
[32m[2022-09-01 14:38:53,695] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 14:39:01,290] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-600/tokenizer_config.json[0m
[32m[2022-09-01 14:39:01,290] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-600/special_tokens_map.json[0m
[32m[2022-09-01 14:39:16,976] [    INFO][0m - loss: 4e-08, learning_rate: 1.1700000000000001e-05, global_step: 610, interval_runtime: 24.0245, interval_samples_per_second: 0.333, interval_steps_per_second: 0.416, epoch: 30.5[0m
[32m[2022-09-01 14:39:18,136] [    INFO][0m - loss: 4e-08, learning_rate: 1.1400000000000001e-05, global_step: 620, interval_runtime: 1.16, interval_samples_per_second: 6.897, interval_steps_per_second: 8.621, epoch: 31.0[0m
[32m[2022-09-01 14:39:19,344] [    INFO][0m - loss: 2.7e-07, learning_rate: 1.11e-05, global_step: 630, interval_runtime: 1.2082, interval_samples_per_second: 6.622, interval_steps_per_second: 8.277, epoch: 31.5[0m
[32m[2022-09-01 14:39:20,507] [    INFO][0m - loss: 9.1e-07, learning_rate: 1.08e-05, global_step: 640, interval_runtime: 1.1627, interval_samples_per_second: 6.88, interval_steps_per_second: 8.6, epoch: 32.0[0m
[32m[2022-09-01 14:39:21,719] [    INFO][0m - loss: 6e-08, learning_rate: 1.05e-05, global_step: 650, interval_runtime: 1.2121, interval_samples_per_second: 6.6, interval_steps_per_second: 8.25, epoch: 32.5[0m
[32m[2022-09-01 14:39:22,881] [    INFO][0m - loss: 1e-08, learning_rate: 1.02e-05, global_step: 660, interval_runtime: 1.1621, interval_samples_per_second: 6.884, interval_steps_per_second: 8.605, epoch: 33.0[0m
[32m[2022-09-01 14:39:24,090] [    INFO][0m - loss: 7e-08, learning_rate: 9.9e-06, global_step: 670, interval_runtime: 1.209, interval_samples_per_second: 6.617, interval_steps_per_second: 8.271, epoch: 33.5[0m
[32m[2022-09-01 14:39:25,258] [    INFO][0m - loss: 1e-08, learning_rate: 9.600000000000001e-06, global_step: 680, interval_runtime: 1.1676, interval_samples_per_second: 6.851, interval_steps_per_second: 8.564, epoch: 34.0[0m
[32m[2022-09-01 14:39:26,467] [    INFO][0m - loss: 2e-08, learning_rate: 9.3e-06, global_step: 690, interval_runtime: 1.2095, interval_samples_per_second: 6.614, interval_steps_per_second: 8.268, epoch: 34.5[0m
[32m[2022-09-01 14:39:27,631] [    INFO][0m - loss: 3e-08, learning_rate: 9e-06, global_step: 700, interval_runtime: 1.1638, interval_samples_per_second: 6.874, interval_steps_per_second: 8.592, epoch: 35.0[0m
[32m[2022-09-01 14:39:27,632] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 14:39:27,632] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-01 14:39:27,632] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 14:39:27,632] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 14:39:27,632] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-09-01 14:39:28,371] [    INFO][0m - eval_loss: 3.125092029571533, eval_accuracy: 0.8125, eval_runtime: 0.7391, eval_samples_per_second: 216.477, eval_steps_per_second: 6.765, epoch: 35.0[0m
[32m[2022-09-01 14:39:28,372] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-700[0m
[32m[2022-09-01 14:39:28,372] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 14:39:36,076] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-700/tokenizer_config.json[0m
[32m[2022-09-01 14:39:36,077] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-700/special_tokens_map.json[0m
[32m[2022-09-01 14:39:51,744] [    INFO][0m - loss: 3e-08, learning_rate: 8.7e-06, global_step: 710, interval_runtime: 24.1133, interval_samples_per_second: 0.332, interval_steps_per_second: 0.415, epoch: 35.5[0m
[32m[2022-09-01 14:39:52,910] [    INFO][0m - loss: 0.0, learning_rate: 8.400000000000001e-06, global_step: 720, interval_runtime: 1.1657, interval_samples_per_second: 6.863, interval_steps_per_second: 8.578, epoch: 36.0[0m
[32m[2022-09-01 14:39:54,122] [    INFO][0m - loss: 7e-08, learning_rate: 8.1e-06, global_step: 730, interval_runtime: 1.2115, interval_samples_per_second: 6.603, interval_steps_per_second: 8.254, epoch: 36.5[0m
[32m[2022-09-01 14:39:55,286] [    INFO][0m - loss: 3e-08, learning_rate: 7.8e-06, global_step: 740, interval_runtime: 1.1641, interval_samples_per_second: 6.872, interval_steps_per_second: 8.59, epoch: 37.0[0m
[32m[2022-09-01 14:39:56,494] [    INFO][0m - loss: 9.578e-05, learning_rate: 7.5e-06, global_step: 750, interval_runtime: 1.2079, interval_samples_per_second: 6.623, interval_steps_per_second: 8.279, epoch: 37.5[0m
[32m[2022-09-01 14:39:57,780] [    INFO][0m - loss: 2e-08, learning_rate: 7.2e-06, global_step: 760, interval_runtime: 1.2859, interval_samples_per_second: 6.221, interval_steps_per_second: 7.777, epoch: 38.0[0m
[32m[2022-09-01 14:39:58,987] [    INFO][0m - loss: 8e-08, learning_rate: 6.900000000000001e-06, global_step: 770, interval_runtime: 1.2079, interval_samples_per_second: 6.623, interval_steps_per_second: 8.279, epoch: 38.5[0m
[32m[2022-09-01 14:40:00,152] [    INFO][0m - loss: 4.6e-07, learning_rate: 6.6e-06, global_step: 780, interval_runtime: 1.165, interval_samples_per_second: 6.867, interval_steps_per_second: 8.584, epoch: 39.0[0m
[32m[2022-09-01 14:40:01,361] [    INFO][0m - loss: 1e-08, learning_rate: 6.3e-06, global_step: 790, interval_runtime: 1.2082, interval_samples_per_second: 6.621, interval_steps_per_second: 8.277, epoch: 39.5[0m
[32m[2022-09-01 14:40:02,524] [    INFO][0m - loss: 7e-08, learning_rate: 6e-06, global_step: 800, interval_runtime: 1.1631, interval_samples_per_second: 6.878, interval_steps_per_second: 8.598, epoch: 40.0[0m
[32m[2022-09-01 14:40:02,524] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 14:40:02,524] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-01 14:40:02,524] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 14:40:02,524] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 14:40:02,525] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-09-01 14:40:03,263] [    INFO][0m - eval_loss: 3.6607613563537598, eval_accuracy: 0.80625, eval_runtime: 0.7387, eval_samples_per_second: 216.6, eval_steps_per_second: 6.769, epoch: 40.0[0m
[32m[2022-09-01 14:40:03,264] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/checkpoint-800[0m
[32m[2022-09-01 14:40:03,264] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 14:40:10,799] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/checkpoint-800/tokenizer_config.json[0m
[32m[2022-09-01 14:40:10,799] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/checkpoint-800/special_tokens_map.json[0m
[32m[2022-09-01 14:40:25,046] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-01 14:40:25,047] [    INFO][0m - Loading best model from ./checkpoints_iflytek/checkpoint-400 (score: 0.825).[0m
[32m[2022-09-01 14:40:27,351] [    INFO][0m - train_runtime: 283.3191, train_samples_per_second: 28.237, train_steps_per_second: 3.53, train_loss: 0.033592132463926554, epoch: 40.0[0m
[32m[2022-09-01 14:40:27,353] [    INFO][0m - Saving model checkpoint to ./checkpoints_iflytek/[0m
[32m[2022-09-01 14:40:27,353] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 14:40:34,381] [    INFO][0m - tokenizer config file saved in ./checkpoints_iflytek/tokenizer_config.json[0m
[32m[2022-09-01 14:40:34,382] [    INFO][0m - Special tokens file saved in ./checkpoints_iflytek/special_tokens_map.json[0m
[32m[2022-09-01 14:40:34,383] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-01 14:40:34,383] [    INFO][0m -   epoch                    =       40.0[0m
[32m[2022-09-01 14:40:34,384] [    INFO][0m -   train_loss               =     0.0336[0m
[32m[2022-09-01 14:40:34,384] [    INFO][0m -   train_runtime            = 0:04:43.31[0m
[32m[2022-09-01 14:40:34,384] [    INFO][0m -   train_samples_per_second =     28.237[0m
[32m[2022-09-01 14:40:34,384] [    INFO][0m -   train_steps_per_second   =       3.53[0m
[32m[2022-09-01 14:40:34,388] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-01 14:40:34,388] [    INFO][0m -   Num examples = 1772[0m
[32m[2022-09-01 14:40:34,388] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 14:40:34,388] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 14:40:34,389] [    INFO][0m -   Total prediction steps = 56[0m
[32m[2022-09-01 14:40:42,547] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-01 14:40:42,547] [    INFO][0m -   test_accuracy           =     0.7641[0m
[32m[2022-09-01 14:40:42,547] [    INFO][0m -   test_loss               =      4.812[0m
[32m[2022-09-01 14:40:42,548] [    INFO][0m -   test_runtime            = 0:00:08.15[0m
[32m[2022-09-01 14:40:42,548] [    INFO][0m -   test_samples_per_second =     217.19[0m
[32m[2022-09-01 14:40:42,548] [    INFO][0m -   test_steps_per_second   =      6.864[0m
[32m[2022-09-01 14:40:42,548] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-01 14:40:42,548] [    INFO][0m -   Num examples = 2000[0m
[32m[2022-09-01 14:40:42,548] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 14:40:42,548] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 14:40:42,548] [    INFO][0m -   Total prediction steps = 63[0m
[32m[2022-09-01 14:40:53,263] [    INFO][0m - Predictions for bustm saved to ./fewclue_submit_examples.[0m
[]
run.sh: line 78: --freeze_plm: command not found
