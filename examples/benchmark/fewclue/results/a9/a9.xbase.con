 
==========
csldcp
==========
 
[33m[2022-09-01 10:18:48,935] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-09-01 10:18:48,935] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-01 10:18:48,935] [    INFO][0m - ============================================================[0m
[32m[2022-09-01 10:18:48,935] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-01 10:18:48,935] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-01 10:18:48,935] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-01 10:18:48,935] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-01 10:18:48,936] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-01 10:18:48,936] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-01 10:18:48,936] [    INFO][0m - model_name_or_path            :ernie-3.0-xbase-zh[0m
[32m[2022-09-01 10:18:48,936] [    INFO][0m - [0m
[32m[2022-09-01 10:18:48,936] [    INFO][0m - ============================================================[0m
[32m[2022-09-01 10:18:48,936] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-01 10:18:48,936] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-01 10:18:48,936] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-01 10:18:48,936] [    INFO][0m - prompt                        :{'hard':'阅读下边有关'}{'mask'}{'mask'}{'hard':'的材料'}{'text':'text_a'}[0m
[32m[2022-09-01 10:18:48,936] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-01 10:18:48,936] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-01 10:18:48,936] [    INFO][0m - task_name                     :csldcp[0m
[32m[2022-09-01 10:18:48,936] [    INFO][0m - [0m
[32m[2022-09-01 10:18:48,937] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/ernie_3.0_xbase_zh.pdparams[0m
W0901 10:18:48.938745 68491 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0901 10:18:48.942932 68491 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-01 10:18:54,527] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/ernie_3.0_xbase_zh_vocab.txt[0m
[32m[2022-09-01 10:18:54,554] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/tokenizer_config.json[0m
[32m[2022-09-01 10:18:54,554] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/special_tokens_map.json[0m
[32m[2022-09-01 10:18:54,556] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '阅读下边有关'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': '的材料'}, {'add_prefix_space': '', 'text': 'text_a'}][0m
[32m[2022-09-01 10:18:54,576] [    INFO][0m - {'中医学/中药学': 0, '中国语言文学': 1, '交通运输工程': 2, '体育学': 3, '作物学': 4, '信息与通信工程': 5, '光学工程': 6, '公共卫生与预防医学': 7, '公共管理': 8, '兵器科学与技术': 9, '军事学': 10, '农业工程': 11, '农业资源利用': 12, '农林经济管理': 13, '冶金工程': 14, '力学': 15, '动力工程及工程热物理': 16, '化学/化学工程与技术': 17, '历史学': 18, '口腔医学': 19, '哲学': 20, '园艺学': 21, '图书馆、情报与档案管理': 22, '土木工程': 23, '地球物理学': 24, '地理学': 25, '地质学/地质资源与地质工程': 26, '基础医学/临床医学': 27, '大气科学': 28, '天文学': 29, '工商管理': 30, '应用经济学': 31, '建筑学': 32, '心理学': 33, '控制科学与工程': 34, '政治学': 35, '教育学': 36, '数学': 37, '新闻传播学': 38, '机械工程': 39, '材料科学与工程': 40, '林学/林业工程': 41, '核科学与技术': 42, '植物保护': 43, '民族学': 44, '水产': 45, '水利工程': 46, '法学': 47, '测绘科学与技术': 48, '海洋科学': 49, '物理学': 50, '环境科学与工程': 51, '理论经济学': 52, '生物学/生物科学与工程': 53, '电子科学与技术': 54, '电气工程': 55, '畜牧学/兽医学': 56, '石油与天然气工程': 57, '矿业工程': 58, '社会学': 59, '纺织科学与工程': 60, '航空宇航科学与技术': 61, '船舶与海洋工程': 62, '艺术学': 63, '药学': 64, '计算机科学与技术': 65, '食品科学与工程': 66}[0m
2022-09-01 10:18:54,578 INFO [download.py:119] unique_endpoints {''}
[32m[2022-09-01 10:18:54,756] [    INFO][0m - ============================================================[0m
[32m[2022-09-01 10:18:54,756] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-01 10:18:54,756] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-01 10:18:54,756] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-01 10:18:54,757] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-01 10:18:54,757] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-01 10:18:54,757] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-01 10:18:54,757] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-01 10:18:54,757] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-01 10:18:54,757] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-01 10:18:54,757] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-01 10:18:54,757] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-01 10:18:54,757] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-01 10:18:54,757] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-01 10:18:54,757] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-01 10:18:54,757] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-01 10:18:54,758] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-01 10:18:54,758] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-01 10:18:54,758] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-09-01 10:18:54,758] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-09-01 10:18:54,758] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-09-01 10:18:54,758] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-01 10:18:54,758] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-01 10:18:54,758] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-01 10:18:54,758] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-01 10:18:54,758] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-01 10:18:54,759] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-01 10:18:54,759] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-01 10:18:54,759] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-01 10:18:54,759] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-01 10:18:54,759] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-09-01 10:18:54,759] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-01 10:18:54,759] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-01 10:18:54,759] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-01 10:18:54,759] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-01 10:18:54,760] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-01 10:18:54,760] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-01 10:18:54,760] [    INFO][0m - logging_dir                   :./checkpoints/runs/Sep01_10-18-48_instance-3bwob41y-01[0m
[32m[2022-09-01 10:18:54,760] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-01 10:18:54,760] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-01 10:18:54,760] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-01 10:18:54,760] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-01 10:18:54,760] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-01 10:18:54,760] [    INFO][0m - max_seq_length                :256[0m
[32m[2022-09-01 10:18:54,761] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-01 10:18:54,761] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-01 10:18:54,761] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-01 10:18:54,761] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-01 10:18:54,761] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-09-01 10:18:54,761] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-01 10:18:54,761] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-01 10:18:54,761] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-09-01 10:18:54,761] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-01 10:18:54,761] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-01 10:18:54,762] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-09-01 10:18:54,762] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-09-01 10:18:54,762] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-01 10:18:54,762] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-01 10:18:54,762] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-01 10:18:54,762] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-09-01 10:18:54,762] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-01 10:18:54,762] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-01 10:18:54,762] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-01 10:18:54,762] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-01 10:18:54,762] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-01 10:18:54,762] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-01 10:18:54,762] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-09-01 10:18:54,763] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-01 10:18:54,763] [    INFO][0m - save_steps                    :100[0m
[32m[2022-09-01 10:18:54,763] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-09-01 10:18:54,763] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-01 10:18:54,763] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-01 10:18:54,763] [    INFO][0m - seed                          :42[0m
[32m[2022-09-01 10:18:54,763] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-01 10:18:54,763] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-01 10:18:54,763] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-01 10:18:54,763] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-09-01 10:18:54,763] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-01 10:18:54,763] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-01 10:18:54,764] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-01 10:18:54,764] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-01 10:18:54,764] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-01 10:18:54,764] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-01 10:18:54,764] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-01 10:18:54,764] [    INFO][0m - [0m
[32m[2022-09-01 10:18:54,767] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-01 10:18:54,767] [    INFO][0m -   Num examples = 2036[0m
[32m[2022-09-01 10:18:54,767] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-09-01 10:18:54,767] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-09-01 10:18:54,767] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-09-01 10:18:54,767] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-01 10:18:54,767] [    INFO][0m -   Total optimization steps = 12750.0[0m
[32m[2022-09-01 10:18:54,767] [    INFO][0m -   Total num train samples = 101800[0m
[32m[2022-09-01 10:18:59,767] [    INFO][0m - loss: 3.72249222, learning_rate: 2.9976470588235296e-05, global_step: 10, interval_runtime: 4.998, interval_samples_per_second: 1.601, interval_steps_per_second: 2.001, epoch: 0.0392[0m
[32m[2022-09-01 10:19:03,538] [    INFO][0m - loss: 2.93492355, learning_rate: 2.9952941176470588e-05, global_step: 20, interval_runtime: 3.7711, interval_samples_per_second: 2.121, interval_steps_per_second: 2.652, epoch: 0.0784[0m
[32m[2022-09-01 10:19:07,320] [    INFO][0m - loss: 2.82269897, learning_rate: 2.9929411764705883e-05, global_step: 30, interval_runtime: 3.7829, interval_samples_per_second: 2.115, interval_steps_per_second: 2.643, epoch: 0.1176[0m
[32m[2022-09-01 10:19:11,096] [    INFO][0m - loss: 2.59030361, learning_rate: 2.9905882352941175e-05, global_step: 40, interval_runtime: 3.7755, interval_samples_per_second: 2.119, interval_steps_per_second: 2.649, epoch: 0.1569[0m
[32m[2022-09-01 10:19:14,885] [    INFO][0m - loss: 2.31718941, learning_rate: 2.988235294117647e-05, global_step: 50, interval_runtime: 3.7889, interval_samples_per_second: 2.111, interval_steps_per_second: 2.639, epoch: 0.1961[0m
[32m[2022-09-01 10:19:18,686] [    INFO][0m - loss: 2.58090019, learning_rate: 2.9858823529411763e-05, global_step: 60, interval_runtime: 3.8008, interval_samples_per_second: 2.105, interval_steps_per_second: 2.631, epoch: 0.2353[0m
[32m[2022-09-01 10:19:22,481] [    INFO][0m - loss: 2.34970608, learning_rate: 2.9835294117647058e-05, global_step: 70, interval_runtime: 3.7956, interval_samples_per_second: 2.108, interval_steps_per_second: 2.635, epoch: 0.2745[0m
[32m[2022-09-01 10:19:26,256] [    INFO][0m - loss: 1.88845005, learning_rate: 2.9811764705882357e-05, global_step: 80, interval_runtime: 3.7752, interval_samples_per_second: 2.119, interval_steps_per_second: 2.649, epoch: 0.3137[0m
[32m[2022-09-01 10:19:30,044] [    INFO][0m - loss: 2.35090561, learning_rate: 2.978823529411765e-05, global_step: 90, interval_runtime: 3.7881, interval_samples_per_second: 2.112, interval_steps_per_second: 2.64, epoch: 0.3529[0m
[32m[2022-09-01 10:19:33,823] [    INFO][0m - loss: 2.03666611, learning_rate: 2.9764705882352944e-05, global_step: 100, interval_runtime: 3.7783, interval_samples_per_second: 2.117, interval_steps_per_second: 2.647, epoch: 0.3922[0m
[32m[2022-09-01 10:19:33,824] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 10:19:33,824] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-01 10:19:33,824] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 10:19:33,824] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 10:19:33,824] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-09-01 10:20:09,338] [    INFO][0m - eval_loss: 1.8901926279067993, eval_accuracy: 0.5154738878143134, eval_runtime: 35.5141, eval_samples_per_second: 58.23, eval_steps_per_second: 1.83, epoch: 0.3922[0m
[32m[2022-09-01 10:20:09,339] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-09-01 10:20:09,339] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 10:20:13,037] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-01 10:20:13,038] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-01 10:20:22,981] [    INFO][0m - loss: 2.55233879, learning_rate: 2.9741176470588236e-05, global_step: 110, interval_runtime: 49.1578, interval_samples_per_second: 0.163, interval_steps_per_second: 0.203, epoch: 0.4314[0m
[32m[2022-09-01 10:20:26,748] [    INFO][0m - loss: 1.99291286, learning_rate: 2.971764705882353e-05, global_step: 120, interval_runtime: 3.7677, interval_samples_per_second: 2.123, interval_steps_per_second: 2.654, epoch: 0.4706[0m
[32m[2022-09-01 10:20:30,524] [    INFO][0m - loss: 2.33195057, learning_rate: 2.9694117647058823e-05, global_step: 130, interval_runtime: 3.7762, interval_samples_per_second: 2.119, interval_steps_per_second: 2.648, epoch: 0.5098[0m
[32m[2022-09-01 10:20:34,310] [    INFO][0m - loss: 2.05065594, learning_rate: 2.967058823529412e-05, global_step: 140, interval_runtime: 3.7857, interval_samples_per_second: 2.113, interval_steps_per_second: 2.642, epoch: 0.549[0m
[32m[2022-09-01 10:20:38,120] [    INFO][0m - loss: 1.93597603, learning_rate: 2.9647058823529414e-05, global_step: 150, interval_runtime: 3.8096, interval_samples_per_second: 2.1, interval_steps_per_second: 2.625, epoch: 0.5882[0m
[32m[2022-09-01 10:20:41,888] [    INFO][0m - loss: 2.22879868, learning_rate: 2.9623529411764706e-05, global_step: 160, interval_runtime: 3.7687, interval_samples_per_second: 2.123, interval_steps_per_second: 2.653, epoch: 0.6275[0m
[32m[2022-09-01 10:20:45,662] [    INFO][0m - loss: 1.88680553, learning_rate: 2.96e-05, global_step: 170, interval_runtime: 3.7734, interval_samples_per_second: 2.12, interval_steps_per_second: 2.65, epoch: 0.6667[0m
[32m[2022-09-01 10:20:49,444] [    INFO][0m - loss: 1.69340439, learning_rate: 2.9576470588235293e-05, global_step: 180, interval_runtime: 3.7821, interval_samples_per_second: 2.115, interval_steps_per_second: 2.644, epoch: 0.7059[0m
[32m[2022-09-01 10:20:53,246] [    INFO][0m - loss: 2.04398365, learning_rate: 2.955294117647059e-05, global_step: 190, interval_runtime: 3.8016, interval_samples_per_second: 2.104, interval_steps_per_second: 2.63, epoch: 0.7451[0m
[32m[2022-09-01 10:20:57,027] [    INFO][0m - loss: 2.18485107, learning_rate: 2.952941176470588e-05, global_step: 200, interval_runtime: 3.7813, interval_samples_per_second: 2.116, interval_steps_per_second: 2.645, epoch: 0.7843[0m
[32m[2022-09-01 10:20:57,027] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 10:20:57,028] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-01 10:20:57,028] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 10:20:57,028] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 10:20:57,028] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-09-01 10:21:32,546] [    INFO][0m - eval_loss: 1.749307632446289, eval_accuracy: 0.5498065764023211, eval_runtime: 35.5178, eval_samples_per_second: 58.224, eval_steps_per_second: 1.83, epoch: 0.7843[0m
[32m[2022-09-01 10:21:32,547] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-09-01 10:21:32,547] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 10:21:35,969] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-09-01 10:21:35,969] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-09-01 10:21:47,464] [    INFO][0m - loss: 1.86378212, learning_rate: 2.9505882352941176e-05, global_step: 210, interval_runtime: 50.4369, interval_samples_per_second: 0.159, interval_steps_per_second: 0.198, epoch: 0.8235[0m
[32m[2022-09-01 10:21:51,275] [    INFO][0m - loss: 1.63688431, learning_rate: 2.948235294117647e-05, global_step: 220, interval_runtime: 3.8105, interval_samples_per_second: 2.099, interval_steps_per_second: 2.624, epoch: 0.8627[0m
[32m[2022-09-01 10:21:55,040] [    INFO][0m - loss: 1.71763592, learning_rate: 2.9458823529411763e-05, global_step: 230, interval_runtime: 3.7656, interval_samples_per_second: 2.124, interval_steps_per_second: 2.656, epoch: 0.902[0m
[32m[2022-09-01 10:21:58,871] [    INFO][0m - loss: 2.52220993, learning_rate: 2.9435294117647062e-05, global_step: 240, interval_runtime: 3.831, interval_samples_per_second: 2.088, interval_steps_per_second: 2.61, epoch: 0.9412[0m
[32m[2022-09-01 10:22:02,674] [    INFO][0m - loss: 1.91464348, learning_rate: 2.9411764705882354e-05, global_step: 250, interval_runtime: 3.8031, interval_samples_per_second: 2.104, interval_steps_per_second: 2.629, epoch: 0.9804[0m
[32m[2022-09-01 10:22:06,343] [    INFO][0m - loss: 1.59021988, learning_rate: 2.938823529411765e-05, global_step: 260, interval_runtime: 3.6683, interval_samples_per_second: 2.181, interval_steps_per_second: 2.726, epoch: 1.0196[0m
[32m[2022-09-01 10:22:10,140] [    INFO][0m - loss: 1.20336962, learning_rate: 2.9364705882352944e-05, global_step: 270, interval_runtime: 3.7972, interval_samples_per_second: 2.107, interval_steps_per_second: 2.634, epoch: 1.0588[0m
[32m[2022-09-01 10:22:13,965] [    INFO][0m - loss: 1.19054565, learning_rate: 2.9341176470588236e-05, global_step: 280, interval_runtime: 3.8249, interval_samples_per_second: 2.092, interval_steps_per_second: 2.614, epoch: 1.098[0m
[32m[2022-09-01 10:22:17,763] [    INFO][0m - loss: 1.40375614, learning_rate: 2.9317647058823532e-05, global_step: 290, interval_runtime: 3.7977, interval_samples_per_second: 2.107, interval_steps_per_second: 2.633, epoch: 1.1373[0m
[32m[2022-09-01 10:22:21,570] [    INFO][0m - loss: 1.43271866, learning_rate: 2.9294117647058824e-05, global_step: 300, interval_runtime: 3.8082, interval_samples_per_second: 2.101, interval_steps_per_second: 2.626, epoch: 1.1765[0m
[32m[2022-09-01 10:22:21,571] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 10:22:21,571] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-01 10:22:21,571] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 10:22:21,571] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 10:22:21,571] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-09-01 10:22:56,975] [    INFO][0m - eval_loss: 1.698254942893982, eval_accuracy: 0.5565764023210832, eval_runtime: 35.4031, eval_samples_per_second: 58.413, eval_steps_per_second: 1.836, epoch: 1.1765[0m
[32m[2022-09-01 10:22:56,976] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-09-01 10:22:56,976] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 10:23:00,335] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-09-01 10:23:00,335] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-09-01 10:23:09,826] [    INFO][0m - loss: 1.03547153, learning_rate: 2.927058823529412e-05, global_step: 310, interval_runtime: 48.2557, interval_samples_per_second: 0.166, interval_steps_per_second: 0.207, epoch: 1.2157[0m
[32m[2022-09-01 10:23:13,624] [    INFO][0m - loss: 1.23164387, learning_rate: 2.924705882352941e-05, global_step: 320, interval_runtime: 3.798, interval_samples_per_second: 2.106, interval_steps_per_second: 2.633, epoch: 1.2549[0m
[32m[2022-09-01 10:23:17,406] [    INFO][0m - loss: 1.15699854, learning_rate: 2.9223529411764706e-05, global_step: 330, interval_runtime: 3.7802, interval_samples_per_second: 2.116, interval_steps_per_second: 2.645, epoch: 1.2941[0m
[32m[2022-09-01 10:23:21,213] [    INFO][0m - loss: 1.57327509, learning_rate: 2.92e-05, global_step: 340, interval_runtime: 3.8081, interval_samples_per_second: 2.101, interval_steps_per_second: 2.626, epoch: 1.3333[0m
[32m[2022-09-01 10:23:24,991] [    INFO][0m - loss: 1.69932861, learning_rate: 2.9176470588235294e-05, global_step: 350, interval_runtime: 3.779, interval_samples_per_second: 2.117, interval_steps_per_second: 2.646, epoch: 1.3725[0m
[32m[2022-09-01 10:23:28,812] [    INFO][0m - loss: 1.41672592, learning_rate: 2.915294117647059e-05, global_step: 360, interval_runtime: 3.8209, interval_samples_per_second: 2.094, interval_steps_per_second: 2.617, epoch: 1.4118[0m
[32m[2022-09-01 10:23:32,612] [    INFO][0m - loss: 1.6600687, learning_rate: 2.912941176470588e-05, global_step: 370, interval_runtime: 3.7998, interval_samples_per_second: 2.105, interval_steps_per_second: 2.632, epoch: 1.451[0m
[32m[2022-09-01 10:23:36,400] [    INFO][0m - loss: 1.32154503, learning_rate: 2.9105882352941176e-05, global_step: 380, interval_runtime: 3.7883, interval_samples_per_second: 2.112, interval_steps_per_second: 2.64, epoch: 1.4902[0m
[32m[2022-09-01 10:23:40,202] [    INFO][0m - loss: 1.35575361, learning_rate: 2.908235294117647e-05, global_step: 390, interval_runtime: 3.8019, interval_samples_per_second: 2.104, interval_steps_per_second: 2.63, epoch: 1.5294[0m
[32m[2022-09-01 10:23:44,006] [    INFO][0m - loss: 1.28908119, learning_rate: 2.9058823529411767e-05, global_step: 400, interval_runtime: 3.8035, interval_samples_per_second: 2.103, interval_steps_per_second: 2.629, epoch: 1.5686[0m
[32m[2022-09-01 10:23:44,006] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 10:23:44,007] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-01 10:23:44,007] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 10:23:44,007] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 10:23:44,007] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-09-01 10:24:19,511] [    INFO][0m - eval_loss: 1.693776249885559, eval_accuracy: 0.5701160541586073, eval_runtime: 35.504, eval_samples_per_second: 58.247, eval_steps_per_second: 1.831, epoch: 1.5686[0m
[32m[2022-09-01 10:24:19,512] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-09-01 10:24:19,512] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 10:24:23,008] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-09-01 10:24:23,009] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-09-01 10:24:33,438] [    INFO][0m - loss: 1.34515429, learning_rate: 2.9035294117647062e-05, global_step: 410, interval_runtime: 49.4323, interval_samples_per_second: 0.162, interval_steps_per_second: 0.202, epoch: 1.6078[0m
[32m[2022-09-01 10:24:37,237] [    INFO][0m - loss: 1.42788239, learning_rate: 2.9011764705882354e-05, global_step: 420, interval_runtime: 3.7985, interval_samples_per_second: 2.106, interval_steps_per_second: 2.633, epoch: 1.6471[0m
[32m[2022-09-01 10:24:41,062] [    INFO][0m - loss: 1.17024899, learning_rate: 2.898823529411765e-05, global_step: 430, interval_runtime: 3.8249, interval_samples_per_second: 2.092, interval_steps_per_second: 2.614, epoch: 1.6863[0m
[32m[2022-09-01 10:24:44,896] [    INFO][0m - loss: 1.40287237, learning_rate: 2.896470588235294e-05, global_step: 440, interval_runtime: 3.8345, interval_samples_per_second: 2.086, interval_steps_per_second: 2.608, epoch: 1.7255[0m
[32m[2022-09-01 10:24:48,686] [    INFO][0m - loss: 1.43804588, learning_rate: 2.8941176470588237e-05, global_step: 450, interval_runtime: 3.7898, interval_samples_per_second: 2.111, interval_steps_per_second: 2.639, epoch: 1.7647[0m
[32m[2022-09-01 10:24:52,473] [    INFO][0m - loss: 1.48411169, learning_rate: 2.891764705882353e-05, global_step: 460, interval_runtime: 3.7865, interval_samples_per_second: 2.113, interval_steps_per_second: 2.641, epoch: 1.8039[0m
[32m[2022-09-01 10:24:56,285] [    INFO][0m - loss: 1.46418209, learning_rate: 2.8894117647058824e-05, global_step: 470, interval_runtime: 3.8127, interval_samples_per_second: 2.098, interval_steps_per_second: 2.623, epoch: 1.8431[0m
[32m[2022-09-01 10:25:00,081] [    INFO][0m - loss: 1.43771915, learning_rate: 2.887058823529412e-05, global_step: 480, interval_runtime: 3.796, interval_samples_per_second: 2.108, interval_steps_per_second: 2.634, epoch: 1.8824[0m
[32m[2022-09-01 10:25:03,878] [    INFO][0m - loss: 1.34306946, learning_rate: 2.884705882352941e-05, global_step: 490, interval_runtime: 3.7973, interval_samples_per_second: 2.107, interval_steps_per_second: 2.633, epoch: 1.9216[0m
[32m[2022-09-01 10:25:07,674] [    INFO][0m - loss: 1.52816906, learning_rate: 2.8823529411764707e-05, global_step: 500, interval_runtime: 3.7957, interval_samples_per_second: 2.108, interval_steps_per_second: 2.635, epoch: 1.9608[0m
[32m[2022-09-01 10:25:07,675] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 10:25:07,675] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-01 10:25:07,675] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 10:25:07,675] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 10:25:07,675] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-09-01 10:25:43,407] [    INFO][0m - eval_loss: 1.5717973709106445, eval_accuracy: 0.5846228239845261, eval_runtime: 35.731, eval_samples_per_second: 57.877, eval_steps_per_second: 1.819, epoch: 1.9608[0m
[32m[2022-09-01 10:25:43,407] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-09-01 10:25:43,408] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 10:25:46,623] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-09-01 10:25:46,623] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-09-01 10:25:55,766] [    INFO][0m - loss: 1.12224503, learning_rate: 2.88e-05, global_step: 510, interval_runtime: 48.092, interval_samples_per_second: 0.166, interval_steps_per_second: 0.208, epoch: 2.0[0m
[32m[2022-09-01 10:25:59,680] [    INFO][0m - loss: 0.9328804, learning_rate: 2.8776470588235294e-05, global_step: 520, interval_runtime: 3.9141, interval_samples_per_second: 2.044, interval_steps_per_second: 2.555, epoch: 2.0392[0m
[32m[2022-09-01 10:26:03,462] [    INFO][0m - loss: 0.76091418, learning_rate: 2.8752941176470586e-05, global_step: 530, interval_runtime: 3.7817, interval_samples_per_second: 2.115, interval_steps_per_second: 2.644, epoch: 2.0784[0m
[32m[2022-09-01 10:26:07,275] [    INFO][0m - loss: 0.77236347, learning_rate: 2.872941176470588e-05, global_step: 540, interval_runtime: 3.8128, interval_samples_per_second: 2.098, interval_steps_per_second: 2.623, epoch: 2.1176[0m
[32m[2022-09-01 10:26:11,069] [    INFO][0m - loss: 0.93994703, learning_rate: 2.870588235294118e-05, global_step: 550, interval_runtime: 3.7941, interval_samples_per_second: 2.109, interval_steps_per_second: 2.636, epoch: 2.1569[0m
[32m[2022-09-01 10:26:14,856] [    INFO][0m - loss: 1.00558929, learning_rate: 2.8682352941176472e-05, global_step: 560, interval_runtime: 3.7873, interval_samples_per_second: 2.112, interval_steps_per_second: 2.64, epoch: 2.1961[0m
[32m[2022-09-01 10:26:18,672] [    INFO][0m - loss: 0.77432985, learning_rate: 2.8658823529411767e-05, global_step: 570, interval_runtime: 3.8152, interval_samples_per_second: 2.097, interval_steps_per_second: 2.621, epoch: 2.2353[0m
[32m[2022-09-01 10:26:22,475] [    INFO][0m - loss: 1.02313662, learning_rate: 2.863529411764706e-05, global_step: 580, interval_runtime: 3.8036, interval_samples_per_second: 2.103, interval_steps_per_second: 2.629, epoch: 2.2745[0m
[32m[2022-09-01 10:26:26,284] [    INFO][0m - loss: 1.19706125, learning_rate: 2.8611764705882355e-05, global_step: 590, interval_runtime: 3.809, interval_samples_per_second: 2.1, interval_steps_per_second: 2.625, epoch: 2.3137[0m
[32m[2022-09-01 10:26:30,074] [    INFO][0m - loss: 0.9039773, learning_rate: 2.8588235294117647e-05, global_step: 600, interval_runtime: 3.7896, interval_samples_per_second: 2.111, interval_steps_per_second: 2.639, epoch: 2.3529[0m
[32m[2022-09-01 10:26:30,074] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 10:26:30,074] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-01 10:26:30,074] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 10:26:30,075] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 10:26:30,075] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-09-01 10:27:05,674] [    INFO][0m - eval_loss: 1.784335732460022, eval_accuracy: 0.5831721470019342, eval_runtime: 35.5988, eval_samples_per_second: 58.092, eval_steps_per_second: 1.826, epoch: 2.3529[0m
[32m[2022-09-01 10:27:05,674] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-09-01 10:27:05,675] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 10:27:08,873] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-09-01 10:27:08,874] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-09-01 10:27:18,340] [    INFO][0m - loss: 0.99632797, learning_rate: 2.8564705882352942e-05, global_step: 610, interval_runtime: 48.2663, interval_samples_per_second: 0.166, interval_steps_per_second: 0.207, epoch: 2.3922[0m
[32m[2022-09-01 10:27:22,118] [    INFO][0m - loss: 0.86410265, learning_rate: 2.8541176470588237e-05, global_step: 620, interval_runtime: 3.778, interval_samples_per_second: 2.117, interval_steps_per_second: 2.647, epoch: 2.4314[0m
[32m[2022-09-01 10:27:25,918] [    INFO][0m - loss: 1.08217173, learning_rate: 2.851764705882353e-05, global_step: 630, interval_runtime: 3.7999, interval_samples_per_second: 2.105, interval_steps_per_second: 2.632, epoch: 2.4706[0m
[32m[2022-09-01 10:27:29,715] [    INFO][0m - loss: 0.6338799, learning_rate: 2.8494117647058825e-05, global_step: 640, interval_runtime: 3.7971, interval_samples_per_second: 2.107, interval_steps_per_second: 2.634, epoch: 2.5098[0m
[32m[2022-09-01 10:27:33,505] [    INFO][0m - loss: 0.68104224, learning_rate: 2.8470588235294117e-05, global_step: 650, interval_runtime: 3.7905, interval_samples_per_second: 2.111, interval_steps_per_second: 2.638, epoch: 2.549[0m
[32m[2022-09-01 10:27:37,318] [    INFO][0m - loss: 0.92324247, learning_rate: 2.8447058823529412e-05, global_step: 660, interval_runtime: 3.8126, interval_samples_per_second: 2.098, interval_steps_per_second: 2.623, epoch: 2.5882[0m
[32m[2022-09-01 10:27:41,108] [    INFO][0m - loss: 1.08842831, learning_rate: 2.8423529411764707e-05, global_step: 670, interval_runtime: 3.7902, interval_samples_per_second: 2.111, interval_steps_per_second: 2.638, epoch: 2.6275[0m
[32m[2022-09-01 10:27:44,927] [    INFO][0m - loss: 1.04215899, learning_rate: 2.84e-05, global_step: 680, interval_runtime: 3.818, interval_samples_per_second: 2.095, interval_steps_per_second: 2.619, epoch: 2.6667[0m
[32m[2022-09-01 10:27:48,744] [    INFO][0m - loss: 0.8278161, learning_rate: 2.8376470588235294e-05, global_step: 690, interval_runtime: 3.8172, interval_samples_per_second: 2.096, interval_steps_per_second: 2.62, epoch: 2.7059[0m
[32m[2022-09-01 10:27:52,572] [    INFO][0m - loss: 0.98395681, learning_rate: 2.835294117647059e-05, global_step: 700, interval_runtime: 3.8023, interval_samples_per_second: 2.104, interval_steps_per_second: 2.63, epoch: 2.7451[0m
[32m[2022-09-01 10:27:52,573] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 10:27:52,573] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-01 10:27:52,573] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 10:27:52,573] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 10:27:52,573] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-09-01 10:28:28,256] [    INFO][0m - eval_loss: 1.7529351711273193, eval_accuracy: 0.5802707930367504, eval_runtime: 35.6824, eval_samples_per_second: 57.956, eval_steps_per_second: 1.822, epoch: 2.7451[0m
[32m[2022-09-01 10:28:28,257] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-700[0m
[32m[2022-09-01 10:28:28,257] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 10:28:31,457] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-700/tokenizer_config.json[0m
[32m[2022-09-01 10:28:31,458] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-700/special_tokens_map.json[0m
[32m[2022-09-01 10:28:40,781] [    INFO][0m - loss: 1.00683508, learning_rate: 2.8329411764705885e-05, global_step: 710, interval_runtime: 48.2351, interval_samples_per_second: 0.166, interval_steps_per_second: 0.207, epoch: 2.7843[0m
[32m[2022-09-01 10:28:44,592] [    INFO][0m - loss: 0.88453035, learning_rate: 2.8305882352941177e-05, global_step: 720, interval_runtime: 3.811, interval_samples_per_second: 2.099, interval_steps_per_second: 2.624, epoch: 2.8235[0m
[32m[2022-09-01 10:28:48,372] [    INFO][0m - loss: 1.15645208, learning_rate: 2.8282352941176472e-05, global_step: 730, interval_runtime: 3.7798, interval_samples_per_second: 2.117, interval_steps_per_second: 2.646, epoch: 2.8627[0m
[32m[2022-09-01 10:28:52,196] [    INFO][0m - loss: 1.01216507, learning_rate: 2.8258823529411768e-05, global_step: 740, interval_runtime: 3.8245, interval_samples_per_second: 2.092, interval_steps_per_second: 2.615, epoch: 2.902[0m
[32m[2022-09-01 10:28:56,005] [    INFO][0m - loss: 1.02635832, learning_rate: 2.823529411764706e-05, global_step: 750, interval_runtime: 3.8087, interval_samples_per_second: 2.1, interval_steps_per_second: 2.626, epoch: 2.9412[0m
[32m[2022-09-01 10:28:59,798] [    INFO][0m - loss: 0.95771036, learning_rate: 2.8211764705882355e-05, global_step: 760, interval_runtime: 3.7928, interval_samples_per_second: 2.109, interval_steps_per_second: 2.637, epoch: 2.9804[0m
[32m[2022-09-01 10:29:03,551] [    INFO][0m - loss: 0.67727981, learning_rate: 2.8188235294117647e-05, global_step: 770, interval_runtime: 3.7522, interval_samples_per_second: 2.132, interval_steps_per_second: 2.665, epoch: 3.0196[0m
[32m[2022-09-01 10:29:07,367] [    INFO][0m - loss: 0.46473732, learning_rate: 2.8164705882352942e-05, global_step: 780, interval_runtime: 3.8171, interval_samples_per_second: 2.096, interval_steps_per_second: 2.62, epoch: 3.0588[0m
[32m[2022-09-01 10:29:11,178] [    INFO][0m - loss: 0.50923114, learning_rate: 2.8141176470588234e-05, global_step: 790, interval_runtime: 3.8107, interval_samples_per_second: 2.099, interval_steps_per_second: 2.624, epoch: 3.098[0m
[32m[2022-09-01 10:29:14,985] [    INFO][0m - loss: 0.64522963, learning_rate: 2.811764705882353e-05, global_step: 800, interval_runtime: 3.8072, interval_samples_per_second: 2.101, interval_steps_per_second: 2.627, epoch: 3.1373[0m
[32m[2022-09-01 10:29:14,986] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 10:29:14,986] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-01 10:29:14,986] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 10:29:14,986] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 10:29:14,986] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-09-01 10:29:50,699] [    INFO][0m - eval_loss: 1.9902877807617188, eval_accuracy: 0.601063829787234, eval_runtime: 35.7119, eval_samples_per_second: 57.908, eval_steps_per_second: 1.82, epoch: 3.1373[0m
[32m[2022-09-01 10:29:50,699] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-800[0m
[32m[2022-09-01 10:29:50,699] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 10:29:58,123] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-800/tokenizer_config.json[0m
[32m[2022-09-01 10:29:58,123] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-800/special_tokens_map.json[0m
[32m[2022-09-01 10:30:07,524] [    INFO][0m - loss: 0.25334532, learning_rate: 2.8094117647058825e-05, global_step: 810, interval_runtime: 52.5388, interval_samples_per_second: 0.152, interval_steps_per_second: 0.19, epoch: 3.1765[0m
[32m[2022-09-01 10:30:11,317] [    INFO][0m - loss: 0.50279317, learning_rate: 2.8070588235294117e-05, global_step: 820, interval_runtime: 3.7932, interval_samples_per_second: 2.109, interval_steps_per_second: 2.636, epoch: 3.2157[0m
[32m[2022-09-01 10:30:15,100] [    INFO][0m - loss: 0.58442416, learning_rate: 2.8047058823529412e-05, global_step: 830, interval_runtime: 3.7835, interval_samples_per_second: 2.114, interval_steps_per_second: 2.643, epoch: 3.2549[0m
[32m[2022-09-01 10:30:18,931] [    INFO][0m - loss: 0.58105731, learning_rate: 2.8023529411764704e-05, global_step: 840, interval_runtime: 3.8309, interval_samples_per_second: 2.088, interval_steps_per_second: 2.61, epoch: 3.2941[0m
[32m[2022-09-01 10:30:22,787] [    INFO][0m - loss: 0.84963694, learning_rate: 2.8e-05, global_step: 850, interval_runtime: 3.8557, interval_samples_per_second: 2.075, interval_steps_per_second: 2.594, epoch: 3.3333[0m
[32m[2022-09-01 10:30:26,602] [    INFO][0m - loss: 0.49683127, learning_rate: 2.7976470588235295e-05, global_step: 860, interval_runtime: 3.8149, interval_samples_per_second: 2.097, interval_steps_per_second: 2.621, epoch: 3.3725[0m
[32m[2022-09-01 10:30:30,396] [    INFO][0m - loss: 0.56397386, learning_rate: 2.795294117647059e-05, global_step: 870, interval_runtime: 3.7943, interval_samples_per_second: 2.108, interval_steps_per_second: 2.636, epoch: 3.4118[0m
[32m[2022-09-01 10:30:34,197] [    INFO][0m - loss: 0.69874735, learning_rate: 2.7929411764705886e-05, global_step: 880, interval_runtime: 3.8003, interval_samples_per_second: 2.105, interval_steps_per_second: 2.631, epoch: 3.451[0m
[32m[2022-09-01 10:30:38,003] [    INFO][0m - loss: 0.67677984, learning_rate: 2.7905882352941178e-05, global_step: 890, interval_runtime: 3.8067, interval_samples_per_second: 2.102, interval_steps_per_second: 2.627, epoch: 3.4902[0m
[32m[2022-09-01 10:30:41,800] [    INFO][0m - loss: 0.51853237, learning_rate: 2.7882352941176473e-05, global_step: 900, interval_runtime: 3.7964, interval_samples_per_second: 2.107, interval_steps_per_second: 2.634, epoch: 3.5294[0m
[32m[2022-09-01 10:30:41,800] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 10:30:41,800] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-01 10:30:41,800] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 10:30:41,800] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 10:30:41,801] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-09-01 10:31:17,468] [    INFO][0m - eval_loss: 2.1169352531433105, eval_accuracy: 0.5981624758220503, eval_runtime: 35.6671, eval_samples_per_second: 57.981, eval_steps_per_second: 1.822, epoch: 3.5294[0m
[32m[2022-09-01 10:31:17,469] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-900[0m
[32m[2022-09-01 10:31:17,469] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 10:31:20,724] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-900/tokenizer_config.json[0m
[32m[2022-09-01 10:31:20,725] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-900/special_tokens_map.json[0m
[32m[2022-09-01 10:31:30,096] [    INFO][0m - loss: 0.81237736, learning_rate: 2.7858823529411765e-05, global_step: 910, interval_runtime: 48.2961, interval_samples_per_second: 0.166, interval_steps_per_second: 0.207, epoch: 3.5686[0m
[32m[2022-09-01 10:31:33,875] [    INFO][0m - loss: 1.00141878, learning_rate: 2.783529411764706e-05, global_step: 920, interval_runtime: 3.7794, interval_samples_per_second: 2.117, interval_steps_per_second: 2.646, epoch: 3.6078[0m
[32m[2022-09-01 10:31:37,645] [    INFO][0m - loss: 0.49954658, learning_rate: 2.7811764705882352e-05, global_step: 930, interval_runtime: 3.77, interval_samples_per_second: 2.122, interval_steps_per_second: 2.653, epoch: 3.6471[0m
[32m[2022-09-01 10:31:41,447] [    INFO][0m - loss: 0.57267976, learning_rate: 2.7788235294117647e-05, global_step: 940, interval_runtime: 3.8016, interval_samples_per_second: 2.104, interval_steps_per_second: 2.63, epoch: 3.6863[0m
[32m[2022-09-01 10:31:45,247] [    INFO][0m - loss: 0.93482609, learning_rate: 2.7764705882352943e-05, global_step: 950, interval_runtime: 3.7993, interval_samples_per_second: 2.106, interval_steps_per_second: 2.632, epoch: 3.7255[0m
[32m[2022-09-01 10:31:49,076] [    INFO][0m - loss: 1.11177473, learning_rate: 2.7741176470588235e-05, global_step: 960, interval_runtime: 3.8304, interval_samples_per_second: 2.089, interval_steps_per_second: 2.611, epoch: 3.7647[0m
[32m[2022-09-01 10:31:57,928] [    INFO][0m - loss: 0.69562397, learning_rate: 2.771764705882353e-05, global_step: 970, interval_runtime: 3.792, interval_samples_per_second: 2.11, interval_steps_per_second: 2.637, epoch: 3.8039[0m
[32m[2022-09-01 10:32:01,926] [    INFO][0m - loss: 0.70398307, learning_rate: 2.7694117647058822e-05, global_step: 980, interval_runtime: 8.847, interval_samples_per_second: 0.904, interval_steps_per_second: 1.13, epoch: 3.8431[0m
[32m[2022-09-01 10:32:05,719] [    INFO][0m - loss: 0.63737116, learning_rate: 2.7670588235294117e-05, global_step: 990, interval_runtime: 4.0031, interval_samples_per_second: 1.998, interval_steps_per_second: 2.498, epoch: 3.8824[0m
[32m[2022-09-01 10:32:09,552] [    INFO][0m - loss: 0.88101254, learning_rate: 2.764705882352941e-05, global_step: 1000, interval_runtime: 3.8336, interval_samples_per_second: 2.087, interval_steps_per_second: 2.609, epoch: 3.9216[0m
[32m[2022-09-01 10:32:09,553] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 10:32:09,553] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-01 10:32:09,553] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 10:32:09,553] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 10:32:09,553] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-09-01 10:32:45,060] [    INFO][0m - eval_loss: 1.8925243616104126, eval_accuracy: 0.6112185686653772, eval_runtime: 35.5065, eval_samples_per_second: 58.243, eval_steps_per_second: 1.831, epoch: 3.9216[0m
[32m[2022-09-01 10:32:45,061] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1000[0m
[32m[2022-09-01 10:32:45,061] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 10:32:48,104] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1000/tokenizer_config.json[0m
[32m[2022-09-01 10:32:48,104] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1000/special_tokens_map.json[0m
[32m[2022-09-01 10:32:57,229] [    INFO][0m - loss: 0.60037036, learning_rate: 2.7623529411764705e-05, global_step: 1010, interval_runtime: 47.6762, interval_samples_per_second: 0.168, interval_steps_per_second: 0.21, epoch: 3.9608[0m
[32m[2022-09-01 10:33:00,817] [    INFO][0m - loss: 0.64697223, learning_rate: 2.7600000000000003e-05, global_step: 1020, interval_runtime: 3.5885, interval_samples_per_second: 2.229, interval_steps_per_second: 2.787, epoch: 4.0[0m
[32m[2022-09-01 10:33:04,719] [    INFO][0m - loss: 0.60935898, learning_rate: 2.7576470588235295e-05, global_step: 1030, interval_runtime: 3.902, interval_samples_per_second: 2.05, interval_steps_per_second: 2.563, epoch: 4.0392[0m
[32m[2022-09-01 10:33:08,537] [    INFO][0m - loss: 0.17295151, learning_rate: 2.755294117647059e-05, global_step: 1040, interval_runtime: 3.8166, interval_samples_per_second: 2.096, interval_steps_per_second: 2.62, epoch: 4.0784[0m
[32m[2022-09-01 10:33:12,357] [    INFO][0m - loss: 0.47755666, learning_rate: 2.7529411764705883e-05, global_step: 1050, interval_runtime: 3.8212, interval_samples_per_second: 2.094, interval_steps_per_second: 2.617, epoch: 4.1176[0m
[32m[2022-09-01 10:33:16,170] [    INFO][0m - loss: 0.45012102, learning_rate: 2.7505882352941178e-05, global_step: 1060, interval_runtime: 3.813, interval_samples_per_second: 2.098, interval_steps_per_second: 2.623, epoch: 4.1569[0m
[32m[2022-09-01 10:33:19,982] [    INFO][0m - loss: 0.19194869, learning_rate: 2.7482352941176473e-05, global_step: 1070, interval_runtime: 3.812, interval_samples_per_second: 2.099, interval_steps_per_second: 2.623, epoch: 4.1961[0m
[32m[2022-09-01 10:33:23,786] [    INFO][0m - loss: 0.14301702, learning_rate: 2.7458823529411765e-05, global_step: 1080, interval_runtime: 3.8043, interval_samples_per_second: 2.103, interval_steps_per_second: 2.629, epoch: 4.2353[0m
[32m[2022-09-01 10:33:27,592] [    INFO][0m - loss: 0.2970937, learning_rate: 2.743529411764706e-05, global_step: 1090, interval_runtime: 3.784, interval_samples_per_second: 2.114, interval_steps_per_second: 2.643, epoch: 4.2745[0m
[32m[2022-09-01 10:33:31,392] [    INFO][0m - loss: 0.67094078, learning_rate: 2.7411764705882353e-05, global_step: 1100, interval_runtime: 3.8215, interval_samples_per_second: 2.093, interval_steps_per_second: 2.617, epoch: 4.3137[0m
[32m[2022-09-01 10:33:31,392] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 10:33:31,392] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-01 10:33:31,392] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 10:33:31,392] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 10:33:31,393] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-09-01 10:34:07,087] [    INFO][0m - eval_loss: 2.5089588165283203, eval_accuracy: 0.6107350096711799, eval_runtime: 35.6943, eval_samples_per_second: 57.936, eval_steps_per_second: 1.821, epoch: 4.3137[0m
[32m[2022-09-01 10:34:07,088] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1100[0m
[32m[2022-09-01 10:34:07,088] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 10:34:10,296] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1100/tokenizer_config.json[0m
[32m[2022-09-01 10:34:10,297] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1100/special_tokens_map.json[0m
[32m[2022-09-01 10:34:19,410] [    INFO][0m - loss: 0.49167399, learning_rate: 2.7388235294117648e-05, global_step: 1110, interval_runtime: 48.0184, interval_samples_per_second: 0.167, interval_steps_per_second: 0.208, epoch: 4.3529[0m
[32m[2022-09-01 10:34:23,208] [    INFO][0m - loss: 0.4032692, learning_rate: 2.736470588235294e-05, global_step: 1120, interval_runtime: 3.7983, interval_samples_per_second: 2.106, interval_steps_per_second: 2.633, epoch: 4.3922[0m
[32m[2022-09-01 10:34:27,012] [    INFO][0m - loss: 0.50003757, learning_rate: 2.7341176470588235e-05, global_step: 1130, interval_runtime: 3.8033, interval_samples_per_second: 2.103, interval_steps_per_second: 2.629, epoch: 4.4314[0m
[32m[2022-09-01 10:34:30,808] [    INFO][0m - loss: 0.68569264, learning_rate: 2.731764705882353e-05, global_step: 1140, interval_runtime: 3.7967, interval_samples_per_second: 2.107, interval_steps_per_second: 2.634, epoch: 4.4706[0m
[32m[2022-09-01 10:34:34,633] [    INFO][0m - loss: 0.55821686, learning_rate: 2.7294117647058822e-05, global_step: 1150, interval_runtime: 3.8249, interval_samples_per_second: 2.092, interval_steps_per_second: 2.614, epoch: 4.5098[0m
[32m[2022-09-01 10:34:38,443] [    INFO][0m - loss: 0.25223346, learning_rate: 2.7270588235294118e-05, global_step: 1160, interval_runtime: 3.8092, interval_samples_per_second: 2.1, interval_steps_per_second: 2.625, epoch: 4.549[0m
[32m[2022-09-01 10:34:42,255] [    INFO][0m - loss: 0.29743538, learning_rate: 2.7247058823529413e-05, global_step: 1170, interval_runtime: 3.8121, interval_samples_per_second: 2.099, interval_steps_per_second: 2.623, epoch: 4.5882[0m
[32m[2022-09-01 10:34:46,068] [    INFO][0m - loss: 0.38945909, learning_rate: 2.722352941176471e-05, global_step: 1180, interval_runtime: 3.8131, interval_samples_per_second: 2.098, interval_steps_per_second: 2.623, epoch: 4.6275[0m
[32m[2022-09-01 10:34:51,174] [    INFO][0m - loss: 0.62143722, learning_rate: 2.72e-05, global_step: 1190, interval_runtime: 3.8126, interval_samples_per_second: 2.098, interval_steps_per_second: 2.623, epoch: 4.6667[0m
[32m[2022-09-01 10:34:54,972] [    INFO][0m - loss: 0.49368873, learning_rate: 2.7176470588235296e-05, global_step: 1200, interval_runtime: 5.0919, interval_samples_per_second: 1.571, interval_steps_per_second: 1.964, epoch: 4.7059[0m
[32m[2022-09-01 10:34:54,973] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 10:34:54,973] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-01 10:34:54,973] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 10:34:54,973] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 10:34:54,973] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-09-01 10:35:30,623] [    INFO][0m - eval_loss: 2.740532159805298, eval_accuracy: 0.589458413926499, eval_runtime: 35.6492, eval_samples_per_second: 58.01, eval_steps_per_second: 1.823, epoch: 4.7059[0m
[32m[2022-09-01 10:35:30,624] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1200[0m
[32m[2022-09-01 10:35:30,624] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 10:35:33,762] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1200/tokenizer_config.json[0m
[32m[2022-09-01 10:35:33,762] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1200/special_tokens_map.json[0m
[32m[2022-09-01 10:35:47,500] [    INFO][0m - loss: 0.23561301, learning_rate: 2.715294117647059e-05, global_step: 1210, interval_runtime: 52.5272, interval_samples_per_second: 0.152, interval_steps_per_second: 0.19, epoch: 4.7451[0m
[32m[2022-09-01 10:35:51,308] [    INFO][0m - loss: 0.42960844, learning_rate: 2.7129411764705883e-05, global_step: 1220, interval_runtime: 3.8083, interval_samples_per_second: 2.101, interval_steps_per_second: 2.626, epoch: 4.7843[0m
[32m[2022-09-01 10:35:55,102] [    INFO][0m - loss: 0.32220056, learning_rate: 2.710588235294118e-05, global_step: 1230, interval_runtime: 3.7938, interval_samples_per_second: 2.109, interval_steps_per_second: 2.636, epoch: 4.8235[0m
[32m[2022-09-01 10:35:58,918] [    INFO][0m - loss: 0.81067324, learning_rate: 2.708235294117647e-05, global_step: 1240, interval_runtime: 3.8169, interval_samples_per_second: 2.096, interval_steps_per_second: 2.62, epoch: 4.8627[0m
[32m[2022-09-01 10:36:02,713] [    INFO][0m - loss: 0.4564137, learning_rate: 2.7058823529411766e-05, global_step: 1250, interval_runtime: 3.7947, interval_samples_per_second: 2.108, interval_steps_per_second: 2.635, epoch: 4.902[0m
[32m[2022-09-01 10:36:06,513] [    INFO][0m - loss: 0.49442053, learning_rate: 2.7035294117647058e-05, global_step: 1260, interval_runtime: 3.7994, interval_samples_per_second: 2.106, interval_steps_per_second: 2.632, epoch: 4.9412[0m
[32m[2022-09-01 10:36:10,294] [    INFO][0m - loss: 0.68658333, learning_rate: 2.7011764705882353e-05, global_step: 1270, interval_runtime: 3.7793, interval_samples_per_second: 2.117, interval_steps_per_second: 2.646, epoch: 4.9804[0m
[32m[2022-09-01 10:36:13,970] [    INFO][0m - loss: 0.45281401, learning_rate: 2.698823529411765e-05, global_step: 1280, interval_runtime: 3.678, interval_samples_per_second: 2.175, interval_steps_per_second: 2.719, epoch: 5.0196[0m
[32m[2022-09-01 10:36:17,787] [    INFO][0m - loss: 0.33356333, learning_rate: 2.696470588235294e-05, global_step: 1290, interval_runtime: 3.8174, interval_samples_per_second: 2.096, interval_steps_per_second: 2.62, epoch: 5.0588[0m
[32m[2022-09-01 10:36:21,593] [    INFO][0m - loss: 0.29876113, learning_rate: 2.6941176470588236e-05, global_step: 1300, interval_runtime: 3.8055, interval_samples_per_second: 2.102, interval_steps_per_second: 2.628, epoch: 5.098[0m
[32m[2022-09-01 10:36:21,593] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 10:36:21,594] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-01 10:36:21,594] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 10:36:21,594] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 10:36:21,594] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-09-01 10:36:57,048] [    INFO][0m - eval_loss: 2.5063235759735107, eval_accuracy: 0.6020309477756286, eval_runtime: 35.4533, eval_samples_per_second: 58.33, eval_steps_per_second: 1.833, epoch: 5.098[0m
[32m[2022-09-01 10:36:57,049] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1300[0m
[32m[2022-09-01 10:36:57,049] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 10:37:00,149] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1300/tokenizer_config.json[0m
[32m[2022-09-01 10:37:00,149] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1300/special_tokens_map.json[0m
[32m[2022-09-01 10:37:09,226] [    INFO][0m - loss: 0.35782609, learning_rate: 2.6917647058823528e-05, global_step: 1310, interval_runtime: 47.6332, interval_samples_per_second: 0.168, interval_steps_per_second: 0.21, epoch: 5.1373[0m
[32m[2022-09-01 10:37:16,669] [    INFO][0m - loss: 0.37295389, learning_rate: 2.6894117647058823e-05, global_step: 1320, interval_runtime: 3.8042, interval_samples_per_second: 2.103, interval_steps_per_second: 2.629, epoch: 5.1765[0m
[32m[2022-09-01 10:37:20,465] [    INFO][0m - loss: 0.31878316, learning_rate: 2.6870588235294118e-05, global_step: 1330, interval_runtime: 7.4351, interval_samples_per_second: 1.076, interval_steps_per_second: 1.345, epoch: 5.2157[0m
[32m[2022-09-01 10:37:24,265] [    INFO][0m - loss: 0.21735322, learning_rate: 2.6847058823529414e-05, global_step: 1340, interval_runtime: 3.7998, interval_samples_per_second: 2.105, interval_steps_per_second: 2.632, epoch: 5.2549[0m
[32m[2022-09-01 10:37:28,065] [    INFO][0m - loss: 0.43237934, learning_rate: 2.682352941176471e-05, global_step: 1350, interval_runtime: 3.7998, interval_samples_per_second: 2.105, interval_steps_per_second: 2.632, epoch: 5.2941[0m
[32m[2022-09-01 10:37:31,873] [    INFO][0m - loss: 0.24518447, learning_rate: 2.68e-05, global_step: 1360, interval_runtime: 3.808, interval_samples_per_second: 2.101, interval_steps_per_second: 2.626, epoch: 5.3333[0m
[32m[2022-09-01 10:37:40,286] [    INFO][0m - loss: 0.42552743, learning_rate: 2.6776470588235296e-05, global_step: 1370, interval_runtime: 3.8114, interval_samples_per_second: 2.099, interval_steps_per_second: 2.624, epoch: 5.3725[0m
[32m[2022-09-01 10:37:44,095] [    INFO][0m - loss: 0.26146336, learning_rate: 2.6752941176470588e-05, global_step: 1380, interval_runtime: 8.4105, interval_samples_per_second: 0.951, interval_steps_per_second: 1.189, epoch: 5.4118[0m
[32m[2022-09-01 10:37:47,893] [    INFO][0m - loss: 0.36264255, learning_rate: 2.6729411764705884e-05, global_step: 1390, interval_runtime: 3.7972, interval_samples_per_second: 2.107, interval_steps_per_second: 2.633, epoch: 5.451[0m
[32m[2022-09-01 10:37:51,688] [    INFO][0m - loss: 0.42430105, learning_rate: 2.6705882352941175e-05, global_step: 1400, interval_runtime: 3.7958, interval_samples_per_second: 2.108, interval_steps_per_second: 2.634, epoch: 5.4902[0m
[32m[2022-09-01 10:37:51,689] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 10:37:51,689] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-01 10:37:51,689] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 10:37:51,689] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 10:37:51,689] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-09-01 10:38:27,523] [    INFO][0m - eval_loss: 3.054391384124756, eval_accuracy: 0.6170212765957447, eval_runtime: 35.8329, eval_samples_per_second: 57.712, eval_steps_per_second: 1.814, epoch: 5.4902[0m
[32m[2022-09-01 10:38:27,523] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1400[0m
[32m[2022-09-01 10:38:27,524] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 10:38:30,497] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1400/tokenizer_config.json[0m
[32m[2022-09-01 10:38:30,497] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1400/special_tokens_map.json[0m
[32m[2022-09-01 10:38:39,476] [    INFO][0m - loss: 0.87366104, learning_rate: 2.668235294117647e-05, global_step: 1410, interval_runtime: 47.7879, interval_samples_per_second: 0.167, interval_steps_per_second: 0.209, epoch: 5.5294[0m
[32m[2022-09-01 10:38:43,280] [    INFO][0m - loss: 0.41630778, learning_rate: 2.6658823529411766e-05, global_step: 1420, interval_runtime: 3.8034, interval_samples_per_second: 2.103, interval_steps_per_second: 2.629, epoch: 5.5686[0m
[32m[2022-09-01 10:38:47,094] [    INFO][0m - loss: 0.33325098, learning_rate: 2.6635294117647058e-05, global_step: 1430, interval_runtime: 3.8148, interval_samples_per_second: 2.097, interval_steps_per_second: 2.621, epoch: 5.6078[0m
[32m[2022-09-01 10:38:52,694] [    INFO][0m - loss: 0.17360255, learning_rate: 2.6611764705882353e-05, global_step: 1440, interval_runtime: 3.7989, interval_samples_per_second: 2.106, interval_steps_per_second: 2.632, epoch: 5.6471[0m
[32m[2022-09-01 10:38:56,491] [    INFO][0m - loss: 0.54430342, learning_rate: 2.6588235294117645e-05, global_step: 1450, interval_runtime: 5.5981, interval_samples_per_second: 1.429, interval_steps_per_second: 1.786, epoch: 5.6863[0m
[32m[2022-09-01 10:39:00,322] [    INFO][0m - loss: 0.36144824, learning_rate: 2.656470588235294e-05, global_step: 1460, interval_runtime: 3.8308, interval_samples_per_second: 2.088, interval_steps_per_second: 2.61, epoch: 5.7255[0m
[32m[2022-09-01 10:39:04,112] [    INFO][0m - loss: 0.52147288, learning_rate: 2.6541176470588236e-05, global_step: 1470, interval_runtime: 3.7895, interval_samples_per_second: 2.111, interval_steps_per_second: 2.639, epoch: 5.7647[0m
[32m[2022-09-01 10:39:07,903] [    INFO][0m - loss: 0.25487144, learning_rate: 2.651764705882353e-05, global_step: 1480, interval_runtime: 3.7909, interval_samples_per_second: 2.11, interval_steps_per_second: 2.638, epoch: 5.8039[0m
[32m[2022-09-01 10:39:11,738] [    INFO][0m - loss: 0.38186772, learning_rate: 2.6494117647058827e-05, global_step: 1490, interval_runtime: 3.8355, interval_samples_per_second: 2.086, interval_steps_per_second: 2.607, epoch: 5.8431[0m
[32m[2022-09-01 10:39:15,539] [    INFO][0m - loss: 0.51027741, learning_rate: 2.647058823529412e-05, global_step: 1500, interval_runtime: 3.8009, interval_samples_per_second: 2.105, interval_steps_per_second: 2.631, epoch: 5.8824[0m
[32m[2022-09-01 10:39:15,539] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 10:39:15,540] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-01 10:39:15,540] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 10:39:15,540] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 10:39:15,540] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-09-01 10:39:51,228] [    INFO][0m - eval_loss: 2.8949379920959473, eval_accuracy: 0.6073500967117988, eval_runtime: 35.6874, eval_samples_per_second: 57.948, eval_steps_per_second: 1.821, epoch: 5.8824[0m
[32m[2022-09-01 10:39:51,228] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1500[0m
[32m[2022-09-01 10:39:51,228] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 10:39:54,449] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1500/tokenizer_config.json[0m
[32m[2022-09-01 10:39:54,449] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1500/special_tokens_map.json[0m
[32m[2022-09-01 10:40:03,760] [    INFO][0m - loss: 0.2045707, learning_rate: 2.6447058823529414e-05, global_step: 1510, interval_runtime: 48.2212, interval_samples_per_second: 0.166, interval_steps_per_second: 0.207, epoch: 5.9216[0m
[32m[2022-09-01 10:40:07,566] [    INFO][0m - loss: 0.22486007, learning_rate: 2.6423529411764706e-05, global_step: 1520, interval_runtime: 3.8062, interval_samples_per_second: 2.102, interval_steps_per_second: 2.627, epoch: 5.9608[0m
[32m[2022-09-01 10:40:11,163] [    INFO][0m - loss: 0.32346737, learning_rate: 2.64e-05, global_step: 1530, interval_runtime: 3.5963, interval_samples_per_second: 2.224, interval_steps_per_second: 2.781, epoch: 6.0[0m
[32m[2022-09-01 10:40:15,053] [    INFO][0m - loss: 0.08512474, learning_rate: 2.6376470588235297e-05, global_step: 1540, interval_runtime: 3.8904, interval_samples_per_second: 2.056, interval_steps_per_second: 2.57, epoch: 6.0392[0m
[32m[2022-09-01 10:40:18,864] [    INFO][0m - loss: 0.16873244, learning_rate: 2.635294117647059e-05, global_step: 1550, interval_runtime: 3.8106, interval_samples_per_second: 2.099, interval_steps_per_second: 2.624, epoch: 6.0784[0m
[32m[2022-09-01 10:40:22,684] [    INFO][0m - loss: 0.24385295, learning_rate: 2.6329411764705884e-05, global_step: 1560, interval_runtime: 3.82, interval_samples_per_second: 2.094, interval_steps_per_second: 2.618, epoch: 6.1176[0m
[32m[2022-09-01 10:40:30,143] [    INFO][0m - loss: 0.19151375, learning_rate: 2.6305882352941176e-05, global_step: 1570, interval_runtime: 3.8085, interval_samples_per_second: 2.101, interval_steps_per_second: 2.626, epoch: 6.1569[0m
[32m[2022-09-01 10:40:33,929] [    INFO][0m - loss: 0.30284634, learning_rate: 2.628235294117647e-05, global_step: 1580, interval_runtime: 7.4366, interval_samples_per_second: 1.076, interval_steps_per_second: 1.345, epoch: 6.1961[0m
[32m[2022-09-01 10:40:37,715] [    INFO][0m - loss: 0.58586054, learning_rate: 2.6258823529411763e-05, global_step: 1590, interval_runtime: 3.7863, interval_samples_per_second: 2.113, interval_steps_per_second: 2.641, epoch: 6.2353[0m
[32m[2022-09-01 10:40:41,519] [    INFO][0m - loss: 0.3424644, learning_rate: 2.623529411764706e-05, global_step: 1600, interval_runtime: 3.8036, interval_samples_per_second: 2.103, interval_steps_per_second: 2.629, epoch: 6.2745[0m
[32m[2022-09-01 10:40:41,519] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 10:40:41,519] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-01 10:40:41,520] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 10:40:41,520] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 10:40:41,520] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-09-01 10:41:17,304] [    INFO][0m - eval_loss: 3.0779221057891846, eval_accuracy: 0.6146034816247582, eval_runtime: 35.7839, eval_samples_per_second: 57.791, eval_steps_per_second: 1.816, epoch: 6.2745[0m
[32m[2022-09-01 10:41:17,305] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1600[0m
[32m[2022-09-01 10:41:17,305] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 10:41:20,352] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1600/tokenizer_config.json[0m
[32m[2022-09-01 10:41:20,353] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1600/special_tokens_map.json[0m
[32m[2022-09-01 10:41:29,445] [    INFO][0m - loss: 0.39181244, learning_rate: 2.6211764705882354e-05, global_step: 1610, interval_runtime: 47.9257, interval_samples_per_second: 0.167, interval_steps_per_second: 0.209, epoch: 6.3137[0m
[32m[2022-09-01 10:41:33,239] [    INFO][0m - loss: 0.51654835, learning_rate: 2.6188235294117646e-05, global_step: 1620, interval_runtime: 3.7941, interval_samples_per_second: 2.109, interval_steps_per_second: 2.636, epoch: 6.3529[0m
[32m[2022-09-01 10:41:37,041] [    INFO][0m - loss: 0.07639267, learning_rate: 2.616470588235294e-05, global_step: 1630, interval_runtime: 3.8021, interval_samples_per_second: 2.104, interval_steps_per_second: 2.63, epoch: 6.3922[0m
[32m[2022-09-01 10:41:40,828] [    INFO][0m - loss: 0.30345097, learning_rate: 2.6141176470588237e-05, global_step: 1640, interval_runtime: 3.7869, interval_samples_per_second: 2.113, interval_steps_per_second: 2.641, epoch: 6.4314[0m
[32m[2022-09-01 10:41:44,622] [    INFO][0m - loss: 0.08049409, learning_rate: 2.6117647058823532e-05, global_step: 1650, interval_runtime: 3.7942, interval_samples_per_second: 2.108, interval_steps_per_second: 2.636, epoch: 6.4706[0m
[32m[2022-09-01 10:41:48,406] [    INFO][0m - loss: 0.39235663, learning_rate: 2.6094117647058824e-05, global_step: 1660, interval_runtime: 3.7837, interval_samples_per_second: 2.114, interval_steps_per_second: 2.643, epoch: 6.5098[0m
[32m[2022-09-01 10:41:52,193] [    INFO][0m - loss: 0.42281337, learning_rate: 2.607058823529412e-05, global_step: 1670, interval_runtime: 3.7872, interval_samples_per_second: 2.112, interval_steps_per_second: 2.64, epoch: 6.549[0m
[32m[2022-09-01 10:41:56,001] [    INFO][0m - loss: 0.22274303, learning_rate: 2.6047058823529414e-05, global_step: 1680, interval_runtime: 3.8084, interval_samples_per_second: 2.101, interval_steps_per_second: 2.626, epoch: 6.5882[0m
[32m[2022-09-01 10:41:59,796] [    INFO][0m - loss: 0.28963146, learning_rate: 2.6023529411764706e-05, global_step: 1690, interval_runtime: 3.7943, interval_samples_per_second: 2.108, interval_steps_per_second: 2.636, epoch: 6.6275[0m
[32m[2022-09-01 10:42:03,596] [    INFO][0m - loss: 0.21830781, learning_rate: 2.6000000000000002e-05, global_step: 1700, interval_runtime: 3.8003, interval_samples_per_second: 2.105, interval_steps_per_second: 2.631, epoch: 6.6667[0m
[32m[2022-09-01 10:42:03,596] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 10:42:03,596] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-01 10:42:03,597] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 10:42:03,597] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 10:42:03,597] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-09-01 10:42:39,154] [    INFO][0m - eval_loss: 3.516582727432251, eval_accuracy: 0.6025145067698259, eval_runtime: 35.5565, eval_samples_per_second: 58.161, eval_steps_per_second: 1.828, epoch: 6.6667[0m
[32m[2022-09-01 10:42:39,154] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1700[0m
[32m[2022-09-01 10:42:39,155] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 10:42:42,433] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1700/tokenizer_config.json[0m
[32m[2022-09-01 10:42:42,433] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1700/special_tokens_map.json[0m
[32m[2022-09-01 10:42:51,940] [    INFO][0m - loss: 0.3585598, learning_rate: 2.5976470588235294e-05, global_step: 1710, interval_runtime: 48.3441, interval_samples_per_second: 0.165, interval_steps_per_second: 0.207, epoch: 6.7059[0m
[32m[2022-09-01 10:42:55,729] [    INFO][0m - loss: 0.43632674, learning_rate: 2.595294117647059e-05, global_step: 1720, interval_runtime: 3.7886, interval_samples_per_second: 2.112, interval_steps_per_second: 2.64, epoch: 6.7451[0m
[32m[2022-09-01 10:42:59,523] [    INFO][0m - loss: 0.25494714, learning_rate: 2.592941176470588e-05, global_step: 1730, interval_runtime: 3.7944, interval_samples_per_second: 2.108, interval_steps_per_second: 2.635, epoch: 6.7843[0m
[32m[2022-09-01 10:43:03,334] [    INFO][0m - loss: 0.12717052, learning_rate: 2.5905882352941176e-05, global_step: 1740, interval_runtime: 3.8115, interval_samples_per_second: 2.099, interval_steps_per_second: 2.624, epoch: 6.8235[0m
[32m[2022-09-01 10:43:07,182] [    INFO][0m - loss: 0.22774315, learning_rate: 2.5882352941176472e-05, global_step: 1750, interval_runtime: 3.8474, interval_samples_per_second: 2.079, interval_steps_per_second: 2.599, epoch: 6.8627[0m
[32m[2022-09-01 10:43:10,987] [    INFO][0m - loss: 0.18319217, learning_rate: 2.5858823529411764e-05, global_step: 1760, interval_runtime: 3.8047, interval_samples_per_second: 2.103, interval_steps_per_second: 2.628, epoch: 6.902[0m
[32m[2022-09-01 10:43:15,182] [    INFO][0m - loss: 0.32070384, learning_rate: 2.583529411764706e-05, global_step: 1770, interval_runtime: 3.7726, interval_samples_per_second: 2.121, interval_steps_per_second: 2.651, epoch: 6.9412[0m
[32m[2022-09-01 10:43:18,999] [    INFO][0m - loss: 0.13849814, learning_rate: 2.581176470588235e-05, global_step: 1780, interval_runtime: 4.24, interval_samples_per_second: 1.887, interval_steps_per_second: 2.358, epoch: 6.9804[0m
[32m[2022-09-01 10:43:22,696] [    INFO][0m - loss: 0.01353988, learning_rate: 2.578823529411765e-05, global_step: 1790, interval_runtime: 3.697, interval_samples_per_second: 2.164, interval_steps_per_second: 2.705, epoch: 7.0196[0m
[32m[2022-09-01 10:43:26,496] [    INFO][0m - loss: 0.18295081, learning_rate: 2.576470588235294e-05, global_step: 1800, interval_runtime: 3.7996, interval_samples_per_second: 2.105, interval_steps_per_second: 2.632, epoch: 7.0588[0m
[32m[2022-09-01 10:43:26,496] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 10:43:26,497] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-01 10:43:26,497] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 10:43:26,497] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 10:43:26,497] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-09-01 10:44:02,106] [    INFO][0m - eval_loss: 3.5282235145568848, eval_accuracy: 0.6088007736943907, eval_runtime: 35.6053, eval_samples_per_second: 58.081, eval_steps_per_second: 1.826, epoch: 7.0588[0m
[32m[2022-09-01 10:44:02,107] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1800[0m
[32m[2022-09-01 10:44:02,107] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 10:44:05,230] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1800/tokenizer_config.json[0m
[32m[2022-09-01 10:44:05,230] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1800/special_tokens_map.json[0m
[32m[2022-09-01 10:44:10,598] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-01 10:44:10,598] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-1400 (score: 0.6170212765957447).[0m
[32m[2022-09-01 10:44:12,738] [    INFO][0m - train_runtime: 1517.9698, train_samples_per_second: 67.063, train_steps_per_second: 8.399, train_loss: 0.8906719084994661, epoch: 7.0588[0m
[32m[2022-09-01 10:44:12,739] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-09-01 10:44:12,740] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 10:44:15,611] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-09-01 10:44:15,611] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-09-01 10:44:15,613] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-01 10:44:15,613] [    INFO][0m -   epoch                    =     7.0588[0m
[32m[2022-09-01 10:44:15,613] [    INFO][0m -   train_loss               =     0.8907[0m
[32m[2022-09-01 10:44:15,613] [    INFO][0m -   train_runtime            = 0:25:17.96[0m
[32m[2022-09-01 10:44:15,613] [    INFO][0m -   train_samples_per_second =     67.063[0m
[32m[2022-09-01 10:44:15,613] [    INFO][0m -   train_steps_per_second   =      8.399[0m
[32m[2022-09-01 10:44:15,621] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-01 10:44:15,621] [    INFO][0m -   Num examples = 1784[0m
[32m[2022-09-01 10:44:15,621] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 10:44:15,621] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 10:44:15,621] [    INFO][0m -   Total prediction steps = 56[0m
[32m[2022-09-01 10:44:46,498] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-01 10:44:46,498] [    INFO][0m -   test_accuracy           =     0.6177[0m
[32m[2022-09-01 10:44:46,498] [    INFO][0m -   test_loss               =     3.0671[0m
[32m[2022-09-01 10:44:46,498] [    INFO][0m -   test_runtime            = 0:00:30.87[0m
[32m[2022-09-01 10:44:46,499] [    INFO][0m -   test_samples_per_second =     57.779[0m
[32m[2022-09-01 10:44:46,499] [    INFO][0m -   test_steps_per_second   =      1.814[0m
[32m[2022-09-01 10:44:46,499] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-01 10:44:46,499] [    INFO][0m -   Num examples = 2999[0m
[32m[2022-09-01 10:44:46,499] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 10:44:46,499] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 10:44:46,499] [    INFO][0m -   Total prediction steps = 94[0m
[32m[2022-09-01 10:45:44,088] [    INFO][0m - Predictions for csldcp saved to ./fewclue_submit_examples.[0m
[]
run.sh: line 78: --freeze_plm: command not found
 
==========
iflytek
==========
 
[33m[2022-09-01 10:45:48,579] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-09-01 10:45:48,580] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-01 10:45:48,580] [    INFO][0m - ============================================================[0m
[32m[2022-09-01 10:45:48,580] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-01 10:45:48,580] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-01 10:45:48,580] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-01 10:45:48,580] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-01 10:45:48,580] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-01 10:45:48,580] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-01 10:45:48,581] [    INFO][0m - model_name_or_path            :ernie-3.0-xbase-zh[0m
[32m[2022-09-01 10:45:48,581] [    INFO][0m - [0m
[32m[2022-09-01 10:45:48,581] [    INFO][0m - ============================================================[0m
[32m[2022-09-01 10:45:48,581] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-01 10:45:48,581] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-01 10:45:48,581] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-01 10:45:48,581] [    INFO][0m - prompt                        :{'text':'text_a'}{'hard':'这款应用是'}{'mask'}{'mask'}{'hard':'类型的。'}[0m
[32m[2022-09-01 10:45:48,581] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-01 10:45:48,581] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-01 10:45:48,581] [    INFO][0m - task_name                     :iflytek[0m
[32m[2022-09-01 10:45:48,581] [    INFO][0m - [0m
[32m[2022-09-01 10:45:48,582] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/ernie_3.0_xbase_zh.pdparams[0m
W0901 10:45:48.583633 45989 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0901 10:45:48.587827 45989 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-01 10:45:53,656] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/ernie_3.0_xbase_zh_vocab.txt[0m
[32m[2022-09-01 10:45:53,685] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/tokenizer_config.json[0m
[32m[2022-09-01 10:45:53,686] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/special_tokens_map.json[0m
[32m[2022-09-01 10:45:53,687] [    INFO][0m - Using template: [{'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '这款应用是'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': '类型的。'}][0m
[32m[2022-09-01 10:45:53,732] [    INFO][0m - {'K歌': 0, 'MOBA': 1, '中小学': 2, '买房': 3, '二手': 4, '亲子儿童': 5, '仙侠': 6, '休闲益智': 7, '体育咨讯': 8, '体育竞技': 9, '保险': 10, '借贷': 11, '免费WIFI': 12, '公共交通': 13, '公务员': 14, '其他': 15, '养生保健': 16, '兼职': 17, '减肥瘦身': 18, '出国': 19, '办公': 20, '动作类': 21, '医疗服务': 22, '卡牌': 23, '即时通讯': 24, '同城服务': 25, '团购': 26, '地图导航': 27, '外卖': 28, '女性': 29, '婚庆': 30, '婚恋社交': 31, '家政': 32, '射击游戏': 33, '小说': 34, '工作社交': 35, '工具': 36, '彩票': 37, '影像剪辑': 38, '影视娱乐': 39, '微博博客': 40, '快递物流': 41, '情侣社交': 42, '成人': 43, '成人教育': 44, '打车': 45, '技术': 46, '搞笑': 47, '摄影修图': 48, '支付': 49, '收款': 50, '政务': 51, '教辅': 52, '新闻': 53, '旅游资讯': 54, '日常养车': 55, '日程管理': 56, '杂志': 57, '棋牌中心': 58, '母婴': 59, '民宿短租': 60, '民航': 61, '求职': 62, '汽车交易': 63, '汽车咨询': 64, '漫画': 65, '理财': 66, '生活社交': 67, '电台': 68, '电商': 69, '电子产品': 70, '电影票务': 71, '百科': 72, '直播': 73, '相机': 74, '短视频': 75, '社交工具': 76, '社区服务': 77, '社区超市': 78, '租房': 79, '租车': 80, '笔记': 81, '策略': 82, '约会社交': 83, '经营': 84, '经营养成': 85, '绘画': 86, '综合预定': 87, '美妆美业': 88, '美颜': 89, '职考': 90, '股票': 91, '艺术': 92, '英语': 93, '菜谱': 94, '薅羊毛': 95, '行程管理': 96, '行车辅助': 97, '装修家居': 98, '视频': 99, '视频教育': 100, '记账': 101, '论坛圈子': 102, '语言(非英语)': 103, '购物咨询': 104, '辅助工具': 105, '运动健身': 106, '违章': 107, '酒店': 108, '铁路': 109, '银行': 110, '问答交流': 111, '问诊挂号': 112, '音乐': 113, '飞行空战': 114, '餐饮店': 115, '驾校': 116, '高等教育': 117, '魔幻': 118}[0m
2022-09-01 10:45:53,733 INFO [download.py:119] unique_endpoints {''}
[32m[2022-09-01 10:45:53,907] [    INFO][0m - ============================================================[0m
[32m[2022-09-01 10:45:53,907] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-01 10:45:53,908] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-01 10:45:53,908] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-01 10:45:53,908] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-01 10:45:53,908] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-01 10:45:53,908] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-01 10:45:53,908] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-01 10:45:53,908] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-01 10:45:53,908] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-01 10:45:53,908] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-01 10:45:53,908] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-01 10:45:53,909] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-01 10:45:53,909] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-01 10:45:53,909] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-01 10:45:53,909] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-01 10:45:53,909] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-01 10:45:53,909] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-01 10:45:53,909] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-09-01 10:45:53,909] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-09-01 10:45:53,909] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-09-01 10:45:53,909] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-01 10:45:53,909] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-01 10:45:53,909] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-01 10:45:53,909] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-01 10:45:53,910] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-01 10:45:53,910] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-01 10:45:53,910] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-01 10:45:53,910] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-01 10:45:53,910] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-01 10:45:53,910] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-09-01 10:45:53,910] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-01 10:45:53,910] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-01 10:45:53,910] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-01 10:45:53,910] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-01 10:45:53,910] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-01 10:45:53,910] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-01 10:45:53,910] [    INFO][0m - logging_dir                   :./checkpoints/runs/Sep01_10-45-48_instance-3bwob41y-01[0m
[32m[2022-09-01 10:45:53,910] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-01 10:45:53,911] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-01 10:45:53,911] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-01 10:45:53,911] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-01 10:45:53,911] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-01 10:45:53,911] [    INFO][0m - max_seq_length                :320[0m
[32m[2022-09-01 10:45:53,911] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-01 10:45:53,911] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-01 10:45:53,911] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-01 10:45:53,911] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-01 10:45:53,911] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-09-01 10:45:53,911] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-01 10:45:53,911] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-01 10:45:53,911] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-09-01 10:45:53,912] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-01 10:45:53,912] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-01 10:45:53,912] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-09-01 10:45:53,912] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-09-01 10:45:53,912] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-01 10:45:53,912] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-01 10:45:53,912] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-01 10:45:53,912] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-09-01 10:45:53,912] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-01 10:45:53,912] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-01 10:45:53,912] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-01 10:45:53,912] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-01 10:45:53,912] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-01 10:45:53,913] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-01 10:45:53,913] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-09-01 10:45:53,913] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-01 10:45:53,913] [    INFO][0m - save_steps                    :100[0m
[32m[2022-09-01 10:45:53,913] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-09-01 10:45:53,913] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-01 10:45:53,913] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-01 10:45:53,913] [    INFO][0m - seed                          :42[0m
[32m[2022-09-01 10:45:53,913] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-01 10:45:53,913] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-01 10:45:53,913] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-01 10:45:53,913] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-09-01 10:45:53,913] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-01 10:45:53,913] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-01 10:45:53,914] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-01 10:45:53,914] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-01 10:45:53,914] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-01 10:45:53,914] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-01 10:45:53,914] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-01 10:45:53,914] [    INFO][0m - [0m
[32m[2022-09-01 10:45:53,916] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-01 10:45:53,916] [    INFO][0m -   Num examples = 3024[0m
[32m[2022-09-01 10:45:53,916] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-09-01 10:45:53,917] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-09-01 10:45:53,917] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-09-01 10:45:53,917] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-01 10:45:53,917] [    INFO][0m -   Total optimization steps = 18900.0[0m
[32m[2022-09-01 10:45:53,917] [    INFO][0m -   Total num train samples = 151200[0m
[32m[2022-09-01 10:45:59,779] [    INFO][0m - loss: 4.01183815, learning_rate: 2.9984126984126986e-05, global_step: 10, interval_runtime: 5.8611, interval_samples_per_second: 1.365, interval_steps_per_second: 1.706, epoch: 0.0265[0m
[32m[2022-09-01 10:46:04,285] [    INFO][0m - loss: 3.01567192, learning_rate: 2.9968253968253967e-05, global_step: 20, interval_runtime: 4.5056, interval_samples_per_second: 1.776, interval_steps_per_second: 2.219, epoch: 0.0529[0m
[32m[2022-09-01 10:46:08,772] [    INFO][0m - loss: 3.36379662, learning_rate: 2.9952380952380952e-05, global_step: 30, interval_runtime: 4.4873, interval_samples_per_second: 1.783, interval_steps_per_second: 2.229, epoch: 0.0794[0m
[32m[2022-09-01 10:46:13,237] [    INFO][0m - loss: 2.67840576, learning_rate: 2.9936507936507937e-05, global_step: 40, interval_runtime: 4.465, interval_samples_per_second: 1.792, interval_steps_per_second: 2.24, epoch: 0.1058[0m
[32m[2022-09-01 10:46:17,779] [    INFO][0m - loss: 2.98733521, learning_rate: 2.992063492063492e-05, global_step: 50, interval_runtime: 4.5419, interval_samples_per_second: 1.761, interval_steps_per_second: 2.202, epoch: 0.1323[0m
[32m[2022-09-01 10:46:22,283] [    INFO][0m - loss: 3.03406467, learning_rate: 2.9904761904761907e-05, global_step: 60, interval_runtime: 4.5042, interval_samples_per_second: 1.776, interval_steps_per_second: 2.22, epoch: 0.1587[0m
[32m[2022-09-01 10:46:26,770] [    INFO][0m - loss: 2.93484612, learning_rate: 2.9888888888888892e-05, global_step: 70, interval_runtime: 4.4859, interval_samples_per_second: 1.783, interval_steps_per_second: 2.229, epoch: 0.1852[0m
[32m[2022-09-01 10:46:31,237] [    INFO][0m - loss: 2.417873, learning_rate: 2.9873015873015874e-05, global_step: 80, interval_runtime: 4.4682, interval_samples_per_second: 1.79, interval_steps_per_second: 2.238, epoch: 0.2116[0m
[32m[2022-09-01 10:46:35,710] [    INFO][0m - loss: 2.61512299, learning_rate: 2.985714285714286e-05, global_step: 90, interval_runtime: 4.472, interval_samples_per_second: 1.789, interval_steps_per_second: 2.236, epoch: 0.2381[0m
[32m[2022-09-01 10:46:40,171] [    INFO][0m - loss: 2.67261143, learning_rate: 2.984126984126984e-05, global_step: 100, interval_runtime: 4.4624, interval_samples_per_second: 1.793, interval_steps_per_second: 2.241, epoch: 0.2646[0m
[32m[2022-09-01 10:46:40,172] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 10:46:40,172] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-01 10:46:40,172] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 10:46:40,172] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 10:46:40,172] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-09-01 10:47:09,820] [    INFO][0m - eval_loss: 2.297119379043579, eval_accuracy: 0.4654042243262928, eval_runtime: 29.6473, eval_samples_per_second: 46.311, eval_steps_per_second: 1.45, epoch: 0.2646[0m
[32m[2022-09-01 10:47:09,821] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-09-01 10:47:09,821] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 10:47:17,775] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-01 10:47:17,776] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-01 10:47:37,076] [    INFO][0m - loss: 2.46394501, learning_rate: 2.9825396825396825e-05, global_step: 110, interval_runtime: 56.9039, interval_samples_per_second: 0.141, interval_steps_per_second: 0.176, epoch: 0.291[0m
[32m[2022-09-01 10:47:41,546] [    INFO][0m - loss: 2.4290575, learning_rate: 2.980952380952381e-05, global_step: 120, interval_runtime: 4.4699, interval_samples_per_second: 1.79, interval_steps_per_second: 2.237, epoch: 0.3175[0m
[32m[2022-09-01 10:47:45,994] [    INFO][0m - loss: 2.52507191, learning_rate: 2.9793650793650792e-05, global_step: 130, interval_runtime: 4.4483, interval_samples_per_second: 1.798, interval_steps_per_second: 2.248, epoch: 0.3439[0m
[32m[2022-09-01 10:47:50,488] [    INFO][0m - loss: 2.46730194, learning_rate: 2.9777777777777777e-05, global_step: 140, interval_runtime: 4.4944, interval_samples_per_second: 1.78, interval_steps_per_second: 2.225, epoch: 0.3704[0m
[32m[2022-09-01 10:47:54,999] [    INFO][0m - loss: 2.7017025, learning_rate: 2.9761904761904762e-05, global_step: 150, interval_runtime: 4.511, interval_samples_per_second: 1.773, interval_steps_per_second: 2.217, epoch: 0.3968[0m
[32m[2022-09-01 10:47:59,480] [    INFO][0m - loss: 2.49224434, learning_rate: 2.9746031746031747e-05, global_step: 160, interval_runtime: 4.4806, interval_samples_per_second: 1.785, interval_steps_per_second: 2.232, epoch: 0.4233[0m
[32m[2022-09-01 10:48:04,002] [    INFO][0m - loss: 2.19622021, learning_rate: 2.9730158730158732e-05, global_step: 170, interval_runtime: 4.5225, interval_samples_per_second: 1.769, interval_steps_per_second: 2.211, epoch: 0.4497[0m
[32m[2022-09-01 10:48:09,497] [    INFO][0m - loss: 2.51712151, learning_rate: 2.9714285714285717e-05, global_step: 180, interval_runtime: 4.4797, interval_samples_per_second: 1.786, interval_steps_per_second: 2.232, epoch: 0.4762[0m
[32m[2022-09-01 10:48:13,994] [    INFO][0m - loss: 2.09426899, learning_rate: 2.96984126984127e-05, global_step: 190, interval_runtime: 5.5124, interval_samples_per_second: 1.451, interval_steps_per_second: 1.814, epoch: 0.5026[0m
[32m[2022-09-01 10:48:18,473] [    INFO][0m - loss: 2.21250038, learning_rate: 2.9682539682539683e-05, global_step: 200, interval_runtime: 4.4787, interval_samples_per_second: 1.786, interval_steps_per_second: 2.233, epoch: 0.5291[0m
[32m[2022-09-01 10:48:18,474] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 10:48:18,474] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-01 10:48:18,474] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 10:48:18,474] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 10:48:18,474] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-09-01 10:48:48,003] [    INFO][0m - eval_loss: 2.2657406330108643, eval_accuracy: 0.4493809176984705, eval_runtime: 29.5282, eval_samples_per_second: 46.498, eval_steps_per_second: 1.456, epoch: 0.5291[0m
[32m[2022-09-01 10:48:48,004] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-09-01 10:48:48,004] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 10:48:56,098] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-09-01 10:48:56,099] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-09-01 10:49:14,975] [    INFO][0m - loss: 2.29682999, learning_rate: 2.966666666666667e-05, global_step: 210, interval_runtime: 56.5018, interval_samples_per_second: 0.142, interval_steps_per_second: 0.177, epoch: 0.5556[0m
[32m[2022-09-01 10:49:19,438] [    INFO][0m - loss: 2.53025551, learning_rate: 2.965079365079365e-05, global_step: 220, interval_runtime: 4.4624, interval_samples_per_second: 1.793, interval_steps_per_second: 2.241, epoch: 0.582[0m
[32m[2022-09-01 10:49:23,912] [    INFO][0m - loss: 1.97493248, learning_rate: 2.9634920634920635e-05, global_step: 230, interval_runtime: 4.4743, interval_samples_per_second: 1.788, interval_steps_per_second: 2.235, epoch: 0.6085[0m
[32m[2022-09-01 10:49:32,113] [    INFO][0m - loss: 2.20894356, learning_rate: 2.961904761904762e-05, global_step: 240, interval_runtime: 4.4803, interval_samples_per_second: 1.786, interval_steps_per_second: 2.232, epoch: 0.6349[0m
[32m[2022-09-01 10:49:36,573] [    INFO][0m - loss: 2.11528492, learning_rate: 2.96031746031746e-05, global_step: 250, interval_runtime: 8.1811, interval_samples_per_second: 0.978, interval_steps_per_second: 1.222, epoch: 0.6614[0m
[32m[2022-09-01 10:49:41,072] [    INFO][0m - loss: 2.35233688, learning_rate: 2.958730158730159e-05, global_step: 260, interval_runtime: 4.4986, interval_samples_per_second: 1.778, interval_steps_per_second: 2.223, epoch: 0.6878[0m
[32m[2022-09-01 10:49:45,512] [    INFO][0m - loss: 1.90321808, learning_rate: 2.9571428571428575e-05, global_step: 270, interval_runtime: 4.4407, interval_samples_per_second: 1.802, interval_steps_per_second: 2.252, epoch: 0.7143[0m
[32m[2022-09-01 10:49:49,958] [    INFO][0m - loss: 2.27277832, learning_rate: 2.9555555555555556e-05, global_step: 280, interval_runtime: 4.4456, interval_samples_per_second: 1.8, interval_steps_per_second: 2.249, epoch: 0.7407[0m
[32m[2022-09-01 10:49:54,437] [    INFO][0m - loss: 2.15774231, learning_rate: 2.953968253968254e-05, global_step: 290, interval_runtime: 4.4794, interval_samples_per_second: 1.786, interval_steps_per_second: 2.232, epoch: 0.7672[0m
[32m[2022-09-01 10:49:58,898] [    INFO][0m - loss: 2.08219566, learning_rate: 2.9523809523809523e-05, global_step: 300, interval_runtime: 4.461, interval_samples_per_second: 1.793, interval_steps_per_second: 2.242, epoch: 0.7937[0m
[32m[2022-09-01 10:49:58,899] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 10:49:58,899] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-01 10:49:58,899] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 10:49:58,899] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 10:49:58,900] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-09-01 10:50:28,544] [    INFO][0m - eval_loss: 2.0942795276641846, eval_accuracy: 0.48725418790968683, eval_runtime: 29.6441, eval_samples_per_second: 46.316, eval_steps_per_second: 1.451, epoch: 0.7937[0m
[32m[2022-09-01 10:50:28,544] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-09-01 10:50:28,545] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 10:50:36,379] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-09-01 10:50:36,380] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-09-01 10:50:55,142] [    INFO][0m - loss: 2.38975887, learning_rate: 2.9507936507936508e-05, global_step: 310, interval_runtime: 56.2435, interval_samples_per_second: 0.142, interval_steps_per_second: 0.178, epoch: 0.8201[0m
[32m[2022-09-01 10:50:59,689] [    INFO][0m - loss: 1.65517788, learning_rate: 2.9492063492063493e-05, global_step: 320, interval_runtime: 4.547, interval_samples_per_second: 1.759, interval_steps_per_second: 2.199, epoch: 0.8466[0m
[32m[2022-09-01 10:51:04,147] [    INFO][0m - loss: 2.10718575, learning_rate: 2.9476190476190475e-05, global_step: 330, interval_runtime: 4.4583, interval_samples_per_second: 1.794, interval_steps_per_second: 2.243, epoch: 0.873[0m
[32m[2022-09-01 10:51:08,628] [    INFO][0m - loss: 2.09928074, learning_rate: 2.946031746031746e-05, global_step: 340, interval_runtime: 4.4809, interval_samples_per_second: 1.785, interval_steps_per_second: 2.232, epoch: 0.8995[0m
[32m[2022-09-01 10:51:13,079] [    INFO][0m - loss: 2.33419247, learning_rate: 2.9444444444444445e-05, global_step: 350, interval_runtime: 4.4515, interval_samples_per_second: 1.797, interval_steps_per_second: 2.246, epoch: 0.9259[0m
[32m[2022-09-01 10:51:17,556] [    INFO][0m - loss: 2.34749489, learning_rate: 2.942857142857143e-05, global_step: 360, interval_runtime: 4.4764, interval_samples_per_second: 1.787, interval_steps_per_second: 2.234, epoch: 0.9524[0m
[32m[2022-09-01 10:51:22,046] [    INFO][0m - loss: 2.16990509, learning_rate: 2.9412698412698414e-05, global_step: 370, interval_runtime: 4.4904, interval_samples_per_second: 1.782, interval_steps_per_second: 2.227, epoch: 0.9788[0m
[32m[2022-09-01 10:51:26,644] [    INFO][0m - loss: 1.61857376, learning_rate: 2.93968253968254e-05, global_step: 380, interval_runtime: 4.598, interval_samples_per_second: 1.74, interval_steps_per_second: 2.175, epoch: 1.0053[0m
[32m[2022-09-01 10:51:33,331] [    INFO][0m - loss: 1.5593812, learning_rate: 2.938095238095238e-05, global_step: 390, interval_runtime: 4.4933, interval_samples_per_second: 1.78, interval_steps_per_second: 2.226, epoch: 1.0317[0m
[32m[2022-09-01 10:51:37,797] [    INFO][0m - loss: 1.46961231, learning_rate: 2.9365079365079366e-05, global_step: 400, interval_runtime: 6.659, interval_samples_per_second: 1.201, interval_steps_per_second: 1.502, epoch: 1.0582[0m
[32m[2022-09-01 10:51:37,797] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 10:51:37,798] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-01 10:51:37,798] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 10:51:37,798] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 10:51:37,798] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-09-01 10:52:07,315] [    INFO][0m - eval_loss: 2.1491291522979736, eval_accuracy: 0.4989075018208303, eval_runtime: 29.5167, eval_samples_per_second: 46.516, eval_steps_per_second: 1.457, epoch: 1.0582[0m
[32m[2022-09-01 10:52:07,316] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-09-01 10:52:07,316] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 10:52:15,347] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-09-01 10:52:15,347] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-09-01 10:52:35,229] [    INFO][0m - loss: 1.53520088, learning_rate: 2.934920634920635e-05, global_step: 410, interval_runtime: 57.432, interval_samples_per_second: 0.139, interval_steps_per_second: 0.174, epoch: 1.0847[0m
[32m[2022-09-01 10:52:39,711] [    INFO][0m - loss: 1.69205723, learning_rate: 2.9333333333333333e-05, global_step: 420, interval_runtime: 4.4818, interval_samples_per_second: 1.785, interval_steps_per_second: 2.231, epoch: 1.1111[0m
[32m[2022-09-01 10:52:44,175] [    INFO][0m - loss: 1.54964962, learning_rate: 2.9317460317460318e-05, global_step: 430, interval_runtime: 4.4639, interval_samples_per_second: 1.792, interval_steps_per_second: 2.24, epoch: 1.1376[0m
[32m[2022-09-01 10:52:48,639] [    INFO][0m - loss: 1.45020018, learning_rate: 2.9301587301587303e-05, global_step: 440, interval_runtime: 4.4648, interval_samples_per_second: 1.792, interval_steps_per_second: 2.24, epoch: 1.164[0m
[32m[2022-09-01 10:52:53,115] [    INFO][0m - loss: 1.65719738, learning_rate: 2.9285714285714284e-05, global_step: 450, interval_runtime: 4.4763, interval_samples_per_second: 1.787, interval_steps_per_second: 2.234, epoch: 1.1905[0m
[32m[2022-09-01 10:52:57,604] [    INFO][0m - loss: 1.83387699, learning_rate: 2.9269841269841272e-05, global_step: 460, interval_runtime: 4.4888, interval_samples_per_second: 1.782, interval_steps_per_second: 2.228, epoch: 1.2169[0m
[32m[2022-09-01 10:53:02,085] [    INFO][0m - loss: 1.61774635, learning_rate: 2.9253968253968257e-05, global_step: 470, interval_runtime: 4.481, interval_samples_per_second: 1.785, interval_steps_per_second: 2.232, epoch: 1.2434[0m
[32m[2022-09-01 10:53:06,553] [    INFO][0m - loss: 1.57924833, learning_rate: 2.923809523809524e-05, global_step: 480, interval_runtime: 4.4673, interval_samples_per_second: 1.791, interval_steps_per_second: 2.238, epoch: 1.2698[0m
[32m[2022-09-01 10:53:11,033] [    INFO][0m - loss: 1.56314945, learning_rate: 2.9222222222222224e-05, global_step: 490, interval_runtime: 4.4804, interval_samples_per_second: 1.786, interval_steps_per_second: 2.232, epoch: 1.2963[0m
[32m[2022-09-01 10:53:15,496] [    INFO][0m - loss: 1.87278233, learning_rate: 2.9206349206349206e-05, global_step: 500, interval_runtime: 4.4632, interval_samples_per_second: 1.792, interval_steps_per_second: 2.241, epoch: 1.3228[0m
[32m[2022-09-01 10:53:15,497] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 10:53:15,497] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-01 10:53:15,497] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 10:53:15,497] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 10:53:15,497] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-09-01 10:53:45,008] [    INFO][0m - eval_loss: 2.178807020187378, eval_accuracy: 0.48434085943190097, eval_runtime: 29.5106, eval_samples_per_second: 46.526, eval_steps_per_second: 1.457, epoch: 1.3228[0m
[32m[2022-09-01 10:53:45,009] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-09-01 10:53:45,009] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 10:53:52,773] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-09-01 10:53:52,774] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-09-01 10:54:11,707] [    INFO][0m - loss: 2.0163538, learning_rate: 2.919047619047619e-05, global_step: 510, interval_runtime: 56.2108, interval_samples_per_second: 0.142, interval_steps_per_second: 0.178, epoch: 1.3492[0m
[32m[2022-09-01 10:54:16,170] [    INFO][0m - loss: 1.81589775, learning_rate: 2.9174603174603176e-05, global_step: 520, interval_runtime: 4.4626, interval_samples_per_second: 1.793, interval_steps_per_second: 2.241, epoch: 1.3757[0m
[32m[2022-09-01 10:54:20,624] [    INFO][0m - loss: 1.60423412, learning_rate: 2.9158730158730157e-05, global_step: 530, interval_runtime: 4.4542, interval_samples_per_second: 1.796, interval_steps_per_second: 2.245, epoch: 1.4021[0m
[32m[2022-09-01 10:54:25,147] [    INFO][0m - loss: 1.77859688, learning_rate: 2.9142857142857142e-05, global_step: 540, interval_runtime: 4.5232, interval_samples_per_second: 1.769, interval_steps_per_second: 2.211, epoch: 1.4286[0m
[32m[2022-09-01 10:54:29,614] [    INFO][0m - loss: 1.47977219, learning_rate: 2.9126984126984127e-05, global_step: 550, interval_runtime: 4.4671, interval_samples_per_second: 1.791, interval_steps_per_second: 2.239, epoch: 1.455[0m
[32m[2022-09-01 10:54:34,104] [    INFO][0m - loss: 1.64157753, learning_rate: 2.9111111111111112e-05, global_step: 560, interval_runtime: 4.4889, interval_samples_per_second: 1.782, interval_steps_per_second: 2.228, epoch: 1.4815[0m
[32m[2022-09-01 10:54:38,562] [    INFO][0m - loss: 1.73969574, learning_rate: 2.9095238095238097e-05, global_step: 570, interval_runtime: 4.4585, interval_samples_per_second: 1.794, interval_steps_per_second: 2.243, epoch: 1.5079[0m
[32m[2022-09-01 10:54:43,063] [    INFO][0m - loss: 1.71040478, learning_rate: 2.9079365079365082e-05, global_step: 580, interval_runtime: 4.501, interval_samples_per_second: 1.777, interval_steps_per_second: 2.222, epoch: 1.5344[0m
[32m[2022-09-01 10:54:48,817] [    INFO][0m - loss: 1.76955853, learning_rate: 2.9063492063492064e-05, global_step: 590, interval_runtime: 4.4708, interval_samples_per_second: 1.789, interval_steps_per_second: 2.237, epoch: 1.5608[0m
[32m[2022-09-01 10:54:53,323] [    INFO][0m - loss: 2.54197006, learning_rate: 2.904761904761905e-05, global_step: 600, interval_runtime: 5.7894, interval_samples_per_second: 1.382, interval_steps_per_second: 1.727, epoch: 1.5873[0m
[32m[2022-09-01 10:54:53,324] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 10:54:53,324] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-01 10:54:53,324] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 10:54:53,324] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 10:54:53,324] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-09-01 10:55:23,070] [    INFO][0m - eval_loss: 2.2918009757995605, eval_accuracy: 0.4442825928623452, eval_runtime: 29.7456, eval_samples_per_second: 46.158, eval_steps_per_second: 1.446, epoch: 1.5873[0m
[32m[2022-09-01 10:55:23,071] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-09-01 10:55:23,071] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 10:55:31,248] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-09-01 10:55:31,249] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-09-01 10:55:50,791] [    INFO][0m - loss: 1.76871948, learning_rate: 2.9031746031746034e-05, global_step: 610, interval_runtime: 57.4676, interval_samples_per_second: 0.139, interval_steps_per_second: 0.174, epoch: 1.6138[0m
[32m[2022-09-01 10:55:55,270] [    INFO][0m - loss: 1.75149269, learning_rate: 2.9015873015873015e-05, global_step: 620, interval_runtime: 4.4791, interval_samples_per_second: 1.786, interval_steps_per_second: 2.233, epoch: 1.6402[0m
[32m[2022-09-01 10:55:59,752] [    INFO][0m - loss: 1.5000206, learning_rate: 2.9e-05, global_step: 630, interval_runtime: 4.4817, interval_samples_per_second: 1.785, interval_steps_per_second: 2.231, epoch: 1.6667[0m
[32m[2022-09-01 10:56:04,273] [    INFO][0m - loss: 1.75890102, learning_rate: 2.8984126984126985e-05, global_step: 640, interval_runtime: 4.5212, interval_samples_per_second: 1.769, interval_steps_per_second: 2.212, epoch: 1.6931[0m
[32m[2022-09-01 10:56:08,743] [    INFO][0m - loss: 1.44135551, learning_rate: 2.8968253968253967e-05, global_step: 650, interval_runtime: 4.4698, interval_samples_per_second: 1.79, interval_steps_per_second: 2.237, epoch: 1.7196[0m
[32m[2022-09-01 10:56:13,215] [    INFO][0m - loss: 1.98869381, learning_rate: 2.8952380952380955e-05, global_step: 660, interval_runtime: 4.4717, interval_samples_per_second: 1.789, interval_steps_per_second: 2.236, epoch: 1.746[0m
[32m[2022-09-01 10:56:17,654] [    INFO][0m - loss: 1.8917614, learning_rate: 2.893650793650794e-05, global_step: 670, interval_runtime: 4.4397, interval_samples_per_second: 1.802, interval_steps_per_second: 2.252, epoch: 1.7725[0m
[32m[2022-09-01 10:56:22,104] [    INFO][0m - loss: 1.80440292, learning_rate: 2.892063492063492e-05, global_step: 680, interval_runtime: 4.4505, interval_samples_per_second: 1.798, interval_steps_per_second: 2.247, epoch: 1.7989[0m
[32m[2022-09-01 10:56:31,536] [    INFO][0m - loss: 1.62889385, learning_rate: 2.8904761904761907e-05, global_step: 690, interval_runtime: 4.4499, interval_samples_per_second: 1.798, interval_steps_per_second: 2.247, epoch: 1.8254[0m
[32m[2022-09-01 10:56:36,000] [    INFO][0m - loss: 1.53408175, learning_rate: 2.8888888888888888e-05, global_step: 700, interval_runtime: 9.4457, interval_samples_per_second: 0.847, interval_steps_per_second: 1.059, epoch: 1.8519[0m
[32m[2022-09-01 10:56:36,000] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 10:56:36,001] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-01 10:56:36,001] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 10:56:36,001] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 10:56:36,001] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-09-01 10:57:05,650] [    INFO][0m - eval_loss: 2.073206663131714, eval_accuracy: 0.504734158776402, eval_runtime: 29.6489, eval_samples_per_second: 46.309, eval_steps_per_second: 1.45, epoch: 1.8519[0m
[32m[2022-09-01 10:57:05,651] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-700[0m
[32m[2022-09-01 10:57:05,651] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 10:57:20,745] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-700/tokenizer_config.json[0m
[32m[2022-09-01 10:57:21,507] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-700/special_tokens_map.json[0m
[32m[2022-09-01 10:57:40,437] [    INFO][0m - loss: 1.57001925, learning_rate: 2.8873015873015873e-05, global_step: 710, interval_runtime: 64.4367, interval_samples_per_second: 0.124, interval_steps_per_second: 0.155, epoch: 1.8783[0m
[32m[2022-09-01 10:57:44,906] [    INFO][0m - loss: 1.53299627, learning_rate: 2.8857142857142858e-05, global_step: 720, interval_runtime: 4.4694, interval_samples_per_second: 1.79, interval_steps_per_second: 2.237, epoch: 1.9048[0m
[32m[2022-09-01 10:57:49,379] [    INFO][0m - loss: 1.8449091, learning_rate: 2.884126984126984e-05, global_step: 730, interval_runtime: 4.4721, interval_samples_per_second: 1.789, interval_steps_per_second: 2.236, epoch: 1.9312[0m
[32m[2022-09-01 10:57:53,846] [    INFO][0m - loss: 1.44465981, learning_rate: 2.8825396825396825e-05, global_step: 740, interval_runtime: 4.4679, interval_samples_per_second: 1.791, interval_steps_per_second: 2.238, epoch: 1.9577[0m
[32m[2022-09-01 10:57:58,324] [    INFO][0m - loss: 1.52525492, learning_rate: 2.880952380952381e-05, global_step: 750, interval_runtime: 4.4772, interval_samples_per_second: 1.787, interval_steps_per_second: 2.234, epoch: 1.9841[0m
[32m[2022-09-01 10:58:02,848] [    INFO][0m - loss: 1.26647797, learning_rate: 2.8793650793650795e-05, global_step: 760, interval_runtime: 4.5241, interval_samples_per_second: 1.768, interval_steps_per_second: 2.21, epoch: 2.0106[0m
[32m[2022-09-01 10:58:07,318] [    INFO][0m - loss: 0.89849348, learning_rate: 2.877777777777778e-05, global_step: 770, interval_runtime: 4.47, interval_samples_per_second: 1.79, interval_steps_per_second: 2.237, epoch: 2.037[0m
[32m[2022-09-01 10:58:11,807] [    INFO][0m - loss: 1.26595287, learning_rate: 2.8761904761904765e-05, global_step: 780, interval_runtime: 4.4892, interval_samples_per_second: 1.782, interval_steps_per_second: 2.228, epoch: 2.0635[0m
[32m[2022-09-01 10:58:16,287] [    INFO][0m - loss: 1.15442829, learning_rate: 2.8746031746031746e-05, global_step: 790, interval_runtime: 4.48, interval_samples_per_second: 1.786, interval_steps_per_second: 2.232, epoch: 2.0899[0m
[32m[2022-09-01 10:58:20,753] [    INFO][0m - loss: 0.9560091, learning_rate: 2.873015873015873e-05, global_step: 800, interval_runtime: 4.4659, interval_samples_per_second: 1.791, interval_steps_per_second: 2.239, epoch: 2.1164[0m
[32m[2022-09-01 10:58:20,753] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 10:58:20,753] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-01 10:58:20,753] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 10:58:20,753] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 10:58:20,754] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-09-01 10:58:50,406] [    INFO][0m - eval_loss: 2.3100810050964355, eval_accuracy: 0.5018208302986161, eval_runtime: 29.6515, eval_samples_per_second: 46.305, eval_steps_per_second: 1.45, epoch: 2.1164[0m
[32m[2022-09-01 10:58:50,406] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-800[0m
[32m[2022-09-01 10:58:50,406] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 10:58:57,811] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-800/tokenizer_config.json[0m
[32m[2022-09-01 10:58:57,812] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-800/special_tokens_map.json[0m
[32m[2022-09-01 10:59:16,525] [    INFO][0m - loss: 1.09513102, learning_rate: 2.8714285714285716e-05, global_step: 810, interval_runtime: 55.7723, interval_samples_per_second: 0.143, interval_steps_per_second: 0.179, epoch: 2.1429[0m
[32m[2022-09-01 10:59:20,959] [    INFO][0m - loss: 1.19549818, learning_rate: 2.8698412698412698e-05, global_step: 820, interval_runtime: 4.4339, interval_samples_per_second: 1.804, interval_steps_per_second: 2.255, epoch: 2.1693[0m
[32m[2022-09-01 10:59:25,418] [    INFO][0m - loss: 0.92464333, learning_rate: 2.8682539682539683e-05, global_step: 830, interval_runtime: 4.4593, interval_samples_per_second: 1.794, interval_steps_per_second: 2.242, epoch: 2.1958[0m
[32m[2022-09-01 10:59:29,884] [    INFO][0m - loss: 1.16122742, learning_rate: 2.8666666666666668e-05, global_step: 840, interval_runtime: 4.4659, interval_samples_per_second: 1.791, interval_steps_per_second: 2.239, epoch: 2.2222[0m
[32m[2022-09-01 10:59:34,352] [    INFO][0m - loss: 1.4353693, learning_rate: 2.865079365079365e-05, global_step: 850, interval_runtime: 4.4676, interval_samples_per_second: 1.791, interval_steps_per_second: 2.238, epoch: 2.2487[0m
[32m[2022-09-01 10:59:38,851] [    INFO][0m - loss: 1.18995075, learning_rate: 2.8634920634920638e-05, global_step: 860, interval_runtime: 4.4991, interval_samples_per_second: 1.778, interval_steps_per_second: 2.223, epoch: 2.2751[0m
[32m[2022-09-01 10:59:43,304] [    INFO][0m - loss: 1.33043766, learning_rate: 2.8619047619047623e-05, global_step: 870, interval_runtime: 4.4531, interval_samples_per_second: 1.796, interval_steps_per_second: 2.246, epoch: 2.3016[0m
[32m[2022-09-01 10:59:47,767] [    INFO][0m - loss: 1.19883175, learning_rate: 2.8603174603174604e-05, global_step: 880, interval_runtime: 4.4621, interval_samples_per_second: 1.793, interval_steps_per_second: 2.241, epoch: 2.328[0m
[32m[2022-09-01 10:59:52,267] [    INFO][0m - loss: 1.07152729, learning_rate: 2.858730158730159e-05, global_step: 890, interval_runtime: 4.5008, interval_samples_per_second: 1.777, interval_steps_per_second: 2.222, epoch: 2.3545[0m
[32m[2022-09-01 10:59:56,733] [    INFO][0m - loss: 1.04418316, learning_rate: 2.857142857142857e-05, global_step: 900, interval_runtime: 4.4656, interval_samples_per_second: 1.791, interval_steps_per_second: 2.239, epoch: 2.381[0m
[32m[2022-09-01 10:59:56,733] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 10:59:56,733] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-01 10:59:56,733] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 10:59:56,733] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 10:59:56,733] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-09-01 11:00:26,339] [    INFO][0m - eval_loss: 2.207563638687134, eval_accuracy: 0.5025491624180627, eval_runtime: 29.6048, eval_samples_per_second: 46.378, eval_steps_per_second: 1.452, epoch: 2.381[0m
[32m[2022-09-01 11:00:26,340] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-900[0m
[32m[2022-09-01 11:00:26,340] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 11:00:33,541] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-900/tokenizer_config.json[0m
[32m[2022-09-01 11:00:33,542] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-900/special_tokens_map.json[0m
[32m[2022-09-01 11:00:52,285] [    INFO][0m - loss: 1.25001907, learning_rate: 2.8555555555555556e-05, global_step: 910, interval_runtime: 55.5526, interval_samples_per_second: 0.144, interval_steps_per_second: 0.18, epoch: 2.4074[0m
[32m[2022-09-01 11:00:56,777] [    INFO][0m - loss: 1.42278357, learning_rate: 2.853968253968254e-05, global_step: 920, interval_runtime: 4.492, interval_samples_per_second: 1.781, interval_steps_per_second: 2.226, epoch: 2.4339[0m
[32m[2022-09-01 11:01:01,307] [    INFO][0m - loss: 1.50135593, learning_rate: 2.8523809523809522e-05, global_step: 930, interval_runtime: 4.5291, interval_samples_per_second: 1.766, interval_steps_per_second: 2.208, epoch: 2.4603[0m
[32m[2022-09-01 11:01:05,787] [    INFO][0m - loss: 1.05242195, learning_rate: 2.8507936507936507e-05, global_step: 940, interval_runtime: 4.4806, interval_samples_per_second: 1.785, interval_steps_per_second: 2.232, epoch: 2.4868[0m
[32m[2022-09-01 11:01:10,283] [    INFO][0m - loss: 1.30325165, learning_rate: 2.8492063492063492e-05, global_step: 950, interval_runtime: 4.496, interval_samples_per_second: 1.779, interval_steps_per_second: 2.224, epoch: 2.5132[0m
[32m[2022-09-01 11:01:14,783] [    INFO][0m - loss: 1.22980652, learning_rate: 2.8476190476190477e-05, global_step: 960, interval_runtime: 4.5, interval_samples_per_second: 1.778, interval_steps_per_second: 2.222, epoch: 2.5397[0m
[32m[2022-09-01 11:01:19,285] [    INFO][0m - loss: 1.49864769, learning_rate: 2.8460317460317462e-05, global_step: 970, interval_runtime: 4.5024, interval_samples_per_second: 1.777, interval_steps_per_second: 2.221, epoch: 2.5661[0m
[32m[2022-09-01 11:01:23,770] [    INFO][0m - loss: 1.18987865, learning_rate: 2.8444444444444447e-05, global_step: 980, interval_runtime: 4.4848, interval_samples_per_second: 1.784, interval_steps_per_second: 2.23, epoch: 2.5926[0m
[32m[2022-09-01 11:01:28,228] [    INFO][0m - loss: 1.23357935, learning_rate: 2.842857142857143e-05, global_step: 990, interval_runtime: 4.4577, interval_samples_per_second: 1.795, interval_steps_per_second: 2.243, epoch: 2.619[0m
[32m[2022-09-01 11:01:32,705] [    INFO][0m - loss: 1.2178668, learning_rate: 2.8412698412698414e-05, global_step: 1000, interval_runtime: 4.4765, interval_samples_per_second: 1.787, interval_steps_per_second: 2.234, epoch: 2.6455[0m
[32m[2022-09-01 11:01:32,705] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 11:01:32,705] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-01 11:01:32,706] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 11:01:32,706] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 11:01:32,706] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-09-01 11:02:02,287] [    INFO][0m - eval_loss: 2.2776436805725098, eval_accuracy: 0.48434085943190097, eval_runtime: 29.5812, eval_samples_per_second: 46.415, eval_steps_per_second: 1.454, epoch: 2.6455[0m
[32m[2022-09-01 11:02:02,288] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1000[0m
[32m[2022-09-01 11:02:02,288] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 11:02:14,597] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1000/tokenizer_config.json[0m
[32m[2022-09-01 11:02:14,598] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1000/special_tokens_map.json[0m
[32m[2022-09-01 11:02:34,003] [    INFO][0m - loss: 1.33192806, learning_rate: 2.83968253968254e-05, global_step: 1010, interval_runtime: 61.298, interval_samples_per_second: 0.131, interval_steps_per_second: 0.163, epoch: 2.672[0m
[32m[2022-09-01 11:02:38,525] [    INFO][0m - loss: 1.38342247, learning_rate: 2.838095238095238e-05, global_step: 1020, interval_runtime: 4.5223, interval_samples_per_second: 1.769, interval_steps_per_second: 2.211, epoch: 2.6984[0m
[32m[2022-09-01 11:02:42,975] [    INFO][0m - loss: 1.38164043, learning_rate: 2.8365079365079365e-05, global_step: 1030, interval_runtime: 4.4492, interval_samples_per_second: 1.798, interval_steps_per_second: 2.248, epoch: 2.7249[0m
[32m[2022-09-01 11:02:47,457] [    INFO][0m - loss: 1.32451229, learning_rate: 2.834920634920635e-05, global_step: 1040, interval_runtime: 4.483, interval_samples_per_second: 1.785, interval_steps_per_second: 2.231, epoch: 2.7513[0m
[32m[2022-09-01 11:02:51,921] [    INFO][0m - loss: 1.42609463, learning_rate: 2.8333333333333332e-05, global_step: 1050, interval_runtime: 4.4631, interval_samples_per_second: 1.792, interval_steps_per_second: 2.241, epoch: 2.7778[0m
[32m[2022-09-01 11:02:56,453] [    INFO][0m - loss: 1.29503193, learning_rate: 2.8317460317460317e-05, global_step: 1060, interval_runtime: 4.5325, interval_samples_per_second: 1.765, interval_steps_per_second: 2.206, epoch: 2.8042[0m
[32m[2022-09-01 11:03:00,915] [    INFO][0m - loss: 1.23803558, learning_rate: 2.8301587301587305e-05, global_step: 1070, interval_runtime: 4.4625, interval_samples_per_second: 1.793, interval_steps_per_second: 2.241, epoch: 2.8307[0m
[32m[2022-09-01 11:03:05,387] [    INFO][0m - loss: 1.33872938, learning_rate: 2.8285714285714287e-05, global_step: 1080, interval_runtime: 4.4712, interval_samples_per_second: 1.789, interval_steps_per_second: 2.237, epoch: 2.8571[0m
[32m[2022-09-01 11:03:09,854] [    INFO][0m - loss: 1.23858423, learning_rate: 2.8269841269841272e-05, global_step: 1090, interval_runtime: 4.4679, interval_samples_per_second: 1.791, interval_steps_per_second: 2.238, epoch: 2.8836[0m
[32m[2022-09-01 11:03:14,346] [    INFO][0m - loss: 1.07234688, learning_rate: 2.8253968253968253e-05, global_step: 1100, interval_runtime: 4.4918, interval_samples_per_second: 1.781, interval_steps_per_second: 2.226, epoch: 2.9101[0m
[32m[2022-09-01 11:03:14,347] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 11:03:14,347] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-01 11:03:14,347] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 11:03:14,347] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 11:03:14,347] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-09-01 11:03:43,920] [    INFO][0m - eval_loss: 2.1334850788116455, eval_accuracy: 0.515659140568099, eval_runtime: 29.5702, eval_samples_per_second: 46.432, eval_steps_per_second: 1.454, epoch: 2.9101[0m
[32m[2022-09-01 11:03:43,920] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1100[0m
[32m[2022-09-01 11:03:43,921] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 11:03:52,149] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1100/tokenizer_config.json[0m
[32m[2022-09-01 11:03:52,150] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1100/special_tokens_map.json[0m
[32m[2022-09-01 11:04:10,017] [    INFO][0m - loss: 1.24694557, learning_rate: 2.823809523809524e-05, global_step: 1110, interval_runtime: 55.6705, interval_samples_per_second: 0.144, interval_steps_per_second: 0.18, epoch: 2.9365[0m
[32m[2022-09-01 11:04:14,472] [    INFO][0m - loss: 1.04087467, learning_rate: 2.8222222222222223e-05, global_step: 1120, interval_runtime: 4.4551, interval_samples_per_second: 1.796, interval_steps_per_second: 2.245, epoch: 2.963[0m
[32m[2022-09-01 11:04:18,932] [    INFO][0m - loss: 1.41411123, learning_rate: 2.8206349206349205e-05, global_step: 1130, interval_runtime: 4.4603, interval_samples_per_second: 1.794, interval_steps_per_second: 2.242, epoch: 2.9894[0m
[32m[2022-09-01 11:04:23,501] [    INFO][0m - loss: 1.04833241, learning_rate: 2.819047619047619e-05, global_step: 1140, interval_runtime: 4.5684, interval_samples_per_second: 1.751, interval_steps_per_second: 2.189, epoch: 3.0159[0m
[32m[2022-09-01 11:04:27,985] [    INFO][0m - loss: 0.85532598, learning_rate: 2.8174603174603175e-05, global_step: 1150, interval_runtime: 4.4841, interval_samples_per_second: 1.784, interval_steps_per_second: 2.23, epoch: 3.0423[0m
[32m[2022-09-01 11:04:32,452] [    INFO][0m - loss: 0.71936913, learning_rate: 2.8158730158730157e-05, global_step: 1160, interval_runtime: 4.4668, interval_samples_per_second: 1.791, interval_steps_per_second: 2.239, epoch: 3.0688[0m
[32m[2022-09-01 11:04:36,906] [    INFO][0m - loss: 0.99399929, learning_rate: 2.8142857142857145e-05, global_step: 1170, interval_runtime: 4.4545, interval_samples_per_second: 1.796, interval_steps_per_second: 2.245, epoch: 3.0952[0m
[32m[2022-09-01 11:04:41,380] [    INFO][0m - loss: 0.82260275, learning_rate: 2.812698412698413e-05, global_step: 1180, interval_runtime: 4.4739, interval_samples_per_second: 1.788, interval_steps_per_second: 2.235, epoch: 3.1217[0m
[32m[2022-09-01 11:04:45,846] [    INFO][0m - loss: 0.70817523, learning_rate: 2.811111111111111e-05, global_step: 1190, interval_runtime: 4.4656, interval_samples_per_second: 1.791, interval_steps_per_second: 2.239, epoch: 3.1481[0m
[32m[2022-09-01 11:04:50,371] [    INFO][0m - loss: 1.04346437, learning_rate: 2.8095238095238096e-05, global_step: 1200, interval_runtime: 4.5247, interval_samples_per_second: 1.768, interval_steps_per_second: 2.21, epoch: 3.1746[0m
[32m[2022-09-01 11:04:50,371] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 11:04:50,371] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-01 11:04:50,372] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 11:04:50,372] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 11:04:50,372] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-09-01 11:05:20,029] [    INFO][0m - eval_loss: 2.486595869064331, eval_accuracy: 0.4865258557902403, eval_runtime: 29.6568, eval_samples_per_second: 46.296, eval_steps_per_second: 1.45, epoch: 3.1746[0m
[32m[2022-09-01 11:05:20,029] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1200[0m
[32m[2022-09-01 11:05:20,030] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 11:05:27,443] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1200/tokenizer_config.json[0m
[32m[2022-09-01 11:05:27,443] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1200/special_tokens_map.json[0m
[32m[2022-09-01 11:05:45,918] [    INFO][0m - loss: 1.08497963, learning_rate: 2.807936507936508e-05, global_step: 1210, interval_runtime: 55.5472, interval_samples_per_second: 0.144, interval_steps_per_second: 0.18, epoch: 3.2011[0m
[32m[2022-09-01 11:05:50,378] [    INFO][0m - loss: 0.94721222, learning_rate: 2.8063492063492063e-05, global_step: 1220, interval_runtime: 4.4596, interval_samples_per_second: 1.794, interval_steps_per_second: 2.242, epoch: 3.2275[0m
[32m[2022-09-01 11:05:54,850] [    INFO][0m - loss: 0.85855875, learning_rate: 2.8047619047619048e-05, global_step: 1230, interval_runtime: 4.4728, interval_samples_per_second: 1.789, interval_steps_per_second: 2.236, epoch: 3.254[0m
[32m[2022-09-01 11:05:59,303] [    INFO][0m - loss: 0.80670967, learning_rate: 2.8031746031746033e-05, global_step: 1240, interval_runtime: 4.4532, interval_samples_per_second: 1.796, interval_steps_per_second: 2.246, epoch: 3.2804[0m
[32m[2022-09-01 11:06:03,770] [    INFO][0m - loss: 0.99496193, learning_rate: 2.8015873015873015e-05, global_step: 1250, interval_runtime: 4.4664, interval_samples_per_second: 1.791, interval_steps_per_second: 2.239, epoch: 3.3069[0m
[32m[2022-09-01 11:06:08,245] [    INFO][0m - loss: 0.92512226, learning_rate: 2.8e-05, global_step: 1260, interval_runtime: 4.4757, interval_samples_per_second: 1.787, interval_steps_per_second: 2.234, epoch: 3.3333[0m
[32m[2022-09-01 11:06:12,717] [    INFO][0m - loss: 0.59224496, learning_rate: 2.7984126984126988e-05, global_step: 1270, interval_runtime: 4.4713, interval_samples_per_second: 1.789, interval_steps_per_second: 2.236, epoch: 3.3598[0m
[32m[2022-09-01 11:06:17,204] [    INFO][0m - loss: 1.21368856, learning_rate: 2.796825396825397e-05, global_step: 1280, interval_runtime: 4.4874, interval_samples_per_second: 1.783, interval_steps_per_second: 2.228, epoch: 3.3862[0m
[32m[2022-09-01 11:06:21,828] [    INFO][0m - loss: 0.88427544, learning_rate: 2.7952380952380955e-05, global_step: 1290, interval_runtime: 4.6235, interval_samples_per_second: 1.73, interval_steps_per_second: 2.163, epoch: 3.4127[0m
[32m[2022-09-01 11:06:26,295] [    INFO][0m - loss: 0.61522093, learning_rate: 2.7936507936507936e-05, global_step: 1300, interval_runtime: 4.4669, interval_samples_per_second: 1.791, interval_steps_per_second: 2.239, epoch: 3.4392[0m
[32m[2022-09-01 11:06:26,295] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 11:06:26,295] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-01 11:06:26,295] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 11:06:26,296] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 11:06:26,296] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-09-01 11:06:56,055] [    INFO][0m - eval_loss: 2.6999399662017822, eval_accuracy: 0.4996358339402768, eval_runtime: 29.7588, eval_samples_per_second: 46.138, eval_steps_per_second: 1.445, epoch: 3.4392[0m
[32m[2022-09-01 11:06:56,055] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1300[0m
[32m[2022-09-01 11:06:56,056] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 11:07:03,356] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1300/tokenizer_config.json[0m
[32m[2022-09-01 11:07:03,356] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1300/special_tokens_map.json[0m
[32m[2022-09-01 11:07:20,928] [    INFO][0m - loss: 0.77088022, learning_rate: 2.792063492063492e-05, global_step: 1310, interval_runtime: 54.6336, interval_samples_per_second: 0.146, interval_steps_per_second: 0.183, epoch: 3.4656[0m
[32m[2022-09-01 11:07:27,738] [    INFO][0m - loss: 1.03554287, learning_rate: 2.7904761904761906e-05, global_step: 1320, interval_runtime: 4.4895, interval_samples_per_second: 1.782, interval_steps_per_second: 2.227, epoch: 3.4921[0m
[32m[2022-09-01 11:07:32,186] [    INFO][0m - loss: 0.6164691, learning_rate: 2.7888888888888888e-05, global_step: 1330, interval_runtime: 6.7681, interval_samples_per_second: 1.182, interval_steps_per_second: 1.478, epoch: 3.5185[0m
[32m[2022-09-01 11:07:36,652] [    INFO][0m - loss: 1.24315319, learning_rate: 2.7873015873015873e-05, global_step: 1340, interval_runtime: 4.4653, interval_samples_per_second: 1.792, interval_steps_per_second: 2.239, epoch: 3.545[0m
[32m[2022-09-01 11:07:41,136] [    INFO][0m - loss: 0.98988724, learning_rate: 2.7857142857142858e-05, global_step: 1350, interval_runtime: 4.4847, interval_samples_per_second: 1.784, interval_steps_per_second: 2.23, epoch: 3.5714[0m
[32m[2022-09-01 11:07:45,639] [    INFO][0m - loss: 0.95602846, learning_rate: 2.784126984126984e-05, global_step: 1360, interval_runtime: 4.5031, interval_samples_per_second: 1.777, interval_steps_per_second: 2.221, epoch: 3.5979[0m
[32m[2022-09-01 11:07:50,118] [    INFO][0m - loss: 0.93636026, learning_rate: 2.7825396825396828e-05, global_step: 1370, interval_runtime: 4.4786, interval_samples_per_second: 1.786, interval_steps_per_second: 2.233, epoch: 3.6243[0m
[32m[2022-09-01 11:07:54,575] [    INFO][0m - loss: 0.96150885, learning_rate: 2.7809523809523813e-05, global_step: 1380, interval_runtime: 4.4566, interval_samples_per_second: 1.795, interval_steps_per_second: 2.244, epoch: 3.6508[0m
[32m[2022-09-01 11:07:59,061] [    INFO][0m - loss: 0.89764061, learning_rate: 2.7793650793650794e-05, global_step: 1390, interval_runtime: 4.4865, interval_samples_per_second: 1.783, interval_steps_per_second: 2.229, epoch: 3.6772[0m
[32m[2022-09-01 11:08:03,511] [    INFO][0m - loss: 0.95217438, learning_rate: 2.777777777777778e-05, global_step: 1400, interval_runtime: 4.4499, interval_samples_per_second: 1.798, interval_steps_per_second: 2.247, epoch: 3.7037[0m
[32m[2022-09-01 11:08:03,512] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 11:08:03,512] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-01 11:08:03,512] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 11:08:03,512] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 11:08:03,512] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-09-01 11:08:33,075] [    INFO][0m - eval_loss: 2.522301197052002, eval_accuracy: 0.48943918426802624, eval_runtime: 29.5627, eval_samples_per_second: 46.444, eval_steps_per_second: 1.455, epoch: 3.7037[0m
[32m[2022-09-01 11:08:33,076] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1400[0m
[32m[2022-09-01 11:08:33,076] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 11:08:43,664] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1400/tokenizer_config.json[0m
[32m[2022-09-01 11:08:43,665] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1400/special_tokens_map.json[0m
[32m[2022-09-01 11:09:07,921] [    INFO][0m - loss: 0.5482605, learning_rate: 2.7761904761904764e-05, global_step: 1410, interval_runtime: 64.4102, interval_samples_per_second: 0.124, interval_steps_per_second: 0.155, epoch: 3.7302[0m
[32m[2022-09-01 11:09:12,398] [    INFO][0m - loss: 1.09615746, learning_rate: 2.7746031746031746e-05, global_step: 1420, interval_runtime: 4.4769, interval_samples_per_second: 1.787, interval_steps_per_second: 2.234, epoch: 3.7566[0m
[32m[2022-09-01 11:09:16,857] [    INFO][0m - loss: 1.01015482, learning_rate: 2.773015873015873e-05, global_step: 1430, interval_runtime: 4.4587, interval_samples_per_second: 1.794, interval_steps_per_second: 2.243, epoch: 3.7831[0m
[32m[2022-09-01 11:09:21,346] [    INFO][0m - loss: 0.83366594, learning_rate: 2.7714285714285716e-05, global_step: 1440, interval_runtime: 4.4893, interval_samples_per_second: 1.782, interval_steps_per_second: 2.227, epoch: 3.8095[0m
[32m[2022-09-01 11:09:25,840] [    INFO][0m - loss: 1.29842663, learning_rate: 2.7698412698412697e-05, global_step: 1450, interval_runtime: 4.4936, interval_samples_per_second: 1.78, interval_steps_per_second: 2.225, epoch: 3.836[0m
[32m[2022-09-01 11:09:30,322] [    INFO][0m - loss: 1.0154932, learning_rate: 2.7682539682539682e-05, global_step: 1460, interval_runtime: 4.4821, interval_samples_per_second: 1.785, interval_steps_per_second: 2.231, epoch: 3.8624[0m
[32m[2022-09-01 11:09:34,813] [    INFO][0m - loss: 1.10357971, learning_rate: 2.766666666666667e-05, global_step: 1470, interval_runtime: 4.4914, interval_samples_per_second: 1.781, interval_steps_per_second: 2.226, epoch: 3.8889[0m
[32m[2022-09-01 11:09:39,268] [    INFO][0m - loss: 1.15563841, learning_rate: 2.7650793650793652e-05, global_step: 1480, interval_runtime: 4.455, interval_samples_per_second: 1.796, interval_steps_per_second: 2.245, epoch: 3.9153[0m
[32m[2022-09-01 11:09:43,798] [    INFO][0m - loss: 0.89031277, learning_rate: 2.7634920634920637e-05, global_step: 1490, interval_runtime: 4.5297, interval_samples_per_second: 1.766, interval_steps_per_second: 2.208, epoch: 3.9418[0m
[32m[2022-09-01 11:09:48,304] [    INFO][0m - loss: 1.33643465, learning_rate: 2.761904761904762e-05, global_step: 1500, interval_runtime: 4.5059, interval_samples_per_second: 1.775, interval_steps_per_second: 2.219, epoch: 3.9683[0m
[32m[2022-09-01 11:09:48,305] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 11:09:48,305] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-01 11:09:48,305] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 11:09:48,305] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 11:09:48,305] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-09-01 11:10:18,198] [    INFO][0m - eval_loss: 2.4345576763153076, eval_accuracy: 0.4901675163874727, eval_runtime: 29.8931, eval_samples_per_second: 45.93, eval_steps_per_second: 1.438, epoch: 3.9683[0m
[32m[2022-09-01 11:10:18,199] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1500[0m
[32m[2022-09-01 11:10:18,199] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 11:10:26,279] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1500/tokenizer_config.json[0m
[32m[2022-09-01 11:10:26,279] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1500/special_tokens_map.json[0m
[32m[2022-09-01 11:10:40,226] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-01 11:10:40,226] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-1100 (score: 0.515659140568099).[0m
[32m[2022-09-01 11:10:42,327] [    INFO][0m - train_runtime: 1488.4099, train_samples_per_second: 101.585, train_steps_per_second: 12.698, train_loss: 1.576623041152954, epoch: 3.9683[0m
[32m[2022-09-01 11:10:42,329] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-09-01 11:10:42,329] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 11:10:50,253] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-09-01 11:10:50,675] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-09-01 11:10:50,678] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-01 11:10:50,678] [    INFO][0m -   epoch                    =     3.9683[0m
[32m[2022-09-01 11:10:50,678] [    INFO][0m -   train_loss               =     1.5766[0m
[32m[2022-09-01 11:10:50,679] [    INFO][0m -   train_runtime            = 0:24:48.40[0m
[32m[2022-09-01 11:10:50,679] [    INFO][0m -   train_samples_per_second =    101.585[0m
[32m[2022-09-01 11:10:50,679] [    INFO][0m -   train_steps_per_second   =     12.698[0m
[32m[2022-09-01 11:10:50,686] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-01 11:10:50,687] [    INFO][0m -   Num examples = 1749[0m
[32m[2022-09-01 11:10:50,687] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 11:10:50,687] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 11:10:50,687] [    INFO][0m -   Total prediction steps = 55[0m
[32m[2022-09-01 11:11:28,342] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-01 11:11:28,343] [    INFO][0m -   test_accuracy           =      0.518[0m
[32m[2022-09-01 11:11:28,343] [    INFO][0m -   test_loss               =     2.1199[0m
[32m[2022-09-01 11:11:28,344] [    INFO][0m -   test_runtime            = 0:00:37.65[0m
[32m[2022-09-01 11:11:28,344] [    INFO][0m -   test_samples_per_second =     46.447[0m
[32m[2022-09-01 11:11:28,344] [    INFO][0m -   test_steps_per_second   =      1.461[0m
[32m[2022-09-01 11:11:28,344] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-01 11:11:28,345] [    INFO][0m -   Num examples = 2600[0m
[32m[2022-09-01 11:11:28,345] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 11:11:28,345] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 11:11:28,345] [    INFO][0m -   Total prediction steps = 82[0m
[]
Traceback (most recent call last):
  File "train_single.py", line 174, in <module>
    main()
  File "train_single.py", line 168, in main
    test_ret = postprocess(test_ret, test_ds, data_args.task_name,
  File "/ssd2/wanghuijuan03/prompt/PaddleNLP/examples/benchmark/fewclue/a9/postprocess.py", line 61, in postprocess
    "label": remap[id_to_label[preds[idx]]]
TypeError: string indices must be integers
run.sh: line 78: --freeze_plm: command not found
 
==========
chid
==========
 
[33m[2022-09-01 11:12:28,767] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-09-01 11:12:28,767] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-01 11:12:28,767] [    INFO][0m - ============================================================[0m
[32m[2022-09-01 11:12:28,768] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-01 11:12:28,768] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-01 11:12:28,768] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-01 11:12:28,768] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-01 11:12:28,768] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-01 11:12:28,768] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-01 11:12:28,768] [    INFO][0m - model_name_or_path            :ernie-3.0-xbase-zh[0m
[32m[2022-09-01 11:12:28,768] [    INFO][0m - [0m
[32m[2022-09-01 11:12:28,768] [    INFO][0m - ============================================================[0m
[32m[2022-09-01 11:12:28,768] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-01 11:12:28,768] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-01 11:12:28,768] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-01 11:12:28,769] [    INFO][0m - prompt                        :{'text':'text_a'}{'sep'}{'hard':'这句话中的成语使用'}{'mask'}{'mask'}[0m
[32m[2022-09-01 11:12:28,769] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-01 11:12:28,769] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-01 11:12:28,769] [    INFO][0m - task_name                     :chid[0m
[32m[2022-09-01 11:12:28,769] [    INFO][0m - [0m
[32m[2022-09-01 11:12:28,769] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/ernie_3.0_xbase_zh.pdparams[0m
W0901 11:12:28.771109 20458 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0901 11:12:28.775290 20458 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-01 11:12:34,017] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/ernie_3.0_xbase_zh_vocab.txt[0m
[32m[2022-09-01 11:12:34,045] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/tokenizer_config.json[0m
[32m[2022-09-01 11:12:34,045] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/special_tokens_map.json[0m
[32m[2022-09-01 11:12:34,047] [    INFO][0m - Using template: [{'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'sep': None}, {'add_prefix_space': '', 'hard': '这句话中的成语使用'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}][0m
[32m[2022-09-01 11:12:34,049] [    INFO][0m - {0: 0, 1: 1}[0m
2022-09-01 11:12:34,051 INFO [download.py:119] unique_endpoints {''}
[32m[2022-09-01 11:12:34,379] [    INFO][0m - ============================================================[0m
[32m[2022-09-01 11:12:34,379] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-01 11:12:34,379] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-01 11:12:34,379] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-01 11:12:34,379] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-01 11:12:34,379] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-01 11:12:34,380] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-01 11:12:34,380] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-01 11:12:34,380] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-01 11:12:34,380] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-01 11:12:34,380] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-01 11:12:34,380] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-01 11:12:34,380] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-01 11:12:34,380] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-01 11:12:34,380] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-01 11:12:34,380] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-01 11:12:34,380] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-01 11:12:34,380] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-01 11:12:34,380] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-09-01 11:12:34,381] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-09-01 11:12:34,381] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-09-01 11:12:34,381] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-01 11:12:34,381] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-01 11:12:34,381] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-01 11:12:34,381] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-01 11:12:34,381] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-01 11:12:34,381] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-01 11:12:34,381] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-01 11:12:34,381] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-01 11:12:34,381] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-01 11:12:34,381] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-09-01 11:12:34,381] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-01 11:12:34,381] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-01 11:12:34,382] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-01 11:12:34,382] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-01 11:12:34,382] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-01 11:12:34,382] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-01 11:12:34,382] [    INFO][0m - logging_dir                   :./checkpoints/runs/Sep01_11-12-28_instance-3bwob41y-01[0m
[32m[2022-09-01 11:12:34,382] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-01 11:12:34,382] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-01 11:12:34,382] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-01 11:12:34,382] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-01 11:12:34,382] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-01 11:12:34,382] [    INFO][0m - max_seq_length                :256[0m
[32m[2022-09-01 11:12:34,382] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-01 11:12:34,382] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-01 11:12:34,383] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-01 11:12:34,383] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-01 11:12:34,383] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-09-01 11:12:34,383] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-01 11:12:34,383] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-01 11:12:34,383] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-09-01 11:12:34,383] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-01 11:12:34,383] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-01 11:12:34,383] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-09-01 11:12:34,383] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-09-01 11:12:34,383] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-01 11:12:34,383] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-01 11:12:34,383] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-01 11:12:34,383] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-09-01 11:12:34,384] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-01 11:12:34,384] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-01 11:12:34,384] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-01 11:12:34,384] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-01 11:12:34,384] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-01 11:12:34,384] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-01 11:12:34,384] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-09-01 11:12:34,384] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-01 11:12:34,384] [    INFO][0m - save_steps                    :100[0m
[32m[2022-09-01 11:12:34,384] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-09-01 11:12:34,384] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-01 11:12:34,384] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-01 11:12:34,384] [    INFO][0m - seed                          :42[0m
[32m[2022-09-01 11:12:34,384] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-01 11:12:34,385] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-01 11:12:34,385] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-01 11:12:34,385] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-09-01 11:12:34,385] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-01 11:12:34,385] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-01 11:12:34,385] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-01 11:12:34,385] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-01 11:12:34,385] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-01 11:12:34,385] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-01 11:12:34,385] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-01 11:12:34,385] [    INFO][0m - [0m
[32m[2022-09-01 11:12:34,387] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-01 11:12:34,387] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-01 11:12:34,388] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-09-01 11:12:34,388] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-09-01 11:12:34,388] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-09-01 11:12:34,388] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-01 11:12:34,388] [    INFO][0m -   Total optimization steps = 8850.0[0m
[32m[2022-09-01 11:12:34,388] [    INFO][0m -   Total num train samples = 70700[0m
[32m[2022-09-01 11:12:39,476] [    INFO][0m - loss: 1.17503109, learning_rate: 2.9966101694915256e-05, global_step: 10, interval_runtime: 5.0872, interval_samples_per_second: 1.573, interval_steps_per_second: 1.966, epoch: 0.0565[0m
[32m[2022-09-01 11:12:43,193] [    INFO][0m - loss: 0.51250639, learning_rate: 2.9932203389830508e-05, global_step: 20, interval_runtime: 3.7168, interval_samples_per_second: 2.152, interval_steps_per_second: 2.69, epoch: 0.113[0m
[32m[2022-09-01 11:12:46,928] [    INFO][0m - loss: 0.51181355, learning_rate: 2.9898305084745767e-05, global_step: 30, interval_runtime: 3.7347, interval_samples_per_second: 2.142, interval_steps_per_second: 2.678, epoch: 0.1695[0m
[32m[2022-09-01 11:12:50,696] [    INFO][0m - loss: 0.48778987, learning_rate: 2.986440677966102e-05, global_step: 40, interval_runtime: 3.7683, interval_samples_per_second: 2.123, interval_steps_per_second: 2.654, epoch: 0.226[0m
[32m[2022-09-01 11:12:54,457] [    INFO][0m - loss: 0.47271919, learning_rate: 2.9830508474576274e-05, global_step: 50, interval_runtime: 3.7612, interval_samples_per_second: 2.127, interval_steps_per_second: 2.659, epoch: 0.2825[0m
[32m[2022-09-01 11:12:58,199] [    INFO][0m - loss: 0.22996192, learning_rate: 2.9796610169491526e-05, global_step: 60, interval_runtime: 3.7423, interval_samples_per_second: 2.138, interval_steps_per_second: 2.672, epoch: 0.339[0m
[32m[2022-09-01 11:13:01,931] [    INFO][0m - loss: 0.78864412, learning_rate: 2.976271186440678e-05, global_step: 70, interval_runtime: 3.7309, interval_samples_per_second: 2.144, interval_steps_per_second: 2.68, epoch: 0.3955[0m
[32m[2022-09-01 11:13:05,679] [    INFO][0m - loss: 0.68310423, learning_rate: 2.9728813559322033e-05, global_step: 80, interval_runtime: 3.7488, interval_samples_per_second: 2.134, interval_steps_per_second: 2.668, epoch: 0.452[0m
[32m[2022-09-01 11:13:09,411] [    INFO][0m - loss: 0.47625799, learning_rate: 2.9694915254237292e-05, global_step: 90, interval_runtime: 3.7325, interval_samples_per_second: 2.143, interval_steps_per_second: 2.679, epoch: 0.5085[0m
[32m[2022-09-01 11:13:13,159] [    INFO][0m - loss: 0.41217799, learning_rate: 2.9661016949152544e-05, global_step: 100, interval_runtime: 3.7471, interval_samples_per_second: 2.135, interval_steps_per_second: 2.669, epoch: 0.565[0m
[32m[2022-09-01 11:13:13,160] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 11:13:13,160] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-01 11:13:13,160] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 11:13:13,160] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 11:13:13,160] [    INFO][0m -   Total prediction steps = 45[0m
[32m[2022-09-01 11:13:36,667] [    INFO][0m - eval_loss: 0.4101242125034332, eval_accuracy: 0.4752475247524752, eval_runtime: 23.5063, eval_samples_per_second: 60.154, eval_steps_per_second: 1.914, epoch: 0.565[0m
[32m[2022-09-01 11:13:36,667] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-09-01 11:13:36,668] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 11:13:50,988] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-01 11:13:50,988] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-01 11:14:11,947] [    INFO][0m - loss: 0.43763528, learning_rate: 2.96271186440678e-05, global_step: 110, interval_runtime: 58.7882, interval_samples_per_second: 0.136, interval_steps_per_second: 0.17, epoch: 0.6215[0m
[32m[2022-09-01 11:14:15,659] [    INFO][0m - loss: 0.34087832, learning_rate: 2.959322033898305e-05, global_step: 120, interval_runtime: 3.7121, interval_samples_per_second: 2.155, interval_steps_per_second: 2.694, epoch: 0.678[0m
[32m[2022-09-01 11:14:19,381] [    INFO][0m - loss: 0.51554112, learning_rate: 2.9559322033898306e-05, global_step: 130, interval_runtime: 3.7151, interval_samples_per_second: 2.153, interval_steps_per_second: 2.692, epoch: 0.7345[0m
[32m[2022-09-01 11:14:23,104] [    INFO][0m - loss: 0.46104436, learning_rate: 2.9525423728813558e-05, global_step: 140, interval_runtime: 3.7296, interval_samples_per_second: 2.145, interval_steps_per_second: 2.681, epoch: 0.791[0m
[32m[2022-09-01 11:14:26,822] [    INFO][0m - loss: 0.4975791, learning_rate: 2.9491525423728817e-05, global_step: 150, interval_runtime: 3.7182, interval_samples_per_second: 2.152, interval_steps_per_second: 2.689, epoch: 0.8475[0m
[32m[2022-09-01 11:14:30,564] [    INFO][0m - loss: 0.45118933, learning_rate: 2.945762711864407e-05, global_step: 160, interval_runtime: 3.7414, interval_samples_per_second: 2.138, interval_steps_per_second: 2.673, epoch: 0.904[0m
[32m[2022-09-01 11:14:34,286] [    INFO][0m - loss: 0.40144486, learning_rate: 2.9423728813559324e-05, global_step: 170, interval_runtime: 3.7227, interval_samples_per_second: 2.149, interval_steps_per_second: 2.686, epoch: 0.9605[0m
[32m[2022-09-01 11:14:37,979] [    INFO][0m - loss: 0.40421166, learning_rate: 2.9389830508474576e-05, global_step: 180, interval_runtime: 3.6934, interval_samples_per_second: 2.166, interval_steps_per_second: 2.708, epoch: 1.0169[0m
[32m[2022-09-01 11:14:41,997] [    INFO][0m - loss: 0.48026719, learning_rate: 2.935593220338983e-05, global_step: 190, interval_runtime: 3.726, interval_samples_per_second: 2.147, interval_steps_per_second: 2.684, epoch: 1.0734[0m
[32m[2022-09-01 11:14:45,712] [    INFO][0m - loss: 0.44766779, learning_rate: 2.9322033898305087e-05, global_step: 200, interval_runtime: 4.0068, interval_samples_per_second: 1.997, interval_steps_per_second: 2.496, epoch: 1.1299[0m
[32m[2022-09-01 11:14:45,713] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 11:14:45,713] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-01 11:14:45,713] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 11:14:45,713] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 11:14:45,713] [    INFO][0m -   Total prediction steps = 45[0m
[32m[2022-09-01 11:15:09,055] [    INFO][0m - eval_loss: 0.42319580912590027, eval_accuracy: 0.034653465346534656, eval_runtime: 23.3411, eval_samples_per_second: 60.58, eval_steps_per_second: 1.928, epoch: 1.1299[0m
[32m[2022-09-01 11:15:09,055] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-09-01 11:15:09,055] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 11:15:17,461] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-09-01 11:15:17,461] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-09-01 11:15:37,218] [    INFO][0m - loss: 0.36702042, learning_rate: 2.9288135593220342e-05, global_step: 210, interval_runtime: 51.5058, interval_samples_per_second: 0.155, interval_steps_per_second: 0.194, epoch: 1.1864[0m
[32m[2022-09-01 11:15:40,932] [    INFO][0m - loss: 0.58051987, learning_rate: 2.9254237288135594e-05, global_step: 220, interval_runtime: 3.7139, interval_samples_per_second: 2.154, interval_steps_per_second: 2.693, epoch: 1.2429[0m
[32m[2022-09-01 11:15:44,648] [    INFO][0m - loss: 0.42938604, learning_rate: 2.922033898305085e-05, global_step: 230, interval_runtime: 3.7164, interval_samples_per_second: 2.153, interval_steps_per_second: 2.691, epoch: 1.2994[0m
[32m[2022-09-01 11:15:48,363] [    INFO][0m - loss: 0.45561447, learning_rate: 2.91864406779661e-05, global_step: 240, interval_runtime: 3.7145, interval_samples_per_second: 2.154, interval_steps_per_second: 2.692, epoch: 1.3559[0m
[32m[2022-09-01 11:15:52,100] [    INFO][0m - loss: 0.31254029, learning_rate: 2.9152542372881356e-05, global_step: 250, interval_runtime: 3.7368, interval_samples_per_second: 2.141, interval_steps_per_second: 2.676, epoch: 1.4124[0m
[32m[2022-09-01 11:15:55,824] [    INFO][0m - loss: 0.48653255, learning_rate: 2.911864406779661e-05, global_step: 260, interval_runtime: 3.7243, interval_samples_per_second: 2.148, interval_steps_per_second: 2.685, epoch: 1.4689[0m
[32m[2022-09-01 11:15:59,551] [    INFO][0m - loss: 0.38738351, learning_rate: 2.9084745762711867e-05, global_step: 270, interval_runtime: 3.7272, interval_samples_per_second: 2.146, interval_steps_per_second: 2.683, epoch: 1.5254[0m
[32m[2022-09-01 11:16:03,270] [    INFO][0m - loss: 0.48425117, learning_rate: 2.905084745762712e-05, global_step: 280, interval_runtime: 3.7187, interval_samples_per_second: 2.151, interval_steps_per_second: 2.689, epoch: 1.5819[0m
[32m[2022-09-01 11:16:06,996] [    INFO][0m - loss: 0.31551137, learning_rate: 2.9016949152542374e-05, global_step: 290, interval_runtime: 3.7258, interval_samples_per_second: 2.147, interval_steps_per_second: 2.684, epoch: 1.6384[0m
[32m[2022-09-01 11:16:10,722] [    INFO][0m - loss: 0.50479145, learning_rate: 2.8983050847457626e-05, global_step: 300, interval_runtime: 3.7259, interval_samples_per_second: 2.147, interval_steps_per_second: 2.684, epoch: 1.6949[0m
[32m[2022-09-01 11:16:10,723] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 11:16:10,723] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-01 11:16:10,723] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 11:16:10,723] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 11:16:10,723] [    INFO][0m -   Total prediction steps = 45[0m
[32m[2022-09-01 11:16:34,228] [    INFO][0m - eval_loss: 0.41651833057403564, eval_accuracy: 0.4752475247524752, eval_runtime: 23.5047, eval_samples_per_second: 60.158, eval_steps_per_second: 1.915, epoch: 1.6949[0m
[32m[2022-09-01 11:16:34,229] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-09-01 11:16:34,229] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 11:16:42,324] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-09-01 11:16:42,324] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-09-01 11:17:01,572] [    INFO][0m - loss: 0.38747137, learning_rate: 2.894915254237288e-05, global_step: 310, interval_runtime: 50.8502, interval_samples_per_second: 0.157, interval_steps_per_second: 0.197, epoch: 1.7514[0m
[32m[2022-09-01 11:17:05,294] [    INFO][0m - loss: 0.40386028, learning_rate: 2.8915254237288137e-05, global_step: 320, interval_runtime: 3.7218, interval_samples_per_second: 2.15, interval_steps_per_second: 2.687, epoch: 1.8079[0m
[32m[2022-09-01 11:17:09,010] [    INFO][0m - loss: 0.52816172, learning_rate: 2.8881355932203392e-05, global_step: 330, interval_runtime: 3.7157, interval_samples_per_second: 2.153, interval_steps_per_second: 2.691, epoch: 1.8644[0m
[32m[2022-09-01 11:17:12,725] [    INFO][0m - loss: 0.3382705, learning_rate: 2.8847457627118644e-05, global_step: 340, interval_runtime: 3.7158, interval_samples_per_second: 2.153, interval_steps_per_second: 2.691, epoch: 1.9209[0m
[32m[2022-09-01 11:17:16,445] [    INFO][0m - loss: 0.34947703, learning_rate: 2.88135593220339e-05, global_step: 350, interval_runtime: 3.7199, interval_samples_per_second: 2.151, interval_steps_per_second: 2.688, epoch: 1.9774[0m
[32m[2022-09-01 11:17:20,141] [    INFO][0m - loss: 0.36721036, learning_rate: 2.877966101694915e-05, global_step: 360, interval_runtime: 3.6958, interval_samples_per_second: 2.165, interval_steps_per_second: 2.706, epoch: 2.0339[0m
[32m[2022-09-01 11:17:23,865] [    INFO][0m - loss: 0.33138156, learning_rate: 2.874576271186441e-05, global_step: 370, interval_runtime: 3.7241, interval_samples_per_second: 2.148, interval_steps_per_second: 2.685, epoch: 2.0904[0m
[32m[2022-09-01 11:17:27,598] [    INFO][0m - loss: 0.49217653, learning_rate: 2.8711864406779662e-05, global_step: 380, interval_runtime: 3.7323, interval_samples_per_second: 2.143, interval_steps_per_second: 2.679, epoch: 2.1469[0m
[32m[2022-09-01 11:17:31,324] [    INFO][0m - loss: 0.51831479, learning_rate: 2.8677966101694917e-05, global_step: 390, interval_runtime: 3.7264, interval_samples_per_second: 2.147, interval_steps_per_second: 2.684, epoch: 2.2034[0m
[32m[2022-09-01 11:17:35,046] [    INFO][0m - loss: 0.44106169, learning_rate: 2.864406779661017e-05, global_step: 400, interval_runtime: 3.7227, interval_samples_per_second: 2.149, interval_steps_per_second: 2.686, epoch: 2.2599[0m
[32m[2022-09-01 11:17:35,047] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 11:17:35,047] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-01 11:17:35,047] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 11:17:35,047] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 11:17:35,047] [    INFO][0m -   Total prediction steps = 45[0m
[32m[2022-09-01 11:17:58,495] [    INFO][0m - eval_loss: 0.41033828258514404, eval_accuracy: 0.034653465346534656, eval_runtime: 23.4469, eval_samples_per_second: 60.306, eval_steps_per_second: 1.919, epoch: 2.2599[0m
[32m[2022-09-01 11:17:58,495] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-09-01 11:17:58,495] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 11:18:06,339] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-09-01 11:18:06,649] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-09-01 11:18:25,562] [    INFO][0m - loss: 0.49528818, learning_rate: 2.8610169491525424e-05, global_step: 410, interval_runtime: 50.5157, interval_samples_per_second: 0.158, interval_steps_per_second: 0.198, epoch: 2.3164[0m
[32m[2022-09-01 11:18:29,287] [    INFO][0m - loss: 0.34000626, learning_rate: 2.8576271186440676e-05, global_step: 420, interval_runtime: 3.7246, interval_samples_per_second: 2.148, interval_steps_per_second: 2.685, epoch: 2.3729[0m
[32m[2022-09-01 11:18:33,007] [    INFO][0m - loss: 0.4212534, learning_rate: 2.8542372881355935e-05, global_step: 430, interval_runtime: 3.7206, interval_samples_per_second: 2.15, interval_steps_per_second: 2.688, epoch: 2.4294[0m
[32m[2022-09-01 11:18:36,722] [    INFO][0m - loss: 0.48924785, learning_rate: 2.8508474576271187e-05, global_step: 440, interval_runtime: 3.7145, interval_samples_per_second: 2.154, interval_steps_per_second: 2.692, epoch: 2.4859[0m
[32m[2022-09-01 11:18:40,441] [    INFO][0m - loss: 0.45488663, learning_rate: 2.8474576271186442e-05, global_step: 450, interval_runtime: 3.7193, interval_samples_per_second: 2.151, interval_steps_per_second: 2.689, epoch: 2.5424[0m
[32m[2022-09-01 11:18:44,167] [    INFO][0m - loss: 0.57539172, learning_rate: 2.8440677966101694e-05, global_step: 460, interval_runtime: 3.7256, interval_samples_per_second: 2.147, interval_steps_per_second: 2.684, epoch: 2.5989[0m
[32m[2022-09-01 11:18:47,881] [    INFO][0m - loss: 0.50351028, learning_rate: 2.840677966101695e-05, global_step: 470, interval_runtime: 3.7141, interval_samples_per_second: 2.154, interval_steps_per_second: 2.692, epoch: 2.6554[0m
[32m[2022-09-01 11:18:51,601] [    INFO][0m - loss: 0.41483655, learning_rate: 2.8372881355932205e-05, global_step: 480, interval_runtime: 3.7201, interval_samples_per_second: 2.151, interval_steps_per_second: 2.688, epoch: 2.7119[0m
[32m[2022-09-01 11:18:55,337] [    INFO][0m - loss: 0.39621217, learning_rate: 2.833898305084746e-05, global_step: 490, interval_runtime: 3.7359, interval_samples_per_second: 2.141, interval_steps_per_second: 2.677, epoch: 2.7684[0m
[32m[2022-09-01 11:18:59,062] [    INFO][0m - loss: 0.37738237, learning_rate: 2.8305084745762712e-05, global_step: 500, interval_runtime: 3.7245, interval_samples_per_second: 2.148, interval_steps_per_second: 2.685, epoch: 2.8249[0m
[32m[2022-09-01 11:18:59,062] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 11:18:59,063] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-01 11:18:59,063] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 11:18:59,063] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 11:18:59,063] [    INFO][0m -   Total prediction steps = 45[0m
[32m[2022-09-01 11:19:22,357] [    INFO][0m - eval_loss: 0.4121142029762268, eval_accuracy: 0.15841584158415842, eval_runtime: 23.2934, eval_samples_per_second: 60.704, eval_steps_per_second: 1.932, epoch: 2.8249[0m
[32m[2022-09-01 11:19:22,357] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-09-01 11:19:22,357] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 11:19:31,195] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-09-01 11:19:31,196] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-09-01 11:19:45,648] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-01 11:19:45,648] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-100 (score: 0.4752475247524752).[0m
[32m[2022-09-01 11:19:48,259] [    INFO][0m - train_runtime: 433.8704, train_samples_per_second: 162.952, train_steps_per_second: 20.398, train_loss: 0.46272835445404054, epoch: 2.8249[0m
[32m[2022-09-01 11:19:48,262] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-09-01 11:19:48,262] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 11:19:56,343] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-09-01 11:19:56,343] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-09-01 11:19:56,345] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-01 11:19:56,345] [    INFO][0m -   epoch                    =     2.8249[0m
[32m[2022-09-01 11:19:56,346] [    INFO][0m -   train_loss               =     0.4627[0m
[32m[2022-09-01 11:19:56,346] [    INFO][0m -   train_runtime            = 0:07:13.87[0m
[32m[2022-09-01 11:19:56,346] [    INFO][0m -   train_samples_per_second =    162.952[0m
[32m[2022-09-01 11:19:56,346] [    INFO][0m -   train_steps_per_second   =     20.398[0m
[32m[2022-09-01 11:19:56,349] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-01 11:19:56,349] [    INFO][0m -   Num examples = 14014[0m
[32m[2022-09-01 11:19:56,349] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 11:19:56,350] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 11:19:56,350] [    INFO][0m -   Total prediction steps = 438[0m
[32m[2022-09-01 11:23:47,385] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-01 11:23:47,386] [    INFO][0m -   test_accuracy           =     0.4605[0m
[32m[2022-09-01 11:23:47,386] [    INFO][0m -   test_loss               =     0.4101[0m
[32m[2022-09-01 11:23:47,386] [    INFO][0m -   test_runtime            = 0:03:51.03[0m
[32m[2022-09-01 11:23:47,386] [    INFO][0m -   test_samples_per_second =     60.657[0m
[32m[2022-09-01 11:23:47,386] [    INFO][0m -   test_steps_per_second   =      1.896[0m
[32m[2022-09-01 11:23:47,387] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-01 11:23:47,387] [    INFO][0m -   Num examples = 14000[0m
[32m[2022-09-01 11:23:47,387] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 11:23:47,387] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 11:23:47,387] [    INFO][0m -   Total prediction steps = 438[0m
[32m[2022-09-01 11:27:57,550] [    INFO][0m - Predictions for chidf saved to ./fewclue_submit_examples.[0m
[]
run.sh: line 78: --freeze_plm: command not found
 
==========
csl
==========
 
[33m[2022-09-01 11:28:02,293] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-09-01 11:28:02,294] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-01 11:28:02,294] [    INFO][0m - ============================================================[0m
[32m[2022-09-01 11:28:02,294] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-01 11:28:02,294] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-01 11:28:02,294] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-01 11:28:02,294] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-01 11:28:02,294] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-01 11:28:02,294] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-01 11:28:02,294] [    INFO][0m - model_name_or_path            :ernie-3.0-xbase-zh[0m
[32m[2022-09-01 11:28:02,294] [    INFO][0m - [0m
[32m[2022-09-01 11:28:02,295] [    INFO][0m - ============================================================[0m
[32m[2022-09-01 11:28:02,295] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-01 11:28:02,295] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-01 11:28:02,295] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-01 11:28:02,295] [    INFO][0m - prompt                        :{'text': 'text_a'}{'hard':'上文中'}{'mask'}{'hard': '这些关键词：'}{'text':'text_b'}[0m
[32m[2022-09-01 11:28:02,295] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-01 11:28:02,295] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-01 11:28:02,295] [    INFO][0m - task_name                     :csl[0m
[32m[2022-09-01 11:28:02,295] [    INFO][0m - [0m
[32m[2022-09-01 11:28:02,295] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/ernie_3.0_xbase_zh.pdparams[0m
W0901 11:28:02.297449 22382 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0901 11:28:02.301714 22382 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-01 11:28:07,184] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/ernie_3.0_xbase_zh_vocab.txt[0m
[32m[2022-09-01 11:28:07,208] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/tokenizer_config.json[0m
[32m[2022-09-01 11:28:07,208] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/special_tokens_map.json[0m
[32m[2022-09-01 11:28:07,210] [    INFO][0m - Using template: [{'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '上文中'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': '这些关键词：'}, {'add_prefix_space': '', 'text': 'text_b'}][0m
[32m[2022-09-01 11:28:07,212] [    INFO][0m - {'0': 0, '1': 1}[0m
2022-09-01 11:28:07,213 INFO [download.py:119] unique_endpoints {''}
[32m[2022-09-01 11:28:07,375] [    INFO][0m - ============================================================[0m
[32m[2022-09-01 11:28:07,376] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-01 11:28:07,376] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-01 11:28:07,376] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-01 11:28:07,376] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-01 11:28:07,376] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-01 11:28:07,376] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-01 11:28:07,376] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-01 11:28:07,376] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-01 11:28:07,376] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-01 11:28:07,376] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-01 11:28:07,377] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-01 11:28:07,377] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-01 11:28:07,377] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-01 11:28:07,377] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-01 11:28:07,377] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-01 11:28:07,377] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-01 11:28:07,377] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-01 11:28:07,377] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-09-01 11:28:07,377] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-09-01 11:28:07,377] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-09-01 11:28:07,377] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-01 11:28:07,377] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-01 11:28:07,377] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-01 11:28:07,378] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-01 11:28:07,378] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-01 11:28:07,378] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-01 11:28:07,378] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-01 11:28:07,378] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-01 11:28:07,378] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-01 11:28:07,378] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-09-01 11:28:07,378] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-01 11:28:07,378] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-01 11:28:07,378] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-01 11:28:07,378] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-01 11:28:07,378] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-01 11:28:07,378] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-01 11:28:07,378] [    INFO][0m - logging_dir                   :./checkpoints/runs/Sep01_11-28-02_instance-3bwob41y-01[0m
[32m[2022-09-01 11:28:07,379] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-01 11:28:07,379] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-01 11:28:07,379] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-01 11:28:07,379] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-01 11:28:07,379] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-01 11:28:07,379] [    INFO][0m - max_seq_length                :320[0m
[32m[2022-09-01 11:28:07,379] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-01 11:28:07,379] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-01 11:28:07,379] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-01 11:28:07,379] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-01 11:28:07,379] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-09-01 11:28:07,379] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-01 11:28:07,379] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-01 11:28:07,380] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-09-01 11:28:07,380] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-01 11:28:07,380] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-01 11:28:07,380] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-09-01 11:28:07,380] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-09-01 11:28:07,380] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-01 11:28:07,380] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-01 11:28:07,380] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-01 11:28:07,380] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-09-01 11:28:07,380] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-01 11:28:07,380] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-01 11:28:07,380] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-01 11:28:07,380] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-01 11:28:07,381] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-01 11:28:07,381] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-01 11:28:07,381] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-09-01 11:28:07,381] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-01 11:28:07,381] [    INFO][0m - save_steps                    :100[0m
[32m[2022-09-01 11:28:07,381] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-09-01 11:28:07,381] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-01 11:28:07,381] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-01 11:28:07,381] [    INFO][0m - seed                          :42[0m
[32m[2022-09-01 11:28:07,381] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-01 11:28:07,381] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-01 11:28:07,381] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-01 11:28:07,381] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-09-01 11:28:07,381] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-01 11:28:07,382] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-01 11:28:07,382] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-01 11:28:07,382] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-01 11:28:07,382] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-01 11:28:07,382] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-01 11:28:07,382] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-01 11:28:07,382] [    INFO][0m - [0m
[32m[2022-09-01 11:28:07,385] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-01 11:28:07,385] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-01 11:28:07,385] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-09-01 11:28:07,385] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-09-01 11:28:07,385] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-09-01 11:28:07,385] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-01 11:28:07,385] [    INFO][0m -   Total optimization steps = 1000.0[0m
[32m[2022-09-01 11:28:07,385] [    INFO][0m -   Total num train samples = 8000[0m
[32m[2022-09-01 11:28:12,987] [    INFO][0m - loss: 1.19550667, learning_rate: 2.97e-05, global_step: 10, interval_runtime: 5.5997, interval_samples_per_second: 1.429, interval_steps_per_second: 1.786, epoch: 0.5[0m
[32m[2022-09-01 11:28:17,438] [    INFO][0m - loss: 0.96598959, learning_rate: 2.94e-05, global_step: 20, interval_runtime: 4.4522, interval_samples_per_second: 1.797, interval_steps_per_second: 2.246, epoch: 1.0[0m
[32m[2022-09-01 11:28:22,044] [    INFO][0m - loss: 0.89157143, learning_rate: 2.91e-05, global_step: 30, interval_runtime: 4.6056, interval_samples_per_second: 1.737, interval_steps_per_second: 2.171, epoch: 1.5[0m
[32m[2022-09-01 11:28:26,498] [    INFO][0m - loss: 0.81767731, learning_rate: 2.88e-05, global_step: 40, interval_runtime: 4.4537, interval_samples_per_second: 1.796, interval_steps_per_second: 2.245, epoch: 2.0[0m
[32m[2022-09-01 11:28:31,097] [    INFO][0m - loss: 0.76791105, learning_rate: 2.8499999999999998e-05, global_step: 50, interval_runtime: 4.5999, interval_samples_per_second: 1.739, interval_steps_per_second: 2.174, epoch: 2.5[0m
[32m[2022-09-01 11:28:35,538] [    INFO][0m - loss: 0.7000598, learning_rate: 2.8199999999999998e-05, global_step: 60, interval_runtime: 4.4405, interval_samples_per_second: 1.802, interval_steps_per_second: 2.252, epoch: 3.0[0m
[32m[2022-09-01 11:28:40,117] [    INFO][0m - loss: 0.75269461, learning_rate: 2.79e-05, global_step: 70, interval_runtime: 4.5795, interval_samples_per_second: 1.747, interval_steps_per_second: 2.184, epoch: 3.5[0m
[32m[2022-09-01 11:28:44,568] [    INFO][0m - loss: 0.70973234, learning_rate: 2.7600000000000003e-05, global_step: 80, interval_runtime: 4.4507, interval_samples_per_second: 1.797, interval_steps_per_second: 2.247, epoch: 4.0[0m
[32m[2022-09-01 11:28:49,192] [    INFO][0m - loss: 0.72770433, learning_rate: 2.7300000000000003e-05, global_step: 90, interval_runtime: 4.6234, interval_samples_per_second: 1.73, interval_steps_per_second: 2.163, epoch: 4.5[0m
[32m[2022-09-01 11:28:53,629] [    INFO][0m - loss: 0.67438927, learning_rate: 2.7000000000000002e-05, global_step: 100, interval_runtime: 4.437, interval_samples_per_second: 1.803, interval_steps_per_second: 2.254, epoch: 5.0[0m
[32m[2022-09-01 11:28:53,629] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 11:28:53,629] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-01 11:28:53,630] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 11:28:53,630] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 11:28:53,630] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-09-01 11:28:57,062] [    INFO][0m - eval_loss: 0.7054165005683899, eval_accuracy: 0.50625, eval_runtime: 3.4317, eval_samples_per_second: 46.625, eval_steps_per_second: 1.457, epoch: 5.0[0m
[32m[2022-09-01 11:28:57,063] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-09-01 11:28:57,063] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 11:29:05,052] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-01 11:29:05,053] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-01 11:29:25,016] [    INFO][0m - loss: 0.74457722, learning_rate: 2.6700000000000002e-05, global_step: 110, interval_runtime: 31.3876, interval_samples_per_second: 0.255, interval_steps_per_second: 0.319, epoch: 5.5[0m
[32m[2022-09-01 11:29:29,431] [    INFO][0m - loss: 0.68635483, learning_rate: 2.64e-05, global_step: 120, interval_runtime: 4.4147, interval_samples_per_second: 1.812, interval_steps_per_second: 2.265, epoch: 6.0[0m
[32m[2022-09-01 11:29:34,014] [    INFO][0m - loss: 0.80195847, learning_rate: 2.61e-05, global_step: 130, interval_runtime: 4.5832, interval_samples_per_second: 1.746, interval_steps_per_second: 2.182, epoch: 6.5[0m
[32m[2022-09-01 11:29:38,452] [    INFO][0m - loss: 0.71291137, learning_rate: 2.58e-05, global_step: 140, interval_runtime: 4.438, interval_samples_per_second: 1.803, interval_steps_per_second: 2.253, epoch: 7.0[0m
[32m[2022-09-01 11:29:43,079] [    INFO][0m - loss: 0.67779112, learning_rate: 2.55e-05, global_step: 150, interval_runtime: 4.6264, interval_samples_per_second: 1.729, interval_steps_per_second: 2.162, epoch: 7.5[0m
[32m[2022-09-01 11:29:47,502] [    INFO][0m - loss: 0.74187531, learning_rate: 2.52e-05, global_step: 160, interval_runtime: 4.4233, interval_samples_per_second: 1.809, interval_steps_per_second: 2.261, epoch: 8.0[0m
[32m[2022-09-01 11:29:52,443] [    INFO][0m - loss: 0.69300089, learning_rate: 2.49e-05, global_step: 170, interval_runtime: 4.6098, interval_samples_per_second: 1.735, interval_steps_per_second: 2.169, epoch: 8.5[0m
[32m[2022-09-01 11:29:56,858] [    INFO][0m - loss: 0.79133625, learning_rate: 2.4599999999999998e-05, global_step: 180, interval_runtime: 4.7466, interval_samples_per_second: 1.685, interval_steps_per_second: 2.107, epoch: 9.0[0m
[32m[2022-09-01 11:30:01,459] [    INFO][0m - loss: 0.71590629, learning_rate: 2.43e-05, global_step: 190, interval_runtime: 4.6014, interval_samples_per_second: 1.739, interval_steps_per_second: 2.173, epoch: 9.5[0m
[32m[2022-09-01 11:30:05,895] [    INFO][0m - loss: 0.69913292, learning_rate: 2.4e-05, global_step: 200, interval_runtime: 4.4353, interval_samples_per_second: 1.804, interval_steps_per_second: 2.255, epoch: 10.0[0m
[32m[2022-09-01 11:30:05,895] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 11:30:05,895] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-01 11:30:05,895] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 11:30:05,895] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 11:30:05,896] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-09-01 11:30:09,337] [    INFO][0m - eval_loss: 0.6993524432182312, eval_accuracy: 0.50625, eval_runtime: 3.4411, eval_samples_per_second: 46.497, eval_steps_per_second: 1.453, epoch: 10.0[0m
[32m[2022-09-01 11:30:09,338] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-09-01 11:30:09,338] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 11:30:17,719] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-09-01 11:30:17,719] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-09-01 11:30:40,512] [    INFO][0m - loss: 0.70359931, learning_rate: 2.37e-05, global_step: 210, interval_runtime: 34.6171, interval_samples_per_second: 0.231, interval_steps_per_second: 0.289, epoch: 10.5[0m
[32m[2022-09-01 11:30:44,931] [    INFO][0m - loss: 0.72399712, learning_rate: 2.3400000000000003e-05, global_step: 220, interval_runtime: 4.4187, interval_samples_per_second: 1.81, interval_steps_per_second: 2.263, epoch: 11.0[0m
[32m[2022-09-01 11:30:49,504] [    INFO][0m - loss: 0.77197642, learning_rate: 2.3100000000000002e-05, global_step: 230, interval_runtime: 4.5733, interval_samples_per_second: 1.749, interval_steps_per_second: 2.187, epoch: 11.5[0m
[32m[2022-09-01 11:30:53,924] [    INFO][0m - loss: 0.67511773, learning_rate: 2.2800000000000002e-05, global_step: 240, interval_runtime: 4.4203, interval_samples_per_second: 1.81, interval_steps_per_second: 2.262, epoch: 12.0[0m
[32m[2022-09-01 11:30:58,509] [    INFO][0m - loss: 0.69246244, learning_rate: 2.25e-05, global_step: 250, interval_runtime: 4.5847, interval_samples_per_second: 1.745, interval_steps_per_second: 2.181, epoch: 12.5[0m
[32m[2022-09-01 11:31:02,933] [    INFO][0m - loss: 0.73337326, learning_rate: 2.22e-05, global_step: 260, interval_runtime: 4.424, interval_samples_per_second: 1.808, interval_steps_per_second: 2.26, epoch: 13.0[0m
[32m[2022-09-01 11:31:07,527] [    INFO][0m - loss: 0.66560249, learning_rate: 2.19e-05, global_step: 270, interval_runtime: 4.5936, interval_samples_per_second: 1.742, interval_steps_per_second: 2.177, epoch: 13.5[0m
[32m[2022-09-01 11:31:11,959] [    INFO][0m - loss: 0.69507437, learning_rate: 2.16e-05, global_step: 280, interval_runtime: 4.432, interval_samples_per_second: 1.805, interval_steps_per_second: 2.256, epoch: 14.0[0m
[32m[2022-09-01 11:31:16,540] [    INFO][0m - loss: 0.69145904, learning_rate: 2.13e-05, global_step: 290, interval_runtime: 4.5811, interval_samples_per_second: 1.746, interval_steps_per_second: 2.183, epoch: 14.5[0m
[32m[2022-09-01 11:31:21,028] [    INFO][0m - loss: 0.68097529, learning_rate: 2.1e-05, global_step: 300, interval_runtime: 4.4878, interval_samples_per_second: 1.783, interval_steps_per_second: 2.228, epoch: 15.0[0m
[32m[2022-09-01 11:31:21,028] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 11:31:21,028] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-01 11:31:21,029] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 11:31:21,029] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 11:31:21,029] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-09-01 11:31:24,437] [    INFO][0m - eval_loss: 0.7017013430595398, eval_accuracy: 0.50625, eval_runtime: 3.4081, eval_samples_per_second: 46.947, eval_steps_per_second: 1.467, epoch: 15.0[0m
[32m[2022-09-01 11:31:24,437] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-09-01 11:31:24,438] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 11:31:31,996] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-09-01 11:31:31,997] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-09-01 11:31:56,077] [    INFO][0m - loss: 0.74525151, learning_rate: 2.07e-05, global_step: 310, interval_runtime: 35.0494, interval_samples_per_second: 0.228, interval_steps_per_second: 0.285, epoch: 15.5[0m
[32m[2022-09-01 11:32:00,479] [    INFO][0m - loss: 0.89594088, learning_rate: 2.04e-05, global_step: 320, interval_runtime: 4.4025, interval_samples_per_second: 1.817, interval_steps_per_second: 2.271, epoch: 16.0[0m
[32m[2022-09-01 11:32:05,061] [    INFO][0m - loss: 0.7829432, learning_rate: 2.01e-05, global_step: 330, interval_runtime: 4.5812, interval_samples_per_second: 1.746, interval_steps_per_second: 2.183, epoch: 16.5[0m
[32m[2022-09-01 11:32:09,480] [    INFO][0m - loss: 0.71149564, learning_rate: 1.98e-05, global_step: 340, interval_runtime: 4.4199, interval_samples_per_second: 1.81, interval_steps_per_second: 2.262, epoch: 17.0[0m
[32m[2022-09-01 11:32:14,046] [    INFO][0m - loss: 0.73896561, learning_rate: 1.95e-05, global_step: 350, interval_runtime: 4.5653, interval_samples_per_second: 1.752, interval_steps_per_second: 2.19, epoch: 17.5[0m
[32m[2022-09-01 11:32:18,477] [    INFO][0m - loss: 0.71999798, learning_rate: 1.9200000000000003e-05, global_step: 360, interval_runtime: 4.4317, interval_samples_per_second: 1.805, interval_steps_per_second: 2.256, epoch: 18.0[0m
[32m[2022-09-01 11:32:23,056] [    INFO][0m - loss: 0.78947229, learning_rate: 1.8900000000000002e-05, global_step: 370, interval_runtime: 4.579, interval_samples_per_second: 1.747, interval_steps_per_second: 2.184, epoch: 18.5[0m
[32m[2022-09-01 11:32:27,469] [    INFO][0m - loss: 0.82619305, learning_rate: 1.86e-05, global_step: 380, interval_runtime: 4.4127, interval_samples_per_second: 1.813, interval_steps_per_second: 2.266, epoch: 19.0[0m
[32m[2022-09-01 11:32:32,048] [    INFO][0m - loss: 0.73106022, learning_rate: 1.83e-05, global_step: 390, interval_runtime: 4.5783, interval_samples_per_second: 1.747, interval_steps_per_second: 2.184, epoch: 19.5[0m
[32m[2022-09-01 11:32:36,478] [    INFO][0m - loss: 0.67958879, learning_rate: 1.8e-05, global_step: 400, interval_runtime: 4.4297, interval_samples_per_second: 1.806, interval_steps_per_second: 2.257, epoch: 20.0[0m
[32m[2022-09-01 11:32:36,478] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 11:32:36,479] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-01 11:32:36,479] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 11:32:36,479] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 11:32:36,479] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-09-01 11:32:39,907] [    INFO][0m - eval_loss: 0.6978608965873718, eval_accuracy: 0.50625, eval_runtime: 3.428, eval_samples_per_second: 46.674, eval_steps_per_second: 1.459, epoch: 20.0[0m
[32m[2022-09-01 11:32:39,908] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-09-01 11:32:39,908] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 11:32:52,303] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-09-01 11:32:52,303] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-09-01 11:33:12,396] [    INFO][0m - loss: 0.65386696, learning_rate: 1.77e-05, global_step: 410, interval_runtime: 35.9182, interval_samples_per_second: 0.223, interval_steps_per_second: 0.278, epoch: 20.5[0m
[32m[2022-09-01 11:33:16,804] [    INFO][0m - loss: 0.61851015, learning_rate: 1.74e-05, global_step: 420, interval_runtime: 4.4085, interval_samples_per_second: 1.815, interval_steps_per_second: 2.268, epoch: 21.0[0m
[32m[2022-09-01 11:33:21,389] [    INFO][0m - loss: 0.58005195, learning_rate: 1.71e-05, global_step: 430, interval_runtime: 4.5846, interval_samples_per_second: 1.745, interval_steps_per_second: 2.181, epoch: 21.5[0m
[32m[2022-09-01 11:33:25,813] [    INFO][0m - loss: 0.67639694, learning_rate: 1.6800000000000002e-05, global_step: 440, interval_runtime: 4.4248, interval_samples_per_second: 1.808, interval_steps_per_second: 2.26, epoch: 22.0[0m
[32m[2022-09-01 11:33:30,413] [    INFO][0m - loss: 1.01674671, learning_rate: 1.65e-05, global_step: 450, interval_runtime: 4.6, interval_samples_per_second: 1.739, interval_steps_per_second: 2.174, epoch: 22.5[0m
[32m[2022-09-01 11:33:34,844] [    INFO][0m - loss: 0.67653217, learning_rate: 1.62e-05, global_step: 460, interval_runtime: 4.4308, interval_samples_per_second: 1.806, interval_steps_per_second: 2.257, epoch: 23.0[0m
[32m[2022-09-01 11:33:39,426] [    INFO][0m - loss: 0.67028732, learning_rate: 1.59e-05, global_step: 470, interval_runtime: 4.5816, interval_samples_per_second: 1.746, interval_steps_per_second: 2.183, epoch: 23.5[0m
[32m[2022-09-01 11:33:43,876] [    INFO][0m - loss: 0.62462978, learning_rate: 1.56e-05, global_step: 480, interval_runtime: 4.4505, interval_samples_per_second: 1.798, interval_steps_per_second: 2.247, epoch: 24.0[0m
[32m[2022-09-01 11:33:48,505] [    INFO][0m - loss: 0.56931009, learning_rate: 1.53e-05, global_step: 490, interval_runtime: 4.6288, interval_samples_per_second: 1.728, interval_steps_per_second: 2.16, epoch: 24.5[0m
[32m[2022-09-01 11:33:52,929] [    INFO][0m - loss: 0.87549353, learning_rate: 1.5e-05, global_step: 500, interval_runtime: 4.423, interval_samples_per_second: 1.809, interval_steps_per_second: 2.261, epoch: 25.0[0m
[32m[2022-09-01 11:33:52,930] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 11:33:52,930] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-01 11:33:52,930] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 11:33:52,930] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 11:33:52,930] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-09-01 11:33:56,344] [    INFO][0m - eval_loss: 0.6993009448051453, eval_accuracy: 0.49375, eval_runtime: 3.4137, eval_samples_per_second: 46.869, eval_steps_per_second: 1.465, epoch: 25.0[0m
[32m[2022-09-01 11:33:56,344] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-09-01 11:33:56,344] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 11:34:03,893] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-09-01 11:34:03,893] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-09-01 11:34:20,248] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-01 11:34:20,248] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-100 (score: 0.50625).[0m
[32m[2022-09-01 11:34:22,827] [    INFO][0m - train_runtime: 375.4408, train_samples_per_second: 21.308, train_steps_per_second: 2.664, train_loss: 0.7416890659332276, epoch: 25.0[0m
[32m[2022-09-01 11:34:22,829] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-09-01 11:34:22,830] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 11:34:31,329] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-09-01 11:34:31,330] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-09-01 11:34:31,332] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-01 11:34:31,332] [    INFO][0m -   epoch                    =       25.0[0m
[32m[2022-09-01 11:34:31,332] [    INFO][0m -   train_loss               =     0.7417[0m
[32m[2022-09-01 11:34:31,332] [    INFO][0m -   train_runtime            = 0:06:15.44[0m
[32m[2022-09-01 11:34:31,332] [    INFO][0m -   train_samples_per_second =     21.308[0m
[32m[2022-09-01 11:34:31,333] [    INFO][0m -   train_steps_per_second   =      2.664[0m
[32m[2022-09-01 11:34:31,336] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-01 11:34:31,336] [    INFO][0m -   Num examples = 2838[0m
[32m[2022-09-01 11:34:31,336] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 11:34:31,336] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 11:34:31,336] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-01 11:35:32,533] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-01 11:35:32,534] [    INFO][0m -   test_accuracy           =        0.5[0m
[32m[2022-09-01 11:35:32,535] [    INFO][0m -   test_loss               =     0.7074[0m
[32m[2022-09-01 11:35:32,535] [    INFO][0m -   test_runtime            = 0:01:01.19[0m
[32m[2022-09-01 11:35:32,535] [    INFO][0m -   test_samples_per_second =     46.375[0m
[32m[2022-09-01 11:35:32,535] [    INFO][0m -   test_steps_per_second   =      1.454[0m
[32m[2022-09-01 11:35:32,536] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-01 11:35:32,536] [    INFO][0m -   Num examples = 3000[0m
[32m[2022-09-01 11:35:32,536] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 11:35:32,536] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 11:35:32,536] [    INFO][0m -   Total prediction steps = 94[0m
[32m[2022-09-01 11:36:44,897] [    INFO][0m - Predictions for cslf saved to ./fewclue_submit_examples.[0m
[]
run.sh: line 78: --freeze_plm: command not found
 
==========
cmnli
==========
 
[33m[2022-09-01 11:36:49,288] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-09-01 11:36:49,289] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-01 11:36:49,289] [    INFO][0m - ============================================================[0m
[32m[2022-09-01 11:36:49,289] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-01 11:36:49,289] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-01 11:36:49,289] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-01 11:36:49,289] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-01 11:36:49,289] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-01 11:36:49,290] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-01 11:36:49,290] [    INFO][0m - model_name_or_path            :ernie-3.0-xbase-zh[0m
[32m[2022-09-01 11:36:49,290] [    INFO][0m - [0m
[32m[2022-09-01 11:36:49,290] [    INFO][0m - ============================================================[0m
[32m[2022-09-01 11:36:49,290] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-01 11:36:49,290] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-01 11:36:49,290] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-01 11:36:49,290] [    INFO][0m - prompt                        :“{'text':'text_a'}”和“{'text':'text_b'}”之间的逻辑关系是{'mask'}{'mask'}。[0m
[32m[2022-09-01 11:36:49,290] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-01 11:36:49,290] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-01 11:36:49,290] [    INFO][0m - task_name                     :cmnli[0m
[32m[2022-09-01 11:36:49,290] [    INFO][0m - [0m
[32m[2022-09-01 11:36:49,291] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/ernie_3.0_xbase_zh.pdparams[0m
W0901 11:36:49.292253 71250 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0901 11:36:49.296480 71250 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-01 11:36:54,301] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/ernie_3.0_xbase_zh_vocab.txt[0m
[32m[2022-09-01 11:36:54,328] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/tokenizer_config.json[0m
[32m[2022-09-01 11:36:54,329] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/special_tokens_map.json[0m
[32m[2022-09-01 11:36:54,330] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '“'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '”和“'}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': '”之间的逻辑关系是'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': '。'}][0m
[32m[2022-09-01 11:36:54,333] [    INFO][0m - {'contradiction': 0, 'entailment': 1, 'neutral': 2}[0m
2022-09-01 11:36:54,333 INFO [download.py:119] unique_endpoints {''}
2022-09-01 11:36:56,570 INFO [download.py:119] unique_endpoints {''}
[32m[2022-09-01 11:36:56,763] [    INFO][0m - ============================================================[0m
[32m[2022-09-01 11:36:56,763] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-01 11:36:56,763] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-01 11:36:56,763] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-01 11:36:56,764] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-01 11:36:56,764] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-01 11:36:56,764] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-01 11:36:56,764] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-01 11:36:56,764] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-01 11:36:56,764] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-01 11:36:56,764] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-01 11:36:56,764] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-01 11:36:56,764] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-01 11:36:56,764] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-01 11:36:56,764] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-01 11:36:56,764] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-01 11:36:56,765] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-01 11:36:56,765] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-01 11:36:56,765] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-09-01 11:36:56,765] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-09-01 11:36:56,765] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-09-01 11:36:56,765] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-01 11:36:56,765] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-01 11:36:56,765] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-01 11:36:56,765] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-01 11:36:56,765] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-01 11:36:56,765] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-01 11:36:56,765] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-01 11:36:56,765] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-01 11:36:56,766] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-01 11:36:56,766] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-09-01 11:36:56,766] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-01 11:36:56,766] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-01 11:36:56,766] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-01 11:36:56,766] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-01 11:36:56,766] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-01 11:36:56,766] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-01 11:36:56,766] [    INFO][0m - logging_dir                   :./checkpoints/runs/Sep01_11-36-49_instance-3bwob41y-01[0m
[32m[2022-09-01 11:36:56,766] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-01 11:36:56,766] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-01 11:36:56,766] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-01 11:36:56,766] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-01 11:36:56,766] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-01 11:36:56,767] [    INFO][0m - max_seq_length                :128[0m
[32m[2022-09-01 11:36:56,767] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-01 11:36:56,767] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-01 11:36:56,767] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-01 11:36:56,767] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-01 11:36:56,767] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-09-01 11:36:56,767] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-01 11:36:56,767] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-01 11:36:56,767] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-09-01 11:36:56,767] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-01 11:36:56,767] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-01 11:36:56,767] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-09-01 11:36:56,767] [    INFO][0m - per_device_train_batch_size   :32[0m
[32m[2022-09-01 11:36:56,768] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-01 11:36:56,768] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-01 11:36:56,768] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-01 11:36:56,768] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-09-01 11:36:56,768] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-01 11:36:56,768] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-01 11:36:56,768] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-01 11:36:56,768] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-01 11:36:56,768] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-01 11:36:56,768] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-01 11:36:56,768] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-09-01 11:36:56,768] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-01 11:36:56,768] [    INFO][0m - save_steps                    :100[0m
[32m[2022-09-01 11:36:56,768] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-09-01 11:36:56,769] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-01 11:36:56,769] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-01 11:36:56,769] [    INFO][0m - seed                          :42[0m
[32m[2022-09-01 11:36:56,769] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-01 11:36:56,769] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-01 11:36:56,769] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-01 11:36:56,769] [    INFO][0m - train_batch_size              :32[0m
[32m[2022-09-01 11:36:56,769] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-01 11:36:56,769] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-01 11:36:56,769] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-01 11:36:56,769] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-01 11:36:56,769] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-01 11:36:56,769] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-01 11:36:56,770] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-01 11:36:56,770] [    INFO][0m - [0m
[32m[2022-09-01 11:36:56,773] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-01 11:36:56,773] [    INFO][0m -   Num examples = 391783[0m
[32m[2022-09-01 11:36:56,773] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-09-01 11:36:56,773] [    INFO][0m -   Instantaneous batch size per device = 32[0m
[32m[2022-09-01 11:36:56,773] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 32[0m
[32m[2022-09-01 11:36:56,773] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-01 11:36:56,773] [    INFO][0m -   Total optimization steps = 612200.0[0m
[32m[2022-09-01 11:36:56,773] [    INFO][0m -   Total num train samples = 19589150[0m
[32m[2022-09-01 11:37:05,124] [    INFO][0m - loss: 1.6124424, learning_rate: 2.999950996406403e-05, global_step: 10, interval_runtime: 8.3494, interval_samples_per_second: 3.833, interval_steps_per_second: 1.198, epoch: 0.0008[0m
[32m[2022-09-01 11:37:12,065] [    INFO][0m - loss: 1.29179258, learning_rate: 2.9999019928128062e-05, global_step: 20, interval_runtime: 6.9413, interval_samples_per_second: 4.61, interval_steps_per_second: 1.441, epoch: 0.0016[0m
[32m[2022-09-01 11:37:18,965] [    INFO][0m - loss: 1.24049835, learning_rate: 2.9998529892192096e-05, global_step: 30, interval_runtime: 6.9002, interval_samples_per_second: 4.638, interval_steps_per_second: 1.449, epoch: 0.0025[0m
[32m[2022-09-01 11:37:25,868] [    INFO][0m - loss: 1.19535151, learning_rate: 2.9998039856256126e-05, global_step: 40, interval_runtime: 6.9032, interval_samples_per_second: 4.636, interval_steps_per_second: 1.449, epoch: 0.0033[0m
[32m[2022-09-01 11:37:32,766] [    INFO][0m - loss: 1.15912457, learning_rate: 2.9997549820320157e-05, global_step: 50, interval_runtime: 6.8976, interval_samples_per_second: 4.639, interval_steps_per_second: 1.45, epoch: 0.0041[0m
[32m[2022-09-01 11:37:39,731] [    INFO][0m - loss: 1.18903351, learning_rate: 2.9997059784384187e-05, global_step: 60, interval_runtime: 6.9649, interval_samples_per_second: 4.594, interval_steps_per_second: 1.436, epoch: 0.0049[0m
[32m[2022-09-01 11:37:46,623] [    INFO][0m - loss: 1.1660491, learning_rate: 2.999656974844822e-05, global_step: 70, interval_runtime: 6.8924, interval_samples_per_second: 4.643, interval_steps_per_second: 1.451, epoch: 0.0057[0m
[32m[2022-09-01 11:37:53,512] [    INFO][0m - loss: 1.13229456, learning_rate: 2.999607971251225e-05, global_step: 80, interval_runtime: 6.8891, interval_samples_per_second: 4.645, interval_steps_per_second: 1.452, epoch: 0.0065[0m
[32m[2022-09-01 11:38:00,416] [    INFO][0m - loss: 1.16963224, learning_rate: 2.9995589676576282e-05, global_step: 90, interval_runtime: 6.9037, interval_samples_per_second: 4.635, interval_steps_per_second: 1.448, epoch: 0.0074[0m
[32m[2022-09-01 11:38:07,312] [    INFO][0m - loss: 1.12410107, learning_rate: 2.9995099640640313e-05, global_step: 100, interval_runtime: 6.8962, interval_samples_per_second: 4.64, interval_steps_per_second: 1.45, epoch: 0.0082[0m
[32m[2022-09-01 11:38:07,313] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 11:38:07,313] [    INFO][0m -   Num examples = 12241[0m
[32m[2022-09-01 11:38:07,313] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 11:38:07,313] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 11:38:07,313] [    INFO][0m -   Total prediction steps = 383[0m
[32m[2022-09-01 11:39:52,098] [    INFO][0m - eval_loss: 1.1307058334350586, eval_accuracy: 0.3669634833755412, eval_runtime: 104.7837, eval_samples_per_second: 116.822, eval_steps_per_second: 3.655, epoch: 0.0082[0m
[32m[2022-09-01 11:39:52,099] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-09-01 11:39:52,099] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 11:40:00,097] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-01 11:40:00,097] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-01 11:40:21,880] [    INFO][0m - loss: 1.12496023, learning_rate: 2.9994609604704346e-05, global_step: 110, interval_runtime: 134.5672, interval_samples_per_second: 0.238, interval_steps_per_second: 0.074, epoch: 0.009[0m
[32m[2022-09-01 11:40:28,788] [    INFO][0m - loss: 1.12652655, learning_rate: 2.9994119568768377e-05, global_step: 120, interval_runtime: 6.9084, interval_samples_per_second: 4.632, interval_steps_per_second: 1.448, epoch: 0.0098[0m
[32m[2022-09-01 11:40:35,678] [    INFO][0m - loss: 1.15736742, learning_rate: 2.9993629532832407e-05, global_step: 130, interval_runtime: 6.8902, interval_samples_per_second: 4.644, interval_steps_per_second: 1.451, epoch: 0.0106[0m
[32m[2022-09-01 11:40:42,573] [    INFO][0m - loss: 1.16321516, learning_rate: 2.9993139496896438e-05, global_step: 140, interval_runtime: 6.8952, interval_samples_per_second: 4.641, interval_steps_per_second: 1.45, epoch: 0.0114[0m
[32m[2022-09-01 11:40:49,467] [    INFO][0m - loss: 1.12299023, learning_rate: 2.999264946096047e-05, global_step: 150, interval_runtime: 6.8936, interval_samples_per_second: 4.642, interval_steps_per_second: 1.451, epoch: 0.0123[0m
[32m[2022-09-01 11:40:56,344] [    INFO][0m - loss: 1.10303841, learning_rate: 2.9992159425024502e-05, global_step: 160, interval_runtime: 6.8769, interval_samples_per_second: 4.653, interval_steps_per_second: 1.454, epoch: 0.0131[0m
[32m[2022-09-01 11:41:03,212] [    INFO][0m - loss: 1.13754978, learning_rate: 2.9991669389088533e-05, global_step: 170, interval_runtime: 6.8686, interval_samples_per_second: 4.659, interval_steps_per_second: 1.456, epoch: 0.0139[0m
[32m[2022-09-01 11:41:10,100] [    INFO][0m - loss: 1.11255827, learning_rate: 2.9991179353152563e-05, global_step: 180, interval_runtime: 6.8874, interval_samples_per_second: 4.646, interval_steps_per_second: 1.452, epoch: 0.0147[0m
[32m[2022-09-01 11:41:16,994] [    INFO][0m - loss: 1.09702988, learning_rate: 2.9990689317216594e-05, global_step: 190, interval_runtime: 6.8938, interval_samples_per_second: 4.642, interval_steps_per_second: 1.451, epoch: 0.0155[0m
[32m[2022-09-01 11:41:23,934] [    INFO][0m - loss: 1.10847235, learning_rate: 2.9990199281280628e-05, global_step: 200, interval_runtime: 6.9396, interval_samples_per_second: 4.611, interval_steps_per_second: 1.441, epoch: 0.0163[0m
[32m[2022-09-01 11:41:23,934] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 11:41:23,935] [    INFO][0m -   Num examples = 12241[0m
[32m[2022-09-01 11:41:23,935] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 11:41:23,935] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 11:41:23,935] [    INFO][0m -   Total prediction steps = 383[0m
[32m[2022-09-01 11:43:09,126] [    INFO][0m - eval_loss: 1.0845975875854492, eval_accuracy: 0.35732374805979905, eval_runtime: 105.1905, eval_samples_per_second: 116.37, eval_steps_per_second: 3.641, epoch: 0.0163[0m
[32m[2022-09-01 11:43:09,126] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-09-01 11:43:09,127] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 11:43:20,460] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-09-01 11:43:20,461] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-09-01 11:43:41,547] [    INFO][0m - loss: 1.11523895, learning_rate: 2.9989709245344658e-05, global_step: 210, interval_runtime: 137.6138, interval_samples_per_second: 0.233, interval_steps_per_second: 0.073, epoch: 0.0172[0m
[32m[2022-09-01 11:43:48,375] [    INFO][0m - loss: 1.09316921, learning_rate: 2.998921920940869e-05, global_step: 220, interval_runtime: 6.8281, interval_samples_per_second: 4.686, interval_steps_per_second: 1.465, epoch: 0.018[0m
[32m[2022-09-01 11:43:55,199] [    INFO][0m - loss: 1.07033253, learning_rate: 2.998872917347272e-05, global_step: 230, interval_runtime: 6.8239, interval_samples_per_second: 4.689, interval_steps_per_second: 1.465, epoch: 0.0188[0m
[32m[2022-09-01 11:44:02,112] [    INFO][0m - loss: 1.06048965, learning_rate: 2.9988239137536753e-05, global_step: 240, interval_runtime: 6.913, interval_samples_per_second: 4.629, interval_steps_per_second: 1.447, epoch: 0.0196[0m
[32m[2022-09-01 11:44:09,025] [    INFO][0m - loss: 1.03711109, learning_rate: 2.9987749101600787e-05, global_step: 250, interval_runtime: 6.9122, interval_samples_per_second: 4.629, interval_steps_per_second: 1.447, epoch: 0.0204[0m
[32m[2022-09-01 11:44:15,945] [    INFO][0m - loss: 0.9966404, learning_rate: 2.9987259065664818e-05, global_step: 260, interval_runtime: 6.9207, interval_samples_per_second: 4.624, interval_steps_per_second: 1.445, epoch: 0.0212[0m
[32m[2022-09-01 11:44:22,804] [    INFO][0m - loss: 0.98153849, learning_rate: 2.9986769029728848e-05, global_step: 270, interval_runtime: 6.859, interval_samples_per_second: 4.665, interval_steps_per_second: 1.458, epoch: 0.0221[0m
[32m[2022-09-01 11:44:29,683] [    INFO][0m - loss: 0.98820848, learning_rate: 2.9986278993792882e-05, global_step: 280, interval_runtime: 6.8787, interval_samples_per_second: 4.652, interval_steps_per_second: 1.454, epoch: 0.0229[0m
[32m[2022-09-01 11:44:36,585] [    INFO][0m - loss: 0.96859303, learning_rate: 2.9985788957856913e-05, global_step: 290, interval_runtime: 6.9019, interval_samples_per_second: 4.636, interval_steps_per_second: 1.449, epoch: 0.0237[0m
[32m[2022-09-01 11:44:43,615] [    INFO][0m - loss: 0.92543974, learning_rate: 2.9985298921920943e-05, global_step: 300, interval_runtime: 6.9191, interval_samples_per_second: 4.625, interval_steps_per_second: 1.445, epoch: 0.0245[0m
[32m[2022-09-01 11:44:43,616] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 11:44:43,616] [    INFO][0m -   Num examples = 12241[0m
[32m[2022-09-01 11:44:43,616] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 11:44:43,616] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 11:44:43,616] [    INFO][0m -   Total prediction steps = 383[0m
[32m[2022-09-01 11:46:28,562] [    INFO][0m - eval_loss: 0.7790156006813049, eval_accuracy: 0.6842578220733601, eval_runtime: 104.9451, eval_samples_per_second: 116.642, eval_steps_per_second: 3.65, epoch: 0.0245[0m
[32m[2022-09-01 11:46:28,562] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-09-01 11:46:28,563] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 11:46:36,226] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-09-01 11:46:36,226] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-09-01 11:46:57,014] [    INFO][0m - loss: 0.82665529, learning_rate: 2.9984808885984974e-05, global_step: 310, interval_runtime: 133.5096, interval_samples_per_second: 0.24, interval_steps_per_second: 0.075, epoch: 0.0253[0m
[32m[2022-09-01 11:47:03,901] [    INFO][0m - loss: 0.80151577, learning_rate: 2.9984318850049004e-05, global_step: 320, interval_runtime: 6.8873, interval_samples_per_second: 4.646, interval_steps_per_second: 1.452, epoch: 0.0261[0m
[32m[2022-09-01 11:47:10,783] [    INFO][0m - loss: 0.83516951, learning_rate: 2.9983828814113038e-05, global_step: 330, interval_runtime: 6.8822, interval_samples_per_second: 4.65, interval_steps_per_second: 1.453, epoch: 0.027[0m
[32m[2022-09-01 11:47:17,720] [    INFO][0m - loss: 0.84604912, learning_rate: 2.998333877817707e-05, global_step: 340, interval_runtime: 6.9364, interval_samples_per_second: 4.613, interval_steps_per_second: 1.442, epoch: 0.0278[0m
[32m[2022-09-01 11:47:24,639] [    INFO][0m - loss: 0.81149158, learning_rate: 2.99828487422411e-05, global_step: 350, interval_runtime: 6.9194, interval_samples_per_second: 4.625, interval_steps_per_second: 1.445, epoch: 0.0286[0m
[32m[2022-09-01 11:47:31,516] [    INFO][0m - loss: 0.81561108, learning_rate: 2.998235870630513e-05, global_step: 360, interval_runtime: 6.8773, interval_samples_per_second: 4.653, interval_steps_per_second: 1.454, epoch: 0.0294[0m
[32m[2022-09-01 11:47:38,378] [    INFO][0m - loss: 0.7767724, learning_rate: 2.9981868670369163e-05, global_step: 370, interval_runtime: 6.8612, interval_samples_per_second: 4.664, interval_steps_per_second: 1.457, epoch: 0.0302[0m
[32m[2022-09-01 11:47:45,282] [    INFO][0m - loss: 0.75081339, learning_rate: 2.9981378634433194e-05, global_step: 380, interval_runtime: 6.9041, interval_samples_per_second: 4.635, interval_steps_per_second: 1.448, epoch: 0.031[0m
[32m[2022-09-01 11:47:52,200] [    INFO][0m - loss: 0.76172895, learning_rate: 2.9980888598497224e-05, global_step: 390, interval_runtime: 6.9186, interval_samples_per_second: 4.625, interval_steps_per_second: 1.445, epoch: 0.0319[0m
[32m[2022-09-01 11:47:59,084] [    INFO][0m - loss: 0.69308648, learning_rate: 2.9980398562561255e-05, global_step: 400, interval_runtime: 6.8838, interval_samples_per_second: 4.649, interval_steps_per_second: 1.453, epoch: 0.0327[0m
[32m[2022-09-01 11:47:59,084] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 11:47:59,085] [    INFO][0m -   Num examples = 12241[0m
[32m[2022-09-01 11:47:59,085] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 11:47:59,085] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 11:47:59,085] [    INFO][0m -   Total prediction steps = 383[0m
[32m[2022-09-01 11:49:43,866] [    INFO][0m - eval_loss: 0.6403657793998718, eval_accuracy: 0.7466710236091822, eval_runtime: 104.7808, eval_samples_per_second: 116.825, eval_steps_per_second: 3.655, epoch: 0.0327[0m
[32m[2022-09-01 11:49:43,867] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-09-01 11:49:43,867] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 11:49:51,521] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-09-01 11:49:51,522] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-09-01 11:50:12,265] [    INFO][0m - loss: 0.71289062, learning_rate: 2.997990852662529e-05, global_step: 410, interval_runtime: 133.1806, interval_samples_per_second: 0.24, interval_steps_per_second: 0.075, epoch: 0.0335[0m
[32m[2022-09-01 11:50:19,147] [    INFO][0m - loss: 0.74175038, learning_rate: 2.997941849068932e-05, global_step: 420, interval_runtime: 6.8828, interval_samples_per_second: 4.649, interval_steps_per_second: 1.453, epoch: 0.0343[0m
[32m[2022-09-01 11:50:26,024] [    INFO][0m - loss: 0.7584691, learning_rate: 2.997892845475335e-05, global_step: 430, interval_runtime: 6.8765, interval_samples_per_second: 4.654, interval_steps_per_second: 1.454, epoch: 0.0351[0m
[32m[2022-09-01 11:50:32,959] [    INFO][0m - loss: 0.69684944, learning_rate: 2.997843841881738e-05, global_step: 440, interval_runtime: 6.9348, interval_samples_per_second: 4.614, interval_steps_per_second: 1.442, epoch: 0.0359[0m
[32m[2022-09-01 11:50:39,863] [    INFO][0m - loss: 0.71323385, learning_rate: 2.997794838288141e-05, global_step: 450, interval_runtime: 6.9043, interval_samples_per_second: 4.635, interval_steps_per_second: 1.448, epoch: 0.0368[0m
[32m[2022-09-01 11:50:46,765] [    INFO][0m - loss: 0.79579816, learning_rate: 2.9977458346945445e-05, global_step: 460, interval_runtime: 6.9019, interval_samples_per_second: 4.636, interval_steps_per_second: 1.449, epoch: 0.0376[0m
[32m[2022-09-01 11:50:53,670] [    INFO][0m - loss: 0.66806073, learning_rate: 2.9976968311009475e-05, global_step: 470, interval_runtime: 6.9046, interval_samples_per_second: 4.635, interval_steps_per_second: 1.448, epoch: 0.0384[0m
[32m[2022-09-01 11:51:00,600] [    INFO][0m - loss: 0.6901957, learning_rate: 2.9976478275073506e-05, global_step: 480, interval_runtime: 6.9304, interval_samples_per_second: 4.617, interval_steps_per_second: 1.443, epoch: 0.0392[0m
[32m[2022-09-01 11:51:07,522] [    INFO][0m - loss: 0.71615505, learning_rate: 2.9975988239137536e-05, global_step: 490, interval_runtime: 6.922, interval_samples_per_second: 4.623, interval_steps_per_second: 1.445, epoch: 0.04[0m
[32m[2022-09-01 11:51:14,496] [    INFO][0m - loss: 0.76809654, learning_rate: 2.997549820320157e-05, global_step: 500, interval_runtime: 6.9736, interval_samples_per_second: 4.589, interval_steps_per_second: 1.434, epoch: 0.0408[0m
[32m[2022-09-01 11:51:14,496] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 11:51:14,496] [    INFO][0m -   Num examples = 12241[0m
[32m[2022-09-01 11:51:14,496] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 11:51:14,496] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 11:51:14,497] [    INFO][0m -   Total prediction steps = 383[0m
[32m[2022-09-01 11:52:59,296] [    INFO][0m - eval_loss: 0.6982192993164062, eval_accuracy: 0.6845845927620292, eval_runtime: 104.7986, eval_samples_per_second: 116.805, eval_steps_per_second: 3.655, epoch: 0.0408[0m
[32m[2022-09-01 11:52:59,297] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-09-01 11:52:59,297] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 11:53:06,721] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-09-01 11:53:06,722] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-09-01 11:53:28,078] [    INFO][0m - loss: 0.7410306, learning_rate: 2.99750081672656e-05, global_step: 510, interval_runtime: 133.5824, interval_samples_per_second: 0.24, interval_steps_per_second: 0.075, epoch: 0.0417[0m
[32m[2022-09-01 11:53:34,933] [    INFO][0m - loss: 0.75812087, learning_rate: 2.997451813132963e-05, global_step: 520, interval_runtime: 6.8552, interval_samples_per_second: 4.668, interval_steps_per_second: 1.459, epoch: 0.0425[0m
[32m[2022-09-01 11:53:41,806] [    INFO][0m - loss: 0.66175365, learning_rate: 2.997402809539366e-05, global_step: 530, interval_runtime: 6.8726, interval_samples_per_second: 4.656, interval_steps_per_second: 1.455, epoch: 0.0433[0m
[32m[2022-09-01 11:53:48,690] [    INFO][0m - loss: 0.69720092, learning_rate: 2.9973538059457695e-05, global_step: 540, interval_runtime: 6.8843, interval_samples_per_second: 4.648, interval_steps_per_second: 1.453, epoch: 0.0441[0m
[32m[2022-09-01 11:53:55,596] [    INFO][0m - loss: 0.73919277, learning_rate: 2.9973048023521726e-05, global_step: 550, interval_runtime: 6.9057, interval_samples_per_second: 4.634, interval_steps_per_second: 1.448, epoch: 0.0449[0m
[32m[2022-09-01 11:54:02,504] [    INFO][0m - loss: 0.67847748, learning_rate: 2.9972557987585756e-05, global_step: 560, interval_runtime: 6.908, interval_samples_per_second: 4.632, interval_steps_per_second: 1.448, epoch: 0.0457[0m
[32m[2022-09-01 11:54:09,472] [    INFO][0m - loss: 0.71366005, learning_rate: 2.9972067951649787e-05, global_step: 570, interval_runtime: 6.9674, interval_samples_per_second: 4.593, interval_steps_per_second: 1.435, epoch: 0.0466[0m
[32m[2022-09-01 11:54:16,391] [    INFO][0m - loss: 0.62870755, learning_rate: 2.9971577915713817e-05, global_step: 580, interval_runtime: 6.9195, interval_samples_per_second: 4.625, interval_steps_per_second: 1.445, epoch: 0.0474[0m
[32m[2022-09-01 11:54:23,254] [    INFO][0m - loss: 0.64782543, learning_rate: 2.997108787977785e-05, global_step: 590, interval_runtime: 6.8624, interval_samples_per_second: 4.663, interval_steps_per_second: 1.457, epoch: 0.0482[0m
[32m[2022-09-01 11:54:30,139] [    INFO][0m - loss: 0.71219072, learning_rate: 2.9970597843841882e-05, global_step: 600, interval_runtime: 6.8857, interval_samples_per_second: 4.647, interval_steps_per_second: 1.452, epoch: 0.049[0m
[32m[2022-09-01 11:54:30,140] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 11:54:30,140] [    INFO][0m -   Num examples = 12241[0m
[32m[2022-09-01 11:54:30,140] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 11:54:30,140] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 11:54:30,140] [    INFO][0m -   Total prediction steps = 383[0m
[32m[2022-09-01 11:56:14,806] [    INFO][0m - eval_loss: 0.6162481307983398, eval_accuracy: 0.7578629196961033, eval_runtime: 104.6656, eval_samples_per_second: 116.953, eval_steps_per_second: 3.659, epoch: 0.049[0m
[32m[2022-09-01 11:56:14,807] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-09-01 11:56:14,807] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 11:56:27,225] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-09-01 11:56:27,520] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-09-01 11:56:48,655] [    INFO][0m - loss: 0.63429704, learning_rate: 2.9970107807905912e-05, global_step: 610, interval_runtime: 138.5161, interval_samples_per_second: 0.231, interval_steps_per_second: 0.072, epoch: 0.0498[0m
[32m[2022-09-01 11:56:55,509] [    INFO][0m - loss: 0.64632673, learning_rate: 2.9969617771969943e-05, global_step: 620, interval_runtime: 6.8539, interval_samples_per_second: 4.669, interval_steps_per_second: 1.459, epoch: 0.0506[0m
[32m[2022-09-01 11:57:02,409] [    INFO][0m - loss: 0.6356163, learning_rate: 2.9969127736033977e-05, global_step: 630, interval_runtime: 6.8993, interval_samples_per_second: 4.638, interval_steps_per_second: 1.449, epoch: 0.0515[0m
[32m[2022-09-01 11:57:09,299] [    INFO][0m - loss: 0.60351486, learning_rate: 2.9968637700098007e-05, global_step: 640, interval_runtime: 6.8904, interval_samples_per_second: 4.644, interval_steps_per_second: 1.451, epoch: 0.0523[0m
[32m[2022-09-01 11:57:16,175] [    INFO][0m - loss: 0.6660264, learning_rate: 2.9968147664162038e-05, global_step: 650, interval_runtime: 6.8757, interval_samples_per_second: 4.654, interval_steps_per_second: 1.454, epoch: 0.0531[0m
[32m[2022-09-01 11:57:23,092] [    INFO][0m - loss: 0.6718503, learning_rate: 2.9967657628226068e-05, global_step: 660, interval_runtime: 6.9176, interval_samples_per_second: 4.626, interval_steps_per_second: 1.446, epoch: 0.0539[0m
[32m[2022-09-01 11:57:29,968] [    INFO][0m - loss: 0.62623663, learning_rate: 2.9967167592290102e-05, global_step: 670, interval_runtime: 6.8758, interval_samples_per_second: 4.654, interval_steps_per_second: 1.454, epoch: 0.0547[0m
[32m[2022-09-01 11:57:36,884] [    INFO][0m - loss: 0.6316884, learning_rate: 2.9966677556354133e-05, global_step: 680, interval_runtime: 6.9162, interval_samples_per_second: 4.627, interval_steps_per_second: 1.446, epoch: 0.0555[0m
[32m[2022-09-01 11:57:43,789] [    INFO][0m - loss: 0.68161211, learning_rate: 2.9966187520418163e-05, global_step: 690, interval_runtime: 6.9051, interval_samples_per_second: 4.634, interval_steps_per_second: 1.448, epoch: 0.0564[0m
[32m[2022-09-01 11:57:50,698] [    INFO][0m - loss: 0.63821144, learning_rate: 2.9965697484482194e-05, global_step: 700, interval_runtime: 6.9086, interval_samples_per_second: 4.632, interval_steps_per_second: 1.447, epoch: 0.0572[0m
[32m[2022-09-01 11:57:50,699] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 11:57:50,699] [    INFO][0m -   Num examples = 12241[0m
[32m[2022-09-01 11:57:50,699] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 11:57:50,699] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 11:57:50,699] [    INFO][0m -   Total prediction steps = 383[0m
[32m[2022-09-01 11:59:35,366] [    INFO][0m - eval_loss: 0.566649317741394, eval_accuracy: 0.7796748631647741, eval_runtime: 104.6662, eval_samples_per_second: 116.953, eval_steps_per_second: 3.659, epoch: 0.0572[0m
[32m[2022-09-01 11:59:35,366] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-700[0m
[32m[2022-09-01 11:59:35,366] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 11:59:43,331] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-700/tokenizer_config.json[0m
[32m[2022-09-01 11:59:43,332] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-700/special_tokens_map.json[0m
[32m[2022-09-01 12:00:04,407] [    INFO][0m - loss: 0.54111872, learning_rate: 2.9965207448546228e-05, global_step: 710, interval_runtime: 133.709, interval_samples_per_second: 0.239, interval_steps_per_second: 0.075, epoch: 0.058[0m
[32m[2022-09-01 12:00:11,334] [    INFO][0m - loss: 0.60976133, learning_rate: 2.9964717412610258e-05, global_step: 720, interval_runtime: 6.9272, interval_samples_per_second: 4.619, interval_steps_per_second: 1.444, epoch: 0.0588[0m
[32m[2022-09-01 12:00:18,239] [    INFO][0m - loss: 0.67518883, learning_rate: 2.996422737667429e-05, global_step: 730, interval_runtime: 6.9046, interval_samples_per_second: 4.635, interval_steps_per_second: 1.448, epoch: 0.0596[0m
[32m[2022-09-01 12:00:25,150] [    INFO][0m - loss: 0.61654453, learning_rate: 2.9963737340738322e-05, global_step: 740, interval_runtime: 6.9111, interval_samples_per_second: 4.63, interval_steps_per_second: 1.447, epoch: 0.0604[0m
[32m[2022-09-01 12:00:32,128] [    INFO][0m - loss: 0.72497077, learning_rate: 2.9963247304802353e-05, global_step: 750, interval_runtime: 6.9774, interval_samples_per_second: 4.586, interval_steps_per_second: 1.433, epoch: 0.0613[0m
[32m[2022-09-01 12:00:39,047] [    INFO][0m - loss: 0.61993771, learning_rate: 2.9962757268866387e-05, global_step: 760, interval_runtime: 6.9199, interval_samples_per_second: 4.624, interval_steps_per_second: 1.445, epoch: 0.0621[0m
[32m[2022-09-01 12:00:45,932] [    INFO][0m - loss: 0.65212927, learning_rate: 2.9962267232930417e-05, global_step: 770, interval_runtime: 6.8847, interval_samples_per_second: 4.648, interval_steps_per_second: 1.452, epoch: 0.0629[0m
[32m[2022-09-01 12:00:52,849] [    INFO][0m - loss: 0.64579244, learning_rate: 2.9961777196994448e-05, global_step: 780, interval_runtime: 6.9165, interval_samples_per_second: 4.627, interval_steps_per_second: 1.446, epoch: 0.0637[0m
[32m[2022-09-01 12:00:59,801] [    INFO][0m - loss: 0.55175276, learning_rate: 2.996128716105848e-05, global_step: 790, interval_runtime: 6.9523, interval_samples_per_second: 4.603, interval_steps_per_second: 1.438, epoch: 0.0645[0m
[32m[2022-09-01 12:01:06,704] [    INFO][0m - loss: 0.62130628, learning_rate: 2.9960797125122512e-05, global_step: 800, interval_runtime: 6.9037, interval_samples_per_second: 4.635, interval_steps_per_second: 1.449, epoch: 0.0653[0m
[32m[2022-09-01 12:01:06,705] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 12:01:06,705] [    INFO][0m -   Num examples = 12241[0m
[32m[2022-09-01 12:01:06,705] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 12:01:06,705] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 12:01:06,705] [    INFO][0m -   Total prediction steps = 383[0m
[32m[2022-09-01 12:02:51,720] [    INFO][0m - eval_loss: 0.6086662411689758, eval_accuracy: 0.7646434114859897, eval_runtime: 105.0146, eval_samples_per_second: 116.565, eval_steps_per_second: 3.647, epoch: 0.0653[0m
[32m[2022-09-01 12:02:51,721] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-800[0m
[32m[2022-09-01 12:02:51,721] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 12:03:00,370] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-800/tokenizer_config.json[0m
[32m[2022-09-01 12:03:00,370] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-800/special_tokens_map.json[0m
[32m[2022-09-01 12:03:28,482] [    INFO][0m - loss: 0.73686404, learning_rate: 2.9960307089186543e-05, global_step: 810, interval_runtime: 141.7776, interval_samples_per_second: 0.226, interval_steps_per_second: 0.071, epoch: 0.0662[0m
[32m[2022-09-01 12:03:35,353] [    INFO][0m - loss: 0.61007872, learning_rate: 2.9959817053250573e-05, global_step: 820, interval_runtime: 6.8713, interval_samples_per_second: 4.657, interval_steps_per_second: 1.455, epoch: 0.067[0m
[32m[2022-09-01 12:03:42,273] [    INFO][0m - loss: 0.58217812, learning_rate: 2.9959327017314604e-05, global_step: 830, interval_runtime: 6.9193, interval_samples_per_second: 4.625, interval_steps_per_second: 1.445, epoch: 0.0678[0m
[32m[2022-09-01 12:03:49,179] [    INFO][0m - loss: 0.71353846, learning_rate: 2.9958836981378638e-05, global_step: 840, interval_runtime: 6.9065, interval_samples_per_second: 4.633, interval_steps_per_second: 1.448, epoch: 0.0686[0m
[32m[2022-09-01 12:03:56,138] [    INFO][0m - loss: 0.66734161, learning_rate: 2.9958346945442668e-05, global_step: 850, interval_runtime: 6.9588, interval_samples_per_second: 4.598, interval_steps_per_second: 1.437, epoch: 0.0694[0m
[32m[2022-09-01 12:04:02,965] [    INFO][0m - loss: 0.60415888, learning_rate: 2.99578569095067e-05, global_step: 860, interval_runtime: 6.8272, interval_samples_per_second: 4.687, interval_steps_per_second: 1.465, epoch: 0.0702[0m
[32m[2022-09-01 12:04:09,892] [    INFO][0m - loss: 0.6295301, learning_rate: 2.995736687357073e-05, global_step: 870, interval_runtime: 6.9273, interval_samples_per_second: 4.619, interval_steps_per_second: 1.444, epoch: 0.0711[0m
[32m[2022-09-01 12:04:16,755] [    INFO][0m - loss: 0.62188244, learning_rate: 2.995687683763476e-05, global_step: 880, interval_runtime: 6.8627, interval_samples_per_second: 4.663, interval_steps_per_second: 1.457, epoch: 0.0719[0m
[32m[2022-09-01 12:04:23,662] [    INFO][0m - loss: 0.67726226, learning_rate: 2.9956386801698794e-05, global_step: 890, interval_runtime: 6.9073, interval_samples_per_second: 4.633, interval_steps_per_second: 1.448, epoch: 0.0727[0m
[32m[2022-09-01 12:04:30,579] [    INFO][0m - loss: 0.57649469, learning_rate: 2.9955896765762824e-05, global_step: 900, interval_runtime: 6.9166, interval_samples_per_second: 4.627, interval_steps_per_second: 1.446, epoch: 0.0735[0m
[32m[2022-09-01 12:04:30,579] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 12:04:30,580] [    INFO][0m -   Num examples = 12241[0m
[32m[2022-09-01 12:04:30,580] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 12:04:30,580] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 12:04:30,580] [    INFO][0m -   Total prediction steps = 383[0m
[32m[2022-09-01 12:06:15,613] [    INFO][0m - eval_loss: 0.6061053276062012, eval_accuracy: 0.7609672412384609, eval_runtime: 105.0329, eval_samples_per_second: 116.544, eval_steps_per_second: 3.646, epoch: 0.0735[0m
[32m[2022-09-01 12:06:15,614] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-900[0m
[32m[2022-09-01 12:06:15,614] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 12:06:23,345] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-900/tokenizer_config.json[0m
[32m[2022-09-01 12:06:23,345] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-900/special_tokens_map.json[0m
[32m[2022-09-01 12:06:44,328] [    INFO][0m - loss: 0.61601963, learning_rate: 2.9955406729826855e-05, global_step: 910, interval_runtime: 133.7485, interval_samples_per_second: 0.239, interval_steps_per_second: 0.075, epoch: 0.0743[0m
[32m[2022-09-01 12:06:51,234] [    INFO][0m - loss: 0.6353991, learning_rate: 2.9954916693890885e-05, global_step: 920, interval_runtime: 6.906, interval_samples_per_second: 4.634, interval_steps_per_second: 1.448, epoch: 0.0751[0m
[32m[2022-09-01 12:06:58,134] [    INFO][0m - loss: 0.59300289, learning_rate: 2.995442665795492e-05, global_step: 930, interval_runtime: 6.9, interval_samples_per_second: 4.638, interval_steps_per_second: 1.449, epoch: 0.076[0m
[32m[2022-09-01 12:07:04,993] [    INFO][0m - loss: 0.7059289, learning_rate: 2.995393662201895e-05, global_step: 940, interval_runtime: 6.8597, interval_samples_per_second: 4.665, interval_steps_per_second: 1.458, epoch: 0.0768[0m
[32m[2022-09-01 12:07:11,927] [    INFO][0m - loss: 0.63906832, learning_rate: 2.995344658608298e-05, global_step: 950, interval_runtime: 6.9338, interval_samples_per_second: 4.615, interval_steps_per_second: 1.442, epoch: 0.0776[0m
[32m[2022-09-01 12:07:18,824] [    INFO][0m - loss: 0.61857262, learning_rate: 2.995295655014701e-05, global_step: 960, interval_runtime: 6.8963, interval_samples_per_second: 4.64, interval_steps_per_second: 1.45, epoch: 0.0784[0m
[32m[2022-09-01 12:07:25,735] [    INFO][0m - loss: 0.72147064, learning_rate: 2.9952466514211044e-05, global_step: 970, interval_runtime: 6.9113, interval_samples_per_second: 4.63, interval_steps_per_second: 1.447, epoch: 0.0792[0m
[32m[2022-09-01 12:07:32,647] [    INFO][0m - loss: 0.70121427, learning_rate: 2.9951976478275075e-05, global_step: 980, interval_runtime: 6.9119, interval_samples_per_second: 4.63, interval_steps_per_second: 1.447, epoch: 0.08[0m
[32m[2022-09-01 12:07:39,615] [    INFO][0m - loss: 0.55190272, learning_rate: 2.9951486442339105e-05, global_step: 990, interval_runtime: 6.9688, interval_samples_per_second: 4.592, interval_steps_per_second: 1.435, epoch: 0.0809[0m
[32m[2022-09-01 12:07:46,548] [    INFO][0m - loss: 0.58265982, learning_rate: 2.9950996406403136e-05, global_step: 1000, interval_runtime: 6.9325, interval_samples_per_second: 4.616, interval_steps_per_second: 1.442, epoch: 0.0817[0m
[32m[2022-09-01 12:07:46,548] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 12:07:46,548] [    INFO][0m -   Num examples = 12241[0m
[32m[2022-09-01 12:07:46,549] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 12:07:46,549] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 12:07:46,549] [    INFO][0m -   Total prediction steps = 383[0m
[32m[2022-09-01 12:09:31,643] [    INFO][0m - eval_loss: 0.6215369701385498, eval_accuracy: 0.7683195817335186, eval_runtime: 105.0942, eval_samples_per_second: 116.476, eval_steps_per_second: 3.644, epoch: 0.0817[0m
[32m[2022-09-01 12:09:31,644] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1000[0m
[32m[2022-09-01 12:09:31,644] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 12:09:39,614] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1000/tokenizer_config.json[0m
[32m[2022-09-01 12:09:39,615] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1000/special_tokens_map.json[0m
[32m[2022-09-01 12:10:05,411] [    INFO][0m - loss: 0.71224856, learning_rate: 2.995050637046717e-05, global_step: 1010, interval_runtime: 138.8624, interval_samples_per_second: 0.23, interval_steps_per_second: 0.072, epoch: 0.0825[0m
[32m[2022-09-01 12:10:12,260] [    INFO][0m - loss: 0.6003294, learning_rate: 2.99500163345312e-05, global_step: 1020, interval_runtime: 6.8495, interval_samples_per_second: 4.672, interval_steps_per_second: 1.46, epoch: 0.0833[0m
[32m[2022-09-01 12:10:19,177] [    INFO][0m - loss: 0.58899884, learning_rate: 2.994952629859523e-05, global_step: 1030, interval_runtime: 6.9173, interval_samples_per_second: 4.626, interval_steps_per_second: 1.446, epoch: 0.0841[0m
[32m[2022-09-01 12:10:26,043] [    INFO][0m - loss: 0.5988111, learning_rate: 2.994903626265926e-05, global_step: 1040, interval_runtime: 6.8654, interval_samples_per_second: 4.661, interval_steps_per_second: 1.457, epoch: 0.0849[0m
[32m[2022-09-01 12:10:32,923] [    INFO][0m - loss: 0.72471333, learning_rate: 2.9948546226723292e-05, global_step: 1050, interval_runtime: 6.8804, interval_samples_per_second: 4.651, interval_steps_per_second: 1.453, epoch: 0.0858[0m
[32m[2022-09-01 12:10:39,823] [    INFO][0m - loss: 0.60458817, learning_rate: 2.9948056190787326e-05, global_step: 1060, interval_runtime: 6.8998, interval_samples_per_second: 4.638, interval_steps_per_second: 1.449, epoch: 0.0866[0m
[32m[2022-09-01 12:10:46,781] [    INFO][0m - loss: 0.55230818, learning_rate: 2.9947566154851356e-05, global_step: 1070, interval_runtime: 6.9579, interval_samples_per_second: 4.599, interval_steps_per_second: 1.437, epoch: 0.0874[0m
[32m[2022-09-01 12:10:53,723] [    INFO][0m - loss: 0.62610092, learning_rate: 2.9947076118915387e-05, global_step: 1080, interval_runtime: 6.942, interval_samples_per_second: 4.61, interval_steps_per_second: 1.441, epoch: 0.0882[0m
[32m[2022-09-01 12:11:00,592] [    INFO][0m - loss: 0.6473002, learning_rate: 2.9946586082979417e-05, global_step: 1090, interval_runtime: 6.8689, interval_samples_per_second: 4.659, interval_steps_per_second: 1.456, epoch: 0.089[0m
[32m[2022-09-01 12:11:07,436] [    INFO][0m - loss: 0.58332734, learning_rate: 2.994609604704345e-05, global_step: 1100, interval_runtime: 6.844, interval_samples_per_second: 4.676, interval_steps_per_second: 1.461, epoch: 0.0898[0m
[32m[2022-09-01 12:11:07,437] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 12:11:07,437] [    INFO][0m -   Num examples = 12241[0m
[32m[2022-09-01 12:11:07,437] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 12:11:07,437] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 12:11:07,437] [    INFO][0m -   Total prediction steps = 383[0m
[32m[2022-09-01 12:12:51,943] [    INFO][0m - eval_loss: 0.5936331748962402, eval_accuracy: 0.7842496528061433, eval_runtime: 104.5051, eval_samples_per_second: 117.133, eval_steps_per_second: 3.665, epoch: 0.0898[0m
[32m[2022-09-01 12:12:51,943] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1100[0m
[32m[2022-09-01 12:12:51,943] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 12:12:59,727] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1100/tokenizer_config.json[0m
[32m[2022-09-01 12:12:59,728] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1100/special_tokens_map.json[0m
[32m[2022-09-01 12:13:20,626] [    INFO][0m - loss: 0.72761278, learning_rate: 2.994560601110748e-05, global_step: 1110, interval_runtime: 133.1902, interval_samples_per_second: 0.24, interval_steps_per_second: 0.075, epoch: 0.0907[0m
[32m[2022-09-01 12:13:27,464] [    INFO][0m - loss: 0.58892312, learning_rate: 2.9945115975171512e-05, global_step: 1120, interval_runtime: 6.8378, interval_samples_per_second: 4.68, interval_steps_per_second: 1.462, epoch: 0.0915[0m
[32m[2022-09-01 12:13:34,362] [    INFO][0m - loss: 0.64513044, learning_rate: 2.9944625939235543e-05, global_step: 1130, interval_runtime: 6.8984, interval_samples_per_second: 4.639, interval_steps_per_second: 1.45, epoch: 0.0923[0m
[32m[2022-09-01 12:13:41,312] [    INFO][0m - loss: 0.56411753, learning_rate: 2.9944135903299577e-05, global_step: 1140, interval_runtime: 6.9503, interval_samples_per_second: 4.604, interval_steps_per_second: 1.439, epoch: 0.0931[0m
[32m[2022-09-01 12:13:48,225] [    INFO][0m - loss: 0.64052124, learning_rate: 2.9943645867363607e-05, global_step: 1150, interval_runtime: 6.9127, interval_samples_per_second: 4.629, interval_steps_per_second: 1.447, epoch: 0.0939[0m
[32m[2022-09-01 12:13:55,147] [    INFO][0m - loss: 0.54615946, learning_rate: 2.9943155831427638e-05, global_step: 1160, interval_runtime: 6.9224, interval_samples_per_second: 4.623, interval_steps_per_second: 1.445, epoch: 0.0947[0m
[32m[2022-09-01 12:14:02,087] [    INFO][0m - loss: 0.62947154, learning_rate: 2.9942665795491668e-05, global_step: 1170, interval_runtime: 6.9388, interval_samples_per_second: 4.612, interval_steps_per_second: 1.441, epoch: 0.0956[0m
[32m[2022-09-01 12:14:09,005] [    INFO][0m - loss: 0.59906282, learning_rate: 2.99421757595557e-05, global_step: 1180, interval_runtime: 6.9191, interval_samples_per_second: 4.625, interval_steps_per_second: 1.445, epoch: 0.0964[0m
[32m[2022-09-01 12:14:15,920] [    INFO][0m - loss: 0.50917969, learning_rate: 2.9941685723619732e-05, global_step: 1190, interval_runtime: 6.9135, interval_samples_per_second: 4.629, interval_steps_per_second: 1.446, epoch: 0.0972[0m
[32m[2022-09-01 12:14:22,859] [    INFO][0m - loss: 0.68763094, learning_rate: 2.9941195687683763e-05, global_step: 1200, interval_runtime: 6.9397, interval_samples_per_second: 4.611, interval_steps_per_second: 1.441, epoch: 0.098[0m
[32m[2022-09-01 12:14:22,859] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 12:14:22,859] [    INFO][0m -   Num examples = 12241[0m
[32m[2022-09-01 12:14:22,859] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 12:14:22,860] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 12:14:22,860] [    INFO][0m -   Total prediction steps = 383[0m
[32m[2022-09-01 12:16:07,558] [    INFO][0m - eval_loss: 0.6072450280189514, eval_accuracy: 0.7574544563352668, eval_runtime: 104.6982, eval_samples_per_second: 116.917, eval_steps_per_second: 3.658, epoch: 0.098[0m
[32m[2022-09-01 12:16:07,559] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1200[0m
[32m[2022-09-01 12:16:07,559] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 12:16:17,412] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1200/tokenizer_config.json[0m
[32m[2022-09-01 12:16:17,413] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1200/special_tokens_map.json[0m
[32m[2022-09-01 12:16:38,465] [    INFO][0m - loss: 0.68927245, learning_rate: 2.9940705651747793e-05, global_step: 1210, interval_runtime: 135.6063, interval_samples_per_second: 0.236, interval_steps_per_second: 0.074, epoch: 0.0988[0m
[32m[2022-09-01 12:16:45,375] [    INFO][0m - loss: 0.53392782, learning_rate: 2.9940215615811824e-05, global_step: 1220, interval_runtime: 6.91, interval_samples_per_second: 4.631, interval_steps_per_second: 1.447, epoch: 0.0996[0m
[32m[2022-09-01 12:16:52,286] [    INFO][0m - loss: 0.60410743, learning_rate: 2.9939725579875858e-05, global_step: 1230, interval_runtime: 6.9108, interval_samples_per_second: 4.63, interval_steps_per_second: 1.447, epoch: 0.1005[0m
[32m[2022-09-01 12:16:59,207] [    INFO][0m - loss: 0.59920664, learning_rate: 2.9939235543939892e-05, global_step: 1240, interval_runtime: 6.9206, interval_samples_per_second: 4.624, interval_steps_per_second: 1.445, epoch: 0.1013[0m
[32m[2022-09-01 12:17:06,040] [    INFO][0m - loss: 0.60087767, learning_rate: 2.9938745508003922e-05, global_step: 1250, interval_runtime: 6.8336, interval_samples_per_second: 4.683, interval_steps_per_second: 1.463, epoch: 0.1021[0m
[32m[2022-09-01 12:17:13,027] [    INFO][0m - loss: 0.61316524, learning_rate: 2.9938255472067953e-05, global_step: 1260, interval_runtime: 6.9867, interval_samples_per_second: 4.58, interval_steps_per_second: 1.431, epoch: 0.1029[0m
[32m[2022-09-01 12:17:19,982] [    INFO][0m - loss: 0.64111843, learning_rate: 2.9937765436131987e-05, global_step: 1270, interval_runtime: 6.9545, interval_samples_per_second: 4.601, interval_steps_per_second: 1.438, epoch: 0.1037[0m
[32m[2022-09-01 12:17:26,911] [    INFO][0m - loss: 0.58497233, learning_rate: 2.9937275400196017e-05, global_step: 1280, interval_runtime: 6.9297, interval_samples_per_second: 4.618, interval_steps_per_second: 1.443, epoch: 0.1045[0m
[32m[2022-09-01 12:17:33,805] [    INFO][0m - loss: 0.48350224, learning_rate: 2.9936785364260048e-05, global_step: 1290, interval_runtime: 6.8942, interval_samples_per_second: 4.642, interval_steps_per_second: 1.45, epoch: 0.1054[0m
[32m[2022-09-01 12:17:40,693] [    INFO][0m - loss: 0.61706185, learning_rate: 2.9936295328324078e-05, global_step: 1300, interval_runtime: 6.8874, interval_samples_per_second: 4.646, interval_steps_per_second: 1.452, epoch: 0.1062[0m
[32m[2022-09-01 12:17:40,693] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 12:17:40,693] [    INFO][0m -   Num examples = 12241[0m
[32m[2022-09-01 12:17:40,693] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 12:17:40,693] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 12:17:40,694] [    INFO][0m -   Total prediction steps = 383[0m
[32m[2022-09-01 12:19:25,611] [    INFO][0m - eval_loss: 0.5676512718200684, eval_accuracy: 0.7867821256433298, eval_runtime: 104.9166, eval_samples_per_second: 116.674, eval_steps_per_second: 3.651, epoch: 0.1062[0m
[32m[2022-09-01 12:19:25,611] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1300[0m
[32m[2022-09-01 12:19:25,611] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 12:19:33,639] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1300/tokenizer_config.json[0m
[32m[2022-09-01 12:19:33,640] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1300/special_tokens_map.json[0m
[32m[2022-09-01 12:19:53,921] [    INFO][0m - loss: 0.59156513, learning_rate: 2.993580529238811e-05, global_step: 1310, interval_runtime: 133.2281, interval_samples_per_second: 0.24, interval_steps_per_second: 0.075, epoch: 0.107[0m
[32m[2022-09-01 12:20:00,851] [    INFO][0m - loss: 0.57618952, learning_rate: 2.9935315256452143e-05, global_step: 1320, interval_runtime: 6.9303, interval_samples_per_second: 4.617, interval_steps_per_second: 1.443, epoch: 0.1078[0m
[32m[2022-09-01 12:20:07,769] [    INFO][0m - loss: 0.59141827, learning_rate: 2.9934825220516173e-05, global_step: 1330, interval_runtime: 6.9181, interval_samples_per_second: 4.626, interval_steps_per_second: 1.445, epoch: 0.1086[0m
[32m[2022-09-01 12:20:14,682] [    INFO][0m - loss: 0.65829749, learning_rate: 2.9934335184580204e-05, global_step: 1340, interval_runtime: 6.9125, interval_samples_per_second: 4.629, interval_steps_per_second: 1.447, epoch: 0.1094[0m
[32m[2022-09-01 12:20:21,657] [    INFO][0m - loss: 0.60647902, learning_rate: 2.9933845148644234e-05, global_step: 1350, interval_runtime: 6.9747, interval_samples_per_second: 4.588, interval_steps_per_second: 1.434, epoch: 0.1103[0m
[32m[2022-09-01 12:20:28,655] [    INFO][0m - loss: 0.6408783, learning_rate: 2.9933355112708268e-05, global_step: 1360, interval_runtime: 6.9981, interval_samples_per_second: 4.573, interval_steps_per_second: 1.429, epoch: 0.1111[0m
[32m[2022-09-01 12:20:35,631] [    INFO][0m - loss: 0.60824122, learning_rate: 2.99328650767723e-05, global_step: 1370, interval_runtime: 6.9765, interval_samples_per_second: 4.587, interval_steps_per_second: 1.433, epoch: 0.1119[0m
[32m[2022-09-01 12:20:42,607] [    INFO][0m - loss: 0.62702227, learning_rate: 2.993237504083633e-05, global_step: 1380, interval_runtime: 6.9753, interval_samples_per_second: 4.588, interval_steps_per_second: 1.434, epoch: 0.1127[0m
[32m[2022-09-01 12:20:49,543] [    INFO][0m - loss: 0.59172716, learning_rate: 2.993188500490036e-05, global_step: 1390, interval_runtime: 6.9363, interval_samples_per_second: 4.613, interval_steps_per_second: 1.442, epoch: 0.1135[0m
[32m[2022-09-01 12:20:56,523] [    INFO][0m - loss: 0.62193203, learning_rate: 2.9931394968964393e-05, global_step: 1400, interval_runtime: 6.9802, interval_samples_per_second: 4.584, interval_steps_per_second: 1.433, epoch: 0.1143[0m
[32m[2022-09-01 12:20:56,524] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 12:20:56,524] [    INFO][0m -   Num examples = 12241[0m
[32m[2022-09-01 12:20:56,524] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 12:20:56,524] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 12:20:56,524] [    INFO][0m -   Total prediction steps = 383[0m
[32m[2022-09-01 12:22:41,407] [    INFO][0m - eval_loss: 0.5352177023887634, eval_accuracy: 0.7920921493342047, eval_runtime: 104.8827, eval_samples_per_second: 116.711, eval_steps_per_second: 3.652, epoch: 0.1143[0m
[32m[2022-09-01 12:22:41,408] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1400[0m
[32m[2022-09-01 12:22:41,408] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 12:22:49,583] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1400/tokenizer_config.json[0m
[32m[2022-09-01 12:22:49,584] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1400/special_tokens_map.json[0m
[32m[2022-09-01 12:23:11,100] [    INFO][0m - loss: 0.62839966, learning_rate: 2.9930904933028424e-05, global_step: 1410, interval_runtime: 134.5769, interval_samples_per_second: 0.238, interval_steps_per_second: 0.074, epoch: 0.1152[0m
[32m[2022-09-01 12:23:18,055] [    INFO][0m - loss: 0.5785923, learning_rate: 2.9930414897092454e-05, global_step: 1420, interval_runtime: 6.9547, interval_samples_per_second: 4.601, interval_steps_per_second: 1.438, epoch: 0.116[0m
[32m[2022-09-01 12:23:24,952] [    INFO][0m - loss: 0.54428906, learning_rate: 2.9929924861156485e-05, global_step: 1430, interval_runtime: 6.897, interval_samples_per_second: 4.64, interval_steps_per_second: 1.45, epoch: 0.1168[0m
[32m[2022-09-01 12:23:31,836] [    INFO][0m - loss: 0.57270422, learning_rate: 2.992943482522052e-05, global_step: 1440, interval_runtime: 6.8844, interval_samples_per_second: 4.648, interval_steps_per_second: 1.453, epoch: 0.1176[0m
[32m[2022-09-01 12:23:38,750] [    INFO][0m - loss: 0.57195525, learning_rate: 2.992894478928455e-05, global_step: 1450, interval_runtime: 6.9138, interval_samples_per_second: 4.628, interval_steps_per_second: 1.446, epoch: 0.1184[0m
[32m[2022-09-01 12:23:45,669] [    INFO][0m - loss: 0.58897924, learning_rate: 2.992845475334858e-05, global_step: 1460, interval_runtime: 6.9195, interval_samples_per_second: 4.625, interval_steps_per_second: 1.445, epoch: 0.1192[0m
[32m[2022-09-01 12:23:52,613] [    INFO][0m - loss: 0.70266957, learning_rate: 2.992796471741261e-05, global_step: 1470, interval_runtime: 6.9432, interval_samples_per_second: 4.609, interval_steps_per_second: 1.44, epoch: 0.1201[0m
[32m[2022-09-01 12:23:59,506] [    INFO][0m - loss: 0.5555913, learning_rate: 2.992747468147664e-05, global_step: 1480, interval_runtime: 6.894, interval_samples_per_second: 4.642, interval_steps_per_second: 1.451, epoch: 0.1209[0m
[32m[2022-09-01 12:24:06,455] [    INFO][0m - loss: 0.67889037, learning_rate: 2.9926984645540675e-05, global_step: 1490, interval_runtime: 6.9475, interval_samples_per_second: 4.606, interval_steps_per_second: 1.439, epoch: 0.1217[0m
[32m[2022-09-01 12:24:13,377] [    INFO][0m - loss: 0.58501811, learning_rate: 2.9926494609604705e-05, global_step: 1500, interval_runtime: 6.9226, interval_samples_per_second: 4.623, interval_steps_per_second: 1.445, epoch: 0.1225[0m
[32m[2022-09-01 12:24:13,377] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 12:24:13,378] [    INFO][0m -   Num examples = 12241[0m
[32m[2022-09-01 12:24:13,378] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 12:24:13,378] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 12:24:13,378] [    INFO][0m -   Total prediction steps = 383[0m
[32m[2022-09-01 12:25:58,244] [    INFO][0m - eval_loss: 0.526285707950592, eval_accuracy: 0.7933992320888816, eval_runtime: 104.8655, eval_samples_per_second: 116.731, eval_steps_per_second: 3.652, epoch: 0.1225[0m
[32m[2022-09-01 12:25:58,244] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1500[0m
[32m[2022-09-01 12:25:58,245] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 12:26:06,042] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1500/tokenizer_config.json[0m
[32m[2022-09-01 12:26:06,043] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1500/special_tokens_map.json[0m
[32m[2022-09-01 12:26:27,270] [    INFO][0m - loss: 0.61021838, learning_rate: 2.9926004573668736e-05, global_step: 1510, interval_runtime: 133.8931, interval_samples_per_second: 0.239, interval_steps_per_second: 0.075, epoch: 0.1233[0m
[32m[2022-09-01 12:26:34,133] [    INFO][0m - loss: 0.58575053, learning_rate: 2.9925514537732766e-05, global_step: 1520, interval_runtime: 6.8636, interval_samples_per_second: 4.662, interval_steps_per_second: 1.457, epoch: 0.1241[0m
[32m[2022-09-01 12:26:41,044] [    INFO][0m - loss: 0.54290342, learning_rate: 2.99250245017968e-05, global_step: 1530, interval_runtime: 6.9106, interval_samples_per_second: 4.631, interval_steps_per_second: 1.447, epoch: 0.125[0m
[32m[2022-09-01 12:26:47,977] [    INFO][0m - loss: 0.69173918, learning_rate: 2.992453446586083e-05, global_step: 1540, interval_runtime: 6.9329, interval_samples_per_second: 4.616, interval_steps_per_second: 1.442, epoch: 0.1258[0m
[32m[2022-09-01 12:26:54,920] [    INFO][0m - loss: 0.5978786, learning_rate: 2.992404442992486e-05, global_step: 1550, interval_runtime: 6.9429, interval_samples_per_second: 4.609, interval_steps_per_second: 1.44, epoch: 0.1266[0m
[32m[2022-09-01 12:27:01,822] [    INFO][0m - loss: 0.67826114, learning_rate: 2.992355439398889e-05, global_step: 1560, interval_runtime: 6.902, interval_samples_per_second: 4.636, interval_steps_per_second: 1.449, epoch: 0.1274[0m
[32m[2022-09-01 12:27:08,727] [    INFO][0m - loss: 0.62148919, learning_rate: 2.9923064358052926e-05, global_step: 1570, interval_runtime: 6.9055, interval_samples_per_second: 4.634, interval_steps_per_second: 1.448, epoch: 0.1282[0m
[32m[2022-09-01 12:27:15,655] [    INFO][0m - loss: 0.58697271, learning_rate: 2.9922574322116956e-05, global_step: 1580, interval_runtime: 6.9279, interval_samples_per_second: 4.619, interval_steps_per_second: 1.443, epoch: 0.129[0m
[32m[2022-09-01 12:27:22,592] [    INFO][0m - loss: 0.56411505, learning_rate: 2.9922084286180987e-05, global_step: 1590, interval_runtime: 6.9369, interval_samples_per_second: 4.613, interval_steps_per_second: 1.442, epoch: 0.1299[0m
[32m[2022-09-01 12:27:29,524] [    INFO][0m - loss: 0.59425507, learning_rate: 2.9921594250245017e-05, global_step: 1600, interval_runtime: 6.9319, interval_samples_per_second: 4.616, interval_steps_per_second: 1.443, epoch: 0.1307[0m
[32m[2022-09-01 12:27:29,525] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 12:27:29,525] [    INFO][0m -   Num examples = 12241[0m
[32m[2022-09-01 12:27:29,525] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 12:27:29,525] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 12:27:29,525] [    INFO][0m -   Total prediction steps = 383[0m
[32m[2022-09-01 12:29:14,533] [    INFO][0m - eval_loss: 0.5785406231880188, eval_accuracy: 0.7741197614573972, eval_runtime: 105.0069, eval_samples_per_second: 116.573, eval_steps_per_second: 3.647, epoch: 0.1307[0m
[32m[2022-09-01 12:29:14,533] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1600[0m
[32m[2022-09-01 12:29:14,533] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 12:29:22,271] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1600/tokenizer_config.json[0m
[32m[2022-09-01 12:29:22,271] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1600/special_tokens_map.json[0m
[32m[2022-09-01 12:29:43,607] [    INFO][0m - loss: 0.64215984, learning_rate: 2.9921104214309048e-05, global_step: 1610, interval_runtime: 134.0826, interval_samples_per_second: 0.239, interval_steps_per_second: 0.075, epoch: 0.1315[0m
[32m[2022-09-01 12:29:50,484] [    INFO][0m - loss: 0.56887999, learning_rate: 2.992061417837308e-05, global_step: 1620, interval_runtime: 6.8767, interval_samples_per_second: 4.653, interval_steps_per_second: 1.454, epoch: 0.1323[0m
[32m[2022-09-01 12:29:57,402] [    INFO][0m - loss: 0.52948213, learning_rate: 2.9920124142437112e-05, global_step: 1630, interval_runtime: 6.9179, interval_samples_per_second: 4.626, interval_steps_per_second: 1.446, epoch: 0.1331[0m
[32m[2022-09-01 12:30:04,276] [    INFO][0m - loss: 0.59607382, learning_rate: 2.9919634106501142e-05, global_step: 1640, interval_runtime: 6.8738, interval_samples_per_second: 4.655, interval_steps_per_second: 1.455, epoch: 0.1339[0m
[32m[2022-09-01 12:30:11,177] [    INFO][0m - loss: 0.65548739, learning_rate: 2.9919144070565173e-05, global_step: 1650, interval_runtime: 6.9014, interval_samples_per_second: 4.637, interval_steps_per_second: 1.449, epoch: 0.1348[0m
[32m[2022-09-01 12:30:18,075] [    INFO][0m - loss: 0.60017667, learning_rate: 2.9918654034629207e-05, global_step: 1660, interval_runtime: 6.8979, interval_samples_per_second: 4.639, interval_steps_per_second: 1.45, epoch: 0.1356[0m
[32m[2022-09-01 12:30:24,956] [    INFO][0m - loss: 0.65048256, learning_rate: 2.9918163998693237e-05, global_step: 1670, interval_runtime: 6.8814, interval_samples_per_second: 4.65, interval_steps_per_second: 1.453, epoch: 0.1364[0m
[32m[2022-09-01 12:30:31,852] [    INFO][0m - loss: 0.61015739, learning_rate: 2.9917673962757268e-05, global_step: 1680, interval_runtime: 6.8958, interval_samples_per_second: 4.641, interval_steps_per_second: 1.45, epoch: 0.1372[0m
[32m[2022-09-01 12:30:38,770] [    INFO][0m - loss: 0.5690074, learning_rate: 2.99171839268213e-05, global_step: 1690, interval_runtime: 6.9182, interval_samples_per_second: 4.625, interval_steps_per_second: 1.445, epoch: 0.138[0m
[32m[2022-09-01 12:30:45,671] [    INFO][0m - loss: 0.60836296, learning_rate: 2.9916693890885332e-05, global_step: 1700, interval_runtime: 6.9012, interval_samples_per_second: 4.637, interval_steps_per_second: 1.449, epoch: 0.1388[0m
[32m[2022-09-01 12:30:45,672] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 12:30:45,672] [    INFO][0m -   Num examples = 12241[0m
[32m[2022-09-01 12:30:45,672] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 12:30:45,672] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 12:30:45,672] [    INFO][0m -   Total prediction steps = 383[0m
[32m[2022-09-01 12:32:30,543] [    INFO][0m - eval_loss: 0.5351061224937439, eval_accuracy: 0.7949513928600604, eval_runtime: 104.8705, eval_samples_per_second: 116.725, eval_steps_per_second: 3.652, epoch: 0.1388[0m
[32m[2022-09-01 12:32:30,544] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1700[0m
[32m[2022-09-01 12:32:30,544] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 12:32:38,272] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1700/tokenizer_config.json[0m
[32m[2022-09-01 12:32:38,273] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1700/special_tokens_map.json[0m
[32m[2022-09-01 12:32:59,083] [    INFO][0m - loss: 0.69819827, learning_rate: 2.9916203854949363e-05, global_step: 1710, interval_runtime: 133.4118, interval_samples_per_second: 0.24, interval_steps_per_second: 0.075, epoch: 0.1397[0m
[32m[2022-09-01 12:33:06,004] [    INFO][0m - loss: 0.57502522, learning_rate: 2.9915713819013397e-05, global_step: 1720, interval_runtime: 6.9208, interval_samples_per_second: 4.624, interval_steps_per_second: 1.445, epoch: 0.1405[0m
[32m[2022-09-01 12:33:12,928] [    INFO][0m - loss: 0.56174393, learning_rate: 2.9915223783077427e-05, global_step: 1730, interval_runtime: 6.9243, interval_samples_per_second: 4.621, interval_steps_per_second: 1.444, epoch: 0.1413[0m
[32m[2022-09-01 12:33:20,153] [    INFO][0m - loss: 0.61483326, learning_rate: 2.9914733747141458e-05, global_step: 1740, interval_runtime: 6.9547, interval_samples_per_second: 4.601, interval_steps_per_second: 1.438, epoch: 0.1421[0m
[32m[2022-09-01 12:33:27,095] [    INFO][0m - loss: 0.64162498, learning_rate: 2.991424371120549e-05, global_step: 1750, interval_runtime: 7.212, interval_samples_per_second: 4.437, interval_steps_per_second: 1.387, epoch: 0.1429[0m
[32m[2022-09-01 12:33:34,033] [    INFO][0m - loss: 0.59418988, learning_rate: 2.9913753675269522e-05, global_step: 1760, interval_runtime: 6.9377, interval_samples_per_second: 4.612, interval_steps_per_second: 1.441, epoch: 0.1437[0m
[32m[2022-09-01 12:33:40,900] [    INFO][0m - loss: 0.54962735, learning_rate: 2.9913263639333553e-05, global_step: 1770, interval_runtime: 6.8673, interval_samples_per_second: 4.66, interval_steps_per_second: 1.456, epoch: 0.1446[0m
[32m[2022-09-01 12:33:51,604] [    INFO][0m - loss: 0.51869874, learning_rate: 2.9912773603397583e-05, global_step: 1780, interval_runtime: 6.9438, interval_samples_per_second: 4.608, interval_steps_per_second: 1.44, epoch: 0.1454[0m
[32m[2022-09-01 12:33:58,482] [    INFO][0m - loss: 0.69255795, learning_rate: 2.9912283567461617e-05, global_step: 1790, interval_runtime: 10.6385, interval_samples_per_second: 3.008, interval_steps_per_second: 0.94, epoch: 0.1462[0m
[32m[2022-09-01 12:34:05,415] [    INFO][0m - loss: 0.54121957, learning_rate: 2.9911793531525647e-05, global_step: 1800, interval_runtime: 6.933, interval_samples_per_second: 4.616, interval_steps_per_second: 1.442, epoch: 0.147[0m
[32m[2022-09-01 12:34:05,416] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 12:34:05,416] [    INFO][0m -   Num examples = 12241[0m
[32m[2022-09-01 12:34:05,416] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 12:34:05,416] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 12:34:05,416] [    INFO][0m -   Total prediction steps = 383[0m
[32m[2022-09-01 12:35:50,199] [    INFO][0m - eval_loss: 0.532271146774292, eval_accuracy: 0.8009966506004411, eval_runtime: 104.7817, eval_samples_per_second: 116.824, eval_steps_per_second: 3.655, epoch: 0.147[0m
[32m[2022-09-01 12:35:50,199] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1800[0m
[32m[2022-09-01 12:35:50,199] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 12:35:57,860] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1800/tokenizer_config.json[0m
[32m[2022-09-01 12:35:57,861] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1800/special_tokens_map.json[0m
[32m[2022-09-01 12:36:18,818] [    INFO][0m - loss: 0.62455993, learning_rate: 2.9911303495589678e-05, global_step: 1810, interval_runtime: 133.4021, interval_samples_per_second: 0.24, interval_steps_per_second: 0.075, epoch: 0.1478[0m
[32m[2022-09-01 12:36:25,701] [    INFO][0m - loss: 0.50494075, learning_rate: 2.991081345965371e-05, global_step: 1820, interval_runtime: 6.8836, interval_samples_per_second: 4.649, interval_steps_per_second: 1.453, epoch: 0.1486[0m
[32m[2022-09-01 12:36:32,599] [    INFO][0m - loss: 0.66562839, learning_rate: 2.9910323423717742e-05, global_step: 1830, interval_runtime: 6.8973, interval_samples_per_second: 4.639, interval_steps_per_second: 1.45, epoch: 0.1495[0m
[32m[2022-09-01 12:36:39,469] [    INFO][0m - loss: 0.52733765, learning_rate: 2.9909833387781773e-05, global_step: 1840, interval_runtime: 6.8701, interval_samples_per_second: 4.658, interval_steps_per_second: 1.456, epoch: 0.1503[0m
[32m[2022-09-01 12:36:46,363] [    INFO][0m - loss: 0.62589207, learning_rate: 2.9909343351845803e-05, global_step: 1850, interval_runtime: 6.8945, interval_samples_per_second: 4.641, interval_steps_per_second: 1.45, epoch: 0.1511[0m
[32m[2022-09-01 12:36:53,267] [    INFO][0m - loss: 0.54319444, learning_rate: 2.9908853315909834e-05, global_step: 1860, interval_runtime: 6.9038, interval_samples_per_second: 4.635, interval_steps_per_second: 1.448, epoch: 0.1519[0m
[32m[2022-09-01 12:37:00,132] [    INFO][0m - loss: 0.62475567, learning_rate: 2.9908363279973868e-05, global_step: 1870, interval_runtime: 6.8654, interval_samples_per_second: 4.661, interval_steps_per_second: 1.457, epoch: 0.1527[0m
[32m[2022-09-01 12:37:06,994] [    INFO][0m - loss: 0.57669373, learning_rate: 2.9907873244037898e-05, global_step: 1880, interval_runtime: 6.8618, interval_samples_per_second: 4.664, interval_steps_per_second: 1.457, epoch: 0.1535[0m
[32m[2022-09-01 12:37:13,878] [    INFO][0m - loss: 0.5616991, learning_rate: 2.990738320810193e-05, global_step: 1890, interval_runtime: 6.884, interval_samples_per_second: 4.648, interval_steps_per_second: 1.453, epoch: 0.1544[0m
[32m[2022-09-01 12:37:20,773] [    INFO][0m - loss: 0.52945647, learning_rate: 2.990689317216596e-05, global_step: 1900, interval_runtime: 6.8946, interval_samples_per_second: 4.641, interval_steps_per_second: 1.45, epoch: 0.1552[0m
[32m[2022-09-01 12:37:20,773] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 12:37:20,774] [    INFO][0m -   Num examples = 12241[0m
[32m[2022-09-01 12:37:20,774] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 12:37:20,774] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 12:37:20,774] [    INFO][0m -   Total prediction steps = 383[0m
[32m[2022-09-01 12:39:05,458] [    INFO][0m - eval_loss: 0.5471518635749817, eval_accuracy: 0.7995261825014296, eval_runtime: 104.6834, eval_samples_per_second: 116.933, eval_steps_per_second: 3.659, epoch: 0.1552[0m
[32m[2022-09-01 12:39:05,458] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1900[0m
[32m[2022-09-01 12:39:05,459] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 12:39:08,797] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1900/tokenizer_config.json[0m
[32m[2022-09-01 12:39:08,797] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1900/special_tokens_map.json[0m
[32m[2022-09-01 12:39:21,121] [    INFO][0m - loss: 0.65217781, learning_rate: 2.990640313622999e-05, global_step: 1910, interval_runtime: 120.3483, interval_samples_per_second: 0.266, interval_steps_per_second: 0.083, epoch: 0.156[0m
[32m[2022-09-01 12:39:28,046] [    INFO][0m - loss: 0.58155017, learning_rate: 2.9905913100294024e-05, global_step: 1920, interval_runtime: 6.9247, interval_samples_per_second: 4.621, interval_steps_per_second: 1.444, epoch: 0.1568[0m
[32m[2022-09-01 12:39:34,964] [    INFO][0m - loss: 0.60247378, learning_rate: 2.9905423064358054e-05, global_step: 1930, interval_runtime: 6.9188, interval_samples_per_second: 4.625, interval_steps_per_second: 1.445, epoch: 0.1576[0m
[32m[2022-09-01 12:39:41,890] [    INFO][0m - loss: 0.50823088, learning_rate: 2.9904933028422085e-05, global_step: 1940, interval_runtime: 6.9257, interval_samples_per_second: 4.62, interval_steps_per_second: 1.444, epoch: 0.1584[0m
[32m[2022-09-01 12:39:48,876] [    INFO][0m - loss: 0.56358175, learning_rate: 2.9904442992486115e-05, global_step: 1950, interval_runtime: 6.9857, interval_samples_per_second: 4.581, interval_steps_per_second: 1.431, epoch: 0.1593[0m
[32m[2022-09-01 12:39:55,821] [    INFO][0m - loss: 0.69267583, learning_rate: 2.990395295655015e-05, global_step: 1960, interval_runtime: 6.9454, interval_samples_per_second: 4.607, interval_steps_per_second: 1.44, epoch: 0.1601[0m
[32m[2022-09-01 12:40:02,783] [    INFO][0m - loss: 0.55498934, learning_rate: 2.990346292061418e-05, global_step: 1970, interval_runtime: 6.9612, interval_samples_per_second: 4.597, interval_steps_per_second: 1.437, epoch: 0.1609[0m
[32m[2022-09-01 12:40:09,708] [    INFO][0m - loss: 0.60528002, learning_rate: 2.990297288467821e-05, global_step: 1980, interval_runtime: 6.9248, interval_samples_per_second: 4.621, interval_steps_per_second: 1.444, epoch: 0.1617[0m
[32m[2022-09-01 12:40:16,629] [    INFO][0m - loss: 0.48349509, learning_rate: 2.990248284874224e-05, global_step: 1990, interval_runtime: 6.9217, interval_samples_per_second: 4.623, interval_steps_per_second: 1.445, epoch: 0.1625[0m
[32m[2022-09-01 12:40:23,620] [    INFO][0m - loss: 0.53599477, learning_rate: 2.9901992812806275e-05, global_step: 2000, interval_runtime: 6.9908, interval_samples_per_second: 4.577, interval_steps_per_second: 1.43, epoch: 0.1633[0m
[32m[2022-09-01 12:40:23,621] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 12:40:23,621] [    INFO][0m -   Num examples = 12241[0m
[32m[2022-09-01 12:40:23,621] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 12:40:23,621] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 12:40:23,621] [    INFO][0m -   Total prediction steps = 383[0m
[32m[2022-09-01 12:42:08,536] [    INFO][0m - eval_loss: 0.5078681111335754, eval_accuracy: 0.8017318846499469, eval_runtime: 104.9137, eval_samples_per_second: 116.677, eval_steps_per_second: 3.651, epoch: 0.1633[0m
[32m[2022-09-01 12:42:08,536] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2000[0m
[32m[2022-09-01 12:42:08,536] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 12:42:12,005] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2000/tokenizer_config.json[0m
[32m[2022-09-01 12:42:12,005] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2000/special_tokens_map.json[0m
[32m[2022-09-01 12:42:25,024] [    INFO][0m - loss: 0.50794382, learning_rate: 2.9901502776870305e-05, global_step: 2010, interval_runtime: 121.4038, interval_samples_per_second: 0.264, interval_steps_per_second: 0.082, epoch: 0.1642[0m
[32m[2022-09-01 12:42:31,923] [    INFO][0m - loss: 0.51773396, learning_rate: 2.9901012740934336e-05, global_step: 2020, interval_runtime: 6.8987, interval_samples_per_second: 4.639, interval_steps_per_second: 1.45, epoch: 0.165[0m
[32m[2022-09-01 12:42:38,818] [    INFO][0m - loss: 0.66832094, learning_rate: 2.9900522704998366e-05, global_step: 2030, interval_runtime: 6.8956, interval_samples_per_second: 4.641, interval_steps_per_second: 1.45, epoch: 0.1658[0m
[32m[2022-09-01 12:42:45,700] [    INFO][0m - loss: 0.54362812, learning_rate: 2.9900032669062397e-05, global_step: 2040, interval_runtime: 6.8824, interval_samples_per_second: 4.65, interval_steps_per_second: 1.453, epoch: 0.1666[0m
[32m[2022-09-01 12:42:52,560] [    INFO][0m - loss: 0.56044807, learning_rate: 2.989954263312643e-05, global_step: 2050, interval_runtime: 6.8593, interval_samples_per_second: 4.665, interval_steps_per_second: 1.458, epoch: 0.1674[0m
[32m[2022-09-01 12:42:59,431] [    INFO][0m - loss: 0.58704863, learning_rate: 2.989905259719046e-05, global_step: 2060, interval_runtime: 6.871, interval_samples_per_second: 4.657, interval_steps_per_second: 1.455, epoch: 0.1682[0m
[32m[2022-09-01 12:43:06,340] [    INFO][0m - loss: 0.511869, learning_rate: 2.989856256125449e-05, global_step: 2070, interval_runtime: 6.9094, interval_samples_per_second: 4.631, interval_steps_per_second: 1.447, epoch: 0.1691[0m
[32m[2022-09-01 12:43:13,223] [    INFO][0m - loss: 0.52141623, learning_rate: 2.9898072525318522e-05, global_step: 2080, interval_runtime: 6.8831, interval_samples_per_second: 4.649, interval_steps_per_second: 1.453, epoch: 0.1699[0m
[32m[2022-09-01 12:43:20,127] [    INFO][0m - loss: 0.59384985, learning_rate: 2.9897582489382556e-05, global_step: 2090, interval_runtime: 6.9032, interval_samples_per_second: 4.636, interval_steps_per_second: 1.449, epoch: 0.1707[0m
[32m[2022-09-01 12:43:27,018] [    INFO][0m - loss: 0.61095166, learning_rate: 2.9897092453446586e-05, global_step: 2100, interval_runtime: 6.8917, interval_samples_per_second: 4.643, interval_steps_per_second: 1.451, epoch: 0.1715[0m
[32m[2022-09-01 12:43:27,019] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 12:43:27,019] [    INFO][0m -   Num examples = 12241[0m
[32m[2022-09-01 12:43:27,019] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 12:43:27,019] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 12:43:27,019] [    INFO][0m -   Total prediction steps = 383[0m
[32m[2022-09-01 12:45:11,764] [    INFO][0m - eval_loss: 0.5344372391700745, eval_accuracy: 0.791356915284699, eval_runtime: 104.7443, eval_samples_per_second: 116.866, eval_steps_per_second: 3.657, epoch: 0.1715[0m
[32m[2022-09-01 12:45:11,765] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2100[0m
[32m[2022-09-01 12:45:11,765] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 12:45:15,275] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2100/tokenizer_config.json[0m
[32m[2022-09-01 12:45:15,275] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2100/special_tokens_map.json[0m
[32m[2022-09-01 12:45:27,989] [    INFO][0m - loss: 0.579954, learning_rate: 2.9896602417510617e-05, global_step: 2110, interval_runtime: 120.9704, interval_samples_per_second: 0.265, interval_steps_per_second: 0.083, epoch: 0.1723[0m
[32m[2022-09-01 12:45:34,884] [    INFO][0m - loss: 0.60062556, learning_rate: 2.9896112381574647e-05, global_step: 2120, interval_runtime: 6.8952, interval_samples_per_second: 4.641, interval_steps_per_second: 1.45, epoch: 0.1731[0m
[32m[2022-09-01 12:45:41,823] [    INFO][0m - loss: 0.54553456, learning_rate: 2.989562234563868e-05, global_step: 2130, interval_runtime: 6.9391, interval_samples_per_second: 4.612, interval_steps_per_second: 1.441, epoch: 0.174[0m
[32m[2022-09-01 12:45:48,689] [    INFO][0m - loss: 0.57029858, learning_rate: 2.9895132309702712e-05, global_step: 2140, interval_runtime: 6.8662, interval_samples_per_second: 4.66, interval_steps_per_second: 1.456, epoch: 0.1748[0m
[32m[2022-09-01 12:45:55,590] [    INFO][0m - loss: 0.56574554, learning_rate: 2.9894642273766742e-05, global_step: 2150, interval_runtime: 6.9009, interval_samples_per_second: 4.637, interval_steps_per_second: 1.449, epoch: 0.1756[0m
[32m[2022-09-01 12:46:02,460] [    INFO][0m - loss: 0.62448392, learning_rate: 2.9894152237830773e-05, global_step: 2160, interval_runtime: 6.8696, interval_samples_per_second: 4.658, interval_steps_per_second: 1.456, epoch: 0.1764[0m
[32m[2022-09-01 12:46:09,362] [    INFO][0m - loss: 0.55177021, learning_rate: 2.9893662201894807e-05, global_step: 2170, interval_runtime: 6.9025, interval_samples_per_second: 4.636, interval_steps_per_second: 1.449, epoch: 0.1772[0m
[32m[2022-09-01 12:46:16,274] [    INFO][0m - loss: 0.56179109, learning_rate: 2.9893172165958837e-05, global_step: 2180, interval_runtime: 6.9117, interval_samples_per_second: 4.63, interval_steps_per_second: 1.447, epoch: 0.178[0m
[32m[2022-09-01 12:46:23,167] [    INFO][0m - loss: 0.46733747, learning_rate: 2.9892682130022868e-05, global_step: 2190, interval_runtime: 6.8932, interval_samples_per_second: 4.642, interval_steps_per_second: 1.451, epoch: 0.1789[0m
[32m[2022-09-01 12:46:30,151] [    INFO][0m - loss: 0.55176578, learning_rate: 2.9892192094086898e-05, global_step: 2200, interval_runtime: 6.984, interval_samples_per_second: 4.582, interval_steps_per_second: 1.432, epoch: 0.1797[0m
[32m[2022-09-01 12:46:30,152] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 12:46:30,152] [    INFO][0m -   Num examples = 12241[0m
[32m[2022-09-01 12:46:30,152] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 12:46:30,152] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 12:46:30,152] [    INFO][0m -   Total prediction steps = 383[0m
[32m[2022-09-01 12:48:15,108] [    INFO][0m - eval_loss: 0.5721917152404785, eval_accuracy: 0.784984886855649, eval_runtime: 104.9553, eval_samples_per_second: 116.631, eval_steps_per_second: 3.649, epoch: 0.1797[0m
[32m[2022-09-01 12:48:15,108] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2200[0m
[32m[2022-09-01 12:48:15,109] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 12:48:18,158] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2200/tokenizer_config.json[0m
[32m[2022-09-01 12:48:18,158] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2200/special_tokens_map.json[0m
[32m[2022-09-01 12:48:30,549] [    INFO][0m - loss: 0.59401665, learning_rate: 2.9891702058150932e-05, global_step: 2210, interval_runtime: 120.3979, interval_samples_per_second: 0.266, interval_steps_per_second: 0.083, epoch: 0.1805[0m
[32m[2022-09-01 12:48:37,468] [    INFO][0m - loss: 0.54439955, learning_rate: 2.9891212022214966e-05, global_step: 2220, interval_runtime: 6.919, interval_samples_per_second: 4.625, interval_steps_per_second: 1.445, epoch: 0.1813[0m
[32m[2022-09-01 12:48:44,360] [    INFO][0m - loss: 0.5489706, learning_rate: 2.9890721986278996e-05, global_step: 2230, interval_runtime: 6.8917, interval_samples_per_second: 4.643, interval_steps_per_second: 1.451, epoch: 0.1821[0m
[32m[2022-09-01 12:48:51,275] [    INFO][0m - loss: 0.61823163, learning_rate: 2.9890231950343027e-05, global_step: 2240, interval_runtime: 6.9147, interval_samples_per_second: 4.628, interval_steps_per_second: 1.446, epoch: 0.1829[0m
[32m[2022-09-01 12:48:58,159] [    INFO][0m - loss: 0.53747301, learning_rate: 2.9889741914407057e-05, global_step: 2250, interval_runtime: 6.8845, interval_samples_per_second: 4.648, interval_steps_per_second: 1.453, epoch: 0.1838[0m
[32m[2022-09-01 12:49:05,049] [    INFO][0m - loss: 0.64948468, learning_rate: 2.988925187847109e-05, global_step: 2260, interval_runtime: 6.8898, interval_samples_per_second: 4.645, interval_steps_per_second: 1.451, epoch: 0.1846[0m
[32m[2022-09-01 12:49:11,948] [    INFO][0m - loss: 0.59858341, learning_rate: 2.9888761842535122e-05, global_step: 2270, interval_runtime: 6.8988, interval_samples_per_second: 4.638, interval_steps_per_second: 1.45, epoch: 0.1854[0m
[32m[2022-09-01 12:49:18,827] [    INFO][0m - loss: 0.6382731, learning_rate: 2.9888271806599152e-05, global_step: 2280, interval_runtime: 6.8793, interval_samples_per_second: 4.652, interval_steps_per_second: 1.454, epoch: 0.1862[0m
[32m[2022-09-01 12:49:25,717] [    INFO][0m - loss: 0.63387341, learning_rate: 2.9887781770663183e-05, global_step: 2290, interval_runtime: 6.89, interval_samples_per_second: 4.644, interval_steps_per_second: 1.451, epoch: 0.187[0m
[32m[2022-09-01 12:49:32,615] [    INFO][0m - loss: 0.56075416, learning_rate: 2.9887291734727217e-05, global_step: 2300, interval_runtime: 6.8978, interval_samples_per_second: 4.639, interval_steps_per_second: 1.45, epoch: 0.1878[0m
[32m[2022-09-01 12:49:32,616] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 12:49:32,616] [    INFO][0m -   Num examples = 12241[0m
[32m[2022-09-01 12:49:32,616] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 12:49:32,616] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 12:49:32,616] [    INFO][0m -   Total prediction steps = 383[0m
[32m[2022-09-01 12:51:17,208] [    INFO][0m - eval_loss: 0.5533162355422974, eval_accuracy: 0.7947063148435586, eval_runtime: 104.591, eval_samples_per_second: 117.037, eval_steps_per_second: 3.662, epoch: 0.1878[0m
[32m[2022-09-01 12:51:17,208] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2300[0m
[32m[2022-09-01 12:51:17,209] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 12:51:20,253] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2300/tokenizer_config.json[0m
[32m[2022-09-01 12:51:20,254] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2300/special_tokens_map.json[0m
[32m[2022-09-01 12:51:32,633] [    INFO][0m - loss: 0.61018944, learning_rate: 2.9886801698791247e-05, global_step: 2310, interval_runtime: 120.018, interval_samples_per_second: 0.267, interval_steps_per_second: 0.083, epoch: 0.1887[0m
[32m[2022-09-01 12:51:41,327] [    INFO][0m - loss: 0.54878221, learning_rate: 2.9886311662855278e-05, global_step: 2320, interval_runtime: 6.9423, interval_samples_per_second: 4.609, interval_steps_per_second: 1.44, epoch: 0.1895[0m
[32m[2022-09-01 12:51:48,284] [    INFO][0m - loss: 0.57779193, learning_rate: 2.9885821626919308e-05, global_step: 2330, interval_runtime: 8.7084, interval_samples_per_second: 3.675, interval_steps_per_second: 1.148, epoch: 0.1903[0m
[32m[2022-09-01 12:51:55,216] [    INFO][0m - loss: 0.48828712, learning_rate: 2.988533159098334e-05, global_step: 2340, interval_runtime: 6.9323, interval_samples_per_second: 4.616, interval_steps_per_second: 1.443, epoch: 0.1911[0m
[32m[2022-09-01 12:52:02,149] [    INFO][0m - loss: 0.61877971, learning_rate: 2.9884841555047373e-05, global_step: 2350, interval_runtime: 6.9327, interval_samples_per_second: 4.616, interval_steps_per_second: 1.442, epoch: 0.1919[0m
[32m[2022-09-01 12:52:09,072] [    INFO][0m - loss: 0.6017364, learning_rate: 2.9884351519111403e-05, global_step: 2360, interval_runtime: 6.9223, interval_samples_per_second: 4.623, interval_steps_per_second: 1.445, epoch: 0.1927[0m
[32m[2022-09-01 12:52:16,009] [    INFO][0m - loss: 0.52565536, learning_rate: 2.9883861483175434e-05, global_step: 2370, interval_runtime: 6.9381, interval_samples_per_second: 4.612, interval_steps_per_second: 1.441, epoch: 0.1936[0m
[32m[2022-09-01 12:52:22,841] [    INFO][0m - loss: 0.5308341, learning_rate: 2.9883371447239464e-05, global_step: 2380, interval_runtime: 6.8319, interval_samples_per_second: 4.684, interval_steps_per_second: 1.464, epoch: 0.1944[0m
[32m[2022-09-01 12:52:29,775] [    INFO][0m - loss: 0.60137253, learning_rate: 2.9882881411303498e-05, global_step: 2390, interval_runtime: 6.9342, interval_samples_per_second: 4.615, interval_steps_per_second: 1.442, epoch: 0.1952[0m
[32m[2022-09-01 12:52:36,714] [    INFO][0m - loss: 0.50405917, learning_rate: 2.988239137536753e-05, global_step: 2400, interval_runtime: 6.9384, interval_samples_per_second: 4.612, interval_steps_per_second: 1.441, epoch: 0.196[0m
[32m[2022-09-01 12:52:36,714] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 12:52:36,714] [    INFO][0m -   Num examples = 12241[0m
[32m[2022-09-01 12:52:36,714] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 12:52:36,715] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 12:52:36,715] [    INFO][0m -   Total prediction steps = 383[0m
[32m[2022-09-01 12:54:21,867] [    INFO][0m - eval_loss: 0.5007820129394531, eval_accuracy: 0.8072052936851565, eval_runtime: 105.152, eval_samples_per_second: 116.412, eval_steps_per_second: 3.642, epoch: 0.196[0m
[32m[2022-09-01 12:54:21,868] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2400[0m
[32m[2022-09-01 12:54:21,868] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 12:54:24,918] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2400/tokenizer_config.json[0m
[32m[2022-09-01 12:54:24,918] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2400/special_tokens_map.json[0m
[32m[2022-09-01 12:54:38,100] [    INFO][0m - loss: 0.64953604, learning_rate: 2.988190133943156e-05, global_step: 2410, interval_runtime: 121.3857, interval_samples_per_second: 0.264, interval_steps_per_second: 0.082, epoch: 0.1968[0m
[32m[2022-09-01 12:54:44,986] [    INFO][0m - loss: 0.52889414, learning_rate: 2.988141130349559e-05, global_step: 2420, interval_runtime: 6.8862, interval_samples_per_second: 4.647, interval_steps_per_second: 1.452, epoch: 0.1976[0m
[32m[2022-09-01 12:54:51,890] [    INFO][0m - loss: 0.58225226, learning_rate: 2.9880921267559623e-05, global_step: 2430, interval_runtime: 6.9042, interval_samples_per_second: 4.635, interval_steps_per_second: 1.448, epoch: 0.1985[0m
[32m[2022-09-01 12:54:58,814] [    INFO][0m - loss: 0.50279393, learning_rate: 2.9880431231623654e-05, global_step: 2440, interval_runtime: 6.9242, interval_samples_per_second: 4.621, interval_steps_per_second: 1.444, epoch: 0.1993[0m
[32m[2022-09-01 12:55:05,746] [    INFO][0m - loss: 0.5932076, learning_rate: 2.9879941195687684e-05, global_step: 2450, interval_runtime: 6.9317, interval_samples_per_second: 4.616, interval_steps_per_second: 1.443, epoch: 0.2001[0m
[32m[2022-09-01 12:55:12,687] [    INFO][0m - loss: 0.59891648, learning_rate: 2.9879451159751715e-05, global_step: 2460, interval_runtime: 6.9415, interval_samples_per_second: 4.61, interval_steps_per_second: 1.441, epoch: 0.2009[0m
[32m[2022-09-01 12:55:19,628] [    INFO][0m - loss: 0.6167057, learning_rate: 2.9878961123815745e-05, global_step: 2470, interval_runtime: 6.9403, interval_samples_per_second: 4.611, interval_steps_per_second: 1.441, epoch: 0.2017[0m
[32m[2022-09-01 12:55:26,555] [    INFO][0m - loss: 0.5454638, learning_rate: 2.987847108787978e-05, global_step: 2480, interval_runtime: 6.9274, interval_samples_per_second: 4.619, interval_steps_per_second: 1.444, epoch: 0.2025[0m
[32m[2022-09-01 12:55:33,467] [    INFO][0m - loss: 0.61920662, learning_rate: 2.987798105194381e-05, global_step: 2490, interval_runtime: 6.9123, interval_samples_per_second: 4.629, interval_steps_per_second: 1.447, epoch: 0.2034[0m
[32m[2022-09-01 12:55:40,403] [    INFO][0m - loss: 0.54576368, learning_rate: 2.987749101600784e-05, global_step: 2500, interval_runtime: 6.9354, interval_samples_per_second: 4.614, interval_steps_per_second: 1.442, epoch: 0.2042[0m
[32m[2022-09-01 12:55:40,403] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 12:55:40,403] [    INFO][0m -   Num examples = 12241[0m
[32m[2022-09-01 12:55:40,404] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 12:55:40,404] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 12:55:40,404] [    INFO][0m -   Total prediction steps = 383[0m
[32m[2022-09-01 12:57:25,226] [    INFO][0m - eval_loss: 0.555669367313385, eval_accuracy: 0.7853933502164856, eval_runtime: 104.8223, eval_samples_per_second: 116.779, eval_steps_per_second: 3.654, epoch: 0.2042[0m
[32m[2022-09-01 12:57:25,227] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2500[0m
[32m[2022-09-01 12:57:25,227] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 12:57:28,268] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2500/tokenizer_config.json[0m
[32m[2022-09-01 12:57:28,268] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2500/special_tokens_map.json[0m
[32m[2022-09-01 12:57:42,360] [    INFO][0m - loss: 0.50160151, learning_rate: 2.987700098007187e-05, global_step: 2510, interval_runtime: 121.6836, interval_samples_per_second: 0.263, interval_steps_per_second: 0.082, epoch: 0.205[0m
[32m[2022-09-01 12:57:49,237] [    INFO][0m - loss: 0.54619575, learning_rate: 2.9876510944135905e-05, global_step: 2520, interval_runtime: 7.1509, interval_samples_per_second: 4.475, interval_steps_per_second: 1.398, epoch: 0.2058[0m
[32m[2022-09-01 12:57:58,584] [    INFO][0m - loss: 0.5326828, learning_rate: 2.9876020908199935e-05, global_step: 2530, interval_runtime: 6.9355, interval_samples_per_second: 4.614, interval_steps_per_second: 1.442, epoch: 0.2066[0m
[32m[2022-09-01 12:58:05,507] [    INFO][0m - loss: 0.54663358, learning_rate: 2.9875530872263966e-05, global_step: 2540, interval_runtime: 9.3346, interval_samples_per_second: 3.428, interval_steps_per_second: 1.071, epoch: 0.2074[0m
[32m[2022-09-01 12:58:12,374] [    INFO][0m - loss: 0.57391443, learning_rate: 2.9875040836327996e-05, global_step: 2550, interval_runtime: 6.8667, interval_samples_per_second: 4.66, interval_steps_per_second: 1.456, epoch: 0.2083[0m
[32m[2022-09-01 12:58:19,228] [    INFO][0m - loss: 0.60100656, learning_rate: 2.987455080039203e-05, global_step: 2560, interval_runtime: 6.8537, interval_samples_per_second: 4.669, interval_steps_per_second: 1.459, epoch: 0.2091[0m
[32m[2022-09-01 12:58:26,108] [    INFO][0m - loss: 0.54054999, learning_rate: 2.987406076445606e-05, global_step: 2570, interval_runtime: 6.8796, interval_samples_per_second: 4.651, interval_steps_per_second: 1.454, epoch: 0.2099[0m
[32m[2022-09-01 12:58:33,017] [    INFO][0m - loss: 0.66669536, learning_rate: 2.987357072852009e-05, global_step: 2580, interval_runtime: 6.9089, interval_samples_per_second: 4.632, interval_steps_per_second: 1.447, epoch: 0.2107[0m
[32m[2022-09-01 12:58:39,911] [    INFO][0m - loss: 0.57823954, learning_rate: 2.9873080692584122e-05, global_step: 2590, interval_runtime: 6.895, interval_samples_per_second: 4.641, interval_steps_per_second: 1.45, epoch: 0.2115[0m
[32m[2022-09-01 12:58:46,821] [    INFO][0m - loss: 0.51321125, learning_rate: 2.9872590656648156e-05, global_step: 2600, interval_runtime: 6.9095, interval_samples_per_second: 4.631, interval_steps_per_second: 1.447, epoch: 0.2123[0m
[32m[2022-09-01 12:58:46,821] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 12:58:46,822] [    INFO][0m -   Num examples = 12241[0m
[32m[2022-09-01 12:58:46,822] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 12:58:46,822] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 12:58:46,822] [    INFO][0m -   Total prediction steps = 383[0m
[32m[2022-09-01 13:00:31,416] [    INFO][0m - eval_loss: 0.5502644777297974, eval_accuracy: 0.8042643574871334, eval_runtime: 104.594, eval_samples_per_second: 117.034, eval_steps_per_second: 3.662, epoch: 0.2123[0m
[32m[2022-09-01 13:00:31,417] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2600[0m
[32m[2022-09-01 13:00:31,417] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 13:00:35,020] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2600/tokenizer_config.json[0m
[32m[2022-09-01 13:00:35,021] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2600/special_tokens_map.json[0m
[32m[2022-09-01 13:00:49,002] [    INFO][0m - loss: 0.53839855, learning_rate: 2.9872100620712186e-05, global_step: 2610, interval_runtime: 122.1806, interval_samples_per_second: 0.262, interval_steps_per_second: 0.082, epoch: 0.2132[0m
[32m[2022-09-01 13:00:55,834] [    INFO][0m - loss: 0.62943931, learning_rate: 2.9871610584776217e-05, global_step: 2620, interval_runtime: 6.8328, interval_samples_per_second: 4.683, interval_steps_per_second: 1.464, epoch: 0.214[0m
[32m[2022-09-01 13:01:02,674] [    INFO][0m - loss: 0.58005114, learning_rate: 2.9871120548840247e-05, global_step: 2630, interval_runtime: 6.8393, interval_samples_per_second: 4.679, interval_steps_per_second: 1.462, epoch: 0.2148[0m
[32m[2022-09-01 13:01:09,562] [    INFO][0m - loss: 0.64695926, learning_rate: 2.9870630512904278e-05, global_step: 2640, interval_runtime: 6.8879, interval_samples_per_second: 4.646, interval_steps_per_second: 1.452, epoch: 0.2156[0m
[32m[2022-09-01 13:01:16,435] [    INFO][0m - loss: 0.54050903, learning_rate: 2.987014047696831e-05, global_step: 2650, interval_runtime: 6.873, interval_samples_per_second: 4.656, interval_steps_per_second: 1.455, epoch: 0.2164[0m
[32m[2022-09-01 13:01:23,328] [    INFO][0m - loss: 0.62638745, learning_rate: 2.9869650441032342e-05, global_step: 2660, interval_runtime: 6.8936, interval_samples_per_second: 4.642, interval_steps_per_second: 1.451, epoch: 0.2172[0m
[32m[2022-09-01 13:01:30,150] [    INFO][0m - loss: 0.58512573, learning_rate: 2.9869160405096373e-05, global_step: 2670, interval_runtime: 6.8216, interval_samples_per_second: 4.691, interval_steps_per_second: 1.466, epoch: 0.2181[0m
[32m[2022-09-01 13:01:37,013] [    INFO][0m - loss: 0.58182445, learning_rate: 2.9868670369160403e-05, global_step: 2680, interval_runtime: 6.8632, interval_samples_per_second: 4.663, interval_steps_per_second: 1.457, epoch: 0.2189[0m
[32m[2022-09-01 13:01:43,856] [    INFO][0m - loss: 0.55086784, learning_rate: 2.9868180333224437e-05, global_step: 2690, interval_runtime: 6.8428, interval_samples_per_second: 4.676, interval_steps_per_second: 1.461, epoch: 0.2197[0m
[32m[2022-09-01 13:01:50,700] [    INFO][0m - loss: 0.61877365, learning_rate: 2.9867690297288467e-05, global_step: 2700, interval_runtime: 6.8436, interval_samples_per_second: 4.676, interval_steps_per_second: 1.461, epoch: 0.2205[0m
[32m[2022-09-01 13:01:50,700] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 13:01:50,700] [    INFO][0m -   Num examples = 12241[0m
[32m[2022-09-01 13:01:50,700] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 13:01:50,700] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 13:01:50,701] [    INFO][0m -   Total prediction steps = 383[0m
[32m[2022-09-01 13:03:34,920] [    INFO][0m - eval_loss: 0.5359067320823669, eval_accuracy: 0.8018952699942815, eval_runtime: 104.2185, eval_samples_per_second: 117.455, eval_steps_per_second: 3.675, epoch: 0.2205[0m
[32m[2022-09-01 13:03:34,920] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2700[0m
[32m[2022-09-01 13:03:34,920] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 13:03:39,016] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2700/tokenizer_config.json[0m
[32m[2022-09-01 13:03:39,016] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2700/special_tokens_map.json[0m
[32m[2022-09-01 13:03:53,905] [    INFO][0m - loss: 0.57903447, learning_rate: 2.98672002613525e-05, global_step: 2710, interval_runtime: 123.205, interval_samples_per_second: 0.26, interval_steps_per_second: 0.081, epoch: 0.2213[0m
[32m[2022-09-01 13:04:00,752] [    INFO][0m - loss: 0.57701511, learning_rate: 2.9866710225416532e-05, global_step: 2720, interval_runtime: 6.8478, interval_samples_per_second: 4.673, interval_steps_per_second: 1.46, epoch: 0.2221[0m
[32m[2022-09-01 13:04:07,573] [    INFO][0m - loss: 0.55554852, learning_rate: 2.9866220189480566e-05, global_step: 2730, interval_runtime: 6.8204, interval_samples_per_second: 4.692, interval_steps_per_second: 1.466, epoch: 0.223[0m
[32m[2022-09-01 13:04:14,455] [    INFO][0m - loss: 0.57248821, learning_rate: 2.9865730153544596e-05, global_step: 2740, interval_runtime: 6.8823, interval_samples_per_second: 4.65, interval_steps_per_second: 1.453, epoch: 0.2238[0m
[32m[2022-09-01 13:04:21,356] [    INFO][0m - loss: 0.61892314, learning_rate: 2.9865240117608627e-05, global_step: 2750, interval_runtime: 6.9001, interval_samples_per_second: 4.638, interval_steps_per_second: 1.449, epoch: 0.2246[0m
[32m[2022-09-01 13:04:28,347] [    INFO][0m - loss: 0.46075964, learning_rate: 2.9864750081672657e-05, global_step: 2760, interval_runtime: 6.9916, interval_samples_per_second: 4.577, interval_steps_per_second: 1.43, epoch: 0.2254[0m
[32m[2022-09-01 13:04:35,148] [    INFO][0m - loss: 0.59685874, learning_rate: 2.9864260045736688e-05, global_step: 2770, interval_runtime: 6.8005, interval_samples_per_second: 4.706, interval_steps_per_second: 1.47, epoch: 0.2262[0m
[32m[2022-09-01 13:04:42,000] [    INFO][0m - loss: 0.57070675, learning_rate: 2.986377000980072e-05, global_step: 2780, interval_runtime: 6.8527, interval_samples_per_second: 4.67, interval_steps_per_second: 1.459, epoch: 0.227[0m
[32m[2022-09-01 13:04:48,863] [    INFO][0m - loss: 0.49092097, learning_rate: 2.9863279973864752e-05, global_step: 2790, interval_runtime: 6.863, interval_samples_per_second: 4.663, interval_steps_per_second: 1.457, epoch: 0.2279[0m
[32m[2022-09-01 13:04:55,747] [    INFO][0m - loss: 0.56453438, learning_rate: 2.9862789937928783e-05, global_step: 2800, interval_runtime: 6.8836, interval_samples_per_second: 4.649, interval_steps_per_second: 1.453, epoch: 0.2287[0m
[32m[2022-09-01 13:04:55,747] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 13:04:55,748] [    INFO][0m -   Num examples = 12241[0m
[32m[2022-09-01 13:04:55,748] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 13:04:55,748] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 13:04:55,748] [    INFO][0m -   Total prediction steps = 383[0m
[32m[2022-09-01 13:06:39,919] [    INFO][0m - eval_loss: 0.51372891664505, eval_accuracy: 0.8117800833265256, eval_runtime: 104.1707, eval_samples_per_second: 117.509, eval_steps_per_second: 3.677, epoch: 0.2287[0m
[32m[2022-09-01 13:06:39,920] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2800[0m
[32m[2022-09-01 13:06:39,920] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 13:06:43,801] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2800/tokenizer_config.json[0m
[32m[2022-09-01 13:06:43,801] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2800/special_tokens_map.json[0m
[32m[2022-09-01 13:06:57,953] [    INFO][0m - loss: 0.57301731, learning_rate: 2.9862299901992813e-05, global_step: 2810, interval_runtime: 122.2059, interval_samples_per_second: 0.262, interval_steps_per_second: 0.082, epoch: 0.2295[0m
[32m[2022-09-01 13:07:05,818] [    INFO][0m - loss: 0.61696706, learning_rate: 2.9861809866056847e-05, global_step: 2820, interval_runtime: 6.818, interval_samples_per_second: 4.693, interval_steps_per_second: 1.467, epoch: 0.2303[0m
[32m[2022-09-01 13:07:12,715] [    INFO][0m - loss: 0.62122035, learning_rate: 2.9861319830120878e-05, global_step: 2830, interval_runtime: 7.9439, interval_samples_per_second: 4.028, interval_steps_per_second: 1.259, epoch: 0.2311[0m
[32m[2022-09-01 13:07:19,599] [    INFO][0m - loss: 0.60157232, learning_rate: 2.9860829794184908e-05, global_step: 2840, interval_runtime: 6.8842, interval_samples_per_second: 4.648, interval_steps_per_second: 1.453, epoch: 0.232[0m
[32m[2022-09-01 13:07:26,438] [    INFO][0m - loss: 0.55164218, learning_rate: 2.986033975824894e-05, global_step: 2850, interval_runtime: 6.8388, interval_samples_per_second: 4.679, interval_steps_per_second: 1.462, epoch: 0.2328[0m
[32m[2022-09-01 13:07:33,328] [    INFO][0m - loss: 0.49083023, learning_rate: 2.9859849722312972e-05, global_step: 2860, interval_runtime: 6.8887, interval_samples_per_second: 4.645, interval_steps_per_second: 1.452, epoch: 0.2336[0m
[32m[2022-09-01 13:07:40,207] [    INFO][0m - loss: 0.65427084, learning_rate: 2.9859359686377003e-05, global_step: 2870, interval_runtime: 6.8806, interval_samples_per_second: 4.651, interval_steps_per_second: 1.453, epoch: 0.2344[0m
[32m[2022-09-01 13:07:47,100] [    INFO][0m - loss: 0.51596837, learning_rate: 2.9858869650441033e-05, global_step: 2880, interval_runtime: 6.8925, interval_samples_per_second: 4.643, interval_steps_per_second: 1.451, epoch: 0.2352[0m
[32m[2022-09-01 13:07:53,982] [    INFO][0m - loss: 0.58789282, learning_rate: 2.9858379614505064e-05, global_step: 2890, interval_runtime: 6.8819, interval_samples_per_second: 4.65, interval_steps_per_second: 1.453, epoch: 0.236[0m
[32m[2022-09-01 13:08:00,872] [    INFO][0m - loss: 0.52865863, learning_rate: 2.9857889578569094e-05, global_step: 2900, interval_runtime: 6.891, interval_samples_per_second: 4.644, interval_steps_per_second: 1.451, epoch: 0.2369[0m
[32m[2022-09-01 13:08:00,873] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 13:08:00,873] [    INFO][0m -   Num examples = 12241[0m
[32m[2022-09-01 13:08:00,873] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 13:08:00,873] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 13:08:00,873] [    INFO][0m -   Total prediction steps = 383[0m
[32m[2022-09-01 13:09:45,010] [    INFO][0m - eval_loss: 0.5028645992279053, eval_accuracy: 0.8053263622253084, eval_runtime: 104.1366, eval_samples_per_second: 117.548, eval_steps_per_second: 3.678, epoch: 0.2369[0m
[32m[2022-09-01 13:09:45,011] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2900[0m
[32m[2022-09-01 13:09:45,011] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 13:09:48,758] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2900/tokenizer_config.json[0m
[32m[2022-09-01 13:09:48,759] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2900/special_tokens_map.json[0m
[32m[2022-09-01 13:10:03,276] [    INFO][0m - loss: 0.57731638, learning_rate: 2.985739954263313e-05, global_step: 2910, interval_runtime: 122.4038, interval_samples_per_second: 0.261, interval_steps_per_second: 0.082, epoch: 0.2377[0m
[32m[2022-09-01 13:10:10,120] [    INFO][0m - loss: 0.57038493, learning_rate: 2.985690950669716e-05, global_step: 2920, interval_runtime: 6.844, interval_samples_per_second: 4.676, interval_steps_per_second: 1.461, epoch: 0.2385[0m
[32m[2022-09-01 13:10:16,959] [    INFO][0m - loss: 0.53585563, learning_rate: 2.985641947076119e-05, global_step: 2930, interval_runtime: 6.8386, interval_samples_per_second: 4.679, interval_steps_per_second: 1.462, epoch: 0.2393[0m
[32m[2022-09-01 13:10:23,872] [    INFO][0m - loss: 0.54074135, learning_rate: 2.985592943482522e-05, global_step: 2940, interval_runtime: 6.9131, interval_samples_per_second: 4.629, interval_steps_per_second: 1.447, epoch: 0.2401[0m
[32m[2022-09-01 13:10:30,692] [    INFO][0m - loss: 0.59466486, learning_rate: 2.9855439398889254e-05, global_step: 2950, interval_runtime: 6.8204, interval_samples_per_second: 4.692, interval_steps_per_second: 1.466, epoch: 0.2409[0m
[32m[2022-09-01 13:10:37,549] [    INFO][0m - loss: 0.55620041, learning_rate: 2.9854949362953284e-05, global_step: 2960, interval_runtime: 6.8567, interval_samples_per_second: 4.667, interval_steps_per_second: 1.458, epoch: 0.2418[0m
[32m[2022-09-01 13:10:44,411] [    INFO][0m - loss: 0.59681096, learning_rate: 2.9854459327017315e-05, global_step: 2970, interval_runtime: 6.8624, interval_samples_per_second: 4.663, interval_steps_per_second: 1.457, epoch: 0.2426[0m
[32m[2022-09-01 13:10:51,234] [    INFO][0m - loss: 0.62565107, learning_rate: 2.9853969291081345e-05, global_step: 2980, interval_runtime: 6.8226, interval_samples_per_second: 4.69, interval_steps_per_second: 1.466, epoch: 0.2434[0m
[32m[2022-09-01 13:10:58,082] [    INFO][0m - loss: 0.57268019, learning_rate: 2.985347925514538e-05, global_step: 2990, interval_runtime: 6.8473, interval_samples_per_second: 4.673, interval_steps_per_second: 1.46, epoch: 0.2442[0m
[32m[2022-09-01 13:11:04,951] [    INFO][0m - loss: 0.57548141, learning_rate: 2.985298921920941e-05, global_step: 3000, interval_runtime: 6.8691, interval_samples_per_second: 4.659, interval_steps_per_second: 1.456, epoch: 0.245[0m
[32m[2022-09-01 13:11:04,951] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 13:11:04,951] [    INFO][0m -   Num examples = 12241[0m
[32m[2022-09-01 13:11:04,951] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 13:11:04,951] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 13:11:04,951] [    INFO][0m -   Total prediction steps = 383[0m
[32m[2022-09-01 13:12:49,258] [    INFO][0m - eval_loss: 0.4871939420700073, eval_accuracy: 0.8121068540151949, eval_runtime: 104.3059, eval_samples_per_second: 117.357, eval_steps_per_second: 3.672, epoch: 0.245[0m
[32m[2022-09-01 13:12:49,259] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-3000[0m
[32m[2022-09-01 13:12:49,259] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 13:12:53,229] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-3000/tokenizer_config.json[0m
[32m[2022-09-01 13:12:53,229] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-3000/special_tokens_map.json[0m
[32m[2022-09-01 13:13:07,804] [    INFO][0m - loss: 0.48367724, learning_rate: 2.985249918327344e-05, global_step: 3010, interval_runtime: 122.8535, interval_samples_per_second: 0.26, interval_steps_per_second: 0.081, epoch: 0.2458[0m
[32m[2022-09-01 13:13:14,675] [    INFO][0m - loss: 0.54544811, learning_rate: 2.985200914733747e-05, global_step: 3020, interval_runtime: 6.8704, interval_samples_per_second: 4.658, interval_steps_per_second: 1.456, epoch: 0.2467[0m
[32m[2022-09-01 13:13:21,537] [    INFO][0m - loss: 0.54049344, learning_rate: 2.9851519111401505e-05, global_step: 3030, interval_runtime: 6.8624, interval_samples_per_second: 4.663, interval_steps_per_second: 1.457, epoch: 0.2475[0m
[32m[2022-09-01 13:13:28,368] [    INFO][0m - loss: 0.61911774, learning_rate: 2.9851029075465535e-05, global_step: 3040, interval_runtime: 6.831, interval_samples_per_second: 4.685, interval_steps_per_second: 1.464, epoch: 0.2483[0m
[32m[2022-09-01 13:13:35,193] [    INFO][0m - loss: 0.58427472, learning_rate: 2.9850539039529566e-05, global_step: 3050, interval_runtime: 6.8249, interval_samples_per_second: 4.689, interval_steps_per_second: 1.465, epoch: 0.2491[0m
[32m[2022-09-01 13:13:42,059] [    INFO][0m - loss: 0.53445492, learning_rate: 2.9850049003593596e-05, global_step: 3060, interval_runtime: 6.866, interval_samples_per_second: 4.661, interval_steps_per_second: 1.456, epoch: 0.2499[0m
[32m[2022-09-01 13:13:48,895] [    INFO][0m - loss: 0.5872591, learning_rate: 2.9849558967657627e-05, global_step: 3070, interval_runtime: 6.8361, interval_samples_per_second: 4.681, interval_steps_per_second: 1.463, epoch: 0.2507[0m
[32m[2022-09-01 13:13:55,770] [    INFO][0m - loss: 0.56207223, learning_rate: 2.984906893172166e-05, global_step: 3080, interval_runtime: 6.875, interval_samples_per_second: 4.655, interval_steps_per_second: 1.455, epoch: 0.2516[0m
[32m[2022-09-01 13:14:02,665] [    INFO][0m - loss: 0.55662746, learning_rate: 2.984857889578569e-05, global_step: 3090, interval_runtime: 6.8946, interval_samples_per_second: 4.641, interval_steps_per_second: 1.45, epoch: 0.2524[0m
[32m[2022-09-01 13:14:09,540] [    INFO][0m - loss: 0.50961752, learning_rate: 2.984808885984972e-05, global_step: 3100, interval_runtime: 6.8757, interval_samples_per_second: 4.654, interval_steps_per_second: 1.454, epoch: 0.2532[0m
[32m[2022-09-01 13:14:09,541] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 13:14:09,541] [    INFO][0m -   Num examples = 12241[0m
[32m[2022-09-01 13:14:09,541] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 13:14:09,541] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 13:14:09,541] [    INFO][0m -   Total prediction steps = 383[0m
[32m[2022-09-01 13:15:53,776] [    INFO][0m - eval_loss: 0.5209737420082092, eval_accuracy: 0.8103096152275141, eval_runtime: 104.2345, eval_samples_per_second: 117.437, eval_steps_per_second: 3.674, epoch: 0.2532[0m
[32m[2022-09-01 13:15:53,777] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-3100[0m
[32m[2022-09-01 13:15:53,777] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 13:15:57,387] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-3100/tokenizer_config.json[0m
[32m[2022-09-01 13:15:57,387] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-3100/special_tokens_map.json[0m
[32m[2022-09-01 13:16:11,749] [    INFO][0m - loss: 0.61953745, learning_rate: 2.9847598823913752e-05, global_step: 3110, interval_runtime: 122.2087, interval_samples_per_second: 0.262, interval_steps_per_second: 0.082, epoch: 0.254[0m
[32m[2022-09-01 13:16:18,628] [    INFO][0m - loss: 0.58367991, learning_rate: 2.9847108787977786e-05, global_step: 3120, interval_runtime: 6.8785, interval_samples_per_second: 4.652, interval_steps_per_second: 1.454, epoch: 0.2548[0m
[32m[2022-09-01 13:16:25,516] [    INFO][0m - loss: 0.52270517, learning_rate: 2.9846618752041816e-05, global_step: 3130, interval_runtime: 6.8883, interval_samples_per_second: 4.646, interval_steps_per_second: 1.452, epoch: 0.2556[0m
[32m[2022-09-01 13:16:32,433] [    INFO][0m - loss: 0.53288331, learning_rate: 2.9846128716105847e-05, global_step: 3140, interval_runtime: 6.9174, interval_samples_per_second: 4.626, interval_steps_per_second: 1.446, epoch: 0.2565[0m
[32m[2022-09-01 13:16:39,330] [    INFO][0m - loss: 0.67117229, learning_rate: 2.9845638680169877e-05, global_step: 3150, interval_runtime: 6.8969, interval_samples_per_second: 4.64, interval_steps_per_second: 1.45, epoch: 0.2573[0m
[32m[2022-09-01 13:16:46,244] [    INFO][0m - loss: 0.60278454, learning_rate: 2.984514864423391e-05, global_step: 3160, interval_runtime: 6.9139, interval_samples_per_second: 4.628, interval_steps_per_second: 1.446, epoch: 0.2581[0m
[32m[2022-09-01 13:16:53,108] [    INFO][0m - loss: 0.5265862, learning_rate: 2.9844658608297942e-05, global_step: 3170, interval_runtime: 6.8639, interval_samples_per_second: 4.662, interval_steps_per_second: 1.457, epoch: 0.2589[0m
[32m[2022-09-01 13:16:59,932] [    INFO][0m - loss: 0.49938726, learning_rate: 2.9844168572361972e-05, global_step: 3180, interval_runtime: 6.8247, interval_samples_per_second: 4.689, interval_steps_per_second: 1.465, epoch: 0.2597[0m
[32m[2022-09-01 13:17:06,790] [    INFO][0m - loss: 0.52171655, learning_rate: 2.9843678536426006e-05, global_step: 3190, interval_runtime: 6.8573, interval_samples_per_second: 4.667, interval_steps_per_second: 1.458, epoch: 0.2605[0m
[32m[2022-09-01 13:17:13,622] [    INFO][0m - loss: 0.55201058, learning_rate: 2.9843188500490037e-05, global_step: 3200, interval_runtime: 6.832, interval_samples_per_second: 4.684, interval_steps_per_second: 1.464, epoch: 0.2614[0m
[32m[2022-09-01 13:17:13,622] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 13:17:13,623] [    INFO][0m -   Num examples = 12241[0m
[32m[2022-09-01 13:17:13,623] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 13:17:13,623] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 13:17:13,623] [    INFO][0m -   Total prediction steps = 383[0m
[32m[2022-09-01 13:18:57,607] [    INFO][0m - eval_loss: 0.5080932974815369, eval_accuracy: 0.8072052936851565, eval_runtime: 103.9833, eval_samples_per_second: 117.721, eval_steps_per_second: 3.683, epoch: 0.2614[0m
[32m[2022-09-01 13:18:57,608] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-3200[0m
[32m[2022-09-01 13:18:57,608] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 13:19:01,072] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-3200/tokenizer_config.json[0m
[32m[2022-09-01 13:19:01,072] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-3200/special_tokens_map.json[0m
[32m[2022-09-01 13:19:14,488] [    INFO][0m - loss: 0.56997657, learning_rate: 2.984269846455407e-05, global_step: 3210, interval_runtime: 120.866, interval_samples_per_second: 0.265, interval_steps_per_second: 0.083, epoch: 0.2622[0m
[32m[2022-09-01 13:19:21,330] [    INFO][0m - loss: 0.53819323, learning_rate: 2.98422084286181e-05, global_step: 3220, interval_runtime: 6.842, interval_samples_per_second: 4.677, interval_steps_per_second: 1.462, epoch: 0.263[0m
[32m[2022-09-01 13:19:28,224] [    INFO][0m - loss: 0.58850689, learning_rate: 2.984171839268213e-05, global_step: 3230, interval_runtime: 6.8936, interval_samples_per_second: 4.642, interval_steps_per_second: 1.451, epoch: 0.2638[0m
[32m[2022-09-01 13:19:36,553] [    INFO][0m - loss: 0.54682636, learning_rate: 2.9841228356746162e-05, global_step: 3240, interval_runtime: 8.3286, interval_samples_per_second: 3.842, interval_steps_per_second: 1.201, epoch: 0.2646[0m
[32m[2022-09-01 13:19:43,416] [    INFO][0m - loss: 0.48307815, learning_rate: 2.9840738320810196e-05, global_step: 3250, interval_runtime: 6.8628, interval_samples_per_second: 4.663, interval_steps_per_second: 1.457, epoch: 0.2654[0m
[32m[2022-09-01 13:19:50,313] [    INFO][0m - loss: 0.52542005, learning_rate: 2.9840248284874227e-05, global_step: 3260, interval_runtime: 6.8981, interval_samples_per_second: 4.639, interval_steps_per_second: 1.45, epoch: 0.2663[0m
[32m[2022-09-01 13:19:57,171] [    INFO][0m - loss: 0.57179904, learning_rate: 2.9839758248938257e-05, global_step: 3270, interval_runtime: 6.8582, interval_samples_per_second: 4.666, interval_steps_per_second: 1.458, epoch: 0.2671[0m
[32m[2022-09-01 13:20:04,047] [    INFO][0m - loss: 0.51952505, learning_rate: 2.9839268213002288e-05, global_step: 3280, interval_runtime: 6.8758, interval_samples_per_second: 4.654, interval_steps_per_second: 1.454, epoch: 0.2679[0m
[32m[2022-09-01 13:20:10,886] [    INFO][0m - loss: 0.56449089, learning_rate: 2.983877817706632e-05, global_step: 3290, interval_runtime: 6.8389, interval_samples_per_second: 4.679, interval_steps_per_second: 1.462, epoch: 0.2687[0m
[32m[2022-09-01 13:20:17,772] [    INFO][0m - loss: 0.56993146, learning_rate: 2.9838288141130352e-05, global_step: 3300, interval_runtime: 6.8853, interval_samples_per_second: 4.648, interval_steps_per_second: 1.452, epoch: 0.2695[0m
[32m[2022-09-01 13:20:17,772] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 13:20:17,772] [    INFO][0m -   Num examples = 12241[0m
[32m[2022-09-01 13:20:17,773] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 13:20:17,773] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 13:20:17,773] [    INFO][0m -   Total prediction steps = 383[0m
[32m[2022-09-01 13:22:02,398] [    INFO][0m - eval_loss: 0.5117071866989136, eval_accuracy: 0.8012417286169431, eval_runtime: 104.6244, eval_samples_per_second: 116.999, eval_steps_per_second: 3.661, epoch: 0.2695[0m
[32m[2022-09-01 13:22:02,398] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-3300[0m
[32m[2022-09-01 13:22:02,398] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 13:22:05,962] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-3300/tokenizer_config.json[0m
[32m[2022-09-01 13:22:05,962] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-3300/special_tokens_map.json[0m
[32m[2022-09-01 13:22:19,314] [    INFO][0m - loss: 0.57961302, learning_rate: 2.9837798105194382e-05, global_step: 3310, interval_runtime: 121.5416, interval_samples_per_second: 0.263, interval_steps_per_second: 0.082, epoch: 0.2703[0m
[32m[2022-09-01 13:22:27,448] [    INFO][0m - loss: 0.58423324, learning_rate: 2.9837308069258413e-05, global_step: 3320, interval_runtime: 6.8413, interval_samples_per_second: 4.677, interval_steps_per_second: 1.462, epoch: 0.2712[0m
[32m[2022-09-01 13:22:34,290] [    INFO][0m - loss: 0.51584988, learning_rate: 2.9836818033322447e-05, global_step: 3330, interval_runtime: 8.1354, interval_samples_per_second: 3.933, interval_steps_per_second: 1.229, epoch: 0.272[0m
[32m[2022-09-01 13:22:41,151] [    INFO][0m - loss: 0.53621082, learning_rate: 2.9836327997386477e-05, global_step: 3340, interval_runtime: 6.861, interval_samples_per_second: 4.664, interval_steps_per_second: 1.458, epoch: 0.2728[0m
[32m[2022-09-01 13:22:48,007] [    INFO][0m - loss: 0.63678055, learning_rate: 2.9835837961450508e-05, global_step: 3350, interval_runtime: 6.8552, interval_samples_per_second: 4.668, interval_steps_per_second: 1.459, epoch: 0.2736[0m
[32m[2022-09-01 13:22:54,847] [    INFO][0m - loss: 0.6223897, learning_rate: 2.983534792551454e-05, global_step: 3360, interval_runtime: 6.8407, interval_samples_per_second: 4.678, interval_steps_per_second: 1.462, epoch: 0.2744[0m
[32m[2022-09-01 13:23:01,676] [    INFO][0m - loss: 0.6001914, learning_rate: 2.983485788957857e-05, global_step: 3370, interval_runtime: 6.8288, interval_samples_per_second: 4.686, interval_steps_per_second: 1.464, epoch: 0.2752[0m
[32m[2022-09-01 13:23:08,535] [    INFO][0m - loss: 0.51070938, learning_rate: 2.9834367853642603e-05, global_step: 3380, interval_runtime: 6.8593, interval_samples_per_second: 4.665, interval_steps_per_second: 1.458, epoch: 0.2761[0m
[32m[2022-09-01 13:23:15,366] [    INFO][0m - loss: 0.60791626, learning_rate: 2.9833877817706633e-05, global_step: 3390, interval_runtime: 6.8306, interval_samples_per_second: 4.685, interval_steps_per_second: 1.464, epoch: 0.2769[0m
[32m[2022-09-01 13:23:22,259] [    INFO][0m - loss: 0.53955102, learning_rate: 2.9833387781770664e-05, global_step: 3400, interval_runtime: 6.8933, interval_samples_per_second: 4.642, interval_steps_per_second: 1.451, epoch: 0.2777[0m
[32m[2022-09-01 13:23:22,260] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 13:23:22,260] [    INFO][0m -   Num examples = 12241[0m
[32m[2022-09-01 13:23:22,260] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 13:23:22,260] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 13:23:22,260] [    INFO][0m -   Total prediction steps = 383[0m
[32m[2022-09-01 13:25:06,604] [    INFO][0m - eval_loss: 0.5124077796936035, eval_accuracy: 0.8049995915366391, eval_runtime: 104.344, eval_samples_per_second: 117.314, eval_steps_per_second: 3.671, epoch: 0.2777[0m
[32m[2022-09-01 13:25:06,605] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-3400[0m
[32m[2022-09-01 13:25:06,605] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 13:25:10,212] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-3400/tokenizer_config.json[0m
[32m[2022-09-01 13:25:10,213] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-3400/special_tokens_map.json[0m
[32m[2022-09-01 13:25:20,533] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-01 13:25:20,534] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-3000 (score: 0.8121068540151949).[0m
[32m[2022-09-01 13:25:22,586] [    INFO][0m - train_runtime: 6505.8115, train_samples_per_second: 3011.023, train_steps_per_second: 94.1, train_loss: 0.6500757350641139, epoch: 0.2777[0m
[32m[2022-09-01 13:25:22,588] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-09-01 13:25:22,589] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 13:25:30,328] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-09-01 13:25:30,329] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-09-01 13:25:30,331] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-01 13:25:30,332] [    INFO][0m -   epoch                    =     0.2777[0m
[32m[2022-09-01 13:25:30,332] [    INFO][0m -   train_loss               =     0.6501[0m
[32m[2022-09-01 13:25:30,332] [    INFO][0m -   train_runtime            = 1:48:25.81[0m
[32m[2022-09-01 13:25:30,332] [    INFO][0m -   train_samples_per_second =   3011.023[0m
[32m[2022-09-01 13:25:30,332] [    INFO][0m -   train_steps_per_second   =       94.1[0m
[32m[2022-09-01 13:25:30,347] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-01 13:25:30,348] [    INFO][0m -   Num examples = 13880[0m
[32m[2022-09-01 13:25:30,348] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 13:25:30,348] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 13:25:30,348] [    INFO][0m -   Total prediction steps = 434[0m
[32m[2022-09-01 13:27:28,559] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-01 13:27:28,559] [    INFO][0m -   test_runtime            = 0:01:58.21[0m
[32m[2022-09-01 13:27:28,559] [    INFO][0m -   test_samples_per_second =    117.417[0m
[32m[2022-09-01 13:27:28,559] [    INFO][0m -   test_steps_per_second   =      3.671[0m
[32m[2022-09-01 13:27:28,560] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-01 13:27:28,560] [    INFO][0m -   Num examples = 13880[0m
[32m[2022-09-01 13:27:28,560] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 13:27:28,560] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 13:27:28,560] [    INFO][0m -   Total prediction steps = 434[0m
[32m[2022-09-01 13:29:42,501] [    INFO][0m - Predictions for cmnlif saved to ./fewclue_submit_examples.[0m
[]
run.sh: line 78: --freeze_plm: command not found
