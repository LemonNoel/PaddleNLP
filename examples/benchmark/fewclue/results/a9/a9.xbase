 
==========
eprstmt
==========
 
[33m[2022-08-31 22:19:05,792] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-31 22:19:05,792] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-31 22:19:05,793] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 22:19:05,793] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-31 22:19:05,793] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 22:19:05,793] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-31 22:19:05,793] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-31 22:19:05,793] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-08-31 22:19:05,793] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-31 22:19:05,793] [    INFO][0m - model_name_or_path            :ernie-3.0-xbase-zh[0m
[32m[2022-08-31 22:19:05,793] [    INFO][0m - [0m
[32m[2022-08-31 22:19:05,793] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 22:19:05,793] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-31 22:19:05,794] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 22:19:05,794] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-31 22:19:05,794] [    INFO][0m - prompt                        :{'text':'text_a'}{'hard':'Ëøô‰∏™Âè•ËØùË°®Á§∫Êàë'}{'mask'}{'hard':'ÂñúÊ¨¢Ëøô‰∏™‰∏úË•ø'}[0m
[32m[2022-08-31 22:19:05,794] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-31 22:19:05,794] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-31 22:19:05,794] [    INFO][0m - task_name                     :eprstmt[0m
[32m[2022-08-31 22:19:05,794] [    INFO][0m - [0m
[32m[2022-08-31 22:19:05,794] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/ernie_3.0_xbase_zh.pdparams[0m
W0831 22:19:05.796013 27464 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0831 22:19:05.800099 27464 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-31 22:19:16,958] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/ernie_3.0_xbase_zh_vocab.txt[0m
[32m[2022-08-31 22:19:16,985] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/tokenizer_config.json[0m
[32m[2022-08-31 22:19:16,985] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/special_tokens_map.json[0m
[32m[2022-08-31 22:19:16,986] [    INFO][0m - Using template: [{'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': 'Ëøô‰∏™Âè•ËØùË°®Á§∫Êàë'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'ÂñúÊ¨¢Ëøô‰∏™‰∏úË•ø'}][0m
[32m[2022-08-31 22:19:17,010] [    INFO][0m - {'Negative': 0, 'Positive': 1}[0m
2022-08-31 22:19:17,011 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-31 22:19:17,112] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 22:19:17,112] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-31 22:19:17,112] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 22:19:17,112] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-31 22:19:17,113] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-31 22:19:17,113] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-31 22:19:17,113] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-31 22:19:17,113] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-31 22:19:17,113] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-31 22:19:17,113] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-31 22:19:17,113] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-31 22:19:17,113] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-31 22:19:17,113] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-31 22:19:17,113] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-31 22:19:17,113] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-31 22:19:17,113] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-31 22:19:17,114] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-31 22:19:17,114] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-31 22:19:17,114] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-31 22:19:17,114] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-31 22:19:17,114] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-31 22:19:17,114] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-31 22:19:17,114] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-31 22:19:17,114] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-31 22:19:17,114] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-31 22:19:17,114] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-31 22:19:17,114] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-31 22:19:17,114] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-31 22:19:17,114] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-31 22:19:17,115] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-31 22:19:17,115] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-31 22:19:17,115] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-31 22:19:17,115] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-31 22:19:17,115] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-31 22:19:17,115] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-31 22:19:17,115] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-31 22:19:17,115] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-31 22:19:17,115] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug31_22-19-05_instance-3bwob41y-01[0m
[32m[2022-08-31 22:19:17,115] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-31 22:19:17,115] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-31 22:19:17,115] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-31 22:19:17,115] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-31 22:19:17,115] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-31 22:19:17,116] [    INFO][0m - max_seq_length                :128[0m
[32m[2022-08-31 22:19:17,116] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-31 22:19:17,116] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-31 22:19:17,116] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-31 22:19:17,116] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-31 22:19:17,116] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-08-31 22:19:17,116] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-31 22:19:17,116] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-31 22:19:17,116] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-31 22:19:17,116] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-31 22:19:17,116] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-31 22:19:17,116] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-31 22:19:17,116] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-31 22:19:17,117] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-31 22:19:17,117] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-31 22:19:17,117] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-31 22:19:17,117] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-31 22:19:17,117] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-31 22:19:17,117] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-31 22:19:17,117] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-31 22:19:17,117] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-31 22:19:17,117] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-31 22:19:17,117] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-31 22:19:17,117] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-31 22:19:17,117] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-31 22:19:17,117] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-31 22:19:17,117] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-31 22:19:17,118] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-31 22:19:17,118] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-31 22:19:17,118] [    INFO][0m - seed                          :42[0m
[32m[2022-08-31 22:19:17,118] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-31 22:19:17,118] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-31 22:19:17,118] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-31 22:19:17,118] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-31 22:19:17,118] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-31 22:19:17,118] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-31 22:19:17,118] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-31 22:19:17,118] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-31 22:19:17,118] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-31 22:19:17,118] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-31 22:19:17,118] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-31 22:19:17,119] [    INFO][0m - [0m
[32m[2022-08-31 22:19:17,121] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-31 22:19:17,121] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 22:19:17,121] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-08-31 22:19:17,121] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-31 22:19:17,121] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-31 22:19:17,121] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-31 22:19:17,121] [    INFO][0m -   Total optimization steps = 1000.0[0m
[32m[2022-08-31 22:19:17,121] [    INFO][0m -   Total num train samples = 8000[0m
[32m[2022-08-31 22:19:24,737] [    INFO][0m - loss: 1.04979773, learning_rate: 2.97e-05, global_step: 10, interval_runtime: 7.6137, interval_samples_per_second: 1.051, interval_steps_per_second: 1.313, epoch: 0.5[0m
[32m[2022-08-31 22:19:29,844] [    INFO][0m - loss: 0.70727854, learning_rate: 2.94e-05, global_step: 20, interval_runtime: 5.1074, interval_samples_per_second: 1.566, interval_steps_per_second: 1.958, epoch: 1.0[0m
[32m[2022-08-31 22:19:35,038] [    INFO][0m - loss: 0.61590443, learning_rate: 2.91e-05, global_step: 30, interval_runtime: 5.1941, interval_samples_per_second: 1.54, interval_steps_per_second: 1.925, epoch: 1.5[0m
[32m[2022-08-31 22:19:40,171] [    INFO][0m - loss: 0.46834593, learning_rate: 2.88e-05, global_step: 40, interval_runtime: 5.1326, interval_samples_per_second: 1.559, interval_steps_per_second: 1.948, epoch: 2.0[0m
[32m[2022-08-31 22:19:45,403] [    INFO][0m - loss: 0.39988477, learning_rate: 2.8499999999999998e-05, global_step: 50, interval_runtime: 5.2323, interval_samples_per_second: 1.529, interval_steps_per_second: 1.911, epoch: 2.5[0m
[32m[2022-08-31 22:19:50,540] [    INFO][0m - loss: 0.38163178, learning_rate: 2.8199999999999998e-05, global_step: 60, interval_runtime: 5.1371, interval_samples_per_second: 1.557, interval_steps_per_second: 1.947, epoch: 3.0[0m
[32m[2022-08-31 22:19:55,821] [    INFO][0m - loss: 0.44444938, learning_rate: 2.79e-05, global_step: 70, interval_runtime: 5.2809, interval_samples_per_second: 1.515, interval_steps_per_second: 1.894, epoch: 3.5[0m
[32m[2022-08-31 22:20:01,346] [    INFO][0m - loss: 0.33489523, learning_rate: 2.7600000000000003e-05, global_step: 80, interval_runtime: 5.5248, interval_samples_per_second: 1.448, interval_steps_per_second: 1.81, epoch: 4.0[0m
[32m[2022-08-31 22:20:07,734] [    INFO][0m - loss: 0.19877769, learning_rate: 2.7300000000000003e-05, global_step: 90, interval_runtime: 6.3877, interval_samples_per_second: 1.252, interval_steps_per_second: 1.566, epoch: 4.5[0m
[32m[2022-08-31 22:20:14,012] [    INFO][0m - loss: 0.33196762, learning_rate: 2.7000000000000002e-05, global_step: 100, interval_runtime: 6.2783, interval_samples_per_second: 1.274, interval_steps_per_second: 1.593, epoch: 5.0[0m
[32m[2022-08-31 22:20:14,013] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 22:20:14,013] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 22:20:14,013] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 22:20:14,013] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 22:20:14,013] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 22:20:17,875] [    INFO][0m - eval_loss: 2.078293561935425, eval_accuracy: 0.83125, eval_runtime: 3.8623, eval_samples_per_second: 41.427, eval_steps_per_second: 1.295, epoch: 5.0[0m
[32m[2022-08-31 22:20:17,876] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-31 22:20:17,876] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 22:20:21,200] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-31 22:20:21,200] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-31 22:20:32,551] [    INFO][0m - loss: 0.32102087, learning_rate: 2.6700000000000002e-05, global_step: 110, interval_runtime: 18.5389, interval_samples_per_second: 0.432, interval_steps_per_second: 0.539, epoch: 5.5[0m
[32m[2022-08-31 22:20:38,930] [    INFO][0m - loss: 0.73538451, learning_rate: 2.64e-05, global_step: 120, interval_runtime: 5.1408, interval_samples_per_second: 1.556, interval_steps_per_second: 1.945, epoch: 6.0[0m
[32m[2022-08-31 22:20:44,148] [    INFO][0m - loss: 0.18405035, learning_rate: 2.61e-05, global_step: 130, interval_runtime: 6.4563, interval_samples_per_second: 1.239, interval_steps_per_second: 1.549, epoch: 6.5[0m
[32m[2022-08-31 22:20:49,234] [    INFO][0m - loss: 0.05903298, learning_rate: 2.58e-05, global_step: 140, interval_runtime: 5.0859, interval_samples_per_second: 1.573, interval_steps_per_second: 1.966, epoch: 7.0[0m
[32m[2022-08-31 22:20:54,516] [    INFO][0m - loss: 0.05532602, learning_rate: 2.55e-05, global_step: 150, interval_runtime: 5.2825, interval_samples_per_second: 1.514, interval_steps_per_second: 1.893, epoch: 7.5[0m
[32m[2022-08-31 22:20:59,603] [    INFO][0m - loss: 0.02739203, learning_rate: 2.52e-05, global_step: 160, interval_runtime: 5.087, interval_samples_per_second: 1.573, interval_steps_per_second: 1.966, epoch: 8.0[0m
[32m[2022-08-31 22:21:04,863] [    INFO][0m - loss: 0.14612355, learning_rate: 2.49e-05, global_step: 170, interval_runtime: 5.2604, interval_samples_per_second: 1.521, interval_steps_per_second: 1.901, epoch: 8.5[0m
[32m[2022-08-31 22:21:09,975] [    INFO][0m - loss: 2.908e-05, learning_rate: 2.4599999999999998e-05, global_step: 180, interval_runtime: 5.1111, interval_samples_per_second: 1.565, interval_steps_per_second: 1.957, epoch: 9.0[0m
[32m[2022-08-31 22:21:15,150] [    INFO][0m - loss: 0.00115166, learning_rate: 2.43e-05, global_step: 190, interval_runtime: 5.1751, interval_samples_per_second: 1.546, interval_steps_per_second: 1.932, epoch: 9.5[0m
[32m[2022-08-31 22:21:20,220] [    INFO][0m - loss: 0.15739723, learning_rate: 2.4e-05, global_step: 200, interval_runtime: 5.0702, interval_samples_per_second: 1.578, interval_steps_per_second: 1.972, epoch: 10.0[0m
[32m[2022-08-31 22:21:20,221] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 22:21:20,221] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 22:21:20,221] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 22:21:20,221] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 22:21:20,221] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 22:21:23,409] [    INFO][0m - eval_loss: 1.1032506227493286, eval_accuracy: 0.8875, eval_runtime: 3.1871, eval_samples_per_second: 50.202, eval_steps_per_second: 1.569, epoch: 10.0[0m
[32m[2022-08-31 22:21:23,410] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-31 22:21:23,410] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 22:21:26,679] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-31 22:21:26,680] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-31 22:21:42,390] [    INFO][0m - loss: 2.893e-05, learning_rate: 2.37e-05, global_step: 210, interval_runtime: 22.1704, interval_samples_per_second: 0.361, interval_steps_per_second: 0.451, epoch: 10.5[0m
[32m[2022-08-31 22:21:47,006] [    INFO][0m - loss: 0.00863924, learning_rate: 2.3400000000000003e-05, global_step: 220, interval_runtime: 4.6157, interval_samples_per_second: 1.733, interval_steps_per_second: 2.167, epoch: 11.0[0m
[32m[2022-08-31 22:21:51,612] [    INFO][0m - loss: 2.313e-05, learning_rate: 2.3100000000000002e-05, global_step: 230, interval_runtime: 4.6063, interval_samples_per_second: 1.737, interval_steps_per_second: 2.171, epoch: 11.5[0m
[32m[2022-08-31 22:21:56,190] [    INFO][0m - loss: 0.03418703, learning_rate: 2.2800000000000002e-05, global_step: 240, interval_runtime: 4.5777, interval_samples_per_second: 1.748, interval_steps_per_second: 2.185, epoch: 12.0[0m
[32m[2022-08-31 22:22:00,753] [    INFO][0m - loss: 1.406e-05, learning_rate: 2.25e-05, global_step: 250, interval_runtime: 4.5635, interval_samples_per_second: 1.753, interval_steps_per_second: 2.191, epoch: 12.5[0m
[32m[2022-08-31 22:22:04,736] [    INFO][0m - loss: 0.00442741, learning_rate: 2.22e-05, global_step: 260, interval_runtime: 3.9824, interval_samples_per_second: 2.009, interval_steps_per_second: 2.511, epoch: 13.0[0m
[32m[2022-08-31 22:22:08,118] [    INFO][0m - loss: 0.00011497, learning_rate: 2.19e-05, global_step: 270, interval_runtime: 3.382, interval_samples_per_second: 2.365, interval_steps_per_second: 2.957, epoch: 13.5[0m
[32m[2022-08-31 22:22:11,420] [    INFO][0m - loss: 1.521e-05, learning_rate: 2.16e-05, global_step: 280, interval_runtime: 3.302, interval_samples_per_second: 2.423, interval_steps_per_second: 3.028, epoch: 14.0[0m
[32m[2022-08-31 22:22:14,868] [    INFO][0m - loss: 0.14237962, learning_rate: 2.13e-05, global_step: 290, interval_runtime: 3.4486, interval_samples_per_second: 2.32, interval_steps_per_second: 2.9, epoch: 14.5[0m
[32m[2022-08-31 22:22:18,175] [    INFO][0m - loss: 0.06839387, learning_rate: 2.1e-05, global_step: 300, interval_runtime: 3.3066, interval_samples_per_second: 2.419, interval_steps_per_second: 3.024, epoch: 15.0[0m
[32m[2022-08-31 22:22:18,175] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 22:22:18,176] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 22:22:18,176] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 22:22:18,176] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 22:22:18,176] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 22:22:20,284] [    INFO][0m - eval_loss: 1.4519551992416382, eval_accuracy: 0.875, eval_runtime: 2.1073, eval_samples_per_second: 75.928, eval_steps_per_second: 2.373, epoch: 15.0[0m
[32m[2022-08-31 22:22:20,285] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-31 22:22:20,285] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 22:22:24,088] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-31 22:22:24,088] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-31 22:22:39,569] [    INFO][0m - loss: 0.13363112, learning_rate: 2.07e-05, global_step: 310, interval_runtime: 16.9212, interval_samples_per_second: 0.473, interval_steps_per_second: 0.591, epoch: 15.5[0m
[32m[2022-08-31 22:22:44,692] [    INFO][0m - loss: 0.01094311, learning_rate: 2.04e-05, global_step: 320, interval_runtime: 9.5957, interval_samples_per_second: 0.834, interval_steps_per_second: 1.042, epoch: 16.0[0m
[32m[2022-08-31 22:22:50,063] [    INFO][0m - loss: 2.088e-05, learning_rate: 2.01e-05, global_step: 330, interval_runtime: 5.3708, interval_samples_per_second: 1.49, interval_steps_per_second: 1.862, epoch: 16.5[0m
[32m[2022-08-31 22:22:56,340] [    INFO][0m - loss: 0.01544886, learning_rate: 1.98e-05, global_step: 340, interval_runtime: 6.2773, interval_samples_per_second: 1.274, interval_steps_per_second: 1.593, epoch: 17.0[0m
[32m[2022-08-31 22:23:02,754] [    INFO][0m - loss: 0.00026118, learning_rate: 1.95e-05, global_step: 350, interval_runtime: 6.3892, interval_samples_per_second: 1.252, interval_steps_per_second: 1.565, epoch: 17.5[0m
[32m[2022-08-31 22:23:11,522] [    INFO][0m - loss: 2.8e-06, learning_rate: 1.9200000000000003e-05, global_step: 360, interval_runtime: 6.2414, interval_samples_per_second: 1.282, interval_steps_per_second: 1.602, epoch: 18.0[0m
[32m[2022-08-31 22:23:17,954] [    INFO][0m - loss: 5e-06, learning_rate: 1.8900000000000002e-05, global_step: 370, interval_runtime: 8.9829, interval_samples_per_second: 0.891, interval_steps_per_second: 1.113, epoch: 18.5[0m
[32m[2022-08-31 22:23:25,967] [    INFO][0m - loss: 7.14e-06, learning_rate: 1.86e-05, global_step: 380, interval_runtime: 5.3732, interval_samples_per_second: 1.489, interval_steps_per_second: 1.861, epoch: 19.0[0m
[32m[2022-08-31 22:23:31,224] [    INFO][0m - loss: 4.9e-06, learning_rate: 1.83e-05, global_step: 390, interval_runtime: 7.8973, interval_samples_per_second: 1.013, interval_steps_per_second: 1.266, epoch: 19.5[0m
[32m[2022-08-31 22:23:36,312] [    INFO][0m - loss: 1.77e-06, learning_rate: 1.8e-05, global_step: 400, interval_runtime: 5.0881, interval_samples_per_second: 1.572, interval_steps_per_second: 1.965, epoch: 20.0[0m
[32m[2022-08-31 22:23:36,313] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 22:23:36,313] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 22:23:36,313] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 22:23:36,313] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 22:23:36,313] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 22:23:39,505] [    INFO][0m - eval_loss: 1.3325645923614502, eval_accuracy: 0.8875, eval_runtime: 3.1913, eval_samples_per_second: 50.137, eval_steps_per_second: 1.567, epoch: 20.0[0m
[32m[2022-08-31 22:23:39,505] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-31 22:23:39,506] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 22:23:43,781] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-31 22:23:43,781] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-31 22:23:56,967] [    INFO][0m - loss: 0.00055165, learning_rate: 1.77e-05, global_step: 410, interval_runtime: 20.6546, interval_samples_per_second: 0.387, interval_steps_per_second: 0.484, epoch: 20.5[0m
[32m[2022-08-31 22:24:01,471] [    INFO][0m - loss: 3.9e-07, learning_rate: 1.74e-05, global_step: 420, interval_runtime: 4.5033, interval_samples_per_second: 1.776, interval_steps_per_second: 2.221, epoch: 21.0[0m
[32m[2022-08-31 22:24:06,654] [    INFO][0m - loss: 0.14531326, learning_rate: 1.71e-05, global_step: 430, interval_runtime: 5.1836, interval_samples_per_second: 1.543, interval_steps_per_second: 1.929, epoch: 21.5[0m
[32m[2022-08-31 22:24:11,647] [    INFO][0m - loss: 0.06946115, learning_rate: 1.6800000000000002e-05, global_step: 440, interval_runtime: 4.9934, interval_samples_per_second: 1.602, interval_steps_per_second: 2.003, epoch: 22.0[0m
[32m[2022-08-31 22:24:17,728] [    INFO][0m - loss: 2.24e-06, learning_rate: 1.65e-05, global_step: 450, interval_runtime: 6.0803, interval_samples_per_second: 1.316, interval_steps_per_second: 1.645, epoch: 22.5[0m
[32m[2022-08-31 22:24:23,964] [    INFO][0m - loss: 1.06e-06, learning_rate: 1.62e-05, global_step: 460, interval_runtime: 6.2359, interval_samples_per_second: 1.283, interval_steps_per_second: 1.604, epoch: 23.0[0m
[32m[2022-08-31 22:24:30,355] [    INFO][0m - loss: 1.41e-06, learning_rate: 1.59e-05, global_step: 470, interval_runtime: 6.3919, interval_samples_per_second: 1.252, interval_steps_per_second: 1.564, epoch: 23.5[0m
[32m[2022-08-31 22:24:36,571] [    INFO][0m - loss: 0.01685315, learning_rate: 1.56e-05, global_step: 480, interval_runtime: 6.2156, interval_samples_per_second: 1.287, interval_steps_per_second: 1.609, epoch: 24.0[0m
[32m[2022-08-31 22:24:42,935] [    INFO][0m - loss: 1.44e-06, learning_rate: 1.53e-05, global_step: 490, interval_runtime: 6.3638, interval_samples_per_second: 1.257, interval_steps_per_second: 1.571, epoch: 24.5[0m
[32m[2022-08-31 22:24:48,328] [    INFO][0m - loss: 1.81e-06, learning_rate: 1.5e-05, global_step: 500, interval_runtime: 5.3929, interval_samples_per_second: 1.483, interval_steps_per_second: 1.854, epoch: 25.0[0m
[32m[2022-08-31 22:24:48,329] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 22:24:48,329] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 22:24:48,329] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 22:24:48,329] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 22:24:48,329] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 22:24:51,483] [    INFO][0m - eval_loss: 1.360690951347351, eval_accuracy: 0.9, eval_runtime: 3.1532, eval_samples_per_second: 50.741, eval_steps_per_second: 1.586, epoch: 25.0[0m
[32m[2022-08-31 22:24:51,483] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-08-31 22:24:51,483] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 22:24:54,707] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-08-31 22:24:54,707] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-08-31 22:25:06,044] [    INFO][0m - loss: 0.02748704, learning_rate: 1.47e-05, global_step: 510, interval_runtime: 17.7165, interval_samples_per_second: 0.452, interval_steps_per_second: 0.564, epoch: 25.5[0m
[32m[2022-08-31 22:25:14,240] [    INFO][0m - loss: 1.4e-06, learning_rate: 1.44e-05, global_step: 520, interval_runtime: 5.1351, interval_samples_per_second: 1.558, interval_steps_per_second: 1.947, epoch: 26.0[0m
[32m[2022-08-31 22:25:19,446] [    INFO][0m - loss: 1.46e-06, learning_rate: 1.4099999999999999e-05, global_step: 530, interval_runtime: 8.2672, interval_samples_per_second: 0.968, interval_steps_per_second: 1.21, epoch: 26.5[0m
[32m[2022-08-31 22:25:24,566] [    INFO][0m - loss: 1.258e-05, learning_rate: 1.3800000000000002e-05, global_step: 540, interval_runtime: 5.1196, interval_samples_per_second: 1.563, interval_steps_per_second: 1.953, epoch: 27.0[0m
[32m[2022-08-31 22:25:29,824] [    INFO][0m - loss: 0.01989571, learning_rate: 1.3500000000000001e-05, global_step: 550, interval_runtime: 5.2575, interval_samples_per_second: 1.522, interval_steps_per_second: 1.902, epoch: 27.5[0m
[32m[2022-08-31 22:25:34,987] [    INFO][0m - loss: 7e-07, learning_rate: 1.32e-05, global_step: 560, interval_runtime: 5.1635, interval_samples_per_second: 1.549, interval_steps_per_second: 1.937, epoch: 28.0[0m
[32m[2022-08-31 22:25:40,561] [    INFO][0m - loss: 1.61e-06, learning_rate: 1.29e-05, global_step: 570, interval_runtime: 5.5744, interval_samples_per_second: 1.435, interval_steps_per_second: 1.794, epoch: 28.5[0m
[32m[2022-08-31 22:25:46,873] [    INFO][0m - loss: 1.73e-06, learning_rate: 1.26e-05, global_step: 580, interval_runtime: 6.3113, interval_samples_per_second: 1.268, interval_steps_per_second: 1.584, epoch: 29.0[0m
[32m[2022-08-31 22:25:53,297] [    INFO][0m - loss: 1.27e-06, learning_rate: 1.2299999999999999e-05, global_step: 590, interval_runtime: 6.4237, interval_samples_per_second: 1.245, interval_steps_per_second: 1.557, epoch: 29.5[0m
[32m[2022-08-31 22:25:59,578] [    INFO][0m - loss: 1.65e-06, learning_rate: 1.2e-05, global_step: 600, interval_runtime: 6.282, interval_samples_per_second: 1.273, interval_steps_per_second: 1.592, epoch: 30.0[0m
[32m[2022-08-31 22:25:59,579] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 22:25:59,579] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 22:25:59,579] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 22:25:59,579] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 22:25:59,579] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 22:26:03,487] [    INFO][0m - eval_loss: 1.8121402263641357, eval_accuracy: 0.875, eval_runtime: 3.907, eval_samples_per_second: 40.952, eval_steps_per_second: 1.28, epoch: 30.0[0m
[32m[2022-08-31 22:26:03,487] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-08-31 22:26:03,487] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 22:26:06,908] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-08-31 22:26:06,908] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-08-31 22:26:18,775] [    INFO][0m - loss: 7.3e-07, learning_rate: 1.1700000000000001e-05, global_step: 610, interval_runtime: 19.1963, interval_samples_per_second: 0.417, interval_steps_per_second: 0.521, epoch: 30.5[0m
[32m[2022-08-31 22:26:23,890] [    INFO][0m - loss: 1.33e-06, learning_rate: 1.1400000000000001e-05, global_step: 620, interval_runtime: 5.1159, interval_samples_per_second: 1.564, interval_steps_per_second: 1.955, epoch: 31.0[0m
[32m[2022-08-31 22:26:29,180] [    INFO][0m - loss: 1.24e-06, learning_rate: 1.11e-05, global_step: 630, interval_runtime: 5.2888, interval_samples_per_second: 1.513, interval_steps_per_second: 1.891, epoch: 31.5[0m
[32m[2022-08-31 22:26:34,299] [    INFO][0m - loss: 7.97e-06, learning_rate: 1.08e-05, global_step: 640, interval_runtime: 5.1192, interval_samples_per_second: 1.563, interval_steps_per_second: 1.953, epoch: 32.0[0m
[32m[2022-08-31 22:26:39,592] [    INFO][0m - loss: 9.8e-07, learning_rate: 1.05e-05, global_step: 650, interval_runtime: 5.2935, interval_samples_per_second: 1.511, interval_steps_per_second: 1.889, epoch: 32.5[0m
[32m[2022-08-31 22:26:44,746] [    INFO][0m - loss: 5.3e-07, learning_rate: 1.02e-05, global_step: 660, interval_runtime: 5.1541, interval_samples_per_second: 1.552, interval_steps_per_second: 1.94, epoch: 33.0[0m
[32m[2022-08-31 22:26:50,028] [    INFO][0m - loss: 5.2e-07, learning_rate: 9.9e-06, global_step: 670, interval_runtime: 5.2811, interval_samples_per_second: 1.515, interval_steps_per_second: 1.894, epoch: 33.5[0m
[32m[2022-08-31 22:26:55,135] [    INFO][0m - loss: 2.66e-06, learning_rate: 9.600000000000001e-06, global_step: 680, interval_runtime: 5.1072, interval_samples_per_second: 1.566, interval_steps_per_second: 1.958, epoch: 34.0[0m
[32m[2022-08-31 22:27:00,362] [    INFO][0m - loss: 1.64e-06, learning_rate: 9.3e-06, global_step: 690, interval_runtime: 5.2274, interval_samples_per_second: 1.53, interval_steps_per_second: 1.913, epoch: 34.5[0m
[32m[2022-08-31 22:27:05,721] [    INFO][0m - loss: 7.6e-07, learning_rate: 9e-06, global_step: 700, interval_runtime: 5.3592, interval_samples_per_second: 1.493, interval_steps_per_second: 1.866, epoch: 35.0[0m
[32m[2022-08-31 22:27:05,722] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 22:27:05,722] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 22:27:05,722] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 22:27:05,722] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 22:27:05,723] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 22:27:09,637] [    INFO][0m - eval_loss: 1.6881816387176514, eval_accuracy: 0.8625, eval_runtime: 3.9137, eval_samples_per_second: 40.882, eval_steps_per_second: 1.278, epoch: 35.0[0m
[32m[2022-08-31 22:27:09,637] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-700[0m
[32m[2022-08-31 22:27:09,638] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 22:27:13,791] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-700/tokenizer_config.json[0m
[32m[2022-08-31 22:27:13,791] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-700/special_tokens_map.json[0m
[32m[2022-08-31 22:27:27,109] [    INFO][0m - loss: 1.35e-06, learning_rate: 8.7e-06, global_step: 710, interval_runtime: 21.3868, interval_samples_per_second: 0.374, interval_steps_per_second: 0.468, epoch: 35.5[0m
[32m[2022-08-31 22:27:32,906] [    INFO][0m - loss: 1.87e-06, learning_rate: 8.400000000000001e-06, global_step: 720, interval_runtime: 5.7981, interval_samples_per_second: 1.38, interval_steps_per_second: 1.725, epoch: 36.0[0m
[32m[2022-08-31 22:27:38,138] [    INFO][0m - loss: 5.4e-07, learning_rate: 8.1e-06, global_step: 730, interval_runtime: 5.2316, interval_samples_per_second: 1.529, interval_steps_per_second: 1.911, epoch: 36.5[0m
[32m[2022-08-31 22:27:44,358] [    INFO][0m - loss: 6.7e-07, learning_rate: 7.8e-06, global_step: 740, interval_runtime: 5.0897, interval_samples_per_second: 1.572, interval_steps_per_second: 1.965, epoch: 37.0[0m
[32m[2022-08-31 22:27:49,593] [    INFO][0m - loss: 1.16e-06, learning_rate: 7.5e-06, global_step: 750, interval_runtime: 6.3651, interval_samples_per_second: 1.257, interval_steps_per_second: 1.571, epoch: 37.5[0m
[32m[2022-08-31 22:27:54,748] [    INFO][0m - loss: 1.11e-06, learning_rate: 7.2e-06, global_step: 760, interval_runtime: 5.1547, interval_samples_per_second: 1.552, interval_steps_per_second: 1.94, epoch: 38.0[0m
[32m[2022-08-31 22:28:00,056] [    INFO][0m - loss: 1.69e-06, learning_rate: 6.900000000000001e-06, global_step: 770, interval_runtime: 5.3085, interval_samples_per_second: 1.507, interval_steps_per_second: 1.884, epoch: 38.5[0m
[32m[2022-08-31 22:28:05,176] [    INFO][0m - loss: 2.483e-05, learning_rate: 6.6e-06, global_step: 780, interval_runtime: 5.1203, interval_samples_per_second: 1.562, interval_steps_per_second: 1.953, epoch: 39.0[0m
[32m[2022-08-31 22:28:10,479] [    INFO][0m - loss: 2.66e-06, learning_rate: 6.3e-06, global_step: 790, interval_runtime: 5.3027, interval_samples_per_second: 1.509, interval_steps_per_second: 1.886, epoch: 39.5[0m
[32m[2022-08-31 22:28:15,671] [    INFO][0m - loss: 0.02370063, learning_rate: 6e-06, global_step: 800, interval_runtime: 5.1924, interval_samples_per_second: 1.541, interval_steps_per_second: 1.926, epoch: 40.0[0m
[32m[2022-08-31 22:28:15,672] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 22:28:15,672] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 22:28:15,672] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 22:28:15,672] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 22:28:15,672] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 22:28:18,892] [    INFO][0m - eval_loss: 1.6531002521514893, eval_accuracy: 0.86875, eval_runtime: 3.2192, eval_samples_per_second: 49.702, eval_steps_per_second: 1.553, epoch: 40.0[0m
[32m[2022-08-31 22:28:18,892] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-800[0m
[32m[2022-08-31 22:28:18,892] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 22:28:22,430] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-800/tokenizer_config.json[0m
[32m[2022-08-31 22:28:22,431] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-800/special_tokens_map.json[0m
[32m[2022-08-31 22:28:34,206] [    INFO][0m - loss: 1.73e-06, learning_rate: 5.7000000000000005e-06, global_step: 810, interval_runtime: 18.5341, interval_samples_per_second: 0.432, interval_steps_per_second: 0.54, epoch: 40.5[0m
[32m[2022-08-31 22:28:39,342] [    INFO][0m - loss: 6.6e-07, learning_rate: 5.4e-06, global_step: 820, interval_runtime: 5.1362, interval_samples_per_second: 1.558, interval_steps_per_second: 1.947, epoch: 41.0[0m
[32m[2022-08-31 22:28:45,739] [    INFO][0m - loss: 1.09e-06, learning_rate: 5.1e-06, global_step: 830, interval_runtime: 6.3976, interval_samples_per_second: 1.25, interval_steps_per_second: 1.563, epoch: 41.5[0m
[32m[2022-08-31 22:28:51,998] [    INFO][0m - loss: 5.6e-06, learning_rate: 4.800000000000001e-06, global_step: 840, interval_runtime: 6.2588, interval_samples_per_second: 1.278, interval_steps_per_second: 1.598, epoch: 42.0[0m
[32m[2022-08-31 22:29:00,450] [    INFO][0m - loss: 1e-06, learning_rate: 4.5e-06, global_step: 850, interval_runtime: 6.3584, interval_samples_per_second: 1.258, interval_steps_per_second: 1.573, epoch: 42.5[0m
[32m[2022-08-31 22:29:06,730] [    INFO][0m - loss: 5.3e-07, learning_rate: 4.2000000000000004e-06, global_step: 860, interval_runtime: 8.3738, interval_samples_per_second: 0.955, interval_steps_per_second: 1.194, epoch: 43.0[0m
[32m[2022-08-31 22:29:12,188] [    INFO][0m - loss: 0.06596295, learning_rate: 3.9e-06, global_step: 870, interval_runtime: 5.458, interval_samples_per_second: 1.466, interval_steps_per_second: 1.832, epoch: 43.5[0m
[32m[2022-08-31 22:29:17,285] [    INFO][0m - loss: 8.2e-07, learning_rate: 3.6e-06, global_step: 880, interval_runtime: 5.0968, interval_samples_per_second: 1.57, interval_steps_per_second: 1.962, epoch: 44.0[0m
[32m[2022-08-31 22:29:23,375] [    INFO][0m - loss: 1.14e-06, learning_rate: 3.3e-06, global_step: 890, interval_runtime: 5.2905, interval_samples_per_second: 1.512, interval_steps_per_second: 1.89, epoch: 44.5[0m
[32m[2022-08-31 22:29:28,627] [    INFO][0m - loss: 2.88e-06, learning_rate: 3e-06, global_step: 900, interval_runtime: 5.8949, interval_samples_per_second: 1.357, interval_steps_per_second: 1.696, epoch: 45.0[0m
[32m[2022-08-31 22:29:28,628] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 22:29:28,628] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 22:29:28,628] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 22:29:28,628] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 22:29:28,628] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 22:29:32,550] [    INFO][0m - eval_loss: 1.4227511882781982, eval_accuracy: 0.89375, eval_runtime: 3.2129, eval_samples_per_second: 49.799, eval_steps_per_second: 1.556, epoch: 45.0[0m
[32m[2022-08-31 22:29:32,551] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-900[0m
[32m[2022-08-31 22:29:32,551] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 22:29:35,765] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-900/tokenizer_config.json[0m
[32m[2022-08-31 22:29:35,766] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-900/special_tokens_map.json[0m
[32m[2022-08-31 22:29:42,138] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-31 22:29:42,138] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-500 (score: 0.9).[0m
[32m[2022-08-31 22:29:45,179] [    INFO][0m - train_runtime: 628.0573, train_samples_per_second: 12.738, train_steps_per_second: 1.592, train_loss: 0.08230797275322352, epoch: 45.0[0m
[32m[2022-08-31 22:29:45,181] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-31 22:29:45,181] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 22:29:48,414] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-31 22:29:48,414] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-31 22:29:48,415] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-31 22:29:48,416] [    INFO][0m -   epoch                    =       45.0[0m
[32m[2022-08-31 22:29:48,416] [    INFO][0m -   train_loss               =     0.0823[0m
[32m[2022-08-31 22:29:48,416] [    INFO][0m -   train_runtime            = 0:10:28.05[0m
[32m[2022-08-31 22:29:48,416] [    INFO][0m -   train_samples_per_second =     12.738[0m
[32m[2022-08-31 22:29:48,416] [    INFO][0m -   train_steps_per_second   =      1.592[0m
[32m[2022-08-31 22:29:48,420] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-31 22:29:48,420] [    INFO][0m -   Num examples = 610[0m
[32m[2022-08-31 22:29:48,420] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 22:29:48,420] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 22:29:48,420] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-08-31 22:30:00,584] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-31 22:30:00,584] [    INFO][0m -   test_accuracy           =     0.9033[0m
[32m[2022-08-31 22:30:00,584] [    INFO][0m -   test_loss               =     1.5241[0m
[32m[2022-08-31 22:30:00,584] [    INFO][0m -   test_runtime            = 0:00:12.16[0m
[32m[2022-08-31 22:30:00,585] [    INFO][0m -   test_samples_per_second =     50.149[0m
[32m[2022-08-31 22:30:00,585] [    INFO][0m -   test_steps_per_second   =      1.644[0m
[32m[2022-08-31 22:30:00,585] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-31 22:30:00,585] [    INFO][0m -   Num examples = 753[0m
[32m[2022-08-31 22:30:00,585] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 22:30:00,585] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 22:30:00,585] [    INFO][0m -   Total prediction steps = 24[0m
[32m[2022-08-31 22:30:19,244] [    INFO][0m - Predictions for eprstmt saved to ./fewclue_submit_examples.[0m
[]
run.sh: line 78: --freeze_plm: command not found
 
==========
csldcp
==========
 
[33m[2022-08-31 22:30:23,788] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-31 22:30:23,788] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-31 22:30:23,789] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 22:30:23,789] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-31 22:30:23,789] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 22:30:23,789] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-31 22:30:23,789] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-31 22:30:23,789] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-08-31 22:30:23,789] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-31 22:30:23,789] [    INFO][0m - model_name_or_path            :ernie-3.0-xbase-zh[0m
[32m[2022-08-31 22:30:23,789] [    INFO][0m - [0m
[32m[2022-08-31 22:30:23,789] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 22:30:23,789] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-31 22:30:23,789] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 22:30:23,790] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-31 22:30:23,790] [    INFO][0m - prompt                        :{'hard':'ÈòÖËØª‰∏ãËæπÊúâÂÖ≥'}{'mask'}{'mask'}{'hard':'ÁöÑÊùêÊñô'}{'text':'text_a'}[0m
[32m[2022-08-31 22:30:23,790] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-31 22:30:23,790] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-31 22:30:23,790] [    INFO][0m - task_name                     :csldcp[0m
[32m[2022-08-31 22:30:23,790] [    INFO][0m - [0m
[32m[2022-08-31 22:30:23,790] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/ernie_3.0_xbase_zh.pdparams[0m
W0831 22:30:23.792258 16118 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0831 22:30:23.796531 16118 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-31 22:30:33,140] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/ernie_3.0_xbase_zh_vocab.txt[0m
[32m[2022-08-31 22:30:33,165] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/tokenizer_config.json[0m
[32m[2022-08-31 22:30:33,166] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/special_tokens_map.json[0m
[32m[2022-08-31 22:30:33,167] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': 'ÈòÖËØª‰∏ãËæπÊúâÂÖ≥'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'ÁöÑÊùêÊñô'}, {'add_prefix_space': '', 'text': 'text_a'}][0m
[32m[2022-08-31 22:30:33,193] [    INFO][0m - {'‰∏≠ÂåªÂ≠¶/‰∏≠ËçØÂ≠¶': 0, '‰∏≠ÂõΩËØ≠Ë®ÄÊñáÂ≠¶': 1, '‰∫§ÈÄöËøêËæìÂ∑•Á®ã': 2, '‰ΩìËÇ≤Â≠¶': 3, '‰ΩúÁâ©Â≠¶': 4, '‰ø°ÊÅØ‰∏éÈÄö‰ø°Â∑•Á®ã': 5, 'ÂÖâÂ≠¶Â∑•Á®ã': 6, 'ÂÖ¨ÂÖ±Âç´Áîü‰∏éÈ¢ÑÈò≤ÂåªÂ≠¶': 7, 'ÂÖ¨ÂÖ±ÁÆ°ÁêÜ': 8, 'ÂÖµÂô®ÁßëÂ≠¶‰∏éÊäÄÊúØ': 9, 'ÂÜõ‰∫ãÂ≠¶': 10, 'ÂÜú‰∏öÂ∑•Á®ã': 11, 'ÂÜú‰∏öËµÑÊ∫êÂà©Áî®': 12, 'ÂÜúÊûóÁªèÊµéÁÆ°ÁêÜ': 13, 'ÂÜ∂ÈáëÂ∑•Á®ã': 14, 'ÂäõÂ≠¶': 15, 'Âä®ÂäõÂ∑•Á®ãÂèäÂ∑•Á®ãÁÉ≠Áâ©ÁêÜ': 16, 'ÂåñÂ≠¶/ÂåñÂ≠¶Â∑•Á®ã‰∏éÊäÄÊúØ': 17, 'ÂéÜÂè≤Â≠¶': 18, 'Âè£ËÖîÂåªÂ≠¶': 19, 'Âì≤Â≠¶': 20, 'Âõ≠Ëâ∫Â≠¶': 21, 'Âõæ‰π¶È¶Ü„ÄÅÊÉÖÊä•‰∏éÊ°£Ê°àÁÆ°ÁêÜ': 22, 'ÂúüÊú®Â∑•Á®ã': 23, 'Âú∞ÁêÉÁâ©ÁêÜÂ≠¶': 24, 'Âú∞ÁêÜÂ≠¶': 25, 'Âú∞Ë¥®Â≠¶/Âú∞Ë¥®ËµÑÊ∫ê‰∏éÂú∞Ë¥®Â∑•Á®ã': 26, 'Âü∫Á°ÄÂåªÂ≠¶/‰∏¥Â∫äÂåªÂ≠¶': 27, 'Â§ßÊ∞îÁßëÂ≠¶': 28, 'Â§©ÊñáÂ≠¶': 29, 'Â∑•ÂïÜÁÆ°ÁêÜ': 30, 'Â∫îÁî®ÁªèÊµéÂ≠¶': 31, 'Âª∫Á≠ëÂ≠¶': 32, 'ÂøÉÁêÜÂ≠¶': 33, 'ÊéßÂà∂ÁßëÂ≠¶‰∏éÂ∑•Á®ã': 34, 'ÊîøÊ≤ªÂ≠¶': 35, 'ÊïôËÇ≤Â≠¶': 36, 'Êï∞Â≠¶': 37, 'Êñ∞Èóª‰º†Êí≠Â≠¶': 38, 'Êú∫Ê¢∞Â∑•Á®ã': 39, 'ÊùêÊñôÁßëÂ≠¶‰∏éÂ∑•Á®ã': 40, 'ÊûóÂ≠¶/Êûó‰∏öÂ∑•Á®ã': 41, 'Ê†∏ÁßëÂ≠¶‰∏éÊäÄÊúØ': 42, 'Ê§çÁâ©‰øùÊä§': 43, 'Ê∞ëÊóèÂ≠¶': 44, 'Ê∞¥‰∫ß': 45, 'Ê∞¥Âà©Â∑•Á®ã': 46, 'Ê≥ïÂ≠¶': 47, 'ÊµãÁªòÁßëÂ≠¶‰∏éÊäÄÊúØ': 48, 'Êµ∑Ê¥ãÁßëÂ≠¶': 49, 'Áâ©ÁêÜÂ≠¶': 50, 'ÁéØÂ¢ÉÁßëÂ≠¶‰∏éÂ∑•Á®ã': 51, 'ÁêÜËÆ∫ÁªèÊµéÂ≠¶': 52, 'ÁîüÁâ©Â≠¶/ÁîüÁâ©ÁßëÂ≠¶‰∏éÂ∑•Á®ã': 53, 'ÁîµÂ≠êÁßëÂ≠¶‰∏éÊäÄÊúØ': 54, 'ÁîµÊ∞îÂ∑•Á®ã': 55, 'ÁïúÁâßÂ≠¶/ÂÖΩÂåªÂ≠¶': 56, 'Áü≥Ê≤π‰∏éÂ§©ÁÑ∂Ê∞îÂ∑•Á®ã': 57, 'Áüø‰∏öÂ∑•Á®ã': 58, 'Á§æ‰ºöÂ≠¶': 59, 'Á∫∫ÁªáÁßëÂ≠¶‰∏éÂ∑•Á®ã': 60, 'Ëà™Á©∫ÂÆáËà™ÁßëÂ≠¶‰∏éÊäÄÊúØ': 61, 'ËàπËà∂‰∏éÊµ∑Ê¥ãÂ∑•Á®ã': 62, 'Ëâ∫ÊúØÂ≠¶': 63, 'ËçØÂ≠¶': 64, 'ËÆ°ÁÆóÊú∫ÁßëÂ≠¶‰∏éÊäÄÊúØ': 65, 'È£üÂìÅÁßëÂ≠¶‰∏éÂ∑•Á®ã': 66}[0m
2022-08-31 22:30:33,195 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-31 22:30:33,354] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 22:30:33,354] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-31 22:30:33,354] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 22:30:33,354] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-31 22:30:33,354] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-31 22:30:33,354] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-31 22:30:33,355] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-31 22:30:33,355] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-31 22:30:33,355] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-31 22:30:33,355] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-31 22:30:33,355] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-31 22:30:33,355] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-31 22:30:33,355] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-31 22:30:33,355] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-31 22:30:33,355] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-31 22:30:33,355] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-31 22:30:33,355] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-31 22:30:33,355] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-31 22:30:33,355] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-31 22:30:33,356] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-31 22:30:33,356] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-31 22:30:33,356] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-31 22:30:33,356] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-31 22:30:33,356] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-31 22:30:33,356] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-31 22:30:33,356] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-31 22:30:33,356] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-31 22:30:33,356] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-31 22:30:33,356] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-31 22:30:33,356] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-31 22:30:33,356] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-31 22:30:33,356] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-31 22:30:33,357] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-31 22:30:33,357] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-31 22:30:33,357] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-31 22:30:33,357] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-31 22:30:33,357] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-31 22:30:33,357] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug31_22-30-23_instance-3bwob41y-01[0m
[32m[2022-08-31 22:30:33,357] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-31 22:30:33,357] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-31 22:30:33,357] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-31 22:30:33,357] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-31 22:30:33,357] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-31 22:30:33,357] [    INFO][0m - max_seq_length                :256[0m
[32m[2022-08-31 22:30:33,357] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-31 22:30:33,358] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-31 22:30:33,358] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-31 22:30:33,358] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-31 22:30:33,358] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-08-31 22:30:33,358] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-31 22:30:33,358] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-31 22:30:33,358] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-31 22:30:33,358] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-31 22:30:33,358] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-31 22:30:33,358] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-31 22:30:33,358] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-31 22:30:33,358] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-31 22:30:33,358] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-31 22:30:33,358] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-31 22:30:33,359] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-31 22:30:33,359] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-31 22:30:33,359] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-31 22:30:33,359] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-31 22:30:33,359] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-31 22:30:33,359] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-31 22:30:33,359] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-31 22:30:33,359] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-31 22:30:33,359] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-31 22:30:33,359] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-31 22:30:33,359] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-31 22:30:33,359] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-31 22:30:33,359] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-31 22:30:33,360] [    INFO][0m - seed                          :42[0m
[32m[2022-08-31 22:30:33,360] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-31 22:30:33,360] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-31 22:30:33,360] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-31 22:30:33,360] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-31 22:30:33,360] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-31 22:30:33,360] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-31 22:30:33,360] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-31 22:30:33,360] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-31 22:30:33,360] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-31 22:30:33,360] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-31 22:30:33,360] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-31 22:30:33,360] [    INFO][0m - [0m
[32m[2022-08-31 22:30:33,363] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-31 22:30:33,363] [    INFO][0m -   Num examples = 2036[0m
[32m[2022-08-31 22:30:33,363] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-08-31 22:30:33,363] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-31 22:30:33,363] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-31 22:30:33,363] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-31 22:30:33,363] [    INFO][0m -   Total optimization steps = 12750.0[0m
[32m[2022-08-31 22:30:33,363] [    INFO][0m -   Total num train samples = 101800[0m
[32m[2022-08-31 22:30:45,203] [    INFO][0m - loss: 3.72249603, learning_rate: 2.9976470588235296e-05, global_step: 10, interval_runtime: 11.8385, interval_samples_per_second: 0.676, interval_steps_per_second: 0.845, epoch: 0.0392[0m
[32m[2022-08-31 22:30:54,780] [    INFO][0m - loss: 2.93499794, learning_rate: 2.9952941176470588e-05, global_step: 20, interval_runtime: 9.5767, interval_samples_per_second: 0.835, interval_steps_per_second: 1.044, epoch: 0.0784[0m
[32m[2022-08-31 22:31:06,755] [    INFO][0m - loss: 2.82253799, learning_rate: 2.9929411764705883e-05, global_step: 30, interval_runtime: 11.975, interval_samples_per_second: 0.668, interval_steps_per_second: 0.835, epoch: 0.1176[0m
[32m[2022-08-31 22:31:16,258] [    INFO][0m - loss: 2.59374275, learning_rate: 2.9905882352941175e-05, global_step: 40, interval_runtime: 9.5036, interval_samples_per_second: 0.842, interval_steps_per_second: 1.052, epoch: 0.1569[0m
[32m[2022-08-31 22:31:26,141] [    INFO][0m - loss: 2.37762508, learning_rate: 2.988235294117647e-05, global_step: 50, interval_runtime: 9.8822, interval_samples_per_second: 0.81, interval_steps_per_second: 1.012, epoch: 0.1961[0m
[32m[2022-08-31 22:31:37,785] [    INFO][0m - loss: 2.46724358, learning_rate: 2.9858823529411763e-05, global_step: 60, interval_runtime: 11.6436, interval_samples_per_second: 0.687, interval_steps_per_second: 0.859, epoch: 0.2353[0m
[32m[2022-08-31 22:31:49,534] [    INFO][0m - loss: 2.38828011, learning_rate: 2.9835294117647058e-05, global_step: 70, interval_runtime: 11.7501, interval_samples_per_second: 0.681, interval_steps_per_second: 0.851, epoch: 0.2745[0m
[32m[2022-08-31 22:32:00,157] [    INFO][0m - loss: 1.89304504, learning_rate: 2.9811764705882357e-05, global_step: 80, interval_runtime: 10.6224, interval_samples_per_second: 0.753, interval_steps_per_second: 0.941, epoch: 0.3137[0m
[32m[2022-08-31 22:32:09,750] [    INFO][0m - loss: 2.25717964, learning_rate: 2.978823529411765e-05, global_step: 90, interval_runtime: 9.5938, interval_samples_per_second: 0.834, interval_steps_per_second: 1.042, epoch: 0.3529[0m
[32m[2022-08-31 22:32:19,294] [    INFO][0m - loss: 1.92918873, learning_rate: 2.9764705882352944e-05, global_step: 100, interval_runtime: 9.5432, interval_samples_per_second: 0.838, interval_steps_per_second: 1.048, epoch: 0.3922[0m
[32m[2022-08-31 22:32:19,294] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 22:32:19,295] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-31 22:32:19,295] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 22:32:19,295] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 22:32:19,295] [    INFO][0m -   Total prediction steps = 65[0m
terminate called after throwing an instance of 'paddle::memory::allocation::BadAlloc'
  what():  

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   add_final_state_dygraph_function(paddle::experimental::Tensor const&, paddle::experimental::Tensor const&)
1   paddle::experimental::add(paddle::experimental::Tensor const&, paddle::experimental::Tensor const&)
2   void phi::AddRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*)
3   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, paddle::experimental::DataType, unsigned long, bool) const
4   phi::DenseTensor::AllocateFrom(phi::Allocator*, paddle::experimental::DataType, unsigned long)
5   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
6   paddle::memory::allocation::Allocator::Allocate(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
10  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
11  phi::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 1.220703GB memory on GPU 0, 30.787842GB memory has been allocated and available memory is only 983.750000MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
If the above ways do not solve the out of memory problem, you can try to use CUDA managed memory. The command is `export FLAGS_use_cuda_managed_memory=false`.
 (at /paddle/paddle/fluid/memory/allocation/cuda_allocator.cc:95)

run.sh: line 72: 16118 Aborted                 CUDA_VISIBLE_DEVICES=$device python train_single.py --output_dir ./checkpoints/ --prompt "$prompt" --max_seq_length $max_length --learning_rate 3e-5 --ppt_learning_rate 3e-4 --num_train_epochs 50 --logging_steps 10 --do_save True --do_test --eval_steps 100 --save_steps 100 --per_device_eval_batch_size 32 --per_device_train_batch_size $batch_size --model_name_or_path ernie-3.0-xbase-zh --split_id few_all --task_name $task_name --metric_for_best_model accuracy --disable_tqdm True --do_train --do_predict --do_eval --load_best_model_at_end
run.sh: line 78: --freeze_plm: command not found
 
==========
tnews
==========
 
[33m[2022-08-31 22:32:34,469] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-31 22:32:34,470] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-31 22:32:34,470] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 22:32:34,470] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-31 22:32:34,470] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 22:32:34,470] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-31 22:32:34,470] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-31 22:32:34,470] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-08-31 22:32:34,470] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-31 22:32:34,470] [    INFO][0m - model_name_or_path            :ernie-3.0-xbase-zh[0m
[32m[2022-08-31 22:32:34,471] [    INFO][0m - [0m
[32m[2022-08-31 22:32:34,471] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 22:32:34,471] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-31 22:32:34,471] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 22:32:34,471] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-31 22:32:34,471] [    INFO][0m - prompt                        :{'hard':'‰∏ãËæπÊí≠Êä•‰∏ÄÂàô'}{'mask'}{'mask'}{'hard':'Êñ∞ÈóªÔºö'}{'text':'text_a'}[0m
[32m[2022-08-31 22:32:34,471] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-31 22:32:34,471] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-31 22:32:34,471] [    INFO][0m - task_name                     :tnews[0m
[32m[2022-08-31 22:32:34,471] [    INFO][0m - [0m
[32m[2022-08-31 22:32:34,471] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/ernie_3.0_xbase_zh.pdparams[0m
W0831 22:32:34.473155 28735 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0831 22:32:34.477411 28735 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-31 22:32:43,611] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/ernie_3.0_xbase_zh_vocab.txt[0m
[32m[2022-08-31 22:32:43,646] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/tokenizer_config.json[0m
[32m[2022-08-31 22:32:43,646] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/special_tokens_map.json[0m
[32m[2022-08-31 22:32:43,648] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‰∏ãËæπÊí≠Êä•‰∏ÄÂàô'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'Êñ∞ÈóªÔºö'}, {'add_prefix_space': '', 'text': 'text_a'}][0m
[32m[2022-08-31 22:32:43,663] [    INFO][0m - {'news_agriculture': 0, 'news_car': 1, 'news_culture': 2, 'news_edu': 3, 'news_entertainment': 4, 'news_finance': 5, 'news_game': 6, 'news_house': 7, 'news_military': 8, 'news_sports': 9, 'news_stock': 10, 'news_story': 11, 'news_tech': 12, 'news_travel': 13, 'news_world': 14}[0m
2022-08-31 22:32:43,667 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-31 22:32:43,791] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 22:32:43,792] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-31 22:32:43,792] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 22:32:43,792] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-31 22:32:43,792] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-31 22:32:43,792] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-31 22:32:43,792] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-31 22:32:43,792] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-31 22:32:43,792] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-31 22:32:43,792] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-31 22:32:43,793] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-31 22:32:43,793] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-31 22:32:43,793] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-31 22:32:43,793] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-31 22:32:43,793] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-31 22:32:43,793] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-31 22:32:43,793] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-31 22:32:43,793] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-31 22:32:43,793] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-31 22:32:43,793] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-31 22:32:43,793] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-31 22:32:43,793] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-31 22:32:43,793] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-31 22:32:43,794] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-31 22:32:43,794] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-31 22:32:43,794] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-31 22:32:43,794] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-31 22:32:43,794] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-31 22:32:43,794] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-31 22:32:43,794] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-31 22:32:43,794] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-31 22:32:43,794] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-31 22:32:43,794] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-31 22:32:43,794] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-31 22:32:43,794] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-31 22:32:43,794] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-31 22:32:43,795] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-31 22:32:43,795] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug31_22-32-34_instance-3bwob41y-01[0m
[32m[2022-08-31 22:32:43,795] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-31 22:32:43,795] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-31 22:32:43,795] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-31 22:32:43,795] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-31 22:32:43,795] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-31 22:32:43,795] [    INFO][0m - max_seq_length                :64[0m
[32m[2022-08-31 22:32:43,795] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-31 22:32:43,795] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-31 22:32:43,795] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-31 22:32:43,795] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-31 22:32:43,795] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-08-31 22:32:43,795] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-31 22:32:43,796] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-31 22:32:43,796] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-31 22:32:43,796] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-31 22:32:43,796] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-31 22:32:43,796] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-31 22:32:43,796] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-31 22:32:43,796] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-31 22:32:43,796] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-31 22:32:43,796] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-31 22:32:43,796] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-31 22:32:43,796] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-31 22:32:43,796] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-31 22:32:43,796] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-31 22:32:43,796] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-31 22:32:43,797] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-31 22:32:43,797] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-31 22:32:43,797] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-31 22:32:43,797] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-31 22:32:43,797] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-31 22:32:43,797] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-31 22:32:43,797] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-31 22:32:43,797] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-31 22:32:43,797] [    INFO][0m - seed                          :42[0m
[32m[2022-08-31 22:32:43,797] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-31 22:32:43,797] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-31 22:32:43,797] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-31 22:32:43,797] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-31 22:32:43,798] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-31 22:32:43,798] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-31 22:32:43,798] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-31 22:32:43,798] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-31 22:32:43,798] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-31 22:32:43,798] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-31 22:32:43,798] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-31 22:32:43,798] [    INFO][0m - [0m
[32m[2022-08-31 22:32:43,801] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-31 22:32:43,801] [    INFO][0m -   Num examples = 1185[0m
[32m[2022-08-31 22:32:43,801] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-08-31 22:32:43,801] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-31 22:32:43,801] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-31 22:32:43,801] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-31 22:32:43,804] [    INFO][0m -   Total optimization steps = 7450.0[0m
[32m[2022-08-31 22:32:43,804] [    INFO][0m -   Total num train samples = 59250[0m
[32m[2022-08-31 22:32:49,945] [    INFO][0m - loss: 2.67413197, learning_rate: 2.9959731543624162e-05, global_step: 10, interval_runtime: 6.1395, interval_samples_per_second: 1.303, interval_steps_per_second: 1.629, epoch: 0.0671[0m
[32m[2022-08-31 22:32:53,773] [    INFO][0m - loss: 2.59858608, learning_rate: 2.9919463087248323e-05, global_step: 20, interval_runtime: 3.8278, interval_samples_per_second: 2.09, interval_steps_per_second: 2.612, epoch: 0.1342[0m
[32m[2022-08-31 22:32:57,883] [    INFO][0m - loss: 1.77561951, learning_rate: 2.9879194630872484e-05, global_step: 30, interval_runtime: 3.8015, interval_samples_per_second: 2.104, interval_steps_per_second: 2.631, epoch: 0.2013[0m
[32m[2022-08-31 22:33:01,717] [    INFO][0m - loss: 2.09570847, learning_rate: 2.9838926174496645e-05, global_step: 40, interval_runtime: 4.143, interval_samples_per_second: 1.931, interval_steps_per_second: 2.414, epoch: 0.2685[0m
[32m[2022-08-31 22:33:05,550] [    INFO][0m - loss: 1.7760355, learning_rate: 2.9798657718120806e-05, global_step: 50, interval_runtime: 3.833, interval_samples_per_second: 2.087, interval_steps_per_second: 2.609, epoch: 0.3356[0m
[32m[2022-08-31 22:33:09,368] [    INFO][0m - loss: 1.40765953, learning_rate: 2.9758389261744967e-05, global_step: 60, interval_runtime: 3.8174, interval_samples_per_second: 2.096, interval_steps_per_second: 2.62, epoch: 0.4027[0m
[32m[2022-08-31 22:33:13,229] [    INFO][0m - loss: 1.63952675, learning_rate: 2.9718120805369125e-05, global_step: 70, interval_runtime: 3.8606, interval_samples_per_second: 2.072, interval_steps_per_second: 2.59, epoch: 0.4698[0m
[32m[2022-08-31 22:33:17,034] [    INFO][0m - loss: 1.7238369, learning_rate: 2.967785234899329e-05, global_step: 80, interval_runtime: 3.8052, interval_samples_per_second: 2.102, interval_steps_per_second: 2.628, epoch: 0.5369[0m
[32m[2022-08-31 22:33:20,122] [    INFO][0m - loss: 2.0387064, learning_rate: 2.963758389261745e-05, global_step: 90, interval_runtime: 3.0879, interval_samples_per_second: 2.591, interval_steps_per_second: 3.238, epoch: 0.604[0m
[32m[2022-08-31 22:33:23,268] [    INFO][0m - loss: 1.37491693, learning_rate: 2.9597315436241612e-05, global_step: 100, interval_runtime: 3.1463, interval_samples_per_second: 2.543, interval_steps_per_second: 3.178, epoch: 0.6711[0m
[32m[2022-08-31 22:33:23,269] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 22:33:23,269] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-08-31 22:33:23,269] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 22:33:23,269] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 22:33:23,270] [    INFO][0m -   Total prediction steps = 35[0m
[32m[2022-08-31 22:33:35,570] [    INFO][0m - eval_loss: 1.521762728691101, eval_accuracy: 0.5418943533697632, eval_runtime: 12.2996, eval_samples_per_second: 89.271, eval_steps_per_second: 2.846, epoch: 0.6711[0m
[32m[2022-08-31 22:33:35,571] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-31 22:33:35,571] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 22:33:53,831] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-31 22:33:53,832] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-31 22:34:12,519] [    INFO][0m - loss: 1.73139191, learning_rate: 2.9557046979865773e-05, global_step: 110, interval_runtime: 49.2506, interval_samples_per_second: 0.162, interval_steps_per_second: 0.203, epoch: 0.7383[0m
[32m[2022-08-31 22:34:16,361] [    INFO][0m - loss: 1.7670332, learning_rate: 2.951677852348993e-05, global_step: 120, interval_runtime: 3.8431, interval_samples_per_second: 2.082, interval_steps_per_second: 2.602, epoch: 0.8054[0m
[32m[2022-08-31 22:34:20,202] [    INFO][0m - loss: 1.82548027, learning_rate: 2.9476510067114095e-05, global_step: 130, interval_runtime: 3.8402, interval_samples_per_second: 2.083, interval_steps_per_second: 2.604, epoch: 0.8725[0m
[32m[2022-08-31 22:34:24,007] [    INFO][0m - loss: 1.93894711, learning_rate: 2.9436241610738256e-05, global_step: 140, interval_runtime: 3.8051, interval_samples_per_second: 2.102, interval_steps_per_second: 2.628, epoch: 0.9396[0m
[32m[2022-08-31 22:34:27,659] [    INFO][0m - loss: 1.68099289, learning_rate: 2.9395973154362418e-05, global_step: 150, interval_runtime: 3.6522, interval_samples_per_second: 2.19, interval_steps_per_second: 2.738, epoch: 1.0067[0m
[32m[2022-08-31 22:34:31,490] [    INFO][0m - loss: 1.30874615, learning_rate: 2.935570469798658e-05, global_step: 160, interval_runtime: 3.8305, interval_samples_per_second: 2.089, interval_steps_per_second: 2.611, epoch: 1.0738[0m
[32m[2022-08-31 22:34:35,245] [    INFO][0m - loss: 1.26354256, learning_rate: 2.9315436241610736e-05, global_step: 170, interval_runtime: 3.7548, interval_samples_per_second: 2.131, interval_steps_per_second: 2.663, epoch: 1.1409[0m
[32m[2022-08-31 22:34:38,371] [    INFO][0m - loss: 1.26220264, learning_rate: 2.92751677852349e-05, global_step: 180, interval_runtime: 3.1269, interval_samples_per_second: 2.558, interval_steps_per_second: 3.198, epoch: 1.2081[0m
[32m[2022-08-31 22:34:41,454] [    INFO][0m - loss: 1.69331665, learning_rate: 2.9234899328859062e-05, global_step: 190, interval_runtime: 3.0826, interval_samples_per_second: 2.595, interval_steps_per_second: 3.244, epoch: 1.2752[0m
[32m[2022-08-31 22:34:44,575] [    INFO][0m - loss: 1.19676523, learning_rate: 2.9194630872483223e-05, global_step: 200, interval_runtime: 3.1206, interval_samples_per_second: 2.564, interval_steps_per_second: 3.205, epoch: 1.3423[0m
[32m[2022-08-31 22:34:44,577] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 22:34:44,577] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-08-31 22:34:44,577] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 22:34:44,577] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 22:34:44,577] [    INFO][0m -   Total prediction steps = 35[0m
[32m[2022-08-31 22:34:56,779] [    INFO][0m - eval_loss: 1.8110390901565552, eval_accuracy: 0.5446265938069217, eval_runtime: 12.2018, eval_samples_per_second: 89.987, eval_steps_per_second: 2.868, epoch: 1.3423[0m
[32m[2022-08-31 22:34:56,780] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-31 22:34:56,780] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 22:35:05,171] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-31 22:35:05,172] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-31 22:35:23,244] [    INFO][0m - loss: 1.45158463, learning_rate: 2.915436241610738e-05, global_step: 210, interval_runtime: 38.6691, interval_samples_per_second: 0.207, interval_steps_per_second: 0.259, epoch: 1.4094[0m
[32m[2022-08-31 22:35:26,019] [    INFO][0m - loss: 1.23943949, learning_rate: 2.9114093959731542e-05, global_step: 220, interval_runtime: 2.7755, interval_samples_per_second: 2.882, interval_steps_per_second: 3.603, epoch: 1.4765[0m
[32m[2022-08-31 22:35:28,781] [    INFO][0m - loss: 1.47040348, learning_rate: 2.9073825503355706e-05, global_step: 230, interval_runtime: 2.7618, interval_samples_per_second: 2.897, interval_steps_per_second: 3.621, epoch: 1.5436[0m
[32m[2022-08-31 22:35:31,546] [    INFO][0m - loss: 1.60074692, learning_rate: 2.9033557046979868e-05, global_step: 240, interval_runtime: 2.7653, interval_samples_per_second: 2.893, interval_steps_per_second: 3.616, epoch: 1.6107[0m
[32m[2022-08-31 22:35:34,876] [    INFO][0m - loss: 1.30490799, learning_rate: 2.899328859060403e-05, global_step: 250, interval_runtime: 3.3297, interval_samples_per_second: 2.403, interval_steps_per_second: 3.003, epoch: 1.6779[0m
[32m[2022-08-31 22:35:38,727] [    INFO][0m - loss: 1.5132658, learning_rate: 2.8953020134228186e-05, global_step: 260, interval_runtime: 3.8508, interval_samples_per_second: 2.077, interval_steps_per_second: 2.597, epoch: 1.745[0m
[32m[2022-08-31 22:35:42,530] [    INFO][0m - loss: 1.40945511, learning_rate: 2.891275167785235e-05, global_step: 270, interval_runtime: 3.8007, interval_samples_per_second: 2.105, interval_steps_per_second: 2.631, epoch: 1.8121[0m
[32m[2022-08-31 22:35:46,363] [    INFO][0m - loss: 1.29815779, learning_rate: 2.8872483221476512e-05, global_step: 280, interval_runtime: 3.8351, interval_samples_per_second: 2.086, interval_steps_per_second: 2.608, epoch: 1.8792[0m
[32m[2022-08-31 22:35:50,204] [    INFO][0m - loss: 1.27143116, learning_rate: 2.8832214765100673e-05, global_step: 290, interval_runtime: 3.8417, interval_samples_per_second: 2.082, interval_steps_per_second: 2.603, epoch: 1.9463[0m
[32m[2022-08-31 22:35:53,880] [    INFO][0m - loss: 1.07835779, learning_rate: 2.879194630872483e-05, global_step: 300, interval_runtime: 3.6731, interval_samples_per_second: 2.178, interval_steps_per_second: 2.722, epoch: 2.0134[0m
[32m[2022-08-31 22:35:53,881] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 22:35:53,881] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-08-31 22:35:53,881] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 22:35:53,881] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 22:35:53,882] [    INFO][0m -   Total prediction steps = 35[0m
[32m[2022-08-31 22:36:07,885] [    INFO][0m - eval_loss: 1.727831244468689, eval_accuracy: 0.4990892531876138, eval_runtime: 14.0037, eval_samples_per_second: 78.408, eval_steps_per_second: 2.499, epoch: 2.0134[0m
[32m[2022-08-31 22:36:07,886] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-31 22:36:07,886] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 22:36:15,592] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-31 22:36:15,596] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-31 22:36:33,147] [    INFO][0m - loss: 0.9752986, learning_rate: 2.8751677852348992e-05, global_step: 310, interval_runtime: 39.2699, interval_samples_per_second: 0.204, interval_steps_per_second: 0.255, epoch: 2.0805[0m
[32m[2022-08-31 22:36:36,282] [    INFO][0m - loss: 0.90975952, learning_rate: 2.8711409395973157e-05, global_step: 320, interval_runtime: 3.1349, interval_samples_per_second: 2.552, interval_steps_per_second: 3.19, epoch: 2.1477[0m
[32m[2022-08-31 22:36:39,377] [    INFO][0m - loss: 1.14066429, learning_rate: 2.8671140939597318e-05, global_step: 330, interval_runtime: 3.095, interval_samples_per_second: 2.585, interval_steps_per_second: 3.231, epoch: 2.2148[0m
[32m[2022-08-31 22:36:42,477] [    INFO][0m - loss: 1.21137009, learning_rate: 2.863087248322148e-05, global_step: 340, interval_runtime: 3.0993, interval_samples_per_second: 2.581, interval_steps_per_second: 3.226, epoch: 2.2819[0m
[32m[2022-08-31 22:36:45,631] [    INFO][0m - loss: 0.94051018, learning_rate: 2.8590604026845637e-05, global_step: 350, interval_runtime: 3.1546, interval_samples_per_second: 2.536, interval_steps_per_second: 3.17, epoch: 2.349[0m
[32m[2022-08-31 22:36:48,718] [    INFO][0m - loss: 0.85538225, learning_rate: 2.8550335570469798e-05, global_step: 360, interval_runtime: 3.0867, interval_samples_per_second: 2.592, interval_steps_per_second: 3.24, epoch: 2.4161[0m
[32m[2022-08-31 22:36:52,405] [    INFO][0m - loss: 0.79871774, learning_rate: 2.8510067114093962e-05, global_step: 370, interval_runtime: 3.1402, interval_samples_per_second: 2.548, interval_steps_per_second: 3.185, epoch: 2.4832[0m
[32m[2022-08-31 22:36:55,564] [    INFO][0m - loss: 0.90730839, learning_rate: 2.8469798657718123e-05, global_step: 380, interval_runtime: 3.7057, interval_samples_per_second: 2.159, interval_steps_per_second: 2.699, epoch: 2.5503[0m
[32m[2022-08-31 22:36:59,354] [    INFO][0m - loss: 1.00061808, learning_rate: 2.8429530201342284e-05, global_step: 390, interval_runtime: 3.79, interval_samples_per_second: 2.111, interval_steps_per_second: 2.639, epoch: 2.6174[0m
[32m[2022-08-31 22:37:03,187] [    INFO][0m - loss: 0.96307201, learning_rate: 2.8389261744966442e-05, global_step: 400, interval_runtime: 3.8332, interval_samples_per_second: 2.087, interval_steps_per_second: 2.609, epoch: 2.6846[0m
[32m[2022-08-31 22:37:03,188] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 22:37:03,188] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-08-31 22:37:03,188] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 22:37:03,188] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 22:37:03,188] [    INFO][0m -   Total prediction steps = 35[0m
[32m[2022-08-31 22:37:17,942] [    INFO][0m - eval_loss: 1.7888507843017578, eval_accuracy: 0.5200364298724954, eval_runtime: 14.7534, eval_samples_per_second: 74.424, eval_steps_per_second: 2.372, epoch: 2.6846[0m
[32m[2022-08-31 22:37:17,943] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-31 22:37:17,943] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 22:37:30,695] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-31 22:37:30,696] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-31 22:37:52,624] [    INFO][0m - loss: 1.14764738, learning_rate: 2.8348993288590603e-05, global_step: 410, interval_runtime: 49.4367, interval_samples_per_second: 0.162, interval_steps_per_second: 0.202, epoch: 2.7517[0m
[32m[2022-08-31 22:37:55,162] [    INFO][0m - loss: 1.15866404, learning_rate: 2.8308724832214768e-05, global_step: 420, interval_runtime: 1.9975, interval_samples_per_second: 4.005, interval_steps_per_second: 5.006, epoch: 2.8188[0m
[32m[2022-08-31 22:37:57,146] [    INFO][0m - loss: 0.9948884, learning_rate: 2.826845637583893e-05, global_step: 430, interval_runtime: 2.5248, interval_samples_per_second: 3.169, interval_steps_per_second: 3.961, epoch: 2.8859[0m
[32m[2022-08-31 22:37:59,203] [    INFO][0m - loss: 0.94982224, learning_rate: 2.8228187919463087e-05, global_step: 440, interval_runtime: 2.0572, interval_samples_per_second: 3.889, interval_steps_per_second: 4.861, epoch: 2.953[0m
[32m[2022-08-31 22:38:00,877] [    INFO][0m - loss: 0.96601381, learning_rate: 2.8187919463087248e-05, global_step: 450, interval_runtime: 1.6737, interval_samples_per_second: 4.78, interval_steps_per_second: 5.975, epoch: 3.0201[0m
[32m[2022-08-31 22:38:03,478] [    INFO][0m - loss: 0.52552876, learning_rate: 2.814765100671141e-05, global_step: 460, interval_runtime: 2.601, interval_samples_per_second: 3.076, interval_steps_per_second: 3.845, epoch: 3.0872[0m
[32m[2022-08-31 22:38:06,189] [    INFO][0m - loss: 0.59112167, learning_rate: 2.8107382550335573e-05, global_step: 470, interval_runtime: 2.7115, interval_samples_per_second: 2.95, interval_steps_per_second: 3.688, epoch: 3.1544[0m
[32m[2022-08-31 22:38:08,904] [    INFO][0m - loss: 0.49163589, learning_rate: 2.8067114093959734e-05, global_step: 480, interval_runtime: 2.7144, interval_samples_per_second: 2.947, interval_steps_per_second: 3.684, epoch: 3.2215[0m
[32m[2022-08-31 22:38:11,570] [    INFO][0m - loss: 0.67290497, learning_rate: 2.8026845637583892e-05, global_step: 490, interval_runtime: 2.6664, interval_samples_per_second: 3.0, interval_steps_per_second: 3.75, epoch: 3.2886[0m
[32m[2022-08-31 22:38:14,361] [    INFO][0m - loss: 0.81242599, learning_rate: 2.7986577181208053e-05, global_step: 500, interval_runtime: 2.7913, interval_samples_per_second: 2.866, interval_steps_per_second: 3.583, epoch: 3.3557[0m
[32m[2022-08-31 22:38:14,362] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 22:38:14,362] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-08-31 22:38:14,362] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 22:38:14,362] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 22:38:14,362] [    INFO][0m -   Total prediction steps = 35[0m
[32m[2022-08-31 22:38:24,468] [    INFO][0m - eval_loss: 2.2979652881622314, eval_accuracy: 0.5318761384335154, eval_runtime: 10.1055, eval_samples_per_second: 108.654, eval_steps_per_second: 3.463, epoch: 3.3557[0m
[32m[2022-08-31 22:38:24,469] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-08-31 22:38:24,469] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 22:38:31,928] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-08-31 22:38:31,929] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-08-31 22:38:48,929] [    INFO][0m - loss: 0.96972752, learning_rate: 2.7946308724832214e-05, global_step: 510, interval_runtime: 34.5671, interval_samples_per_second: 0.231, interval_steps_per_second: 0.289, epoch: 3.4228[0m
[32m[2022-08-31 22:38:52,036] [    INFO][0m - loss: 0.63962698, learning_rate: 2.790604026845638e-05, global_step: 520, interval_runtime: 3.1075, interval_samples_per_second: 2.574, interval_steps_per_second: 3.218, epoch: 3.4899[0m
[32m[2022-08-31 22:38:55,122] [    INFO][0m - loss: 0.63860025, learning_rate: 2.7865771812080537e-05, global_step: 530, interval_runtime: 3.086, interval_samples_per_second: 2.592, interval_steps_per_second: 3.24, epoch: 3.557[0m
[32m[2022-08-31 22:38:58,228] [    INFO][0m - loss: 0.78068876, learning_rate: 2.7825503355704698e-05, global_step: 540, interval_runtime: 3.1062, interval_samples_per_second: 2.575, interval_steps_per_second: 3.219, epoch: 3.6242[0m
[32m[2022-08-31 22:39:01,318] [    INFO][0m - loss: 0.72749271, learning_rate: 2.778523489932886e-05, global_step: 550, interval_runtime: 3.0894, interval_samples_per_second: 2.59, interval_steps_per_second: 3.237, epoch: 3.6913[0m
[32m[2022-08-31 22:39:04,418] [    INFO][0m - loss: 0.71575828, learning_rate: 2.774496644295302e-05, global_step: 560, interval_runtime: 3.1003, interval_samples_per_second: 2.58, interval_steps_per_second: 3.226, epoch: 3.7584[0m
[32m[2022-08-31 22:39:07,540] [    INFO][0m - loss: 0.57230411, learning_rate: 2.7704697986577185e-05, global_step: 570, interval_runtime: 3.1223, interval_samples_per_second: 2.562, interval_steps_per_second: 3.203, epoch: 3.8255[0m
[32m[2022-08-31 22:39:10,672] [    INFO][0m - loss: 0.360921, learning_rate: 2.7664429530201342e-05, global_step: 580, interval_runtime: 3.1314, interval_samples_per_second: 2.555, interval_steps_per_second: 3.193, epoch: 3.8926[0m
[32m[2022-08-31 22:39:13,723] [    INFO][0m - loss: 0.72416358, learning_rate: 2.7624161073825503e-05, global_step: 590, interval_runtime: 3.0514, interval_samples_per_second: 2.622, interval_steps_per_second: 3.277, epoch: 3.9597[0m
[32m[2022-08-31 22:39:16,533] [    INFO][0m - loss: 0.61682806, learning_rate: 2.7583892617449664e-05, global_step: 600, interval_runtime: 2.8093, interval_samples_per_second: 2.848, interval_steps_per_second: 3.56, epoch: 4.0268[0m
[32m[2022-08-31 22:39:16,533] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 22:39:16,533] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-08-31 22:39:16,534] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 22:39:16,534] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 22:39:16,534] [    INFO][0m -   Total prediction steps = 35[0m
[32m[2022-08-31 22:39:31,305] [    INFO][0m - eval_loss: 2.1040682792663574, eval_accuracy: 0.52367941712204, eval_runtime: 14.7715, eval_samples_per_second: 74.332, eval_steps_per_second: 2.369, epoch: 4.0268[0m
[32m[2022-08-31 22:39:31,306] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-08-31 22:39:31,306] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 22:39:38,656] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-08-31 22:39:38,657] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-08-31 22:39:52,207] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-31 22:39:52,207] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-200 (score: 0.5446265938069217).[0m
[32m[2022-08-31 22:39:55,183] [    INFO][0m - train_runtime: 431.3776, train_samples_per_second: 137.351, train_steps_per_second: 17.27, train_loss: 1.2195060388247172, epoch: 4.0268[0m
[32m[2022-08-31 22:39:55,184] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-31 22:39:55,184] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 22:40:06,654] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-31 22:40:06,655] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-31 22:40:06,657] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-31 22:40:06,658] [    INFO][0m -   epoch                    =     4.0268[0m
[32m[2022-08-31 22:40:06,658] [    INFO][0m -   train_loss               =     1.2195[0m
[32m[2022-08-31 22:40:06,658] [    INFO][0m -   train_runtime            = 0:07:11.37[0m
[32m[2022-08-31 22:40:06,658] [    INFO][0m -   train_samples_per_second =    137.351[0m
[32m[2022-08-31 22:40:06,659] [    INFO][0m -   train_steps_per_second   =      17.27[0m
[32m[2022-08-31 22:40:06,664] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-31 22:40:06,664] [    INFO][0m -   Num examples = 2010[0m
[32m[2022-08-31 22:40:06,664] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 22:40:06,664] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 22:40:06,665] [    INFO][0m -   Total prediction steps = 63[0m
[32m[2022-08-31 22:40:28,772] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-31 22:40:28,773] [    INFO][0m -   test_accuracy           =     0.5682[0m
[32m[2022-08-31 22:40:28,773] [    INFO][0m -   test_loss               =     1.6588[0m
[32m[2022-08-31 22:40:28,773] [    INFO][0m -   test_runtime            = 0:00:22.10[0m
[32m[2022-08-31 22:40:28,773] [    INFO][0m -   test_samples_per_second =     90.918[0m
[32m[2022-08-31 22:40:28,773] [    INFO][0m -   test_steps_per_second   =       2.85[0m
[32m[2022-08-31 22:40:28,774] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-31 22:40:28,774] [    INFO][0m -   Num examples = 1500[0m
[32m[2022-08-31 22:40:28,774] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 22:40:28,774] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 22:40:28,774] [    INFO][0m -   Total prediction steps = 47[0m
[32m[2022-08-31 22:40:48,295] [    INFO][0m - Predictions for tnewsf saved to ./fewclue_submit_examples.[0m
[]
run.sh: line 78: --freeze_plm: command not found
 
==========
iflytek
==========
 
[33m[2022-08-31 22:40:52,976] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-31 22:40:52,976] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-31 22:40:52,976] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 22:40:52,977] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-31 22:40:52,977] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 22:40:52,977] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-31 22:40:52,977] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-31 22:40:52,977] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-08-31 22:40:52,977] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-31 22:40:52,977] [    INFO][0m - model_name_or_path            :ernie-3.0-xbase-zh[0m
[32m[2022-08-31 22:40:52,977] [    INFO][0m - [0m
[32m[2022-08-31 22:40:52,977] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 22:40:52,977] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-31 22:40:52,977] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 22:40:52,977] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-31 22:40:52,977] [    INFO][0m - prompt                        :{'text':'text_a'}{'hard':'ËøôÊ¨æÂ∫îÁî®ÊòØ'}{'mask'}{'mask'}{'hard':'Á±ªÂûãÁöÑ„ÄÇ'}[0m
[32m[2022-08-31 22:40:52,978] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-31 22:40:52,978] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-31 22:40:52,978] [    INFO][0m - task_name                     :iflytek[0m
[32m[2022-08-31 22:40:52,978] [    INFO][0m - [0m
[32m[2022-08-31 22:40:52,978] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/ernie_3.0_xbase_zh.pdparams[0m
W0831 22:40:52.979586   816 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0831 22:40:52.983768   816 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-31 22:41:02,171] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/ernie_3.0_xbase_zh_vocab.txt[0m
[32m[2022-08-31 22:41:02,214] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/tokenizer_config.json[0m
[32m[2022-08-31 22:41:02,215] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/special_tokens_map.json[0m
[32m[2022-08-31 22:41:02,217] [    INFO][0m - Using template: [{'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': 'ËøôÊ¨æÂ∫îÁî®ÊòØ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'Á±ªÂûãÁöÑ„ÄÇ'}][0m
[32m[2022-08-31 22:41:02,265] [    INFO][0m - {'KÊ≠å': 0, 'MOBA': 1, '‰∏≠Â∞èÂ≠¶': 2, '‰π∞Êàø': 3, '‰∫åÊâã': 4, '‰∫≤Â≠êÂÑøÁ´•': 5, '‰ªô‰æ†': 6, '‰ºëÈó≤ÁõäÊô∫': 7, '‰ΩìËÇ≤Âí®ËÆØ': 8, '‰ΩìËÇ≤Á´ûÊäÄ': 9, '‰øùÈô©': 10, 'ÂÄüË¥∑': 11, 'ÂÖçË¥πWIFI': 12, 'ÂÖ¨ÂÖ±‰∫§ÈÄö': 13, 'ÂÖ¨Âä°Âëò': 14, 'ÂÖ∂‰ªñ': 15, 'ÂÖªÁîü‰øùÂÅ•': 16, 'ÂÖºËÅå': 17, 'ÂáèËÇ•Áò¶Ë∫´': 18, 'Âá∫ÂõΩ': 19, 'ÂäûÂÖ¨': 20, 'Âä®‰ΩúÁ±ª': 21, 'ÂåªÁñóÊúçÂä°': 22, 'Âç°Áâå': 23, 'Âç≥Êó∂ÈÄöËÆØ': 24, 'ÂêåÂüéÊúçÂä°': 25, 'Âõ¢Ë¥≠': 26, 'Âú∞ÂõæÂØºËà™': 27, 'Â§ñÂçñ': 28, 'Â•≥ÊÄß': 29, 'Â©öÂ∫Ü': 30, 'Â©öÊÅãÁ§æ‰∫§': 31, 'ÂÆ∂Êîø': 32, 'Â∞ÑÂáªÊ∏∏Êàè': 33, 'Â∞èËØ¥': 34, 'Â∑•‰ΩúÁ§æ‰∫§': 35, 'Â∑•ÂÖ∑': 36, 'ÂΩ©Á•®': 37, 'ÂΩ±ÂÉèÂâ™Ëæë': 38, 'ÂΩ±ËßÜÂ®±‰πê': 39, 'ÂæÆÂçöÂçöÂÆ¢': 40, 'Âø´ÈÄíÁâ©ÊµÅ': 41, 'ÊÉÖ‰æ£Á§æ‰∫§': 42, 'Êàê‰∫∫': 43, 'Êàê‰∫∫ÊïôËÇ≤': 44, 'ÊâìËΩ¶': 45, 'ÊäÄÊúØ': 46, 'ÊêûÁ¨ë': 47, 'ÊëÑÂΩ±‰øÆÂõæ': 48, 'ÊîØ‰ªò': 49, 'Êî∂Ê¨æ': 50, 'ÊîøÂä°': 51, 'ÊïôËæÖ': 52, 'Êñ∞Èóª': 53, 'ÊóÖÊ∏∏ËµÑËÆØ': 54, 'Êó•Â∏∏ÂÖªËΩ¶': 55, 'Êó•Á®ãÁÆ°ÁêÜ': 56, 'ÊùÇÂøó': 57, 'Ê£ãÁâå‰∏≠ÂøÉ': 58, 'ÊØçÂ©¥': 59, 'Ê∞ëÂÆøÁü≠Áßü': 60, 'Ê∞ëËà™': 61, 'Ê±ÇËÅå': 62, 'Ê±ΩËΩ¶‰∫§Êòì': 63, 'Ê±ΩËΩ¶Âí®ËØ¢': 64, 'Êº´Áîª': 65, 'ÁêÜË¥¢': 66, 'ÁîüÊ¥ªÁ§æ‰∫§': 67, 'ÁîµÂè∞': 68, 'ÁîµÂïÜ': 69, 'ÁîµÂ≠ê‰∫ßÂìÅ': 70, 'ÁîµÂΩ±Á•®Âä°': 71, 'ÁôæÁßë': 72, 'Áõ¥Êí≠': 73, 'Áõ∏Êú∫': 74, 'Áü≠ËßÜÈ¢ë': 75, 'Á§æ‰∫§Â∑•ÂÖ∑': 76, 'Á§æÂå∫ÊúçÂä°': 77, 'Á§æÂå∫Ë∂ÖÂ∏Ç': 78, 'ÁßüÊàø': 79, 'ÁßüËΩ¶': 80, 'Á¨îËÆ∞': 81, 'Á≠ñÁï•': 82, 'Á∫¶‰ºöÁ§æ‰∫§': 83, 'ÁªèËê•': 84, 'ÁªèËê•ÂÖªÊàê': 85, 'ÁªòÁîª': 86, 'ÁªºÂêàÈ¢ÑÂÆö': 87, 'ÁæéÂ¶ÜÁæé‰∏ö': 88, 'ÁæéÈ¢ú': 89, 'ËÅåËÄÉ': 90, 'ËÇ°Á•®': 91, 'Ëâ∫ÊúØ': 92, 'Ëã±ËØ≠': 93, 'ËèúË∞±': 94, 'ËñÖÁæäÊØõ': 95, 'Ë°åÁ®ãÁÆ°ÁêÜ': 96, 'Ë°åËΩ¶ËæÖÂä©': 97, 'Ë£Ö‰øÆÂÆ∂Â±Ö': 98, 'ËßÜÈ¢ë': 99, 'ËßÜÈ¢ëÊïôËÇ≤': 100, 'ËÆ∞Ë¥¶': 101, 'ËÆ∫ÂùõÂúàÂ≠ê': 102, 'ËØ≠Ë®Ä(ÈùûËã±ËØ≠)': 103, 'Ë¥≠Áâ©Âí®ËØ¢': 104, 'ËæÖÂä©Â∑•ÂÖ∑': 105, 'ËøêÂä®ÂÅ•Ë∫´': 106, 'ËøùÁ´†': 107, 'ÈÖíÂ∫ó': 108, 'ÈìÅË∑Ø': 109, 'Èì∂Ë°å': 110, 'ÈóÆÁ≠î‰∫§ÊµÅ': 111, 'ÈóÆËØäÊåÇÂè∑': 112, 'Èü≥‰πê': 113, 'È£ûË°åÁ©∫Êàò': 114, 'È§êÈ•ÆÂ∫ó': 115, 'È©æÊ†°': 116, 'È´òÁ≠âÊïôËÇ≤': 117, 'È≠îÂπª': 118}[0m
2022-08-31 22:41:02,267 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-31 22:41:02,463] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 22:41:02,463] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-31 22:41:02,463] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 22:41:02,464] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-31 22:41:02,464] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-31 22:41:02,464] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-31 22:41:02,464] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-31 22:41:02,464] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-31 22:41:02,464] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-31 22:41:02,464] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-31 22:41:02,464] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-31 22:41:02,464] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-31 22:41:02,464] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-31 22:41:02,464] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-31 22:41:02,464] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-31 22:41:02,465] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-31 22:41:02,465] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-31 22:41:02,465] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-31 22:41:02,465] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-31 22:41:02,465] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-31 22:41:02,465] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-31 22:41:02,465] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-31 22:41:02,465] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-31 22:41:02,465] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-31 22:41:02,465] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-31 22:41:02,465] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-31 22:41:02,465] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-31 22:41:02,465] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-31 22:41:02,466] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-31 22:41:02,466] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-31 22:41:02,466] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-31 22:41:02,466] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-31 22:41:02,466] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-31 22:41:02,466] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-31 22:41:02,466] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-31 22:41:02,466] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-31 22:41:02,466] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-31 22:41:02,466] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug31_22-40-52_instance-3bwob41y-01[0m
[32m[2022-08-31 22:41:02,466] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-31 22:41:02,466] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-31 22:41:02,466] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-31 22:41:02,467] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-31 22:41:02,467] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-31 22:41:02,467] [    INFO][0m - max_seq_length                :320[0m
[32m[2022-08-31 22:41:02,467] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-31 22:41:02,467] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-31 22:41:02,467] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-31 22:41:02,467] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-31 22:41:02,467] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-08-31 22:41:02,467] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-31 22:41:02,467] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-31 22:41:02,467] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-31 22:41:02,467] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-31 22:41:02,467] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-31 22:41:02,467] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-31 22:41:02,468] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-31 22:41:02,468] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-31 22:41:02,468] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-31 22:41:02,468] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-31 22:41:02,468] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-31 22:41:02,468] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-31 22:41:02,468] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-31 22:41:02,468] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-31 22:41:02,468] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-31 22:41:02,468] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-31 22:41:02,468] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-31 22:41:02,468] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-31 22:41:02,468] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-31 22:41:02,468] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-31 22:41:02,469] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-31 22:41:02,469] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-31 22:41:02,469] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-31 22:41:02,469] [    INFO][0m - seed                          :42[0m
[32m[2022-08-31 22:41:02,469] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-31 22:41:02,469] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-31 22:41:02,469] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-31 22:41:02,469] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-31 22:41:02,469] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-31 22:41:02,469] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-31 22:41:02,469] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-31 22:41:02,469] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-31 22:41:02,469] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-31 22:41:02,470] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-31 22:41:02,470] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-31 22:41:02,470] [    INFO][0m - [0m
[32m[2022-08-31 22:41:02,473] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-31 22:41:02,473] [    INFO][0m -   Num examples = 3024[0m
[32m[2022-08-31 22:41:02,473] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-08-31 22:41:02,473] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-31 22:41:02,473] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-31 22:41:02,473] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-31 22:41:02,473] [    INFO][0m -   Total optimization steps = 18900.0[0m
[32m[2022-08-31 22:41:02,473] [    INFO][0m -   Total num train samples = 151200[0m
[32m[2022-08-31 22:41:15,994] [    INFO][0m - loss: 4.01184578, learning_rate: 2.9984126984126986e-05, global_step: 10, interval_runtime: 13.5191, interval_samples_per_second: 0.592, interval_steps_per_second: 0.74, epoch: 0.0265[0m
[32m[2022-08-31 22:41:27,135] [    INFO][0m - loss: 3.01579037, learning_rate: 2.9968253968253967e-05, global_step: 20, interval_runtime: 11.1415, interval_samples_per_second: 0.718, interval_steps_per_second: 0.898, epoch: 0.0529[0m
[32m[2022-08-31 22:41:38,271] [    INFO][0m - loss: 3.36465378, learning_rate: 2.9952380952380952e-05, global_step: 30, interval_runtime: 11.1364, interval_samples_per_second: 0.718, interval_steps_per_second: 0.898, epoch: 0.0794[0m
[32m[2022-08-31 22:41:49,369] [    INFO][0m - loss: 2.67311268, learning_rate: 2.9936507936507937e-05, global_step: 40, interval_runtime: 11.0978, interval_samples_per_second: 0.721, interval_steps_per_second: 0.901, epoch: 0.1058[0m
[32m[2022-08-31 22:41:59,919] [    INFO][0m - loss: 2.98452988, learning_rate: 2.992063492063492e-05, global_step: 50, interval_runtime: 10.5489, interval_samples_per_second: 0.758, interval_steps_per_second: 0.948, epoch: 0.1323[0m
[32m[2022-08-31 22:42:11,276] [    INFO][0m - loss: 3.05699921, learning_rate: 2.9904761904761907e-05, global_step: 60, interval_runtime: 11.3576, interval_samples_per_second: 0.704, interval_steps_per_second: 0.88, epoch: 0.1587[0m
[32m[2022-08-31 22:42:24,810] [    INFO][0m - loss: 2.85227013, learning_rate: 2.9888888888888892e-05, global_step: 70, interval_runtime: 13.534, interval_samples_per_second: 0.591, interval_steps_per_second: 0.739, epoch: 0.1852[0m
[32m[2022-08-31 22:42:38,269] [    INFO][0m - loss: 2.44028549, learning_rate: 2.9873015873015874e-05, global_step: 80, interval_runtime: 13.4585, interval_samples_per_second: 0.594, interval_steps_per_second: 0.743, epoch: 0.2116[0m
[32m[2022-08-31 22:42:49,353] [    INFO][0m - loss: 2.56647968, learning_rate: 2.985714285714286e-05, global_step: 90, interval_runtime: 11.0845, interval_samples_per_second: 0.722, interval_steps_per_second: 0.902, epoch: 0.2381[0m
[32m[2022-08-31 22:43:00,499] [    INFO][0m - loss: 2.58823013, learning_rate: 2.984126984126984e-05, global_step: 100, interval_runtime: 11.1458, interval_samples_per_second: 0.718, interval_steps_per_second: 0.897, epoch: 0.2646[0m
[32m[2022-08-31 22:43:00,499] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 22:43:00,500] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-31 22:43:00,500] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 22:43:00,500] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 22:43:00,500] [    INFO][0m -   Total prediction steps = 43[0m
terminate called after throwing an instance of 'paddle::memory::allocation::BadAlloc'
  what():  

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::experimental::full_like(paddle::experimental::Tensor const&, paddle::experimental::ScalarBase<paddle::experimental::Tensor> const&, paddle::experimental::DataType, phi::Place const&)
1   void phi::FullLikeKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::ScalarBase<phi::DenseTensor> const&, paddle::experimental::DataType, phi::DenseTensor*)
2   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, paddle::experimental::DataType, unsigned long, bool) const
3   phi::DenseTensor::AllocateFrom(phi::Allocator*, paddle::experimental::DataType, unsigned long)
4   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
5   paddle::memory::allocation::Allocator::Allocate(unsigned long)
6   paddle::memory::allocation::Allocator::Allocate(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  phi::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 1.525879GB memory on GPU 0, 30.367920GB memory has been allocated and available memory is only 1.380615GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
If the above ways do not solve the out of memory problem, you can try to use CUDA managed memory. The command is `export FLAGS_use_cuda_managed_memory=false`.
 (at /paddle/paddle/fluid/memory/allocation/cuda_allocator.cc:95)

run.sh: line 72:   816 Aborted                 CUDA_VISIBLE_DEVICES=$device python train_single.py --output_dir ./checkpoints/ --prompt "$prompt" --max_seq_length $max_length --learning_rate 3e-5 --ppt_learning_rate 3e-4 --num_train_epochs 50 --logging_steps 10 --do_save True --do_test --eval_steps 100 --save_steps 100 --per_device_eval_batch_size 32 --per_device_train_batch_size $batch_size --model_name_or_path ernie-3.0-xbase-zh --split_id few_all --task_name $task_name --metric_for_best_model accuracy --disable_tqdm True --do_train --do_predict --do_eval --load_best_model_at_end
run.sh: line 78: --freeze_plm: command not found
 
==========
ocnli
==========
 
[33m[2022-08-31 22:43:16,225] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-31 22:43:16,437] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-31 22:43:16,438] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 22:43:16,438] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-31 22:43:16,438] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 22:43:16,438] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-31 22:43:16,438] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-31 22:43:16,438] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-08-31 22:43:16,438] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-31 22:43:16,438] [    INFO][0m - model_name_or_path            :ernie-3.0-xbase-zh[0m
[32m[2022-08-31 22:43:16,438] [    INFO][0m - [0m
[32m[2022-08-31 22:43:16,438] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 22:43:16,438] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-31 22:43:16,439] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 22:43:16,439] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-31 22:43:16,439] [    INFO][0m - prompt                        :‚Äú{'text':'text_a'}‚ÄùÂíå‚Äú{'text':'text_b'}‚Äù‰πãÈó¥ÁöÑÈÄªËæëÂÖ≥Á≥ªÊòØ{'mask'}{'mask'}„ÄÇ[0m
[32m[2022-08-31 22:43:16,439] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-31 22:43:16,439] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-31 22:43:16,439] [    INFO][0m - task_name                     :ocnli[0m
[32m[2022-08-31 22:43:16,439] [    INFO][0m - [0m
[32m[2022-08-31 22:43:16,439] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/ernie_3.0_xbase_zh.pdparams[0m
W0831 22:43:16.441341 15091 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0831 22:43:16.445888 15091 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-31 22:43:24,773] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/ernie_3.0_xbase_zh_vocab.txt[0m
[32m[2022-08-31 22:43:25,913] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/tokenizer_config.json[0m
[32m[2022-08-31 22:43:25,914] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/special_tokens_map.json[0m
[32m[2022-08-31 22:43:25,916] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùÂíå‚Äú'}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': '‚Äù‰πãÈó¥ÁöÑÈÄªËæëÂÖ≥Á≥ªÊòØ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': '„ÄÇ'}][0m
[32m[2022-08-31 22:43:25,930] [    INFO][0m - {'contradiction': 0, 'entailment': 1, 'neutral': 2}[0m
2022-08-31 22:43:25,932 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-31 22:43:26,070] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 22:43:26,070] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-31 22:43:26,070] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 22:43:26,070] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-31 22:43:26,070] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-31 22:43:26,070] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-31 22:43:26,071] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-31 22:43:26,071] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-31 22:43:26,071] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-31 22:43:26,071] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-31 22:43:26,071] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-31 22:43:26,071] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-31 22:43:26,071] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-31 22:43:26,071] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-31 22:43:26,071] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-31 22:43:26,072] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-31 22:43:26,072] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-31 22:43:26,072] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-31 22:43:26,072] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-31 22:43:26,072] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-31 22:43:26,072] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-31 22:43:26,072] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-31 22:43:26,072] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-31 22:43:26,072] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-31 22:43:26,072] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-31 22:43:26,072] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-31 22:43:26,072] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-31 22:43:26,073] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-31 22:43:26,073] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-31 22:43:26,073] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-31 22:43:26,073] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-31 22:43:26,073] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-31 22:43:26,073] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-31 22:43:26,073] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-31 22:43:26,073] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-31 22:43:26,073] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-31 22:43:26,073] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-31 22:43:26,073] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug31_22-43-16_instance-3bwob41y-01[0m
[32m[2022-08-31 22:43:26,073] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-31 22:43:26,073] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-31 22:43:26,074] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-31 22:43:26,074] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-31 22:43:26,074] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-31 22:43:26,074] [    INFO][0m - max_seq_length                :128[0m
[32m[2022-08-31 22:43:26,074] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-31 22:43:26,074] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-31 22:43:26,074] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-31 22:43:26,074] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-31 22:43:26,074] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-08-31 22:43:26,074] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-31 22:43:26,074] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-31 22:43:26,074] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-31 22:43:26,075] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-31 22:43:26,075] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-31 22:43:26,075] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-31 22:43:26,075] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-31 22:43:26,075] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-31 22:43:26,075] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-31 22:43:26,075] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-31 22:43:26,075] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-31 22:43:26,075] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-31 22:43:26,075] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-31 22:43:26,075] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-31 22:43:26,075] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-31 22:43:26,075] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-31 22:43:26,076] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-31 22:43:26,076] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-31 22:43:26,076] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-31 22:43:26,076] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-31 22:43:26,076] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-31 22:43:26,076] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-31 22:43:26,076] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-31 22:43:26,076] [    INFO][0m - seed                          :42[0m
[32m[2022-08-31 22:43:26,076] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-31 22:43:26,076] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-31 22:43:26,076] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-31 22:43:26,076] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-31 22:43:26,076] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-31 22:43:26,077] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-31 22:43:26,077] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-31 22:43:26,077] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-31 22:43:26,077] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-31 22:43:26,077] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-31 22:43:26,077] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-31 22:43:26,077] [    INFO][0m - [0m
[32m[2022-08-31 22:43:26,080] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-31 22:43:26,080] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 22:43:26,080] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-08-31 22:43:26,080] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-31 22:43:26,080] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-31 22:43:26,080] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-31 22:43:26,080] [    INFO][0m -   Total optimization steps = 1000.0[0m
[32m[2022-08-31 22:43:26,080] [    INFO][0m -   Total num train samples = 8000[0m
[32m[2022-08-31 22:43:33,984] [    INFO][0m - loss: 1.77706718, learning_rate: 2.97e-05, global_step: 10, interval_runtime: 7.902, interval_samples_per_second: 1.012, interval_steps_per_second: 1.266, epoch: 0.5[0m
[32m[2022-08-31 22:43:40,273] [    INFO][0m - loss: 1.43528328, learning_rate: 2.94e-05, global_step: 20, interval_runtime: 6.2896, interval_samples_per_second: 1.272, interval_steps_per_second: 1.59, epoch: 1.0[0m
[32m[2022-08-31 22:43:46,712] [    INFO][0m - loss: 1.28182354, learning_rate: 2.91e-05, global_step: 30, interval_runtime: 6.4392, interval_samples_per_second: 1.242, interval_steps_per_second: 1.553, epoch: 1.5[0m
[32m[2022-08-31 22:43:53,011] [    INFO][0m - loss: 1.21150599, learning_rate: 2.88e-05, global_step: 40, interval_runtime: 6.2986, interval_samples_per_second: 1.27, interval_steps_per_second: 1.588, epoch: 2.0[0m
[32m[2022-08-31 22:43:59,484] [    INFO][0m - loss: 1.25649614, learning_rate: 2.8499999999999998e-05, global_step: 50, interval_runtime: 6.4729, interval_samples_per_second: 1.236, interval_steps_per_second: 1.545, epoch: 2.5[0m
[32m[2022-08-31 22:44:04,711] [    INFO][0m - loss: 1.19595814, learning_rate: 2.8199999999999998e-05, global_step: 60, interval_runtime: 5.2269, interval_samples_per_second: 1.531, interval_steps_per_second: 1.913, epoch: 3.0[0m
[32m[2022-08-31 22:44:10,007] [    INFO][0m - loss: 1.17328587, learning_rate: 2.79e-05, global_step: 70, interval_runtime: 5.2962, interval_samples_per_second: 1.511, interval_steps_per_second: 1.888, epoch: 3.5[0m
[32m[2022-08-31 22:44:15,144] [    INFO][0m - loss: 1.23401823, learning_rate: 2.7600000000000003e-05, global_step: 80, interval_runtime: 5.1375, interval_samples_per_second: 1.557, interval_steps_per_second: 1.946, epoch: 4.0[0m
[32m[2022-08-31 22:44:20,389] [    INFO][0m - loss: 1.04074678, learning_rate: 2.7300000000000003e-05, global_step: 90, interval_runtime: 5.2447, interval_samples_per_second: 1.525, interval_steps_per_second: 1.907, epoch: 4.5[0m
[32m[2022-08-31 22:44:26,893] [    INFO][0m - loss: 1.30303431, learning_rate: 2.7000000000000002e-05, global_step: 100, interval_runtime: 5.176, interval_samples_per_second: 1.546, interval_steps_per_second: 1.932, epoch: 5.0[0m
[32m[2022-08-31 22:44:26,895] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 22:44:26,895] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 22:44:26,895] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 22:44:26,895] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 22:44:26,895] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 22:44:30,158] [    INFO][0m - eval_loss: 1.403017520904541, eval_accuracy: 0.375, eval_runtime: 3.263, eval_samples_per_second: 49.035, eval_steps_per_second: 1.532, epoch: 5.0[0m
[32m[2022-08-31 22:44:30,159] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-31 22:44:30,159] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 22:44:38,358] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-31 22:44:38,359] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-31 22:45:01,305] [    INFO][0m - loss: 0.98220606, learning_rate: 2.6700000000000002e-05, global_step: 110, interval_runtime: 35.7401, interval_samples_per_second: 0.224, interval_steps_per_second: 0.28, epoch: 5.5[0m
[32m[2022-08-31 22:45:07,570] [    INFO][0m - loss: 1.14646025, learning_rate: 2.64e-05, global_step: 120, interval_runtime: 6.2652, interval_samples_per_second: 1.277, interval_steps_per_second: 1.596, epoch: 6.0[0m
[32m[2022-08-31 22:45:13,983] [    INFO][0m - loss: 0.90110073, learning_rate: 2.61e-05, global_step: 130, interval_runtime: 6.4119, interval_samples_per_second: 1.248, interval_steps_per_second: 1.56, epoch: 6.5[0m
[32m[2022-08-31 22:45:20,244] [    INFO][0m - loss: 0.83178253, learning_rate: 2.58e-05, global_step: 140, interval_runtime: 6.2622, interval_samples_per_second: 1.278, interval_steps_per_second: 1.597, epoch: 7.0[0m
[32m[2022-08-31 22:45:25,612] [    INFO][0m - loss: 0.46993318, learning_rate: 2.55e-05, global_step: 150, interval_runtime: 5.3673, interval_samples_per_second: 1.49, interval_steps_per_second: 1.863, epoch: 7.5[0m
[32m[2022-08-31 22:45:30,719] [    INFO][0m - loss: 0.34979129, learning_rate: 2.52e-05, global_step: 160, interval_runtime: 5.1073, interval_samples_per_second: 1.566, interval_steps_per_second: 1.958, epoch: 8.0[0m
[32m[2022-08-31 22:45:35,987] [    INFO][0m - loss: 0.21969299, learning_rate: 2.49e-05, global_step: 170, interval_runtime: 5.2678, interval_samples_per_second: 1.519, interval_steps_per_second: 1.898, epoch: 8.5[0m
[32m[2022-08-31 22:45:41,143] [    INFO][0m - loss: 0.56301289, learning_rate: 2.4599999999999998e-05, global_step: 180, interval_runtime: 5.156, interval_samples_per_second: 1.552, interval_steps_per_second: 1.939, epoch: 9.0[0m
[32m[2022-08-31 22:45:46,469] [    INFO][0m - loss: 0.26409438, learning_rate: 2.43e-05, global_step: 190, interval_runtime: 5.326, interval_samples_per_second: 1.502, interval_steps_per_second: 1.878, epoch: 9.5[0m
[32m[2022-08-31 22:45:51,615] [    INFO][0m - loss: 0.32753706, learning_rate: 2.4e-05, global_step: 200, interval_runtime: 5.1465, interval_samples_per_second: 1.554, interval_steps_per_second: 1.943, epoch: 10.0[0m
[32m[2022-08-31 22:45:51,616] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 22:45:51,616] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 22:45:51,616] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 22:45:51,616] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 22:45:51,616] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 22:45:54,873] [    INFO][0m - eval_loss: 3.3444716930389404, eval_accuracy: 0.4, eval_runtime: 3.2561, eval_samples_per_second: 49.139, eval_steps_per_second: 1.536, epoch: 10.0[0m
[32m[2022-08-31 22:45:54,873] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-31 22:45:54,873] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 22:46:10,189] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-31 22:46:10,190] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-31 22:46:34,874] [    INFO][0m - loss: 0.08449795, learning_rate: 2.37e-05, global_step: 210, interval_runtime: 43.2589, interval_samples_per_second: 0.185, interval_steps_per_second: 0.231, epoch: 10.5[0m
[32m[2022-08-31 22:46:40,764] [    INFO][0m - loss: 0.16325942, learning_rate: 2.3400000000000003e-05, global_step: 220, interval_runtime: 5.8893, interval_samples_per_second: 1.358, interval_steps_per_second: 1.698, epoch: 11.0[0m
[32m[2022-08-31 22:46:46,059] [    INFO][0m - loss: 0.06433809, learning_rate: 2.3100000000000002e-05, global_step: 230, interval_runtime: 5.296, interval_samples_per_second: 1.511, interval_steps_per_second: 1.888, epoch: 11.5[0m
[32m[2022-08-31 22:46:51,140] [    INFO][0m - loss: 0.23459244, learning_rate: 2.2800000000000002e-05, global_step: 240, interval_runtime: 5.0803, interval_samples_per_second: 1.575, interval_steps_per_second: 1.968, epoch: 12.0[0m
[32m[2022-08-31 22:46:56,366] [    INFO][0m - loss: 0.02700521, learning_rate: 2.25e-05, global_step: 250, interval_runtime: 5.2257, interval_samples_per_second: 1.531, interval_steps_per_second: 1.914, epoch: 12.5[0m
[32m[2022-08-31 22:47:01,519] [    INFO][0m - loss: 0.07181628, learning_rate: 2.22e-05, global_step: 260, interval_runtime: 5.1531, interval_samples_per_second: 1.552, interval_steps_per_second: 1.941, epoch: 13.0[0m
[32m[2022-08-31 22:47:06,771] [    INFO][0m - loss: 0.02479313, learning_rate: 2.19e-05, global_step: 270, interval_runtime: 5.2521, interval_samples_per_second: 1.523, interval_steps_per_second: 1.904, epoch: 13.5[0m
[32m[2022-08-31 22:47:11,945] [    INFO][0m - loss: 0.08221893, learning_rate: 2.16e-05, global_step: 280, interval_runtime: 5.1738, interval_samples_per_second: 1.546, interval_steps_per_second: 1.933, epoch: 14.0[0m
[32m[2022-08-31 22:47:21,524] [    INFO][0m - loss: 0.09084761, learning_rate: 2.13e-05, global_step: 290, interval_runtime: 9.5795, interval_samples_per_second: 0.835, interval_steps_per_second: 1.044, epoch: 14.5[0m
[32m[2022-08-31 22:47:27,516] [    INFO][0m - loss: 0.02527102, learning_rate: 2.1e-05, global_step: 300, interval_runtime: 5.1507, interval_samples_per_second: 1.553, interval_steps_per_second: 1.941, epoch: 15.0[0m
[32m[2022-08-31 22:47:27,517] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 22:47:27,517] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 22:47:27,517] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 22:47:27,517] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 22:47:27,517] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 22:47:32,537] [    INFO][0m - eval_loss: 7.382301330566406, eval_accuracy: 0.39375, eval_runtime: 3.2398, eval_samples_per_second: 49.386, eval_steps_per_second: 1.543, epoch: 15.0[0m
[32m[2022-08-31 22:47:32,538] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-31 22:47:32,538] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 22:47:40,278] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-31 22:47:40,279] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-31 22:48:00,010] [    INFO][0m - loss: 0.01759996, learning_rate: 2.07e-05, global_step: 310, interval_runtime: 33.3349, interval_samples_per_second: 0.24, interval_steps_per_second: 0.3, epoch: 15.5[0m
[32m[2022-08-31 22:48:09,458] [    INFO][0m - loss: 0.00144603, learning_rate: 2.04e-05, global_step: 320, interval_runtime: 5.1208, interval_samples_per_second: 1.562, interval_steps_per_second: 1.953, epoch: 16.0[0m
[32m[2022-08-31 22:48:14,710] [    INFO][0m - loss: 5.53e-06, learning_rate: 2.01e-05, global_step: 330, interval_runtime: 9.5795, interval_samples_per_second: 0.835, interval_steps_per_second: 1.044, epoch: 16.5[0m
[32m[2022-08-31 22:48:19,874] [    INFO][0m - loss: 0.00015784, learning_rate: 1.98e-05, global_step: 340, interval_runtime: 5.1638, interval_samples_per_second: 1.549, interval_steps_per_second: 1.937, epoch: 17.0[0m
[32m[2022-08-31 22:48:25,193] [    INFO][0m - loss: 4.643e-05, learning_rate: 1.95e-05, global_step: 350, interval_runtime: 5.3185, interval_samples_per_second: 1.504, interval_steps_per_second: 1.88, epoch: 17.5[0m
[32m[2022-08-31 22:48:30,361] [    INFO][0m - loss: 0.04439152, learning_rate: 1.9200000000000003e-05, global_step: 360, interval_runtime: 5.1691, interval_samples_per_second: 1.548, interval_steps_per_second: 1.935, epoch: 18.0[0m
[32m[2022-08-31 22:48:35,631] [    INFO][0m - loss: 0.00160304, learning_rate: 1.8900000000000002e-05, global_step: 370, interval_runtime: 5.269, interval_samples_per_second: 1.518, interval_steps_per_second: 1.898, epoch: 18.5[0m
[32m[2022-08-31 22:48:40,843] [    INFO][0m - loss: 5.29e-06, learning_rate: 1.86e-05, global_step: 380, interval_runtime: 5.2131, interval_samples_per_second: 1.535, interval_steps_per_second: 1.918, epoch: 19.0[0m
[32m[2022-08-31 22:48:46,109] [    INFO][0m - loss: 5.683e-05, learning_rate: 1.83e-05, global_step: 390, interval_runtime: 5.2654, interval_samples_per_second: 1.519, interval_steps_per_second: 1.899, epoch: 19.5[0m
[32m[2022-08-31 22:48:51,239] [    INFO][0m - loss: 2.001e-05, learning_rate: 1.8e-05, global_step: 400, interval_runtime: 5.1301, interval_samples_per_second: 1.559, interval_steps_per_second: 1.949, epoch: 20.0[0m
[32m[2022-08-31 22:48:51,240] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 22:48:51,240] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 22:48:51,240] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 22:48:51,240] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 22:48:51,240] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 22:48:54,350] [    INFO][0m - eval_loss: 8.37675952911377, eval_accuracy: 0.38125, eval_runtime: 3.1091, eval_samples_per_second: 51.462, eval_steps_per_second: 1.608, epoch: 20.0[0m
[32m[2022-08-31 22:48:54,350] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-31 22:48:54,350] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 22:49:02,211] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-31 22:49:02,211] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-31 22:49:23,081] [    INFO][0m - loss: 0.00011475, learning_rate: 1.77e-05, global_step: 410, interval_runtime: 31.8422, interval_samples_per_second: 0.251, interval_steps_per_second: 0.314, epoch: 20.5[0m
[32m[2022-08-31 22:49:29,338] [    INFO][0m - loss: 3e-08, learning_rate: 1.74e-05, global_step: 420, interval_runtime: 6.2571, interval_samples_per_second: 1.279, interval_steps_per_second: 1.598, epoch: 21.0[0m
[32m[2022-08-31 22:49:35,347] [    INFO][0m - loss: 0.00114653, learning_rate: 1.71e-05, global_step: 430, interval_runtime: 6.0085, interval_samples_per_second: 1.331, interval_steps_per_second: 1.664, epoch: 21.5[0m
[32m[2022-08-31 22:49:40,482] [    INFO][0m - loss: 1.29e-06, learning_rate: 1.6800000000000002e-05, global_step: 440, interval_runtime: 5.135, interval_samples_per_second: 1.558, interval_steps_per_second: 1.947, epoch: 22.0[0m
[32m[2022-08-31 22:49:45,756] [    INFO][0m - loss: 0.18625913, learning_rate: 1.65e-05, global_step: 450, interval_runtime: 5.2739, interval_samples_per_second: 1.517, interval_steps_per_second: 1.896, epoch: 22.5[0m
[32m[2022-08-31 22:49:50,924] [    INFO][0m - loss: 3.99e-06, learning_rate: 1.62e-05, global_step: 460, interval_runtime: 5.1676, interval_samples_per_second: 1.548, interval_steps_per_second: 1.935, epoch: 23.0[0m
[32m[2022-08-31 22:49:56,210] [    INFO][0m - loss: 0.00136915, learning_rate: 1.59e-05, global_step: 470, interval_runtime: 5.2856, interval_samples_per_second: 1.514, interval_steps_per_second: 1.892, epoch: 23.5[0m
[32m[2022-08-31 22:50:01,401] [    INFO][0m - loss: 0.00027209, learning_rate: 1.56e-05, global_step: 480, interval_runtime: 5.191, interval_samples_per_second: 1.541, interval_steps_per_second: 1.926, epoch: 24.0[0m
[32m[2022-08-31 22:50:06,651] [    INFO][0m - loss: 7.33e-06, learning_rate: 1.53e-05, global_step: 490, interval_runtime: 5.2509, interval_samples_per_second: 1.524, interval_steps_per_second: 1.904, epoch: 24.5[0m
[32m[2022-08-31 22:50:11,904] [    INFO][0m - loss: 1e-08, learning_rate: 1.5e-05, global_step: 500, interval_runtime: 5.1948, interval_samples_per_second: 1.54, interval_steps_per_second: 1.925, epoch: 25.0[0m
[32m[2022-08-31 22:50:11,904] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 22:50:11,904] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 22:50:11,904] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 22:50:11,905] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 22:50:11,905] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 22:50:15,215] [    INFO][0m - eval_loss: 8.469453811645508, eval_accuracy: 0.43125, eval_runtime: 3.3107, eval_samples_per_second: 48.328, eval_steps_per_second: 1.51, epoch: 25.0[0m
[32m[2022-08-31 22:50:15,216] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-08-31 22:50:15,216] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 22:50:24,791] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-08-31 22:50:24,792] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-08-31 22:50:54,031] [    INFO][0m - loss: 5e-08, learning_rate: 1.47e-05, global_step: 510, interval_runtime: 42.1849, interval_samples_per_second: 0.19, interval_steps_per_second: 0.237, epoch: 25.5[0m
[32m[2022-08-31 22:50:59,123] [    INFO][0m - loss: 1.52e-06, learning_rate: 1.44e-05, global_step: 520, interval_runtime: 5.0916, interval_samples_per_second: 1.571, interval_steps_per_second: 1.964, epoch: 26.0[0m
[32m[2022-08-31 22:51:04,432] [    INFO][0m - loss: 5.1e-07, learning_rate: 1.4099999999999999e-05, global_step: 530, interval_runtime: 5.3102, interval_samples_per_second: 1.507, interval_steps_per_second: 1.883, epoch: 26.5[0m
[32m[2022-08-31 22:51:09,595] [    INFO][0m - loss: 0.00389107, learning_rate: 1.3800000000000002e-05, global_step: 540, interval_runtime: 5.1627, interval_samples_per_second: 1.55, interval_steps_per_second: 1.937, epoch: 27.0[0m
[32m[2022-08-31 22:51:14,849] [    INFO][0m - loss: 0.2385685, learning_rate: 1.3500000000000001e-05, global_step: 550, interval_runtime: 5.2541, interval_samples_per_second: 1.523, interval_steps_per_second: 1.903, epoch: 27.5[0m
[32m[2022-08-31 22:51:20,021] [    INFO][0m - loss: 1.574e-05, learning_rate: 1.32e-05, global_step: 560, interval_runtime: 5.1711, interval_samples_per_second: 1.547, interval_steps_per_second: 1.934, epoch: 28.0[0m
[32m[2022-08-31 22:51:25,313] [    INFO][0m - loss: 0.04850522, learning_rate: 1.29e-05, global_step: 570, interval_runtime: 5.2921, interval_samples_per_second: 1.512, interval_steps_per_second: 1.89, epoch: 28.5[0m
[32m[2022-08-31 22:51:30,481] [    INFO][0m - loss: 1.92e-05, learning_rate: 1.26e-05, global_step: 580, interval_runtime: 5.1688, interval_samples_per_second: 1.548, interval_steps_per_second: 1.935, epoch: 29.0[0m
[32m[2022-08-31 22:51:35,598] [    INFO][0m - loss: 0.00347418, learning_rate: 1.2299999999999999e-05, global_step: 590, interval_runtime: 5.1171, interval_samples_per_second: 1.563, interval_steps_per_second: 1.954, epoch: 29.5[0m
[32m[2022-08-31 22:51:40,750] [    INFO][0m - loss: 0.01528234, learning_rate: 1.2e-05, global_step: 600, interval_runtime: 5.1515, interval_samples_per_second: 1.553, interval_steps_per_second: 1.941, epoch: 30.0[0m
[32m[2022-08-31 22:51:40,751] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 22:51:40,751] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 22:51:40,751] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 22:51:40,751] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 22:51:40,751] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 22:51:44,001] [    INFO][0m - eval_loss: 9.433088302612305, eval_accuracy: 0.38125, eval_runtime: 3.2495, eval_samples_per_second: 49.239, eval_steps_per_second: 1.539, epoch: 30.0[0m
[32m[2022-08-31 22:51:44,001] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-08-31 22:51:44,001] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 22:51:51,366] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-08-31 22:51:51,366] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-08-31 22:52:11,246] [    INFO][0m - loss: 0.00828827, learning_rate: 1.1700000000000001e-05, global_step: 610, interval_runtime: 30.4956, interval_samples_per_second: 0.262, interval_steps_per_second: 0.328, epoch: 30.5[0m
[32m[2022-08-31 22:52:17,408] [    INFO][0m - loss: 4.36e-06, learning_rate: 1.1400000000000001e-05, global_step: 620, interval_runtime: 6.1622, interval_samples_per_second: 1.298, interval_steps_per_second: 1.623, epoch: 31.0[0m
[32m[2022-08-31 22:52:22,645] [    INFO][0m - loss: 0.01058358, learning_rate: 1.11e-05, global_step: 630, interval_runtime: 5.2371, interval_samples_per_second: 1.528, interval_steps_per_second: 1.909, epoch: 31.5[0m
[32m[2022-08-31 22:52:27,798] [    INFO][0m - loss: 1e-08, learning_rate: 1.08e-05, global_step: 640, interval_runtime: 5.1529, interval_samples_per_second: 1.553, interval_steps_per_second: 1.941, epoch: 32.0[0m
[32m[2022-08-31 22:52:33,059] [    INFO][0m - loss: 1.3e-07, learning_rate: 1.05e-05, global_step: 650, interval_runtime: 5.261, interval_samples_per_second: 1.521, interval_steps_per_second: 1.901, epoch: 32.5[0m
[32m[2022-08-31 22:52:38,222] [    INFO][0m - loss: 2e-08, learning_rate: 1.02e-05, global_step: 660, interval_runtime: 5.1629, interval_samples_per_second: 1.55, interval_steps_per_second: 1.937, epoch: 33.0[0m
[32m[2022-08-31 22:52:43,554] [    INFO][0m - loss: 0.00027908, learning_rate: 9.9e-06, global_step: 670, interval_runtime: 5.3322, interval_samples_per_second: 1.5, interval_steps_per_second: 1.875, epoch: 33.5[0m
[32m[2022-08-31 22:52:48,731] [    INFO][0m - loss: 3.2e-07, learning_rate: 9.600000000000001e-06, global_step: 680, interval_runtime: 5.1771, interval_samples_per_second: 1.545, interval_steps_per_second: 1.932, epoch: 34.0[0m
[32m[2022-08-31 22:52:53,951] [    INFO][0m - loss: 6e-08, learning_rate: 9.3e-06, global_step: 690, interval_runtime: 5.22, interval_samples_per_second: 1.533, interval_steps_per_second: 1.916, epoch: 34.5[0m
[32m[2022-08-31 22:52:57,311] [    INFO][0m - loss: 5e-08, learning_rate: 9e-06, global_step: 700, interval_runtime: 3.3596, interval_samples_per_second: 2.381, interval_steps_per_second: 2.977, epoch: 35.0[0m
[32m[2022-08-31 22:52:57,311] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 22:52:57,311] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 22:52:57,311] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 22:52:57,311] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 22:52:57,312] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 22:52:59,434] [    INFO][0m - eval_loss: 9.281294822692871, eval_accuracy: 0.36875, eval_runtime: 2.1222, eval_samples_per_second: 75.393, eval_steps_per_second: 2.356, epoch: 35.0[0m
[32m[2022-08-31 22:52:59,434] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-700[0m
[32m[2022-08-31 22:52:59,434] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 22:53:07,081] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-700/tokenizer_config.json[0m
[32m[2022-08-31 22:53:07,081] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-700/special_tokens_map.json[0m
[32m[2022-08-31 22:53:25,584] [    INFO][0m - loss: 6.3e-07, learning_rate: 8.7e-06, global_step: 710, interval_runtime: 28.273, interval_samples_per_second: 0.283, interval_steps_per_second: 0.354, epoch: 35.5[0m
[32m[2022-08-31 22:53:30,196] [    INFO][0m - loss: 0.00139191, learning_rate: 8.400000000000001e-06, global_step: 720, interval_runtime: 4.6116, interval_samples_per_second: 1.735, interval_steps_per_second: 2.168, epoch: 36.0[0m
[32m[2022-08-31 22:53:33,873] [    INFO][0m - loss: 3e-08, learning_rate: 8.1e-06, global_step: 730, interval_runtime: 3.6777, interval_samples_per_second: 2.175, interval_steps_per_second: 2.719, epoch: 36.5[0m
[32m[2022-08-31 22:53:37,054] [    INFO][0m - loss: 4e-08, learning_rate: 7.8e-06, global_step: 740, interval_runtime: 3.1809, interval_samples_per_second: 2.515, interval_steps_per_second: 3.144, epoch: 37.0[0m
[32m[2022-08-31 22:53:40,482] [    INFO][0m - loss: 1.95e-06, learning_rate: 7.5e-06, global_step: 750, interval_runtime: 3.4282, interval_samples_per_second: 2.334, interval_steps_per_second: 2.917, epoch: 37.5[0m
[32m[2022-08-31 22:53:43,912] [    INFO][0m - loss: 1.7e-07, learning_rate: 7.2e-06, global_step: 760, interval_runtime: 3.4297, interval_samples_per_second: 2.333, interval_steps_per_second: 2.916, epoch: 38.0[0m
[32m[2022-08-31 22:53:47,396] [    INFO][0m - loss: 4e-08, learning_rate: 6.900000000000001e-06, global_step: 770, interval_runtime: 3.4846, interval_samples_per_second: 2.296, interval_steps_per_second: 2.87, epoch: 38.5[0m
[32m[2022-08-31 22:53:50,811] [    INFO][0m - loss: 8.786e-05, learning_rate: 6.6e-06, global_step: 780, interval_runtime: 3.4147, interval_samples_per_second: 2.343, interval_steps_per_second: 2.929, epoch: 39.0[0m
[32m[2022-08-31 22:53:54,175] [    INFO][0m - loss: 0.0002446, learning_rate: 6.3e-06, global_step: 790, interval_runtime: 3.364, interval_samples_per_second: 2.378, interval_steps_per_second: 2.973, epoch: 39.5[0m
[32m[2022-08-31 22:53:57,427] [    INFO][0m - loss: 0.3582376, learning_rate: 6e-06, global_step: 800, interval_runtime: 3.2517, interval_samples_per_second: 2.46, interval_steps_per_second: 3.075, epoch: 40.0[0m
[32m[2022-08-31 22:53:57,428] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 22:53:57,428] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 22:53:57,428] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 22:53:57,428] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 22:53:57,428] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 22:54:00,341] [    INFO][0m - eval_loss: 9.467733383178711, eval_accuracy: 0.3875, eval_runtime: 2.9126, eval_samples_per_second: 54.933, eval_steps_per_second: 1.717, epoch: 40.0[0m
[32m[2022-08-31 22:54:00,342] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-800[0m
[32m[2022-08-31 22:54:00,342] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 22:54:12,767] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-800/tokenizer_config.json[0m
[32m[2022-08-31 22:54:12,768] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-800/special_tokens_map.json[0m
[32m[2022-08-31 22:54:33,727] [    INFO][0m - loss: 4.513e-05, learning_rate: 5.7000000000000005e-06, global_step: 810, interval_runtime: 36.2999, interval_samples_per_second: 0.22, interval_steps_per_second: 0.275, epoch: 40.5[0m
[32m[2022-08-31 22:54:38,934] [    INFO][0m - loss: 1e-08, learning_rate: 5.4e-06, global_step: 820, interval_runtime: 5.2074, interval_samples_per_second: 1.536, interval_steps_per_second: 1.92, epoch: 41.0[0m
[32m[2022-08-31 22:54:45,402] [    INFO][0m - loss: 5.13e-06, learning_rate: 5.1e-06, global_step: 830, interval_runtime: 6.4682, interval_samples_per_second: 1.237, interval_steps_per_second: 1.546, epoch: 41.5[0m
[32m[2022-08-31 22:54:50,794] [    INFO][0m - loss: 8e-07, learning_rate: 4.800000000000001e-06, global_step: 840, interval_runtime: 5.3914, interval_samples_per_second: 1.484, interval_steps_per_second: 1.855, epoch: 42.0[0m
[32m[2022-08-31 22:54:56,096] [    INFO][0m - loss: 0.00041871, learning_rate: 4.5e-06, global_step: 850, interval_runtime: 5.3018, interval_samples_per_second: 1.509, interval_steps_per_second: 1.886, epoch: 42.5[0m
[32m[2022-08-31 22:55:01,206] [    INFO][0m - loss: 3e-08, learning_rate: 4.2000000000000004e-06, global_step: 860, interval_runtime: 5.1108, interval_samples_per_second: 1.565, interval_steps_per_second: 1.957, epoch: 43.0[0m
[32m[2022-08-31 22:55:06,464] [    INFO][0m - loss: 2.11e-06, learning_rate: 3.9e-06, global_step: 870, interval_runtime: 5.2574, interval_samples_per_second: 1.522, interval_steps_per_second: 1.902, epoch: 43.5[0m
[32m[2022-08-31 22:55:11,616] [    INFO][0m - loss: 1.21e-06, learning_rate: 3.6e-06, global_step: 880, interval_runtime: 5.1522, interval_samples_per_second: 1.553, interval_steps_per_second: 1.941, epoch: 44.0[0m
[32m[2022-08-31 22:55:16,920] [    INFO][0m - loss: 6.17e-06, learning_rate: 3.3e-06, global_step: 890, interval_runtime: 5.3043, interval_samples_per_second: 1.508, interval_steps_per_second: 1.885, epoch: 44.5[0m
[32m[2022-08-31 22:55:26,876] [    INFO][0m - loss: 0.03759277, learning_rate: 3e-06, global_step: 900, interval_runtime: 5.103, interval_samples_per_second: 1.568, interval_steps_per_second: 1.96, epoch: 45.0[0m
[32m[2022-08-31 22:55:26,876] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 22:55:26,876] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 22:55:26,876] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 22:55:26,876] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 22:55:26,877] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 22:55:30,140] [    INFO][0m - eval_loss: 9.129314422607422, eval_accuracy: 0.43125, eval_runtime: 3.2629, eval_samples_per_second: 49.036, eval_steps_per_second: 1.532, epoch: 45.0[0m
[32m[2022-08-31 22:55:30,140] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-900[0m
[32m[2022-08-31 22:55:30,140] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 22:55:38,253] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-900/tokenizer_config.json[0m
[32m[2022-08-31 22:55:38,254] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-900/special_tokens_map.json[0m
[32m[2022-08-31 22:55:52,608] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-31 22:55:52,609] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-500 (score: 0.43125).[0m
[32m[2022-08-31 22:55:55,922] [    INFO][0m - train_runtime: 749.841, train_samples_per_second: 10.669, train_steps_per_second: 1.334, train_loss: 0.231276986624722, epoch: 45.0[0m
[32m[2022-08-31 22:55:55,924] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-31 22:55:55,924] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 22:56:04,506] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-31 22:56:06,218] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-31 22:56:06,220] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-31 22:56:06,220] [    INFO][0m -   epoch                    =       45.0[0m
[32m[2022-08-31 22:56:06,220] [    INFO][0m -   train_loss               =     0.2313[0m
[32m[2022-08-31 22:56:06,220] [    INFO][0m -   train_runtime            = 0:12:29.84[0m
[32m[2022-08-31 22:56:06,221] [    INFO][0m -   train_samples_per_second =     10.669[0m
[32m[2022-08-31 22:56:06,221] [    INFO][0m -   train_steps_per_second   =      1.334[0m
[32m[2022-08-31 22:56:06,225] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-31 22:56:06,226] [    INFO][0m -   Num examples = 2520[0m
[32m[2022-08-31 22:56:06,226] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 22:56:06,226] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 22:56:06,226] [    INFO][0m -   Total prediction steps = 79[0m
[32m[2022-08-31 22:56:59,636] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-31 22:56:59,637] [    INFO][0m -   test_accuracy           =     0.3706[0m
[32m[2022-08-31 22:56:59,637] [    INFO][0m -   test_loss               =     9.0658[0m
[32m[2022-08-31 22:56:59,637] [    INFO][0m -   test_runtime            = 0:00:53.41[0m
[32m[2022-08-31 22:56:59,637] [    INFO][0m -   test_samples_per_second =     47.182[0m
[32m[2022-08-31 22:56:59,637] [    INFO][0m -   test_steps_per_second   =      1.479[0m
[32m[2022-08-31 22:56:59,637] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-31 22:56:59,638] [    INFO][0m -   Num examples = 3000[0m
[32m[2022-08-31 22:56:59,638] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 22:56:59,638] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 22:56:59,638] [    INFO][0m -   Total prediction steps = 94[0m
[32m[2022-08-31 22:58:07,682] [    INFO][0m - Predictions for ocnlif saved to ./fewclue_submit_examples.[0m
[]
run.sh: line 78: --freeze_plm: command not found
 
==========
bustm
==========
 
[33m[2022-08-31 22:58:12,280] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-31 22:58:12,281] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-31 22:58:12,281] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 22:58:12,281] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-31 22:58:12,281] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 22:58:12,281] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-31 22:58:12,282] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-31 22:58:12,282] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-08-31 22:58:12,282] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-31 22:58:12,282] [    INFO][0m - model_name_or_path            :ernie-3.0-xbase-zh[0m
[32m[2022-08-31 22:58:12,282] [    INFO][0m - [0m
[32m[2022-08-31 22:58:12,282] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 22:58:12,282] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-31 22:58:12,282] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 22:58:12,282] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-31 22:58:12,282] [    INFO][0m - prompt                        :‚Äú{'text':'text_a'}‚ÄùÂíå‚Äú{'text':'text_b'}‚Äù‰πãÈó¥ÁöÑÈÄªËæëÂÖ≥Á≥ªÊòØ{'mask'}{'mask'}„ÄÇ[0m
[32m[2022-08-31 22:58:12,282] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-31 22:58:12,282] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-31 22:58:12,282] [    INFO][0m - task_name                     :bustm[0m
[32m[2022-08-31 22:58:12,282] [    INFO][0m - [0m
[32m[2022-08-31 22:58:12,283] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/ernie_3.0_xbase_zh.pdparams[0m
W0831 22:58:12.284394 28818 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0831 22:58:12.288676 28818 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-31 22:58:20,417] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/ernie_3.0_xbase_zh_vocab.txt[0m
[32m[2022-08-31 22:58:20,447] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/tokenizer_config.json[0m
[32m[2022-08-31 22:58:20,448] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/special_tokens_map.json[0m
[32m[2022-08-31 22:58:20,449] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùÂíå‚Äú'}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': '‚Äù‰πãÈó¥ÁöÑÈÄªËæëÂÖ≥Á≥ªÊòØ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': '„ÄÇ'}][0m
[32m[2022-08-31 22:58:20,465] [    INFO][0m - {'0': 0, '1': 1}[0m
2022-08-31 22:58:20,467 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-31 22:58:20,587] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 22:58:20,587] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-31 22:58:20,587] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 22:58:20,587] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-31 22:58:20,588] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-31 22:58:20,588] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-31 22:58:20,588] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-31 22:58:20,588] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-31 22:58:20,588] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-31 22:58:20,588] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-31 22:58:20,588] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-31 22:58:20,588] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-31 22:58:20,588] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-31 22:58:20,588] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-31 22:58:20,588] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-31 22:58:20,589] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-31 22:58:20,589] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-31 22:58:20,589] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-31 22:58:20,589] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-31 22:58:20,589] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-31 22:58:20,589] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-31 22:58:20,589] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-31 22:58:20,589] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-31 22:58:20,589] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-31 22:58:20,589] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-31 22:58:20,589] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-31 22:58:20,589] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-31 22:58:20,589] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-31 22:58:20,590] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-31 22:58:20,590] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-31 22:58:20,590] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-31 22:58:20,590] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-31 22:58:20,590] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-31 22:58:20,590] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-31 22:58:20,590] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-31 22:58:20,590] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-31 22:58:20,590] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-31 22:58:20,590] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug31_22-58-12_instance-3bwob41y-01[0m
[32m[2022-08-31 22:58:20,590] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-31 22:58:20,590] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-31 22:58:20,590] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-31 22:58:20,590] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-31 22:58:20,591] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-31 22:58:20,591] [    INFO][0m - max_seq_length                :64[0m
[32m[2022-08-31 22:58:20,591] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-31 22:58:20,591] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-31 22:58:20,591] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-31 22:58:20,591] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-31 22:58:20,591] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-08-31 22:58:20,591] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-31 22:58:20,591] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-31 22:58:20,591] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-31 22:58:20,591] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-31 22:58:20,591] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-31 22:58:20,591] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-31 22:58:20,592] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-31 22:58:20,592] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-31 22:58:20,592] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-31 22:58:20,592] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-31 22:58:20,592] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-31 22:58:20,592] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-31 22:58:20,592] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-31 22:58:20,592] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-31 22:58:20,592] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-31 22:58:20,592] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-31 22:58:20,592] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-31 22:58:20,592] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-31 22:58:20,592] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-31 22:58:20,593] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-31 22:58:20,593] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-31 22:58:20,593] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-31 22:58:20,593] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-31 22:58:20,593] [    INFO][0m - seed                          :42[0m
[32m[2022-08-31 22:58:20,593] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-31 22:58:20,593] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-31 22:58:20,593] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-31 22:58:20,593] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-31 22:58:20,593] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-31 22:58:20,593] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-31 22:58:20,593] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-31 22:58:20,593] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-31 22:58:20,593] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-31 22:58:20,594] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-31 22:58:20,594] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-31 22:58:20,594] [    INFO][0m - [0m
[32m[2022-08-31 22:58:20,597] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-31 22:58:20,597] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 22:58:20,597] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-08-31 22:58:20,597] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-31 22:58:20,597] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-31 22:58:20,597] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-31 22:58:20,597] [    INFO][0m -   Total optimization steps = 1000.0[0m
[32m[2022-08-31 22:58:20,598] [    INFO][0m -   Total num train samples = 8000[0m
[32m[2022-08-31 22:58:26,322] [    INFO][0m - loss: 1.08716812, learning_rate: 2.97e-05, global_step: 10, interval_runtime: 5.7231, interval_samples_per_second: 1.398, interval_steps_per_second: 1.747, epoch: 0.5[0m
[32m[2022-08-31 22:58:29,401] [    INFO][0m - loss: 0.79287186, learning_rate: 2.94e-05, global_step: 20, interval_runtime: 3.079, interval_samples_per_second: 2.598, interval_steps_per_second: 3.248, epoch: 1.0[0m
[32m[2022-08-31 22:58:32,530] [    INFO][0m - loss: 0.7194335, learning_rate: 2.91e-05, global_step: 30, interval_runtime: 3.1296, interval_samples_per_second: 2.556, interval_steps_per_second: 3.195, epoch: 1.5[0m
[32m[2022-08-31 22:58:35,604] [    INFO][0m - loss: 0.72222738, learning_rate: 2.88e-05, global_step: 40, interval_runtime: 3.0739, interval_samples_per_second: 2.603, interval_steps_per_second: 3.253, epoch: 2.0[0m
[32m[2022-08-31 22:58:38,609] [    INFO][0m - loss: 0.64284153, learning_rate: 2.8499999999999998e-05, global_step: 50, interval_runtime: 3.0055, interval_samples_per_second: 2.662, interval_steps_per_second: 3.327, epoch: 2.5[0m
[32m[2022-08-31 22:58:42,328] [    INFO][0m - loss: 0.63559785, learning_rate: 2.8199999999999998e-05, global_step: 60, interval_runtime: 3.7186, interval_samples_per_second: 2.151, interval_steps_per_second: 2.689, epoch: 3.0[0m
[32m[2022-08-31 22:58:46,237] [    INFO][0m - loss: 0.80030289, learning_rate: 2.79e-05, global_step: 70, interval_runtime: 3.9085, interval_samples_per_second: 2.047, interval_steps_per_second: 2.559, epoch: 3.5[0m
[32m[2022-08-31 22:58:49,967] [    INFO][0m - loss: 0.70517688, learning_rate: 2.7600000000000003e-05, global_step: 80, interval_runtime: 3.7303, interval_samples_per_second: 2.145, interval_steps_per_second: 2.681, epoch: 4.0[0m
[32m[2022-08-31 22:58:53,873] [    INFO][0m - loss: 0.57835712, learning_rate: 2.7300000000000003e-05, global_step: 90, interval_runtime: 3.9054, interval_samples_per_second: 2.048, interval_steps_per_second: 2.561, epoch: 4.5[0m
[32m[2022-08-31 22:58:57,588] [    INFO][0m - loss: 0.34420738, learning_rate: 2.7000000000000002e-05, global_step: 100, interval_runtime: 3.7158, interval_samples_per_second: 2.153, interval_steps_per_second: 2.691, epoch: 5.0[0m
[32m[2022-08-31 22:58:57,589] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 22:58:57,589] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 22:58:57,590] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 22:58:57,590] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 22:58:57,590] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 22:58:59,745] [    INFO][0m - eval_loss: 1.6899505853652954, eval_accuracy: 0.68125, eval_runtime: 2.1548, eval_samples_per_second: 74.252, eval_steps_per_second: 2.32, epoch: 5.0[0m
[32m[2022-08-31 22:58:59,746] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-31 22:58:59,747] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 22:59:07,294] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-31 22:59:07,295] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-31 22:59:25,043] [    INFO][0m - loss: 0.41245179, learning_rate: 2.6700000000000002e-05, global_step: 110, interval_runtime: 27.4546, interval_samples_per_second: 0.291, interval_steps_per_second: 0.364, epoch: 5.5[0m
[32m[2022-08-31 22:59:28,061] [    INFO][0m - loss: 0.16884875, learning_rate: 2.64e-05, global_step: 120, interval_runtime: 3.0179, interval_samples_per_second: 2.651, interval_steps_per_second: 3.314, epoch: 6.0[0m
[32m[2022-08-31 22:59:31,261] [    INFO][0m - loss: 0.59414454, learning_rate: 2.61e-05, global_step: 130, interval_runtime: 3.2003, interval_samples_per_second: 2.5, interval_steps_per_second: 3.125, epoch: 6.5[0m
[32m[2022-08-31 22:59:34,356] [    INFO][0m - loss: 0.47192636, learning_rate: 2.58e-05, global_step: 140, interval_runtime: 3.0958, interval_samples_per_second: 2.584, interval_steps_per_second: 3.23, epoch: 7.0[0m
[32m[2022-08-31 22:59:37,484] [    INFO][0m - loss: 0.28332593, learning_rate: 2.55e-05, global_step: 150, interval_runtime: 3.127, interval_samples_per_second: 2.558, interval_steps_per_second: 3.198, epoch: 7.5[0m
[32m[2022-08-31 22:59:40,556] [    INFO][0m - loss: 0.04584443, learning_rate: 2.52e-05, global_step: 160, interval_runtime: 3.0721, interval_samples_per_second: 2.604, interval_steps_per_second: 3.255, epoch: 8.0[0m
[32m[2022-08-31 22:59:43,752] [    INFO][0m - loss: 0.10045691, learning_rate: 2.49e-05, global_step: 170, interval_runtime: 3.1959, interval_samples_per_second: 2.503, interval_steps_per_second: 3.129, epoch: 8.5[0m
[32m[2022-08-31 22:59:46,807] [    INFO][0m - loss: 0.0577136, learning_rate: 2.4599999999999998e-05, global_step: 180, interval_runtime: 3.0545, interval_samples_per_second: 2.619, interval_steps_per_second: 3.274, epoch: 9.0[0m
[32m[2022-08-31 22:59:49,999] [    INFO][0m - loss: 0.2736135, learning_rate: 2.43e-05, global_step: 190, interval_runtime: 3.1918, interval_samples_per_second: 2.506, interval_steps_per_second: 3.133, epoch: 9.5[0m
[32m[2022-08-31 22:59:53,032] [    INFO][0m - loss: 0.14628124, learning_rate: 2.4e-05, global_step: 200, interval_runtime: 3.0339, interval_samples_per_second: 2.637, interval_steps_per_second: 3.296, epoch: 10.0[0m
[32m[2022-08-31 22:59:53,033] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 22:59:53,033] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 22:59:53,033] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 22:59:53,033] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 22:59:53,033] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 22:59:54,857] [    INFO][0m - eval_loss: 4.184718608856201, eval_accuracy: 0.68125, eval_runtime: 1.824, eval_samples_per_second: 87.719, eval_steps_per_second: 2.741, epoch: 10.0[0m
[32m[2022-08-31 22:59:54,858] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-31 22:59:54,858] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 23:00:01,518] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-31 23:00:01,518] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-31 23:00:20,466] [    INFO][0m - loss: 0.03169977, learning_rate: 2.37e-05, global_step: 210, interval_runtime: 27.4339, interval_samples_per_second: 0.292, interval_steps_per_second: 0.365, epoch: 10.5[0m
[32m[2022-08-31 23:00:24,200] [    INFO][0m - loss: 0.03171306, learning_rate: 2.3400000000000003e-05, global_step: 220, interval_runtime: 3.7337, interval_samples_per_second: 2.143, interval_steps_per_second: 2.678, epoch: 11.0[0m
[32m[2022-08-31 23:00:27,606] [    INFO][0m - loss: 0.07827036, learning_rate: 2.3100000000000002e-05, global_step: 230, interval_runtime: 3.4063, interval_samples_per_second: 2.349, interval_steps_per_second: 2.936, epoch: 11.5[0m
[32m[2022-08-31 23:00:30,676] [    INFO][0m - loss: 0.05975059, learning_rate: 2.2800000000000002e-05, global_step: 240, interval_runtime: 3.0702, interval_samples_per_second: 2.606, interval_steps_per_second: 3.257, epoch: 12.0[0m
[32m[2022-08-31 23:00:33,866] [    INFO][0m - loss: 0.10781322, learning_rate: 2.25e-05, global_step: 250, interval_runtime: 3.1898, interval_samples_per_second: 2.508, interval_steps_per_second: 3.135, epoch: 12.5[0m
[32m[2022-08-31 23:00:36,921] [    INFO][0m - loss: 0.03146216, learning_rate: 2.22e-05, global_step: 260, interval_runtime: 3.0556, interval_samples_per_second: 2.618, interval_steps_per_second: 3.273, epoch: 13.0[0m
[32m[2022-08-31 23:00:40,114] [    INFO][0m - loss: 0.04830034, learning_rate: 2.19e-05, global_step: 270, interval_runtime: 3.1925, interval_samples_per_second: 2.506, interval_steps_per_second: 3.132, epoch: 13.5[0m
[32m[2022-08-31 23:00:43,175] [    INFO][0m - loss: 9.364e-05, learning_rate: 2.16e-05, global_step: 280, interval_runtime: 3.0608, interval_samples_per_second: 2.614, interval_steps_per_second: 3.267, epoch: 14.0[0m
[32m[2022-08-31 23:00:46,384] [    INFO][0m - loss: 0.00036697, learning_rate: 2.13e-05, global_step: 290, interval_runtime: 3.2089, interval_samples_per_second: 2.493, interval_steps_per_second: 3.116, epoch: 14.5[0m
[32m[2022-08-31 23:00:49,468] [    INFO][0m - loss: 1.58e-06, learning_rate: 2.1e-05, global_step: 300, interval_runtime: 3.0844, interval_samples_per_second: 2.594, interval_steps_per_second: 3.242, epoch: 15.0[0m
[32m[2022-08-31 23:00:49,469] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 23:00:49,469] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 23:00:49,469] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 23:00:49,469] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 23:00:49,469] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 23:00:51,337] [    INFO][0m - eval_loss: 5.025351047515869, eval_accuracy: 0.65, eval_runtime: 1.8673, eval_samples_per_second: 85.684, eval_steps_per_second: 2.678, epoch: 15.0[0m
[32m[2022-08-31 23:00:51,337] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-31 23:00:51,337] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 23:01:03,669] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-31 23:01:03,670] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-31 23:01:29,563] [    INFO][0m - loss: 3.58e-06, learning_rate: 2.07e-05, global_step: 310, interval_runtime: 40.0951, interval_samples_per_second: 0.2, interval_steps_per_second: 0.249, epoch: 15.5[0m
[32m[2022-08-31 23:01:33,302] [    INFO][0m - loss: 2.89e-06, learning_rate: 2.04e-05, global_step: 320, interval_runtime: 3.7383, interval_samples_per_second: 2.14, interval_steps_per_second: 2.675, epoch: 16.0[0m
[32m[2022-08-31 23:01:37,172] [    INFO][0m - loss: 0.01870621, learning_rate: 2.01e-05, global_step: 330, interval_runtime: 3.8701, interval_samples_per_second: 2.067, interval_steps_per_second: 2.584, epoch: 16.5[0m
[32m[2022-08-31 23:01:40,964] [    INFO][0m - loss: 0.04732057, learning_rate: 1.98e-05, global_step: 340, interval_runtime: 3.792, interval_samples_per_second: 2.11, interval_steps_per_second: 2.637, epoch: 17.0[0m
[32m[2022-08-31 23:01:45,575] [    INFO][0m - loss: 3.136e-05, learning_rate: 1.95e-05, global_step: 350, interval_runtime: 3.6927, interval_samples_per_second: 2.166, interval_steps_per_second: 2.708, epoch: 17.5[0m
[32m[2022-08-31 23:01:48,660] [    INFO][0m - loss: 8.52e-06, learning_rate: 1.9200000000000003e-05, global_step: 360, interval_runtime: 4.0031, interval_samples_per_second: 1.998, interval_steps_per_second: 2.498, epoch: 18.0[0m
[32m[2022-08-31 23:01:52,655] [    INFO][0m - loss: 0.00036733, learning_rate: 1.8900000000000002e-05, global_step: 370, interval_runtime: 3.2213, interval_samples_per_second: 2.483, interval_steps_per_second: 3.104, epoch: 18.5[0m
[32m[2022-08-31 23:01:55,713] [    INFO][0m - loss: 2.442e-05, learning_rate: 1.86e-05, global_step: 380, interval_runtime: 3.8324, interval_samples_per_second: 2.087, interval_steps_per_second: 2.609, epoch: 19.0[0m
[32m[2022-08-31 23:02:00,612] [    INFO][0m - loss: 0.10875484, learning_rate: 1.83e-05, global_step: 390, interval_runtime: 3.2151, interval_samples_per_second: 2.488, interval_steps_per_second: 3.11, epoch: 19.5[0m
[32m[2022-08-31 23:02:03,680] [    INFO][0m - loss: 6.6e-07, learning_rate: 1.8e-05, global_step: 400, interval_runtime: 4.7519, interval_samples_per_second: 1.684, interval_steps_per_second: 2.104, epoch: 20.0[0m
[32m[2022-08-31 23:02:03,681] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 23:02:03,681] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 23:02:03,681] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 23:02:03,681] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 23:02:03,681] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 23:02:05,499] [    INFO][0m - eval_loss: 5.42112922668457, eval_accuracy: 0.7, eval_runtime: 1.8172, eval_samples_per_second: 88.046, eval_steps_per_second: 2.751, epoch: 20.0[0m
[32m[2022-08-31 23:02:05,499] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-31 23:02:05,499] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 23:02:13,216] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-31 23:02:13,216] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-31 23:02:34,506] [    INFO][0m - loss: 1.063e-05, learning_rate: 1.77e-05, global_step: 410, interval_runtime: 30.8252, interval_samples_per_second: 0.26, interval_steps_per_second: 0.324, epoch: 20.5[0m
[32m[2022-08-31 23:02:37,272] [    INFO][0m - loss: 6.2e-06, learning_rate: 1.74e-05, global_step: 420, interval_runtime: 2.7664, interval_samples_per_second: 2.892, interval_steps_per_second: 3.615, epoch: 21.0[0m
[32m[2022-08-31 23:02:42,345] [    INFO][0m - loss: 7.5e-07, learning_rate: 1.71e-05, global_step: 430, interval_runtime: 5.0735, interval_samples_per_second: 1.577, interval_steps_per_second: 1.971, epoch: 21.5[0m
[32m[2022-08-31 23:02:45,066] [    INFO][0m - loss: 7.375e-05, learning_rate: 1.6800000000000002e-05, global_step: 440, interval_runtime: 2.7205, interval_samples_per_second: 2.941, interval_steps_per_second: 3.676, epoch: 22.0[0m
[32m[2022-08-31 23:02:48,558] [    INFO][0m - loss: 8.5e-07, learning_rate: 1.65e-05, global_step: 450, interval_runtime: 3.4922, interval_samples_per_second: 2.291, interval_steps_per_second: 2.864, epoch: 22.5[0m
[32m[2022-08-31 23:02:52,310] [    INFO][0m - loss: 9e-07, learning_rate: 1.62e-05, global_step: 460, interval_runtime: 3.7524, interval_samples_per_second: 2.132, interval_steps_per_second: 2.665, epoch: 23.0[0m
[32m[2022-08-31 23:02:56,259] [    INFO][0m - loss: 1.6e-07, learning_rate: 1.59e-05, global_step: 470, interval_runtime: 3.9459, interval_samples_per_second: 2.027, interval_steps_per_second: 2.534, epoch: 23.5[0m
[32m[2022-08-31 23:03:00,046] [    INFO][0m - loss: 3.4e-07, learning_rate: 1.56e-05, global_step: 480, interval_runtime: 3.7894, interval_samples_per_second: 2.111, interval_steps_per_second: 2.639, epoch: 24.0[0m
[32m[2022-08-31 23:03:03,987] [    INFO][0m - loss: 3.8e-07, learning_rate: 1.53e-05, global_step: 490, interval_runtime: 3.9409, interval_samples_per_second: 2.03, interval_steps_per_second: 2.537, epoch: 24.5[0m
[32m[2022-08-31 23:03:07,749] [    INFO][0m - loss: 3.1e-07, learning_rate: 1.5e-05, global_step: 500, interval_runtime: 3.762, interval_samples_per_second: 2.127, interval_steps_per_second: 2.658, epoch: 25.0[0m
[32m[2022-08-31 23:03:07,750] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 23:03:07,750] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 23:03:07,750] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 23:03:07,750] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 23:03:07,750] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 23:03:09,961] [    INFO][0m - eval_loss: 5.416335582733154, eval_accuracy: 0.69375, eval_runtime: 2.2108, eval_samples_per_second: 72.373, eval_steps_per_second: 2.262, epoch: 25.0[0m
[32m[2022-08-31 23:03:09,961] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-08-31 23:03:09,961] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 23:03:22,044] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-08-31 23:03:22,044] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-08-31 23:03:43,861] [    INFO][0m - loss: 1.37e-06, learning_rate: 1.47e-05, global_step: 510, interval_runtime: 36.1122, interval_samples_per_second: 0.222, interval_steps_per_second: 0.277, epoch: 25.5[0m
[32m[2022-08-31 23:03:46,947] [    INFO][0m - loss: 0.10785872, learning_rate: 1.44e-05, global_step: 520, interval_runtime: 3.0861, interval_samples_per_second: 2.592, interval_steps_per_second: 3.24, epoch: 26.0[0m
[32m[2022-08-31 23:03:50,139] [    INFO][0m - loss: 0.00075833, learning_rate: 1.4099999999999999e-05, global_step: 530, interval_runtime: 3.1923, interval_samples_per_second: 2.506, interval_steps_per_second: 3.133, epoch: 26.5[0m
[32m[2022-08-31 23:03:53,174] [    INFO][0m - loss: 2.6e-07, learning_rate: 1.3800000000000002e-05, global_step: 540, interval_runtime: 3.0349, interval_samples_per_second: 2.636, interval_steps_per_second: 3.295, epoch: 27.0[0m
[32m[2022-08-31 23:03:56,411] [    INFO][0m - loss: 1.46e-06, learning_rate: 1.3500000000000001e-05, global_step: 550, interval_runtime: 3.237, interval_samples_per_second: 2.471, interval_steps_per_second: 3.089, epoch: 27.5[0m
[32m[2022-08-31 23:03:59,500] [    INFO][0m - loss: 1.8e-07, learning_rate: 1.32e-05, global_step: 560, interval_runtime: 3.0889, interval_samples_per_second: 2.59, interval_steps_per_second: 3.237, epoch: 28.0[0m
[32m[2022-08-31 23:04:02,672] [    INFO][0m - loss: 4e-07, learning_rate: 1.29e-05, global_step: 570, interval_runtime: 3.1715, interval_samples_per_second: 2.522, interval_steps_per_second: 3.153, epoch: 28.5[0m
[32m[2022-08-31 23:04:05,656] [    INFO][0m - loss: 8e-08, learning_rate: 1.26e-05, global_step: 580, interval_runtime: 2.984, interval_samples_per_second: 2.681, interval_steps_per_second: 3.351, epoch: 29.0[0m
[32m[2022-08-31 23:04:09,564] [    INFO][0m - loss: 1.5e-07, learning_rate: 1.2299999999999999e-05, global_step: 590, interval_runtime: 3.9085, interval_samples_per_second: 2.047, interval_steps_per_second: 2.559, epoch: 29.5[0m
[32m[2022-08-31 23:04:13,254] [    INFO][0m - loss: 0.0222524, learning_rate: 1.2e-05, global_step: 600, interval_runtime: 3.6895, interval_samples_per_second: 2.168, interval_steps_per_second: 2.71, epoch: 30.0[0m
[32m[2022-08-31 23:04:13,254] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 23:04:13,254] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 23:04:13,254] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 23:04:13,254] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 23:04:13,255] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 23:04:15,507] [    INFO][0m - eval_loss: 5.277956962585449, eval_accuracy: 0.69375, eval_runtime: 2.252, eval_samples_per_second: 71.048, eval_steps_per_second: 2.22, epoch: 30.0[0m
[32m[2022-08-31 23:04:15,507] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-08-31 23:04:15,507] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 23:04:22,998] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-08-31 23:04:23,251] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-08-31 23:04:39,777] [    INFO][0m - loss: 5.6e-07, learning_rate: 1.1700000000000001e-05, global_step: 610, interval_runtime: 26.5239, interval_samples_per_second: 0.302, interval_steps_per_second: 0.377, epoch: 30.5[0m
[32m[2022-08-31 23:04:42,854] [    INFO][0m - loss: 2.2e-07, learning_rate: 1.1400000000000001e-05, global_step: 620, interval_runtime: 3.0764, interval_samples_per_second: 2.6, interval_steps_per_second: 3.251, epoch: 31.0[0m
[32m[2022-08-31 23:04:46,022] [    INFO][0m - loss: 9.5e-07, learning_rate: 1.11e-05, global_step: 630, interval_runtime: 3.1676, interval_samples_per_second: 2.526, interval_steps_per_second: 3.157, epoch: 31.5[0m
[32m[2022-08-31 23:04:49,104] [    INFO][0m - loss: 7.5e-07, learning_rate: 1.08e-05, global_step: 640, interval_runtime: 3.0828, interval_samples_per_second: 2.595, interval_steps_per_second: 3.244, epoch: 32.0[0m
[32m[2022-08-31 23:04:52,261] [    INFO][0m - loss: 2.68e-06, learning_rate: 1.05e-05, global_step: 650, interval_runtime: 3.1569, interval_samples_per_second: 2.534, interval_steps_per_second: 3.168, epoch: 32.5[0m
[32m[2022-08-31 23:04:55,317] [    INFO][0m - loss: 2.9e-07, learning_rate: 1.02e-05, global_step: 660, interval_runtime: 3.0555, interval_samples_per_second: 2.618, interval_steps_per_second: 3.273, epoch: 33.0[0m
[32m[2022-08-31 23:04:58,504] [    INFO][0m - loss: 4e-07, learning_rate: 9.9e-06, global_step: 670, interval_runtime: 3.1875, interval_samples_per_second: 2.51, interval_steps_per_second: 3.137, epoch: 33.5[0m
[32m[2022-08-31 23:05:01,529] [    INFO][0m - loss: 1.29e-06, learning_rate: 9.600000000000001e-06, global_step: 680, interval_runtime: 3.0248, interval_samples_per_second: 2.645, interval_steps_per_second: 3.306, epoch: 34.0[0m
[32m[2022-08-31 23:05:04,745] [    INFO][0m - loss: 3.1e-07, learning_rate: 9.3e-06, global_step: 690, interval_runtime: 3.2156, interval_samples_per_second: 2.488, interval_steps_per_second: 3.11, epoch: 34.5[0m
[32m[2022-08-31 23:05:07,772] [    INFO][0m - loss: 0.01034974, learning_rate: 9e-06, global_step: 700, interval_runtime: 3.0273, interval_samples_per_second: 2.643, interval_steps_per_second: 3.303, epoch: 35.0[0m
[32m[2022-08-31 23:05:07,773] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 23:05:07,773] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 23:05:07,773] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 23:05:07,773] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 23:05:07,773] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 23:05:09,535] [    INFO][0m - eval_loss: 5.483853340148926, eval_accuracy: 0.69375, eval_runtime: 1.7617, eval_samples_per_second: 90.821, eval_steps_per_second: 2.838, epoch: 35.0[0m
[32m[2022-08-31 23:05:09,535] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-700[0m
[32m[2022-08-31 23:05:09,535] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 23:05:17,139] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-700/tokenizer_config.json[0m
[32m[2022-08-31 23:05:17,140] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-700/special_tokens_map.json[0m
[32m[2022-08-31 23:05:34,518] [    INFO][0m - loss: 1.9e-07, learning_rate: 8.7e-06, global_step: 710, interval_runtime: 26.746, interval_samples_per_second: 0.299, interval_steps_per_second: 0.374, epoch: 35.5[0m
[32m[2022-08-31 23:05:38,269] [    INFO][0m - loss: 6e-07, learning_rate: 8.400000000000001e-06, global_step: 720, interval_runtime: 3.7502, interval_samples_per_second: 2.133, interval_steps_per_second: 2.667, epoch: 36.0[0m
[32m[2022-08-31 23:05:42,142] [    INFO][0m - loss: 1.1e-07, learning_rate: 8.1e-06, global_step: 730, interval_runtime: 3.8721, interval_samples_per_second: 2.066, interval_steps_per_second: 2.583, epoch: 36.5[0m
[32m[2022-08-31 23:05:45,810] [    INFO][0m - loss: 9e-08, learning_rate: 7.8e-06, global_step: 740, interval_runtime: 3.668, interval_samples_per_second: 2.181, interval_steps_per_second: 2.726, epoch: 37.0[0m
[32m[2022-08-31 23:05:49,722] [    INFO][0m - loss: 1.8e-07, learning_rate: 7.5e-06, global_step: 750, interval_runtime: 3.9133, interval_samples_per_second: 2.044, interval_steps_per_second: 2.555, epoch: 37.5[0m
[32m[2022-08-31 23:05:53,352] [    INFO][0m - loss: 1.7e-07, learning_rate: 7.2e-06, global_step: 760, interval_runtime: 3.6298, interval_samples_per_second: 2.204, interval_steps_per_second: 2.755, epoch: 38.0[0m
[32m[2022-08-31 23:05:56,535] [    INFO][0m - loss: 1e-07, learning_rate: 6.900000000000001e-06, global_step: 770, interval_runtime: 3.1828, interval_samples_per_second: 2.514, interval_steps_per_second: 3.142, epoch: 38.5[0m
[32m[2022-08-31 23:05:59,601] [    INFO][0m - loss: 4.4e-07, learning_rate: 6.6e-06, global_step: 780, interval_runtime: 3.0666, interval_samples_per_second: 2.609, interval_steps_per_second: 3.261, epoch: 39.0[0m
[32m[2022-08-31 23:06:02,821] [    INFO][0m - loss: 7e-08, learning_rate: 6.3e-06, global_step: 790, interval_runtime: 3.2201, interval_samples_per_second: 2.484, interval_steps_per_second: 3.105, epoch: 39.5[0m
[32m[2022-08-31 23:06:05,887] [    INFO][0m - loss: 7e-08, learning_rate: 6e-06, global_step: 800, interval_runtime: 3.0652, interval_samples_per_second: 2.61, interval_steps_per_second: 3.262, epoch: 40.0[0m
[32m[2022-08-31 23:06:05,887] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 23:06:05,888] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 23:06:05,888] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 23:06:05,888] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 23:06:05,888] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 23:06:07,725] [    INFO][0m - eval_loss: 5.101541042327881, eval_accuracy: 0.68125, eval_runtime: 1.8373, eval_samples_per_second: 87.084, eval_steps_per_second: 2.721, epoch: 40.0[0m
[32m[2022-08-31 23:06:07,726] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-800[0m
[32m[2022-08-31 23:06:07,726] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 23:06:15,446] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-800/tokenizer_config.json[0m
[32m[2022-08-31 23:06:15,446] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-800/special_tokens_map.json[0m
[32m[2022-08-31 23:06:30,811] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-31 23:06:30,812] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-400 (score: 0.7).[0m
[32m[2022-08-31 23:06:34,296] [    INFO][0m - train_runtime: 493.6968, train_samples_per_second: 16.204, train_steps_per_second: 2.026, train_loss: 0.1286101305886627, epoch: 40.0[0m
[32m[2022-08-31 23:06:34,419] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-31 23:06:34,420] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 23:06:42,564] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-31 23:06:42,565] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-31 23:06:42,568] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-31 23:06:42,568] [    INFO][0m -   epoch                    =       40.0[0m
[32m[2022-08-31 23:06:42,568] [    INFO][0m -   train_loss               =     0.1286[0m
[32m[2022-08-31 23:06:42,568] [    INFO][0m -   train_runtime            = 0:08:13.69[0m
[32m[2022-08-31 23:06:42,568] [    INFO][0m -   train_samples_per_second =     16.204[0m
[32m[2022-08-31 23:06:42,568] [    INFO][0m -   train_steps_per_second   =      2.026[0m
[32m[2022-08-31 23:06:42,574] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-31 23:06:42,574] [    INFO][0m -   Num examples = 1772[0m
[32m[2022-08-31 23:06:42,574] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 23:06:42,574] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 23:06:42,574] [    INFO][0m -   Total prediction steps = 56[0m
[32m[2022-08-31 23:07:06,953] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-31 23:07:06,954] [    INFO][0m -   test_accuracy           =     0.6817[0m
[32m[2022-08-31 23:07:06,954] [    INFO][0m -   test_loss               =     5.7205[0m
[32m[2022-08-31 23:07:06,954] [    INFO][0m -   test_runtime            = 0:00:24.37[0m
[32m[2022-08-31 23:07:06,954] [    INFO][0m -   test_samples_per_second =     72.685[0m
[32m[2022-08-31 23:07:06,954] [    INFO][0m -   test_steps_per_second   =      2.297[0m
[32m[2022-08-31 23:07:06,955] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-31 23:07:06,955] [    INFO][0m -   Num examples = 2000[0m
[32m[2022-08-31 23:07:06,955] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 23:07:06,955] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 23:07:06,955] [    INFO][0m -   Total prediction steps = 63[0m
[32m[2022-08-31 23:07:31,070] [    INFO][0m - Predictions for bustm saved to ./fewclue_submit_examples.[0m
[]
run.sh: line 78: --freeze_plm: command not found
 
==========
chid
==========
 
[33m[2022-08-31 23:07:36,473] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-31 23:07:36,473] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-31 23:07:36,474] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 23:07:36,474] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-31 23:07:36,474] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 23:07:36,474] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-31 23:07:36,474] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-31 23:07:36,474] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-08-31 23:07:36,474] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-31 23:07:36,474] [    INFO][0m - model_name_or_path            :ernie-3.0-xbase-zh[0m
[32m[2022-08-31 23:07:36,474] [    INFO][0m - [0m
[32m[2022-08-31 23:07:36,474] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 23:07:36,475] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-31 23:07:36,475] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 23:07:36,475] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-31 23:07:36,475] [    INFO][0m - prompt                        :{'text':'text_a'}{'sep'}{'hard':'ËøôÂè•ËØù‰∏≠ÁöÑÊàêËØ≠‰ΩøÁî®'}{'mask'}{'mask'}[0m
[32m[2022-08-31 23:07:36,475] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-31 23:07:36,475] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-31 23:07:36,475] [    INFO][0m - task_name                     :chid[0m
[32m[2022-08-31 23:07:36,475] [    INFO][0m - [0m
[32m[2022-08-31 23:07:36,475] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/ernie_3.0_xbase_zh.pdparams[0m
W0831 23:07:36.476949  9125 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0831 23:07:36.481034  9125 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-31 23:07:42,136] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/ernie_3.0_xbase_zh_vocab.txt[0m
[32m[2022-08-31 23:07:42,162] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/tokenizer_config.json[0m
[32m[2022-08-31 23:07:42,162] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/special_tokens_map.json[0m
[32m[2022-08-31 23:07:42,183] [    INFO][0m - Using template: [{'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'sep': None}, {'add_prefix_space': '', 'hard': 'ËøôÂè•ËØù‰∏≠ÁöÑÊàêËØ≠‰ΩøÁî®'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}][0m
[32m[2022-08-31 23:07:42,189] [    INFO][0m - {0: 0, 1: 1}[0m
2022-08-31 23:07:42,191 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-31 23:07:42,492] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 23:07:42,492] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-31 23:07:42,492] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 23:07:42,493] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-31 23:07:42,493] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-31 23:07:42,493] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-31 23:07:42,493] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-31 23:07:42,493] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-31 23:07:42,493] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-31 23:07:42,493] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-31 23:07:42,493] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-31 23:07:42,493] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-31 23:07:42,493] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-31 23:07:42,494] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-31 23:07:42,494] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-31 23:07:42,494] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-31 23:07:42,494] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-31 23:07:42,494] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-31 23:07:42,494] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-31 23:07:42,494] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-31 23:07:42,494] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-31 23:07:42,494] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-31 23:07:42,494] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-31 23:07:42,494] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-31 23:07:42,494] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-31 23:07:42,494] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-31 23:07:42,494] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-31 23:07:42,495] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-31 23:07:42,495] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-31 23:07:42,495] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-31 23:07:42,495] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-31 23:07:42,495] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-31 23:07:42,495] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-31 23:07:42,495] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-31 23:07:42,495] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-31 23:07:42,495] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-31 23:07:42,495] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-31 23:07:42,495] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug31_23-07-36_instance-3bwob41y-01[0m
[32m[2022-08-31 23:07:42,495] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-31 23:07:42,495] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-31 23:07:42,496] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-31 23:07:42,496] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-31 23:07:42,496] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-31 23:07:42,496] [    INFO][0m - max_seq_length                :256[0m
[32m[2022-08-31 23:07:42,496] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-31 23:07:42,496] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-31 23:07:42,496] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-31 23:07:42,496] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-31 23:07:42,496] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-08-31 23:07:42,496] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-31 23:07:42,496] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-31 23:07:42,496] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-31 23:07:42,496] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-31 23:07:42,497] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-31 23:07:42,497] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-31 23:07:42,497] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-31 23:07:42,497] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-31 23:07:42,497] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-31 23:07:42,497] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-31 23:07:42,497] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-31 23:07:42,498] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-31 23:07:42,498] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-31 23:07:42,498] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-31 23:07:42,498] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-31 23:07:42,498] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-31 23:07:42,498] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-31 23:07:42,498] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-31 23:07:42,498] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-31 23:07:42,498] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-31 23:07:42,498] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-31 23:07:42,498] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-31 23:07:42,499] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-31 23:07:42,499] [    INFO][0m - seed                          :42[0m
[32m[2022-08-31 23:07:42,499] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-31 23:07:42,499] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-31 23:07:42,499] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-31 23:07:42,499] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-31 23:07:42,499] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-31 23:07:42,499] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-31 23:07:42,499] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-31 23:07:42,499] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-31 23:07:42,499] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-31 23:07:42,499] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-31 23:07:42,499] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-31 23:07:42,500] [    INFO][0m - [0m
[32m[2022-08-31 23:07:42,502] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-31 23:07:42,502] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-08-31 23:07:42,502] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-08-31 23:07:42,502] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-31 23:07:42,502] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-31 23:07:42,502] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-31 23:07:42,502] [    INFO][0m -   Total optimization steps = 8850.0[0m
[32m[2022-08-31 23:07:42,502] [    INFO][0m -   Total num train samples = 70700[0m
[32m[2022-08-31 23:07:53,032] [    INFO][0m - loss: 1.1750308, learning_rate: 2.9966101694915256e-05, global_step: 10, interval_runtime: 7.7212, interval_samples_per_second: 1.036, interval_steps_per_second: 1.295, epoch: 0.0565[0m
[32m[2022-08-31 23:08:00,350] [    INFO][0m - loss: 0.5123167, learning_rate: 2.9932203389830508e-05, global_step: 20, interval_runtime: 10.1244, interval_samples_per_second: 0.79, interval_steps_per_second: 0.988, epoch: 0.113[0m
[32m[2022-08-31 23:08:08,525] [    INFO][0m - loss: 0.51115685, learning_rate: 2.9898305084745767e-05, global_step: 30, interval_runtime: 8.1754, interval_samples_per_second: 0.979, interval_steps_per_second: 1.223, epoch: 0.1695[0m
[32m[2022-08-31 23:08:17,042] [    INFO][0m - loss: 0.49028211, learning_rate: 2.986440677966102e-05, global_step: 40, interval_runtime: 8.5171, interval_samples_per_second: 0.939, interval_steps_per_second: 1.174, epoch: 0.226[0m
[32m[2022-08-31 23:08:24,500] [    INFO][0m - loss: 0.47716484, learning_rate: 2.9830508474576274e-05, global_step: 50, interval_runtime: 7.4581, interval_samples_per_second: 1.073, interval_steps_per_second: 1.341, epoch: 0.2825[0m
[32m[2022-08-31 23:08:30,689] [    INFO][0m - loss: 0.25058327, learning_rate: 2.9796610169491526e-05, global_step: 60, interval_runtime: 6.189, interval_samples_per_second: 1.293, interval_steps_per_second: 1.616, epoch: 0.339[0m
[32m[2022-08-31 23:08:37,158] [    INFO][0m - loss: 0.85516396, learning_rate: 2.976271186440678e-05, global_step: 70, interval_runtime: 6.4697, interval_samples_per_second: 1.237, interval_steps_per_second: 1.546, epoch: 0.3955[0m
[32m[2022-08-31 23:08:44,646] [    INFO][0m - loss: 0.90493183, learning_rate: 2.9728813559322033e-05, global_step: 80, interval_runtime: 7.4881, interval_samples_per_second: 1.068, interval_steps_per_second: 1.335, epoch: 0.452[0m
[32m[2022-08-31 23:08:54,029] [    INFO][0m - loss: 0.64861789, learning_rate: 2.9694915254237292e-05, global_step: 90, interval_runtime: 9.382, interval_samples_per_second: 0.853, interval_steps_per_second: 1.066, epoch: 0.5085[0m
[32m[2022-08-31 23:09:03,464] [    INFO][0m - loss: 0.45872383, learning_rate: 2.9661016949152544e-05, global_step: 100, interval_runtime: 9.4357, interval_samples_per_second: 0.848, interval_steps_per_second: 1.06, epoch: 0.565[0m
[32m[2022-08-31 23:09:03,465] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 23:09:03,465] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-08-31 23:09:03,465] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 23:09:03,465] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 23:09:03,465] [    INFO][0m -   Total prediction steps = 45[0m
terminate called after throwing an instance of 'paddle::memory::allocation::BadAlloc'
  what():  

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   add_final_state_dygraph_function(paddle::experimental::Tensor const&, paddle::experimental::Tensor const&)
1   paddle::experimental::add(paddle::experimental::Tensor const&, paddle::experimental::Tensor const&)
2   void phi::AddRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*)
3   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, paddle::experimental::DataType, unsigned long, bool) const
4   phi::DenseTensor::AllocateFrom(phi::Allocator*, paddle::experimental::DataType, unsigned long)
5   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
6   paddle::memory::allocation::Allocator::Allocate(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
10  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
11  phi::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 1.220703GB memory on GPU 0, 30.850342GB memory has been allocated and available memory is only 919.750000MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
If the above ways do not solve the out of memory problem, you can try to use CUDA managed memory. The command is `export FLAGS_use_cuda_managed_memory=false`.
 (at /paddle/paddle/fluid/memory/allocation/cuda_allocator.cc:95)

run.sh: line 72:  9125 Aborted                 CUDA_VISIBLE_DEVICES=$device python train_single.py --output_dir ./checkpoints/ --prompt "$prompt" --max_seq_length $max_length --learning_rate 3e-5 --ppt_learning_rate 3e-4 --num_train_epochs 50 --logging_steps 10 --do_save True --do_test --eval_steps 100 --save_steps 100 --per_device_eval_batch_size 32 --per_device_train_batch_size $batch_size --model_name_or_path ernie-3.0-xbase-zh --split_id few_all --task_name $task_name --metric_for_best_model accuracy --disable_tqdm True --do_train --do_predict --do_eval --load_best_model_at_end
run.sh: line 78: --freeze_plm: command not found
 
==========
csl
==========
 
[33m[2022-08-31 23:09:18,760] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-31 23:09:18,761] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-31 23:09:18,761] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 23:09:18,761] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-31 23:09:18,761] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 23:09:18,761] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-31 23:09:18,762] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-31 23:09:18,762] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-08-31 23:09:18,762] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-31 23:09:18,762] [    INFO][0m - model_name_or_path            :ernie-3.0-xbase-zh[0m
[32m[2022-08-31 23:09:18,762] [    INFO][0m - [0m
[32m[2022-08-31 23:09:18,762] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 23:09:18,762] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-31 23:09:18,762] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 23:09:18,762] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-31 23:09:18,762] [    INFO][0m - prompt                        :{'text': 'text_a'}{'hard':'‰∏äÊñá‰∏≠'}{'mask'}{'hard': 'Ëøô‰∫õÂÖ≥ÈîÆËØçÔºö'}{'text':'text_b'}[0m
[32m[2022-08-31 23:09:18,762] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-31 23:09:18,762] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-31 23:09:18,762] [    INFO][0m - task_name                     :csl[0m
[32m[2022-08-31 23:09:18,763] [    INFO][0m - [0m
[32m[2022-08-31 23:09:18,763] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/ernie_3.0_xbase_zh.pdparams[0m
W0831 23:09:18.764869 17431 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0831 23:09:18.769132 17431 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-31 23:09:27,680] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/ernie_3.0_xbase_zh_vocab.txt[0m
[32m[2022-08-31 23:09:27,706] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/tokenizer_config.json[0m
[32m[2022-08-31 23:09:27,707] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/special_tokens_map.json[0m
[32m[2022-08-31 23:09:27,708] [    INFO][0m - Using template: [{'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‰∏äÊñá‰∏≠'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'Ëøô‰∫õÂÖ≥ÈîÆËØçÔºö'}, {'add_prefix_space': '', 'text': 'text_b'}][0m
[32m[2022-08-31 23:09:27,731] [    INFO][0m - {'0': 0, '1': 1}[0m
2022-08-31 23:09:27,733 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-31 23:09:27,893] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 23:09:27,893] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-31 23:09:27,893] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 23:09:27,893] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-31 23:09:27,893] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-31 23:09:27,893] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-31 23:09:27,894] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-31 23:09:27,894] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-31 23:09:27,894] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-31 23:09:27,894] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-31 23:09:27,894] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-31 23:09:27,894] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-31 23:09:27,894] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-31 23:09:27,894] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-31 23:09:27,894] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-31 23:09:27,894] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-31 23:09:27,894] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-31 23:09:27,894] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-31 23:09:27,895] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-31 23:09:27,895] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-31 23:09:27,895] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-31 23:09:27,895] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-31 23:09:27,895] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-31 23:09:27,895] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-31 23:09:27,895] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-31 23:09:27,895] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-31 23:09:27,895] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-31 23:09:27,895] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-31 23:09:27,895] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-31 23:09:27,895] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-31 23:09:27,895] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-31 23:09:27,896] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-31 23:09:27,896] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-31 23:09:27,896] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-31 23:09:27,896] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-31 23:09:27,896] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-31 23:09:27,896] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-31 23:09:27,896] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug31_23-09-18_instance-3bwob41y-01[0m
[32m[2022-08-31 23:09:27,896] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-31 23:09:27,896] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-31 23:09:27,896] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-31 23:09:27,896] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-31 23:09:27,896] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-31 23:09:27,896] [    INFO][0m - max_seq_length                :320[0m
[32m[2022-08-31 23:09:27,896] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-31 23:09:27,897] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-31 23:09:27,897] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-31 23:09:27,897] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-31 23:09:27,897] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-08-31 23:09:27,897] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-31 23:09:27,897] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-31 23:09:27,897] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-31 23:09:27,897] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-31 23:09:27,897] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-31 23:09:27,897] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-31 23:09:27,897] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-31 23:09:27,897] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-31 23:09:27,897] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-31 23:09:27,898] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-31 23:09:27,898] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-31 23:09:27,898] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-31 23:09:27,898] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-31 23:09:27,898] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-31 23:09:27,898] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-31 23:09:27,898] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-31 23:09:27,898] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-31 23:09:27,898] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-31 23:09:27,898] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-31 23:09:27,898] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-31 23:09:27,898] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-31 23:09:27,898] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-31 23:09:27,898] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-31 23:09:27,899] [    INFO][0m - seed                          :42[0m
[32m[2022-08-31 23:09:27,899] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-31 23:09:27,899] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-31 23:09:27,899] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-31 23:09:27,899] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-31 23:09:27,899] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-31 23:09:27,899] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-31 23:09:27,899] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-31 23:09:27,899] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-31 23:09:27,899] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-31 23:09:27,899] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-31 23:09:27,899] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-31 23:09:27,899] [    INFO][0m - [0m
[32m[2022-08-31 23:09:27,903] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-31 23:09:27,903] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 23:09:27,903] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-08-31 23:09:27,903] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-31 23:09:27,903] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-31 23:09:27,903] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-31 23:09:27,903] [    INFO][0m -   Total optimization steps = 1000.0[0m
[32m[2022-08-31 23:09:27,904] [    INFO][0m -   Total num train samples = 8000[0m
[32m[2022-08-31 23:09:44,573] [    INFO][0m - loss: 1.19550867, learning_rate: 2.97e-05, global_step: 10, interval_runtime: 16.6683, interval_samples_per_second: 0.48, interval_steps_per_second: 0.6, epoch: 0.5[0m
[32m[2022-08-31 23:09:57,039] [    INFO][0m - loss: 0.96560974, learning_rate: 2.94e-05, global_step: 20, interval_runtime: 12.4656, interval_samples_per_second: 0.642, interval_steps_per_second: 0.802, epoch: 1.0[0m
[32m[2022-08-31 23:10:08,301] [    INFO][0m - loss: 0.90481701, learning_rate: 2.91e-05, global_step: 30, interval_runtime: 11.2626, interval_samples_per_second: 0.71, interval_steps_per_second: 0.888, epoch: 1.5[0m
[32m[2022-08-31 23:10:19,265] [    INFO][0m - loss: 0.96101627, learning_rate: 2.88e-05, global_step: 40, interval_runtime: 10.9631, interval_samples_per_second: 0.73, interval_steps_per_second: 0.912, epoch: 2.0[0m
[32m[2022-08-31 23:10:30,635] [    INFO][0m - loss: 0.89064245, learning_rate: 2.8499999999999998e-05, global_step: 50, interval_runtime: 11.3705, interval_samples_per_second: 0.704, interval_steps_per_second: 0.879, epoch: 2.5[0m
[32m[2022-08-31 23:10:41,721] [    INFO][0m - loss: 0.78510723, learning_rate: 2.8199999999999998e-05, global_step: 60, interval_runtime: 11.0863, interval_samples_per_second: 0.722, interval_steps_per_second: 0.902, epoch: 3.0[0m
[32m[2022-08-31 23:10:53,370] [    INFO][0m - loss: 0.71548934, learning_rate: 2.79e-05, global_step: 70, interval_runtime: 11.649, interval_samples_per_second: 0.687, interval_steps_per_second: 0.858, epoch: 3.5[0m
[32m[2022-08-31 23:11:06,841] [    INFO][0m - loss: 0.72961097, learning_rate: 2.7600000000000003e-05, global_step: 80, interval_runtime: 13.4704, interval_samples_per_second: 0.594, interval_steps_per_second: 0.742, epoch: 4.0[0m
[32m[2022-08-31 23:11:20,517] [    INFO][0m - loss: 0.74798269, learning_rate: 2.7300000000000003e-05, global_step: 90, interval_runtime: 13.6764, interval_samples_per_second: 0.585, interval_steps_per_second: 0.731, epoch: 4.5[0m
[32m[2022-08-31 23:11:32,028] [    INFO][0m - loss: 0.68052726, learning_rate: 2.7000000000000002e-05, global_step: 100, interval_runtime: 11.5104, interval_samples_per_second: 0.695, interval_steps_per_second: 0.869, epoch: 5.0[0m
[32m[2022-08-31 23:11:32,030] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 23:11:32,030] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 23:11:32,030] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 23:11:32,030] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 23:11:32,030] [    INFO][0m -   Total prediction steps = 5[0m
terminate called after throwing an instance of 'paddle::memory::allocation::BadAlloc'
  what():  

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   add_final_state_dygraph_function(paddle::experimental::Tensor const&, paddle::experimental::Tensor const&)
1   paddle::experimental::add(paddle::experimental::Tensor const&, paddle::experimental::Tensor const&)
2   void phi::AddRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*)
3   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, paddle::experimental::DataType, unsigned long, bool) const
4   phi::DenseTensor::AllocateFrom(phi::Allocator*, paddle::experimental::DataType, unsigned long)
5   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
6   paddle::memory::allocation::Allocator::Allocate(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
10  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
11  phi::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 1.525879GB memory on GPU 0, 31.334717GB memory has been allocated and available memory is only 423.750000MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
If the above ways do not solve the out of memory problem, you can try to use CUDA managed memory. The command is `export FLAGS_use_cuda_managed_memory=false`.
 (at /paddle/paddle/fluid/memory/allocation/cuda_allocator.cc:95)

run.sh: line 72: 17431 Aborted                 CUDA_VISIBLE_DEVICES=$device python train_single.py --output_dir ./checkpoints/ --prompt "$prompt" --max_seq_length $max_length --learning_rate 3e-5 --ppt_learning_rate 3e-4 --num_train_epochs 50 --logging_steps 10 --do_save True --do_test --eval_steps 100 --save_steps 100 --per_device_eval_batch_size 32 --per_device_train_batch_size $batch_size --model_name_or_path ernie-3.0-xbase-zh --split_id few_all --task_name $task_name --metric_for_best_model accuracy --disable_tqdm True --do_train --do_predict --do_eval --load_best_model_at_end
run.sh: line 78: --freeze_plm: command not found
 
==========
cluewsc
==========
 
[33m[2022-08-31 23:11:47,679] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-31 23:11:47,679] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-31 23:11:47,680] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 23:11:47,680] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-31 23:11:47,680] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 23:11:47,680] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-31 23:11:47,680] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-31 23:11:47,680] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-08-31 23:11:47,680] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-31 23:11:47,680] [    INFO][0m - model_name_or_path            :ernie-3.0-xbase-zh[0m
[32m[2022-08-31 23:11:47,680] [    INFO][0m - [0m
[32m[2022-08-31 23:11:47,680] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 23:11:47,681] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-31 23:11:47,681] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 23:11:47,681] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-31 23:11:47,681] [    INFO][0m - prompt                        :{'text':'text_a'}{'hard':'ÂÖ∂‰∏≠‰ª£ËØç‰ΩøÁî®'}{'mask'}{'mask'}[0m
[32m[2022-08-31 23:11:47,681] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-31 23:11:47,681] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-31 23:11:47,681] [    INFO][0m - task_name                     :cluewsc[0m
[32m[2022-08-31 23:11:47,681] [    INFO][0m - [0m
[32m[2022-08-31 23:11:47,681] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/ernie_3.0_xbase_zh.pdparams[0m
W0831 23:11:47.682947 34720 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0831 23:11:47.687160 34720 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-31 23:11:56,137] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/ernie_3.0_xbase_zh_vocab.txt[0m
[32m[2022-08-31 23:11:56,199] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/tokenizer_config.json[0m
[32m[2022-08-31 23:11:56,199] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/special_tokens_map.json[0m
[32m[2022-08-31 23:11:56,201] [    INFO][0m - Using template: [{'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': 'ÂÖ∂‰∏≠‰ª£ËØç‰ΩøÁî®'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}][0m
[32m[2022-08-31 23:11:56,213] [    INFO][0m - {'false': 0, 'true': 1}[0m
2022-08-31 23:11:56,215 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-31 23:11:56,321] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 23:11:56,321] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-31 23:11:56,321] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 23:11:56,321] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-31 23:11:56,322] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-31 23:11:56,322] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-31 23:11:56,322] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-31 23:11:56,322] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-31 23:11:56,322] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-31 23:11:56,322] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-31 23:11:56,322] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-31 23:11:56,322] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-31 23:11:56,322] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-31 23:11:56,322] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-31 23:11:56,322] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-31 23:11:56,322] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-31 23:11:56,323] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-31 23:11:56,323] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-31 23:11:56,323] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-31 23:11:56,323] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-31 23:11:56,323] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-31 23:11:56,323] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-31 23:11:56,323] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-31 23:11:56,323] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-31 23:11:56,323] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-31 23:11:56,323] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-31 23:11:56,323] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-31 23:11:56,323] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-31 23:11:56,323] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-31 23:11:56,324] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-31 23:11:56,324] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-31 23:11:56,324] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-31 23:11:56,324] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-31 23:11:56,324] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-31 23:11:56,324] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-31 23:11:56,324] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-31 23:11:56,324] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-31 23:11:56,324] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug31_23-11-47_instance-3bwob41y-01[0m
[32m[2022-08-31 23:11:56,324] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-31 23:11:56,324] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-31 23:11:56,324] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-31 23:11:56,324] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-31 23:11:56,325] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-31 23:11:56,325] [    INFO][0m - max_seq_length                :128[0m
[32m[2022-08-31 23:11:56,325] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-31 23:11:56,325] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-31 23:11:56,325] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-31 23:11:56,325] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-31 23:11:56,325] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-08-31 23:11:56,325] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-31 23:11:56,325] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-31 23:11:56,325] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-31 23:11:56,325] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-31 23:11:56,325] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-31 23:11:56,325] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-31 23:11:56,326] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-31 23:11:56,326] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-31 23:11:56,326] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-31 23:11:56,326] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-31 23:11:56,326] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-31 23:11:56,326] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-31 23:11:56,326] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-31 23:11:56,326] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-31 23:11:56,326] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-31 23:11:56,326] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-31 23:11:56,326] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-31 23:11:56,326] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-31 23:11:56,326] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-31 23:11:56,327] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-31 23:11:56,327] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-31 23:11:56,327] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-31 23:11:56,327] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-31 23:11:56,327] [    INFO][0m - seed                          :42[0m
[32m[2022-08-31 23:11:56,327] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-31 23:11:56,327] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-31 23:11:56,327] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-31 23:11:56,327] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-31 23:11:56,327] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-31 23:11:56,327] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-31 23:11:56,327] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-31 23:11:56,327] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-31 23:11:56,328] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-31 23:11:56,328] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-31 23:11:56,328] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-31 23:11:56,328] [    INFO][0m - [0m
[32m[2022-08-31 23:11:56,330] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-31 23:11:56,330] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 23:11:56,330] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-08-31 23:11:56,330] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-31 23:11:56,331] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-31 23:11:56,331] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-31 23:11:56,331] [    INFO][0m -   Total optimization steps = 1000.0[0m
[32m[2022-08-31 23:11:56,331] [    INFO][0m -   Total num train samples = 8000[0m
[32m[2022-08-31 23:12:04,167] [    INFO][0m - loss: 1.00748634, learning_rate: 2.97e-05, global_step: 10, interval_runtime: 7.8349, interval_samples_per_second: 1.021, interval_steps_per_second: 1.276, epoch: 0.5[0m
[32m[2022-08-31 23:12:09,323] [    INFO][0m - loss: 0.92825136, learning_rate: 2.94e-05, global_step: 20, interval_runtime: 5.1555, interval_samples_per_second: 1.552, interval_steps_per_second: 1.94, epoch: 1.0[0m
[32m[2022-08-31 23:12:14,967] [    INFO][0m - loss: 0.78088131, learning_rate: 2.91e-05, global_step: 30, interval_runtime: 5.6449, interval_samples_per_second: 1.417, interval_steps_per_second: 1.771, epoch: 1.5[0m
[32m[2022-08-31 23:12:21,267] [    INFO][0m - loss: 0.83534021, learning_rate: 2.88e-05, global_step: 40, interval_runtime: 6.3, interval_samples_per_second: 1.27, interval_steps_per_second: 1.587, epoch: 2.0[0m
[32m[2022-08-31 23:12:27,737] [    INFO][0m - loss: 0.92042789, learning_rate: 2.8499999999999998e-05, global_step: 50, interval_runtime: 6.4696, interval_samples_per_second: 1.237, interval_steps_per_second: 1.546, epoch: 2.5[0m
[32m[2022-08-31 23:12:34,026] [    INFO][0m - loss: 0.87766762, learning_rate: 2.8199999999999998e-05, global_step: 60, interval_runtime: 6.2895, interval_samples_per_second: 1.272, interval_steps_per_second: 1.59, epoch: 3.0[0m
[32m[2022-08-31 23:12:40,485] [    INFO][0m - loss: 0.73414655, learning_rate: 2.79e-05, global_step: 70, interval_runtime: 6.4578, interval_samples_per_second: 1.239, interval_steps_per_second: 1.549, epoch: 3.5[0m
[32m[2022-08-31 23:12:46,369] [    INFO][0m - loss: 0.79698205, learning_rate: 2.7600000000000003e-05, global_step: 80, interval_runtime: 5.8849, interval_samples_per_second: 1.359, interval_steps_per_second: 1.699, epoch: 4.0[0m
[32m[2022-08-31 23:12:51,649] [    INFO][0m - loss: 0.7140635, learning_rate: 2.7300000000000003e-05, global_step: 90, interval_runtime: 5.2796, interval_samples_per_second: 1.515, interval_steps_per_second: 1.894, epoch: 4.5[0m
[32m[2022-08-31 23:12:56,854] [    INFO][0m - loss: 0.8346839, learning_rate: 2.7000000000000002e-05, global_step: 100, interval_runtime: 5.2047, interval_samples_per_second: 1.537, interval_steps_per_second: 1.921, epoch: 5.0[0m
[32m[2022-08-31 23:12:56,854] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 23:12:56,854] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-31 23:12:56,855] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 23:12:56,855] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 23:12:56,855] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 23:13:00,764] [    INFO][0m - eval_loss: 0.7226462960243225, eval_accuracy: 0.5031446540880503, eval_runtime: 3.209, eval_samples_per_second: 49.548, eval_steps_per_second: 1.558, epoch: 5.0[0m
[32m[2022-08-31 23:13:00,764] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-31 23:13:00,765] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 23:13:13,519] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-31 23:13:13,520] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-31 23:13:40,503] [    INFO][0m - loss: 0.69793205, learning_rate: 2.6700000000000002e-05, global_step: 110, interval_runtime: 43.6492, interval_samples_per_second: 0.183, interval_steps_per_second: 0.229, epoch: 5.5[0m
[32m[2022-08-31 23:13:46,755] [    INFO][0m - loss: 0.6685401, learning_rate: 2.64e-05, global_step: 120, interval_runtime: 6.2517, interval_samples_per_second: 1.28, interval_steps_per_second: 1.6, epoch: 6.0[0m
[32m[2022-08-31 23:13:53,190] [    INFO][0m - loss: 0.57136354, learning_rate: 2.61e-05, global_step: 130, interval_runtime: 6.435, interval_samples_per_second: 1.243, interval_steps_per_second: 1.554, epoch: 6.5[0m
[32m[2022-08-31 23:13:59,476] [    INFO][0m - loss: 0.66399665, learning_rate: 2.58e-05, global_step: 140, interval_runtime: 6.287, interval_samples_per_second: 1.272, interval_steps_per_second: 1.591, epoch: 7.0[0m
[32m[2022-08-31 23:14:05,306] [    INFO][0m - loss: 0.53191948, learning_rate: 2.55e-05, global_step: 150, interval_runtime: 5.8293, interval_samples_per_second: 1.372, interval_steps_per_second: 1.715, epoch: 7.5[0m
[32m[2022-08-31 23:14:10,438] [    INFO][0m - loss: 0.36771975, learning_rate: 2.52e-05, global_step: 160, interval_runtime: 5.132, interval_samples_per_second: 1.559, interval_steps_per_second: 1.949, epoch: 8.0[0m
[32m[2022-08-31 23:14:15,729] [    INFO][0m - loss: 0.48113303, learning_rate: 2.49e-05, global_step: 170, interval_runtime: 5.2917, interval_samples_per_second: 1.512, interval_steps_per_second: 1.89, epoch: 8.5[0m
[32m[2022-08-31 23:14:20,907] [    INFO][0m - loss: 0.38355677, learning_rate: 2.4599999999999998e-05, global_step: 180, interval_runtime: 5.1779, interval_samples_per_second: 1.545, interval_steps_per_second: 1.931, epoch: 9.0[0m
[32m[2022-08-31 23:14:26,249] [    INFO][0m - loss: 0.32092874, learning_rate: 2.43e-05, global_step: 190, interval_runtime: 5.3418, interval_samples_per_second: 1.498, interval_steps_per_second: 1.872, epoch: 9.5[0m
[32m[2022-08-31 23:14:31,430] [    INFO][0m - loss: 0.34178331, learning_rate: 2.4e-05, global_step: 200, interval_runtime: 5.1813, interval_samples_per_second: 1.544, interval_steps_per_second: 1.93, epoch: 10.0[0m
[32m[2022-08-31 23:14:31,431] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 23:14:31,431] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-31 23:14:31,431] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 23:14:31,431] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 23:14:31,431] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 23:14:34,687] [    INFO][0m - eval_loss: 2.845897912979126, eval_accuracy: 0.5094339622641509, eval_runtime: 3.2561, eval_samples_per_second: 48.832, eval_steps_per_second: 1.536, epoch: 10.0[0m
[32m[2022-08-31 23:14:34,688] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-31 23:14:34,688] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 23:14:42,169] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-31 23:14:42,169] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-31 23:15:03,024] [    INFO][0m - loss: 0.53423052, learning_rate: 2.37e-05, global_step: 210, interval_runtime: 31.5938, interval_samples_per_second: 0.253, interval_steps_per_second: 0.317, epoch: 10.5[0m
[32m[2022-08-31 23:15:09,337] [    INFO][0m - loss: 0.6126658, learning_rate: 2.3400000000000003e-05, global_step: 220, interval_runtime: 6.3115, interval_samples_per_second: 1.268, interval_steps_per_second: 1.584, epoch: 11.0[0m
[32m[2022-08-31 23:15:15,838] [    INFO][0m - loss: 0.30703964, learning_rate: 2.3100000000000002e-05, global_step: 230, interval_runtime: 6.5021, interval_samples_per_second: 1.23, interval_steps_per_second: 1.538, epoch: 11.5[0m
[32m[2022-08-31 23:15:22,128] [    INFO][0m - loss: 0.45121517, learning_rate: 2.2800000000000002e-05, global_step: 240, interval_runtime: 6.2897, interval_samples_per_second: 1.272, interval_steps_per_second: 1.59, epoch: 12.0[0m
[32m[2022-08-31 23:15:27,978] [    INFO][0m - loss: 0.21832433, learning_rate: 2.25e-05, global_step: 250, interval_runtime: 5.8499, interval_samples_per_second: 1.368, interval_steps_per_second: 1.709, epoch: 12.5[0m
[32m[2022-08-31 23:15:33,088] [    INFO][0m - loss: 0.51126432, learning_rate: 2.22e-05, global_step: 260, interval_runtime: 5.1095, interval_samples_per_second: 1.566, interval_steps_per_second: 1.957, epoch: 13.0[0m
[32m[2022-08-31 23:15:38,394] [    INFO][0m - loss: 0.37419832, learning_rate: 2.19e-05, global_step: 270, interval_runtime: 5.3073, interval_samples_per_second: 1.507, interval_steps_per_second: 1.884, epoch: 13.5[0m
[32m[2022-08-31 23:15:43,594] [    INFO][0m - loss: 0.12853892, learning_rate: 2.16e-05, global_step: 280, interval_runtime: 5.1991, interval_samples_per_second: 1.539, interval_steps_per_second: 1.923, epoch: 14.0[0m
[32m[2022-08-31 23:15:48,889] [    INFO][0m - loss: 0.47606745, learning_rate: 2.13e-05, global_step: 290, interval_runtime: 5.2952, interval_samples_per_second: 1.511, interval_steps_per_second: 1.889, epoch: 14.5[0m
[32m[2022-08-31 23:15:54,112] [    INFO][0m - loss: 0.1440017, learning_rate: 2.1e-05, global_step: 300, interval_runtime: 5.2238, interval_samples_per_second: 1.531, interval_steps_per_second: 1.914, epoch: 15.0[0m
[32m[2022-08-31 23:15:54,113] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 23:15:54,113] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-31 23:15:54,113] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 23:15:54,114] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 23:15:54,114] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 23:15:57,343] [    INFO][0m - eval_loss: 4.120815753936768, eval_accuracy: 0.5157232704402516, eval_runtime: 3.2286, eval_samples_per_second: 49.247, eval_steps_per_second: 1.549, epoch: 15.0[0m
[32m[2022-08-31 23:15:57,343] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-31 23:15:57,343] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 23:16:09,559] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-31 23:16:09,559] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-31 23:16:27,908] [    INFO][0m - loss: 0.17497663, learning_rate: 2.07e-05, global_step: 310, interval_runtime: 33.7949, interval_samples_per_second: 0.237, interval_steps_per_second: 0.296, epoch: 15.5[0m
[32m[2022-08-31 23:16:33,858] [    INFO][0m - loss: 0.08041135, learning_rate: 2.04e-05, global_step: 320, interval_runtime: 5.9503, interval_samples_per_second: 1.344, interval_steps_per_second: 1.681, epoch: 16.0[0m
[32m[2022-08-31 23:16:40,284] [    INFO][0m - loss: 0.13963636, learning_rate: 2.01e-05, global_step: 330, interval_runtime: 6.4261, interval_samples_per_second: 1.245, interval_steps_per_second: 1.556, epoch: 16.5[0m
[32m[2022-08-31 23:16:46,552] [    INFO][0m - loss: 0.12570614, learning_rate: 1.98e-05, global_step: 340, interval_runtime: 6.2669, interval_samples_per_second: 1.277, interval_steps_per_second: 1.596, epoch: 17.0[0m
[32m[2022-08-31 23:16:52,977] [    INFO][0m - loss: 0.45543165, learning_rate: 1.95e-05, global_step: 350, interval_runtime: 6.4266, interval_samples_per_second: 1.245, interval_steps_per_second: 1.556, epoch: 17.5[0m
[32m[2022-08-31 23:16:59,271] [    INFO][0m - loss: 0.06010227, learning_rate: 1.9200000000000003e-05, global_step: 360, interval_runtime: 6.2938, interval_samples_per_second: 1.271, interval_steps_per_second: 1.589, epoch: 18.0[0m
[32m[2022-08-31 23:17:04,907] [    INFO][0m - loss: 0.26900563, learning_rate: 1.8900000000000002e-05, global_step: 370, interval_runtime: 5.6362, interval_samples_per_second: 1.419, interval_steps_per_second: 1.774, epoch: 18.5[0m
[32m[2022-08-31 23:17:10,117] [    INFO][0m - loss: 0.25501738, learning_rate: 1.86e-05, global_step: 380, interval_runtime: 5.2085, interval_samples_per_second: 1.536, interval_steps_per_second: 1.92, epoch: 19.0[0m
[32m[2022-08-31 23:17:15,407] [    INFO][0m - loss: 0.02184255, learning_rate: 1.83e-05, global_step: 390, interval_runtime: 5.2915, interval_samples_per_second: 1.512, interval_steps_per_second: 1.89, epoch: 19.5[0m
[32m[2022-08-31 23:17:20,644] [    INFO][0m - loss: 0.16412544, learning_rate: 1.8e-05, global_step: 400, interval_runtime: 5.2366, interval_samples_per_second: 1.528, interval_steps_per_second: 1.91, epoch: 20.0[0m
[32m[2022-08-31 23:17:20,644] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 23:17:20,645] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-31 23:17:20,645] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 23:17:20,645] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 23:17:20,645] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 23:17:23,875] [    INFO][0m - eval_loss: 3.352264642715454, eval_accuracy: 0.5911949685534591, eval_runtime: 3.23, eval_samples_per_second: 49.226, eval_steps_per_second: 1.548, epoch: 20.0[0m
[32m[2022-08-31 23:17:23,876] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-31 23:17:23,876] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 23:17:31,627] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-31 23:17:31,628] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-31 23:17:50,451] [    INFO][0m - loss: 0.09257433, learning_rate: 1.77e-05, global_step: 410, interval_runtime: 29.8072, interval_samples_per_second: 0.268, interval_steps_per_second: 0.335, epoch: 20.5[0m
[32m[2022-08-31 23:17:56,453] [    INFO][0m - loss: 0.15890304, learning_rate: 1.74e-05, global_step: 420, interval_runtime: 6.0019, interval_samples_per_second: 1.333, interval_steps_per_second: 1.666, epoch: 21.0[0m
[32m[2022-08-31 23:18:05,394] [    INFO][0m - loss: 0.03781771, learning_rate: 1.71e-05, global_step: 430, interval_runtime: 6.4158, interval_samples_per_second: 1.247, interval_steps_per_second: 1.559, epoch: 21.5[0m
[32m[2022-08-31 23:18:11,687] [    INFO][0m - loss: 0.03519401, learning_rate: 1.6800000000000002e-05, global_step: 440, interval_runtime: 8.8177, interval_samples_per_second: 0.907, interval_steps_per_second: 1.134, epoch: 22.0[0m
[32m[2022-08-31 23:18:18,209] [    INFO][0m - loss: 0.18743058, learning_rate: 1.65e-05, global_step: 450, interval_runtime: 6.5226, interval_samples_per_second: 1.226, interval_steps_per_second: 1.533, epoch: 22.5[0m
[32m[2022-08-31 23:18:24,169] [    INFO][0m - loss: 0.08253011, learning_rate: 1.62e-05, global_step: 460, interval_runtime: 5.9601, interval_samples_per_second: 1.342, interval_steps_per_second: 1.678, epoch: 23.0[0m
[32m[2022-08-31 23:18:29,458] [    INFO][0m - loss: 0.04358106, learning_rate: 1.59e-05, global_step: 470, interval_runtime: 5.2887, interval_samples_per_second: 1.513, interval_steps_per_second: 1.891, epoch: 23.5[0m
[32m[2022-08-31 23:18:34,610] [    INFO][0m - loss: 0.00217819, learning_rate: 1.56e-05, global_step: 480, interval_runtime: 5.1518, interval_samples_per_second: 1.553, interval_steps_per_second: 1.941, epoch: 24.0[0m
[32m[2022-08-31 23:18:39,913] [    INFO][0m - loss: 0.03769636, learning_rate: 1.53e-05, global_step: 490, interval_runtime: 5.3032, interval_samples_per_second: 1.509, interval_steps_per_second: 1.886, epoch: 24.5[0m
[32m[2022-08-31 23:18:45,140] [    INFO][0m - loss: 1.801e-05, learning_rate: 1.5e-05, global_step: 500, interval_runtime: 5.2271, interval_samples_per_second: 1.53, interval_steps_per_second: 1.913, epoch: 25.0[0m
[32m[2022-08-31 23:18:45,140] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 23:18:45,141] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-31 23:18:45,141] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 23:18:45,141] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 23:18:45,141] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 23:18:48,395] [    INFO][0m - eval_loss: 6.278111934661865, eval_accuracy: 0.6163522012578616, eval_runtime: 3.2544, eval_samples_per_second: 48.856, eval_steps_per_second: 1.536, epoch: 25.0[0m
[32m[2022-08-31 23:18:48,396] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-08-31 23:18:48,396] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 23:19:00,451] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-08-31 23:19:00,451] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-08-31 23:19:20,037] [    INFO][0m - loss: 0.09969501, learning_rate: 1.47e-05, global_step: 510, interval_runtime: 34.8962, interval_samples_per_second: 0.229, interval_steps_per_second: 0.287, epoch: 25.5[0m
[32m[2022-08-31 23:19:26,303] [    INFO][0m - loss: 0.2253998, learning_rate: 1.44e-05, global_step: 520, interval_runtime: 6.2665, interval_samples_per_second: 1.277, interval_steps_per_second: 1.596, epoch: 26.0[0m
[32m[2022-08-31 23:19:32,768] [    INFO][0m - loss: 0.0001549, learning_rate: 1.4099999999999999e-05, global_step: 530, interval_runtime: 6.4645, interval_samples_per_second: 1.238, interval_steps_per_second: 1.547, epoch: 26.5[0m
[32m[2022-08-31 23:19:39,075] [    INFO][0m - loss: 0.0641135, learning_rate: 1.3800000000000002e-05, global_step: 540, interval_runtime: 6.3074, interval_samples_per_second: 1.268, interval_steps_per_second: 1.585, epoch: 27.0[0m
[32m[2022-08-31 23:19:47,644] [    INFO][0m - loss: 0.17998792, learning_rate: 1.3500000000000001e-05, global_step: 550, interval_runtime: 6.4771, interval_samples_per_second: 1.235, interval_steps_per_second: 1.544, epoch: 27.5[0m
[32m[2022-08-31 23:19:52,822] [    INFO][0m - loss: 0.05838858, learning_rate: 1.32e-05, global_step: 560, interval_runtime: 7.2701, interval_samples_per_second: 1.1, interval_steps_per_second: 1.375, epoch: 28.0[0m
[32m[2022-08-31 23:19:58,085] [    INFO][0m - loss: 0.00014876, learning_rate: 1.29e-05, global_step: 570, interval_runtime: 5.2633, interval_samples_per_second: 1.52, interval_steps_per_second: 1.9, epoch: 28.5[0m
[32m[2022-08-31 23:20:03,304] [    INFO][0m - loss: 6.241e-05, learning_rate: 1.26e-05, global_step: 580, interval_runtime: 5.219, interval_samples_per_second: 1.533, interval_steps_per_second: 1.916, epoch: 29.0[0m
[32m[2022-08-31 23:20:08,516] [    INFO][0m - loss: 0.02636287, learning_rate: 1.2299999999999999e-05, global_step: 590, interval_runtime: 5.2115, interval_samples_per_second: 1.535, interval_steps_per_second: 1.919, epoch: 29.5[0m
[32m[2022-08-31 23:20:13,651] [    INFO][0m - loss: 0.11426251, learning_rate: 1.2e-05, global_step: 600, interval_runtime: 5.1345, interval_samples_per_second: 1.558, interval_steps_per_second: 1.948, epoch: 30.0[0m
[32m[2022-08-31 23:20:13,651] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 23:20:13,651] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-31 23:20:13,651] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 23:20:13,652] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 23:20:13,652] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 23:20:16,833] [    INFO][0m - eval_loss: 3.9635815620422363, eval_accuracy: 0.6352201257861635, eval_runtime: 3.1814, eval_samples_per_second: 49.977, eval_steps_per_second: 1.572, epoch: 30.0[0m
[32m[2022-08-31 23:20:16,834] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-08-31 23:20:16,834] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 23:20:24,246] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-08-31 23:20:24,246] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-08-31 23:20:51,687] [    INFO][0m - loss: 0.03935306, learning_rate: 1.1700000000000001e-05, global_step: 610, interval_runtime: 38.037, interval_samples_per_second: 0.21, interval_steps_per_second: 0.263, epoch: 30.5[0m
[32m[2022-08-31 23:20:58,043] [    INFO][0m - loss: 1.68e-05, learning_rate: 1.1400000000000001e-05, global_step: 620, interval_runtime: 6.3551, interval_samples_per_second: 1.259, interval_steps_per_second: 1.574, epoch: 31.0[0m
[32m[2022-08-31 23:21:04,515] [    INFO][0m - loss: 1.354e-05, learning_rate: 1.11e-05, global_step: 630, interval_runtime: 6.4726, interval_samples_per_second: 1.236, interval_steps_per_second: 1.545, epoch: 31.5[0m
[32m[2022-08-31 23:21:09,966] [    INFO][0m - loss: 0.00024637, learning_rate: 1.08e-05, global_step: 640, interval_runtime: 5.4501, interval_samples_per_second: 1.468, interval_steps_per_second: 1.835, epoch: 32.0[0m
[32m[2022-08-31 23:21:15,312] [    INFO][0m - loss: 7.006e-05, learning_rate: 1.05e-05, global_step: 650, interval_runtime: 5.3462, interval_samples_per_second: 1.496, interval_steps_per_second: 1.87, epoch: 32.5[0m
[32m[2022-08-31 23:21:20,495] [    INFO][0m - loss: 2.474e-05, learning_rate: 1.02e-05, global_step: 660, interval_runtime: 5.1833, interval_samples_per_second: 1.543, interval_steps_per_second: 1.929, epoch: 33.0[0m
[32m[2022-08-31 23:21:25,837] [    INFO][0m - loss: 0.14534453, learning_rate: 9.9e-06, global_step: 670, interval_runtime: 5.3417, interval_samples_per_second: 1.498, interval_steps_per_second: 1.872, epoch: 33.5[0m
[32m[2022-08-31 23:21:30,946] [    INFO][0m - loss: 2.531e-05, learning_rate: 9.600000000000001e-06, global_step: 680, interval_runtime: 5.1096, interval_samples_per_second: 1.566, interval_steps_per_second: 1.957, epoch: 34.0[0m
[32m[2022-08-31 23:21:36,253] [    INFO][0m - loss: 2.763e-05, learning_rate: 9.3e-06, global_step: 690, interval_runtime: 5.3062, interval_samples_per_second: 1.508, interval_steps_per_second: 1.885, epoch: 34.5[0m
[32m[2022-08-31 23:21:41,398] [    INFO][0m - loss: 5.766e-05, learning_rate: 9e-06, global_step: 700, interval_runtime: 5.1457, interval_samples_per_second: 1.555, interval_steps_per_second: 1.943, epoch: 35.0[0m
[32m[2022-08-31 23:21:41,399] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 23:21:41,399] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-31 23:21:41,399] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 23:21:41,399] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 23:21:41,399] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 23:21:46,052] [    INFO][0m - eval_loss: 4.675146102905273, eval_accuracy: 0.5849056603773585, eval_runtime: 3.2494, eval_samples_per_second: 48.932, eval_steps_per_second: 1.539, epoch: 35.0[0m
[32m[2022-08-31 23:21:46,053] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-700[0m
[32m[2022-08-31 23:21:46,053] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 23:21:53,798] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-700/tokenizer_config.json[0m
[32m[2022-08-31 23:21:53,798] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-700/special_tokens_map.json[0m
[32m[2022-08-31 23:22:13,558] [    INFO][0m - loss: 7.636e-05, learning_rate: 8.7e-06, global_step: 710, interval_runtime: 32.1592, interval_samples_per_second: 0.249, interval_steps_per_second: 0.311, epoch: 35.5[0m
[32m[2022-08-31 23:22:19,897] [    INFO][0m - loss: 3.705e-05, learning_rate: 8.400000000000001e-06, global_step: 720, interval_runtime: 6.3396, interval_samples_per_second: 1.262, interval_steps_per_second: 1.577, epoch: 36.0[0m
[32m[2022-08-31 23:22:26,326] [    INFO][0m - loss: 1.49e-05, learning_rate: 8.1e-06, global_step: 730, interval_runtime: 6.429, interval_samples_per_second: 1.244, interval_steps_per_second: 1.555, epoch: 36.5[0m
[32m[2022-08-31 23:22:31,935] [    INFO][0m - loss: 2.691e-05, learning_rate: 7.8e-06, global_step: 740, interval_runtime: 5.6087, interval_samples_per_second: 1.426, interval_steps_per_second: 1.783, epoch: 37.0[0m
[32m[2022-08-31 23:22:37,283] [    INFO][0m - loss: 2.35e-05, learning_rate: 7.5e-06, global_step: 750, interval_runtime: 5.3484, interval_samples_per_second: 1.496, interval_steps_per_second: 1.87, epoch: 37.5[0m
[32m[2022-08-31 23:22:42,440] [    INFO][0m - loss: 1.527e-05, learning_rate: 7.2e-06, global_step: 760, interval_runtime: 5.1573, interval_samples_per_second: 1.551, interval_steps_per_second: 1.939, epoch: 38.0[0m
[32m[2022-08-31 23:22:47,749] [    INFO][0m - loss: 1.245e-05, learning_rate: 6.900000000000001e-06, global_step: 770, interval_runtime: 5.308, interval_samples_per_second: 1.507, interval_steps_per_second: 1.884, epoch: 38.5[0m
[32m[2022-08-31 23:22:52,909] [    INFO][0m - loss: 8.07e-06, learning_rate: 6.6e-06, global_step: 780, interval_runtime: 5.1607, interval_samples_per_second: 1.55, interval_steps_per_second: 1.938, epoch: 39.0[0m
[32m[2022-08-31 23:22:58,115] [    INFO][0m - loss: 0.06260786, learning_rate: 6.3e-06, global_step: 790, interval_runtime: 5.2058, interval_samples_per_second: 1.537, interval_steps_per_second: 1.921, epoch: 39.5[0m
[32m[2022-08-31 23:23:03,325] [    INFO][0m - loss: 1.523e-05, learning_rate: 6e-06, global_step: 800, interval_runtime: 5.2103, interval_samples_per_second: 1.535, interval_steps_per_second: 1.919, epoch: 40.0[0m
[32m[2022-08-31 23:23:03,326] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 23:23:03,326] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-31 23:23:03,326] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 23:23:03,326] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 23:23:03,326] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 23:23:06,521] [    INFO][0m - eval_loss: 4.359075546264648, eval_accuracy: 0.6037735849056604, eval_runtime: 3.1943, eval_samples_per_second: 49.776, eval_steps_per_second: 1.565, epoch: 40.0[0m
[32m[2022-08-31 23:23:06,522] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-800[0m
[32m[2022-08-31 23:23:06,522] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 23:23:18,393] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-800/tokenizer_config.json[0m
[32m[2022-08-31 23:23:18,394] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-800/special_tokens_map.json[0m
[32m[2022-08-31 23:23:52,675] [    INFO][0m - loss: 0.00011924, learning_rate: 5.7000000000000005e-06, global_step: 810, interval_runtime: 49.3489, interval_samples_per_second: 0.162, interval_steps_per_second: 0.203, epoch: 40.5[0m
[32m[2022-08-31 23:23:56,532] [    INFO][0m - loss: 0.0036019, learning_rate: 5.4e-06, global_step: 820, interval_runtime: 3.8576, interval_samples_per_second: 2.074, interval_steps_per_second: 2.592, epoch: 41.0[0m
[32m[2022-08-31 23:24:00,026] [    INFO][0m - loss: 4.364e-05, learning_rate: 5.1e-06, global_step: 830, interval_runtime: 3.4945, interval_samples_per_second: 2.289, interval_steps_per_second: 2.862, epoch: 41.5[0m
[32m[2022-08-31 23:24:03,856] [    INFO][0m - loss: 1.21e-05, learning_rate: 4.800000000000001e-06, global_step: 840, interval_runtime: 3.3401, interval_samples_per_second: 2.395, interval_steps_per_second: 2.994, epoch: 42.0[0m
[32m[2022-08-31 23:24:11,260] [    INFO][0m - loss: 1.243e-05, learning_rate: 4.5e-06, global_step: 850, interval_runtime: 3.9003, interval_samples_per_second: 2.051, interval_steps_per_second: 2.564, epoch: 42.5[0m
[32m[2022-08-31 23:24:14,587] [    INFO][0m - loss: 7.64e-06, learning_rate: 4.2000000000000004e-06, global_step: 860, interval_runtime: 7.3206, interval_samples_per_second: 1.093, interval_steps_per_second: 1.366, epoch: 43.0[0m
[32m[2022-08-31 23:24:18,066] [    INFO][0m - loss: 0.00019633, learning_rate: 3.9e-06, global_step: 870, interval_runtime: 3.4783, interval_samples_per_second: 2.3, interval_steps_per_second: 2.875, epoch: 43.5[0m
[32m[2022-08-31 23:24:21,566] [    INFO][0m - loss: 0.09859357, learning_rate: 3.6e-06, global_step: 880, interval_runtime: 3.5007, interval_samples_per_second: 2.285, interval_steps_per_second: 2.857, epoch: 44.0[0m
[32m[2022-08-31 23:24:25,061] [    INFO][0m - loss: 6.13e-06, learning_rate: 3.3e-06, global_step: 890, interval_runtime: 3.4944, interval_samples_per_second: 2.289, interval_steps_per_second: 2.862, epoch: 44.5[0m
[32m[2022-08-31 23:24:28,475] [    INFO][0m - loss: 1.683e-05, learning_rate: 3e-06, global_step: 900, interval_runtime: 3.4148, interval_samples_per_second: 2.343, interval_steps_per_second: 2.928, epoch: 45.0[0m
[32m[2022-08-31 23:24:28,476] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 23:24:28,476] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-31 23:24:28,476] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 23:24:28,476] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 23:24:28,476] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 23:24:31,702] [    INFO][0m - eval_loss: 4.356646537780762, eval_accuracy: 0.6352201257861635, eval_runtime: 3.2259, eval_samples_per_second: 49.289, eval_steps_per_second: 1.55, epoch: 45.0[0m
[32m[2022-08-31 23:24:31,703] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-900[0m
[32m[2022-08-31 23:24:31,703] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 23:24:39,342] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-900/tokenizer_config.json[0m
[32m[2022-08-31 23:24:39,343] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-900/special_tokens_map.json[0m
[32m[2022-08-31 23:24:59,135] [    INFO][0m - loss: 1.314e-05, learning_rate: 2.7e-06, global_step: 910, interval_runtime: 30.6588, interval_samples_per_second: 0.261, interval_steps_per_second: 0.326, epoch: 45.5[0m
[32m[2022-08-31 23:25:05,477] [    INFO][0m - loss: 0.13180152, learning_rate: 2.4000000000000003e-06, global_step: 920, interval_runtime: 6.3425, interval_samples_per_second: 1.261, interval_steps_per_second: 1.577, epoch: 46.0[0m
[32m[2022-08-31 23:25:10,910] [    INFO][0m - loss: 1.239e-05, learning_rate: 2.1000000000000002e-06, global_step: 930, interval_runtime: 5.4334, interval_samples_per_second: 1.472, interval_steps_per_second: 1.84, epoch: 46.5[0m
[32m[2022-08-31 23:25:16,011] [    INFO][0m - loss: 1.209e-05, learning_rate: 1.8e-06, global_step: 940, interval_runtime: 5.101, interval_samples_per_second: 1.568, interval_steps_per_second: 1.96, epoch: 47.0[0m
[32m[2022-08-31 23:25:21,259] [    INFO][0m - loss: 6.87e-06, learning_rate: 1.5e-06, global_step: 950, interval_runtime: 5.2481, interval_samples_per_second: 1.524, interval_steps_per_second: 1.905, epoch: 47.5[0m
[32m[2022-08-31 23:25:26,366] [    INFO][0m - loss: 1.664e-05, learning_rate: 1.2000000000000002e-06, global_step: 960, interval_runtime: 5.1063, interval_samples_per_second: 1.567, interval_steps_per_second: 1.958, epoch: 48.0[0m
[32m[2022-08-31 23:25:31,697] [    INFO][0m - loss: 5.2e-06, learning_rate: 9e-07, global_step: 970, interval_runtime: 5.3318, interval_samples_per_second: 1.5, interval_steps_per_second: 1.876, epoch: 48.5[0m
[32m[2022-08-31 23:25:36,817] [    INFO][0m - loss: 2.42e-05, learning_rate: 6.000000000000001e-07, global_step: 980, interval_runtime: 5.1192, interval_samples_per_second: 1.563, interval_steps_per_second: 1.953, epoch: 49.0[0m
[32m[2022-08-31 23:25:42,104] [    INFO][0m - loss: 6.36e-06, learning_rate: 3.0000000000000004e-07, global_step: 990, interval_runtime: 5.287, interval_samples_per_second: 1.513, interval_steps_per_second: 1.891, epoch: 49.5[0m
[32m[2022-08-31 23:25:47,306] [    INFO][0m - loss: 1.542e-05, learning_rate: 0.0, global_step: 1000, interval_runtime: 5.2023, interval_samples_per_second: 1.538, interval_steps_per_second: 1.922, epoch: 50.0[0m
[32m[2022-08-31 23:25:47,307] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 23:25:47,307] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-31 23:25:47,307] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 23:25:47,307] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 23:25:47,307] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-31 23:25:50,507] [    INFO][0m - eval_loss: 4.28448486328125, eval_accuracy: 0.6289308176100629, eval_runtime: 3.1996, eval_samples_per_second: 49.693, eval_steps_per_second: 1.563, epoch: 50.0[0m
[32m[2022-08-31 23:25:50,508] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1000[0m
[32m[2022-08-31 23:25:50,508] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 23:25:54,451] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1000/tokenizer_config.json[0m
[32m[2022-08-31 23:25:54,452] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1000/special_tokens_map.json[0m
[32m[2022-08-31 23:26:01,950] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-31 23:26:01,950] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-600 (score: 0.6352201257861635).[0m
[32m[2022-08-31 23:26:05,475] [    INFO][0m - train_runtime: 849.144, train_samples_per_second: 9.421, train_steps_per_second: 1.178, train_loss: 0.2089164630088708, epoch: 50.0[0m
[32m[2022-08-31 23:26:05,477] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-31 23:26:05,477] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 23:26:13,019] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-31 23:26:13,329] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-31 23:26:13,331] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-31 23:26:13,331] [    INFO][0m -   epoch                    =       50.0[0m
[32m[2022-08-31 23:26:13,331] [    INFO][0m -   train_loss               =     0.2089[0m
[32m[2022-08-31 23:26:13,331] [    INFO][0m -   train_runtime            = 0:14:09.14[0m
[32m[2022-08-31 23:26:13,331] [    INFO][0m -   train_samples_per_second =      9.421[0m
[32m[2022-08-31 23:26:13,331] [    INFO][0m -   train_steps_per_second   =      1.178[0m
[32m[2022-08-31 23:26:13,336] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-31 23:26:13,336] [    INFO][0m -   Num examples = 976[0m
[32m[2022-08-31 23:26:13,336] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 23:26:13,336] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 23:26:13,337] [    INFO][0m -   Total prediction steps = 31[0m
[32m[2022-08-31 23:26:35,870] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-31 23:26:35,871] [    INFO][0m -   test_accuracy           =     0.5236[0m
[32m[2022-08-31 23:26:35,871] [    INFO][0m -   test_loss               =      5.516[0m
[32m[2022-08-31 23:26:35,871] [    INFO][0m -   test_runtime            = 0:00:22.53[0m
[32m[2022-08-31 23:26:35,872] [    INFO][0m -   test_samples_per_second =     43.312[0m
[32m[2022-08-31 23:26:35,872] [    INFO][0m -   test_steps_per_second   =      1.376[0m
[32m[2022-08-31 23:26:35,872] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-31 23:26:35,872] [    INFO][0m -   Num examples = 290[0m
[32m[2022-08-31 23:26:35,872] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-31 23:26:35,872] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-31 23:26:35,873] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-08-31 23:26:41,889] [    INFO][0m - Predictions for cluewscf saved to ./fewclue_submit_examples.[0m
[]
run.sh: line 78: --freeze_plm: command not found
 
==========
cmnli
==========
 
[33m[2022-08-31 23:26:46,985] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-31 23:26:46,985] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-31 23:26:46,986] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 23:26:46,986] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-31 23:26:46,986] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 23:26:46,986] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-31 23:26:46,986] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-31 23:26:46,986] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-08-31 23:26:46,986] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-31 23:26:46,986] [    INFO][0m - model_name_or_path            :ernie-3.0-xbase-zh[0m
[32m[2022-08-31 23:26:46,986] [    INFO][0m - [0m
[32m[2022-08-31 23:26:46,986] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 23:26:46,986] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-31 23:26:46,986] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 23:26:46,986] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-31 23:26:46,987] [    INFO][0m - prompt                        :‚Äú{'text':'text_a'}‚ÄùÂíå‚Äú{'text':'text_b'}‚Äù‰πãÈó¥ÁöÑÈÄªËæëÂÖ≥Á≥ªÊòØ{'mask'}{'mask'}„ÄÇ[0m
[32m[2022-08-31 23:26:46,987] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-31 23:26:46,987] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-31 23:26:46,987] [    INFO][0m - task_name                     :cmnli[0m
[32m[2022-08-31 23:26:46,987] [    INFO][0m - [0m
[32m[2022-08-31 23:26:46,987] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/ernie_3.0_xbase_zh.pdparams[0m
W0831 23:26:46.988739 49335 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0831 23:26:46.992892 49335 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-31 23:26:55,458] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/ernie_3.0_xbase_zh_vocab.txt[0m
[32m[2022-08-31 23:26:55,491] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/tokenizer_config.json[0m
[32m[2022-08-31 23:26:55,492] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-xbase-zh/special_tokens_map.json[0m
[32m[2022-08-31 23:26:55,493] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùÂíå‚Äú'}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': '‚Äù‰πãÈó¥ÁöÑÈÄªËæëÂÖ≥Á≥ªÊòØ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': '„ÄÇ'}][0m
[32m[2022-08-31 23:26:55,517] [    INFO][0m - {'contradiction': 0, 'entailment': 1, 'neutral': 2}[0m
2022-08-31 23:26:55,519 INFO [download.py:119] unique_endpoints {''}
2022-08-31 23:26:57,867 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-31 23:26:58,042] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 23:26:58,043] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-31 23:26:58,043] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 23:26:58,043] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-31 23:26:58,043] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-31 23:26:58,043] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-31 23:26:58,043] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-31 23:26:58,043] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-31 23:26:58,043] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-31 23:26:58,043] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-31 23:26:58,044] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-31 23:26:58,044] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-31 23:26:58,044] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-31 23:26:58,044] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-31 23:26:58,044] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-31 23:26:58,044] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-31 23:26:58,044] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-31 23:26:58,044] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-31 23:26:58,044] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-31 23:26:58,044] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-31 23:26:58,044] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-31 23:26:58,044] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-31 23:26:58,044] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-31 23:26:58,045] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-31 23:26:58,045] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-31 23:26:58,045] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-31 23:26:58,045] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-31 23:26:58,045] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-31 23:26:58,045] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-31 23:26:58,045] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-31 23:26:58,045] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-31 23:26:58,045] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-31 23:26:58,045] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-31 23:26:58,045] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-31 23:26:58,045] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-31 23:26:58,045] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-31 23:26:58,046] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-31 23:26:58,046] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug31_23-26-46_instance-3bwob41y-01[0m
[32m[2022-08-31 23:26:58,046] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-31 23:26:58,046] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-31 23:26:58,046] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-31 23:26:58,046] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-31 23:26:58,046] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-31 23:26:58,046] [    INFO][0m - max_seq_length                :128[0m
[32m[2022-08-31 23:26:58,046] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-31 23:26:58,046] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-31 23:26:58,046] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-31 23:26:58,046] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-31 23:26:58,047] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-08-31 23:26:58,047] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-31 23:26:58,047] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-31 23:26:58,047] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-31 23:26:58,047] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-31 23:26:58,047] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-31 23:26:58,047] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-31 23:26:58,047] [    INFO][0m - per_device_train_batch_size   :32[0m
[32m[2022-08-31 23:26:58,047] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-31 23:26:58,047] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-31 23:26:58,047] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-31 23:26:58,047] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-31 23:26:58,047] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-31 23:26:58,048] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-31 23:26:58,048] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-31 23:26:58,048] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-31 23:26:58,048] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-31 23:26:58,048] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-31 23:26:58,048] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-31 23:26:58,048] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-31 23:26:58,048] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-31 23:26:58,048] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-31 23:26:58,048] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-31 23:26:58,048] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-31 23:26:58,048] [    INFO][0m - seed                          :42[0m
[32m[2022-08-31 23:26:58,048] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-31 23:26:58,049] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-31 23:26:58,049] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-31 23:26:58,049] [    INFO][0m - train_batch_size              :32[0m
[32m[2022-08-31 23:26:58,049] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-31 23:26:58,049] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-31 23:26:58,049] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-31 23:26:58,049] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-31 23:26:58,049] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-31 23:26:58,049] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-31 23:26:58,049] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-31 23:26:58,049] [    INFO][0m - [0m
[32m[2022-08-31 23:26:58,053] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-31 23:26:58,053] [    INFO][0m -   Num examples = 391783[0m
[32m[2022-08-31 23:26:58,053] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-08-31 23:26:58,053] [    INFO][0m -   Instantaneous batch size per device = 32[0m
[32m[2022-08-31 23:26:58,053] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 32[0m
[32m[2022-08-31 23:26:58,053] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-31 23:26:58,054] [    INFO][0m -   Total optimization steps = 612200.0[0m
[32m[2022-08-31 23:26:58,054] [    INFO][0m -   Total num train samples = 19589150[0m
terminate called after throwing an instance of 'paddle::memory::allocation::BadAlloc'
  what():  

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::experimental::full_like(paddle::experimental::Tensor const&, paddle::experimental::ScalarBase<paddle::experimental::Tensor> const&, paddle::experimental::DataType, phi::Place const&)
1   void phi::FullLikeKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::ScalarBase<phi::DenseTensor> const&, paddle::experimental::DataType, phi::DenseTensor*)
2   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, paddle::experimental::DataType, unsigned long, bool) const
3   phi::DenseTensor::AllocateFrom(phi::Allocator*, paddle::experimental::DataType, unsigned long)
4   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
5   paddle::memory::allocation::Allocator::Allocate(unsigned long)
6   paddle::memory::allocation::Allocator::Allocate(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  phi::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 625.000000MB memory on GPU 0, 31.322998GB memory has been allocated and available memory is only 435.750000MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
If the above ways do not solve the out of memory problem, you can try to use CUDA managed memory. The command is `export FLAGS_use_cuda_managed_memory=false`.
 (at /paddle/paddle/fluid/memory/allocation/cuda_allocator.cc:95)

run.sh: line 72: 49335 Aborted                 CUDA_VISIBLE_DEVICES=$device python train_single.py --output_dir ./checkpoints/ --prompt "$prompt" --max_seq_length $max_length --learning_rate 3e-5 --ppt_learning_rate 3e-4 --num_train_epochs 50 --logging_steps 10 --do_save True --do_test --eval_steps 100 --save_steps 100 --per_device_eval_batch_size 32 --per_device_train_batch_size $batch_size --model_name_or_path ernie-3.0-xbase-zh --split_id few_all --task_name $task_name --metric_for_best_model accuracy --disable_tqdm True --do_train --do_predict --do_eval --load_best_model_at_end
run.sh: line 78: --freeze_plm: command not found
