[33m[2022-08-31 15:49:40,717] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-31 15:49:40,717] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-31 15:49:40,718] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 15:49:40,718] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-31 15:49:40,718] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 15:49:40,718] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-31 15:49:40,718] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-31 15:49:40,718] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-08-31 15:49:40,718] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-31 15:49:40,718] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-08-31 15:49:40,718] [    INFO][0m - [0m
[32m[2022-08-31 15:49:40,718] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 15:49:40,719] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-31 15:49:40,719] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 15:49:40,719] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-31 15:49:40,719] [    INFO][0m - prompt                        :‚Äú{'text':'text_a'}‚ÄùÂíå‚Äú{'text':'text_b'}‚Äù‰πãÈó¥ÁöÑÈÄªËæëÂÖ≥Á≥ªÊòØ{'mask'}{'mask'}„ÄÇ[0m
[32m[2022-08-31 15:49:40,719] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-31 15:49:40,719] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-31 15:49:40,719] [    INFO][0m - task_name                     :bustm[0m
[32m[2022-08-31 15:49:40,719] [    INFO][0m - [0m
[32m[2022-08-31 15:49:40,719] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0831 15:49:40.720801 70552 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0831 15:49:40.724920 70552 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-31 15:49:43,708] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-08-31 15:49:43,734] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-08-31 15:49:43,735] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[32m[2022-08-31 15:49:43,736] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùÂíå‚Äú'}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': '‚Äù‰πãÈó¥ÁöÑÈÄªËæëÂÖ≥Á≥ªÊòØ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': '„ÄÇ'}][0m
[32m[2022-08-31 15:49:43,739] [    INFO][0m - {'0': 0, '1': 1}[0m
2022-08-31 15:49:43,741 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-31 15:49:44,786] [    INFO][0m - ============================================================[0m
[32m[2022-08-31 15:49:44,786] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-31 15:49:44,786] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-31 15:49:44,786] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-31 15:49:44,786] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-31 15:49:44,787] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-31 15:49:44,787] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-31 15:49:44,787] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-31 15:49:44,787] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-31 15:49:44,787] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-31 15:49:44,787] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-31 15:49:44,787] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-31 15:49:44,787] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-31 15:49:44,787] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-31 15:49:44,787] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-31 15:49:44,787] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-31 15:49:44,787] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-31 15:49:44,787] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-31 15:49:44,788] [    INFO][0m - eval_batch_size               :64[0m
[32m[2022-08-31 15:49:44,788] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-31 15:49:44,788] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-31 15:49:44,788] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-31 15:49:44,788] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-31 15:49:44,788] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-31 15:49:44,788] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-31 15:49:44,788] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-31 15:49:44,788] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-31 15:49:44,788] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-31 15:49:44,788] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-31 15:49:44,788] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-31 15:49:44,788] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-31 15:49:44,788] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-31 15:49:44,789] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-31 15:49:44,789] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-31 15:49:44,789] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-31 15:49:44,789] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-31 15:49:44,789] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-31 15:49:44,789] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug31_15-49-40_instance-3bwob41y-01[0m
[32m[2022-08-31 15:49:44,789] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-31 15:49:44,789] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-31 15:49:44,789] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-31 15:49:44,789] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-31 15:49:44,789] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-31 15:49:44,789] [    INFO][0m - max_seq_length                :64[0m
[32m[2022-08-31 15:49:44,789] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-31 15:49:44,789] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-31 15:49:44,790] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-31 15:49:44,790] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-31 15:49:44,790] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-08-31 15:49:44,790] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-31 15:49:44,790] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-31 15:49:44,790] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-31 15:49:44,790] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-31 15:49:44,790] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-31 15:49:44,790] [    INFO][0m - per_device_eval_batch_size    :64[0m
[32m[2022-08-31 15:49:44,790] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-31 15:49:44,790] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-31 15:49:44,790] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-31 15:49:44,790] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-31 15:49:44,790] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-31 15:49:44,791] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-31 15:49:44,791] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-31 15:49:44,791] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-31 15:49:44,791] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-31 15:49:44,791] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-31 15:49:44,791] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-31 15:49:44,791] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-31 15:49:44,791] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-31 15:49:44,791] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-31 15:49:44,791] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-31 15:49:44,791] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-31 15:49:44,791] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-31 15:49:44,791] [    INFO][0m - seed                          :42[0m
[32m[2022-08-31 15:49:44,792] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-31 15:49:44,792] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-31 15:49:44,792] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-31 15:49:44,792] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-31 15:49:44,792] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-31 15:49:44,792] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-31 15:49:44,792] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-31 15:49:44,792] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-31 15:49:44,792] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-31 15:49:44,792] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-31 15:49:44,792] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-31 15:49:44,792] [    INFO][0m - [0m
[32m[2022-08-31 15:49:44,794] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-31 15:49:44,794] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 15:49:44,794] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-08-31 15:49:44,794] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-31 15:49:44,794] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-31 15:49:44,794] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-31 15:49:44,794] [    INFO][0m -   Total optimization steps = 1000.0[0m
[32m[2022-08-31 15:49:44,794] [    INFO][0m -   Total num train samples = 8000[0m
[32m[2022-08-31 15:49:46,525] [    INFO][0m - loss: 0.53009381, learning_rate: 2.97e-05, global_step: 10, interval_runtime: 1.7303, interval_samples_per_second: 4.624, interval_steps_per_second: 5.779, epoch: 0.5[0m
[32m[2022-08-31 15:49:47,115] [    INFO][0m - loss: 0.85496683, learning_rate: 2.94e-05, global_step: 20, interval_runtime: 0.59, interval_samples_per_second: 13.56, interval_steps_per_second: 16.95, epoch: 1.0[0m
[32m[2022-08-31 15:49:47,941] [    INFO][0m - loss: 0.47231197, learning_rate: 2.91e-05, global_step: 30, interval_runtime: 0.8257, interval_samples_per_second: 9.689, interval_steps_per_second: 12.111, epoch: 1.5[0m
[32m[2022-08-31 15:49:48,584] [    INFO][0m - loss: 0.25272055, learning_rate: 2.88e-05, global_step: 40, interval_runtime: 0.6425, interval_samples_per_second: 12.451, interval_steps_per_second: 15.564, epoch: 2.0[0m
[32m[2022-08-31 15:49:49,290] [    INFO][0m - loss: 0.13367699, learning_rate: 2.8499999999999998e-05, global_step: 50, interval_runtime: 0.7061, interval_samples_per_second: 11.331, interval_steps_per_second: 14.163, epoch: 2.5[0m
[32m[2022-08-31 15:49:49,870] [    INFO][0m - loss: 0.24263487, learning_rate: 2.8199999999999998e-05, global_step: 60, interval_runtime: 0.5802, interval_samples_per_second: 13.789, interval_steps_per_second: 17.236, epoch: 3.0[0m
[32m[2022-08-31 15:49:50,556] [    INFO][0m - loss: 0.10123183, learning_rate: 2.79e-05, global_step: 70, interval_runtime: 0.6862, interval_samples_per_second: 11.659, interval_steps_per_second: 14.573, epoch: 3.5[0m
[32m[2022-08-31 15:49:51,158] [    INFO][0m - loss: 0.0564448, learning_rate: 2.7600000000000003e-05, global_step: 80, interval_runtime: 0.602, interval_samples_per_second: 13.288, interval_steps_per_second: 16.61, epoch: 4.0[0m
[32m[2022-08-31 15:49:51,814] [    INFO][0m - loss: 0.00166205, learning_rate: 2.7300000000000003e-05, global_step: 90, interval_runtime: 0.6564, interval_samples_per_second: 12.188, interval_steps_per_second: 15.235, epoch: 4.5[0m
[32m[2022-08-31 15:49:52,488] [    INFO][0m - loss: 0.16254222, learning_rate: 2.7000000000000002e-05, global_step: 100, interval_runtime: 0.6738, interval_samples_per_second: 11.873, interval_steps_per_second: 14.842, epoch: 5.0[0m
[32m[2022-08-31 15:49:52,489] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 15:49:52,489] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 15:49:52,489] [    INFO][0m -   Pre device batch size = 64[0m
[32m[2022-08-31 15:49:52,489] [    INFO][0m -   Total Batch size = 64[0m
[32m[2022-08-31 15:49:52,489] [    INFO][0m -   Total prediction steps = 3[0m
[32m[2022-08-31 15:49:52,900] [    INFO][0m - eval_loss: 1.7297694683074951, eval_accuracy: 0.78125, eval_runtime: 0.4105, eval_samples_per_second: 389.741, eval_steps_per_second: 7.308, epoch: 5.0[0m
[32m[2022-08-31 15:49:52,900] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-31 15:49:52,900] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 15:49:56,094] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-31 15:49:56,095] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-31 15:50:01,970] [    INFO][0m - loss: 5.633e-05, learning_rate: 2.6700000000000002e-05, global_step: 110, interval_runtime: 9.4816, interval_samples_per_second: 0.844, interval_steps_per_second: 1.055, epoch: 5.5[0m
[32m[2022-08-31 15:50:02,546] [    INFO][0m - loss: 4.03e-05, learning_rate: 2.64e-05, global_step: 120, interval_runtime: 0.5763, interval_samples_per_second: 13.883, interval_steps_per_second: 17.353, epoch: 6.0[0m
[32m[2022-08-31 15:50:03,223] [    INFO][0m - loss: 0.10664558, learning_rate: 2.61e-05, global_step: 130, interval_runtime: 0.6767, interval_samples_per_second: 11.822, interval_steps_per_second: 14.778, epoch: 6.5[0m
[32m[2022-08-31 15:50:03,867] [    INFO][0m - loss: 0.00030262, learning_rate: 2.58e-05, global_step: 140, interval_runtime: 0.6441, interval_samples_per_second: 12.42, interval_steps_per_second: 15.525, epoch: 7.0[0m
[32m[2022-08-31 15:50:04,568] [    INFO][0m - loss: 0.00516004, learning_rate: 2.55e-05, global_step: 150, interval_runtime: 0.7008, interval_samples_per_second: 11.416, interval_steps_per_second: 14.27, epoch: 7.5[0m
[32m[2022-08-31 15:50:05,151] [    INFO][0m - loss: 0.00022946, learning_rate: 2.52e-05, global_step: 160, interval_runtime: 0.5831, interval_samples_per_second: 13.719, interval_steps_per_second: 17.148, epoch: 8.0[0m
[32m[2022-08-31 15:50:05,832] [    INFO][0m - loss: 3.596e-05, learning_rate: 2.49e-05, global_step: 170, interval_runtime: 0.6806, interval_samples_per_second: 11.754, interval_steps_per_second: 14.692, epoch: 8.5[0m
[32m[2022-08-31 15:50:06,450] [    INFO][0m - loss: 0.10071321, learning_rate: 2.4599999999999998e-05, global_step: 180, interval_runtime: 0.6184, interval_samples_per_second: 12.937, interval_steps_per_second: 16.171, epoch: 9.0[0m
[32m[2022-08-31 15:50:07,140] [    INFO][0m - loss: 0.00078606, learning_rate: 2.43e-05, global_step: 190, interval_runtime: 0.6905, interval_samples_per_second: 11.586, interval_steps_per_second: 14.482, epoch: 9.5[0m
[32m[2022-08-31 15:50:07,735] [    INFO][0m - loss: 0.00207668, learning_rate: 2.4e-05, global_step: 200, interval_runtime: 0.5944, interval_samples_per_second: 13.458, interval_steps_per_second: 16.823, epoch: 10.0[0m
[32m[2022-08-31 15:50:07,736] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 15:50:07,736] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 15:50:07,736] [    INFO][0m -   Pre device batch size = 64[0m
[32m[2022-08-31 15:50:07,736] [    INFO][0m -   Total Batch size = 64[0m
[32m[2022-08-31 15:50:07,736] [    INFO][0m -   Total prediction steps = 3[0m
[32m[2022-08-31 15:50:08,135] [    INFO][0m - eval_loss: 2.0876755714416504, eval_accuracy: 0.7625, eval_runtime: 0.3987, eval_samples_per_second: 401.276, eval_steps_per_second: 7.524, epoch: 10.0[0m
[32m[2022-08-31 15:50:08,135] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-31 15:50:08,136] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 15:50:11,262] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-31 15:50:11,263] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-31 15:50:17,162] [    INFO][0m - loss: 2.908e-05, learning_rate: 2.37e-05, global_step: 210, interval_runtime: 9.4269, interval_samples_per_second: 0.849, interval_steps_per_second: 1.061, epoch: 10.5[0m
[32m[2022-08-31 15:50:17,746] [    INFO][0m - loss: 1.788e-05, learning_rate: 2.3400000000000003e-05, global_step: 220, interval_runtime: 0.5843, interval_samples_per_second: 13.691, interval_steps_per_second: 17.114, epoch: 11.0[0m
[32m[2022-08-31 15:50:18,400] [    INFO][0m - loss: 0.00175646, learning_rate: 2.3100000000000002e-05, global_step: 230, interval_runtime: 0.6544, interval_samples_per_second: 12.224, interval_steps_per_second: 15.28, epoch: 11.5[0m
[32m[2022-08-31 15:50:18,997] [    INFO][0m - loss: 3.34e-06, learning_rate: 2.2800000000000002e-05, global_step: 240, interval_runtime: 0.5962, interval_samples_per_second: 13.419, interval_steps_per_second: 16.774, epoch: 12.0[0m
[32m[2022-08-31 15:50:19,674] [    INFO][0m - loss: 1.302e-05, learning_rate: 2.25e-05, global_step: 250, interval_runtime: 0.6768, interval_samples_per_second: 11.82, interval_steps_per_second: 14.775, epoch: 12.5[0m
[32m[2022-08-31 15:50:20,246] [    INFO][0m - loss: 3.165e-05, learning_rate: 2.22e-05, global_step: 260, interval_runtime: 0.5722, interval_samples_per_second: 13.98, interval_steps_per_second: 17.475, epoch: 13.0[0m
[32m[2022-08-31 15:50:20,946] [    INFO][0m - loss: 3.42e-06, learning_rate: 2.19e-05, global_step: 270, interval_runtime: 0.7, interval_samples_per_second: 11.429, interval_steps_per_second: 14.286, epoch: 13.5[0m
[32m[2022-08-31 15:50:21,569] [    INFO][0m - loss: 1.114e-05, learning_rate: 2.16e-05, global_step: 280, interval_runtime: 0.6229, interval_samples_per_second: 12.843, interval_steps_per_second: 16.053, epoch: 14.0[0m
[32m[2022-08-31 15:50:22,231] [    INFO][0m - loss: 3.26e-06, learning_rate: 2.13e-05, global_step: 290, interval_runtime: 0.6626, interval_samples_per_second: 12.074, interval_steps_per_second: 15.093, epoch: 14.5[0m
[32m[2022-08-31 15:50:22,814] [    INFO][0m - loss: 2.703e-05, learning_rate: 2.1e-05, global_step: 300, interval_runtime: 0.5829, interval_samples_per_second: 13.725, interval_steps_per_second: 17.156, epoch: 15.0[0m
[32m[2022-08-31 15:50:22,815] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 15:50:22,815] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 15:50:22,815] [    INFO][0m -   Pre device batch size = 64[0m
[32m[2022-08-31 15:50:22,815] [    INFO][0m -   Total Batch size = 64[0m
[32m[2022-08-31 15:50:22,815] [    INFO][0m -   Total prediction steps = 3[0m
[32m[2022-08-31 15:50:23,210] [    INFO][0m - eval_loss: 2.4382638931274414, eval_accuracy: 0.78125, eval_runtime: 0.3945, eval_samples_per_second: 405.571, eval_steps_per_second: 7.604, epoch: 15.0[0m
[32m[2022-08-31 15:50:23,210] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-31 15:50:23,210] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 15:50:26,622] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-31 15:50:26,622] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-31 15:50:32,389] [    INFO][0m - loss: 1e-06, learning_rate: 2.07e-05, global_step: 310, interval_runtime: 9.5748, interval_samples_per_second: 0.836, interval_steps_per_second: 1.044, epoch: 15.5[0m
[32m[2022-08-31 15:50:32,974] [    INFO][0m - loss: 3.63e-06, learning_rate: 2.04e-05, global_step: 320, interval_runtime: 0.5845, interval_samples_per_second: 13.687, interval_steps_per_second: 17.108, epoch: 16.0[0m
[32m[2022-08-31 15:50:33,642] [    INFO][0m - loss: 0.0019314, learning_rate: 2.01e-05, global_step: 330, interval_runtime: 0.6687, interval_samples_per_second: 11.963, interval_steps_per_second: 14.954, epoch: 16.5[0m
[32m[2022-08-31 15:50:34,238] [    INFO][0m - loss: 0.02900798, learning_rate: 1.98e-05, global_step: 340, interval_runtime: 0.5955, interval_samples_per_second: 13.434, interval_steps_per_second: 16.792, epoch: 17.0[0m
[32m[2022-08-31 15:50:34,908] [    INFO][0m - loss: 1.41e-06, learning_rate: 1.95e-05, global_step: 350, interval_runtime: 0.6705, interval_samples_per_second: 11.931, interval_steps_per_second: 14.913, epoch: 17.5[0m
[32m[2022-08-31 15:50:35,510] [    INFO][0m - loss: 1.71e-06, learning_rate: 1.9200000000000003e-05, global_step: 360, interval_runtime: 0.6013, interval_samples_per_second: 13.305, interval_steps_per_second: 16.631, epoch: 18.0[0m
[32m[2022-08-31 15:50:36,201] [    INFO][0m - loss: 3.62e-06, learning_rate: 1.8900000000000002e-05, global_step: 370, interval_runtime: 0.6912, interval_samples_per_second: 11.575, interval_steps_per_second: 14.469, epoch: 18.5[0m
[32m[2022-08-31 15:50:36,776] [    INFO][0m - loss: 4.55e-06, learning_rate: 1.86e-05, global_step: 380, interval_runtime: 0.5752, interval_samples_per_second: 13.907, interval_steps_per_second: 17.384, epoch: 19.0[0m
[32m[2022-08-31 15:50:37,459] [    INFO][0m - loss: 2.12e-06, learning_rate: 1.83e-05, global_step: 390, interval_runtime: 0.6824, interval_samples_per_second: 11.723, interval_steps_per_second: 14.653, epoch: 19.5[0m
[32m[2022-08-31 15:50:38,065] [    INFO][0m - loss: 0.02314749, learning_rate: 1.8e-05, global_step: 400, interval_runtime: 0.607, interval_samples_per_second: 13.18, interval_steps_per_second: 16.475, epoch: 20.0[0m
[32m[2022-08-31 15:50:38,066] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 15:50:38,066] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 15:50:38,066] [    INFO][0m -   Pre device batch size = 64[0m
[32m[2022-08-31 15:50:38,066] [    INFO][0m -   Total Batch size = 64[0m
[32m[2022-08-31 15:50:38,066] [    INFO][0m -   Total prediction steps = 3[0m
[32m[2022-08-31 15:50:38,463] [    INFO][0m - eval_loss: 2.472679376602173, eval_accuracy: 0.75625, eval_runtime: 0.3969, eval_samples_per_second: 403.152, eval_steps_per_second: 7.559, epoch: 20.0[0m
[32m[2022-08-31 15:50:38,464] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-31 15:50:38,464] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 15:50:41,689] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-31 15:50:41,690] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-31 15:50:47,395] [    INFO][0m - loss: 7.92e-06, learning_rate: 1.77e-05, global_step: 410, interval_runtime: 9.3296, interval_samples_per_second: 0.857, interval_steps_per_second: 1.072, epoch: 20.5[0m
[32m[2022-08-31 15:50:47,980] [    INFO][0m - loss: 0.00030283, learning_rate: 1.74e-05, global_step: 420, interval_runtime: 0.5852, interval_samples_per_second: 13.67, interval_steps_per_second: 17.088, epoch: 21.0[0m
[32m[2022-08-31 15:50:48,650] [    INFO][0m - loss: 0.02762895, learning_rate: 1.71e-05, global_step: 430, interval_runtime: 0.6692, interval_samples_per_second: 11.955, interval_steps_per_second: 14.944, epoch: 21.5[0m
[32m[2022-08-31 15:50:49,253] [    INFO][0m - loss: 9.66e-05, learning_rate: 1.6800000000000002e-05, global_step: 440, interval_runtime: 0.6036, interval_samples_per_second: 13.253, interval_steps_per_second: 16.566, epoch: 22.0[0m
[32m[2022-08-31 15:50:49,924] [    INFO][0m - loss: 0.00024553, learning_rate: 1.65e-05, global_step: 450, interval_runtime: 0.6712, interval_samples_per_second: 11.919, interval_steps_per_second: 14.899, epoch: 22.5[0m
[32m[2022-08-31 15:50:50,519] [    INFO][0m - loss: 9.5e-07, learning_rate: 1.62e-05, global_step: 460, interval_runtime: 0.5948, interval_samples_per_second: 13.449, interval_steps_per_second: 16.812, epoch: 23.0[0m
[32m[2022-08-31 15:50:51,166] [    INFO][0m - loss: 6.631e-05, learning_rate: 1.59e-05, global_step: 470, interval_runtime: 0.6472, interval_samples_per_second: 12.36, interval_steps_per_second: 15.451, epoch: 23.5[0m
[32m[2022-08-31 15:50:51,766] [    INFO][0m - loss: 1.08e-06, learning_rate: 1.56e-05, global_step: 480, interval_runtime: 0.5994, interval_samples_per_second: 13.346, interval_steps_per_second: 16.683, epoch: 24.0[0m
[32m[2022-08-31 15:50:52,449] [    INFO][0m - loss: 1.4e-07, learning_rate: 1.53e-05, global_step: 490, interval_runtime: 0.6833, interval_samples_per_second: 11.708, interval_steps_per_second: 14.635, epoch: 24.5[0m
[32m[2022-08-31 15:50:53,039] [    INFO][0m - loss: 0.00300268, learning_rate: 1.5e-05, global_step: 500, interval_runtime: 0.5896, interval_samples_per_second: 13.568, interval_steps_per_second: 16.959, epoch: 25.0[0m
[32m[2022-08-31 15:50:53,039] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-31 15:50:53,039] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-31 15:50:53,039] [    INFO][0m -   Pre device batch size = 64[0m
[32m[2022-08-31 15:50:53,039] [    INFO][0m -   Total Batch size = 64[0m
[32m[2022-08-31 15:50:53,040] [    INFO][0m -   Total prediction steps = 3[0m
[32m[2022-08-31 15:50:53,448] [    INFO][0m - eval_loss: 2.94050931930542, eval_accuracy: 0.7625, eval_runtime: 0.4084, eval_samples_per_second: 391.741, eval_steps_per_second: 7.345, epoch: 25.0[0m
[32m[2022-08-31 15:50:53,449] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-08-31 15:50:53,449] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 15:50:56,482] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-08-31 15:50:56,483] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-08-31 15:51:01,802] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-31 15:51:01,803] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-100 (score: 0.78125).[0m
[32m[2022-08-31 15:51:02,902] [    INFO][0m - train_runtime: 78.1064, train_samples_per_second: 102.424, train_steps_per_second: 12.803, train_loss: 0.06223372678447254, epoch: 25.0[0m
[32m[2022-08-31 15:51:02,903] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-31 15:51:02,903] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-31 15:51:06,005] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-31 15:51:06,005] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-31 15:51:06,006] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-31 15:51:06,006] [    INFO][0m -   epoch                    =       25.0[0m
[32m[2022-08-31 15:51:06,006] [    INFO][0m -   train_loss               =     0.0622[0m
[32m[2022-08-31 15:51:06,007] [    INFO][0m -   train_runtime            = 0:01:18.10[0m
[32m[2022-08-31 15:51:06,007] [    INFO][0m -   train_samples_per_second =    102.424[0m
[32m[2022-08-31 15:51:06,007] [    INFO][0m -   train_steps_per_second   =     12.803[0m
[32m[2022-08-31 15:51:06,010] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-31 15:51:06,010] [    INFO][0m -   Num examples = 1772[0m
[32m[2022-08-31 15:51:06,010] [    INFO][0m -   Pre device batch size = 64[0m
[32m[2022-08-31 15:51:06,010] [    INFO][0m -   Total Batch size = 64[0m
[32m[2022-08-31 15:51:06,010] [    INFO][0m -   Total prediction steps = 28[0m
[32m[2022-08-31 15:51:10,412] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-31 15:51:10,413] [    INFO][0m -   test_accuracy           =     0.7449[0m
[32m[2022-08-31 15:51:10,413] [    INFO][0m -   test_loss               =     2.3192[0m
[32m[2022-08-31 15:51:10,413] [    INFO][0m -   test_runtime            = 0:00:04.40[0m
[32m[2022-08-31 15:51:10,413] [    INFO][0m -   test_samples_per_second =    402.535[0m
[32m[2022-08-31 15:51:10,413] [    INFO][0m -   test_steps_per_second   =      6.361[0m
[32m[2022-08-31 15:51:10,413] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-31 15:51:10,413] [    INFO][0m -   Num examples = 2000[0m
[32m[2022-08-31 15:51:10,414] [    INFO][0m -   Pre device batch size = 64[0m
[32m[2022-08-31 15:51:10,414] [    INFO][0m -   Total Batch size = 64[0m
[32m[2022-08-31 15:51:10,414] [    INFO][0m -   Total prediction steps = 32[0m
[32m[2022-08-31 15:51:17,081] [    INFO][0m - Predictions for bustm saved to ./fewclue_submit_examples.[0m
[]
run.sh: line 72: --freeze_plm: command not found
