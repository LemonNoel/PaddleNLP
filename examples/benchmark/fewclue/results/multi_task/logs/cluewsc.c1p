[33m[2022-09-05 12:53:25,013] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-09-05 12:53:25,013] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-05 12:53:25,014] [    INFO][0m - ============================================================[0m
[32m[2022-09-05 12:53:25,014] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-05 12:53:25,014] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-05 12:53:25,014] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-05 12:53:25,014] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-05 12:53:25,014] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-05 12:53:25,014] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-05 12:53:25,014] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-09-05 12:53:25,014] [    INFO][0m - [0m
[32m[2022-09-05 12:53:25,014] [    INFO][0m - ============================================================[0m
[32m[2022-09-05 12:53:25,015] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-05 12:53:25,015] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-05 12:53:25,015] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-05 12:53:25,015] [    INFO][0m - prompt                        :[0m
[32m[2022-09-05 12:53:25,015] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-05 12:53:25,015] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-05 12:53:25,015] [    INFO][0m - task_name                     :cluewsc[0m
[32m[2022-09-05 12:53:25,015] [    INFO][0m - [0m
[32m[2022-09-05 12:53:25,015] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0905 12:53:25.016733 64357 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0905 12:53:25.020548 64357 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-05 12:53:29,073] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-09-05 12:53:29,104] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-09-05 12:53:29,105] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[32m[2022-09-05 12:53:29,106] [    INFO][0m - Using template: [{'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': 'ÂÖ∂‰∏≠‰ª£ËØç‰ΩøÁî®'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}][0m
2022-09-05 12:53:29,111 INFO [download.py:119] unique_endpoints {''}
[32m[2022-09-05 12:53:29,210] [    INFO][0m - ============================================================[0m
[32m[2022-09-05 12:53:29,210] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-05 12:53:29,211] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-05 12:53:29,211] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-05 12:53:29,211] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-05 12:53:29,211] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-05 12:53:29,211] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-05 12:53:29,211] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-05 12:53:29,211] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-05 12:53:29,211] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-05 12:53:29,211] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-05 12:53:29,211] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-05 12:53:29,212] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-05 12:53:29,212] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-05 12:53:29,212] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-05 12:53:29,212] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-05 12:53:29,212] [    INFO][0m - do_predict                    :False[0m
[32m[2022-09-05 12:53:29,212] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-05 12:53:29,212] [    INFO][0m - eval_batch_size               :8[0m
[32m[2022-09-05 12:53:29,212] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-09-05 12:53:29,212] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-09-05 12:53:29,212] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-05 12:53:29,212] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-05 12:53:29,212] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-05 12:53:29,212] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-05 12:53:29,213] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-05 12:53:29,213] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-05 12:53:29,213] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-05 12:53:29,213] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-05 12:53:29,213] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-05 12:53:29,213] [    INFO][0m - learning_rate                 :6e-05[0m
[32m[2022-09-05 12:53:29,213] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-05 12:53:29,213] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-05 12:53:29,213] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-05 12:53:29,213] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-05 12:53:29,213] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-05 12:53:29,213] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-05 12:53:29,213] [    INFO][0m - logging_dir                   :./checkpoints_cluewsc/runs/Sep05_12-53-25_instance-3bwob41y-01[0m
[32m[2022-09-05 12:53:29,214] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-05 12:53:29,214] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-05 12:53:29,214] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-05 12:53:29,214] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-05 12:53:29,214] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-05 12:53:29,214] [    INFO][0m - max_seq_length                :128[0m
[32m[2022-09-05 12:53:29,214] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-05 12:53:29,214] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-05 12:53:29,214] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-05 12:53:29,214] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-05 12:53:29,214] [    INFO][0m - num_train_epochs              :20.0[0m
[32m[2022-09-05 12:53:29,214] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-05 12:53:29,214] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-05 12:53:29,214] [    INFO][0m - output_dir                    :./checkpoints_cluewsc/[0m
[32m[2022-09-05 12:53:29,215] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-05 12:53:29,215] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-05 12:53:29,215] [    INFO][0m - per_device_eval_batch_size    :8[0m
[32m[2022-09-05 12:53:29,215] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-09-05 12:53:29,215] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-05 12:53:29,215] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-05 12:53:29,215] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-05 12:53:29,215] [    INFO][0m - ppt_learning_rate             :6e-05[0m
[32m[2022-09-05 12:53:29,215] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-05 12:53:29,215] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-05 12:53:29,215] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-05 12:53:29,215] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-05 12:53:29,215] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-05 12:53:29,216] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-05 12:53:29,216] [    INFO][0m - run_name                      :./checkpoints_cluewsc/[0m
[32m[2022-09-05 12:53:29,216] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-05 12:53:29,216] [    INFO][0m - save_steps                    :100[0m
[32m[2022-09-05 12:53:29,216] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-09-05 12:53:29,216] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-05 12:53:29,216] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-05 12:53:29,216] [    INFO][0m - seed                          :42[0m
[32m[2022-09-05 12:53:29,216] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-05 12:53:29,216] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-05 12:53:29,216] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-05 12:53:29,216] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-09-05 12:53:29,216] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-05 12:53:29,217] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-05 12:53:29,217] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-05 12:53:29,217] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-05 12:53:29,217] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-05 12:53:29,217] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-05 12:53:29,217] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-05 12:53:29,217] [    INFO][0m - [0m
[32m[2022-09-05 12:53:29,219] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-05 12:53:29,219] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-05 12:53:29,219] [    INFO][0m -   Num Epochs = 20[0m
[32m[2022-09-05 12:53:29,219] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-09-05 12:53:29,219] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-09-05 12:53:29,219] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-05 12:53:29,219] [    INFO][0m -   Total optimization steps = 400.0[0m
[32m[2022-09-05 12:53:29,219] [    INFO][0m -   Total num train samples = 3200[0m
[32m[2022-09-05 12:53:31,299] [    INFO][0m - loss: 2.42511635, learning_rate: 5.85e-05, global_step: 10, interval_runtime: 2.0781, interval_samples_per_second: 3.85, interval_steps_per_second: 4.812, epoch: 0.5[0m
[32m[2022-09-05 12:53:32,149] [    INFO][0m - loss: 0.90878029, learning_rate: 5.6999999999999996e-05, global_step: 20, interval_runtime: 0.8508, interval_samples_per_second: 9.403, interval_steps_per_second: 11.754, epoch: 1.0[0m
[32m[2022-09-05 12:53:33,054] [    INFO][0m - loss: 0.84395914, learning_rate: 5.550000000000001e-05, global_step: 30, interval_runtime: 0.9048, interval_samples_per_second: 8.842, interval_steps_per_second: 11.052, epoch: 1.5[0m
[32m[2022-09-05 12:53:33,907] [    INFO][0m - loss: 0.82158623, learning_rate: 5.4000000000000005e-05, global_step: 40, interval_runtime: 0.8538, interval_samples_per_second: 9.37, interval_steps_per_second: 11.712, epoch: 2.0[0m
[32m[2022-09-05 12:53:34,809] [    INFO][0m - loss: 0.81260557, learning_rate: 5.25e-05, global_step: 50, interval_runtime: 0.9014, interval_samples_per_second: 8.875, interval_steps_per_second: 11.094, epoch: 2.5[0m
[32m[2022-09-05 12:53:35,661] [    INFO][0m - loss: 0.79414682, learning_rate: 5.1e-05, global_step: 60, interval_runtime: 0.8525, interval_samples_per_second: 9.384, interval_steps_per_second: 11.73, epoch: 3.0[0m
[32m[2022-09-05 12:53:36,562] [    INFO][0m - loss: 0.6431282, learning_rate: 4.95e-05, global_step: 70, interval_runtime: 0.8999, interval_samples_per_second: 8.89, interval_steps_per_second: 11.112, epoch: 3.5[0m
[32m[2022-09-05 12:53:37,410] [    INFO][0m - loss: 1.00554781, learning_rate: 4.8e-05, global_step: 80, interval_runtime: 0.8486, interval_samples_per_second: 9.428, interval_steps_per_second: 11.785, epoch: 4.0[0m
[32m[2022-09-05 12:53:38,310] [    INFO][0m - loss: 0.62786598, learning_rate: 4.6500000000000005e-05, global_step: 90, interval_runtime: 0.9004, interval_samples_per_second: 8.885, interval_steps_per_second: 11.107, epoch: 4.5[0m
[32m[2022-09-05 12:53:39,159] [    INFO][0m - loss: 0.60871358, learning_rate: 4.5e-05, global_step: 100, interval_runtime: 0.849, interval_samples_per_second: 9.423, interval_steps_per_second: 11.779, epoch: 5.0[0m
[32m[2022-09-05 12:53:39,160] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 12:53:39,160] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-05 12:53:39,160] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 12:53:39,160] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 12:53:39,160] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-05 12:53:40,002] [    INFO][0m - eval_loss: 0.8695719242095947, eval_accuracy: 0.5283018867924528, eval_runtime: 0.8416, eval_samples_per_second: 188.936, eval_steps_per_second: 23.766, epoch: 5.0[0m
[32m[2022-09-05 12:53:40,009] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-100[0m
[32m[2022-09-05 12:53:40,009] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 12:53:43,202] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-05 12:53:43,203] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-05 12:53:49,389] [    INFO][0m - loss: 0.5113296, learning_rate: 4.35e-05, global_step: 110, interval_runtime: 10.2297, interval_samples_per_second: 0.782, interval_steps_per_second: 0.978, epoch: 5.5[0m
[32m[2022-09-05 12:53:50,237] [    INFO][0m - loss: 0.50696387, learning_rate: 4.2e-05, global_step: 120, interval_runtime: 0.8485, interval_samples_per_second: 9.428, interval_steps_per_second: 11.785, epoch: 6.0[0m
[32m[2022-09-05 12:53:51,137] [    INFO][0m - loss: 0.49094114, learning_rate: 4.05e-05, global_step: 130, interval_runtime: 0.8994, interval_samples_per_second: 8.894, interval_steps_per_second: 11.118, epoch: 6.5[0m
[32m[2022-09-05 12:53:51,988] [    INFO][0m - loss: 0.28984246, learning_rate: 3.9e-05, global_step: 140, interval_runtime: 0.8512, interval_samples_per_second: 9.399, interval_steps_per_second: 11.748, epoch: 7.0[0m
[32m[2022-09-05 12:53:52,886] [    INFO][0m - loss: 0.52431355, learning_rate: 3.7500000000000003e-05, global_step: 150, interval_runtime: 0.8977, interval_samples_per_second: 8.912, interval_steps_per_second: 11.14, epoch: 7.5[0m
[32m[2022-09-05 12:53:53,737] [    INFO][0m - loss: 0.20750639, learning_rate: 3.6e-05, global_step: 160, interval_runtime: 0.8509, interval_samples_per_second: 9.402, interval_steps_per_second: 11.752, epoch: 8.0[0m
[32m[2022-09-05 12:53:54,637] [    INFO][0m - loss: 0.3691045, learning_rate: 3.45e-05, global_step: 170, interval_runtime: 0.9001, interval_samples_per_second: 8.888, interval_steps_per_second: 11.11, epoch: 8.5[0m
[32m[2022-09-05 12:53:55,488] [    INFO][0m - loss: 0.29111829, learning_rate: 3.3e-05, global_step: 180, interval_runtime: 0.8511, interval_samples_per_second: 9.4, interval_steps_per_second: 11.75, epoch: 9.0[0m
[32m[2022-09-05 12:53:56,397] [    INFO][0m - loss: 0.37391052, learning_rate: 3.15e-05, global_step: 190, interval_runtime: 0.9088, interval_samples_per_second: 8.802, interval_steps_per_second: 11.003, epoch: 9.5[0m
[32m[2022-09-05 12:53:57,248] [    INFO][0m - loss: 0.33479362, learning_rate: 3e-05, global_step: 200, interval_runtime: 0.851, interval_samples_per_second: 9.401, interval_steps_per_second: 11.751, epoch: 10.0[0m
[32m[2022-09-05 12:53:57,248] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 12:53:57,248] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-05 12:53:57,248] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 12:53:57,248] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 12:53:57,249] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-05 12:53:58,077] [    INFO][0m - eval_loss: 1.5416159629821777, eval_accuracy: 0.5283018867924528, eval_runtime: 0.8279, eval_samples_per_second: 192.048, eval_steps_per_second: 24.157, epoch: 10.0[0m
[32m[2022-09-05 12:53:58,084] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-200[0m
[32m[2022-09-05 12:53:58,085] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 12:54:01,615] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-200/tokenizer_config.json[0m
[32m[2022-09-05 12:54:01,615] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-200/special_tokens_map.json[0m
[32m[2022-09-05 12:54:08,748] [    INFO][0m - loss: 0.19992018, learning_rate: 2.8499999999999998e-05, global_step: 210, interval_runtime: 11.4997, interval_samples_per_second: 0.696, interval_steps_per_second: 0.87, epoch: 10.5[0m
[32m[2022-09-05 12:54:09,597] [    INFO][0m - loss: 0.21953316, learning_rate: 2.7000000000000002e-05, global_step: 220, interval_runtime: 0.8499, interval_samples_per_second: 9.413, interval_steps_per_second: 11.766, epoch: 11.0[0m
[32m[2022-09-05 12:54:10,503] [    INFO][0m - loss: 0.15794064, learning_rate: 2.55e-05, global_step: 230, interval_runtime: 0.9057, interval_samples_per_second: 8.833, interval_steps_per_second: 11.041, epoch: 11.5[0m
[32m[2022-09-05 12:54:11,361] [    INFO][0m - loss: 0.24971695, learning_rate: 2.4e-05, global_step: 240, interval_runtime: 0.8582, interval_samples_per_second: 9.322, interval_steps_per_second: 11.652, epoch: 12.0[0m
[32m[2022-09-05 12:54:12,269] [    INFO][0m - loss: 0.08249285, learning_rate: 2.25e-05, global_step: 250, interval_runtime: 0.9081, interval_samples_per_second: 8.809, interval_steps_per_second: 11.011, epoch: 12.5[0m
[32m[2022-09-05 12:54:13,127] [    INFO][0m - loss: 0.22233622, learning_rate: 2.1e-05, global_step: 260, interval_runtime: 0.8573, interval_samples_per_second: 9.332, interval_steps_per_second: 11.665, epoch: 13.0[0m
[32m[2022-09-05 12:54:14,036] [    INFO][0m - loss: 0.10232141, learning_rate: 1.95e-05, global_step: 270, interval_runtime: 0.9097, interval_samples_per_second: 8.794, interval_steps_per_second: 10.993, epoch: 13.5[0m
[32m[2022-09-05 12:54:14,891] [    INFO][0m - loss: 0.16397972, learning_rate: 1.8e-05, global_step: 280, interval_runtime: 0.8544, interval_samples_per_second: 9.363, interval_steps_per_second: 11.704, epoch: 14.0[0m
[32m[2022-09-05 12:54:15,802] [    INFO][0m - loss: 0.11864798, learning_rate: 1.65e-05, global_step: 290, interval_runtime: 0.9113, interval_samples_per_second: 8.779, interval_steps_per_second: 10.974, epoch: 14.5[0m
[32m[2022-09-05 12:54:16,660] [    INFO][0m - loss: 0.35097332, learning_rate: 1.5e-05, global_step: 300, interval_runtime: 0.8582, interval_samples_per_second: 9.322, interval_steps_per_second: 11.653, epoch: 15.0[0m
[32m[2022-09-05 12:54:16,661] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 12:54:16,661] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-05 12:54:16,661] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 12:54:16,661] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 12:54:16,661] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-05 12:54:17,482] [    INFO][0m - eval_loss: 2.5384159088134766, eval_accuracy: 0.5031446540880503, eval_runtime: 0.8206, eval_samples_per_second: 193.758, eval_steps_per_second: 24.372, epoch: 15.0[0m
[32m[2022-09-05 12:54:17,487] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-300[0m
[32m[2022-09-05 12:54:17,488] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 12:54:20,516] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-300/tokenizer_config.json[0m
[32m[2022-09-05 12:54:20,516] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-300/special_tokens_map.json[0m
[32m[2022-09-05 12:54:26,372] [    INFO][0m - loss: 0.0902176, learning_rate: 1.3500000000000001e-05, global_step: 310, interval_runtime: 9.7111, interval_samples_per_second: 0.824, interval_steps_per_second: 1.03, epoch: 15.5[0m
[32m[2022-09-05 12:54:27,226] [    INFO][0m - loss: 0.04582311, learning_rate: 1.2e-05, global_step: 320, interval_runtime: 0.8548, interval_samples_per_second: 9.359, interval_steps_per_second: 11.699, epoch: 16.0[0m
[32m[2022-09-05 12:54:28,135] [    INFO][0m - loss: 0.05026967, learning_rate: 1.05e-05, global_step: 330, interval_runtime: 0.9092, interval_samples_per_second: 8.799, interval_steps_per_second: 10.999, epoch: 16.5[0m
[32m[2022-09-05 12:54:28,993] [    INFO][0m - loss: 0.02132454, learning_rate: 9e-06, global_step: 340, interval_runtime: 0.8574, interval_samples_per_second: 9.33, interval_steps_per_second: 11.663, epoch: 17.0[0m
[32m[2022-09-05 12:54:29,897] [    INFO][0m - loss: 0.05567656, learning_rate: 7.5e-06, global_step: 350, interval_runtime: 0.9045, interval_samples_per_second: 8.844, interval_steps_per_second: 11.056, epoch: 17.5[0m
[32m[2022-09-05 12:54:30,751] [    INFO][0m - loss: 0.03398293, learning_rate: 6e-06, global_step: 360, interval_runtime: 0.8535, interval_samples_per_second: 9.373, interval_steps_per_second: 11.716, epoch: 18.0[0m
[32m[2022-09-05 12:54:31,653] [    INFO][0m - loss: 0.05663049, learning_rate: 4.5e-06, global_step: 370, interval_runtime: 0.9026, interval_samples_per_second: 8.863, interval_steps_per_second: 11.079, epoch: 18.5[0m
[32m[2022-09-05 12:54:32,504] [    INFO][0m - loss: 0.0173925, learning_rate: 3e-06, global_step: 380, interval_runtime: 0.851, interval_samples_per_second: 9.401, interval_steps_per_second: 11.751, epoch: 19.0[0m
[32m[2022-09-05 12:54:33,412] [    INFO][0m - loss: 0.02255355, learning_rate: 1.5e-06, global_step: 390, interval_runtime: 0.9071, interval_samples_per_second: 8.82, interval_steps_per_second: 11.025, epoch: 19.5[0m
[32m[2022-09-05 12:54:34,268] [    INFO][0m - loss: 0.0025538, learning_rate: 0.0, global_step: 400, interval_runtime: 0.8566, interval_samples_per_second: 9.34, interval_steps_per_second: 11.675, epoch: 20.0[0m
[32m[2022-09-05 12:54:34,268] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 12:54:34,269] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-05 12:54:34,269] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 12:54:34,269] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 12:54:34,269] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-05 12:54:35,107] [    INFO][0m - eval_loss: 3.0770199298858643, eval_accuracy: 0.46540880503144655, eval_runtime: 0.8377, eval_samples_per_second: 189.806, eval_steps_per_second: 23.875, epoch: 20.0[0m
[32m[2022-09-05 12:54:35,115] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-400[0m
[32m[2022-09-05 12:54:35,115] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 12:54:38,146] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-400/tokenizer_config.json[0m
[32m[2022-09-05 12:54:38,147] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-400/special_tokens_map.json[0m
[32m[2022-09-05 12:54:43,427] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-05 12:54:43,428] [    INFO][0m - Loading best model from ./checkpoints_cluewsc/checkpoint-100 (score: 0.5283018867924528).[0m
[32m[2022-09-05 12:54:44,294] [    INFO][0m - train_runtime: 75.0738, train_samples_per_second: 42.625, train_steps_per_second: 5.328, train_loss: 0.39138902714010326, epoch: 20.0[0m
[32m[2022-09-05 12:54:44,325] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/[0m
[32m[2022-09-05 12:54:44,326] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 12:54:47,422] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/tokenizer_config.json[0m
[32m[2022-09-05 12:54:47,422] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/special_tokens_map.json[0m
[32m[2022-09-05 12:54:47,424] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-05 12:54:47,424] [    INFO][0m -   epoch                    =       20.0[0m
[32m[2022-09-05 12:54:47,424] [    INFO][0m -   train_loss               =     0.3914[0m
[32m[2022-09-05 12:54:47,424] [    INFO][0m -   train_runtime            = 0:01:15.07[0m
[32m[2022-09-05 12:54:47,424] [    INFO][0m -   train_samples_per_second =     42.625[0m
[32m[2022-09-05 12:54:47,424] [    INFO][0m -   train_steps_per_second   =      5.328[0m
[32m[2022-09-05 12:54:47,428] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-05 12:54:47,428] [    INFO][0m -   Num examples = 976[0m
[32m[2022-09-05 12:54:47,428] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 12:54:47,428] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 12:54:47,428] [    INFO][0m -   Total prediction steps = 122[0m
[32m[2022-09-05 12:54:52,558] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-05 12:54:52,559] [    INFO][0m -   test_accuracy           =     0.4816[0m
[32m[2022-09-05 12:54:52,559] [    INFO][0m -   test_loss               =     1.0273[0m
[32m[2022-09-05 12:54:52,559] [    INFO][0m -   test_runtime            = 0:00:05.12[0m
[32m[2022-09-05 12:54:52,559] [    INFO][0m -   test_samples_per_second =    190.255[0m
[32m[2022-09-05 12:54:52,559] [    INFO][0m -   test_steps_per_second   =     23.782[0m
[33m[2022-09-05 12:54:53,473] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-09-05 12:54:53,474] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-05 12:54:53,474] [    INFO][0m - ============================================================[0m
[32m[2022-09-05 12:54:53,474] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-05 12:54:53,474] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-05 12:54:53,474] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-05 12:54:53,474] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-05 12:54:53,474] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-05 12:54:53,474] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-05 12:54:53,474] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-09-05 12:54:53,474] [    INFO][0m - [0m
[32m[2022-09-05 12:54:53,475] [    INFO][0m - ============================================================[0m
[32m[2022-09-05 12:54:53,475] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-05 12:54:53,475] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-05 12:54:53,475] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-05 12:54:53,475] [    INFO][0m - prompt                        :[0m
[32m[2022-09-05 12:54:53,475] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-05 12:54:53,475] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-05 12:54:53,475] [    INFO][0m - task_name                     :cluewsc[0m
[32m[2022-09-05 12:54:53,475] [    INFO][0m - [0m
[32m[2022-09-05 12:54:53,476] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
[32m[2022-09-05 12:54:54,851] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-09-05 12:54:54,876] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-09-05 12:54:54,876] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[32m[2022-09-05 12:54:54,877] [    INFO][0m - Using template: [{'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': 'ËøôÂè•ËØù‰∏≠‰ª£ËØç'}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': 'ÂØπÂêóÔºü'}, {'add_prefix_space': '', 'mask': None}][0m
2022-09-05 12:54:54,880 INFO [download.py:119] unique_endpoints {''}
[32m[2022-09-05 12:54:55,055] [    INFO][0m - ============================================================[0m
[32m[2022-09-05 12:54:55,056] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-05 12:54:55,056] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-05 12:54:55,056] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-05 12:54:55,056] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-05 12:54:55,056] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-05 12:54:55,056] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-05 12:54:55,056] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-05 12:54:55,056] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-05 12:54:55,056] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-05 12:54:55,056] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-05 12:54:55,057] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-05 12:54:55,057] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-05 12:54:55,057] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-05 12:54:55,057] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-05 12:54:55,057] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-05 12:54:55,057] [    INFO][0m - do_predict                    :False[0m
[32m[2022-09-05 12:54:55,057] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-05 12:54:55,057] [    INFO][0m - eval_batch_size               :8[0m
[32m[2022-09-05 12:54:55,057] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-09-05 12:54:55,057] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-09-05 12:54:55,057] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-05 12:54:55,057] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-05 12:54:55,057] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-05 12:54:55,058] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-05 12:54:55,058] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-05 12:54:55,058] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-05 12:54:55,058] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-05 12:54:55,058] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-05 12:54:55,058] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-05 12:54:55,058] [    INFO][0m - learning_rate                 :6e-05[0m
[32m[2022-09-05 12:54:55,058] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-05 12:54:55,058] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-05 12:54:55,058] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-05 12:54:55,058] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-05 12:54:55,058] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-05 12:54:55,058] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-05 12:54:55,059] [    INFO][0m - logging_dir                   :./checkpoints_cluewsc/runs/Sep05_12-54-53_instance-3bwob41y-01[0m
[32m[2022-09-05 12:54:55,059] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-05 12:54:55,059] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-05 12:54:55,059] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-05 12:54:55,059] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-05 12:54:55,059] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-05 12:54:55,059] [    INFO][0m - max_seq_length                :128[0m
[32m[2022-09-05 12:54:55,059] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-05 12:54:55,059] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-05 12:54:55,059] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-05 12:54:55,059] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-05 12:54:55,059] [    INFO][0m - num_train_epochs              :20.0[0m
[32m[2022-09-05 12:54:55,059] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-05 12:54:55,059] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-05 12:54:55,060] [    INFO][0m - output_dir                    :./checkpoints_cluewsc/[0m
[32m[2022-09-05 12:54:55,060] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-05 12:54:55,060] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-05 12:54:55,060] [    INFO][0m - per_device_eval_batch_size    :8[0m
[32m[2022-09-05 12:54:55,060] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-09-05 12:54:55,060] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-05 12:54:55,060] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-05 12:54:55,060] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-05 12:54:55,060] [    INFO][0m - ppt_learning_rate             :6e-05[0m
[32m[2022-09-05 12:54:55,060] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-05 12:54:55,060] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-05 12:54:55,060] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-05 12:54:55,060] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-05 12:54:55,061] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-05 12:54:55,061] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-05 12:54:55,061] [    INFO][0m - run_name                      :./checkpoints_cluewsc/[0m
[32m[2022-09-05 12:54:55,061] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-05 12:54:55,061] [    INFO][0m - save_steps                    :100[0m
[32m[2022-09-05 12:54:55,061] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-09-05 12:54:55,061] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-05 12:54:55,061] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-05 12:54:55,061] [    INFO][0m - seed                          :42[0m
[32m[2022-09-05 12:54:55,061] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-05 12:54:55,061] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-05 12:54:55,061] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-05 12:54:55,061] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-09-05 12:54:55,062] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-05 12:54:55,062] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-05 12:54:55,062] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-05 12:54:55,062] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-05 12:54:55,062] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-05 12:54:55,062] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-05 12:54:55,062] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-05 12:54:55,062] [    INFO][0m - [0m
[32m[2022-09-05 12:54:55,063] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-05 12:54:55,063] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-05 12:54:55,064] [    INFO][0m -   Num Epochs = 20[0m
[32m[2022-09-05 12:54:55,064] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-09-05 12:54:55,064] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-09-05 12:54:55,064] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-05 12:54:55,064] [    INFO][0m -   Total optimization steps = 400.0[0m
[32m[2022-09-05 12:54:55,064] [    INFO][0m -   Total num train samples = 3200[0m
[32m[2022-09-05 12:54:56,023] [    INFO][0m - loss: 2.44831581, learning_rate: 5.85e-05, global_step: 10, interval_runtime: 0.9576, interval_samples_per_second: 8.354, interval_steps_per_second: 10.443, epoch: 0.5[0m
[32m[2022-09-05 12:54:56,878] [    INFO][0m - loss: 1.07582407, learning_rate: 5.6999999999999996e-05, global_step: 20, interval_runtime: 0.8559, interval_samples_per_second: 9.347, interval_steps_per_second: 11.684, epoch: 1.0[0m
[32m[2022-09-05 12:54:57,822] [    INFO][0m - loss: 1.0978528, learning_rate: 5.550000000000001e-05, global_step: 30, interval_runtime: 0.9431, interval_samples_per_second: 8.482, interval_steps_per_second: 10.603, epoch: 1.5[0m
[32m[2022-09-05 12:54:58,683] [    INFO][0m - loss: 0.79818273, learning_rate: 5.4000000000000005e-05, global_step: 40, interval_runtime: 0.8616, interval_samples_per_second: 9.285, interval_steps_per_second: 11.606, epoch: 2.0[0m
[32m[2022-09-05 12:54:59,596] [    INFO][0m - loss: 0.76520696, learning_rate: 5.25e-05, global_step: 50, interval_runtime: 0.9127, interval_samples_per_second: 8.765, interval_steps_per_second: 10.957, epoch: 2.5[0m
[32m[2022-09-05 12:55:00,450] [    INFO][0m - loss: 0.77223978, learning_rate: 5.1e-05, global_step: 60, interval_runtime: 0.8548, interval_samples_per_second: 9.359, interval_steps_per_second: 11.699, epoch: 3.0[0m
[32m[2022-09-05 12:55:01,364] [    INFO][0m - loss: 0.74750257, learning_rate: 4.95e-05, global_step: 70, interval_runtime: 0.9133, interval_samples_per_second: 8.76, interval_steps_per_second: 10.95, epoch: 3.5[0m
[32m[2022-09-05 12:55:02,221] [    INFO][0m - loss: 1.15831671, learning_rate: 4.8e-05, global_step: 80, interval_runtime: 0.8567, interval_samples_per_second: 9.338, interval_steps_per_second: 11.672, epoch: 4.0[0m
[32m[2022-09-05 12:55:03,134] [    INFO][0m - loss: 0.74244947, learning_rate: 4.6500000000000005e-05, global_step: 90, interval_runtime: 0.9138, interval_samples_per_second: 8.755, interval_steps_per_second: 10.943, epoch: 4.5[0m
[32m[2022-09-05 12:55:03,989] [    INFO][0m - loss: 0.7427712, learning_rate: 4.5e-05, global_step: 100, interval_runtime: 0.8553, interval_samples_per_second: 9.353, interval_steps_per_second: 11.691, epoch: 5.0[0m
[32m[2022-09-05 12:55:03,990] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 12:55:03,990] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-05 12:55:03,990] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 12:55:03,990] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 12:55:03,990] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-05 12:55:04,770] [    INFO][0m - eval_loss: 0.8116286396980286, eval_accuracy: 0.5534591194968553, eval_runtime: 0.7797, eval_samples_per_second: 203.934, eval_steps_per_second: 25.652, epoch: 5.0[0m
[32m[2022-09-05 12:55:04,771] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-100[0m
[32m[2022-09-05 12:55:04,771] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 12:55:07,931] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-05 12:55:07,931] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-05 12:55:14,566] [    INFO][0m - loss: 0.65346203, learning_rate: 4.35e-05, global_step: 110, interval_runtime: 10.576, interval_samples_per_second: 0.756, interval_steps_per_second: 0.946, epoch: 5.5[0m
[32m[2022-09-05 12:55:15,418] [    INFO][0m - loss: 0.44328914, learning_rate: 4.2e-05, global_step: 120, interval_runtime: 0.8523, interval_samples_per_second: 9.387, interval_steps_per_second: 11.733, epoch: 6.0[0m
[32m[2022-09-05 12:55:16,333] [    INFO][0m - loss: 0.39121718, learning_rate: 4.05e-05, global_step: 130, interval_runtime: 0.9149, interval_samples_per_second: 8.744, interval_steps_per_second: 10.93, epoch: 6.5[0m
[32m[2022-09-05 12:55:17,189] [    INFO][0m - loss: 0.26415017, learning_rate: 3.9e-05, global_step: 140, interval_runtime: 0.8563, interval_samples_per_second: 9.343, interval_steps_per_second: 11.679, epoch: 7.0[0m
[32m[2022-09-05 12:55:18,098] [    INFO][0m - loss: 0.25285056, learning_rate: 3.7500000000000003e-05, global_step: 150, interval_runtime: 0.9092, interval_samples_per_second: 8.799, interval_steps_per_second: 10.998, epoch: 7.5[0m
[32m[2022-09-05 12:55:18,955] [    INFO][0m - loss: 0.39690139, learning_rate: 3.6e-05, global_step: 160, interval_runtime: 0.8564, interval_samples_per_second: 9.341, interval_steps_per_second: 11.676, epoch: 8.0[0m
[32m[2022-09-05 12:55:19,867] [    INFO][0m - loss: 0.13409811, learning_rate: 3.45e-05, global_step: 170, interval_runtime: 0.912, interval_samples_per_second: 8.772, interval_steps_per_second: 10.965, epoch: 8.5[0m
[32m[2022-09-05 12:55:20,724] [    INFO][0m - loss: 0.45122423, learning_rate: 3.3e-05, global_step: 180, interval_runtime: 0.8573, interval_samples_per_second: 9.331, interval_steps_per_second: 11.664, epoch: 9.0[0m
[32m[2022-09-05 12:55:21,639] [    INFO][0m - loss: 0.19935486, learning_rate: 3.15e-05, global_step: 190, interval_runtime: 0.9148, interval_samples_per_second: 8.745, interval_steps_per_second: 10.932, epoch: 9.5[0m
[32m[2022-09-05 12:55:22,501] [    INFO][0m - loss: 0.10878752, learning_rate: 3e-05, global_step: 200, interval_runtime: 0.8623, interval_samples_per_second: 9.277, interval_steps_per_second: 11.597, epoch: 10.0[0m
[32m[2022-09-05 12:55:22,502] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 12:55:22,502] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-05 12:55:22,502] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 12:55:22,502] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 12:55:22,502] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-05 12:55:23,288] [    INFO][0m - eval_loss: 3.4341721534729004, eval_accuracy: 0.4968553459119497, eval_runtime: 0.7861, eval_samples_per_second: 202.262, eval_steps_per_second: 25.442, epoch: 10.0[0m
[32m[2022-09-05 12:55:23,289] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-200[0m
[32m[2022-09-05 12:55:23,289] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 12:55:26,412] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-200/tokenizer_config.json[0m
[32m[2022-09-05 12:55:26,413] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-200/special_tokens_map.json[0m
[32m[2022-09-05 12:55:32,457] [    INFO][0m - loss: 0.0729233, learning_rate: 2.8499999999999998e-05, global_step: 210, interval_runtime: 9.9555, interval_samples_per_second: 0.804, interval_steps_per_second: 1.004, epoch: 10.5[0m
[32m[2022-09-05 12:55:33,314] [    INFO][0m - loss: 0.06010789, learning_rate: 2.7000000000000002e-05, global_step: 220, interval_runtime: 0.8576, interval_samples_per_second: 9.328, interval_steps_per_second: 11.66, epoch: 11.0[0m
[32m[2022-09-05 12:55:34,232] [    INFO][0m - loss: 0.04791601, learning_rate: 2.55e-05, global_step: 230, interval_runtime: 0.9182, interval_samples_per_second: 8.713, interval_steps_per_second: 10.891, epoch: 11.5[0m
[32m[2022-09-05 12:55:35,092] [    INFO][0m - loss: 0.08208423, learning_rate: 2.4e-05, global_step: 240, interval_runtime: 0.8597, interval_samples_per_second: 9.306, interval_steps_per_second: 11.632, epoch: 12.0[0m
[32m[2022-09-05 12:55:36,025] [    INFO][0m - loss: 0.0925156, learning_rate: 2.25e-05, global_step: 250, interval_runtime: 0.9328, interval_samples_per_second: 8.576, interval_steps_per_second: 10.72, epoch: 12.5[0m
[32m[2022-09-05 12:55:36,888] [    INFO][0m - loss: 0.11926476, learning_rate: 2.1e-05, global_step: 260, interval_runtime: 0.863, interval_samples_per_second: 9.27, interval_steps_per_second: 11.588, epoch: 13.0[0m
[32m[2022-09-05 12:55:37,862] [    INFO][0m - loss: 0.31324544, learning_rate: 1.95e-05, global_step: 270, interval_runtime: 0.9736, interval_samples_per_second: 8.217, interval_steps_per_second: 10.271, epoch: 13.5[0m
[32m[2022-09-05 12:55:38,724] [    INFO][0m - loss: 0.00080981, learning_rate: 1.8e-05, global_step: 280, interval_runtime: 0.8629, interval_samples_per_second: 9.271, interval_steps_per_second: 11.589, epoch: 14.0[0m
[32m[2022-09-05 12:55:39,640] [    INFO][0m - loss: 0.00768714, learning_rate: 1.65e-05, global_step: 290, interval_runtime: 0.916, interval_samples_per_second: 8.734, interval_steps_per_second: 10.917, epoch: 14.5[0m
[32m[2022-09-05 12:55:40,500] [    INFO][0m - loss: 0.00878063, learning_rate: 1.5e-05, global_step: 300, interval_runtime: 0.8595, interval_samples_per_second: 9.308, interval_steps_per_second: 11.635, epoch: 15.0[0m
[32m[2022-09-05 12:55:40,501] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 12:55:40,501] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-05 12:55:40,501] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 12:55:40,501] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 12:55:40,501] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-05 12:55:41,427] [    INFO][0m - eval_loss: 4.372812747955322, eval_accuracy: 0.4968553459119497, eval_runtime: 0.9259, eval_samples_per_second: 171.716, eval_steps_per_second: 21.6, epoch: 15.0[0m
[32m[2022-09-05 12:55:41,428] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-300[0m
[32m[2022-09-05 12:55:41,428] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 12:55:44,779] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-300/tokenizer_config.json[0m
[32m[2022-09-05 12:55:44,779] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-300/special_tokens_map.json[0m
[32m[2022-09-05 12:55:51,141] [    INFO][0m - loss: 0.10796857, learning_rate: 1.3500000000000001e-05, global_step: 310, interval_runtime: 10.6405, interval_samples_per_second: 0.752, interval_steps_per_second: 0.94, epoch: 15.5[0m
[32m[2022-09-05 12:55:51,997] [    INFO][0m - loss: 0.04274213, learning_rate: 1.2e-05, global_step: 320, interval_runtime: 0.8562, interval_samples_per_second: 9.344, interval_steps_per_second: 11.68, epoch: 16.0[0m
[32m[2022-09-05 12:55:52,908] [    INFO][0m - loss: 0.00081178, learning_rate: 1.05e-05, global_step: 330, interval_runtime: 0.9118, interval_samples_per_second: 8.774, interval_steps_per_second: 10.967, epoch: 16.5[0m
[32m[2022-09-05 12:55:53,765] [    INFO][0m - loss: 0.00058787, learning_rate: 9e-06, global_step: 340, interval_runtime: 0.8565, interval_samples_per_second: 9.34, interval_steps_per_second: 11.675, epoch: 17.0[0m
[32m[2022-09-05 12:55:54,679] [    INFO][0m - loss: 6.348e-05, learning_rate: 7.5e-06, global_step: 350, interval_runtime: 0.9133, interval_samples_per_second: 8.759, interval_steps_per_second: 10.949, epoch: 17.5[0m
[32m[2022-09-05 12:55:55,536] [    INFO][0m - loss: 7.355e-05, learning_rate: 6e-06, global_step: 360, interval_runtime: 0.8575, interval_samples_per_second: 9.33, interval_steps_per_second: 11.662, epoch: 18.0[0m
[32m[2022-09-05 12:55:56,450] [    INFO][0m - loss: 0.00868854, learning_rate: 4.5e-06, global_step: 370, interval_runtime: 0.9143, interval_samples_per_second: 8.749, interval_steps_per_second: 10.937, epoch: 18.5[0m
[32m[2022-09-05 12:55:57,306] [    INFO][0m - loss: 9.921e-05, learning_rate: 3e-06, global_step: 380, interval_runtime: 0.856, interval_samples_per_second: 9.346, interval_steps_per_second: 11.682, epoch: 19.0[0m
[32m[2022-09-05 12:55:58,219] [    INFO][0m - loss: 0.02848444, learning_rate: 1.5e-06, global_step: 390, interval_runtime: 0.9133, interval_samples_per_second: 8.759, interval_steps_per_second: 10.949, epoch: 19.5[0m
[32m[2022-09-05 12:55:59,078] [    INFO][0m - loss: 0.00017408, learning_rate: 0.0, global_step: 400, interval_runtime: 0.8585, interval_samples_per_second: 9.319, interval_steps_per_second: 11.649, epoch: 20.0[0m
[32m[2022-09-05 12:55:59,079] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 12:55:59,079] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-05 12:55:59,079] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 12:55:59,079] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 12:55:59,079] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-05 12:55:59,879] [    INFO][0m - eval_loss: 4.8945465087890625, eval_accuracy: 0.4968553459119497, eval_runtime: 0.7992, eval_samples_per_second: 198.943, eval_steps_per_second: 25.024, epoch: 20.0[0m
[32m[2022-09-05 12:55:59,879] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-400[0m
[32m[2022-09-05 12:55:59,879] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 12:56:02,831] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-400/tokenizer_config.json[0m
[32m[2022-09-05 12:56:02,831] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-400/special_tokens_map.json[0m
[32m[2022-09-05 12:56:08,138] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-05 12:56:08,139] [    INFO][0m - Loading best model from ./checkpoints_cluewsc/checkpoint-100 (score: 0.5534591194968553).[0m
[32m[2022-09-05 12:56:09,091] [    INFO][0m - train_runtime: 74.0256, train_samples_per_second: 43.228, train_steps_per_second: 5.404, train_loss: 0.3659756439624471, epoch: 20.0[0m
[32m[2022-09-05 12:56:09,131] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/[0m
[32m[2022-09-05 12:56:09,132] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 12:56:13,182] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/tokenizer_config.json[0m
[32m[2022-09-05 12:56:13,183] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/special_tokens_map.json[0m
[32m[2022-09-05 12:56:13,184] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-05 12:56:13,184] [    INFO][0m -   epoch                    =       20.0[0m
[32m[2022-09-05 12:56:13,185] [    INFO][0m -   train_loss               =      0.366[0m
[32m[2022-09-05 12:56:13,185] [    INFO][0m -   train_runtime            = 0:01:14.02[0m
[32m[2022-09-05 12:56:13,185] [    INFO][0m -   train_samples_per_second =     43.228[0m
[32m[2022-09-05 12:56:13,185] [    INFO][0m -   train_steps_per_second   =      5.404[0m
[32m[2022-09-05 12:56:13,188] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-05 12:56:13,188] [    INFO][0m -   Num examples = 976[0m
[32m[2022-09-05 12:56:13,188] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 12:56:13,188] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 12:56:13,188] [    INFO][0m -   Total prediction steps = 122[0m
[32m[2022-09-05 12:56:17,933] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-05 12:56:17,934] [    INFO][0m -   test_accuracy           =     0.4939[0m
[32m[2022-09-05 12:56:17,934] [    INFO][0m -   test_loss               =     0.8814[0m
[32m[2022-09-05 12:56:17,934] [    INFO][0m -   test_runtime            = 0:00:04.74[0m
[32m[2022-09-05 12:56:17,934] [    INFO][0m -   test_samples_per_second =    205.689[0m
[32m[2022-09-05 12:56:17,934] [    INFO][0m -   test_steps_per_second   =     25.711[0m
[33m[2022-09-05 12:56:18,951] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-09-05 12:56:18,951] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-05 12:56:18,951] [    INFO][0m - ============================================================[0m
[32m[2022-09-05 12:56:18,951] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-05 12:56:18,951] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-05 12:56:18,952] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-05 12:56:18,952] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-05 12:56:18,952] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-05 12:56:18,952] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-05 12:56:18,952] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-09-05 12:56:18,952] [    INFO][0m - [0m
[32m[2022-09-05 12:56:18,952] [    INFO][0m - ============================================================[0m
[32m[2022-09-05 12:56:18,952] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-05 12:56:18,952] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-05 12:56:18,952] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-05 12:56:18,952] [    INFO][0m - prompt                        :[0m
[32m[2022-09-05 12:56:18,952] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-05 12:56:18,953] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-05 12:56:18,953] [    INFO][0m - task_name                     :cluewsc[0m
[32m[2022-09-05 12:56:18,953] [    INFO][0m - [0m
[32m[2022-09-05 12:56:18,953] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
[32m[2022-09-05 12:56:20,217] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-09-05 12:56:20,241] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-09-05 12:56:20,241] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[32m[2022-09-05 12:56:20,243] [    INFO][0m - Using template: [{'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': 'ËøôÂè•ËØù‰∏≠ËØçËØ≠'}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': 'ÂØπÂêóÔºü'}, {'add_prefix_space': '', 'mask': None}][0m
2022-09-05 12:56:20,245 INFO [download.py:119] unique_endpoints {''}
[32m[2022-09-05 12:56:20,259] [    INFO][0m - ============================================================[0m
[32m[2022-09-05 12:56:20,259] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-05 12:56:20,259] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-05 12:56:20,259] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-05 12:56:20,259] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-05 12:56:20,260] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-05 12:56:20,260] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-05 12:56:20,260] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-05 12:56:20,260] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-05 12:56:20,260] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-05 12:56:20,260] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-05 12:56:20,260] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-05 12:56:20,260] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-05 12:56:20,260] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-05 12:56:20,260] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-05 12:56:20,260] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-05 12:56:20,260] [    INFO][0m - do_predict                    :False[0m
[32m[2022-09-05 12:56:20,260] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-05 12:56:20,261] [    INFO][0m - eval_batch_size               :8[0m
[32m[2022-09-05 12:56:20,261] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-09-05 12:56:20,261] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-09-05 12:56:20,261] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-05 12:56:20,261] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-05 12:56:20,261] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-05 12:56:20,261] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-05 12:56:20,261] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-05 12:56:20,261] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-05 12:56:20,261] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-05 12:56:20,261] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-05 12:56:20,261] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-05 12:56:20,261] [    INFO][0m - learning_rate                 :6e-05[0m
[32m[2022-09-05 12:56:20,262] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-05 12:56:20,262] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-05 12:56:20,262] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-05 12:56:20,262] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-05 12:56:20,262] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-05 12:56:20,262] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-05 12:56:20,262] [    INFO][0m - logging_dir                   :./checkpoints_cluewsc/runs/Sep05_12-56-18_instance-3bwob41y-01[0m
[32m[2022-09-05 12:56:20,262] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-05 12:56:20,262] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-05 12:56:20,262] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-05 12:56:20,262] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-05 12:56:20,262] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-05 12:56:20,262] [    INFO][0m - max_seq_length                :128[0m
[32m[2022-09-05 12:56:20,263] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-05 12:56:20,263] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-05 12:56:20,263] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-05 12:56:20,263] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-05 12:56:20,263] [    INFO][0m - num_train_epochs              :20.0[0m
[32m[2022-09-05 12:56:20,263] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-05 12:56:20,263] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-05 12:56:20,263] [    INFO][0m - output_dir                    :./checkpoints_cluewsc/[0m
[32m[2022-09-05 12:56:20,263] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-05 12:56:20,263] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-05 12:56:20,263] [    INFO][0m - per_device_eval_batch_size    :8[0m
[32m[2022-09-05 12:56:20,263] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-09-05 12:56:20,263] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-05 12:56:20,264] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-05 12:56:20,264] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-05 12:56:20,264] [    INFO][0m - ppt_learning_rate             :6e-05[0m
[32m[2022-09-05 12:56:20,264] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-05 12:56:20,264] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-05 12:56:20,264] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-05 12:56:20,264] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-05 12:56:20,264] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-05 12:56:20,264] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-05 12:56:20,264] [    INFO][0m - run_name                      :./checkpoints_cluewsc/[0m
[32m[2022-09-05 12:56:20,264] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-05 12:56:20,264] [    INFO][0m - save_steps                    :100[0m
[32m[2022-09-05 12:56:20,264] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-09-05 12:56:20,264] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-05 12:56:20,265] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-05 12:56:20,265] [    INFO][0m - seed                          :42[0m
[32m[2022-09-05 12:56:20,265] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-05 12:56:20,265] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-05 12:56:20,265] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-05 12:56:20,265] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-09-05 12:56:20,265] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-05 12:56:20,265] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-05 12:56:20,265] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-05 12:56:20,265] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-05 12:56:20,265] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-05 12:56:20,265] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-05 12:56:20,265] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-05 12:56:20,266] [    INFO][0m - [0m
[32m[2022-09-05 12:56:20,267] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-05 12:56:20,267] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-05 12:56:20,267] [    INFO][0m -   Num Epochs = 20[0m
[32m[2022-09-05 12:56:20,267] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-09-05 12:56:20,267] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-09-05 12:56:20,268] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-05 12:56:20,268] [    INFO][0m -   Total optimization steps = 400.0[0m
[32m[2022-09-05 12:56:20,268] [    INFO][0m -   Total num train samples = 3200[0m
[32m[2022-09-05 12:56:21,233] [    INFO][0m - loss: 2.44413185, learning_rate: 5.85e-05, global_step: 10, interval_runtime: 0.9645, interval_samples_per_second: 8.294, interval_steps_per_second: 10.368, epoch: 0.5[0m
[32m[2022-09-05 12:56:22,088] [    INFO][0m - loss: 1.08934011, learning_rate: 5.6999999999999996e-05, global_step: 20, interval_runtime: 0.8552, interval_samples_per_second: 9.354, interval_steps_per_second: 11.693, epoch: 1.0[0m
[32m[2022-09-05 12:56:22,998] [    INFO][0m - loss: 0.98345718, learning_rate: 5.550000000000001e-05, global_step: 30, interval_runtime: 0.9101, interval_samples_per_second: 8.79, interval_steps_per_second: 10.987, epoch: 1.5[0m
[32m[2022-09-05 12:56:23,859] [    INFO][0m - loss: 0.77117167, learning_rate: 5.4000000000000005e-05, global_step: 40, interval_runtime: 0.861, interval_samples_per_second: 9.292, interval_steps_per_second: 11.615, epoch: 2.0[0m
[32m[2022-09-05 12:56:24,779] [    INFO][0m - loss: 0.80707188, learning_rate: 5.25e-05, global_step: 50, interval_runtime: 0.9195, interval_samples_per_second: 8.7, interval_steps_per_second: 10.875, epoch: 2.5[0m
[32m[2022-09-05 12:56:25,640] [    INFO][0m - loss: 0.83173714, learning_rate: 5.1e-05, global_step: 60, interval_runtime: 0.8616, interval_samples_per_second: 9.285, interval_steps_per_second: 11.606, epoch: 3.0[0m
[32m[2022-09-05 12:56:26,556] [    INFO][0m - loss: 0.60555162, learning_rate: 4.95e-05, global_step: 70, interval_runtime: 0.9159, interval_samples_per_second: 8.735, interval_steps_per_second: 10.918, epoch: 3.5[0m
[32m[2022-09-05 12:56:27,416] [    INFO][0m - loss: 1.1109067, learning_rate: 4.8e-05, global_step: 80, interval_runtime: 0.8592, interval_samples_per_second: 9.311, interval_steps_per_second: 11.638, epoch: 4.0[0m
[32m[2022-09-05 12:56:28,330] [    INFO][0m - loss: 0.82426147, learning_rate: 4.6500000000000005e-05, global_step: 90, interval_runtime: 0.9149, interval_samples_per_second: 8.744, interval_steps_per_second: 10.93, epoch: 4.5[0m
[32m[2022-09-05 12:56:29,188] [    INFO][0m - loss: 0.44213247, learning_rate: 4.5e-05, global_step: 100, interval_runtime: 0.8576, interval_samples_per_second: 9.328, interval_steps_per_second: 11.66, epoch: 5.0[0m
[32m[2022-09-05 12:56:29,189] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 12:56:29,189] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-05 12:56:29,189] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 12:56:29,189] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 12:56:29,189] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-05 12:56:29,968] [    INFO][0m - eval_loss: 0.8319646716117859, eval_accuracy: 0.5911949685534591, eval_runtime: 0.7787, eval_samples_per_second: 204.185, eval_steps_per_second: 25.684, epoch: 5.0[0m
[32m[2022-09-05 12:56:29,968] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-100[0m
[32m[2022-09-05 12:56:29,968] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 12:56:34,029] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-05 12:56:34,029] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-05 12:56:40,474] [    INFO][0m - loss: 0.38342595, learning_rate: 4.35e-05, global_step: 110, interval_runtime: 11.2856, interval_samples_per_second: 0.709, interval_steps_per_second: 0.886, epoch: 5.5[0m
[32m[2022-09-05 12:56:41,336] [    INFO][0m - loss: 0.21873641, learning_rate: 4.2e-05, global_step: 120, interval_runtime: 0.8623, interval_samples_per_second: 9.278, interval_steps_per_second: 11.597, epoch: 6.0[0m
[32m[2022-09-05 12:56:42,253] [    INFO][0m - loss: 0.21211827, learning_rate: 4.05e-05, global_step: 130, interval_runtime: 0.917, interval_samples_per_second: 8.724, interval_steps_per_second: 10.905, epoch: 6.5[0m
[32m[2022-09-05 12:56:43,117] [    INFO][0m - loss: 0.25445039, learning_rate: 3.9e-05, global_step: 140, interval_runtime: 0.8631, interval_samples_per_second: 9.268, interval_steps_per_second: 11.586, epoch: 7.0[0m
[32m[2022-09-05 12:56:44,029] [    INFO][0m - loss: 0.65083327, learning_rate: 3.7500000000000003e-05, global_step: 150, interval_runtime: 0.913, interval_samples_per_second: 8.762, interval_steps_per_second: 10.953, epoch: 7.5[0m
[32m[2022-09-05 12:56:44,893] [    INFO][0m - loss: 0.35134804, learning_rate: 3.6e-05, global_step: 160, interval_runtime: 0.864, interval_samples_per_second: 9.259, interval_steps_per_second: 11.574, epoch: 8.0[0m
[32m[2022-09-05 12:56:45,813] [    INFO][0m - loss: 0.08618571, learning_rate: 3.45e-05, global_step: 170, interval_runtime: 0.9192, interval_samples_per_second: 8.703, interval_steps_per_second: 10.879, epoch: 8.5[0m
[32m[2022-09-05 12:56:46,676] [    INFO][0m - loss: 0.05599232, learning_rate: 3.3e-05, global_step: 180, interval_runtime: 0.8634, interval_samples_per_second: 9.266, interval_steps_per_second: 11.583, epoch: 9.0[0m
[32m[2022-09-05 12:56:47,597] [    INFO][0m - loss: 0.1484318, learning_rate: 3.15e-05, global_step: 190, interval_runtime: 0.9212, interval_samples_per_second: 8.684, interval_steps_per_second: 10.855, epoch: 9.5[0m
[32m[2022-09-05 12:56:48,463] [    INFO][0m - loss: 0.11554067, learning_rate: 3e-05, global_step: 200, interval_runtime: 0.8658, interval_samples_per_second: 9.24, interval_steps_per_second: 11.55, epoch: 10.0[0m
[32m[2022-09-05 12:56:48,463] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 12:56:48,464] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-05 12:56:48,464] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 12:56:48,464] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 12:56:48,464] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-05 12:56:49,246] [    INFO][0m - eval_loss: 4.258177757263184, eval_accuracy: 0.5408805031446541, eval_runtime: 0.7818, eval_samples_per_second: 203.37, eval_steps_per_second: 25.581, epoch: 10.0[0m
[32m[2022-09-05 12:56:49,246] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-200[0m
[32m[2022-09-05 12:56:49,246] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 12:56:52,638] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-200/tokenizer_config.json[0m
[32m[2022-09-05 12:56:52,638] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-200/special_tokens_map.json[0m
[32m[2022-09-05 12:56:59,709] [    INFO][0m - loss: 0.00023457, learning_rate: 2.8499999999999998e-05, global_step: 210, interval_runtime: 11.2451, interval_samples_per_second: 0.711, interval_steps_per_second: 0.889, epoch: 10.5[0m
[32m[2022-09-05 12:57:00,563] [    INFO][0m - loss: 0.06957939, learning_rate: 2.7000000000000002e-05, global_step: 220, interval_runtime: 0.8553, interval_samples_per_second: 9.354, interval_steps_per_second: 11.692, epoch: 11.0[0m
[32m[2022-09-05 12:57:01,477] [    INFO][0m - loss: 0.00152939, learning_rate: 2.55e-05, global_step: 230, interval_runtime: 0.9134, interval_samples_per_second: 8.758, interval_steps_per_second: 10.948, epoch: 11.5[0m
[32m[2022-09-05 12:57:02,337] [    INFO][0m - loss: 0.00047269, learning_rate: 2.4e-05, global_step: 240, interval_runtime: 0.8597, interval_samples_per_second: 9.306, interval_steps_per_second: 11.632, epoch: 12.0[0m
[32m[2022-09-05 12:57:03,254] [    INFO][0m - loss: 0.0503822, learning_rate: 2.25e-05, global_step: 250, interval_runtime: 0.9177, interval_samples_per_second: 8.717, interval_steps_per_second: 10.896, epoch: 12.5[0m
[32m[2022-09-05 12:57:04,130] [    INFO][0m - loss: 0.00022482, learning_rate: 2.1e-05, global_step: 260, interval_runtime: 0.8753, interval_samples_per_second: 9.139, interval_steps_per_second: 11.424, epoch: 13.0[0m
[32m[2022-09-05 12:57:05,043] [    INFO][0m - loss: 0.00022664, learning_rate: 1.95e-05, global_step: 270, interval_runtime: 0.9133, interval_samples_per_second: 8.759, interval_steps_per_second: 10.949, epoch: 13.5[0m
[32m[2022-09-05 12:57:05,902] [    INFO][0m - loss: 0.00039243, learning_rate: 1.8e-05, global_step: 280, interval_runtime: 0.8593, interval_samples_per_second: 9.31, interval_steps_per_second: 11.638, epoch: 14.0[0m
[32m[2022-09-05 12:57:06,824] [    INFO][0m - loss: 5.231e-05, learning_rate: 1.65e-05, global_step: 290, interval_runtime: 0.9216, interval_samples_per_second: 8.68, interval_steps_per_second: 10.851, epoch: 14.5[0m
[32m[2022-09-05 12:57:07,685] [    INFO][0m - loss: 4.559e-05, learning_rate: 1.5e-05, global_step: 300, interval_runtime: 0.8606, interval_samples_per_second: 9.296, interval_steps_per_second: 11.619, epoch: 15.0[0m
[32m[2022-09-05 12:57:07,685] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 12:57:07,685] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-05 12:57:07,685] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 12:57:07,686] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 12:57:07,686] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-05 12:57:08,479] [    INFO][0m - eval_loss: 5.350002288818359, eval_accuracy: 0.5220125786163522, eval_runtime: 0.793, eval_samples_per_second: 200.492, eval_steps_per_second: 25.219, epoch: 15.0[0m
[32m[2022-09-05 12:57:08,479] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-300[0m
[32m[2022-09-05 12:57:08,479] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 12:57:11,648] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-300/tokenizer_config.json[0m
[32m[2022-09-05 12:57:11,649] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-300/special_tokens_map.json[0m
[32m[2022-09-05 12:57:17,974] [    INFO][0m - loss: 0.0001354, learning_rate: 1.3500000000000001e-05, global_step: 310, interval_runtime: 10.2891, interval_samples_per_second: 0.778, interval_steps_per_second: 0.972, epoch: 15.5[0m
[32m[2022-09-05 12:57:18,831] [    INFO][0m - loss: 3.476e-05, learning_rate: 1.2e-05, global_step: 320, interval_runtime: 0.8576, interval_samples_per_second: 9.329, interval_steps_per_second: 11.661, epoch: 16.0[0m
[32m[2022-09-05 12:57:19,744] [    INFO][0m - loss: 1.566e-05, learning_rate: 1.05e-05, global_step: 330, interval_runtime: 0.912, interval_samples_per_second: 8.772, interval_steps_per_second: 10.965, epoch: 16.5[0m
[32m[2022-09-05 12:57:20,603] [    INFO][0m - loss: 6.939e-05, learning_rate: 9e-06, global_step: 340, interval_runtime: 0.8601, interval_samples_per_second: 9.301, interval_steps_per_second: 11.626, epoch: 17.0[0m
[32m[2022-09-05 12:57:21,516] [    INFO][0m - loss: 0.05390742, learning_rate: 7.5e-06, global_step: 350, interval_runtime: 0.9129, interval_samples_per_second: 8.763, interval_steps_per_second: 10.954, epoch: 17.5[0m
[32m[2022-09-05 12:57:22,379] [    INFO][0m - loss: 1.536e-05, learning_rate: 6e-06, global_step: 360, interval_runtime: 0.8631, interval_samples_per_second: 9.269, interval_steps_per_second: 11.586, epoch: 18.0[0m
[32m[2022-09-05 12:57:23,305] [    INFO][0m - loss: 2.353e-05, learning_rate: 4.5e-06, global_step: 370, interval_runtime: 0.9256, interval_samples_per_second: 8.643, interval_steps_per_second: 10.804, epoch: 18.5[0m
[32m[2022-09-05 12:57:24,168] [    INFO][0m - loss: 2.188e-05, learning_rate: 3e-06, global_step: 380, interval_runtime: 0.8626, interval_samples_per_second: 9.274, interval_steps_per_second: 11.593, epoch: 19.0[0m
[32m[2022-09-05 12:57:25,096] [    INFO][0m - loss: 5.987e-05, learning_rate: 1.5e-06, global_step: 390, interval_runtime: 0.9279, interval_samples_per_second: 8.622, interval_steps_per_second: 10.777, epoch: 19.5[0m
[32m[2022-09-05 12:57:25,954] [    INFO][0m - loss: 0.03498826, learning_rate: 0.0, global_step: 400, interval_runtime: 0.8587, interval_samples_per_second: 9.316, interval_steps_per_second: 11.645, epoch: 20.0[0m
[32m[2022-09-05 12:57:25,955] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 12:57:25,955] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-05 12:57:25,955] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 12:57:25,955] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 12:57:25,955] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-05 12:57:26,752] [    INFO][0m - eval_loss: 5.478175640106201, eval_accuracy: 0.5157232704402516, eval_runtime: 0.7966, eval_samples_per_second: 199.589, eval_steps_per_second: 25.106, epoch: 20.0[0m
[32m[2022-09-05 12:57:26,752] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-400[0m
[32m[2022-09-05 12:57:26,752] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 12:57:30,203] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-400/tokenizer_config.json[0m
[32m[2022-09-05 12:57:30,203] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-400/special_tokens_map.json[0m
[32m[2022-09-05 12:57:35,861] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-05 12:57:35,861] [    INFO][0m - Loading best model from ./checkpoints_cluewsc/checkpoint-100 (score: 0.5911949685534591).[0m
[32m[2022-09-05 12:57:36,857] [    INFO][0m - train_runtime: 76.5881, train_samples_per_second: 41.782, train_steps_per_second: 5.223, train_loss: 0.3149809117463883, epoch: 20.0[0m
[32m[2022-09-05 12:57:36,906] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/[0m
[32m[2022-09-05 12:57:36,906] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 12:57:41,632] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/tokenizer_config.json[0m
[32m[2022-09-05 12:57:41,633] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/special_tokens_map.json[0m
[32m[2022-09-05 12:57:41,634] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-05 12:57:41,634] [    INFO][0m -   epoch                    =       20.0[0m
[32m[2022-09-05 12:57:41,635] [    INFO][0m -   train_loss               =      0.315[0m
[32m[2022-09-05 12:57:41,635] [    INFO][0m -   train_runtime            = 0:01:16.58[0m
[32m[2022-09-05 12:57:41,635] [    INFO][0m -   train_samples_per_second =     41.782[0m
[32m[2022-09-05 12:57:41,635] [    INFO][0m -   train_steps_per_second   =      5.223[0m
[32m[2022-09-05 12:57:41,638] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-05 12:57:41,638] [    INFO][0m -   Num examples = 976[0m
[32m[2022-09-05 12:57:41,638] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 12:57:41,639] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 12:57:41,639] [    INFO][0m -   Total prediction steps = 122[0m
[32m[2022-09-05 12:57:46,406] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-05 12:57:46,406] [    INFO][0m -   test_accuracy           =     0.4846[0m
[32m[2022-09-05 12:57:46,407] [    INFO][0m -   test_loss               =     1.1266[0m
[32m[2022-09-05 12:57:46,407] [    INFO][0m -   test_runtime            = 0:00:04.76[0m
[32m[2022-09-05 12:57:46,407] [    INFO][0m -   test_samples_per_second =    204.722[0m
[32m[2022-09-05 12:57:46,407] [    INFO][0m -   test_steps_per_second   =      25.59[0m
[33m[2022-09-05 12:57:47,440] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-09-05 12:57:47,440] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-05 12:57:47,440] [    INFO][0m - ============================================================[0m
[32m[2022-09-05 12:57:47,440] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-05 12:57:47,440] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-05 12:57:47,440] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-05 12:57:47,440] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-05 12:57:47,440] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-05 12:57:47,440] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-05 12:57:47,441] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-09-05 12:57:47,441] [    INFO][0m - [0m
[32m[2022-09-05 12:57:47,441] [    INFO][0m - ============================================================[0m
[32m[2022-09-05 12:57:47,441] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-05 12:57:47,441] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-05 12:57:47,441] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-05 12:57:47,441] [    INFO][0m - prompt                        :[0m
[32m[2022-09-05 12:57:47,441] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-05 12:57:47,441] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-05 12:57:47,441] [    INFO][0m - task_name                     :cluewsc[0m
[32m[2022-09-05 12:57:47,441] [    INFO][0m - [0m
[32m[2022-09-05 12:57:47,442] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
[32m[2022-09-05 12:57:48,852] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-09-05 12:57:48,875] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-09-05 12:57:48,876] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[32m[2022-09-05 12:57:48,877] [    INFO][0m - Using template: [{'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': 'ÂÖ∂‰∏≠'}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': '„ÄÇËøôÂè•ËØùÊèèËø∞Ê≠£Á°ÆÂêóÔºü'}, {'add_prefix_space': '', 'mask': None}][0m
2022-09-05 12:57:48,879 INFO [download.py:119] unique_endpoints {''}
[32m[2022-09-05 12:57:48,892] [    INFO][0m - ============================================================[0m
[32m[2022-09-05 12:57:48,892] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-05 12:57:48,892] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-05 12:57:48,892] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-05 12:57:48,893] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-05 12:57:48,893] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-05 12:57:48,893] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-05 12:57:48,893] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-05 12:57:48,893] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-05 12:57:48,893] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-05 12:57:48,893] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-05 12:57:48,893] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-05 12:57:48,893] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-05 12:57:48,893] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-05 12:57:48,893] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-05 12:57:48,893] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-05 12:57:48,894] [    INFO][0m - do_predict                    :False[0m
[32m[2022-09-05 12:57:48,894] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-05 12:57:48,894] [    INFO][0m - eval_batch_size               :8[0m
[32m[2022-09-05 12:57:48,894] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-09-05 12:57:48,894] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-09-05 12:57:48,894] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-05 12:57:48,894] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-05 12:57:48,894] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-05 12:57:48,894] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-05 12:57:48,894] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-05 12:57:48,894] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-05 12:57:48,894] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-05 12:57:48,895] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-05 12:57:48,895] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-05 12:57:48,895] [    INFO][0m - learning_rate                 :6e-05[0m
[32m[2022-09-05 12:57:48,895] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-05 12:57:48,895] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-05 12:57:48,895] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-05 12:57:48,895] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-05 12:57:48,895] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-05 12:57:48,895] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-05 12:57:48,895] [    INFO][0m - logging_dir                   :./checkpoints_cluewsc/runs/Sep05_12-57-47_instance-3bwob41y-01[0m
[32m[2022-09-05 12:57:48,895] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-05 12:57:48,895] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-05 12:57:48,895] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-05 12:57:48,896] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-05 12:57:48,896] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-05 12:57:48,896] [    INFO][0m - max_seq_length                :128[0m
[32m[2022-09-05 12:57:48,896] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-05 12:57:48,896] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-05 12:57:48,896] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-05 12:57:48,896] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-05 12:57:48,896] [    INFO][0m - num_train_epochs              :20.0[0m
[32m[2022-09-05 12:57:48,896] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-05 12:57:48,896] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-05 12:57:48,896] [    INFO][0m - output_dir                    :./checkpoints_cluewsc/[0m
[32m[2022-09-05 12:57:48,896] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-05 12:57:48,896] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-05 12:57:48,897] [    INFO][0m - per_device_eval_batch_size    :8[0m
[32m[2022-09-05 12:57:48,897] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-09-05 12:57:48,897] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-05 12:57:48,897] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-05 12:57:48,897] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-05 12:57:48,897] [    INFO][0m - ppt_learning_rate             :6e-05[0m
[32m[2022-09-05 12:57:48,897] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-05 12:57:48,897] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-05 12:57:48,897] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-05 12:57:48,897] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-05 12:57:48,897] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-05 12:57:48,897] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-05 12:57:48,897] [    INFO][0m - run_name                      :./checkpoints_cluewsc/[0m
[32m[2022-09-05 12:57:48,898] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-05 12:57:48,898] [    INFO][0m - save_steps                    :100[0m
[32m[2022-09-05 12:57:48,898] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-09-05 12:57:48,898] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-05 12:57:48,898] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-05 12:57:48,898] [    INFO][0m - seed                          :42[0m
[32m[2022-09-05 12:57:48,898] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-05 12:57:48,898] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-05 12:57:48,898] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-05 12:57:48,898] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-09-05 12:57:48,898] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-05 12:57:48,898] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-05 12:57:48,898] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-05 12:57:48,899] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-05 12:57:48,899] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-05 12:57:48,899] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-05 12:57:48,899] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-05 12:57:48,899] [    INFO][0m - [0m
[32m[2022-09-05 12:57:48,900] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-05 12:57:48,901] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-05 12:57:48,901] [    INFO][0m -   Num Epochs = 20[0m
[32m[2022-09-05 12:57:48,901] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-09-05 12:57:48,901] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-09-05 12:57:48,901] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-05 12:57:48,901] [    INFO][0m -   Total optimization steps = 400.0[0m
[32m[2022-09-05 12:57:48,901] [    INFO][0m -   Total num train samples = 3200[0m
[32m[2022-09-05 12:57:49,886] [    INFO][0m - loss: 2.4839159, learning_rate: 5.85e-05, global_step: 10, interval_runtime: 0.9843, interval_samples_per_second: 8.128, interval_steps_per_second: 10.16, epoch: 0.5[0m
[32m[2022-09-05 12:57:50,747] [    INFO][0m - loss: 0.90198183, learning_rate: 5.6999999999999996e-05, global_step: 20, interval_runtime: 0.8612, interval_samples_per_second: 9.289, interval_steps_per_second: 11.611, epoch: 1.0[0m
[32m[2022-09-05 12:57:51,663] [    INFO][0m - loss: 1.14276857, learning_rate: 5.550000000000001e-05, global_step: 30, interval_runtime: 0.9152, interval_samples_per_second: 8.742, interval_steps_per_second: 10.927, epoch: 1.5[0m
[32m[2022-09-05 12:57:52,526] [    INFO][0m - loss: 0.92585545, learning_rate: 5.4000000000000005e-05, global_step: 40, interval_runtime: 0.8634, interval_samples_per_second: 9.266, interval_steps_per_second: 11.583, epoch: 2.0[0m
[32m[2022-09-05 12:57:53,450] [    INFO][0m - loss: 0.9275012, learning_rate: 5.25e-05, global_step: 50, interval_runtime: 0.924, interval_samples_per_second: 8.658, interval_steps_per_second: 10.822, epoch: 2.5[0m
[32m[2022-09-05 12:57:54,318] [    INFO][0m - loss: 0.77581892, learning_rate: 5.1e-05, global_step: 60, interval_runtime: 0.8686, interval_samples_per_second: 9.21, interval_steps_per_second: 11.513, epoch: 3.0[0m
[32m[2022-09-05 12:57:55,239] [    INFO][0m - loss: 0.73726711, learning_rate: 4.95e-05, global_step: 70, interval_runtime: 0.92, interval_samples_per_second: 8.695, interval_steps_per_second: 10.869, epoch: 3.5[0m
[32m[2022-09-05 12:57:56,112] [    INFO][0m - loss: 1.13268032, learning_rate: 4.8e-05, global_step: 80, interval_runtime: 0.8737, interval_samples_per_second: 9.157, interval_steps_per_second: 11.446, epoch: 4.0[0m
[32m[2022-09-05 12:57:57,039] [    INFO][0m - loss: 0.80729151, learning_rate: 4.6500000000000005e-05, global_step: 90, interval_runtime: 0.926, interval_samples_per_second: 8.64, interval_steps_per_second: 10.799, epoch: 4.5[0m
[32m[2022-09-05 12:57:57,907] [    INFO][0m - loss: 0.70070705, learning_rate: 4.5e-05, global_step: 100, interval_runtime: 0.8684, interval_samples_per_second: 9.212, interval_steps_per_second: 11.515, epoch: 5.0[0m
[32m[2022-09-05 12:57:57,907] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 12:57:57,907] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-05 12:57:57,907] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 12:57:57,907] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 12:57:57,908] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-05 12:57:58,693] [    INFO][0m - eval_loss: 1.0166066884994507, eval_accuracy: 0.5534591194968553, eval_runtime: 0.7852, eval_samples_per_second: 202.486, eval_steps_per_second: 25.47, epoch: 5.0[0m
[32m[2022-09-05 12:57:58,693] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-100[0m
[32m[2022-09-05 12:57:58,693] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 12:58:01,818] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-05 12:58:02,124] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-05 12:58:11,418] [    INFO][0m - loss: 0.75040855, learning_rate: 4.35e-05, global_step: 110, interval_runtime: 13.5111, interval_samples_per_second: 0.592, interval_steps_per_second: 0.74, epoch: 5.5[0m
[32m[2022-09-05 12:58:12,281] [    INFO][0m - loss: 0.52832947, learning_rate: 4.2e-05, global_step: 120, interval_runtime: 0.8631, interval_samples_per_second: 9.269, interval_steps_per_second: 11.586, epoch: 6.0[0m
[32m[2022-09-05 12:58:13,199] [    INFO][0m - loss: 0.34882638, learning_rate: 4.05e-05, global_step: 130, interval_runtime: 0.9177, interval_samples_per_second: 8.718, interval_steps_per_second: 10.897, epoch: 6.5[0m
[32m[2022-09-05 12:58:14,057] [    INFO][0m - loss: 0.23855562, learning_rate: 3.9e-05, global_step: 140, interval_runtime: 0.8584, interval_samples_per_second: 9.319, interval_steps_per_second: 11.649, epoch: 7.0[0m
[32m[2022-09-05 12:58:14,975] [    INFO][0m - loss: 0.52342615, learning_rate: 3.7500000000000003e-05, global_step: 150, interval_runtime: 0.9178, interval_samples_per_second: 8.716, interval_steps_per_second: 10.895, epoch: 7.5[0m
[32m[2022-09-05 12:58:15,837] [    INFO][0m - loss: 0.18543698, learning_rate: 3.6e-05, global_step: 160, interval_runtime: 0.8622, interval_samples_per_second: 9.279, interval_steps_per_second: 11.599, epoch: 8.0[0m
[32m[2022-09-05 12:58:16,753] [    INFO][0m - loss: 0.35400348, learning_rate: 3.45e-05, global_step: 170, interval_runtime: 0.9158, interval_samples_per_second: 8.736, interval_steps_per_second: 10.92, epoch: 8.5[0m
[32m[2022-09-05 12:58:17,614] [    INFO][0m - loss: 0.16507361, learning_rate: 3.3e-05, global_step: 180, interval_runtime: 0.8614, interval_samples_per_second: 9.288, interval_steps_per_second: 11.61, epoch: 9.0[0m
[32m[2022-09-05 12:58:18,530] [    INFO][0m - loss: 0.04523424, learning_rate: 3.15e-05, global_step: 190, interval_runtime: 0.9158, interval_samples_per_second: 8.736, interval_steps_per_second: 10.92, epoch: 9.5[0m
[32m[2022-09-05 12:58:19,391] [    INFO][0m - loss: 0.29602253, learning_rate: 3e-05, global_step: 200, interval_runtime: 0.8611, interval_samples_per_second: 9.29, interval_steps_per_second: 11.612, epoch: 10.0[0m
[32m[2022-09-05 12:58:19,391] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 12:58:19,391] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-05 12:58:19,392] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 12:58:19,392] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 12:58:19,392] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-05 12:58:20,181] [    INFO][0m - eval_loss: 4.288048267364502, eval_accuracy: 0.5345911949685535, eval_runtime: 0.7889, eval_samples_per_second: 201.537, eval_steps_per_second: 25.351, epoch: 10.0[0m
[32m[2022-09-05 12:58:20,181] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-200[0m
[32m[2022-09-05 12:58:20,181] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 12:58:23,227] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-200/tokenizer_config.json[0m
[32m[2022-09-05 12:58:23,227] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-200/special_tokens_map.json[0m
[32m[2022-09-05 12:58:29,476] [    INFO][0m - loss: 0.03153451, learning_rate: 2.8499999999999998e-05, global_step: 210, interval_runtime: 10.0848, interval_samples_per_second: 0.793, interval_steps_per_second: 0.992, epoch: 10.5[0m
[32m[2022-09-05 12:58:30,335] [    INFO][0m - loss: 0.00260589, learning_rate: 2.7000000000000002e-05, global_step: 220, interval_runtime: 0.8591, interval_samples_per_second: 9.312, interval_steps_per_second: 11.64, epoch: 11.0[0m
[32m[2022-09-05 12:58:31,250] [    INFO][0m - loss: 0.00156365, learning_rate: 2.55e-05, global_step: 230, interval_runtime: 0.9152, interval_samples_per_second: 8.742, interval_steps_per_second: 10.927, epoch: 11.5[0m
[32m[2022-09-05 12:58:32,110] [    INFO][0m - loss: 0.04542234, learning_rate: 2.4e-05, global_step: 240, interval_runtime: 0.8594, interval_samples_per_second: 9.309, interval_steps_per_second: 11.637, epoch: 12.0[0m
[32m[2022-09-05 12:58:33,024] [    INFO][0m - loss: 0.1070465, learning_rate: 2.25e-05, global_step: 250, interval_runtime: 0.9149, interval_samples_per_second: 8.744, interval_steps_per_second: 10.93, epoch: 12.5[0m
[32m[2022-09-05 12:58:33,892] [    INFO][0m - loss: 0.01443364, learning_rate: 2.1e-05, global_step: 260, interval_runtime: 0.8677, interval_samples_per_second: 9.22, interval_steps_per_second: 11.525, epoch: 13.0[0m
[32m[2022-09-05 12:58:34,811] [    INFO][0m - loss: 0.00527209, learning_rate: 1.95e-05, global_step: 270, interval_runtime: 0.9194, interval_samples_per_second: 8.701, interval_steps_per_second: 10.877, epoch: 13.5[0m
[32m[2022-09-05 12:58:35,671] [    INFO][0m - loss: 0.03977391, learning_rate: 1.8e-05, global_step: 280, interval_runtime: 0.8596, interval_samples_per_second: 9.307, interval_steps_per_second: 11.634, epoch: 14.0[0m
[32m[2022-09-05 12:58:36,591] [    INFO][0m - loss: 3.029e-05, learning_rate: 1.65e-05, global_step: 290, interval_runtime: 0.9201, interval_samples_per_second: 8.695, interval_steps_per_second: 10.869, epoch: 14.5[0m
[32m[2022-09-05 12:58:37,454] [    INFO][0m - loss: 0.00097449, learning_rate: 1.5e-05, global_step: 300, interval_runtime: 0.8628, interval_samples_per_second: 9.272, interval_steps_per_second: 11.59, epoch: 15.0[0m
[32m[2022-09-05 12:58:37,455] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 12:58:37,455] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-05 12:58:37,455] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 12:58:37,455] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 12:58:37,455] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-05 12:58:38,255] [    INFO][0m - eval_loss: 4.1655592918396, eval_accuracy: 0.5723270440251572, eval_runtime: 0.7989, eval_samples_per_second: 199.031, eval_steps_per_second: 25.035, epoch: 15.0[0m
[32m[2022-09-05 12:58:38,255] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-300[0m
[32m[2022-09-05 12:58:38,255] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 12:58:42,425] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-300/tokenizer_config.json[0m
[32m[2022-09-05 12:58:42,426] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-300/special_tokens_map.json[0m
[32m[2022-09-05 12:58:52,621] [    INFO][0m - loss: 2.207e-05, learning_rate: 1.3500000000000001e-05, global_step: 310, interval_runtime: 15.1665, interval_samples_per_second: 0.527, interval_steps_per_second: 0.659, epoch: 15.5[0m
[32m[2022-09-05 12:58:53,486] [    INFO][0m - loss: 0.01327921, learning_rate: 1.2e-05, global_step: 320, interval_runtime: 0.8657, interval_samples_per_second: 9.241, interval_steps_per_second: 11.552, epoch: 16.0[0m
[32m[2022-09-05 12:58:54,414] [    INFO][0m - loss: 1.805e-05, learning_rate: 1.05e-05, global_step: 330, interval_runtime: 0.928, interval_samples_per_second: 8.62, interval_steps_per_second: 10.776, epoch: 16.5[0m
[32m[2022-09-05 12:58:55,282] [    INFO][0m - loss: 7.388e-05, learning_rate: 9e-06, global_step: 340, interval_runtime: 0.868, interval_samples_per_second: 9.217, interval_steps_per_second: 11.521, epoch: 17.0[0m
[32m[2022-09-05 12:58:56,202] [    INFO][0m - loss: 0.03679294, learning_rate: 7.5e-06, global_step: 350, interval_runtime: 0.9198, interval_samples_per_second: 8.697, interval_steps_per_second: 10.872, epoch: 17.5[0m
[32m[2022-09-05 12:58:57,066] [    INFO][0m - loss: 0.0001821, learning_rate: 6e-06, global_step: 360, interval_runtime: 0.8642, interval_samples_per_second: 9.257, interval_steps_per_second: 11.572, epoch: 18.0[0m
[32m[2022-09-05 12:58:57,997] [    INFO][0m - loss: 0.00498291, learning_rate: 4.5e-06, global_step: 370, interval_runtime: 0.9305, interval_samples_per_second: 8.598, interval_steps_per_second: 10.747, epoch: 18.5[0m
[32m[2022-09-05 12:58:58,864] [    INFO][0m - loss: 1.93e-05, learning_rate: 3e-06, global_step: 380, interval_runtime: 0.8669, interval_samples_per_second: 9.228, interval_steps_per_second: 11.535, epoch: 19.0[0m
[32m[2022-09-05 12:58:59,790] [    INFO][0m - loss: 1.73e-05, learning_rate: 1.5e-06, global_step: 390, interval_runtime: 0.9267, interval_samples_per_second: 8.633, interval_steps_per_second: 10.791, epoch: 19.5[0m
[32m[2022-09-05 12:59:00,667] [    INFO][0m - loss: 1.834e-05, learning_rate: 0.0, global_step: 400, interval_runtime: 0.8764, interval_samples_per_second: 9.129, interval_steps_per_second: 11.411, epoch: 20.0[0m
[32m[2022-09-05 12:59:00,668] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 12:59:00,669] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-05 12:59:00,669] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 12:59:00,669] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 12:59:00,669] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-05 12:59:01,467] [    INFO][0m - eval_loss: 4.080735683441162, eval_accuracy: 0.5723270440251572, eval_runtime: 0.7977, eval_samples_per_second: 199.314, eval_steps_per_second: 25.071, epoch: 20.0[0m
[32m[2022-09-05 12:59:01,467] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-400[0m
[32m[2022-09-05 12:59:01,468] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 12:59:04,809] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-400/tokenizer_config.json[0m
[32m[2022-09-05 12:59:04,809] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-400/special_tokens_map.json[0m
[32m[2022-09-05 12:59:10,615] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-05 12:59:10,615] [    INFO][0m - Loading best model from ./checkpoints_cluewsc/checkpoint-300 (score: 0.5723270440251572).[0m
[32m[2022-09-05 12:59:11,553] [    INFO][0m - train_runtime: 82.6509, train_samples_per_second: 38.717, train_steps_per_second: 4.84, train_loss: 0.3568792073691657, epoch: 20.0[0m
[32m[2022-09-05 12:59:11,600] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/[0m
[32m[2022-09-05 12:59:11,600] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 12:59:14,975] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/tokenizer_config.json[0m
[32m[2022-09-05 12:59:14,976] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/special_tokens_map.json[0m
[32m[2022-09-05 12:59:14,978] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-05 12:59:14,978] [    INFO][0m -   epoch                    =       20.0[0m
[32m[2022-09-05 12:59:14,978] [    INFO][0m -   train_loss               =     0.3569[0m
[32m[2022-09-05 12:59:14,978] [    INFO][0m -   train_runtime            = 0:01:22.65[0m
[32m[2022-09-05 12:59:14,978] [    INFO][0m -   train_samples_per_second =     38.717[0m
[32m[2022-09-05 12:59:14,978] [    INFO][0m -   train_steps_per_second   =       4.84[0m
[32m[2022-09-05 12:59:14,982] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-05 12:59:14,982] [    INFO][0m -   Num examples = 976[0m
[32m[2022-09-05 12:59:14,982] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 12:59:14,982] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 12:59:14,982] [    INFO][0m -   Total prediction steps = 122[0m
[32m[2022-09-05 12:59:19,859] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-05 12:59:19,860] [    INFO][0m -   test_accuracy           =     0.5195[0m
[32m[2022-09-05 12:59:19,860] [    INFO][0m -   test_loss               =     5.3519[0m
[32m[2022-09-05 12:59:19,860] [    INFO][0m -   test_runtime            = 0:00:04.87[0m
[32m[2022-09-05 12:59:19,860] [    INFO][0m -   test_samples_per_second =     200.12[0m
[32m[2022-09-05 12:59:19,860] [    INFO][0m -   test_steps_per_second   =     25.015[0m
[33m[2022-09-05 12:59:20,962] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-09-05 12:59:20,963] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-05 12:59:20,963] [    INFO][0m - ============================================================[0m
[32m[2022-09-05 12:59:20,963] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-05 12:59:20,963] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-05 12:59:20,963] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-05 12:59:20,963] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-05 12:59:20,963] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-05 12:59:20,963] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-05 12:59:20,964] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-09-05 12:59:20,964] [    INFO][0m - [0m
[32m[2022-09-05 12:59:20,964] [    INFO][0m - ============================================================[0m
[32m[2022-09-05 12:59:20,964] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-05 12:59:20,964] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-05 12:59:20,964] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-05 12:59:20,964] [    INFO][0m - prompt                        :[0m
[32m[2022-09-05 12:59:20,964] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-05 12:59:20,964] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-05 12:59:20,964] [    INFO][0m - task_name                     :cluewsc[0m
[32m[2022-09-05 12:59:20,964] [    INFO][0m - [0m
[32m[2022-09-05 12:59:20,965] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
[32m[2022-09-05 12:59:22,385] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-09-05 12:59:22,409] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-09-05 12:59:22,409] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[32m[2022-09-05 12:59:22,410] [    INFO][0m - Using template: [{'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': 'ÂÖ∂‰∏≠'}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': '„ÄÇËøôÂè•ËØùÊèèËø∞Ê≠£Á°ÆÂêóÔºü'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}][0m
2022-09-05 12:59:22,413 INFO [download.py:119] unique_endpoints {''}
[32m[2022-09-05 12:59:22,426] [    INFO][0m - ============================================================[0m
[32m[2022-09-05 12:59:22,426] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-05 12:59:22,426] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-05 12:59:22,426] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-05 12:59:22,427] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-05 12:59:22,427] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-05 12:59:22,427] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-05 12:59:22,427] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-05 12:59:22,427] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-05 12:59:22,427] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-05 12:59:22,427] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-05 12:59:22,427] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-05 12:59:22,427] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-05 12:59:22,427] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-05 12:59:22,427] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-05 12:59:22,428] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-05 12:59:22,428] [    INFO][0m - do_predict                    :False[0m
[32m[2022-09-05 12:59:22,428] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-05 12:59:22,428] [    INFO][0m - eval_batch_size               :8[0m
[32m[2022-09-05 12:59:22,428] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-09-05 12:59:22,428] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-09-05 12:59:22,428] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-05 12:59:22,428] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-05 12:59:22,428] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-05 12:59:22,428] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-05 12:59:22,428] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-05 12:59:22,428] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-05 12:59:22,429] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-05 12:59:22,429] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-05 12:59:22,429] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-05 12:59:22,429] [    INFO][0m - learning_rate                 :6e-05[0m
[32m[2022-09-05 12:59:22,429] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-05 12:59:22,429] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-05 12:59:22,429] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-05 12:59:22,429] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-05 12:59:22,429] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-05 12:59:22,429] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-05 12:59:22,429] [    INFO][0m - logging_dir                   :./checkpoints_cluewsc/runs/Sep05_12-59-20_instance-3bwob41y-01[0m
[32m[2022-09-05 12:59:22,429] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-05 12:59:22,429] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-05 12:59:22,430] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-05 12:59:22,430] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-05 12:59:22,430] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-05 12:59:22,430] [    INFO][0m - max_seq_length                :128[0m
[32m[2022-09-05 12:59:22,430] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-05 12:59:22,430] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-05 12:59:22,430] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-05 12:59:22,430] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-05 12:59:22,430] [    INFO][0m - num_train_epochs              :20.0[0m
[32m[2022-09-05 12:59:22,430] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-05 12:59:22,430] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-05 12:59:22,430] [    INFO][0m - output_dir                    :./checkpoints_cluewsc/[0m
[32m[2022-09-05 12:59:22,431] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-05 12:59:22,431] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-05 12:59:22,431] [    INFO][0m - per_device_eval_batch_size    :8[0m
[32m[2022-09-05 12:59:22,431] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-09-05 12:59:22,431] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-05 12:59:22,431] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-05 12:59:22,431] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-05 12:59:22,431] [    INFO][0m - ppt_learning_rate             :6e-05[0m
[32m[2022-09-05 12:59:22,431] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-05 12:59:22,431] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-05 12:59:22,431] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-05 12:59:22,431] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-05 12:59:22,431] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-05 12:59:22,432] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-05 12:59:22,432] [    INFO][0m - run_name                      :./checkpoints_cluewsc/[0m
[32m[2022-09-05 12:59:22,432] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-05 12:59:22,432] [    INFO][0m - save_steps                    :100[0m
[32m[2022-09-05 12:59:22,432] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-09-05 12:59:22,432] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-05 12:59:22,432] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-05 12:59:22,432] [    INFO][0m - seed                          :42[0m
[32m[2022-09-05 12:59:22,432] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-05 12:59:22,432] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-05 12:59:22,432] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-05 12:59:22,432] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-09-05 12:59:22,432] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-05 12:59:22,433] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-05 12:59:22,433] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-05 12:59:22,433] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-05 12:59:22,433] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-05 12:59:22,433] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-05 12:59:22,433] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-05 12:59:22,433] [    INFO][0m - [0m
[32m[2022-09-05 12:59:22,435] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-05 12:59:22,435] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-05 12:59:22,435] [    INFO][0m -   Num Epochs = 20[0m
[32m[2022-09-05 12:59:22,435] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-09-05 12:59:22,435] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-09-05 12:59:22,435] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-05 12:59:22,435] [    INFO][0m -   Total optimization steps = 400.0[0m
[32m[2022-09-05 12:59:22,436] [    INFO][0m -   Total num train samples = 3200[0m
[32m[2022-09-05 12:59:23,418] [    INFO][0m - loss: 2.19369278, learning_rate: 5.85e-05, global_step: 10, interval_runtime: 0.9809, interval_samples_per_second: 8.155, interval_steps_per_second: 10.194, epoch: 0.5[0m
[32m[2022-09-05 12:59:24,279] [    INFO][0m - loss: 0.99861679, learning_rate: 5.6999999999999996e-05, global_step: 20, interval_runtime: 0.8615, interval_samples_per_second: 9.286, interval_steps_per_second: 11.608, epoch: 1.0[0m
[32m[2022-09-05 12:59:25,205] [    INFO][0m - loss: 1.04504471, learning_rate: 5.550000000000001e-05, global_step: 30, interval_runtime: 0.9262, interval_samples_per_second: 8.637, interval_steps_per_second: 10.796, epoch: 1.5[0m
[32m[2022-09-05 12:59:26,064] [    INFO][0m - loss: 0.91865215, learning_rate: 5.4000000000000005e-05, global_step: 40, interval_runtime: 0.8586, interval_samples_per_second: 9.317, interval_steps_per_second: 11.646, epoch: 2.0[0m
[32m[2022-09-05 12:59:26,980] [    INFO][0m - loss: 0.89689131, learning_rate: 5.25e-05, global_step: 50, interval_runtime: 0.9168, interval_samples_per_second: 8.726, interval_steps_per_second: 10.907, epoch: 2.5[0m
[32m[2022-09-05 12:59:27,839] [    INFO][0m - loss: 0.73322258, learning_rate: 5.1e-05, global_step: 60, interval_runtime: 0.8578, interval_samples_per_second: 9.326, interval_steps_per_second: 11.658, epoch: 3.0[0m
[32m[2022-09-05 12:59:28,764] [    INFO][0m - loss: 0.75819507, learning_rate: 4.95e-05, global_step: 70, interval_runtime: 0.9253, interval_samples_per_second: 8.646, interval_steps_per_second: 10.807, epoch: 3.5[0m
[32m[2022-09-05 12:59:29,626] [    INFO][0m - loss: 0.93303719, learning_rate: 4.8e-05, global_step: 80, interval_runtime: 0.8622, interval_samples_per_second: 9.279, interval_steps_per_second: 11.599, epoch: 4.0[0m
[32m[2022-09-05 12:59:30,549] [    INFO][0m - loss: 0.70770845, learning_rate: 4.6500000000000005e-05, global_step: 90, interval_runtime: 0.9226, interval_samples_per_second: 8.671, interval_steps_per_second: 10.839, epoch: 4.5[0m
[32m[2022-09-05 12:59:31,424] [    INFO][0m - loss: 0.71554112, learning_rate: 4.5e-05, global_step: 100, interval_runtime: 0.8752, interval_samples_per_second: 9.141, interval_steps_per_second: 11.426, epoch: 5.0[0m
[32m[2022-09-05 12:59:31,424] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 12:59:31,424] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-05 12:59:31,424] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 12:59:31,425] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 12:59:31,425] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-05 12:59:32,301] [    INFO][0m - eval_loss: 0.8864619135856628, eval_accuracy: 0.5157232704402516, eval_runtime: 0.8761, eval_samples_per_second: 181.483, eval_steps_per_second: 22.828, epoch: 5.0[0m
[32m[2022-09-05 12:59:32,310] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-100[0m
[32m[2022-09-05 12:59:32,310] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 12:59:35,588] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-05 12:59:35,589] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-05 12:59:41,892] [    INFO][0m - loss: 0.58359694, learning_rate: 4.35e-05, global_step: 110, interval_runtime: 10.4679, interval_samples_per_second: 0.764, interval_steps_per_second: 0.955, epoch: 5.5[0m
[32m[2022-09-05 12:59:42,754] [    INFO][0m - loss: 0.56406469, learning_rate: 4.2e-05, global_step: 120, interval_runtime: 0.8624, interval_samples_per_second: 9.276, interval_steps_per_second: 11.595, epoch: 6.0[0m
[32m[2022-09-05 12:59:43,669] [    INFO][0m - loss: 0.37953615, learning_rate: 4.05e-05, global_step: 130, interval_runtime: 0.9152, interval_samples_per_second: 8.741, interval_steps_per_second: 10.927, epoch: 6.5[0m
[32m[2022-09-05 12:59:44,534] [    INFO][0m - loss: 0.40341349, learning_rate: 3.9e-05, global_step: 140, interval_runtime: 0.8652, interval_samples_per_second: 9.246, interval_steps_per_second: 11.558, epoch: 7.0[0m
[32m[2022-09-05 12:59:45,464] [    INFO][0m - loss: 0.45048633, learning_rate: 3.7500000000000003e-05, global_step: 150, interval_runtime: 0.9294, interval_samples_per_second: 8.608, interval_steps_per_second: 10.76, epoch: 7.5[0m
[32m[2022-09-05 12:59:46,328] [    INFO][0m - loss: 0.28776617, learning_rate: 3.6e-05, global_step: 160, interval_runtime: 0.864, interval_samples_per_second: 9.259, interval_steps_per_second: 11.574, epoch: 8.0[0m
[32m[2022-09-05 12:59:47,247] [    INFO][0m - loss: 0.28189385, learning_rate: 3.45e-05, global_step: 170, interval_runtime: 0.9196, interval_samples_per_second: 8.7, interval_steps_per_second: 10.875, epoch: 8.5[0m
[32m[2022-09-05 12:59:48,113] [    INFO][0m - loss: 0.22646222, learning_rate: 3.3e-05, global_step: 180, interval_runtime: 0.866, interval_samples_per_second: 9.238, interval_steps_per_second: 11.547, epoch: 9.0[0m
[32m[2022-09-05 12:59:49,039] [    INFO][0m - loss: 0.18501682, learning_rate: 3.15e-05, global_step: 190, interval_runtime: 0.9258, interval_samples_per_second: 8.641, interval_steps_per_second: 10.802, epoch: 9.5[0m
[32m[2022-09-05 12:59:49,912] [    INFO][0m - loss: 0.23259568, learning_rate: 3e-05, global_step: 200, interval_runtime: 0.8726, interval_samples_per_second: 9.168, interval_steps_per_second: 11.46, epoch: 10.0[0m
[32m[2022-09-05 12:59:49,913] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 12:59:49,913] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-05 12:59:49,913] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 12:59:49,913] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 12:59:49,913] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-05 12:59:50,779] [    INFO][0m - eval_loss: 2.9690024852752686, eval_accuracy: 0.5345911949685535, eval_runtime: 0.866, eval_samples_per_second: 183.6, eval_steps_per_second: 23.094, epoch: 10.0[0m
[32m[2022-09-05 12:59:50,785] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-200[0m
[32m[2022-09-05 12:59:50,785] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 12:59:54,597] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-200/tokenizer_config.json[0m
[32m[2022-09-05 12:59:54,597] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-200/special_tokens_map.json[0m
[32m[2022-09-05 13:00:03,566] [    INFO][0m - loss: 0.07227318, learning_rate: 2.8499999999999998e-05, global_step: 210, interval_runtime: 13.6539, interval_samples_per_second: 0.586, interval_steps_per_second: 0.732, epoch: 10.5[0m
[32m[2022-09-05 13:00:04,425] [    INFO][0m - loss: 0.19905668, learning_rate: 2.7000000000000002e-05, global_step: 220, interval_runtime: 0.8593, interval_samples_per_second: 9.309, interval_steps_per_second: 11.637, epoch: 11.0[0m
[32m[2022-09-05 13:00:05,345] [    INFO][0m - loss: 0.0214626, learning_rate: 2.55e-05, global_step: 230, interval_runtime: 0.9197, interval_samples_per_second: 8.699, interval_steps_per_second: 10.873, epoch: 11.5[0m
[32m[2022-09-05 13:00:06,204] [    INFO][0m - loss: 0.12087377, learning_rate: 2.4e-05, global_step: 240, interval_runtime: 0.8589, interval_samples_per_second: 9.315, interval_steps_per_second: 11.643, epoch: 12.0[0m
[32m[2022-09-05 13:00:07,125] [    INFO][0m - loss: 0.16405997, learning_rate: 2.25e-05, global_step: 250, interval_runtime: 0.921, interval_samples_per_second: 8.686, interval_steps_per_second: 10.858, epoch: 12.5[0m
[32m[2022-09-05 13:00:07,987] [    INFO][0m - loss: 0.01753509, learning_rate: 2.1e-05, global_step: 260, interval_runtime: 0.862, interval_samples_per_second: 9.281, interval_steps_per_second: 11.601, epoch: 13.0[0m
[32m[2022-09-05 13:00:08,911] [    INFO][0m - loss: 0.03069881, learning_rate: 1.95e-05, global_step: 270, interval_runtime: 0.9244, interval_samples_per_second: 8.654, interval_steps_per_second: 10.817, epoch: 13.5[0m
[32m[2022-09-05 13:00:09,781] [    INFO][0m - loss: 0.00158623, learning_rate: 1.8e-05, global_step: 280, interval_runtime: 0.8693, interval_samples_per_second: 9.203, interval_steps_per_second: 11.503, epoch: 14.0[0m
[32m[2022-09-05 13:00:10,705] [    INFO][0m - loss: 0.00078641, learning_rate: 1.65e-05, global_step: 290, interval_runtime: 0.9248, interval_samples_per_second: 8.65, interval_steps_per_second: 10.813, epoch: 14.5[0m
[32m[2022-09-05 13:00:11,572] [    INFO][0m - loss: 0.01130386, learning_rate: 1.5e-05, global_step: 300, interval_runtime: 0.8666, interval_samples_per_second: 9.232, interval_steps_per_second: 11.54, epoch: 15.0[0m
[32m[2022-09-05 13:00:11,572] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 13:00:11,572] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-05 13:00:11,572] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 13:00:11,572] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 13:00:11,573] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-05 13:00:12,433] [    INFO][0m - eval_loss: 3.8509628772735596, eval_accuracy: 0.559748427672956, eval_runtime: 0.8604, eval_samples_per_second: 184.791, eval_steps_per_second: 23.244, epoch: 15.0[0m
[32m[2022-09-05 13:00:12,440] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-300[0m
[32m[2022-09-05 13:00:12,440] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 13:00:15,599] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-300/tokenizer_config.json[0m
[32m[2022-09-05 13:00:15,599] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-300/special_tokens_map.json[0m
[32m[2022-09-05 13:00:22,076] [    INFO][0m - loss: 0.00334499, learning_rate: 1.3500000000000001e-05, global_step: 310, interval_runtime: 10.5038, interval_samples_per_second: 0.762, interval_steps_per_second: 0.952, epoch: 15.5[0m
[32m[2022-09-05 13:00:22,941] [    INFO][0m - loss: 0.07908166, learning_rate: 1.2e-05, global_step: 320, interval_runtime: 0.8654, interval_samples_per_second: 9.244, interval_steps_per_second: 11.555, epoch: 16.0[0m
[32m[2022-09-05 13:00:23,862] [    INFO][0m - loss: 0.00269178, learning_rate: 1.05e-05, global_step: 330, interval_runtime: 0.9209, interval_samples_per_second: 8.687, interval_steps_per_second: 10.859, epoch: 16.5[0m
[32m[2022-09-05 13:00:24,726] [    INFO][0m - loss: 0.00031998, learning_rate: 9e-06, global_step: 340, interval_runtime: 0.8638, interval_samples_per_second: 9.261, interval_steps_per_second: 11.577, epoch: 17.0[0m
[32m[2022-09-05 13:00:25,652] [    INFO][0m - loss: 0.00076208, learning_rate: 7.5e-06, global_step: 350, interval_runtime: 0.9261, interval_samples_per_second: 8.639, interval_steps_per_second: 10.798, epoch: 17.5[0m
[32m[2022-09-05 13:00:26,518] [    INFO][0m - loss: 0.00043882, learning_rate: 6e-06, global_step: 360, interval_runtime: 0.8666, interval_samples_per_second: 9.231, interval_steps_per_second: 11.539, epoch: 18.0[0m
[32m[2022-09-05 13:00:27,447] [    INFO][0m - loss: 0.04994913, learning_rate: 4.5e-06, global_step: 370, interval_runtime: 0.9291, interval_samples_per_second: 8.611, interval_steps_per_second: 10.763, epoch: 18.5[0m
[32m[2022-09-05 13:00:28,310] [    INFO][0m - loss: 0.01461361, learning_rate: 3e-06, global_step: 380, interval_runtime: 0.8629, interval_samples_per_second: 9.271, interval_steps_per_second: 11.589, epoch: 19.0[0m
[32m[2022-09-05 13:00:29,239] [    INFO][0m - loss: 0.00013768, learning_rate: 1.5e-06, global_step: 390, interval_runtime: 0.9288, interval_samples_per_second: 8.614, interval_steps_per_second: 10.767, epoch: 19.5[0m
[32m[2022-09-05 13:00:30,105] [    INFO][0m - loss: 0.05821552, learning_rate: 0.0, global_step: 400, interval_runtime: 0.8661, interval_samples_per_second: 9.237, interval_steps_per_second: 11.546, epoch: 20.0[0m
[32m[2022-09-05 13:00:30,106] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 13:00:30,106] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-05 13:00:30,106] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 13:00:30,106] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 13:00:30,106] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-09-05 13:00:31,047] [    INFO][0m - eval_loss: 4.056990146636963, eval_accuracy: 0.5471698113207547, eval_runtime: 0.9403, eval_samples_per_second: 169.091, eval_steps_per_second: 21.269, epoch: 20.0[0m
[32m[2022-09-05 13:00:31,054] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/checkpoint-400[0m
[32m[2022-09-05 13:00:31,054] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 13:00:37,266] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/checkpoint-400/tokenizer_config.json[0m
[32m[2022-09-05 13:00:37,266] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/checkpoint-400/special_tokens_map.json[0m
[32m[2022-09-05 13:00:43,044] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-05 13:00:43,044] [    INFO][0m - Loading best model from ./checkpoints_cluewsc/checkpoint-300 (score: 0.559748427672956).[0m
[32m[2022-09-05 13:00:43,962] [    INFO][0m - train_runtime: 81.5259, train_samples_per_second: 39.251, train_steps_per_second: 4.906, train_loss: 0.35861565929895733, epoch: 20.0[0m
[32m[2022-09-05 13:00:44,004] [    INFO][0m - Saving model checkpoint to ./checkpoints_cluewsc/[0m
[32m[2022-09-05 13:00:44,004] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 13:00:47,747] [    INFO][0m - tokenizer config file saved in ./checkpoints_cluewsc/tokenizer_config.json[0m
[32m[2022-09-05 13:00:47,748] [    INFO][0m - Special tokens file saved in ./checkpoints_cluewsc/special_tokens_map.json[0m
[32m[2022-09-05 13:00:47,749] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-05 13:00:47,749] [    INFO][0m -   epoch                    =       20.0[0m
[32m[2022-09-05 13:00:47,749] [    INFO][0m -   train_loss               =     0.3586[0m
[32m[2022-09-05 13:00:47,749] [    INFO][0m -   train_runtime            = 0:01:21.52[0m
[32m[2022-09-05 13:00:47,749] [    INFO][0m -   train_samples_per_second =     39.251[0m
[32m[2022-09-05 13:00:47,749] [    INFO][0m -   train_steps_per_second   =      4.906[0m
[32m[2022-09-05 13:00:47,753] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-05 13:00:47,753] [    INFO][0m -   Num examples = 976[0m
[32m[2022-09-05 13:00:47,753] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 13:00:47,753] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 13:00:47,753] [    INFO][0m -   Total prediction steps = 122[0m
[32m[2022-09-05 13:00:53,299] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-05 13:00:53,299] [    INFO][0m -   test_accuracy           =     0.5154[0m
[32m[2022-09-05 13:00:53,299] [    INFO][0m -   test_loss               =     4.3506[0m
[32m[2022-09-05 13:00:53,300] [    INFO][0m -   test_runtime            = 0:00:05.54[0m
[32m[2022-09-05 13:00:53,300] [    INFO][0m -   test_samples_per_second =    175.984[0m
[32m[2022-09-05 13:00:53,300] [    INFO][0m -   test_steps_per_second   =     21.998[0m
====================
1
= 0 ===================
[('Èîô', 290), ('Ê≠£', 198)]
----------
[('Ê≠£', 290), ('Èîô', 198)]
----------
[('Á°Æ', 360), ('ËØØ', 128)]
----------
[('ËØØ', 359), ('Á°Æ', 128), ('ÂÅ•', 1)]
----------
[('ÊÅ∞', 268), ('ÂáÜ', 185), ('‰∏ç', 12), ('Èáç', 4), ('ÂÖ®', 4), ('ÂØπ', 3), ('Âèç', 2), ('ÂÅ•', 2), ('Ë¥ü', 2), ('Ê®°', 1), ('Áóõ', 1), ('Ê†á', 1), ('Êñπ', 1), ('ÁóÖ', 1), ('‰∫∫', 1)]
----------
= 1 ===================
[('ËØØ', 283), ('Á°Æ', 205)]
----------
[('Á°Æ', 283), ('ËØØ', 205)]
----------
[('Èîô', 354), ('Ê≠£', 134)]
----------
[('Ê≠£', 354), ('Èîô', 131), ('Â∏∏', 2), ('Âàá', 1)]
----------
[('‰π±', 242), ('Â∏∏', 195), ('Ê•ö', 18), ('ÂΩì', 12), ('Âàá', 11), ('Ê≥ï', 3), ('Â≠¶', 1), ('Èîô', 1), ('‰æø', 1), ('Âç¥', 1), ('ÈÉ®', 1), ('ÁÇπ', 1), ('Áã¨', 1)]
----------
0
= 0 ===================
[('Èîô', 275), ('Ê≠£', 213)]
----------
[('Ê≠£', 275), ('Èîô', 213)]
----------
[('Á°Æ', 375), ('ËØØ', 113)]
----------
[('ËØØ', 375), ('Á°Æ', 113)]
----------
[('ÊÅ∞', 307), ('ÂáÜ', 156), ('‰∏ç', 7), ('ÂÖ®', 3), ('Èáç', 3), ('Ê∞î', 2), ('ÂØπ', 2), ('Âèç', 1), ('ÁóÖ', 1), ('Ê®°', 1), ('Â•≥', 1), ('Ê†á', 1), ('ÂÅ•', 1), ('Áóõ', 1), ('‰∫∫', 1)]
----------
= 1 ===================
[('ËØØ', 271), ('Á°Æ', 217)]
----------
[('Á°Æ', 271), ('ËØØ', 217)]
----------
[('Èîô', 336), ('Ê≠£', 152)]
----------
[('Ê≠£', 336), ('Èîô', 152)]
----------
[('‰π±', 247), ('Â∏∏', 186), ('ÂΩì', 21), ('Ê•ö', 16), ('Âàá', 7), ('Ê≥ï', 4), ('Âç¥', 3), ('ÁÇπ', 2), ('ÁóÖ', 1), ('Áã¨', 1)]
----------
====================
1
= 0 ===================
[('Èîô', 249), ('ÂØπ', 239)]
----------
[('ÂØπ', 249), ('Èîô', 239)]
----------
[('Ôºå', 341), ('Êúâ', 134), ('ÁöÑ', 13)]
----------
[('Êúâ', 211), ('ÁöÑ', 148), ('Ôºå', 123), ('Ëøá', 6)]
----------
[('ÁöÑ', 324), ('Êúâ', 101), ('Ëøá', 37), ('Ôºå', 24), ('Âú®', 2)]
----------
0
= 0 ===================
[('ÂØπ', 245), ('Èîô', 243)]
----------
[('Èîô', 245), ('ÂØπ', 243)]
----------
[('Ôºå', 348), ('Êúâ', 127), ('ÁöÑ', 13)]
----------
[('Êúâ', 220), ('ÁöÑ', 135), ('Ôºå', 128), ('Ëøá', 5)]
----------
[('ÁöÑ', 340), ('Êúâ', 102), ('Ëøá', 33), ('Ôºå', 12), ('Â∑Æ', 1)]
----------
====================
1
= 0 ===================
[('Èîô', 321), ('ÂØπ', 167)]
----------
[('ÂØπ', 321), ('Èîô', 167)]
----------
[('ÁöÑ', 410), ('Êúâ', 40), ('Ëøá', 35), ('Â∑Æ', 3)]
----------
[('Êúâ', 220), ('Ëøá', 188), ('ÁöÑ', 51), ('Â∑Æ', 25), ('Âèç', 3), ('Âæó', 1)]
----------
[('Ëøá', 234), ('Êúâ', 140), ('Â∑Æ', 70), ('ÁöÑ', 21), ('Âæó', 14), ('Âèç', 4), ('Áúü', 3), ('Ôºå', 2)]
----------
0
= 0 ===================
[('Èîô', 306), ('ÂØπ', 182)]
----------
[('ÂØπ', 306), ('Èîô', 182)]
----------
[('ÁöÑ', 418), ('Êúâ', 37), ('Ëøá', 29), ('Â∑Æ', 4)]
----------
[('Êúâ', 213), ('Ëøá', 191), ('ÁöÑ', 53), ('Â∑Æ', 26), ('Âæó', 2), ('Âèç', 2), ('Ôºå', 1)]
----------
[('Ëøá', 235), ('Êúâ', 137), ('Â∑Æ', 82), ('ÁöÑ', 11), ('Âèç', 11), ('Âæó', 9), ('Ôºå', 3)]
----------
====================
1
= 0 ===================
[('ÊòØ', 373), ('Âê¶', 115)]
----------
[('Âê¶', 235), ('ÊòØ', 115), ('„ÄÇ', 100), ('ÁöÑ', 35), ('Êúâ', 2), ('‰∏∫', 1)]
----------
[('„ÄÇ', 152), ('ÁöÑ', 70), ('‰∏ç', 52), ('Êúâ', 47), ('Âê¶', 46), ('ÁÑ∂', 36), ('‰∏∫', 33), ('Ôºå', 12), ('Âì™', 11), ('‰Ωï', 10), ('Êú™', 9), ('‰∏Ä', 2), ('Êó†', 2), ('Ë∞Å', 2), ('Âú®', 1), ('Á°Æ', 1), ('Èùû', 1), ('‚Äù', 1)]
----------
[('Êúâ', 117), ('ÁöÑ', 107), ('‰∏ç', 59), ('‰∏∫', 41), ('Âê¶', 37), ('Âì™', 26), ('„ÄÇ', 23), ('Êó†', 16), ('Êú™', 13), ('Ôºå', 9), ('ÁÑ∂', 9), ('Èùû', 7), ('‰Ωï', 6), ('‰∏Ä', 6), ('Ë∞Å', 2), ('Ëã•', 2), ('Ëøò', 2), ('Êàñ', 1), ('„ÄÅ', 1), ('ÂèØ', 1), ('Âú®', 1), ('Â§™', 1), ('‚Äú', 1)]
----------
[('Êúâ', 105), ('‰∏∫', 79), ('‰∏ç', 52), ('ÁöÑ', 41), ('Âê¶', 28), ('„ÄÇ', 21), ('‰Ωï', 20), ('‰∏Ä', 20), ('Èùû', 18), ('Êú™', 13), ('Êó†', 11), ('Ôºå', 11), ('Âì™', 11), ('Âú®', 10), ('ÁÑ∂', 10), ('Ë∞Å', 4), ('Âêó', 4), ('„ÄÅ', 4), ('Ëøò', 4), ('Ë¶Å', 3), ('Ôºü', 3), ('Â¶Ç', 3), ('Âàô', 2), ('ÂèØ', 2), ('Êàñ', 2), ('‚Äù', 2), ('Áªà', 1), ('ÂàÜ', 1), ('Âπ∂', 1), ('‰πà', 1), ('ÊÄé', 1)]
----------
0
= 0 ===================
[('ÊòØ', 354), ('Âê¶', 134)]
----------
[('Âê¶', 253), ('ÊòØ', 134), ('„ÄÇ', 86), ('ÁöÑ', 13), ('Êúâ', 2)]
----------
[('„ÄÇ', 160), ('ÁöÑ', 75), ('‰∏ç', 63), ('ÁÑ∂', 41), ('Âê¶', 40), ('Êúâ', 34), ('‰∏∫', 23), ('Âì™', 12), ('Êú™', 11), ('Ôºå', 9), ('‰Ωï', 5), ('Êó†', 5), ('Èùû', 2), ('Êàñ', 2), ('Â¶Ç', 1), ('‚Äù', 1), ('Ëøò', 1), ('‰∏Ä', 1), ('Ë∞Å', 1), ('Ëã•', 1)]
----------
[('Êúâ', 115), ('ÁöÑ', 94), ('‰∏ç', 68), ('‰∏∫', 37), ('Âì™', 28), ('„ÄÇ', 27), ('Âê¶', 27), ('Êú™', 14), ('ÁÑ∂', 13), ('Êó†', 11), ('Èùû', 11), ('Ôºå', 9), ('‰Ωï', 8), ('ÂèØ', 5), ('Âπ∂', 3), ('‰∏Ä', 3), ('Âú®', 3), ('Â∞ë', 2), ('Ë∞Å', 2), ('Â¶Ç', 2), ('Ëøò', 1), ('Ê≤°', 1), ('Ôºü', 1), ('Êàñ', 1), ('Á°Æ', 1), ('Áúã', 1)]
----------
[('Êúâ', 97), ('ÁöÑ', 69), ('‰∏∫', 62), ('‰∏ç', 56), ('Ôºå', 22), ('Êó†', 21), ('„ÄÇ', 18), ('Âê¶', 17), ('Êú™', 16), ('‰∏Ä', 15), ('‰Ωï', 15), ('Âì™', 10), ('Ëøò', 10), ('Èùû', 7), ('Ë∞Å', 6), ('Âπ∂', 6), ('ÁÑ∂', 6), ('Âú®', 6), ('Âêó', 3), ('Ë¶Å', 3), ('Ê≤°', 2), ('‚Äù', 2), ('Á°Æ', 2), ('Êó∂', 2), ('Êàñ', 2), ('Â¶Ç', 2), ('ÂèØ', 2), ('Ôºü', 2), ('Ëøá', 2), ('Â∞ë', 1), ('„ÄÅ', 1), ('‚Äú', 1), ('Âàô', 1), ('‰πà', 1)]
----------
====================
1
= 0 ===================
[('Ê≠£', 380), ('Èîô', 108)]
----------
[('Èîô', 280), ('Ê≠£', 100), ('Á°Æ', 97), ('ËØØ', 8), ('Âçö', 1), ('‰∏≠', 1), ('Êàê', 1)]
----------
[('Á°Æ', 269), ('ËØØ', 42), ('Êàê', 37), ('Èîô', 34), ('ÂØπ', 13), ('Â§ß', 12), ('ËÄÅ', 6), ('Â∞è', 4), ('‰Ωú', 4), ('Á≠â', 4), ('Âàö', 3), ('Êòé', 3), ('Ê≠£', 3), ('Ê£Æ', 3), ('ÂÖ®', 3), ('Ê∞¥', 3), ('ÂáÜ', 3), ('Ëøá', 2), ('Âá∫', 2), ('Âèë', 2), ('Ëá™', 2), ('Èáç', 2), ('È©¨', 2), ('Èõ™', 1), ('Êú∫', 1), ('Ê≤â', 1), ('Â§ú', 1), ('Âêé', 1), ('Áúü', 1), ('‰∏≠', 1), ('ÂÆÅ', 1), ('Ëøô', 1), ('‰∫î', 1), ('Â•≥', 1), ('Â±±', 1), ('ÂèØ', 1), ('Âêå', 1), ('‰∏ä', 1), ('Êóó', 1), ('Áâπ', 1), ('Â§±', 1), ('Âπ≥', 1), ('Ë°å', 1), ('Áæé', 1), ('Áé∞', 1), ('Â∏å', 1), ('Êï¥', 1), ('‰∫∫', 1), ('Âæó', 1), ('Èöè', 1), ('‰æù', 1), ('ÈÅì', 1), ('Ê∏Ö', 1)]
----------
[('Êàê', 81), ('Á°Æ', 64), ('ÂØπ', 64), ('Â§ß', 36), ('Âàö', 27), ('Êòé', 20), ('ÂáÜ', 19), ('Èîô', 19), ('ËØØ', 16), ('Áúü', 13), ('‰∏≠', 8), ('ËÄÅ', 6), ('Âá∫', 6), ('Âêå', 5), ('ÂèØ', 5), ('Ëøô', 5), ('Ê≠£', 5), ('ÊÅ∞', 5), ('Ëøá', 4), ('ÂÖ®', 4), ('Âêé', 3), ('Â§±', 3), ('È©¨', 3), ('Êùé', 3), ('È´ò', 2), ('‰∫∫', 2), ('Êúã', 2), ('Âπ¥', 2), ('Â•≥', 2), ('Èáë', 2), ('Èïø', 2), ('‰Ωú', 1), ('ÁÜü', 1), ('ÊÑ§', 1), ('Êñ∞', 1), ('‰øù', 1), ('ÂÅ•', 1), ('Âíå', 1), ('Êúâ', 1), ('ÁÉß', 1), ('‰∏ä', 1), ('Âà∂', 1), ('Ê∏Ö', 1), ('Âª∫', 1), ('Áº∫', 1), ('ÂÖ¨', 1), ('Áé∞', 1), ('ÊØç', 1), ('Ë∑Ø', 1), ('Ê∞¥', 1), ('ÁÆÄ', 1), ('Áóõ', 1), ('Êñá', 1), ('È£é', 1), ('Âπ∏', 1), ('Ê∞î', 1), ('ËÆ°', 1), ('Êì¶', 1), ('ÂÆù', 1), ('Â∞è', 1), ('ÂÆû', 1), ('Âú∞', 1), ('Êó†', 1), ('Êàò', 1), ('Â¢®', 1), ('Â•∂', 1), ('Ê†ë', 1), ('ÂØå', 1), ('Á•ñ', 1), ('Êâã', 1), ('Èó®', 1), ('ÈÉΩ', 1), ('‰∫Ü', 1), ('‰∏ç', 1), ('Â§™', 1), ('ÊãÖ', 1), ('Âπ≥', 1), ('Â§©', 1), ('Áî∑', 1), ('ËÉ°', 1), ('Áü≠', 1)]
----------
[('ÂØπ', 81), ('Êàê', 53), ('Â§ß', 48), ('Âàö', 38), ('ÂáÜ', 31), ('Á°Æ', 24), ('Áúü', 23), ('Êòé', 21), ('Â§±', 10), ('Âêå', 10), ('ËØØ', 8), ('Èîô', 8), ('Â∞è', 7), ('ÂèØ', 7), ('‰∏ç', 7), ('Âéü', 6), ('ÂÖ®', 6), ('Ëøá', 4), ('Âπ¥', 4), ('Ëøô', 4), ('ÊÅ∞', 4), ('ÂÖ¨', 3), ('Â•≥', 3), ('Áé∞', 3), ('‰∏≠', 3), ('Ëá™', 2), ('Áà∂', 2), ('Ê†á', 2), ('ËÄÅ', 2), ('ÁêÜ', 2), ('Êûó', 2), ('ÂÅ•', 2), ('Âêé', 2), ('Áõ¥', 2), ('Êó∂', 2), ('Áî∑', 2), ('Èïø', 2), ('È´ò', 2), ('ÊãÖ', 1), ('Âèë', 1), ('Áßë', 1), ('ÁôΩ', 1), ('ÊÄª', 1), ('ÈÉΩ', 1), ('ÂÆå', 1), ('Êñá', 1), ('ÈÅì', 1), ('ÂÖ≥', 1), ('Èáç', 1), ('Âá∫', 1), ('Èáë', 1), ('Ê≥ï', 1), ('Âú∞', 1), ('Êï¥', 1), ('Â§ç', 1), ('Áæé', 1), ('‰∏ä', 1), ('Êùé', 1), ('Á≠â', 1), ('Âø´', 1), ('Êúâ', 1), ('Áúã', 1), ('Êù•', 1), ('Ëøò', 1), ('Áõ∏', 1), ('Áªù', 1), ('‰∫ë', 1), ('Áº∫', 1), ('ÊòØ', 1), ('‰π¶', 1), ('Âåó', 1), ('Êºè', 1), ('Áªè', 1), ('ÁÆÄ', 1), ('È©¨', 1), ('Êîø', 1), ('‰∫î', 1), ('ÊñØ', 1), ('ÊôØ', 1), ('‰∏Ä', 1), ('Êó†', 1), ('ÂÆã', 1), ('Âª∫', 1), ('Ê∞¥', 1)]
----------
= 1 ===================
[('Á°Æ', 390), ('ËØØ', 98)]
----------
[('ËØØ', 324), ('Á°Æ', 88), ('Ê≠£', 55), ('Èîô', 10), ('ÂÆû', 5), ('‰∫∫', 3), ('Â§ß', 1), ('Âπ¥', 1), ('Áîü', 1)]
----------
[('Ê≠£', 195), ('Èîô', 108), ('ÂÆû', 41), ('‰∫∫', 31), ('ËØØ', 24), ('Á°Æ', 10), ('ÂΩì', 8), ('Â∏∏', 6), ('ÁöÑ', 4), ('ÂáÜ', 4), ('Â≠ê', 4), ('ÂÆö', 3), ('ÂÖ∞', 3), ('ÂøÉ', 2), ('Èùô', 2), ('Â≠¶', 2), ('Ê†∑', 2), ('Êúâ', 2), ('Âø´', 2), ('Áîü', 2), ('ÂæÖ', 2), ('Â∫è', 2), ('ÊÄí', 1), ('Èô©', 1), ('‰∫≤', 1), ('Êôö', 1), ('Â∏Æ', 1), ('Âπ≥', 1), ('ËÆ∫', 1), ('‰∏™', 1), ('‰ª•', 1), ('Ëã¶', 1), ('Â£´', 1), ('Êñá', 1), ('ÂÖ®', 1), ('ÂõΩ', 1), ('‰ª£', 1), ('Êúõ', 1), ('‰ª¨', 1), ('ÊÑè', 1), ('‰∏≠', 1), ('Èáç', 1), ('Âúü', 1), ('Ë∑Ø', 1), ('‰æø', 1), ('Â®ú', 1), ('ÊÉÖ', 1), ('‰∏Ä', 1), ('Áâ©', 1), ('‰Ωì', 1), ('Ëß£', 1)]
----------
[('Ê≠£', 84), ('ÂÆû', 74), ('Â∏∏', 43), ('‰∫∫', 34), ('Èîô', 34), ('ÂáÜ', 22), ('ÁöÑ', 19), ('ÂΩì', 19), ('‰∏≠', 13), ('ÂÆö', 12), ('ËØØ', 10), ('Â≠ê', 6), ('‰∫≤', 5), ('ËÆ§', 4), ('ÂÜ≤', 4), ('Ê£Æ', 3), ('ÂÖ∞', 3), ('Êúç', 3), ('ÊÑè', 3), ('Ëøá', 3), ('Êñá', 3), ('ÂÆ∂', 2), ('ÂøÉ', 2), ('È©¨', 2), ('ÈÅì', 2), ('‰ªñ', 2), ('Á∫¢', 2), ('ÁôΩ', 2), ('Âπ≥', 2), ('Âêé', 2), ('ÂºÄ', 2), ('ÂæÖ', 2), ('ÊÇâ', 1), ('ÊûÑ', 1), ('Èªò', 1), ('Êûú', 1), ('Âà∞', 1), ('Â§ú', 1), ('‰∫ë', 1), ('Â∑¥', 1), ('Ê†∑', 1), ('ÂÖ•', 1), ('Èí¢', 1), ('Êçü', 1), ('ÊÉú', 1), ('ÂàÜ', 1), ('Âéª', 1), ('Ëø´', 1), ('Âè∞', 1), ('Èù¢', 1), ('ÂØü', 1), ('Êúü', 1), ('Âçï', 1), ('Ê±ü', 1), ('Âéå', 1), ('ÂÅ∂', 1), ('Á∫ß', 1), ('ÊòØ', 1), ('Á¶è', 1), ('Â§©', 1), ('Ê≥®', 1), ('Ëøê', 1), ('Ê∞ë', 1), ('Èòü', 1), ('Âçá', 1), ('Âäõ', 1), ('ÁÉ≠', 1), ('Âõæ', 1), ('Âú®', 1), ('Êâã', 1), ('Êü±', 1), ('Ê±Ç', 1), ('Áïè', 1), ('‰π±', 1), ('Êú¨', 1), ('Âíå', 1), ('ÂÖ®', 1), ('Âêë', 1), ('Â≠¶', 1), ('‰Ωç', 1), ('Âç∞', 1), ('Èáå', 1), ('ÂºÉ', 1), ('ÁõÜ', 1), ('Ê†º', 1), ('Â∞è', 1), ('Êàø', 1), ('Ëß£', 1), ('ËêΩ', 1), ('Êúà', 1), ('Âà©', 1), ('Ë∞¢', 1), ('Êù•', 1), ('Ê∑°', 1), ('ÊÑü', 1), ('Áã¨', 1), ('Âõ∫', 1)]
----------
[('ÂÆû', 77), ('Ê≠£', 51), ('Â∏∏', 48), ('‰∫∫', 38), ('ÂΩì', 33), ('ÂáÜ', 31), ('ÂÆö', 30), ('Èîô', 22), ('ÁöÑ', 21), ('‰∏≠', 13), ('ËØØ', 6), ('Â≠¶', 6), ('ÂØπ', 6), ('Êï¥', 5), ('Â§±', 4), ('Âêé', 4), ('Âíå', 4), ('‰∫≤', 4), ('Âéª', 3), ('ÂÆâ', 2), ('‰ª¨', 2), ('Áîü', 2), ('Èó¥', 2), ('Â≠ê', 2), ('Ë∞±', 2), ('ÂøÉ', 2), ('ËÆ§', 2), ('Âèë', 2), ('Êú¨', 2), ('Á≥ª', 2), ('Êù•', 2), ('Ëµ∑', 2), ('ÊòØ', 2), ('Áúü', 2), ('ÊÑè', 2), ('ÊµÅ', 2), ('ÊØç', 2), ('Êñá', 1), ('Âà©', 1), ('ÊÄß', 1), ('ËÉå', 1), ('Â§ñ', 1), ('Â§©', 1), ('Âä®', 1), ('‰ªñ', 1), ('ÊÜæ', 1), ('‰æù', 1), ('ÈæÑ', 1), ('Ë∑Ø', 1), ('Âú®', 1), ('Á≥ä', 1), ('Áâõ', 1), ('ÈÖí', 1), ('Âõ∫', 1), ('Âç∞', 1), ('Ëøá', 1), ('Ëø´', 1), ('Âçó', 1), ('ÈÉΩ', 1), ('ÂºÄ', 1), ('Èùô', 1), ('Êòé', 1), ('‰∫ã', 1), ('ÂÆ∂', 1), ('Êúõ', 1), ('Èòü', 1), ('ÁΩó', 1), ('Â∫¶', 1), ('Â∞±', 1), ('‰∏™', 1), ('ÂÜ≤', 1), ('‰π±', 1), ('ËΩª', 1), ('Ê†°', 1), ('Áà∂', 1), ('Âπ¥', 1), ('Ëß£', 1), ('ÂÄº', 1), ('Â®ú', 1), ('Êòì', 1), ('ÊÉÖ', 1), ('ÂÖ®', 1), ('Èïø', 1)]
----------
0
= 0 ===================
[('Ê≠£', 365), ('Èîô', 123)]
----------
[('Èîô', 274), ('Ê≠£', 113), ('Á°Æ', 87), ('ËØØ', 10), ('Êàê', 3), ('Â§ß', 1)]
----------
[('Á°Æ', 252), ('ËØØ', 48), ('Êàê', 33), ('Èîô', 33), ('Â§ß', 21), ('Ê≠£', 10), ('ÂØπ', 9), ('Â∞è', 8), ('‰∏≠', 5), ('Áúü', 4), ('Áé∞', 3), ('Ëøá', 3), ('Âπ≥', 2), ('‰∏Ä', 2), ('Â•≥', 2), ('ËÄÅ', 2), ('Á≠â', 2), ('Èáç', 2), ('Âêå', 2), ('Â±±', 2), ('‰∏ä', 2), ('Âàö', 2), ('ÂÖ®', 2), ('‰∫∫', 2), ('È´ò', 2), ('‰Ωú', 1), ('Ê∞¥', 1), ('È£é', 1), ('Âø†', 1), ('Áîü', 1), ('Â§©', 1), ('‰øù', 1), ('Áæé', 1), ('Âèë', 1), ('Á∫∏', 1), ('Âπ¥', 1), ('Êú¨', 1), ('Ëä±', 1), ('Âäõ', 1), ('‰∏ñ', 1), ('ÂÆâ', 1), ('È¶ñ', 1), ('ÂÆå', 1), ('ÂáÜ', 1), ('Êîø', 1), ('ÈªÑ', 1), ('ÂÆø', 1), ('‰º§', 1), ('Ê†á', 1), ('Ëçâ', 1), ('Âπ≤', 1), ('Êûï', 1), ('ÊØç', 1), ('Â≠¶', 1), ('Èù¢', 1), ('Èáë', 1), ('Êòé', 1), ('Èõ∑', 1)]
----------
[('Á°Æ', 91), ('Êàê', 65), ('ÂØπ', 55), ('Â§ß', 38), ('Âàö', 25), ('Êòé', 15), ('ÂáÜ', 12), ('ËØØ', 10), ('Èîô', 10), ('Áúü', 9), ('ÂÖ®', 8), ('Ëøô', 7), ('Áé∞', 6), ('Ëøá', 5), ('Â∞è', 5), ('Èïø', 5), ('‰∏≠', 5), ('‰Ωú', 4), ('Á≠â', 3), ('Êñ∞', 3), ('Âêå', 3), ('ËÄÅ', 3), ('ÂÅ•', 3), ('‰∏ç', 3), ('Ëá™', 2), ('Â•Ω', 2), ('Âæó', 2), ('ÂÆÅ', 2), ('Ëæõ', 2), ('Âêé', 2), ('Â•≥', 2), ('ÂÆ∂', 2), ('‰π¶', 2), ('Âá∫', 2), ('ÂèØ', 2), ('Áî∑', 2), ('Ê∞¥', 2), ('Êãñ', 2), ('È©¨', 2), ('ÂÆ¢', 2), ('Â§¥', 2), ('Â§±', 2), ('Êóß', 1), ('ÈÅì', 1), ('Êºè', 1), ('ÁÅ´', 1), ('Ê∫™', 1), ('Èöè', 1), ('ÂÖ≥', 1), ('‰ºó', 1), ('ÂÖâ', 1), ('Áªè', 1), ('Âπ¥', 1), ('Èõ∑', 1), ('ËÉÉ', 1), ('Áîü', 1), ('Êó©', 1), ('È¶ñ', 1), ('Êñá', 1), ('Ëî°', 1), ('ÊØç', 1), ('Ê≤º', 1), ('ÂÖ¨', 1), ('Èáë', 1), ('ÈªÑ', 1), ('ÊòØ', 1), ('ÂÆã', 1), ('Êñ≠', 1), ('Ë•ø', 1), ('ÊÅ∞', 1), ('Êùé', 1), ('Áîµ', 1), ('‰∫∫', 1), ('Ëãè', 1), ('Ê∏Ö', 1), ('Á•ñ', 1), ('Êàë', 1), ('Ë∫´', 1), ('Ëçâ', 1), ('‰∏ª', 1), ('Âà´', 1), ('Ê£Æ', 1), ('Áªø', 1), ('Êàø', 1), ('Âèë', 1), ('Â¶à', 1), ('Ëßí', 1), ('È´ò', 1), ('ÈóÆ', 1), ('‰∫ã', 1), ('Âåó', 1), ('Á©∫', 1), ('ÂÜõ', 1), ('Âú∞', 1), ('Áà±', 1), ('Êú®', 1), ('Ëæâ', 1), ('Ë°£', 1), ('Êï∞', 1), ('Ëä±', 1), ('Âê¨', 1)]
----------
[('ÂØπ', 67), ('Êàê', 46), ('Â§ß', 43), ('Áúü', 42), ('Âàö', 32), ('Á°Æ', 24), ('ÂáÜ', 19), ('ËØØ', 17), ('Êòé', 14), ('Ëøô', 9), ('Â∞è', 8), ('ÂèØ', 8), ('Èîô', 8), ('Ëøá', 8), ('‰∏≠', 6), ('Â§±', 5), ('Âêå', 5), ('Áúã', 4), ('Âø´', 4), ('Êñ∞', 3), ('Âú∞', 3), ('Âèë', 3), ('Áº∫', 3), ('Êùé', 3), ('ÂÖ®', 3), ('‰∏ç', 3), ('ÈìÅ', 3), ('Âêé', 3), ('Âéü', 3), ('Áî∑', 2), ('‰º§', 2), ('Êñá', 2), ('Áîµ', 2), ('ÂÅ•', 2), ('‰Ωú', 2), ('Âú®', 2), ('Ê∞¥', 2), ('ÊØç', 2), ('ÁÅ´', 2), ('ÂÆû', 2), ('‰∏â', 2), ('ÁêÜ', 2), ('ÊòØ', 2), ('Â§¥', 2), ('Â•≥', 2), ('Èáë', 2), ('Áªì', 2), ('‰∏ä', 2), ('Áõ∏', 2), ('Ëçâ', 2), ('‰∫ã', 1), ('ÂÜç', 1), ('Ë∑Ø', 1), ('ÂÆå', 1), ('Êù•', 1), ('Âåª', 1), ('Èùí', 1), ('Ê≤ô', 1), ('Âª∫', 1), ('Áà∂', 1), ('Âìà', 1), ('‰ª•', 1), ('‰∏™', 1), ('Âñú', 1), ('Áúº', 1), ('Ê≤≥', 1), ('Èïø', 1), ('ÈªÑ', 1), ('Ëá™', 1), ('Êâç', 1), ('Êó∂', 1), ('Èòø', 1), ('Âçì', 1), ('Êõ¥', 1), ('ÂõΩ', 1), ('Èáç', 1), ('ÂàÜ', 1), ('ÂÄí', 1), ('Áü≠', 1), ('Ë°®', 1), ('Êàò', 1), ('È´ò', 1), ('ÁöÆ', 1), ('Â§ö', 1), ('Áîü', 1), ('Áé∞', 1), ('Âπ¥', 1), ('Áù°', 1), ('ÂÆ∂', 1), ('Èöæ', 1), ('Èù¢', 1), ('Ëàå', 1), ('ÈÅì', 1), ('Áæé', 1), ('‰∏ì', 1), ('Èªë', 1), ('Âäõ', 1)]
----------
= 1 ===================
[('Á°Æ', 374), ('ËØØ', 114)]
----------
[('ËØØ', 315), ('Á°Æ', 106), ('Ê≠£', 44), ('‰∫∫', 8), ('Èîô', 8), ('ÂÆû', 3), ('Â≠ê', 2), ('Âπ¥', 1), ('Â§ß', 1)]
----------
[('Ê≠£', 171), ('Èîô', 110), ('ÂÆû', 53), ('‰∫∫', 20), ('ËØØ', 18), ('Â∏∏', 9), ('Á°Æ', 8), ('Â≠ê', 8), ('ÁöÑ', 7), ('ÂáÜ', 6), ('ÂΩì', 5), ('‰∫≤', 3), ('Áîü', 3), ('Âè£', 3), ('Èó¥', 3), ('Â≠¶', 3), ('È©¨', 2), ('Ê∞¥', 2), ('Á∫ß', 2), ('ÂÖ∞', 2), ('Âà∞', 2), ('ÂæÖ', 2), ('Âú∞', 2), ('Â∫è', 2), ('ËÉΩ', 2), ('Â∫¶', 2), ('ÂÆ∂', 2), ('‰∏≠', 2), ('Âπ¥', 2), ('Ê•º', 1), ('ÁêÜ', 1), ('ÊôØ', 1), ('‰æø', 1), ('Á≥ª', 1), ('Â∏Ç', 1), ('Ë•ø', 1), ('ÁôΩ', 1), ('Áîµ', 1), ('Ëã¶', 1), ('È§ê', 1), ('Ê≤≥', 1), ('ÈÉΩ', 1), ('Ê≥Ω', 1), ('Êñá', 1), ('Èáå', 1), ('Èùô', 1), ('Êéí', 1), ('Ê†∑', 1), ('ÂøÉ', 1), ('Â§ß', 1), ('Âø´', 1), ('‰ª¨', 1), ('Êàø', 1), ('Âèë', 1), ('Â§¥', 1), ('Â∑¥', 1), ('ÂõΩ', 1), ('Êòé', 1), ('Èù¢', 1), ('Êúç', 1), ('Ëßâ', 1)]
----------
[('ÂÆû', 93), ('Ê≠£', 81), ('Èîô', 32), ('Â∏∏', 28), ('‰∫∫', 28), ('ÁöÑ', 20), ('‰∏≠', 15), ('Â≠ê', 13), ('ÂΩì', 13), ('ÂÆö', 10), ('ÂáÜ', 9), ('‰∫≤', 8), ('ËØØ', 6), ('Ëøá', 4), ('Âè£', 3), ('ÊÑè', 3), ('ÂæÖ', 3), ('Êñ∞', 3), ('Â∫¶', 3), ('Â≠¶', 3), ('Áªù', 3), ('Â§ú', 3), ('ÂÖ∞', 3), ('Âéª', 3), ('ÂõΩ', 3), ('‰Ωú', 2), ('ÂÆ∂', 2), ('È©¨', 2), ('Ëæπ', 2), ('ÂºÄ', 2), ('Âçá', 2), ('ËØ¥', 2), ('ÁΩó', 1), ('Â§±', 1), ('ËßÅ', 1), ('Âêë', 1), ('Âà©', 1), ('Â¶π', 1), ('ÂÄç', 1), ('ÁôΩ', 1), ('ËØö', 1), ('Èùô', 1), ('Á¶ª', 1), ('Âêé', 1), ('Á≤í', 1), ('‰æØ', 1), ('Ëâ≤', 1), ('Êòé', 1), ('Á´ô', 1), ('Âêâ', 1), ('Èô©', 1), ('Ê±ü', 1), ('ÂÆπ', 1), ('Â•Ω', 1), ('Âéå', 1), ('‰ªñ', 1), ('Ë∞∑', 1), ('Êàê', 1), ('ÂºÉ', 1), ('ÂÖã', 1), ('‰∏ú', 1), ('Êñ≠', 1), ('Ëóè', 1), ('Ê¥ª', 1), ('Êñá', 1), ('Ê∏∏', 1), ('‰Ωï', 1), ('Èáå', 1), ('‰∏™', 1), ('Èù¢', 1), ('Â∞±', 1), ('Ê£Ä', 1), ('Âíå', 1), ('ÈÉΩ', 1), ('Âú∞', 1), ('ÊòØ', 1), ('ÂÖ®', 1), ('Èïø', 1), ('ËÄÖ', 1), ('‰ºØ', 1), ('ÂøÉ', 1), ('Êù≠', 1), ('Áñë', 1), ('Áü≥', 1), ('Á≥ª', 1), ('Ëøê', 1), ('ÊÑü', 1), ('È°∂', 1), ('Âì•', 1), ('Áîü', 1), ('Ê£Æ', 1), ('Ëµ∑', 1), ('ËêΩ', 1), ('ÈÉ®', 1), ('Ê±Ç', 1), ('Ê†°', 1), ('Âç∞', 1), ('‰∫¨', 1), ('Êúõ', 1), ('Âú∫', 1), ('Ë∞¢', 1), ('‰π±', 1), ('Â§ß', 1), ('Êµ∑', 1), ('Êâã', 1), ('Ê≠å', 1), ('Ëææ', 1), ('‰∏ö', 1), ('Êòü', 1), ('Êöó', 1), ('ËßÇ', 1), ('ÊÉÖ', 1), ('Áªè', 1)]
----------
[('ÂÆû', 66), ('Ê≠£', 61), ('ÂÆö', 34), ('ÁöÑ', 34), ('‰∫∫', 30), ('ÂΩì', 29), ('Â∏∏', 27), ('‰∏≠', 24), ('ÂáÜ', 22), ('Èîô', 18), ('ÂØπ', 8), ('ËØØ', 8), ('Á≥ª', 5), ('Ëøá', 4), ('Âéª', 4), ('Â≠ê', 3), ('ÊòØ', 3), ('Âèë', 3), ('‰∫≤', 3), ('‰Ω©', 3), ('ÂõΩ', 3), ('Â≠¶', 3), ('Âπ¥', 2), ('Ê≠¢', 2), ('Èí¢', 2), ('‰ªñ', 2), ('Èù¢', 2), ('Âä®', 2), ('Êï¥', 2), ('ÊÑè', 2), ('Â∞±', 2), ('Êòé', 2), ('Ëèú', 2), ('Â∫¶', 2), ('Áîü', 2), ('Ê∞¥', 2), ('ÂÖ®', 2), ('ÁÅ´', 1), ('Êâã', 1), ('ÂëΩ', 1), ('Áâ©', 1), ('Ëææ', 1), ('Êûú', 1), ('Âêé', 1), ('Âüé', 1), ('ËΩ¶', 1), ('Â•Ω', 1), ('Ê¥ª', 1), ('Êó∂', 1), ('Ê∏ê', 1), ('Êúõ', 1), ('Êòè', 1), ('ÂØü', 1), ('Áü≥', 1), ('‰∏™', 1), ('Ëßâ', 1), ('ÊµÅ', 1), ('ÂÆ∂', 1), ('Âπ≥', 1), ('Â±±', 1), ('Áïå', 1), ('Èòü', 1), ('Êµ∑', 1), ('ÊöÇ', 1), ('Ë¢ã', 1), ('Â§±', 1), ('Êù•', 1), ('‰∏∫', 1), ('Áúü', 1), ('Ê±ü', 1), ('Á±≥', 1), ('ÈÅá', 1), ('Èáë', 1), ('Èáç', 1), ('Ë∑Ø', 1), ('Â§ñ', 1), ('Âà©', 1), ('Èó¥', 1), ('Êú¨', 1), ('Âú∞', 1), ('ÈÉ®', 1), ('‰æø', 1), ('Ëâ≤', 1), ('Á≥ä', 1), ('Á´•', 1), ('Áªù', 1), ('Èáå', 1), ('Èöæ', 1), ('Âú®', 1), ('Ê∞î', 1), ('Â§ú', 1), ('ËÉΩ', 1), ('ËÅî', 1), ('Âè£', 1), ('ËÉÅ', 1), ('Â§ß', 1), ('Á≠â', 1), ('ÈÄü', 1), ('Êúü', 1), ('Èùû', 1)]
----------
run.sh: line 58: --freeze_plm: command not found
