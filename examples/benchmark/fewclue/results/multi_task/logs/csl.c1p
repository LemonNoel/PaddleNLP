[33m[2022-09-05 12:54:03,050] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-09-05 12:54:03,050] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-05 12:54:03,050] [    INFO][0m - ============================================================[0m
[32m[2022-09-05 12:54:03,050] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-05 12:54:03,050] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-05 12:54:03,050] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-05 12:54:03,050] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-05 12:54:03,051] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-05 12:54:03,051] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-05 12:54:03,051] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-09-05 12:54:03,051] [    INFO][0m - [0m
[32m[2022-09-05 12:54:03,051] [    INFO][0m - ============================================================[0m
[32m[2022-09-05 12:54:03,051] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-05 12:54:03,051] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-05 12:54:03,051] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-05 12:54:03,051] [    INFO][0m - prompt                        :[0m
[32m[2022-09-05 12:54:03,051] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-05 12:54:03,051] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-05 12:54:03,051] [    INFO][0m - task_name                     :csl[0m
[32m[2022-09-05 12:54:03,051] [    INFO][0m - [0m
[32m[2022-09-05 12:54:03,052] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0905 12:54:03.053207 64966 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0905 12:54:03.057368 64966 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-05 12:54:08,737] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-09-05 12:54:08,763] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-09-05 12:54:08,764] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[32m[2022-09-05 12:54:08,765] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': 'Ê†áÈ¢ò‚Äú'}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': '‚ÄùÊ≠£ÊñáÔºö‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùÈ¢òÁõÆÂíåÊñáÁ´†ÊèèËø∞'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'Á¨¶„ÄÇ'}][0m
2022-09-05 12:54:08,769 INFO [download.py:119] unique_endpoints {''}
[32m[2022-09-05 12:54:09,267] [    INFO][0m - ============================================================[0m
[32m[2022-09-05 12:54:09,267] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-05 12:54:09,267] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-05 12:54:09,267] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-05 12:54:09,267] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-05 12:54:09,268] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-05 12:54:09,268] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-05 12:54:09,268] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-05 12:54:09,268] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-05 12:54:09,268] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-05 12:54:09,268] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-05 12:54:09,268] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-05 12:54:09,268] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-05 12:54:09,268] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-05 12:54:09,268] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-05 12:54:09,268] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-05 12:54:09,268] [    INFO][0m - do_predict                    :False[0m
[32m[2022-09-05 12:54:09,268] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-05 12:54:09,269] [    INFO][0m - eval_batch_size               :8[0m
[32m[2022-09-05 12:54:09,269] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-09-05 12:54:09,269] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-09-05 12:54:09,269] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-05 12:54:09,269] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-05 12:54:09,269] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-05 12:54:09,269] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-05 12:54:09,269] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-05 12:54:09,269] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-05 12:54:09,269] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-05 12:54:09,269] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-05 12:54:09,269] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-05 12:54:09,269] [    INFO][0m - learning_rate                 :1e-05[0m
[32m[2022-09-05 12:54:09,269] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-05 12:54:09,270] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-05 12:54:09,270] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-05 12:54:09,270] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-05 12:54:09,270] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-05 12:54:09,270] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-05 12:54:09,270] [    INFO][0m - logging_dir                   :./checkpoints_csl/runs/Sep05_12-54-03_instance-3bwob41y-01[0m
[32m[2022-09-05 12:54:09,270] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-05 12:54:09,270] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-05 12:54:09,270] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-05 12:54:09,270] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-05 12:54:09,270] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-05 12:54:09,270] [    INFO][0m - max_seq_length                :320[0m
[32m[2022-09-05 12:54:09,270] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-05 12:54:09,271] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-05 12:54:09,271] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-05 12:54:09,271] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-05 12:54:09,271] [    INFO][0m - num_train_epochs              :20.0[0m
[32m[2022-09-05 12:54:09,271] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-05 12:54:09,271] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-05 12:54:09,271] [    INFO][0m - output_dir                    :./checkpoints_csl/[0m
[32m[2022-09-05 12:54:09,271] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-05 12:54:09,271] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-05 12:54:09,271] [    INFO][0m - per_device_eval_batch_size    :8[0m
[32m[2022-09-05 12:54:09,271] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-09-05 12:54:09,271] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-05 12:54:09,271] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-05 12:54:09,271] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-05 12:54:09,272] [    INFO][0m - ppt_learning_rate             :1e-05[0m
[32m[2022-09-05 12:54:09,272] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-05 12:54:09,272] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-05 12:54:09,272] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-05 12:54:09,272] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-05 12:54:09,272] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-05 12:54:09,272] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-05 12:54:09,272] [    INFO][0m - run_name                      :./checkpoints_csl/[0m
[32m[2022-09-05 12:54:09,272] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-05 12:54:09,272] [    INFO][0m - save_steps                    :100[0m
[32m[2022-09-05 12:54:09,272] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-09-05 12:54:09,272] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-05 12:54:09,272] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-05 12:54:09,272] [    INFO][0m - seed                          :42[0m
[32m[2022-09-05 12:54:09,273] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-05 12:54:09,273] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-05 12:54:09,273] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-05 12:54:09,273] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-09-05 12:54:09,273] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-05 12:54:09,273] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-05 12:54:09,273] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-05 12:54:09,273] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-05 12:54:09,273] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-05 12:54:09,273] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-05 12:54:09,273] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-05 12:54:09,273] [    INFO][0m - [0m
[32m[2022-09-05 12:54:09,275] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-05 12:54:09,276] [    INFO][0m -   Num examples = 2642[0m
[32m[2022-09-05 12:54:09,276] [    INFO][0m -   Num Epochs = 20[0m
[32m[2022-09-05 12:54:09,276] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-09-05 12:54:09,276] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-09-05 12:54:09,276] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-05 12:54:09,276] [    INFO][0m -   Total optimization steps = 6620.0[0m
[32m[2022-09-05 12:54:09,276] [    INFO][0m -   Total num train samples = 52840[0m
[32m[2022-09-05 12:54:12,377] [    INFO][0m - loss: 3.13285065, learning_rate: 9.984894259818732e-06, global_step: 10, interval_runtime: 3.0999, interval_samples_per_second: 2.581, interval_steps_per_second: 3.226, epoch: 0.0302[0m
[32m[2022-09-05 12:54:14,196] [    INFO][0m - loss: 1.24983149, learning_rate: 9.969788519637464e-06, global_step: 20, interval_runtime: 1.8191, interval_samples_per_second: 4.398, interval_steps_per_second: 5.497, epoch: 0.0604[0m
[32m[2022-09-05 12:54:16,017] [    INFO][0m - loss: 1.01945868, learning_rate: 9.954682779456193e-06, global_step: 30, interval_runtime: 1.8175, interval_samples_per_second: 4.402, interval_steps_per_second: 5.502, epoch: 0.0906[0m
[32m[2022-09-05 12:54:17,858] [    INFO][0m - loss: 0.98434181, learning_rate: 9.939577039274926e-06, global_step: 40, interval_runtime: 1.8448, interval_samples_per_second: 4.336, interval_steps_per_second: 5.421, epoch: 0.1208[0m
[32m[2022-09-05 12:54:19,679] [    INFO][0m - loss: 0.80478859, learning_rate: 9.924471299093656e-06, global_step: 50, interval_runtime: 1.8204, interval_samples_per_second: 4.395, interval_steps_per_second: 5.493, epoch: 0.1511[0m
[32m[2022-09-05 12:54:21,496] [    INFO][0m - loss: 0.68195767, learning_rate: 9.909365558912388e-06, global_step: 60, interval_runtime: 1.8167, interval_samples_per_second: 4.404, interval_steps_per_second: 5.504, epoch: 0.1813[0m
[32m[2022-09-05 12:54:23,320] [    INFO][0m - loss: 0.6740551, learning_rate: 9.894259818731119e-06, global_step: 70, interval_runtime: 1.8245, interval_samples_per_second: 4.385, interval_steps_per_second: 5.481, epoch: 0.2115[0m
[32m[2022-09-05 12:54:25,138] [    INFO][0m - loss: 0.78562236, learning_rate: 9.87915407854985e-06, global_step: 80, interval_runtime: 1.8184, interval_samples_per_second: 4.399, interval_steps_per_second: 5.499, epoch: 0.2417[0m
[32m[2022-09-05 12:54:26,966] [    INFO][0m - loss: 0.79402037, learning_rate: 9.86404833836858e-06, global_step: 90, interval_runtime: 1.8281, interval_samples_per_second: 4.376, interval_steps_per_second: 5.47, epoch: 0.2719[0m
[32m[2022-09-05 12:54:28,798] [    INFO][0m - loss: 0.86393871, learning_rate: 9.848942598187312e-06, global_step: 100, interval_runtime: 1.8315, interval_samples_per_second: 4.368, interval_steps_per_second: 5.46, epoch: 0.3021[0m
[32m[2022-09-05 12:54:28,799] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 12:54:28,799] [    INFO][0m -   Num examples = 2712[0m
[32m[2022-09-05 12:54:28,799] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 12:54:28,799] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 12:54:28,799] [    INFO][0m -   Total prediction steps = 339[0m
[32m[2022-09-05 12:55:12,724] [    INFO][0m - eval_loss: 0.7509852051734924, eval_accuracy: 0.43768436578171094, eval_runtime: 43.924, eval_samples_per_second: 61.743, eval_steps_per_second: 7.718, epoch: 0.3021[0m
[32m[2022-09-05 12:55:12,785] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-100[0m
[32m[2022-09-05 12:55:12,786] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 12:55:16,038] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-05 12:55:16,038] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-05 12:55:23,377] [    INFO][0m - loss: 0.68094659, learning_rate: 9.833836858006043e-06, global_step: 110, interval_runtime: 54.5786, interval_samples_per_second: 0.147, interval_steps_per_second: 0.183, epoch: 0.3323[0m
[32m[2022-09-05 12:55:25,192] [    INFO][0m - loss: 0.75033107, learning_rate: 9.818731117824774e-06, global_step: 120, interval_runtime: 1.816, interval_samples_per_second: 4.405, interval_steps_per_second: 5.507, epoch: 0.3625[0m
[32m[2022-09-05 12:55:27,017] [    INFO][0m - loss: 0.62122769, learning_rate: 9.803625377643506e-06, global_step: 130, interval_runtime: 1.8243, interval_samples_per_second: 4.385, interval_steps_per_second: 5.482, epoch: 0.3927[0m
[32m[2022-09-05 12:55:28,837] [    INFO][0m - loss: 0.78251009, learning_rate: 9.788519637462236e-06, global_step: 140, interval_runtime: 1.8204, interval_samples_per_second: 4.395, interval_steps_per_second: 5.493, epoch: 0.423[0m
[32m[2022-09-05 12:55:30,659] [    INFO][0m - loss: 0.88030586, learning_rate: 9.773413897280967e-06, global_step: 150, interval_runtime: 1.8215, interval_samples_per_second: 4.392, interval_steps_per_second: 5.49, epoch: 0.4532[0m
[32m[2022-09-05 12:55:32,483] [    INFO][0m - loss: 0.64126353, learning_rate: 9.758308157099698e-06, global_step: 160, interval_runtime: 1.8248, interval_samples_per_second: 4.384, interval_steps_per_second: 5.48, epoch: 0.4834[0m
[32m[2022-09-05 12:55:34,318] [    INFO][0m - loss: 0.73394322, learning_rate: 9.74320241691843e-06, global_step: 170, interval_runtime: 1.8346, interval_samples_per_second: 4.361, interval_steps_per_second: 5.451, epoch: 0.5136[0m
[32m[2022-09-05 12:55:36,147] [    INFO][0m - loss: 0.53645058, learning_rate: 9.728096676737161e-06, global_step: 180, interval_runtime: 1.829, interval_samples_per_second: 4.374, interval_steps_per_second: 5.468, epoch: 0.5438[0m
[32m[2022-09-05 12:55:37,976] [    INFO][0m - loss: 0.5889205, learning_rate: 9.712990936555891e-06, global_step: 190, interval_runtime: 1.829, interval_samples_per_second: 4.374, interval_steps_per_second: 5.468, epoch: 0.574[0m
[32m[2022-09-05 12:55:39,803] [    INFO][0m - loss: 0.62612782, learning_rate: 9.697885196374624e-06, global_step: 200, interval_runtime: 1.8269, interval_samples_per_second: 4.379, interval_steps_per_second: 5.474, epoch: 0.6042[0m
[32m[2022-09-05 12:55:39,803] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 12:55:39,803] [    INFO][0m -   Num examples = 2712[0m
[32m[2022-09-05 12:55:39,803] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 12:55:39,803] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 12:55:39,804] [    INFO][0m -   Total prediction steps = 339[0m
[32m[2022-09-05 12:56:23,673] [    INFO][0m - eval_loss: 1.064668893814087, eval_accuracy: 0.4918879056047198, eval_runtime: 43.8689, eval_samples_per_second: 61.821, eval_steps_per_second: 7.728, epoch: 0.6042[0m
[32m[2022-09-05 12:56:23,714] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-200[0m
[32m[2022-09-05 12:56:23,715] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 12:56:26,801] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-200/tokenizer_config.json[0m
[32m[2022-09-05 12:56:26,802] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-200/special_tokens_map.json[0m
[32m[2022-09-05 12:56:35,890] [    INFO][0m - loss: 0.61612358, learning_rate: 9.682779456193354e-06, global_step: 210, interval_runtime: 56.0873, interval_samples_per_second: 0.143, interval_steps_per_second: 0.178, epoch: 0.6344[0m
[32m[2022-09-05 12:56:37,719] [    INFO][0m - loss: 0.46097956, learning_rate: 9.667673716012085e-06, global_step: 220, interval_runtime: 1.8285, interval_samples_per_second: 4.375, interval_steps_per_second: 5.469, epoch: 0.6647[0m
[32m[2022-09-05 12:56:39,552] [    INFO][0m - loss: 0.54877357, learning_rate: 9.652567975830817e-06, global_step: 230, interval_runtime: 1.8339, interval_samples_per_second: 4.362, interval_steps_per_second: 5.453, epoch: 0.6949[0m
[32m[2022-09-05 12:56:41,381] [    INFO][0m - loss: 0.49765067, learning_rate: 9.637462235649548e-06, global_step: 240, interval_runtime: 1.8287, interval_samples_per_second: 4.375, interval_steps_per_second: 5.468, epoch: 0.7251[0m
[32m[2022-09-05 12:56:43,203] [    INFO][0m - loss: 0.38563743, learning_rate: 9.62235649546828e-06, global_step: 250, interval_runtime: 1.822, interval_samples_per_second: 4.391, interval_steps_per_second: 5.488, epoch: 0.7553[0m
[32m[2022-09-05 12:56:45,027] [    INFO][0m - loss: 0.37161715, learning_rate: 9.60725075528701e-06, global_step: 260, interval_runtime: 1.8232, interval_samples_per_second: 4.388, interval_steps_per_second: 5.485, epoch: 0.7855[0m
[32m[2022-09-05 12:56:46,855] [    INFO][0m - loss: 0.33609028, learning_rate: 9.59214501510574e-06, global_step: 270, interval_runtime: 1.8282, interval_samples_per_second: 4.376, interval_steps_per_second: 5.47, epoch: 0.8157[0m
[32m[2022-09-05 12:56:48,672] [    INFO][0m - loss: 0.2560483, learning_rate: 9.577039274924472e-06, global_step: 280, interval_runtime: 1.8175, interval_samples_per_second: 4.402, interval_steps_per_second: 5.502, epoch: 0.8459[0m
[32m[2022-09-05 12:56:50,498] [    INFO][0m - loss: 0.41540718, learning_rate: 9.561933534743203e-06, global_step: 290, interval_runtime: 1.8252, interval_samples_per_second: 4.383, interval_steps_per_second: 5.479, epoch: 0.8761[0m
[32m[2022-09-05 12:56:52,322] [    INFO][0m - loss: 0.7115962, learning_rate: 9.546827794561935e-06, global_step: 300, interval_runtime: 1.8249, interval_samples_per_second: 4.384, interval_steps_per_second: 5.48, epoch: 0.9063[0m
[32m[2022-09-05 12:56:52,323] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 12:56:52,323] [    INFO][0m -   Num examples = 2712[0m
[32m[2022-09-05 12:56:52,323] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 12:56:52,323] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 12:56:52,323] [    INFO][0m -   Total prediction steps = 339[0m
[32m[2022-09-05 12:57:36,190] [    INFO][0m - eval_loss: 1.6740679740905762, eval_accuracy: 0.47345132743362833, eval_runtime: 43.8658, eval_samples_per_second: 61.825, eval_steps_per_second: 7.728, epoch: 0.9063[0m
[32m[2022-09-05 12:57:36,243] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-300[0m
[32m[2022-09-05 12:57:36,243] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 12:57:40,102] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-300/tokenizer_config.json[0m
[32m[2022-09-05 12:57:40,102] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-300/special_tokens_map.json[0m
[32m[2022-09-05 12:57:47,162] [    INFO][0m - loss: 0.27307904, learning_rate: 9.531722054380666e-06, global_step: 310, interval_runtime: 54.84, interval_samples_per_second: 0.146, interval_steps_per_second: 0.182, epoch: 0.9366[0m
[32m[2022-09-05 12:57:48,989] [    INFO][0m - loss: 0.30757475, learning_rate: 9.516616314199396e-06, global_step: 320, interval_runtime: 1.8272, interval_samples_per_second: 4.378, interval_steps_per_second: 5.473, epoch: 0.9668[0m
[32m[2022-09-05 12:57:50,800] [    INFO][0m - loss: 0.13119555, learning_rate: 9.501510574018127e-06, global_step: 330, interval_runtime: 1.8102, interval_samples_per_second: 4.419, interval_steps_per_second: 5.524, epoch: 0.997[0m
[32m[2022-09-05 12:57:52,619] [    INFO][0m - loss: 0.29676778, learning_rate: 9.486404833836859e-06, global_step: 340, interval_runtime: 1.8193, interval_samples_per_second: 4.397, interval_steps_per_second: 5.497, epoch: 1.0272[0m
[32m[2022-09-05 12:57:54,446] [    INFO][0m - loss: 0.12739125, learning_rate: 9.47129909365559e-06, global_step: 350, interval_runtime: 1.8272, interval_samples_per_second: 4.378, interval_steps_per_second: 5.473, epoch: 1.0574[0m
[32m[2022-09-05 12:57:56,273] [    INFO][0m - loss: 0.22090542, learning_rate: 9.456193353474322e-06, global_step: 360, interval_runtime: 1.8266, interval_samples_per_second: 4.38, interval_steps_per_second: 5.475, epoch: 1.0876[0m
[32m[2022-09-05 12:57:58,103] [    INFO][0m - loss: 0.07925768, learning_rate: 9.441087613293051e-06, global_step: 370, interval_runtime: 1.8299, interval_samples_per_second: 4.372, interval_steps_per_second: 5.465, epoch: 1.1178[0m
[32m[2022-09-05 12:57:59,926] [    INFO][0m - loss: 0.2194998, learning_rate: 9.425981873111783e-06, global_step: 380, interval_runtime: 1.8233, interval_samples_per_second: 4.388, interval_steps_per_second: 5.485, epoch: 1.148[0m
[32m[2022-09-05 12:58:02,124] [    INFO][0m - loss: 0.06192513, learning_rate: 9.410876132930514e-06, global_step: 390, interval_runtime: 1.8422, interval_samples_per_second: 4.343, interval_steps_per_second: 5.428, epoch: 1.1782[0m
[32m[2022-09-05 12:58:03,959] [    INFO][0m - loss: 0.1904585, learning_rate: 9.395770392749246e-06, global_step: 400, interval_runtime: 2.1908, interval_samples_per_second: 3.652, interval_steps_per_second: 4.565, epoch: 1.2085[0m
[32m[2022-09-05 12:58:03,960] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 12:58:03,960] [    INFO][0m -   Num examples = 2712[0m
[32m[2022-09-05 12:58:03,960] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 12:58:03,960] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 12:58:03,960] [    INFO][0m -   Total prediction steps = 339[0m
[32m[2022-09-05 12:58:47,704] [    INFO][0m - eval_loss: 3.341078042984009, eval_accuracy: 0.45132743362831856, eval_runtime: 43.7435, eval_samples_per_second: 61.998, eval_steps_per_second: 7.75, epoch: 1.2085[0m
[32m[2022-09-05 12:58:47,763] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-400[0m
[32m[2022-09-05 12:58:47,763] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 12:58:51,652] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-400/tokenizer_config.json[0m
[32m[2022-09-05 12:58:51,652] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-400/special_tokens_map.json[0m
[32m[2022-09-05 12:58:58,799] [    INFO][0m - loss: 0.08503172, learning_rate: 9.380664652567977e-06, global_step: 410, interval_runtime: 54.8401, interval_samples_per_second: 0.146, interval_steps_per_second: 0.182, epoch: 1.2387[0m
[32m[2022-09-05 12:59:00,625] [    INFO][0m - loss: 0.13983811, learning_rate: 9.365558912386707e-06, global_step: 420, interval_runtime: 1.8259, interval_samples_per_second: 4.381, interval_steps_per_second: 5.477, epoch: 1.2689[0m
[32m[2022-09-05 12:59:02,449] [    INFO][0m - loss: 0.11529305, learning_rate: 9.35045317220544e-06, global_step: 430, interval_runtime: 1.8234, interval_samples_per_second: 4.387, interval_steps_per_second: 5.484, epoch: 1.2991[0m
[32m[2022-09-05 12:59:04,270] [    INFO][0m - loss: 0.1145475, learning_rate: 9.33534743202417e-06, global_step: 440, interval_runtime: 1.8217, interval_samples_per_second: 4.391, interval_steps_per_second: 5.489, epoch: 1.3293[0m
[32m[2022-09-05 12:59:06,099] [    INFO][0m - loss: 0.02449254, learning_rate: 9.320241691842901e-06, global_step: 450, interval_runtime: 1.829, interval_samples_per_second: 4.374, interval_steps_per_second: 5.468, epoch: 1.3595[0m
[32m[2022-09-05 12:59:07,926] [    INFO][0m - loss: 0.23014069, learning_rate: 9.305135951661632e-06, global_step: 460, interval_runtime: 1.8265, interval_samples_per_second: 4.38, interval_steps_per_second: 5.475, epoch: 1.3897[0m
[32m[2022-09-05 12:59:09,753] [    INFO][0m - loss: 0.07190692, learning_rate: 9.290030211480364e-06, global_step: 470, interval_runtime: 1.8275, interval_samples_per_second: 4.378, interval_steps_per_second: 5.472, epoch: 1.4199[0m
[32m[2022-09-05 12:59:11,579] [    INFO][0m - loss: 0.15139648, learning_rate: 9.274924471299095e-06, global_step: 480, interval_runtime: 1.8259, interval_samples_per_second: 4.381, interval_steps_per_second: 5.477, epoch: 1.4502[0m
[32m[2022-09-05 12:59:13,416] [    INFO][0m - loss: 0.08966846, learning_rate: 9.259818731117825e-06, global_step: 490, interval_runtime: 1.8369, interval_samples_per_second: 4.355, interval_steps_per_second: 5.444, epoch: 1.4804[0m
[32m[2022-09-05 12:59:15,248] [    INFO][0m - loss: 0.02216162, learning_rate: 9.244712990936556e-06, global_step: 500, interval_runtime: 1.8316, interval_samples_per_second: 4.368, interval_steps_per_second: 5.46, epoch: 1.5106[0m
[32m[2022-09-05 12:59:15,248] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 12:59:15,248] [    INFO][0m -   Num examples = 2712[0m
[32m[2022-09-05 12:59:15,248] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 12:59:15,248] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 12:59:15,249] [    INFO][0m -   Total prediction steps = 339[0m
[32m[2022-09-05 12:59:59,120] [    INFO][0m - eval_loss: 4.354831218719482, eval_accuracy: 0.4575958702064897, eval_runtime: 43.8713, eval_samples_per_second: 61.817, eval_steps_per_second: 7.727, epoch: 1.5106[0m
[32m[2022-09-05 12:59:59,174] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-500[0m
[32m[2022-09-05 12:59:59,174] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 13:00:03,795] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-500/tokenizer_config.json[0m
[32m[2022-09-05 13:00:03,796] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-500/special_tokens_map.json[0m
[32m[2022-09-05 13:00:11,167] [    INFO][0m - loss: 0.06223025, learning_rate: 9.229607250755288e-06, global_step: 510, interval_runtime: 55.9192, interval_samples_per_second: 0.143, interval_steps_per_second: 0.179, epoch: 1.5408[0m
[32m[2022-09-05 13:00:12,989] [    INFO][0m - loss: 0.0068071, learning_rate: 9.21450151057402e-06, global_step: 520, interval_runtime: 1.8213, interval_samples_per_second: 4.392, interval_steps_per_second: 5.49, epoch: 1.571[0m
[32m[2022-09-05 13:00:14,824] [    INFO][0m - loss: 0.09422733, learning_rate: 9.199395770392749e-06, global_step: 530, interval_runtime: 1.8355, interval_samples_per_second: 4.358, interval_steps_per_second: 5.448, epoch: 1.6012[0m
[32m[2022-09-05 13:00:16,654] [    INFO][0m - loss: 0.10887334, learning_rate: 9.184290030211482e-06, global_step: 540, interval_runtime: 1.8301, interval_samples_per_second: 4.371, interval_steps_per_second: 5.464, epoch: 1.6314[0m
[32m[2022-09-05 13:00:18,481] [    INFO][0m - loss: 0.04486, learning_rate: 9.169184290030212e-06, global_step: 550, interval_runtime: 1.827, interval_samples_per_second: 4.379, interval_steps_per_second: 5.473, epoch: 1.6616[0m
[32m[2022-09-05 13:00:20,304] [    INFO][0m - loss: 0.0351502, learning_rate: 9.154078549848943e-06, global_step: 560, interval_runtime: 1.8227, interval_samples_per_second: 4.389, interval_steps_per_second: 5.486, epoch: 1.6918[0m
[32m[2022-09-05 13:00:22,137] [    INFO][0m - loss: 0.00208266, learning_rate: 9.138972809667675e-06, global_step: 570, interval_runtime: 1.8336, interval_samples_per_second: 4.363, interval_steps_per_second: 5.454, epoch: 1.7221[0m
[32m[2022-09-05 13:00:23,972] [    INFO][0m - loss: 0.01314839, learning_rate: 9.123867069486404e-06, global_step: 580, interval_runtime: 1.8351, interval_samples_per_second: 4.359, interval_steps_per_second: 5.449, epoch: 1.7523[0m
[32m[2022-09-05 13:00:25,805] [    INFO][0m - loss: 0.00669667, learning_rate: 9.108761329305137e-06, global_step: 590, interval_runtime: 1.833, interval_samples_per_second: 4.364, interval_steps_per_second: 5.455, epoch: 1.7825[0m
[32m[2022-09-05 13:00:27,629] [    INFO][0m - loss: 0.05779094, learning_rate: 9.093655589123867e-06, global_step: 600, interval_runtime: 1.8232, interval_samples_per_second: 4.388, interval_steps_per_second: 5.485, epoch: 1.8127[0m
[32m[2022-09-05 13:00:27,629] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 13:00:27,629] [    INFO][0m -   Num examples = 2712[0m
[32m[2022-09-05 13:00:27,629] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 13:00:27,629] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 13:00:27,629] [    INFO][0m -   Total prediction steps = 339[0m
[32m[2022-09-05 13:01:11,423] [    INFO][0m - eval_loss: 5.184395790100098, eval_accuracy: 0.4693952802359882, eval_runtime: 43.7927, eval_samples_per_second: 61.928, eval_steps_per_second: 7.741, epoch: 1.8127[0m
[32m[2022-09-05 13:01:11,474] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-600[0m
[32m[2022-09-05 13:01:11,475] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 13:01:19,199] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-600/tokenizer_config.json[0m
[32m[2022-09-05 13:01:20,609] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-600/special_tokens_map.json[0m
[32m[2022-09-05 13:01:26,751] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-05 13:01:26,752] [    INFO][0m - Loading best model from ./checkpoints_csl/checkpoint-200 (score: 0.4918879056047198).[0m
[32m[2022-09-05 13:01:27,825] [    INFO][0m - train_runtime: 438.5476, train_samples_per_second: 120.489, train_steps_per_second: 15.095, train_loss: 0.4302869196422398, epoch: 1.8127[0m
[32m[2022-09-05 13:01:27,826] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/[0m
[32m[2022-09-05 13:01:27,826] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 13:01:33,263] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/tokenizer_config.json[0m
[32m[2022-09-05 13:01:33,769] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/special_tokens_map.json[0m
[32m[2022-09-05 13:01:33,770] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-05 13:01:33,771] [    INFO][0m -   epoch                    =     1.8127[0m
[32m[2022-09-05 13:01:33,771] [    INFO][0m -   train_loss               =     0.4303[0m
[32m[2022-09-05 13:01:33,771] [    INFO][0m -   train_runtime            = 0:07:18.54[0m
[32m[2022-09-05 13:01:33,771] [    INFO][0m -   train_samples_per_second =    120.489[0m
[32m[2022-09-05 13:01:33,771] [    INFO][0m -   train_steps_per_second   =     15.095[0m
[32m[2022-09-05 13:01:33,776] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-05 13:01:33,777] [    INFO][0m -   Num examples = 49340[0m
[32m[2022-09-05 13:01:33,777] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 13:01:33,777] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 13:01:33,777] [    INFO][0m -   Total prediction steps = 6168[0m
[32m[2022-09-05 13:27:44,907] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-05 13:27:44,907] [    INFO][0m -   test_accuracy           =     0.4803[0m
[32m[2022-09-05 13:27:44,907] [    INFO][0m -   test_loss               =     1.0234[0m
[32m[2022-09-05 13:27:44,907] [    INFO][0m -   test_runtime            = 0:26:11.13[0m
[32m[2022-09-05 13:27:44,908] [    INFO][0m -   test_samples_per_second =     31.404[0m
[32m[2022-09-05 13:27:44,908] [    INFO][0m -   test_steps_per_second   =      3.926[0m
[33m[2022-09-05 13:29:58,237] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-09-05 13:29:58,238] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-05 13:29:58,238] [    INFO][0m - ============================================================[0m
[32m[2022-09-05 13:29:58,238] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-05 13:29:58,238] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-05 13:29:58,238] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-05 13:29:58,238] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-05 13:29:58,238] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-05 13:29:58,239] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-05 13:29:58,239] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-09-05 13:29:58,239] [    INFO][0m - [0m
[32m[2022-09-05 13:29:58,239] [    INFO][0m - ============================================================[0m
[32m[2022-09-05 13:29:58,239] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-05 13:29:58,239] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-05 13:29:58,239] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-05 13:29:58,239] [    INFO][0m - prompt                        :[0m
[32m[2022-09-05 13:29:58,239] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-05 13:29:58,239] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-05 13:29:58,239] [    INFO][0m - task_name                     :csl[0m
[32m[2022-09-05 13:29:58,239] [    INFO][0m - [0m
[32m[2022-09-05 13:29:58,240] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
[32m[2022-09-05 13:29:59,684] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-09-05 13:29:59,707] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-09-05 13:29:59,707] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[32m[2022-09-05 13:29:59,708] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùËøôÂè•ËØù‰∏≠ËÆ®ËÆ∫ÁöÑÂÖ≥ÈîÆËØç'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'ÂåÖÊã¨‚Äú'}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': '‚Äù'}][0m
2022-09-05 13:29:59,710 INFO [download.py:119] unique_endpoints {''}
[32m[2022-09-05 13:30:00,071] [    INFO][0m - ============================================================[0m
[32m[2022-09-05 13:30:00,071] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-05 13:30:00,071] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-05 13:30:00,071] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-05 13:30:00,072] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-05 13:30:00,072] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-05 13:30:00,072] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-05 13:30:00,072] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-05 13:30:00,072] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-05 13:30:00,072] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-05 13:30:00,072] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-05 13:30:00,072] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-05 13:30:00,072] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-05 13:30:00,072] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-05 13:30:00,072] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-05 13:30:00,072] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-05 13:30:00,072] [    INFO][0m - do_predict                    :False[0m
[32m[2022-09-05 13:30:00,073] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-05 13:30:00,073] [    INFO][0m - eval_batch_size               :8[0m
[32m[2022-09-05 13:30:00,073] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-09-05 13:30:00,073] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-09-05 13:30:00,073] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-05 13:30:00,073] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-05 13:30:00,073] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-05 13:30:00,073] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-05 13:30:00,073] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-05 13:30:00,073] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-05 13:30:00,073] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-05 13:30:00,073] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-05 13:30:00,073] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-05 13:30:00,074] [    INFO][0m - learning_rate                 :1e-05[0m
[32m[2022-09-05 13:30:00,074] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-05 13:30:00,074] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-05 13:30:00,074] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-05 13:30:00,074] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-05 13:30:00,074] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-05 13:30:00,074] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-05 13:30:00,074] [    INFO][0m - logging_dir                   :./checkpoints_csl/runs/Sep05_13-29-58_instance-3bwob41y-01[0m
[32m[2022-09-05 13:30:00,074] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-05 13:30:00,074] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-05 13:30:00,074] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-05 13:30:00,074] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-05 13:30:00,075] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-05 13:30:00,075] [    INFO][0m - max_seq_length                :320[0m
[32m[2022-09-05 13:30:00,075] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-05 13:30:00,075] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-05 13:30:00,075] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-05 13:30:00,075] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-05 13:30:00,075] [    INFO][0m - num_train_epochs              :20.0[0m
[32m[2022-09-05 13:30:00,075] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-05 13:30:00,075] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-05 13:30:00,075] [    INFO][0m - output_dir                    :./checkpoints_csl/[0m
[32m[2022-09-05 13:30:00,075] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-05 13:30:00,075] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-05 13:30:00,075] [    INFO][0m - per_device_eval_batch_size    :8[0m
[32m[2022-09-05 13:30:00,076] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-09-05 13:30:00,076] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-05 13:30:00,076] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-05 13:30:00,076] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-05 13:30:00,076] [    INFO][0m - ppt_learning_rate             :1e-05[0m
[32m[2022-09-05 13:30:00,076] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-05 13:30:00,076] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-05 13:30:00,076] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-05 13:30:00,076] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-05 13:30:00,076] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-05 13:30:00,076] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-05 13:30:00,076] [    INFO][0m - run_name                      :./checkpoints_csl/[0m
[32m[2022-09-05 13:30:00,076] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-05 13:30:00,077] [    INFO][0m - save_steps                    :100[0m
[32m[2022-09-05 13:30:00,077] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-09-05 13:30:00,077] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-05 13:30:00,077] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-05 13:30:00,077] [    INFO][0m - seed                          :42[0m
[32m[2022-09-05 13:30:00,077] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-05 13:30:00,077] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-05 13:30:00,077] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-05 13:30:00,077] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-09-05 13:30:00,077] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-05 13:30:00,077] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-05 13:30:00,077] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-05 13:30:00,077] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-05 13:30:00,078] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-05 13:30:00,078] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-05 13:30:00,078] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-05 13:30:00,078] [    INFO][0m - [0m
[32m[2022-09-05 13:30:00,079] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-05 13:30:00,079] [    INFO][0m -   Num examples = 2642[0m
[32m[2022-09-05 13:30:00,079] [    INFO][0m -   Num Epochs = 20[0m
[32m[2022-09-05 13:30:00,079] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-09-05 13:30:00,080] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-09-05 13:30:00,080] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-05 13:30:00,080] [    INFO][0m -   Total optimization steps = 6620.0[0m
[32m[2022-09-05 13:30:00,080] [    INFO][0m -   Total num train samples = 52840[0m
[32m[2022-09-05 13:30:02,032] [    INFO][0m - loss: 3.30301361, learning_rate: 9.984894259818732e-06, global_step: 10, interval_runtime: 1.951, interval_samples_per_second: 4.1, interval_steps_per_second: 5.126, epoch: 0.0302[0m
[32m[2022-09-05 13:30:03,848] [    INFO][0m - loss: 1.31430702, learning_rate: 9.969788519637464e-06, global_step: 20, interval_runtime: 1.8167, interval_samples_per_second: 4.403, interval_steps_per_second: 5.504, epoch: 0.0604[0m
[32m[2022-09-05 13:30:05,659] [    INFO][0m - loss: 0.77689829, learning_rate: 9.954682779456193e-06, global_step: 30, interval_runtime: 1.8107, interval_samples_per_second: 4.418, interval_steps_per_second: 5.523, epoch: 0.0906[0m
[32m[2022-09-05 13:30:07,467] [    INFO][0m - loss: 0.85782309, learning_rate: 9.939577039274926e-06, global_step: 40, interval_runtime: 1.8082, interval_samples_per_second: 4.424, interval_steps_per_second: 5.53, epoch: 0.1208[0m
[32m[2022-09-05 13:30:09,281] [    INFO][0m - loss: 0.81323166, learning_rate: 9.924471299093656e-06, global_step: 50, interval_runtime: 1.8139, interval_samples_per_second: 4.41, interval_steps_per_second: 5.513, epoch: 0.1511[0m
[32m[2022-09-05 13:30:11,097] [    INFO][0m - loss: 0.78385019, learning_rate: 9.909365558912388e-06, global_step: 60, interval_runtime: 1.816, interval_samples_per_second: 4.405, interval_steps_per_second: 5.507, epoch: 0.1813[0m
[32m[2022-09-05 13:30:12,911] [    INFO][0m - loss: 0.65217824, learning_rate: 9.894259818731119e-06, global_step: 70, interval_runtime: 1.8138, interval_samples_per_second: 4.411, interval_steps_per_second: 5.513, epoch: 0.2115[0m
[32m[2022-09-05 13:30:14,722] [    INFO][0m - loss: 0.66706495, learning_rate: 9.87915407854985e-06, global_step: 80, interval_runtime: 1.8108, interval_samples_per_second: 4.418, interval_steps_per_second: 5.522, epoch: 0.2417[0m
[32m[2022-09-05 13:30:16,539] [    INFO][0m - loss: 0.73739185, learning_rate: 9.86404833836858e-06, global_step: 90, interval_runtime: 1.8171, interval_samples_per_second: 4.403, interval_steps_per_second: 5.503, epoch: 0.2719[0m
[32m[2022-09-05 13:30:18,361] [    INFO][0m - loss: 0.74174557, learning_rate: 9.848942598187312e-06, global_step: 100, interval_runtime: 1.8222, interval_samples_per_second: 4.39, interval_steps_per_second: 5.488, epoch: 0.3021[0m
[32m[2022-09-05 13:30:18,362] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 13:30:18,362] [    INFO][0m -   Num examples = 2712[0m
[32m[2022-09-05 13:30:18,362] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 13:30:18,362] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 13:30:18,362] [    INFO][0m -   Total prediction steps = 339[0m
[32m[2022-09-05 13:31:01,703] [    INFO][0m - eval_loss: 0.8153711557388306, eval_accuracy: 0.47123893805309736, eval_runtime: 43.3411, eval_samples_per_second: 62.573, eval_steps_per_second: 7.822, epoch: 0.3021[0m
[32m[2022-09-05 13:31:01,739] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-100[0m
[32m[2022-09-05 13:31:01,739] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 13:31:06,592] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-05 13:31:06,593] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-05 13:31:16,334] [    INFO][0m - loss: 0.62268472, learning_rate: 9.833836858006043e-06, global_step: 110, interval_runtime: 57.9728, interval_samples_per_second: 0.138, interval_steps_per_second: 0.172, epoch: 0.3323[0m
[32m[2022-09-05 13:31:18,153] [    INFO][0m - loss: 0.83133574, learning_rate: 9.818731117824774e-06, global_step: 120, interval_runtime: 1.8185, interval_samples_per_second: 4.399, interval_steps_per_second: 5.499, epoch: 0.3625[0m
[32m[2022-09-05 13:31:19,962] [    INFO][0m - loss: 0.6017211, learning_rate: 9.803625377643506e-06, global_step: 130, interval_runtime: 1.8096, interval_samples_per_second: 4.421, interval_steps_per_second: 5.526, epoch: 0.3927[0m
[32m[2022-09-05 13:31:21,781] [    INFO][0m - loss: 0.81961498, learning_rate: 9.788519637462236e-06, global_step: 140, interval_runtime: 1.8195, interval_samples_per_second: 4.397, interval_steps_per_second: 5.496, epoch: 0.423[0m
[32m[2022-09-05 13:31:23,609] [    INFO][0m - loss: 0.68166461, learning_rate: 9.773413897280967e-06, global_step: 150, interval_runtime: 1.8269, interval_samples_per_second: 4.379, interval_steps_per_second: 5.474, epoch: 0.4532[0m
[32m[2022-09-05 13:31:25,418] [    INFO][0m - loss: 0.5553978, learning_rate: 9.758308157099698e-06, global_step: 160, interval_runtime: 1.8101, interval_samples_per_second: 4.42, interval_steps_per_second: 5.525, epoch: 0.4834[0m
[32m[2022-09-05 13:31:27,234] [    INFO][0m - loss: 0.63674307, learning_rate: 9.74320241691843e-06, global_step: 170, interval_runtime: 1.8155, interval_samples_per_second: 4.407, interval_steps_per_second: 5.508, epoch: 0.5136[0m
[32m[2022-09-05 13:31:29,057] [    INFO][0m - loss: 0.57571054, learning_rate: 9.728096676737161e-06, global_step: 180, interval_runtime: 1.8232, interval_samples_per_second: 4.388, interval_steps_per_second: 5.485, epoch: 0.5438[0m
[32m[2022-09-05 13:31:30,879] [    INFO][0m - loss: 0.57365999, learning_rate: 9.712990936555891e-06, global_step: 190, interval_runtime: 1.8216, interval_samples_per_second: 4.392, interval_steps_per_second: 5.49, epoch: 0.574[0m
[32m[2022-09-05 13:31:32,700] [    INFO][0m - loss: 0.58594527, learning_rate: 9.697885196374624e-06, global_step: 200, interval_runtime: 1.821, interval_samples_per_second: 4.393, interval_steps_per_second: 5.491, epoch: 0.6042[0m
[32m[2022-09-05 13:31:32,700] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 13:31:32,700] [    INFO][0m -   Num examples = 2712[0m
[32m[2022-09-05 13:31:32,700] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 13:31:32,701] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 13:31:32,701] [    INFO][0m -   Total prediction steps = 339[0m
[32m[2022-09-05 13:32:16,277] [    INFO][0m - eval_loss: 1.0546505451202393, eval_accuracy: 0.4808259587020649, eval_runtime: 43.5761, eval_samples_per_second: 62.236, eval_steps_per_second: 7.779, epoch: 0.6042[0m
[32m[2022-09-05 13:32:16,334] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-200[0m
[32m[2022-09-05 13:32:16,335] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 13:32:20,406] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-200/tokenizer_config.json[0m
[32m[2022-09-05 13:32:20,709] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-200/special_tokens_map.json[0m
[32m[2022-09-05 13:32:29,737] [    INFO][0m - loss: 0.56232395, learning_rate: 9.682779456193354e-06, global_step: 210, interval_runtime: 57.037, interval_samples_per_second: 0.14, interval_steps_per_second: 0.175, epoch: 0.6344[0m
[32m[2022-09-05 13:32:31,580] [    INFO][0m - loss: 0.45086536, learning_rate: 9.667673716012085e-06, global_step: 220, interval_runtime: 1.843, interval_samples_per_second: 4.341, interval_steps_per_second: 5.426, epoch: 0.6647[0m
[32m[2022-09-05 13:32:33,406] [    INFO][0m - loss: 0.50233622, learning_rate: 9.652567975830817e-06, global_step: 230, interval_runtime: 1.8259, interval_samples_per_second: 4.381, interval_steps_per_second: 5.477, epoch: 0.6949[0m
[32m[2022-09-05 13:32:35,223] [    INFO][0m - loss: 0.46808243, learning_rate: 9.637462235649548e-06, global_step: 240, interval_runtime: 1.8174, interval_samples_per_second: 4.402, interval_steps_per_second: 5.502, epoch: 0.7251[0m
[32m[2022-09-05 13:32:37,042] [    INFO][0m - loss: 0.48208351, learning_rate: 9.62235649546828e-06, global_step: 250, interval_runtime: 1.8186, interval_samples_per_second: 4.399, interval_steps_per_second: 5.499, epoch: 0.7553[0m
[32m[2022-09-05 13:32:38,865] [    INFO][0m - loss: 0.35067852, learning_rate: 9.60725075528701e-06, global_step: 260, interval_runtime: 1.8234, interval_samples_per_second: 4.387, interval_steps_per_second: 5.484, epoch: 0.7855[0m
[32m[2022-09-05 13:32:40,693] [    INFO][0m - loss: 0.35345817, learning_rate: 9.59214501510574e-06, global_step: 270, interval_runtime: 1.8279, interval_samples_per_second: 4.377, interval_steps_per_second: 5.471, epoch: 0.8157[0m
[32m[2022-09-05 13:32:42,509] [    INFO][0m - loss: 0.23489733, learning_rate: 9.577039274924472e-06, global_step: 280, interval_runtime: 1.8156, interval_samples_per_second: 4.406, interval_steps_per_second: 5.508, epoch: 0.8459[0m
[32m[2022-09-05 13:32:44,346] [    INFO][0m - loss: 0.28402801, learning_rate: 9.561933534743203e-06, global_step: 290, interval_runtime: 1.8368, interval_samples_per_second: 4.355, interval_steps_per_second: 5.444, epoch: 0.8761[0m
[32m[2022-09-05 13:32:46,165] [    INFO][0m - loss: 0.53139181, learning_rate: 9.546827794561935e-06, global_step: 300, interval_runtime: 1.8192, interval_samples_per_second: 4.397, interval_steps_per_second: 5.497, epoch: 0.9063[0m
[32m[2022-09-05 13:32:46,166] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 13:32:46,166] [    INFO][0m -   Num examples = 2712[0m
[32m[2022-09-05 13:32:46,166] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 13:32:46,166] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 13:32:46,166] [    INFO][0m -   Total prediction steps = 339[0m
[32m[2022-09-05 13:33:29,636] [    INFO][0m - eval_loss: 1.7462091445922852, eval_accuracy: 0.5383480825958702, eval_runtime: 43.4697, eval_samples_per_second: 62.388, eval_steps_per_second: 7.799, epoch: 0.9063[0m
[32m[2022-09-05 13:33:29,678] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-300[0m
[32m[2022-09-05 13:33:29,679] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 13:33:33,115] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-300/tokenizer_config.json[0m
[32m[2022-09-05 13:33:33,115] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-300/special_tokens_map.json[0m
[32m[2022-09-05 13:33:41,555] [    INFO][0m - loss: 0.24375906, learning_rate: 9.531722054380666e-06, global_step: 310, interval_runtime: 55.3896, interval_samples_per_second: 0.144, interval_steps_per_second: 0.181, epoch: 0.9366[0m
[32m[2022-09-05 13:33:43,375] [    INFO][0m - loss: 0.29824498, learning_rate: 9.516616314199396e-06, global_step: 320, interval_runtime: 1.8206, interval_samples_per_second: 4.394, interval_steps_per_second: 5.493, epoch: 0.9668[0m
[32m[2022-09-05 13:33:45,187] [    INFO][0m - loss: 0.23021061, learning_rate: 9.501510574018127e-06, global_step: 330, interval_runtime: 1.8125, interval_samples_per_second: 4.414, interval_steps_per_second: 5.517, epoch: 0.997[0m
[32m[2022-09-05 13:33:47,004] [    INFO][0m - loss: 0.24948428, learning_rate: 9.486404833836859e-06, global_step: 340, interval_runtime: 1.8158, interval_samples_per_second: 4.406, interval_steps_per_second: 5.507, epoch: 1.0272[0m
[32m[2022-09-05 13:33:48,833] [    INFO][0m - loss: 0.09716727, learning_rate: 9.47129909365559e-06, global_step: 350, interval_runtime: 1.83, interval_samples_per_second: 4.372, interval_steps_per_second: 5.464, epoch: 1.0574[0m
[32m[2022-09-05 13:33:50,648] [    INFO][0m - loss: 0.3084811, learning_rate: 9.456193353474322e-06, global_step: 360, interval_runtime: 1.815, interval_samples_per_second: 4.408, interval_steps_per_second: 5.51, epoch: 1.0876[0m
[32m[2022-09-05 13:33:52,473] [    INFO][0m - loss: 0.13748378, learning_rate: 9.441087613293051e-06, global_step: 370, interval_runtime: 1.8252, interval_samples_per_second: 4.383, interval_steps_per_second: 5.479, epoch: 1.1178[0m
[32m[2022-09-05 13:33:54,293] [    INFO][0m - loss: 0.31206686, learning_rate: 9.425981873111783e-06, global_step: 380, interval_runtime: 1.8194, interval_samples_per_second: 4.397, interval_steps_per_second: 5.496, epoch: 1.148[0m
[32m[2022-09-05 13:33:56,127] [    INFO][0m - loss: 0.14326246, learning_rate: 9.410876132930514e-06, global_step: 390, interval_runtime: 1.8339, interval_samples_per_second: 4.362, interval_steps_per_second: 5.453, epoch: 1.1782[0m
[32m[2022-09-05 13:33:57,954] [    INFO][0m - loss: 0.01995352, learning_rate: 9.395770392749246e-06, global_step: 400, interval_runtime: 1.8271, interval_samples_per_second: 4.379, interval_steps_per_second: 5.473, epoch: 1.2085[0m
[32m[2022-09-05 13:33:57,954] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 13:33:57,954] [    INFO][0m -   Num examples = 2712[0m
[32m[2022-09-05 13:33:57,955] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 13:33:57,955] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 13:33:57,955] [    INFO][0m -   Total prediction steps = 339[0m
[32m[2022-09-05 13:34:41,746] [    INFO][0m - eval_loss: 5.519614219665527, eval_accuracy: 0.45685840707964603, eval_runtime: 43.7911, eval_samples_per_second: 61.93, eval_steps_per_second: 7.741, epoch: 1.2085[0m
[32m[2022-09-05 13:34:41,795] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-400[0m
[32m[2022-09-05 13:34:41,796] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 13:34:45,224] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-400/tokenizer_config.json[0m
[32m[2022-09-05 13:34:45,224] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-400/special_tokens_map.json[0m
[32m[2022-09-05 13:34:53,261] [    INFO][0m - loss: 0.12258261, learning_rate: 9.380664652567977e-06, global_step: 410, interval_runtime: 55.3069, interval_samples_per_second: 0.145, interval_steps_per_second: 0.181, epoch: 1.2387[0m
[32m[2022-09-05 13:34:55,079] [    INFO][0m - loss: 0.06558491, learning_rate: 9.365558912386707e-06, global_step: 420, interval_runtime: 1.8181, interval_samples_per_second: 4.4, interval_steps_per_second: 5.5, epoch: 1.2689[0m
[32m[2022-09-05 13:34:56,899] [    INFO][0m - loss: 0.20559132, learning_rate: 9.35045317220544e-06, global_step: 430, interval_runtime: 1.8198, interval_samples_per_second: 4.396, interval_steps_per_second: 5.495, epoch: 1.2991[0m
[32m[2022-09-05 13:34:58,715] [    INFO][0m - loss: 0.23699844, learning_rate: 9.33534743202417e-06, global_step: 440, interval_runtime: 1.8156, interval_samples_per_second: 4.406, interval_steps_per_second: 5.508, epoch: 1.3293[0m
[32m[2022-09-05 13:35:00,529] [    INFO][0m - loss: 0.06887318, learning_rate: 9.320241691842901e-06, global_step: 450, interval_runtime: 1.815, interval_samples_per_second: 4.408, interval_steps_per_second: 5.51, epoch: 1.3595[0m
[32m[2022-09-05 13:35:02,358] [    INFO][0m - loss: 0.06526613, learning_rate: 9.305135951661632e-06, global_step: 460, interval_runtime: 1.8287, interval_samples_per_second: 4.375, interval_steps_per_second: 5.468, epoch: 1.3897[0m
[32m[2022-09-05 13:35:04,181] [    INFO][0m - loss: 0.05573705, learning_rate: 9.290030211480364e-06, global_step: 470, interval_runtime: 1.8231, interval_samples_per_second: 4.388, interval_steps_per_second: 5.485, epoch: 1.4199[0m
[32m[2022-09-05 13:35:05,998] [    INFO][0m - loss: 0.02397054, learning_rate: 9.274924471299095e-06, global_step: 480, interval_runtime: 1.8171, interval_samples_per_second: 4.403, interval_steps_per_second: 5.503, epoch: 1.4502[0m
[32m[2022-09-05 13:35:07,821] [    INFO][0m - loss: 0.07839879, learning_rate: 9.259818731117825e-06, global_step: 490, interval_runtime: 1.8228, interval_samples_per_second: 4.389, interval_steps_per_second: 5.486, epoch: 1.4804[0m
[32m[2022-09-05 13:35:09,648] [    INFO][0m - loss: 0.13533024, learning_rate: 9.244712990936556e-06, global_step: 500, interval_runtime: 1.8274, interval_samples_per_second: 4.378, interval_steps_per_second: 5.472, epoch: 1.5106[0m
[32m[2022-09-05 13:35:09,649] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 13:35:09,649] [    INFO][0m -   Num examples = 2712[0m
[32m[2022-09-05 13:35:09,649] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 13:35:09,649] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 13:35:09,649] [    INFO][0m -   Total prediction steps = 339[0m
[32m[2022-09-05 13:35:53,234] [    INFO][0m - eval_loss: 5.907499313354492, eval_accuracy: 0.46755162241887904, eval_runtime: 43.5846, eval_samples_per_second: 62.224, eval_steps_per_second: 7.778, epoch: 1.5106[0m
[32m[2022-09-05 13:35:53,286] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-500[0m
[32m[2022-09-05 13:35:53,286] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 13:35:57,704] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-500/tokenizer_config.json[0m
[32m[2022-09-05 13:35:57,705] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-500/special_tokens_map.json[0m
[32m[2022-09-05 13:36:05,560] [    INFO][0m - loss: 0.03540981, learning_rate: 9.229607250755288e-06, global_step: 510, interval_runtime: 55.9118, interval_samples_per_second: 0.143, interval_steps_per_second: 0.179, epoch: 1.5408[0m
[32m[2022-09-05 13:36:07,378] [    INFO][0m - loss: 0.15367962, learning_rate: 9.21450151057402e-06, global_step: 520, interval_runtime: 1.8175, interval_samples_per_second: 4.402, interval_steps_per_second: 5.502, epoch: 1.571[0m
[32m[2022-09-05 13:36:09,196] [    INFO][0m - loss: 0.01311996, learning_rate: 9.199395770392749e-06, global_step: 530, interval_runtime: 1.8182, interval_samples_per_second: 4.4, interval_steps_per_second: 5.5, epoch: 1.6012[0m
[32m[2022-09-05 13:36:11,020] [    INFO][0m - loss: 0.13203443, learning_rate: 9.184290030211482e-06, global_step: 540, interval_runtime: 1.8246, interval_samples_per_second: 4.384, interval_steps_per_second: 5.481, epoch: 1.6314[0m
[32m[2022-09-05 13:36:12,848] [    INFO][0m - loss: 0.13775343, learning_rate: 9.169184290030212e-06, global_step: 550, interval_runtime: 1.827, interval_samples_per_second: 4.379, interval_steps_per_second: 5.474, epoch: 1.6616[0m
[32m[2022-09-05 13:36:14,668] [    INFO][0m - loss: 0.09079726, learning_rate: 9.154078549848943e-06, global_step: 560, interval_runtime: 1.8206, interval_samples_per_second: 4.394, interval_steps_per_second: 5.493, epoch: 1.6918[0m
[32m[2022-09-05 13:36:16,490] [    INFO][0m - loss: 0.05431374, learning_rate: 9.138972809667675e-06, global_step: 570, interval_runtime: 1.822, interval_samples_per_second: 4.391, interval_steps_per_second: 5.488, epoch: 1.7221[0m
[32m[2022-09-05 13:36:18,309] [    INFO][0m - loss: 0.03872927, learning_rate: 9.123867069486404e-06, global_step: 580, interval_runtime: 1.8182, interval_samples_per_second: 4.4, interval_steps_per_second: 5.5, epoch: 1.7523[0m
[32m[2022-09-05 13:36:20,133] [    INFO][0m - loss: 0.10288148, learning_rate: 9.108761329305137e-06, global_step: 590, interval_runtime: 1.8241, interval_samples_per_second: 4.386, interval_steps_per_second: 5.482, epoch: 1.7825[0m
[32m[2022-09-05 13:36:21,956] [    INFO][0m - loss: 0.05948756, learning_rate: 9.093655589123867e-06, global_step: 600, interval_runtime: 1.823, interval_samples_per_second: 4.388, interval_steps_per_second: 5.485, epoch: 1.8127[0m
[32m[2022-09-05 13:36:21,956] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 13:36:21,956] [    INFO][0m -   Num examples = 2712[0m
[32m[2022-09-05 13:36:21,956] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 13:36:21,956] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 13:36:21,956] [    INFO][0m -   Total prediction steps = 339[0m
[32m[2022-09-05 13:37:05,702] [    INFO][0m - eval_loss: 6.4758453369140625, eval_accuracy: 0.46865781710914456, eval_runtime: 43.7445, eval_samples_per_second: 61.996, eval_steps_per_second: 7.75, epoch: 1.8127[0m
[32m[2022-09-05 13:37:05,740] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-600[0m
[32m[2022-09-05 13:37:05,740] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 13:37:08,808] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-600/tokenizer_config.json[0m
[32m[2022-09-05 13:37:08,809] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-600/special_tokens_map.json[0m
[32m[2022-09-05 13:37:15,722] [    INFO][0m - loss: 0.00717021, learning_rate: 9.078549848942599e-06, global_step: 610, interval_runtime: 53.7656, interval_samples_per_second: 0.149, interval_steps_per_second: 0.186, epoch: 1.8429[0m
[32m[2022-09-05 13:37:17,532] [    INFO][0m - loss: 0.03471549, learning_rate: 9.06344410876133e-06, global_step: 620, interval_runtime: 1.8107, interval_samples_per_second: 4.418, interval_steps_per_second: 5.523, epoch: 1.8731[0m
[32m[2022-09-05 13:37:19,359] [    INFO][0m - loss: 0.01092732, learning_rate: 9.048338368580061e-06, global_step: 630, interval_runtime: 1.8269, interval_samples_per_second: 4.379, interval_steps_per_second: 5.474, epoch: 1.9033[0m
[32m[2022-09-05 13:37:21,178] [    INFO][0m - loss: 0.01939474, learning_rate: 9.033232628398793e-06, global_step: 640, interval_runtime: 1.8193, interval_samples_per_second: 4.397, interval_steps_per_second: 5.496, epoch: 1.9335[0m
[32m[2022-09-05 13:37:23,005] [    INFO][0m - loss: 0.08102108, learning_rate: 9.018126888217523e-06, global_step: 650, interval_runtime: 1.8267, interval_samples_per_second: 4.379, interval_steps_per_second: 5.474, epoch: 1.9637[0m
[32m[2022-09-05 13:37:24,821] [    INFO][0m - loss: 0.0016583, learning_rate: 9.003021148036256e-06, global_step: 660, interval_runtime: 1.8164, interval_samples_per_second: 4.404, interval_steps_per_second: 5.506, epoch: 1.994[0m
[32m[2022-09-05 13:37:26,621] [    INFO][0m - loss: 0.03794295, learning_rate: 8.987915407854985e-06, global_step: 670, interval_runtime: 1.8, interval_samples_per_second: 4.445, interval_steps_per_second: 5.556, epoch: 2.0242[0m
[32m[2022-09-05 13:37:28,444] [    INFO][0m - loss: 0.00073771, learning_rate: 8.972809667673717e-06, global_step: 680, interval_runtime: 1.8225, interval_samples_per_second: 4.39, interval_steps_per_second: 5.487, epoch: 2.0544[0m
[32m[2022-09-05 13:37:30,265] [    INFO][0m - loss: 0.00027479, learning_rate: 8.957703927492448e-06, global_step: 690, interval_runtime: 1.821, interval_samples_per_second: 4.393, interval_steps_per_second: 5.491, epoch: 2.0846[0m
[32m[2022-09-05 13:37:32,091] [    INFO][0m - loss: 0.05621355, learning_rate: 8.94259818731118e-06, global_step: 700, interval_runtime: 1.826, interval_samples_per_second: 4.381, interval_steps_per_second: 5.476, epoch: 2.1148[0m
[32m[2022-09-05 13:37:32,091] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 13:37:32,091] [    INFO][0m -   Num examples = 2712[0m
[32m[2022-09-05 13:37:32,092] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 13:37:32,092] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 13:37:32,092] [    INFO][0m -   Total prediction steps = 339[0m
[32m[2022-09-05 13:38:15,761] [    INFO][0m - eval_loss: 6.590190887451172, eval_accuracy: 0.4944690265486726, eval_runtime: 43.6686, eval_samples_per_second: 62.104, eval_steps_per_second: 7.763, epoch: 2.1148[0m
[32m[2022-09-05 13:38:15,802] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-700[0m
[32m[2022-09-05 13:38:15,802] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 13:38:19,256] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-700/tokenizer_config.json[0m
[32m[2022-09-05 13:38:19,257] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-700/special_tokens_map.json[0m
[32m[2022-09-05 13:38:24,575] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-05 13:38:24,576] [    INFO][0m - Loading best model from ./checkpoints_csl/checkpoint-300 (score: 0.5383480825958702).[0m
[32m[2022-09-05 13:38:25,625] [    INFO][0m - train_runtime: 505.5443, train_samples_per_second: 104.521, train_steps_per_second: 13.095, train_loss: 0.36455482051508237, epoch: 2.1148[0m
[32m[2022-09-05 13:38:25,667] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/[0m
[32m[2022-09-05 13:38:25,668] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 13:38:29,100] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/tokenizer_config.json[0m
[32m[2022-09-05 13:38:29,100] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/special_tokens_map.json[0m
[32m[2022-09-05 13:38:29,102] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-05 13:38:29,102] [    INFO][0m -   epoch                    =     2.1148[0m
[32m[2022-09-05 13:38:29,102] [    INFO][0m -   train_loss               =     0.3646[0m
[32m[2022-09-05 13:38:29,102] [    INFO][0m -   train_runtime            = 0:08:25.54[0m
[32m[2022-09-05 13:38:29,102] [    INFO][0m -   train_samples_per_second =    104.521[0m
[32m[2022-09-05 13:38:29,103] [    INFO][0m -   train_steps_per_second   =     13.095[0m
[32m[2022-09-05 13:38:29,109] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-05 13:38:29,109] [    INFO][0m -   Num examples = 49340[0m
[32m[2022-09-05 13:38:29,109] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 13:38:29,109] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 13:38:29,109] [    INFO][0m -   Total prediction steps = 6168[0m
terminate called after throwing an instance of 'paddle::memory::allocation::BadAlloc'
  what():  

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   softmax_final_state_dygraph_function(paddle::experimental::Tensor const&, int)
1   paddle::experimental::softmax(paddle::experimental::Tensor const&, int)
2   void phi::SoftmaxGPUDNNKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, int, phi::DenseTensor*)
3   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, paddle::experimental::DataType, unsigned long, bool) const
4   phi::DenseTensor::AllocateFrom(phi::Allocator*, paddle::experimental::DataType, unsigned long)
5   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
6   paddle::memory::allocation::Allocator::Allocate(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
10  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
11  phi::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 7.352233GB memory on GPU 0, 26.141357GB memory has been allocated and available memory is only 5.607178GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
If the above ways do not solve the out of memory problem, you can try to use CUDA managed memory. The command is `export FLAGS_use_cuda_managed_memory=false`.
 (at /paddle/paddle/fluid/memory/allocation/cuda_allocator.cc:95)

run.sh: line 52: 64966 Aborted                 CUDA_VISIBLE_DEVICES=$device python train_single.py --output_dir ./checkpoints_$task_name/ --prompt "$prompt" --max_seq_length $max_length --per_device_eval_batch_size $batch_size --per_device_train_batch_size $batch_size --model_name_or_path ernie-3.0-base-zh --split_id few_all --task_name $task_name --metric_for_best_model accuracy --disable_tqdm True --do_test --eval_steps 100 --save_steps 100 --num_train_epochs 20 --logging_steps 10 --learning_rate 1e-5 --ppt_learning_rate 1e-5 --load_best_model_at_end $is_train --do_train $is_train --do_eval $is_train --do_save $is_train
run.sh: line 58: --freeze_plm: command not found
