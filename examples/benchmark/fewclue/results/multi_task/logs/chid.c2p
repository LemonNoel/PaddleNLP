[33m[2022-09-05 11:26:43,255] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-09-05 11:26:43,255] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-05 11:26:43,255] [    INFO][0m - ============================================================[0m
[32m[2022-09-05 11:26:43,255] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-05 11:26:43,255] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-05 11:26:43,255] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-05 11:26:43,256] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-05 11:26:43,256] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-05 11:26:43,256] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-05 11:26:43,256] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-09-05 11:26:43,256] [    INFO][0m - [0m
[32m[2022-09-05 11:26:43,256] [    INFO][0m - ============================================================[0m
[32m[2022-09-05 11:26:43,256] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-05 11:26:43,256] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-05 11:26:43,256] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-05 11:26:43,256] [    INFO][0m - prompt                        :[0m
[32m[2022-09-05 11:26:43,256] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-05 11:26:43,256] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-05 11:26:43,256] [    INFO][0m - task_name                     :chid[0m
[32m[2022-09-05 11:26:43,257] [    INFO][0m - [0m
[32m[2022-09-05 11:26:43,257] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0905 11:26:43.258347 38959 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0905 11:26:43.262364 38959 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-05 11:26:47,415] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-09-05 11:26:47,439] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-09-05 11:26:47,440] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[32m[2022-09-05 11:26:47,441] [    INFO][0m - Using template: [{'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': 'Êñá‰∏≠ÊàêËØ≠ÊòØÂê¶Ê≠£Á°ÆÔºü'}, {'add_prefix_space': '', 'mask': None}][0m
2022-09-05 11:26:47,444 INFO [download.py:119] unique_endpoints {''}
[32m[2022-09-05 11:26:47,736] [    INFO][0m - ============================================================[0m
[32m[2022-09-05 11:26:47,737] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-05 11:26:47,737] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-05 11:26:47,737] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-05 11:26:47,737] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-05 11:26:47,737] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-05 11:26:47,737] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-05 11:26:47,737] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-05 11:26:47,737] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-05 11:26:47,737] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-05 11:26:47,738] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-05 11:26:47,738] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-05 11:26:47,738] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-05 11:26:47,738] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-05 11:26:47,738] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-05 11:26:47,738] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-05 11:26:47,738] [    INFO][0m - do_predict                    :False[0m
[32m[2022-09-05 11:26:47,738] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-05 11:26:47,738] [    INFO][0m - eval_batch_size               :8[0m
[32m[2022-09-05 11:26:47,738] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-09-05 11:26:47,738] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-09-05 11:26:47,738] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-05 11:26:47,738] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-05 11:26:47,739] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-05 11:26:47,739] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-05 11:26:47,739] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-05 11:26:47,739] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-05 11:26:47,739] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-05 11:26:47,739] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-05 11:26:47,739] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-05 11:26:47,739] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-09-05 11:26:47,739] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-05 11:26:47,739] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-05 11:26:47,739] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-05 11:26:47,739] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-05 11:26:47,739] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-05 11:26:47,740] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-05 11:26:47,740] [    INFO][0m - logging_dir                   :./checkpoints_chid/runs/Sep05_11-26-43_instance-3bwob41y-01[0m
[32m[2022-09-05 11:26:47,740] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-05 11:26:47,740] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-05 11:26:47,740] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-05 11:26:47,740] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-05 11:26:47,740] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-05 11:26:47,740] [    INFO][0m - max_seq_length                :256[0m
[32m[2022-09-05 11:26:47,740] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-05 11:26:47,740] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-05 11:26:47,740] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-05 11:26:47,740] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-05 11:26:47,740] [    INFO][0m - num_train_epochs              :20.0[0m
[32m[2022-09-05 11:26:47,740] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-05 11:26:47,741] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-05 11:26:47,741] [    INFO][0m - output_dir                    :./checkpoints_chid/[0m
[32m[2022-09-05 11:26:47,741] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-05 11:26:47,741] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-05 11:26:47,741] [    INFO][0m - per_device_eval_batch_size    :8[0m
[32m[2022-09-05 11:26:47,741] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-09-05 11:26:47,741] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-05 11:26:47,741] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-05 11:26:47,741] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-05 11:26:47,741] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-09-05 11:26:47,741] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-05 11:26:47,741] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-05 11:26:47,741] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-05 11:26:47,742] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-05 11:26:47,742] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-05 11:26:47,742] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-05 11:26:47,742] [    INFO][0m - run_name                      :./checkpoints_chid/[0m
[32m[2022-09-05 11:26:47,742] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-05 11:26:47,742] [    INFO][0m - save_steps                    :100[0m
[32m[2022-09-05 11:26:47,742] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-09-05 11:26:47,742] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-05 11:26:47,742] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-05 11:26:47,742] [    INFO][0m - seed                          :42[0m
[32m[2022-09-05 11:26:47,742] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-05 11:26:47,742] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-05 11:26:47,742] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-05 11:26:47,742] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-09-05 11:26:47,743] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-05 11:26:47,743] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-05 11:26:47,743] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-05 11:26:47,743] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-05 11:26:47,743] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-05 11:26:47,743] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-05 11:26:47,743] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-05 11:26:47,743] [    INFO][0m - [0m
[32m[2022-09-05 11:26:47,745] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-05 11:26:47,745] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-05 11:26:47,745] [    INFO][0m -   Num Epochs = 20[0m
[32m[2022-09-05 11:26:47,745] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-09-05 11:26:47,745] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-09-05 11:26:47,745] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-05 11:26:47,745] [    INFO][0m -   Total optimization steps = 3540.0[0m
[32m[2022-09-05 11:26:47,745] [    INFO][0m -   Total num train samples = 28280[0m
[32m[2022-09-05 11:26:50,259] [    INFO][0m - loss: 3.6056675, learning_rate: 2.9915254237288134e-05, global_step: 10, interval_runtime: 2.5132, interval_samples_per_second: 3.183, interval_steps_per_second: 3.979, epoch: 0.0565[0m
[32m[2022-09-05 11:26:51,746] [    INFO][0m - loss: 0.56586895, learning_rate: 2.9830508474576274e-05, global_step: 20, interval_runtime: 1.4872, interval_samples_per_second: 5.379, interval_steps_per_second: 6.724, epoch: 0.113[0m
[32m[2022-09-05 11:26:53,239] [    INFO][0m - loss: 0.64883256, learning_rate: 2.9745762711864407e-05, global_step: 30, interval_runtime: 1.4921, interval_samples_per_second: 5.361, interval_steps_per_second: 6.702, epoch: 0.1695[0m
[32m[2022-09-05 11:26:54,742] [    INFO][0m - loss: 0.62901106, learning_rate: 2.9661016949152544e-05, global_step: 40, interval_runtime: 1.5028, interval_samples_per_second: 5.323, interval_steps_per_second: 6.654, epoch: 0.226[0m
[32m[2022-09-05 11:26:56,251] [    INFO][0m - loss: 0.42838368, learning_rate: 2.9576271186440677e-05, global_step: 50, interval_runtime: 1.5096, interval_samples_per_second: 5.3, interval_steps_per_second: 6.624, epoch: 0.2825[0m
[32m[2022-09-05 11:26:57,756] [    INFO][0m - loss: 0.28398826, learning_rate: 2.9491525423728817e-05, global_step: 60, interval_runtime: 1.5056, interval_samples_per_second: 5.313, interval_steps_per_second: 6.642, epoch: 0.339[0m
[32m[2022-09-05 11:26:59,260] [    INFO][0m - loss: 0.83606186, learning_rate: 2.940677966101695e-05, global_step: 70, interval_runtime: 1.5033, interval_samples_per_second: 5.322, interval_steps_per_second: 6.652, epoch: 0.3955[0m
[32m[2022-09-05 11:27:00,766] [    INFO][0m - loss: 0.59247403, learning_rate: 2.9322033898305087e-05, global_step: 80, interval_runtime: 1.5055, interval_samples_per_second: 5.314, interval_steps_per_second: 6.642, epoch: 0.452[0m
[32m[2022-09-05 11:27:02,271] [    INFO][0m - loss: 0.51546087, learning_rate: 2.923728813559322e-05, global_step: 90, interval_runtime: 1.5053, interval_samples_per_second: 5.315, interval_steps_per_second: 6.643, epoch: 0.5085[0m
[32m[2022-09-05 11:27:03,771] [    INFO][0m - loss: 0.48372283, learning_rate: 2.9152542372881356e-05, global_step: 100, interval_runtime: 1.5006, interval_samples_per_second: 5.331, interval_steps_per_second: 6.664, epoch: 0.565[0m
[32m[2022-09-05 11:27:03,772] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 11:27:03,772] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-05 11:27:03,772] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 11:27:03,772] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 11:27:03,772] [    INFO][0m -   Total prediction steps = 177[0m
[32m[2022-09-05 11:27:15,283] [    INFO][0m - eval_loss: 0.4175790250301361, eval_accuracy: 0.15346534653465346, eval_runtime: 11.5098, eval_samples_per_second: 122.852, eval_steps_per_second: 15.378, epoch: 0.565[0m
[32m[2022-09-05 11:27:15,317] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-100[0m
[32m[2022-09-05 11:27:15,317] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 11:27:18,713] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-05 11:27:18,714] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-05 11:27:27,011] [    INFO][0m - loss: 0.47742229, learning_rate: 2.9067796610169493e-05, global_step: 110, interval_runtime: 23.2392, interval_samples_per_second: 0.344, interval_steps_per_second: 0.43, epoch: 0.6215[0m
[32m[2022-09-05 11:27:28,506] [    INFO][0m - loss: 0.39645038, learning_rate: 2.8983050847457626e-05, global_step: 120, interval_runtime: 1.495, interval_samples_per_second: 5.351, interval_steps_per_second: 6.689, epoch: 0.678[0m
[32m[2022-09-05 11:27:30,003] [    INFO][0m - loss: 0.49355526, learning_rate: 2.8898305084745763e-05, global_step: 130, interval_runtime: 1.4977, interval_samples_per_second: 5.341, interval_steps_per_second: 6.677, epoch: 0.7345[0m
[32m[2022-09-05 11:27:31,502] [    INFO][0m - loss: 0.57291079, learning_rate: 2.88135593220339e-05, global_step: 140, interval_runtime: 1.4987, interval_samples_per_second: 5.338, interval_steps_per_second: 6.672, epoch: 0.791[0m
[32m[2022-09-05 11:27:33,008] [    INFO][0m - loss: 0.67444267, learning_rate: 2.8728813559322036e-05, global_step: 150, interval_runtime: 1.5059, interval_samples_per_second: 5.313, interval_steps_per_second: 6.641, epoch: 0.8475[0m
[32m[2022-09-05 11:27:34,504] [    INFO][0m - loss: 0.49936466, learning_rate: 2.864406779661017e-05, global_step: 160, interval_runtime: 1.4958, interval_samples_per_second: 5.348, interval_steps_per_second: 6.685, epoch: 0.904[0m
[32m[2022-09-05 11:27:36,004] [    INFO][0m - loss: 0.47442002, learning_rate: 2.855932203389831e-05, global_step: 170, interval_runtime: 1.5, interval_samples_per_second: 5.333, interval_steps_per_second: 6.667, epoch: 0.9605[0m
[32m[2022-09-05 11:27:37,515] [    INFO][0m - loss: 0.44333124, learning_rate: 2.8474576271186442e-05, global_step: 180, interval_runtime: 1.5111, interval_samples_per_second: 5.294, interval_steps_per_second: 6.618, epoch: 1.0169[0m
[32m[2022-09-05 11:27:39,013] [    INFO][0m - loss: 0.57338791, learning_rate: 2.8389830508474575e-05, global_step: 190, interval_runtime: 1.4978, interval_samples_per_second: 5.341, interval_steps_per_second: 6.676, epoch: 1.0734[0m
[32m[2022-09-05 11:27:40,509] [    INFO][0m - loss: 0.41303062, learning_rate: 2.8305084745762712e-05, global_step: 200, interval_runtime: 1.4966, interval_samples_per_second: 5.346, interval_steps_per_second: 6.682, epoch: 1.1299[0m
[32m[2022-09-05 11:27:40,510] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 11:27:40,510] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-05 11:27:40,510] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 11:27:40,510] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 11:27:40,510] [    INFO][0m -   Total prediction steps = 177[0m
[32m[2022-09-05 11:27:51,566] [    INFO][0m - eval_loss: 0.44645974040031433, eval_accuracy: 0.06435643564356436, eval_runtime: 11.0557, eval_samples_per_second: 127.898, eval_steps_per_second: 16.01, epoch: 1.1299[0m
[32m[2022-09-05 11:27:51,597] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-200[0m
[32m[2022-09-05 11:27:51,597] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 11:27:54,798] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-200/tokenizer_config.json[0m
[32m[2022-09-05 11:27:54,798] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-200/special_tokens_map.json[0m
[32m[2022-09-05 11:28:01,582] [    INFO][0m - loss: 0.41854415, learning_rate: 2.822033898305085e-05, global_step: 210, interval_runtime: 21.0733, interval_samples_per_second: 0.38, interval_steps_per_second: 0.475, epoch: 1.1864[0m
[32m[2022-09-05 11:28:03,081] [    INFO][0m - loss: 0.62138915, learning_rate: 2.8135593220338985e-05, global_step: 220, interval_runtime: 1.4985, interval_samples_per_second: 5.339, interval_steps_per_second: 6.673, epoch: 1.2429[0m
[32m[2022-09-05 11:28:04,573] [    INFO][0m - loss: 0.47677794, learning_rate: 2.805084745762712e-05, global_step: 230, interval_runtime: 1.4919, interval_samples_per_second: 5.362, interval_steps_per_second: 6.703, epoch: 1.2994[0m
[32m[2022-09-05 11:28:06,066] [    INFO][0m - loss: 0.52087398, learning_rate: 2.7966101694915255e-05, global_step: 240, interval_runtime: 1.4928, interval_samples_per_second: 5.359, interval_steps_per_second: 6.699, epoch: 1.3559[0m
[32m[2022-09-05 11:28:07,563] [    INFO][0m - loss: 0.36972952, learning_rate: 2.788135593220339e-05, global_step: 250, interval_runtime: 1.4975, interval_samples_per_second: 5.342, interval_steps_per_second: 6.678, epoch: 1.4124[0m
[32m[2022-09-05 11:28:09,063] [    INFO][0m - loss: 0.46473904, learning_rate: 2.7796610169491528e-05, global_step: 260, interval_runtime: 1.5, interval_samples_per_second: 5.333, interval_steps_per_second: 6.667, epoch: 1.4689[0m
[32m[2022-09-05 11:28:10,567] [    INFO][0m - loss: 0.42565975, learning_rate: 2.771186440677966e-05, global_step: 270, interval_runtime: 1.5037, interval_samples_per_second: 5.32, interval_steps_per_second: 6.65, epoch: 1.5254[0m
[32m[2022-09-05 11:28:12,073] [    INFO][0m - loss: 0.5464108, learning_rate: 2.7627118644067794e-05, global_step: 280, interval_runtime: 1.5055, interval_samples_per_second: 5.314, interval_steps_per_second: 6.642, epoch: 1.5819[0m
[32m[2022-09-05 11:28:13,580] [    INFO][0m - loss: 0.34134507, learning_rate: 2.7542372881355934e-05, global_step: 290, interval_runtime: 1.5074, interval_samples_per_second: 5.307, interval_steps_per_second: 6.634, epoch: 1.6384[0m
[32m[2022-09-05 11:28:15,080] [    INFO][0m - loss: 0.68683653, learning_rate: 2.7457627118644068e-05, global_step: 300, interval_runtime: 1.5002, interval_samples_per_second: 5.332, interval_steps_per_second: 6.666, epoch: 1.6949[0m
[32m[2022-09-05 11:28:15,081] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 11:28:15,081] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-05 11:28:15,081] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 11:28:15,081] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 11:28:15,081] [    INFO][0m -   Total prediction steps = 177[0m
[32m[2022-09-05 11:28:26,152] [    INFO][0m - eval_loss: 0.4147845506668091, eval_accuracy: 0.12871287128712872, eval_runtime: 11.0701, eval_samples_per_second: 127.732, eval_steps_per_second: 15.989, epoch: 1.6949[0m
[32m[2022-09-05 11:28:26,178] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-300[0m
[32m[2022-09-05 11:28:26,178] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 11:28:29,630] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-300/tokenizer_config.json[0m
[32m[2022-09-05 11:28:29,630] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-300/special_tokens_map.json[0m
[32m[2022-09-05 11:28:36,735] [    INFO][0m - loss: 0.377759, learning_rate: 2.7372881355932204e-05, global_step: 310, interval_runtime: 21.6552, interval_samples_per_second: 0.369, interval_steps_per_second: 0.462, epoch: 1.7514[0m
[32m[2022-09-05 11:28:38,232] [    INFO][0m - loss: 0.37902627, learning_rate: 2.7288135593220337e-05, global_step: 320, interval_runtime: 1.4968, interval_samples_per_second: 5.345, interval_steps_per_second: 6.681, epoch: 1.8079[0m
[32m[2022-09-05 11:28:39,729] [    INFO][0m - loss: 0.57716436, learning_rate: 2.7203389830508477e-05, global_step: 330, interval_runtime: 1.4971, interval_samples_per_second: 5.344, interval_steps_per_second: 6.68, epoch: 1.8644[0m
[32m[2022-09-05 11:28:41,223] [    INFO][0m - loss: 0.38501487, learning_rate: 2.711864406779661e-05, global_step: 340, interval_runtime: 1.4941, interval_samples_per_second: 5.354, interval_steps_per_second: 6.693, epoch: 1.9209[0m
[32m[2022-09-05 11:28:42,720] [    INFO][0m - loss: 0.36030221, learning_rate: 2.7033898305084747e-05, global_step: 350, interval_runtime: 1.4971, interval_samples_per_second: 5.344, interval_steps_per_second: 6.679, epoch: 1.9774[0m
[32m[2022-09-05 11:28:44,227] [    INFO][0m - loss: 0.42265205, learning_rate: 2.6949152542372884e-05, global_step: 360, interval_runtime: 1.5064, interval_samples_per_second: 5.311, interval_steps_per_second: 6.639, epoch: 2.0339[0m
[32m[2022-09-05 11:28:45,720] [    INFO][0m - loss: 0.35688007, learning_rate: 2.6864406779661017e-05, global_step: 370, interval_runtime: 1.4931, interval_samples_per_second: 5.358, interval_steps_per_second: 6.697, epoch: 2.0904[0m
[32m[2022-09-05 11:28:47,217] [    INFO][0m - loss: 0.52041297, learning_rate: 2.6779661016949153e-05, global_step: 380, interval_runtime: 1.4969, interval_samples_per_second: 5.344, interval_steps_per_second: 6.68, epoch: 2.1469[0m
[32m[2022-09-05 11:28:48,719] [    INFO][0m - loss: 0.50724869, learning_rate: 2.6694915254237287e-05, global_step: 390, interval_runtime: 1.5025, interval_samples_per_second: 5.325, interval_steps_per_second: 6.656, epoch: 2.2034[0m
[32m[2022-09-05 11:28:50,217] [    INFO][0m - loss: 0.48209858, learning_rate: 2.6610169491525427e-05, global_step: 400, interval_runtime: 1.4977, interval_samples_per_second: 5.341, interval_steps_per_second: 6.677, epoch: 2.2599[0m
[32m[2022-09-05 11:28:50,217] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 11:28:50,217] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-05 11:28:50,217] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 11:28:50,218] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 11:28:50,218] [    INFO][0m -   Total prediction steps = 177[0m
[32m[2022-09-05 11:29:01,397] [    INFO][0m - eval_loss: 0.5894003510475159, eval_accuracy: 0.29207920792079206, eval_runtime: 11.1787, eval_samples_per_second: 126.49, eval_steps_per_second: 15.834, epoch: 2.2599[0m
[32m[2022-09-05 11:29:01,426] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-400[0m
[32m[2022-09-05 11:29:01,426] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 11:29:04,601] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-400/tokenizer_config.json[0m
[32m[2022-09-05 11:29:04,897] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-400/special_tokens_map.json[0m
[32m[2022-09-05 11:29:11,994] [    INFO][0m - loss: 0.61582599, learning_rate: 2.652542372881356e-05, global_step: 410, interval_runtime: 21.7772, interval_samples_per_second: 0.367, interval_steps_per_second: 0.459, epoch: 2.3164[0m
[32m[2022-09-05 11:29:13,502] [    INFO][0m - loss: 0.43777895, learning_rate: 2.6440677966101696e-05, global_step: 420, interval_runtime: 1.5073, interval_samples_per_second: 5.308, interval_steps_per_second: 6.634, epoch: 2.3729[0m
[32m[2022-09-05 11:29:15,003] [    INFO][0m - loss: 0.47412305, learning_rate: 2.635593220338983e-05, global_step: 430, interval_runtime: 1.5015, interval_samples_per_second: 5.328, interval_steps_per_second: 6.66, epoch: 2.4294[0m
[32m[2022-09-05 11:29:16,501] [    INFO][0m - loss: 0.58269682, learning_rate: 2.627118644067797e-05, global_step: 440, interval_runtime: 1.4978, interval_samples_per_second: 5.341, interval_steps_per_second: 6.676, epoch: 2.4859[0m
[32m[2022-09-05 11:29:18,005] [    INFO][0m - loss: 0.45568767, learning_rate: 2.6186440677966103e-05, global_step: 450, interval_runtime: 1.5041, interval_samples_per_second: 5.319, interval_steps_per_second: 6.648, epoch: 2.5424[0m
[32m[2022-09-05 11:29:19,501] [    INFO][0m - loss: 0.59056797, learning_rate: 2.6101694915254236e-05, global_step: 460, interval_runtime: 1.4959, interval_samples_per_second: 5.348, interval_steps_per_second: 6.685, epoch: 2.5989[0m
[32m[2022-09-05 11:29:21,006] [    INFO][0m - loss: 0.58363528, learning_rate: 2.6016949152542372e-05, global_step: 470, interval_runtime: 1.5053, interval_samples_per_second: 5.314, interval_steps_per_second: 6.643, epoch: 2.6554[0m
[32m[2022-09-05 11:29:22,509] [    INFO][0m - loss: 0.44276524, learning_rate: 2.593220338983051e-05, global_step: 480, interval_runtime: 1.5032, interval_samples_per_second: 5.322, interval_steps_per_second: 6.652, epoch: 2.7119[0m
[32m[2022-09-05 11:29:24,012] [    INFO][0m - loss: 0.38816764, learning_rate: 2.5847457627118646e-05, global_step: 490, interval_runtime: 1.5025, interval_samples_per_second: 5.324, interval_steps_per_second: 6.655, epoch: 2.7684[0m
[32m[2022-09-05 11:29:25,513] [    INFO][0m - loss: 0.36423202, learning_rate: 2.576271186440678e-05, global_step: 500, interval_runtime: 1.5011, interval_samples_per_second: 5.329, interval_steps_per_second: 6.662, epoch: 2.8249[0m
[32m[2022-09-05 11:29:25,513] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 11:29:25,514] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-05 11:29:25,514] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 11:29:25,514] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 11:29:25,514] [    INFO][0m -   Total prediction steps = 177[0m
[32m[2022-09-05 11:29:36,822] [    INFO][0m - eval_loss: 0.43756207823753357, eval_accuracy: 0.17326732673267325, eval_runtime: 11.3072, eval_samples_per_second: 125.053, eval_steps_per_second: 15.654, epoch: 2.8249[0m
[32m[2022-09-05 11:29:36,853] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-500[0m
[32m[2022-09-05 11:29:36,853] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 11:29:40,515] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-500/tokenizer_config.json[0m
[32m[2022-09-05 11:29:40,515] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-500/special_tokens_map.json[0m
[32m[2022-09-05 11:29:47,814] [    INFO][0m - loss: 0.35552397, learning_rate: 2.567796610169492e-05, global_step: 510, interval_runtime: 22.3011, interval_samples_per_second: 0.359, interval_steps_per_second: 0.448, epoch: 2.8814[0m
[32m[2022-09-05 11:29:49,312] [    INFO][0m - loss: 0.35885365, learning_rate: 2.5593220338983052e-05, global_step: 520, interval_runtime: 1.4973, interval_samples_per_second: 5.343, interval_steps_per_second: 6.679, epoch: 2.9379[0m
[32m[2022-09-05 11:29:50,809] [    INFO][0m - loss: 0.31692691, learning_rate: 2.550847457627119e-05, global_step: 530, interval_runtime: 1.4972, interval_samples_per_second: 5.343, interval_steps_per_second: 6.679, epoch: 2.9944[0m
[32m[2022-09-05 11:29:52,321] [    INFO][0m - loss: 0.41438127, learning_rate: 2.5423728813559322e-05, global_step: 540, interval_runtime: 1.5123, interval_samples_per_second: 5.29, interval_steps_per_second: 6.612, epoch: 3.0508[0m
[32m[2022-09-05 11:29:53,823] [    INFO][0m - loss: 0.46728196, learning_rate: 2.5338983050847458e-05, global_step: 550, interval_runtime: 1.5018, interval_samples_per_second: 5.327, interval_steps_per_second: 6.658, epoch: 3.1073[0m
[32m[2022-09-05 11:29:55,330] [    INFO][0m - loss: 0.46161704, learning_rate: 2.5254237288135595e-05, global_step: 560, interval_runtime: 1.5069, interval_samples_per_second: 5.309, interval_steps_per_second: 6.636, epoch: 3.1638[0m
[32m[2022-09-05 11:29:56,839] [    INFO][0m - loss: 0.49520073, learning_rate: 2.5169491525423728e-05, global_step: 570, interval_runtime: 1.5091, interval_samples_per_second: 5.301, interval_steps_per_second: 6.626, epoch: 3.2203[0m
[32m[2022-09-05 11:29:58,349] [    INFO][0m - loss: 0.4422142, learning_rate: 2.5084745762711865e-05, global_step: 580, interval_runtime: 1.51, interval_samples_per_second: 5.298, interval_steps_per_second: 6.623, epoch: 3.2768[0m
[32m[2022-09-05 11:29:59,856] [    INFO][0m - loss: 0.45025549, learning_rate: 2.5e-05, global_step: 590, interval_runtime: 1.5072, interval_samples_per_second: 5.308, interval_steps_per_second: 6.635, epoch: 3.3333[0m
[32m[2022-09-05 11:30:01,363] [    INFO][0m - loss: 0.47570276, learning_rate: 2.4915254237288138e-05, global_step: 600, interval_runtime: 1.5067, interval_samples_per_second: 5.309, interval_steps_per_second: 6.637, epoch: 3.3898[0m
[32m[2022-09-05 11:30:01,364] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 11:30:01,364] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-05 11:30:01,364] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 11:30:01,364] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 11:30:01,364] [    INFO][0m -   Total prediction steps = 177[0m
[32m[2022-09-05 11:30:12,721] [    INFO][0m - eval_loss: 0.4117281436920166, eval_accuracy: 0.22772277227722773, eval_runtime: 11.3557, eval_samples_per_second: 124.519, eval_steps_per_second: 15.587, epoch: 3.3898[0m
[32m[2022-09-05 11:30:12,748] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-600[0m
[32m[2022-09-05 11:30:12,748] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 11:30:16,243] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-600/tokenizer_config.json[0m
[32m[2022-09-05 11:30:16,244] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-600/special_tokens_map.json[0m
[32m[2022-09-05 11:30:23,185] [    INFO][0m - loss: 0.37780926, learning_rate: 2.483050847457627e-05, global_step: 610, interval_runtime: 21.8219, interval_samples_per_second: 0.367, interval_steps_per_second: 0.458, epoch: 3.4463[0m
[32m[2022-09-05 11:30:24,685] [    INFO][0m - loss: 0.44554377, learning_rate: 2.4745762711864408e-05, global_step: 620, interval_runtime: 1.4998, interval_samples_per_second: 5.334, interval_steps_per_second: 6.667, epoch: 3.5028[0m
[32m[2022-09-05 11:30:26,181] [    INFO][0m - loss: 0.44154859, learning_rate: 2.4661016949152544e-05, global_step: 630, interval_runtime: 1.496, interval_samples_per_second: 5.347, interval_steps_per_second: 6.684, epoch: 3.5593[0m
[32m[2022-09-05 11:30:27,673] [    INFO][0m - loss: 0.42285872, learning_rate: 2.4576271186440677e-05, global_step: 640, interval_runtime: 1.4929, interval_samples_per_second: 5.359, interval_steps_per_second: 6.698, epoch: 3.6158[0m
[32m[2022-09-05 11:30:29,176] [    INFO][0m - loss: 0.37676313, learning_rate: 2.4491525423728814e-05, global_step: 650, interval_runtime: 1.5017, interval_samples_per_second: 5.327, interval_steps_per_second: 6.659, epoch: 3.6723[0m
[32m[2022-09-05 11:30:30,772] [    INFO][0m - loss: 0.41929455, learning_rate: 2.440677966101695e-05, global_step: 660, interval_runtime: 1.5056, interval_samples_per_second: 5.313, interval_steps_per_second: 6.642, epoch: 3.7288[0m
[32m[2022-09-05 11:30:32,280] [    INFO][0m - loss: 0.33117933, learning_rate: 2.4322033898305087e-05, global_step: 670, interval_runtime: 1.5989, interval_samples_per_second: 5.003, interval_steps_per_second: 6.254, epoch: 3.7853[0m
[32m[2022-09-05 11:30:33,787] [    INFO][0m - loss: 0.40155516, learning_rate: 2.423728813559322e-05, global_step: 680, interval_runtime: 1.5071, interval_samples_per_second: 5.308, interval_steps_per_second: 6.635, epoch: 3.8418[0m
[32m[2022-09-05 11:30:35,289] [    INFO][0m - loss: 0.45234861, learning_rate: 2.4152542372881357e-05, global_step: 690, interval_runtime: 1.5022, interval_samples_per_second: 5.326, interval_steps_per_second: 6.657, epoch: 3.8983[0m
[32m[2022-09-05 11:30:36,792] [    INFO][0m - loss: 0.48492837, learning_rate: 2.4067796610169493e-05, global_step: 700, interval_runtime: 1.5034, interval_samples_per_second: 5.321, interval_steps_per_second: 6.651, epoch: 3.9548[0m
[32m[2022-09-05 11:30:36,793] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 11:30:36,793] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-05 11:30:36,793] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 11:30:36,793] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 11:30:36,794] [    INFO][0m -   Total prediction steps = 177[0m
[32m[2022-09-05 11:30:47,873] [    INFO][0m - eval_loss: 0.4176998436450958, eval_accuracy: 0.1782178217821782, eval_runtime: 11.0789, eval_samples_per_second: 127.63, eval_steps_per_second: 15.976, epoch: 3.9548[0m
[32m[2022-09-05 11:30:47,899] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-700[0m
[32m[2022-09-05 11:30:47,899] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 11:30:51,341] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-700/tokenizer_config.json[0m
[32m[2022-09-05 11:30:51,341] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-700/special_tokens_map.json[0m
[32m[2022-09-05 11:30:58,367] [    INFO][0m - loss: 0.54154758, learning_rate: 2.3983050847457627e-05, global_step: 710, interval_runtime: 21.5747, interval_samples_per_second: 0.371, interval_steps_per_second: 0.464, epoch: 4.0113[0m
[32m[2022-09-05 11:30:59,863] [    INFO][0m - loss: 0.3831872, learning_rate: 2.3898305084745763e-05, global_step: 720, interval_runtime: 1.4952, interval_samples_per_second: 5.351, interval_steps_per_second: 6.688, epoch: 4.0678[0m
[32m[2022-09-05 11:31:01,360] [    INFO][0m - loss: 0.45611567, learning_rate: 2.3813559322033896e-05, global_step: 730, interval_runtime: 1.4973, interval_samples_per_second: 5.343, interval_steps_per_second: 6.679, epoch: 4.1243[0m
[32m[2022-09-05 11:31:02,860] [    INFO][0m - loss: 0.40221591, learning_rate: 2.3728813559322036e-05, global_step: 740, interval_runtime: 1.5005, interval_samples_per_second: 5.332, interval_steps_per_second: 6.665, epoch: 4.1808[0m
[32m[2022-09-05 11:31:04,363] [    INFO][0m - loss: 0.37347317, learning_rate: 2.364406779661017e-05, global_step: 750, interval_runtime: 1.5028, interval_samples_per_second: 5.323, interval_steps_per_second: 6.654, epoch: 4.2373[0m
[32m[2022-09-05 11:31:05,856] [    INFO][0m - loss: 0.53702765, learning_rate: 2.3559322033898306e-05, global_step: 760, interval_runtime: 1.4932, interval_samples_per_second: 5.358, interval_steps_per_second: 6.697, epoch: 4.2938[0m
[32m[2022-09-05 11:31:07,355] [    INFO][0m - loss: 0.42671132, learning_rate: 2.347457627118644e-05, global_step: 770, interval_runtime: 1.4984, interval_samples_per_second: 5.339, interval_steps_per_second: 6.674, epoch: 4.3503[0m
[32m[2022-09-05 11:31:08,851] [    INFO][0m - loss: 0.55595083, learning_rate: 2.338983050847458e-05, global_step: 780, interval_runtime: 1.4962, interval_samples_per_second: 5.347, interval_steps_per_second: 6.684, epoch: 4.4068[0m
[32m[2022-09-05 11:31:10,347] [    INFO][0m - loss: 0.48176775, learning_rate: 2.3305084745762712e-05, global_step: 790, interval_runtime: 1.4956, interval_samples_per_second: 5.349, interval_steps_per_second: 6.686, epoch: 4.4633[0m
[32m[2022-09-05 11:31:11,844] [    INFO][0m - loss: 0.41008778, learning_rate: 2.3220338983050846e-05, global_step: 800, interval_runtime: 1.4973, interval_samples_per_second: 5.343, interval_steps_per_second: 6.679, epoch: 4.5198[0m
[32m[2022-09-05 11:31:11,845] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 11:31:11,845] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-05 11:31:11,845] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 11:31:11,845] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 11:31:11,845] [    INFO][0m -   Total prediction steps = 177[0m
[32m[2022-09-05 11:31:22,955] [    INFO][0m - eval_loss: 0.41640713810920715, eval_accuracy: 0.16831683168316833, eval_runtime: 11.1095, eval_samples_per_second: 127.279, eval_steps_per_second: 15.932, epoch: 4.5198[0m
[32m[2022-09-05 11:31:22,983] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-800[0m
[32m[2022-09-05 11:31:22,983] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 11:31:26,470] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-800/tokenizer_config.json[0m
[32m[2022-09-05 11:31:26,471] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-800/special_tokens_map.json[0m
[32m[2022-09-05 11:31:31,590] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-05 11:31:31,590] [    INFO][0m - Loading best model from ./checkpoints_chid/checkpoint-400 (score: 0.29207920792079206).[0m
[32m[2022-09-05 11:31:32,630] [    INFO][0m - train_runtime: 284.8838, train_samples_per_second: 99.269, train_steps_per_second: 12.426, train_loss: 0.5093000671267509, epoch: 4.5198[0m
[32m[2022-09-05 11:31:32,671] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/[0m
[32m[2022-09-05 11:31:32,671] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 11:31:37,463] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/tokenizer_config.json[0m
[32m[2022-09-05 11:31:37,463] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/special_tokens_map.json[0m
[32m[2022-09-05 11:31:37,464] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-05 11:31:37,465] [    INFO][0m -   epoch                    =     4.5198[0m
[32m[2022-09-05 11:31:37,465] [    INFO][0m -   train_loss               =     0.5093[0m
[32m[2022-09-05 11:31:37,465] [    INFO][0m -   train_runtime            = 0:04:44.88[0m
[32m[2022-09-05 11:31:37,465] [    INFO][0m -   train_samples_per_second =     99.269[0m
[32m[2022-09-05 11:31:37,465] [    INFO][0m -   train_steps_per_second   =     12.426[0m
[32m[2022-09-05 11:31:37,471] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-05 11:31:37,471] [    INFO][0m -   Num examples = 14014[0m
[32m[2022-09-05 11:31:37,471] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 11:31:37,471] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 11:31:37,471] [    INFO][0m -   Total prediction steps = 1752[0m
[32m[2022-09-05 11:34:18,326] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-05 11:34:18,327] [    INFO][0m -   test_accuracy           =     0.1429[0m
[32m[2022-09-05 11:34:18,327] [    INFO][0m -   test_loss               =    14.1755[0m
[32m[2022-09-05 11:34:18,327] [    INFO][0m -   test_runtime            = 0:02:40.85[0m
[32m[2022-09-05 11:34:18,327] [    INFO][0m -   test_samples_per_second =     87.122[0m
[32m[2022-09-05 11:34:18,327] [    INFO][0m -   test_steps_per_second   =     10.892[0m
[33m[2022-09-05 11:34:35,307] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-09-05 11:34:35,307] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-05 11:34:35,308] [    INFO][0m - ============================================================[0m
[32m[2022-09-05 11:34:35,308] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-05 11:34:35,308] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-05 11:34:35,308] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-05 11:34:35,308] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-05 11:34:35,308] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-05 11:34:35,308] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-05 11:34:35,308] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-09-05 11:34:35,308] [    INFO][0m - [0m
[32m[2022-09-05 11:34:35,308] [    INFO][0m - ============================================================[0m
[32m[2022-09-05 11:34:35,309] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-05 11:34:35,309] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-05 11:34:35,309] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-05 11:34:35,309] [    INFO][0m - prompt                        :[0m
[32m[2022-09-05 11:34:35,309] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-05 11:34:35,309] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-05 11:34:35,309] [    INFO][0m - task_name                     :chid[0m
[32m[2022-09-05 11:34:35,309] [    INFO][0m - [0m
[32m[2022-09-05 11:34:35,310] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
[32m[2022-09-05 11:34:36,830] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-09-05 11:34:36,909] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-09-05 11:34:36,909] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[32m[2022-09-05 11:34:36,910] [    INFO][0m - Using template: [{'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': 'ÊàêËØ≠'}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': '‰ΩøÁî®ÊòØÂê¶Ê≠£Á°ÆÔºü'}, {'add_prefix_space': '', 'mask': None}][0m
2022-09-05 11:34:36,912 INFO [download.py:119] unique_endpoints {''}
[32m[2022-09-05 11:34:37,169] [    INFO][0m - ============================================================[0m
[32m[2022-09-05 11:34:37,169] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-05 11:34:37,169] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-05 11:34:37,169] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-05 11:34:37,169] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-05 11:34:37,170] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-05 11:34:37,170] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-05 11:34:37,170] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-05 11:34:37,170] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-05 11:34:37,170] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-05 11:34:37,170] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-05 11:34:37,170] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-05 11:34:37,170] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-05 11:34:37,170] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-05 11:34:37,170] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-05 11:34:37,170] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-05 11:34:37,170] [    INFO][0m - do_predict                    :False[0m
[32m[2022-09-05 11:34:37,170] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-05 11:34:37,171] [    INFO][0m - eval_batch_size               :8[0m
[32m[2022-09-05 11:34:37,171] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-09-05 11:34:37,171] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-09-05 11:34:37,171] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-05 11:34:37,171] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-05 11:34:37,171] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-05 11:34:37,171] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-05 11:34:37,171] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-05 11:34:37,171] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-05 11:34:37,171] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-05 11:34:37,171] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-05 11:34:37,171] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-05 11:34:37,171] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-09-05 11:34:37,172] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-05 11:34:37,172] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-05 11:34:37,172] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-05 11:34:37,172] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-05 11:34:37,172] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-05 11:34:37,172] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-05 11:34:37,172] [    INFO][0m - logging_dir                   :./checkpoints_chid/runs/Sep05_11-34-35_instance-3bwob41y-01[0m
[32m[2022-09-05 11:34:37,172] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-05 11:34:37,172] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-05 11:34:37,172] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-05 11:34:37,172] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-05 11:34:37,172] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-05 11:34:37,172] [    INFO][0m - max_seq_length                :256[0m
[32m[2022-09-05 11:34:37,172] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-05 11:34:37,173] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-05 11:34:37,173] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-05 11:34:37,173] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-05 11:34:37,173] [    INFO][0m - num_train_epochs              :20.0[0m
[32m[2022-09-05 11:34:37,173] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-05 11:34:37,173] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-05 11:34:37,173] [    INFO][0m - output_dir                    :./checkpoints_chid/[0m
[32m[2022-09-05 11:34:37,173] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-05 11:34:37,173] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-05 11:34:37,173] [    INFO][0m - per_device_eval_batch_size    :8[0m
[32m[2022-09-05 11:34:37,173] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-09-05 11:34:37,173] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-05 11:34:37,173] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-05 11:34:37,174] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-05 11:34:37,174] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-09-05 11:34:37,174] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-05 11:34:37,174] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-05 11:34:37,174] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-05 11:34:37,174] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-05 11:34:37,174] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-05 11:34:37,174] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-05 11:34:37,174] [    INFO][0m - run_name                      :./checkpoints_chid/[0m
[32m[2022-09-05 11:34:37,174] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-05 11:34:37,174] [    INFO][0m - save_steps                    :100[0m
[32m[2022-09-05 11:34:37,174] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-09-05 11:34:37,174] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-05 11:34:37,174] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-05 11:34:37,175] [    INFO][0m - seed                          :42[0m
[32m[2022-09-05 11:34:37,175] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-05 11:34:37,175] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-05 11:34:37,175] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-05 11:34:37,175] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-09-05 11:34:37,175] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-05 11:34:37,175] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-05 11:34:37,175] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-05 11:34:37,175] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-05 11:34:37,175] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-05 11:34:37,175] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-05 11:34:37,175] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-05 11:34:37,175] [    INFO][0m - [0m
[32m[2022-09-05 11:34:37,177] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-05 11:34:37,177] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-05 11:34:37,177] [    INFO][0m -   Num Epochs = 20[0m
[32m[2022-09-05 11:34:37,177] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-09-05 11:34:37,177] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-09-05 11:34:37,177] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-05 11:34:37,178] [    INFO][0m -   Total optimization steps = 3540.0[0m
[32m[2022-09-05 11:34:37,178] [    INFO][0m -   Total num train samples = 28280[0m
[32m[2022-09-05 11:34:38,786] [    INFO][0m - loss: 3.46974297, learning_rate: 2.9915254237288134e-05, global_step: 10, interval_runtime: 1.6062, interval_samples_per_second: 4.981, interval_steps_per_second: 6.226, epoch: 0.0565[0m
[32m[2022-09-05 11:34:40,286] [    INFO][0m - loss: 0.77083635, learning_rate: 2.9830508474576274e-05, global_step: 20, interval_runtime: 1.5012, interval_samples_per_second: 5.329, interval_steps_per_second: 6.661, epoch: 0.113[0m
[32m[2022-09-05 11:34:41,780] [    INFO][0m - loss: 0.71471834, learning_rate: 2.9745762711864407e-05, global_step: 30, interval_runtime: 1.4937, interval_samples_per_second: 5.356, interval_steps_per_second: 6.695, epoch: 0.1695[0m
[32m[2022-09-05 11:34:43,274] [    INFO][0m - loss: 0.616049, learning_rate: 2.9661016949152544e-05, global_step: 40, interval_runtime: 1.4945, interval_samples_per_second: 5.353, interval_steps_per_second: 6.691, epoch: 0.226[0m
[32m[2022-09-05 11:34:44,769] [    INFO][0m - loss: 0.44438157, learning_rate: 2.9576271186440677e-05, global_step: 50, interval_runtime: 1.4955, interval_samples_per_second: 5.349, interval_steps_per_second: 6.687, epoch: 0.2825[0m
[32m[2022-09-05 11:34:46,263] [    INFO][0m - loss: 0.34250193, learning_rate: 2.9491525423728817e-05, global_step: 60, interval_runtime: 1.4932, interval_samples_per_second: 5.358, interval_steps_per_second: 6.697, epoch: 0.339[0m
[32m[2022-09-05 11:34:47,760] [    INFO][0m - loss: 0.81271687, learning_rate: 2.940677966101695e-05, global_step: 70, interval_runtime: 1.4974, interval_samples_per_second: 5.343, interval_steps_per_second: 6.678, epoch: 0.3955[0m
[32m[2022-09-05 11:34:49,254] [    INFO][0m - loss: 0.55048575, learning_rate: 2.9322033898305087e-05, global_step: 80, interval_runtime: 1.4936, interval_samples_per_second: 5.356, interval_steps_per_second: 6.695, epoch: 0.452[0m
[32m[2022-09-05 11:34:50,749] [    INFO][0m - loss: 0.45739884, learning_rate: 2.923728813559322e-05, global_step: 90, interval_runtime: 1.495, interval_samples_per_second: 5.351, interval_steps_per_second: 6.689, epoch: 0.5085[0m
[32m[2022-09-05 11:34:52,245] [    INFO][0m - loss: 0.47731266, learning_rate: 2.9152542372881356e-05, global_step: 100, interval_runtime: 1.4961, interval_samples_per_second: 5.347, interval_steps_per_second: 6.684, epoch: 0.565[0m
[32m[2022-09-05 11:34:52,245] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 11:34:52,246] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-05 11:34:52,246] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 11:34:52,246] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 11:34:52,246] [    INFO][0m -   Total prediction steps = 177[0m
[32m[2022-09-05 11:35:03,494] [    INFO][0m - eval_loss: 0.4141439199447632, eval_accuracy: 0.21782178217821782, eval_runtime: 11.2477, eval_samples_per_second: 125.714, eval_steps_per_second: 15.737, epoch: 0.565[0m
[32m[2022-09-05 11:35:03,520] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-100[0m
[32m[2022-09-05 11:35:03,521] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 11:35:06,684] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-05 11:35:06,685] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-05 11:35:13,776] [    INFO][0m - loss: 0.46169529, learning_rate: 2.9067796610169493e-05, global_step: 110, interval_runtime: 21.531, interval_samples_per_second: 0.372, interval_steps_per_second: 0.464, epoch: 0.6215[0m
[32m[2022-09-05 11:35:15,284] [    INFO][0m - loss: 0.38735704, learning_rate: 2.8983050847457626e-05, global_step: 120, interval_runtime: 1.5077, interval_samples_per_second: 5.306, interval_steps_per_second: 6.633, epoch: 0.678[0m
[32m[2022-09-05 11:35:16,785] [    INFO][0m - loss: 0.47157621, learning_rate: 2.8898305084745763e-05, global_step: 130, interval_runtime: 1.5013, interval_samples_per_second: 5.329, interval_steps_per_second: 6.661, epoch: 0.7345[0m
[32m[2022-09-05 11:35:18,284] [    INFO][0m - loss: 0.51539459, learning_rate: 2.88135593220339e-05, global_step: 140, interval_runtime: 1.4982, interval_samples_per_second: 5.34, interval_steps_per_second: 6.675, epoch: 0.791[0m
[32m[2022-09-05 11:35:19,788] [    INFO][0m - loss: 0.59066877, learning_rate: 2.8728813559322036e-05, global_step: 150, interval_runtime: 1.5042, interval_samples_per_second: 5.319, interval_steps_per_second: 6.648, epoch: 0.8475[0m
[32m[2022-09-05 11:35:21,289] [    INFO][0m - loss: 0.62180929, learning_rate: 2.864406779661017e-05, global_step: 160, interval_runtime: 1.5016, interval_samples_per_second: 5.327, interval_steps_per_second: 6.659, epoch: 0.904[0m
[32m[2022-09-05 11:35:22,801] [    INFO][0m - loss: 0.4092104, learning_rate: 2.855932203389831e-05, global_step: 170, interval_runtime: 1.5121, interval_samples_per_second: 5.291, interval_steps_per_second: 6.614, epoch: 0.9605[0m
[32m[2022-09-05 11:35:24,341] [    INFO][0m - loss: 0.43905864, learning_rate: 2.8474576271186442e-05, global_step: 180, interval_runtime: 1.5396, interval_samples_per_second: 5.196, interval_steps_per_second: 6.495, epoch: 1.0169[0m
[32m[2022-09-05 11:35:25,840] [    INFO][0m - loss: 0.57048426, learning_rate: 2.8389830508474575e-05, global_step: 190, interval_runtime: 1.499, interval_samples_per_second: 5.337, interval_steps_per_second: 6.671, epoch: 1.0734[0m
[32m[2022-09-05 11:35:27,339] [    INFO][0m - loss: 0.48533096, learning_rate: 2.8305084745762712e-05, global_step: 200, interval_runtime: 1.4989, interval_samples_per_second: 5.337, interval_steps_per_second: 6.672, epoch: 1.1299[0m
[32m[2022-09-05 11:35:27,339] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 11:35:27,339] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-05 11:35:27,339] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 11:35:27,339] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 11:35:27,339] [    INFO][0m -   Total prediction steps = 177[0m
====================
[2 2 4 3 2 0 3 2 3 5 3 0 2 5 6 6 6 5 3 6 0 1 0 2 0 4 5 1 6 3 5 2 3 6 3 1 5
 3 5 5 5 3 0 5 3 5 0 2 0 5 1 3 2 6 4 0 2 3 2 6 5 1 3 1 3 0 0 1 5 5 3 5 0 4
 1 3 1 4 1 5 0 2 1 3 4 1 1 4 6 0 4 4 5 2 5 0 3 4 6 2 0 0 2 1 3 0 1 5 0 6 2
 2 4 2 2 6 2 5 3 3 5 1 3 1 6 1 1 3 5 2 0 2 1 4 6 4 1 5 4 5 1 5 3 1 5 4 4 4
 5 1 1 6 2 5 3 1 2 1 6 2 4 2 2 3 6 0 2 5 5 0 5 1 5 1 5 4 2 2 6 1 6 3 2 5 4
 5 5 6 4 4 5 3 2 2 5 0 4 1 2 0 2 6]
[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3
 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6
 6 6 6 6 6 6 6 6 6 6 6 6 6 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4
 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
[6 5 5 0 1 2 3 4 2 3 5 4 4 4 4 6 4 3 5 0 4 4 0 3 2 3 4 6 1 6 0 2 5 4 2 1 1
 0 4 4 1 2 3 6 5 0 6 0 1 5 6 4 5 2 5 6 1 3 1 3 3 3 6 1 3 0 1 3 0 0 3 2 0 1
 5 1 3 0 5 5 2 0 3 3 2 1 3 0 6 0 2 0 3 1 2 6 0 1 3 6 3 3 6 1 0 5 3 0 5 2 5
 5 2 1 2 1 0 0 5 4 0 0 4 6 4 5 1 6 1 6 5 6 0 6 2 0 3 6 1 1 0 2 2 1 1 5 4 5
 5 5 0 6 4 1 2 6 5 5 4 6 4 3 0 6 3 5 4 4 1 4 4 2 6 4 4 6 5 0 5 3 2 0 1 3 0
 5 5 3 4 6 5 1 0 3 6 4 4 2 0 3 2 3]
[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3
 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6
 6 6 6 6 6 6 6 6 6 6 6 6 6 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4
 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
[3 1 0 5 3 1 3 3 5 1 3 4 4 3 5 1 6 0 3 0 3 4 1 4 6 3 2 2 5 1 4 0 4 2 4 3 2
 0 4 6 3 0 1 6 4 0 2 0 1 5 3 1 5 2 6 0 1 4 1 6 4 3 1 1 2 4 4 6 3 6 4 5 4 5
 6 3 0 3 4 0 6 0 3 6 4 4 2 4 0 5 0 2 0 1 4 6 2 4 5 2 6 3 6 1 3 1 1 1 1 1 2
 2 4 0 5 6 0 6 3 3 0 0 1 1 4 5 5 0 6 2 0 6 0 5 1 2 5 2 4 5 0 1 2 3 5 0 5 0
 4 1 5 5 1 1 3 1 0 3 6 2 1 2 2 4 0 5 2 2 2 2 4 2 4 5 2 6 4 2 3 0 1 4 1 5 0
 5 4 4 4 6 5 3 3 2 4 2 4 2 2 0 0 3]
[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3
 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6
 6 6 6 6 6 6 6 6 6 6 6 6 6 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4
 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
[3 5 0 0 0 2 3 6 2 3 6 5 0 6 4 4 1 6 3 4 4 6 4 2 6 2 2 4 2 4 4 2 5 2 1 0 4
 1 4 3 3 3 6 0 3 5 2 2 1 1 3 4 5 0 3 6 0 4 1 0 6 6 0 2 2 6 2 3 2 6 6 6 6 3
 1 6 3 1 1 4 4 6 3 1 1 4 0 4 6 5 4 0 0 0 4 4 4 3 2 5 3 0 5 4 3 5 6 0 5 1 4
 1 4 6 3 4 4 2 5 4 6 0 4 0 5 5 4 2 1 5 5 4 1 6 6 4 6 0 4 2 0 5 2 0 2 5 6 0
 2 2 0 0 1 0 1 0 0 5 1 5 3 0 0 0 0 0 6 1 0 4 6 4 4 5 0 3 1 3 0 5 1 5 3 4 0
 5 1 3 2 1 6 6 1 1 2 1 5 1 1 2 0 3]
[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3
 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6
 6 6 6 6 6 6 6 6 6 6 6 6 6 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4
 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
[5 3 6 1 5 0 5 3 2 0 4 1 1 2 0 1 1 1 6 2 4 6 2 4 2 0 5 2 4 3 6 3 6 5 6 3 2
 0 6 3 5 5 2 0 0 1 3 4 1 6 4 3 0 2 3 4 4 1 5 4 6 2 3 3 4 2 4 6 6 4 0 4 1 2
 5 2 3 1 0 0 6 4 2 0 2 3 6 6 3 1 2 2 1 2 4 4 2 0 1 5 6 4 0 1 1 2 2 2 1 1 4
 0 4 5 1 4 0 3 0 3 6 3 2 1 5 3 1 4 4 3 6 4 4 5 1 2 0 5 1 3 2 6 2 6 3 2 1 0
 4 0 5 4 2 2 3 3 2 3 1 5 2 4 2 0 1 3 6 1 0 4 3 5 2 6 5 6 5 1 0 5 5 1 3 2 0
 4 0 2 6 6 3 6 4 6 4 1 6 4 2 1 0 5]
[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3
 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6
 6 6 6 6 6 6 6 6 6 6 6 6 6 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4
 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
[5 3 0 2 2 0 1 2 0 1 4 1 1 2 3 3 1 2 0 6 3 2 1 4 5 1 4 2 5 4 6 3 2 6 1 5 0
 3 0 3 4 0 2 1 0 2 3 3 3 1 5 3 6 2 3 6 4 1 2 4 6 6 3 1 4 5 3 6 1 6 0 4 1 5
 6 3 3 5 5 3 2 4 2 6 2 4 6 6 3 1 2 3 1 4 5 5 2 3 1 4 1 2 6 5 0 2 1 2 1 1 4
 2 4 0 1 5 1 5 0 2 5 4 2 1 5 5 1 4 4 2 6 4 4 5 3 2 5 5 1 3 3 6 5 6 6 0 2 0
 4 3 3 1 5 2 3 6 0 6 1 5 5 4 1 0 1 6 1 3 0 1 6 5 2 0 2 4 4 1 1 0 1 0 2 2 2
 4 1 2 6 1 3 0 1 6 4 1 1 4 0 6 0 3]
[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3
 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6
 6 6 6 6 6 6 6 6 6 6 6 6 6 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4
 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
[1 0 1 1 5 2 2 2 0 2 1 1 5 6 1 3 1 1 3 6 0 3 5 4 5 1 4 2 2 4 5 3 2 5 1 1 3
 3 2 3 5 2 2 1 1 6 2 5 0 6 4 1 0 1 1 5 0 1 3 4 5 6 6 1 2 4 3 3 3 6 5 6 1 5
 2 6 3 6 3 3 2 0 5 5 6 0 6 4 3 5 3 2 1 0 5 3 2 0 5 1 1 6 0 4 0 3 4 1 4 6 4
 1 3 3 0 2 5 5 3 0 5 4 2 6 3 5 1 6 1 5 2 4 2 3 5 3 2 5 2 1 6 6 5 3 4 2 3 6
 4 5 5 1 5 4 3 6 0 2 1 5 5 1 2 5 3 5 6 1 5 3 6 5 2 5 2 5 2 0 1 4 1 0 3 1 6
 5 1 2 0 5 3 6 0 3 3 0 3 2 0 1 0 3]
[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3
 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6
 6 6 6 6 6 6 6 6 6 6 6 6 6 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4
 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
[1 3 6 2 0 0 0 6 2 4 1 2 5 2 0 4 2 4 6 3 4 2 1 0 5 0 1 4 6 1 6 4 4 0 5 4 5
 4 2 1 0 1 5 1 3 5 3 4 3 0 1 2 1 6 0 5 0 1 1 3 6 6 2 3 0 3 0 6 6 0 5 4 5 4
 3 2 1 1 4 3 2 3 2 6 0 3 6 4 3 1 2 1 1 0 2 1 1 3 1 4 0 4 6 6 5 2 2 1 5 5 4
 0 6 6 4 4 4 3 0 2 0 4 2 4 3 5 4 0 6 0 6 3 1 6 0 2 1 1 2 4 2 5 6 1 4 3 2 0
 4 1 0 5 3 6 3 1 2 2 3 3 5 4 5 0 0 3 1 3 0 4 3 6 6 0 5 5 0 1 0 5 5 0 1 4 2
 4 3 5 0 1 3 5 3 5 2 1 2 6 5 1 5 2]
[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3
 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6
 6 6 6 6 6 6 6 6 6 6 6 6 6 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4
 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
(14014, 2)
14014
0
= 0 ===================
[('Èîô', 12012)]
----------
[('ÂØπ', 12012)]
----------
[('Âæó', 12012)]
----------
[('Ëøá', 3868), ('‰∫∫', 3837), ('Êúâ', 3022), ('Áúü', 644), ('Áõ∏', 473), ('Â§ß', 112), ('Â•Ω', 46), ('ÂèØ', 10)]
----------
[('‰∫∫', 4593), ('Ëøá', 2965), ('Êúâ', 2112), ('Áõ∏', 1028), ('Áúü', 798), ('Â§ß', 344), ('Â•Ω', 121), ('ÂèØ', 51)]
----------
1
= 0 ===================
[('Èîô', 2002)]
----------
[('ÂØπ', 2002)]
----------
[('Âæó', 2002)]
----------
[('‰∫∫', 637), ('Êúâ', 589), ('Ëøá', 572), ('Áúü', 107), ('Áõ∏', 74), ('Â§ß', 14), ('Â•Ω', 7), ('ÂèØ', 2)]
----------
[('‰∫∫', 778), ('Ëøá', 487), ('Êúâ', 366), ('Áõ∏', 151), ('Áúü', 141), ('Â§ß', 49), ('Â•Ω', 22), ('ÂèØ', 8)]
----------
====================
[0 4 5 3 6 2 3 5 5 2 2 1 6 3 5 6 3 2 2 1 2 0 5 3 0 6 2 3 2 3 6 1 3 5 3 5 6
 1 1 1 4 0 2 1 6 3 1 4 5 1 2 4 5 3 6 6 5 3 0 5 6 0 3 3 0 5 4 3 1 3 6 4 6 1
 5 3 1 6 4 3 2 6 4 6 0 1 0 3 5 0 1 1 3 5 4 3 1 5 1 6 4 4 2 6 0 4 5 0 4 0 4
 6 4 2 4 5 4 3 0 0 3 6 6 3 4 0 6 0 6 4 4 5 2 2 1 5 6 5 2 5 4 0 5 3 5 6 4 4
 3 3 1 4 0 6 1 3 6 1 2 0 0 6 2 1 4 6 6 4 2 4 0 5 4 0 0 6 1 3 2 5 3 3 4 1 3
 2 3 0 2 1 3 5 1 1 4 6 3 6 2 1 6 5]
[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3
 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6
 6 6 6 6 6 6 6 6 6 6 6 6 6 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4
 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
[32m[2022-09-05 11:35:38,875] [    INFO][0m - eval_loss: 0.415054589509964, eval_accuracy: 0.12871287128712872, eval_runtime: 11.5348, eval_samples_per_second: 122.586, eval_steps_per_second: 15.345, epoch: 1.1299[0m
[32m[2022-09-05 11:35:38,905] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-200[0m
[32m[2022-09-05 11:35:38,905] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 11:35:42,393] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-200/tokenizer_config.json[0m
[32m[2022-09-05 11:35:42,393] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-200/special_tokens_map.json[0m
[32m[2022-09-05 11:35:49,645] [    INFO][0m - loss: 0.38213367, learning_rate: 2.822033898305085e-05, global_step: 210, interval_runtime: 22.3063, interval_samples_per_second: 0.359, interval_steps_per_second: 0.448, epoch: 1.1864[0m
[32m[2022-09-05 11:35:51,137] [    INFO][0m - loss: 0.6534791, learning_rate: 2.8135593220338985e-05, global_step: 220, interval_runtime: 1.4927, interval_samples_per_second: 5.359, interval_steps_per_second: 6.699, epoch: 1.2429[0m
[32m[2022-09-05 11:35:52,633] [    INFO][0m - loss: 0.43375726, learning_rate: 2.805084745762712e-05, global_step: 230, interval_runtime: 1.4953, interval_samples_per_second: 5.35, interval_steps_per_second: 6.688, epoch: 1.2994[0m
[32m[2022-09-05 11:35:54,131] [    INFO][0m - loss: 0.52690296, learning_rate: 2.7966101694915255e-05, global_step: 240, interval_runtime: 1.4975, interval_samples_per_second: 5.342, interval_steps_per_second: 6.678, epoch: 1.3559[0m
[32m[2022-09-05 11:35:55,629] [    INFO][0m - loss: 0.29841199, learning_rate: 2.788135593220339e-05, global_step: 250, interval_runtime: 1.4985, interval_samples_per_second: 5.339, interval_steps_per_second: 6.673, epoch: 1.4124[0m
[32m[2022-09-05 11:35:57,126] [    INFO][0m - loss: 0.74612637, learning_rate: 2.7796610169491528e-05, global_step: 260, interval_runtime: 1.4968, interval_samples_per_second: 5.345, interval_steps_per_second: 6.681, epoch: 1.4689[0m
[32m[2022-09-05 11:35:58,618] [    INFO][0m - loss: 0.51046457, learning_rate: 2.771186440677966e-05, global_step: 270, interval_runtime: 1.492, interval_samples_per_second: 5.362, interval_steps_per_second: 6.702, epoch: 1.5254[0m
[32m[2022-09-05 11:36:00,111] [    INFO][0m - loss: 0.56918936, learning_rate: 2.7627118644067794e-05, global_step: 280, interval_runtime: 1.4936, interval_samples_per_second: 5.356, interval_steps_per_second: 6.695, epoch: 1.5819[0m
[32m[2022-09-05 11:36:01,607] [    INFO][0m - loss: 0.35561588, learning_rate: 2.7542372881355934e-05, global_step: 290, interval_runtime: 1.4952, interval_samples_per_second: 5.35, interval_steps_per_second: 6.688, epoch: 1.6384[0m
[32m[2022-09-05 11:36:03,102] [    INFO][0m - loss: 0.5418467, learning_rate: 2.7457627118644068e-05, global_step: 300, interval_runtime: 1.4953, interval_samples_per_second: 5.35, interval_steps_per_second: 6.688, epoch: 1.6949[0m
[32m[2022-09-05 11:36:03,102] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 11:36:03,102] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-05 11:36:03,103] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 11:36:03,103] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 11:36:03,103] [    INFO][0m -   Total prediction steps = 177[0m
[32m[2022-09-05 11:36:14,334] [    INFO][0m - eval_loss: 0.5484908223152161, eval_accuracy: 0.19801980198019803, eval_runtime: 11.2304, eval_samples_per_second: 125.908, eval_steps_per_second: 15.761, epoch: 1.6949[0m
[32m[2022-09-05 11:36:14,360] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-300[0m
[32m[2022-09-05 11:36:14,360] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 11:36:17,814] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-300/tokenizer_config.json[0m
[32m[2022-09-05 11:36:17,814] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-300/special_tokens_map.json[0m
[32m[2022-09-05 11:36:26,323] [    INFO][0m - loss: 0.4658905, learning_rate: 2.7372881355932204e-05, global_step: 310, interval_runtime: 23.2212, interval_samples_per_second: 0.345, interval_steps_per_second: 0.431, epoch: 1.7514[0m
[32m[2022-09-05 11:36:27,818] [    INFO][0m - loss: 0.44517622, learning_rate: 2.7288135593220337e-05, global_step: 320, interval_runtime: 1.4948, interval_samples_per_second: 5.352, interval_steps_per_second: 6.69, epoch: 1.8079[0m
[32m[2022-09-05 11:36:29,318] [    INFO][0m - loss: 0.71638575, learning_rate: 2.7203389830508477e-05, global_step: 330, interval_runtime: 1.5004, interval_samples_per_second: 5.332, interval_steps_per_second: 6.665, epoch: 1.8644[0m
[32m[2022-09-05 11:36:30,820] [    INFO][0m - loss: 0.38807948, learning_rate: 2.711864406779661e-05, global_step: 340, interval_runtime: 1.5012, interval_samples_per_second: 5.329, interval_steps_per_second: 6.661, epoch: 1.9209[0m
[32m[2022-09-05 11:36:32,319] [    INFO][0m - loss: 0.37487669, learning_rate: 2.7033898305084747e-05, global_step: 350, interval_runtime: 1.5, interval_samples_per_second: 5.333, interval_steps_per_second: 6.667, epoch: 1.9774[0m
[32m[2022-09-05 11:36:33,831] [    INFO][0m - loss: 0.46040311, learning_rate: 2.6949152542372884e-05, global_step: 360, interval_runtime: 1.5115, interval_samples_per_second: 5.293, interval_steps_per_second: 6.616, epoch: 2.0339[0m
[32m[2022-09-05 11:36:35,331] [    INFO][0m - loss: 0.3488337, learning_rate: 2.6864406779661017e-05, global_step: 370, interval_runtime: 1.5, interval_samples_per_second: 5.333, interval_steps_per_second: 6.667, epoch: 2.0904[0m
[32m[2022-09-05 11:36:36,829] [    INFO][0m - loss: 0.48037419, learning_rate: 2.6779661016949153e-05, global_step: 380, interval_runtime: 1.4981, interval_samples_per_second: 5.34, interval_steps_per_second: 6.675, epoch: 2.1469[0m
[32m[2022-09-05 11:36:38,335] [    INFO][0m - loss: 0.48027372, learning_rate: 2.6694915254237287e-05, global_step: 390, interval_runtime: 1.5061, interval_samples_per_second: 5.312, interval_steps_per_second: 6.64, epoch: 2.2034[0m
[32m[2022-09-05 11:36:39,834] [    INFO][0m - loss: 0.4937748, learning_rate: 2.6610169491525427e-05, global_step: 400, interval_runtime: 1.4989, interval_samples_per_second: 5.337, interval_steps_per_second: 6.671, epoch: 2.2599[0m
[32m[2022-09-05 11:36:39,835] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 11:36:39,835] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-05 11:36:39,835] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 11:36:39,835] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 11:36:39,835] [    INFO][0m -   Total prediction steps = 177[0m
[32m[2022-09-05 11:36:51,049] [    INFO][0m - eval_loss: 0.5423343777656555, eval_accuracy: 0.24257425742574257, eval_runtime: 11.2131, eval_samples_per_second: 126.102, eval_steps_per_second: 15.785, epoch: 2.2599[0m
[32m[2022-09-05 11:36:51,078] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-400[0m
[32m[2022-09-05 11:36:51,078] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 11:36:54,607] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-400/tokenizer_config.json[0m
[32m[2022-09-05 11:36:54,607] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-400/special_tokens_map.json[0m
[32m[2022-09-05 11:37:01,785] [    INFO][0m - loss: 0.60880041, learning_rate: 2.652542372881356e-05, global_step: 410, interval_runtime: 21.9507, interval_samples_per_second: 0.364, interval_steps_per_second: 0.456, epoch: 2.3164[0m
[32m[2022-09-05 11:37:03,282] [    INFO][0m - loss: 0.40947695, learning_rate: 2.6440677966101696e-05, global_step: 420, interval_runtime: 1.4972, interval_samples_per_second: 5.343, interval_steps_per_second: 6.679, epoch: 2.3729[0m
[32m[2022-09-05 11:37:04,782] [    INFO][0m - loss: 0.47311788, learning_rate: 2.635593220338983e-05, global_step: 430, interval_runtime: 1.4991, interval_samples_per_second: 5.336, interval_steps_per_second: 6.67, epoch: 2.4294[0m
[32m[2022-09-05 11:37:06,280] [    INFO][0m - loss: 0.52602897, learning_rate: 2.627118644067797e-05, global_step: 440, interval_runtime: 1.4991, interval_samples_per_second: 5.336, interval_steps_per_second: 6.671, epoch: 2.4859[0m
[32m[2022-09-05 11:37:07,779] [    INFO][0m - loss: 0.46317453, learning_rate: 2.6186440677966103e-05, global_step: 450, interval_runtime: 1.4983, interval_samples_per_second: 5.339, interval_steps_per_second: 6.674, epoch: 2.5424[0m
[32m[2022-09-05 11:37:09,273] [    INFO][0m - loss: 0.55899029, learning_rate: 2.6101694915254236e-05, global_step: 460, interval_runtime: 1.4947, interval_samples_per_second: 5.352, interval_steps_per_second: 6.69, epoch: 2.5989[0m
[32m[2022-09-05 11:37:10,766] [    INFO][0m - loss: 0.53318205, learning_rate: 2.6016949152542372e-05, global_step: 470, interval_runtime: 1.4922, interval_samples_per_second: 5.361, interval_steps_per_second: 6.701, epoch: 2.6554[0m
[32m[2022-09-05 11:37:12,264] [    INFO][0m - loss: 0.4428144, learning_rate: 2.593220338983051e-05, global_step: 480, interval_runtime: 1.4989, interval_samples_per_second: 5.337, interval_steps_per_second: 6.672, epoch: 2.7119[0m
[32m[2022-09-05 11:37:13,761] [    INFO][0m - loss: 0.46599197, learning_rate: 2.5847457627118646e-05, global_step: 490, interval_runtime: 1.4967, interval_samples_per_second: 5.345, interval_steps_per_second: 6.681, epoch: 2.7684[0m
[32m[2022-09-05 11:37:15,262] [    INFO][0m - loss: 0.37289989, learning_rate: 2.576271186440678e-05, global_step: 500, interval_runtime: 1.5004, interval_samples_per_second: 5.332, interval_steps_per_second: 6.665, epoch: 2.8249[0m
[32m[2022-09-05 11:37:15,262] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 11:37:15,262] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-05 11:37:15,262] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 11:37:15,262] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 11:37:15,263] [    INFO][0m -   Total prediction steps = 177[0m
[32m[2022-09-05 11:37:26,512] [    INFO][0m - eval_loss: 0.4475823640823364, eval_accuracy: 0.2871287128712871, eval_runtime: 11.2487, eval_samples_per_second: 125.703, eval_steps_per_second: 15.735, epoch: 2.8249[0m
[32m[2022-09-05 11:37:26,538] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-500[0m
[32m[2022-09-05 11:37:26,538] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 11:37:29,855] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-500/tokenizer_config.json[0m
[32m[2022-09-05 11:37:29,855] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-500/special_tokens_map.json[0m
[32m[2022-09-05 11:37:37,246] [    INFO][0m - loss: 0.40141191, learning_rate: 2.567796610169492e-05, global_step: 510, interval_runtime: 21.978, interval_samples_per_second: 0.364, interval_steps_per_second: 0.455, epoch: 2.8814[0m
[32m[2022-09-05 11:37:38,740] [    INFO][0m - loss: 0.37493463, learning_rate: 2.5593220338983052e-05, global_step: 520, interval_runtime: 1.5008, interval_samples_per_second: 5.331, interval_steps_per_second: 6.663, epoch: 2.9379[0m
[32m[2022-09-05 11:37:40,230] [    INFO][0m - loss: 0.29610939, learning_rate: 2.550847457627119e-05, global_step: 530, interval_runtime: 1.4895, interval_samples_per_second: 5.371, interval_steps_per_second: 6.714, epoch: 2.9944[0m
[32m[2022-09-05 11:37:41,741] [    INFO][0m - loss: 0.41440468, learning_rate: 2.5423728813559322e-05, global_step: 540, interval_runtime: 1.5112, interval_samples_per_second: 5.294, interval_steps_per_second: 6.617, epoch: 3.0508[0m
[32m[2022-09-05 11:37:43,243] [    INFO][0m - loss: 0.46251478, learning_rate: 2.5338983050847458e-05, global_step: 550, interval_runtime: 1.5016, interval_samples_per_second: 5.328, interval_steps_per_second: 6.66, epoch: 3.1073[0m
[32m[2022-09-05 11:37:44,743] [    INFO][0m - loss: 0.50406737, learning_rate: 2.5254237288135595e-05, global_step: 560, interval_runtime: 1.5002, interval_samples_per_second: 5.333, interval_steps_per_second: 6.666, epoch: 3.1638[0m
[32m[2022-09-05 11:37:46,243] [    INFO][0m - loss: 0.47796688, learning_rate: 2.5169491525423728e-05, global_step: 570, interval_runtime: 1.4997, interval_samples_per_second: 5.335, interval_steps_per_second: 6.668, epoch: 3.2203[0m
[32m[2022-09-05 11:37:47,738] [    INFO][0m - loss: 0.44546041, learning_rate: 2.5084745762711865e-05, global_step: 580, interval_runtime: 1.4959, interval_samples_per_second: 5.348, interval_steps_per_second: 6.685, epoch: 3.2768[0m
[32m[2022-09-05 11:37:49,238] [    INFO][0m - loss: 0.47660594, learning_rate: 2.5e-05, global_step: 590, interval_runtime: 1.4991, interval_samples_per_second: 5.337, interval_steps_per_second: 6.671, epoch: 3.3333[0m
[32m[2022-09-05 11:37:50,730] [    INFO][0m - loss: 0.46048551, learning_rate: 2.4915254237288138e-05, global_step: 600, interval_runtime: 1.4922, interval_samples_per_second: 5.361, interval_steps_per_second: 6.702, epoch: 3.3898[0m
[32m[2022-09-05 11:37:50,730] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 11:37:50,730] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-05 11:37:50,730] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 11:37:50,730] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 11:37:50,730] [    INFO][0m -   Total prediction steps = 177[0m
[32m[2022-09-05 11:38:02,053] [    INFO][0m - eval_loss: 0.4113209545612335, eval_accuracy: 0.12376237623762376, eval_runtime: 11.3218, eval_samples_per_second: 124.892, eval_steps_per_second: 15.634, epoch: 3.3898[0m
[32m[2022-09-05 11:38:02,083] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-600[0m
[32m[2022-09-05 11:38:02,083] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 11:38:05,585] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-600/tokenizer_config.json[0m
[32m[2022-09-05 11:38:05,586] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-600/special_tokens_map.json[0m
[32m[2022-09-05 11:38:12,619] [    INFO][0m - loss: 0.40124063, learning_rate: 2.483050847457627e-05, global_step: 610, interval_runtime: 21.8891, interval_samples_per_second: 0.365, interval_steps_per_second: 0.457, epoch: 3.4463[0m
[32m[2022-09-05 11:38:14,121] [    INFO][0m - loss: 0.4513207, learning_rate: 2.4745762711864408e-05, global_step: 620, interval_runtime: 1.5007, interval_samples_per_second: 5.331, interval_steps_per_second: 6.663, epoch: 3.5028[0m
[32m[2022-09-05 11:38:15,614] [    INFO][0m - loss: 0.42295337, learning_rate: 2.4661016949152544e-05, global_step: 630, interval_runtime: 1.4945, interval_samples_per_second: 5.353, interval_steps_per_second: 6.691, epoch: 3.5593[0m
[32m[2022-09-05 11:38:17,116] [    INFO][0m - loss: 0.45532393, learning_rate: 2.4576271186440677e-05, global_step: 640, interval_runtime: 1.5013, interval_samples_per_second: 5.329, interval_steps_per_second: 6.661, epoch: 3.6158[0m
[32m[2022-09-05 11:38:18,621] [    INFO][0m - loss: 0.3784389, learning_rate: 2.4491525423728814e-05, global_step: 650, interval_runtime: 1.5056, interval_samples_per_second: 5.314, interval_steps_per_second: 6.642, epoch: 3.6723[0m
[32m[2022-09-05 11:38:20,119] [    INFO][0m - loss: 0.42681689, learning_rate: 2.440677966101695e-05, global_step: 660, interval_runtime: 1.4979, interval_samples_per_second: 5.341, interval_steps_per_second: 6.676, epoch: 3.7288[0m
[32m[2022-09-05 11:38:21,619] [    INFO][0m - loss: 0.33464637, learning_rate: 2.4322033898305087e-05, global_step: 670, interval_runtime: 1.4998, interval_samples_per_second: 5.334, interval_steps_per_second: 6.668, epoch: 3.7853[0m
[32m[2022-09-05 11:38:23,121] [    INFO][0m - loss: 0.4352478, learning_rate: 2.423728813559322e-05, global_step: 680, interval_runtime: 1.4989, interval_samples_per_second: 5.337, interval_steps_per_second: 6.671, epoch: 3.8418[0m
[32m[2022-09-05 11:38:24,620] [    INFO][0m - loss: 0.41056576, learning_rate: 2.4152542372881357e-05, global_step: 690, interval_runtime: 1.5024, interval_samples_per_second: 5.325, interval_steps_per_second: 6.656, epoch: 3.8983[0m
[32m[2022-09-05 11:38:26,119] [    INFO][0m - loss: 0.45119009, learning_rate: 2.4067796610169493e-05, global_step: 700, interval_runtime: 1.4986, interval_samples_per_second: 5.338, interval_steps_per_second: 6.673, epoch: 3.9548[0m
[32m[2022-09-05 11:38:26,119] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 11:38:26,119] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-05 11:38:26,119] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 11:38:26,119] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 11:38:26,120] [    INFO][0m -   Total prediction steps = 177[0m
[32m[2022-09-05 11:38:37,506] [    INFO][0m - eval_loss: 0.41447240114212036, eval_accuracy: 0.18316831683168316, eval_runtime: 11.3859, eval_samples_per_second: 124.189, eval_steps_per_second: 15.546, epoch: 3.9548[0m
[32m[2022-09-05 11:38:37,533] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-700[0m
[32m[2022-09-05 11:38:37,533] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 11:38:40,802] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-700/tokenizer_config.json[0m
[32m[2022-09-05 11:38:40,802] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-700/special_tokens_map.json[0m
[32m[2022-09-05 11:38:47,640] [    INFO][0m - loss: 0.55089712, learning_rate: 2.3983050847457627e-05, global_step: 710, interval_runtime: 21.5214, interval_samples_per_second: 0.372, interval_steps_per_second: 0.465, epoch: 4.0113[0m
[32m[2022-09-05 11:38:49,134] [    INFO][0m - loss: 0.38232291, learning_rate: 2.3898305084745763e-05, global_step: 720, interval_runtime: 1.4937, interval_samples_per_second: 5.356, interval_steps_per_second: 6.695, epoch: 4.0678[0m
[32m[2022-09-05 11:38:50,628] [    INFO][0m - loss: 0.45994124, learning_rate: 2.3813559322033896e-05, global_step: 730, interval_runtime: 1.4942, interval_samples_per_second: 5.354, interval_steps_per_second: 6.693, epoch: 4.1243[0m
[32m[2022-09-05 11:38:52,126] [    INFO][0m - loss: 0.39526858, learning_rate: 2.3728813559322036e-05, global_step: 740, interval_runtime: 1.4983, interval_samples_per_second: 5.339, interval_steps_per_second: 6.674, epoch: 4.1808[0m
[32m[2022-09-05 11:38:53,627] [    INFO][0m - loss: 0.3814301, learning_rate: 2.364406779661017e-05, global_step: 750, interval_runtime: 1.5011, interval_samples_per_second: 5.329, interval_steps_per_second: 6.662, epoch: 4.2373[0m
[32m[2022-09-05 11:38:55,125] [    INFO][0m - loss: 0.53114839, learning_rate: 2.3559322033898306e-05, global_step: 760, interval_runtime: 1.4974, interval_samples_per_second: 5.343, interval_steps_per_second: 6.678, epoch: 4.2938[0m
[32m[2022-09-05 11:38:56,621] [    INFO][0m - loss: 0.43887959, learning_rate: 2.347457627118644e-05, global_step: 770, interval_runtime: 1.4959, interval_samples_per_second: 5.348, interval_steps_per_second: 6.685, epoch: 4.3503[0m
[32m[2022-09-05 11:38:58,114] [    INFO][0m - loss: 0.54957342, learning_rate: 2.338983050847458e-05, global_step: 780, interval_runtime: 1.4934, interval_samples_per_second: 5.357, interval_steps_per_second: 6.696, epoch: 4.4068[0m
[32m[2022-09-05 11:38:59,609] [    INFO][0m - loss: 0.52836256, learning_rate: 2.3305084745762712e-05, global_step: 790, interval_runtime: 1.4952, interval_samples_per_second: 5.35, interval_steps_per_second: 6.688, epoch: 4.4633[0m
[32m[2022-09-05 11:39:01,103] [    INFO][0m - loss: 0.44275827, learning_rate: 2.3220338983050846e-05, global_step: 800, interval_runtime: 1.4939, interval_samples_per_second: 5.355, interval_steps_per_second: 6.694, epoch: 4.5198[0m
[32m[2022-09-05 11:39:01,104] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 11:39:01,104] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-05 11:39:01,104] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 11:39:01,104] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 11:39:01,104] [    INFO][0m -   Total prediction steps = 177[0m
[32m[2022-09-05 11:39:12,419] [    INFO][0m - eval_loss: 0.4282482862472534, eval_accuracy: 0.07920792079207921, eval_runtime: 11.314, eval_samples_per_second: 124.977, eval_steps_per_second: 15.644, epoch: 4.5198[0m
[32m[2022-09-05 11:39:12,449] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-800[0m
[32m[2022-09-05 11:39:12,449] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 11:39:15,882] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-800/tokenizer_config.json[0m
[32m[2022-09-05 11:39:15,883] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-800/special_tokens_map.json[0m
[32m[2022-09-05 11:39:22,683] [    INFO][0m - loss: 0.38495739, learning_rate: 2.3135593220338986e-05, global_step: 810, interval_runtime: 21.5798, interval_samples_per_second: 0.371, interval_steps_per_second: 0.463, epoch: 4.5763[0m
[32m[2022-09-05 11:39:24,176] [    INFO][0m - loss: 0.40984097, learning_rate: 2.305084745762712e-05, global_step: 820, interval_runtime: 1.4931, interval_samples_per_second: 5.358, interval_steps_per_second: 6.697, epoch: 4.6328[0m
[32m[2022-09-05 11:39:25,667] [    INFO][0m - loss: 0.47493477, learning_rate: 2.2966101694915255e-05, global_step: 830, interval_runtime: 1.4906, interval_samples_per_second: 5.367, interval_steps_per_second: 6.709, epoch: 4.6893[0m
[32m[2022-09-05 11:39:27,152] [    INFO][0m - loss: 0.52041879, learning_rate: 2.288135593220339e-05, global_step: 840, interval_runtime: 1.4851, interval_samples_per_second: 5.387, interval_steps_per_second: 6.734, epoch: 4.7458[0m
[32m[2022-09-05 11:39:28,638] [    INFO][0m - loss: 0.50173149, learning_rate: 2.279661016949153e-05, global_step: 850, interval_runtime: 1.4861, interval_samples_per_second: 5.383, interval_steps_per_second: 6.729, epoch: 4.8023[0m
[32m[2022-09-05 11:39:30,125] [    INFO][0m - loss: 0.47340999, learning_rate: 2.271186440677966e-05, global_step: 860, interval_runtime: 1.4868, interval_samples_per_second: 5.381, interval_steps_per_second: 6.726, epoch: 4.8588[0m
[32m[2022-09-05 11:39:31,619] [    INFO][0m - loss: 0.35899532, learning_rate: 2.2627118644067798e-05, global_step: 870, interval_runtime: 1.4946, interval_samples_per_second: 5.353, interval_steps_per_second: 6.691, epoch: 4.9153[0m
[32m[2022-09-05 11:39:33,110] [    INFO][0m - loss: 0.44773464, learning_rate: 2.254237288135593e-05, global_step: 880, interval_runtime: 1.4908, interval_samples_per_second: 5.366, interval_steps_per_second: 6.708, epoch: 4.9718[0m
[32m[2022-09-05 11:39:34,610] [    INFO][0m - loss: 0.36394544, learning_rate: 2.2457627118644068e-05, global_step: 890, interval_runtime: 1.5, interval_samples_per_second: 5.333, interval_steps_per_second: 6.667, epoch: 5.0282[0m
[32m[2022-09-05 11:39:36,100] [    INFO][0m - loss: 0.51576777, learning_rate: 2.2372881355932205e-05, global_step: 900, interval_runtime: 1.4902, interval_samples_per_second: 5.369, interval_steps_per_second: 6.711, epoch: 5.0847[0m
[32m[2022-09-05 11:39:36,101] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 11:39:36,101] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-05 11:39:36,101] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 11:39:36,101] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 11:39:36,101] [    INFO][0m -   Total prediction steps = 177[0m
[32m[2022-09-05 11:39:47,352] [    INFO][0m - eval_loss: 0.5030072331428528, eval_accuracy: 0.21287128712871287, eval_runtime: 11.2507, eval_samples_per_second: 125.681, eval_steps_per_second: 15.732, epoch: 5.0847[0m
[32m[2022-09-05 11:39:47,378] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-900[0m
[32m[2022-09-05 11:39:47,378] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 11:39:50,883] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-900/tokenizer_config.json[0m
[32m[2022-09-05 11:39:50,884] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-900/special_tokens_map.json[0m
[32m[2022-09-05 11:39:56,203] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-05 11:39:56,203] [    INFO][0m - Loading best model from ./checkpoints_chid/checkpoint-500 (score: 0.2871287128712871).[0m
[32m[2022-09-05 11:39:57,066] [    INFO][0m - train_runtime: 319.8876, train_samples_per_second: 88.406, train_steps_per_second: 11.066, train_loss: 0.5081025309032864, epoch: 5.0847[0m
[32m[2022-09-05 11:39:57,108] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/[0m
[32m[2022-09-05 11:39:57,108] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 11:40:00,416] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/tokenizer_config.json[0m
[32m[2022-09-05 11:40:00,417] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/special_tokens_map.json[0m
[32m[2022-09-05 11:40:00,418] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-05 11:40:00,418] [    INFO][0m -   epoch                    =     5.0847[0m
[32m[2022-09-05 11:40:00,419] [    INFO][0m -   train_loss               =     0.5081[0m
[32m[2022-09-05 11:40:00,419] [    INFO][0m -   train_runtime            = 0:05:19.88[0m
[32m[2022-09-05 11:40:00,419] [    INFO][0m -   train_samples_per_second =     88.406[0m
[32m[2022-09-05 11:40:00,419] [    INFO][0m -   train_steps_per_second   =     11.066[0m
[32m[2022-09-05 11:40:00,425] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-05 11:40:00,426] [    INFO][0m -   Num examples = 14014[0m
[32m[2022-09-05 11:40:00,426] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 11:40:00,426] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 11:40:00,426] [    INFO][0m -   Total prediction steps = 1752[0m
[32m[2022-09-05 11:42:31,211] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-05 11:42:31,212] [    INFO][0m -   test_accuracy           =     0.8571[0m
[32m[2022-09-05 11:42:31,212] [    INFO][0m -   test_loss               =     0.4478[0m
[32m[2022-09-05 11:42:31,212] [    INFO][0m -   test_runtime            = 0:02:30.78[0m
[32m[2022-09-05 11:42:31,212] [    INFO][0m -   test_samples_per_second =      92.94[0m
[32m[2022-09-05 11:42:31,212] [    INFO][0m -   test_steps_per_second   =     11.619[0m
[33m[2022-09-05 11:42:49,927] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-09-05 11:42:49,928] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-05 11:42:49,928] [    INFO][0m - ============================================================[0m
[32m[2022-09-05 11:42:49,928] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-05 11:42:49,928] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-05 11:42:49,928] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-05 11:42:49,928] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-05 11:42:49,929] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-05 11:42:49,929] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-05 11:42:49,929] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-09-05 11:42:49,929] [    INFO][0m - [0m
[32m[2022-09-05 11:42:49,929] [    INFO][0m - ============================================================[0m
[32m[2022-09-05 11:42:49,929] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-05 11:42:49,929] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-05 11:42:49,929] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-05 11:42:49,929] [    INFO][0m - prompt                        :[0m
[32m[2022-09-05 11:42:49,929] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-05 11:42:49,929] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-05 11:42:49,929] [    INFO][0m - task_name                     :chid[0m
[32m[2022-09-05 11:42:49,930] [    INFO][0m - [0m
[32m[2022-09-05 11:42:49,930] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
[32m[2022-09-05 11:42:51,214] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-09-05 11:42:51,237] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-09-05 11:42:51,237] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[32m[2022-09-05 11:42:51,238] [    INFO][0m - Using template: [{'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': 'Êã¨Âè∑‰∏≠ÊàêËØ≠Â°´'}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': 'ÂØπÂêóÔºü'}, {'add_prefix_space': '', 'mask': None}][0m
2022-09-05 11:42:51,240 INFO [download.py:119] unique_endpoints {''}
[32m[2022-09-05 11:42:51,331] [    INFO][0m - ============================================================[0m
[32m[2022-09-05 11:42:51,331] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-05 11:42:51,331] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-05 11:42:51,331] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-05 11:42:51,331] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-05 11:42:51,331] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-05 11:42:51,331] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-05 11:42:51,332] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-05 11:42:51,332] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-05 11:42:51,332] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-05 11:42:51,332] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-05 11:42:51,332] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-05 11:42:51,332] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-05 11:42:51,332] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-05 11:42:51,332] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-05 11:42:51,332] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-05 11:42:51,332] [    INFO][0m - do_predict                    :False[0m
[32m[2022-09-05 11:42:51,332] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-05 11:42:51,332] [    INFO][0m - eval_batch_size               :8[0m
[32m[2022-09-05 11:42:51,333] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-09-05 11:42:51,333] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-09-05 11:42:51,333] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-05 11:42:51,333] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-05 11:42:51,333] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-05 11:42:51,333] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-05 11:42:51,333] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-05 11:42:51,333] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-05 11:42:51,333] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-05 11:42:51,333] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-05 11:42:51,333] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-05 11:42:51,333] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-09-05 11:42:51,333] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-05 11:42:51,334] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-05 11:42:51,334] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-05 11:42:51,334] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-05 11:42:51,334] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-05 11:42:51,334] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-05 11:42:51,334] [    INFO][0m - logging_dir                   :./checkpoints_chid/runs/Sep05_11-42-49_instance-3bwob41y-01[0m
[32m[2022-09-05 11:42:51,334] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-05 11:42:51,334] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-05 11:42:51,334] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-05 11:42:51,334] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-05 11:42:51,334] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-05 11:42:51,334] [    INFO][0m - max_seq_length                :256[0m
[32m[2022-09-05 11:42:51,335] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-05 11:42:51,335] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-05 11:42:51,335] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-05 11:42:51,335] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-05 11:42:51,335] [    INFO][0m - num_train_epochs              :20.0[0m
[32m[2022-09-05 11:42:51,335] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-05 11:42:51,335] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-05 11:42:51,335] [    INFO][0m - output_dir                    :./checkpoints_chid/[0m
[32m[2022-09-05 11:42:51,335] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-05 11:42:51,335] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-05 11:42:51,335] [    INFO][0m - per_device_eval_batch_size    :8[0m
[32m[2022-09-05 11:42:51,335] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-09-05 11:42:51,335] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-05 11:42:51,336] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-05 11:42:51,336] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-05 11:42:51,336] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-09-05 11:42:51,336] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-05 11:42:51,336] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-05 11:42:51,336] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-05 11:42:51,336] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-05 11:42:51,336] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-05 11:42:51,336] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-05 11:42:51,336] [    INFO][0m - run_name                      :./checkpoints_chid/[0m
[32m[2022-09-05 11:42:51,336] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-05 11:42:51,336] [    INFO][0m - save_steps                    :100[0m
[32m[2022-09-05 11:42:51,336] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-09-05 11:42:51,337] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-05 11:42:51,337] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-05 11:42:51,337] [    INFO][0m - seed                          :42[0m
[32m[2022-09-05 11:42:51,337] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-05 11:42:51,337] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-05 11:42:51,337] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-05 11:42:51,337] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-09-05 11:42:51,337] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-05 11:42:51,337] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-05 11:42:51,337] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-05 11:42:51,337] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-05 11:42:51,337] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-05 11:42:51,337] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-05 11:42:51,338] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-05 11:42:51,338] [    INFO][0m - [0m
[32m[2022-09-05 11:42:51,339] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-05 11:42:51,339] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-05 11:42:51,340] [    INFO][0m -   Num Epochs = 20[0m
[32m[2022-09-05 11:42:51,340] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-09-05 11:42:51,340] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-09-05 11:42:51,340] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-05 11:42:51,340] [    INFO][0m -   Total optimization steps = 3540.0[0m
[32m[2022-09-05 11:42:51,340] [    INFO][0m -   Total num train samples = 28280[0m
[32m[2022-09-05 11:42:52,953] [    INFO][0m - loss: 3.18697281, learning_rate: 2.9915254237288134e-05, global_step: 10, interval_runtime: 1.6127, interval_samples_per_second: 4.961, interval_steps_per_second: 6.201, epoch: 0.0565[0m
[32m[2022-09-05 11:42:54,446] [    INFO][0m - loss: 0.81436214, learning_rate: 2.9830508474576274e-05, global_step: 20, interval_runtime: 1.4924, interval_samples_per_second: 5.36, interval_steps_per_second: 6.7, epoch: 0.113[0m
[32m[2022-09-05 11:42:55,936] [    INFO][0m - loss: 0.74721618, learning_rate: 2.9745762711864407e-05, global_step: 30, interval_runtime: 1.4901, interval_samples_per_second: 5.369, interval_steps_per_second: 6.711, epoch: 0.1695[0m
[32m[2022-09-05 11:42:57,423] [    INFO][0m - loss: 0.44458175, learning_rate: 2.9661016949152544e-05, global_step: 40, interval_runtime: 1.4862, interval_samples_per_second: 5.383, interval_steps_per_second: 6.729, epoch: 0.226[0m
[32m[2022-09-05 11:42:58,913] [    INFO][0m - loss: 0.41458898, learning_rate: 2.9576271186440677e-05, global_step: 50, interval_runtime: 1.4908, interval_samples_per_second: 5.366, interval_steps_per_second: 6.708, epoch: 0.2825[0m
[32m[2022-09-05 11:43:00,403] [    INFO][0m - loss: 0.28977821, learning_rate: 2.9491525423728817e-05, global_step: 60, interval_runtime: 1.4901, interval_samples_per_second: 5.369, interval_steps_per_second: 6.711, epoch: 0.339[0m
[32m[2022-09-05 11:43:01,894] [    INFO][0m - loss: 0.84938326, learning_rate: 2.940677966101695e-05, global_step: 70, interval_runtime: 1.4904, interval_samples_per_second: 5.368, interval_steps_per_second: 6.71, epoch: 0.3955[0m
[32m[2022-09-05 11:43:03,385] [    INFO][0m - loss: 0.57225723, learning_rate: 2.9322033898305087e-05, global_step: 80, interval_runtime: 1.491, interval_samples_per_second: 5.366, interval_steps_per_second: 6.707, epoch: 0.452[0m
[32m[2022-09-05 11:43:04,870] [    INFO][0m - loss: 0.56982088, learning_rate: 2.923728813559322e-05, global_step: 90, interval_runtime: 1.4859, interval_samples_per_second: 5.384, interval_steps_per_second: 6.73, epoch: 0.5085[0m
[32m[2022-09-05 11:43:06,356] [    INFO][0m - loss: 0.49675393, learning_rate: 2.9152542372881356e-05, global_step: 100, interval_runtime: 1.4859, interval_samples_per_second: 5.384, interval_steps_per_second: 6.73, epoch: 0.565[0m
[32m[2022-09-05 11:43:06,357] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 11:43:06,357] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-05 11:43:06,357] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 11:43:06,357] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 11:43:06,357] [    INFO][0m -   Total prediction steps = 177[0m
[32m[2022-09-05 11:43:17,556] [    INFO][0m - eval_loss: 0.4129151403903961, eval_accuracy: 0.16336633663366337, eval_runtime: 11.1987, eval_samples_per_second: 126.265, eval_steps_per_second: 15.805, epoch: 0.565[0m
[32m[2022-09-05 11:43:17,583] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-100[0m
[32m[2022-09-05 11:43:17,583] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 11:43:21,063] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-05 11:43:21,064] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-05 11:43:28,149] [    INFO][0m - loss: 0.45287018, learning_rate: 2.9067796610169493e-05, global_step: 110, interval_runtime: 21.7933, interval_samples_per_second: 0.367, interval_steps_per_second: 0.459, epoch: 0.6215[0m
[32m[2022-09-05 11:43:29,635] [    INFO][0m - loss: 0.37267354, learning_rate: 2.8983050847457626e-05, global_step: 120, interval_runtime: 1.4849, interval_samples_per_second: 5.388, interval_steps_per_second: 6.734, epoch: 0.678[0m
[32m[2022-09-05 11:43:31,128] [    INFO][0m - loss: 0.5167089, learning_rate: 2.8898305084745763e-05, global_step: 130, interval_runtime: 1.4935, interval_samples_per_second: 5.356, interval_steps_per_second: 6.696, epoch: 0.7345[0m
[32m[2022-09-05 11:43:32,622] [    INFO][0m - loss: 0.57910237, learning_rate: 2.88135593220339e-05, global_step: 140, interval_runtime: 1.4944, interval_samples_per_second: 5.353, interval_steps_per_second: 6.692, epoch: 0.791[0m
[32m[2022-09-05 11:43:34,115] [    INFO][0m - loss: 0.53325286, learning_rate: 2.8728813559322036e-05, global_step: 150, interval_runtime: 1.4925, interval_samples_per_second: 5.36, interval_steps_per_second: 6.7, epoch: 0.8475[0m
[32m[2022-09-05 11:43:35,607] [    INFO][0m - loss: 0.58781877, learning_rate: 2.864406779661017e-05, global_step: 160, interval_runtime: 1.4919, interval_samples_per_second: 5.362, interval_steps_per_second: 6.703, epoch: 0.904[0m
[32m[2022-09-05 11:43:37,100] [    INFO][0m - loss: 0.40409689, learning_rate: 2.855932203389831e-05, global_step: 170, interval_runtime: 1.4936, interval_samples_per_second: 5.356, interval_steps_per_second: 6.695, epoch: 0.9605[0m
[32m[2022-09-05 11:43:38,613] [    INFO][0m - loss: 0.43651509, learning_rate: 2.8474576271186442e-05, global_step: 180, interval_runtime: 1.5128, interval_samples_per_second: 5.288, interval_steps_per_second: 6.61, epoch: 1.0169[0m
[32m[2022-09-05 11:43:40,103] [    INFO][0m - loss: 0.53515215, learning_rate: 2.8389830508474575e-05, global_step: 190, interval_runtime: 1.4896, interval_samples_per_second: 5.37, interval_steps_per_second: 6.713, epoch: 1.0734[0m
[32m[2022-09-05 11:43:41,594] [    INFO][0m - loss: 0.46085458, learning_rate: 2.8305084745762712e-05, global_step: 200, interval_runtime: 1.4914, interval_samples_per_second: 5.364, interval_steps_per_second: 6.705, epoch: 1.1299[0m
[32m[2022-09-05 11:43:41,595] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 11:43:41,595] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-05 11:43:41,595] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 11:43:41,595] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 11:43:41,595] [    INFO][0m -   Total prediction steps = 177[0m
[1 6 5 1 3 2 6 0 2 5 1 5 5 3 1 5 0 6 4 4 2 3 0 5 1 4 1 0 1 6 1 6 2 1 2 6 1
 5 5 2 2 3 3 5 4 5 6 0 6 6 4 5 5 3 2 1 2 3 2 0 5 0 4 4 3 0 2 6 6 4 5 3 0 5
 6 0 3 1 6 6 6 5 6 3 5 5 6 4 1 1 1 4 4 6 5 1 4 4 0 1 0 2 6 6 5 0 6 5 3 0 2
 2 0 3 0 0 3 0 3 4 2 6 6 6 6 1 1 4 2 2 6 2 3 1 3 0 0 3 3 0 1 0 5 0 2 1 5 1
 2 1 5 0 0 0 1 6 1 5 5 4 4 2 3 1 1 4 2 3 2 3 5 1 1 4 4 0 0 0 3 3 0 2 1 0 4
 6 0 2 6 6 0 0 1 2 5 4 6 2 2 2 0 0]
[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3
 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6
 6 6 6 6 6 6 6 6 6 6 6 6 6 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4
 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
[6 0 0 4 5 2 0 1 5 3 5 2 6 3 5 1 5 1 2 4 6 5 1 2 4 6 1 5 0 0 0 4 1 3 1 4 1
 3 5 3 4 3 6 3 3 3 0 6 4 2 4 1 5 1 4 3 5 0 5 3 5 4 4 4 0 0 5 3 3 4 4 6 4 1
 5 2 3 1 6 4 2 5 6 5 2 1 1 0 2 6 2 6 4 4 5 4 6 3 2 5 2 3 5 6 4 6 1 4 4 1 6
 0 6 3 5 2 0 3 5 4 6 4 4 5 1 0 0 1 1 6 6 5 3 2 3 1 3 0 0 3 6 3 3 0 6 6 0 4
 2 3 5 5 4 2 6 6 2 1 2 4 3 0 4 0 0 0 4 5 0 0 5 1 3 1 6 6 5 1 1 4 1 1 3 4 3
 2 5 2 1 6 3 2 5 4 1 1 5 2 4 5 1 2]
[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3
 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6
 6 6 6 6 6 6 6 6 6 6 6 6 6 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4
 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
[0 5 5 5 2 2 1 6 6 5 1 4 5 3 1 1 1 1 2 2 6 6 5 2 2 1 5 2 0 4 0 3 2 1 1 4 6
 3 3 4 4 2 6 4 5 3 3 0 3 3 5 4 6 1 6 6 5 1 2 3 4 6 3 2 5 2 2 6 3 6 5 6 3 5
 6 6 6 1 5 4 2 5 4 0 2 3 3 2 0 1 2 3 4 4 5 5 5 3 4 6 2 2 6 1 0 0 4 4 3 5 2
 6 4 3 4 5 0 5 5 5 5 0 5 6 1 4 5 4 5 2 0 6 5 2 4 0 3 6 5 0 4 4 0 0 1 0 4 0
 6 2 1 0 4 5 6 1 1 2 6 3 4 5 0 4 0 5 2 5 0 2 1 3 3 6 2 4 0 1 3 1 1 3 2 2 1
 6 5 4 3 1 5 3 1 2 3 1 5 2 3 0 5 4]
[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3
 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6
 6 6 6 6 6 6 6 6 6 6 6 6 6 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4
 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
[0 0 5 3 2 2 2 5 5 0 2 5 1 0 1 2 1 1 2 4 1 6 5 1 4 0 2 5 0 4 0 1 5 3 6 4 0
 3 3 3 4 2 6 5 5 3 3 3 1 3 5 4 0 1 3 6 5 0 0 2 5 0 0 2 6 6 5 6 1 6 0 4 1 5
 6 6 0 2 0 6 2 5 1 5 6 3 6 2 6 5 2 6 1 4 4 5 1 0 1 5 2 1 4 2 4 2 4 4 3 1 3
 6 4 2 6 5 6 2 0 4 2 0 3 5 3 5 0 1 0 6 6 5 5 5 3 1 5 0 2 3 6 2 5 0 5 6 0 3
 1 2 3 4 4 2 3 5 0 4 2 5 2 3 4 0 0 2 1 5 0 0 3 2 4 1 5 6 1 5 3 2 1 1 5 4 6
 1 1 2 0 0 6 1 1 0 3 1 2 1 1 1 6 2]
[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3
 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6
 6 6 6 6 6 6 6 6 6 6 6 6 6 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4
 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
[4 6 1 2 3 1 4 6 6 1 4 3 0 2 5 5 6 1 2 4 4 0 5 1 2 0 3 2 0 6 6 5 1 3 6 1 6
 4 4 2 4 4 6 5 5 6 0 6 5 5 4 4 0 3 6 0 6 1 0 1 5 1 0 3 6 0 0 1 1 3 0 1 4 4
 2 2 0 3 1 0 2 5 1 3 2 3 1 4 3 5 2 6 1 4 2 5 3 0 1 5 1 3 6 2 0 6 4 4 3 1 3
 0 6 5 6 6 3 2 1 6 6 0 6 5 6 1 1 1 0 3 5 0 2 0 0 1 2 4 2 1 4 2 5 5 2 1 2 0
 4 3 5 6 4 3 3 6 4 0 3 6 2 5 3 0 0 1 6 6 4 0 5 1 2 1 0 0 5 5 1 4 3 3 0 0 3
 2 1 2 6 1 3 3 6 5 5 0 4 5 2 5 6 0]
[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3
 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6
 6 6 6 6 6 6 6 6 6 6 6 6 6 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4
 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
[0 5 0 0 0 4 5 1 3 1 4 3 6 0 5 0 6 1 2 6 1 4 5 0 0 0 2 1 3 5 6 1 6 3 2 0 3
 6 5 1 5 4 6 1 3 6 2 3 5 3 1 4 0 5 5 4 5 0 0 1 0 0 0 2 6 1 4 6 6 3 6 3 6 3
 2 6 5 5 0 6 4 5 0 1 1 1 6 3 3 1 0 6 2 2 4 0 1 0 4 5 1 1 2 4 1 4 4 4 3 6 4
 6 4 5 3 3 6 2 6 0 6 2 4 4 4 0 0 5 1 6 5 5 5 5 0 3 5 1 2 3 4 0 4 1 4 3 1 4
 1 2 6 6 5 6 4 5 2 5 1 0 3 4 2 6 1 3 4 6 6 4 0 2 6 0 2 2 1 6 6 2 4 6 5 4 5
 2 1 0 0 1 3 6 6 1 1 6 0 0 1 6 6 5]
[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3
 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6
 6 6 6 6 6 6 6 6 6 6 6 6 6 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4
 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
[1 6 4 3 0 4 5 0 3 1 3 3 1 1 4 5 6 1 1 1 0 3 3 1 4 3 4 6 5 6 6 6 1 3 3 1 5
 5 6 2 6 5 2 5 5 6 0 3 6 5 1 2 0 0 5 0 6 3 0 4 5 1 2 1 3 2 0 1 6 4 3 5 4 2
 2 4 1 0 1 5 5 1 1 3 3 2 0 4 4 5 0 4 6 2 2 5 1 5 4 5 6 3 1 6 5 2 2 1 6 1 3
 2 6 1 2 3 3 4 3 3 6 0 2 5 6 3 1 4 0 6 5 5 6 6 5 4 0 4 3 1 1 2 5 1 0 3 3 1
 4 6 5 1 3 3 4 3 4 5 1 6 2 5 3 5 2 0 5 6 3 5 5 5 1 6 6 0 5 5 1 5 0 6 0 3 6
 3 6 2 3 6 6 3 6 5 0 0 4 0 2 5 4 0]
[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3
 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6
 6 6 6 6 6 6 6 6 6 6 6 6 6 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4
 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
[2 0 0 3 2 2 1 6 3 2 6 1 3 6 5 1 1 1 2 4 2 6 5 4 0 6 2 5 2 2 1 1 2 3 0 4 0
 3 3 3 4 6 6 5 3 5 2 2 5 2 6 4 2 5 3 4 5 0 5 3 1 4 4 2 1 5 2 2 2 5 0 4 6 4
 6 6 6 1 5 4 2 0 6 1 2 1 1 3 3 1 4 6 4 4 3 5 2 1 0 5 0 5 2 1 2 0 0 4 1 1 4
 6 2 5 4 4 2 0 0 4 6 4 3 6 3 0 3 5 0 0 6 5 1 5 3 5 3 1 0 6 4 1 4 0 0 2 0 6
 1 3 6 4 0 5 6 5 0 6 1 0 3 0 4 0 1 6 4 1 0 1 4 2 3 5 4 1 1 6 3 3 1 6 5 4 5
 3 5 0 0 1 0 6 0 3 3 1 5 1 0 2 5 4]
[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3
 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6
 6 6 6 6 6 6 6 6 6 6 6 6 6 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4
 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
(14014, 2)
14014
0
= 0 ===================
[('Âê¶', 12012)]
----------
[('ÊòØ', 12012)]
----------
[('‰∏ç', 12012)]
----------
[('‰∏∫', 12012)]
----------
[('Êó†', 7335), ('Êúâ', 4677)]
----------
1
= 0 ===================
[('Âê¶', 2002)]
----------
[('ÊòØ', 2002)]
----------
[('‰∏ç', 2002)]
----------
[('‰∏∫', 2002)]
----------
[('Êó†', 1177), ('Êúâ', 825)]
----------
====================
[6 3 4 0 0 2 5 5 0 0 4 5 5 3 0 0 0 2 5 0 0 3 1 5 4 1 6 3 3 0 0 5 0 6 5 3 4
 0 0 1 3 5 6 2 0 3 1 5 6 2 2 6 3 5 4 4 0 3 4 6 6 3 4 2 2 5 6 6 6 2 2 2 0 5
 0 5 1 4 0 4 3 3 3 3 5 2 0 6 5 2 4 5 0 1 5 1 0 1 5 6 0 3 4 3 0 2 6 2 0 1 4
 4 6 2 2 0 4 5 3 5 3 2 1 0 2 3 5 6 5 2 1 0 2 4 1 1 0 5 3 6 3 5 5 3 5 4 4 3
 6 6 0 5 4 4 5 5 6 1 5 4 1 5 1 5 1 4 6 2 0 3 4 6 3 0 5 2 1 0 6 0 3 4 4 1 6
 6 5 1 1 3 5 6 4 3 1 1 0 6 5 5 2 1]
[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3
 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6
 6 6 6 6 6 6 6 6 6 6 6 6 6 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4
 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
[32m[2022-09-05 11:43:52,845] [    INFO][0m - eval_loss: 0.4598127603530884, eval_accuracy: 0.28217821782178215, eval_runtime: 11.2498, eval_samples_per_second: 125.691, eval_steps_per_second: 15.734, epoch: 1.1299[0m
[32m[2022-09-05 11:43:52,869] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-200[0m
[32m[2022-09-05 11:43:52,869] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 11:43:56,033] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-200/tokenizer_config.json[0m
[32m[2022-09-05 11:43:56,034] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-200/special_tokens_map.json[0m
[32m[2022-09-05 11:44:04,997] [    INFO][0m - loss: 0.39056671, learning_rate: 2.822033898305085e-05, global_step: 210, interval_runtime: 23.4019, interval_samples_per_second: 0.342, interval_steps_per_second: 0.427, epoch: 1.1864[0m
[32m[2022-09-05 11:44:06,490] [    INFO][0m - loss: 0.62042961, learning_rate: 2.8135593220338985e-05, global_step: 220, interval_runtime: 1.4934, interval_samples_per_second: 5.357, interval_steps_per_second: 6.696, epoch: 1.2429[0m
[32m[2022-09-05 11:44:07,985] [    INFO][0m - loss: 0.48237567, learning_rate: 2.805084745762712e-05, global_step: 230, interval_runtime: 1.4949, interval_samples_per_second: 5.352, interval_steps_per_second: 6.689, epoch: 1.2994[0m
[32m[2022-09-05 11:44:09,483] [    INFO][0m - loss: 0.5620985, learning_rate: 2.7966101694915255e-05, global_step: 240, interval_runtime: 1.4987, interval_samples_per_second: 5.338, interval_steps_per_second: 6.672, epoch: 1.3559[0m
[32m[2022-09-05 11:44:10,973] [    INFO][0m - loss: 0.30823627, learning_rate: 2.788135593220339e-05, global_step: 250, interval_runtime: 1.4902, interval_samples_per_second: 5.368, interval_steps_per_second: 6.71, epoch: 1.4124[0m
[32m[2022-09-05 11:44:12,468] [    INFO][0m - loss: 0.64600167, learning_rate: 2.7796610169491528e-05, global_step: 260, interval_runtime: 1.4945, interval_samples_per_second: 5.353, interval_steps_per_second: 6.691, epoch: 1.4689[0m
[32m[2022-09-05 11:44:13,957] [    INFO][0m - loss: 0.48021746, learning_rate: 2.771186440677966e-05, global_step: 270, interval_runtime: 1.4894, interval_samples_per_second: 5.371, interval_steps_per_second: 6.714, epoch: 1.5254[0m
[32m[2022-09-05 11:44:15,448] [    INFO][0m - loss: 0.6029737, learning_rate: 2.7627118644067794e-05, global_step: 280, interval_runtime: 1.4901, interval_samples_per_second: 5.369, interval_steps_per_second: 6.711, epoch: 1.5819[0m
[32m[2022-09-05 11:44:16,938] [    INFO][0m - loss: 0.42136827, learning_rate: 2.7542372881355934e-05, global_step: 290, interval_runtime: 1.4904, interval_samples_per_second: 5.368, interval_steps_per_second: 6.71, epoch: 1.6384[0m
[32m[2022-09-05 11:44:18,424] [    INFO][0m - loss: 1.00478582, learning_rate: 2.7457627118644068e-05, global_step: 300, interval_runtime: 1.4863, interval_samples_per_second: 5.382, interval_steps_per_second: 6.728, epoch: 1.6949[0m
[32m[2022-09-05 11:44:18,424] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 11:44:18,425] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-05 11:44:18,425] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 11:44:18,425] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 11:44:18,425] [    INFO][0m -   Total prediction steps = 177[0m
[32m[2022-09-05 11:44:29,647] [    INFO][0m - eval_loss: 0.7944426536560059, eval_accuracy: 0.14356435643564355, eval_runtime: 11.2215, eval_samples_per_second: 126.009, eval_steps_per_second: 15.773, epoch: 1.6949[0m
[32m[2022-09-05 11:44:29,674] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-300[0m
[32m[2022-09-05 11:44:29,674] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 11:44:32,853] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-300/tokenizer_config.json[0m
[32m[2022-09-05 11:44:32,854] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-300/special_tokens_map.json[0m
[32m[2022-09-05 11:44:39,629] [    INFO][0m - loss: 0.53715668, learning_rate: 2.7372881355932204e-05, global_step: 310, interval_runtime: 21.2052, interval_samples_per_second: 0.377, interval_steps_per_second: 0.472, epoch: 1.7514[0m
[32m[2022-09-05 11:44:41,113] [    INFO][0m - loss: 0.4279705, learning_rate: 2.7288135593220337e-05, global_step: 320, interval_runtime: 1.4841, interval_samples_per_second: 5.39, interval_steps_per_second: 6.738, epoch: 1.8079[0m
[32m[2022-09-05 11:44:42,602] [    INFO][0m - loss: 0.70929618, learning_rate: 2.7203389830508477e-05, global_step: 330, interval_runtime: 1.4887, interval_samples_per_second: 5.374, interval_steps_per_second: 6.717, epoch: 1.8644[0m
[32m[2022-09-05 11:44:44,094] [    INFO][0m - loss: 0.39419951, learning_rate: 2.711864406779661e-05, global_step: 340, interval_runtime: 1.4918, interval_samples_per_second: 5.363, interval_steps_per_second: 6.703, epoch: 1.9209[0m
[32m[2022-09-05 11:44:45,585] [    INFO][0m - loss: 0.39264185, learning_rate: 2.7033898305084747e-05, global_step: 350, interval_runtime: 1.4913, interval_samples_per_second: 5.364, interval_steps_per_second: 6.705, epoch: 1.9774[0m
[32m[2022-09-05 11:44:47,090] [    INFO][0m - loss: 0.42235889, learning_rate: 2.6949152542372884e-05, global_step: 360, interval_runtime: 1.5047, interval_samples_per_second: 5.317, interval_steps_per_second: 6.646, epoch: 2.0339[0m
[32m[2022-09-05 11:44:48,583] [    INFO][0m - loss: 0.37798204, learning_rate: 2.6864406779661017e-05, global_step: 370, interval_runtime: 1.4927, interval_samples_per_second: 5.359, interval_steps_per_second: 6.699, epoch: 2.0904[0m
[32m[2022-09-05 11:44:50,072] [    INFO][0m - loss: 0.62839365, learning_rate: 2.6779661016949153e-05, global_step: 380, interval_runtime: 1.4894, interval_samples_per_second: 5.371, interval_steps_per_second: 6.714, epoch: 2.1469[0m
[32m[2022-09-05 11:44:51,564] [    INFO][0m - loss: 0.5100812, learning_rate: 2.6694915254237287e-05, global_step: 390, interval_runtime: 1.4919, interval_samples_per_second: 5.362, interval_steps_per_second: 6.703, epoch: 2.2034[0m
[32m[2022-09-05 11:44:53,066] [    INFO][0m - loss: 0.40101299, learning_rate: 2.6610169491525427e-05, global_step: 400, interval_runtime: 1.5017, interval_samples_per_second: 5.327, interval_steps_per_second: 6.659, epoch: 2.2599[0m
[32m[2022-09-05 11:44:53,066] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 11:44:53,066] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-05 11:44:53,067] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 11:44:53,067] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 11:44:53,067] [    INFO][0m -   Total prediction steps = 177[0m
[32m[2022-09-05 11:45:04,362] [    INFO][0m - eval_loss: 0.4112757742404938, eval_accuracy: 0.15346534653465346, eval_runtime: 11.2953, eval_samples_per_second: 125.185, eval_steps_per_second: 15.67, epoch: 2.2599[0m
[32m[2022-09-05 11:45:04,390] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-400[0m
[32m[2022-09-05 11:45:04,391] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 11:45:07,598] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-400/tokenizer_config.json[0m
[32m[2022-09-05 11:45:07,598] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-400/special_tokens_map.json[0m
[32m[2022-09-05 11:45:14,328] [    INFO][0m - loss: 0.50490627, learning_rate: 2.652542372881356e-05, global_step: 410, interval_runtime: 21.262, interval_samples_per_second: 0.376, interval_steps_per_second: 0.47, epoch: 2.3164[0m
[32m[2022-09-05 11:45:15,821] [    INFO][0m - loss: 0.38087757, learning_rate: 2.6440677966101696e-05, global_step: 420, interval_runtime: 1.493, interval_samples_per_second: 5.358, interval_steps_per_second: 6.698, epoch: 2.3729[0m
[32m[2022-09-05 11:45:17,312] [    INFO][0m - loss: 0.46279736, learning_rate: 2.635593220338983e-05, global_step: 430, interval_runtime: 1.4908, interval_samples_per_second: 5.366, interval_steps_per_second: 6.708, epoch: 2.4294[0m
[32m[2022-09-05 11:45:18,814] [    INFO][0m - loss: 0.54755287, learning_rate: 2.627118644067797e-05, global_step: 440, interval_runtime: 1.5011, interval_samples_per_second: 5.329, interval_steps_per_second: 6.662, epoch: 2.4859[0m
[32m[2022-09-05 11:45:20,312] [    INFO][0m - loss: 0.44755039, learning_rate: 2.6186440677966103e-05, global_step: 450, interval_runtime: 1.4987, interval_samples_per_second: 5.338, interval_steps_per_second: 6.673, epoch: 2.5424[0m
[32m[2022-09-05 11:45:21,808] [    INFO][0m - loss: 0.55266538, learning_rate: 2.6101694915254236e-05, global_step: 460, interval_runtime: 1.4961, interval_samples_per_second: 5.347, interval_steps_per_second: 6.684, epoch: 2.5989[0m
[32m[2022-09-05 11:45:23,310] [    INFO][0m - loss: 0.54622264, learning_rate: 2.6016949152542372e-05, global_step: 470, interval_runtime: 1.5016, interval_samples_per_second: 5.328, interval_steps_per_second: 6.659, epoch: 2.6554[0m
[32m[2022-09-05 11:45:24,819] [    INFO][0m - loss: 0.45951934, learning_rate: 2.593220338983051e-05, global_step: 480, interval_runtime: 1.5083, interval_samples_per_second: 5.304, interval_steps_per_second: 6.63, epoch: 2.7119[0m
[32m[2022-09-05 11:45:26,314] [    INFO][0m - loss: 0.40372286, learning_rate: 2.5847457627118646e-05, global_step: 490, interval_runtime: 1.4955, interval_samples_per_second: 5.35, interval_steps_per_second: 6.687, epoch: 2.7684[0m
[32m[2022-09-05 11:45:27,804] [    INFO][0m - loss: 0.38116207, learning_rate: 2.576271186440678e-05, global_step: 500, interval_runtime: 1.4913, interval_samples_per_second: 5.365, interval_steps_per_second: 6.706, epoch: 2.8249[0m
[32m[2022-09-05 11:45:27,805] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 11:45:27,805] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-05 11:45:27,805] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 11:45:27,805] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 11:45:27,805] [    INFO][0m -   Total prediction steps = 177[0m
[32m[2022-09-05 11:45:39,070] [    INFO][0m - eval_loss: 0.5018419027328491, eval_accuracy: 0.1485148514851485, eval_runtime: 11.2633, eval_samples_per_second: 125.54, eval_steps_per_second: 15.715, epoch: 2.8249[0m
[32m[2022-09-05 11:45:39,089] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-500[0m
[32m[2022-09-05 11:45:39,089] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 11:45:42,176] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-500/tokenizer_config.json[0m
[32m[2022-09-05 11:45:42,177] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-500/special_tokens_map.json[0m
[32m[2022-09-05 11:45:49,572] [    INFO][0m - loss: 0.41453691, learning_rate: 2.567796610169492e-05, global_step: 510, interval_runtime: 21.7678, interval_samples_per_second: 0.368, interval_steps_per_second: 0.459, epoch: 2.8814[0m
[32m[2022-09-05 11:45:51,061] [    INFO][0m - loss: 0.39078777, learning_rate: 2.5593220338983052e-05, global_step: 520, interval_runtime: 1.4882, interval_samples_per_second: 5.376, interval_steps_per_second: 6.72, epoch: 2.9379[0m
[32m[2022-09-05 11:45:52,553] [    INFO][0m - loss: 0.30042026, learning_rate: 2.550847457627119e-05, global_step: 530, interval_runtime: 1.4928, interval_samples_per_second: 5.359, interval_steps_per_second: 6.699, epoch: 2.9944[0m
[32m[2022-09-05 11:45:54,073] [    INFO][0m - loss: 0.43303704, learning_rate: 2.5423728813559322e-05, global_step: 540, interval_runtime: 1.52, interval_samples_per_second: 5.263, interval_steps_per_second: 6.579, epoch: 3.0508[0m
[32m[2022-09-05 11:45:55,571] [    INFO][0m - loss: 0.5251966, learning_rate: 2.5338983050847458e-05, global_step: 550, interval_runtime: 1.4978, interval_samples_per_second: 5.341, interval_steps_per_second: 6.676, epoch: 3.1073[0m
[32m[2022-09-05 11:45:57,083] [    INFO][0m - loss: 0.48607798, learning_rate: 2.5254237288135595e-05, global_step: 560, interval_runtime: 1.5117, interval_samples_per_second: 5.292, interval_steps_per_second: 6.615, epoch: 3.1638[0m
[32m[2022-09-05 11:45:58,575] [    INFO][0m - loss: 0.45786977, learning_rate: 2.5169491525423728e-05, global_step: 570, interval_runtime: 1.4924, interval_samples_per_second: 5.361, interval_steps_per_second: 6.701, epoch: 3.2203[0m
[32m[2022-09-05 11:46:00,069] [    INFO][0m - loss: 0.45412817, learning_rate: 2.5084745762711865e-05, global_step: 580, interval_runtime: 1.4932, interval_samples_per_second: 5.357, interval_steps_per_second: 6.697, epoch: 3.2768[0m
[32m[2022-09-05 11:46:01,561] [    INFO][0m - loss: 0.43891802, learning_rate: 2.5e-05, global_step: 590, interval_runtime: 1.4926, interval_samples_per_second: 5.36, interval_steps_per_second: 6.7, epoch: 3.3333[0m
[32m[2022-09-05 11:46:03,055] [    INFO][0m - loss: 0.45619798, learning_rate: 2.4915254237288138e-05, global_step: 600, interval_runtime: 1.4942, interval_samples_per_second: 5.354, interval_steps_per_second: 6.693, epoch: 3.3898[0m
[32m[2022-09-05 11:46:03,056] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 11:46:03,056] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-05 11:46:03,057] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 11:46:03,057] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 11:46:03,057] [    INFO][0m -   Total prediction steps = 177[0m
[32m[2022-09-05 11:46:14,426] [    INFO][0m - eval_loss: 0.41088250279426575, eval_accuracy: 0.14356435643564355, eval_runtime: 11.3688, eval_samples_per_second: 124.375, eval_steps_per_second: 15.569, epoch: 3.3898[0m
[32m[2022-09-05 11:46:14,449] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-600[0m
[32m[2022-09-05 11:46:14,449] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 11:46:17,669] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-600/tokenizer_config.json[0m
[32m[2022-09-05 11:46:17,670] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-600/special_tokens_map.json[0m
[32m[2022-09-05 11:46:23,943] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-05 11:46:23,943] [    INFO][0m - Loading best model from ./checkpoints_chid/checkpoint-200 (score: 0.28217821782178215).[0m
[32m[2022-09-05 11:46:24,856] [    INFO][0m - train_runtime: 213.5151, train_samples_per_second: 132.45, train_steps_per_second: 16.58, train_loss: 0.5438175849119822, epoch: 3.3898[0m
[32m[2022-09-05 11:46:24,899] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/[0m
[32m[2022-09-05 11:46:24,900] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 11:46:30,321] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/tokenizer_config.json[0m
[32m[2022-09-05 11:46:30,322] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/special_tokens_map.json[0m
[32m[2022-09-05 11:46:30,349] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-05 11:46:30,349] [    INFO][0m -   epoch                    =     3.3898[0m
[32m[2022-09-05 11:46:30,349] [    INFO][0m -   train_loss               =     0.5438[0m
[32m[2022-09-05 11:46:30,349] [    INFO][0m -   train_runtime            = 0:03:33.51[0m
[32m[2022-09-05 11:46:30,350] [    INFO][0m -   train_samples_per_second =     132.45[0m
[32m[2022-09-05 11:46:30,350] [    INFO][0m -   train_steps_per_second   =      16.58[0m
[32m[2022-09-05 11:46:30,355] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-05 11:46:30,355] [    INFO][0m -   Num examples = 14014[0m
[32m[2022-09-05 11:46:30,355] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 11:46:30,355] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 11:46:30,355] [    INFO][0m -   Total prediction steps = 1752[0m
[32m[2022-09-05 11:49:02,892] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-05 11:49:02,892] [    INFO][0m -   test_accuracy           =     0.8571[0m
[32m[2022-09-05 11:49:02,893] [    INFO][0m -   test_loss               =     0.4611[0m
[32m[2022-09-05 11:49:02,893] [    INFO][0m -   test_runtime            = 0:02:32.53[0m
[32m[2022-09-05 11:49:02,893] [    INFO][0m -   test_samples_per_second =     91.873[0m
[32m[2022-09-05 11:49:02,893] [    INFO][0m -   test_steps_per_second   =     11.486[0m
[0 5 5 2 2 5 1 2 5 6 5 0 6 2 5 1 2 2 4 4 2 6 2 2 6 1 5 3 2 3 2 0 5 3 3 4 1
 0 0 4 3 0 5 4 1 1 3 1 2 1 5 5 6 3 3 1 1 6 6 6 4 3 5 4 1 5 2 5 1 5 2 4 5 5
 2 6 4 5 4 0 6 6 4 6 6 3 0 4 0 3 1 5 3 4 3 2 0 4 1 1 2 0 1 2 1 2 0 0 1 3 4
 1 4 5 4 6 6 3 2 4 0 5 5 5 5 5 1 6 6 0 5 1 0 5 5 6 0 5 0 5 1 6 5 4 0 6 0 2
 3 0 2 0 0 0 2 2 5 0 0 0 4 6 1 2 3 1 2 4 2 3 6 6 5 5 3 3 2 2 2 4 3 3 3 1 1
 2 3 1 0 1 5 0 0 2 6 1 6 6 1 6 1 4]
[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3
 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6
 6 6 6 6 6 6 6 6 6 6 6 6 6 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4
 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
[4 2 3 5 0 4 1 4 6 3 6 0 5 6 1 2 5 1 0 2 6 4 4 2 5 4 1 0 2 2 2 4 0 5 6 3 0
 4 4 1 2 0 0 5 1 1 0 3 6 4 4 1 1 5 1 3 3 4 3 2 6 4 4 2 5 4 0 6 2 5 5 1 1 4
 5 5 3 6 6 5 1 3 0 6 2 3 4 2 5 5 5 6 4 3 6 2 1 2 4 5 6 1 1 0 5 2 3 4 3 5 3
 0 2 4 6 0 2 0 2 4 1 6 2 5 5 2 3 1 4 2 0 1 2 0 6 2 1 4 6 4 1 4 6 0 6 1 3 5
 2 6 1 6 0 2 2 3 0 2 1 4 0 0 5 1 6 6 2 0 2 5 6 3 4 2 6 0 0 2 6 3 1 6 5 0 1
 6 6 2 3 3 5 3 6 3 6 3 1 3 4 1 5 6]
[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3
 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6
 6 6 6 6 6 6 6 6 6 6 6 6 6 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4
 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
[1 3 6 4 4 5 1 2 4 4 0 1 5 3 1 0 2 5 2 6 6 5 3 6 6 0 1 3 0 4 6 2 6 6 6 4 0
 5 3 1 6 4 0 4 2 3 3 3 5 5 1 5 2 0 4 3 5 2 6 3 6 6 4 1 6 6 4 3 6 1 5 1 1 2
 3 1 1 0 2 0 0 1 4 5 5 3 3 2 4 3 6 2 2 6 6 5 6 2 2 2 1 4 6 4 4 5 3 2 1 1 3
 5 0 1 0 2 4 5 2 5 0 0 0 5 3 6 4 0 4 4 4 5 4 6 0 5 0 0 6 6 2 2 3 4 3 6 0 4
 5 6 1 1 4 2 2 4 2 6 5 4 2 3 3 6 5 5 1 1 2 5 5 0 1 4 5 5 3 4 4 3 6 0 1 4 6
 0 3 2 6 0 4 1 0 0 5 1 3 0 1 4 6 5]
[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3
 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6
 6 6 6 6 6 6 6 6 6 6 6 6 6 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4
 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
[4 5 5 1 0 6 6 6 4 3 6 2 5 4 4 4 1 3 6 6 1 0 4 4 6 2 6 6 1 6 4 2 3 2 3 1 4
 3 6 6 0 4 1 3 1 3 6 1 6 5 4 2 0 1 6 5 4 5 2 0 6 4 5 4 0 6 1 0 3 5 3 1 1 2
 0 2 0 2 3 0 5 0 6 4 3 0 6 1 4 5 0 1 4 3 1 4 3 3 1 1 5 1 0 5 3 6 0 6 1 3 2
 2 3 5 3 0 0 4 4 0 4 2 4 0 2 1 4 4 3 2 2 3 4 6 2 0 1 4 0 4 3 5 0 6 5 4 5 4
 1 2 3 5 6 0 3 2 1 1 6 2 0 6 2 2 0 0 4 6 4 4 0 2 6 2 1 2 3 1 0 5 5 1 4 5 2
 1 1 1 5 6 1 1 3 5 5 4 3 1 0 0 5 0]
[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3
 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6
 6 6 6 6 6 6 6 6 6 6 6 6 6 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4
 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
[0 6 4 4 5 1 3 3 2 6 1 3 0 4 1 6 6 3 1 1 4 0 4 1 0 1 3 1 2 3 1 1 4 4 6 6 1
 1 4 6 0 3 6 5 0 4 5 0 1 1 4 4 5 1 4 4 4 5 4 6 2 4 6 4 6 1 3 4 5 3 0 5 0 0
 0 5 4 6 5 5 5 0 2 3 6 1 2 3 6 1 1 0 1 5 0 1 6 4 6 1 4 2 2 0 5 5 2 2 0 2 2
 6 0 3 5 3 3 4 0 0 2 4 5 3 0 6 6 3 0 2 0 2 5 0 2 2 4 1 1 0 5 3 1 5 6 5 5 6
 3 1 3 2 3 3 1 5 3 0 0 2 3 2 1 0 1 3 0 1 4 0 6 6 1 1 1 4 5 5 2 2 0 6 6 5 5
 1 1 4 3 5 5 1 1 3 1 5 0 4 5 0 3 0]
[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3
 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6
 6 6 6 6 6 6 6 6 6 6 6 6 6 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4
 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
(14014, 2)
14014
0
= 0 ===================
[('Èîô', 12012)]
----------
[('ÂØπ', 12012)]
----------
[('ÁöÑ', 9869), ('ËØØ', 1705), ('Áõ∏', 435), ('Â§ö', 1), ('ÂèØ', 1), ('Áúü', 1)]
----------
[('ËØØ', 6616), ('Áõ∏', 3595), ('ÁöÑ', 1488), ('Â§ö', 222), ('Áúü', 48), ('Âæó', 26), ('Â§ß', 8), ('ÂèØ', 6), ('Èïø', 2), ('Â•Ω', 1)]
----------
[('Áõ∏', 6975), ('ËØØ', 3068), ('Â§ö', 823), ('ÁöÑ', 611), ('Áúü', 248), ('Âæó', 195), ('Â§ß', 40), ('Â•Ω', 18), ('ÂèØ', 18), ('Èïø', 14), ('Ëøá', 2)]
----------
1
= 0 ===================
[('Èîô', 2002)]
----------
[('ÂØπ', 2002)]
----------
[('ÁöÑ', 1718), ('ËØØ', 220), ('Áõ∏', 64)]
----------
[('ËØØ', 985), ('Áõ∏', 706), ('ÁöÑ', 212), ('Â§ö', 66), ('Âæó', 17), ('Áúü', 11), ('Â•Ω', 2), ('ÂèØ', 2), ('Â§ß', 1)]
----------
[('Áõ∏', 1009), ('ËØØ', 609), ('Â§ö', 186), ('Âæó', 71), ('ÁöÑ', 66), ('Áúü', 45), ('ÂèØ', 5), ('Èïø', 5), ('Â§ß', 5), ('Â•Ω', 1)]
----------
run.sh: line 58: --freeze_plm: command not found
