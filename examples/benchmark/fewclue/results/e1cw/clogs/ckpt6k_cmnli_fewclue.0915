 
==========
eprstmt
==========
 
[32m[2022-09-15 14:08:45,887] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-15 14:08:45,888] [    INFO][0m - ============================================================[0m
[32m[2022-09-15 14:08:45,888] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-15 14:08:45,888] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-15 14:08:45,888] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-15 14:08:45,888] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-15 14:08:45,888] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-15 14:08:45,888] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-15 14:08:45,888] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-15 14:08:45,888] [    INFO][0m - [0m
[32m[2022-09-15 14:08:45,888] [    INFO][0m - ============================================================[0m
[32m[2022-09-15 14:08:45,888] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-15 14:08:45,888] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-15 14:08:45,889] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-15 14:08:45,889] [    INFO][0m - pretrained                    :./checkpoints_cmnli/checkpoint-6000/model_state.pdparams[0m
[32m[2022-09-15 14:08:45,889] [    INFO][0m - prompt                        :‚Äú{'text':'text_a'}‚ÄùËøôÊù°ËØÑËÆ∫ÁöÑÊÉÖÊÑüÂÄæÂêëÊòØ{'mask'}{'mask'}ÁöÑ„ÄÇ[0m
[32m[2022-09-15 14:08:45,889] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-15 14:08:45,889] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-15 14:08:45,889] [    INFO][0m - task_name                     :eprstmt[0m
[32m[2022-09-15 14:08:45,889] [    INFO][0m - [0m
[32m[2022-09-15 14:08:45,889] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0915 14:08:45.890801 58689 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0915 14:08:45.895074 58689 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-15 14:08:52,217] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-15 14:08:52,229] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-15 14:08:52,229] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-15 14:08:52,230] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùËøôÊù°ËØÑËÆ∫ÁöÑÊÉÖÊÑüÂÄæÂêëÊòØ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'ÁöÑ„ÄÇ'}][0m
[32m[2022-09-15 14:08:54,021] [    INFO][0m - ============================================================[0m
[32m[2022-09-15 14:08:54,021] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-15 14:08:54,021] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-15 14:08:54,022] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-15 14:08:54,022] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-15 14:08:54,022] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-15 14:08:54,022] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-15 14:08:54,022] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-15 14:08:54,022] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-15 14:08:54,022] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-15 14:08:54,022] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-15 14:08:54,022] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-15 14:08:54,022] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-15 14:08:54,022] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-15 14:08:54,022] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-15 14:08:54,023] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-15 14:08:54,023] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-15 14:08:54,023] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-15 14:08:54,023] [    INFO][0m - eval_batch_size               :16[0m
[32m[2022-09-15 14:08:54,023] [    INFO][0m - eval_steps                    :200[0m
[32m[2022-09-15 14:08:54,023] [    INFO][0m - evaluation_strategy           :IntervalStrategy.EPOCH[0m
[32m[2022-09-15 14:08:54,023] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-15 14:08:54,023] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-15 14:08:54,023] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-15 14:08:54,023] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-15 14:08:54,023] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-15 14:08:54,023] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-15 14:08:54,023] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-15 14:08:54,023] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-15 14:08:54,024] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-15 14:08:54,024] [    INFO][0m - learning_rate                 :3e-06[0m
[32m[2022-09-15 14:08:54,024] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-15 14:08:54,024] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-15 14:08:54,024] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-15 14:08:54,024] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-15 14:08:54,024] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-15 14:08:54,024] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-15 14:08:54,024] [    INFO][0m - logging_dir                   :./checkpoints/runs/Sep15_14-08-45_instance-3bwob41y-01[0m
[32m[2022-09-15 14:08:54,024] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-15 14:08:54,024] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-15 14:08:54,024] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-15 14:08:54,024] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-15 14:08:54,024] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-15 14:08:54,025] [    INFO][0m - max_seq_length                :128[0m
[32m[2022-09-15 14:08:54,025] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-15 14:08:54,025] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-15 14:08:54,025] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-15 14:08:54,025] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-15 14:08:54,025] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-09-15 14:08:54,025] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-15 14:08:54,025] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-15 14:08:54,025] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-09-15 14:08:54,025] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-15 14:08:54,025] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-15 14:08:54,025] [    INFO][0m - per_device_eval_batch_size    :16[0m
[32m[2022-09-15 14:08:54,025] [    INFO][0m - per_device_train_batch_size   :16[0m
[32m[2022-09-15 14:08:54,026] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-15 14:08:54,026] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-15 14:08:54,026] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-15 14:08:54,026] [    INFO][0m - ppt_learning_rate             :3e-05[0m
[32m[2022-09-15 14:08:54,026] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-15 14:08:54,026] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-15 14:08:54,026] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-15 14:08:54,026] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-15 14:08:54,026] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-15 14:08:54,026] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-15 14:08:54,026] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-09-15 14:08:54,026] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-15 14:08:54,026] [    INFO][0m - save_steps                    :200[0m
[32m[2022-09-15 14:08:54,026] [    INFO][0m - save_strategy                 :IntervalStrategy.EPOCH[0m
[32m[2022-09-15 14:08:54,027] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-15 14:08:54,027] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-15 14:08:54,027] [    INFO][0m - seed                          :42[0m
[32m[2022-09-15 14:08:54,027] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-15 14:08:54,027] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-15 14:08:54,027] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-15 14:08:54,027] [    INFO][0m - train_batch_size              :16[0m
[32m[2022-09-15 14:08:54,027] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-15 14:08:54,027] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-15 14:08:54,027] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-15 14:08:54,027] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-15 14:08:54,027] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-15 14:08:54,027] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-15 14:08:54,028] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-15 14:08:54,028] [    INFO][0m - [0m
[32m[2022-09-15 14:08:54,030] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-15 14:08:54,031] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-15 14:08:54,031] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-09-15 14:08:54,031] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2022-09-15 14:08:54,031] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2022-09-15 14:08:54,031] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-15 14:08:54,031] [    INFO][0m -   Total optimization steps = 500.0[0m
[32m[2022-09-15 14:08:54,031] [    INFO][0m -   Total num train samples = 8000[0m
[32m[2022-09-15 14:08:58,587] [    INFO][0m - loss: 3.86144676, learning_rate: 2.9400000000000002e-06, global_step: 10, interval_runtime: 4.5551, interval_samples_per_second: 3.513, interval_steps_per_second: 2.195, epoch: 1.0[0m
[32m[2022-09-15 14:08:58,588] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 14:08:58,588] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-15 14:08:58,588] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 14:08:58,589] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 14:08:58,589] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-15 14:08:59,913] [    INFO][0m - eval_loss: 0.7359307408332825, eval_accuracy: 0.8125, eval_runtime: 1.3241, eval_samples_per_second: 120.834, eval_steps_per_second: 7.552, epoch: 1.0[0m
[32m[2022-09-15 14:08:59,913] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-10[0m
[32m[2022-09-15 14:08:59,914] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 14:09:03,693] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-10/tokenizer_config.json[0m
[32m[2022-09-15 14:09:03,694] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-10/special_tokens_map.json[0m
[32m[2022-09-15 14:09:12,639] [    INFO][0m - loss: 0.43850832, learning_rate: 2.88e-06, global_step: 20, interval_runtime: 14.0516, interval_samples_per_second: 1.139, interval_steps_per_second: 0.712, epoch: 2.0[0m
[32m[2022-09-15 14:09:12,640] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 14:09:12,640] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-15 14:09:12,640] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 14:09:12,640] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 14:09:12,641] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-15 14:09:13,965] [    INFO][0m - eval_loss: 0.14719729125499725, eval_accuracy: 0.90625, eval_runtime: 1.324, eval_samples_per_second: 120.848, eval_steps_per_second: 7.553, epoch: 2.0[0m
[32m[2022-09-15 14:09:13,966] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-20[0m
[32m[2022-09-15 14:09:13,966] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 14:09:16,792] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-20/tokenizer_config.json[0m
[32m[2022-09-15 14:09:16,792] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-20/special_tokens_map.json[0m
[32m[2022-09-15 14:09:26,029] [    INFO][0m - loss: 0.14430245, learning_rate: 2.82e-06, global_step: 30, interval_runtime: 13.3907, interval_samples_per_second: 1.195, interval_steps_per_second: 0.747, epoch: 3.0[0m
[32m[2022-09-15 14:09:26,030] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 14:09:26,030] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-15 14:09:26,031] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 14:09:26,031] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 14:09:26,031] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-15 14:09:27,350] [    INFO][0m - eval_loss: 0.12446030229330063, eval_accuracy: 0.88125, eval_runtime: 1.3189, eval_samples_per_second: 121.31, eval_steps_per_second: 7.582, epoch: 3.0[0m
[32m[2022-09-15 14:09:27,351] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-30[0m
[32m[2022-09-15 14:09:27,351] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 14:09:30,548] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-30/tokenizer_config.json[0m
[32m[2022-09-15 14:09:30,548] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-30/special_tokens_map.json[0m
[32m[2022-09-15 14:09:39,450] [    INFO][0m - loss: 0.12628684, learning_rate: 2.7600000000000003e-06, global_step: 40, interval_runtime: 13.4205, interval_samples_per_second: 1.192, interval_steps_per_second: 0.745, epoch: 4.0[0m
[32m[2022-09-15 14:09:39,451] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 14:09:39,451] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-15 14:09:39,451] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 14:09:39,451] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 14:09:39,451] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-15 14:09:40,802] [    INFO][0m - eval_loss: 0.13015031814575195, eval_accuracy: 0.875, eval_runtime: 1.3508, eval_samples_per_second: 118.449, eval_steps_per_second: 7.403, epoch: 4.0[0m
[32m[2022-09-15 14:09:40,803] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-40[0m
[32m[2022-09-15 14:09:40,803] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 14:09:43,273] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-40/tokenizer_config.json[0m
[32m[2022-09-15 14:09:43,273] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-40/special_tokens_map.json[0m
[32m[2022-09-15 14:09:52,210] [    INFO][0m - loss: 0.0962234, learning_rate: 2.7e-06, global_step: 50, interval_runtime: 12.7605, interval_samples_per_second: 1.254, interval_steps_per_second: 0.784, epoch: 5.0[0m
[32m[2022-09-15 14:09:52,211] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 14:09:52,212] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-15 14:09:52,212] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 14:09:52,212] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 14:09:52,212] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-15 14:09:53,538] [    INFO][0m - eval_loss: 0.15274325013160706, eval_accuracy: 0.875, eval_runtime: 1.3261, eval_samples_per_second: 120.655, eval_steps_per_second: 7.541, epoch: 5.0[0m
[32m[2022-09-15 14:09:58,708] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-50[0m
[32m[2022-09-15 14:09:58,709] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 14:10:01,439] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-50/tokenizer_config.json[0m
[32m[2022-09-15 14:10:01,440] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-50/special_tokens_map.json[0m
[32m[2022-09-15 14:10:10,219] [    INFO][0m - loss: 0.0659977, learning_rate: 2.64e-06, global_step: 60, interval_runtime: 18.0082, interval_samples_per_second: 0.888, interval_steps_per_second: 0.555, epoch: 6.0[0m
[32m[2022-09-15 14:10:10,220] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 14:10:10,220] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-15 14:10:10,220] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 14:10:10,220] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 14:10:10,220] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-15 14:10:11,534] [    INFO][0m - eval_loss: 0.1579967737197876, eval_accuracy: 0.9, eval_runtime: 1.314, eval_samples_per_second: 121.768, eval_steps_per_second: 7.61, epoch: 6.0[0m
[32m[2022-09-15 14:10:11,534] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-60[0m
[32m[2022-09-15 14:10:11,535] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 14:10:14,181] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-60/tokenizer_config.json[0m
[32m[2022-09-15 14:10:14,182] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-60/special_tokens_map.json[0m
[32m[2022-09-15 14:10:18,903] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-15 14:10:18,903] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-20 (score: 0.90625).[0m
[32m[2022-09-15 14:10:20,554] [    INFO][0m - train_runtime: 86.5222, train_samples_per_second: 92.462, train_steps_per_second: 5.779, train_loss: 0.7887942453225454, epoch: 6.0[0m
[32m[2022-09-15 14:10:20,586] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-09-15 14:10:20,587] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 14:10:22,996] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-09-15 14:10:22,996] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-09-15 14:10:22,997] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-15 14:10:22,998] [    INFO][0m -   epoch                    =        6.0[0m
[32m[2022-09-15 14:10:22,998] [    INFO][0m -   train_loss               =     0.7888[0m
[32m[2022-09-15 14:10:22,998] [    INFO][0m -   train_runtime            = 0:01:26.52[0m
[32m[2022-09-15 14:10:22,998] [    INFO][0m -   train_samples_per_second =     92.462[0m
[32m[2022-09-15 14:10:22,998] [    INFO][0m -   train_steps_per_second   =      5.779[0m
[32m[2022-09-15 14:10:22,999] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-15 14:10:22,999] [    INFO][0m -   Num examples = 610[0m
[32m[2022-09-15 14:10:23,000] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 14:10:23,000] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 14:10:23,000] [    INFO][0m -   Total prediction steps = 39[0m
[32m[2022-09-15 14:10:28,108] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-15 14:10:28,109] [    INFO][0m -   test_accuracy           =     0.8902[0m
[32m[2022-09-15 14:10:28,110] [    INFO][0m -   test_loss               =     0.1564[0m
[32m[2022-09-15 14:10:28,110] [    INFO][0m -   test_runtime            = 0:00:05.10[0m
[32m[2022-09-15 14:10:28,110] [    INFO][0m -   test_samples_per_second =    119.413[0m
[32m[2022-09-15 14:10:28,110] [    INFO][0m -   test_steps_per_second   =      7.635[0m
[32m[2022-09-15 14:10:28,111] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-15 14:10:28,111] [    INFO][0m -   Num examples = 753[0m
[32m[2022-09-15 14:10:28,111] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 14:10:28,112] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 14:10:28,112] [    INFO][0m -   Total prediction steps = 48[0m
terminate called after throwing an instance of 'paddle::memory::allocation::BadAlloc'
  what():  

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   full_dygraph_function(paddle::experimental::IntArrayBase<paddle::experimental::Tensor>, paddle::experimental::ScalarBase<paddle::experimental::Tensor>, paddle::experimental::DataType, phi::Place)
1   paddle::experimental::full(paddle::experimental::IntArrayBase<paddle::experimental::Tensor> const&, paddle::experimental::ScalarBase<paddle::experimental::Tensor> const&, paddle::experimental::DataType, phi::Place const&)
2   void phi::FullKernel<float, phi::GPUContext>(phi::GPUContext const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, paddle::experimental::ScalarBase<phi::DenseTensor> const&, paddle::experimental::DataType, phi::DenseTensor*)
3   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
4   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, paddle::experimental::DataType, unsigned long, bool) const
5   phi::DenseTensor::AllocateFrom(phi::Allocator*, paddle::experimental::DataType, unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  phi::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 910.073519GB memory on GPU 0, 7.566162GB memory has been allocated and available memory is only 24.182373GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
If the above ways do not solve the out of memory problem, you can try to use CUDA managed memory. The command is `export FLAGS_use_cuda_managed_memory=false`.
 (at /paddle/paddle/fluid/memory/allocation/cuda_allocator.cc:95)

run.sh: line 64: 58689 Aborted                 CUDA_VISIBLE_DEVICES=$device python train_single.py --output_dir ./checkpoints/ --prompt "$prompt" --max_seq_length $max_length --learning_rate 3e-6 --ppt_learning_rate 3e-5 --num_train_epochs 50 --logging_steps 10 --do_save True --do_test --eval_steps 200 --save_steps 200 --per_device_eval_batch_size 16 --per_device_train_batch_size 16 --model_name_or_path ernie-1.0-large-zh-cw --split_id few_all --task_name $task_name --metric_for_best_model accuracy --disable_tqdm True --do_train --do_predict --do_eval --load_best_model_at_end --pretrained "./checkpoints_cmnli/checkpoint-6000/model_state.pdparams" --evaluation_strategy epoch --save_strategy epoch
 
==========
csldcp
==========
 
[32m[2022-09-15 14:10:47,857] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-15 14:10:47,857] [    INFO][0m - ============================================================[0m
[32m[2022-09-15 14:10:47,857] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-15 14:10:47,857] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-15 14:10:47,857] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-15 14:10:47,858] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-15 14:10:47,858] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-15 14:10:47,858] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-15 14:10:47,858] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-15 14:10:47,858] [    INFO][0m - [0m
[32m[2022-09-15 14:10:47,858] [    INFO][0m - ============================================================[0m
[32m[2022-09-15 14:10:47,858] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-15 14:10:47,858] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-15 14:10:47,858] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-15 14:10:47,858] [    INFO][0m - pretrained                    :./checkpoints_cmnli/checkpoint-6000/model_state.pdparams[0m
[32m[2022-09-15 14:10:47,858] [    INFO][0m - prompt                        :‚Äú{'text':'text_a'}‚ÄùËøôÁØáÊñáÁåÆÁöÑÁ±ªÂà´ÊòØ{'mask'}{'mask'}„ÄÇ[0m
[32m[2022-09-15 14:10:47,858] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-15 14:10:47,858] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-15 14:10:47,858] [    INFO][0m - task_name                     :csldcp[0m
[32m[2022-09-15 14:10:47,859] [    INFO][0m - [0m
[32m[2022-09-15 14:10:47,859] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0915 14:10:47.860355 60661 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0915 14:10:47.864399 60661 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-15 14:10:54,490] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-15 14:10:55,588] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-15 14:10:55,588] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-15 14:10:55,589] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùËøôÁØáÊñáÁåÆÁöÑÁ±ªÂà´ÊòØ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': '„ÄÇ'}][0m
[32m[2022-09-15 14:10:57,752] [    INFO][0m - ============================================================[0m
[32m[2022-09-15 14:10:57,753] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-15 14:10:57,753] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-15 14:10:57,753] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-15 14:10:57,753] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-15 14:10:57,753] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-15 14:10:57,753] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-15 14:10:57,753] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-15 14:10:57,753] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-15 14:10:57,754] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-15 14:10:57,754] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-15 14:10:57,754] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-15 14:10:57,754] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-15 14:10:57,754] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-15 14:10:57,754] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-15 14:10:57,754] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-15 14:10:57,754] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-15 14:10:57,754] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-15 14:10:57,754] [    INFO][0m - eval_batch_size               :16[0m
[32m[2022-09-15 14:10:57,754] [    INFO][0m - eval_steps                    :200[0m
[32m[2022-09-15 14:10:57,754] [    INFO][0m - evaluation_strategy           :IntervalStrategy.EPOCH[0m
[32m[2022-09-15 14:10:57,755] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-15 14:10:57,755] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-15 14:10:57,755] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-15 14:10:57,755] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-15 14:10:57,755] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-15 14:10:57,755] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-15 14:10:57,755] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-15 14:10:57,755] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-15 14:10:57,755] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-15 14:10:57,755] [    INFO][0m - learning_rate                 :3e-06[0m
[32m[2022-09-15 14:10:57,755] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-15 14:10:57,755] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-15 14:10:57,755] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-15 14:10:57,755] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-15 14:10:57,756] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-15 14:10:57,756] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-15 14:10:57,756] [    INFO][0m - logging_dir                   :./checkpoints/runs/Sep15_14-10-47_instance-3bwob41y-01[0m
[32m[2022-09-15 14:10:57,756] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-15 14:10:57,756] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-15 14:10:57,756] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-15 14:10:57,756] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-15 14:10:57,756] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-15 14:10:57,756] [    INFO][0m - max_seq_length                :256[0m
[32m[2022-09-15 14:10:57,756] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-15 14:10:57,756] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-15 14:10:57,756] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-15 14:10:57,756] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-15 14:10:57,756] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-09-15 14:10:57,757] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-15 14:10:57,757] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-15 14:10:57,757] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-09-15 14:10:57,757] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-15 14:10:57,757] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-15 14:10:57,757] [    INFO][0m - per_device_eval_batch_size    :16[0m
[32m[2022-09-15 14:10:57,757] [    INFO][0m - per_device_train_batch_size   :16[0m
[32m[2022-09-15 14:10:57,757] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-15 14:10:57,757] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-15 14:10:57,757] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-15 14:10:57,757] [    INFO][0m - ppt_learning_rate             :3e-05[0m
[32m[2022-09-15 14:10:57,757] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-15 14:10:57,757] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-15 14:10:57,758] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-15 14:10:57,758] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-15 14:10:57,758] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-15 14:10:57,758] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-15 14:10:57,758] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-09-15 14:10:57,758] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-15 14:10:57,758] [    INFO][0m - save_steps                    :200[0m
[32m[2022-09-15 14:10:57,758] [    INFO][0m - save_strategy                 :IntervalStrategy.EPOCH[0m
[32m[2022-09-15 14:10:57,758] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-15 14:10:57,758] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-15 14:10:57,758] [    INFO][0m - seed                          :42[0m
[32m[2022-09-15 14:10:57,758] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-15 14:10:57,758] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-15 14:10:57,758] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-15 14:10:57,759] [    INFO][0m - train_batch_size              :16[0m
[32m[2022-09-15 14:10:57,759] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-15 14:10:57,759] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-15 14:10:57,759] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-15 14:10:57,759] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-15 14:10:57,759] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-15 14:10:57,759] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-15 14:10:57,759] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-15 14:10:57,759] [    INFO][0m - [0m
[32m[2022-09-15 14:10:57,762] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-15 14:10:57,762] [    INFO][0m -   Num examples = 2036[0m
[32m[2022-09-15 14:10:57,762] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-09-15 14:10:57,762] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2022-09-15 14:10:57,762] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2022-09-15 14:10:57,762] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-15 14:10:57,763] [    INFO][0m -   Total optimization steps = 6400.0[0m
[32m[2022-09-15 14:10:57,763] [    INFO][0m -   Total num train samples = 101800[0m
[33m[2022-09-15 14:10:57,802] [ WARNING][0m - Token indices sequence length is longer than the specified maximum sequence length for this model (517 > 512). Running this sequence through the model will result in indexing errors[0m
[32m[2022-09-15 14:11:05,409] [    INFO][0m - loss: 5.13248596, learning_rate: 2.9953125e-06, global_step: 10, interval_runtime: 7.645, interval_samples_per_second: 2.093, interval_steps_per_second: 1.308, epoch: 0.0781[0m
[32m[2022-09-15 14:11:11,925] [    INFO][0m - loss: 3.0286644, learning_rate: 2.990625e-06, global_step: 20, interval_runtime: 6.5167, interval_samples_per_second: 2.455, interval_steps_per_second: 1.535, epoch: 0.1562[0m
[32m[2022-09-15 14:11:18,418] [    INFO][0m - loss: 2.67984543, learning_rate: 2.9859375e-06, global_step: 30, interval_runtime: 6.4929, interval_samples_per_second: 2.464, interval_steps_per_second: 1.54, epoch: 0.2344[0m
[32m[2022-09-15 14:11:24,960] [    INFO][0m - loss: 2.52425156, learning_rate: 2.98125e-06, global_step: 40, interval_runtime: 6.5416, interval_samples_per_second: 2.446, interval_steps_per_second: 1.529, epoch: 0.3125[0m
[32m[2022-09-15 14:11:31,497] [    INFO][0m - loss: 2.38088913, learning_rate: 2.9765625e-06, global_step: 50, interval_runtime: 6.5372, interval_samples_per_second: 2.448, interval_steps_per_second: 1.53, epoch: 0.3906[0m
[32m[2022-09-15 14:11:38,007] [    INFO][0m - loss: 2.31837406, learning_rate: 2.971875e-06, global_step: 60, interval_runtime: 6.5104, interval_samples_per_second: 2.458, interval_steps_per_second: 1.536, epoch: 0.4688[0m
[32m[2022-09-15 14:11:44,540] [    INFO][0m - loss: 2.23650017, learning_rate: 2.9671875e-06, global_step: 70, interval_runtime: 6.5317, interval_samples_per_second: 2.45, interval_steps_per_second: 1.531, epoch: 0.5469[0m
[32m[2022-09-15 14:11:51,099] [    INFO][0m - loss: 2.03894672, learning_rate: 2.9625e-06, global_step: 80, interval_runtime: 6.5595, interval_samples_per_second: 2.439, interval_steps_per_second: 1.525, epoch: 0.625[0m
[32m[2022-09-15 14:11:57,645] [    INFO][0m - loss: 2.01696339, learning_rate: 2.9578125000000003e-06, global_step: 90, interval_runtime: 6.5464, interval_samples_per_second: 2.444, interval_steps_per_second: 1.528, epoch: 0.7031[0m
[32m[2022-09-15 14:12:04,180] [    INFO][0m - loss: 2.0433136, learning_rate: 2.953125e-06, global_step: 100, interval_runtime: 6.5345, interval_samples_per_second: 2.449, interval_steps_per_second: 1.53, epoch: 0.7812[0m
[32m[2022-09-15 14:12:10,747] [    INFO][0m - loss: 1.88973122, learning_rate: 2.9484375e-06, global_step: 110, interval_runtime: 6.5675, interval_samples_per_second: 2.436, interval_steps_per_second: 1.523, epoch: 0.8594[0m
[32m[2022-09-15 14:12:17,301] [    INFO][0m - loss: 1.91637993, learning_rate: 2.94375e-06, global_step: 120, interval_runtime: 6.5538, interval_samples_per_second: 2.441, interval_steps_per_second: 1.526, epoch: 0.9375[0m
[32m[2022-09-15 14:12:21,951] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 14:12:21,951] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-15 14:12:21,951] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 14:12:21,951] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 14:12:21,952] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-15 14:12:55,158] [    INFO][0m - eval_loss: 1.62920343875885, eval_accuracy: 0.4990328820116054, eval_runtime: 33.2064, eval_samples_per_second: 62.277, eval_steps_per_second: 3.915, epoch: 1.0[0m
[32m[2022-09-15 14:12:55,180] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-128[0m
[32m[2022-09-15 14:12:55,180] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 14:12:58,326] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-128/tokenizer_config.json[0m
[32m[2022-09-15 14:12:58,326] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-128/special_tokens_map.json[0m
[32m[2022-09-15 14:13:07,477] [    INFO][0m - loss: 2.07980728, learning_rate: 2.9390625000000003e-06, global_step: 130, interval_runtime: 50.1751, interval_samples_per_second: 0.319, interval_steps_per_second: 0.199, epoch: 1.0156[0m
[32m[2022-09-15 14:13:13,983] [    INFO][0m - loss: 1.46992741, learning_rate: 2.934375e-06, global_step: 140, interval_runtime: 6.5061, interval_samples_per_second: 2.459, interval_steps_per_second: 1.537, epoch: 1.0938[0m
[32m[2022-09-15 14:13:20,526] [    INFO][0m - loss: 1.68135605, learning_rate: 2.9296875e-06, global_step: 150, interval_runtime: 6.5434, interval_samples_per_second: 2.445, interval_steps_per_second: 1.528, epoch: 1.1719[0m
[32m[2022-09-15 14:13:27,077] [    INFO][0m - loss: 1.4816185, learning_rate: 2.925e-06, global_step: 160, interval_runtime: 6.5509, interval_samples_per_second: 2.442, interval_steps_per_second: 1.526, epoch: 1.25[0m
[32m[2022-09-15 14:13:33,647] [    INFO][0m - loss: 1.70761185, learning_rate: 2.9203125e-06, global_step: 170, interval_runtime: 6.5702, interval_samples_per_second: 2.435, interval_steps_per_second: 1.522, epoch: 1.3281[0m
[32m[2022-09-15 14:13:40,178] [    INFO][0m - loss: 1.61950245, learning_rate: 2.915625e-06, global_step: 180, interval_runtime: 6.5308, interval_samples_per_second: 2.45, interval_steps_per_second: 1.531, epoch: 1.4062[0m
[32m[2022-09-15 14:13:46,696] [    INFO][0m - loss: 1.55146551, learning_rate: 2.9109375e-06, global_step: 190, interval_runtime: 6.5182, interval_samples_per_second: 2.455, interval_steps_per_second: 1.534, epoch: 1.4844[0m
[32m[2022-09-15 14:13:53,211] [    INFO][0m - loss: 1.70956287, learning_rate: 2.90625e-06, global_step: 200, interval_runtime: 6.5158, interval_samples_per_second: 2.456, interval_steps_per_second: 1.535, epoch: 1.5625[0m
[32m[2022-09-15 14:13:59,725] [    INFO][0m - loss: 1.66714706, learning_rate: 2.9015625e-06, global_step: 210, interval_runtime: 6.5128, interval_samples_per_second: 2.457, interval_steps_per_second: 1.535, epoch: 1.6406[0m
[32m[2022-09-15 14:14:06,271] [    INFO][0m - loss: 1.54974632, learning_rate: 2.896875e-06, global_step: 220, interval_runtime: 6.5471, interval_samples_per_second: 2.444, interval_steps_per_second: 1.527, epoch: 1.7188[0m
[32m[2022-09-15 14:14:12,779] [    INFO][0m - loss: 1.58662434, learning_rate: 2.8921875e-06, global_step: 230, interval_runtime: 6.5078, interval_samples_per_second: 2.459, interval_steps_per_second: 1.537, epoch: 1.7969[0m
[32m[2022-09-15 14:14:19,305] [    INFO][0m - loss: 1.60412083, learning_rate: 2.8875000000000003e-06, global_step: 240, interval_runtime: 6.526, interval_samples_per_second: 2.452, interval_steps_per_second: 1.532, epoch: 1.875[0m
[32m[2022-09-15 14:14:25,838] [    INFO][0m - loss: 1.64252205, learning_rate: 2.8828125e-06, global_step: 250, interval_runtime: 6.5326, interval_samples_per_second: 2.449, interval_steps_per_second: 1.531, epoch: 1.9531[0m
[32m[2022-09-15 14:14:29,153] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 14:14:29,153] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-15 14:14:29,153] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 14:14:29,153] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 14:14:29,153] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-15 14:15:02,449] [    INFO][0m - eval_loss: 1.4075231552124023, eval_accuracy: 0.5338491295938105, eval_runtime: 33.2953, eval_samples_per_second: 62.111, eval_steps_per_second: 3.904, epoch: 2.0[0m
[32m[2022-09-15 14:15:02,479] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-256[0m
[32m[2022-09-15 14:15:02,479] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 14:15:06,264] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-256/tokenizer_config.json[0m
[32m[2022-09-15 14:15:06,264] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-256/special_tokens_map.json[0m
[32m[2022-09-15 14:15:15,336] [    INFO][0m - loss: 1.35199022, learning_rate: 2.878125e-06, global_step: 260, interval_runtime: 49.4982, interval_samples_per_second: 0.323, interval_steps_per_second: 0.202, epoch: 2.0312[0m
[32m[2022-09-15 14:15:21,806] [    INFO][0m - loss: 1.38641806, learning_rate: 2.8734375e-06, global_step: 270, interval_runtime: 6.4706, interval_samples_per_second: 2.473, interval_steps_per_second: 1.545, epoch: 2.1094[0m
[32m[2022-09-15 14:15:28,309] [    INFO][0m - loss: 1.3784502, learning_rate: 2.8687500000000003e-06, global_step: 280, interval_runtime: 6.503, interval_samples_per_second: 2.46, interval_steps_per_second: 1.538, epoch: 2.1875[0m
[32m[2022-09-15 14:15:34,832] [    INFO][0m - loss: 1.19663696, learning_rate: 2.8640625e-06, global_step: 290, interval_runtime: 6.5226, interval_samples_per_second: 2.453, interval_steps_per_second: 1.533, epoch: 2.2656[0m
[32m[2022-09-15 14:15:41,332] [    INFO][0m - loss: 1.51613817, learning_rate: 2.859375e-06, global_step: 300, interval_runtime: 6.4998, interval_samples_per_second: 2.462, interval_steps_per_second: 1.539, epoch: 2.3438[0m
[32m[2022-09-15 14:15:47,839] [    INFO][0m - loss: 1.38462276, learning_rate: 2.8546875e-06, global_step: 310, interval_runtime: 6.5068, interval_samples_per_second: 2.459, interval_steps_per_second: 1.537, epoch: 2.4219[0m
[32m[2022-09-15 14:15:54,360] [    INFO][0m - loss: 1.25804968, learning_rate: 2.85e-06, global_step: 320, interval_runtime: 6.5218, interval_samples_per_second: 2.453, interval_steps_per_second: 1.533, epoch: 2.5[0m
[32m[2022-09-15 14:16:00,866] [    INFO][0m - loss: 1.23204813, learning_rate: 2.8453125e-06, global_step: 330, interval_runtime: 6.5057, interval_samples_per_second: 2.459, interval_steps_per_second: 1.537, epoch: 2.5781[0m
[32m[2022-09-15 14:16:07,395] [    INFO][0m - loss: 1.26828356, learning_rate: 2.840625e-06, global_step: 340, interval_runtime: 6.5286, interval_samples_per_second: 2.451, interval_steps_per_second: 1.532, epoch: 2.6562[0m
[32m[2022-09-15 14:16:13,918] [    INFO][0m - loss: 1.2788744, learning_rate: 2.8359375e-06, global_step: 350, interval_runtime: 6.5237, interval_samples_per_second: 2.453, interval_steps_per_second: 1.533, epoch: 2.7344[0m
[32m[2022-09-15 14:16:20,442] [    INFO][0m - loss: 1.29277744, learning_rate: 2.83125e-06, global_step: 360, interval_runtime: 6.5235, interval_samples_per_second: 2.453, interval_steps_per_second: 1.533, epoch: 2.8125[0m
[32m[2022-09-15 14:16:26,970] [    INFO][0m - loss: 1.37155495, learning_rate: 2.8265625e-06, global_step: 370, interval_runtime: 6.5283, interval_samples_per_second: 2.451, interval_steps_per_second: 1.532, epoch: 2.8906[0m
[32m[2022-09-15 14:16:33,451] [    INFO][0m - loss: 1.29844093, learning_rate: 2.8218750000000004e-06, global_step: 380, interval_runtime: 6.48, interval_samples_per_second: 2.469, interval_steps_per_second: 1.543, epoch: 2.9688[0m
[32m[2022-09-15 14:16:35,520] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 14:16:35,520] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-15 14:16:35,520] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 14:16:35,520] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 14:16:35,521] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-15 14:17:08,394] [    INFO][0m - eval_loss: 1.290859580039978, eval_accuracy: 0.5517408123791102, eval_runtime: 32.8731, eval_samples_per_second: 62.908, eval_steps_per_second: 3.955, epoch: 3.0[0m
[32m[2022-09-15 14:17:08,421] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-384[0m
[32m[2022-09-15 14:17:08,421] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 14:17:11,717] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-384/tokenizer_config.json[0m
[32m[2022-09-15 14:17:11,717] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-384/special_tokens_map.json[0m
[32m[2022-09-15 14:17:21,832] [    INFO][0m - loss: 1.1907259, learning_rate: 2.8171875000000003e-06, global_step: 390, interval_runtime: 48.382, interval_samples_per_second: 0.331, interval_steps_per_second: 0.207, epoch: 3.0469[0m
[32m[2022-09-15 14:17:28,331] [    INFO][0m - loss: 1.23358974, learning_rate: 2.8125e-06, global_step: 400, interval_runtime: 6.4984, interval_samples_per_second: 2.462, interval_steps_per_second: 1.539, epoch: 3.125[0m
[32m[2022-09-15 14:17:34,816] [    INFO][0m - loss: 1.07821474, learning_rate: 2.8078125e-06, global_step: 410, interval_runtime: 6.4851, interval_samples_per_second: 2.467, interval_steps_per_second: 1.542, epoch: 3.2031[0m
[32m[2022-09-15 14:17:41,346] [    INFO][0m - loss: 0.98482246, learning_rate: 2.803125e-06, global_step: 420, interval_runtime: 6.5299, interval_samples_per_second: 2.45, interval_steps_per_second: 1.531, epoch: 3.2812[0m
[32m[2022-09-15 14:17:47,871] [    INFO][0m - loss: 0.97668819, learning_rate: 2.7984375000000003e-06, global_step: 430, interval_runtime: 6.5251, interval_samples_per_second: 2.452, interval_steps_per_second: 1.533, epoch: 3.3594[0m
[32m[2022-09-15 14:17:54,402] [    INFO][0m - loss: 0.9678957, learning_rate: 2.79375e-06, global_step: 440, interval_runtime: 6.5312, interval_samples_per_second: 2.45, interval_steps_per_second: 1.531, epoch: 3.4375[0m
[32m[2022-09-15 14:18:00,925] [    INFO][0m - loss: 0.92168713, learning_rate: 2.7890625e-06, global_step: 450, interval_runtime: 6.5229, interval_samples_per_second: 2.453, interval_steps_per_second: 1.533, epoch: 3.5156[0m
[32m[2022-09-15 14:18:07,453] [    INFO][0m - loss: 1.25181913, learning_rate: 2.784375e-06, global_step: 460, interval_runtime: 6.5283, interval_samples_per_second: 2.451, interval_steps_per_second: 1.532, epoch: 3.5938[0m
[32m[2022-09-15 14:18:13,961] [    INFO][0m - loss: 1.04909792, learning_rate: 2.7796875e-06, global_step: 470, interval_runtime: 6.5075, interval_samples_per_second: 2.459, interval_steps_per_second: 1.537, epoch: 3.6719[0m
[32m[2022-09-15 14:18:20,502] [    INFO][0m - loss: 1.19653149, learning_rate: 2.775e-06, global_step: 480, interval_runtime: 6.5404, interval_samples_per_second: 2.446, interval_steps_per_second: 1.529, epoch: 3.75[0m
[32m[2022-09-15 14:18:27,060] [    INFO][0m - loss: 1.15517368, learning_rate: 2.7703125e-06, global_step: 490, interval_runtime: 6.5581, interval_samples_per_second: 2.44, interval_steps_per_second: 1.525, epoch: 3.8281[0m
[32m[2022-09-15 14:18:37,459] [    INFO][0m - loss: 1.06780052, learning_rate: 2.765625e-06, global_step: 500, interval_runtime: 6.5551, interval_samples_per_second: 2.441, interval_steps_per_second: 1.526, epoch: 3.9062[0m
[32m[2022-09-15 14:18:43,897] [    INFO][0m - loss: 1.09586401, learning_rate: 2.7609375e-06, global_step: 510, interval_runtime: 10.2826, interval_samples_per_second: 1.556, interval_steps_per_second: 0.973, epoch: 3.9844[0m
[32m[2022-09-15 14:18:44,712] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 14:18:44,712] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-15 14:18:44,712] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 14:18:44,712] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 14:18:44,712] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-15 14:19:17,797] [    INFO][0m - eval_loss: 1.285066843032837, eval_accuracy: 0.566247582205029, eval_runtime: 33.0847, eval_samples_per_second: 62.506, eval_steps_per_second: 3.929, epoch: 4.0[0m
[32m[2022-09-15 14:19:17,832] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-512[0m
[32m[2022-09-15 14:19:17,832] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 14:19:20,535] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-512/tokenizer_config.json[0m
[32m[2022-09-15 14:19:20,536] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-512/special_tokens_map.json[0m
[32m[2022-09-15 14:19:32,611] [    INFO][0m - loss: 1.11126413, learning_rate: 2.75625e-06, global_step: 520, interval_runtime: 48.7134, interval_samples_per_second: 0.328, interval_steps_per_second: 0.205, epoch: 4.0625[0m
[32m[2022-09-15 14:19:39,237] [    INFO][0m - loss: 0.89029951, learning_rate: 2.7515625000000004e-06, global_step: 530, interval_runtime: 6.6268, interval_samples_per_second: 2.414, interval_steps_per_second: 1.509, epoch: 4.1406[0m
[32m[2022-09-15 14:19:45,751] [    INFO][0m - loss: 0.90699043, learning_rate: 2.7468750000000003e-06, global_step: 540, interval_runtime: 6.5138, interval_samples_per_second: 2.456, interval_steps_per_second: 1.535, epoch: 4.2188[0m
[32m[2022-09-15 14:19:52,263] [    INFO][0m - loss: 0.879251, learning_rate: 2.7421875e-06, global_step: 550, interval_runtime: 6.5113, interval_samples_per_second: 2.457, interval_steps_per_second: 1.536, epoch: 4.2969[0m
[32m[2022-09-15 14:19:58,768] [    INFO][0m - loss: 0.84107122, learning_rate: 2.7375e-06, global_step: 560, interval_runtime: 6.5056, interval_samples_per_second: 2.459, interval_steps_per_second: 1.537, epoch: 4.375[0m
[32m[2022-09-15 14:20:05,305] [    INFO][0m - loss: 1.12620735, learning_rate: 2.7328125e-06, global_step: 570, interval_runtime: 6.537, interval_samples_per_second: 2.448, interval_steps_per_second: 1.53, epoch: 4.4531[0m
[32m[2022-09-15 14:20:11,865] [    INFO][0m - loss: 1.02236986, learning_rate: 2.7281250000000002e-06, global_step: 580, interval_runtime: 6.5602, interval_samples_per_second: 2.439, interval_steps_per_second: 1.524, epoch: 4.5312[0m
[32m[2022-09-15 14:20:18,385] [    INFO][0m - loss: 1.04302502, learning_rate: 2.7234375e-06, global_step: 590, interval_runtime: 6.5196, interval_samples_per_second: 2.454, interval_steps_per_second: 1.534, epoch: 4.6094[0m
[32m[2022-09-15 14:20:24,919] [    INFO][0m - loss: 1.01706581, learning_rate: 2.71875e-06, global_step: 600, interval_runtime: 6.5344, interval_samples_per_second: 2.449, interval_steps_per_second: 1.53, epoch: 4.6875[0m
[32m[2022-09-15 14:20:31,440] [    INFO][0m - loss: 0.82062035, learning_rate: 2.7140625e-06, global_step: 610, interval_runtime: 6.5208, interval_samples_per_second: 2.454, interval_steps_per_second: 1.534, epoch: 4.7656[0m
[32m[2022-09-15 14:20:37,971] [    INFO][0m - loss: 0.90208893, learning_rate: 2.7093749999999998e-06, global_step: 620, interval_runtime: 6.5308, interval_samples_per_second: 2.45, interval_steps_per_second: 1.531, epoch: 4.8438[0m
[32m[2022-09-15 14:20:44,496] [    INFO][0m - loss: 0.96115637, learning_rate: 2.7046875e-06, global_step: 630, interval_runtime: 6.5251, interval_samples_per_second: 2.452, interval_steps_per_second: 1.533, epoch: 4.9219[0m
[32m[2022-09-15 14:20:50,435] [    INFO][0m - loss: 1.12913742, learning_rate: 2.7e-06, global_step: 640, interval_runtime: 5.9392, interval_samples_per_second: 2.694, interval_steps_per_second: 1.684, epoch: 5.0[0m
[32m[2022-09-15 14:20:50,436] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 14:20:50,436] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-15 14:20:50,436] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 14:20:50,436] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 14:20:50,436] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-15 14:21:23,514] [    INFO][0m - eval_loss: 1.2678351402282715, eval_accuracy: 0.5643133462282398, eval_runtime: 33.0776, eval_samples_per_second: 62.52, eval_steps_per_second: 3.93, epoch: 5.0[0m
[32m[2022-09-15 14:21:23,550] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-640[0m
[32m[2022-09-15 14:21:23,551] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 14:21:26,727] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-640/tokenizer_config.json[0m
[32m[2022-09-15 14:21:26,727] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-640/special_tokens_map.json[0m
[32m[2022-09-15 14:21:40,300] [    INFO][0m - loss: 0.8265583, learning_rate: 2.6953125e-06, global_step: 650, interval_runtime: 49.865, interval_samples_per_second: 0.321, interval_steps_per_second: 0.201, epoch: 5.0781[0m
[32m[2022-09-15 14:21:46,807] [    INFO][0m - loss: 0.79625311, learning_rate: 2.690625e-06, global_step: 660, interval_runtime: 6.5068, interval_samples_per_second: 2.459, interval_steps_per_second: 1.537, epoch: 5.1562[0m
[32m[2022-09-15 14:21:53,340] [    INFO][0m - loss: 0.8008194, learning_rate: 2.6859375e-06, global_step: 670, interval_runtime: 6.5323, interval_samples_per_second: 2.449, interval_steps_per_second: 1.531, epoch: 5.2344[0m
[32m[2022-09-15 14:21:59,863] [    INFO][0m - loss: 0.87232361, learning_rate: 2.6812500000000004e-06, global_step: 680, interval_runtime: 6.5239, interval_samples_per_second: 2.453, interval_steps_per_second: 1.533, epoch: 5.3125[0m
[32m[2022-09-15 14:22:06,386] [    INFO][0m - loss: 0.87478418, learning_rate: 2.6765625000000003e-06, global_step: 690, interval_runtime: 6.5224, interval_samples_per_second: 2.453, interval_steps_per_second: 1.533, epoch: 5.3906[0m
[32m[2022-09-15 14:22:12,904] [    INFO][0m - loss: 0.80987129, learning_rate: 2.671875e-06, global_step: 700, interval_runtime: 6.5186, interval_samples_per_second: 2.455, interval_steps_per_second: 1.534, epoch: 5.4688[0m
[32m[2022-09-15 14:22:19,438] [    INFO][0m - loss: 0.8410429, learning_rate: 2.6671875e-06, global_step: 710, interval_runtime: 6.5341, interval_samples_per_second: 2.449, interval_steps_per_second: 1.53, epoch: 5.5469[0m
[32m[2022-09-15 14:22:25,945] [    INFO][0m - loss: 0.71275883, learning_rate: 2.6625e-06, global_step: 720, interval_runtime: 6.5062, interval_samples_per_second: 2.459, interval_steps_per_second: 1.537, epoch: 5.625[0m
[32m[2022-09-15 14:22:32,457] [    INFO][0m - loss: 0.77946234, learning_rate: 2.6578125000000002e-06, global_step: 730, interval_runtime: 6.5119, interval_samples_per_second: 2.457, interval_steps_per_second: 1.536, epoch: 5.7031[0m
[32m[2022-09-15 14:22:38,982] [    INFO][0m - loss: 0.873598, learning_rate: 2.653125e-06, global_step: 740, interval_runtime: 6.5259, interval_samples_per_second: 2.452, interval_steps_per_second: 1.532, epoch: 5.7812[0m
[32m[2022-09-15 14:22:45,500] [    INFO][0m - loss: 0.68971367, learning_rate: 2.6484375e-06, global_step: 750, interval_runtime: 6.5174, interval_samples_per_second: 2.455, interval_steps_per_second: 1.534, epoch: 5.8594[0m
[32m[2022-09-15 14:22:52,047] [    INFO][0m - loss: 0.68318214, learning_rate: 2.64375e-06, global_step: 760, interval_runtime: 6.5473, interval_samples_per_second: 2.444, interval_steps_per_second: 1.527, epoch: 5.9375[0m
[32m[2022-09-15 14:22:56,668] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 14:22:56,668] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-15 14:22:56,668] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 14:22:56,668] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 14:22:56,668] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-15 14:23:29,601] [    INFO][0m - eval_loss: 1.2984508275985718, eval_accuracy: 0.5715667311411993, eval_runtime: 32.9318, eval_samples_per_second: 62.796, eval_steps_per_second: 3.948, epoch: 6.0[0m
[32m[2022-09-15 14:23:29,636] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-768[0m
[32m[2022-09-15 14:23:29,636] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 14:23:32,976] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-768/tokenizer_config.json[0m
[32m[2022-09-15 14:23:32,976] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-768/special_tokens_map.json[0m
[32m[2022-09-15 14:23:41,014] [    INFO][0m - loss: 0.67725921, learning_rate: 2.6390624999999998e-06, global_step: 770, interval_runtime: 48.9673, interval_samples_per_second: 0.327, interval_steps_per_second: 0.204, epoch: 6.0156[0m
[32m[2022-09-15 14:23:47,535] [    INFO][0m - loss: 0.4820075, learning_rate: 2.634375e-06, global_step: 780, interval_runtime: 6.5208, interval_samples_per_second: 2.454, interval_steps_per_second: 1.534, epoch: 6.0938[0m
[32m[2022-09-15 14:23:54,035] [    INFO][0m - loss: 0.61226964, learning_rate: 2.6296875e-06, global_step: 790, interval_runtime: 6.4999, interval_samples_per_second: 2.462, interval_steps_per_second: 1.538, epoch: 6.1719[0m
[32m[2022-09-15 14:24:00,553] [    INFO][0m - loss: 0.6837203, learning_rate: 2.6250000000000003e-06, global_step: 800, interval_runtime: 6.5171, interval_samples_per_second: 2.455, interval_steps_per_second: 1.534, epoch: 6.25[0m
[32m[2022-09-15 14:24:07,111] [    INFO][0m - loss: 0.61712885, learning_rate: 2.6203125e-06, global_step: 810, interval_runtime: 6.5591, interval_samples_per_second: 2.439, interval_steps_per_second: 1.525, epoch: 6.3281[0m
[32m[2022-09-15 14:24:15,583] [    INFO][0m - loss: 0.65112, learning_rate: 2.615625e-06, global_step: 820, interval_runtime: 6.5801, interval_samples_per_second: 2.432, interval_steps_per_second: 1.52, epoch: 6.4062[0m
[32m[2022-09-15 14:24:22,073] [    INFO][0m - loss: 0.66079879, learning_rate: 2.6109375000000004e-06, global_step: 830, interval_runtime: 8.3818, interval_samples_per_second: 1.909, interval_steps_per_second: 1.193, epoch: 6.4844[0m
[32m[2022-09-15 14:24:28,571] [    INFO][0m - loss: 0.86652393, learning_rate: 2.6062500000000002e-06, global_step: 840, interval_runtime: 6.4975, interval_samples_per_second: 2.462, interval_steps_per_second: 1.539, epoch: 6.5625[0m
[32m[2022-09-15 14:24:35,072] [    INFO][0m - loss: 0.57594419, learning_rate: 2.6015625e-06, global_step: 850, interval_runtime: 6.5008, interval_samples_per_second: 2.461, interval_steps_per_second: 1.538, epoch: 6.6406[0m
[32m[2022-09-15 14:24:41,600] [    INFO][0m - loss: 0.66400776, learning_rate: 2.596875e-06, global_step: 860, interval_runtime: 6.5275, interval_samples_per_second: 2.451, interval_steps_per_second: 1.532, epoch: 6.7188[0m
[32m[2022-09-15 14:24:48,132] [    INFO][0m - loss: 0.67786999, learning_rate: 2.5921875e-06, global_step: 870, interval_runtime: 6.5324, interval_samples_per_second: 2.449, interval_steps_per_second: 1.531, epoch: 6.7969[0m
[32m[2022-09-15 14:24:54,680] [    INFO][0m - loss: 0.6706943, learning_rate: 2.5875000000000002e-06, global_step: 880, interval_runtime: 6.5478, interval_samples_per_second: 2.444, interval_steps_per_second: 1.527, epoch: 6.875[0m
[32m[2022-09-15 14:25:01,204] [    INFO][0m - loss: 0.82318621, learning_rate: 2.5828125e-06, global_step: 890, interval_runtime: 6.5249, interval_samples_per_second: 2.452, interval_steps_per_second: 1.533, epoch: 6.9531[0m
[32m[2022-09-15 14:25:04,523] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 14:25:04,524] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-15 14:25:04,524] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 14:25:04,524] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 14:25:04,524] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-15 14:25:37,491] [    INFO][0m - eval_loss: 1.2970150709152222, eval_accuracy: 0.574468085106383, eval_runtime: 32.9667, eval_samples_per_second: 62.73, eval_steps_per_second: 3.943, epoch: 7.0[0m
[32m[2022-09-15 14:25:37,526] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-896[0m
[32m[2022-09-15 14:25:37,526] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 14:25:40,776] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-896/tokenizer_config.json[0m
[32m[2022-09-15 14:25:40,777] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-896/special_tokens_map.json[0m
[32m[2022-09-15 14:25:49,633] [    INFO][0m - loss: 0.67529283, learning_rate: 2.578125e-06, global_step: 900, interval_runtime: 48.4289, interval_samples_per_second: 0.33, interval_steps_per_second: 0.206, epoch: 7.0312[0m
[32m[2022-09-15 14:25:56,156] [    INFO][0m - loss: 0.49628921, learning_rate: 2.5734375e-06, global_step: 910, interval_runtime: 6.5229, interval_samples_per_second: 2.453, interval_steps_per_second: 1.533, epoch: 7.1094[0m
[32m[2022-09-15 14:26:02,671] [    INFO][0m - loss: 0.53920479, learning_rate: 2.5687499999999998e-06, global_step: 920, interval_runtime: 6.514, interval_samples_per_second: 2.456, interval_steps_per_second: 1.535, epoch: 7.1875[0m
[32m[2022-09-15 14:26:09,204] [    INFO][0m - loss: 0.46367888, learning_rate: 2.5640625e-06, global_step: 930, interval_runtime: 6.5337, interval_samples_per_second: 2.449, interval_steps_per_second: 1.531, epoch: 7.2656[0m
[32m[2022-09-15 14:26:15,732] [    INFO][0m - loss: 0.62212811, learning_rate: 2.559375e-06, global_step: 940, interval_runtime: 6.528, interval_samples_per_second: 2.451, interval_steps_per_second: 1.532, epoch: 7.3438[0m
[32m[2022-09-15 14:26:22,271] [    INFO][0m - loss: 0.46659408, learning_rate: 2.5546875000000003e-06, global_step: 950, interval_runtime: 6.5388, interval_samples_per_second: 2.447, interval_steps_per_second: 1.529, epoch: 7.4219[0m
[32m[2022-09-15 14:26:29,734] [    INFO][0m - loss: 0.57659526, learning_rate: 2.55e-06, global_step: 960, interval_runtime: 6.5611, interval_samples_per_second: 2.439, interval_steps_per_second: 1.524, epoch: 7.5[0m
[32m[2022-09-15 14:26:37,438] [    INFO][0m - loss: 0.46827574, learning_rate: 2.5453125e-06, global_step: 970, interval_runtime: 7.4467, interval_samples_per_second: 2.149, interval_steps_per_second: 1.343, epoch: 7.5781[0m
[32m[2022-09-15 14:26:44,004] [    INFO][0m - loss: 0.50113144, learning_rate: 2.5406250000000003e-06, global_step: 980, interval_runtime: 7.725, interval_samples_per_second: 2.071, interval_steps_per_second: 1.294, epoch: 7.6562[0m
[32m[2022-09-15 14:26:50,530] [    INFO][0m - loss: 0.56087451, learning_rate: 2.5359375000000002e-06, global_step: 990, interval_runtime: 6.5261, interval_samples_per_second: 2.452, interval_steps_per_second: 1.532, epoch: 7.7344[0m
[32m[2022-09-15 14:26:57,061] [    INFO][0m - loss: 0.51735044, learning_rate: 2.53125e-06, global_step: 1000, interval_runtime: 6.531, interval_samples_per_second: 2.45, interval_steps_per_second: 1.531, epoch: 7.8125[0m
[32m[2022-09-15 14:27:03,587] [    INFO][0m - loss: 0.59776382, learning_rate: 2.5265625e-06, global_step: 1010, interval_runtime: 6.5263, interval_samples_per_second: 2.452, interval_steps_per_second: 1.532, epoch: 7.8906[0m
[32m[2022-09-15 14:27:10,071] [    INFO][0m - loss: 0.7219079, learning_rate: 2.521875e-06, global_step: 1020, interval_runtime: 6.4836, interval_samples_per_second: 2.468, interval_steps_per_second: 1.542, epoch: 7.9688[0m
[32m[2022-09-15 14:27:12,132] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 14:27:12,133] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-15 14:27:12,133] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 14:27:12,133] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 14:27:12,133] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-15 14:27:45,209] [    INFO][0m - eval_loss: 1.3831849098205566, eval_accuracy: 0.5739845261121856, eval_runtime: 33.0758, eval_samples_per_second: 62.523, eval_steps_per_second: 3.93, epoch: 8.0[0m
[32m[2022-09-15 14:27:45,243] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1024[0m
[32m[2022-09-15 14:27:45,244] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 14:27:48,076] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1024/tokenizer_config.json[0m
[32m[2022-09-15 14:27:48,077] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1024/special_tokens_map.json[0m
[32m[2022-09-15 14:27:58,121] [    INFO][0m - loss: 0.44454613, learning_rate: 2.5171875e-06, global_step: 1030, interval_runtime: 48.0498, interval_samples_per_second: 0.333, interval_steps_per_second: 0.208, epoch: 8.0469[0m
[32m[2022-09-15 14:28:04,622] [    INFO][0m - loss: 0.41902156, learning_rate: 2.5125e-06, global_step: 1040, interval_runtime: 6.5016, interval_samples_per_second: 2.461, interval_steps_per_second: 1.538, epoch: 8.125[0m
[32m[2022-09-15 14:28:11,140] [    INFO][0m - loss: 0.3835711, learning_rate: 2.5078125e-06, global_step: 1050, interval_runtime: 6.5172, interval_samples_per_second: 2.455, interval_steps_per_second: 1.534, epoch: 8.2031[0m
[32m[2022-09-15 14:28:17,664] [    INFO][0m - loss: 0.41307645, learning_rate: 2.503125e-06, global_step: 1060, interval_runtime: 6.5243, interval_samples_per_second: 2.452, interval_steps_per_second: 1.533, epoch: 8.2812[0m
[32m[2022-09-15 14:28:24,203] [    INFO][0m - loss: 0.47217121, learning_rate: 2.4984374999999997e-06, global_step: 1070, interval_runtime: 6.5392, interval_samples_per_second: 2.447, interval_steps_per_second: 1.529, epoch: 8.3594[0m
[32m[2022-09-15 14:28:30,722] [    INFO][0m - loss: 0.45733905, learning_rate: 2.49375e-06, global_step: 1080, interval_runtime: 6.519, interval_samples_per_second: 2.454, interval_steps_per_second: 1.534, epoch: 8.4375[0m
[32m[2022-09-15 14:28:37,236] [    INFO][0m - loss: 0.47494001, learning_rate: 2.4890625e-06, global_step: 1090, interval_runtime: 6.5145, interval_samples_per_second: 2.456, interval_steps_per_second: 1.535, epoch: 8.5156[0m
[32m[2022-09-15 14:28:43,761] [    INFO][0m - loss: 0.44246912, learning_rate: 2.4843750000000002e-06, global_step: 1100, interval_runtime: 6.5245, interval_samples_per_second: 2.452, interval_steps_per_second: 1.533, epoch: 8.5938[0m
[32m[2022-09-15 14:28:50,301] [    INFO][0m - loss: 0.36775982, learning_rate: 2.4796875e-06, global_step: 1110, interval_runtime: 6.54, interval_samples_per_second: 2.446, interval_steps_per_second: 1.529, epoch: 8.6719[0m
[32m[2022-09-15 14:28:56,822] [    INFO][0m - loss: 0.49450264, learning_rate: 2.475e-06, global_step: 1120, interval_runtime: 6.5211, interval_samples_per_second: 2.454, interval_steps_per_second: 1.533, epoch: 8.75[0m
[32m[2022-09-15 14:29:03,363] [    INFO][0m - loss: 0.28444896, learning_rate: 2.4703125000000003e-06, global_step: 1130, interval_runtime: 6.5408, interval_samples_per_second: 2.446, interval_steps_per_second: 1.529, epoch: 8.8281[0m
[32m[2022-09-15 14:29:09,903] [    INFO][0m - loss: 0.42578478, learning_rate: 2.4656250000000002e-06, global_step: 1140, interval_runtime: 6.5401, interval_samples_per_second: 2.446, interval_steps_per_second: 1.529, epoch: 8.9062[0m
[32m[2022-09-15 14:29:16,381] [    INFO][0m - loss: 0.42185178, learning_rate: 2.4609375e-06, global_step: 1150, interval_runtime: 6.4782, interval_samples_per_second: 2.47, interval_steps_per_second: 1.544, epoch: 8.9844[0m
[32m[2022-09-15 14:29:17,192] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 14:29:17,192] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-15 14:29:17,192] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 14:29:17,192] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 14:29:17,192] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-15 14:29:50,090] [    INFO][0m - eval_loss: 1.4350433349609375, eval_accuracy: 0.5851063829787234, eval_runtime: 32.8975, eval_samples_per_second: 62.862, eval_steps_per_second: 3.952, epoch: 9.0[0m
[32m[2022-09-15 14:29:50,125] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1152[0m
[32m[2022-09-15 14:29:50,125] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 14:29:52,945] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1152/tokenizer_config.json[0m
[32m[2022-09-15 14:29:52,946] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1152/special_tokens_map.json[0m
[32m[2022-09-15 14:30:05,303] [    INFO][0m - loss: 0.39338408, learning_rate: 2.45625e-06, global_step: 1160, interval_runtime: 48.9218, interval_samples_per_second: 0.327, interval_steps_per_second: 0.204, epoch: 9.0625[0m
[32m[2022-09-15 14:30:11,814] [    INFO][0m - loss: 0.24201269, learning_rate: 2.4515625e-06, global_step: 1170, interval_runtime: 6.511, interval_samples_per_second: 2.457, interval_steps_per_second: 1.536, epoch: 9.1406[0m
[32m[2022-09-15 14:30:18,334] [    INFO][0m - loss: 0.30827625, learning_rate: 2.446875e-06, global_step: 1180, interval_runtime: 6.5196, interval_samples_per_second: 2.454, interval_steps_per_second: 1.534, epoch: 9.2188[0m
[32m[2022-09-15 14:30:24,837] [    INFO][0m - loss: 0.26401908, learning_rate: 2.4421875e-06, global_step: 1190, interval_runtime: 6.5034, interval_samples_per_second: 2.46, interval_steps_per_second: 1.538, epoch: 9.2969[0m
[32m[2022-09-15 14:30:31,360] [    INFO][0m - loss: 0.33069606, learning_rate: 2.4375e-06, global_step: 1200, interval_runtime: 6.5232, interval_samples_per_second: 2.453, interval_steps_per_second: 1.533, epoch: 9.375[0m
[32m[2022-09-15 14:30:37,891] [    INFO][0m - loss: 0.40293531, learning_rate: 2.4328125e-06, global_step: 1210, interval_runtime: 6.5308, interval_samples_per_second: 2.45, interval_steps_per_second: 1.531, epoch: 9.4531[0m
[32m[2022-09-15 14:30:44,399] [    INFO][0m - loss: 0.38864446, learning_rate: 2.4281249999999997e-06, global_step: 1220, interval_runtime: 6.5077, interval_samples_per_second: 2.459, interval_steps_per_second: 1.537, epoch: 9.5312[0m
[32m[2022-09-15 14:30:50,923] [    INFO][0m - loss: 0.37282474, learning_rate: 2.4234375e-06, global_step: 1230, interval_runtime: 6.5247, interval_samples_per_second: 2.452, interval_steps_per_second: 1.533, epoch: 9.6094[0m
[32m[2022-09-15 14:30:57,444] [    INFO][0m - loss: 0.25877028, learning_rate: 2.41875e-06, global_step: 1240, interval_runtime: 6.5202, interval_samples_per_second: 2.454, interval_steps_per_second: 1.534, epoch: 9.6875[0m
[32m[2022-09-15 14:31:03,969] [    INFO][0m - loss: 0.41535101, learning_rate: 2.4140625000000002e-06, global_step: 1250, interval_runtime: 6.5248, interval_samples_per_second: 2.452, interval_steps_per_second: 1.533, epoch: 9.7656[0m
[32m[2022-09-15 14:31:10,483] [    INFO][0m - loss: 0.33150091, learning_rate: 2.409375e-06, global_step: 1260, interval_runtime: 6.5145, interval_samples_per_second: 2.456, interval_steps_per_second: 1.535, epoch: 9.8438[0m
[32m[2022-09-15 14:31:17,016] [    INFO][0m - loss: 0.42823277, learning_rate: 2.4046875e-06, global_step: 1270, interval_runtime: 6.5334, interval_samples_per_second: 2.449, interval_steps_per_second: 1.531, epoch: 9.9219[0m
[32m[2022-09-15 14:31:22,972] [    INFO][0m - loss: 0.42424064, learning_rate: 2.4000000000000003e-06, global_step: 1280, interval_runtime: 5.9555, interval_samples_per_second: 2.687, interval_steps_per_second: 1.679, epoch: 10.0[0m
[32m[2022-09-15 14:31:22,973] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 14:31:22,973] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-15 14:31:22,973] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 14:31:22,973] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 14:31:22,973] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-15 14:31:55,905] [    INFO][0m - eval_loss: 1.529868721961975, eval_accuracy: 0.5725338491295938, eval_runtime: 32.9317, eval_samples_per_second: 62.797, eval_steps_per_second: 3.948, epoch: 10.0[0m
[32m[2022-09-15 14:31:55,942] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1280[0m
[32m[2022-09-15 14:31:55,942] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 14:31:58,866] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1280/tokenizer_config.json[0m
[32m[2022-09-15 14:31:58,866] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1280/special_tokens_map.json[0m
[32m[2022-09-15 14:32:10,844] [    INFO][0m - loss: 0.25707541, learning_rate: 2.3953125e-06, global_step: 1290, interval_runtime: 47.8721, interval_samples_per_second: 0.334, interval_steps_per_second: 0.209, epoch: 10.0781[0m
[32m[2022-09-15 14:32:17,379] [    INFO][0m - loss: 0.27392375, learning_rate: 2.390625e-06, global_step: 1300, interval_runtime: 6.5347, interval_samples_per_second: 2.448, interval_steps_per_second: 1.53, epoch: 10.1562[0m
[32m[2022-09-15 14:32:23,906] [    INFO][0m - loss: 0.22913392, learning_rate: 2.3859375e-06, global_step: 1310, interval_runtime: 6.5276, interval_samples_per_second: 2.451, interval_steps_per_second: 1.532, epoch: 10.2344[0m
[32m[2022-09-15 14:32:30,396] [    INFO][0m - loss: 0.25603559, learning_rate: 2.38125e-06, global_step: 1320, interval_runtime: 6.4896, interval_samples_per_second: 2.465, interval_steps_per_second: 1.541, epoch: 10.3125[0m
[32m[2022-09-15 14:32:36,909] [    INFO][0m - loss: 0.237466, learning_rate: 2.3765625e-06, global_step: 1330, interval_runtime: 6.5129, interval_samples_per_second: 2.457, interval_steps_per_second: 1.535, epoch: 10.3906[0m
[32m[2022-09-15 14:32:43,447] [    INFO][0m - loss: 0.24108558, learning_rate: 2.371875e-06, global_step: 1340, interval_runtime: 6.5384, interval_samples_per_second: 2.447, interval_steps_per_second: 1.529, epoch: 10.4688[0m
[32m[2022-09-15 14:32:49,962] [    INFO][0m - loss: 0.22035077, learning_rate: 2.3671875e-06, global_step: 1350, interval_runtime: 6.5147, interval_samples_per_second: 2.456, interval_steps_per_second: 1.535, epoch: 10.5469[0m
[32m[2022-09-15 14:32:56,516] [    INFO][0m - loss: 0.31129563, learning_rate: 2.3625e-06, global_step: 1360, interval_runtime: 6.5543, interval_samples_per_second: 2.441, interval_steps_per_second: 1.526, epoch: 10.625[0m
[32m[2022-09-15 14:33:03,056] [    INFO][0m - loss: 0.29613843, learning_rate: 2.3578125e-06, global_step: 1370, interval_runtime: 6.5393, interval_samples_per_second: 2.447, interval_steps_per_second: 1.529, epoch: 10.7031[0m
[32m[2022-09-15 14:33:09,585] [    INFO][0m - loss: 0.2341996, learning_rate: 2.353125e-06, global_step: 1380, interval_runtime: 6.5295, interval_samples_per_second: 2.45, interval_steps_per_second: 1.532, epoch: 10.7812[0m
[32m[2022-09-15 14:33:16,126] [    INFO][0m - loss: 0.28600864, learning_rate: 2.3484375000000003e-06, global_step: 1390, interval_runtime: 6.5407, interval_samples_per_second: 2.446, interval_steps_per_second: 1.529, epoch: 10.8594[0m
[32m[2022-09-15 14:33:22,662] [    INFO][0m - loss: 0.29936278, learning_rate: 2.3437500000000002e-06, global_step: 1400, interval_runtime: 6.5362, interval_samples_per_second: 2.448, interval_steps_per_second: 1.53, epoch: 10.9375[0m
[32m[2022-09-15 14:33:27,314] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 14:33:27,315] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-15 14:33:27,315] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 14:33:27,315] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 14:33:27,315] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-15 14:34:00,317] [    INFO][0m - eval_loss: 1.5636439323425293, eval_accuracy: 0.5672147001934236, eval_runtime: 33.0013, eval_samples_per_second: 62.664, eval_steps_per_second: 3.939, epoch: 11.0[0m
[32m[2022-09-15 14:34:00,351] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1408[0m
[32m[2022-09-15 14:34:00,352] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 14:34:03,438] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1408/tokenizer_config.json[0m
[32m[2022-09-15 14:34:03,438] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1408/special_tokens_map.json[0m
[32m[2022-09-15 14:34:11,902] [    INFO][0m - loss: 0.25987597, learning_rate: 2.3390625e-06, global_step: 1410, interval_runtime: 49.2394, interval_samples_per_second: 0.325, interval_steps_per_second: 0.203, epoch: 11.0156[0m
[32m[2022-09-15 14:34:18,605] [    INFO][0m - loss: 0.214621, learning_rate: 2.334375e-06, global_step: 1420, interval_runtime: 6.5131, interval_samples_per_second: 2.457, interval_steps_per_second: 1.535, epoch: 11.0938[0m
[32m[2022-09-15 14:34:25,130] [    INFO][0m - loss: 0.17384081, learning_rate: 2.3296875000000003e-06, global_step: 1430, interval_runtime: 6.7151, interval_samples_per_second: 2.383, interval_steps_per_second: 1.489, epoch: 11.1719[0m
[32m[2022-09-15 14:34:31,655] [    INFO][0m - loss: 0.16224066, learning_rate: 2.325e-06, global_step: 1440, interval_runtime: 6.5257, interval_samples_per_second: 2.452, interval_steps_per_second: 1.532, epoch: 11.25[0m
[32m[2022-09-15 14:34:41,408] [    INFO][0m - loss: 0.23671882, learning_rate: 2.3203125e-06, global_step: 1450, interval_runtime: 6.5478, interval_samples_per_second: 2.444, interval_steps_per_second: 1.527, epoch: 11.3281[0m
[32m[2022-09-15 14:34:47,933] [    INFO][0m - loss: 0.17586865, learning_rate: 2.315625e-06, global_step: 1460, interval_runtime: 9.7294, interval_samples_per_second: 1.644, interval_steps_per_second: 1.028, epoch: 11.4062[0m
[32m[2022-09-15 14:34:54,478] [    INFO][0m - loss: 0.25849154, learning_rate: 2.3109375e-06, global_step: 1470, interval_runtime: 6.545, interval_samples_per_second: 2.445, interval_steps_per_second: 1.528, epoch: 11.4844[0m
[32m[2022-09-15 14:35:01,020] [    INFO][0m - loss: 0.29041812, learning_rate: 2.30625e-06, global_step: 1480, interval_runtime: 6.5421, interval_samples_per_second: 2.446, interval_steps_per_second: 1.529, epoch: 11.5625[0m
[32m[2022-09-15 14:35:07,574] [    INFO][0m - loss: 0.22155406, learning_rate: 2.3015625e-06, global_step: 1490, interval_runtime: 6.5547, interval_samples_per_second: 2.441, interval_steps_per_second: 1.526, epoch: 11.6406[0m
[32m[2022-09-15 14:35:14,132] [    INFO][0m - loss: 0.21424062, learning_rate: 2.296875e-06, global_step: 1500, interval_runtime: 6.5572, interval_samples_per_second: 2.44, interval_steps_per_second: 1.525, epoch: 11.7188[0m
[32m[2022-09-15 14:35:20,689] [    INFO][0m - loss: 0.17296708, learning_rate: 2.2921875e-06, global_step: 1510, interval_runtime: 6.5569, interval_samples_per_second: 2.44, interval_steps_per_second: 1.525, epoch: 11.7969[0m
[32m[2022-09-15 14:35:27,215] [    INFO][0m - loss: 0.19141703, learning_rate: 2.2875e-06, global_step: 1520, interval_runtime: 6.5264, interval_samples_per_second: 2.452, interval_steps_per_second: 1.532, epoch: 11.875[0m
[32m[2022-09-15 14:35:33,763] [    INFO][0m - loss: 0.15467952, learning_rate: 2.2828125e-06, global_step: 1530, interval_runtime: 6.5484, interval_samples_per_second: 2.443, interval_steps_per_second: 1.527, epoch: 11.9531[0m
[32m[2022-09-15 14:35:37,091] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 14:35:37,091] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-15 14:35:37,091] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 14:35:37,091] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 14:35:37,091] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-15 14:36:09,996] [    INFO][0m - eval_loss: 1.6802445650100708, eval_accuracy: 0.5710831721470019, eval_runtime: 32.9051, eval_samples_per_second: 62.847, eval_steps_per_second: 3.951, epoch: 12.0[0m
[32m[2022-09-15 14:36:10,032] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1536[0m
[32m[2022-09-15 14:36:10,032] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 14:36:13,401] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1536/tokenizer_config.json[0m
[32m[2022-09-15 14:36:13,402] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1536/special_tokens_map.json[0m
[32m[2022-09-15 14:36:22,751] [    INFO][0m - loss: 0.15883898, learning_rate: 2.2781250000000003e-06, global_step: 1540, interval_runtime: 48.9873, interval_samples_per_second: 0.327, interval_steps_per_second: 0.204, epoch: 12.0312[0m
[32m[2022-09-15 14:36:29,277] [    INFO][0m - loss: 0.13359691, learning_rate: 2.2734375e-06, global_step: 1550, interval_runtime: 6.5262, interval_samples_per_second: 2.452, interval_steps_per_second: 1.532, epoch: 12.1094[0m
[32m[2022-09-15 14:36:35,796] [    INFO][0m - loss: 0.12116987, learning_rate: 2.26875e-06, global_step: 1560, interval_runtime: 6.5186, interval_samples_per_second: 2.455, interval_steps_per_second: 1.534, epoch: 12.1875[0m
[32m[2022-09-15 14:36:42,307] [    INFO][0m - loss: 0.10677999, learning_rate: 2.2640625e-06, global_step: 1570, interval_runtime: 6.5121, interval_samples_per_second: 2.457, interval_steps_per_second: 1.536, epoch: 12.2656[0m
[32m[2022-09-15 14:36:48,890] [    INFO][0m - loss: 0.13787301, learning_rate: 2.2593750000000003e-06, global_step: 1580, interval_runtime: 6.5278, interval_samples_per_second: 2.451, interval_steps_per_second: 1.532, epoch: 12.3438[0m
[32m[2022-09-15 14:36:55,415] [    INFO][0m - loss: 0.21897209, learning_rate: 2.2546875e-06, global_step: 1590, interval_runtime: 6.5798, interval_samples_per_second: 2.432, interval_steps_per_second: 1.52, epoch: 12.4219[0m
[32m[2022-09-15 14:37:01,951] [    INFO][0m - loss: 0.1273855, learning_rate: 2.25e-06, global_step: 1600, interval_runtime: 6.5357, interval_samples_per_second: 2.448, interval_steps_per_second: 1.53, epoch: 12.5[0m
[32m[2022-09-15 14:37:08,510] [    INFO][0m - loss: 0.13832498, learning_rate: 2.2453125e-06, global_step: 1610, interval_runtime: 6.5589, interval_samples_per_second: 2.439, interval_steps_per_second: 1.525, epoch: 12.5781[0m
[32m[2022-09-15 14:37:15,049] [    INFO][0m - loss: 0.14098752, learning_rate: 2.240625e-06, global_step: 1620, interval_runtime: 6.5393, interval_samples_per_second: 2.447, interval_steps_per_second: 1.529, epoch: 12.6562[0m
[32m[2022-09-15 14:37:21,560] [    INFO][0m - loss: 0.17267016, learning_rate: 2.2359375e-06, global_step: 1630, interval_runtime: 6.5107, interval_samples_per_second: 2.457, interval_steps_per_second: 1.536, epoch: 12.7344[0m
[32m[2022-09-15 14:37:28,085] [    INFO][0m - loss: 0.18381441, learning_rate: 2.23125e-06, global_step: 1640, interval_runtime: 6.5256, interval_samples_per_second: 2.452, interval_steps_per_second: 1.532, epoch: 12.8125[0m
[32m[2022-09-15 14:37:34,592] [    INFO][0m - loss: 0.10459331, learning_rate: 2.2265625e-06, global_step: 1650, interval_runtime: 6.5063, interval_samples_per_second: 2.459, interval_steps_per_second: 1.537, epoch: 12.8906[0m
[32m[2022-09-15 14:37:41,049] [    INFO][0m - loss: 0.2349221, learning_rate: 2.221875e-06, global_step: 1660, interval_runtime: 6.4575, interval_samples_per_second: 2.478, interval_steps_per_second: 1.549, epoch: 12.9688[0m
[32m[2022-09-15 14:37:43,113] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 14:37:43,113] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-09-15 14:37:43,113] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 14:37:43,113] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 14:37:43,113] [    INFO][0m -   Total prediction steps = 130[0m
[32m[2022-09-15 14:38:16,050] [    INFO][0m - eval_loss: 1.768030047416687, eval_accuracy: 0.5643133462282398, eval_runtime: 32.9359, eval_samples_per_second: 62.789, eval_steps_per_second: 3.947, epoch: 13.0[0m
[32m[2022-09-15 14:38:16,084] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1664[0m
[32m[2022-09-15 14:38:16,085] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 14:38:19,253] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1664/tokenizer_config.json[0m
[32m[2022-09-15 14:38:19,253] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1664/special_tokens_map.json[0m
[32m[2022-09-15 14:38:25,376] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-15 14:38:25,376] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-1152 (score: 0.5851063829787234).[0m
[32m[2022-09-15 14:38:26,938] [    INFO][0m - train_runtime: 1649.1751, train_samples_per_second: 61.728, train_steps_per_second: 3.881, train_loss: 0.8351400644661715, epoch: 13.0[0m
[32m[2022-09-15 14:38:26,940] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-09-15 14:38:26,940] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 14:38:37,129] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-09-15 14:38:37,130] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-09-15 14:38:37,131] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-15 14:38:37,131] [    INFO][0m -   epoch                    =       13.0[0m
[32m[2022-09-15 14:38:37,132] [    INFO][0m -   train_loss               =     0.8351[0m
[32m[2022-09-15 14:38:37,132] [    INFO][0m -   train_runtime            = 0:27:29.17[0m
[32m[2022-09-15 14:38:37,132] [    INFO][0m -   train_samples_per_second =     61.728[0m
[32m[2022-09-15 14:38:37,132] [    INFO][0m -   train_steps_per_second   =      3.881[0m
[32m[2022-09-15 14:38:37,139] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-15 14:38:37,139] [    INFO][0m -   Num examples = 1784[0m
[32m[2022-09-15 14:38:37,139] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 14:38:37,139] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 14:38:37,140] [    INFO][0m -   Total prediction steps = 112[0m
[32m[2022-09-15 14:39:05,499] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-15 14:39:05,499] [    INFO][0m -   test_accuracy           =     0.5667[0m
[32m[2022-09-15 14:39:05,499] [    INFO][0m -   test_loss               =     1.5237[0m
[32m[2022-09-15 14:39:05,499] [    INFO][0m -   test_runtime            = 0:00:28.35[0m
[32m[2022-09-15 14:39:05,500] [    INFO][0m -   test_samples_per_second =     62.907[0m
[32m[2022-09-15 14:39:05,500] [    INFO][0m -   test_steps_per_second   =      3.949[0m
[32m[2022-09-15 14:39:05,500] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-15 14:39:05,500] [    INFO][0m -   Num examples = 2999[0m
[32m[2022-09-15 14:39:05,500] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 14:39:05,500] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 14:39:05,500] [    INFO][0m -   Total prediction steps = 188[0m
{
  "labels": 26,
  "text_a": "\u672c\u6587\u7814\u7a76\u4e86\u8d35\u5dde\u4e1c\u5357\u90e8\u7684\u5929\u67f1\u3001\u9526\u5c4f\u548c\u9ece\u5e73\u5730\u533a\u7684\u5730\u8d28\u6784\u9020\u7279\u5f81\u3001\u8be5\u533a\u91d1\u77ff\u5e8a\u7279\u5f81\u53ca\u5176\u4e0e\u5730\u8d28\u6784\u9020\u7684\u5173\u7cfb.\u8ba4\u4e3a\u533a\u5185\u5317\u90e8\u548c\u5357\u90e8\u76f8\u8ddd\u6570\u5341\u516c\u91cc\u7684\u4e24\u6761\u8fd1\u4e1c\u897f\u5411\u7684\u57fa\u5e95\u526a\u5207\u65ad\u88c2\u5e26\u6784\u6210\u4e86\u672c\u533a\u5730\u5811\u5f0f\u7684\u6784\u9020\u683c\u5c40.\u7531\u4e8e\u8fd9\u4e24\u6761\u4e1c\u897f\u5411\u526a\u5207\u65ad\u88c2\u7684\u526a\u5207\u4f5c\u7528,\u5f62\u6210\u4e86\u672c\u533a\u4ee5\u5317\u4e1c\u5411\u4e3a\u4e3b\u7684\u8936\u76b1\u548c\u526a\u5207\u5e26.\u5317\u4e1c\u5411\u8936\u76b1\u548c\u526a\u5207\u5e26\u6784\u9020\u662f\u5728\u52a0\u91cc\u4e1c\u671f\u5f62\u6210\u7684,\u540c\u65f6\u4e5f\u53d1\u751f\u4e86\u7eff\u7247\u5ca9\u76f8\u7684\u53d8\u8d28\u4f5c\u7528.\u5728\u80cc\u659c\u5f62\u6210\u7684\u540c\u65f6\u6216\u7a0d\u540e\u526a\u5207\u4f5c\u7528\u5f00\u59cb\u53d1\u751f,\u526a\u5207\u4f5c\u7528\u7ee7\u627f\u7740\u5317\u4e1c\u5411,\u5f62\u6210\u4e86\u82e5\u5e72\u6761\u4e0e\u8936\u76b1\u8f74\u5e73\u884c\u6216\u76f8\u4ea4\u7684\u526a\u5207\u5e26.\u6210\u77ff\u6d41\u4f53\u6cbf\u7740\u526a\u5207\u5e26\u4e0a\u5347,\u4e00\u65b9\u9762\u5728\u526a\u5207\u5e26\u4e2d\u6c89\u6dc0\u51fa\u7a7f\u5c42\u4ea7\u51fa\u7684\u4e0d\u6574\u5408\u7834\u788e\u5e26\u578b\u91d1\u77ff\u4f53,\u5373\u900f\u955c\u72b6\u542b\u91d1\u77f3\u82f1\u8109;\u540c\u65f6\u5927\u90e8\u5206\u5145\u586b\u5230\u7531\u80cc\u659c\u548c\u526a\u5207\u4f5c\u7528\u5f62\u6210\u7684\u5c42\u95f4\u88c2\u9699\u6216\u5c42\u95f4\u7834\u788e\u5e26\u4e2d,\u4ece\u800c\u5f62\u6210\u8d4b\u5b58\u4e8e\u6d4a\u79ef\u5ca9\u7684\u987a\u5c42\u4ea7\u51fa\u7684\u5c42\u72b6\u542b\u91d1\u77f3\u82f1\u8109\u91d1\u77ff\u5e8a.\u8fd9\u79cd\u7c7b\u578b\u7684\u542b\u91d1\u77f3\u82f1\u8109,\u65e0\u8bba\u5728\u54c1\u4f4d(\u591a\u6570\u53ef\u89c1\u660e\u91d1)\u3001\u50a8\u91cf\u548c\u4ea7\u91cf\u65b9\u9762\u5747\u5f88\u6709\u524d\u666f,\u5e76\u4e14\u6709\u7740\u5341\u5206\u91cd\u8981\u7684\u5b66\u672f\u610f\u4e49,\u56e0\u4e3a\u8fd9\u5728\u56fd\u5185\u8fd8\u662f\u9996\u6b21\u53d1\u73b0.",
  "text_b": "",
  "uid": 44
}

Traceback (most recent call last):
  File "train_single.py", line 216, in <module>
    main()
  File "train_single.py", line 210, in main
    test_ret = postprocess(test_ret, test_ds, data_args.task_name,
  File "/ssd2/wanghuijuan03/prompt/PaddleNLP/examples/benchmark/fewclue/a7/e1cw/postprocess.py", line 57, in postprocess
    ret_list.append({"id": uid, "label": id_to_label[preds[idx]]})
TypeError: unhashable type: 'numpy.ndarray'
 
==========
tnews
==========
 
[32m[2022-09-15 14:39:59,533] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-15 14:39:59,534] [    INFO][0m - ============================================================[0m
[32m[2022-09-15 14:39:59,534] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-15 14:39:59,534] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-15 14:39:59,534] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-15 14:39:59,534] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-15 14:39:59,534] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-15 14:39:59,534] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-15 14:39:59,534] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-15 14:39:59,534] [    INFO][0m - [0m
[32m[2022-09-15 14:39:59,534] [    INFO][0m - ============================================================[0m
[32m[2022-09-15 14:39:59,534] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-15 14:39:59,534] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-15 14:39:59,535] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-15 14:39:59,535] [    INFO][0m - pretrained                    :./checkpoints_cmnli/checkpoint-6000/model_state.pdparams[0m
[32m[2022-09-15 14:39:59,535] [    INFO][0m - prompt                        :‚Äú{'text':'text_a'}‚Äù‰∏äËø∞Êñ∞ÈóªÈÄâËá™{'mask'}{'mask'}‰∏ìÊ†è„ÄÇ[0m
[32m[2022-09-15 14:39:59,535] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-15 14:39:59,535] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-15 14:39:59,535] [    INFO][0m - task_name                     :tnews[0m
[32m[2022-09-15 14:39:59,535] [    INFO][0m - [0m
[32m[2022-09-15 14:39:59,535] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0915 14:39:59.537214 14367 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0915 14:39:59.541457 14367 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-15 14:40:07,099] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-15 14:40:07,110] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-15 14:40:07,110] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-15 14:40:07,111] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚Äù‰∏äËø∞Êñ∞ÈóªÈÄâËá™'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': '‰∏ìÊ†è„ÄÇ'}][0m
[32m[2022-09-15 14:40:09,071] [    INFO][0m - ============================================================[0m
[32m[2022-09-15 14:40:09,071] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-15 14:40:09,071] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-15 14:40:09,071] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-15 14:40:09,072] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-15 14:40:09,072] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-15 14:40:09,072] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-15 14:40:09,072] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-15 14:40:09,072] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-15 14:40:09,072] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-15 14:40:09,072] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-15 14:40:09,072] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-15 14:40:09,072] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-15 14:40:09,072] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-15 14:40:09,072] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-15 14:40:09,072] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-15 14:40:09,072] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-15 14:40:09,073] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-15 14:40:09,073] [    INFO][0m - eval_batch_size               :16[0m
[32m[2022-09-15 14:40:09,073] [    INFO][0m - eval_steps                    :200[0m
[32m[2022-09-15 14:40:09,073] [    INFO][0m - evaluation_strategy           :IntervalStrategy.EPOCH[0m
[32m[2022-09-15 14:40:09,073] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-15 14:40:09,073] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-15 14:40:09,073] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-15 14:40:09,073] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-15 14:40:09,073] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-15 14:40:09,073] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-15 14:40:09,073] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-15 14:40:09,073] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-15 14:40:09,074] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-15 14:40:09,074] [    INFO][0m - learning_rate                 :3e-06[0m
[32m[2022-09-15 14:40:09,074] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-15 14:40:09,074] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-15 14:40:09,074] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-15 14:40:09,074] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-15 14:40:09,074] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-15 14:40:09,074] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-15 14:40:09,074] [    INFO][0m - logging_dir                   :./checkpoints/runs/Sep15_14-39-59_instance-3bwob41y-01[0m
[32m[2022-09-15 14:40:09,074] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-15 14:40:09,074] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-15 14:40:09,074] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-15 14:40:09,074] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-15 14:40:09,075] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-15 14:40:09,075] [    INFO][0m - max_seq_length                :64[0m
[32m[2022-09-15 14:40:09,075] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-15 14:40:09,075] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-15 14:40:09,075] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-15 14:40:09,075] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-15 14:40:09,075] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-09-15 14:40:09,075] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-15 14:40:09,075] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-15 14:40:09,075] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-09-15 14:40:09,075] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-15 14:40:09,075] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-15 14:40:09,075] [    INFO][0m - per_device_eval_batch_size    :16[0m
[32m[2022-09-15 14:40:09,076] [    INFO][0m - per_device_train_batch_size   :16[0m
[32m[2022-09-15 14:40:09,076] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-15 14:40:09,076] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-15 14:40:09,076] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-15 14:40:09,076] [    INFO][0m - ppt_learning_rate             :3e-05[0m
[32m[2022-09-15 14:40:09,076] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-15 14:40:09,076] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-15 14:40:09,076] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-15 14:40:09,076] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-15 14:40:09,076] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-15 14:40:09,076] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-15 14:40:09,076] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-09-15 14:40:09,076] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-15 14:40:09,077] [    INFO][0m - save_steps                    :200[0m
[32m[2022-09-15 14:40:09,077] [    INFO][0m - save_strategy                 :IntervalStrategy.EPOCH[0m
[32m[2022-09-15 14:40:09,077] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-15 14:40:09,077] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-15 14:40:09,077] [    INFO][0m - seed                          :42[0m
[32m[2022-09-15 14:40:09,077] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-15 14:40:09,077] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-15 14:40:09,077] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-15 14:40:09,077] [    INFO][0m - train_batch_size              :16[0m
[32m[2022-09-15 14:40:09,077] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-15 14:40:09,077] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-15 14:40:09,077] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-15 14:40:09,077] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-15 14:40:09,078] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-15 14:40:09,078] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-15 14:40:09,078] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-15 14:40:09,078] [    INFO][0m - [0m
[32m[2022-09-15 14:40:09,080] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-15 14:40:09,080] [    INFO][0m -   Num examples = 1185[0m
[32m[2022-09-15 14:40:09,080] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-09-15 14:40:09,080] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2022-09-15 14:40:09,081] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2022-09-15 14:40:09,081] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-15 14:40:09,081] [    INFO][0m -   Total optimization steps = 3750.0[0m
[32m[2022-09-15 14:40:09,081] [    INFO][0m -   Total num train samples = 59250[0m
[32m[2022-09-15 14:40:12,283] [    INFO][0m - loss: 4.56236916, learning_rate: 2.992e-06, global_step: 10, interval_runtime: 3.201, interval_samples_per_second: 4.998, interval_steps_per_second: 3.124, epoch: 0.1333[0m
[32m[2022-09-15 14:40:14,188] [    INFO][0m - loss: 2.80390663, learning_rate: 2.984e-06, global_step: 20, interval_runtime: 1.9051, interval_samples_per_second: 8.398, interval_steps_per_second: 5.249, epoch: 0.2667[0m
[32m[2022-09-15 14:40:16,081] [    INFO][0m - loss: 2.28055954, learning_rate: 2.976e-06, global_step: 30, interval_runtime: 1.8933, interval_samples_per_second: 8.451, interval_steps_per_second: 5.282, epoch: 0.4[0m
[32m[2022-09-15 14:40:17,963] [    INFO][0m - loss: 2.07067509, learning_rate: 2.968e-06, global_step: 40, interval_runtime: 1.8812, interval_samples_per_second: 8.505, interval_steps_per_second: 5.316, epoch: 0.5333[0m
[32m[2022-09-15 14:40:19,849] [    INFO][0m - loss: 1.90684433, learning_rate: 2.96e-06, global_step: 50, interval_runtime: 1.8864, interval_samples_per_second: 8.482, interval_steps_per_second: 5.301, epoch: 0.6667[0m
[32m[2022-09-15 14:40:21,749] [    INFO][0m - loss: 1.83865395, learning_rate: 2.952e-06, global_step: 60, interval_runtime: 1.9004, interval_samples_per_second: 8.419, interval_steps_per_second: 5.262, epoch: 0.8[0m
[32m[2022-09-15 14:40:23,633] [    INFO][0m - loss: 1.76196995, learning_rate: 2.944e-06, global_step: 70, interval_runtime: 1.8841, interval_samples_per_second: 8.492, interval_steps_per_second: 5.308, epoch: 0.9333[0m
[32m[2022-09-15 14:40:24,474] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 14:40:24,474] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-15 14:40:24,474] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 14:40:24,474] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 14:40:24,474] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-15 14:40:29,804] [    INFO][0m - eval_loss: 1.5083558559417725, eval_accuracy: 0.5510018214936248, eval_runtime: 5.3296, eval_samples_per_second: 206.019, eval_steps_per_second: 12.947, epoch: 1.0[0m
[32m[2022-09-15 14:40:29,826] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-75[0m
[32m[2022-09-15 14:40:29,826] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 14:40:33,092] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-75/tokenizer_config.json[0m
[32m[2022-09-15 14:40:33,092] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-75/special_tokens_map.json[0m
[32m[2022-09-15 14:40:41,882] [    INFO][0m - loss: 1.43684969, learning_rate: 2.936e-06, global_step: 80, interval_runtime: 18.2482, interval_samples_per_second: 0.877, interval_steps_per_second: 0.548, epoch: 1.0667[0m
[32m[2022-09-15 14:40:43,792] [    INFO][0m - loss: 1.38118887, learning_rate: 2.928e-06, global_step: 90, interval_runtime: 1.9097, interval_samples_per_second: 8.378, interval_steps_per_second: 5.236, epoch: 1.2[0m
[32m[2022-09-15 14:40:45,683] [    INFO][0m - loss: 1.42663059, learning_rate: 2.9200000000000004e-06, global_step: 100, interval_runtime: 1.8913, interval_samples_per_second: 8.46, interval_steps_per_second: 5.287, epoch: 1.3333[0m
[32m[2022-09-15 14:40:47,566] [    INFO][0m - loss: 1.67813377, learning_rate: 2.9120000000000002e-06, global_step: 110, interval_runtime: 1.884, interval_samples_per_second: 8.492, interval_steps_per_second: 5.308, epoch: 1.4667[0m
[32m[2022-09-15 14:40:49,451] [    INFO][0m - loss: 1.39503241, learning_rate: 2.904e-06, global_step: 120, interval_runtime: 1.8842, interval_samples_per_second: 8.492, interval_steps_per_second: 5.307, epoch: 1.6[0m
[32m[2022-09-15 14:40:51,339] [    INFO][0m - loss: 1.32316999, learning_rate: 2.8960000000000003e-06, global_step: 130, interval_runtime: 1.8886, interval_samples_per_second: 8.472, interval_steps_per_second: 5.295, epoch: 1.7333[0m
[32m[2022-09-15 14:40:53,275] [    INFO][0m - loss: 1.35100002, learning_rate: 2.888e-06, global_step: 140, interval_runtime: 1.9351, interval_samples_per_second: 8.268, interval_steps_per_second: 5.168, epoch: 1.8667[0m
[32m[2022-09-15 14:40:55,088] [    INFO][0m - loss: 1.1558712, learning_rate: 2.88e-06, global_step: 150, interval_runtime: 1.8136, interval_samples_per_second: 8.822, interval_steps_per_second: 5.514, epoch: 2.0[0m
[32m[2022-09-15 14:40:55,089] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 14:40:55,089] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-15 14:40:55,089] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 14:40:55,089] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 14:40:55,090] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-15 14:41:00,220] [    INFO][0m - eval_loss: 1.3860305547714233, eval_accuracy: 0.5673952641165756, eval_runtime: 5.1304, eval_samples_per_second: 214.02, eval_steps_per_second: 13.449, epoch: 2.0[0m
[32m[2022-09-15 14:41:00,239] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-150[0m
[32m[2022-09-15 14:41:00,239] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 14:41:03,492] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-150/tokenizer_config.json[0m
[32m[2022-09-15 14:41:03,492] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-150/special_tokens_map.json[0m
[32m[2022-09-15 14:41:11,459] [    INFO][0m - loss: 1.24973516, learning_rate: 2.872e-06, global_step: 160, interval_runtime: 16.3709, interval_samples_per_second: 0.977, interval_steps_per_second: 0.611, epoch: 2.1333[0m
[32m[2022-09-15 14:41:13,355] [    INFO][0m - loss: 1.40421066, learning_rate: 2.864e-06, global_step: 170, interval_runtime: 1.896, interval_samples_per_second: 8.439, interval_steps_per_second: 5.274, epoch: 2.2667[0m
[32m[2022-09-15 14:41:15,243] [    INFO][0m - loss: 1.07706203, learning_rate: 2.856e-06, global_step: 180, interval_runtime: 1.8878, interval_samples_per_second: 8.475, interval_steps_per_second: 5.297, epoch: 2.4[0m
[32m[2022-09-15 14:41:17,133] [    INFO][0m - loss: 1.09805822, learning_rate: 2.848e-06, global_step: 190, interval_runtime: 1.8899, interval_samples_per_second: 8.466, interval_steps_per_second: 5.291, epoch: 2.5333[0m
[32m[2022-09-15 14:41:19,022] [    INFO][0m - loss: 1.23740129, learning_rate: 2.84e-06, global_step: 200, interval_runtime: 1.8887, interval_samples_per_second: 8.472, interval_steps_per_second: 5.295, epoch: 2.6667[0m
[32m[2022-09-15 14:41:20,915] [    INFO][0m - loss: 1.33036222, learning_rate: 2.8319999999999997e-06, global_step: 210, interval_runtime: 1.8941, interval_samples_per_second: 8.447, interval_steps_per_second: 5.28, epoch: 2.8[0m
[32m[2022-09-15 14:41:22,815] [    INFO][0m - loss: 1.16636639, learning_rate: 2.824e-06, global_step: 220, interval_runtime: 1.9, interval_samples_per_second: 8.421, interval_steps_per_second: 5.263, epoch: 2.9333[0m
[32m[2022-09-15 14:41:23,662] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 14:41:23,662] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-15 14:41:23,662] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 14:41:23,663] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 14:41:23,663] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-15 14:41:28,799] [    INFO][0m - eval_loss: 1.3709796667099, eval_accuracy: 0.5673952641165756, eval_runtime: 5.1366, eval_samples_per_second: 213.761, eval_steps_per_second: 13.433, epoch: 3.0[0m
[32m[2022-09-15 14:41:28,818] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-225[0m
[32m[2022-09-15 14:41:28,819] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 14:41:31,793] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-225/tokenizer_config.json[0m
[32m[2022-09-15 14:41:31,793] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-225/special_tokens_map.json[0m
[32m[2022-09-15 14:41:39,013] [    INFO][0m - loss: 1.13151884, learning_rate: 2.8160000000000002e-06, global_step: 230, interval_runtime: 16.198, interval_samples_per_second: 0.988, interval_steps_per_second: 0.617, epoch: 3.0667[0m
[32m[2022-09-15 14:41:40,902] [    INFO][0m - loss: 1.02267551, learning_rate: 2.808e-06, global_step: 240, interval_runtime: 1.8886, interval_samples_per_second: 8.472, interval_steps_per_second: 5.295, epoch: 3.2[0m
[32m[2022-09-15 14:41:42,804] [    INFO][0m - loss: 1.07580442, learning_rate: 2.8000000000000003e-06, global_step: 250, interval_runtime: 1.902, interval_samples_per_second: 8.412, interval_steps_per_second: 5.258, epoch: 3.3333[0m
[32m[2022-09-15 14:41:44,694] [    INFO][0m - loss: 1.28059368, learning_rate: 2.792e-06, global_step: 260, interval_runtime: 1.8899, interval_samples_per_second: 8.466, interval_steps_per_second: 5.291, epoch: 3.4667[0m
[32m[2022-09-15 14:41:46,581] [    INFO][0m - loss: 1.09441271, learning_rate: 2.7840000000000004e-06, global_step: 270, interval_runtime: 1.8875, interval_samples_per_second: 8.477, interval_steps_per_second: 5.298, epoch: 3.6[0m
[32m[2022-09-15 14:41:48,479] [    INFO][0m - loss: 1.10854874, learning_rate: 2.776e-06, global_step: 280, interval_runtime: 1.898, interval_samples_per_second: 8.43, interval_steps_per_second: 5.269, epoch: 3.7333[0m
[32m[2022-09-15 14:41:50,375] [    INFO][0m - loss: 0.99596386, learning_rate: 2.768e-06, global_step: 290, interval_runtime: 1.8954, interval_samples_per_second: 8.442, interval_steps_per_second: 5.276, epoch: 3.8667[0m
[32m[2022-09-15 14:41:52,170] [    INFO][0m - loss: 1.0838666, learning_rate: 2.7600000000000003e-06, global_step: 300, interval_runtime: 1.7949, interval_samples_per_second: 8.914, interval_steps_per_second: 5.571, epoch: 4.0[0m
[32m[2022-09-15 14:41:52,170] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 14:41:52,170] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-15 14:41:52,171] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 14:41:52,171] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 14:41:52,171] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-15 14:41:57,365] [    INFO][0m - eval_loss: 1.4190113544464111, eval_accuracy: 0.5546448087431693, eval_runtime: 5.1942, eval_samples_per_second: 211.388, eval_steps_per_second: 13.284, epoch: 4.0[0m
[32m[2022-09-15 14:41:57,384] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-09-15 14:41:57,385] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 14:42:00,255] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-09-15 14:42:00,256] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-09-15 14:42:08,264] [    INFO][0m - loss: 0.97378254, learning_rate: 2.752e-06, global_step: 310, interval_runtime: 16.0945, interval_samples_per_second: 0.994, interval_steps_per_second: 0.621, epoch: 4.1333[0m
[32m[2022-09-15 14:42:12,315] [    INFO][0m - loss: 0.97322931, learning_rate: 2.744e-06, global_step: 320, interval_runtime: 1.8979, interval_samples_per_second: 8.43, interval_steps_per_second: 5.269, epoch: 4.2667[0m
[32m[2022-09-15 14:42:14,222] [    INFO][0m - loss: 1.00481749, learning_rate: 2.736e-06, global_step: 330, interval_runtime: 4.0594, interval_samples_per_second: 3.941, interval_steps_per_second: 2.463, epoch: 4.4[0m
[32m[2022-09-15 14:42:16,119] [    INFO][0m - loss: 1.02248535, learning_rate: 2.728e-06, global_step: 340, interval_runtime: 1.8973, interval_samples_per_second: 8.433, interval_steps_per_second: 5.271, epoch: 4.5333[0m
[32m[2022-09-15 14:42:18,010] [    INFO][0m - loss: 1.10523968, learning_rate: 2.72e-06, global_step: 350, interval_runtime: 1.8911, interval_samples_per_second: 8.461, interval_steps_per_second: 5.288, epoch: 4.6667[0m
[32m[2022-09-15 14:42:19,908] [    INFO][0m - loss: 0.97447205, learning_rate: 2.712e-06, global_step: 360, interval_runtime: 1.8985, interval_samples_per_second: 8.428, interval_steps_per_second: 5.267, epoch: 4.8[0m
[32m[2022-09-15 14:42:21,816] [    INFO][0m - loss: 0.89700727, learning_rate: 2.704e-06, global_step: 370, interval_runtime: 1.9074, interval_samples_per_second: 8.389, interval_steps_per_second: 5.243, epoch: 4.9333[0m
[32m[2022-09-15 14:42:22,661] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 14:42:22,661] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-15 14:42:22,661] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 14:42:22,661] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 14:42:22,661] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-15 14:42:27,793] [    INFO][0m - eval_loss: 1.4145839214324951, eval_accuracy: 0.5710382513661202, eval_runtime: 5.1321, eval_samples_per_second: 213.946, eval_steps_per_second: 13.445, epoch: 5.0[0m
[32m[2022-09-15 14:42:27,812] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-375[0m
[32m[2022-09-15 14:42:27,812] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 14:42:30,617] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-375/tokenizer_config.json[0m
[32m[2022-09-15 14:42:30,617] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-375/special_tokens_map.json[0m
[32m[2022-09-15 14:42:37,585] [    INFO][0m - loss: 0.72791009, learning_rate: 2.696e-06, global_step: 380, interval_runtime: 15.7693, interval_samples_per_second: 1.015, interval_steps_per_second: 0.634, epoch: 5.0667[0m
[32m[2022-09-15 14:42:39,476] [    INFO][0m - loss: 0.80746832, learning_rate: 2.688e-06, global_step: 390, interval_runtime: 1.8907, interval_samples_per_second: 8.462, interval_steps_per_second: 5.289, epoch: 5.2[0m
[32m[2022-09-15 14:42:41,363] [    INFO][0m - loss: 1.11199303, learning_rate: 2.68e-06, global_step: 400, interval_runtime: 1.8875, interval_samples_per_second: 8.477, interval_steps_per_second: 5.298, epoch: 5.3333[0m
[32m[2022-09-15 14:42:43,259] [    INFO][0m - loss: 0.9412878, learning_rate: 2.6720000000000004e-06, global_step: 410, interval_runtime: 1.8954, interval_samples_per_second: 8.441, interval_steps_per_second: 5.276, epoch: 5.4667[0m
[32m[2022-09-15 14:42:45,147] [    INFO][0m - loss: 0.73241539, learning_rate: 2.6640000000000002e-06, global_step: 420, interval_runtime: 1.8887, interval_samples_per_second: 8.471, interval_steps_per_second: 5.295, epoch: 5.6[0m
[32m[2022-09-15 14:42:47,044] [    INFO][0m - loss: 0.89761543, learning_rate: 2.656e-06, global_step: 430, interval_runtime: 1.8967, interval_samples_per_second: 8.436, interval_steps_per_second: 5.272, epoch: 5.7333[0m
[32m[2022-09-15 14:42:48,925] [    INFO][0m - loss: 0.8816493, learning_rate: 2.6480000000000003e-06, global_step: 440, interval_runtime: 1.8811, interval_samples_per_second: 8.506, interval_steps_per_second: 5.316, epoch: 5.8667[0m
[32m[2022-09-15 14:42:50,714] [    INFO][0m - loss: 0.84083748, learning_rate: 2.64e-06, global_step: 450, interval_runtime: 1.7888, interval_samples_per_second: 8.945, interval_steps_per_second: 5.59, epoch: 6.0[0m
[32m[2022-09-15 14:42:50,715] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 14:42:50,715] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-15 14:42:50,715] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 14:42:50,715] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 14:42:50,715] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-15 14:42:55,891] [    INFO][0m - eval_loss: 1.469711422920227, eval_accuracy: 0.5655737704918032, eval_runtime: 5.1758, eval_samples_per_second: 212.143, eval_steps_per_second: 13.331, epoch: 6.0[0m
[32m[2022-09-15 14:42:55,910] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-450[0m
[32m[2022-09-15 14:42:55,910] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 14:42:58,816] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-450/tokenizer_config.json[0m
[32m[2022-09-15 14:42:58,817] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-450/special_tokens_map.json[0m
[32m[2022-09-15 14:43:06,864] [    INFO][0m - loss: 0.90497255, learning_rate: 2.632e-06, global_step: 460, interval_runtime: 16.15, interval_samples_per_second: 0.991, interval_steps_per_second: 0.619, epoch: 6.1333[0m
[32m[2022-09-15 14:43:08,753] [    INFO][0m - loss: 0.80673389, learning_rate: 2.624e-06, global_step: 470, interval_runtime: 1.889, interval_samples_per_second: 8.47, interval_steps_per_second: 5.294, epoch: 6.2667[0m
[32m[2022-09-15 14:43:10,645] [    INFO][0m - loss: 0.90587816, learning_rate: 2.616e-06, global_step: 480, interval_runtime: 1.8922, interval_samples_per_second: 8.456, interval_steps_per_second: 5.285, epoch: 6.4[0m
[32m[2022-09-15 14:43:12,546] [    INFO][0m - loss: 0.72368608, learning_rate: 2.608e-06, global_step: 490, interval_runtime: 1.9009, interval_samples_per_second: 8.417, interval_steps_per_second: 5.261, epoch: 6.5333[0m
[32m[2022-09-15 14:43:14,443] [    INFO][0m - loss: 0.56745458, learning_rate: 2.6e-06, global_step: 500, interval_runtime: 1.8962, interval_samples_per_second: 8.438, interval_steps_per_second: 5.274, epoch: 6.6667[0m
[32m[2022-09-15 14:43:16,331] [    INFO][0m - loss: 0.67894602, learning_rate: 2.592e-06, global_step: 510, interval_runtime: 1.8884, interval_samples_per_second: 8.473, interval_steps_per_second: 5.295, epoch: 6.8[0m
[32m[2022-09-15 14:43:18,217] [    INFO][0m - loss: 0.91625834, learning_rate: 2.5839999999999997e-06, global_step: 520, interval_runtime: 1.8859, interval_samples_per_second: 8.484, interval_steps_per_second: 5.303, epoch: 6.9333[0m
[32m[2022-09-15 14:43:19,058] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 14:43:19,058] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-15 14:43:19,058] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 14:43:19,058] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 14:43:19,058] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-15 14:43:24,460] [    INFO][0m - eval_loss: 1.559302806854248, eval_accuracy: 0.5601092896174863, eval_runtime: 5.4012, eval_samples_per_second: 203.288, eval_steps_per_second: 12.775, epoch: 7.0[0m
[32m[2022-09-15 14:43:24,479] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-525[0m
[32m[2022-09-15 14:43:24,479] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 14:43:27,253] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-525/tokenizer_config.json[0m
[32m[2022-09-15 14:43:27,253] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-525/special_tokens_map.json[0m
[32m[2022-09-15 14:43:35,079] [    INFO][0m - loss: 0.60454302, learning_rate: 2.576e-06, global_step: 530, interval_runtime: 16.1872, interval_samples_per_second: 0.988, interval_steps_per_second: 0.618, epoch: 7.0667[0m
[32m[2022-09-15 14:43:36,961] [    INFO][0m - loss: 0.56576304, learning_rate: 2.568e-06, global_step: 540, interval_runtime: 2.5568, interval_samples_per_second: 6.258, interval_steps_per_second: 3.911, epoch: 7.2[0m
[32m[2022-09-15 14:43:38,842] [    INFO][0m - loss: 0.6415266, learning_rate: 2.56e-06, global_step: 550, interval_runtime: 1.8806, interval_samples_per_second: 8.508, interval_steps_per_second: 5.318, epoch: 7.3333[0m
[32m[2022-09-15 14:43:40,721] [    INFO][0m - loss: 0.77560778, learning_rate: 2.5520000000000003e-06, global_step: 560, interval_runtime: 1.8794, interval_samples_per_second: 8.514, interval_steps_per_second: 5.321, epoch: 7.4667[0m
[32m[2022-09-15 14:43:42,617] [    INFO][0m - loss: 0.73338108, learning_rate: 2.544e-06, global_step: 570, interval_runtime: 1.8965, interval_samples_per_second: 8.437, interval_steps_per_second: 5.273, epoch: 7.6[0m
[32m[2022-09-15 14:43:44,502] [    INFO][0m - loss: 0.6210146, learning_rate: 2.5360000000000004e-06, global_step: 580, interval_runtime: 1.8844, interval_samples_per_second: 8.491, interval_steps_per_second: 5.307, epoch: 7.7333[0m
[32m[2022-09-15 14:43:46,385] [    INFO][0m - loss: 0.65342402, learning_rate: 2.528e-06, global_step: 590, interval_runtime: 1.8836, interval_samples_per_second: 8.494, interval_steps_per_second: 5.309, epoch: 7.8667[0m
[32m[2022-09-15 14:43:48,168] [    INFO][0m - loss: 0.58351469, learning_rate: 2.52e-06, global_step: 600, interval_runtime: 1.7823, interval_samples_per_second: 8.977, interval_steps_per_second: 5.611, epoch: 8.0[0m
[32m[2022-09-15 14:43:48,168] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 14:43:48,168] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-15 14:43:48,168] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 14:43:48,168] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 14:43:48,168] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-15 14:43:53,316] [    INFO][0m - eval_loss: 1.6921147108078003, eval_accuracy: 0.5564663023679417, eval_runtime: 5.1474, eval_samples_per_second: 213.312, eval_steps_per_second: 13.405, epoch: 8.0[0m
[32m[2022-09-15 14:43:53,335] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-09-15 14:43:53,335] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 14:43:56,149] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-09-15 14:43:56,150] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-09-15 14:44:03,862] [    INFO][0m - loss: 0.45319676, learning_rate: 2.5120000000000003e-06, global_step: 610, interval_runtime: 15.694, interval_samples_per_second: 1.019, interval_steps_per_second: 0.637, epoch: 8.1333[0m
[32m[2022-09-15 14:44:05,755] [    INFO][0m - loss: 0.57409859, learning_rate: 2.504e-06, global_step: 620, interval_runtime: 1.8936, interval_samples_per_second: 8.45, interval_steps_per_second: 5.281, epoch: 8.2667[0m
[32m[2022-09-15 14:44:07,639] [    INFO][0m - loss: 0.5666965, learning_rate: 2.496e-06, global_step: 630, interval_runtime: 1.8838, interval_samples_per_second: 8.493, interval_steps_per_second: 5.308, epoch: 8.4[0m
[32m[2022-09-15 14:44:09,514] [    INFO][0m - loss: 0.56547084, learning_rate: 2.488e-06, global_step: 640, interval_runtime: 1.8743, interval_samples_per_second: 8.536, interval_steps_per_second: 5.335, epoch: 8.5333[0m
[32m[2022-09-15 14:44:11,393] [    INFO][0m - loss: 0.65440106, learning_rate: 2.48e-06, global_step: 650, interval_runtime: 1.8796, interval_samples_per_second: 8.512, interval_steps_per_second: 5.32, epoch: 8.6667[0m
[32m[2022-09-15 14:44:13,286] [    INFO][0m - loss: 0.5501967, learning_rate: 2.472e-06, global_step: 660, interval_runtime: 1.8928, interval_samples_per_second: 8.453, interval_steps_per_second: 5.283, epoch: 8.8[0m
[32m[2022-09-15 14:44:15,172] [    INFO][0m - loss: 0.61608272, learning_rate: 2.464e-06, global_step: 670, interval_runtime: 1.8863, interval_samples_per_second: 8.482, interval_steps_per_second: 5.301, epoch: 8.9333[0m
[32m[2022-09-15 14:44:16,011] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 14:44:16,011] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-09-15 14:44:16,012] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 14:44:16,012] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 14:44:16,012] [    INFO][0m -   Total prediction steps = 69[0m
[32m[2022-09-15 14:44:21,088] [    INFO][0m - eval_loss: 1.7522379159927368, eval_accuracy: 0.5537340619307832, eval_runtime: 5.0758, eval_samples_per_second: 216.319, eval_steps_per_second: 13.594, epoch: 9.0[0m
[32m[2022-09-15 14:44:21,106] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-675[0m
[32m[2022-09-15 14:44:21,107] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 14:44:23,892] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-675/tokenizer_config.json[0m
[32m[2022-09-15 14:44:23,893] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-675/special_tokens_map.json[0m
[32m[2022-09-15 14:44:29,691] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-15 14:44:29,691] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-375 (score: 0.5710382513661202).[0m
[32m[2022-09-15 14:44:31,519] [    INFO][0m - train_runtime: 262.4377, train_samples_per_second: 225.768, train_steps_per_second: 14.289, train_loss: 1.104189814461602, epoch: 9.0[0m
[32m[2022-09-15 14:44:31,528] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-09-15 14:44:31,529] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 14:44:42,194] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-09-15 14:44:42,195] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-09-15 14:44:42,199] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-15 14:44:42,200] [    INFO][0m -   epoch                    =        9.0[0m
[32m[2022-09-15 14:44:42,200] [    INFO][0m -   train_loss               =     1.1042[0m
[32m[2022-09-15 14:44:42,200] [    INFO][0m -   train_runtime            = 0:04:22.43[0m
[32m[2022-09-15 14:44:42,200] [    INFO][0m -   train_samples_per_second =    225.768[0m
[32m[2022-09-15 14:44:42,200] [    INFO][0m -   train_steps_per_second   =     14.289[0m
[32m[2022-09-15 14:44:42,204] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-15 14:44:42,204] [    INFO][0m -   Num examples = 2010[0m
[32m[2022-09-15 14:44:42,204] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 14:44:42,204] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 14:44:42,204] [    INFO][0m -   Total prediction steps = 126[0m
[32m[2022-09-15 14:44:51,820] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-15 14:44:51,821] [    INFO][0m -   test_accuracy           =     0.5856[0m
[32m[2022-09-15 14:44:51,821] [    INFO][0m -   test_loss               =     1.3568[0m
[32m[2022-09-15 14:44:51,821] [    INFO][0m -   test_runtime            = 0:00:09.61[0m
[32m[2022-09-15 14:44:51,821] [    INFO][0m -   test_samples_per_second =    209.022[0m
[32m[2022-09-15 14:44:51,821] [    INFO][0m -   test_steps_per_second   =     13.103[0m
[32m[2022-09-15 14:44:51,821] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-15 14:44:51,821] [    INFO][0m -   Num examples = 1500[0m
[32m[2022-09-15 14:44:51,822] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 14:44:51,822] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 14:44:51,822] [    INFO][0m -   Total prediction steps = 94[0m
{
  "labels": 11,
  "text_a": "\u5b69\u5b50\u8ddf\u8c01\u7761\uff0c\u5c31\u662f\u8c01\u7684\u5b69\u5b50",
  "text_b": "",
  "uid": 448
}

Traceback (most recent call last):
  File "train_single.py", line 216, in <module>
    main()
  File "train_single.py", line 210, in main
    test_ret = postprocess(test_ret, test_ds, data_args.task_name,
  File "/ssd2/wanghuijuan03/prompt/PaddleNLP/examples/benchmark/fewclue/a7/e1cw/postprocess.py", line 61, in postprocess
    "label": str(remap[id_to_label[preds[idx]]])
TypeError: unhashable type: 'numpy.ndarray'
 
==========
iflytek
==========
 
[32m[2022-09-15 14:45:04,348] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-15 14:45:04,349] [    INFO][0m - ============================================================[0m
[32m[2022-09-15 14:45:04,349] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-15 14:45:04,349] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-15 14:45:04,349] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-15 14:45:04,349] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-15 14:45:04,349] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-15 14:45:04,349] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-15 14:45:04,350] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-15 14:45:04,350] [    INFO][0m - [0m
[32m[2022-09-15 14:45:04,350] [    INFO][0m - ============================================================[0m
[32m[2022-09-15 14:45:04,350] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-15 14:45:04,350] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-15 14:45:04,350] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-15 14:45:04,350] [    INFO][0m - pretrained                    :./checkpoints_cmnli/checkpoint-6000/model_state.pdparams[0m
[32m[2022-09-15 14:45:04,350] [    INFO][0m - prompt                        :‚Äú{'text':'text_a'}‚ÄùÂõ†Ê≠§ÔºåÂ∫îÁî®Á±ªÂà´ÊòØ{'mask'}{'mask'}„ÄÇ[0m
[32m[2022-09-15 14:45:04,350] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-15 14:45:04,350] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-15 14:45:04,350] [    INFO][0m - task_name                     :iflytek[0m
[32m[2022-09-15 14:45:04,350] [    INFO][0m - [0m
[32m[2022-09-15 14:45:04,351] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0915 14:45:04.352249 19293 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0915 14:45:04.356367 19293 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-15 14:45:11,271] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-15 14:45:11,284] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-15 14:45:11,285] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-15 14:45:11,285] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùÂõ†Ê≠§ÔºåÂ∫îÁî®Á±ªÂà´ÊòØ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': '„ÄÇ'}][0m
[32m[2022-09-15 14:45:13,487] [    INFO][0m - ============================================================[0m
[32m[2022-09-15 14:45:13,487] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-15 14:45:13,487] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-15 14:45:13,487] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-15 14:45:13,488] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-15 14:45:13,488] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-15 14:45:13,488] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-15 14:45:13,488] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-15 14:45:13,488] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-15 14:45:13,488] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-15 14:45:13,488] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-15 14:45:13,488] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-15 14:45:13,488] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-15 14:45:13,488] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-15 14:45:13,488] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-15 14:45:13,488] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-15 14:45:13,488] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-15 14:45:13,489] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-15 14:45:13,489] [    INFO][0m - eval_batch_size               :16[0m
[32m[2022-09-15 14:45:13,489] [    INFO][0m - eval_steps                    :200[0m
[32m[2022-09-15 14:45:13,489] [    INFO][0m - evaluation_strategy           :IntervalStrategy.EPOCH[0m
[32m[2022-09-15 14:45:13,489] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-15 14:45:13,489] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-15 14:45:13,489] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-15 14:45:13,489] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-15 14:45:13,489] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-15 14:45:13,489] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-15 14:45:13,489] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-15 14:45:13,489] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-15 14:45:13,490] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-15 14:45:13,490] [    INFO][0m - learning_rate                 :3e-06[0m
[32m[2022-09-15 14:45:13,490] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-15 14:45:13,490] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-15 14:45:13,490] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-15 14:45:13,490] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-15 14:45:13,490] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-15 14:45:13,490] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-15 14:45:13,490] [    INFO][0m - logging_dir                   :./checkpoints/runs/Sep15_14-45-04_instance-3bwob41y-01[0m
[32m[2022-09-15 14:45:13,490] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-15 14:45:13,490] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-15 14:45:13,490] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-15 14:45:13,490] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-15 14:45:13,490] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-15 14:45:13,491] [    INFO][0m - max_seq_length                :320[0m
[32m[2022-09-15 14:45:13,491] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-15 14:45:13,491] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-15 14:45:13,491] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-15 14:45:13,491] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-15 14:45:13,491] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-09-15 14:45:13,491] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-15 14:45:13,491] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-15 14:45:13,491] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-09-15 14:45:13,491] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-15 14:45:13,491] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-15 14:45:13,491] [    INFO][0m - per_device_eval_batch_size    :16[0m
[32m[2022-09-15 14:45:13,491] [    INFO][0m - per_device_train_batch_size   :16[0m
[32m[2022-09-15 14:45:13,492] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-15 14:45:13,492] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-15 14:45:13,492] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-15 14:45:13,492] [    INFO][0m - ppt_learning_rate             :3e-05[0m
[32m[2022-09-15 14:45:13,492] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-15 14:45:13,492] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-15 14:45:13,492] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-15 14:45:13,492] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-15 14:45:13,492] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-15 14:45:13,492] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-15 14:45:13,492] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-09-15 14:45:13,492] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-15 14:45:13,492] [    INFO][0m - save_steps                    :200[0m
[32m[2022-09-15 14:45:13,493] [    INFO][0m - save_strategy                 :IntervalStrategy.EPOCH[0m
[32m[2022-09-15 14:45:13,493] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-15 14:45:13,493] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-15 14:45:13,493] [    INFO][0m - seed                          :42[0m
[32m[2022-09-15 14:45:13,493] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-15 14:45:13,493] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-15 14:45:13,493] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-15 14:45:13,493] [    INFO][0m - train_batch_size              :16[0m
[32m[2022-09-15 14:45:13,493] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-15 14:45:13,493] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-15 14:45:13,493] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-15 14:45:13,493] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-15 14:45:13,493] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-15 14:45:13,493] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-15 14:45:13,494] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-15 14:45:13,494] [    INFO][0m - [0m
[32m[2022-09-15 14:45:13,496] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-15 14:45:13,497] [    INFO][0m -   Num examples = 3024[0m
[32m[2022-09-15 14:45:13,497] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-09-15 14:45:13,497] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2022-09-15 14:45:13,497] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2022-09-15 14:45:13,497] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-15 14:45:13,497] [    INFO][0m -   Total optimization steps = 9450.0[0m
[32m[2022-09-15 14:45:13,497] [    INFO][0m -   Total num train samples = 151200[0m
[33m[2022-09-15 14:45:13,508] [ WARNING][0m - Token indices sequence length is longer than the specified maximum sequence length for this model (637 > 512). Running this sequence through the model will result in indexing errors[0m
[32m[2022-09-15 14:45:22,825] [    INFO][0m - loss: 5.13918304, learning_rate: 2.996825396825397e-06, global_step: 10, interval_runtime: 9.327, interval_samples_per_second: 1.715, interval_steps_per_second: 1.072, epoch: 0.0529[0m
[32m[2022-09-15 14:45:30,751] [    INFO][0m - loss: 4.03969841, learning_rate: 2.993650793650794e-06, global_step: 20, interval_runtime: 7.926, interval_samples_per_second: 2.019, interval_steps_per_second: 1.262, epoch: 0.1058[0m
[32m[2022-09-15 14:45:38,719] [    INFO][0m - loss: 3.84020348, learning_rate: 2.9904761904761907e-06, global_step: 30, interval_runtime: 7.9679, interval_samples_per_second: 2.008, interval_steps_per_second: 1.255, epoch: 0.1587[0m
[32m[2022-09-15 14:45:46,678] [    INFO][0m - loss: 3.50232239, learning_rate: 2.9873015873015875e-06, global_step: 40, interval_runtime: 7.9587, interval_samples_per_second: 2.01, interval_steps_per_second: 1.256, epoch: 0.2116[0m
[32m[2022-09-15 14:45:54,660] [    INFO][0m - loss: 3.08582325, learning_rate: 2.984126984126984e-06, global_step: 50, interval_runtime: 7.9822, interval_samples_per_second: 2.004, interval_steps_per_second: 1.253, epoch: 0.2646[0m
[32m[2022-09-15 14:46:02,626] [    INFO][0m - loss: 3.06117821, learning_rate: 2.980952380952381e-06, global_step: 60, interval_runtime: 7.9662, interval_samples_per_second: 2.008, interval_steps_per_second: 1.255, epoch: 0.3175[0m
[32m[2022-09-15 14:46:10,643] [    INFO][0m - loss: 3.08678169, learning_rate: 2.9777777777777777e-06, global_step: 70, interval_runtime: 8.0163, interval_samples_per_second: 1.996, interval_steps_per_second: 1.247, epoch: 0.3704[0m
[32m[2022-09-15 14:46:18,653] [    INFO][0m - loss: 3.12685261, learning_rate: 2.9746031746031744e-06, global_step: 80, interval_runtime: 8.0099, interval_samples_per_second: 1.998, interval_steps_per_second: 1.248, epoch: 0.4233[0m
[32m[2022-09-15 14:46:26,666] [    INFO][0m - loss: 2.96818428, learning_rate: 2.9714285714285716e-06, global_step: 90, interval_runtime: 8.0137, interval_samples_per_second: 1.997, interval_steps_per_second: 1.248, epoch: 0.4762[0m
[32m[2022-09-15 14:46:34,657] [    INFO][0m - loss: 2.72238979, learning_rate: 2.9682539682539683e-06, global_step: 100, interval_runtime: 7.9913, interval_samples_per_second: 2.002, interval_steps_per_second: 1.251, epoch: 0.5291[0m
[32m[2022-09-15 14:46:42,646] [    INFO][0m - loss: 2.82365913, learning_rate: 2.965079365079365e-06, global_step: 110, interval_runtime: 7.9888, interval_samples_per_second: 2.003, interval_steps_per_second: 1.252, epoch: 0.582[0m
[32m[2022-09-15 14:46:50,664] [    INFO][0m - loss: 2.48329163, learning_rate: 2.9619047619047622e-06, global_step: 120, interval_runtime: 8.0182, interval_samples_per_second: 1.995, interval_steps_per_second: 1.247, epoch: 0.6349[0m
[32m[2022-09-15 14:46:58,674] [    INFO][0m - loss: 2.34346561, learning_rate: 2.958730158730159e-06, global_step: 130, interval_runtime: 8.0099, interval_samples_per_second: 1.998, interval_steps_per_second: 1.248, epoch: 0.6878[0m
[32m[2022-09-15 14:47:06,661] [    INFO][0m - loss: 2.49690475, learning_rate: 2.9555555555555557e-06, global_step: 140, interval_runtime: 7.987, interval_samples_per_second: 2.003, interval_steps_per_second: 1.252, epoch: 0.7407[0m
[32m[2022-09-15 14:47:14,718] [    INFO][0m - loss: 2.27061043, learning_rate: 2.9523809523809525e-06, global_step: 150, interval_runtime: 8.0571, interval_samples_per_second: 1.986, interval_steps_per_second: 1.241, epoch: 0.7937[0m
[32m[2022-09-15 14:47:22,773] [    INFO][0m - loss: 2.32957268, learning_rate: 2.949206349206349e-06, global_step: 160, interval_runtime: 8.0549, interval_samples_per_second: 1.986, interval_steps_per_second: 1.241, epoch: 0.8466[0m
[32m[2022-09-15 14:47:30,780] [    INFO][0m - loss: 2.31970959, learning_rate: 2.946031746031746e-06, global_step: 170, interval_runtime: 8.0065, interval_samples_per_second: 1.998, interval_steps_per_second: 1.249, epoch: 0.8995[0m
[32m[2022-09-15 14:47:38,761] [    INFO][0m - loss: 2.62621956, learning_rate: 2.9428571428571427e-06, global_step: 180, interval_runtime: 7.9818, interval_samples_per_second: 2.005, interval_steps_per_second: 1.253, epoch: 0.9524[0m
[32m[2022-09-15 14:47:45,800] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 14:47:45,800] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-15 14:47:45,800] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 14:47:45,800] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 14:47:45,800] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-15 14:48:13,085] [    INFO][0m - eval_loss: 2.0892975330352783, eval_accuracy: 0.4435542607428988, eval_runtime: 27.2837, eval_samples_per_second: 50.323, eval_steps_per_second: 3.152, epoch: 1.0[0m
[32m[2022-09-15 14:48:13,110] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-189[0m
[32m[2022-09-15 14:48:13,110] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 14:48:16,064] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-189/tokenizer_config.json[0m
[32m[2022-09-15 14:48:16,065] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-189/special_tokens_map.json[0m
[32m[2022-09-15 14:48:22,574] [    INFO][0m - loss: 2.1222847, learning_rate: 2.93968253968254e-06, global_step: 190, interval_runtime: 43.8128, interval_samples_per_second: 0.365, interval_steps_per_second: 0.228, epoch: 1.0053[0m
[32m[2022-09-15 14:48:30,550] [    INFO][0m - loss: 1.86319962, learning_rate: 2.9365079365079366e-06, global_step: 200, interval_runtime: 7.976, interval_samples_per_second: 2.006, interval_steps_per_second: 1.254, epoch: 1.0582[0m
[32m[2022-09-15 14:48:38,555] [    INFO][0m - loss: 2.06846237, learning_rate: 2.9333333333333333e-06, global_step: 210, interval_runtime: 8.0048, interval_samples_per_second: 1.999, interval_steps_per_second: 1.249, epoch: 1.1111[0m
[32m[2022-09-15 14:48:46,563] [    INFO][0m - loss: 1.85936832, learning_rate: 2.9301587301587305e-06, global_step: 220, interval_runtime: 8.0078, interval_samples_per_second: 1.998, interval_steps_per_second: 1.249, epoch: 1.164[0m
[32m[2022-09-15 14:48:54,571] [    INFO][0m - loss: 2.06675453, learning_rate: 2.9269841269841272e-06, global_step: 230, interval_runtime: 8.0087, interval_samples_per_second: 1.998, interval_steps_per_second: 1.249, epoch: 1.2169[0m
[32m[2022-09-15 14:49:02,569] [    INFO][0m - loss: 1.79619884, learning_rate: 2.923809523809524e-06, global_step: 240, interval_runtime: 7.9973, interval_samples_per_second: 2.001, interval_steps_per_second: 1.25, epoch: 1.2698[0m
[32m[2022-09-15 14:49:10,547] [    INFO][0m - loss: 2.11182137, learning_rate: 2.9206349206349207e-06, global_step: 250, interval_runtime: 7.9782, interval_samples_per_second: 2.005, interval_steps_per_second: 1.253, epoch: 1.3228[0m
[32m[2022-09-15 14:49:18,564] [    INFO][0m - loss: 2.35321522, learning_rate: 2.9174603174603175e-06, global_step: 260, interval_runtime: 8.0165, interval_samples_per_second: 1.996, interval_steps_per_second: 1.247, epoch: 1.3757[0m
[32m[2022-09-15 14:49:26,659] [    INFO][0m - loss: 2.05295105, learning_rate: 2.9142857142857142e-06, global_step: 270, interval_runtime: 8.0952, interval_samples_per_second: 1.976, interval_steps_per_second: 1.235, epoch: 1.4286[0m
[32m[2022-09-15 14:49:34,680] [    INFO][0m - loss: 2.09860764, learning_rate: 2.911111111111111e-06, global_step: 280, interval_runtime: 8.0216, interval_samples_per_second: 1.995, interval_steps_per_second: 1.247, epoch: 1.4815[0m
[32m[2022-09-15 14:49:42,695] [    INFO][0m - loss: 1.95933514, learning_rate: 2.907936507936508e-06, global_step: 290, interval_runtime: 8.0149, interval_samples_per_second: 1.996, interval_steps_per_second: 1.248, epoch: 1.5344[0m
[32m[2022-09-15 14:49:50,734] [    INFO][0m - loss: 2.00259228, learning_rate: 2.904761904761905e-06, global_step: 300, interval_runtime: 8.0387, interval_samples_per_second: 1.99, interval_steps_per_second: 1.244, epoch: 1.5873[0m
[32m[2022-09-15 14:49:58,763] [    INFO][0m - loss: 2.1494524, learning_rate: 2.9015873015873016e-06, global_step: 310, interval_runtime: 8.0291, interval_samples_per_second: 1.993, interval_steps_per_second: 1.245, epoch: 1.6402[0m
[32m[2022-09-15 14:50:06,814] [    INFO][0m - loss: 1.76785393, learning_rate: 2.8984126984126988e-06, global_step: 320, interval_runtime: 8.0504, interval_samples_per_second: 1.987, interval_steps_per_second: 1.242, epoch: 1.6931[0m
[32m[2022-09-15 14:50:14,852] [    INFO][0m - loss: 2.07593651, learning_rate: 2.8952380952380955e-06, global_step: 330, interval_runtime: 8.0387, interval_samples_per_second: 1.99, interval_steps_per_second: 1.244, epoch: 1.746[0m
[32m[2022-09-15 14:50:22,873] [    INFO][0m - loss: 1.9886982, learning_rate: 2.8920634920634923e-06, global_step: 340, interval_runtime: 8.02, interval_samples_per_second: 1.995, interval_steps_per_second: 1.247, epoch: 1.7989[0m
[32m[2022-09-15 14:50:30,857] [    INFO][0m - loss: 1.83812542, learning_rate: 2.888888888888889e-06, global_step: 350, interval_runtime: 7.9842, interval_samples_per_second: 2.004, interval_steps_per_second: 1.252, epoch: 1.8519[0m
[32m[2022-09-15 14:50:38,878] [    INFO][0m - loss: 1.89814663, learning_rate: 2.8857142857142857e-06, global_step: 360, interval_runtime: 8.0217, interval_samples_per_second: 1.995, interval_steps_per_second: 1.247, epoch: 1.9048[0m
[32m[2022-09-15 14:50:46,892] [    INFO][0m - loss: 1.88322601, learning_rate: 2.8825396825396825e-06, global_step: 370, interval_runtime: 8.0135, interval_samples_per_second: 1.997, interval_steps_per_second: 1.248, epoch: 1.9577[0m
[32m[2022-09-15 14:50:53,129] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 14:50:53,129] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-15 14:50:53,129] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 14:50:53,129] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 14:50:53,129] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-15 14:51:20,252] [    INFO][0m - eval_loss: 1.8855303525924683, eval_accuracy: 0.46249089584850694, eval_runtime: 27.1229, eval_samples_per_second: 50.621, eval_steps_per_second: 3.171, epoch: 2.0[0m
[32m[2022-09-15 14:51:20,276] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-378[0m
[32m[2022-09-15 14:51:20,276] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 14:51:23,485] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-378/tokenizer_config.json[0m
[32m[2022-09-15 14:51:23,485] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-378/special_tokens_map.json[0m
[32m[2022-09-15 14:51:32,051] [    INFO][0m - loss: 1.52741556, learning_rate: 2.8793650793650792e-06, global_step: 380, interval_runtime: 45.1593, interval_samples_per_second: 0.354, interval_steps_per_second: 0.221, epoch: 2.0106[0m
[32m[2022-09-15 14:51:40,019] [    INFO][0m - loss: 1.71272678, learning_rate: 2.8761904761904764e-06, global_step: 390, interval_runtime: 7.9678, interval_samples_per_second: 2.008, interval_steps_per_second: 1.255, epoch: 2.0635[0m
[32m[2022-09-15 14:51:47,980] [    INFO][0m - loss: 1.5619791, learning_rate: 2.873015873015873e-06, global_step: 400, interval_runtime: 7.9606, interval_samples_per_second: 2.01, interval_steps_per_second: 1.256, epoch: 2.1164[0m
[32m[2022-09-15 14:51:55,921] [    INFO][0m - loss: 1.80451107, learning_rate: 2.86984126984127e-06, global_step: 410, interval_runtime: 7.9415, interval_samples_per_second: 2.015, interval_steps_per_second: 1.259, epoch: 2.1693[0m
[32m[2022-09-15 14:52:04,618] [    INFO][0m - loss: 1.60816669, learning_rate: 2.866666666666667e-06, global_step: 420, interval_runtime: 7.9873, interval_samples_per_second: 2.003, interval_steps_per_second: 1.252, epoch: 2.2222[0m
[32m[2022-09-15 14:52:12,610] [    INFO][0m - loss: 1.62803631, learning_rate: 2.8634920634920638e-06, global_step: 430, interval_runtime: 8.7014, interval_samples_per_second: 1.839, interval_steps_per_second: 1.149, epoch: 2.2751[0m
[32m[2022-09-15 14:52:20,603] [    INFO][0m - loss: 1.7198101, learning_rate: 2.8603174603174605e-06, global_step: 440, interval_runtime: 7.9927, interval_samples_per_second: 2.002, interval_steps_per_second: 1.251, epoch: 2.328[0m
[32m[2022-09-15 14:52:28,603] [    INFO][0m - loss: 1.54441071, learning_rate: 2.8571428571428573e-06, global_step: 450, interval_runtime: 8.0002, interval_samples_per_second: 2.0, interval_steps_per_second: 1.25, epoch: 2.381[0m
[32m[2022-09-15 14:52:36,636] [    INFO][0m - loss: 1.53399239, learning_rate: 2.853968253968254e-06, global_step: 460, interval_runtime: 8.0331, interval_samples_per_second: 1.992, interval_steps_per_second: 1.245, epoch: 2.4339[0m
[32m[2022-09-15 14:52:44,649] [    INFO][0m - loss: 1.5809845, learning_rate: 2.8507936507936507e-06, global_step: 470, interval_runtime: 8.0133, interval_samples_per_second: 1.997, interval_steps_per_second: 1.248, epoch: 2.4868[0m
[32m[2022-09-15 14:52:52,675] [    INFO][0m - loss: 1.7113287, learning_rate: 2.8476190476190475e-06, global_step: 480, interval_runtime: 8.0254, interval_samples_per_second: 1.994, interval_steps_per_second: 1.246, epoch: 2.5397[0m
[32m[2022-09-15 14:53:00,749] [    INFO][0m - loss: 1.70153198, learning_rate: 2.8444444444444446e-06, global_step: 490, interval_runtime: 8.0741, interval_samples_per_second: 1.982, interval_steps_per_second: 1.239, epoch: 2.5926[0m
[32m[2022-09-15 14:53:08,719] [    INFO][0m - loss: 1.64435101, learning_rate: 2.8412698412698414e-06, global_step: 500, interval_runtime: 7.9707, interval_samples_per_second: 2.007, interval_steps_per_second: 1.255, epoch: 2.6455[0m
[32m[2022-09-15 14:53:16,674] [    INFO][0m - loss: 1.7474411, learning_rate: 2.838095238095238e-06, global_step: 510, interval_runtime: 7.9544, interval_samples_per_second: 2.011, interval_steps_per_second: 1.257, epoch: 2.6984[0m
[32m[2022-09-15 14:53:24,661] [    INFO][0m - loss: 1.71023998, learning_rate: 2.8349206349206353e-06, global_step: 520, interval_runtime: 7.987, interval_samples_per_second: 2.003, interval_steps_per_second: 1.252, epoch: 2.7513[0m
[32m[2022-09-15 14:53:32,665] [    INFO][0m - loss: 1.72378025, learning_rate: 2.831746031746032e-06, global_step: 530, interval_runtime: 8.0042, interval_samples_per_second: 1.999, interval_steps_per_second: 1.249, epoch: 2.8042[0m
[32m[2022-09-15 14:53:40,657] [    INFO][0m - loss: 1.6054903, learning_rate: 2.8285714285714288e-06, global_step: 540, interval_runtime: 7.9921, interval_samples_per_second: 2.002, interval_steps_per_second: 1.251, epoch: 2.8571[0m
[32m[2022-09-15 14:53:48,668] [    INFO][0m - loss: 1.49679699, learning_rate: 2.8253968253968255e-06, global_step: 550, interval_runtime: 8.011, interval_samples_per_second: 1.997, interval_steps_per_second: 1.248, epoch: 2.9101[0m
[32m[2022-09-15 14:53:56,658] [    INFO][0m - loss: 1.55039864, learning_rate: 2.8222222222222223e-06, global_step: 560, interval_runtime: 7.9897, interval_samples_per_second: 2.003, interval_steps_per_second: 1.252, epoch: 2.963[0m
[32m[2022-09-15 14:54:02,075] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 14:54:02,075] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-15 14:54:02,075] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 14:54:02,075] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 14:54:02,075] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-15 14:54:29,182] [    INFO][0m - eval_loss: 1.808472990989685, eval_accuracy: 0.4668608885651857, eval_runtime: 27.1065, eval_samples_per_second: 50.652, eval_steps_per_second: 3.173, epoch: 3.0[0m
[32m[2022-09-15 14:54:29,206] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-567[0m
[32m[2022-09-15 14:54:29,206] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 14:54:32,796] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-567/tokenizer_config.json[0m
[32m[2022-09-15 14:54:32,796] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-567/special_tokens_map.json[0m
[32m[2022-09-15 14:54:42,056] [    INFO][0m - loss: 1.61472416, learning_rate: 2.819047619047619e-06, global_step: 570, interval_runtime: 45.3983, interval_samples_per_second: 0.352, interval_steps_per_second: 0.22, epoch: 3.0159[0m
[32m[2022-09-15 14:54:49,999] [    INFO][0m - loss: 1.45437956, learning_rate: 2.8158730158730157e-06, global_step: 580, interval_runtime: 7.9425, interval_samples_per_second: 2.014, interval_steps_per_second: 1.259, epoch: 3.0688[0m
[32m[2022-09-15 14:54:57,955] [    INFO][0m - loss: 1.62721348, learning_rate: 2.812698412698413e-06, global_step: 590, interval_runtime: 7.9561, interval_samples_per_second: 2.011, interval_steps_per_second: 1.257, epoch: 3.1217[0m
[32m[2022-09-15 14:55:05,915] [    INFO][0m - loss: 1.49522552, learning_rate: 2.8095238095238096e-06, global_step: 600, interval_runtime: 7.9603, interval_samples_per_second: 2.01, interval_steps_per_second: 1.256, epoch: 3.1746[0m
[32m[2022-09-15 14:55:14,698] [    INFO][0m - loss: 1.44498539, learning_rate: 2.8063492063492064e-06, global_step: 610, interval_runtime: 7.9604, interval_samples_per_second: 2.01, interval_steps_per_second: 1.256, epoch: 3.2275[0m
[32m[2022-09-15 14:55:22,671] [    INFO][0m - loss: 1.30369244, learning_rate: 2.8031746031746036e-06, global_step: 620, interval_runtime: 8.7955, interval_samples_per_second: 1.819, interval_steps_per_second: 1.137, epoch: 3.2804[0m
[32m[2022-09-15 14:55:30,639] [    INFO][0m - loss: 1.45552998, learning_rate: 2.8000000000000003e-06, global_step: 630, interval_runtime: 7.9675, interval_samples_per_second: 2.008, interval_steps_per_second: 1.255, epoch: 3.3333[0m
[32m[2022-09-15 14:55:38,652] [    INFO][0m - loss: 1.42021742, learning_rate: 2.796825396825397e-06, global_step: 640, interval_runtime: 8.0135, interval_samples_per_second: 1.997, interval_steps_per_second: 1.248, epoch: 3.3862[0m
[32m[2022-09-15 14:55:46,654] [    INFO][0m - loss: 1.23818798, learning_rate: 2.7936507936507934e-06, global_step: 650, interval_runtime: 8.0022, interval_samples_per_second: 1.999, interval_steps_per_second: 1.25, epoch: 3.4392[0m
[32m[2022-09-15 14:55:54,671] [    INFO][0m - loss: 1.34656553, learning_rate: 2.7904761904761905e-06, global_step: 660, interval_runtime: 8.0171, interval_samples_per_second: 1.996, interval_steps_per_second: 1.247, epoch: 3.4921[0m
[32m[2022-09-15 14:56:02,679] [    INFO][0m - loss: 1.39711847, learning_rate: 2.7873015873015873e-06, global_step: 670, interval_runtime: 8.0079, interval_samples_per_second: 1.998, interval_steps_per_second: 1.249, epoch: 3.545[0m
[32m[2022-09-15 14:56:10,684] [    INFO][0m - loss: 1.32107382, learning_rate: 2.784126984126984e-06, global_step: 680, interval_runtime: 8.0047, interval_samples_per_second: 1.999, interval_steps_per_second: 1.249, epoch: 3.5979[0m
[32m[2022-09-15 14:56:18,703] [    INFO][0m - loss: 1.63548069, learning_rate: 2.780952380952381e-06, global_step: 690, interval_runtime: 8.0188, interval_samples_per_second: 1.995, interval_steps_per_second: 1.247, epoch: 3.6508[0m
[32m[2022-09-15 14:56:26,699] [    INFO][0m - loss: 1.59530401, learning_rate: 2.777777777777778e-06, global_step: 700, interval_runtime: 7.9961, interval_samples_per_second: 2.001, interval_steps_per_second: 1.251, epoch: 3.7037[0m
[32m[2022-09-15 14:56:34,717] [    INFO][0m - loss: 1.09773464, learning_rate: 2.7746031746031747e-06, global_step: 710, interval_runtime: 8.0177, interval_samples_per_second: 1.996, interval_steps_per_second: 1.247, epoch: 3.7566[0m
[32m[2022-09-15 14:56:42,733] [    INFO][0m - loss: 1.26492414, learning_rate: 2.771428571428572e-06, global_step: 720, interval_runtime: 8.0163, interval_samples_per_second: 1.996, interval_steps_per_second: 1.247, epoch: 3.8095[0m
[32m[2022-09-15 14:56:50,764] [    INFO][0m - loss: 1.50488434, learning_rate: 2.7682539682539686e-06, global_step: 730, interval_runtime: 8.0307, interval_samples_per_second: 1.992, interval_steps_per_second: 1.245, epoch: 3.8624[0m
[32m[2022-09-15 14:56:58,798] [    INFO][0m - loss: 1.52271061, learning_rate: 2.7650793650793653e-06, global_step: 740, interval_runtime: 8.0341, interval_samples_per_second: 1.992, interval_steps_per_second: 1.245, epoch: 3.9153[0m
[32m[2022-09-15 14:57:06,847] [    INFO][0m - loss: 1.55991478, learning_rate: 2.7619047619047616e-06, global_step: 750, interval_runtime: 8.0495, interval_samples_per_second: 1.988, interval_steps_per_second: 1.242, epoch: 3.9683[0m
[32m[2022-09-15 14:57:11,489] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 14:57:11,489] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-15 14:57:11,489] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 14:57:11,489] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 14:57:11,489] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-15 14:57:38,756] [    INFO][0m - eval_loss: 1.8422315120697021, eval_accuracy: 0.4697742170429716, eval_runtime: 27.2664, eval_samples_per_second: 50.355, eval_steps_per_second: 3.154, epoch: 4.0[0m
[32m[2022-09-15 14:57:38,779] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-756[0m
[32m[2022-09-15 14:57:38,780] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 14:57:41,887] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-756/tokenizer_config.json[0m
[32m[2022-09-15 14:57:41,887] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-756/special_tokens_map.json[0m
[32m[2022-09-15 14:57:51,521] [    INFO][0m - loss: 1.44188175, learning_rate: 2.7587301587301588e-06, global_step: 760, interval_runtime: 44.6734, interval_samples_per_second: 0.358, interval_steps_per_second: 0.224, epoch: 4.0212[0m
[32m[2022-09-15 14:57:59,523] [    INFO][0m - loss: 1.19814949, learning_rate: 2.7555555555555555e-06, global_step: 770, interval_runtime: 8.0023, interval_samples_per_second: 1.999, interval_steps_per_second: 1.25, epoch: 4.0741[0m
[32m[2022-09-15 14:58:07,554] [    INFO][0m - loss: 1.14423409, learning_rate: 2.7523809523809523e-06, global_step: 780, interval_runtime: 8.031, interval_samples_per_second: 1.992, interval_steps_per_second: 1.245, epoch: 4.127[0m
[32m[2022-09-15 14:58:15,600] [    INFO][0m - loss: 1.16658602, learning_rate: 2.7492063492063494e-06, global_step: 790, interval_runtime: 8.0465, interval_samples_per_second: 1.988, interval_steps_per_second: 1.243, epoch: 4.1799[0m
[32m[2022-09-15 14:58:23,626] [    INFO][0m - loss: 1.07912512, learning_rate: 2.746031746031746e-06, global_step: 800, interval_runtime: 8.0256, interval_samples_per_second: 1.994, interval_steps_per_second: 1.246, epoch: 4.2328[0m
[32m[2022-09-15 14:58:31,598] [    INFO][0m - loss: 1.24111385, learning_rate: 2.742857142857143e-06, global_step: 810, interval_runtime: 7.9717, interval_samples_per_second: 2.007, interval_steps_per_second: 1.254, epoch: 4.2857[0m
[32m[2022-09-15 14:58:39,599] [    INFO][0m - loss: 1.11903305, learning_rate: 2.73968253968254e-06, global_step: 820, interval_runtime: 8.0012, interval_samples_per_second: 2.0, interval_steps_per_second: 1.25, epoch: 4.3386[0m
[32m[2022-09-15 14:58:47,589] [    INFO][0m - loss: 1.51169615, learning_rate: 2.736507936507937e-06, global_step: 830, interval_runtime: 7.99, interval_samples_per_second: 2.002, interval_steps_per_second: 1.252, epoch: 4.3915[0m
[32m[2022-09-15 14:58:55,571] [    INFO][0m - loss: 1.41121063, learning_rate: 2.733333333333333e-06, global_step: 840, interval_runtime: 7.9823, interval_samples_per_second: 2.004, interval_steps_per_second: 1.253, epoch: 4.4444[0m
[32m[2022-09-15 14:59:03,583] [    INFO][0m - loss: 1.14308109, learning_rate: 2.73015873015873e-06, global_step: 850, interval_runtime: 8.0112, interval_samples_per_second: 1.997, interval_steps_per_second: 1.248, epoch: 4.4974[0m
[32m[2022-09-15 14:59:11,570] [    INFO][0m - loss: 1.38172989, learning_rate: 2.726984126984127e-06, global_step: 860, interval_runtime: 7.9874, interval_samples_per_second: 2.003, interval_steps_per_second: 1.252, epoch: 4.5503[0m
[32m[2022-09-15 14:59:19,561] [    INFO][0m - loss: 1.19837475, learning_rate: 2.7238095238095238e-06, global_step: 870, interval_runtime: 7.9914, interval_samples_per_second: 2.002, interval_steps_per_second: 1.251, epoch: 4.6032[0m
[32m[2022-09-15 14:59:27,524] [    INFO][0m - loss: 1.64166679, learning_rate: 2.7206349206349205e-06, global_step: 880, interval_runtime: 7.9623, interval_samples_per_second: 2.009, interval_steps_per_second: 1.256, epoch: 4.6561[0m
[32m[2022-09-15 14:59:35,522] [    INFO][0m - loss: 1.30433836, learning_rate: 2.7174603174603177e-06, global_step: 890, interval_runtime: 7.9984, interval_samples_per_second: 2.0, interval_steps_per_second: 1.25, epoch: 4.709[0m
[32m[2022-09-15 14:59:43,506] [    INFO][0m - loss: 1.23335009, learning_rate: 2.7142857142857144e-06, global_step: 900, interval_runtime: 7.9841, interval_samples_per_second: 2.004, interval_steps_per_second: 1.252, epoch: 4.7619[0m
[32m[2022-09-15 14:59:51,543] [    INFO][0m - loss: 1.35247183, learning_rate: 2.711111111111111e-06, global_step: 910, interval_runtime: 8.0363, interval_samples_per_second: 1.991, interval_steps_per_second: 1.244, epoch: 4.8148[0m
[32m[2022-09-15 14:59:59,511] [    INFO][0m - loss: 1.26678028, learning_rate: 2.7079365079365083e-06, global_step: 920, interval_runtime: 7.9683, interval_samples_per_second: 2.008, interval_steps_per_second: 1.255, epoch: 4.8677[0m
[32m[2022-09-15 15:00:07,541] [    INFO][0m - loss: 1.26283331, learning_rate: 2.704761904761905e-06, global_step: 930, interval_runtime: 8.0298, interval_samples_per_second: 1.993, interval_steps_per_second: 1.245, epoch: 4.9206[0m
[32m[2022-09-15 15:00:15,530] [    INFO][0m - loss: 1.19444838, learning_rate: 2.7015873015873014e-06, global_step: 940, interval_runtime: 7.9892, interval_samples_per_second: 2.003, interval_steps_per_second: 1.252, epoch: 4.9735[0m
[32m[2022-09-15 15:00:19,338] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 15:00:19,339] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-15 15:00:19,339] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 15:00:19,339] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 15:00:19,339] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-15 15:00:46,522] [    INFO][0m - eval_loss: 1.8684215545654297, eval_accuracy: 0.4646758922068463, eval_runtime: 27.1829, eval_samples_per_second: 50.51, eval_steps_per_second: 3.164, epoch: 5.0[0m
[32m[2022-09-15 15:00:46,548] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-945[0m
[32m[2022-09-15 15:00:46,549] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 15:00:49,692] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-945/tokenizer_config.json[0m
[32m[2022-09-15 15:00:49,692] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-945/special_tokens_map.json[0m
[32m[2022-09-15 15:00:59,150] [    INFO][0m - loss: 1.01665516, learning_rate: 2.698412698412698e-06, global_step: 950, interval_runtime: 43.6205, interval_samples_per_second: 0.367, interval_steps_per_second: 0.229, epoch: 5.0265[0m
[32m[2022-09-15 15:01:07,138] [    INFO][0m - loss: 1.00291891, learning_rate: 2.6952380952380953e-06, global_step: 960, interval_runtime: 7.9875, interval_samples_per_second: 2.003, interval_steps_per_second: 1.252, epoch: 5.0794[0m
[32m[2022-09-15 15:01:15,105] [    INFO][0m - loss: 0.94461107, learning_rate: 2.692063492063492e-06, global_step: 970, interval_runtime: 7.9677, interval_samples_per_second: 2.008, interval_steps_per_second: 1.255, epoch: 5.1323[0m
[32m[2022-09-15 15:01:23,066] [    INFO][0m - loss: 1.31017017, learning_rate: 2.688888888888889e-06, global_step: 980, interval_runtime: 7.9603, interval_samples_per_second: 2.01, interval_steps_per_second: 1.256, epoch: 5.1852[0m
[32m[2022-09-15 15:01:31,050] [    INFO][0m - loss: 0.9160677, learning_rate: 2.685714285714286e-06, global_step: 990, interval_runtime: 7.9847, interval_samples_per_second: 2.004, interval_steps_per_second: 1.252, epoch: 5.2381[0m
[32m[2022-09-15 15:01:39,025] [    INFO][0m - loss: 1.05708466, learning_rate: 2.6825396825396827e-06, global_step: 1000, interval_runtime: 7.9748, interval_samples_per_second: 2.006, interval_steps_per_second: 1.254, epoch: 5.291[0m
[32m[2022-09-15 15:01:46,991] [    INFO][0m - loss: 1.14835482, learning_rate: 2.6793650793650794e-06, global_step: 1010, interval_runtime: 7.9659, interval_samples_per_second: 2.009, interval_steps_per_second: 1.255, epoch: 5.3439[0m
[32m[2022-09-15 15:01:54,986] [    INFO][0m - loss: 0.91896124, learning_rate: 2.6761904761904766e-06, global_step: 1020, interval_runtime: 7.9953, interval_samples_per_second: 2.001, interval_steps_per_second: 1.251, epoch: 5.3968[0m
[32m[2022-09-15 15:02:02,987] [    INFO][0m - loss: 1.10538521, learning_rate: 2.6730158730158733e-06, global_step: 1030, interval_runtime: 8.0007, interval_samples_per_second: 2.0, interval_steps_per_second: 1.25, epoch: 5.4497[0m
[32m[2022-09-15 15:02:10,953] [    INFO][0m - loss: 1.22922764, learning_rate: 2.6698412698412697e-06, global_step: 1040, interval_runtime: 7.9659, interval_samples_per_second: 2.009, interval_steps_per_second: 1.255, epoch: 5.5026[0m
[32m[2022-09-15 15:02:18,980] [    INFO][0m - loss: 1.25540123, learning_rate: 2.6666666666666664e-06, global_step: 1050, interval_runtime: 8.027, interval_samples_per_second: 1.993, interval_steps_per_second: 1.246, epoch: 5.5556[0m
[32m[2022-09-15 15:02:26,984] [    INFO][0m - loss: 1.39400663, learning_rate: 2.6634920634920636e-06, global_step: 1060, interval_runtime: 8.0034, interval_samples_per_second: 1.999, interval_steps_per_second: 1.249, epoch: 5.6085[0m
[32m[2022-09-15 15:02:34,952] [    INFO][0m - loss: 1.07194595, learning_rate: 2.6603174603174603e-06, global_step: 1070, interval_runtime: 7.9688, interval_samples_per_second: 2.008, interval_steps_per_second: 1.255, epoch: 5.6614[0m
[32m[2022-09-15 15:02:42,930] [    INFO][0m - loss: 1.4255991, learning_rate: 2.657142857142857e-06, global_step: 1080, interval_runtime: 7.978, interval_samples_per_second: 2.006, interval_steps_per_second: 1.253, epoch: 5.7143[0m
[32m[2022-09-15 15:02:50,929] [    INFO][0m - loss: 1.1178937, learning_rate: 2.6539682539682542e-06, global_step: 1090, interval_runtime: 7.9986, interval_samples_per_second: 2.0, interval_steps_per_second: 1.25, epoch: 5.7672[0m
[32m[2022-09-15 15:02:58,944] [    INFO][0m - loss: 1.03337088, learning_rate: 2.650793650793651e-06, global_step: 1100, interval_runtime: 8.0153, interval_samples_per_second: 1.996, interval_steps_per_second: 1.248, epoch: 5.8201[0m
[32m[2022-09-15 15:03:06,979] [    INFO][0m - loss: 1.09918623, learning_rate: 2.6476190476190477e-06, global_step: 1110, interval_runtime: 8.0347, interval_samples_per_second: 1.991, interval_steps_per_second: 1.245, epoch: 5.873[0m
[32m[2022-09-15 15:03:14,977] [    INFO][0m - loss: 1.0337656, learning_rate: 2.644444444444445e-06, global_step: 1120, interval_runtime: 7.998, interval_samples_per_second: 2.0, interval_steps_per_second: 1.25, epoch: 5.9259[0m
[32m[2022-09-15 15:03:22,958] [    INFO][0m - loss: 1.07894793, learning_rate: 2.641269841269841e-06, global_step: 1130, interval_runtime: 7.9811, interval_samples_per_second: 2.005, interval_steps_per_second: 1.253, epoch: 5.9788[0m
[32m[2022-09-15 15:03:26,006] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 15:03:26,006] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-15 15:03:26,007] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 15:03:26,007] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 15:03:26,007] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-15 15:03:53,174] [    INFO][0m - eval_loss: 1.9190502166748047, eval_accuracy: 0.461034231609614, eval_runtime: 27.1674, eval_samples_per_second: 50.539, eval_steps_per_second: 3.166, epoch: 6.0[0m
[32m[2022-09-15 15:03:53,198] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1134[0m
[32m[2022-09-15 15:03:53,198] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 15:03:55,992] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1134/tokenizer_config.json[0m
[32m[2022-09-15 15:03:55,992] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1134/special_tokens_map.json[0m
[32m[2022-09-15 15:04:07,303] [    INFO][0m - loss: 0.97639923, learning_rate: 2.638095238095238e-06, global_step: 1140, interval_runtime: 44.3454, interval_samples_per_second: 0.361, interval_steps_per_second: 0.226, epoch: 6.0317[0m
[32m[2022-09-15 15:04:15,282] [    INFO][0m - loss: 0.9829937, learning_rate: 2.6349206349206347e-06, global_step: 1150, interval_runtime: 7.9785, interval_samples_per_second: 2.005, interval_steps_per_second: 1.253, epoch: 6.0847[0m
[32m[2022-09-15 15:04:23,298] [    INFO][0m - loss: 0.85259418, learning_rate: 2.631746031746032e-06, global_step: 1160, interval_runtime: 8.0157, interval_samples_per_second: 1.996, interval_steps_per_second: 1.248, epoch: 6.1376[0m
[32m[2022-09-15 15:04:31,312] [    INFO][0m - loss: 1.16758165, learning_rate: 2.6285714285714286e-06, global_step: 1170, interval_runtime: 8.0142, interval_samples_per_second: 1.996, interval_steps_per_second: 1.248, epoch: 6.1905[0m
[32m[2022-09-15 15:04:39,309] [    INFO][0m - loss: 0.82483187, learning_rate: 2.6253968253968253e-06, global_step: 1180, interval_runtime: 7.9965, interval_samples_per_second: 2.001, interval_steps_per_second: 1.251, epoch: 6.2434[0m
[32m[2022-09-15 15:04:47,314] [    INFO][0m - loss: 0.92758007, learning_rate: 2.6222222222222225e-06, global_step: 1190, interval_runtime: 8.0056, interval_samples_per_second: 1.999, interval_steps_per_second: 1.249, epoch: 6.2963[0m
[32m[2022-09-15 15:04:55,399] [    INFO][0m - loss: 0.89434795, learning_rate: 2.6190476190476192e-06, global_step: 1200, interval_runtime: 8.0849, interval_samples_per_second: 1.979, interval_steps_per_second: 1.237, epoch: 6.3492[0m
[32m[2022-09-15 15:05:03,418] [    INFO][0m - loss: 0.78767452, learning_rate: 2.615873015873016e-06, global_step: 1210, interval_runtime: 8.0193, interval_samples_per_second: 1.995, interval_steps_per_second: 1.247, epoch: 6.4021[0m
[32m[2022-09-15 15:05:11,453] [    INFO][0m - loss: 0.99630728, learning_rate: 2.612698412698413e-06, global_step: 1220, interval_runtime: 8.0348, interval_samples_per_second: 1.991, interval_steps_per_second: 1.245, epoch: 6.455[0m
[32m[2022-09-15 15:05:19,457] [    INFO][0m - loss: 1.07523088, learning_rate: 2.6095238095238094e-06, global_step: 1230, interval_runtime: 8.0039, interval_samples_per_second: 1.999, interval_steps_per_second: 1.249, epoch: 6.5079[0m
[32m[2022-09-15 15:05:27,459] [    INFO][0m - loss: 1.03032627, learning_rate: 2.606349206349206e-06, global_step: 1240, interval_runtime: 8.0017, interval_samples_per_second: 2.0, interval_steps_per_second: 1.25, epoch: 6.5608[0m
[32m[2022-09-15 15:05:35,500] [    INFO][0m - loss: 1.09047041, learning_rate: 2.603174603174603e-06, global_step: 1250, interval_runtime: 8.0415, interval_samples_per_second: 1.99, interval_steps_per_second: 1.244, epoch: 6.6138[0m
[32m[2022-09-15 15:05:43,479] [    INFO][0m - loss: 1.01443729, learning_rate: 2.6e-06, global_step: 1260, interval_runtime: 7.9793, interval_samples_per_second: 2.005, interval_steps_per_second: 1.253, epoch: 6.6667[0m
[32m[2022-09-15 15:05:51,502] [    INFO][0m - loss: 0.99893351, learning_rate: 2.596825396825397e-06, global_step: 1270, interval_runtime: 8.0224, interval_samples_per_second: 1.994, interval_steps_per_second: 1.247, epoch: 6.7196[0m
[32m[2022-09-15 15:05:59,567] [    INFO][0m - loss: 0.96525993, learning_rate: 2.5936507936507936e-06, global_step: 1280, interval_runtime: 8.0645, interval_samples_per_second: 1.984, interval_steps_per_second: 1.24, epoch: 6.7725[0m
[32m[2022-09-15 15:06:07,576] [    INFO][0m - loss: 1.01779356, learning_rate: 2.5904761904761907e-06, global_step: 1290, interval_runtime: 8.0096, interval_samples_per_second: 1.998, interval_steps_per_second: 1.248, epoch: 6.8254[0m
[32m[2022-09-15 15:06:15,575] [    INFO][0m - loss: 1.02257576, learning_rate: 2.5873015873015875e-06, global_step: 1300, interval_runtime: 7.9992, interval_samples_per_second: 2.0, interval_steps_per_second: 1.25, epoch: 6.8783[0m
[32m[2022-09-15 15:06:23,561] [    INFO][0m - loss: 0.76976175, learning_rate: 2.5841269841269842e-06, global_step: 1310, interval_runtime: 7.9857, interval_samples_per_second: 2.004, interval_steps_per_second: 1.252, epoch: 6.9312[0m
[32m[2022-09-15 15:06:31,488] [    INFO][0m - loss: 1.04354477, learning_rate: 2.580952380952381e-06, global_step: 1320, interval_runtime: 7.9274, interval_samples_per_second: 2.018, interval_steps_per_second: 1.261, epoch: 6.9841[0m
[32m[2022-09-15 15:06:33,769] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 15:06:33,769] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-15 15:06:33,769] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 15:06:33,769] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 15:06:33,770] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-15 15:07:00,867] [    INFO][0m - eval_loss: 1.9480154514312744, eval_accuracy: 0.4646758922068463, eval_runtime: 27.0968, eval_samples_per_second: 50.67, eval_steps_per_second: 3.174, epoch: 7.0[0m
[32m[2022-09-15 15:07:00,891] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1323[0m
[32m[2022-09-15 15:07:00,891] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 15:07:04,002] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1323/tokenizer_config.json[0m
[32m[2022-09-15 15:07:04,002] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1323/special_tokens_map.json[0m
[32m[2022-09-15 15:07:16,399] [    INFO][0m - loss: 0.86838665, learning_rate: 2.5777777777777777e-06, global_step: 1330, interval_runtime: 44.9108, interval_samples_per_second: 0.356, interval_steps_per_second: 0.223, epoch: 7.037[0m
[32m[2022-09-15 15:07:24,357] [    INFO][0m - loss: 0.76350355, learning_rate: 2.5746031746031744e-06, global_step: 1340, interval_runtime: 7.9577, interval_samples_per_second: 2.011, interval_steps_per_second: 1.257, epoch: 7.0899[0m
[32m[2022-09-15 15:07:32,352] [    INFO][0m - loss: 0.70778179, learning_rate: 2.571428571428571e-06, global_step: 1350, interval_runtime: 7.9946, interval_samples_per_second: 2.001, interval_steps_per_second: 1.251, epoch: 7.1429[0m
[32m[2022-09-15 15:07:40,326] [    INFO][0m - loss: 0.83502502, learning_rate: 2.5682539682539684e-06, global_step: 1360, interval_runtime: 7.9743, interval_samples_per_second: 2.006, interval_steps_per_second: 1.254, epoch: 7.1958[0m
[32m[2022-09-15 15:07:48,307] [    INFO][0m - loss: 0.92578402, learning_rate: 2.565079365079365e-06, global_step: 1370, interval_runtime: 7.9814, interval_samples_per_second: 2.005, interval_steps_per_second: 1.253, epoch: 7.2487[0m
[32m[2022-09-15 15:07:56,323] [    INFO][0m - loss: 0.93615971, learning_rate: 2.561904761904762e-06, global_step: 1380, interval_runtime: 8.0156, interval_samples_per_second: 1.996, interval_steps_per_second: 1.248, epoch: 7.3016[0m
[32m[2022-09-15 15:08:04,329] [    INFO][0m - loss: 0.93690405, learning_rate: 2.558730158730159e-06, global_step: 1390, interval_runtime: 8.0063, interval_samples_per_second: 1.998, interval_steps_per_second: 1.249, epoch: 7.3545[0m
[32m[2022-09-15 15:08:12,292] [    INFO][0m - loss: 0.83226738, learning_rate: 2.5555555555555557e-06, global_step: 1400, interval_runtime: 7.9628, interval_samples_per_second: 2.009, interval_steps_per_second: 1.256, epoch: 7.4074[0m
[32m[2022-09-15 15:08:20,286] [    INFO][0m - loss: 0.86104183, learning_rate: 2.5523809523809525e-06, global_step: 1410, interval_runtime: 7.9937, interval_samples_per_second: 2.002, interval_steps_per_second: 1.251, epoch: 7.4603[0m
[32m[2022-09-15 15:08:28,354] [    INFO][0m - loss: 0.77017212, learning_rate: 2.5492063492063492e-06, global_step: 1420, interval_runtime: 8.0677, interval_samples_per_second: 1.983, interval_steps_per_second: 1.24, epoch: 7.5132[0m
[32m[2022-09-15 15:08:36,355] [    INFO][0m - loss: 0.80173979, learning_rate: 2.546031746031746e-06, global_step: 1430, interval_runtime: 8.0019, interval_samples_per_second: 2.0, interval_steps_per_second: 1.25, epoch: 7.5661[0m
[32m[2022-09-15 15:08:44,322] [    INFO][0m - loss: 0.79711967, learning_rate: 2.5428571428571427e-06, global_step: 1440, interval_runtime: 7.9668, interval_samples_per_second: 2.008, interval_steps_per_second: 1.255, epoch: 7.619[0m
[32m[2022-09-15 15:08:52,332] [    INFO][0m - loss: 0.83276691, learning_rate: 2.5396825396825395e-06, global_step: 1450, interval_runtime: 8.0099, interval_samples_per_second: 1.998, interval_steps_per_second: 1.248, epoch: 7.672[0m
[32m[2022-09-15 15:09:00,338] [    INFO][0m - loss: 0.87565136, learning_rate: 2.5365079365079366e-06, global_step: 1460, interval_runtime: 8.0061, interval_samples_per_second: 1.998, interval_steps_per_second: 1.249, epoch: 7.7249[0m
[32m[2022-09-15 15:09:08,343] [    INFO][0m - loss: 0.86428652, learning_rate: 2.5333333333333334e-06, global_step: 1470, interval_runtime: 8.0042, interval_samples_per_second: 1.999, interval_steps_per_second: 1.249, epoch: 7.7778[0m
[32m[2022-09-15 15:09:16,375] [    INFO][0m - loss: 0.77517343, learning_rate: 2.53015873015873e-06, global_step: 1480, interval_runtime: 8.0327, interval_samples_per_second: 1.992, interval_steps_per_second: 1.245, epoch: 7.8307[0m
[32m[2022-09-15 15:09:24,414] [    INFO][0m - loss: 0.78642788, learning_rate: 2.5269841269841273e-06, global_step: 1490, interval_runtime: 8.0393, interval_samples_per_second: 1.99, interval_steps_per_second: 1.244, epoch: 7.8836[0m
[32m[2022-09-15 15:09:32,393] [    INFO][0m - loss: 1.02264109, learning_rate: 2.523809523809524e-06, global_step: 1500, interval_runtime: 7.9787, interval_samples_per_second: 2.005, interval_steps_per_second: 1.253, epoch: 7.9365[0m
[32m[2022-09-15 15:09:40,277] [    INFO][0m - loss: 0.90784254, learning_rate: 2.5206349206349207e-06, global_step: 1510, interval_runtime: 7.8834, interval_samples_per_second: 2.03, interval_steps_per_second: 1.268, epoch: 7.9894[0m
[32m[2022-09-15 15:09:41,793] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 15:09:41,794] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-15 15:09:41,794] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 15:09:41,794] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 15:09:41,794] [    INFO][0m -   Total prediction steps = 86[0m
[32m[2022-09-15 15:10:08,904] [    INFO][0m - eval_loss: 2.0897536277770996, eval_accuracy: 0.45957756737072103, eval_runtime: 27.1102, eval_samples_per_second: 50.645, eval_steps_per_second: 3.172, epoch: 8.0[0m
[32m[2022-09-15 15:10:08,929] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1512[0m
[32m[2022-09-15 15:10:08,929] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 15:10:12,577] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1512/tokenizer_config.json[0m
[32m[2022-09-15 15:10:12,578] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1512/special_tokens_map.json[0m
[32m[2022-09-15 15:10:19,477] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-15 15:10:19,477] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-756 (score: 0.4697742170429716).[0m
[32m[2022-09-15 15:10:21,193] [    INFO][0m - train_runtime: 1507.6949, train_samples_per_second: 100.286, train_steps_per_second: 6.268, train_loss: 1.525966874742634, epoch: 8.0[0m
[32m[2022-09-15 15:10:21,247] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-09-15 15:10:21,248] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 15:10:28,103] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-09-15 15:10:28,104] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-09-15 15:10:28,109] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-15 15:10:28,109] [    INFO][0m -   epoch                    =        8.0[0m
[32m[2022-09-15 15:10:28,109] [    INFO][0m -   train_loss               =      1.526[0m
[32m[2022-09-15 15:10:28,109] [    INFO][0m -   train_runtime            = 0:25:07.69[0m
[32m[2022-09-15 15:10:28,109] [    INFO][0m -   train_samples_per_second =    100.286[0m
[32m[2022-09-15 15:10:28,109] [    INFO][0m -   train_steps_per_second   =      6.268[0m
[32m[2022-09-15 15:10:28,116] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-15 15:10:28,116] [    INFO][0m -   Num examples = 1749[0m
[32m[2022-09-15 15:10:28,116] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 15:10:28,116] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 15:10:28,116] [    INFO][0m -   Total prediction steps = 110[0m
[32m[2022-09-15 15:11:02,641] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-15 15:11:02,642] [    INFO][0m -   test_accuracy           =     0.4706[0m
[32m[2022-09-15 15:11:02,642] [    INFO][0m -   test_loss               =     1.8388[0m
[32m[2022-09-15 15:11:02,642] [    INFO][0m -   test_runtime            = 0:00:34.52[0m
[32m[2022-09-15 15:11:02,642] [    INFO][0m -   test_samples_per_second =     50.659[0m
[32m[2022-09-15 15:11:02,642] [    INFO][0m -   test_steps_per_second   =      3.186[0m
[32m[2022-09-15 15:11:02,642] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-15 15:11:02,643] [    INFO][0m -   Num examples = 2600[0m
[32m[2022-09-15 15:11:02,643] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 15:11:02,643] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 15:11:02,643] [    INFO][0m -   Total prediction steps = 163[0m
{
  "labels": 77,
  "text_a": "\u884c\u8baf\u901a\u662f\u5e7f\u5dde\u4ea4\u901a\u4fe1\u606f\u5316\u5efa\u8bbe\u6295\u8d44\u8425\u8fd0\u6709\u9650\u516c\u53f8\u5728\u5e7f\u5dde\u5e02\u4ea4\u901a\u8fd0\u8f93\u5c40\u7684\u6307\u5bfc\u4e0b\uff0c\u63a8\u51fa\u7684\u4e00\u6b3e\u63d0\u4f9b\u4ea4\u901a\u4fe1\u606f\u670d\u52a1\u7684\u624b\u673a\u7ec8\u7aef\u8f6f\u4ef6\uff0c\u652f\u6301Android\u548ciPhone\u7cfb\u7edf\u3002\u4e3b\u8981\u5305\u62ec\u8def\u51b5\u4fe1\u606f\u3001\u5b9e\u65f6\u516c\u4ea4\u3001\u505c\u8f66\u670d\u52a1\u3001\u7684\u58eb\u67e5\u8be2\u3001\u51fa\u884c\u89c4\u5212\u3001\u5730\u94c1\u4fe1\u606f\u3001\u822a\u7a7a\u4fe1\u606f\u3001\u94c1\u8def\u4fe1\u606f\u3001\u5ba2\u8fd0\u4fe1\u606f\u3001\u9a7e\u57f9\u4fe1\u606f\u3001\u4ea4\u901a\u8d44\u8baf\u3001WIFI\u70ed\u70b9\u7b49\u529f\u80fd\u6a21\u5757\u3002\u66f4\u65b0\u5185\u5bb91\u3001\u4f18\u5316\u5df2\u77e5bug\u3002",
  "text_b": "",
  "uid": 0
}

Traceback (most recent call last):
  File "train_single.py", line 216, in <module>
    main()
  File "train_single.py", line 210, in main
    test_ret = postprocess(test_ret, test_ds, data_args.task_name,
  File "/ssd2/wanghuijuan03/prompt/PaddleNLP/examples/benchmark/fewclue/a7/e1cw/postprocess.py", line 61, in postprocess
    preds = np.argmax(preds_list, axis=1)
TypeError: unhashable type: 'numpy.ndarray'
 
==========
ocnli
==========
 
[32m[2022-09-15 15:11:59,599] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-15 15:11:59,600] [    INFO][0m - ============================================================[0m
[32m[2022-09-15 15:11:59,600] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-15 15:11:59,600] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-15 15:11:59,600] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-15 15:11:59,600] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-15 15:11:59,600] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-15 15:11:59,600] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-15 15:11:59,600] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-15 15:11:59,600] [    INFO][0m - [0m
[32m[2022-09-15 15:11:59,600] [    INFO][0m - ============================================================[0m
[32m[2022-09-15 15:11:59,600] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-15 15:11:59,600] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-15 15:11:59,601] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-15 15:11:59,601] [    INFO][0m - pretrained                    :./checkpoints_cmnli/checkpoint-6000/model_state.pdparams[0m
[32m[2022-09-15 15:11:59,601] [    INFO][0m - prompt                        :‚Äú{'text':'text_a'}‚Äù{'mask'}{'mask'}Ôºå‚Äú{'text':'text_b'}‚Äù[0m
[32m[2022-09-15 15:11:59,601] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-15 15:11:59,601] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-15 15:11:59,601] [    INFO][0m - task_name                     :ocnli[0m
[32m[2022-09-15 15:11:59,601] [    INFO][0m - [0m
[32m[2022-09-15 15:11:59,601] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0915 15:11:59.603375 57595 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0915 15:11:59.607625 57595 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-15 15:12:07,951] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-15 15:12:07,963] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-15 15:12:07,963] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-15 15:12:07,964] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚Äù'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'Ôºå‚Äú'}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': '‚Äù'}][0m
[32m[2022-09-15 15:12:10,716] [    INFO][0m - ============================================================[0m
[32m[2022-09-15 15:12:10,717] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-15 15:12:10,717] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-15 15:12:10,717] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-15 15:12:10,717] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-15 15:12:10,717] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-15 15:12:10,717] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-15 15:12:10,717] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-15 15:12:10,717] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-15 15:12:10,717] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-15 15:12:10,718] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-15 15:12:10,718] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-15 15:12:10,718] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-15 15:12:10,718] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-15 15:12:10,718] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-15 15:12:10,718] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-15 15:12:10,718] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-15 15:12:10,718] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-15 15:12:10,718] [    INFO][0m - eval_batch_size               :16[0m
[32m[2022-09-15 15:12:10,718] [    INFO][0m - eval_steps                    :200[0m
[32m[2022-09-15 15:12:10,718] [    INFO][0m - evaluation_strategy           :IntervalStrategy.EPOCH[0m
[32m[2022-09-15 15:12:10,718] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-15 15:12:10,718] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-15 15:12:10,719] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-15 15:12:10,719] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-15 15:12:10,719] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-15 15:12:10,719] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-15 15:12:10,719] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-15 15:12:10,719] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-15 15:12:10,719] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-15 15:12:10,719] [    INFO][0m - learning_rate                 :3e-06[0m
[32m[2022-09-15 15:12:10,719] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-15 15:12:10,719] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-15 15:12:10,719] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-15 15:12:10,719] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-15 15:12:10,719] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-15 15:12:10,720] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-15 15:12:10,720] [    INFO][0m - logging_dir                   :./checkpoints/runs/Sep15_15-11-59_instance-3bwob41y-01[0m
[32m[2022-09-15 15:12:10,720] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-15 15:12:10,720] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-15 15:12:10,720] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-15 15:12:10,720] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-15 15:12:10,720] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-15 15:12:10,720] [    INFO][0m - max_seq_length                :128[0m
[32m[2022-09-15 15:12:10,720] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-15 15:12:10,720] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-15 15:12:10,720] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-15 15:12:10,720] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-15 15:12:10,720] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-09-15 15:12:10,720] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-15 15:12:10,721] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-15 15:12:10,721] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-09-15 15:12:10,721] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-15 15:12:10,721] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-15 15:12:10,721] [    INFO][0m - per_device_eval_batch_size    :16[0m
[32m[2022-09-15 15:12:10,721] [    INFO][0m - per_device_train_batch_size   :16[0m
[32m[2022-09-15 15:12:10,721] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-15 15:12:10,721] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-15 15:12:10,721] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-15 15:12:10,721] [    INFO][0m - ppt_learning_rate             :3e-05[0m
[32m[2022-09-15 15:12:10,721] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-15 15:12:10,721] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-15 15:12:10,721] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-15 15:12:10,722] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-15 15:12:10,722] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-15 15:12:10,722] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-15 15:12:10,722] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-09-15 15:12:10,722] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-15 15:12:10,722] [    INFO][0m - save_steps                    :200[0m
[32m[2022-09-15 15:12:10,722] [    INFO][0m - save_strategy                 :IntervalStrategy.EPOCH[0m
[32m[2022-09-15 15:12:10,722] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-15 15:12:10,722] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-15 15:12:10,722] [    INFO][0m - seed                          :42[0m
[32m[2022-09-15 15:12:10,722] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-15 15:12:10,722] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-15 15:12:10,722] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-15 15:12:10,723] [    INFO][0m - train_batch_size              :16[0m
[32m[2022-09-15 15:12:10,723] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-15 15:12:10,723] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-15 15:12:10,723] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-15 15:12:10,723] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-15 15:12:10,723] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-15 15:12:10,723] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-15 15:12:10,723] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-15 15:12:10,723] [    INFO][0m - [0m
[32m[2022-09-15 15:12:10,726] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-15 15:12:10,726] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-15 15:12:10,726] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-09-15 15:12:10,727] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2022-09-15 15:12:10,727] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2022-09-15 15:12:10,727] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-15 15:12:10,727] [    INFO][0m -   Total optimization steps = 500.0[0m
[32m[2022-09-15 15:12:10,727] [    INFO][0m -   Total num train samples = 8000[0m
[32m[2022-09-15 15:12:15,457] [    INFO][0m - loss: 3.04745502, learning_rate: 2.9400000000000002e-06, global_step: 10, interval_runtime: 4.7289, interval_samples_per_second: 3.383, interval_steps_per_second: 2.115, epoch: 1.0[0m
[32m[2022-09-15 15:12:15,458] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 15:12:15,458] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-15 15:12:15,458] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 15:12:15,458] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 15:12:15,459] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-15 15:12:16,800] [    INFO][0m - eval_loss: 1.7847416400909424, eval_accuracy: 0.64375, eval_runtime: 1.3406, eval_samples_per_second: 119.346, eval_steps_per_second: 7.459, epoch: 1.0[0m
[32m[2022-09-15 15:12:16,800] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-10[0m
[32m[2022-09-15 15:12:16,800] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 15:12:23,951] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-10/tokenizer_config.json[0m
[32m[2022-09-15 15:12:23,952] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-10/special_tokens_map.json[0m
[32m[2022-09-15 15:12:48,189] [    INFO][0m - loss: 1.56154613, learning_rate: 2.88e-06, global_step: 20, interval_runtime: 32.7317, interval_samples_per_second: 0.489, interval_steps_per_second: 0.306, epoch: 2.0[0m
[32m[2022-09-15 15:12:48,189] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 15:12:48,190] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-15 15:12:48,190] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 15:12:48,190] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 15:12:48,190] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-15 15:12:49,525] [    INFO][0m - eval_loss: 1.1528797149658203, eval_accuracy: 0.675, eval_runtime: 1.3342, eval_samples_per_second: 119.92, eval_steps_per_second: 7.495, epoch: 2.0[0m
[32m[2022-09-15 15:12:49,525] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-20[0m
[32m[2022-09-15 15:12:49,525] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 15:12:56,553] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-20/tokenizer_config.json[0m
[32m[2022-09-15 15:12:56,553] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-20/special_tokens_map.json[0m
[32m[2022-09-15 15:13:14,050] [    INFO][0m - loss: 0.99582195, learning_rate: 2.82e-06, global_step: 30, interval_runtime: 25.8618, interval_samples_per_second: 0.619, interval_steps_per_second: 0.387, epoch: 3.0[0m
[32m[2022-09-15 15:13:14,051] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 15:13:14,051] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-15 15:13:14,051] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 15:13:14,051] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 15:13:14,051] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-15 15:13:15,364] [    INFO][0m - eval_loss: 0.9250739812850952, eval_accuracy: 0.68125, eval_runtime: 1.3122, eval_samples_per_second: 121.932, eval_steps_per_second: 7.621, epoch: 3.0[0m
[32m[2022-09-15 15:13:15,364] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-30[0m
[32m[2022-09-15 15:13:15,364] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 15:13:22,606] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-30/tokenizer_config.json[0m
[32m[2022-09-15 15:13:22,607] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-30/special_tokens_map.json[0m
[32m[2022-09-15 15:13:38,368] [    INFO][0m - loss: 0.75800099, learning_rate: 2.7600000000000003e-06, global_step: 40, interval_runtime: 24.3171, interval_samples_per_second: 0.658, interval_steps_per_second: 0.411, epoch: 4.0[0m
[32m[2022-09-15 15:13:38,368] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 15:13:38,369] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-15 15:13:38,369] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 15:13:38,369] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 15:13:38,369] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-15 15:13:39,715] [    INFO][0m - eval_loss: 0.8584966659545898, eval_accuracy: 0.675, eval_runtime: 1.3455, eval_samples_per_second: 118.911, eval_steps_per_second: 7.432, epoch: 4.0[0m
[32m[2022-09-15 15:13:39,715] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-40[0m
[32m[2022-09-15 15:13:39,716] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 15:13:46,564] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-40/tokenizer_config.json[0m
[32m[2022-09-15 15:13:46,565] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-40/special_tokens_map.json[0m
[32m[2022-09-15 15:14:03,961] [    INFO][0m - loss: 0.58267293, learning_rate: 2.7e-06, global_step: 50, interval_runtime: 25.5934, interval_samples_per_second: 0.625, interval_steps_per_second: 0.391, epoch: 5.0[0m
[32m[2022-09-15 15:14:03,961] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 15:14:03,962] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-15 15:14:03,962] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 15:14:03,962] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 15:14:03,962] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-15 15:14:05,274] [    INFO][0m - eval_loss: 0.8483186960220337, eval_accuracy: 0.68125, eval_runtime: 1.3119, eval_samples_per_second: 121.962, eval_steps_per_second: 7.623, epoch: 5.0[0m
[32m[2022-09-15 15:14:05,275] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-50[0m
[32m[2022-09-15 15:14:05,275] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 15:14:12,030] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-50/tokenizer_config.json[0m
[32m[2022-09-15 15:14:12,030] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-50/special_tokens_map.json[0m
[32m[2022-09-15 15:14:29,061] [    INFO][0m - loss: 0.42856193, learning_rate: 2.64e-06, global_step: 60, interval_runtime: 25.0999, interval_samples_per_second: 0.637, interval_steps_per_second: 0.398, epoch: 6.0[0m
[32m[2022-09-15 15:14:29,062] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 15:14:29,062] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-15 15:14:29,062] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 15:14:29,062] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 15:14:29,062] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-15 15:14:30,403] [    INFO][0m - eval_loss: 0.8115615844726562, eval_accuracy: 0.7, eval_runtime: 1.3405, eval_samples_per_second: 119.359, eval_steps_per_second: 7.46, epoch: 6.0[0m
[32m[2022-09-15 15:14:30,404] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-60[0m
[32m[2022-09-15 15:14:30,404] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 15:14:37,855] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-60/tokenizer_config.json[0m
[32m[2022-09-15 15:14:37,856] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-60/special_tokens_map.json[0m
[32m[2022-09-15 15:14:55,808] [    INFO][0m - loss: 0.32229469, learning_rate: 2.58e-06, global_step: 70, interval_runtime: 26.7474, interval_samples_per_second: 0.598, interval_steps_per_second: 0.374, epoch: 7.0[0m
[32m[2022-09-15 15:14:55,809] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 15:14:55,809] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-15 15:14:55,809] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 15:14:55,809] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 15:14:55,809] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-15 15:14:57,125] [    INFO][0m - eval_loss: 0.8696405291557312, eval_accuracy: 0.7125, eval_runtime: 1.3151, eval_samples_per_second: 121.666, eval_steps_per_second: 7.604, epoch: 7.0[0m
[32m[2022-09-15 15:14:57,125] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-70[0m
[32m[2022-09-15 15:14:57,125] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 15:15:00,322] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-70/tokenizer_config.json[0m
[32m[2022-09-15 15:15:00,322] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-70/special_tokens_map.json[0m
[32m[2022-09-15 15:15:10,779] [    INFO][0m - loss: 0.25541286, learning_rate: 2.52e-06, global_step: 80, interval_runtime: 14.9712, interval_samples_per_second: 1.069, interval_steps_per_second: 0.668, epoch: 8.0[0m
[32m[2022-09-15 15:15:10,780] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 15:15:10,781] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-15 15:15:10,781] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 15:15:10,781] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 15:15:10,781] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-15 15:15:12,139] [    INFO][0m - eval_loss: 0.8863223791122437, eval_accuracy: 0.73125, eval_runtime: 1.3569, eval_samples_per_second: 117.918, eval_steps_per_second: 7.37, epoch: 8.0[0m
[32m[2022-09-15 15:15:12,140] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-80[0m
[32m[2022-09-15 15:15:12,140] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 15:15:15,823] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-80/tokenizer_config.json[0m
[32m[2022-09-15 15:15:15,824] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-80/special_tokens_map.json[0m
[32m[2022-09-15 15:15:26,427] [    INFO][0m - loss: 0.18263712, learning_rate: 2.4599999999999997e-06, global_step: 90, interval_runtime: 15.648, interval_samples_per_second: 1.022, interval_steps_per_second: 0.639, epoch: 9.0[0m
[32m[2022-09-15 15:15:26,428] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 15:15:26,428] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-15 15:15:26,429] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 15:15:26,429] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 15:15:26,429] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-15 15:15:27,798] [    INFO][0m - eval_loss: 0.9472225308418274, eval_accuracy: 0.73125, eval_runtime: 1.3689, eval_samples_per_second: 116.886, eval_steps_per_second: 7.305, epoch: 9.0[0m
[32m[2022-09-15 15:15:27,799] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-90[0m
[32m[2022-09-15 15:15:27,799] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 15:15:31,495] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-90/tokenizer_config.json[0m
[32m[2022-09-15 15:15:31,496] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-90/special_tokens_map.json[0m
[32m[2022-09-15 15:15:44,047] [    INFO][0m - loss: 0.10217457, learning_rate: 2.4000000000000003e-06, global_step: 100, interval_runtime: 15.4801, interval_samples_per_second: 1.034, interval_steps_per_second: 0.646, epoch: 10.0[0m
[32m[2022-09-15 15:15:44,048] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 15:15:44,049] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-15 15:15:44,049] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 15:15:44,049] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 15:15:44,049] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-15 15:15:45,361] [    INFO][0m - eval_loss: 1.0851736068725586, eval_accuracy: 0.675, eval_runtime: 1.3118, eval_samples_per_second: 121.97, eval_steps_per_second: 7.623, epoch: 10.0[0m
[32m[2022-09-15 15:15:45,361] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-09-15 15:15:45,362] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 15:15:47,984] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-15 15:15:47,984] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-15 15:15:57,934] [    INFO][0m - loss: 0.04791221, learning_rate: 2.34e-06, global_step: 110, interval_runtime: 16.0263, interval_samples_per_second: 0.998, interval_steps_per_second: 0.624, epoch: 11.0[0m
[32m[2022-09-15 15:15:57,935] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 15:15:57,935] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-15 15:15:57,936] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 15:15:57,936] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 15:15:57,936] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-15 15:15:59,279] [    INFO][0m - eval_loss: 1.1905144453048706, eval_accuracy: 0.7125, eval_runtime: 1.3426, eval_samples_per_second: 119.171, eval_steps_per_second: 7.448, epoch: 11.0[0m
[32m[2022-09-15 15:15:59,279] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-110[0m
[32m[2022-09-15 15:15:59,279] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 15:16:02,086] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-110/tokenizer_config.json[0m
[32m[2022-09-15 15:16:02,086] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-110/special_tokens_map.json[0m
[32m[2022-09-15 15:16:13,954] [    INFO][0m - loss: 0.02959188, learning_rate: 2.28e-06, global_step: 120, interval_runtime: 16.0199, interval_samples_per_second: 0.999, interval_steps_per_second: 0.624, epoch: 12.0[0m
[32m[2022-09-15 15:16:13,955] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 15:16:13,955] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-15 15:16:13,955] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 15:16:13,955] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 15:16:13,955] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-15 15:16:15,285] [    INFO][0m - eval_loss: 1.4064184427261353, eval_accuracy: 0.71875, eval_runtime: 1.3299, eval_samples_per_second: 120.307, eval_steps_per_second: 7.519, epoch: 12.0[0m
[32m[2022-09-15 15:16:15,286] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-120[0m
[32m[2022-09-15 15:16:15,286] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 15:16:18,544] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-120/tokenizer_config.json[0m
[32m[2022-09-15 15:16:18,545] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-120/special_tokens_map.json[0m
[32m[2022-09-15 15:16:25,128] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-15 15:16:25,128] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-80 (score: 0.73125).[0m
[32m[2022-09-15 15:16:26,885] [    INFO][0m - train_runtime: 256.1579, train_samples_per_second: 31.231, train_steps_per_second: 1.952, train_loss: 0.6928401909768581, epoch: 12.0[0m
[32m[2022-09-15 15:16:26,887] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-09-15 15:16:26,887] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 15:16:33,407] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-09-15 15:16:33,408] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-09-15 15:16:33,608] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-15 15:16:33,609] [    INFO][0m -   epoch                    =       12.0[0m
[32m[2022-09-15 15:16:33,609] [    INFO][0m -   train_loss               =     0.6928[0m
[32m[2022-09-15 15:16:33,609] [    INFO][0m -   train_runtime            = 0:04:16.15[0m
[32m[2022-09-15 15:16:33,609] [    INFO][0m -   train_samples_per_second =     31.231[0m
[32m[2022-09-15 15:16:33,609] [    INFO][0m -   train_steps_per_second   =      1.952[0m
[32m[2022-09-15 15:16:33,611] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-15 15:16:33,611] [    INFO][0m -   Num examples = 2520[0m
[32m[2022-09-15 15:16:33,612] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 15:16:33,612] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 15:16:33,612] [    INFO][0m -   Total prediction steps = 158[0m
[32m[2022-09-15 15:16:55,407] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-15 15:16:55,407] [    INFO][0m -   test_accuracy           =     0.6956[0m
[32m[2022-09-15 15:16:55,407] [    INFO][0m -   test_loss               =      0.899[0m
[32m[2022-09-15 15:16:55,408] [    INFO][0m -   test_runtime            = 0:00:21.79[0m
[32m[2022-09-15 15:16:55,408] [    INFO][0m -   test_samples_per_second =    115.622[0m
[32m[2022-09-15 15:16:55,408] [    INFO][0m -   test_steps_per_second   =      7.249[0m
[32m[2022-09-15 15:16:55,408] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-15 15:16:55,408] [    INFO][0m -   Num examples = 3000[0m
[32m[2022-09-15 15:16:55,408] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 15:16:55,408] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 15:16:55,408] [    INFO][0m -   Total prediction steps = 188[0m
{
  "labels": 2,
  "text_a": "\u4e03\u4e94\u671f\u95f4\u5f00\u59cb,\u56fd\u5bb6\u53c8\u6295\u8d44\u5c06\u6b66\u6c49\u5e02\u533a\u7684\u90e8\u5206\u571f\u5824\u6539\u5efa\u4e3a\u94a2\u7b4b\u6ce5\u51dd\u571f\u9632\u6c34\u5899",
  "text_b": "\u516b\u4e94\u671f\u95f4\u4f1a\u628a\u5269\u4e0b\u7684\u571f\u5824\u90fd\u6539\u5efa\u5b8c",
  "uid": 0
}

Traceback (most recent call last):
  File "train_single.py", line 216, in <module>
    main()
  File "train_single.py", line 210, in main
    test_ret = postprocess(test_ret, test_ds, data_args.task_name,
  File "/ssd2/wanghuijuan03/prompt/PaddleNLP/examples/benchmark/fewclue/a7/e1cw/postprocess.py", line 50, in postprocess
    preds = paddle.nn.functional.softmax(
NameError: name 'paddle' is not defined
 
==========
bustm
==========
 
[32m[2022-09-15 15:17:24,907] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-15 15:17:24,907] [    INFO][0m - ============================================================[0m
[32m[2022-09-15 15:17:24,907] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-15 15:17:24,907] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-15 15:17:24,907] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-15 15:17:24,907] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-15 15:17:24,908] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-15 15:17:24,908] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-15 15:17:24,908] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-15 15:17:24,908] [    INFO][0m - [0m
[32m[2022-09-15 15:17:24,908] [    INFO][0m - ============================================================[0m
[32m[2022-09-15 15:17:24,908] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-15 15:17:24,908] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-15 15:17:24,908] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-15 15:17:24,908] [    INFO][0m - pretrained                    :./checkpoints_cmnli/checkpoint-6000/model_state.pdparams[0m
[32m[2022-09-15 15:17:24,908] [    INFO][0m - prompt                        :‚Äú{'text':'text_a'}‚ÄùÂíå‚Äú{'text':'text_b'}‚ÄùÊèèËø∞ÁöÑÊòØ{'mask'}{'mask'}ÁöÑ‰∫ãÊÉÖ„ÄÇ[0m
[32m[2022-09-15 15:17:24,908] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-15 15:17:24,908] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-15 15:17:24,908] [    INFO][0m - task_name                     :bustm[0m
[32m[2022-09-15 15:17:24,909] [    INFO][0m - [0m
[32m[2022-09-15 15:17:24,909] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0915 15:17:24.910399 67115 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0915 15:17:24.915364 67115 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-15 15:17:33,638] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-15 15:17:33,649] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-15 15:17:33,650] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-15 15:17:33,651] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùÂíå‚Äú'}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': '‚ÄùÊèèËø∞ÁöÑÊòØ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'ÁöÑ‰∫ãÊÉÖ„ÄÇ'}][0m
[32m[2022-09-15 15:17:35,819] [    INFO][0m - ============================================================[0m
[32m[2022-09-15 15:17:35,819] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-15 15:17:35,820] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-15 15:17:35,820] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-15 15:17:35,820] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-15 15:17:35,820] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-15 15:17:35,820] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-15 15:17:35,820] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-15 15:17:35,820] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-15 15:17:35,820] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-15 15:17:35,820] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-15 15:17:35,820] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-15 15:17:35,821] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-15 15:17:35,821] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-15 15:17:35,821] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-15 15:17:35,821] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-15 15:17:35,821] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-15 15:17:35,821] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-15 15:17:35,821] [    INFO][0m - eval_batch_size               :16[0m
[32m[2022-09-15 15:17:35,821] [    INFO][0m - eval_steps                    :200[0m
[32m[2022-09-15 15:17:35,821] [    INFO][0m - evaluation_strategy           :IntervalStrategy.EPOCH[0m
[32m[2022-09-15 15:17:35,821] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-15 15:17:35,821] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-15 15:17:35,821] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-15 15:17:35,821] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-15 15:17:35,822] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-15 15:17:35,822] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-15 15:17:35,822] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-15 15:17:35,822] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-15 15:17:35,822] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-15 15:17:35,822] [    INFO][0m - learning_rate                 :3e-06[0m
[32m[2022-09-15 15:17:35,822] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-15 15:17:35,822] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-15 15:17:35,822] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-15 15:17:35,822] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-15 15:17:35,822] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-15 15:17:35,822] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-15 15:17:35,822] [    INFO][0m - logging_dir                   :./checkpoints/runs/Sep15_15-17-24_instance-3bwob41y-01[0m
[32m[2022-09-15 15:17:35,823] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-15 15:17:35,823] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-15 15:17:35,823] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-15 15:17:35,823] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-15 15:17:35,823] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-15 15:17:35,823] [    INFO][0m - max_seq_length                :64[0m
[32m[2022-09-15 15:17:35,823] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-15 15:17:35,823] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-15 15:17:35,823] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-15 15:17:35,823] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-15 15:17:35,823] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-09-15 15:17:35,823] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-15 15:17:35,823] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-15 15:17:35,824] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-09-15 15:17:35,824] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-15 15:17:35,824] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-15 15:17:35,824] [    INFO][0m - per_device_eval_batch_size    :16[0m
[32m[2022-09-15 15:17:35,824] [    INFO][0m - per_device_train_batch_size   :16[0m
[32m[2022-09-15 15:17:35,824] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-15 15:17:35,824] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-15 15:17:35,824] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-15 15:17:35,824] [    INFO][0m - ppt_learning_rate             :3e-05[0m
[32m[2022-09-15 15:17:35,824] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-15 15:17:35,824] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-15 15:17:35,824] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-15 15:17:35,824] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-15 15:17:35,825] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-15 15:17:35,825] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-15 15:17:35,825] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-09-15 15:17:35,825] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-15 15:17:35,825] [    INFO][0m - save_steps                    :200[0m
[32m[2022-09-15 15:17:35,825] [    INFO][0m - save_strategy                 :IntervalStrategy.EPOCH[0m
[32m[2022-09-15 15:17:35,825] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-15 15:17:35,825] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-15 15:17:35,825] [    INFO][0m - seed                          :42[0m
[32m[2022-09-15 15:17:35,825] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-15 15:17:35,825] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-15 15:17:35,825] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-15 15:17:35,825] [    INFO][0m - train_batch_size              :16[0m
[32m[2022-09-15 15:17:35,826] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-15 15:17:35,826] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-15 15:17:35,826] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-15 15:17:35,826] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-15 15:17:35,826] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-15 15:17:35,826] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-15 15:17:35,826] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-15 15:17:35,826] [    INFO][0m - [0m
[32m[2022-09-15 15:17:35,829] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-15 15:17:35,829] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-15 15:17:35,829] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-09-15 15:17:35,829] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2022-09-15 15:17:35,829] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2022-09-15 15:17:35,829] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-15 15:17:35,830] [    INFO][0m -   Total optimization steps = 500.0[0m
[32m[2022-09-15 15:17:35,830] [    INFO][0m -   Total num train samples = 8000[0m
[32m[2022-09-15 15:17:39,007] [    INFO][0m - loss: 2.13697243, learning_rate: 2.9400000000000002e-06, global_step: 10, interval_runtime: 3.1753, interval_samples_per_second: 5.039, interval_steps_per_second: 3.149, epoch: 1.0[0m
[32m[2022-09-15 15:17:39,009] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 15:17:39,009] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-15 15:17:39,009] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 15:17:39,009] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 15:17:39,009] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-15 15:17:39,773] [    INFO][0m - eval_loss: 0.8403539657592773, eval_accuracy: 0.5125, eval_runtime: 0.7635, eval_samples_per_second: 209.556, eval_steps_per_second: 13.097, epoch: 1.0[0m
[32m[2022-09-15 15:17:39,774] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-10[0m
[32m[2022-09-15 15:17:39,774] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 15:17:46,518] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-10/tokenizer_config.json[0m
[32m[2022-09-15 15:17:46,519] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-10/special_tokens_map.json[0m
[32m[2022-09-15 15:18:01,490] [    INFO][0m - loss: 0.6163167, learning_rate: 2.88e-06, global_step: 20, interval_runtime: 22.4838, interval_samples_per_second: 0.712, interval_steps_per_second: 0.445, epoch: 2.0[0m
[32m[2022-09-15 15:18:01,490] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 15:18:01,491] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-15 15:18:01,491] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 15:18:01,491] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 15:18:01,491] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-15 15:18:02,295] [    INFO][0m - eval_loss: 0.39151257276535034, eval_accuracy: 0.69375, eval_runtime: 0.8035, eval_samples_per_second: 199.137, eval_steps_per_second: 12.446, epoch: 2.0[0m
[32m[2022-09-15 15:18:02,295] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-20[0m
[32m[2022-09-15 15:18:02,295] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 15:18:08,543] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-20/tokenizer_config.json[0m
[32m[2022-09-15 15:18:08,543] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-20/special_tokens_map.json[0m
[32m[2022-09-15 15:18:23,610] [    INFO][0m - loss: 0.35249701, learning_rate: 2.82e-06, global_step: 30, interval_runtime: 22.1206, interval_samples_per_second: 0.723, interval_steps_per_second: 0.452, epoch: 3.0[0m
[32m[2022-09-15 15:18:23,611] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 15:18:23,611] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-15 15:18:23,611] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 15:18:23,611] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 15:18:23,611] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-15 15:18:24,356] [    INFO][0m - eval_loss: 0.2822049558162689, eval_accuracy: 0.7375, eval_runtime: 0.745, eval_samples_per_second: 214.779, eval_steps_per_second: 13.424, epoch: 3.0[0m
[32m[2022-09-15 15:18:24,357] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-30[0m
[32m[2022-09-15 15:18:24,357] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 15:18:30,943] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-30/tokenizer_config.json[0m
[32m[2022-09-15 15:18:30,943] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-30/special_tokens_map.json[0m
[32m[2022-09-15 15:18:45,082] [    INFO][0m - loss: 0.277386, learning_rate: 2.7600000000000003e-06, global_step: 40, interval_runtime: 21.4721, interval_samples_per_second: 0.745, interval_steps_per_second: 0.466, epoch: 4.0[0m
[32m[2022-09-15 15:18:45,083] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 15:18:45,083] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-15 15:18:45,083] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 15:18:45,083] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 15:18:45,083] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-15 15:18:45,851] [    INFO][0m - eval_loss: 0.2711659371852875, eval_accuracy: 0.7375, eval_runtime: 0.7678, eval_samples_per_second: 208.388, eval_steps_per_second: 13.024, epoch: 4.0[0m
[32m[2022-09-15 15:18:45,852] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-40[0m
[32m[2022-09-15 15:18:45,852] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 15:18:52,395] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-40/tokenizer_config.json[0m
[32m[2022-09-15 15:18:52,396] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-40/special_tokens_map.json[0m
[32m[2022-09-15 15:19:06,513] [    INFO][0m - loss: 0.23136163, learning_rate: 2.7e-06, global_step: 50, interval_runtime: 21.4304, interval_samples_per_second: 0.747, interval_steps_per_second: 0.467, epoch: 5.0[0m
[32m[2022-09-15 15:19:06,513] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 15:19:06,513] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-15 15:19:06,513] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 15:19:06,513] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 15:19:06,514] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-15 15:19:07,261] [    INFO][0m - eval_loss: 0.2577205300331116, eval_accuracy: 0.80625, eval_runtime: 0.7474, eval_samples_per_second: 214.08, eval_steps_per_second: 13.38, epoch: 5.0[0m
[32m[2022-09-15 15:19:07,261] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-50[0m
[32m[2022-09-15 15:19:07,262] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 15:19:13,542] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-50/tokenizer_config.json[0m
[32m[2022-09-15 15:19:13,542] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-50/special_tokens_map.json[0m
[32m[2022-09-15 15:19:27,789] [    INFO][0m - loss: 0.20003514, learning_rate: 2.64e-06, global_step: 60, interval_runtime: 21.2768, interval_samples_per_second: 0.752, interval_steps_per_second: 0.47, epoch: 6.0[0m
[32m[2022-09-15 15:19:27,790] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 15:19:27,790] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-15 15:19:27,790] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 15:19:27,790] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 15:19:27,790] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-15 15:19:28,530] [    INFO][0m - eval_loss: 0.25281792879104614, eval_accuracy: 0.81875, eval_runtime: 0.7398, eval_samples_per_second: 216.267, eval_steps_per_second: 13.517, epoch: 6.0[0m
[32m[2022-09-15 15:19:28,531] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-60[0m
[32m[2022-09-15 15:19:28,531] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 15:19:35,040] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-60/tokenizer_config.json[0m
[32m[2022-09-15 15:19:35,041] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-60/special_tokens_map.json[0m
[32m[2022-09-15 15:19:50,046] [    INFO][0m - loss: 0.15697988, learning_rate: 2.58e-06, global_step: 70, interval_runtime: 22.2562, interval_samples_per_second: 0.719, interval_steps_per_second: 0.449, epoch: 7.0[0m
[32m[2022-09-15 15:19:50,046] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 15:19:50,047] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-15 15:19:50,047] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 15:19:50,047] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 15:19:50,047] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-15 15:19:50,837] [    INFO][0m - eval_loss: 0.26998797059059143, eval_accuracy: 0.81875, eval_runtime: 0.7898, eval_samples_per_second: 202.583, eval_steps_per_second: 12.661, epoch: 7.0[0m
[32m[2022-09-15 15:19:50,837] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-70[0m
[32m[2022-09-15 15:19:50,837] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 15:19:57,564] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-70/tokenizer_config.json[0m
[32m[2022-09-15 15:19:57,564] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-70/special_tokens_map.json[0m
[32m[2022-09-15 15:20:11,920] [    INFO][0m - loss: 0.13717892, learning_rate: 2.52e-06, global_step: 80, interval_runtime: 21.8743, interval_samples_per_second: 0.731, interval_steps_per_second: 0.457, epoch: 8.0[0m
[32m[2022-09-15 15:20:11,921] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 15:20:11,921] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-15 15:20:11,921] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 15:20:11,921] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 15:20:11,921] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-15 15:20:12,665] [    INFO][0m - eval_loss: 0.2883946895599365, eval_accuracy: 0.8125, eval_runtime: 0.7437, eval_samples_per_second: 215.135, eval_steps_per_second: 13.446, epoch: 8.0[0m
[32m[2022-09-15 15:20:12,665] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-80[0m
[32m[2022-09-15 15:20:12,666] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 15:20:18,828] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-80/tokenizer_config.json[0m
[32m[2022-09-15 15:20:18,828] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-80/special_tokens_map.json[0m
[32m[2022-09-15 15:20:40,117] [    INFO][0m - loss: 0.1108747, learning_rate: 2.4599999999999997e-06, global_step: 90, interval_runtime: 28.1969, interval_samples_per_second: 0.567, interval_steps_per_second: 0.355, epoch: 9.0[0m
[32m[2022-09-15 15:20:40,118] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 15:20:40,118] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-15 15:20:40,118] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 15:20:40,118] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 15:20:40,118] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-15 15:20:40,867] [    INFO][0m - eval_loss: 0.32874760031700134, eval_accuracy: 0.8125, eval_runtime: 0.7493, eval_samples_per_second: 213.526, eval_steps_per_second: 13.345, epoch: 9.0[0m
[32m[2022-09-15 15:20:40,868] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-90[0m
[32m[2022-09-15 15:20:40,868] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 15:20:47,104] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-90/tokenizer_config.json[0m
[32m[2022-09-15 15:20:47,105] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-90/special_tokens_map.json[0m
[32m[2022-09-15 15:21:00,765] [    INFO][0m - loss: 0.07812775, learning_rate: 2.4000000000000003e-06, global_step: 100, interval_runtime: 20.6483, interval_samples_per_second: 0.775, interval_steps_per_second: 0.484, epoch: 10.0[0m
[32m[2022-09-15 15:21:00,766] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 15:21:00,766] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-15 15:21:00,766] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 15:21:00,766] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 15:21:00,766] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-15 15:21:01,521] [    INFO][0m - eval_loss: 0.35005003213882446, eval_accuracy: 0.7875, eval_runtime: 0.755, eval_samples_per_second: 211.926, eval_steps_per_second: 13.245, epoch: 10.0[0m
[32m[2022-09-15 15:21:01,522] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-09-15 15:21:01,522] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 15:21:07,831] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-15 15:21:07,832] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-15 15:21:20,048] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-15 15:21:20,048] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-60 (score: 0.81875).[0m
[32m[2022-09-15 15:21:21,912] [    INFO][0m - train_runtime: 226.082, train_samples_per_second: 35.385, train_steps_per_second: 2.212, train_loss: 0.4297730153799057, epoch: 10.0[0m
[32m[2022-09-15 15:21:21,937] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-09-15 15:21:21,938] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 15:21:28,106] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-09-15 15:21:28,107] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-09-15 15:21:28,109] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-15 15:21:28,109] [    INFO][0m -   epoch                    =       10.0[0m
[32m[2022-09-15 15:21:28,109] [    INFO][0m -   train_loss               =     0.4298[0m
[32m[2022-09-15 15:21:28,109] [    INFO][0m -   train_runtime            = 0:03:46.08[0m
[32m[2022-09-15 15:21:28,109] [    INFO][0m -   train_samples_per_second =     35.385[0m
[32m[2022-09-15 15:21:28,109] [    INFO][0m -   train_steps_per_second   =      2.212[0m
[32m[2022-09-15 15:21:28,111] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-15 15:21:28,111] [    INFO][0m -   Num examples = 1772[0m
[32m[2022-09-15 15:21:28,111] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 15:21:28,112] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 15:21:28,112] [    INFO][0m -   Total prediction steps = 111[0m
[32m[2022-09-15 15:21:36,609] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-15 15:21:36,610] [    INFO][0m -   test_accuracy           =     0.7624[0m
[32m[2022-09-15 15:21:36,610] [    INFO][0m -   test_loss               =     0.2786[0m
[32m[2022-09-15 15:21:36,610] [    INFO][0m -   test_runtime            = 0:00:08.49[0m
[32m[2022-09-15 15:21:36,610] [    INFO][0m -   test_samples_per_second =    208.522[0m
[32m[2022-09-15 15:21:36,610] [    INFO][0m -   test_steps_per_second   =     13.062[0m
[32m[2022-09-15 15:21:36,610] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-15 15:21:36,610] [    INFO][0m -   Num examples = 2000[0m
[32m[2022-09-15 15:21:36,610] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 15:21:36,610] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 15:21:36,611] [    INFO][0m -   Total prediction steps = 125[0m
{
  "labels": 1,
  "text_a": "\u53eb\u7238\u7238\u53eb\u4e00\u58f0\u6211\u542c\u542c",
  "text_b": "\u90a3\u4f60\u53eb\u6211\u4e00\u58f0\u7238\u7238",
  "uid": 0
}

Traceback (most recent call last):
  File "train_single.py", line 216, in <module>
    main()
  File "train_single.py", line 210, in main
    test_ret = postprocess(test_ret, test_ds, data_args.task_name,
  File "/ssd2/wanghuijuan03/prompt/PaddleNLP/examples/benchmark/fewclue/a7/e1cw/postprocess.py", line 50, in postprocess
    if verbalizer is not None:
NameError: name 'paddle' is not defined
 
==========
chid
==========
 
[33m[2022-09-15 15:21:51,173] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-09-15 15:21:51,173] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-15 15:21:51,173] [    INFO][0m - ============================================================[0m
[32m[2022-09-15 15:21:51,174] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-15 15:21:51,174] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-15 15:21:51,174] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-15 15:21:51,174] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-15 15:21:51,174] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-15 15:21:51,174] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-15 15:21:51,174] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-15 15:21:51,174] [    INFO][0m - [0m
[32m[2022-09-15 15:21:51,174] [    INFO][0m - ============================================================[0m
[32m[2022-09-15 15:21:51,174] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-15 15:21:51,174] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-15 15:21:51,174] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-15 15:21:51,175] [    INFO][0m - pretrained                    :./checkpoints_cmnli/checkpoint-6000/model_state.pdparams[0m
[32m[2022-09-15 15:21:51,175] [    INFO][0m - prompt                        :‚Äú{'text':'text_a'}‚ÄùËøôÂè•ËØù‰∏≠ÊàêËØ≠[{'text':'text_b'}]ÁöÑÁêÜËß£Ê≠£Á°ÆÂêóÔºü{'mask'}{'mask'}„ÄÇ[0m
[32m[2022-09-15 15:21:51,175] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-15 15:21:51,175] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-15 15:21:51,175] [    INFO][0m - task_name                     :chid[0m
[32m[2022-09-15 15:21:51,175] [    INFO][0m - [0m
[32m[2022-09-15 15:21:51,175] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0915 15:21:51.176859 75035 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0915 15:21:51.181304 75035 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-15 15:21:57,692] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-15 15:21:57,704] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-15 15:21:57,704] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-15 15:21:57,705] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùËøôÂè•ËØù‰∏≠ÊàêËØ≠['}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': ']ÁöÑÁêÜËß£Ê≠£Á°ÆÂêóÔºü'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': '„ÄÇ'}][0m
[32m[2022-09-15 15:21:59,787] [    INFO][0m - ============================================================[0m
[32m[2022-09-15 15:21:59,787] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-15 15:21:59,787] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-15 15:21:59,788] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-15 15:21:59,788] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-15 15:21:59,788] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-15 15:21:59,788] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-15 15:21:59,788] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-15 15:21:59,788] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-15 15:21:59,788] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-15 15:21:59,788] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-15 15:21:59,788] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-15 15:21:59,789] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-15 15:21:59,789] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-15 15:21:59,789] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-15 15:21:59,789] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-15 15:21:59,789] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-15 15:21:59,789] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-15 15:21:59,789] [    INFO][0m - eval_batch_size               :16[0m
[32m[2022-09-15 15:21:59,789] [    INFO][0m - eval_steps                    :500[0m
[32m[2022-09-15 15:21:59,789] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-09-15 15:21:59,789] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-15 15:21:59,789] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-15 15:21:59,789] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-15 15:21:59,789] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-15 15:21:59,790] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-15 15:21:59,790] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-15 15:21:59,790] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-15 15:21:59,790] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-15 15:21:59,790] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-15 15:21:59,790] [    INFO][0m - learning_rate                 :3e-06[0m
[32m[2022-09-15 15:21:59,790] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-15 15:21:59,790] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-15 15:21:59,790] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-15 15:21:59,790] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-15 15:21:59,790] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-15 15:21:59,790] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-15 15:21:59,791] [    INFO][0m - logging_dir                   :./checkpoints/runs/Sep15_15-21-51_instance-3bwob41y-01[0m
[32m[2022-09-15 15:21:59,791] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-15 15:21:59,791] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-15 15:21:59,791] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-15 15:21:59,791] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-15 15:21:59,791] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-15 15:21:59,791] [    INFO][0m - max_seq_length                :256[0m
[32m[2022-09-15 15:21:59,791] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-15 15:21:59,791] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-15 15:21:59,791] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-15 15:21:59,791] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-15 15:21:59,791] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-09-15 15:21:59,792] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-15 15:21:59,792] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-15 15:21:59,792] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-09-15 15:21:59,792] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-15 15:21:59,792] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-15 15:21:59,792] [    INFO][0m - per_device_eval_batch_size    :16[0m
[32m[2022-09-15 15:21:59,792] [    INFO][0m - per_device_train_batch_size   :16[0m
[32m[2022-09-15 15:21:59,792] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-15 15:21:59,792] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-15 15:21:59,792] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-15 15:21:59,792] [    INFO][0m - ppt_learning_rate             :3e-05[0m
[32m[2022-09-15 15:21:59,792] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-15 15:21:59,793] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-15 15:21:59,793] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-15 15:21:59,793] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-15 15:21:59,793] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-15 15:21:59,793] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-15 15:21:59,793] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-09-15 15:21:59,793] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-15 15:21:59,793] [    INFO][0m - save_steps                    :500[0m
[32m[2022-09-15 15:21:59,793] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-09-15 15:21:59,793] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-15 15:21:59,793] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-15 15:21:59,793] [    INFO][0m - seed                          :42[0m
[32m[2022-09-15 15:21:59,793] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-15 15:21:59,794] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-15 15:21:59,794] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-15 15:21:59,794] [    INFO][0m - train_batch_size              :16[0m
[32m[2022-09-15 15:21:59,794] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-15 15:21:59,794] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-15 15:21:59,794] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-15 15:21:59,794] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-15 15:21:59,794] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-15 15:21:59,794] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-15 15:21:59,794] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-15 15:21:59,794] [    INFO][0m - [0m
[32m[2022-09-15 15:21:59,798] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-15 15:21:59,798] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-15 15:21:59,798] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-09-15 15:21:59,798] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2022-09-15 15:21:59,798] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2022-09-15 15:21:59,798] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-15 15:21:59,798] [    INFO][0m -   Total optimization steps = 4450.0[0m
[32m[2022-09-15 15:21:59,798] [    INFO][0m -   Total num train samples = 70700[0m
[32m[2022-09-15 15:22:08,058] [    INFO][0m - loss: 2.69837399, learning_rate: 2.993258426966292e-06, global_step: 10, interval_runtime: 8.2591, interval_samples_per_second: 1.937, interval_steps_per_second: 1.211, epoch: 0.1124[0m
[32m[2022-09-15 15:22:14,766] [    INFO][0m - loss: 0.528828, learning_rate: 2.9865168539325843e-06, global_step: 20, interval_runtime: 6.7072, interval_samples_per_second: 2.386, interval_steps_per_second: 1.491, epoch: 0.2247[0m
[32m[2022-09-15 15:22:26,588] [    INFO][0m - loss: 0.331317, learning_rate: 2.979775280898877e-06, global_step: 30, interval_runtime: 11.8225, interval_samples_per_second: 1.353, interval_steps_per_second: 0.846, epoch: 0.3371[0m
[32m[2022-09-15 15:22:40,832] [    INFO][0m - loss: 0.53713961, learning_rate: 2.9730337078651685e-06, global_step: 40, interval_runtime: 14.2441, interval_samples_per_second: 1.123, interval_steps_per_second: 0.702, epoch: 0.4494[0m
[32m[2022-09-15 15:22:55,211] [    INFO][0m - loss: 0.46861639, learning_rate: 2.9662921348314606e-06, global_step: 50, interval_runtime: 14.3784, interval_samples_per_second: 1.113, interval_steps_per_second: 0.695, epoch: 0.5618[0m
[32m[2022-09-15 15:23:09,395] [    INFO][0m - loss: 0.42380743, learning_rate: 2.9595505617977527e-06, global_step: 60, interval_runtime: 14.1835, interval_samples_per_second: 1.128, interval_steps_per_second: 0.705, epoch: 0.6742[0m
[32m[2022-09-15 15:23:23,639] [    INFO][0m - loss: 0.42203059, learning_rate: 2.9528089887640452e-06, global_step: 70, interval_runtime: 14.2445, interval_samples_per_second: 1.123, interval_steps_per_second: 0.702, epoch: 0.7865[0m
[32m[2022-09-15 15:23:37,882] [    INFO][0m - loss: 0.4970068, learning_rate: 2.9460674157303373e-06, global_step: 80, interval_runtime: 14.243, interval_samples_per_second: 1.123, interval_steps_per_second: 0.702, epoch: 0.8989[0m
[32m[2022-09-15 15:23:51,373] [    INFO][0m - loss: 0.44060216, learning_rate: 2.9393258426966294e-06, global_step: 90, interval_runtime: 13.4916, interval_samples_per_second: 1.186, interval_steps_per_second: 0.741, epoch: 1.0112[0m
[32m[2022-09-15 15:24:05,747] [    INFO][0m - loss: 0.49042912, learning_rate: 2.932584269662921e-06, global_step: 100, interval_runtime: 14.3742, interval_samples_per_second: 1.113, interval_steps_per_second: 0.696, epoch: 1.1236[0m
[32m[2022-09-15 15:24:19,984] [    INFO][0m - loss: 0.46109776, learning_rate: 2.9258426966292136e-06, global_step: 110, interval_runtime: 14.2372, interval_samples_per_second: 1.124, interval_steps_per_second: 0.702, epoch: 1.236[0m
[32m[2022-09-15 15:24:34,206] [    INFO][0m - loss: 0.47518787, learning_rate: 2.9191011235955057e-06, global_step: 120, interval_runtime: 14.2213, interval_samples_per_second: 1.125, interval_steps_per_second: 0.703, epoch: 1.3483[0m
[32m[2022-09-15 15:24:48,447] [    INFO][0m - loss: 0.43779922, learning_rate: 2.912359550561798e-06, global_step: 130, interval_runtime: 14.2414, interval_samples_per_second: 1.123, interval_steps_per_second: 0.702, epoch: 1.4607[0m
[32m[2022-09-15 15:25:02,693] [    INFO][0m - loss: 0.43273506, learning_rate: 2.90561797752809e-06, global_step: 140, interval_runtime: 14.2459, interval_samples_per_second: 1.123, interval_steps_per_second: 0.702, epoch: 1.573[0m
[32m[2022-09-15 15:25:16,952] [    INFO][0m - loss: 0.44344726, learning_rate: 2.898876404494382e-06, global_step: 150, interval_runtime: 14.2589, interval_samples_per_second: 1.122, interval_steps_per_second: 0.701, epoch: 1.6854[0m
[32m[2022-09-15 15:25:31,394] [    INFO][0m - loss: 0.35573447, learning_rate: 2.892134831460674e-06, global_step: 160, interval_runtime: 14.4423, interval_samples_per_second: 1.108, interval_steps_per_second: 0.692, epoch: 1.7978[0m
[32m[2022-09-15 15:25:45,658] [    INFO][0m - loss: 0.43965859, learning_rate: 2.8853932584269663e-06, global_step: 170, interval_runtime: 14.2639, interval_samples_per_second: 1.122, interval_steps_per_second: 0.701, epoch: 1.9101[0m
[32m[2022-09-15 15:25:59,206] [    INFO][0m - loss: 0.38404012, learning_rate: 2.8786516853932584e-06, global_step: 180, interval_runtime: 13.5477, interval_samples_per_second: 1.181, interval_steps_per_second: 0.738, epoch: 2.0225[0m
[32m[2022-09-15 15:26:13,484] [    INFO][0m - loss: 0.43621817, learning_rate: 2.8719101123595505e-06, global_step: 190, interval_runtime: 14.278, interval_samples_per_second: 1.121, interval_steps_per_second: 0.7, epoch: 2.1348[0m
[32m[2022-09-15 15:26:27,744] [    INFO][0m - loss: 0.44523602, learning_rate: 2.8651685393258426e-06, global_step: 200, interval_runtime: 14.2606, interval_samples_per_second: 1.122, interval_steps_per_second: 0.701, epoch: 2.2472[0m
[32m[2022-09-15 15:26:42,079] [    INFO][0m - loss: 0.42855344, learning_rate: 2.858426966292135e-06, global_step: 210, interval_runtime: 14.335, interval_samples_per_second: 1.116, interval_steps_per_second: 0.698, epoch: 2.3596[0m
[32m[2022-09-15 15:26:56,354] [    INFO][0m - loss: 0.4878047, learning_rate: 2.8516853932584272e-06, global_step: 220, interval_runtime: 14.2745, interval_samples_per_second: 1.121, interval_steps_per_second: 0.701, epoch: 2.4719[0m
[32m[2022-09-15 15:27:10,613] [    INFO][0m - loss: 0.49231648, learning_rate: 2.8449438202247193e-06, global_step: 230, interval_runtime: 14.2593, interval_samples_per_second: 1.122, interval_steps_per_second: 0.701, epoch: 2.5843[0m
[32m[2022-09-15 15:27:24,817] [    INFO][0m - loss: 0.45467348, learning_rate: 2.838202247191011e-06, global_step: 240, interval_runtime: 14.204, interval_samples_per_second: 1.126, interval_steps_per_second: 0.704, epoch: 2.6966[0m
[32m[2022-09-15 15:27:39,058] [    INFO][0m - loss: 0.39466743, learning_rate: 2.8314606741573035e-06, global_step: 250, interval_runtime: 14.2411, interval_samples_per_second: 1.124, interval_steps_per_second: 0.702, epoch: 2.809[0m
[32m[2022-09-15 15:27:53,407] [    INFO][0m - loss: 0.37468393, learning_rate: 2.8247191011235956e-06, global_step: 260, interval_runtime: 14.3487, interval_samples_per_second: 1.115, interval_steps_per_second: 0.697, epoch: 2.9213[0m
[32m[2022-09-15 15:28:07,152] [    INFO][0m - loss: 0.34869542, learning_rate: 2.8179775280898877e-06, global_step: 270, interval_runtime: 13.7451, interval_samples_per_second: 1.164, interval_steps_per_second: 0.728, epoch: 3.0337[0m
[32m[2022-09-15 15:28:21,460] [    INFO][0m - loss: 0.43775439, learning_rate: 2.81123595505618e-06, global_step: 280, interval_runtime: 14.3078, interval_samples_per_second: 1.118, interval_steps_per_second: 0.699, epoch: 3.1461[0m
[32m[2022-09-15 15:28:35,741] [    INFO][0m - loss: 0.42056065, learning_rate: 2.804494382022472e-06, global_step: 290, interval_runtime: 14.2812, interval_samples_per_second: 1.12, interval_steps_per_second: 0.7, epoch: 3.2584[0m
[32m[2022-09-15 15:28:49,792] [    INFO][0m - loss: 0.44016023, learning_rate: 2.797752808988764e-06, global_step: 300, interval_runtime: 14.0512, interval_samples_per_second: 1.139, interval_steps_per_second: 0.712, epoch: 3.3708[0m
[32m[2022-09-15 15:28:56,240] [    INFO][0m - loss: 0.44741874, learning_rate: 2.791011235955056e-06, global_step: 310, interval_runtime: 6.4474, interval_samples_per_second: 2.482, interval_steps_per_second: 1.551, epoch: 3.4831[0m
[32m[2022-09-15 15:29:02,848] [    INFO][0m - loss: 0.42942753, learning_rate: 2.7842696629213483e-06, global_step: 320, interval_runtime: 6.6087, interval_samples_per_second: 2.421, interval_steps_per_second: 1.513, epoch: 3.5955[0m
[32m[2022-09-15 15:29:09,318] [    INFO][0m - loss: 0.42879233, learning_rate: 2.7775280898876404e-06, global_step: 330, interval_runtime: 6.4698, interval_samples_per_second: 2.473, interval_steps_per_second: 1.546, epoch: 3.7079[0m
[32m[2022-09-15 15:29:21,092] [    INFO][0m - loss: 0.37380924, learning_rate: 2.770786516853933e-06, global_step: 340, interval_runtime: 11.774, interval_samples_per_second: 1.359, interval_steps_per_second: 0.849, epoch: 3.8202[0m
[32m[2022-09-15 15:29:34,083] [    INFO][0m - loss: 0.43376398, learning_rate: 2.764044943820225e-06, global_step: 350, interval_runtime: 12.9913, interval_samples_per_second: 1.232, interval_steps_per_second: 0.77, epoch: 3.9326[0m
[32m[2022-09-15 15:29:46,285] [    INFO][0m - loss: 0.48611393, learning_rate: 2.757303370786517e-06, global_step: 360, interval_runtime: 12.2017, interval_samples_per_second: 1.311, interval_steps_per_second: 0.82, epoch: 4.0449[0m
[32m[2022-09-15 15:29:58,348] [    INFO][0m - loss: 0.42475357, learning_rate: 2.7505617977528088e-06, global_step: 370, interval_runtime: 12.0608, interval_samples_per_second: 1.327, interval_steps_per_second: 0.829, epoch: 4.1573[0m
[32m[2022-09-15 15:30:11,152] [    INFO][0m - loss: 0.46529145, learning_rate: 2.7438202247191013e-06, global_step: 380, interval_runtime: 12.8057, interval_samples_per_second: 1.249, interval_steps_per_second: 0.781, epoch: 4.2697[0m
[32m[2022-09-15 15:30:24,424] [    INFO][0m - loss: 0.39956114, learning_rate: 2.7370786516853934e-06, global_step: 390, interval_runtime: 13.2722, interval_samples_per_second: 1.206, interval_steps_per_second: 0.753, epoch: 4.382[0m
[32m[2022-09-15 15:30:37,631] [    INFO][0m - loss: 0.43165994, learning_rate: 2.7303370786516855e-06, global_step: 400, interval_runtime: 13.2069, interval_samples_per_second: 1.211, interval_steps_per_second: 0.757, epoch: 4.4944[0m
[32m[2022-09-15 15:30:50,878] [    INFO][0m - loss: 0.39877539, learning_rate: 2.7235955056179776e-06, global_step: 410, interval_runtime: 13.2476, interval_samples_per_second: 1.208, interval_steps_per_second: 0.755, epoch: 4.6067[0m
[32m[2022-09-15 15:30:58,460] [    INFO][0m - loss: 0.46272483, learning_rate: 2.7168539325842697e-06, global_step: 420, interval_runtime: 7.5812, interval_samples_per_second: 2.11, interval_steps_per_second: 1.319, epoch: 4.7191[0m
[32m[2022-09-15 15:31:04,965] [    INFO][0m - loss: 0.46648874, learning_rate: 2.710112359550562e-06, global_step: 430, interval_runtime: 6.5055, interval_samples_per_second: 2.459, interval_steps_per_second: 1.537, epoch: 4.8315[0m
[32m[2022-09-15 15:31:11,375] [    INFO][0m - loss: 0.46816545, learning_rate: 2.703370786516854e-06, global_step: 440, interval_runtime: 6.4103, interval_samples_per_second: 2.496, interval_steps_per_second: 1.56, epoch: 4.9438[0m
[32m[2022-09-15 15:31:17,504] [    INFO][0m - loss: 0.3246876, learning_rate: 2.696629213483146e-06, global_step: 450, interval_runtime: 6.1289, interval_samples_per_second: 2.611, interval_steps_per_second: 1.632, epoch: 5.0562[0m
[32m[2022-09-15 15:31:23,942] [    INFO][0m - loss: 0.54281139, learning_rate: 2.689887640449438e-06, global_step: 460, interval_runtime: 6.4378, interval_samples_per_second: 2.485, interval_steps_per_second: 1.553, epoch: 5.1685[0m
[32m[2022-09-15 15:31:30,382] [    INFO][0m - loss: 0.47945724, learning_rate: 2.6831460674157302e-06, global_step: 470, interval_runtime: 6.4389, interval_samples_per_second: 2.485, interval_steps_per_second: 1.553, epoch: 5.2809[0m
[32m[2022-09-15 15:31:36,835] [    INFO][0m - loss: 0.42604222, learning_rate: 2.6764044943820228e-06, global_step: 480, interval_runtime: 6.4531, interval_samples_per_second: 2.479, interval_steps_per_second: 1.55, epoch: 5.3933[0m
[32m[2022-09-15 15:31:43,265] [    INFO][0m - loss: 0.49651527, learning_rate: 2.669662921348315e-06, global_step: 490, interval_runtime: 6.43, interval_samples_per_second: 2.488, interval_steps_per_second: 1.555, epoch: 5.5056[0m
[32m[2022-09-15 15:31:49,688] [    INFO][0m - loss: 0.3857125, learning_rate: 2.6629213483146066e-06, global_step: 500, interval_runtime: 6.4229, interval_samples_per_second: 2.491, interval_steps_per_second: 1.557, epoch: 5.618[0m
[32m[2022-09-15 15:31:49,689] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 15:31:49,689] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-15 15:31:49,689] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 15:31:49,689] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 15:31:49,689] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-15 15:32:11,642] [    INFO][0m - eval_loss: 0.4111795723438263, eval_accuracy: 0.8571428571428571, eval_runtime: 21.9525, eval_samples_per_second: 64.412, eval_steps_per_second: 4.054, epoch: 5.618[0m
[32m[2022-09-15 15:32:11,671] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-09-15 15:32:11,671] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 15:32:17,426] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-09-15 15:32:17,427] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-09-15 15:32:36,338] [    INFO][0m - loss: 0.40344825, learning_rate: 2.6561797752808987e-06, global_step: 510, interval_runtime: 46.6501, interval_samples_per_second: 0.343, interval_steps_per_second: 0.214, epoch: 5.7303[0m
[32m[2022-09-15 15:32:42,723] [    INFO][0m - loss: 0.4037322, learning_rate: 2.649438202247191e-06, global_step: 520, interval_runtime: 6.385, interval_samples_per_second: 2.506, interval_steps_per_second: 1.566, epoch: 5.8427[0m
[32m[2022-09-15 15:32:49,095] [    INFO][0m - loss: 0.36708627, learning_rate: 2.6426966292134833e-06, global_step: 530, interval_runtime: 6.3726, interval_samples_per_second: 2.511, interval_steps_per_second: 1.569, epoch: 5.9551[0m
[32m[2022-09-15 15:32:55,203] [    INFO][0m - loss: 0.49329381, learning_rate: 2.6359550561797754e-06, global_step: 540, interval_runtime: 6.1078, interval_samples_per_second: 2.62, interval_steps_per_second: 1.637, epoch: 6.0674[0m
[32m[2022-09-15 15:33:01,606] [    INFO][0m - loss: 0.45108595, learning_rate: 2.6292134831460675e-06, global_step: 550, interval_runtime: 6.4028, interval_samples_per_second: 2.499, interval_steps_per_second: 1.562, epoch: 6.1798[0m
[32m[2022-09-15 15:33:08,012] [    INFO][0m - loss: 0.40622749, learning_rate: 2.6224719101123596e-06, global_step: 560, interval_runtime: 6.4062, interval_samples_per_second: 2.498, interval_steps_per_second: 1.561, epoch: 6.2921[0m
[32m[2022-09-15 15:33:14,440] [    INFO][0m - loss: 0.5283103, learning_rate: 2.6157303370786517e-06, global_step: 570, interval_runtime: 6.4276, interval_samples_per_second: 2.489, interval_steps_per_second: 1.556, epoch: 6.4045[0m
[32m[2022-09-15 15:33:20,857] [    INFO][0m - loss: 0.32862415, learning_rate: 2.608988764044944e-06, global_step: 580, interval_runtime: 6.4169, interval_samples_per_second: 2.493, interval_steps_per_second: 1.558, epoch: 6.5169[0m
[32m[2022-09-15 15:33:27,265] [    INFO][0m - loss: 0.37934666, learning_rate: 2.602247191011236e-06, global_step: 590, interval_runtime: 6.4087, interval_samples_per_second: 2.497, interval_steps_per_second: 1.56, epoch: 6.6292[0m
[32m[2022-09-15 15:33:33,680] [    INFO][0m - loss: 0.36850872, learning_rate: 2.595505617977528e-06, global_step: 600, interval_runtime: 6.4151, interval_samples_per_second: 2.494, interval_steps_per_second: 1.559, epoch: 6.7416[0m
[32m[2022-09-15 15:33:40,127] [    INFO][0m - loss: 0.49968147, learning_rate: 2.5887640449438206e-06, global_step: 610, interval_runtime: 6.4465, interval_samples_per_second: 2.482, interval_steps_per_second: 1.551, epoch: 6.8539[0m
[32m[2022-09-15 15:33:46,497] [    INFO][0m - loss: 0.37986202, learning_rate: 2.5820224719101127e-06, global_step: 620, interval_runtime: 6.3704, interval_samples_per_second: 2.512, interval_steps_per_second: 1.57, epoch: 6.9663[0m
[32m[2022-09-15 15:33:52,635] [    INFO][0m - loss: 0.44152217, learning_rate: 2.5752808988764048e-06, global_step: 630, interval_runtime: 6.1377, interval_samples_per_second: 2.607, interval_steps_per_second: 1.629, epoch: 7.0787[0m
[32m[2022-09-15 15:33:59,105] [    INFO][0m - loss: 0.49741259, learning_rate: 2.5685393258426964e-06, global_step: 640, interval_runtime: 6.4706, interval_samples_per_second: 2.473, interval_steps_per_second: 1.545, epoch: 7.191[0m
[32m[2022-09-15 15:34:05,570] [    INFO][0m - loss: 0.40786443, learning_rate: 2.561797752808989e-06, global_step: 650, interval_runtime: 6.4642, interval_samples_per_second: 2.475, interval_steps_per_second: 1.547, epoch: 7.3034[0m
[32m[2022-09-15 15:34:12,004] [    INFO][0m - loss: 0.40099301, learning_rate: 2.555056179775281e-06, global_step: 660, interval_runtime: 6.4344, interval_samples_per_second: 2.487, interval_steps_per_second: 1.554, epoch: 7.4157[0m
[32m[2022-09-15 15:34:18,437] [    INFO][0m - loss: 0.4743227, learning_rate: 2.548314606741573e-06, global_step: 670, interval_runtime: 6.4327, interval_samples_per_second: 2.487, interval_steps_per_second: 1.555, epoch: 7.5281[0m
[32m[2022-09-15 15:34:24,871] [    INFO][0m - loss: 0.41772218, learning_rate: 2.5415730337078653e-06, global_step: 680, interval_runtime: 6.434, interval_samples_per_second: 2.487, interval_steps_per_second: 1.554, epoch: 7.6404[0m
[32m[2022-09-15 15:34:31,309] [    INFO][0m - loss: 0.3496866, learning_rate: 2.5348314606741574e-06, global_step: 690, interval_runtime: 6.4381, interval_samples_per_second: 2.485, interval_steps_per_second: 1.553, epoch: 7.7528[0m
[32m[2022-09-15 15:34:37,740] [    INFO][0m - loss: 0.38871255, learning_rate: 2.5280898876404495e-06, global_step: 700, interval_runtime: 6.4305, interval_samples_per_second: 2.488, interval_steps_per_second: 1.555, epoch: 7.8652[0m
[32m[2022-09-15 15:34:44,094] [    INFO][0m - loss: 0.42941303, learning_rate: 2.5213483146067416e-06, global_step: 710, interval_runtime: 6.3545, interval_samples_per_second: 2.518, interval_steps_per_second: 1.574, epoch: 7.9775[0m
[32m[2022-09-15 15:34:50,266] [    INFO][0m - loss: 0.44171124, learning_rate: 2.5146067415730337e-06, global_step: 720, interval_runtime: 6.1721, interval_samples_per_second: 2.592, interval_steps_per_second: 1.62, epoch: 8.0899[0m
[32m[2022-09-15 15:34:56,689] [    INFO][0m - loss: 0.47746305, learning_rate: 2.507865168539326e-06, global_step: 730, interval_runtime: 6.4229, interval_samples_per_second: 2.491, interval_steps_per_second: 1.557, epoch: 8.2022[0m
[32m[2022-09-15 15:35:03,132] [    INFO][0m - loss: 0.46282849, learning_rate: 2.501123595505618e-06, global_step: 740, interval_runtime: 6.4425, interval_samples_per_second: 2.484, interval_steps_per_second: 1.552, epoch: 8.3146[0m
[32m[2022-09-15 15:35:09,569] [    INFO][0m - loss: 0.31371369, learning_rate: 2.4943820224719104e-06, global_step: 750, interval_runtime: 6.4368, interval_samples_per_second: 2.486, interval_steps_per_second: 1.554, epoch: 8.427[0m
[32m[2022-09-15 15:35:16,025] [    INFO][0m - loss: 0.47575402, learning_rate: 2.4876404494382025e-06, global_step: 760, interval_runtime: 6.4559, interval_samples_per_second: 2.478, interval_steps_per_second: 1.549, epoch: 8.5393[0m
[32m[2022-09-15 15:35:22,458] [    INFO][0m - loss: 0.35354681, learning_rate: 2.4808988764044942e-06, global_step: 770, interval_runtime: 6.4336, interval_samples_per_second: 2.487, interval_steps_per_second: 1.554, epoch: 8.6517[0m
[32m[2022-09-15 15:35:29,633] [    INFO][0m - loss: 0.40147276, learning_rate: 2.4741573033707863e-06, global_step: 780, interval_runtime: 6.4565, interval_samples_per_second: 2.478, interval_steps_per_second: 1.549, epoch: 8.764[0m
[32m[2022-09-15 15:35:36,123] [    INFO][0m - loss: 0.41193857, learning_rate: 2.467415730337079e-06, global_step: 790, interval_runtime: 7.2078, interval_samples_per_second: 2.22, interval_steps_per_second: 1.387, epoch: 8.8764[0m
[32m[2022-09-15 15:35:42,507] [    INFO][0m - loss: 0.35527625, learning_rate: 2.460674157303371e-06, global_step: 800, interval_runtime: 6.3848, interval_samples_per_second: 2.506, interval_steps_per_second: 1.566, epoch: 8.9888[0m
[32m[2022-09-15 15:35:48,708] [    INFO][0m - loss: 0.4064755, learning_rate: 2.453932584269663e-06, global_step: 810, interval_runtime: 6.2009, interval_samples_per_second: 2.58, interval_steps_per_second: 1.613, epoch: 9.1011[0m
[32m[2022-09-15 15:35:55,166] [    INFO][0m - loss: 0.36939182, learning_rate: 2.447191011235955e-06, global_step: 820, interval_runtime: 6.4582, interval_samples_per_second: 2.477, interval_steps_per_second: 1.548, epoch: 9.2135[0m
[32m[2022-09-15 15:36:01,635] [    INFO][0m - loss: 0.40255594, learning_rate: 2.4404494382022473e-06, global_step: 830, interval_runtime: 6.4683, interval_samples_per_second: 2.474, interval_steps_per_second: 1.546, epoch: 9.3258[0m
[32m[2022-09-15 15:36:08,084] [    INFO][0m - loss: 0.30922999, learning_rate: 2.4337078651685394e-06, global_step: 840, interval_runtime: 6.4494, interval_samples_per_second: 2.481, interval_steps_per_second: 1.551, epoch: 9.4382[0m
[32m[2022-09-15 15:36:14,526] [    INFO][0m - loss: 0.38878937, learning_rate: 2.4269662921348315e-06, global_step: 850, interval_runtime: 6.4413, interval_samples_per_second: 2.484, interval_steps_per_second: 1.552, epoch: 9.5506[0m
[32m[2022-09-15 15:36:20,975] [    INFO][0m - loss: 0.39467223, learning_rate: 2.4202247191011236e-06, global_step: 860, interval_runtime: 6.4499, interval_samples_per_second: 2.481, interval_steps_per_second: 1.55, epoch: 9.6629[0m
[32m[2022-09-15 15:36:27,425] [    INFO][0m - loss: 0.4122251, learning_rate: 2.4134831460674157e-06, global_step: 870, interval_runtime: 6.4493, interval_samples_per_second: 2.481, interval_steps_per_second: 1.551, epoch: 9.7753[0m
[32m[2022-09-15 15:36:33,870] [    INFO][0m - loss: 0.3640089, learning_rate: 2.406741573033708e-06, global_step: 880, interval_runtime: 6.4448, interval_samples_per_second: 2.483, interval_steps_per_second: 1.552, epoch: 9.8876[0m
[32m[2022-09-15 15:36:39,861] [    INFO][0m - loss: 0.37521338, learning_rate: 2.4000000000000003e-06, global_step: 890, interval_runtime: 5.9916, interval_samples_per_second: 2.67, interval_steps_per_second: 1.669, epoch: 10.0[0m
[32m[2022-09-15 15:36:46,447] [    INFO][0m - loss: 0.37477515, learning_rate: 2.393258426966292e-06, global_step: 900, interval_runtime: 6.5852, interval_samples_per_second: 2.43, interval_steps_per_second: 1.519, epoch: 10.1124[0m
[32m[2022-09-15 15:36:52,911] [    INFO][0m - loss: 0.21378505, learning_rate: 2.386516853932584e-06, global_step: 910, interval_runtime: 6.4649, interval_samples_per_second: 2.475, interval_steps_per_second: 1.547, epoch: 10.2247[0m
[32m[2022-09-15 15:36:59,371] [    INFO][0m - loss: 0.34659877, learning_rate: 2.379775280898876e-06, global_step: 920, interval_runtime: 6.4604, interval_samples_per_second: 2.477, interval_steps_per_second: 1.548, epoch: 10.3371[0m
[32m[2022-09-15 15:37:05,817] [    INFO][0m - loss: 0.36796999, learning_rate: 2.3730337078651687e-06, global_step: 930, interval_runtime: 6.4454, interval_samples_per_second: 2.482, interval_steps_per_second: 1.552, epoch: 10.4494[0m
[32m[2022-09-15 15:37:12,258] [    INFO][0m - loss: 0.35945072, learning_rate: 2.366292134831461e-06, global_step: 940, interval_runtime: 6.4415, interval_samples_per_second: 2.484, interval_steps_per_second: 1.552, epoch: 10.5618[0m
[32m[2022-09-15 15:37:18,694] [    INFO][0m - loss: 0.35666902, learning_rate: 2.359550561797753e-06, global_step: 950, interval_runtime: 6.4355, interval_samples_per_second: 2.486, interval_steps_per_second: 1.554, epoch: 10.6742[0m
[32m[2022-09-15 15:37:25,131] [    INFO][0m - loss: 0.2281703, learning_rate: 2.352808988764045e-06, global_step: 960, interval_runtime: 6.4375, interval_samples_per_second: 2.485, interval_steps_per_second: 1.553, epoch: 10.7865[0m
[32m[2022-09-15 15:37:31,569] [    INFO][0m - loss: 0.31574755, learning_rate: 2.346067415730337e-06, global_step: 970, interval_runtime: 6.4378, interval_samples_per_second: 2.485, interval_steps_per_second: 1.553, epoch: 10.8989[0m
[32m[2022-09-15 15:37:37,679] [    INFO][0m - loss: 0.30967517, learning_rate: 2.3393258426966293e-06, global_step: 980, interval_runtime: 6.1104, interval_samples_per_second: 2.618, interval_steps_per_second: 1.637, epoch: 11.0112[0m
[32m[2022-09-15 15:37:44,126] [    INFO][0m - loss: 0.26758544, learning_rate: 2.3325842696629214e-06, global_step: 990, interval_runtime: 6.4466, interval_samples_per_second: 2.482, interval_steps_per_second: 1.551, epoch: 11.1236[0m
[32m[2022-09-15 15:37:50,554] [    INFO][0m - loss: 0.21469929, learning_rate: 2.3258426966292135e-06, global_step: 1000, interval_runtime: 6.4284, interval_samples_per_second: 2.489, interval_steps_per_second: 1.556, epoch: 11.236[0m
[32m[2022-09-15 15:37:50,555] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 15:37:50,555] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-15 15:37:50,555] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 15:37:50,555] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 15:37:50,555] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-15 15:38:12,440] [    INFO][0m - eval_loss: 0.3898468613624573, eval_accuracy: 0.8507779349363508, eval_runtime: 21.8839, eval_samples_per_second: 64.614, eval_steps_per_second: 4.067, epoch: 11.236[0m
[32m[2022-09-15 15:38:12,466] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1000[0m
[32m[2022-09-15 15:38:12,466] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 15:38:15,263] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1000/tokenizer_config.json[0m
[32m[2022-09-15 15:38:15,263] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1000/special_tokens_map.json[0m
[32m[2022-09-15 15:38:27,338] [    INFO][0m - loss: 0.32850437, learning_rate: 2.3191011235955056e-06, global_step: 1010, interval_runtime: 36.7832, interval_samples_per_second: 0.435, interval_steps_per_second: 0.272, epoch: 11.3483[0m
[32m[2022-09-15 15:38:33,732] [    INFO][0m - loss: 0.22497385, learning_rate: 2.312359550561798e-06, global_step: 1020, interval_runtime: 6.3943, interval_samples_per_second: 2.502, interval_steps_per_second: 1.564, epoch: 11.4607[0m
[32m[2022-09-15 15:38:40,137] [    INFO][0m - loss: 0.21997826, learning_rate: 2.3056179775280898e-06, global_step: 1030, interval_runtime: 6.4046, interval_samples_per_second: 2.498, interval_steps_per_second: 1.561, epoch: 11.573[0m
[32m[2022-09-15 15:38:46,535] [    INFO][0m - loss: 0.3765732, learning_rate: 2.298876404494382e-06, global_step: 1040, interval_runtime: 6.3986, interval_samples_per_second: 2.501, interval_steps_per_second: 1.563, epoch: 11.6854[0m
[32m[2022-09-15 15:38:52,947] [    INFO][0m - loss: 0.21769843, learning_rate: 2.292134831460674e-06, global_step: 1050, interval_runtime: 6.412, interval_samples_per_second: 2.495, interval_steps_per_second: 1.56, epoch: 11.7978[0m
[32m[2022-09-15 15:38:59,363] [    INFO][0m - loss: 0.42396874, learning_rate: 2.2853932584269665e-06, global_step: 1060, interval_runtime: 6.4158, interval_samples_per_second: 2.494, interval_steps_per_second: 1.559, epoch: 11.9101[0m
[32m[2022-09-15 15:39:05,449] [    INFO][0m - loss: 0.28108397, learning_rate: 2.2786516853932586e-06, global_step: 1070, interval_runtime: 6.0856, interval_samples_per_second: 2.629, interval_steps_per_second: 1.643, epoch: 12.0225[0m
[32m[2022-09-15 15:39:11,928] [    INFO][0m - loss: 0.32541156, learning_rate: 2.2719101123595507e-06, global_step: 1080, interval_runtime: 6.4792, interval_samples_per_second: 2.469, interval_steps_per_second: 1.543, epoch: 12.1348[0m
[32m[2022-09-15 15:39:18,374] [    INFO][0m - loss: 0.16869462, learning_rate: 2.265168539325843e-06, global_step: 1090, interval_runtime: 6.4458, interval_samples_per_second: 2.482, interval_steps_per_second: 1.551, epoch: 12.2472[0m
[32m[2022-09-15 15:39:24,811] [    INFO][0m - loss: 0.33100936, learning_rate: 2.258426966292135e-06, global_step: 1100, interval_runtime: 6.4371, interval_samples_per_second: 2.486, interval_steps_per_second: 1.554, epoch: 12.3596[0m
[32m[2022-09-15 15:39:31,235] [    INFO][0m - loss: 0.26442277, learning_rate: 2.251685393258427e-06, global_step: 1110, interval_runtime: 6.4241, interval_samples_per_second: 2.491, interval_steps_per_second: 1.557, epoch: 12.4719[0m
[32m[2022-09-15 15:39:37,671] [    INFO][0m - loss: 0.19944277, learning_rate: 2.244943820224719e-06, global_step: 1120, interval_runtime: 6.4366, interval_samples_per_second: 2.486, interval_steps_per_second: 1.554, epoch: 12.5843[0m
[32m[2022-09-15 15:39:44,133] [    INFO][0m - loss: 0.23227077, learning_rate: 2.2382022471910112e-06, global_step: 1130, interval_runtime: 6.462, interval_samples_per_second: 2.476, interval_steps_per_second: 1.548, epoch: 12.6966[0m
[32m[2022-09-15 15:39:50,570] [    INFO][0m - loss: 0.20216522, learning_rate: 2.2314606741573033e-06, global_step: 1140, interval_runtime: 6.4363, interval_samples_per_second: 2.486, interval_steps_per_second: 1.554, epoch: 12.809[0m
[32m[2022-09-15 15:39:56,993] [    INFO][0m - loss: 0.26884475, learning_rate: 2.224719101123596e-06, global_step: 1150, interval_runtime: 6.4236, interval_samples_per_second: 2.491, interval_steps_per_second: 1.557, epoch: 12.9213[0m
[32m[2022-09-15 15:40:03,078] [    INFO][0m - loss: 0.19352125, learning_rate: 2.217977528089888e-06, global_step: 1160, interval_runtime: 6.0845, interval_samples_per_second: 2.63, interval_steps_per_second: 1.644, epoch: 13.0337[0m
[32m[2022-09-15 15:40:09,502] [    INFO][0m - loss: 0.14037606, learning_rate: 2.2112359550561797e-06, global_step: 1170, interval_runtime: 6.4241, interval_samples_per_second: 2.491, interval_steps_per_second: 1.557, epoch: 13.1461[0m
[32m[2022-09-15 15:40:15,927] [    INFO][0m - loss: 0.2386754, learning_rate: 2.2044943820224718e-06, global_step: 1180, interval_runtime: 6.4254, interval_samples_per_second: 2.49, interval_steps_per_second: 1.556, epoch: 13.2584[0m
[32m[2022-09-15 15:40:22,358] [    INFO][0m - loss: 0.11919825, learning_rate: 2.197752808988764e-06, global_step: 1190, interval_runtime: 6.4307, interval_samples_per_second: 2.488, interval_steps_per_second: 1.555, epoch: 13.3708[0m
[32m[2022-09-15 15:40:28,785] [    INFO][0m - loss: 0.1934587, learning_rate: 2.1910112359550564e-06, global_step: 1200, interval_runtime: 6.4268, interval_samples_per_second: 2.49, interval_steps_per_second: 1.556, epoch: 13.4831[0m
[32m[2022-09-15 15:40:35,216] [    INFO][0m - loss: 0.280299, learning_rate: 2.1842696629213485e-06, global_step: 1210, interval_runtime: 6.4313, interval_samples_per_second: 2.488, interval_steps_per_second: 1.555, epoch: 13.5955[0m
[32m[2022-09-15 15:40:41,663] [    INFO][0m - loss: 0.22097962, learning_rate: 2.1775280898876406e-06, global_step: 1220, interval_runtime: 6.4461, interval_samples_per_second: 2.482, interval_steps_per_second: 1.551, epoch: 13.7079[0m
[32m[2022-09-15 15:40:48,076] [    INFO][0m - loss: 0.39747324, learning_rate: 2.1707865168539323e-06, global_step: 1230, interval_runtime: 6.4137, interval_samples_per_second: 2.495, interval_steps_per_second: 1.559, epoch: 13.8202[0m
[32m[2022-09-15 15:40:54,521] [    INFO][0m - loss: 0.23957973, learning_rate: 2.164044943820225e-06, global_step: 1240, interval_runtime: 6.4455, interval_samples_per_second: 2.482, interval_steps_per_second: 1.551, epoch: 13.9326[0m
[32m[2022-09-15 15:41:00,668] [    INFO][0m - loss: 0.18965341, learning_rate: 2.157303370786517e-06, global_step: 1250, interval_runtime: 6.1466, interval_samples_per_second: 2.603, interval_steps_per_second: 1.627, epoch: 14.0449[0m
[32m[2022-09-15 15:41:07,080] [    INFO][0m - loss: 0.15626276, learning_rate: 2.150561797752809e-06, global_step: 1260, interval_runtime: 6.4117, interval_samples_per_second: 2.495, interval_steps_per_second: 1.56, epoch: 14.1573[0m
[32m[2022-09-15 15:41:13,511] [    INFO][0m - loss: 0.25258341, learning_rate: 2.143820224719101e-06, global_step: 1270, interval_runtime: 6.4314, interval_samples_per_second: 2.488, interval_steps_per_second: 1.555, epoch: 14.2697[0m
[32m[2022-09-15 15:41:19,946] [    INFO][0m - loss: 0.13426411, learning_rate: 2.1370786516853932e-06, global_step: 1280, interval_runtime: 6.4347, interval_samples_per_second: 2.487, interval_steps_per_second: 1.554, epoch: 14.382[0m
[32m[2022-09-15 15:41:26,379] [    INFO][0m - loss: 0.14281362, learning_rate: 2.1303370786516858e-06, global_step: 1290, interval_runtime: 6.4324, interval_samples_per_second: 2.487, interval_steps_per_second: 1.555, epoch: 14.4944[0m
[32m[2022-09-15 15:41:32,802] [    INFO][0m - loss: 0.14080268, learning_rate: 2.1235955056179774e-06, global_step: 1300, interval_runtime: 6.4236, interval_samples_per_second: 2.491, interval_steps_per_second: 1.557, epoch: 14.6067[0m
[32m[2022-09-15 15:41:39,229] [    INFO][0m - loss: 0.17254542, learning_rate: 2.1168539325842695e-06, global_step: 1310, interval_runtime: 6.4267, interval_samples_per_second: 2.49, interval_steps_per_second: 1.556, epoch: 14.7191[0m
[32m[2022-09-15 15:41:45,663] [    INFO][0m - loss: 0.19526784, learning_rate: 2.1101123595505616e-06, global_step: 1320, interval_runtime: 6.4343, interval_samples_per_second: 2.487, interval_steps_per_second: 1.554, epoch: 14.8315[0m
[32m[2022-09-15 15:41:52,069] [    INFO][0m - loss: 0.19840822, learning_rate: 2.103370786516854e-06, global_step: 1330, interval_runtime: 6.4058, interval_samples_per_second: 2.498, interval_steps_per_second: 1.561, epoch: 14.9438[0m
[32m[2022-09-15 15:41:58,190] [    INFO][0m - loss: 0.18715858, learning_rate: 2.0966292134831463e-06, global_step: 1340, interval_runtime: 6.121, interval_samples_per_second: 2.614, interval_steps_per_second: 1.634, epoch: 15.0562[0m
[32m[2022-09-15 15:42:04,710] [    INFO][0m - loss: 0.07869344, learning_rate: 2.0898876404494384e-06, global_step: 1350, interval_runtime: 6.5202, interval_samples_per_second: 2.454, interval_steps_per_second: 1.534, epoch: 15.1685[0m
[32m[2022-09-15 15:42:11,759] [    INFO][0m - loss: 0.12610904, learning_rate: 2.0831460674157305e-06, global_step: 1360, interval_runtime: 7.0495, interval_samples_per_second: 2.27, interval_steps_per_second: 1.419, epoch: 15.2809[0m
[32m[2022-09-15 15:42:18,518] [    INFO][0m - loss: 0.2322845, learning_rate: 2.0764044943820226e-06, global_step: 1370, interval_runtime: 6.7589, interval_samples_per_second: 2.367, interval_steps_per_second: 1.48, epoch: 15.3933[0m
[32m[2022-09-15 15:42:29,212] [    INFO][0m - loss: 0.166871, learning_rate: 2.0696629213483147e-06, global_step: 1380, interval_runtime: 10.6938, interval_samples_per_second: 1.496, interval_steps_per_second: 0.935, epoch: 15.5056[0m
[32m[2022-09-15 15:42:43,383] [    INFO][0m - loss: 0.17992791, learning_rate: 2.062921348314607e-06, global_step: 1390, interval_runtime: 14.1705, interval_samples_per_second: 1.129, interval_steps_per_second: 0.706, epoch: 15.618[0m
[32m[2022-09-15 15:42:57,592] [    INFO][0m - loss: 0.12951603, learning_rate: 2.056179775280899e-06, global_step: 1400, interval_runtime: 14.2099, interval_samples_per_second: 1.126, interval_steps_per_second: 0.704, epoch: 15.7303[0m
[32m[2022-09-15 15:43:11,780] [    INFO][0m - loss: 0.14428748, learning_rate: 2.049438202247191e-06, global_step: 1410, interval_runtime: 14.1877, interval_samples_per_second: 1.128, interval_steps_per_second: 0.705, epoch: 15.8427[0m
[32m[2022-09-15 15:43:23,341] [    INFO][0m - loss: 0.20550981, learning_rate: 2.0426966292134835e-06, global_step: 1420, interval_runtime: 11.5603, interval_samples_per_second: 1.384, interval_steps_per_second: 0.865, epoch: 15.9551[0m
[32m[2022-09-15 15:43:36,862] [    INFO][0m - loss: 0.14410431, learning_rate: 2.0359550561797752e-06, global_step: 1430, interval_runtime: 13.5215, interval_samples_per_second: 1.183, interval_steps_per_second: 0.74, epoch: 16.0674[0m
[32m[2022-09-15 15:43:51,098] [    INFO][0m - loss: 0.10280493, learning_rate: 2.0292134831460673e-06, global_step: 1440, interval_runtime: 14.2362, interval_samples_per_second: 1.124, interval_steps_per_second: 0.702, epoch: 16.1798[0m
[32m[2022-09-15 15:44:05,507] [    INFO][0m - loss: 0.18567157, learning_rate: 2.0224719101123594e-06, global_step: 1450, interval_runtime: 14.4083, interval_samples_per_second: 1.11, interval_steps_per_second: 0.694, epoch: 16.2921[0m
[32m[2022-09-15 15:44:19,726] [    INFO][0m - loss: 0.14637126, learning_rate: 2.0157303370786515e-06, global_step: 1460, interval_runtime: 14.2195, interval_samples_per_second: 1.125, interval_steps_per_second: 0.703, epoch: 16.4045[0m
[32m[2022-09-15 15:44:33,938] [    INFO][0m - loss: 0.19573963, learning_rate: 2.008988764044944e-06, global_step: 1470, interval_runtime: 14.2122, interval_samples_per_second: 1.126, interval_steps_per_second: 0.704, epoch: 16.5169[0m
[32m[2022-09-15 15:44:48,184] [    INFO][0m - loss: 0.16877632, learning_rate: 2.002247191011236e-06, global_step: 1480, interval_runtime: 14.2453, interval_samples_per_second: 1.123, interval_steps_per_second: 0.702, epoch: 16.6292[0m
[32m[2022-09-15 15:45:02,439] [    INFO][0m - loss: 0.156973, learning_rate: 1.9955056179775283e-06, global_step: 1490, interval_runtime: 14.2553, interval_samples_per_second: 1.122, interval_steps_per_second: 0.701, epoch: 16.7416[0m
[32m[2022-09-15 15:45:16,887] [    INFO][0m - loss: 0.06355652, learning_rate: 1.98876404494382e-06, global_step: 1500, interval_runtime: 14.4477, interval_samples_per_second: 1.107, interval_steps_per_second: 0.692, epoch: 16.8539[0m
[32m[2022-09-15 15:45:16,887] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 15:45:16,887] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-15 15:45:16,888] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 15:45:16,888] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 15:45:16,888] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-15 15:46:02,208] [    INFO][0m - eval_loss: 0.6723108887672424, eval_accuracy: 0.847949080622348, eval_runtime: 45.3202, eval_samples_per_second: 31.2, eval_steps_per_second: 1.964, epoch: 16.8539[0m
[32m[2022-09-15 15:46:02,234] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1500[0m
[32m[2022-09-15 15:46:02,234] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 15:46:04,941] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1500/tokenizer_config.json[0m
[32m[2022-09-15 15:46:04,942] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1500/special_tokens_map.json[0m
[32m[2022-09-15 15:46:24,239] [    INFO][0m - loss: 0.23204436, learning_rate: 1.9820224719101125e-06, global_step: 1510, interval_runtime: 67.3529, interval_samples_per_second: 0.238, interval_steps_per_second: 0.148, epoch: 16.9663[0m
[32m[2022-09-15 15:46:37,743] [    INFO][0m - loss: 0.11257892, learning_rate: 1.9752808988764046e-06, global_step: 1520, interval_runtime: 13.5035, interval_samples_per_second: 1.185, interval_steps_per_second: 0.741, epoch: 17.0787[0m
[32m[2022-09-15 15:46:51,958] [    INFO][0m - loss: 0.09158152, learning_rate: 1.9685393258426967e-06, global_step: 1530, interval_runtime: 14.2156, interval_samples_per_second: 1.126, interval_steps_per_second: 0.703, epoch: 17.191[0m
[32m[2022-09-15 15:47:06,227] [    INFO][0m - loss: 0.18602034, learning_rate: 1.9617977528089888e-06, global_step: 1540, interval_runtime: 14.2688, interval_samples_per_second: 1.121, interval_steps_per_second: 0.701, epoch: 17.3034[0m
[32m[2022-09-15 15:47:20,480] [    INFO][0m - loss: 0.04132944, learning_rate: 1.955056179775281e-06, global_step: 1550, interval_runtime: 14.2526, interval_samples_per_second: 1.123, interval_steps_per_second: 0.702, epoch: 17.4157[0m
[32m[2022-09-15 15:47:34,875] [    INFO][0m - loss: 0.24007456, learning_rate: 1.9483146067415734e-06, global_step: 1560, interval_runtime: 14.3948, interval_samples_per_second: 1.112, interval_steps_per_second: 0.695, epoch: 17.5281[0m
[32m[2022-09-15 15:47:49,079] [    INFO][0m - loss: 0.05678682, learning_rate: 1.941573033707865e-06, global_step: 1570, interval_runtime: 14.2044, interval_samples_per_second: 1.126, interval_steps_per_second: 0.704, epoch: 17.6404[0m
[32m[2022-09-15 15:48:03,247] [    INFO][0m - loss: 0.2599263, learning_rate: 1.934831460674157e-06, global_step: 1580, interval_runtime: 14.1675, interval_samples_per_second: 1.129, interval_steps_per_second: 0.706, epoch: 17.7528[0m
[32m[2022-09-15 15:48:17,438] [    INFO][0m - loss: 0.1170664, learning_rate: 1.9280898876404493e-06, global_step: 1590, interval_runtime: 14.1915, interval_samples_per_second: 1.127, interval_steps_per_second: 0.705, epoch: 17.8652[0m
[32m[2022-09-15 15:48:31,645] [    INFO][0m - loss: 0.22063427, learning_rate: 1.921348314606742e-06, global_step: 1600, interval_runtime: 14.2069, interval_samples_per_second: 1.126, interval_steps_per_second: 0.704, epoch: 17.9775[0m
[32m[2022-09-15 15:48:45,289] [    INFO][0m - loss: 0.10373894, learning_rate: 1.914606741573034e-06, global_step: 1610, interval_runtime: 13.6443, interval_samples_per_second: 1.173, interval_steps_per_second: 0.733, epoch: 18.0899[0m
[32m[2022-09-15 15:48:52,166] [    INFO][0m - loss: 0.02759099, learning_rate: 1.907865168539326e-06, global_step: 1620, interval_runtime: 6.8768, interval_samples_per_second: 2.327, interval_steps_per_second: 1.454, epoch: 18.2022[0m
[32m[2022-09-15 15:49:03,776] [    INFO][0m - loss: 0.14397749, learning_rate: 1.901123595505618e-06, global_step: 1630, interval_runtime: 11.6098, interval_samples_per_second: 1.378, interval_steps_per_second: 0.861, epoch: 18.3146[0m
[32m[2022-09-15 15:49:16,656] [    INFO][0m - loss: 0.12456694, learning_rate: 1.89438202247191e-06, global_step: 1640, interval_runtime: 12.8802, interval_samples_per_second: 1.242, interval_steps_per_second: 0.776, epoch: 18.427[0m
[32m[2022-09-15 15:49:29,293] [    INFO][0m - loss: 0.11957332, learning_rate: 1.8876404494382021e-06, global_step: 1650, interval_runtime: 12.637, interval_samples_per_second: 1.266, interval_steps_per_second: 0.791, epoch: 18.5393[0m
[32m[2022-09-15 15:49:41,398] [    INFO][0m - loss: 0.12358459, learning_rate: 1.8808988764044945e-06, global_step: 1660, interval_runtime: 12.1044, interval_samples_per_second: 1.322, interval_steps_per_second: 0.826, epoch: 18.6517[0m
[32m[2022-09-15 15:49:53,778] [    INFO][0m - loss: 0.12947286, learning_rate: 1.8741573033707866e-06, global_step: 1670, interval_runtime: 12.3807, interval_samples_per_second: 1.292, interval_steps_per_second: 0.808, epoch: 18.764[0m
[32m[2022-09-15 15:50:07,200] [    INFO][0m - loss: 0.17495689, learning_rate: 1.8674157303370789e-06, global_step: 1680, interval_runtime: 13.4208, interval_samples_per_second: 1.192, interval_steps_per_second: 0.745, epoch: 18.8764[0m
[32m[2022-09-15 15:50:20,315] [    INFO][0m - loss: 0.0929392, learning_rate: 1.860674157303371e-06, global_step: 1690, interval_runtime: 13.1157, interval_samples_per_second: 1.22, interval_steps_per_second: 0.762, epoch: 18.9888[0m
[32m[2022-09-15 15:50:32,720] [    INFO][0m - loss: 0.08841075, learning_rate: 1.8539325842696629e-06, global_step: 1700, interval_runtime: 12.4057, interval_samples_per_second: 1.29, interval_steps_per_second: 0.806, epoch: 19.1011[0m
[32m[2022-09-15 15:50:41,024] [    INFO][0m - loss: 0.190996, learning_rate: 1.847191011235955e-06, global_step: 1710, interval_runtime: 8.3033, interval_samples_per_second: 1.927, interval_steps_per_second: 1.204, epoch: 19.2135[0m
[32m[2022-09-15 15:50:47,471] [    INFO][0m - loss: 0.07109151, learning_rate: 1.8404494382022473e-06, global_step: 1720, interval_runtime: 6.4475, interval_samples_per_second: 2.482, interval_steps_per_second: 1.551, epoch: 19.3258[0m
[32m[2022-09-15 15:50:53,964] [    INFO][0m - loss: 0.12770909, learning_rate: 1.8337078651685394e-06, global_step: 1730, interval_runtime: 6.4922, interval_samples_per_second: 2.464, interval_steps_per_second: 1.54, epoch: 19.4382[0m
[32m[2022-09-15 15:51:00,416] [    INFO][0m - loss: 0.08585672, learning_rate: 1.8269662921348315e-06, global_step: 1740, interval_runtime: 6.4529, interval_samples_per_second: 2.48, interval_steps_per_second: 1.55, epoch: 19.5506[0m
[32m[2022-09-15 15:51:06,877] [    INFO][0m - loss: 0.16927589, learning_rate: 1.8202247191011238e-06, global_step: 1750, interval_runtime: 6.4608, interval_samples_per_second: 2.476, interval_steps_per_second: 1.548, epoch: 19.6629[0m
[32m[2022-09-15 15:51:13,326] [    INFO][0m - loss: 0.09348199, learning_rate: 1.813483146067416e-06, global_step: 1760, interval_runtime: 6.4489, interval_samples_per_second: 2.481, interval_steps_per_second: 1.551, epoch: 19.7753[0m
[32m[2022-09-15 15:51:19,801] [    INFO][0m - loss: 0.13406252, learning_rate: 1.8067415730337078e-06, global_step: 1770, interval_runtime: 6.4747, interval_samples_per_second: 2.471, interval_steps_per_second: 1.544, epoch: 19.8876[0m
[32m[2022-09-15 15:51:25,813] [    INFO][0m - loss: 0.04361591, learning_rate: 1.8e-06, global_step: 1780, interval_runtime: 6.0121, interval_samples_per_second: 2.661, interval_steps_per_second: 1.663, epoch: 20.0[0m
[32m[2022-09-15 15:51:32,876] [    INFO][0m - loss: 0.14441966, learning_rate: 1.7932584269662922e-06, global_step: 1790, interval_runtime: 6.6396, interval_samples_per_second: 2.41, interval_steps_per_second: 1.506, epoch: 20.1124[0m
[32m[2022-09-15 15:51:39,369] [    INFO][0m - loss: 0.14062738, learning_rate: 1.7865168539325843e-06, global_step: 1800, interval_runtime: 6.9165, interval_samples_per_second: 2.313, interval_steps_per_second: 1.446, epoch: 20.2247[0m
[32m[2022-09-15 15:51:45,866] [    INFO][0m - loss: 0.10219556, learning_rate: 1.7797752808988764e-06, global_step: 1810, interval_runtime: 6.4965, interval_samples_per_second: 2.463, interval_steps_per_second: 1.539, epoch: 20.3371[0m
[32m[2022-09-15 15:51:52,360] [    INFO][0m - loss: 0.04899897, learning_rate: 1.7730337078651688e-06, global_step: 1820, interval_runtime: 6.494, interval_samples_per_second: 2.464, interval_steps_per_second: 1.54, epoch: 20.4494[0m
[32m[2022-09-15 15:51:58,826] [    INFO][0m - loss: 0.08852473, learning_rate: 1.7662921348314607e-06, global_step: 1830, interval_runtime: 6.4663, interval_samples_per_second: 2.474, interval_steps_per_second: 1.546, epoch: 20.5618[0m
[32m[2022-09-15 15:52:05,305] [    INFO][0m - loss: 0.01818078, learning_rate: 1.7595505617977528e-06, global_step: 1840, interval_runtime: 6.4788, interval_samples_per_second: 2.47, interval_steps_per_second: 1.543, epoch: 20.6742[0m
[32m[2022-09-15 15:52:11,775] [    INFO][0m - loss: 0.10877022, learning_rate: 1.7528089887640449e-06, global_step: 1850, interval_runtime: 6.4699, interval_samples_per_second: 2.473, interval_steps_per_second: 1.546, epoch: 20.7865[0m
[32m[2022-09-15 15:52:18,232] [    INFO][0m - loss: 0.17889762, learning_rate: 1.7460674157303372e-06, global_step: 1860, interval_runtime: 6.4573, interval_samples_per_second: 2.478, interval_steps_per_second: 1.549, epoch: 20.8989[0m
[32m[2022-09-15 15:52:24,373] [    INFO][0m - loss: 0.10031594, learning_rate: 1.7393258426966293e-06, global_step: 1870, interval_runtime: 6.141, interval_samples_per_second: 2.605, interval_steps_per_second: 1.628, epoch: 21.0112[0m
[32m[2022-09-15 15:52:30,843] [    INFO][0m - loss: 0.05168433, learning_rate: 1.7325842696629216e-06, global_step: 1880, interval_runtime: 6.4697, interval_samples_per_second: 2.473, interval_steps_per_second: 1.546, epoch: 21.1236[0m
[32m[2022-09-15 15:52:37,307] [    INFO][0m - loss: 0.10531976, learning_rate: 1.7258426966292137e-06, global_step: 1890, interval_runtime: 6.4648, interval_samples_per_second: 2.475, interval_steps_per_second: 1.547, epoch: 21.236[0m
[32m[2022-09-15 15:52:43,766] [    INFO][0m - loss: 0.14689763, learning_rate: 1.7191011235955056e-06, global_step: 1900, interval_runtime: 6.4585, interval_samples_per_second: 2.477, interval_steps_per_second: 1.548, epoch: 21.3483[0m
[32m[2022-09-15 15:52:50,244] [    INFO][0m - loss: 0.0539707, learning_rate: 1.7123595505617977e-06, global_step: 1910, interval_runtime: 6.4778, interval_samples_per_second: 2.47, interval_steps_per_second: 1.544, epoch: 21.4607[0m
[32m[2022-09-15 15:52:56,708] [    INFO][0m - loss: 0.1950105, learning_rate: 1.7056179775280898e-06, global_step: 1920, interval_runtime: 6.4636, interval_samples_per_second: 2.475, interval_steps_per_second: 1.547, epoch: 21.573[0m
[32m[2022-09-15 15:53:03,181] [    INFO][0m - loss: 0.10836505, learning_rate: 1.6988764044943821e-06, global_step: 1930, interval_runtime: 6.4728, interval_samples_per_second: 2.472, interval_steps_per_second: 1.545, epoch: 21.6854[0m
[32m[2022-09-15 15:53:09,634] [    INFO][0m - loss: 0.04923714, learning_rate: 1.6921348314606742e-06, global_step: 1940, interval_runtime: 6.4538, interval_samples_per_second: 2.479, interval_steps_per_second: 1.549, epoch: 21.7978[0m
[32m[2022-09-15 15:53:16,097] [    INFO][0m - loss: 0.08814219, learning_rate: 1.6853932584269665e-06, global_step: 1950, interval_runtime: 6.4624, interval_samples_per_second: 2.476, interval_steps_per_second: 1.547, epoch: 21.9101[0m
[32m[2022-09-15 15:53:22,242] [    INFO][0m - loss: 0.03871883, learning_rate: 1.6786516853932582e-06, global_step: 1960, interval_runtime: 6.145, interval_samples_per_second: 2.604, interval_steps_per_second: 1.627, epoch: 22.0225[0m
[32m[2022-09-15 15:53:28,748] [    INFO][0m - loss: 0.06673006, learning_rate: 1.6719101123595505e-06, global_step: 1970, interval_runtime: 6.5065, interval_samples_per_second: 2.459, interval_steps_per_second: 1.537, epoch: 22.1348[0m
[32m[2022-09-15 15:53:35,225] [    INFO][0m - loss: 0.04607479, learning_rate: 1.6651685393258426e-06, global_step: 1980, interval_runtime: 6.4765, interval_samples_per_second: 2.47, interval_steps_per_second: 1.544, epoch: 22.2472[0m
[32m[2022-09-15 15:53:41,698] [    INFO][0m - loss: 0.07956802, learning_rate: 1.658426966292135e-06, global_step: 1990, interval_runtime: 6.4735, interval_samples_per_second: 2.472, interval_steps_per_second: 1.545, epoch: 22.3596[0m
[32m[2022-09-15 15:53:48,164] [    INFO][0m - loss: 0.11042018, learning_rate: 1.651685393258427e-06, global_step: 2000, interval_runtime: 6.4659, interval_samples_per_second: 2.475, interval_steps_per_second: 1.547, epoch: 22.4719[0m
[32m[2022-09-15 15:53:48,164] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 15:53:48,164] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-15 15:53:48,165] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 15:53:48,165] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 15:53:48,165] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-15 15:54:10,053] [    INFO][0m - eval_loss: 1.0306106805801392, eval_accuracy: 0.8507779349363508, eval_runtime: 21.8882, eval_samples_per_second: 64.601, eval_steps_per_second: 4.066, epoch: 22.4719[0m
[32m[2022-09-15 15:54:10,080] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2000[0m
[32m[2022-09-15 15:54:10,080] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 15:54:12,905] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2000/tokenizer_config.json[0m
[32m[2022-09-15 15:54:12,905] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2000/special_tokens_map.json[0m
[32m[2022-09-15 15:54:25,563] [    INFO][0m - loss: 0.0781988, learning_rate: 1.6449438202247192e-06, global_step: 2010, interval_runtime: 37.3987, interval_samples_per_second: 0.428, interval_steps_per_second: 0.267, epoch: 22.5843[0m
[32m[2022-09-15 15:54:31,980] [    INFO][0m - loss: 0.01042365, learning_rate: 1.6382022471910115e-06, global_step: 2020, interval_runtime: 6.4179, interval_samples_per_second: 2.493, interval_steps_per_second: 1.558, epoch: 22.6966[0m
[32m[2022-09-15 15:54:38,400] [    INFO][0m - loss: 0.18942454, learning_rate: 1.6314606741573034e-06, global_step: 2030, interval_runtime: 6.4196, interval_samples_per_second: 2.492, interval_steps_per_second: 1.558, epoch: 22.809[0m
[32m[2022-09-15 15:54:44,822] [    INFO][0m - loss: 0.0673174, learning_rate: 1.6247191011235955e-06, global_step: 2040, interval_runtime: 6.4223, interval_samples_per_second: 2.491, interval_steps_per_second: 1.557, epoch: 22.9213[0m
[32m[2022-09-15 15:54:50,925] [    INFO][0m - loss: 0.09026644, learning_rate: 1.6179775280898876e-06, global_step: 2050, interval_runtime: 6.1025, interval_samples_per_second: 2.622, interval_steps_per_second: 1.639, epoch: 23.0337[0m
[32m[2022-09-15 15:54:57,375] [    INFO][0m - loss: 0.00084912, learning_rate: 1.61123595505618e-06, global_step: 2060, interval_runtime: 6.4504, interval_samples_per_second: 2.48, interval_steps_per_second: 1.55, epoch: 23.1461[0m
[32m[2022-09-15 15:55:03,811] [    INFO][0m - loss: 0.08964905, learning_rate: 1.604494382022472e-06, global_step: 2070, interval_runtime: 6.4359, interval_samples_per_second: 2.486, interval_steps_per_second: 1.554, epoch: 23.2584[0m
[32m[2022-09-15 15:55:10,244] [    INFO][0m - loss: 0.04430194, learning_rate: 1.597752808988764e-06, global_step: 2080, interval_runtime: 6.4323, interval_samples_per_second: 2.487, interval_steps_per_second: 1.555, epoch: 23.3708[0m
[32m[2022-09-15 15:55:16,695] [    INFO][0m - loss: 0.09734159, learning_rate: 1.5910112359550564e-06, global_step: 2090, interval_runtime: 6.4512, interval_samples_per_second: 2.48, interval_steps_per_second: 1.55, epoch: 23.4831[0m
[32m[2022-09-15 15:55:23,140] [    INFO][0m - loss: 0.15163475, learning_rate: 1.5842696629213483e-06, global_step: 2100, interval_runtime: 6.445, interval_samples_per_second: 2.483, interval_steps_per_second: 1.552, epoch: 23.5955[0m
[32m[2022-09-15 15:55:29,583] [    INFO][0m - loss: 0.00426948, learning_rate: 1.5775280898876404e-06, global_step: 2110, interval_runtime: 6.443, interval_samples_per_second: 2.483, interval_steps_per_second: 1.552, epoch: 23.7079[0m
[32m[2022-09-15 15:55:36,063] [    INFO][0m - loss: 0.14686143, learning_rate: 1.5707865168539325e-06, global_step: 2120, interval_runtime: 6.4796, interval_samples_per_second: 2.469, interval_steps_per_second: 1.543, epoch: 23.8202[0m
[32m[2022-09-15 15:55:42,550] [    INFO][0m - loss: 0.09229078, learning_rate: 1.5640449438202248e-06, global_step: 2130, interval_runtime: 6.4872, interval_samples_per_second: 2.466, interval_steps_per_second: 1.542, epoch: 23.9326[0m
[32m[2022-09-15 15:55:48,702] [    INFO][0m - loss: 0.10440787, learning_rate: 1.557303370786517e-06, global_step: 2140, interval_runtime: 6.152, interval_samples_per_second: 2.601, interval_steps_per_second: 1.625, epoch: 24.0449[0m
[32m[2022-09-15 15:55:55,182] [    INFO][0m - loss: 0.13425322, learning_rate: 1.5505617977528093e-06, global_step: 2150, interval_runtime: 6.4803, interval_samples_per_second: 2.469, interval_steps_per_second: 1.543, epoch: 24.1573[0m
[32m[2022-09-15 15:56:01,666] [    INFO][0m - loss: 0.03495489, learning_rate: 1.543820224719101e-06, global_step: 2160, interval_runtime: 6.4844, interval_samples_per_second: 2.467, interval_steps_per_second: 1.542, epoch: 24.2697[0m
[32m[2022-09-15 15:56:08,153] [    INFO][0m - loss: 0.15668507, learning_rate: 1.5370786516853933e-06, global_step: 2170, interval_runtime: 6.4866, interval_samples_per_second: 2.467, interval_steps_per_second: 1.542, epoch: 24.382[0m
[32m[2022-09-15 15:56:14,628] [    INFO][0m - loss: 0.13299825, learning_rate: 1.5303370786516854e-06, global_step: 2180, interval_runtime: 6.4755, interval_samples_per_second: 2.471, interval_steps_per_second: 1.544, epoch: 24.4944[0m
[32m[2022-09-15 15:56:21,137] [    INFO][0m - loss: 0.01865064, learning_rate: 1.5235955056179775e-06, global_step: 2190, interval_runtime: 6.5081, interval_samples_per_second: 2.458, interval_steps_per_second: 1.537, epoch: 24.6067[0m
[32m[2022-09-15 15:56:27,626] [    INFO][0m - loss: 0.08814331, learning_rate: 1.5168539325842698e-06, global_step: 2200, interval_runtime: 6.4899, interval_samples_per_second: 2.465, interval_steps_per_second: 1.541, epoch: 24.7191[0m
[32m[2022-09-15 15:56:34,114] [    INFO][0m - loss: 0.01833351, learning_rate: 1.5101123595505619e-06, global_step: 2210, interval_runtime: 6.487, interval_samples_per_second: 2.466, interval_steps_per_second: 1.542, epoch: 24.8315[0m
[32m[2022-09-15 15:56:40,585] [    INFO][0m - loss: 0.01155534, learning_rate: 1.5033707865168542e-06, global_step: 2220, interval_runtime: 6.4722, interval_samples_per_second: 2.472, interval_steps_per_second: 1.545, epoch: 24.9438[0m
[32m[2022-09-15 15:56:46,762] [    INFO][0m - loss: 0.00382284, learning_rate: 1.496629213483146e-06, global_step: 2230, interval_runtime: 6.1768, interval_samples_per_second: 2.59, interval_steps_per_second: 1.619, epoch: 25.0562[0m
[32m[2022-09-15 15:56:53,295] [    INFO][0m - loss: 0.08163853, learning_rate: 1.4898876404494384e-06, global_step: 2240, interval_runtime: 6.5329, interval_samples_per_second: 2.449, interval_steps_per_second: 1.531, epoch: 25.1685[0m
[32m[2022-09-15 15:56:59,794] [    INFO][0m - loss: 0.05831785, learning_rate: 1.4831460674157303e-06, global_step: 2250, interval_runtime: 6.499, interval_samples_per_second: 2.462, interval_steps_per_second: 1.539, epoch: 25.2809[0m
[32m[2022-09-15 15:57:06,290] [    INFO][0m - loss: 0.096034, learning_rate: 1.4764044943820226e-06, global_step: 2260, interval_runtime: 6.4958, interval_samples_per_second: 2.463, interval_steps_per_second: 1.539, epoch: 25.3933[0m
[32m[2022-09-15 15:57:12,837] [    INFO][0m - loss: 0.00529015, learning_rate: 1.4696629213483147e-06, global_step: 2270, interval_runtime: 6.5468, interval_samples_per_second: 2.444, interval_steps_per_second: 1.527, epoch: 25.5056[0m
[32m[2022-09-15 15:57:19,327] [    INFO][0m - loss: 0.05810816, learning_rate: 1.4629213483146068e-06, global_step: 2280, interval_runtime: 6.49, interval_samples_per_second: 2.465, interval_steps_per_second: 1.541, epoch: 25.618[0m
[32m[2022-09-15 15:57:25,833] [    INFO][0m - loss: 0.10494874, learning_rate: 1.456179775280899e-06, global_step: 2290, interval_runtime: 6.5062, interval_samples_per_second: 2.459, interval_steps_per_second: 1.537, epoch: 25.7303[0m
[32m[2022-09-15 15:57:32,334] [    INFO][0m - loss: 0.09797933, learning_rate: 1.449438202247191e-06, global_step: 2300, interval_runtime: 6.5007, interval_samples_per_second: 2.461, interval_steps_per_second: 1.538, epoch: 25.8427[0m
[32m[2022-09-15 15:57:38,797] [    INFO][0m - loss: 0.07499064, learning_rate: 1.4426966292134831e-06, global_step: 2310, interval_runtime: 6.4629, interval_samples_per_second: 2.476, interval_steps_per_second: 1.547, epoch: 25.9551[0m
[32m[2022-09-15 15:57:45,610] [    INFO][0m - loss: 0.00214262, learning_rate: 1.4359550561797752e-06, global_step: 2320, interval_runtime: 6.8136, interval_samples_per_second: 2.348, interval_steps_per_second: 1.468, epoch: 26.0674[0m
[32m[2022-09-15 15:57:52,425] [    INFO][0m - loss: 0.03298099, learning_rate: 1.4292134831460676e-06, global_step: 2330, interval_runtime: 6.8141, interval_samples_per_second: 2.348, interval_steps_per_second: 1.468, epoch: 26.1798[0m
[32m[2022-09-15 15:58:02,324] [    INFO][0m - loss: 0.03397104, learning_rate: 1.4224719101123597e-06, global_step: 2340, interval_runtime: 9.8997, interval_samples_per_second: 1.616, interval_steps_per_second: 1.01, epoch: 26.2921[0m
[32m[2022-09-15 15:58:08,806] [    INFO][0m - loss: 0.07080674, learning_rate: 1.4157303370786518e-06, global_step: 2350, interval_runtime: 6.4811, interval_samples_per_second: 2.469, interval_steps_per_second: 1.543, epoch: 26.4045[0m
[32m[2022-09-15 15:58:15,267] [    INFO][0m - loss: 0.047606, learning_rate: 1.4089887640449439e-06, global_step: 2360, interval_runtime: 6.462, interval_samples_per_second: 2.476, interval_steps_per_second: 1.548, epoch: 26.5169[0m
[32m[2022-09-15 15:58:22,924] [    INFO][0m - loss: 0.05868418, learning_rate: 1.402247191011236e-06, global_step: 2370, interval_runtime: 7.6569, interval_samples_per_second: 2.09, interval_steps_per_second: 1.306, epoch: 26.6292[0m
[32m[2022-09-15 15:58:31,384] [    INFO][0m - loss: 0.01722095, learning_rate: 1.395505617977528e-06, global_step: 2380, interval_runtime: 8.459, interval_samples_per_second: 1.891, interval_steps_per_second: 1.182, epoch: 26.7416[0m
[32m[2022-09-15 15:58:37,869] [    INFO][0m - loss: 0.07927303, learning_rate: 1.3887640449438202e-06, global_step: 2390, interval_runtime: 6.4854, interval_samples_per_second: 2.467, interval_steps_per_second: 1.542, epoch: 26.8539[0m
[32m[2022-09-15 15:58:44,293] [    INFO][0m - loss: 0.07211787, learning_rate: 1.3820224719101125e-06, global_step: 2400, interval_runtime: 6.4244, interval_samples_per_second: 2.49, interval_steps_per_second: 1.557, epoch: 26.9663[0m
[32m[2022-09-15 15:58:53,656] [    INFO][0m - loss: 0.02428937, learning_rate: 1.3752808988764044e-06, global_step: 2410, interval_runtime: 9.363, interval_samples_per_second: 1.709, interval_steps_per_second: 1.068, epoch: 27.0787[0m
[32m[2022-09-15 15:59:00,128] [    INFO][0m - loss: 0.00557804, learning_rate: 1.3685393258426967e-06, global_step: 2420, interval_runtime: 6.4717, interval_samples_per_second: 2.472, interval_steps_per_second: 1.545, epoch: 27.191[0m
[32m[2022-09-15 15:59:06,603] [    INFO][0m - loss: 0.03070449, learning_rate: 1.3617977528089888e-06, global_step: 2430, interval_runtime: 6.4753, interval_samples_per_second: 2.471, interval_steps_per_second: 1.544, epoch: 27.3034[0m
[32m[2022-09-15 15:59:13,080] [    INFO][0m - loss: 0.00890617, learning_rate: 1.355056179775281e-06, global_step: 2440, interval_runtime: 6.4762, interval_samples_per_second: 2.471, interval_steps_per_second: 1.544, epoch: 27.4157[0m
[32m[2022-09-15 15:59:22,724] [    INFO][0m - loss: 0.11304802, learning_rate: 1.348314606741573e-06, global_step: 2450, interval_runtime: 9.6444, interval_samples_per_second: 1.659, interval_steps_per_second: 1.037, epoch: 27.5281[0m
[32m[2022-09-15 15:59:29,198] [    INFO][0m - loss: 0.12750202, learning_rate: 1.3415730337078651e-06, global_step: 2460, interval_runtime: 6.4743, interval_samples_per_second: 2.471, interval_steps_per_second: 1.545, epoch: 27.6404[0m
[32m[2022-09-15 15:59:35,671] [    INFO][0m - loss: 0.05452776, learning_rate: 1.3348314606741574e-06, global_step: 2470, interval_runtime: 6.4723, interval_samples_per_second: 2.472, interval_steps_per_second: 1.545, epoch: 27.7528[0m
[32m[2022-09-15 15:59:42,182] [    INFO][0m - loss: 0.02801423, learning_rate: 1.3280898876404493e-06, global_step: 2480, interval_runtime: 6.5111, interval_samples_per_second: 2.457, interval_steps_per_second: 1.536, epoch: 27.8652[0m
[32m[2022-09-15 15:59:51,767] [    INFO][0m - loss: 0.12494999, learning_rate: 1.3213483146067416e-06, global_step: 2490, interval_runtime: 9.585, interval_samples_per_second: 1.669, interval_steps_per_second: 1.043, epoch: 27.9775[0m
[32m[2022-09-15 15:59:58,025] [    INFO][0m - loss: 0.03597112, learning_rate: 1.3146067415730338e-06, global_step: 2500, interval_runtime: 6.2582, interval_samples_per_second: 2.557, interval_steps_per_second: 1.598, epoch: 28.0899[0m
[32m[2022-09-15 15:59:58,026] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 15:59:58,026] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-15 15:59:58,026] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 15:59:58,026] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 15:59:58,026] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-09-15 16:00:22,959] [    INFO][0m - eval_loss: 1.0799825191497803, eval_accuracy: 0.8557284299858557, eval_runtime: 24.9318, eval_samples_per_second: 56.715, eval_steps_per_second: 3.57, epoch: 28.0899[0m
[32m[2022-09-15 16:00:22,991] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-2500[0m
[32m[2022-09-15 16:00:22,991] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 16:00:25,970] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-2500/tokenizer_config.json[0m
[32m[2022-09-15 16:00:25,970] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-2500/special_tokens_map.json[0m
[32m[2022-09-15 16:00:31,417] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-15 16:00:31,417] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-500 (score: 0.8571428571428571).[0m
[32m[2022-09-15 16:00:33,110] [    INFO][0m - train_runtime: 2313.3103, train_samples_per_second: 30.562, train_steps_per_second: 1.924, train_loss: 0.2510448382008821, epoch: 28.0899[0m
[32m[2022-09-15 16:00:33,169] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-09-15 16:00:33,169] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 16:00:43,521] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-09-15 16:00:43,521] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-09-15 16:00:43,523] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-15 16:00:43,523] [    INFO][0m -   epoch                    =    28.0899[0m
[32m[2022-09-15 16:00:43,523] [    INFO][0m -   train_loss               =      0.251[0m
[32m[2022-09-15 16:00:43,523] [    INFO][0m -   train_runtime            = 0:38:33.31[0m
[32m[2022-09-15 16:00:43,523] [    INFO][0m -   train_samples_per_second =     30.562[0m
[32m[2022-09-15 16:00:43,523] [    INFO][0m -   train_steps_per_second   =      1.924[0m
[32m[2022-09-15 16:00:43,534] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-15 16:00:43,534] [    INFO][0m -   Num examples = 14014[0m
[32m[2022-09-15 16:00:43,534] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 16:00:43,534] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 16:00:43,534] [    INFO][0m -   Total prediction steps = 876[0m
[32m[2022-09-15 16:05:05,797] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-15 16:05:05,798] [    INFO][0m -   test_accuracy           =     0.8571[0m
[32m[2022-09-15 16:05:05,798] [    INFO][0m -   test_loss               =      0.411[0m
[32m[2022-09-15 16:05:05,798] [    INFO][0m -   test_runtime            = 0:04:22.26[0m
[32m[2022-09-15 16:05:05,799] [    INFO][0m -   test_samples_per_second =     53.435[0m
[32m[2022-09-15 16:05:05,799] [    INFO][0m -   test_steps_per_second   =       3.34[0m
[32m[2022-09-15 16:05:05,799] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-15 16:05:05,799] [    INFO][0m -   Num examples = 14000[0m
[32m[2022-09-15 16:05:05,799] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 16:05:05,800] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 16:05:05,800] [    INFO][0m -   Total prediction steps = 875[0m
[32m[2022-09-15 16:09:17,370] [    INFO][0m - Predictions for chidf saved to ./fewclue_submit_examples.[0m
{
  "labels": 0,
  "text_a": "\u5927\u90e8\u5206\u7684\u5927\u529f\u544a\u6210\u90fd\u662f\u7531\u65e0\u6570\u4e2a\u9020\u8c23\u751f\u4e8b\u7d2f\u79ef\u800c\u6765,\u505a\u597d\u6bcf\u4e00\u4ef6\u5e94\u8be5\u505a\u597d\u7684\u5c0f\u4e8b\u7edd\u4e0d\u662f\u4e00\u4ef6\u5c0f\u4e8b\u3002 \u300a\u4f60\u597d\u674e\u7115\u82f1\u300b\u4e3b\u8981\u8bb2\u4e86\u8d3e\u73b2\u610f\u5916\u7a7f\u8d8a\u5230\u4e86\u6bcd\u4eb2\u5e74\u8f7b\u7684\u65f6\u5019,\u548c\u6bcd\u4eb2\u4e00\u8d77\u611f\u53d7\u4e86\u5979\u7684\u751f\u6d3b\u548c\u60c5\u611f\u3002\u8fd9\u6545\u4e8b\u662f\u6839\u636e\u8d3e\u73b2\u7684\u4eb2\u751f\u7ecf\u5386\u6539\u7f16\u800c\u6765, ...",
  "text_b": "",
  "uid": 24
}

 
==========
csl
==========
 
[32m[2022-09-15 16:09:22,656] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-15 16:09:22,657] [    INFO][0m - ============================================================[0m
[32m[2022-09-15 16:09:22,657] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-15 16:09:22,657] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-15 16:09:22,657] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-15 16:09:22,657] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-15 16:09:22,657] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-15 16:09:22,657] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-15 16:09:22,657] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-15 16:09:22,657] [    INFO][0m - [0m
[32m[2022-09-15 16:09:22,657] [    INFO][0m - ============================================================[0m
[32m[2022-09-15 16:09:22,657] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-15 16:09:22,657] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-15 16:09:22,658] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-15 16:09:22,658] [    INFO][0m - pretrained                    :./checkpoints_cmnli/checkpoint-6000/model_state.pdparams[0m
[32m[2022-09-15 16:09:22,658] [    INFO][0m - prompt                        :‚Äú{'text':'text_a'}‚ÄùÂÖ∂‰∏≠‚Äú{'text':'text_b'}‚Äù{'mask'}{'mask'}ËøôÂè•ËØùÁöÑÂÖ≥ÈîÆËØç„ÄÇ[0m
[32m[2022-09-15 16:09:22,658] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-15 16:09:22,658] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-15 16:09:22,658] [    INFO][0m - task_name                     :csl[0m
[32m[2022-09-15 16:09:22,658] [    INFO][0m - [0m
[32m[2022-09-15 16:09:22,658] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0915 16:09:22.660331 54856 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0915 16:09:22.664659 54856 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-15 16:09:30,456] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-15 16:09:30,467] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-15 16:09:30,467] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-15 16:09:30,468] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùÂÖ∂‰∏≠‚Äú'}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': '‚Äù'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'ËøôÂè•ËØùÁöÑÂÖ≥ÈîÆËØç„ÄÇ'}][0m
Traceback (most recent call last):
  File "train_single.py", line 216, in <module>
    main()
  File "train_single.py", line 101, in main
    for x in train_ds:
  File "/ssd2/wanghuijuan03/prompt/PaddleNLP/paddlenlp/datasets/dataset.py", line 275, in __getitem__
    return self._transform(
  File "/ssd2/wanghuijuan03/prompt/PaddleNLP/paddlenlp/datasets/dataset.py", line 267, in _transform
    data = fn(data)
  File "/ssd2/wanghuijuan03/prompt/PaddleNLP/examples/benchmark/fewclue/a7/e1cw/utils.py", line 148, in convert_labels_to_ids
    example.labels = label_dict[example.labels]
KeyError: '1'
 
==========
cluewsc
==========
 
[32m[2022-09-15 16:09:34,781] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-15 16:09:34,782] [    INFO][0m - ============================================================[0m
[32m[2022-09-15 16:09:34,782] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-15 16:09:34,782] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-15 16:09:34,782] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-15 16:09:34,782] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-15 16:09:34,782] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-15 16:09:34,782] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-15 16:09:34,783] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-15 16:09:34,783] [    INFO][0m - [0m
[32m[2022-09-15 16:09:34,783] [    INFO][0m - ============================================================[0m
[32m[2022-09-15 16:09:34,783] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-15 16:09:34,783] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-15 16:09:34,783] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-15 16:09:34,783] [    INFO][0m - pretrained                    :./checkpoints_cmnli/checkpoint-6000/model_state.pdparams[0m
[32m[2022-09-15 16:09:34,783] [    INFO][0m - prompt                        :‚Äú{'text':'text_a'}‚Äù{'text':'text_b'}ËøôÈáå‰ª£ËØç‰ΩøÁî®Ê≠£Á°ÆÂêóÔºü{'mask'}{'mask'}[0m
[32m[2022-09-15 16:09:34,783] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-15 16:09:34,783] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-15 16:09:34,783] [    INFO][0m - task_name                     :cluewsc[0m
[32m[2022-09-15 16:09:34,783] [    INFO][0m - [0m
[32m[2022-09-15 16:09:34,784] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0915 16:09:34.785699 55219 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0915 16:09:34.790055 55219 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-15 16:09:41,640] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-15 16:09:41,676] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-15 16:09:41,677] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-15 16:09:41,677] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚Äù'}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': 'ËøôÈáå‰ª£ËØç‰ΩøÁî®Ê≠£Á°ÆÂêóÔºü'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}][0m
[32m[2022-09-15 16:09:43,777] [    INFO][0m - ============================================================[0m
[32m[2022-09-15 16:09:43,778] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-15 16:09:43,778] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-15 16:09:43,778] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-15 16:09:43,778] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-15 16:09:43,778] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-15 16:09:43,778] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-15 16:09:43,778] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-15 16:09:43,778] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-15 16:09:43,779] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-15 16:09:43,779] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-15 16:09:43,779] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-15 16:09:43,779] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-15 16:09:43,779] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-15 16:09:43,779] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-15 16:09:43,779] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-15 16:09:43,779] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-15 16:09:43,779] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-15 16:09:43,779] [    INFO][0m - eval_batch_size               :16[0m
[32m[2022-09-15 16:09:43,779] [    INFO][0m - eval_steps                    :200[0m
[32m[2022-09-15 16:09:43,780] [    INFO][0m - evaluation_strategy           :IntervalStrategy.EPOCH[0m
[32m[2022-09-15 16:09:43,780] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-15 16:09:43,780] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-15 16:09:43,780] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-15 16:09:43,780] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-15 16:09:43,780] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-15 16:09:43,780] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-15 16:09:43,780] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-15 16:09:43,780] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-15 16:09:43,780] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-15 16:09:43,780] [    INFO][0m - learning_rate                 :3e-06[0m
[32m[2022-09-15 16:09:43,780] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-15 16:09:43,780] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-15 16:09:43,780] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-15 16:09:43,781] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-15 16:09:43,781] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-15 16:09:43,781] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-15 16:09:43,781] [    INFO][0m - logging_dir                   :./checkpoints/runs/Sep15_16-09-34_instance-3bwob41y-01[0m
[32m[2022-09-15 16:09:43,781] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-15 16:09:43,781] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-15 16:09:43,781] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-15 16:09:43,781] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-15 16:09:43,781] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-15 16:09:43,781] [    INFO][0m - max_seq_length                :128[0m
[32m[2022-09-15 16:09:43,781] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-15 16:09:43,781] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-15 16:09:43,782] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-15 16:09:43,782] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-15 16:09:43,782] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-09-15 16:09:43,782] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-15 16:09:43,782] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-15 16:09:43,782] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-09-15 16:09:43,782] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-15 16:09:43,782] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-15 16:09:43,782] [    INFO][0m - per_device_eval_batch_size    :16[0m
[32m[2022-09-15 16:09:43,782] [    INFO][0m - per_device_train_batch_size   :16[0m
[32m[2022-09-15 16:09:43,782] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-15 16:09:43,782] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-15 16:09:43,782] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-15 16:09:43,782] [    INFO][0m - ppt_learning_rate             :3e-05[0m
[32m[2022-09-15 16:09:43,783] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-15 16:09:43,783] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-15 16:09:43,783] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-15 16:09:43,783] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-15 16:09:43,783] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-15 16:09:43,783] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-15 16:09:43,783] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-09-15 16:09:43,783] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-15 16:09:43,783] [    INFO][0m - save_steps                    :200[0m
[32m[2022-09-15 16:09:43,783] [    INFO][0m - save_strategy                 :IntervalStrategy.EPOCH[0m
[32m[2022-09-15 16:09:43,783] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-15 16:09:43,783] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-15 16:09:43,784] [    INFO][0m - seed                          :42[0m
[32m[2022-09-15 16:09:43,784] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-15 16:09:43,784] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-15 16:09:43,784] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-15 16:09:43,784] [    INFO][0m - train_batch_size              :16[0m
[32m[2022-09-15 16:09:43,784] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-15 16:09:43,784] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-15 16:09:43,784] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-15 16:09:43,784] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-15 16:09:43,784] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-15 16:09:43,784] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-15 16:09:43,784] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-15 16:09:43,784] [    INFO][0m - [0m
[32m[2022-09-15 16:09:43,788] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-15 16:09:43,789] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-15 16:09:43,789] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-09-15 16:09:43,789] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2022-09-15 16:09:43,789] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2022-09-15 16:09:43,789] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-15 16:09:43,789] [    INFO][0m -   Total optimization steps = 500.0[0m
[32m[2022-09-15 16:09:43,789] [    INFO][0m -   Total num train samples = 8000[0m
[32m[2022-09-15 16:09:48,430] [    INFO][0m - loss: 2.60700779, learning_rate: 2.9400000000000002e-06, global_step: 10, interval_runtime: 4.6398, interval_samples_per_second: 3.448, interval_steps_per_second: 2.155, epoch: 1.0[0m
[32m[2022-09-15 16:09:48,431] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 16:09:48,432] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-15 16:09:48,432] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 16:09:48,432] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 16:09:48,432] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-15 16:09:49,811] [    INFO][0m - eval_loss: 0.9342237710952759, eval_accuracy: 0.49056603773584906, eval_runtime: 1.3789, eval_samples_per_second: 115.306, eval_steps_per_second: 7.252, epoch: 1.0[0m
[32m[2022-09-15 16:09:49,812] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-10[0m
[32m[2022-09-15 16:09:49,812] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 16:09:57,573] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-10/tokenizer_config.json[0m
[32m[2022-09-15 16:09:57,575] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-10/special_tokens_map.json[0m
[32m[2022-09-15 16:10:15,797] [    INFO][0m - loss: 0.85867968, learning_rate: 2.88e-06, global_step: 20, interval_runtime: 27.3674, interval_samples_per_second: 0.585, interval_steps_per_second: 0.365, epoch: 2.0[0m
[32m[2022-09-15 16:10:15,798] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 16:10:15,799] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-15 16:10:15,799] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 16:10:15,799] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 16:10:15,799] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-15 16:10:17,163] [    INFO][0m - eval_loss: 0.7206301689147949, eval_accuracy: 0.5471698113207547, eval_runtime: 1.3638, eval_samples_per_second: 116.583, eval_steps_per_second: 7.332, epoch: 2.0[0m
[32m[2022-09-15 16:10:17,163] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-20[0m
[32m[2022-09-15 16:10:17,164] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 16:10:25,086] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-20/tokenizer_config.json[0m
[32m[2022-09-15 16:10:25,087] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-20/special_tokens_map.json[0m
[32m[2022-09-15 16:10:42,458] [    INFO][0m - loss: 0.74376569, learning_rate: 2.82e-06, global_step: 30, interval_runtime: 26.6603, interval_samples_per_second: 0.6, interval_steps_per_second: 0.375, epoch: 3.0[0m
[32m[2022-09-15 16:10:42,459] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 16:10:42,459] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-15 16:10:42,459] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 16:10:42,459] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 16:10:42,459] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-15 16:10:43,834] [    INFO][0m - eval_loss: 0.6874555349349976, eval_accuracy: 0.5660377358490566, eval_runtime: 1.374, eval_samples_per_second: 115.718, eval_steps_per_second: 7.278, epoch: 3.0[0m
[32m[2022-09-15 16:10:43,834] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-30[0m
[32m[2022-09-15 16:10:43,835] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 16:10:50,637] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-30/tokenizer_config.json[0m
[32m[2022-09-15 16:10:50,637] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-30/special_tokens_map.json[0m
[32m[2022-09-15 16:11:08,653] [    INFO][0m - loss: 0.66425714, learning_rate: 2.7600000000000003e-06, global_step: 40, interval_runtime: 26.1955, interval_samples_per_second: 0.611, interval_steps_per_second: 0.382, epoch: 4.0[0m
[32m[2022-09-15 16:11:08,654] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 16:11:08,655] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-15 16:11:08,655] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 16:11:08,655] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 16:11:08,655] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-15 16:11:10,013] [    INFO][0m - eval_loss: 0.6740447878837585, eval_accuracy: 0.5911949685534591, eval_runtime: 1.3578, eval_samples_per_second: 117.098, eval_steps_per_second: 7.365, epoch: 4.0[0m
[32m[2022-09-15 16:11:10,013] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-40[0m
[32m[2022-09-15 16:11:10,013] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 16:11:16,780] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-40/tokenizer_config.json[0m
[32m[2022-09-15 16:11:16,780] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-40/special_tokens_map.json[0m
[32m[2022-09-15 16:11:35,152] [    INFO][0m - loss: 0.58481836, learning_rate: 2.7e-06, global_step: 50, interval_runtime: 26.4992, interval_samples_per_second: 0.604, interval_steps_per_second: 0.377, epoch: 5.0[0m
[32m[2022-09-15 16:11:35,153] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 16:11:35,154] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-15 16:11:35,154] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 16:11:35,154] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 16:11:35,154] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-15 16:11:36,518] [    INFO][0m - eval_loss: 0.6929044127464294, eval_accuracy: 0.5723270440251572, eval_runtime: 1.3633, eval_samples_per_second: 116.627, eval_steps_per_second: 7.335, epoch: 5.0[0m
[32m[2022-09-15 16:11:36,518] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-50[0m
[32m[2022-09-15 16:11:36,518] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 16:11:43,236] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-50/tokenizer_config.json[0m
[32m[2022-09-15 16:11:43,237] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-50/special_tokens_map.json[0m
[32m[2022-09-15 16:12:02,293] [    INFO][0m - loss: 0.4920064, learning_rate: 2.64e-06, global_step: 60, interval_runtime: 27.14, interval_samples_per_second: 0.59, interval_steps_per_second: 0.368, epoch: 6.0[0m
[32m[2022-09-15 16:12:02,293] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 16:12:02,294] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-15 16:12:02,294] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 16:12:02,294] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 16:12:02,294] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-15 16:12:03,655] [    INFO][0m - eval_loss: 0.7499016523361206, eval_accuracy: 0.5849056603773585, eval_runtime: 1.3608, eval_samples_per_second: 116.843, eval_steps_per_second: 7.349, epoch: 6.0[0m
[32m[2022-09-15 16:12:03,656] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-60[0m
[32m[2022-09-15 16:12:03,656] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 16:12:11,145] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-60/tokenizer_config.json[0m
[32m[2022-09-15 16:12:11,146] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-60/special_tokens_map.json[0m
[32m[2022-09-15 16:12:29,653] [    INFO][0m - loss: 0.4319798, learning_rate: 2.58e-06, global_step: 70, interval_runtime: 27.3606, interval_samples_per_second: 0.585, interval_steps_per_second: 0.365, epoch: 7.0[0m
[32m[2022-09-15 16:12:29,654] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 16:12:29,654] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-15 16:12:29,654] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 16:12:29,654] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 16:12:29,654] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-15 16:12:31,003] [    INFO][0m - eval_loss: 0.89177006483078, eval_accuracy: 0.6352201257861635, eval_runtime: 1.3483, eval_samples_per_second: 117.931, eval_steps_per_second: 7.417, epoch: 7.0[0m
[32m[2022-09-15 16:12:31,003] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-70[0m
[32m[2022-09-15 16:12:31,003] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 16:12:41,886] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-70/tokenizer_config.json[0m
[32m[2022-09-15 16:12:41,887] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-70/special_tokens_map.json[0m
[32m[2022-09-15 16:13:05,967] [    INFO][0m - loss: 0.27465751, learning_rate: 2.52e-06, global_step: 80, interval_runtime: 36.3138, interval_samples_per_second: 0.441, interval_steps_per_second: 0.275, epoch: 8.0[0m
[32m[2022-09-15 16:13:05,968] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 16:13:05,968] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-15 16:13:05,968] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 16:13:05,968] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 16:13:05,968] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-15 16:13:07,330] [    INFO][0m - eval_loss: 0.9107450246810913, eval_accuracy: 0.6352201257861635, eval_runtime: 1.3605, eval_samples_per_second: 116.871, eval_steps_per_second: 7.35, epoch: 8.0[0m
[32m[2022-09-15 16:13:07,331] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-80[0m
[32m[2022-09-15 16:13:07,331] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 16:13:15,866] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-80/tokenizer_config.json[0m
[32m[2022-09-15 16:13:16,198] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-80/special_tokens_map.json[0m
[32m[2022-09-15 16:13:33,287] [    INFO][0m - loss: 0.28972731, learning_rate: 2.4599999999999997e-06, global_step: 90, interval_runtime: 27.32, interval_samples_per_second: 0.586, interval_steps_per_second: 0.366, epoch: 9.0[0m
[32m[2022-09-15 16:13:33,288] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 16:13:33,288] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-15 16:13:33,288] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 16:13:33,289] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 16:13:33,289] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-15 16:13:34,671] [    INFO][0m - eval_loss: 1.1772172451019287, eval_accuracy: 0.6289308176100629, eval_runtime: 1.3823, eval_samples_per_second: 115.026, eval_steps_per_second: 7.234, epoch: 9.0[0m
[32m[2022-09-15 16:13:34,672] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-90[0m
[32m[2022-09-15 16:13:34,672] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 16:13:41,213] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-90/tokenizer_config.json[0m
[32m[2022-09-15 16:13:41,214] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-90/special_tokens_map.json[0m
[32m[2022-09-15 16:14:03,905] [    INFO][0m - loss: 0.16650022, learning_rate: 2.4000000000000003e-06, global_step: 100, interval_runtime: 30.6181, interval_samples_per_second: 0.523, interval_steps_per_second: 0.327, epoch: 10.0[0m
[32m[2022-09-15 16:14:03,906] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 16:14:03,906] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-15 16:14:03,906] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 16:14:03,906] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 16:14:03,906] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-15 16:14:05,269] [    INFO][0m - eval_loss: 1.1053065061569214, eval_accuracy: 0.6666666666666666, eval_runtime: 1.3621, eval_samples_per_second: 116.735, eval_steps_per_second: 7.342, epoch: 10.0[0m
[32m[2022-09-15 16:14:05,269] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-09-15 16:14:05,269] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 16:14:12,736] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-15 16:14:12,736] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-15 16:14:29,649] [    INFO][0m - loss: 0.14183077, learning_rate: 2.34e-06, global_step: 110, interval_runtime: 25.7437, interval_samples_per_second: 0.622, interval_steps_per_second: 0.388, epoch: 11.0[0m
[32m[2022-09-15 16:14:29,649] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 16:14:29,650] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-15 16:14:29,650] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 16:14:29,650] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 16:14:29,650] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-15 16:14:31,028] [    INFO][0m - eval_loss: 1.3384169340133667, eval_accuracy: 0.6666666666666666, eval_runtime: 1.3778, eval_samples_per_second: 115.402, eval_steps_per_second: 7.258, epoch: 11.0[0m
[32m[2022-09-15 16:14:31,029] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-110[0m
[32m[2022-09-15 16:14:31,029] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 16:14:38,471] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-110/tokenizer_config.json[0m
[32m[2022-09-15 16:14:38,472] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-110/special_tokens_map.json[0m
[32m[2022-09-15 16:14:56,136] [    INFO][0m - loss: 0.10608994, learning_rate: 2.28e-06, global_step: 120, interval_runtime: 26.4872, interval_samples_per_second: 0.604, interval_steps_per_second: 0.378, epoch: 12.0[0m
[32m[2022-09-15 16:14:56,137] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 16:14:56,137] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-15 16:14:56,137] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 16:14:56,137] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 16:14:56,137] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-15 16:14:57,500] [    INFO][0m - eval_loss: 1.5764778852462769, eval_accuracy: 0.6729559748427673, eval_runtime: 1.3619, eval_samples_per_second: 116.748, eval_steps_per_second: 7.343, epoch: 12.0[0m
[32m[2022-09-15 16:14:57,500] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-120[0m
[32m[2022-09-15 16:14:57,500] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 16:15:04,409] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-120/tokenizer_config.json[0m
[32m[2022-09-15 16:15:04,409] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-120/special_tokens_map.json[0m
[32m[2022-09-15 16:15:25,382] [    INFO][0m - loss: 0.06482171, learning_rate: 2.22e-06, global_step: 130, interval_runtime: 29.2459, interval_samples_per_second: 0.547, interval_steps_per_second: 0.342, epoch: 13.0[0m
[32m[2022-09-15 16:15:25,383] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 16:15:25,384] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-15 16:15:25,384] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 16:15:25,384] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 16:15:25,384] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-15 16:15:26,787] [    INFO][0m - eval_loss: 1.8497848510742188, eval_accuracy: 0.660377358490566, eval_runtime: 1.4025, eval_samples_per_second: 113.37, eval_steps_per_second: 7.13, epoch: 13.0[0m
[32m[2022-09-15 16:15:26,787] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-130[0m
[32m[2022-09-15 16:15:26,788] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 16:15:29,852] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-130/tokenizer_config.json[0m
[32m[2022-09-15 16:15:29,852] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-130/special_tokens_map.json[0m
[32m[2022-09-15 16:15:41,537] [    INFO][0m - loss: 0.05245073, learning_rate: 2.16e-06, global_step: 140, interval_runtime: 16.1557, interval_samples_per_second: 0.99, interval_steps_per_second: 0.619, epoch: 14.0[0m
[32m[2022-09-15 16:15:41,538] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 16:15:41,538] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-15 16:15:41,538] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 16:15:41,539] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 16:15:41,539] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-15 16:15:42,930] [    INFO][0m - eval_loss: 2.2722842693328857, eval_accuracy: 0.6792452830188679, eval_runtime: 1.3911, eval_samples_per_second: 114.302, eval_steps_per_second: 7.189, epoch: 14.0[0m
[32m[2022-09-15 16:15:42,930] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-140[0m
[32m[2022-09-15 16:15:42,931] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 16:15:45,902] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-140/tokenizer_config.json[0m
[32m[2022-09-15 16:15:45,902] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-140/special_tokens_map.json[0m
[32m[2022-09-15 16:16:00,434] [    INFO][0m - loss: 0.04934513, learning_rate: 2.1e-06, global_step: 150, interval_runtime: 18.8969, interval_samples_per_second: 0.847, interval_steps_per_second: 0.529, epoch: 15.0[0m
[32m[2022-09-15 16:16:00,436] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 16:16:00,436] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-15 16:16:00,436] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 16:16:00,436] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 16:16:00,436] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-15 16:16:03,002] [    INFO][0m - eval_loss: 2.532975435256958, eval_accuracy: 0.660377358490566, eval_runtime: 2.5655, eval_samples_per_second: 61.976, eval_steps_per_second: 3.898, epoch: 15.0[0m
[32m[2022-09-15 16:16:03,003] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-150[0m
[32m[2022-09-15 16:16:03,003] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 16:16:15,188] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-150/tokenizer_config.json[0m
[32m[2022-09-15 16:16:15,188] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-150/special_tokens_map.json[0m
[32m[2022-09-15 16:16:46,465] [    INFO][0m - loss: 0.05384279, learning_rate: 2.0400000000000004e-06, global_step: 160, interval_runtime: 46.0303, interval_samples_per_second: 0.348, interval_steps_per_second: 0.217, epoch: 16.0[0m
[32m[2022-09-15 16:16:46,466] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 16:16:46,466] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-15 16:16:46,466] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 16:16:46,466] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 16:16:46,466] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-15 16:16:49,471] [    INFO][0m - eval_loss: 2.7504045963287354, eval_accuracy: 0.660377358490566, eval_runtime: 3.0049, eval_samples_per_second: 52.914, eval_steps_per_second: 3.328, epoch: 16.0[0m
[32m[2022-09-15 16:16:49,472] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-160[0m
[32m[2022-09-15 16:16:49,472] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 16:16:52,529] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-160/tokenizer_config.json[0m
[32m[2022-09-15 16:16:52,529] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-160/special_tokens_map.json[0m
[32m[2022-09-15 16:17:01,797] [    INFO][0m - loss: 0.05331111, learning_rate: 1.98e-06, global_step: 170, interval_runtime: 15.332, interval_samples_per_second: 1.044, interval_steps_per_second: 0.652, epoch: 17.0[0m
[32m[2022-09-15 16:17:01,798] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 16:17:01,798] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-15 16:17:01,798] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 16:17:01,799] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 16:17:01,799] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-15 16:17:03,172] [    INFO][0m - eval_loss: 2.912200689315796, eval_accuracy: 0.660377358490566, eval_runtime: 1.3731, eval_samples_per_second: 115.794, eval_steps_per_second: 7.283, epoch: 17.0[0m
[32m[2022-09-15 16:17:03,172] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-170[0m
[32m[2022-09-15 16:17:03,172] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 16:17:06,583] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-170/tokenizer_config.json[0m
[32m[2022-09-15 16:17:06,583] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-170/special_tokens_map.json[0m
[32m[2022-09-15 16:17:15,587] [    INFO][0m - loss: 0.00073739, learning_rate: 1.9200000000000003e-06, global_step: 180, interval_runtime: 13.7907, interval_samples_per_second: 1.16, interval_steps_per_second: 0.725, epoch: 18.0[0m
[32m[2022-09-15 16:17:15,588] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 16:17:15,588] [    INFO][0m -   Num examples = 159[0m
[32m[2022-09-15 16:17:15,588] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 16:17:15,588] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 16:17:15,589] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-15 16:17:16,957] [    INFO][0m - eval_loss: 3.0535972118377686, eval_accuracy: 0.6666666666666666, eval_runtime: 1.3678, eval_samples_per_second: 116.247, eval_steps_per_second: 7.311, epoch: 18.0[0m
[32m[2022-09-15 16:17:17,638] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-180[0m
[32m[2022-09-15 16:17:17,639] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 16:17:20,467] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-180/tokenizer_config.json[0m
[32m[2022-09-15 16:17:20,468] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-180/special_tokens_map.json[0m
[32m[2022-09-15 16:17:26,594] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-15 16:17:26,594] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-140 (score: 0.6792452830188679).[0m
[32m[2022-09-15 16:17:28,852] [    INFO][0m - train_runtime: 465.0618, train_samples_per_second: 17.202, train_steps_per_second: 1.075, train_loss: 0.42421274787177227, epoch: 18.0[0m
[32m[2022-09-15 16:17:28,906] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-09-15 16:17:28,907] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 16:17:36,416] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-09-15 16:17:36,417] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-09-15 16:17:36,418] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-15 16:17:36,418] [    INFO][0m -   epoch                    =       18.0[0m
[32m[2022-09-15 16:17:36,418] [    INFO][0m -   train_loss               =     0.4242[0m
[32m[2022-09-15 16:17:36,418] [    INFO][0m -   train_runtime            = 0:07:45.06[0m
[32m[2022-09-15 16:17:36,418] [    INFO][0m -   train_samples_per_second =     17.202[0m
[32m[2022-09-15 16:17:36,418] [    INFO][0m -   train_steps_per_second   =      1.075[0m
[32m[2022-09-15 16:17:36,421] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-15 16:17:36,421] [    INFO][0m -   Num examples = 976[0m
[32m[2022-09-15 16:17:36,421] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 16:17:36,421] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 16:17:36,421] [    INFO][0m -   Total prediction steps = 61[0m
[32m[2022-09-15 16:17:44,870] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-15 16:17:45,105] [    INFO][0m -   test_accuracy           =     0.7602[0m
[32m[2022-09-15 16:17:45,105] [    INFO][0m -   test_loss               =     1.5083[0m
[32m[2022-09-15 16:17:45,105] [    INFO][0m -   test_runtime            = 0:00:08.44[0m
[32m[2022-09-15 16:17:45,105] [    INFO][0m -   test_samples_per_second =    115.517[0m
[32m[2022-09-15 16:17:45,105] [    INFO][0m -   test_steps_per_second   =       7.22[0m
[32m[2022-09-15 16:17:45,105] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-15 16:17:45,105] [    INFO][0m -   Num examples = 290[0m
[32m[2022-09-15 16:17:45,106] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 16:17:45,106] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 16:17:45,106] [    INFO][0m -   Total prediction steps = 19[0m
[32m[2022-09-15 16:17:47,903] [    INFO][0m - Predictions for cluewscf saved to ./fewclue_submit_examples.[0m
{
  "labels": 0,
  "text_a": "\u4e3a\u4ec0\u4e48\u8981\u51fa\u73b0\u4e00\u4e2a\u8eab\u7a7f\u519b\u88c5\u7684\u9ad8\u5927\u7537\u4eba\uff1f\u5c31\u50cf\u4e00\u7247\u6811\u53f6\u98d8\u5165\u4e86\u6811\u6797\uff0c\u4ed6\u8d70\u5230\u4e86\u6211\u7684\u5bb6\u4eba\u4e2d\u95f4\u3002",
  "text_b": "\u5176\u4e2d\u4ed6\u6307\u7684\u662f\u6811\u53f6"
}

