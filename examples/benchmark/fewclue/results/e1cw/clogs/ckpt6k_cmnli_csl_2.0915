[33m[2022-09-15 17:29:32,733] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-09-15 17:29:32,733] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-15 17:29:32,733] [    INFO][0m - ============================================================[0m
[32m[2022-09-15 17:29:32,733] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-15 17:29:32,733] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-15 17:29:32,733] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-15 17:29:32,734] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-15 17:29:32,734] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-15 17:29:32,734] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-15 17:29:32,734] [    INFO][0m - model_name_or_path            :ernie-1.0-large-zh-cw[0m
[32m[2022-09-15 17:29:32,734] [    INFO][0m - [0m
[32m[2022-09-15 17:29:32,734] [    INFO][0m - ============================================================[0m
[32m[2022-09-15 17:29:32,734] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-15 17:29:32,734] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-15 17:29:32,734] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-15 17:29:32,734] [    INFO][0m - pretrained                    :./checkpoints_cmnli/checkpoint-6000/model_state.pdparams[0m
[32m[2022-09-15 17:29:32,734] [    INFO][0m - prompt                        :‚Äú{'text':'text_a'}‚ÄùÊú¨ÊñáÁöÑÂÜÖÂÆπ{'mask'}{'mask'}‚Äú{'text':'text_b'}‚Äù[0m
[32m[2022-09-15 17:29:32,734] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-15 17:29:32,735] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-15 17:29:32,735] [    INFO][0m - task_name                     :csl[0m
[32m[2022-09-15 17:29:32,735] [    INFO][0m - [0m
[32m[2022-09-15 17:29:32,735] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/ernie_1.0_large_zh_cw.pdparams[0m
W0915 17:29:32.736794 81602 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0915 17:29:32.742200 81602 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-15 17:29:41,022] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/vocab.txt[0m
[32m[2022-09-15 17:29:41,033] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/tokenizer_config.json[0m
[32m[2022-09-15 17:29:41,033] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-1.0-large-zh-cw/special_tokens_map.json[0m
[32m[2022-09-15 17:29:41,034] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùÊú¨ÊñáÁöÑÂÜÖÂÆπ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': '‚Äù'}][0m
[32m[2022-09-15 17:29:43,257] [    INFO][0m - ============================================================[0m
[32m[2022-09-15 17:29:43,257] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-15 17:29:43,258] [    INFO][0m - paddle commit id              :269bd1fe1f9b7d4bf043b9d7a039b9f4cd53ebb7[0m
[32m[2022-09-15 17:29:43,258] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-15 17:29:43,258] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-15 17:29:43,258] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-15 17:29:43,258] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-15 17:29:43,258] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-15 17:29:43,258] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-15 17:29:43,258] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-15 17:29:43,258] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-15 17:29:43,258] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-15 17:29:43,259] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-15 17:29:43,259] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-15 17:29:43,259] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-15 17:29:43,259] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-15 17:29:43,259] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-15 17:29:43,259] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-15 17:29:43,259] [    INFO][0m - eval_batch_size               :16[0m
[32m[2022-09-15 17:29:43,259] [    INFO][0m - eval_steps                    :200[0m
[32m[2022-09-15 17:29:43,259] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-09-15 17:29:43,259] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-15 17:29:43,259] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-15 17:29:43,259] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-15 17:29:43,259] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-15 17:29:43,260] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-15 17:29:43,260] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-15 17:29:43,260] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-15 17:29:43,260] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-15 17:29:43,260] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-15 17:29:43,260] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-09-15 17:29:43,260] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-15 17:29:43,260] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-15 17:29:43,260] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-15 17:29:43,260] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-15 17:29:43,260] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-15 17:29:43,260] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-15 17:29:43,260] [    INFO][0m - logging_dir                   :./checkpoints/runs/Sep15_17-29-32_instance-3bwob41y-01[0m
[32m[2022-09-15 17:29:43,261] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-15 17:29:43,261] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-15 17:29:43,261] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-15 17:29:43,261] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-15 17:29:43,261] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-15 17:29:43,261] [    INFO][0m - max_seq_length                :320[0m
[32m[2022-09-15 17:29:43,261] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-15 17:29:43,261] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-15 17:29:43,261] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-15 17:29:43,261] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-15 17:29:43,261] [    INFO][0m - num_train_epochs              :100.0[0m
[32m[2022-09-15 17:29:43,261] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-15 17:29:43,261] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-15 17:29:43,262] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-09-15 17:29:43,262] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-15 17:29:43,262] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-15 17:29:43,262] [    INFO][0m - per_device_eval_batch_size    :16[0m
[32m[2022-09-15 17:29:43,262] [    INFO][0m - per_device_train_batch_size   :16[0m
[32m[2022-09-15 17:29:43,262] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-15 17:29:43,262] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-15 17:29:43,262] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-15 17:29:43,262] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-09-15 17:29:43,262] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-15 17:29:43,262] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-15 17:29:43,262] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-15 17:29:43,262] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-15 17:29:43,262] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-15 17:29:43,263] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-15 17:29:43,263] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-09-15 17:29:43,263] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-15 17:29:43,263] [    INFO][0m - save_steps                    :200[0m
[32m[2022-09-15 17:29:43,263] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-09-15 17:29:43,263] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-15 17:29:43,263] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-15 17:29:43,263] [    INFO][0m - seed                          :42[0m
[32m[2022-09-15 17:29:43,263] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-15 17:29:43,263] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-15 17:29:43,263] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-15 17:29:43,263] [    INFO][0m - train_batch_size              :16[0m
[32m[2022-09-15 17:29:43,263] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-15 17:29:43,264] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-15 17:29:43,264] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-15 17:29:43,264] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-15 17:29:43,264] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-15 17:29:43,264] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-15 17:29:43,264] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-15 17:29:43,264] [    INFO][0m - [0m
[32m[2022-09-15 17:29:43,267] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-15 17:29:43,267] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-15 17:29:43,267] [    INFO][0m -   Num Epochs = 100[0m
[32m[2022-09-15 17:29:43,267] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2022-09-15 17:29:43,267] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2022-09-15 17:29:43,267] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-15 17:29:43,268] [    INFO][0m -   Total optimization steps = 1000.0[0m
[32m[2022-09-15 17:29:43,268] [    INFO][0m -   Total num train samples = 16000[0m
[33m[2022-09-15 17:29:43,459] [ WARNING][0m - Token indices sequence length is longer than the specified maximum sequence length for this model (608 > 512). Running this sequence through the model will result in indexing errors[0m
[32m[2022-09-15 17:29:52,165] [    INFO][0m - loss: 1.03490515, learning_rate: 2.97e-05, global_step: 10, interval_runtime: 8.8967, interval_samples_per_second: 1.798, interval_steps_per_second: 1.124, epoch: 1.0[0m
[32m[2022-09-15 17:30:00,124] [    INFO][0m - loss: 0.36930146, learning_rate: 2.94e-05, global_step: 20, interval_runtime: 7.9585, interval_samples_per_second: 2.01, interval_steps_per_second: 1.257, epoch: 2.0[0m
[32m[2022-09-15 17:30:08,121] [    INFO][0m - loss: 0.35724556, learning_rate: 2.91e-05, global_step: 30, interval_runtime: 7.9973, interval_samples_per_second: 2.001, interval_steps_per_second: 1.25, epoch: 3.0[0m
[32m[2022-09-15 17:30:16,133] [    INFO][0m - loss: 0.21346121, learning_rate: 2.88e-05, global_step: 40, interval_runtime: 8.0117, interval_samples_per_second: 1.997, interval_steps_per_second: 1.248, epoch: 4.0[0m
[32m[2022-09-15 17:30:24,159] [    INFO][0m - loss: 0.17809445, learning_rate: 2.8499999999999998e-05, global_step: 50, interval_runtime: 8.0263, interval_samples_per_second: 1.993, interval_steps_per_second: 1.246, epoch: 5.0[0m
[32m[2022-09-15 17:30:32,183] [    INFO][0m - loss: 0.05940079, learning_rate: 2.8199999999999998e-05, global_step: 60, interval_runtime: 8.0242, interval_samples_per_second: 1.994, interval_steps_per_second: 1.246, epoch: 6.0[0m
[32m[2022-09-15 17:30:40,215] [    INFO][0m - loss: 0.07351077, learning_rate: 2.79e-05, global_step: 70, interval_runtime: 8.032, interval_samples_per_second: 1.992, interval_steps_per_second: 1.245, epoch: 7.0[0m
[32m[2022-09-15 17:30:48,275] [    INFO][0m - loss: 0.16460869, learning_rate: 2.7600000000000003e-05, global_step: 80, interval_runtime: 8.0596, interval_samples_per_second: 1.985, interval_steps_per_second: 1.241, epoch: 8.0[0m
[32m[2022-09-15 17:30:56,337] [    INFO][0m - loss: 0.08298429, learning_rate: 2.7300000000000003e-05, global_step: 90, interval_runtime: 8.0626, interval_samples_per_second: 1.984, interval_steps_per_second: 1.24, epoch: 9.0[0m
[32m[2022-09-15 17:31:04,388] [    INFO][0m - loss: 0.00139634, learning_rate: 2.7000000000000002e-05, global_step: 100, interval_runtime: 8.0504, interval_samples_per_second: 1.987, interval_steps_per_second: 1.242, epoch: 10.0[0m
[32m[2022-09-15 17:31:12,445] [    INFO][0m - loss: 0.01047482, learning_rate: 2.6700000000000002e-05, global_step: 110, interval_runtime: 8.0569, interval_samples_per_second: 1.986, interval_steps_per_second: 1.241, epoch: 11.0[0m
[32m[2022-09-15 17:31:20,506] [    INFO][0m - loss: 0.00014647, learning_rate: 2.64e-05, global_step: 120, interval_runtime: 8.061, interval_samples_per_second: 1.985, interval_steps_per_second: 1.241, epoch: 12.0[0m
[32m[2022-09-15 17:31:28,559] [    INFO][0m - loss: 0.01886604, learning_rate: 2.61e-05, global_step: 130, interval_runtime: 8.0531, interval_samples_per_second: 1.987, interval_steps_per_second: 1.242, epoch: 13.0[0m
[32m[2022-09-15 17:31:36,604] [    INFO][0m - loss: 0.00016444, learning_rate: 2.58e-05, global_step: 140, interval_runtime: 8.0453, interval_samples_per_second: 1.989, interval_steps_per_second: 1.243, epoch: 14.0[0m
[32m[2022-09-15 17:31:44,668] [    INFO][0m - loss: 1.211e-05, learning_rate: 2.55e-05, global_step: 150, interval_runtime: 8.0642, interval_samples_per_second: 1.984, interval_steps_per_second: 1.24, epoch: 15.0[0m
[32m[2022-09-15 17:31:52,712] [    INFO][0m - loss: 1.032e-05, learning_rate: 2.52e-05, global_step: 160, interval_runtime: 8.0437, interval_samples_per_second: 1.989, interval_steps_per_second: 1.243, epoch: 16.0[0m
[32m[2022-09-15 17:32:00,767] [    INFO][0m - loss: 6.21e-06, learning_rate: 2.49e-05, global_step: 170, interval_runtime: 8.055, interval_samples_per_second: 1.986, interval_steps_per_second: 1.241, epoch: 17.0[0m
[32m[2022-09-15 17:32:08,814] [    INFO][0m - loss: 2.53e-06, learning_rate: 2.4599999999999998e-05, global_step: 180, interval_runtime: 8.0469, interval_samples_per_second: 1.988, interval_steps_per_second: 1.243, epoch: 18.0[0m
[32m[2022-09-15 17:32:16,863] [    INFO][0m - loss: 7.1e-06, learning_rate: 2.43e-05, global_step: 190, interval_runtime: 8.0491, interval_samples_per_second: 1.988, interval_steps_per_second: 1.242, epoch: 19.0[0m
[32m[2022-09-15 17:32:24,931] [    INFO][0m - loss: 0.00015786, learning_rate: 2.4e-05, global_step: 200, interval_runtime: 8.0684, interval_samples_per_second: 1.983, interval_steps_per_second: 1.239, epoch: 20.0[0m
[32m[2022-09-15 17:32:24,932] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 17:32:24,932] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-15 17:32:24,932] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 17:32:24,932] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 17:32:24,932] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-15 17:32:28,116] [    INFO][0m - eval_loss: 3.1887378692626953, eval_accuracy: 0.575, eval_runtime: 3.184, eval_samples_per_second: 50.251, eval_steps_per_second: 3.141, epoch: 20.0[0m
[32m[2022-09-15 17:32:28,117] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-09-15 17:32:28,117] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 17:32:36,981] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-09-15 17:32:36,982] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-09-15 17:32:58,332] [    INFO][0m - loss: 0.02466539, learning_rate: 2.37e-05, global_step: 210, interval_runtime: 33.4003, interval_samples_per_second: 0.479, interval_steps_per_second: 0.299, epoch: 21.0[0m
[32m[2022-09-15 17:33:06,381] [    INFO][0m - loss: 0.02619936, learning_rate: 2.3400000000000003e-05, global_step: 220, interval_runtime: 8.0493, interval_samples_per_second: 1.988, interval_steps_per_second: 1.242, epoch: 22.0[0m
[32m[2022-09-15 17:33:14,451] [    INFO][0m - loss: 3.668e-05, learning_rate: 2.3100000000000002e-05, global_step: 230, interval_runtime: 8.07, interval_samples_per_second: 1.983, interval_steps_per_second: 1.239, epoch: 23.0[0m
[32m[2022-09-15 17:33:22,498] [    INFO][0m - loss: 0.0008672, learning_rate: 2.2800000000000002e-05, global_step: 240, interval_runtime: 8.0463, interval_samples_per_second: 1.988, interval_steps_per_second: 1.243, epoch: 24.0[0m
[32m[2022-09-15 17:33:30,550] [    INFO][0m - loss: 0.00016265, learning_rate: 2.25e-05, global_step: 250, interval_runtime: 8.0524, interval_samples_per_second: 1.987, interval_steps_per_second: 1.242, epoch: 25.0[0m
[32m[2022-09-15 17:33:38,605] [    INFO][0m - loss: 1.344e-05, learning_rate: 2.22e-05, global_step: 260, interval_runtime: 8.0553, interval_samples_per_second: 1.986, interval_steps_per_second: 1.241, epoch: 26.0[0m
[32m[2022-09-15 17:33:46,678] [    INFO][0m - loss: 0.00013828, learning_rate: 2.19e-05, global_step: 270, interval_runtime: 8.0733, interval_samples_per_second: 1.982, interval_steps_per_second: 1.239, epoch: 27.0[0m
[32m[2022-09-15 17:33:54,729] [    INFO][0m - loss: 1.78e-06, learning_rate: 2.16e-05, global_step: 280, interval_runtime: 8.0505, interval_samples_per_second: 1.987, interval_steps_per_second: 1.242, epoch: 28.0[0m
[32m[2022-09-15 17:34:02,811] [    INFO][0m - loss: 0.0002685, learning_rate: 2.13e-05, global_step: 290, interval_runtime: 8.0822, interval_samples_per_second: 1.98, interval_steps_per_second: 1.237, epoch: 29.0[0m
[32m[2022-09-15 17:34:10,879] [    INFO][0m - loss: 0.00503563, learning_rate: 2.1e-05, global_step: 300, interval_runtime: 8.0676, interval_samples_per_second: 1.983, interval_steps_per_second: 1.24, epoch: 30.0[0m
[32m[2022-09-15 17:34:18,938] [    INFO][0m - loss: 1.48e-06, learning_rate: 2.07e-05, global_step: 310, interval_runtime: 8.0593, interval_samples_per_second: 1.985, interval_steps_per_second: 1.241, epoch: 31.0[0m
[32m[2022-09-15 17:34:27,004] [    INFO][0m - loss: 6.14e-06, learning_rate: 2.04e-05, global_step: 320, interval_runtime: 8.0657, interval_samples_per_second: 1.984, interval_steps_per_second: 1.24, epoch: 32.0[0m
[32m[2022-09-15 17:34:35,059] [    INFO][0m - loss: 0.00575836, learning_rate: 2.01e-05, global_step: 330, interval_runtime: 8.0551, interval_samples_per_second: 1.986, interval_steps_per_second: 1.241, epoch: 33.0[0m
[32m[2022-09-15 17:34:43,128] [    INFO][0m - loss: 7.3e-07, learning_rate: 1.98e-05, global_step: 340, interval_runtime: 8.0693, interval_samples_per_second: 1.983, interval_steps_per_second: 1.239, epoch: 34.0[0m
[32m[2022-09-15 17:34:51,191] [    INFO][0m - loss: 0.00020694, learning_rate: 1.95e-05, global_step: 350, interval_runtime: 8.0627, interval_samples_per_second: 1.984, interval_steps_per_second: 1.24, epoch: 35.0[0m
[32m[2022-09-15 17:34:59,249] [    INFO][0m - loss: 7.49e-06, learning_rate: 1.9200000000000003e-05, global_step: 360, interval_runtime: 8.0579, interval_samples_per_second: 1.986, interval_steps_per_second: 1.241, epoch: 36.0[0m
[32m[2022-09-15 17:35:07,312] [    INFO][0m - loss: 3.48e-06, learning_rate: 1.8900000000000002e-05, global_step: 370, interval_runtime: 8.0636, interval_samples_per_second: 1.984, interval_steps_per_second: 1.24, epoch: 37.0[0m
[32m[2022-09-15 17:35:15,391] [    INFO][0m - loss: 2.53e-06, learning_rate: 1.86e-05, global_step: 380, interval_runtime: 8.0789, interval_samples_per_second: 1.98, interval_steps_per_second: 1.238, epoch: 38.0[0m
[32m[2022-09-15 17:35:23,479] [    INFO][0m - loss: 2.4e-07, learning_rate: 1.83e-05, global_step: 390, interval_runtime: 8.0875, interval_samples_per_second: 1.978, interval_steps_per_second: 1.236, epoch: 39.0[0m
[32m[2022-09-15 17:35:31,543] [    INFO][0m - loss: 1.53e-06, learning_rate: 1.8e-05, global_step: 400, interval_runtime: 8.0644, interval_samples_per_second: 1.984, interval_steps_per_second: 1.24, epoch: 40.0[0m
[32m[2022-09-15 17:35:31,544] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 17:35:31,544] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-15 17:35:31,544] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 17:35:31,544] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 17:35:31,544] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-15 17:35:34,717] [    INFO][0m - eval_loss: 2.1656088829040527, eval_accuracy: 0.675, eval_runtime: 3.1731, eval_samples_per_second: 50.423, eval_steps_per_second: 3.151, epoch: 40.0[0m
[32m[2022-09-15 17:35:34,718] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-09-15 17:35:34,718] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 17:35:45,092] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-09-15 17:35:45,092] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-09-15 17:36:06,741] [    INFO][0m - loss: 8.6e-07, learning_rate: 1.77e-05, global_step: 410, interval_runtime: 35.1971, interval_samples_per_second: 0.455, interval_steps_per_second: 0.284, epoch: 41.0[0m
[32m[2022-09-15 17:36:14,798] [    INFO][0m - loss: 5e-07, learning_rate: 1.74e-05, global_step: 420, interval_runtime: 8.0573, interval_samples_per_second: 1.986, interval_steps_per_second: 1.241, epoch: 42.0[0m
[32m[2022-09-15 17:36:22,899] [    INFO][0m - loss: 7.2e-07, learning_rate: 1.71e-05, global_step: 430, interval_runtime: 8.1007, interval_samples_per_second: 1.975, interval_steps_per_second: 1.234, epoch: 43.0[0m
[32m[2022-09-15 17:36:31,018] [    INFO][0m - loss: 3.4e-07, learning_rate: 1.6800000000000002e-05, global_step: 440, interval_runtime: 8.119, interval_samples_per_second: 1.971, interval_steps_per_second: 1.232, epoch: 44.0[0m
[32m[2022-09-15 17:36:39,097] [    INFO][0m - loss: 2e-07, learning_rate: 1.65e-05, global_step: 450, interval_runtime: 8.0798, interval_samples_per_second: 1.98, interval_steps_per_second: 1.238, epoch: 45.0[0m
[32m[2022-09-15 17:36:47,197] [    INFO][0m - loss: 2.6e-07, learning_rate: 1.62e-05, global_step: 460, interval_runtime: 8.0993, interval_samples_per_second: 1.975, interval_steps_per_second: 1.235, epoch: 46.0[0m
[32m[2022-09-15 17:36:55,264] [    INFO][0m - loss: 1.5e-07, learning_rate: 1.59e-05, global_step: 470, interval_runtime: 8.0674, interval_samples_per_second: 1.983, interval_steps_per_second: 1.24, epoch: 47.0[0m
[32m[2022-09-15 17:37:03,338] [    INFO][0m - loss: 2.5e-07, learning_rate: 1.56e-05, global_step: 480, interval_runtime: 8.0737, interval_samples_per_second: 1.982, interval_steps_per_second: 1.239, epoch: 48.0[0m
[32m[2022-09-15 17:37:11,403] [    INFO][0m - loss: 1.84e-06, learning_rate: 1.53e-05, global_step: 490, interval_runtime: 8.0652, interval_samples_per_second: 1.984, interval_steps_per_second: 1.24, epoch: 49.0[0m
[32m[2022-09-15 17:37:19,463] [    INFO][0m - loss: 1.5e-07, learning_rate: 1.5e-05, global_step: 500, interval_runtime: 8.0601, interval_samples_per_second: 1.985, interval_steps_per_second: 1.241, epoch: 50.0[0m
[32m[2022-09-15 17:37:27,531] [    INFO][0m - loss: 2.2e-07, learning_rate: 1.47e-05, global_step: 510, interval_runtime: 8.0683, interval_samples_per_second: 1.983, interval_steps_per_second: 1.239, epoch: 51.0[0m
[32m[2022-09-15 17:37:35,581] [    INFO][0m - loss: 1.7e-07, learning_rate: 1.44e-05, global_step: 520, interval_runtime: 8.0499, interval_samples_per_second: 1.988, interval_steps_per_second: 1.242, epoch: 52.0[0m
[32m[2022-09-15 17:37:43,629] [    INFO][0m - loss: 1.66e-06, learning_rate: 1.4099999999999999e-05, global_step: 530, interval_runtime: 8.0475, interval_samples_per_second: 1.988, interval_steps_per_second: 1.243, epoch: 53.0[0m
[32m[2022-09-15 17:37:51,688] [    INFO][0m - loss: 1.2e-07, learning_rate: 1.3800000000000002e-05, global_step: 540, interval_runtime: 8.0589, interval_samples_per_second: 1.985, interval_steps_per_second: 1.241, epoch: 54.0[0m
[32m[2022-09-15 17:37:59,748] [    INFO][0m - loss: 2.8e-07, learning_rate: 1.3500000000000001e-05, global_step: 550, interval_runtime: 8.0606, interval_samples_per_second: 1.985, interval_steps_per_second: 1.241, epoch: 55.0[0m
[32m[2022-09-15 17:38:07,810] [    INFO][0m - loss: 1.4e-07, learning_rate: 1.32e-05, global_step: 560, interval_runtime: 8.0612, interval_samples_per_second: 1.985, interval_steps_per_second: 1.241, epoch: 56.0[0m
[32m[2022-09-15 17:38:15,869] [    INFO][0m - loss: 1.2e-07, learning_rate: 1.29e-05, global_step: 570, interval_runtime: 8.0594, interval_samples_per_second: 1.985, interval_steps_per_second: 1.241, epoch: 57.0[0m
[32m[2022-09-15 17:38:23,944] [    INFO][0m - loss: 1.2e-07, learning_rate: 1.26e-05, global_step: 580, interval_runtime: 8.0755, interval_samples_per_second: 1.981, interval_steps_per_second: 1.238, epoch: 58.0[0m
[32m[2022-09-15 17:38:32,055] [    INFO][0m - loss: 1.4e-07, learning_rate: 1.2299999999999999e-05, global_step: 590, interval_runtime: 8.1099, interval_samples_per_second: 1.973, interval_steps_per_second: 1.233, epoch: 59.0[0m
[32m[2022-09-15 17:38:40,119] [    INFO][0m - loss: 8e-08, learning_rate: 1.2e-05, global_step: 600, interval_runtime: 8.0642, interval_samples_per_second: 1.984, interval_steps_per_second: 1.24, epoch: 60.0[0m
[32m[2022-09-15 17:38:40,119] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 17:38:40,119] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-15 17:38:40,119] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 17:38:40,119] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 17:38:40,119] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-15 17:38:43,287] [    INFO][0m - eval_loss: 2.2106847763061523, eval_accuracy: 0.6875, eval_runtime: 3.1671, eval_samples_per_second: 50.519, eval_steps_per_second: 3.157, epoch: 60.0[0m
[32m[2022-09-15 17:38:43,287] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-09-15 17:38:43,287] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 17:38:53,787] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-09-15 17:38:53,787] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-09-15 17:39:15,102] [    INFO][0m - loss: 1.9e-07, learning_rate: 1.1700000000000001e-05, global_step: 610, interval_runtime: 34.9829, interval_samples_per_second: 0.457, interval_steps_per_second: 0.286, epoch: 61.0[0m
[32m[2022-09-15 17:39:23,149] [    INFO][0m - loss: 2.2e-07, learning_rate: 1.1400000000000001e-05, global_step: 620, interval_runtime: 8.0472, interval_samples_per_second: 1.988, interval_steps_per_second: 1.243, epoch: 62.0[0m
[32m[2022-09-15 17:39:31,235] [    INFO][0m - loss: 1.2e-07, learning_rate: 1.11e-05, global_step: 630, interval_runtime: 8.0864, interval_samples_per_second: 1.979, interval_steps_per_second: 1.237, epoch: 63.0[0m
[32m[2022-09-15 17:39:39,313] [    INFO][0m - loss: 1.7e-07, learning_rate: 1.08e-05, global_step: 640, interval_runtime: 8.0773, interval_samples_per_second: 1.981, interval_steps_per_second: 1.238, epoch: 64.0[0m
[32m[2022-09-15 17:39:47,379] [    INFO][0m - loss: 1e-07, learning_rate: 1.05e-05, global_step: 650, interval_runtime: 8.0661, interval_samples_per_second: 1.984, interval_steps_per_second: 1.24, epoch: 65.0[0m
[32m[2022-09-15 17:39:55,453] [    INFO][0m - loss: 1e-07, learning_rate: 1.02e-05, global_step: 660, interval_runtime: 8.0744, interval_samples_per_second: 1.982, interval_steps_per_second: 1.238, epoch: 66.0[0m
[32m[2022-09-15 17:40:03,523] [    INFO][0m - loss: 2.1e-07, learning_rate: 9.9e-06, global_step: 670, interval_runtime: 8.0694, interval_samples_per_second: 1.983, interval_steps_per_second: 1.239, epoch: 67.0[0m
[32m[2022-09-15 17:40:11,599] [    INFO][0m - loss: 9e-08, learning_rate: 9.600000000000001e-06, global_step: 680, interval_runtime: 8.0769, interval_samples_per_second: 1.981, interval_steps_per_second: 1.238, epoch: 68.0[0m
[32m[2022-09-15 17:40:19,766] [    INFO][0m - loss: 2.2e-07, learning_rate: 9.3e-06, global_step: 690, interval_runtime: 8.167, interval_samples_per_second: 1.959, interval_steps_per_second: 1.224, epoch: 69.0[0m
[32m[2022-09-15 17:40:27,849] [    INFO][0m - loss: 1.1e-07, learning_rate: 9e-06, global_step: 700, interval_runtime: 8.083, interval_samples_per_second: 1.979, interval_steps_per_second: 1.237, epoch: 70.0[0m
[32m[2022-09-15 17:40:35,921] [    INFO][0m - loss: 6e-08, learning_rate: 8.7e-06, global_step: 710, interval_runtime: 8.0721, interval_samples_per_second: 1.982, interval_steps_per_second: 1.239, epoch: 71.0[0m
[32m[2022-09-15 17:40:43,988] [    INFO][0m - loss: 1.2e-07, learning_rate: 8.400000000000001e-06, global_step: 720, interval_runtime: 8.0662, interval_samples_per_second: 1.984, interval_steps_per_second: 1.24, epoch: 72.0[0m
[32m[2022-09-15 17:40:52,057] [    INFO][0m - loss: 1.5e-07, learning_rate: 8.1e-06, global_step: 730, interval_runtime: 8.0695, interval_samples_per_second: 1.983, interval_steps_per_second: 1.239, epoch: 73.0[0m
[32m[2022-09-15 17:41:00,140] [    INFO][0m - loss: 1.2e-07, learning_rate: 7.8e-06, global_step: 740, interval_runtime: 8.0831, interval_samples_per_second: 1.979, interval_steps_per_second: 1.237, epoch: 74.0[0m
[32m[2022-09-15 17:41:08,206] [    INFO][0m - loss: 2.3e-07, learning_rate: 7.5e-06, global_step: 750, interval_runtime: 8.066, interval_samples_per_second: 1.984, interval_steps_per_second: 1.24, epoch: 75.0[0m
[32m[2022-09-15 17:41:16,487] [    INFO][0m - loss: 1.6e-07, learning_rate: 7.2e-06, global_step: 760, interval_runtime: 8.2813, interval_samples_per_second: 1.932, interval_steps_per_second: 1.208, epoch: 76.0[0m
[32m[2022-09-15 17:41:24,542] [    INFO][0m - loss: 9e-08, learning_rate: 6.900000000000001e-06, global_step: 770, interval_runtime: 8.055, interval_samples_per_second: 1.986, interval_steps_per_second: 1.241, epoch: 77.0[0m
[32m[2022-09-15 17:41:32,788] [    INFO][0m - loss: 8e-08, learning_rate: 6.6e-06, global_step: 780, interval_runtime: 8.2452, interval_samples_per_second: 1.941, interval_steps_per_second: 1.213, epoch: 78.0[0m
[32m[2022-09-15 17:41:40,836] [    INFO][0m - loss: 1.5e-07, learning_rate: 6.3e-06, global_step: 790, interval_runtime: 8.0474, interval_samples_per_second: 1.988, interval_steps_per_second: 1.243, epoch: 79.0[0m
[32m[2022-09-15 17:41:48,899] [    INFO][0m - loss: 9e-08, learning_rate: 6e-06, global_step: 800, interval_runtime: 8.0636, interval_samples_per_second: 1.984, interval_steps_per_second: 1.24, epoch: 80.0[0m
[32m[2022-09-15 17:41:48,900] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 17:41:48,900] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-15 17:41:48,900] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 17:41:48,900] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 17:41:48,900] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-15 17:41:52,090] [    INFO][0m - eval_loss: 2.2331929206848145, eval_accuracy: 0.6875, eval_runtime: 3.1898, eval_samples_per_second: 50.161, eval_steps_per_second: 3.135, epoch: 80.0[0m
[32m[2022-09-15 17:41:52,091] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-800[0m
[32m[2022-09-15 17:41:52,091] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 17:42:04,293] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-800/tokenizer_config.json[0m
[32m[2022-09-15 17:42:04,293] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-800/special_tokens_map.json[0m
[32m[2022-09-15 17:42:31,641] [    INFO][0m - loss: 6e-08, learning_rate: 5.7000000000000005e-06, global_step: 810, interval_runtime: 42.7414, interval_samples_per_second: 0.374, interval_steps_per_second: 0.234, epoch: 81.0[0m
[32m[2022-09-15 17:42:39,718] [    INFO][0m - loss: 1.8e-07, learning_rate: 5.4e-06, global_step: 820, interval_runtime: 8.0777, interval_samples_per_second: 1.981, interval_steps_per_second: 1.238, epoch: 82.0[0m
[32m[2022-09-15 17:42:47,792] [    INFO][0m - loss: 1.6e-07, learning_rate: 5.1e-06, global_step: 830, interval_runtime: 8.074, interval_samples_per_second: 1.982, interval_steps_per_second: 1.239, epoch: 83.0[0m
[32m[2022-09-15 17:42:55,865] [    INFO][0m - loss: 1.3e-07, learning_rate: 4.800000000000001e-06, global_step: 840, interval_runtime: 8.0733, interval_samples_per_second: 1.982, interval_steps_per_second: 1.239, epoch: 84.0[0m
[32m[2022-09-15 17:43:03,955] [    INFO][0m - loss: 9e-08, learning_rate: 4.5e-06, global_step: 850, interval_runtime: 8.09, interval_samples_per_second: 1.978, interval_steps_per_second: 1.236, epoch: 85.0[0m
[32m[2022-09-15 17:43:12,028] [    INFO][0m - loss: 1.1e-07, learning_rate: 4.2000000000000004e-06, global_step: 860, interval_runtime: 8.0731, interval_samples_per_second: 1.982, interval_steps_per_second: 1.239, epoch: 86.0[0m
[32m[2022-09-15 17:43:20,105] [    INFO][0m - loss: 1.4e-07, learning_rate: 3.9e-06, global_step: 870, interval_runtime: 8.0766, interval_samples_per_second: 1.981, interval_steps_per_second: 1.238, epoch: 87.0[0m
[32m[2022-09-15 17:43:28,201] [    INFO][0m - loss: 1.3e-07, learning_rate: 3.6e-06, global_step: 880, interval_runtime: 8.0958, interval_samples_per_second: 1.976, interval_steps_per_second: 1.235, epoch: 88.0[0m
[32m[2022-09-15 17:43:36,540] [    INFO][0m - loss: 8e-08, learning_rate: 3.3e-06, global_step: 890, interval_runtime: 8.3397, interval_samples_per_second: 1.919, interval_steps_per_second: 1.199, epoch: 89.0[0m
[32m[2022-09-15 17:43:44,937] [    INFO][0m - loss: 9e-08, learning_rate: 3e-06, global_step: 900, interval_runtime: 8.3965, interval_samples_per_second: 1.906, interval_steps_per_second: 1.191, epoch: 90.0[0m
[32m[2022-09-15 17:43:53,016] [    INFO][0m - loss: 1.3e-07, learning_rate: 2.7e-06, global_step: 910, interval_runtime: 8.0792, interval_samples_per_second: 1.98, interval_steps_per_second: 1.238, epoch: 91.0[0m
[32m[2022-09-15 17:44:01,149] [    INFO][0m - loss: 1.1e-07, learning_rate: 2.4000000000000003e-06, global_step: 920, interval_runtime: 8.1329, interval_samples_per_second: 1.967, interval_steps_per_second: 1.23, epoch: 92.0[0m
[32m[2022-09-15 17:44:09,219] [    INFO][0m - loss: 9e-08, learning_rate: 2.1000000000000002e-06, global_step: 930, interval_runtime: 8.0695, interval_samples_per_second: 1.983, interval_steps_per_second: 1.239, epoch: 93.0[0m
[32m[2022-09-15 17:44:17,293] [    INFO][0m - loss: 9e-08, learning_rate: 1.8e-06, global_step: 940, interval_runtime: 8.0747, interval_samples_per_second: 1.981, interval_steps_per_second: 1.238, epoch: 94.0[0m
[32m[2022-09-15 17:44:25,370] [    INFO][0m - loss: 2e-07, learning_rate: 1.5e-06, global_step: 950, interval_runtime: 8.0761, interval_samples_per_second: 1.981, interval_steps_per_second: 1.238, epoch: 95.0[0m
[32m[2022-09-15 17:44:33,470] [    INFO][0m - loss: 1.1e-07, learning_rate: 1.2000000000000002e-06, global_step: 960, interval_runtime: 8.1005, interval_samples_per_second: 1.975, interval_steps_per_second: 1.234, epoch: 96.0[0m
[32m[2022-09-15 17:44:41,598] [    INFO][0m - loss: 1.3e-07, learning_rate: 9e-07, global_step: 970, interval_runtime: 8.1277, interval_samples_per_second: 1.969, interval_steps_per_second: 1.23, epoch: 97.0[0m
[32m[2022-09-15 17:44:49,712] [    INFO][0m - loss: 3.1e-07, learning_rate: 6.000000000000001e-07, global_step: 980, interval_runtime: 8.1136, interval_samples_per_second: 1.972, interval_steps_per_second: 1.232, epoch: 98.0[0m
[32m[2022-09-15 17:44:57,838] [    INFO][0m - loss: 1e-07, learning_rate: 3.0000000000000004e-07, global_step: 990, interval_runtime: 8.1268, interval_samples_per_second: 1.969, interval_steps_per_second: 1.23, epoch: 99.0[0m
[32m[2022-09-15 17:45:05,970] [    INFO][0m - loss: 8e-08, learning_rate: 0.0, global_step: 1000, interval_runtime: 8.1323, interval_samples_per_second: 1.967, interval_steps_per_second: 1.23, epoch: 100.0[0m
[32m[2022-09-15 17:45:05,971] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-15 17:45:05,971] [    INFO][0m -   Num examples = 160[0m
[32m[2022-09-15 17:45:05,971] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 17:45:05,971] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 17:45:05,971] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-09-15 17:45:09,178] [    INFO][0m - eval_loss: 2.2397565841674805, eval_accuracy: 0.6875, eval_runtime: 3.2067, eval_samples_per_second: 49.896, eval_steps_per_second: 3.118, epoch: 100.0[0m
[32m[2022-09-15 17:45:09,178] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1000[0m
[32m[2022-09-15 17:45:09,179] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 17:45:15,497] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1000/tokenizer_config.json[0m
[32m[2022-09-15 17:45:15,497] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1000/special_tokens_map.json[0m
[32m[2022-09-15 17:45:27,670] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-15 17:45:27,670] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-600 (score: 0.6875).[0m
[32m[2022-09-15 17:45:29,397] [    INFO][0m - train_runtime: 946.1285, train_samples_per_second: 16.911, train_steps_per_second: 1.057, train_loss: 0.026281480888375255, epoch: 100.0[0m
[32m[2022-09-15 17:45:29,398] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-09-15 17:45:29,399] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-15 17:45:35,891] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-09-15 17:45:35,891] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-09-15 17:45:35,893] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-15 17:45:35,893] [    INFO][0m -   epoch                    =      100.0[0m
[32m[2022-09-15 17:45:35,893] [    INFO][0m -   train_loss               =     0.0263[0m
[32m[2022-09-15 17:45:35,893] [    INFO][0m -   train_runtime            = 0:15:46.12[0m
[32m[2022-09-15 17:45:35,894] [    INFO][0m -   train_samples_per_second =     16.911[0m
[32m[2022-09-15 17:45:35,894] [    INFO][0m -   train_steps_per_second   =      1.057[0m
[32m[2022-09-15 17:45:35,898] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-15 17:45:35,898] [    INFO][0m -   Num examples = 2838[0m
[32m[2022-09-15 17:45:35,899] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 17:45:35,899] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 17:45:35,899] [    INFO][0m -   Total prediction steps = 178[0m
[32m[2022-09-15 17:46:32,791] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-15 17:46:32,791] [    INFO][0m -   test_accuracy           =     0.6159[0m
[32m[2022-09-15 17:46:32,792] [    INFO][0m -   test_loss               =     2.7905[0m
[32m[2022-09-15 17:46:32,793] [    INFO][0m -   test_runtime            = 0:00:56.89[0m
[32m[2022-09-15 17:46:32,793] [    INFO][0m -   test_samples_per_second =     49.884[0m
[32m[2022-09-15 17:46:32,793] [    INFO][0m -   test_steps_per_second   =      3.129[0m
[32m[2022-09-15 17:46:32,794] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-15 17:46:32,794] [    INFO][0m -   Num examples = 3000[0m
[32m[2022-09-15 17:46:32,794] [    INFO][0m -   Pre device batch size = 16[0m
[32m[2022-09-15 17:46:32,794] [    INFO][0m -   Total Batch size = 16[0m
[32m[2022-09-15 17:46:32,794] [    INFO][0m -   Total prediction steps = 188[0m
[32m[2022-09-15 17:47:40,964] [    INFO][0m - Predictions for cslf saved to ./fewclue_submit_examples.[0m
{
  "labels": 1,
  "text_a": "\u65e0\u7ebf\u4f20\u611f\u5668\u7f51\u7edc\u4e2d\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u901a\u7528\u8fd1\u4f3c\u67e5\u8be2\u662f\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\uff0e\u6587\u4e2d\u63d0\u51fa\u4e00\u79cd\u65e0\u7ebf\u4f20\u611f\u5668\u7f51\u7edc\u4e2d\u9690\u79c1\u4fdd\u62a4\u901a\u7528\u8fd1\u4f3c\u67e5\u8be2\u534f\u8baePGAQ\uff0ePGAQ\u5c06\u4f20\u611f\u5668\u8282\u70b9\u7f16\u53f7\u548c\u5176\u91c7\u96c6\u6570\u636e\u9690\u85cf\u4e8e\u8bbe\u8ba1\u7684\u6570\u636e\u7ed3\u6784\u4e2d\uff0c\u5728\u57fa\u7ad9\u6784\u9020\u7ebf\u6027\u65b9\u7a0b\u7ec4\u89e3\u51fa\u76f4\u65b9\u56fe\uff0c\u6839\u636e\u76f4\u65b9\u56fe\u5177\u6709\u7684\u7edf\u8ba1\u4fe1\u606f\uff0c\u4e0d\u6cc4\u9732\u9690\u79c1\u5730\u5b8c\u6210Top-k\u67e5\u8be2\u3001\u8303\u56f4\u67e5\u8be2\u3001SUM\u3001MAX/MIN\u3001Median\u3001Histogram\u7b49\u8fd1\u4f3c\u67e5\u8be2\uff0ePGAQ\u4f7f\u7528\u7f51\u5185\u6c42\u548c\u805a\u96c6\u4ee5\u51cf\u5c11\u80fd\u91cf\u6d88\u8017\uff0c\u5e76\u4e14\u80fd\u591f\u901a\u8fc7\u8c03\u8282\u76f4\u65b9\u56fe\u5212\u5206\u7c92\u5ea6\u6765\u5e73\u8861\u67e5\u8be2\u7cbe\u5ea6\u4e0e\u80fd\u91cf\u6d88\u8017\uff0ePGAQ\u534f\u8bae\u5206\u4e3aH-PGAQ\u548cF-PGAQ\u4e24\u79cd\u6a21\u5f0f\uff0eH-PGAQ\u6a21\u5f0f\u4f7f\u7528\u6570\u636e\u6270\u52a8\u6280\u672f\u52a0\u5f3a\u6570\u636e\u5b89\u5168\u6027\uff0cF-PGAQ\u4f7f\u7528\u8fc7\u6ee4\u5668\u51cf\u5c11\u8fde\u7eed\u67e5\u8be2\u901a\u4fe1\u91cf\uff0e\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u4f7f\u7528\u771f\u5b9e\u6570\u636e\u96c6\u5b9e\u9a8c\u9a8c\u8bc1\u4e86PGAQ\u7684\u5b89\u5168\u6027\u548c\u6709\u6548\u6027\uff0e",
  "text_b": "\u65e0\u7ebf\u4f20\u611f\u5668\u7f51\u7edc,\u6570\u636e\u805a\u96c6,\u7269\u8054\u7f51,\u8fd1\u4f3c\u67e5\u8be2",
  "uid": 0
}

