 
==========
ocnli
==========
 
[33m[2022-08-29 19:59:15,830] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-29 19:59:15,830] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-29 19:59:15,830] [    INFO][0m - ============================================================[0m
[32m[2022-08-29 19:59:15,831] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-29 19:59:15,831] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-29 19:59:15,831] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-29 19:59:15,831] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-29 19:59:15,831] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-08-29 19:59:15,831] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-29 19:59:15,831] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-08-29 19:59:15,831] [    INFO][0m - [0m
[32m[2022-08-29 19:59:15,831] [    INFO][0m - ============================================================[0m
[32m[2022-08-29 19:59:15,831] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-29 19:59:15,831] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-29 19:59:15,831] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-29 19:59:15,831] [    INFO][0m - prompt                        :{'text':'text_a'}{'sep'}{'text':'text_b'}[0m
[32m[2022-08-29 19:59:15,832] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-29 19:59:15,832] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-29 19:59:15,832] [    INFO][0m - task_name                     :ocnli[0m
[32m[2022-08-29 19:59:15,832] [    INFO][0m - [0m
[32m[2022-08-29 19:59:15,832] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0829 19:59:15.833547 22955 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0829 19:59:15.837661 22955 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-29 19:59:18,607] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-08-29 19:59:18,633] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-08-29 19:59:18,634] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[33m[2022-08-29 19:59:18,641] [ WARNING][0m - Encoder has already set as lstm, change `prompt_encoder` will reset parameters.[0m
[33m[2022-08-29 19:59:18,646] [ WARNING][0m - No soft tokens in template. Use ManualTemplate for better performance.[0m
[33m[2022-08-29 19:59:18,646] [ WARNING][0m - No soft tokens in template. Use ManualTemplate for better performance.[0m
[32m[2022-08-29 19:59:18,646] [    INFO][0m - Using template: [{'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'sep': None}, {'add_prefix_space': '', 'text': 'text_b'}][0m
2022-08-29 19:59:18,648 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-29 19:59:18,777] [    INFO][0m - ============================================================[0m
[32m[2022-08-29 19:59:18,778] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-29 19:59:18,778] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-29 19:59:18,778] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-29 19:59:18,778] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-29 19:59:18,778] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-29 19:59:18,778] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-29 19:59:18,778] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-29 19:59:18,778] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-29 19:59:18,779] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-29 19:59:18,779] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-29 19:59:18,779] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-29 19:59:18,779] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-29 19:59:18,779] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-29 19:59:18,779] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-29 19:59:18,779] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-29 19:59:18,779] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-29 19:59:18,779] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-29 19:59:18,779] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-29 19:59:18,779] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-29 19:59:18,779] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-29 19:59:18,779] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-29 19:59:18,780] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-29 19:59:18,780] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-29 19:59:18,780] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-29 19:59:18,780] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-29 19:59:18,780] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-29 19:59:18,780] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-29 19:59:18,780] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-29 19:59:18,780] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-29 19:59:18,780] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-29 19:59:18,780] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-29 19:59:18,780] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-29 19:59:18,780] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-29 19:59:18,780] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-29 19:59:18,780] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-29 19:59:18,781] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-29 19:59:18,781] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug29_19-59-15_instance-3bwob41y-01[0m
[32m[2022-08-29 19:59:18,781] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-29 19:59:18,781] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-29 19:59:18,781] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-29 19:59:18,781] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-29 19:59:18,781] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-29 19:59:18,781] [    INFO][0m - max_seq_length                :64[0m
[32m[2022-08-29 19:59:18,781] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-29 19:59:18,781] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-29 19:59:18,781] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-29 19:59:18,781] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-29 19:59:18,781] [    INFO][0m - num_train_epochs              :20.0[0m
[32m[2022-08-29 19:59:18,782] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-29 19:59:18,782] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-29 19:59:18,782] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-29 19:59:18,782] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-29 19:59:18,782] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-29 19:59:18,782] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-29 19:59:18,782] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-29 19:59:18,782] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-29 19:59:18,782] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-29 19:59:18,782] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-29 19:59:18,782] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-29 19:59:18,782] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-29 19:59:18,782] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-29 19:59:18,783] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-29 19:59:18,783] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-29 19:59:18,783] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-29 19:59:18,783] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-29 19:59:18,783] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-29 19:59:18,783] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-29 19:59:18,783] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-29 19:59:18,783] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-29 19:59:18,783] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-29 19:59:18,783] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-29 19:59:18,783] [    INFO][0m - seed                          :42[0m
[32m[2022-08-29 19:59:18,783] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-29 19:59:18,783] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-29 19:59:18,784] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-29 19:59:18,784] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-29 19:59:18,784] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-29 19:59:18,784] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-29 19:59:18,784] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-29 19:59:18,784] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-29 19:59:18,784] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-29 19:59:18,784] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-29 19:59:18,784] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-29 19:59:18,784] [    INFO][0m - [0m
[32m[2022-08-29 19:59:18,786] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-29 19:59:18,786] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-29 19:59:18,786] [    INFO][0m -   Num Epochs = 20[0m
[32m[2022-08-29 19:59:18,786] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-29 19:59:18,787] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-29 19:59:18,787] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-29 19:59:18,787] [    INFO][0m -   Total optimization steps = 400.0[0m
[32m[2022-08-29 19:59:18,787] [    INFO][0m -   Total num train samples = 3200[0m
[32m[2022-08-29 19:59:20,501] [    INFO][0m - loss: 1.15352068, learning_rate: 2.925e-05, global_step: 10, interval_runtime: 1.7136, interval_samples_per_second: 4.669, interval_steps_per_second: 5.836, epoch: 0.5[0m
[32m[2022-08-29 19:59:21,032] [    INFO][0m - loss: 1.17087755, learning_rate: 2.8499999999999998e-05, global_step: 20, interval_runtime: 0.5307, interval_samples_per_second: 15.074, interval_steps_per_second: 18.842, epoch: 1.0[0m
[32m[2022-08-29 19:59:21,667] [    INFO][0m - loss: 1.05603161, learning_rate: 2.7750000000000004e-05, global_step: 30, interval_runtime: 0.6347, interval_samples_per_second: 12.604, interval_steps_per_second: 15.755, epoch: 1.5[0m
[32m[2022-08-29 19:59:22,224] [    INFO][0m - loss: 1.09234533, learning_rate: 2.7000000000000002e-05, global_step: 40, interval_runtime: 0.5574, interval_samples_per_second: 14.352, interval_steps_per_second: 17.94, epoch: 2.0[0m
[32m[2022-08-29 19:59:22,889] [    INFO][0m - loss: 1.02434368, learning_rate: 2.625e-05, global_step: 50, interval_runtime: 0.6648, interval_samples_per_second: 12.034, interval_steps_per_second: 15.043, epoch: 2.5[0m
[32m[2022-08-29 19:59:23,449] [    INFO][0m - loss: 1.03351603, learning_rate: 2.55e-05, global_step: 60, interval_runtime: 0.5608, interval_samples_per_second: 14.266, interval_steps_per_second: 17.833, epoch: 3.0[0m
[32m[2022-08-29 19:59:24,113] [    INFO][0m - loss: 0.93115788, learning_rate: 2.475e-05, global_step: 70, interval_runtime: 0.6633, interval_samples_per_second: 12.061, interval_steps_per_second: 15.076, epoch: 3.5[0m
[32m[2022-08-29 19:59:24,654] [    INFO][0m - loss: 0.91170168, learning_rate: 2.4e-05, global_step: 80, interval_runtime: 0.541, interval_samples_per_second: 14.787, interval_steps_per_second: 18.484, epoch: 4.0[0m
[32m[2022-08-29 19:59:25,353] [    INFO][0m - loss: 0.7852541, learning_rate: 2.3250000000000003e-05, global_step: 90, interval_runtime: 0.6992, interval_samples_per_second: 11.442, interval_steps_per_second: 14.302, epoch: 4.5[0m
[32m[2022-08-29 19:59:25,912] [    INFO][0m - loss: 0.7955327, learning_rate: 2.25e-05, global_step: 100, interval_runtime: 0.5595, interval_samples_per_second: 14.299, interval_steps_per_second: 17.874, epoch: 5.0[0m
[32m[2022-08-29 19:59:25,913] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-29 19:59:25,913] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-29 19:59:25,913] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-29 19:59:25,913] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-29 19:59:25,913] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-29 19:59:26,315] [    INFO][0m - eval_loss: 1.2584311962127686, eval_accuracy: 0.34375, eval_runtime: 0.4017, eval_samples_per_second: 398.356, eval_steps_per_second: 12.449, epoch: 5.0[0m
[32m[2022-08-29 19:59:26,315] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-29 19:59:26,316] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-29 19:59:29,329] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-29 19:59:29,329] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-29 19:59:33,805] [    INFO][0m - loss: 0.76472058, learning_rate: 2.175e-05, global_step: 110, interval_runtime: 7.8922, interval_samples_per_second: 1.014, interval_steps_per_second: 1.267, epoch: 5.5[0m
[32m[2022-08-29 19:59:34,394] [    INFO][0m - loss: 0.78428445, learning_rate: 2.1e-05, global_step: 120, interval_runtime: 0.5894, interval_samples_per_second: 13.573, interval_steps_per_second: 16.966, epoch: 6.0[0m
[32m[2022-08-29 19:59:35,182] [    INFO][0m - loss: 0.65805006, learning_rate: 2.025e-05, global_step: 130, interval_runtime: 0.7883, interval_samples_per_second: 10.149, interval_steps_per_second: 12.686, epoch: 6.5[0m
[32m[2022-08-29 19:59:35,779] [    INFO][0m - loss: 0.56142635, learning_rate: 1.95e-05, global_step: 140, interval_runtime: 0.5962, interval_samples_per_second: 13.417, interval_steps_per_second: 16.772, epoch: 7.0[0m
[32m[2022-08-29 19:59:36,604] [    INFO][0m - loss: 0.63387198, learning_rate: 1.8750000000000002e-05, global_step: 150, interval_runtime: 0.8255, interval_samples_per_second: 9.691, interval_steps_per_second: 12.114, epoch: 7.5[0m
[32m[2022-08-29 19:59:37,211] [    INFO][0m - loss: 0.44552312, learning_rate: 1.8e-05, global_step: 160, interval_runtime: 0.6072, interval_samples_per_second: 13.175, interval_steps_per_second: 16.468, epoch: 8.0[0m
[32m[2022-08-29 19:59:38,047] [    INFO][0m - loss: 0.39324164, learning_rate: 1.725e-05, global_step: 170, interval_runtime: 0.836, interval_samples_per_second: 9.569, interval_steps_per_second: 11.962, epoch: 8.5[0m
[32m[2022-08-29 19:59:38,655] [    INFO][0m - loss: 0.26949291, learning_rate: 1.65e-05, global_step: 180, interval_runtime: 0.6076, interval_samples_per_second: 13.166, interval_steps_per_second: 16.457, epoch: 9.0[0m
[32m[2022-08-29 19:59:39,537] [    INFO][0m - loss: 0.31787703, learning_rate: 1.575e-05, global_step: 190, interval_runtime: 0.8823, interval_samples_per_second: 9.067, interval_steps_per_second: 11.333, epoch: 9.5[0m
[32m[2022-08-29 19:59:40,165] [    INFO][0m - loss: 0.20640471, learning_rate: 1.5e-05, global_step: 200, interval_runtime: 0.6272, interval_samples_per_second: 12.755, interval_steps_per_second: 15.943, epoch: 10.0[0m
[32m[2022-08-29 19:59:40,165] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-29 19:59:40,165] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-29 19:59:40,165] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-29 19:59:40,165] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-29 19:59:40,165] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-29 19:59:40,674] [    INFO][0m - eval_loss: 1.4680458307266235, eval_accuracy: 0.43125, eval_runtime: 0.5079, eval_samples_per_second: 315.022, eval_steps_per_second: 9.844, epoch: 10.0[0m
[32m[2022-08-29 19:59:40,675] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-29 19:59:40,675] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-29 19:59:43,707] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-29 19:59:43,708] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-29 19:59:48,347] [    INFO][0m - loss: 0.30407636, learning_rate: 1.4249999999999999e-05, global_step: 210, interval_runtime: 8.1817, interval_samples_per_second: 0.978, interval_steps_per_second: 1.222, epoch: 10.5[0m
[32m[2022-08-29 19:59:49,121] [    INFO][0m - loss: 0.20184469, learning_rate: 1.3500000000000001e-05, global_step: 220, interval_runtime: 0.7751, interval_samples_per_second: 10.322, interval_steps_per_second: 12.902, epoch: 11.0[0m
[32m[2022-08-29 19:59:50,197] [    INFO][0m - loss: 0.10568732, learning_rate: 1.275e-05, global_step: 230, interval_runtime: 1.0754, interval_samples_per_second: 7.439, interval_steps_per_second: 9.299, epoch: 11.5[0m
[32m[2022-08-29 19:59:50,869] [    INFO][0m - loss: 0.15207371, learning_rate: 1.2e-05, global_step: 240, interval_runtime: 0.6721, interval_samples_per_second: 11.903, interval_steps_per_second: 14.879, epoch: 12.0[0m
[32m[2022-08-29 19:59:51,885] [    INFO][0m - loss: 0.12712252, learning_rate: 1.125e-05, global_step: 250, interval_runtime: 1.0159, interval_samples_per_second: 7.875, interval_steps_per_second: 9.843, epoch: 12.5[0m
[32m[2022-08-29 19:59:52,570] [    INFO][0m - loss: 0.05397452, learning_rate: 1.05e-05, global_step: 260, interval_runtime: 0.685, interval_samples_per_second: 11.679, interval_steps_per_second: 14.599, epoch: 13.0[0m
[32m[2022-08-29 19:59:53,608] [    INFO][0m - loss: 0.05465897, learning_rate: 9.75e-06, global_step: 270, interval_runtime: 1.0383, interval_samples_per_second: 7.705, interval_steps_per_second: 9.631, epoch: 13.5[0m
[32m[2022-08-29 19:59:54,308] [    INFO][0m - loss: 0.03504953, learning_rate: 9e-06, global_step: 280, interval_runtime: 0.7002, interval_samples_per_second: 11.426, interval_steps_per_second: 14.282, epoch: 14.0[0m
[32m[2022-08-29 19:59:55,425] [    INFO][0m - loss: 0.04828208, learning_rate: 8.25e-06, global_step: 290, interval_runtime: 1.1168, interval_samples_per_second: 7.164, interval_steps_per_second: 8.954, epoch: 14.5[0m
[32m[2022-08-29 19:59:56,130] [    INFO][0m - loss: 0.01766341, learning_rate: 7.5e-06, global_step: 300, interval_runtime: 0.705, interval_samples_per_second: 11.348, interval_steps_per_second: 14.185, epoch: 15.0[0m
[32m[2022-08-29 19:59:56,130] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-29 19:59:56,131] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-29 19:59:56,131] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-29 19:59:56,131] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-29 19:59:56,131] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-29 19:59:56,784] [    INFO][0m - eval_loss: 2.2048568725585938, eval_accuracy: 0.425, eval_runtime: 0.6536, eval_samples_per_second: 244.811, eval_steps_per_second: 7.65, epoch: 15.0[0m
[32m[2022-08-29 19:59:56,785] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-29 19:59:56,785] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-29 19:59:59,924] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-29 19:59:59,925] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-29 20:00:04,568] [    INFO][0m - loss: 0.02859311, learning_rate: 6.750000000000001e-06, global_step: 310, interval_runtime: 8.4376, interval_samples_per_second: 0.948, interval_steps_per_second: 1.185, epoch: 15.5[0m
[32m[2022-08-29 20:00:05,268] [    INFO][0m - loss: 0.01841011, learning_rate: 6e-06, global_step: 320, interval_runtime: 0.7004, interval_samples_per_second: 11.423, interval_steps_per_second: 14.278, epoch: 16.0[0m
[32m[2022-08-29 20:00:06,391] [    INFO][0m - loss: 0.00887614, learning_rate: 5.25e-06, global_step: 330, interval_runtime: 1.1233, interval_samples_per_second: 7.122, interval_steps_per_second: 8.902, epoch: 16.5[0m
[32m[2022-08-29 20:00:07,108] [    INFO][0m - loss: 0.00877618, learning_rate: 4.5e-06, global_step: 340, interval_runtime: 0.7169, interval_samples_per_second: 11.159, interval_steps_per_second: 13.949, epoch: 17.0[0m
[32m[2022-08-29 20:00:08,286] [    INFO][0m - loss: 0.01401017, learning_rate: 3.75e-06, global_step: 350, interval_runtime: 1.178, interval_samples_per_second: 6.791, interval_steps_per_second: 8.489, epoch: 17.5[0m
[32m[2022-08-29 20:00:09,026] [    INFO][0m - loss: 0.0116537, learning_rate: 3e-06, global_step: 360, interval_runtime: 0.7395, interval_samples_per_second: 10.818, interval_steps_per_second: 13.522, epoch: 18.0[0m
[32m[2022-08-29 20:00:10,228] [    INFO][0m - loss: 0.01141552, learning_rate: 2.25e-06, global_step: 370, interval_runtime: 1.2019, interval_samples_per_second: 6.656, interval_steps_per_second: 8.32, epoch: 18.5[0m
[32m[2022-08-29 20:00:10,972] [    INFO][0m - loss: 0.00636376, learning_rate: 1.5e-06, global_step: 380, interval_runtime: 0.7441, interval_samples_per_second: 10.752, interval_steps_per_second: 13.44, epoch: 19.0[0m
[32m[2022-08-29 20:00:12,212] [    INFO][0m - loss: 0.01419642, learning_rate: 7.5e-07, global_step: 390, interval_runtime: 1.24, interval_samples_per_second: 6.452, interval_steps_per_second: 8.064, epoch: 19.5[0m
[32m[2022-08-29 20:00:12,984] [    INFO][0m - loss: 0.00706406, learning_rate: 0.0, global_step: 400, interval_runtime: 0.7719, interval_samples_per_second: 10.364, interval_steps_per_second: 12.955, epoch: 20.0[0m
[32m[2022-08-29 20:00:12,984] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-29 20:00:12,984] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-29 20:00:12,984] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-29 20:00:12,984] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-29 20:00:12,984] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-29 20:00:13,728] [    INFO][0m - eval_loss: 2.5553271770477295, eval_accuracy: 0.41875, eval_runtime: 0.744, eval_samples_per_second: 215.051, eval_steps_per_second: 6.72, epoch: 20.0[0m
[32m[2022-08-29 20:00:13,729] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-29 20:00:13,729] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-29 20:00:17,076] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-29 20:00:17,077] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-29 20:00:20,913] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-29 20:00:20,914] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-200 (score: 0.43125).[0m
[32m[2022-08-29 20:00:21,992] [    INFO][0m - train_runtime: 63.2045, train_samples_per_second: 50.629, train_steps_per_second: 6.329, train_loss: 0.4054741589538753, epoch: 20.0[0m
[32m[2022-08-29 20:00:21,993] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-29 20:00:21,994] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-29 20:00:25,013] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-29 20:00:25,013] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-29 20:00:25,014] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-29 20:00:25,014] [    INFO][0m -   epoch                    =       20.0[0m
[32m[2022-08-29 20:00:25,014] [    INFO][0m -   train_loss               =     0.4055[0m
[32m[2022-08-29 20:00:25,014] [    INFO][0m -   train_runtime            = 0:01:03.20[0m
[32m[2022-08-29 20:00:25,015] [    INFO][0m -   train_samples_per_second =     50.629[0m
[32m[2022-08-29 20:00:25,015] [    INFO][0m -   train_steps_per_second   =      6.329[0m
[32m[2022-08-29 20:00:25,017] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-29 20:00:25,017] [    INFO][0m -   Num examples = 2520[0m
[32m[2022-08-29 20:00:25,017] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-29 20:00:25,018] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-29 20:00:25,018] [    INFO][0m -   Total prediction steps = 79[0m
[32m[2022-08-29 20:00:37,395] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-29 20:00:37,395] [    INFO][0m -   test_accuracy           =     0.3754[0m
[32m[2022-08-29 20:00:37,396] [    INFO][0m -   test_loss               =     1.6322[0m
[32m[2022-08-29 20:00:37,396] [    INFO][0m -   test_runtime            = 0:00:12.37[0m
[32m[2022-08-29 20:00:37,396] [    INFO][0m -   test_samples_per_second =    203.596[0m
[32m[2022-08-29 20:00:37,396] [    INFO][0m -   test_steps_per_second   =      6.383[0m
[32m[2022-08-29 20:00:37,396] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-29 20:00:37,396] [    INFO][0m -   Num examples = 3000[0m
[32m[2022-08-29 20:00:37,396] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-29 20:00:37,396] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-29 20:00:37,396] [    INFO][0m -   Total prediction steps = 94[0m
[32m[2022-08-29 20:00:55,550] [    INFO][0m - Predictions for ocnlif saved to ./fewclue_submit_examples.[0m
metric <function main.<locals>.compute_metrics at 0x7f6e2a9a2430>
 
==========
bustm
==========
 
[33m[2022-08-29 20:00:59,567] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-29 20:00:59,567] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-29 20:00:59,567] [    INFO][0m - ============================================================[0m
[32m[2022-08-29 20:00:59,567] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-29 20:00:59,567] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-29 20:00:59,567] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-29 20:00:59,568] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-29 20:00:59,568] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-08-29 20:00:59,568] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-29 20:00:59,568] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-08-29 20:00:59,568] [    INFO][0m - [0m
[32m[2022-08-29 20:00:59,568] [    INFO][0m - ============================================================[0m
[32m[2022-08-29 20:00:59,568] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-29 20:00:59,568] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-29 20:00:59,568] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-29 20:00:59,568] [    INFO][0m - prompt                        :{'text':'text_a'}{'sep'}{'text':'text_b'}[0m
[32m[2022-08-29 20:00:59,568] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-29 20:00:59,568] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-29 20:00:59,568] [    INFO][0m - task_name                     :bustm[0m
[32m[2022-08-29 20:00:59,568] [    INFO][0m - [0m
[32m[2022-08-29 20:00:59,569] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0829 20:00:59.570251 25142 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0829 20:00:59.574267 25142 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-29 20:01:02,211] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-08-29 20:01:02,235] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-08-29 20:01:02,235] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[33m[2022-08-29 20:01:02,242] [ WARNING][0m - Encoder has already set as lstm, change `prompt_encoder` will reset parameters.[0m
[33m[2022-08-29 20:01:02,247] [ WARNING][0m - No soft tokens in template. Use ManualTemplate for better performance.[0m
[33m[2022-08-29 20:01:02,248] [ WARNING][0m - No soft tokens in template. Use ManualTemplate for better performance.[0m
[32m[2022-08-29 20:01:02,248] [    INFO][0m - Using template: [{'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'sep': None}, {'add_prefix_space': '', 'text': 'text_b'}][0m
2022-08-29 20:01:02,249 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-29 20:01:02,356] [    INFO][0m - ============================================================[0m
[32m[2022-08-29 20:01:02,357] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-29 20:01:02,357] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-29 20:01:02,357] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-29 20:01:02,357] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-29 20:01:02,357] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-29 20:01:02,357] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-29 20:01:02,357] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-29 20:01:02,357] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-29 20:01:02,357] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-29 20:01:02,358] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-29 20:01:02,358] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-29 20:01:02,358] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-29 20:01:02,358] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-29 20:01:02,358] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-29 20:01:02,358] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-29 20:01:02,358] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-29 20:01:02,358] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-29 20:01:02,358] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-29 20:01:02,358] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-29 20:01:02,358] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-29 20:01:02,358] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-29 20:01:02,359] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-29 20:01:02,359] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-29 20:01:02,359] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-29 20:01:02,359] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-29 20:01:02,359] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-29 20:01:02,359] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-29 20:01:02,359] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-29 20:01:02,359] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-29 20:01:02,359] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-29 20:01:02,359] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-29 20:01:02,359] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-29 20:01:02,359] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-29 20:01:02,359] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-29 20:01:02,360] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-29 20:01:02,360] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-29 20:01:02,360] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug29_20-00-59_instance-3bwob41y-01[0m
[32m[2022-08-29 20:01:02,360] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-29 20:01:02,360] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-29 20:01:02,360] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-29 20:01:02,360] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-29 20:01:02,360] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-29 20:01:02,360] [    INFO][0m - max_seq_length                :40[0m
[32m[2022-08-29 20:01:02,360] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-29 20:01:02,360] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-29 20:01:02,360] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-29 20:01:02,360] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-29 20:01:02,361] [    INFO][0m - num_train_epochs              :20.0[0m
[32m[2022-08-29 20:01:02,361] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-29 20:01:02,361] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-29 20:01:02,361] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-29 20:01:02,361] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-29 20:01:02,361] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-29 20:01:02,361] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-29 20:01:02,361] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-29 20:01:02,361] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-29 20:01:02,361] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-29 20:01:02,361] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-29 20:01:02,361] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-29 20:01:02,361] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-29 20:01:02,362] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-29 20:01:02,362] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-29 20:01:02,362] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-29 20:01:02,362] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-29 20:01:02,362] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-29 20:01:02,362] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-29 20:01:02,362] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-29 20:01:02,362] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-29 20:01:02,362] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-29 20:01:02,362] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-29 20:01:02,362] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-29 20:01:02,362] [    INFO][0m - seed                          :42[0m
[32m[2022-08-29 20:01:02,362] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-29 20:01:02,363] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-29 20:01:02,363] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-29 20:01:02,363] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-29 20:01:02,363] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-29 20:01:02,363] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-29 20:01:02,363] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-29 20:01:02,363] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-29 20:01:02,363] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-29 20:01:02,363] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-29 20:01:02,363] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-29 20:01:02,363] [    INFO][0m - [0m
[32m[2022-08-29 20:01:02,365] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-29 20:01:02,365] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-29 20:01:02,365] [    INFO][0m -   Num Epochs = 20[0m
[32m[2022-08-29 20:01:02,366] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-29 20:01:02,366] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-29 20:01:02,366] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-29 20:01:02,366] [    INFO][0m -   Total optimization steps = 400.0[0m
[32m[2022-08-29 20:01:02,366] [    INFO][0m -   Total num train samples = 3200[0m
[32m[2022-08-29 20:01:04,037] [    INFO][0m - loss: 0.65911865, learning_rate: 2.925e-05, global_step: 10, interval_runtime: 1.6705, interval_samples_per_second: 4.789, interval_steps_per_second: 5.986, epoch: 0.5[0m
[32m[2022-08-29 20:01:04,561] [    INFO][0m - loss: 0.8104351, learning_rate: 2.8499999999999998e-05, global_step: 20, interval_runtime: 0.5205, interval_samples_per_second: 15.37, interval_steps_per_second: 19.213, epoch: 1.0[0m
[32m[2022-08-29 20:01:05,125] [    INFO][0m - loss: 0.68196239, learning_rate: 2.7750000000000004e-05, global_step: 30, interval_runtime: 0.5676, interval_samples_per_second: 14.094, interval_steps_per_second: 17.618, epoch: 1.5[0m
[32m[2022-08-29 20:01:05,636] [    INFO][0m - loss: 0.64967279, learning_rate: 2.7000000000000002e-05, global_step: 40, interval_runtime: 0.5111, interval_samples_per_second: 15.653, interval_steps_per_second: 19.566, epoch: 2.0[0m
[32m[2022-08-29 20:01:06,295] [    INFO][0m - loss: 0.5689857, learning_rate: 2.625e-05, global_step: 50, interval_runtime: 0.6587, interval_samples_per_second: 12.145, interval_steps_per_second: 15.181, epoch: 2.5[0m
[32m[2022-08-29 20:01:06,841] [    INFO][0m - loss: 0.54924836, learning_rate: 2.55e-05, global_step: 60, interval_runtime: 0.5424, interval_samples_per_second: 14.75, interval_steps_per_second: 18.437, epoch: 3.0[0m
[32m[2022-08-29 20:01:07,478] [    INFO][0m - loss: 0.49693747, learning_rate: 2.475e-05, global_step: 70, interval_runtime: 0.6408, interval_samples_per_second: 12.484, interval_steps_per_second: 15.605, epoch: 3.5[0m
[32m[2022-08-29 20:01:08,016] [    INFO][0m - loss: 0.45194941, learning_rate: 2.4e-05, global_step: 80, interval_runtime: 0.5371, interval_samples_per_second: 14.895, interval_steps_per_second: 18.618, epoch: 4.0[0m
[32m[2022-08-29 20:01:08,701] [    INFO][0m - loss: 0.37814064, learning_rate: 2.3250000000000003e-05, global_step: 90, interval_runtime: 0.6855, interval_samples_per_second: 11.671, interval_steps_per_second: 14.588, epoch: 4.5[0m
[32m[2022-08-29 20:01:09,270] [    INFO][0m - loss: 0.43267112, learning_rate: 2.25e-05, global_step: 100, interval_runtime: 0.5696, interval_samples_per_second: 14.046, interval_steps_per_second: 17.557, epoch: 5.0[0m
[32m[2022-08-29 20:01:09,271] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-29 20:01:09,271] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-29 20:01:09,271] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-29 20:01:09,271] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-29 20:01:09,271] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-29 20:01:09,567] [    INFO][0m - eval_loss: 0.8010618090629578, eval_accuracy: 0.66875, eval_runtime: 0.2956, eval_samples_per_second: 541.247, eval_steps_per_second: 16.914, epoch: 5.0[0m
[32m[2022-08-29 20:01:09,568] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-29 20:01:09,568] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-29 20:01:12,619] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-29 20:01:12,620] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-29 20:01:17,298] [    INFO][0m - loss: 0.24653561, learning_rate: 2.175e-05, global_step: 110, interval_runtime: 8.0272, interval_samples_per_second: 0.997, interval_steps_per_second: 1.246, epoch: 5.5[0m
[32m[2022-08-29 20:01:17,901] [    INFO][0m - loss: 0.24670947, learning_rate: 2.1e-05, global_step: 120, interval_runtime: 0.6037, interval_samples_per_second: 13.252, interval_steps_per_second: 16.565, epoch: 6.0[0m
[32m[2022-08-29 20:01:18,622] [    INFO][0m - loss: 0.16790847, learning_rate: 2.025e-05, global_step: 130, interval_runtime: 0.7213, interval_samples_per_second: 11.091, interval_steps_per_second: 13.864, epoch: 6.5[0m
[32m[2022-08-29 20:01:19,177] [    INFO][0m - loss: 0.1622655, learning_rate: 1.95e-05, global_step: 140, interval_runtime: 0.5538, interval_samples_per_second: 14.446, interval_steps_per_second: 18.057, epoch: 7.0[0m
[32m[2022-08-29 20:01:19,928] [    INFO][0m - loss: 0.07293144, learning_rate: 1.8750000000000002e-05, global_step: 150, interval_runtime: 0.7518, interval_samples_per_second: 10.642, interval_steps_per_second: 13.302, epoch: 7.5[0m
[32m[2022-08-29 20:01:20,528] [    INFO][0m - loss: 0.20405416, learning_rate: 1.8e-05, global_step: 160, interval_runtime: 0.6002, interval_samples_per_second: 13.328, interval_steps_per_second: 16.66, epoch: 8.0[0m
[32m[2022-08-29 20:01:21,354] [    INFO][0m - loss: 0.20915909, learning_rate: 1.725e-05, global_step: 170, interval_runtime: 0.8253, interval_samples_per_second: 9.693, interval_steps_per_second: 12.116, epoch: 8.5[0m
[32m[2022-08-29 20:01:21,974] [    INFO][0m - loss: 0.07344518, learning_rate: 1.65e-05, global_step: 180, interval_runtime: 0.6204, interval_samples_per_second: 12.896, interval_steps_per_second: 16.12, epoch: 9.0[0m
[32m[2022-08-29 20:01:22,823] [    INFO][0m - loss: 0.04759061, learning_rate: 1.575e-05, global_step: 190, interval_runtime: 0.8488, interval_samples_per_second: 9.426, interval_steps_per_second: 11.782, epoch: 9.5[0m
[32m[2022-08-29 20:01:23,447] [    INFO][0m - loss: 0.08452337, learning_rate: 1.5e-05, global_step: 200, interval_runtime: 0.6241, interval_samples_per_second: 12.818, interval_steps_per_second: 16.022, epoch: 10.0[0m
[32m[2022-08-29 20:01:23,447] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-29 20:01:23,448] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-29 20:01:23,448] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-29 20:01:23,448] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-29 20:01:23,448] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-29 20:01:23,842] [    INFO][0m - eval_loss: 1.6865246295928955, eval_accuracy: 0.64375, eval_runtime: 0.3943, eval_samples_per_second: 405.761, eval_steps_per_second: 12.68, epoch: 10.0[0m
[32m[2022-08-29 20:01:23,843] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-29 20:01:23,843] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-29 20:01:27,200] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-29 20:01:27,201] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-29 20:01:32,074] [    INFO][0m - loss: 0.04583197, learning_rate: 1.4249999999999999e-05, global_step: 210, interval_runtime: 8.6267, interval_samples_per_second: 0.927, interval_steps_per_second: 1.159, epoch: 10.5[0m
[32m[2022-08-29 20:01:32,697] [    INFO][0m - loss: 0.03642942, learning_rate: 1.3500000000000001e-05, global_step: 220, interval_runtime: 0.6239, interval_samples_per_second: 12.823, interval_steps_per_second: 16.028, epoch: 11.0[0m
[32m[2022-08-29 20:01:33,657] [    INFO][0m - loss: 0.00870024, learning_rate: 1.275e-05, global_step: 230, interval_runtime: 0.9591, interval_samples_per_second: 8.341, interval_steps_per_second: 10.427, epoch: 11.5[0m
[32m[2022-08-29 20:01:34,295] [    INFO][0m - loss: 0.02891003, learning_rate: 1.2e-05, global_step: 240, interval_runtime: 0.6384, interval_samples_per_second: 12.532, interval_steps_per_second: 15.665, epoch: 12.0[0m
[32m[2022-08-29 20:01:35,233] [    INFO][0m - loss: 0.00385216, learning_rate: 1.125e-05, global_step: 250, interval_runtime: 0.9384, interval_samples_per_second: 8.525, interval_steps_per_second: 10.657, epoch: 12.5[0m
[32m[2022-08-29 20:01:35,882] [    INFO][0m - loss: 0.00335188, learning_rate: 1.05e-05, global_step: 260, interval_runtime: 0.6469, interval_samples_per_second: 12.367, interval_steps_per_second: 15.458, epoch: 13.0[0m
[32m[2022-08-29 20:01:36,854] [    INFO][0m - loss: 0.01391504, learning_rate: 9.75e-06, global_step: 270, interval_runtime: 0.9732, interval_samples_per_second: 8.22, interval_steps_per_second: 10.275, epoch: 13.5[0m
[32m[2022-08-29 20:01:37,506] [    INFO][0m - loss: 0.03936334, learning_rate: 9e-06, global_step: 280, interval_runtime: 0.652, interval_samples_per_second: 12.27, interval_steps_per_second: 15.338, epoch: 14.0[0m
[32m[2022-08-29 20:01:38,480] [    INFO][0m - loss: 0.040333, learning_rate: 8.25e-06, global_step: 290, interval_runtime: 0.9744, interval_samples_per_second: 8.21, interval_steps_per_second: 10.262, epoch: 14.5[0m
[32m[2022-08-29 20:01:39,130] [    INFO][0m - loss: 0.00152311, learning_rate: 7.5e-06, global_step: 300, interval_runtime: 0.6503, interval_samples_per_second: 12.302, interval_steps_per_second: 15.377, epoch: 15.0[0m
[32m[2022-08-29 20:01:39,131] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-29 20:01:39,131] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-29 20:01:39,131] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-29 20:01:39,131] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-29 20:01:39,131] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-29 20:01:39,625] [    INFO][0m - eval_loss: 2.153623342514038, eval_accuracy: 0.63125, eval_runtime: 0.4932, eval_samples_per_second: 324.434, eval_steps_per_second: 10.139, epoch: 15.0[0m
[32m[2022-08-29 20:01:39,625] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-29 20:01:39,626] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-29 20:01:42,951] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-29 20:01:42,951] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-29 20:01:47,824] [    INFO][0m - loss: 0.01122903, learning_rate: 6.750000000000001e-06, global_step: 310, interval_runtime: 8.6935, interval_samples_per_second: 0.92, interval_steps_per_second: 1.15, epoch: 15.5[0m
[32m[2022-08-29 20:01:48,498] [    INFO][0m - loss: 0.00212387, learning_rate: 6e-06, global_step: 320, interval_runtime: 0.674, interval_samples_per_second: 11.869, interval_steps_per_second: 14.837, epoch: 16.0[0m
[32m[2022-08-29 20:01:49,584] [    INFO][0m - loss: 0.00214064, learning_rate: 5.25e-06, global_step: 330, interval_runtime: 1.0864, interval_samples_per_second: 7.364, interval_steps_per_second: 9.204, epoch: 16.5[0m
[32m[2022-08-29 20:01:50,268] [    INFO][0m - loss: 0.00115862, learning_rate: 4.5e-06, global_step: 340, interval_runtime: 0.6836, interval_samples_per_second: 11.703, interval_steps_per_second: 14.629, epoch: 17.0[0m
[32m[2022-08-29 20:01:51,344] [    INFO][0m - loss: 0.00234718, learning_rate: 3.75e-06, global_step: 350, interval_runtime: 1.0758, interval_samples_per_second: 7.437, interval_steps_per_second: 9.296, epoch: 17.5[0m
[32m[2022-08-29 20:01:52,059] [    INFO][0m - loss: 0.0056648, learning_rate: 3e-06, global_step: 360, interval_runtime: 0.7153, interval_samples_per_second: 11.183, interval_steps_per_second: 13.979, epoch: 18.0[0m
[32m[2022-08-29 20:01:53,234] [    INFO][0m - loss: 0.00141953, learning_rate: 2.25e-06, global_step: 370, interval_runtime: 1.1746, interval_samples_per_second: 6.811, interval_steps_per_second: 8.514, epoch: 18.5[0m
[32m[2022-08-29 20:01:53,955] [    INFO][0m - loss: 0.00133966, learning_rate: 1.5e-06, global_step: 380, interval_runtime: 0.7214, interval_samples_per_second: 11.089, interval_steps_per_second: 13.861, epoch: 19.0[0m
[32m[2022-08-29 20:01:55,119] [    INFO][0m - loss: 0.00163269, learning_rate: 7.5e-07, global_step: 390, interval_runtime: 1.1645, interval_samples_per_second: 6.87, interval_steps_per_second: 8.588, epoch: 19.5[0m
[32m[2022-08-29 20:01:55,868] [    INFO][0m - loss: 0.00126889, learning_rate: 0.0, global_step: 400, interval_runtime: 0.7487, interval_samples_per_second: 10.685, interval_steps_per_second: 13.357, epoch: 20.0[0m
[32m[2022-08-29 20:01:55,869] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-29 20:01:55,869] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-29 20:01:55,869] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-29 20:01:55,869] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-29 20:01:55,870] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-29 20:01:56,481] [    INFO][0m - eval_loss: 2.292802095413208, eval_accuracy: 0.63125, eval_runtime: 0.6118, eval_samples_per_second: 261.536, eval_steps_per_second: 8.173, epoch: 20.0[0m
[32m[2022-08-29 20:01:56,482] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-29 20:01:56,482] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-29 20:01:59,514] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-29 20:01:59,515] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-29 20:02:03,334] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-29 20:02:03,335] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-100 (score: 0.66875).[0m
[32m[2022-08-29 20:02:04,327] [    INFO][0m - train_runtime: 61.9606, train_samples_per_second: 51.646, train_steps_per_second: 6.456, train_loss: 0.18616949138464406, epoch: 20.0[0m
[32m[2022-08-29 20:02:04,329] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-29 20:02:04,329] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-29 20:02:07,739] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-29 20:02:07,739] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-29 20:02:07,741] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-29 20:02:07,741] [    INFO][0m -   epoch                    =       20.0[0m
[32m[2022-08-29 20:02:07,741] [    INFO][0m -   train_loss               =     0.1862[0m
[32m[2022-08-29 20:02:07,741] [    INFO][0m -   train_runtime            = 0:01:01.96[0m
[32m[2022-08-29 20:02:07,741] [    INFO][0m -   train_samples_per_second =     51.646[0m
[32m[2022-08-29 20:02:07,741] [    INFO][0m -   train_steps_per_second   =      6.456[0m
[32m[2022-08-29 20:02:07,744] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-29 20:02:07,744] [    INFO][0m -   Num examples = 1772[0m
[32m[2022-08-29 20:02:07,745] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-29 20:02:07,745] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-29 20:02:07,745] [    INFO][0m -   Total prediction steps = 56[0m
[32m[2022-08-29 20:02:14,681] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-29 20:02:14,681] [    INFO][0m -   test_accuracy           =     0.6541[0m
[32m[2022-08-29 20:02:14,681] [    INFO][0m -   test_loss               =     0.8074[0m
[32m[2022-08-29 20:02:14,681] [    INFO][0m -   test_runtime            = 0:00:06.93[0m
[32m[2022-08-29 20:02:14,682] [    INFO][0m -   test_samples_per_second =    255.469[0m
[32m[2022-08-29 20:02:14,682] [    INFO][0m -   test_steps_per_second   =      8.074[0m
[32m[2022-08-29 20:02:14,682] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-29 20:02:14,682] [    INFO][0m -   Num examples = 2000[0m
[32m[2022-08-29 20:02:14,682] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-29 20:02:14,682] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-29 20:02:14,682] [    INFO][0m -   Total prediction steps = 63[0m
[32m[2022-08-29 20:02:23,803] [    INFO][0m - Predictions for bustm saved to ./fewclue_submit_examples.[0m
metric <function main.<locals>.compute_metrics at 0x7fcd8da90b80>
 
==========
cluewsc
==========
 
[33m[2022-08-29 20:02:27,589] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-29 20:02:27,589] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-29 20:02:27,589] [    INFO][0m - ============================================================[0m
[32m[2022-08-29 20:02:27,589] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-29 20:02:27,589] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-29 20:02:27,590] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-29 20:02:27,590] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-29 20:02:27,590] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-08-29 20:02:27,590] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-29 20:02:27,590] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-08-29 20:02:27,590] [    INFO][0m - [0m
[32m[2022-08-29 20:02:27,590] [    INFO][0m - ============================================================[0m
[32m[2022-08-29 20:02:27,590] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-29 20:02:27,590] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-29 20:02:27,590] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-29 20:02:27,590] [    INFO][0m - prompt                        :{'text':'text_a'}[0m
[32m[2022-08-29 20:02:27,590] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-29 20:02:27,590] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-29 20:02:27,591] [    INFO][0m - task_name                     :cluewsc[0m
[32m[2022-08-29 20:02:27,591] [    INFO][0m - [0m
[32m[2022-08-29 20:02:27,591] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0829 20:02:27.592357 27015 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0829 20:02:27.596560 27015 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-29 20:02:30,413] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-08-29 20:02:30,438] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-08-29 20:02:30,438] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[33m[2022-08-29 20:02:30,445] [ WARNING][0m - Encoder has already set as lstm, change `prompt_encoder` will reset parameters.[0m
[33m[2022-08-29 20:02:30,450] [ WARNING][0m - No soft tokens in template. Use ManualTemplate for better performance.[0m
[33m[2022-08-29 20:02:30,450] [ WARNING][0m - No soft tokens in template. Use ManualTemplate for better performance.[0m
[32m[2022-08-29 20:02:30,450] [    INFO][0m - Using template: [{'add_prefix_space': '', 'text': 'text_a'}][0m
2022-08-29 20:02:30,451 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-29 20:02:30,551] [    INFO][0m - ============================================================[0m
[32m[2022-08-29 20:02:30,551] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-29 20:02:30,551] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-29 20:02:30,551] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-29 20:02:30,551] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-29 20:02:30,551] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-29 20:02:30,551] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-29 20:02:30,551] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-29 20:02:30,551] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-29 20:02:30,552] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-29 20:02:30,552] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-29 20:02:30,552] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-29 20:02:30,552] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-29 20:02:30,552] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-29 20:02:30,552] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-29 20:02:30,552] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-29 20:02:30,552] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-29 20:02:30,552] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-29 20:02:30,552] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-29 20:02:30,552] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-29 20:02:30,552] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-29 20:02:30,552] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-29 20:02:30,553] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-29 20:02:30,553] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-29 20:02:30,553] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-29 20:02:30,553] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-29 20:02:30,553] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-29 20:02:30,553] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-29 20:02:30,553] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-29 20:02:30,553] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-29 20:02:30,553] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-29 20:02:30,553] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-29 20:02:30,553] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-29 20:02:30,553] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-29 20:02:30,553] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-29 20:02:30,553] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-29 20:02:30,554] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-29 20:02:30,554] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug29_20-02-27_instance-3bwob41y-01[0m
[32m[2022-08-29 20:02:30,554] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-29 20:02:30,554] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-29 20:02:30,554] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-29 20:02:30,554] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-29 20:02:30,554] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-29 20:02:30,554] [    INFO][0m - max_seq_length                :128[0m
[32m[2022-08-29 20:02:30,554] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-29 20:02:30,554] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-29 20:02:30,554] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-29 20:02:30,554] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-29 20:02:30,554] [    INFO][0m - num_train_epochs              :20.0[0m
[32m[2022-08-29 20:02:30,555] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-29 20:02:30,555] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-29 20:02:30,555] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-29 20:02:30,555] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-29 20:02:30,555] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-29 20:02:30,555] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-29 20:02:30,555] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-29 20:02:30,555] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-29 20:02:30,555] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-29 20:02:30,555] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-29 20:02:30,555] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-29 20:02:30,555] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-29 20:02:30,555] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-29 20:02:30,555] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-29 20:02:30,556] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-29 20:02:30,556] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-29 20:02:30,556] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-29 20:02:30,556] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-29 20:02:30,556] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-29 20:02:30,556] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-29 20:02:30,556] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-29 20:02:30,556] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-29 20:02:30,556] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-29 20:02:30,556] [    INFO][0m - seed                          :42[0m
[32m[2022-08-29 20:02:30,556] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-29 20:02:30,556] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-29 20:02:30,556] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-29 20:02:30,557] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-29 20:02:30,557] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-29 20:02:30,557] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-29 20:02:30,557] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-29 20:02:30,557] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-29 20:02:30,557] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-29 20:02:30,557] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-29 20:02:30,557] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-29 20:02:30,557] [    INFO][0m - [0m
[32m[2022-08-29 20:02:30,559] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-29 20:02:30,559] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-29 20:02:30,559] [    INFO][0m -   Num Epochs = 20[0m
[32m[2022-08-29 20:02:30,559] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-29 20:02:30,559] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-29 20:02:30,559] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-29 20:02:30,559] [    INFO][0m -   Total optimization steps = 400.0[0m
[32m[2022-08-29 20:02:30,560] [    INFO][0m -   Total num train samples = 3200[0m
[32m[2022-08-29 20:02:32,326] [    INFO][0m - loss: 0.72645397, learning_rate: 2.925e-05, global_step: 10, interval_runtime: 1.7657, interval_samples_per_second: 4.531, interval_steps_per_second: 5.663, epoch: 0.5[0m
[32m[2022-08-29 20:02:33,017] [    INFO][0m - loss: 0.70369897, learning_rate: 2.8499999999999998e-05, global_step: 20, interval_runtime: 0.683, interval_samples_per_second: 11.712, interval_steps_per_second: 14.641, epoch: 1.0[0m
[32m[2022-08-29 20:02:33,779] [    INFO][0m - loss: 0.6631361, learning_rate: 2.7750000000000004e-05, global_step: 30, interval_runtime: 0.7692, interval_samples_per_second: 10.4, interval_steps_per_second: 13.0, epoch: 1.5[0m
[32m[2022-08-29 20:02:34,467] [    INFO][0m - loss: 0.63101411, learning_rate: 2.7000000000000002e-05, global_step: 40, interval_runtime: 0.6888, interval_samples_per_second: 11.614, interval_steps_per_second: 14.518, epoch: 2.0[0m
[32m[2022-08-29 20:02:35,270] [    INFO][0m - loss: 0.62765088, learning_rate: 2.625e-05, global_step: 50, interval_runtime: 0.8024, interval_samples_per_second: 9.97, interval_steps_per_second: 12.462, epoch: 2.5[0m
[32m[2022-08-29 20:02:35,974] [    INFO][0m - loss: 0.63691664, learning_rate: 2.55e-05, global_step: 60, interval_runtime: 0.7045, interval_samples_per_second: 11.355, interval_steps_per_second: 14.194, epoch: 3.0[0m
[32m[2022-08-29 20:02:36,819] [    INFO][0m - loss: 0.4595993, learning_rate: 2.475e-05, global_step: 70, interval_runtime: 0.8452, interval_samples_per_second: 9.466, interval_steps_per_second: 11.832, epoch: 3.5[0m
[32m[2022-08-29 20:02:37,544] [    INFO][0m - loss: 0.57168088, learning_rate: 2.4e-05, global_step: 80, interval_runtime: 0.7243, interval_samples_per_second: 11.045, interval_steps_per_second: 13.807, epoch: 4.0[0m
[32m[2022-08-29 20:02:38,445] [    INFO][0m - loss: 0.43337531, learning_rate: 2.3250000000000003e-05, global_step: 90, interval_runtime: 0.9013, interval_samples_per_second: 8.876, interval_steps_per_second: 11.095, epoch: 4.5[0m
[32m[2022-08-29 20:02:39,175] [    INFO][0m - loss: 0.41496654, learning_rate: 2.25e-05, global_step: 100, interval_runtime: 0.7302, interval_samples_per_second: 10.955, interval_steps_per_second: 13.694, epoch: 5.0[0m
[32m[2022-08-29 20:02:39,176] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-29 20:02:39,176] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-29 20:02:39,176] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-29 20:02:39,176] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-29 20:02:39,176] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-29 20:02:39,822] [    INFO][0m - eval_loss: 0.9420862793922424, eval_accuracy: 0.4968553459119497, eval_runtime: 0.6455, eval_samples_per_second: 246.304, eval_steps_per_second: 7.745, epoch: 5.0[0m
[32m[2022-08-29 20:02:39,822] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-29 20:02:39,822] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-29 20:02:43,241] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-29 20:02:43,242] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-29 20:02:47,679] [    INFO][0m - loss: 0.40775661, learning_rate: 2.175e-05, global_step: 110, interval_runtime: 8.5034, interval_samples_per_second: 0.941, interval_steps_per_second: 1.176, epoch: 5.5[0m
[32m[2022-08-29 20:02:48,442] [    INFO][0m - loss: 0.35020759, learning_rate: 2.1e-05, global_step: 120, interval_runtime: 0.7632, interval_samples_per_second: 10.482, interval_steps_per_second: 13.102, epoch: 6.0[0m
[32m[2022-08-29 20:02:49,443] [    INFO][0m - loss: 0.4984005, learning_rate: 2.025e-05, global_step: 130, interval_runtime: 1.0018, interval_samples_per_second: 7.986, interval_steps_per_second: 9.982, epoch: 6.5[0m
[32m[2022-08-29 20:02:50,220] [    INFO][0m - loss: 0.37363679, learning_rate: 1.95e-05, global_step: 140, interval_runtime: 0.7763, interval_samples_per_second: 10.305, interval_steps_per_second: 12.882, epoch: 7.0[0m
[32m[2022-08-29 20:02:51,276] [    INFO][0m - loss: 0.54153347, learning_rate: 1.8750000000000002e-05, global_step: 150, interval_runtime: 1.0566, interval_samples_per_second: 7.572, interval_steps_per_second: 9.464, epoch: 7.5[0m
[32m[2022-08-29 20:02:52,073] [    INFO][0m - loss: 0.20728552, learning_rate: 1.8e-05, global_step: 160, interval_runtime: 0.7969, interval_samples_per_second: 10.039, interval_steps_per_second: 12.549, epoch: 8.0[0m
[32m[2022-08-29 20:02:53,191] [    INFO][0m - loss: 0.34995704, learning_rate: 1.725e-05, global_step: 170, interval_runtime: 1.1176, interval_samples_per_second: 7.158, interval_steps_per_second: 8.948, epoch: 8.5[0m
[32m[2022-08-29 20:02:54,004] [    INFO][0m - loss: 0.28831289, learning_rate: 1.65e-05, global_step: 180, interval_runtime: 0.8135, interval_samples_per_second: 9.834, interval_steps_per_second: 12.293, epoch: 9.0[0m
[32m[2022-08-29 20:02:55,163] [    INFO][0m - loss: 0.2992341, learning_rate: 1.575e-05, global_step: 190, interval_runtime: 1.1582, interval_samples_per_second: 6.907, interval_steps_per_second: 8.634, epoch: 9.5[0m
[32m[2022-08-29 20:02:55,989] [    INFO][0m - loss: 0.34201808, learning_rate: 1.5e-05, global_step: 200, interval_runtime: 0.8262, interval_samples_per_second: 9.683, interval_steps_per_second: 12.104, epoch: 10.0[0m
[32m[2022-08-29 20:02:55,990] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-29 20:02:55,990] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-29 20:02:55,990] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-29 20:02:55,990] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-29 20:02:55,990] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-29 20:02:56,849] [    INFO][0m - eval_loss: 1.2079410552978516, eval_accuracy: 0.5220125786163522, eval_runtime: 0.8584, eval_samples_per_second: 185.23, eval_steps_per_second: 5.825, epoch: 10.0[0m
[32m[2022-08-29 20:02:56,853] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-29 20:02:56,854] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-29 20:03:00,223] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-29 20:03:00,224] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-29 20:03:05,137] [    INFO][0m - loss: 0.22457786, learning_rate: 1.4249999999999999e-05, global_step: 210, interval_runtime: 9.1475, interval_samples_per_second: 0.875, interval_steps_per_second: 1.093, epoch: 10.5[0m
[32m[2022-08-29 20:03:05,956] [    INFO][0m - loss: 0.42309031, learning_rate: 1.3500000000000001e-05, global_step: 220, interval_runtime: 0.8201, interval_samples_per_second: 9.755, interval_steps_per_second: 12.193, epoch: 11.0[0m
[32m[2022-08-29 20:03:07,165] [    INFO][0m - loss: 0.33262067, learning_rate: 1.275e-05, global_step: 230, interval_runtime: 1.2081, interval_samples_per_second: 6.622, interval_steps_per_second: 8.277, epoch: 11.5[0m
[32m[2022-08-29 20:03:08,005] [    INFO][0m - loss: 0.32658172, learning_rate: 1.2e-05, global_step: 240, interval_runtime: 0.84, interval_samples_per_second: 9.524, interval_steps_per_second: 11.905, epoch: 12.0[0m
[32m[2022-08-29 20:03:09,253] [    INFO][0m - loss: 0.23263311, learning_rate: 1.125e-05, global_step: 250, interval_runtime: 1.248, interval_samples_per_second: 6.41, interval_steps_per_second: 8.013, epoch: 12.5[0m
[32m[2022-08-29 20:03:10,102] [    INFO][0m - loss: 0.34981294, learning_rate: 1.05e-05, global_step: 260, interval_runtime: 0.8497, interval_samples_per_second: 9.415, interval_steps_per_second: 11.768, epoch: 13.0[0m
[32m[2022-08-29 20:03:11,428] [    INFO][0m - loss: 0.29277909, learning_rate: 9.75e-06, global_step: 270, interval_runtime: 1.3258, interval_samples_per_second: 6.034, interval_steps_per_second: 7.543, epoch: 13.5[0m
[32m[2022-08-29 20:03:12,302] [    INFO][0m - loss: 0.25914929, learning_rate: 9e-06, global_step: 280, interval_runtime: 0.874, interval_samples_per_second: 9.153, interval_steps_per_second: 11.441, epoch: 14.0[0m
[32m[2022-08-29 20:03:13,636] [    INFO][0m - loss: 0.29923308, learning_rate: 8.25e-06, global_step: 290, interval_runtime: 1.334, interval_samples_per_second: 5.997, interval_steps_per_second: 7.496, epoch: 14.5[0m
[32m[2022-08-29 20:03:14,519] [    INFO][0m - loss: 0.23531039, learning_rate: 7.5e-06, global_step: 300, interval_runtime: 0.8828, interval_samples_per_second: 9.062, interval_steps_per_second: 11.328, epoch: 15.0[0m
[32m[2022-08-29 20:03:14,520] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-29 20:03:14,520] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-29 20:03:14,520] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-29 20:03:14,520] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-29 20:03:14,520] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-29 20:03:15,555] [    INFO][0m - eval_loss: 1.3478796482086182, eval_accuracy: 0.5786163522012578, eval_runtime: 1.0348, eval_samples_per_second: 153.646, eval_steps_per_second: 4.832, epoch: 15.0[0m
[32m[2022-08-29 20:03:15,556] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-29 20:03:15,556] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-29 20:03:18,814] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-29 20:03:18,814] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-29 20:03:23,752] [    INFO][0m - loss: 0.32272251, learning_rate: 6.750000000000001e-06, global_step: 310, interval_runtime: 9.2327, interval_samples_per_second: 0.866, interval_steps_per_second: 1.083, epoch: 15.5[0m
[32m[2022-08-29 20:03:24,668] [    INFO][0m - loss: 0.18189179, learning_rate: 6e-06, global_step: 320, interval_runtime: 0.9162, interval_samples_per_second: 8.732, interval_steps_per_second: 10.915, epoch: 16.0[0m
[32m[2022-08-29 20:03:26,091] [    INFO][0m - loss: 0.19852693, learning_rate: 5.25e-06, global_step: 330, interval_runtime: 1.4231, interval_samples_per_second: 5.622, interval_steps_per_second: 7.027, epoch: 16.5[0m
[32m[2022-08-29 20:03:27,002] [    INFO][0m - loss: 0.27353766, learning_rate: 4.5e-06, global_step: 340, interval_runtime: 0.9115, interval_samples_per_second: 8.777, interval_steps_per_second: 10.971, epoch: 17.0[0m
[32m[2022-08-29 20:03:28,479] [    INFO][0m - loss: 0.2581326, learning_rate: 3.75e-06, global_step: 350, interval_runtime: 1.4768, interval_samples_per_second: 5.417, interval_steps_per_second: 6.772, epoch: 17.5[0m
[32m[2022-08-29 20:03:29,412] [    INFO][0m - loss: 0.13303142, learning_rate: 3e-06, global_step: 360, interval_runtime: 0.933, interval_samples_per_second: 8.574, interval_steps_per_second: 10.718, epoch: 18.0[0m
[32m[2022-08-29 20:03:30,948] [    INFO][0m - loss: 0.20558717, learning_rate: 2.25e-06, global_step: 370, interval_runtime: 1.5358, interval_samples_per_second: 5.209, interval_steps_per_second: 6.511, epoch: 18.5[0m
[32m[2022-08-29 20:03:31,894] [    INFO][0m - loss: 0.21341383, learning_rate: 1.5e-06, global_step: 380, interval_runtime: 0.9461, interval_samples_per_second: 8.456, interval_steps_per_second: 10.57, epoch: 19.0[0m
[32m[2022-08-29 20:03:33,457] [    INFO][0m - loss: 0.2166019, learning_rate: 7.5e-07, global_step: 390, interval_runtime: 1.5633, interval_samples_per_second: 5.118, interval_steps_per_second: 6.397, epoch: 19.5[0m
[32m[2022-08-29 20:03:34,411] [    INFO][0m - loss: 0.15884335, learning_rate: 0.0, global_step: 400, interval_runtime: 0.9533, interval_samples_per_second: 8.392, interval_steps_per_second: 10.49, epoch: 20.0[0m
[32m[2022-08-29 20:03:34,411] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-29 20:03:34,411] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-29 20:03:34,411] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-29 20:03:34,411] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-29 20:03:34,412] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-29 20:03:35,656] [    INFO][0m - eval_loss: 1.3786470890045166, eval_accuracy: 0.559748427672956, eval_runtime: 1.2441, eval_samples_per_second: 127.8, eval_steps_per_second: 4.019, epoch: 20.0[0m
[32m[2022-08-29 20:03:35,656] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-29 20:03:35,656] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-29 20:03:38,990] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-29 20:03:38,991] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-29 20:03:42,492] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-29 20:03:42,493] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-300 (score: 0.5786163522012578).[0m
[32m[2022-08-29 20:03:43,439] [    INFO][0m - train_runtime: 72.8783, train_samples_per_second: 43.909, train_steps_per_second: 5.489, train_loss: 0.3666228225827217, epoch: 20.0[0m
[32m[2022-08-29 20:03:43,441] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-29 20:03:43,441] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-29 20:03:46,456] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-29 20:03:46,456] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-29 20:03:46,457] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-29 20:03:46,457] [    INFO][0m -   epoch                    =       20.0[0m
[32m[2022-08-29 20:03:46,457] [    INFO][0m -   train_loss               =     0.3666[0m
[32m[2022-08-29 20:03:46,457] [    INFO][0m -   train_runtime            = 0:01:12.87[0m
[32m[2022-08-29 20:03:46,457] [    INFO][0m -   train_samples_per_second =     43.909[0m
[32m[2022-08-29 20:03:46,458] [    INFO][0m -   train_steps_per_second   =      5.489[0m
[32m[2022-08-29 20:03:46,460] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-29 20:03:46,460] [    INFO][0m -   Num examples = 976[0m
[32m[2022-08-29 20:03:46,460] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-29 20:03:46,460] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-29 20:03:46,461] [    INFO][0m -   Total prediction steps = 31[0m
[32m[2022-08-29 20:03:54,261] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-29 20:03:54,262] [    INFO][0m -   test_accuracy           =     0.4662[0m
[32m[2022-08-29 20:03:54,262] [    INFO][0m -   test_loss               =     2.0107[0m
[32m[2022-08-29 20:03:54,262] [    INFO][0m -   test_runtime            = 0:00:07.80[0m
[32m[2022-08-29 20:03:54,262] [    INFO][0m -   test_samples_per_second =    125.115[0m
[32m[2022-08-29 20:03:54,262] [    INFO][0m -   test_steps_per_second   =      3.974[0m
[32m[2022-08-29 20:03:54,262] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-29 20:03:54,262] [    INFO][0m -   Num examples = 290[0m
[32m[2022-08-29 20:03:54,263] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-29 20:03:54,263] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-29 20:03:54,263] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-08-29 20:03:56,813] [    INFO][0m - Predictions for cluewscf saved to ./fewclue_submit_examples.[0m
metric <function main.<locals>.compute_metrics at 0x7f1fc12a6550>
