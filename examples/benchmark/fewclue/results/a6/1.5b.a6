 
==========
eprstmt
==========
 
[33m[2022-08-30 16:10:13,068] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-30 16:10:13,068] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-30 16:10:13,068] [    INFO][0m - ============================================================[0m
[32m[2022-08-30 16:10:13,068] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-30 16:10:13,068] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-30 16:10:13,068] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-30 16:10:13,068] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-30 16:10:13,069] [    INFO][0m - early_stop_patience           :3[0m
[32m[2022-08-30 16:10:13,069] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-30 16:10:13,069] [    INFO][0m - model_name_or_path            :/ssd2/wanghuijuan03/ernie-3.0-1.5b-zh[0m
[32m[2022-08-30 16:10:13,069] [    INFO][0m - [0m
[32m[2022-08-30 16:10:13,069] [    INFO][0m - ============================================================[0m
[32m[2022-08-30 16:10:13,069] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-30 16:10:13,069] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-30 16:10:13,069] [    INFO][0m - encoder_hidden_size           :None[0m
[32m[2022-08-30 16:10:13,069] [    INFO][0m - prompt                        :{'text':'text_a'}{'soft':'ÊàëÊÑüËßâ'}{'mask'}{'soft':'ÂñúÊ¨¢„ÄÇ'}[0m
[32m[2022-08-30 16:10:13,069] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-30 16:10:13,069] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-30 16:10:13,069] [    INFO][0m - task_name                     :eprstmt[0m
[32m[2022-08-30 16:10:13,069] [    INFO][0m - [0m
W0830 16:10:13.071005 49093 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0830 16:10:13.075117 49093 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[33m[2022-08-30 16:10:34,485] [ WARNING][0m - Encoder has already set as lstm, change `prompt_encoder` will reset parameters.[0m
[32m[2022-08-30 16:10:34,496] [    INFO][0m - Using template: [{'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'soft': 'Êàë'}, {'add_prefix_space': '', 'soft': 'ÊÑü'}, {'add_prefix_space': '', 'soft': 'Ëßâ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'soft': 'Âñú'}, {'add_prefix_space': '', 'soft': 'Ê¨¢'}, {'add_prefix_space': '', 'soft': '„ÄÇ'}][0m
2022-08-30 16:10:34,501 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-30 16:10:34,599] [    INFO][0m - ============================================================[0m
[32m[2022-08-30 16:10:34,599] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-30 16:10:34,599] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-30 16:10:34,600] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-30 16:10:34,600] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-30 16:10:34,600] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-30 16:10:34,600] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-30 16:10:34,600] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-30 16:10:34,600] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-30 16:10:34,600] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-30 16:10:34,600] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-30 16:10:34,600] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-30 16:10:34,600] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-30 16:10:34,600] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-30 16:10:34,600] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-30 16:10:34,601] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-30 16:10:34,601] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-30 16:10:34,601] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-30 16:10:34,601] [    INFO][0m - eval_batch_size               :8[0m
[32m[2022-08-30 16:10:34,601] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-30 16:10:34,601] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-30 16:10:34,601] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-30 16:10:34,601] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-30 16:10:34,601] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-30 16:10:34,601] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-30 16:10:34,601] [    INFO][0m - freeze_plm                    :True[0m
[32m[2022-08-30 16:10:34,601] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-30 16:10:34,601] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-30 16:10:34,602] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-30 16:10:34,602] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-30 16:10:34,602] [    INFO][0m - learning_rate                 :0.003[0m
[32m[2022-08-30 16:10:34,602] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-30 16:10:34,602] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-30 16:10:34,602] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-30 16:10:34,602] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-30 16:10:34,602] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-30 16:10:34,602] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-30 16:10:34,602] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug30_16-10-13_instance-3bwob41y-01[0m
[32m[2022-08-30 16:10:34,602] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-30 16:10:34,602] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-30 16:10:34,602] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-30 16:10:34,602] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-30 16:10:34,603] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-30 16:10:34,603] [    INFO][0m - max_seq_length                :128[0m
[32m[2022-08-30 16:10:34,603] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-30 16:10:34,603] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-30 16:10:34,603] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-30 16:10:34,603] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-30 16:10:34,603] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-08-30 16:10:34,603] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-30 16:10:34,603] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-30 16:10:34,603] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-30 16:10:34,603] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-30 16:10:34,603] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-30 16:10:34,603] [    INFO][0m - per_device_eval_batch_size    :8[0m
[32m[2022-08-30 16:10:34,604] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-30 16:10:34,604] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-30 16:10:34,604] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-30 16:10:34,604] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-30 16:10:34,604] [    INFO][0m - ppt_learning_rate             :0.03[0m
[32m[2022-08-30 16:10:34,604] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-30 16:10:34,604] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-30 16:10:34,604] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-30 16:10:34,604] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-30 16:10:34,604] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-30 16:10:34,604] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-30 16:10:34,604] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-30 16:10:34,604] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-30 16:10:34,604] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-30 16:10:34,605] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-30 16:10:34,605] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-30 16:10:34,605] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-30 16:10:34,605] [    INFO][0m - seed                          :42[0m
[32m[2022-08-30 16:10:34,605] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-30 16:10:34,605] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-30 16:10:34,605] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-30 16:10:34,605] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-30 16:10:34,605] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-30 16:10:34,605] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-30 16:10:34,605] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-30 16:10:34,605] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-30 16:10:34,605] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-30 16:10:34,606] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-30 16:10:34,606] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-30 16:10:34,606] [    INFO][0m - [0m
[32m[2022-08-30 16:10:34,608] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-30 16:10:34,609] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 16:10:34,609] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-08-30 16:10:34,609] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-30 16:10:34,609] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-30 16:10:34,609] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-30 16:10:34,609] [    INFO][0m -   Total optimization steps = 1000[0m
[32m[2022-08-30 16:10:34,609] [    INFO][0m -   Total num train samples = 8000[0m
[32m[2022-08-30 16:10:42,141] [    INFO][0m - loss: 2.839855, learning_rate: 0.029699999999999997, global_step: 10, interval_runtime: 7.5307, interval_samples_per_second: 1.062, interval_steps_per_second: 1.328, epoch: 0.5[0m
[32m[2022-08-30 16:10:48,307] [    INFO][0m - loss: 1.47236052, learning_rate: 0.0294, global_step: 20, interval_runtime: 6.1661, interval_samples_per_second: 1.297, interval_steps_per_second: 1.622, epoch: 1.0[0m
[32m[2022-08-30 16:10:54,511] [    INFO][0m - loss: 0.81337805, learning_rate: 0.029099999999999997, global_step: 30, interval_runtime: 6.2038, interval_samples_per_second: 1.29, interval_steps_per_second: 1.612, epoch: 1.5[0m
[32m[2022-08-30 16:11:00,707] [    INFO][0m - loss: 5.58643112, learning_rate: 0.0288, global_step: 40, interval_runtime: 6.1965, interval_samples_per_second: 1.291, interval_steps_per_second: 1.614, epoch: 2.0[0m
[32m[2022-08-30 16:11:06,986] [    INFO][0m - loss: 1.72712994, learning_rate: 0.028499999999999998, global_step: 50, interval_runtime: 6.2786, interval_samples_per_second: 1.274, interval_steps_per_second: 1.593, epoch: 2.5[0m
[32m[2022-08-30 16:11:13,215] [    INFO][0m - loss: 0.79493628, learning_rate: 0.028199999999999996, global_step: 60, interval_runtime: 6.2297, interval_samples_per_second: 1.284, interval_steps_per_second: 1.605, epoch: 3.0[0m
[32m[2022-08-30 16:11:19,528] [    INFO][0m - loss: 2.92705765, learning_rate: 0.0279, global_step: 70, interval_runtime: 6.3123, interval_samples_per_second: 1.267, interval_steps_per_second: 1.584, epoch: 3.5[0m
[32m[2022-08-30 16:11:25,774] [    INFO][0m - loss: 2.95189896, learning_rate: 0.0276, global_step: 80, interval_runtime: 6.2461, interval_samples_per_second: 1.281, interval_steps_per_second: 1.601, epoch: 4.0[0m
[32m[2022-08-30 16:11:32,147] [    INFO][0m - loss: 0.40696883, learning_rate: 0.0273, global_step: 90, interval_runtime: 6.3735, interval_samples_per_second: 1.255, interval_steps_per_second: 1.569, epoch: 4.5[0m
[32m[2022-08-30 16:11:38,414] [    INFO][0m - loss: 1.23464613, learning_rate: 0.027, global_step: 100, interval_runtime: 6.267, interval_samples_per_second: 1.277, interval_steps_per_second: 1.596, epoch: 5.0[0m
[32m[2022-08-30 16:11:38,415] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 16:11:38,415] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 16:11:38,415] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-08-30 16:11:38,415] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-08-30 16:11:38,416] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-08-30 16:11:44,807] [    INFO][0m - eval_loss: 2.6725399494171143, eval_accuracy: 0.8125, eval_runtime: 6.3919, eval_samples_per_second: 25.032, eval_steps_per_second: 3.129, epoch: 5.0[0m
[32m[2022-08-30 16:11:44,808] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-30 16:11:44,808] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 16:12:23,646] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-30 16:12:23,646] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-30 16:12:34,150] [    INFO][0m - loss: 0.66554608, learning_rate: 0.026699999999999998, global_step: 110, interval_runtime: 55.735, interval_samples_per_second: 0.144, interval_steps_per_second: 0.179, epoch: 5.5[0m
[32m[2022-08-30 16:12:40,426] [    INFO][0m - loss: 0.86368732, learning_rate: 0.0264, global_step: 120, interval_runtime: 6.2771, interval_samples_per_second: 1.274, interval_steps_per_second: 1.593, epoch: 6.0[0m
[32m[2022-08-30 16:12:46,902] [    INFO][0m - loss: 1.03034182, learning_rate: 0.026099999999999998, global_step: 130, interval_runtime: 6.4759, interval_samples_per_second: 1.235, interval_steps_per_second: 1.544, epoch: 6.5[0m
[32m[2022-08-30 16:12:53,219] [    INFO][0m - loss: 0.42742152, learning_rate: 0.0258, global_step: 140, interval_runtime: 6.3174, interval_samples_per_second: 1.266, interval_steps_per_second: 1.583, epoch: 7.0[0m
[32m[2022-08-30 16:12:59,773] [    INFO][0m - loss: 2.14789848, learning_rate: 0.0255, global_step: 150, interval_runtime: 6.5532, interval_samples_per_second: 1.221, interval_steps_per_second: 1.526, epoch: 7.5[0m
[32m[2022-08-30 16:13:06,118] [    INFO][0m - loss: 2.27228851, learning_rate: 0.025199999999999997, global_step: 160, interval_runtime: 6.3457, interval_samples_per_second: 1.261, interval_steps_per_second: 1.576, epoch: 8.0[0m
[32m[2022-08-30 16:13:12,710] [    INFO][0m - loss: 1.84565697, learning_rate: 0.0249, global_step: 170, interval_runtime: 6.5911, interval_samples_per_second: 1.214, interval_steps_per_second: 1.517, epoch: 8.5[0m
[32m[2022-08-30 16:13:19,086] [    INFO][0m - loss: 3.0446558, learning_rate: 0.024599999999999997, global_step: 180, interval_runtime: 6.3761, interval_samples_per_second: 1.255, interval_steps_per_second: 1.568, epoch: 9.0[0m
[32m[2022-08-30 16:13:25,736] [    INFO][0m - loss: 0.49849401, learning_rate: 0.024300000000000002, global_step: 190, interval_runtime: 6.6508, interval_samples_per_second: 1.203, interval_steps_per_second: 1.504, epoch: 9.5[0m
[32m[2022-08-30 16:13:32,122] [    INFO][0m - loss: 0.52383561, learning_rate: 0.024, global_step: 200, interval_runtime: 6.386, interval_samples_per_second: 1.253, interval_steps_per_second: 1.566, epoch: 10.0[0m
[32m[2022-08-30 16:13:32,123] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 16:13:32,123] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 16:13:32,123] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-08-30 16:13:32,123] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-08-30 16:13:32,124] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-08-30 16:13:38,892] [    INFO][0m - eval_loss: 1.8846145868301392, eval_accuracy: 0.85625, eval_runtime: 6.768, eval_samples_per_second: 23.641, eval_steps_per_second: 2.955, epoch: 10.0[0m
[32m[2022-08-30 16:13:38,892] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-30 16:13:38,893] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 16:14:12,323] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-30 16:14:12,323] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-30 16:14:22,912] [    INFO][0m - loss: 0.49298301, learning_rate: 0.0237, global_step: 210, interval_runtime: 50.7887, interval_samples_per_second: 0.158, interval_steps_per_second: 0.197, epoch: 10.5[0m
[32m[2022-08-30 16:14:29,302] [    INFO][0m - loss: 0.53562093, learning_rate: 0.0234, global_step: 220, interval_runtime: 6.3904, interval_samples_per_second: 1.252, interval_steps_per_second: 1.565, epoch: 11.0[0m
[32m[2022-08-30 16:14:36,039] [    INFO][0m - loss: 0.47446795, learning_rate: 0.0231, global_step: 230, interval_runtime: 6.7371, interval_samples_per_second: 1.187, interval_steps_per_second: 1.484, epoch: 11.5[0m
[32m[2022-08-30 16:14:42,443] [    INFO][0m - loss: 1.09087706, learning_rate: 0.0228, global_step: 240, interval_runtime: 6.4047, interval_samples_per_second: 1.249, interval_steps_per_second: 1.561, epoch: 12.0[0m
[32m[2022-08-30 16:14:49,192] [    INFO][0m - loss: 0.74498219, learning_rate: 0.0225, global_step: 250, interval_runtime: 6.7482, interval_samples_per_second: 1.186, interval_steps_per_second: 1.482, epoch: 12.5[0m
[32m[2022-08-30 16:14:55,610] [    INFO][0m - loss: 1.10920086, learning_rate: 0.022199999999999998, global_step: 260, interval_runtime: 6.418, interval_samples_per_second: 1.247, interval_steps_per_second: 1.558, epoch: 13.0[0m
[32m[2022-08-30 16:15:02,371] [    INFO][0m - loss: 0.75950112, learning_rate: 0.0219, global_step: 270, interval_runtime: 6.7617, interval_samples_per_second: 1.183, interval_steps_per_second: 1.479, epoch: 13.5[0m
[32m[2022-08-30 16:15:08,781] [    INFO][0m - loss: 2.30390453, learning_rate: 0.021599999999999998, global_step: 280, interval_runtime: 6.4097, interval_samples_per_second: 1.248, interval_steps_per_second: 1.56, epoch: 14.0[0m
[32m[2022-08-30 16:15:15,567] [    INFO][0m - loss: 1.78513908, learning_rate: 0.0213, global_step: 290, interval_runtime: 6.7861, interval_samples_per_second: 1.179, interval_steps_per_second: 1.474, epoch: 14.5[0m
[32m[2022-08-30 16:15:21,991] [    INFO][0m - loss: 1.03888588, learning_rate: 0.020999999999999998, global_step: 300, interval_runtime: 6.4241, interval_samples_per_second: 1.245, interval_steps_per_second: 1.557, epoch: 15.0[0m
[32m[2022-08-30 16:15:21,992] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 16:15:21,992] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 16:15:21,992] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-08-30 16:15:21,992] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-08-30 16:15:21,993] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-08-30 16:15:28,904] [    INFO][0m - eval_loss: 2.6670656204223633, eval_accuracy: 0.83125, eval_runtime: 6.9111, eval_samples_per_second: 23.151, eval_steps_per_second: 2.894, epoch: 15.0[0m
[32m[2022-08-30 16:15:28,905] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-30 16:15:28,905] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 16:16:07,022] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-30 16:16:07,023] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-30 16:16:17,567] [    INFO][0m - loss: 1.75313244, learning_rate: 0.020699999999999996, global_step: 310, interval_runtime: 55.5758, interval_samples_per_second: 0.144, interval_steps_per_second: 0.18, epoch: 15.5[0m
[32m[2022-08-30 16:16:23,948] [    INFO][0m - loss: 1.41767569, learning_rate: 0.0204, global_step: 320, interval_runtime: 6.3812, interval_samples_per_second: 1.254, interval_steps_per_second: 1.567, epoch: 16.0[0m
[32m[2022-08-30 16:16:30,868] [    INFO][0m - loss: 2.10349579, learning_rate: 0.0201, global_step: 330, interval_runtime: 6.9199, interval_samples_per_second: 1.156, interval_steps_per_second: 1.445, epoch: 16.5[0m
[32m[2022-08-30 16:16:37,310] [    INFO][0m - loss: 1.29543467, learning_rate: 0.0198, global_step: 340, interval_runtime: 6.4412, interval_samples_per_second: 1.242, interval_steps_per_second: 1.553, epoch: 17.0[0m
[32m[2022-08-30 16:16:44,238] [    INFO][0m - loss: 0.92600527, learning_rate: 0.0195, global_step: 350, interval_runtime: 6.9289, interval_samples_per_second: 1.155, interval_steps_per_second: 1.443, epoch: 17.5[0m
[32m[2022-08-30 16:16:50,682] [    INFO][0m - loss: 1.17699938, learning_rate: 0.0192, global_step: 360, interval_runtime: 6.4434, interval_samples_per_second: 1.242, interval_steps_per_second: 1.552, epoch: 18.0[0m
[32m[2022-08-30 16:16:57,638] [    INFO][0m - loss: 0.40528898, learning_rate: 0.0189, global_step: 370, interval_runtime: 6.9554, interval_samples_per_second: 1.15, interval_steps_per_second: 1.438, epoch: 18.5[0m
[32m[2022-08-30 16:17:04,113] [    INFO][0m - loss: 0.47810378, learning_rate: 0.0186, global_step: 380, interval_runtime: 6.4756, interval_samples_per_second: 1.235, interval_steps_per_second: 1.544, epoch: 19.0[0m
[32m[2022-08-30 16:17:11,109] [    INFO][0m - loss: 0.38091362, learning_rate: 0.0183, global_step: 390, interval_runtime: 6.9964, interval_samples_per_second: 1.143, interval_steps_per_second: 1.429, epoch: 19.5[0m
[32m[2022-08-30 16:17:17,618] [    INFO][0m - loss: 0.36001623, learning_rate: 0.018, global_step: 400, interval_runtime: 6.5082, interval_samples_per_second: 1.229, interval_steps_per_second: 1.537, epoch: 20.0[0m
[32m[2022-08-30 16:17:17,618] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 16:17:17,618] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 16:17:17,618] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-08-30 16:17:17,619] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-08-30 16:17:17,619] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-08-30 16:17:24,816] [    INFO][0m - eval_loss: 2.257255792617798, eval_accuracy: 0.8375, eval_runtime: 7.1966, eval_samples_per_second: 22.233, eval_steps_per_second: 2.779, epoch: 20.0[0m
[32m[2022-08-30 16:17:24,816] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-30 16:17:24,816] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 16:18:01,617] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-30 16:18:01,617] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-30 16:18:12,853] [    INFO][0m - loss: 0.00214164, learning_rate: 0.017699999999999997, global_step: 410, interval_runtime: 55.2349, interval_samples_per_second: 0.145, interval_steps_per_second: 0.181, epoch: 20.5[0m
[32m[2022-08-30 16:18:19,341] [    INFO][0m - loss: 0.30060594, learning_rate: 0.0174, global_step: 420, interval_runtime: 6.4883, interval_samples_per_second: 1.233, interval_steps_per_second: 1.541, epoch: 21.0[0m
[32m[2022-08-30 16:18:26,436] [    INFO][0m - loss: 1.23604326, learning_rate: 0.017099999999999997, global_step: 430, interval_runtime: 7.0948, interval_samples_per_second: 1.128, interval_steps_per_second: 1.409, epoch: 21.5[0m
[32m[2022-08-30 16:18:32,936] [    INFO][0m - loss: 0.03130189, learning_rate: 0.016800000000000002, global_step: 440, interval_runtime: 6.5008, interval_samples_per_second: 1.231, interval_steps_per_second: 1.538, epoch: 22.0[0m
[32m[2022-08-30 16:18:40,070] [    INFO][0m - loss: 0.22615643, learning_rate: 0.0165, global_step: 450, interval_runtime: 7.1335, interval_samples_per_second: 1.121, interval_steps_per_second: 1.402, epoch: 22.5[0m
[32m[2022-08-30 16:18:46,580] [    INFO][0m - loss: 0.34035263, learning_rate: 0.0162, global_step: 460, interval_runtime: 6.5099, interval_samples_per_second: 1.229, interval_steps_per_second: 1.536, epoch: 23.0[0m
[32m[2022-08-30 16:18:53,823] [    INFO][0m - loss: 0.47386312, learning_rate: 0.0159, global_step: 470, interval_runtime: 7.2434, interval_samples_per_second: 1.104, interval_steps_per_second: 1.381, epoch: 23.5[0m
[32m[2022-08-30 16:19:00,383] [    INFO][0m - loss: 0.0256536, learning_rate: 0.0156, global_step: 480, interval_runtime: 6.5601, interval_samples_per_second: 1.22, interval_steps_per_second: 1.524, epoch: 24.0[0m
[32m[2022-08-30 16:19:07,626] [    INFO][0m - loss: 0.07663671, learning_rate: 0.0153, global_step: 490, interval_runtime: 7.2427, interval_samples_per_second: 1.105, interval_steps_per_second: 1.381, epoch: 24.5[0m
[32m[2022-08-30 16:19:14,196] [    INFO][0m - loss: 0.02688232, learning_rate: 0.015, global_step: 500, interval_runtime: 6.5705, interval_samples_per_second: 1.218, interval_steps_per_second: 1.522, epoch: 25.0[0m
[32m[2022-08-30 16:19:14,197] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 16:19:14,197] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 16:19:14,197] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-08-30 16:19:14,197] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-08-30 16:19:14,197] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-08-30 16:19:21,763] [    INFO][0m - eval_loss: 1.9949915409088135, eval_accuracy: 0.85, eval_runtime: 7.5653, eval_samples_per_second: 21.149, eval_steps_per_second: 2.644, epoch: 25.0[0m
[32m[2022-08-30 16:19:21,763] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-08-30 16:19:21,763] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 16:19:57,522] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-08-30 16:19:57,522] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-08-30 16:20:01,161] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-30 16:20:01,161] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-200 (score: 0.85625).[0m
[32m[2022-08-30 16:20:11,910] [    INFO][0m - train_runtime: 577.3006, train_samples_per_second: 13.858, train_steps_per_second: 1.732, train_loss: 1.1489350915290415, epoch: 25.0[0m
[32m[2022-08-30 16:20:11,913] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-30 16:20:11,913] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 16:20:53,136] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-30 16:20:53,137] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-30 16:20:53,138] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-30 16:20:53,139] [    INFO][0m -   epoch                    =       25.0[0m
[32m[2022-08-30 16:20:53,139] [    INFO][0m -   train_loss               =     1.1489[0m
[32m[2022-08-30 16:20:53,139] [    INFO][0m -   train_runtime            = 0:09:37.30[0m
[32m[2022-08-30 16:20:53,139] [    INFO][0m -   train_samples_per_second =     13.858[0m
[32m[2022-08-30 16:20:53,139] [    INFO][0m -   train_steps_per_second   =      1.732[0m
[32m[2022-08-30 16:20:53,142] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-30 16:20:53,142] [    INFO][0m -   Num examples = 610[0m
[32m[2022-08-30 16:20:53,142] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-08-30 16:20:53,142] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-08-30 16:20:53,143] [    INFO][0m -   Total prediction steps = 77[0m
[32m[2022-08-30 16:21:21,979] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-30 16:21:21,980] [    INFO][0m -   test_accuracy           =     0.8213[0m
[32m[2022-08-30 16:21:21,980] [    INFO][0m -   test_loss               =     2.5706[0m
[32m[2022-08-30 16:21:21,980] [    INFO][0m -   test_runtime            = 0:00:28.83[0m
[32m[2022-08-30 16:21:21,980] [    INFO][0m -   test_samples_per_second =     21.153[0m
[32m[2022-08-30 16:21:21,980] [    INFO][0m -   test_steps_per_second   =       2.67[0m
[32m[2022-08-30 16:21:21,980] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-30 16:21:21,980] [    INFO][0m -   Num examples = 753[0m
[32m[2022-08-30 16:21:21,980] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-08-30 16:21:21,981] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-08-30 16:21:21,981] [    INFO][0m -   Total prediction steps = 95[0m
metric <function main.<locals>.compute_metrics at 0x7fa775ab9c10>
Traceback (most recent call last):
  File "train_single.py", line 169, in <module>
    main()
  File "train_single.py", line 162, in main
    test_ret = trainer.predict(test_ds)
  File "/ssd2/wanghuijuan03/prompt/PaddleNLP/paddlenlp/trainer/trainer_base.py", line 1522, in predict
    output = eval_loop(test_dataloader,
  File "/ssd2/wanghuijuan03/prompt/PaddleNLP/paddlenlp/trainer/trainer_base.py", line 1438, in evaluation_loop
    preds_host = logits if preds_host is None else nested_concat(
  File "/ssd2/wanghuijuan03/prompt/PaddleNLP/paddlenlp/trainer/utils/helper.py", line 105, in nested_concat
    return paddle_pad_and_concatenate(tensors,
  File "/ssd2/wanghuijuan03/prompt/PaddleNLP/paddlenlp/trainer/utils/helper.py", line 59, in paddle_pad_and_concatenate
    if len(tensor1.shape) == 1 or tensor1.shape[1] == tensor2.shape[1]:
IndexError: list index out of range
 
==========
csldcp
==========
 
[33m[2022-08-30 16:22:03,126] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-30 16:22:03,126] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-30 16:22:03,126] [    INFO][0m - ============================================================[0m
[32m[2022-08-30 16:22:03,126] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-30 16:22:03,126] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-30 16:22:03,126] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-30 16:22:03,126] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-30 16:22:03,127] [    INFO][0m - early_stop_patience           :3[0m
[32m[2022-08-30 16:22:03,127] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-30 16:22:03,127] [    INFO][0m - model_name_or_path            :/ssd2/wanghuijuan03/ernie-3.0-1.5b-zh[0m
[32m[2022-08-30 16:22:03,127] [    INFO][0m - [0m
[32m[2022-08-30 16:22:03,127] [    INFO][0m - ============================================================[0m
[32m[2022-08-30 16:22:03,127] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-30 16:22:03,127] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-30 16:22:03,127] [    INFO][0m - encoder_hidden_size           :None[0m
[32m[2022-08-30 16:22:03,127] [    INFO][0m - prompt                        :{'soft':'ÈòÖËØª‰∏ãËæπ'}{'mask'}{'mask'}{'soft':'Áõ∏ÂÖ≥ÁöÑÊùêÊñô'}{'text':'text_a'}[0m
[32m[2022-08-30 16:22:03,127] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-30 16:22:03,127] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-30 16:22:03,127] [    INFO][0m - task_name                     :csldcp[0m
[32m[2022-08-30 16:22:03,127] [    INFO][0m - [0m
W0830 16:22:03.129148 66113 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0830 16:22:03.133322 66113 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[33m[2022-08-30 16:22:25,132] [ WARNING][0m - Encoder has already set as lstm, change `prompt_encoder` will reset parameters.[0m
[32m[2022-08-30 16:22:25,144] [    INFO][0m - Using template: [{'add_prefix_space': '', 'soft': 'ÈòÖ'}, {'add_prefix_space': '', 'soft': 'ËØª'}, {'add_prefix_space': '', 'soft': '‰∏ã'}, {'add_prefix_space': '', 'soft': 'Ëæπ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'soft': 'Áõ∏'}, {'add_prefix_space': '', 'soft': 'ÂÖ≥'}, {'add_prefix_space': '', 'soft': 'ÁöÑ'}, {'add_prefix_space': '', 'soft': 'Êùê'}, {'add_prefix_space': '', 'soft': 'Êñô'}, {'add_prefix_space': '', 'text': 'text_a'}][0m
2022-08-30 16:22:25,162 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-30 16:22:25,324] [    INFO][0m - ============================================================[0m
[32m[2022-08-30 16:22:25,324] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-30 16:22:25,324] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-30 16:22:25,324] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-30 16:22:25,324] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-30 16:22:25,324] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-30 16:22:25,324] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-30 16:22:25,324] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-30 16:22:25,324] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-30 16:22:25,325] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-30 16:22:25,325] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-30 16:22:25,325] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-30 16:22:25,325] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-30 16:22:25,325] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-30 16:22:25,325] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-30 16:22:25,325] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-30 16:22:25,325] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-30 16:22:25,325] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-30 16:22:25,325] [    INFO][0m - eval_batch_size               :8[0m
[32m[2022-08-30 16:22:25,325] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-30 16:22:25,325] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-30 16:22:25,325] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-30 16:22:25,326] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-30 16:22:25,326] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-30 16:22:25,326] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-30 16:22:25,326] [    INFO][0m - freeze_plm                    :True[0m
[32m[2022-08-30 16:22:25,326] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-30 16:22:25,326] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-30 16:22:25,326] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-30 16:22:25,326] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-30 16:22:25,326] [    INFO][0m - learning_rate                 :0.003[0m
[32m[2022-08-30 16:22:25,326] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-30 16:22:25,326] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-30 16:22:25,326] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-30 16:22:25,326] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-30 16:22:25,326] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-30 16:22:25,327] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-30 16:22:25,327] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug30_16-22-03_instance-3bwob41y-01[0m
[32m[2022-08-30 16:22:25,327] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-30 16:22:25,327] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-30 16:22:25,327] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-30 16:22:25,327] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-30 16:22:25,327] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-30 16:22:25,327] [    INFO][0m - max_seq_length                :256[0m
[32m[2022-08-30 16:22:25,327] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-30 16:22:25,327] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-30 16:22:25,327] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-30 16:22:25,327] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-30 16:22:25,327] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-08-30 16:22:25,328] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-30 16:22:25,328] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-30 16:22:25,328] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-30 16:22:25,328] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-30 16:22:25,328] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-30 16:22:25,328] [    INFO][0m - per_device_eval_batch_size    :8[0m
[32m[2022-08-30 16:22:25,328] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-30 16:22:25,328] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-30 16:22:25,328] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-30 16:22:25,328] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-30 16:22:25,328] [    INFO][0m - ppt_learning_rate             :0.03[0m
[32m[2022-08-30 16:22:25,328] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-30 16:22:25,328] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-30 16:22:25,328] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-30 16:22:25,329] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-30 16:22:25,329] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-30 16:22:25,329] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-30 16:22:25,329] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-30 16:22:25,329] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-30 16:22:25,329] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-30 16:22:25,329] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-30 16:22:25,329] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-30 16:22:25,329] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-30 16:22:25,329] [    INFO][0m - seed                          :42[0m
[32m[2022-08-30 16:22:25,329] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-30 16:22:25,329] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-30 16:22:25,329] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-30 16:22:25,330] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-30 16:22:25,330] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-30 16:22:25,330] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-30 16:22:25,330] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-30 16:22:25,330] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-30 16:22:25,330] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-30 16:22:25,330] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-30 16:22:25,330] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-30 16:22:25,330] [    INFO][0m - [0m
[32m[2022-08-30 16:22:25,333] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-30 16:22:25,333] [    INFO][0m -   Num examples = 2036[0m
[32m[2022-08-30 16:22:25,333] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-08-30 16:22:25,333] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-30 16:22:25,333] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-30 16:22:25,333] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-30 16:22:25,334] [    INFO][0m -   Total optimization steps = 12750[0m
[32m[2022-08-30 16:22:25,334] [    INFO][0m -   Total num train samples = 101800[0m
[32m[2022-08-30 16:22:38,772] [    INFO][0m - loss: 26.76991272, learning_rate: 0.029976470588235295, global_step: 10, interval_runtime: 13.4365, interval_samples_per_second: 0.595, interval_steps_per_second: 0.744, epoch: 0.0392[0m
[32m[2022-08-30 16:22:51,067] [    INFO][0m - loss: 42.42931213, learning_rate: 0.029952941176470585, global_step: 20, interval_runtime: 12.2956, interval_samples_per_second: 0.651, interval_steps_per_second: 0.813, epoch: 0.0784[0m
[32m[2022-08-30 16:23:03,423] [    INFO][0m - loss: 47.88878174, learning_rate: 0.02992941176470588, global_step: 30, interval_runtime: 12.3561, interval_samples_per_second: 0.647, interval_steps_per_second: 0.809, epoch: 0.1176[0m
[32m[2022-08-30 16:23:15,801] [    INFO][0m - loss: 48.08004761, learning_rate: 0.029905882352941175, global_step: 40, interval_runtime: 12.3773, interval_samples_per_second: 0.646, interval_steps_per_second: 0.808, epoch: 0.1569[0m
[32m[2022-08-30 16:23:28,203] [    INFO][0m - loss: 28.07850952, learning_rate: 0.02988235294117647, global_step: 50, interval_runtime: 12.4026, interval_samples_per_second: 0.645, interval_steps_per_second: 0.806, epoch: 0.1961[0m
[32m[2022-08-30 16:23:40,646] [    INFO][0m - loss: 29.08969421, learning_rate: 0.02985882352941176, global_step: 60, interval_runtime: 12.4428, interval_samples_per_second: 0.643, interval_steps_per_second: 0.804, epoch: 0.2353[0m
[32m[2022-08-30 16:23:53,138] [    INFO][0m - loss: 20.98322906, learning_rate: 0.029835294117647057, global_step: 70, interval_runtime: 12.4892, interval_samples_per_second: 0.641, interval_steps_per_second: 0.801, epoch: 0.2745[0m
[32m[2022-08-30 16:24:05,650] [    INFO][0m - loss: 22.08699646, learning_rate: 0.029811764705882354, global_step: 80, interval_runtime: 12.5153, interval_samples_per_second: 0.639, interval_steps_per_second: 0.799, epoch: 0.3137[0m
[32m[2022-08-30 16:24:18,199] [    INFO][0m - loss: 22.03944397, learning_rate: 0.029788235294117647, global_step: 90, interval_runtime: 12.5485, interval_samples_per_second: 0.638, interval_steps_per_second: 0.797, epoch: 0.3529[0m
[32m[2022-08-30 16:24:30,757] [    INFO][0m - loss: 16.80570221, learning_rate: 0.02976470588235294, global_step: 100, interval_runtime: 12.5584, interval_samples_per_second: 0.637, interval_steps_per_second: 0.796, epoch: 0.3922[0m
[32m[2022-08-30 16:24:30,758] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 16:24:30,758] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-30 16:24:30,758] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-08-30 16:24:30,758] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-08-30 16:24:30,758] [    INFO][0m -   Total prediction steps = 259[0m
[32m[2022-08-30 16:27:16,160] [    INFO][0m - eval_loss: 13.865814208984375, eval_accuracy: 0.00918762088974855, eval_runtime: 165.4016, eval_samples_per_second: 12.503, eval_steps_per_second: 1.566, epoch: 0.3922[0m
[32m[2022-08-30 16:27:16,161] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-30 16:27:16,161] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 16:27:51,397] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-30 16:27:51,398] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-30 16:28:08,040] [    INFO][0m - loss: 15.45818634, learning_rate: 0.029741176470588233, global_step: 110, interval_runtime: 217.2826, interval_samples_per_second: 0.037, interval_steps_per_second: 0.046, epoch: 0.4314[0m
[32m[2022-08-30 16:28:20,751] [    INFO][0m - loss: 16.81828003, learning_rate: 0.02971764705882353, global_step: 120, interval_runtime: 12.7115, interval_samples_per_second: 0.629, interval_steps_per_second: 0.787, epoch: 0.4706[0m
[32m[2022-08-30 16:28:33,517] [    INFO][0m - loss: 14.32360992, learning_rate: 0.029694117647058822, global_step: 130, interval_runtime: 12.7662, interval_samples_per_second: 0.627, interval_steps_per_second: 0.783, epoch: 0.5098[0m
[32m[2022-08-30 16:28:46,364] [    INFO][0m - loss: 13.52091675, learning_rate: 0.029670588235294115, global_step: 140, interval_runtime: 12.8461, interval_samples_per_second: 0.623, interval_steps_per_second: 0.778, epoch: 0.549[0m
[32m[2022-08-30 16:28:59,217] [    INFO][0m - loss: 15.18669891, learning_rate: 0.029647058823529412, global_step: 150, interval_runtime: 12.8537, interval_samples_per_second: 0.622, interval_steps_per_second: 0.778, epoch: 0.5882[0m
[32m[2022-08-30 16:29:12,082] [    INFO][0m - loss: 13.85114594, learning_rate: 0.029623529411764705, global_step: 160, interval_runtime: 12.865, interval_samples_per_second: 0.622, interval_steps_per_second: 0.777, epoch: 0.6275[0m
[32m[2022-08-30 16:29:24,962] [    INFO][0m - loss: 17.03871765, learning_rate: 0.0296, global_step: 170, interval_runtime: 12.8794, interval_samples_per_second: 0.621, interval_steps_per_second: 0.776, epoch: 0.6667[0m
[32m[2022-08-30 16:29:37,843] [    INFO][0m - loss: 17.44395447, learning_rate: 0.02957647058823529, global_step: 180, interval_runtime: 12.8813, interval_samples_per_second: 0.621, interval_steps_per_second: 0.776, epoch: 0.7059[0m
[32m[2022-08-30 16:29:50,752] [    INFO][0m - loss: 22.40310822, learning_rate: 0.029552941176470587, global_step: 190, interval_runtime: 12.9085, interval_samples_per_second: 0.62, interval_steps_per_second: 0.775, epoch: 0.7451[0m
[32m[2022-08-30 16:30:03,688] [    INFO][0m - loss: 16.35648499, learning_rate: 0.02952941176470588, global_step: 200, interval_runtime: 12.9363, interval_samples_per_second: 0.618, interval_steps_per_second: 0.773, epoch: 0.7843[0m
[32m[2022-08-30 16:30:03,689] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 16:30:03,689] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-30 16:30:03,689] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-08-30 16:30:03,689] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-08-30 16:30:03,689] [    INFO][0m -   Total prediction steps = 259[0m
[32m[2022-08-30 16:33:05,405] [    INFO][0m - eval_loss: 14.324016571044922, eval_accuracy: 0.01644100580270793, eval_runtime: 181.7154, eval_samples_per_second: 11.38, eval_steps_per_second: 1.425, epoch: 0.7843[0m
[32m[2022-08-30 16:33:05,406] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-30 16:33:05,406] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 16:33:49,242] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-30 16:33:49,243] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-30 16:34:06,672] [    INFO][0m - loss: 15.37468414, learning_rate: 0.029505882352941173, global_step: 210, interval_runtime: 242.9841, interval_samples_per_second: 0.033, interval_steps_per_second: 0.041, epoch: 0.8235[0m
[32m[2022-08-30 16:34:20,128] [    INFO][0m - loss: 14.82560425, learning_rate: 0.02948235294117647, global_step: 220, interval_runtime: 13.4564, interval_samples_per_second: 0.595, interval_steps_per_second: 0.743, epoch: 0.8627[0m
[32m[2022-08-30 16:34:33,593] [    INFO][0m - loss: 17.13369293, learning_rate: 0.029458823529411763, global_step: 230, interval_runtime: 13.4649, interval_samples_per_second: 0.594, interval_steps_per_second: 0.743, epoch: 0.902[0m
[32m[2022-08-30 16:34:47,108] [    INFO][0m - loss: 15.18858643, learning_rate: 0.02943529411764706, global_step: 240, interval_runtime: 13.5152, interval_samples_per_second: 0.592, interval_steps_per_second: 0.74, epoch: 0.9412[0m
[32m[2022-08-30 16:35:00,625] [    INFO][0m - loss: 15.69389801, learning_rate: 0.02941176470588235, global_step: 250, interval_runtime: 13.5168, interval_samples_per_second: 0.592, interval_steps_per_second: 0.74, epoch: 0.9804[0m
[32m[2022-08-30 16:35:13,987] [    INFO][0m - loss: 14.08874512, learning_rate: 0.029388235294117646, global_step: 260, interval_runtime: 13.3616, interval_samples_per_second: 0.599, interval_steps_per_second: 0.748, epoch: 1.0196[0m
[32m[2022-08-30 16:35:27,595] [    INFO][0m - loss: 18.00582123, learning_rate: 0.029364705882352942, global_step: 270, interval_runtime: 13.6085, interval_samples_per_second: 0.588, interval_steps_per_second: 0.735, epoch: 1.0588[0m
[32m[2022-08-30 16:35:41,212] [    INFO][0m - loss: 16.14679108, learning_rate: 0.029341176470588235, global_step: 280, interval_runtime: 13.6167, interval_samples_per_second: 0.588, interval_steps_per_second: 0.734, epoch: 1.098[0m
[32m[2022-08-30 16:35:54,890] [    INFO][0m - loss: 14.72826233, learning_rate: 0.029317647058823528, global_step: 290, interval_runtime: 13.6778, interval_samples_per_second: 0.585, interval_steps_per_second: 0.731, epoch: 1.1373[0m
[32m[2022-08-30 16:36:08,626] [    INFO][0m - loss: 13.79876251, learning_rate: 0.02929411764705882, global_step: 300, interval_runtime: 13.7362, interval_samples_per_second: 0.582, interval_steps_per_second: 0.728, epoch: 1.1765[0m
[32m[2022-08-30 16:36:08,627] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 16:36:08,627] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-30 16:36:08,627] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-08-30 16:36:08,627] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-08-30 16:36:08,627] [    INFO][0m -   Total prediction steps = 259[0m
[32m[2022-08-30 16:39:29,561] [    INFO][0m - eval_loss: 12.50048542022705, eval_accuracy: 0.019342359767891684, eval_runtime: 200.9341, eval_samples_per_second: 10.292, eval_steps_per_second: 1.289, epoch: 1.1765[0m
[32m[2022-08-30 16:39:29,562] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-30 16:39:29,562] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 16:40:06,492] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-30 16:40:06,492] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-30 16:40:24,365] [    INFO][0m - loss: 13.86422882, learning_rate: 0.029270588235294118, global_step: 310, interval_runtime: 255.7389, interval_samples_per_second: 0.031, interval_steps_per_second: 0.039, epoch: 1.2157[0m
[32m[2022-08-30 16:40:38,570] [    INFO][0m - loss: 11.09763489, learning_rate: 0.02924705882352941, global_step: 320, interval_runtime: 14.2054, interval_samples_per_second: 0.563, interval_steps_per_second: 0.704, epoch: 1.2549[0m
[32m[2022-08-30 16:40:52,861] [    INFO][0m - loss: 15.22350159, learning_rate: 0.029223529411764704, global_step: 330, interval_runtime: 14.2905, interval_samples_per_second: 0.56, interval_steps_per_second: 0.7, epoch: 1.2941[0m
[32m[2022-08-30 16:41:07,170] [    INFO][0m - loss: 15.29293671, learning_rate: 0.0292, global_step: 340, interval_runtime: 14.3085, interval_samples_per_second: 0.559, interval_steps_per_second: 0.699, epoch: 1.3333[0m
[32m[2022-08-30 16:41:21,536] [    INFO][0m - loss: 16.13651123, learning_rate: 0.029176470588235293, global_step: 350, interval_runtime: 14.3667, interval_samples_per_second: 0.557, interval_steps_per_second: 0.696, epoch: 1.3725[0m
[32m[2022-08-30 16:41:35,870] [    INFO][0m - loss: 14.10084686, learning_rate: 0.02915294117647059, global_step: 360, interval_runtime: 14.334, interval_samples_per_second: 0.558, interval_steps_per_second: 0.698, epoch: 1.4118[0m
[32m[2022-08-30 16:41:50,246] [    INFO][0m - loss: 17.95734406, learning_rate: 0.02912941176470588, global_step: 370, interval_runtime: 14.376, interval_samples_per_second: 0.556, interval_steps_per_second: 0.696, epoch: 1.451[0m
[32m[2022-08-30 16:42:04,673] [    INFO][0m - loss: 14.24060669, learning_rate: 0.029105882352941176, global_step: 380, interval_runtime: 14.4266, interval_samples_per_second: 0.555, interval_steps_per_second: 0.693, epoch: 1.4902[0m
[32m[2022-08-30 16:42:19,105] [    INFO][0m - loss: 16.07699738, learning_rate: 0.02908235294117647, global_step: 390, interval_runtime: 14.4316, interval_samples_per_second: 0.554, interval_steps_per_second: 0.693, epoch: 1.5294[0m
[32m[2022-08-30 16:42:33,546] [    INFO][0m - loss: 15.92973785, learning_rate: 0.029058823529411762, global_step: 400, interval_runtime: 14.4418, interval_samples_per_second: 0.554, interval_steps_per_second: 0.692, epoch: 1.5686[0m
[32m[2022-08-30 16:42:33,547] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 16:42:33,547] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-30 16:42:33,547] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-08-30 16:42:33,547] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-08-30 16:42:33,547] [    INFO][0m -   Total prediction steps = 259[0m
[32m[2022-08-30 16:46:13,870] [    INFO][0m - eval_loss: 17.494407653808594, eval_accuracy: 0.015473887814313346, eval_runtime: 220.3219, eval_samples_per_second: 9.386, eval_steps_per_second: 1.176, epoch: 1.5686[0m
[32m[2022-08-30 16:46:13,870] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-30 16:46:13,870] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 16:46:55,635] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-30 16:46:55,636] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-30 16:47:14,422] [    INFO][0m - loss: 15.34044037, learning_rate: 0.02903529411764706, global_step: 410, interval_runtime: 280.875, interval_samples_per_second: 0.028, interval_steps_per_second: 0.036, epoch: 1.6078[0m
[32m[2022-08-30 16:47:29,348] [    INFO][0m - loss: 16.4922348, learning_rate: 0.02901176470588235, global_step: 420, interval_runtime: 14.9266, interval_samples_per_second: 0.536, interval_steps_per_second: 0.67, epoch: 1.6471[0m
[32m[2022-08-30 16:47:44,291] [    INFO][0m - loss: 14.31941376, learning_rate: 0.028988235294117648, global_step: 430, interval_runtime: 14.9432, interval_samples_per_second: 0.535, interval_steps_per_second: 0.669, epoch: 1.6863[0m
[32m[2022-08-30 16:47:59,305] [    INFO][0m - loss: 13.81107025, learning_rate: 0.028964705882352938, global_step: 440, interval_runtime: 15.0137, interval_samples_per_second: 0.533, interval_steps_per_second: 0.666, epoch: 1.7255[0m
[32m[2022-08-30 16:48:14,337] [    INFO][0m - loss: 15.59293671, learning_rate: 0.028941176470588234, global_step: 450, interval_runtime: 15.0319, interval_samples_per_second: 0.532, interval_steps_per_second: 0.665, epoch: 1.7647[0m
[32m[2022-08-30 16:48:29,378] [    INFO][0m - loss: 12.44967346, learning_rate: 0.028917647058823527, global_step: 460, interval_runtime: 15.0414, interval_samples_per_second: 0.532, interval_steps_per_second: 0.665, epoch: 1.8039[0m
[32m[2022-08-30 16:48:44,451] [    INFO][0m - loss: 16.21933289, learning_rate: 0.028894117647058824, global_step: 470, interval_runtime: 15.0728, interval_samples_per_second: 0.531, interval_steps_per_second: 0.663, epoch: 1.8431[0m
[32m[2022-08-30 16:48:59,580] [    INFO][0m - loss: 15.72503204, learning_rate: 0.028870588235294117, global_step: 480, interval_runtime: 15.1294, interval_samples_per_second: 0.529, interval_steps_per_second: 0.661, epoch: 1.8824[0m
[32m[2022-08-30 16:49:14,713] [    INFO][0m - loss: 15.60861511, learning_rate: 0.02884705882352941, global_step: 490, interval_runtime: 15.133, interval_samples_per_second: 0.529, interval_steps_per_second: 0.661, epoch: 1.9216[0m
[32m[2022-08-30 16:49:29,887] [    INFO][0m - loss: 12.03346252, learning_rate: 0.028823529411764706, global_step: 500, interval_runtime: 15.1735, interval_samples_per_second: 0.527, interval_steps_per_second: 0.659, epoch: 1.9608[0m
[32m[2022-08-30 16:49:29,887] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 16:49:29,887] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-30 16:49:29,887] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-08-30 16:49:29,888] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-08-30 16:49:29,888] [    INFO][0m -   Total prediction steps = 259[0m
[32m[2022-08-30 16:53:28,686] [    INFO][0m - eval_loss: 14.78536605834961, eval_accuracy: 0.017408123791102514, eval_runtime: 238.7975, eval_samples_per_second: 8.66, eval_steps_per_second: 1.085, epoch: 1.9608[0m
[32m[2022-08-30 16:53:28,686] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-08-30 16:53:28,686] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 16:54:09,780] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-08-30 16:54:09,780] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-08-30 16:54:26,798] [    INFO][0m - loss: 13.57144775, learning_rate: 0.0288, global_step: 510, interval_runtime: 296.9112, interval_samples_per_second: 0.027, interval_steps_per_second: 0.034, epoch: 2.0[0m
[32m[2022-08-30 16:54:44,500] [    INFO][0m - loss: 13.49805756, learning_rate: 0.028776470588235292, global_step: 520, interval_runtime: 17.7014, interval_samples_per_second: 0.452, interval_steps_per_second: 0.565, epoch: 2.0392[0m
[32m[2022-08-30 16:55:00,197] [    INFO][0m - loss: 15.36812744, learning_rate: 0.028752941176470585, global_step: 530, interval_runtime: 15.6971, interval_samples_per_second: 0.51, interval_steps_per_second: 0.637, epoch: 2.0784[0m
[32m[2022-08-30 16:55:15,920] [    INFO][0m - loss: 13.36844177, learning_rate: 0.028729411764705882, global_step: 540, interval_runtime: 15.723, interval_samples_per_second: 0.509, interval_steps_per_second: 0.636, epoch: 2.1176[0m
[32m[2022-08-30 16:55:31,653] [    INFO][0m - loss: 13.25823059, learning_rate: 0.02870588235294118, global_step: 550, interval_runtime: 15.7338, interval_samples_per_second: 0.508, interval_steps_per_second: 0.636, epoch: 2.1569[0m
[32m[2022-08-30 16:55:47,382] [    INFO][0m - loss: 12.90523834, learning_rate: 0.028682352941176468, global_step: 560, interval_runtime: 15.7279, interval_samples_per_second: 0.509, interval_steps_per_second: 0.636, epoch: 2.1961[0m
[32m[2022-08-30 16:56:03,118] [    INFO][0m - loss: 14.64723663, learning_rate: 0.028658823529411764, global_step: 570, interval_runtime: 15.7364, interval_samples_per_second: 0.508, interval_steps_per_second: 0.635, epoch: 2.2353[0m
[32m[2022-08-30 16:56:18,899] [    INFO][0m - loss: 14.23151398, learning_rate: 0.028635294117647057, global_step: 580, interval_runtime: 15.7815, interval_samples_per_second: 0.507, interval_steps_per_second: 0.634, epoch: 2.2745[0m
[32m[2022-08-30 16:56:34,739] [    INFO][0m - loss: 14.53700867, learning_rate: 0.02861176470588235, global_step: 590, interval_runtime: 15.8399, interval_samples_per_second: 0.505, interval_steps_per_second: 0.631, epoch: 2.3137[0m
[32m[2022-08-30 16:56:50,615] [    INFO][0m - loss: 13.40019226, learning_rate: 0.028588235294117643, global_step: 600, interval_runtime: 15.8752, interval_samples_per_second: 0.504, interval_steps_per_second: 0.63, epoch: 2.3529[0m
[32m[2022-08-30 16:56:50,615] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 16:56:50,616] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-30 16:56:50,616] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-08-30 16:56:50,616] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-08-30 16:56:50,616] [    INFO][0m -   Total prediction steps = 259[0m
[32m[2022-08-30 17:01:08,483] [    INFO][0m - eval_loss: 14.510601997375488, eval_accuracy: 0.03433268858800774, eval_runtime: 257.8664, eval_samples_per_second: 8.02, eval_steps_per_second: 1.004, epoch: 2.3529[0m
[32m[2022-08-30 17:01:08,483] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-08-30 17:01:08,483] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 17:01:45,745] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-08-30 17:01:45,746] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-08-30 17:02:06,518] [    INFO][0m - loss: 14.82214355, learning_rate: 0.02856470588235294, global_step: 610, interval_runtime: 315.9033, interval_samples_per_second: 0.025, interval_steps_per_second: 0.032, epoch: 2.3922[0m
[32m[2022-08-30 17:02:22,939] [    INFO][0m - loss: 15.32424927, learning_rate: 0.028541176470588236, global_step: 620, interval_runtime: 16.4211, interval_samples_per_second: 0.487, interval_steps_per_second: 0.609, epoch: 2.4314[0m
[32m[2022-08-30 17:02:39,427] [    INFO][0m - loss: 14.04227142, learning_rate: 0.028517647058823526, global_step: 630, interval_runtime: 16.4878, interval_samples_per_second: 0.485, interval_steps_per_second: 0.607, epoch: 2.4706[0m
[32m[2022-08-30 17:02:55,881] [    INFO][0m - loss: 14.95039368, learning_rate: 0.028494117647058823, global_step: 640, interval_runtime: 16.4547, interval_samples_per_second: 0.486, interval_steps_per_second: 0.608, epoch: 2.5098[0m
[32m[2022-08-30 17:03:12,367] [    INFO][0m - loss: 14.89619751, learning_rate: 0.028470588235294116, global_step: 650, interval_runtime: 16.4862, interval_samples_per_second: 0.485, interval_steps_per_second: 0.607, epoch: 2.549[0m
[32m[2022-08-30 17:03:28,927] [    INFO][0m - loss: 12.74321213, learning_rate: 0.028447058823529412, global_step: 660, interval_runtime: 16.5595, interval_samples_per_second: 0.483, interval_steps_per_second: 0.604, epoch: 2.5882[0m
[32m[2022-08-30 17:03:45,491] [    INFO][0m - loss: 13.11778259, learning_rate: 0.028423529411764705, global_step: 670, interval_runtime: 16.5637, interval_samples_per_second: 0.483, interval_steps_per_second: 0.604, epoch: 2.6275[0m
[32m[2022-08-30 17:04:02,089] [    INFO][0m - loss: 14.71605377, learning_rate: 0.028399999999999998, global_step: 680, interval_runtime: 16.5978, interval_samples_per_second: 0.482, interval_steps_per_second: 0.602, epoch: 2.6667[0m
[32m[2022-08-30 17:04:18,848] [    INFO][0m - loss: 14.37921143, learning_rate: 0.028376470588235295, global_step: 690, interval_runtime: 16.7594, interval_samples_per_second: 0.477, interval_steps_per_second: 0.597, epoch: 2.7059[0m
[32m[2022-08-30 17:04:35,554] [    INFO][0m - loss: 14.55435944, learning_rate: 0.028352941176470588, global_step: 700, interval_runtime: 16.7058, interval_samples_per_second: 0.479, interval_steps_per_second: 0.599, epoch: 2.7451[0m
[32m[2022-08-30 17:04:35,555] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 17:04:35,555] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-30 17:04:35,555] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-08-30 17:04:35,555] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-08-30 17:04:35,555] [    INFO][0m -   Total prediction steps = 259[0m
[32m[2022-08-30 17:09:12,902] [    INFO][0m - eval_loss: 15.715852737426758, eval_accuracy: 0.02321083172147002, eval_runtime: 277.3466, eval_samples_per_second: 7.456, eval_steps_per_second: 0.934, epoch: 2.7451[0m
[32m[2022-08-30 17:09:12,903] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-700[0m
[32m[2022-08-30 17:09:12,903] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 17:09:50,148] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-700/tokenizer_config.json[0m
[32m[2022-08-30 17:09:50,148] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-700/special_tokens_map.json[0m
[32m[2022-08-30 17:10:10,886] [    INFO][0m - loss: 15.15625305, learning_rate: 0.02832941176470588, global_step: 710, interval_runtime: 335.3316, interval_samples_per_second: 0.024, interval_steps_per_second: 0.03, epoch: 2.7843[0m
[32m[2022-08-30 17:10:27,959] [    INFO][0m - loss: 14.44543762, learning_rate: 0.028305882352941174, global_step: 720, interval_runtime: 17.073, interval_samples_per_second: 0.469, interval_steps_per_second: 0.586, epoch: 2.8235[0m
[32m[2022-08-30 17:10:45,118] [    INFO][0m - loss: 12.10619659, learning_rate: 0.02828235294117647, global_step: 730, interval_runtime: 17.1599, interval_samples_per_second: 0.466, interval_steps_per_second: 0.583, epoch: 2.8627[0m
[32m[2022-08-30 17:11:02,286] [    INFO][0m - loss: 14.71813965, learning_rate: 0.028258823529411767, global_step: 740, interval_runtime: 17.1677, interval_samples_per_second: 0.466, interval_steps_per_second: 0.582, epoch: 2.902[0m
[32m[2022-08-30 17:11:19,511] [    INFO][0m - loss: 11.84618683, learning_rate: 0.028235294117647056, global_step: 750, interval_runtime: 17.2252, interval_samples_per_second: 0.464, interval_steps_per_second: 0.581, epoch: 2.9412[0m
[32m[2022-08-30 17:11:36,640] [    INFO][0m - loss: 11.71127167, learning_rate: 0.028211764705882353, global_step: 760, interval_runtime: 17.1285, interval_samples_per_second: 0.467, interval_steps_per_second: 0.584, epoch: 2.9804[0m
[32m[2022-08-30 17:11:53,735] [    INFO][0m - loss: 11.22808151, learning_rate: 0.028188235294117646, global_step: 770, interval_runtime: 17.095, interval_samples_per_second: 0.468, interval_steps_per_second: 0.585, epoch: 3.0196[0m
[32m[2022-08-30 17:12:11,030] [    INFO][0m - loss: 12.70988541, learning_rate: 0.02816470588235294, global_step: 780, interval_runtime: 17.295, interval_samples_per_second: 0.463, interval_steps_per_second: 0.578, epoch: 3.0588[0m
[32m[2022-08-30 17:12:28,345] [    INFO][0m - loss: 12.99660034, learning_rate: 0.028141176470588232, global_step: 790, interval_runtime: 17.3153, interval_samples_per_second: 0.462, interval_steps_per_second: 0.578, epoch: 3.098[0m
[32m[2022-08-30 17:12:45,730] [    INFO][0m - loss: 12.64839172, learning_rate: 0.02811764705882353, global_step: 800, interval_runtime: 17.385, interval_samples_per_second: 0.46, interval_steps_per_second: 0.575, epoch: 3.1373[0m
[32m[2022-08-30 17:12:45,731] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 17:12:45,731] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-30 17:12:45,731] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-08-30 17:12:45,731] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-08-30 17:12:45,731] [    INFO][0m -   Total prediction steps = 259[0m
[32m[2022-08-30 17:17:42,936] [    INFO][0m - eval_loss: 15.59211254119873, eval_accuracy: 0.018858800773694392, eval_runtime: 297.204, eval_samples_per_second: 6.958, eval_steps_per_second: 0.871, epoch: 3.1373[0m
[32m[2022-08-30 17:17:42,936] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-800[0m
[32m[2022-08-30 17:17:42,937] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 17:18:22,248] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-800/tokenizer_config.json[0m
[32m[2022-08-30 17:18:22,248] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-800/special_tokens_map.json[0m
[32m[2022-08-30 17:18:44,173] [    INFO][0m - loss: 15.02427063, learning_rate: 0.028094117647058825, global_step: 810, interval_runtime: 358.4426, interval_samples_per_second: 0.022, interval_steps_per_second: 0.028, epoch: 3.1765[0m
[32m[2022-08-30 17:19:02,055] [    INFO][0m - loss: 14.60412903, learning_rate: 0.028070588235294115, global_step: 820, interval_runtime: 17.8818, interval_samples_per_second: 0.447, interval_steps_per_second: 0.559, epoch: 3.2157[0m
[32m[2022-08-30 17:19:19,932] [    INFO][0m - loss: 12.8006958, learning_rate: 0.02804705882352941, global_step: 830, interval_runtime: 17.8773, interval_samples_per_second: 0.447, interval_steps_per_second: 0.559, epoch: 3.2549[0m
[32m[2022-08-30 17:19:37,836] [    INFO][0m - loss: 14.46973114, learning_rate: 0.028023529411764704, global_step: 840, interval_runtime: 17.904, interval_samples_per_second: 0.447, interval_steps_per_second: 0.559, epoch: 3.2941[0m
[32m[2022-08-30 17:19:55,808] [    INFO][0m - loss: 12.68057251, learning_rate: 0.028, global_step: 850, interval_runtime: 17.9713, interval_samples_per_second: 0.445, interval_steps_per_second: 0.556, epoch: 3.3333[0m
[32m[2022-08-30 17:20:13,775] [    INFO][0m - loss: 16.52654266, learning_rate: 0.02797647058823529, global_step: 860, interval_runtime: 17.9678, interval_samples_per_second: 0.445, interval_steps_per_second: 0.557, epoch: 3.3725[0m
[32m[2022-08-30 17:20:31,760] [    INFO][0m - loss: 16.61828003, learning_rate: 0.027952941176470587, global_step: 870, interval_runtime: 17.9843, interval_samples_per_second: 0.445, interval_steps_per_second: 0.556, epoch: 3.4118[0m
[32m[2022-08-30 17:20:49,841] [    INFO][0m - loss: 14.08978729, learning_rate: 0.027929411764705883, global_step: 880, interval_runtime: 18.0817, interval_samples_per_second: 0.442, interval_steps_per_second: 0.553, epoch: 3.451[0m
[32m[2022-08-30 17:21:07,928] [    INFO][0m - loss: 14.69610443, learning_rate: 0.027905882352941176, global_step: 890, interval_runtime: 18.0867, interval_samples_per_second: 0.442, interval_steps_per_second: 0.553, epoch: 3.4902[0m
[32m[2022-08-30 17:21:25,976] [    INFO][0m - loss: 14.57679138, learning_rate: 0.02788235294117647, global_step: 900, interval_runtime: 18.048, interval_samples_per_second: 0.443, interval_steps_per_second: 0.554, epoch: 3.5294[0m
[32m[2022-08-30 17:21:25,977] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 17:21:25,977] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-30 17:21:25,977] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-08-30 17:21:25,977] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-08-30 17:21:25,977] [    INFO][0m -   Total prediction steps = 259[0m
[32m[2022-08-30 17:26:40,162] [    INFO][0m - eval_loss: 13.2144775390625, eval_accuracy: 0.08317214700193423, eval_runtime: 314.1846, eval_samples_per_second: 6.582, eval_steps_per_second: 0.824, epoch: 3.5294[0m
[32m[2022-08-30 17:26:40,163] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-900[0m
[32m[2022-08-30 17:26:40,163] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 17:27:16,486] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-900/tokenizer_config.json[0m
[32m[2022-08-30 17:27:16,487] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-900/special_tokens_map.json[0m
[32m[2022-08-30 17:27:38,453] [    INFO][0m - loss: 16.5497818, learning_rate: 0.027858823529411762, global_step: 910, interval_runtime: 372.4765, interval_samples_per_second: 0.021, interval_steps_per_second: 0.027, epoch: 3.5686[0m
[32m[2022-08-30 17:27:56,951] [    INFO][0m - loss: 16.21273041, learning_rate: 0.02783529411764706, global_step: 920, interval_runtime: 18.4981, interval_samples_per_second: 0.432, interval_steps_per_second: 0.541, epoch: 3.6078[0m
[32m[2022-08-30 17:28:15,644] [    INFO][0m - loss: 17.38697815, learning_rate: 0.027811764705882352, global_step: 930, interval_runtime: 18.6927, interval_samples_per_second: 0.428, interval_steps_per_second: 0.535, epoch: 3.6471[0m
[32m[2022-08-30 17:28:34,334] [    INFO][0m - loss: 11.64928131, learning_rate: 0.027788235294117645, global_step: 940, interval_runtime: 18.6901, interval_samples_per_second: 0.428, interval_steps_per_second: 0.535, epoch: 3.6863[0m
[32m[2022-08-30 17:28:52,953] [    INFO][0m - loss: 15.67823029, learning_rate: 0.02776470588235294, global_step: 950, interval_runtime: 18.6196, interval_samples_per_second: 0.43, interval_steps_per_second: 0.537, epoch: 3.7255[0m
[32m[2022-08-30 17:29:11,562] [    INFO][0m - loss: 14.7698349, learning_rate: 0.027741176470588234, global_step: 960, interval_runtime: 18.6084, interval_samples_per_second: 0.43, interval_steps_per_second: 0.537, epoch: 3.7647[0m
[32m[2022-08-30 17:29:30,207] [    INFO][0m - loss: 15.23000488, learning_rate: 0.02771764705882353, global_step: 970, interval_runtime: 18.646, interval_samples_per_second: 0.429, interval_steps_per_second: 0.536, epoch: 3.8039[0m
[32m[2022-08-30 17:29:48,882] [    INFO][0m - loss: 18.31020813, learning_rate: 0.02769411764705882, global_step: 980, interval_runtime: 18.6741, interval_samples_per_second: 0.428, interval_steps_per_second: 0.536, epoch: 3.8431[0m
[32m[2022-08-30 17:30:07,577] [    INFO][0m - loss: 12.16134109, learning_rate: 0.027670588235294117, global_step: 990, interval_runtime: 18.6957, interval_samples_per_second: 0.428, interval_steps_per_second: 0.535, epoch: 3.8824[0m
[32m[2022-08-30 17:30:26,283] [    INFO][0m - loss: 13.25799408, learning_rate: 0.02764705882352941, global_step: 1000, interval_runtime: 18.7059, interval_samples_per_second: 0.428, interval_steps_per_second: 0.535, epoch: 3.9216[0m
[32m[2022-08-30 17:30:26,284] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 17:30:26,284] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-30 17:30:26,284] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-08-30 17:30:26,284] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-08-30 17:30:26,284] [    INFO][0m -   Total prediction steps = 259[0m
[32m[2022-08-30 17:35:59,854] [    INFO][0m - eval_loss: 13.58700942993164, eval_accuracy: 0.04013539651837524, eval_runtime: 333.5692, eval_samples_per_second: 6.2, eval_steps_per_second: 0.776, epoch: 3.9216[0m
[32m[2022-08-30 17:35:59,854] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1000[0m
[32m[2022-08-30 17:35:59,854] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 17:36:40,363] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1000/tokenizer_config.json[0m
[32m[2022-08-30 17:36:40,364] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1000/special_tokens_map.json[0m
[32m[2022-08-30 17:37:03,638] [    INFO][0m - loss: 12.03103485, learning_rate: 0.027623529411764703, global_step: 1010, interval_runtime: 397.3547, interval_samples_per_second: 0.02, interval_steps_per_second: 0.025, epoch: 3.9608[0m
[32m[2022-08-30 17:37:18,684] [    INFO][0m - loss: 14.18451385, learning_rate: 0.0276, global_step: 1020, interval_runtime: 15.0458, interval_samples_per_second: 0.532, interval_steps_per_second: 0.665, epoch: 4.0[0m
[32m[2022-08-30 17:37:42,180] [    INFO][0m - loss: 18.80261993, learning_rate: 0.027576470588235293, global_step: 1030, interval_runtime: 23.4959, interval_samples_per_second: 0.34, interval_steps_per_second: 0.426, epoch: 4.0392[0m
[32m[2022-08-30 17:38:01,741] [    INFO][0m - loss: 14.82453613, learning_rate: 0.02755294117647059, global_step: 1040, interval_runtime: 19.5608, interval_samples_per_second: 0.409, interval_steps_per_second: 0.511, epoch: 4.0784[0m
[32m[2022-08-30 17:38:21,260] [    INFO][0m - loss: 13.51303253, learning_rate: 0.02752941176470588, global_step: 1050, interval_runtime: 19.5195, interval_samples_per_second: 0.41, interval_steps_per_second: 0.512, epoch: 4.1176[0m
[32m[2022-08-30 17:38:40,728] [    INFO][0m - loss: 11.32543106, learning_rate: 0.027505882352941175, global_step: 1060, interval_runtime: 19.4681, interval_samples_per_second: 0.411, interval_steps_per_second: 0.514, epoch: 4.1569[0m
[32m[2022-08-30 17:39:00,236] [    INFO][0m - loss: 13.82129059, learning_rate: 0.02748235294117647, global_step: 1070, interval_runtime: 19.5076, interval_samples_per_second: 0.41, interval_steps_per_second: 0.513, epoch: 4.1961[0m
[32m[2022-08-30 17:39:19,643] [    INFO][0m - loss: 17.69042664, learning_rate: 0.027458823529411765, global_step: 1080, interval_runtime: 19.4066, interval_samples_per_second: 0.412, interval_steps_per_second: 0.515, epoch: 4.2353[0m
[32m[2022-08-30 17:39:39,110] [    INFO][0m - loss: 12.96768494, learning_rate: 0.027435294117647058, global_step: 1090, interval_runtime: 19.4675, interval_samples_per_second: 0.411, interval_steps_per_second: 0.514, epoch: 4.2745[0m
[32m[2022-08-30 17:39:58,507] [    INFO][0m - loss: 11.81686783, learning_rate: 0.02741176470588235, global_step: 1100, interval_runtime: 19.397, interval_samples_per_second: 0.412, interval_steps_per_second: 0.516, epoch: 4.3137[0m
[32m[2022-08-30 17:39:58,508] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 17:39:58,508] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-30 17:39:58,508] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-08-30 17:39:58,508] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-08-30 17:39:58,508] [    INFO][0m -   Total prediction steps = 259[0m
[32m[2022-08-30 17:45:50,583] [    INFO][0m - eval_loss: 14.786933898925781, eval_accuracy: 0.027079303675048357, eval_runtime: 352.0742, eval_samples_per_second: 5.874, eval_steps_per_second: 0.736, epoch: 4.3137[0m
[32m[2022-08-30 17:45:50,583] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1100[0m
[32m[2022-08-30 17:45:50,583] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 17:46:12,683] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1100/tokenizer_config.json[0m
[32m[2022-08-30 17:46:12,684] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1100/special_tokens_map.json[0m
[32m[2022-08-30 17:46:34,450] [    INFO][0m - loss: 12.34273682, learning_rate: 0.027388235294117647, global_step: 1110, interval_runtime: 395.9424, interval_samples_per_second: 0.02, interval_steps_per_second: 0.025, epoch: 4.3529[0m
[32m[2022-08-30 17:46:54,637] [    INFO][0m - loss: 13.77433167, learning_rate: 0.02736470588235294, global_step: 1120, interval_runtime: 20.1873, interval_samples_per_second: 0.396, interval_steps_per_second: 0.495, epoch: 4.3922[0m
[32m[2022-08-30 17:47:14,775] [    INFO][0m - loss: 14.65979156, learning_rate: 0.027341176470588233, global_step: 1130, interval_runtime: 20.1386, interval_samples_per_second: 0.397, interval_steps_per_second: 0.497, epoch: 4.4314[0m
[32m[2022-08-30 17:47:34,844] [    INFO][0m - loss: 16.60864868, learning_rate: 0.02731764705882353, global_step: 1140, interval_runtime: 20.0682, interval_samples_per_second: 0.399, interval_steps_per_second: 0.498, epoch: 4.4706[0m
[32m[2022-08-30 17:47:54,945] [    INFO][0m - loss: 17.47568817, learning_rate: 0.027294117647058823, global_step: 1150, interval_runtime: 20.102, interval_samples_per_second: 0.398, interval_steps_per_second: 0.497, epoch: 4.5098[0m
[32m[2022-08-30 17:48:15,176] [    INFO][0m - loss: 13.60270386, learning_rate: 0.02727058823529412, global_step: 1160, interval_runtime: 20.23, interval_samples_per_second: 0.395, interval_steps_per_second: 0.494, epoch: 4.549[0m
[32m[2022-08-30 17:48:35,340] [    INFO][0m - loss: 16.8221405, learning_rate: 0.02724705882352941, global_step: 1170, interval_runtime: 20.1647, interval_samples_per_second: 0.397, interval_steps_per_second: 0.496, epoch: 4.5882[0m
[32m[2022-08-30 17:48:55,554] [    INFO][0m - loss: 13.7825943, learning_rate: 0.027223529411764705, global_step: 1180, interval_runtime: 20.2136, interval_samples_per_second: 0.396, interval_steps_per_second: 0.495, epoch: 4.6275[0m
[32m[2022-08-30 17:49:15,731] [    INFO][0m - loss: 14.97160339, learning_rate: 0.0272, global_step: 1190, interval_runtime: 20.1773, interval_samples_per_second: 0.396, interval_steps_per_second: 0.496, epoch: 4.6667[0m
[32m[2022-08-30 17:49:35,948] [    INFO][0m - loss: 14.27468567, learning_rate: 0.02717647058823529, global_step: 1200, interval_runtime: 20.217, interval_samples_per_second: 0.396, interval_steps_per_second: 0.495, epoch: 4.7059[0m
[32m[2022-08-30 17:49:35,949] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 17:49:35,949] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-30 17:49:35,949] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-08-30 17:49:35,949] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-08-30 17:49:35,949] [    INFO][0m -   Total prediction steps = 259[0m
[32m[2022-08-30 17:55:49,090] [    INFO][0m - eval_loss: 14.779136657714844, eval_accuracy: 0.04690522243713733, eval_runtime: 373.1404, eval_samples_per_second: 5.542, eval_steps_per_second: 0.694, epoch: 4.7059[0m
[32m[2022-08-30 17:55:49,090] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1200[0m
[32m[2022-08-30 17:55:49,091] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 17:56:04,750] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1200/tokenizer_config.json[0m
[32m[2022-08-30 17:56:04,750] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1200/special_tokens_map.json[0m
[32m[2022-08-30 17:56:06,110] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-30 17:56:06,110] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-900 (score: 0.08317214700193423).[0m
[32m[2022-08-30 17:56:16,368] [    INFO][0m - train_runtime: 5631.0336, train_samples_per_second: 18.078, train_steps_per_second: 2.264, train_loss: 15.980280049641927, epoch: 4.7059[0m
[32m[2022-08-30 17:56:16,370] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-30 17:56:16,370] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 17:57:07,891] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-30 17:57:07,891] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-30 17:57:07,892] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-30 17:57:07,892] [    INFO][0m -   epoch                    =     4.7059[0m
[32m[2022-08-30 17:57:07,893] [    INFO][0m -   train_loss               =    15.9803[0m
[32m[2022-08-30 17:57:07,893] [    INFO][0m -   train_runtime            = 1:33:51.03[0m
[32m[2022-08-30 17:57:07,893] [    INFO][0m -   train_samples_per_second =     18.078[0m
[32m[2022-08-30 17:57:07,893] [    INFO][0m -   train_steps_per_second   =      2.264[0m
[32m[2022-08-30 17:57:07,899] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-30 17:57:07,899] [    INFO][0m -   Num examples = 1784[0m
[32m[2022-08-30 17:57:07,899] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-08-30 17:57:07,899] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-08-30 17:57:07,899] [    INFO][0m -   Total prediction steps = 223[0m
[32m[2022-08-30 18:02:40,070] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-30 18:02:40,070] [    INFO][0m -   test_accuracy           =     0.0667[0m
[32m[2022-08-30 18:02:40,070] [    INFO][0m -   test_loss               =    12.0147[0m
[32m[2022-08-30 18:02:40,070] [    INFO][0m -   test_runtime            = 0:05:32.17[0m
[32m[2022-08-30 18:02:40,071] [    INFO][0m -   test_samples_per_second =      5.371[0m
[32m[2022-08-30 18:02:40,071] [    INFO][0m -   test_steps_per_second   =      0.671[0m
[32m[2022-08-30 18:02:40,071] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-30 18:02:40,071] [    INFO][0m -   Num examples = 2999[0m
[32m[2022-08-30 18:02:40,071] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-08-30 18:02:40,071] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-08-30 18:02:40,071] [    INFO][0m -   Total prediction steps = 375[0m
[32m[2022-08-30 18:12:28,437] [    INFO][0m - Predictions for csldcp saved to ./fewclue_submit_examples.[0m
metric <function main.<locals>.compute_metrics at 0x7f52fe354c10>
 
==========
tnews
==========
 
[33m[2022-08-30 18:12:32,861] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-30 18:12:32,861] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-30 18:12:32,861] [    INFO][0m - ============================================================[0m
[32m[2022-08-30 18:12:32,861] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-30 18:12:32,861] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-30 18:12:32,861] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-30 18:12:32,862] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-30 18:12:32,862] [    INFO][0m - early_stop_patience           :3[0m
[32m[2022-08-30 18:12:32,862] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-30 18:12:32,862] [    INFO][0m - model_name_or_path            :/ssd2/wanghuijuan03/ernie-3.0-1.5b-zh[0m
[32m[2022-08-30 18:12:32,862] [    INFO][0m - [0m
[32m[2022-08-30 18:12:32,862] [    INFO][0m - ============================================================[0m
[32m[2022-08-30 18:12:32,862] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-30 18:12:32,862] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-30 18:12:32,862] [    INFO][0m - encoder_hidden_size           :None[0m
[32m[2022-08-30 18:12:32,862] [    INFO][0m - prompt                        :{'soft':'‰∏ãËæπÊí≠Êä•‰∏ÄÂàô'}{'mask'}{'mask'}{'soft':'Êñ∞ÈóªÔºö'}{'text':'text_a'}[0m
[32m[2022-08-30 18:12:32,862] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-30 18:12:32,862] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-30 18:12:32,863] [    INFO][0m - task_name                     :tnews[0m
[32m[2022-08-30 18:12:32,863] [    INFO][0m - [0m
W0830 18:12:32.864245 56303 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0830 18:12:32.868319 56303 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[33m[2022-08-30 18:12:55,313] [ WARNING][0m - Encoder has already set as lstm, change `prompt_encoder` will reset parameters.[0m
[32m[2022-08-30 18:12:55,327] [    INFO][0m - Using template: [{'add_prefix_space': '', 'soft': '‰∏ã'}, {'add_prefix_space': '', 'soft': 'Ëæπ'}, {'add_prefix_space': '', 'soft': 'Êí≠'}, {'add_prefix_space': '', 'soft': 'Êä•'}, {'add_prefix_space': '', 'soft': '‰∏Ä'}, {'add_prefix_space': '', 'soft': 'Âàô'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'soft': 'Êñ∞'}, {'add_prefix_space': '', 'soft': 'Èóª'}, {'add_prefix_space': '', 'soft': 'Ôºö'}, {'add_prefix_space': '', 'text': 'text_a'}][0m
2022-08-30 18:12:55,336 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-30 18:12:55,463] [    INFO][0m - ============================================================[0m
[32m[2022-08-30 18:12:55,463] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-30 18:12:55,463] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-30 18:12:55,463] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-30 18:12:55,463] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-30 18:12:55,463] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-30 18:12:55,463] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-30 18:12:55,463] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-30 18:12:55,464] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-30 18:12:55,464] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-30 18:12:55,464] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-30 18:12:55,464] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-30 18:12:55,464] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-30 18:12:55,464] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-30 18:12:55,464] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-30 18:12:55,464] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-30 18:12:55,464] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-30 18:12:55,464] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-30 18:12:55,464] [    INFO][0m - eval_batch_size               :8[0m
[32m[2022-08-30 18:12:55,464] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-30 18:12:55,465] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-30 18:12:55,465] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-30 18:12:55,465] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-30 18:12:55,465] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-30 18:12:55,465] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-30 18:12:55,465] [    INFO][0m - freeze_plm                    :True[0m
[32m[2022-08-30 18:12:55,465] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-30 18:12:55,465] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-30 18:12:55,465] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-30 18:12:55,465] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-30 18:12:55,465] [    INFO][0m - learning_rate                 :0.003[0m
[32m[2022-08-30 18:12:55,465] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-30 18:12:55,465] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-30 18:12:55,466] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-30 18:12:55,466] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-30 18:12:55,466] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-30 18:12:55,466] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-30 18:12:55,466] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug30_18-12-32_instance-3bwob41y-01[0m
[32m[2022-08-30 18:12:55,466] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-30 18:12:55,466] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-30 18:12:55,466] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-30 18:12:55,466] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-30 18:12:55,466] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-30 18:12:55,466] [    INFO][0m - max_seq_length                :64[0m
[32m[2022-08-30 18:12:55,466] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-30 18:12:55,467] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-30 18:12:55,467] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-30 18:12:55,467] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-30 18:12:55,467] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-08-30 18:12:55,467] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-30 18:12:55,467] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-30 18:12:55,467] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-30 18:12:55,467] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-30 18:12:55,467] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-30 18:12:55,467] [    INFO][0m - per_device_eval_batch_size    :8[0m
[32m[2022-08-30 18:12:55,467] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-30 18:12:55,467] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-30 18:12:55,467] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-30 18:12:55,468] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-30 18:12:55,468] [    INFO][0m - ppt_learning_rate             :0.03[0m
[32m[2022-08-30 18:12:55,468] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-30 18:12:55,468] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-30 18:12:55,468] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-30 18:12:55,468] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-30 18:12:55,468] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-30 18:12:55,468] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-30 18:12:55,468] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-30 18:12:55,468] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-30 18:12:55,468] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-30 18:12:55,468] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-30 18:12:55,468] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-30 18:12:55,469] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-30 18:12:55,469] [    INFO][0m - seed                          :42[0m
[32m[2022-08-30 18:12:55,469] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-30 18:12:55,469] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-30 18:12:55,469] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-30 18:12:55,469] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-30 18:12:55,469] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-30 18:12:55,469] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-30 18:12:55,469] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-30 18:12:55,469] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-30 18:12:55,469] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-30 18:12:55,469] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-30 18:12:55,469] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-30 18:12:55,470] [    INFO][0m - [0m
[32m[2022-08-30 18:12:55,473] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-30 18:12:55,473] [    INFO][0m -   Num examples = 1185[0m
[32m[2022-08-30 18:12:55,473] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-08-30 18:12:55,473] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-30 18:12:55,473] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-30 18:12:55,473] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-30 18:12:55,473] [    INFO][0m -   Total optimization steps = 7450[0m
[32m[2022-08-30 18:12:55,473] [    INFO][0m -   Total num train samples = 59250[0m
[32m[2022-08-30 18:13:00,252] [    INFO][0m - loss: 23.48543854, learning_rate: 0.02995973154362416, global_step: 10, interval_runtime: 4.7781, interval_samples_per_second: 1.674, interval_steps_per_second: 2.093, epoch: 0.0671[0m
[32m[2022-08-30 18:13:03,632] [    INFO][0m - loss: 24.90718689, learning_rate: 0.029919463087248323, global_step: 20, interval_runtime: 3.3802, interval_samples_per_second: 2.367, interval_steps_per_second: 2.958, epoch: 0.1342[0m
[32m[2022-08-30 18:13:07,022] [    INFO][0m - loss: 11.38685837, learning_rate: 0.02987919463087248, global_step: 30, interval_runtime: 3.3898, interval_samples_per_second: 2.36, interval_steps_per_second: 2.95, epoch: 0.2013[0m
[32m[2022-08-30 18:13:10,421] [    INFO][0m - loss: 15.25684509, learning_rate: 0.02983892617449664, global_step: 40, interval_runtime: 3.3986, interval_samples_per_second: 2.354, interval_steps_per_second: 2.942, epoch: 0.2685[0m
[32m[2022-08-30 18:13:13,831] [    INFO][0m - loss: 14.39519806, learning_rate: 0.029798657718120805, global_step: 50, interval_runtime: 3.4103, interval_samples_per_second: 2.346, interval_steps_per_second: 2.932, epoch: 0.3356[0m
[32m[2022-08-30 18:13:17,272] [    INFO][0m - loss: 13.0731369, learning_rate: 0.029758389261744966, global_step: 60, interval_runtime: 3.4413, interval_samples_per_second: 2.325, interval_steps_per_second: 2.906, epoch: 0.4027[0m
[32m[2022-08-30 18:13:20,716] [    INFO][0m - loss: 12.63814163, learning_rate: 0.029718120805369126, global_step: 70, interval_runtime: 3.4436, interval_samples_per_second: 2.323, interval_steps_per_second: 2.904, epoch: 0.4698[0m
[32m[2022-08-30 18:13:24,181] [    INFO][0m - loss: 8.83959198, learning_rate: 0.029677852348993287, global_step: 80, interval_runtime: 3.4656, interval_samples_per_second: 2.308, interval_steps_per_second: 2.885, epoch: 0.5369[0m
[32m[2022-08-30 18:13:27,644] [    INFO][0m - loss: 11.9007782, learning_rate: 0.029637583892617447, global_step: 90, interval_runtime: 3.4622, interval_samples_per_second: 2.311, interval_steps_per_second: 2.888, epoch: 0.604[0m
[32m[2022-08-30 18:13:31,119] [    INFO][0m - loss: 7.8473793, learning_rate: 0.02959731543624161, global_step: 100, interval_runtime: 3.4758, interval_samples_per_second: 2.302, interval_steps_per_second: 2.877, epoch: 0.6711[0m
[32m[2022-08-30 18:13:31,120] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 18:13:31,120] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-08-30 18:13:31,120] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-08-30 18:13:31,120] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-08-30 18:13:31,120] [    INFO][0m -   Total prediction steps = 138[0m
[32m[2022-08-30 18:13:56,507] [    INFO][0m - eval_loss: 9.81751823425293, eval_accuracy: 0.412568306010929, eval_runtime: 25.3857, eval_samples_per_second: 43.253, eval_steps_per_second: 5.436, epoch: 0.6711[0m
[32m[2022-08-30 18:13:56,507] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-30 18:13:56,507] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 18:14:31,668] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-30 18:14:31,669] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-30 18:14:39,640] [    INFO][0m - loss: 12.24088669, learning_rate: 0.029557046979865772, global_step: 110, interval_runtime: 68.52, interval_samples_per_second: 0.117, interval_steps_per_second: 0.146, epoch: 0.7383[0m
[32m[2022-08-30 18:14:43,286] [    INFO][0m - loss: 10.34259109, learning_rate: 0.02951677852348993, global_step: 120, interval_runtime: 3.646, interval_samples_per_second: 2.194, interval_steps_per_second: 2.743, epoch: 0.8054[0m
[32m[2022-08-30 18:14:46,938] [    INFO][0m - loss: 9.81090469, learning_rate: 0.029476510067114093, global_step: 130, interval_runtime: 3.6523, interval_samples_per_second: 2.19, interval_steps_per_second: 2.738, epoch: 0.8725[0m
[32m[2022-08-30 18:14:50,591] [    INFO][0m - loss: 11.52161255, learning_rate: 0.029436241610738254, global_step: 140, interval_runtime: 3.6535, interval_samples_per_second: 2.19, interval_steps_per_second: 2.737, epoch: 0.9396[0m
[32m[2022-08-30 18:14:54,120] [    INFO][0m - loss: 10.18755646, learning_rate: 0.029395973154362418, global_step: 150, interval_runtime: 3.5289, interval_samples_per_second: 2.267, interval_steps_per_second: 2.834, epoch: 1.0067[0m
[32m[2022-08-30 18:14:57,806] [    INFO][0m - loss: 10.04720078, learning_rate: 0.029355704697986578, global_step: 160, interval_runtime: 3.6858, interval_samples_per_second: 2.17, interval_steps_per_second: 2.713, epoch: 1.0738[0m
[32m[2022-08-30 18:15:01,508] [    INFO][0m - loss: 10.15162506, learning_rate: 0.029315436241610735, global_step: 170, interval_runtime: 3.7022, interval_samples_per_second: 2.161, interval_steps_per_second: 2.701, epoch: 1.1409[0m
[32m[2022-08-30 18:15:05,218] [    INFO][0m - loss: 14.8722641, learning_rate: 0.0292751677852349, global_step: 180, interval_runtime: 3.7103, interval_samples_per_second: 2.156, interval_steps_per_second: 2.695, epoch: 1.2081[0m
[32m[2022-08-30 18:15:08,950] [    INFO][0m - loss: 15.10216522, learning_rate: 0.02923489932885906, global_step: 190, interval_runtime: 3.731, interval_samples_per_second: 2.144, interval_steps_per_second: 2.68, epoch: 1.2752[0m
[32m[2022-08-30 18:15:12,683] [    INFO][0m - loss: 8.11836014, learning_rate: 0.02919463087248322, global_step: 200, interval_runtime: 3.7335, interval_samples_per_second: 2.143, interval_steps_per_second: 2.678, epoch: 1.3423[0m
[32m[2022-08-30 18:15:12,684] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 18:15:12,684] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-08-30 18:15:12,684] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-08-30 18:15:12,684] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-08-30 18:15:12,684] [    INFO][0m -   Total prediction steps = 138[0m
[32m[2022-08-30 18:15:40,958] [    INFO][0m - eval_loss: 10.150819778442383, eval_accuracy: 0.37067395264116576, eval_runtime: 28.274, eval_samples_per_second: 38.834, eval_steps_per_second: 4.881, epoch: 1.3423[0m
[32m[2022-08-30 18:15:40,959] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-30 18:15:40,959] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 18:16:22,462] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-30 18:16:22,463] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-30 18:16:30,688] [    INFO][0m - loss: 8.80572433, learning_rate: 0.02915436241610738, global_step: 210, interval_runtime: 78.0042, interval_samples_per_second: 0.103, interval_steps_per_second: 0.128, epoch: 1.4094[0m
[32m[2022-08-30 18:16:34,524] [    INFO][0m - loss: 6.93235016, learning_rate: 0.02911409395973154, global_step: 220, interval_runtime: 3.8362, interval_samples_per_second: 2.085, interval_steps_per_second: 2.607, epoch: 1.4765[0m
[32m[2022-08-30 18:16:38,347] [    INFO][0m - loss: 10.49241791, learning_rate: 0.029073825503355702, global_step: 230, interval_runtime: 3.8228, interval_samples_per_second: 2.093, interval_steps_per_second: 2.616, epoch: 1.5436[0m
[32m[2022-08-30 18:16:42,207] [    INFO][0m - loss: 13.96211395, learning_rate: 0.029033557046979866, global_step: 240, interval_runtime: 3.8598, interval_samples_per_second: 2.073, interval_steps_per_second: 2.591, epoch: 1.6107[0m
[32m[2022-08-30 18:16:46,072] [    INFO][0m - loss: 10.17311554, learning_rate: 0.028993288590604026, global_step: 250, interval_runtime: 3.8651, interval_samples_per_second: 2.07, interval_steps_per_second: 2.587, epoch: 1.6779[0m
[32m[2022-08-30 18:16:49,935] [    INFO][0m - loss: 13.22896576, learning_rate: 0.028953020134228187, global_step: 260, interval_runtime: 3.8633, interval_samples_per_second: 2.071, interval_steps_per_second: 2.588, epoch: 1.745[0m
[32m[2022-08-30 18:16:53,802] [    INFO][0m - loss: 12.17151413, learning_rate: 0.028912751677852348, global_step: 270, interval_runtime: 3.867, interval_samples_per_second: 2.069, interval_steps_per_second: 2.586, epoch: 1.8121[0m
[32m[2022-08-30 18:16:57,674] [    INFO][0m - loss: 11.18494949, learning_rate: 0.028872483221476508, global_step: 280, interval_runtime: 3.8721, interval_samples_per_second: 2.066, interval_steps_per_second: 2.583, epoch: 1.8792[0m
[32m[2022-08-30 18:17:01,569] [    INFO][0m - loss: 6.86182404, learning_rate: 0.028832214765100672, global_step: 290, interval_runtime: 3.8945, interval_samples_per_second: 2.054, interval_steps_per_second: 2.568, epoch: 1.9463[0m
[32m[2022-08-30 18:17:05,380] [    INFO][0m - loss: 9.69132156, learning_rate: 0.02879194630872483, global_step: 300, interval_runtime: 3.8122, interval_samples_per_second: 2.099, interval_steps_per_second: 2.623, epoch: 2.0134[0m
[32m[2022-08-30 18:17:05,381] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 18:17:05,381] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-08-30 18:17:05,381] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-08-30 18:17:05,381] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-08-30 18:17:05,381] [    INFO][0m -   Total prediction steps = 138[0m
[32m[2022-08-30 18:17:36,586] [    INFO][0m - eval_loss: 12.610599517822266, eval_accuracy: 0.41712204007285975, eval_runtime: 31.2042, eval_samples_per_second: 35.188, eval_steps_per_second: 4.422, epoch: 2.0134[0m
[32m[2022-08-30 18:17:36,586] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-30 18:17:36,587] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 18:18:20,783] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-30 18:18:20,785] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-30 18:18:29,195] [    INFO][0m - loss: 10.07966766, learning_rate: 0.02875167785234899, global_step: 310, interval_runtime: 83.8146, interval_samples_per_second: 0.095, interval_steps_per_second: 0.119, epoch: 2.0805[0m
[32m[2022-08-30 18:18:33,251] [    INFO][0m - loss: 8.07293167, learning_rate: 0.028711409395973154, global_step: 320, interval_runtime: 4.0561, interval_samples_per_second: 1.972, interval_steps_per_second: 2.465, epoch: 2.1477[0m
[32m[2022-08-30 18:18:37,308] [    INFO][0m - loss: 7.88119431, learning_rate: 0.028671140939597314, global_step: 330, interval_runtime: 4.0563, interval_samples_per_second: 1.972, interval_steps_per_second: 2.465, epoch: 2.2148[0m
[32m[2022-08-30 18:18:41,392] [    INFO][0m - loss: 10.22496033, learning_rate: 0.02863087248322148, global_step: 340, interval_runtime: 4.0841, interval_samples_per_second: 1.959, interval_steps_per_second: 2.449, epoch: 2.2819[0m
[32m[2022-08-30 18:18:45,473] [    INFO][0m - loss: 7.53224182, learning_rate: 0.028590604026845635, global_step: 350, interval_runtime: 4.081, interval_samples_per_second: 1.96, interval_steps_per_second: 2.45, epoch: 2.349[0m
[32m[2022-08-30 18:18:49,561] [    INFO][0m - loss: 5.93054047, learning_rate: 0.028550335570469796, global_step: 360, interval_runtime: 4.0882, interval_samples_per_second: 1.957, interval_steps_per_second: 2.446, epoch: 2.4161[0m
[32m[2022-08-30 18:18:53,661] [    INFO][0m - loss: 5.90692062, learning_rate: 0.02851006711409396, global_step: 370, interval_runtime: 4.1003, interval_samples_per_second: 1.951, interval_steps_per_second: 2.439, epoch: 2.4832[0m
[32m[2022-08-30 18:18:57,767] [    INFO][0m - loss: 7.74698715, learning_rate: 0.02846979865771812, global_step: 380, interval_runtime: 4.1059, interval_samples_per_second: 1.948, interval_steps_per_second: 2.436, epoch: 2.5503[0m
[32m[2022-08-30 18:19:01,899] [    INFO][0m - loss: 6.32347031, learning_rate: 0.02842953020134228, global_step: 390, interval_runtime: 4.1322, interval_samples_per_second: 1.936, interval_steps_per_second: 2.42, epoch: 2.6174[0m
[32m[2022-08-30 18:19:06,027] [    INFO][0m - loss: 5.95011902, learning_rate: 0.02838926174496644, global_step: 400, interval_runtime: 4.1274, interval_samples_per_second: 1.938, interval_steps_per_second: 2.423, epoch: 2.6846[0m
[32m[2022-08-30 18:19:06,028] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 18:19:06,028] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-08-30 18:19:06,028] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-08-30 18:19:06,028] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-08-30 18:19:06,028] [    INFO][0m -   Total prediction steps = 138[0m
[32m[2022-08-30 18:19:40,739] [    INFO][0m - eval_loss: 9.822690963745117, eval_accuracy: 0.3825136612021858, eval_runtime: 34.7105, eval_samples_per_second: 31.633, eval_steps_per_second: 3.976, epoch: 2.6846[0m
[32m[2022-08-30 18:19:40,739] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-30 18:19:40,739] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 18:20:23,490] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-30 18:20:23,490] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-30 18:20:32,211] [    INFO][0m - loss: 8.45471268, learning_rate: 0.028348993288590602, global_step: 410, interval_runtime: 86.185, interval_samples_per_second: 0.093, interval_steps_per_second: 0.116, epoch: 2.7517[0m
[32m[2022-08-30 18:20:36,496] [    INFO][0m - loss: 10.0700882, learning_rate: 0.028308724832214763, global_step: 420, interval_runtime: 4.2843, interval_samples_per_second: 1.867, interval_steps_per_second: 2.334, epoch: 2.8188[0m
[32m[2022-08-30 18:20:40,810] [    INFO][0m - loss: 9.39584045, learning_rate: 0.028268456375838927, global_step: 430, interval_runtime: 4.3138, interval_samples_per_second: 1.855, interval_steps_per_second: 2.318, epoch: 2.8859[0m
[32m[2022-08-30 18:20:45,129] [    INFO][0m - loss: 10.08171539, learning_rate: 0.028228187919463084, global_step: 440, interval_runtime: 4.3198, interval_samples_per_second: 1.852, interval_steps_per_second: 2.315, epoch: 2.953[0m
[32m[2022-08-30 18:20:49,407] [    INFO][0m - loss: 5.52569275, learning_rate: 0.028187919463087248, global_step: 450, interval_runtime: 4.2775, interval_samples_per_second: 1.87, interval_steps_per_second: 2.338, epoch: 3.0201[0m
[32m[2022-08-30 18:20:53,773] [    INFO][0m - loss: 3.33216782, learning_rate: 0.02814765100671141, global_step: 460, interval_runtime: 4.3658, interval_samples_per_second: 1.832, interval_steps_per_second: 2.291, epoch: 3.0872[0m
[32m[2022-08-30 18:20:58,149] [    INFO][0m - loss: 6.07887459, learning_rate: 0.02810738255033557, global_step: 470, interval_runtime: 4.376, interval_samples_per_second: 1.828, interval_steps_per_second: 2.285, epoch: 3.1544[0m
[32m[2022-08-30 18:21:02,515] [    INFO][0m - loss: 4.40440674, learning_rate: 0.028067114093959733, global_step: 480, interval_runtime: 4.3656, interval_samples_per_second: 1.833, interval_steps_per_second: 2.291, epoch: 3.2215[0m
[32m[2022-08-30 18:21:06,886] [    INFO][0m - loss: 5.1220295, learning_rate: 0.02802684563758389, global_step: 490, interval_runtime: 4.3715, interval_samples_per_second: 1.83, interval_steps_per_second: 2.288, epoch: 3.2886[0m
[32m[2022-08-30 18:21:11,277] [    INFO][0m - loss: 7.30481339, learning_rate: 0.02798657718120805, global_step: 500, interval_runtime: 4.3907, interval_samples_per_second: 1.822, interval_steps_per_second: 2.278, epoch: 3.3557[0m
[32m[2022-08-30 18:21:11,277] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 18:21:11,277] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-08-30 18:21:11,278] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-08-30 18:21:11,278] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-08-30 18:21:11,278] [    INFO][0m -   Total prediction steps = 138[0m
[32m[2022-08-30 18:21:49,430] [    INFO][0m - eval_loss: 8.836247444152832, eval_accuracy: 0.4544626593806922, eval_runtime: 38.1514, eval_samples_per_second: 28.78, eval_steps_per_second: 3.617, epoch: 3.3557[0m
[32m[2022-08-30 18:21:49,430] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-08-30 18:21:49,430] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 18:22:32,238] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-08-30 18:22:32,238] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-08-30 18:22:41,479] [    INFO][0m - loss: 7.09023743, learning_rate: 0.027946308724832215, global_step: 510, interval_runtime: 90.2027, interval_samples_per_second: 0.089, interval_steps_per_second: 0.111, epoch: 3.4228[0m
[32m[2022-08-30 18:22:46,017] [    INFO][0m - loss: 9.32883072, learning_rate: 0.027906040268456375, global_step: 520, interval_runtime: 4.5381, interval_samples_per_second: 1.763, interval_steps_per_second: 2.204, epoch: 3.4899[0m
[32m[2022-08-30 18:22:50,573] [    INFO][0m - loss: 5.07852325, learning_rate: 0.027865771812080536, global_step: 530, interval_runtime: 4.5556, interval_samples_per_second: 1.756, interval_steps_per_second: 2.195, epoch: 3.557[0m
[32m[2022-08-30 18:22:55,128] [    INFO][0m - loss: 4.16339989, learning_rate: 0.027825503355704696, global_step: 540, interval_runtime: 4.5556, interval_samples_per_second: 1.756, interval_steps_per_second: 2.195, epoch: 3.6242[0m
[32m[2022-08-30 18:22:59,684] [    INFO][0m - loss: 7.4815506, learning_rate: 0.027785234899328857, global_step: 550, interval_runtime: 4.556, interval_samples_per_second: 1.756, interval_steps_per_second: 2.195, epoch: 3.6913[0m
[32m[2022-08-30 18:23:04,255] [    INFO][0m - loss: 5.00414658, learning_rate: 0.02774496644295302, global_step: 560, interval_runtime: 4.5705, interval_samples_per_second: 1.75, interval_steps_per_second: 2.188, epoch: 3.7584[0m
[32m[2022-08-30 18:23:08,831] [    INFO][0m - loss: 5.90145721, learning_rate: 0.02770469798657718, global_step: 570, interval_runtime: 4.5759, interval_samples_per_second: 1.748, interval_steps_per_second: 2.185, epoch: 3.8255[0m
[32m[2022-08-30 18:23:13,431] [    INFO][0m - loss: 5.19404449, learning_rate: 0.02766442953020134, global_step: 580, interval_runtime: 4.5995, interval_samples_per_second: 1.739, interval_steps_per_second: 2.174, epoch: 3.8926[0m
[32m[2022-08-30 18:23:18,040] [    INFO][0m - loss: 4.26388626, learning_rate: 0.027624161073825503, global_step: 590, interval_runtime: 4.6093, interval_samples_per_second: 1.736, interval_steps_per_second: 2.17, epoch: 3.9597[0m
[32m[2022-08-30 18:23:22,537] [    INFO][0m - loss: 5.0061924, learning_rate: 0.027583892617449663, global_step: 600, interval_runtime: 4.4975, interval_samples_per_second: 1.779, interval_steps_per_second: 2.223, epoch: 4.0268[0m
[32m[2022-08-30 18:23:22,538] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 18:23:22,538] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-08-30 18:23:22,538] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-08-30 18:23:22,538] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-08-30 18:23:22,538] [    INFO][0m -   Total prediction steps = 138[0m
[32m[2022-08-30 18:24:03,914] [    INFO][0m - eval_loss: 8.055782318115234, eval_accuracy: 0.424408014571949, eval_runtime: 41.3751, eval_samples_per_second: 26.538, eval_steps_per_second: 3.335, epoch: 4.0268[0m
[32m[2022-08-30 18:24:03,914] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-08-30 18:24:03,914] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 18:24:45,888] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-08-30 18:24:45,889] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-08-30 18:24:55,086] [    INFO][0m - loss: 4.42614403, learning_rate: 0.027543624161073827, global_step: 610, interval_runtime: 92.5482, interval_samples_per_second: 0.086, interval_steps_per_second: 0.108, epoch: 4.094[0m
[32m[2022-08-30 18:24:59,880] [    INFO][0m - loss: 3.19925423, learning_rate: 0.027503355704697988, global_step: 620, interval_runtime: 4.794, interval_samples_per_second: 1.669, interval_steps_per_second: 2.086, epoch: 4.1611[0m
[32m[2022-08-30 18:25:04,669] [    INFO][0m - loss: 6.48668365, learning_rate: 0.027463087248322145, global_step: 630, interval_runtime: 4.7894, interval_samples_per_second: 1.67, interval_steps_per_second: 2.088, epoch: 4.2282[0m
[32m[2022-08-30 18:25:09,473] [    INFO][0m - loss: 2.86348095, learning_rate: 0.02742281879194631, global_step: 640, interval_runtime: 4.8041, interval_samples_per_second: 1.665, interval_steps_per_second: 2.082, epoch: 4.2953[0m
[32m[2022-08-30 18:25:14,303] [    INFO][0m - loss: 6.72602615, learning_rate: 0.02738255033557047, global_step: 650, interval_runtime: 4.8303, interval_samples_per_second: 1.656, interval_steps_per_second: 2.07, epoch: 4.3624[0m
[32m[2022-08-30 18:25:19,116] [    INFO][0m - loss: 5.54843559, learning_rate: 0.02734228187919463, global_step: 660, interval_runtime: 4.8129, interval_samples_per_second: 1.662, interval_steps_per_second: 2.078, epoch: 4.4295[0m
[32m[2022-08-30 18:25:23,961] [    INFO][0m - loss: 4.78678398, learning_rate: 0.02730201342281879, global_step: 670, interval_runtime: 4.8441, interval_samples_per_second: 1.651, interval_steps_per_second: 2.064, epoch: 4.4966[0m
[32m[2022-08-30 18:25:28,824] [    INFO][0m - loss: 4.16243858, learning_rate: 0.02726174496644295, global_step: 680, interval_runtime: 4.8637, interval_samples_per_second: 1.645, interval_steps_per_second: 2.056, epoch: 4.5638[0m
[32m[2022-08-30 18:25:33,669] [    INFO][0m - loss: 4.89575577, learning_rate: 0.02722147651006711, global_step: 690, interval_runtime: 4.8451, interval_samples_per_second: 1.651, interval_steps_per_second: 2.064, epoch: 4.6309[0m
[32m[2022-08-30 18:25:38,519] [    INFO][0m - loss: 3.29192352, learning_rate: 0.027181208053691275, global_step: 700, interval_runtime: 4.8499, interval_samples_per_second: 1.65, interval_steps_per_second: 2.062, epoch: 4.698[0m
[32m[2022-08-30 18:25:38,520] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 18:25:38,520] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-08-30 18:25:38,520] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-08-30 18:25:38,520] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-08-30 18:25:38,520] [    INFO][0m -   Total prediction steps = 138[0m
[32m[2022-08-30 18:26:23,748] [    INFO][0m - eval_loss: 7.343472957611084, eval_accuracy: 0.5, eval_runtime: 45.2273, eval_samples_per_second: 24.277, eval_steps_per_second: 3.051, epoch: 4.698[0m
[32m[2022-08-30 18:26:23,748] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-700[0m
[32m[2022-08-30 18:26:23,748] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 18:27:01,917] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-700/tokenizer_config.json[0m
[32m[2022-08-30 18:27:01,918] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-700/special_tokens_map.json[0m
[32m[2022-08-30 18:27:11,748] [    INFO][0m - loss: 2.55427723, learning_rate: 0.027140939597315436, global_step: 710, interval_runtime: 93.2294, interval_samples_per_second: 0.086, interval_steps_per_second: 0.107, epoch: 4.7651[0m
[32m[2022-08-30 18:27:16,748] [    INFO][0m - loss: 4.64824982, learning_rate: 0.027100671140939597, global_step: 720, interval_runtime: 4.9997, interval_samples_per_second: 1.6, interval_steps_per_second: 2.0, epoch: 4.8322[0m
[32m[2022-08-30 18:27:21,756] [    INFO][0m - loss: 4.91196518, learning_rate: 0.027060402684563757, global_step: 730, interval_runtime: 5.0079, interval_samples_per_second: 1.597, interval_steps_per_second: 1.997, epoch: 4.8993[0m
[32m[2022-08-30 18:27:26,731] [    INFO][0m - loss: 3.671064, learning_rate: 0.027020134228187918, global_step: 740, interval_runtime: 4.9745, interval_samples_per_second: 1.608, interval_steps_per_second: 2.01, epoch: 4.9664[0m
[32m[2022-08-30 18:27:31,650] [    INFO][0m - loss: 5.10896454, learning_rate: 0.02697986577181208, global_step: 750, interval_runtime: 4.9193, interval_samples_per_second: 1.626, interval_steps_per_second: 2.033, epoch: 5.0336[0m
[32m[2022-08-30 18:27:37,489] [    INFO][0m - loss: 1.9387373, learning_rate: 0.02693959731543624, global_step: 760, interval_runtime: 5.0596, interval_samples_per_second: 1.581, interval_steps_per_second: 1.976, epoch: 5.1007[0m
[32m[2022-08-30 18:27:42,556] [    INFO][0m - loss: 4.44543724, learning_rate: 0.0268993288590604, global_step: 770, interval_runtime: 5.8461, interval_samples_per_second: 1.368, interval_steps_per_second: 1.711, epoch: 5.1678[0m
[32m[2022-08-30 18:27:47,633] [    INFO][0m - loss: 3.94121017, learning_rate: 0.026859060402684563, global_step: 780, interval_runtime: 5.0777, interval_samples_per_second: 1.576, interval_steps_per_second: 1.969, epoch: 5.2349[0m
[32m[2022-08-30 18:27:52,832] [    INFO][0m - loss: 4.25832253, learning_rate: 0.026818791946308724, global_step: 790, interval_runtime: 5.1976, interval_samples_per_second: 1.539, interval_steps_per_second: 1.924, epoch: 5.302[0m
[32m[2022-08-30 18:27:57,966] [    INFO][0m - loss: 4.0353138, learning_rate: 0.026778523489932888, global_step: 800, interval_runtime: 5.1337, interval_samples_per_second: 1.558, interval_steps_per_second: 1.948, epoch: 5.3691[0m
[32m[2022-08-30 18:27:57,966] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 18:27:57,967] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-08-30 18:27:57,967] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-08-30 18:27:57,967] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-08-30 18:27:57,967] [    INFO][0m -   Total prediction steps = 138[0m
[32m[2022-08-30 18:28:46,454] [    INFO][0m - eval_loss: 9.502013206481934, eval_accuracy: 0.4808743169398907, eval_runtime: 48.4871, eval_samples_per_second: 22.645, eval_steps_per_second: 2.846, epoch: 5.3691[0m
[32m[2022-08-30 18:28:46,455] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-800[0m
[32m[2022-08-30 18:28:46,455] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 18:29:29,109] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-800/tokenizer_config.json[0m
[32m[2022-08-30 18:29:29,109] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-800/special_tokens_map.json[0m
[32m[2022-08-30 18:29:38,940] [    INFO][0m - loss: 7.29890518, learning_rate: 0.026738255033557045, global_step: 810, interval_runtime: 100.9746, interval_samples_per_second: 0.079, interval_steps_per_second: 0.099, epoch: 5.4362[0m
[32m[2022-08-30 18:29:44,200] [    INFO][0m - loss: 5.18434753, learning_rate: 0.026697986577181206, global_step: 820, interval_runtime: 5.2604, interval_samples_per_second: 1.521, interval_steps_per_second: 1.901, epoch: 5.5034[0m
[32m[2022-08-30 18:29:49,459] [    INFO][0m - loss: 3.24449158, learning_rate: 0.02665771812080537, global_step: 830, interval_runtime: 5.2593, interval_samples_per_second: 1.521, interval_steps_per_second: 1.901, epoch: 5.5705[0m
[32m[2022-08-30 18:29:54,725] [    INFO][0m - loss: 5.10636215, learning_rate: 0.02661744966442953, global_step: 840, interval_runtime: 5.2653, interval_samples_per_second: 1.519, interval_steps_per_second: 1.899, epoch: 5.6376[0m
[32m[2022-08-30 18:30:00,009] [    INFO][0m - loss: 5.71713943, learning_rate: 0.02657718120805369, global_step: 850, interval_runtime: 5.2842, interval_samples_per_second: 1.514, interval_steps_per_second: 1.892, epoch: 5.7047[0m
[32m[2022-08-30 18:30:05,298] [    INFO][0m - loss: 4.03943329, learning_rate: 0.02653691275167785, global_step: 860, interval_runtime: 5.2897, interval_samples_per_second: 1.512, interval_steps_per_second: 1.89, epoch: 5.7718[0m
[32m[2022-08-30 18:30:10,597] [    INFO][0m - loss: 4.38236198, learning_rate: 0.026496644295302012, global_step: 870, interval_runtime: 5.2987, interval_samples_per_second: 1.51, interval_steps_per_second: 1.887, epoch: 5.8389[0m
[32m[2022-08-30 18:30:15,907] [    INFO][0m - loss: 5.62080269, learning_rate: 0.026456375838926172, global_step: 880, interval_runtime: 5.3102, interval_samples_per_second: 1.507, interval_steps_per_second: 1.883, epoch: 5.906[0m
[32m[2022-08-30 18:30:20,982] [    INFO][0m - loss: 4.55130806, learning_rate: 0.026416107382550336, global_step: 890, interval_runtime: 5.0752, interval_samples_per_second: 1.576, interval_steps_per_second: 1.97, epoch: 5.9732[0m
[32m[2022-08-30 18:30:26,433] [    INFO][0m - loss: 4.6641201, learning_rate: 0.026375838926174493, global_step: 900, interval_runtime: 5.4502, interval_samples_per_second: 1.468, interval_steps_per_second: 1.835, epoch: 6.0403[0m
[32m[2022-08-30 18:30:26,433] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 18:30:26,434] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-08-30 18:30:26,434] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-08-30 18:30:26,434] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-08-30 18:30:26,434] [    INFO][0m -   Total prediction steps = 138[0m
[32m[2022-08-30 18:31:18,212] [    INFO][0m - eval_loss: 15.265789031982422, eval_accuracy: 0.39162112932604737, eval_runtime: 51.7777, eval_samples_per_second: 21.206, eval_steps_per_second: 2.665, epoch: 6.0403[0m
[32m[2022-08-30 18:31:18,212] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-900[0m
[32m[2022-08-30 18:31:18,213] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 18:31:54,715] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-900/tokenizer_config.json[0m
[32m[2022-08-30 18:31:54,715] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-900/special_tokens_map.json[0m
[32m[2022-08-30 18:32:04,642] [    INFO][0m - loss: 4.38370972, learning_rate: 0.026335570469798657, global_step: 910, interval_runtime: 98.2091, interval_samples_per_second: 0.081, interval_steps_per_second: 0.102, epoch: 6.1074[0m
[32m[2022-08-30 18:32:10,153] [    INFO][0m - loss: 4.74083176, learning_rate: 0.026295302013422818, global_step: 920, interval_runtime: 5.5112, interval_samples_per_second: 1.452, interval_steps_per_second: 1.814, epoch: 6.1745[0m
[32m[2022-08-30 18:32:15,713] [    INFO][0m - loss: 2.97270927, learning_rate: 0.02625503355704698, global_step: 930, interval_runtime: 5.5596, interval_samples_per_second: 1.439, interval_steps_per_second: 1.799, epoch: 6.2416[0m
[32m[2022-08-30 18:32:21,267] [    INFO][0m - loss: 5.51821213, learning_rate: 0.026214765100671143, global_step: 940, interval_runtime: 5.5543, interval_samples_per_second: 1.44, interval_steps_per_second: 1.8, epoch: 6.3087[0m
[32m[2022-08-30 18:32:26,857] [    INFO][0m - loss: 5.20262833, learning_rate: 0.0261744966442953, global_step: 950, interval_runtime: 5.5898, interval_samples_per_second: 1.431, interval_steps_per_second: 1.789, epoch: 6.3758[0m
[32m[2022-08-30 18:32:32,403] [    INFO][0m - loss: 2.23295364, learning_rate: 0.02613422818791946, global_step: 960, interval_runtime: 5.5461, interval_samples_per_second: 1.442, interval_steps_per_second: 1.803, epoch: 6.443[0m
[32m[2022-08-30 18:32:37,988] [    INFO][0m - loss: 4.44364281, learning_rate: 0.026093959731543624, global_step: 970, interval_runtime: 5.5854, interval_samples_per_second: 1.432, interval_steps_per_second: 1.79, epoch: 6.5101[0m
[32m[2022-08-30 18:32:43,571] [    INFO][0m - loss: 5.57092819, learning_rate: 0.026053691275167785, global_step: 980, interval_runtime: 5.5828, interval_samples_per_second: 1.433, interval_steps_per_second: 1.791, epoch: 6.5772[0m
[32m[2022-08-30 18:32:49,217] [    INFO][0m - loss: 3.28727837, learning_rate: 0.026013422818791945, global_step: 990, interval_runtime: 5.6463, interval_samples_per_second: 1.417, interval_steps_per_second: 1.771, epoch: 6.6443[0m
[32m[2022-08-30 18:32:54,868] [    INFO][0m - loss: 2.8810545, learning_rate: 0.025973154362416106, global_step: 1000, interval_runtime: 5.6501, interval_samples_per_second: 1.416, interval_steps_per_second: 1.77, epoch: 6.7114[0m
[32m[2022-08-30 18:32:54,868] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 18:32:54,868] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-08-30 18:32:54,868] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-08-30 18:32:54,869] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-08-30 18:32:54,869] [    INFO][0m -   Total prediction steps = 138[0m
[32m[2022-08-30 18:33:50,220] [    INFO][0m - eval_loss: 13.892647743225098, eval_accuracy: 0.42531876138433516, eval_runtime: 55.3511, eval_samples_per_second: 19.837, eval_steps_per_second: 2.493, epoch: 6.7114[0m
[32m[2022-08-30 18:33:50,221] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1000[0m
[32m[2022-08-30 18:33:50,221] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 18:34:34,858] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1000/tokenizer_config.json[0m
[32m[2022-08-30 18:34:34,860] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1000/special_tokens_map.json[0m
[32m[2022-08-30 18:34:39,164] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-30 18:34:39,165] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-700 (score: 0.5).[0m
[32m[2022-08-30 18:34:53,584] [    INFO][0m - train_runtime: 1318.1098, train_samples_per_second: 44.951, train_steps_per_second: 5.652, train_loss: 7.405053512573242, epoch: 6.7114[0m
[32m[2022-08-30 18:34:53,588] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-30 18:34:53,588] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 18:35:35,464] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-30 18:35:35,465] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-30 18:35:35,466] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-30 18:35:35,467] [    INFO][0m -   epoch                    =     6.7114[0m
[32m[2022-08-30 18:35:35,467] [    INFO][0m -   train_loss               =     7.4051[0m
[32m[2022-08-30 18:35:35,467] [    INFO][0m -   train_runtime            = 0:21:58.10[0m
[32m[2022-08-30 18:35:35,467] [    INFO][0m -   train_samples_per_second =     44.951[0m
[32m[2022-08-30 18:35:35,467] [    INFO][0m -   train_steps_per_second   =      5.652[0m
[32m[2022-08-30 18:35:35,472] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-30 18:35:35,472] [    INFO][0m -   Num examples = 2010[0m
[32m[2022-08-30 18:35:35,472] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-08-30 18:35:35,472] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-08-30 18:35:35,472] [    INFO][0m -   Total prediction steps = 252[0m
[32m[2022-08-30 18:37:21,880] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-30 18:37:21,880] [    INFO][0m -   test_accuracy           =     0.4985[0m
[32m[2022-08-30 18:37:21,880] [    INFO][0m -   test_loss               =     7.5288[0m
[32m[2022-08-30 18:37:21,881] [    INFO][0m -   test_runtime            = 0:01:46.40[0m
[32m[2022-08-30 18:37:21,881] [    INFO][0m -   test_samples_per_second =      18.89[0m
[32m[2022-08-30 18:37:21,881] [    INFO][0m -   test_steps_per_second   =      2.368[0m
[32m[2022-08-30 18:37:21,881] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-30 18:37:21,881] [    INFO][0m -   Num examples = 1500[0m
[32m[2022-08-30 18:37:21,881] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-08-30 18:37:21,881] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-08-30 18:37:21,881] [    INFO][0m -   Total prediction steps = 188[0m
[32m[2022-08-30 18:38:46,618] [    INFO][0m - Predictions for tnewsf saved to ./fewclue_submit_examples.[0m
metric <function main.<locals>.compute_metrics at 0x7fa206ec4c10>
 
==========
iflytek
==========
 
[33m[2022-08-30 18:38:51,135] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-30 18:38:51,135] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-30 18:38:51,136] [    INFO][0m - ============================================================[0m
[32m[2022-08-30 18:38:51,136] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-30 18:38:51,136] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-30 18:38:51,136] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-30 18:38:51,136] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-30 18:38:51,136] [    INFO][0m - early_stop_patience           :3[0m
[32m[2022-08-30 18:38:51,136] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-30 18:38:51,136] [    INFO][0m - model_name_or_path            :/ssd2/wanghuijuan03/ernie-3.0-1.5b-zh[0m
[32m[2022-08-30 18:38:51,136] [    INFO][0m - [0m
[32m[2022-08-30 18:38:51,136] [    INFO][0m - ============================================================[0m
[32m[2022-08-30 18:38:51,136] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-30 18:38:51,136] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-30 18:38:51,137] [    INFO][0m - encoder_hidden_size           :None[0m
[32m[2022-08-30 18:38:51,137] [    INFO][0m - prompt                        :{'mask'}{'mask'}{'soft':'APPÊõ¥Êñ∞Êó•ÂøóÔºö'}{'text':'text_a'}[0m
[32m[2022-08-30 18:38:51,137] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-30 18:38:51,137] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-30 18:38:51,137] [    INFO][0m - task_name                     :iflytek[0m
[32m[2022-08-30 18:38:51,137] [    INFO][0m - [0m
W0830 18:38:51.138527 20322 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0830 18:38:51.142607 20322 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[33m[2022-08-30 18:39:12,137] [ WARNING][0m - Encoder has already set as lstm, change `prompt_encoder` will reset parameters.[0m
[32m[2022-08-30 18:39:12,148] [    INFO][0m - Using template: [{'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'soft': 'app'}, {'add_prefix_space': '', 'soft': 'Êõ¥'}, {'add_prefix_space': '', 'soft': 'Êñ∞'}, {'add_prefix_space': '', 'soft': 'Êó•'}, {'add_prefix_space': '', 'soft': 'Âøó'}, {'add_prefix_space': '', 'soft': 'Ôºö'}, {'add_prefix_space': '', 'text': 'text_a'}][0m
2022-08-30 18:39:12,174 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-30 18:39:12,349] [    INFO][0m - ============================================================[0m
[32m[2022-08-30 18:39:12,349] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-30 18:39:12,349] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-30 18:39:12,349] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-30 18:39:12,349] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-30 18:39:12,349] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-30 18:39:12,350] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-30 18:39:12,350] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-30 18:39:12,350] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-30 18:39:12,350] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-30 18:39:12,350] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-30 18:39:12,350] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-30 18:39:12,350] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-30 18:39:12,350] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-30 18:39:12,350] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-30 18:39:12,350] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-30 18:39:12,350] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-30 18:39:12,350] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-30 18:39:12,350] [    INFO][0m - eval_batch_size               :8[0m
[32m[2022-08-30 18:39:12,351] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-30 18:39:12,351] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-30 18:39:12,351] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-30 18:39:12,351] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-30 18:39:12,351] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-30 18:39:12,351] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-30 18:39:12,351] [    INFO][0m - freeze_plm                    :True[0m
[32m[2022-08-30 18:39:12,351] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-30 18:39:12,351] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-30 18:39:12,351] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-30 18:39:12,351] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-30 18:39:12,351] [    INFO][0m - learning_rate                 :0.003[0m
[32m[2022-08-30 18:39:12,351] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-30 18:39:12,352] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-30 18:39:12,352] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-30 18:39:12,352] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-30 18:39:12,352] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-30 18:39:12,352] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-30 18:39:12,352] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug30_18-38-51_instance-3bwob41y-01[0m
[32m[2022-08-30 18:39:12,352] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-30 18:39:12,352] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-30 18:39:12,352] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-30 18:39:12,352] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-30 18:39:12,352] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-30 18:39:12,352] [    INFO][0m - max_seq_length                :320[0m
[32m[2022-08-30 18:39:12,353] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-30 18:39:12,353] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-30 18:39:12,353] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-30 18:39:12,353] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-30 18:39:12,353] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-08-30 18:39:12,353] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-30 18:39:12,353] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-30 18:39:12,353] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-30 18:39:12,353] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-30 18:39:12,353] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-30 18:39:12,353] [    INFO][0m - per_device_eval_batch_size    :8[0m
[32m[2022-08-30 18:39:12,353] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-30 18:39:12,353] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-30 18:39:12,354] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-30 18:39:12,354] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-30 18:39:12,354] [    INFO][0m - ppt_learning_rate             :0.03[0m
[32m[2022-08-30 18:39:12,354] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-30 18:39:12,354] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-30 18:39:12,354] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-30 18:39:12,354] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-30 18:39:12,354] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-30 18:39:12,354] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-30 18:39:12,354] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-30 18:39:12,354] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-30 18:39:12,354] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-30 18:39:12,354] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-30 18:39:12,355] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-30 18:39:12,355] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-30 18:39:12,355] [    INFO][0m - seed                          :42[0m
[32m[2022-08-30 18:39:12,355] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-30 18:39:12,355] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-30 18:39:12,355] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-30 18:39:12,355] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-30 18:39:12,355] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-30 18:39:12,355] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-30 18:39:12,355] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-30 18:39:12,355] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-30 18:39:12,355] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-30 18:39:12,355] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-30 18:39:12,355] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-30 18:39:12,356] [    INFO][0m - [0m
[32m[2022-08-30 18:39:12,358] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-30 18:39:12,359] [    INFO][0m -   Num examples = 3024[0m
[32m[2022-08-30 18:39:12,359] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-08-30 18:39:12,359] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-30 18:39:12,359] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-30 18:39:12,359] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-30 18:39:12,359] [    INFO][0m -   Total optimization steps = 18900[0m
[32m[2022-08-30 18:39:12,359] [    INFO][0m -   Total num train samples = 151200[0m
[32m[2022-08-30 18:39:28,727] [    INFO][0m - loss: 24.98187561, learning_rate: 0.029984126984126985, global_step: 10, interval_runtime: 16.3672, interval_samples_per_second: 0.489, interval_steps_per_second: 0.611, epoch: 0.0265[0m
[32m[2022-08-30 18:39:43,992] [    INFO][0m - loss: 34.76703186, learning_rate: 0.029968253968253967, global_step: 20, interval_runtime: 15.2646, interval_samples_per_second: 0.524, interval_steps_per_second: 0.655, epoch: 0.0529[0m
[32m[2022-08-30 18:39:59,366] [    INFO][0m - loss: 35.80215759, learning_rate: 0.029952380952380953, global_step: 30, interval_runtime: 15.3742, interval_samples_per_second: 0.52, interval_steps_per_second: 0.65, epoch: 0.0794[0m
[32m[2022-08-30 18:40:14,743] [    INFO][0m - loss: 32.28535767, learning_rate: 0.029936507936507938, global_step: 40, interval_runtime: 15.3768, interval_samples_per_second: 0.52, interval_steps_per_second: 0.65, epoch: 0.1058[0m
[32m[2022-08-30 18:40:30,231] [    INFO][0m - loss: 25.86035156, learning_rate: 0.029920634920634917, global_step: 50, interval_runtime: 15.4889, interval_samples_per_second: 0.516, interval_steps_per_second: 0.646, epoch: 0.1323[0m
[32m[2022-08-30 18:40:45,728] [    INFO][0m - loss: 28.98338013, learning_rate: 0.029904761904761903, global_step: 60, interval_runtime: 15.4962, interval_samples_per_second: 0.516, interval_steps_per_second: 0.645, epoch: 0.1587[0m
[32m[2022-08-30 18:41:01,198] [    INFO][0m - loss: 28.10673828, learning_rate: 0.02988888888888889, global_step: 70, interval_runtime: 15.4708, interval_samples_per_second: 0.517, interval_steps_per_second: 0.646, epoch: 0.1852[0m
[32m[2022-08-30 18:41:16,716] [    INFO][0m - loss: 28.01719971, learning_rate: 0.02987301587301587, global_step: 80, interval_runtime: 15.5175, interval_samples_per_second: 0.516, interval_steps_per_second: 0.644, epoch: 0.2116[0m
[32m[2022-08-30 18:41:32,244] [    INFO][0m - loss: 23.88384094, learning_rate: 0.029857142857142856, global_step: 90, interval_runtime: 15.5284, interval_samples_per_second: 0.515, interval_steps_per_second: 0.644, epoch: 0.2381[0m
[32m[2022-08-30 18:41:47,754] [    INFO][0m - loss: 23.47805786, learning_rate: 0.02984126984126984, global_step: 100, interval_runtime: 15.5098, interval_samples_per_second: 0.516, interval_steps_per_second: 0.645, epoch: 0.2646[0m
[32m[2022-08-30 18:41:47,755] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 18:41:47,755] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-30 18:41:47,755] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-08-30 18:41:47,755] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-08-30 18:41:47,755] [    INFO][0m -   Total prediction steps = 172[0m
[32m[2022-08-30 18:43:59,820] [    INFO][0m - eval_loss: 23.768962860107422, eval_accuracy: 0.013838310269482883, eval_runtime: 132.0645, eval_samples_per_second: 10.396, eval_steps_per_second: 1.302, epoch: 0.2646[0m
[32m[2022-08-30 18:43:59,821] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-30 18:43:59,821] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 18:44:36,369] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-30 18:44:36,369] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-30 18:44:55,667] [    INFO][0m - loss: 22.36670837, learning_rate: 0.029825396825396824, global_step: 110, interval_runtime: 187.913, interval_samples_per_second: 0.043, interval_steps_per_second: 0.053, epoch: 0.291[0m
[32m[2022-08-30 18:45:11,223] [    INFO][0m - loss: 22.80893097, learning_rate: 0.02980952380952381, global_step: 120, interval_runtime: 15.556, interval_samples_per_second: 0.514, interval_steps_per_second: 0.643, epoch: 0.3175[0m
[32m[2022-08-30 18:45:26,822] [    INFO][0m - loss: 20.58494568, learning_rate: 0.029793650793650792, global_step: 130, interval_runtime: 15.5994, interval_samples_per_second: 0.513, interval_steps_per_second: 0.641, epoch: 0.3439[0m
[32m[2022-08-30 18:45:42,503] [    INFO][0m - loss: 21.18187561, learning_rate: 0.029777777777777778, global_step: 140, interval_runtime: 15.681, interval_samples_per_second: 0.51, interval_steps_per_second: 0.638, epoch: 0.3704[0m
[32m[2022-08-30 18:45:58,174] [    INFO][0m - loss: 23.47172241, learning_rate: 0.02976190476190476, global_step: 150, interval_runtime: 15.6706, interval_samples_per_second: 0.511, interval_steps_per_second: 0.638, epoch: 0.3968[0m
[32m[2022-08-30 18:46:13,868] [    INFO][0m - loss: 19.40976562, learning_rate: 0.029746031746031742, global_step: 160, interval_runtime: 15.6938, interval_samples_per_second: 0.51, interval_steps_per_second: 0.637, epoch: 0.4233[0m
[32m[2022-08-30 18:46:29,598] [    INFO][0m - loss: 20.39406128, learning_rate: 0.029730158730158728, global_step: 170, interval_runtime: 15.7301, interval_samples_per_second: 0.509, interval_steps_per_second: 0.636, epoch: 0.4497[0m
[32m[2022-08-30 18:46:45,350] [    INFO][0m - loss: 20.04848633, learning_rate: 0.029714285714285714, global_step: 180, interval_runtime: 15.7521, interval_samples_per_second: 0.508, interval_steps_per_second: 0.635, epoch: 0.4762[0m
[32m[2022-08-30 18:47:01,180] [    INFO][0m - loss: 18.79334564, learning_rate: 0.029698412698412696, global_step: 190, interval_runtime: 15.8296, interval_samples_per_second: 0.505, interval_steps_per_second: 0.632, epoch: 0.5026[0m
[32m[2022-08-30 18:47:16,986] [    INFO][0m - loss: 21.58179169, learning_rate: 0.029682539682539682, global_step: 200, interval_runtime: 15.8058, interval_samples_per_second: 0.506, interval_steps_per_second: 0.633, epoch: 0.5291[0m
[32m[2022-08-30 18:47:16,986] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 18:47:16,986] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-30 18:47:16,986] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-08-30 18:47:16,986] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-08-30 18:47:16,986] [    INFO][0m -   Total prediction steps = 172[0m
[32m[2022-08-30 18:49:38,014] [    INFO][0m - eval_loss: 18.478660583496094, eval_accuracy: 0.010196649672250545, eval_runtime: 141.0269, eval_samples_per_second: 9.736, eval_steps_per_second: 1.22, epoch: 0.5291[0m
[32m[2022-08-30 18:49:38,014] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-30 18:49:38,014] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 18:50:12,186] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-30 18:50:12,187] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-30 18:50:32,109] [    INFO][0m - loss: 19.31217346, learning_rate: 0.029666666666666668, global_step: 210, interval_runtime: 195.1231, interval_samples_per_second: 0.041, interval_steps_per_second: 0.051, epoch: 0.5556[0m
[32m[2022-08-30 18:50:48,324] [    INFO][0m - loss: 19.8814209, learning_rate: 0.02965079365079365, global_step: 220, interval_runtime: 16.2149, interval_samples_per_second: 0.493, interval_steps_per_second: 0.617, epoch: 0.582[0m
[32m[2022-08-30 18:51:04,613] [    INFO][0m - loss: 20.31616211, learning_rate: 0.029634920634920636, global_step: 230, interval_runtime: 16.2894, interval_samples_per_second: 0.491, interval_steps_per_second: 0.614, epoch: 0.6085[0m
[32m[2022-08-30 18:51:20,899] [    INFO][0m - loss: 19.99133453, learning_rate: 0.029619047619047618, global_step: 240, interval_runtime: 16.2853, interval_samples_per_second: 0.491, interval_steps_per_second: 0.614, epoch: 0.6349[0m
[32m[2022-08-30 18:51:37,248] [    INFO][0m - loss: 19.54971313, learning_rate: 0.0296031746031746, global_step: 250, interval_runtime: 16.3496, interval_samples_per_second: 0.489, interval_steps_per_second: 0.612, epoch: 0.6614[0m
[32m[2022-08-30 18:51:53,598] [    INFO][0m - loss: 20.43628845, learning_rate: 0.029587301587301586, global_step: 260, interval_runtime: 16.3505, interval_samples_per_second: 0.489, interval_steps_per_second: 0.612, epoch: 0.6878[0m
[32m[2022-08-30 18:52:09,985] [    INFO][0m - loss: 19.98449097, learning_rate: 0.02957142857142857, global_step: 270, interval_runtime: 16.3869, interval_samples_per_second: 0.488, interval_steps_per_second: 0.61, epoch: 0.7143[0m
[32m[2022-08-30 18:52:26,374] [    INFO][0m - loss: 16.34186096, learning_rate: 0.029555555555555554, global_step: 280, interval_runtime: 16.3884, interval_samples_per_second: 0.488, interval_steps_per_second: 0.61, epoch: 0.7407[0m
[32m[2022-08-30 18:52:42,825] [    INFO][0m - loss: 20.09738464, learning_rate: 0.02953968253968254, global_step: 290, interval_runtime: 16.4515, interval_samples_per_second: 0.486, interval_steps_per_second: 0.608, epoch: 0.7672[0m
[32m[2022-08-30 18:52:59,276] [    INFO][0m - loss: 16.28553314, learning_rate: 0.02952380952380952, global_step: 300, interval_runtime: 16.4507, interval_samples_per_second: 0.486, interval_steps_per_second: 0.608, epoch: 0.7937[0m
[32m[2022-08-30 18:52:59,277] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 18:52:59,277] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-30 18:52:59,277] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-08-30 18:52:59,277] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-08-30 18:52:59,277] [    INFO][0m -   Total prediction steps = 172[0m
[32m[2022-08-30 18:55:31,654] [    INFO][0m - eval_loss: 16.82807159423828, eval_accuracy: 0.011653313911143482, eval_runtime: 152.3771, eval_samples_per_second: 9.011, eval_steps_per_second: 1.129, epoch: 0.7937[0m
[32m[2022-08-30 18:55:31,655] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-30 18:55:31,655] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 18:56:08,229] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-30 18:56:08,230] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-30 18:56:28,924] [    INFO][0m - loss: 19.76172028, learning_rate: 0.029507936507936507, global_step: 310, interval_runtime: 209.6474, interval_samples_per_second: 0.038, interval_steps_per_second: 0.048, epoch: 0.8201[0m
[32m[2022-08-30 18:56:45,804] [    INFO][0m - loss: 19.65680389, learning_rate: 0.029492063492063493, global_step: 320, interval_runtime: 16.8807, interval_samples_per_second: 0.474, interval_steps_per_second: 0.592, epoch: 0.8466[0m
[32m[2022-08-30 18:57:02,695] [    INFO][0m - loss: 19.25337067, learning_rate: 0.029476190476190475, global_step: 330, interval_runtime: 16.8905, interval_samples_per_second: 0.474, interval_steps_per_second: 0.592, epoch: 0.873[0m
[32m[2022-08-30 18:57:19,673] [    INFO][0m - loss: 17.06845551, learning_rate: 0.029460317460317458, global_step: 340, interval_runtime: 16.9787, interval_samples_per_second: 0.471, interval_steps_per_second: 0.589, epoch: 0.8995[0m
[32m[2022-08-30 18:57:36,645] [    INFO][0m - loss: 16.49962769, learning_rate: 0.029444444444444443, global_step: 350, interval_runtime: 16.9716, interval_samples_per_second: 0.471, interval_steps_per_second: 0.589, epoch: 0.9259[0m
[32m[2022-08-30 18:57:53,658] [    INFO][0m - loss: 18.03419647, learning_rate: 0.029428571428571425, global_step: 360, interval_runtime: 17.0135, interval_samples_per_second: 0.47, interval_steps_per_second: 0.588, epoch: 0.9524[0m
[32m[2022-08-30 18:58:10,700] [    INFO][0m - loss: 18.1040802, learning_rate: 0.02941269841269841, global_step: 370, interval_runtime: 17.0413, interval_samples_per_second: 0.469, interval_steps_per_second: 0.587, epoch: 0.9788[0m
[32m[2022-08-30 18:58:28,169] [    INFO][0m - loss: 17.68120575, learning_rate: 0.029396825396825397, global_step: 380, interval_runtime: 17.4684, interval_samples_per_second: 0.458, interval_steps_per_second: 0.572, epoch: 1.0053[0m
[32m[2022-08-30 18:58:45,296] [    INFO][0m - loss: 18.58188629, learning_rate: 0.02938095238095238, global_step: 390, interval_runtime: 17.1269, interval_samples_per_second: 0.467, interval_steps_per_second: 0.584, epoch: 1.0317[0m
[32m[2022-08-30 18:59:02,418] [    INFO][0m - loss: 17.42799072, learning_rate: 0.029365079365079365, global_step: 400, interval_runtime: 17.1224, interval_samples_per_second: 0.467, interval_steps_per_second: 0.584, epoch: 1.0582[0m
[32m[2022-08-30 18:59:02,418] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 18:59:02,418] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-30 18:59:02,418] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-08-30 18:59:02,418] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-08-30 18:59:02,418] [    INFO][0m -   Total prediction steps = 172[0m
[32m[2022-08-30 19:01:46,274] [    INFO][0m - eval_loss: 16.780643463134766, eval_accuracy: 0.016023306627822288, eval_runtime: 163.8549, eval_samples_per_second: 8.379, eval_steps_per_second: 1.05, epoch: 1.0582[0m
[32m[2022-08-30 19:01:46,274] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-30 19:01:46,274] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 19:02:21,240] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-30 19:02:21,240] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-30 19:02:42,651] [    INFO][0m - loss: 15.05502472, learning_rate: 0.02934920634920635, global_step: 410, interval_runtime: 220.2327, interval_samples_per_second: 0.036, interval_steps_per_second: 0.045, epoch: 1.0847[0m
[32m[2022-08-30 19:03:00,222] [    INFO][0m - loss: 21.10442657, learning_rate: 0.029333333333333333, global_step: 420, interval_runtime: 17.5711, interval_samples_per_second: 0.455, interval_steps_per_second: 0.569, epoch: 1.1111[0m
[32m[2022-08-30 19:03:17,795] [    INFO][0m - loss: 16.7921524, learning_rate: 0.029317460317460315, global_step: 430, interval_runtime: 17.5733, interval_samples_per_second: 0.455, interval_steps_per_second: 0.569, epoch: 1.1376[0m
[32m[2022-08-30 19:03:35,408] [    INFO][0m - loss: 13.72030487, learning_rate: 0.0293015873015873, global_step: 440, interval_runtime: 17.6128, interval_samples_per_second: 0.454, interval_steps_per_second: 0.568, epoch: 1.164[0m
[32m[2022-08-30 19:03:53,050] [    INFO][0m - loss: 18.29953003, learning_rate: 0.029285714285714283, global_step: 450, interval_runtime: 17.6421, interval_samples_per_second: 0.453, interval_steps_per_second: 0.567, epoch: 1.1905[0m
[32m[2022-08-30 19:04:10,747] [    INFO][0m - loss: 17.51312866, learning_rate: 0.02926984126984127, global_step: 460, interval_runtime: 17.6968, interval_samples_per_second: 0.452, interval_steps_per_second: 0.565, epoch: 1.2169[0m
[32m[2022-08-30 19:04:28,474] [    INFO][0m - loss: 17.37488708, learning_rate: 0.029253968253968254, global_step: 470, interval_runtime: 17.7281, interval_samples_per_second: 0.451, interval_steps_per_second: 0.564, epoch: 1.2434[0m
[32m[2022-08-30 19:04:46,216] [    INFO][0m - loss: 15.34051056, learning_rate: 0.029238095238095237, global_step: 480, interval_runtime: 17.7412, interval_samples_per_second: 0.451, interval_steps_per_second: 0.564, epoch: 1.2698[0m
[32m[2022-08-30 19:05:03,985] [    INFO][0m - loss: 14.47445831, learning_rate: 0.029222222222222222, global_step: 490, interval_runtime: 17.769, interval_samples_per_second: 0.45, interval_steps_per_second: 0.563, epoch: 1.2963[0m
[32m[2022-08-30 19:05:21,752] [    INFO][0m - loss: 18.46473999, learning_rate: 0.029206349206349205, global_step: 500, interval_runtime: 17.7676, interval_samples_per_second: 0.45, interval_steps_per_second: 0.563, epoch: 1.3228[0m
[32m[2022-08-30 19:05:21,753] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 19:05:21,753] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-30 19:05:21,753] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-08-30 19:05:21,753] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-08-30 19:05:21,753] [    INFO][0m -   Total prediction steps = 172[0m
[32m[2022-08-30 19:08:17,156] [    INFO][0m - eval_loss: 13.377554893493652, eval_accuracy: 0.09249817916970138, eval_runtime: 175.4025, eval_samples_per_second: 7.828, eval_steps_per_second: 0.981, epoch: 1.3228[0m
[32m[2022-08-30 19:08:17,157] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-08-30 19:08:17,157] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 19:08:53,745] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-08-30 19:08:53,746] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-08-30 19:09:15,524] [    INFO][0m - loss: 15.60081329, learning_rate: 0.02919047619047619, global_step: 510, interval_runtime: 233.771, interval_samples_per_second: 0.034, interval_steps_per_second: 0.043, epoch: 1.3492[0m
[32m[2022-08-30 19:09:33,662] [    INFO][0m - loss: 17.29999847, learning_rate: 0.029174603174603173, global_step: 520, interval_runtime: 18.1383, interval_samples_per_second: 0.441, interval_steps_per_second: 0.551, epoch: 1.3757[0m
[32m[2022-08-30 19:09:51,903] [    INFO][0m - loss: 17.60232544, learning_rate: 0.029158730158730155, global_step: 530, interval_runtime: 18.2414, interval_samples_per_second: 0.439, interval_steps_per_second: 0.548, epoch: 1.4021[0m
[32m[2022-08-30 19:10:10,211] [    INFO][0m - loss: 14.46622925, learning_rate: 0.02914285714285714, global_step: 540, interval_runtime: 18.308, interval_samples_per_second: 0.437, interval_steps_per_second: 0.546, epoch: 1.4286[0m
[32m[2022-08-30 19:10:28,588] [    INFO][0m - loss: 16.64493103, learning_rate: 0.029126984126984126, global_step: 550, interval_runtime: 18.3763, interval_samples_per_second: 0.435, interval_steps_per_second: 0.544, epoch: 1.455[0m
[32m[2022-08-30 19:10:46,908] [    INFO][0m - loss: 17.72711945, learning_rate: 0.02911111111111111, global_step: 560, interval_runtime: 18.3208, interval_samples_per_second: 0.437, interval_steps_per_second: 0.546, epoch: 1.4815[0m
[32m[2022-08-30 19:11:05,303] [    INFO][0m - loss: 15.66746063, learning_rate: 0.029095238095238094, global_step: 570, interval_runtime: 18.3946, interval_samples_per_second: 0.435, interval_steps_per_second: 0.544, epoch: 1.5079[0m
[32m[2022-08-30 19:11:23,769] [    INFO][0m - loss: 14.81523437, learning_rate: 0.02907936507936508, global_step: 580, interval_runtime: 18.4662, interval_samples_per_second: 0.433, interval_steps_per_second: 0.542, epoch: 1.5344[0m
[32m[2022-08-30 19:11:42,220] [    INFO][0m - loss: 15.11932831, learning_rate: 0.029063492063492062, global_step: 590, interval_runtime: 18.4509, interval_samples_per_second: 0.434, interval_steps_per_second: 0.542, epoch: 1.5608[0m
[32m[2022-08-30 19:12:00,728] [    INFO][0m - loss: 18.95741425, learning_rate: 0.029047619047619048, global_step: 600, interval_runtime: 18.5082, interval_samples_per_second: 0.432, interval_steps_per_second: 0.54, epoch: 1.5873[0m
[32m[2022-08-30 19:12:00,729] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 19:12:00,730] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-30 19:12:00,730] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-08-30 19:12:00,730] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-08-30 19:12:00,730] [    INFO][0m -   Total prediction steps = 172[0m
[32m[2022-08-30 19:15:07,276] [    INFO][0m - eval_loss: 12.946592330932617, eval_accuracy: 0.1937363437727604, eval_runtime: 186.5462, eval_samples_per_second: 7.36, eval_steps_per_second: 0.922, epoch: 1.5873[0m
[32m[2022-08-30 19:15:07,277] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-08-30 19:15:07,277] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 19:15:40,134] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-08-30 19:15:40,134] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-08-30 19:16:02,803] [    INFO][0m - loss: 16.22411346, learning_rate: 0.029031746031746034, global_step: 610, interval_runtime: 242.0747, interval_samples_per_second: 0.033, interval_steps_per_second: 0.041, epoch: 1.6138[0m
[32m[2022-08-30 19:16:21,762] [    INFO][0m - loss: 14.1754715, learning_rate: 0.029015873015873012, global_step: 620, interval_runtime: 18.9587, interval_samples_per_second: 0.422, interval_steps_per_second: 0.527, epoch: 1.6402[0m
[32m[2022-08-30 19:16:40,651] [    INFO][0m - loss: 16.08634644, learning_rate: 0.028999999999999998, global_step: 630, interval_runtime: 18.8888, interval_samples_per_second: 0.424, interval_steps_per_second: 0.529, epoch: 1.6667[0m
[32m[2022-08-30 19:16:59,605] [    INFO][0m - loss: 14.74149017, learning_rate: 0.028984126984126984, global_step: 640, interval_runtime: 18.9546, interval_samples_per_second: 0.422, interval_steps_per_second: 0.528, epoch: 1.6931[0m
[32m[2022-08-30 19:17:18,598] [    INFO][0m - loss: 15.34090729, learning_rate: 0.028968253968253966, global_step: 650, interval_runtime: 18.9928, interval_samples_per_second: 0.421, interval_steps_per_second: 0.527, epoch: 1.7196[0m
[32m[2022-08-30 19:17:37,685] [    INFO][0m - loss: 14.45243683, learning_rate: 0.02895238095238095, global_step: 660, interval_runtime: 19.0873, interval_samples_per_second: 0.419, interval_steps_per_second: 0.524, epoch: 1.746[0m
[32m[2022-08-30 19:17:56,753] [    INFO][0m - loss: 21.53649597, learning_rate: 0.028936507936507937, global_step: 670, interval_runtime: 19.0675, interval_samples_per_second: 0.42, interval_steps_per_second: 0.524, epoch: 1.7725[0m
[32m[2022-08-30 19:18:15,830] [    INFO][0m - loss: 16.53020172, learning_rate: 0.02892063492063492, global_step: 680, interval_runtime: 19.0771, interval_samples_per_second: 0.419, interval_steps_per_second: 0.524, epoch: 1.7989[0m
[32m[2022-08-30 19:18:34,951] [    INFO][0m - loss: 16.09372864, learning_rate: 0.028904761904761905, global_step: 690, interval_runtime: 19.1204, interval_samples_per_second: 0.418, interval_steps_per_second: 0.523, epoch: 1.8254[0m
[32m[2022-08-30 19:18:54,107] [    INFO][0m - loss: 22.33669739, learning_rate: 0.028888888888888888, global_step: 700, interval_runtime: 19.1561, interval_samples_per_second: 0.418, interval_steps_per_second: 0.522, epoch: 1.8519[0m
[32m[2022-08-30 19:18:54,107] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 19:18:54,108] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-30 19:18:54,108] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-08-30 19:18:54,108] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-08-30 19:18:54,108] [    INFO][0m -   Total prediction steps = 172[0m
[32m[2022-08-30 19:22:12,389] [    INFO][0m - eval_loss: 13.92636775970459, eval_accuracy: 0.24763292061179898, eval_runtime: 198.2806, eval_samples_per_second: 6.925, eval_steps_per_second: 0.867, epoch: 1.8519[0m
[32m[2022-08-30 19:22:12,389] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-700[0m
[32m[2022-08-30 19:22:12,390] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 19:22:45,661] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-700/tokenizer_config.json[0m
[32m[2022-08-30 19:22:45,661] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-700/special_tokens_map.json[0m
[32m[2022-08-30 19:23:08,902] [    INFO][0m - loss: 20.49178467, learning_rate: 0.02887301587301587, global_step: 710, interval_runtime: 254.7954, interval_samples_per_second: 0.031, interval_steps_per_second: 0.039, epoch: 1.8783[0m
[32m[2022-08-30 19:23:28,486] [    INFO][0m - loss: 14.13315125, learning_rate: 0.028857142857142856, global_step: 720, interval_runtime: 19.5836, interval_samples_per_second: 0.409, interval_steps_per_second: 0.511, epoch: 1.9048[0m
[32m[2022-08-30 19:23:48,088] [    INFO][0m - loss: 18.0147171, learning_rate: 0.028841269841269838, global_step: 730, interval_runtime: 19.6023, interval_samples_per_second: 0.408, interval_steps_per_second: 0.51, epoch: 1.9312[0m
[32m[2022-08-30 19:24:07,679] [    INFO][0m - loss: 17.38300323, learning_rate: 0.028825396825396823, global_step: 740, interval_runtime: 19.5907, interval_samples_per_second: 0.408, interval_steps_per_second: 0.51, epoch: 1.9577[0m
[32m[2022-08-30 19:24:27,333] [    INFO][0m - loss: 15.52897949, learning_rate: 0.02880952380952381, global_step: 750, interval_runtime: 19.6542, interval_samples_per_second: 0.407, interval_steps_per_second: 0.509, epoch: 1.9841[0m
[32m[2022-08-30 19:24:47,388] [    INFO][0m - loss: 13.7784317, learning_rate: 0.02879365079365079, global_step: 760, interval_runtime: 20.0552, interval_samples_per_second: 0.399, interval_steps_per_second: 0.499, epoch: 2.0106[0m
[32m[2022-08-30 19:25:07,148] [    INFO][0m - loss: 10.20412216, learning_rate: 0.028777777777777777, global_step: 770, interval_runtime: 19.7592, interval_samples_per_second: 0.405, interval_steps_per_second: 0.506, epoch: 2.037[0m
[32m[2022-08-30 19:25:26,865] [    INFO][0m - loss: 18.13600464, learning_rate: 0.028761904761904763, global_step: 780, interval_runtime: 19.7172, interval_samples_per_second: 0.406, interval_steps_per_second: 0.507, epoch: 2.0635[0m
[32m[2022-08-30 19:25:46,636] [    INFO][0m - loss: 19.23202362, learning_rate: 0.028746031746031745, global_step: 790, interval_runtime: 19.7712, interval_samples_per_second: 0.405, interval_steps_per_second: 0.506, epoch: 2.0899[0m
[32m[2022-08-30 19:26:06,368] [    INFO][0m - loss: 17.1767334, learning_rate: 0.02873015873015873, global_step: 800, interval_runtime: 19.7325, interval_samples_per_second: 0.405, interval_steps_per_second: 0.507, epoch: 2.1164[0m
[32m[2022-08-30 19:26:06,369] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 19:26:06,369] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-30 19:26:06,369] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-08-30 19:26:06,369] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-08-30 19:26:06,369] [    INFO][0m -   Total prediction steps = 172[0m
[32m[2022-08-30 19:29:36,090] [    INFO][0m - eval_loss: 16.234397888183594, eval_accuracy: 0.3058994901675164, eval_runtime: 209.7197, eval_samples_per_second: 6.547, eval_steps_per_second: 0.82, epoch: 2.1164[0m
[32m[2022-08-30 19:29:36,090] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-800[0m
[32m[2022-08-30 19:29:36,090] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 19:30:10,064] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-800/tokenizer_config.json[0m
[32m[2022-08-30 19:30:10,064] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-800/special_tokens_map.json[0m
[32m[2022-08-30 19:30:34,146] [    INFO][0m - loss: 13.55618744, learning_rate: 0.028714285714285713, global_step: 810, interval_runtime: 267.7774, interval_samples_per_second: 0.03, interval_steps_per_second: 0.037, epoch: 2.1429[0m
[32m[2022-08-30 19:30:54,295] [    INFO][0m - loss: 20.34515533, learning_rate: 0.028698412698412695, global_step: 820, interval_runtime: 20.1492, interval_samples_per_second: 0.397, interval_steps_per_second: 0.496, epoch: 2.1693[0m
[32m[2022-08-30 19:31:14,665] [    INFO][0m - loss: 11.87072678, learning_rate: 0.02868253968253968, global_step: 830, interval_runtime: 20.3706, interval_samples_per_second: 0.393, interval_steps_per_second: 0.491, epoch: 2.1958[0m
[32m[2022-08-30 19:31:35,031] [    INFO][0m - loss: 14.37893219, learning_rate: 0.028666666666666667, global_step: 840, interval_runtime: 20.3654, interval_samples_per_second: 0.393, interval_steps_per_second: 0.491, epoch: 2.2222[0m
[32m[2022-08-30 19:31:55,310] [    INFO][0m - loss: 18.62202606, learning_rate: 0.02865079365079365, global_step: 850, interval_runtime: 20.2794, interval_samples_per_second: 0.394, interval_steps_per_second: 0.493, epoch: 2.2487[0m
[32m[2022-08-30 19:32:15,740] [    INFO][0m - loss: 19.30597534, learning_rate: 0.028634920634920635, global_step: 860, interval_runtime: 20.4292, interval_samples_per_second: 0.392, interval_steps_per_second: 0.489, epoch: 2.2751[0m
[32m[2022-08-30 19:32:36,161] [    INFO][0m - loss: 15.93446655, learning_rate: 0.02861904761904762, global_step: 870, interval_runtime: 20.4212, interval_samples_per_second: 0.392, interval_steps_per_second: 0.49, epoch: 2.3016[0m
[32m[2022-08-30 19:32:56,548] [    INFO][0m - loss: 12.27121887, learning_rate: 0.028603174603174603, global_step: 880, interval_runtime: 20.3877, interval_samples_per_second: 0.392, interval_steps_per_second: 0.49, epoch: 2.328[0m
[32m[2022-08-30 19:33:17,061] [    INFO][0m - loss: 18.78828125, learning_rate: 0.02858730158730159, global_step: 890, interval_runtime: 20.5124, interval_samples_per_second: 0.39, interval_steps_per_second: 0.488, epoch: 2.3545[0m
[32m[2022-08-30 19:33:37,536] [    INFO][0m - loss: 15.16298523, learning_rate: 0.028571428571428567, global_step: 900, interval_runtime: 20.4747, interval_samples_per_second: 0.391, interval_steps_per_second: 0.488, epoch: 2.381[0m
[32m[2022-08-30 19:33:37,536] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 19:33:37,537] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-30 19:33:37,537] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-08-30 19:33:37,537] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-08-30 19:33:37,537] [    INFO][0m -   Total prediction steps = 172[0m
[32m[2022-08-30 19:37:19,648] [    INFO][0m - eval_loss: 18.831058502197266, eval_accuracy: 0.314639475600874, eval_runtime: 222.111, eval_samples_per_second: 6.182, eval_steps_per_second: 0.774, epoch: 2.381[0m
[32m[2022-08-30 19:37:19,649] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-900[0m
[32m[2022-08-30 19:37:19,649] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 19:37:56,522] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-900/tokenizer_config.json[0m
[32m[2022-08-30 19:37:56,522] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-900/special_tokens_map.json[0m
[32m[2022-08-30 19:38:21,240] [    INFO][0m - loss: 18.92933197, learning_rate: 0.028555555555555553, global_step: 910, interval_runtime: 283.7036, interval_samples_per_second: 0.028, interval_steps_per_second: 0.035, epoch: 2.4074[0m
[32m[2022-08-30 19:38:42,145] [    INFO][0m - loss: 13.16730804, learning_rate: 0.02853968253968254, global_step: 920, interval_runtime: 20.9049, interval_samples_per_second: 0.383, interval_steps_per_second: 0.478, epoch: 2.4339[0m
[32m[2022-08-30 19:39:03,133] [    INFO][0m - loss: 22.57319031, learning_rate: 0.02852380952380952, global_step: 930, interval_runtime: 20.9881, interval_samples_per_second: 0.381, interval_steps_per_second: 0.476, epoch: 2.4603[0m
[32m[2022-08-30 19:39:24,098] [    INFO][0m - loss: 18.54168701, learning_rate: 0.028507936507936506, global_step: 940, interval_runtime: 20.9653, interval_samples_per_second: 0.382, interval_steps_per_second: 0.477, epoch: 2.4868[0m
[32m[2022-08-30 19:39:45,091] [    INFO][0m - loss: 13.47857666, learning_rate: 0.028492063492063492, global_step: 950, interval_runtime: 20.9927, interval_samples_per_second: 0.381, interval_steps_per_second: 0.476, epoch: 2.5132[0m
[32m[2022-08-30 19:40:06,152] [    INFO][0m - loss: 16.04219666, learning_rate: 0.028476190476190474, global_step: 960, interval_runtime: 21.0614, interval_samples_per_second: 0.38, interval_steps_per_second: 0.475, epoch: 2.5397[0m
[32m[2022-08-30 19:40:27,242] [    INFO][0m - loss: 18.09218445, learning_rate: 0.02846031746031746, global_step: 970, interval_runtime: 21.0898, interval_samples_per_second: 0.379, interval_steps_per_second: 0.474, epoch: 2.5661[0m
[32m[2022-08-30 19:40:48,347] [    INFO][0m - loss: 14.89299622, learning_rate: 0.028444444444444446, global_step: 980, interval_runtime: 21.1049, interval_samples_per_second: 0.379, interval_steps_per_second: 0.474, epoch: 2.5926[0m
[32m[2022-08-30 19:41:09,441] [    INFO][0m - loss: 17.63078918, learning_rate: 0.028428571428571428, global_step: 990, interval_runtime: 21.0945, interval_samples_per_second: 0.379, interval_steps_per_second: 0.474, epoch: 2.619[0m
[32m[2022-08-30 19:41:30,546] [    INFO][0m - loss: 18.03143311, learning_rate: 0.02841269841269841, global_step: 1000, interval_runtime: 21.1046, interval_samples_per_second: 0.379, interval_steps_per_second: 0.474, epoch: 2.6455[0m
[32m[2022-08-30 19:41:30,546] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 19:41:30,547] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-30 19:41:30,547] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-08-30 19:41:30,547] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-08-30 19:41:30,547] [    INFO][0m -   Total prediction steps = 172[0m
[32m[2022-08-30 19:45:23,747] [    INFO][0m - eval_loss: 15.094660758972168, eval_accuracy: 0.370721048798252, eval_runtime: 233.1993, eval_samples_per_second: 5.888, eval_steps_per_second: 0.738, epoch: 2.6455[0m
[32m[2022-08-30 19:45:23,747] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1000[0m
[32m[2022-08-30 19:45:23,747] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 19:46:01,111] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1000/tokenizer_config.json[0m
[32m[2022-08-30 19:46:01,111] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1000/special_tokens_map.json[0m
[32m[2022-08-30 19:46:26,296] [    INFO][0m - loss: 16.37147064, learning_rate: 0.028396825396825396, global_step: 1010, interval_runtime: 295.7505, interval_samples_per_second: 0.027, interval_steps_per_second: 0.034, epoch: 2.672[0m
[32m[2022-08-30 19:46:47,746] [    INFO][0m - loss: 21.56495819, learning_rate: 0.028380952380952378, global_step: 1020, interval_runtime: 21.4502, interval_samples_per_second: 0.373, interval_steps_per_second: 0.466, epoch: 2.6984[0m
[32m[2022-08-30 19:47:09,250] [    INFO][0m - loss: 14.89711914, learning_rate: 0.028365079365079364, global_step: 1030, interval_runtime: 21.5043, interval_samples_per_second: 0.372, interval_steps_per_second: 0.465, epoch: 2.7249[0m
[32m[2022-08-30 19:47:30,822] [    INFO][0m - loss: 20.81963501, learning_rate: 0.02834920634920635, global_step: 1040, interval_runtime: 21.5712, interval_samples_per_second: 0.371, interval_steps_per_second: 0.464, epoch: 2.7513[0m
[32m[2022-08-30 19:47:52,509] [    INFO][0m - loss: 16.05888519, learning_rate: 0.028333333333333332, global_step: 1050, interval_runtime: 21.6871, interval_samples_per_second: 0.369, interval_steps_per_second: 0.461, epoch: 2.7778[0m
[32m[2022-08-30 19:48:14,310] [    INFO][0m - loss: 16.76568604, learning_rate: 0.028317460317460318, global_step: 1060, interval_runtime: 21.8012, interval_samples_per_second: 0.367, interval_steps_per_second: 0.459, epoch: 2.8042[0m
[32m[2022-08-30 19:48:36,204] [    INFO][0m - loss: 17.47857666, learning_rate: 0.028301587301587303, global_step: 1070, interval_runtime: 21.8944, interval_samples_per_second: 0.365, interval_steps_per_second: 0.457, epoch: 2.8307[0m
[32m[2022-08-30 19:48:58,192] [    INFO][0m - loss: 17.045401, learning_rate: 0.028285714285714286, global_step: 1080, interval_runtime: 21.9877, interval_samples_per_second: 0.364, interval_steps_per_second: 0.455, epoch: 2.8571[0m
[32m[2022-08-30 19:49:20,083] [    INFO][0m - loss: 13.92635498, learning_rate: 0.028269841269841268, global_step: 1090, interval_runtime: 21.891, interval_samples_per_second: 0.365, interval_steps_per_second: 0.457, epoch: 2.8836[0m
[32m[2022-08-30 19:49:42,100] [    INFO][0m - loss: 20.31603699, learning_rate: 0.02825396825396825, global_step: 1100, interval_runtime: 22.0166, interval_samples_per_second: 0.363, interval_steps_per_second: 0.454, epoch: 2.9101[0m
[32m[2022-08-30 19:49:42,101] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 19:49:42,101] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-30 19:49:42,101] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-08-30 19:49:42,101] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-08-30 19:49:42,101] [    INFO][0m -   Total prediction steps = 172[0m
[32m[2022-08-30 19:53:46,550] [    INFO][0m - eval_loss: 16.576480865478516, eval_accuracy: 0.3641660597232338, eval_runtime: 244.448, eval_samples_per_second: 5.617, eval_steps_per_second: 0.704, epoch: 2.9101[0m
[32m[2022-08-30 19:53:46,550] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1100[0m
[32m[2022-08-30 19:53:46,550] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 19:54:23,259] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1100/tokenizer_config.json[0m
[32m[2022-08-30 19:54:23,260] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1100/special_tokens_map.json[0m
[32m[2022-08-30 19:54:49,200] [    INFO][0m - loss: 16.77194824, learning_rate: 0.028238095238095236, global_step: 1110, interval_runtime: 307.0996, interval_samples_per_second: 0.026, interval_steps_per_second: 0.033, epoch: 2.9365[0m
[32m[2022-08-30 19:55:11,334] [    INFO][0m - loss: 14.33197784, learning_rate: 0.02822222222222222, global_step: 1120, interval_runtime: 22.1343, interval_samples_per_second: 0.361, interval_steps_per_second: 0.452, epoch: 2.963[0m
[32m[2022-08-30 19:55:32,822] [    INFO][0m - loss: 16.2071167, learning_rate: 0.028206349206349204, global_step: 1130, interval_runtime: 21.488, interval_samples_per_second: 0.372, interval_steps_per_second: 0.465, epoch: 2.9894[0m
[32m[2022-08-30 19:55:56,186] [    INFO][0m - loss: 18.01553955, learning_rate: 0.02819047619047619, global_step: 1140, interval_runtime: 23.3639, interval_samples_per_second: 0.342, interval_steps_per_second: 0.428, epoch: 3.0159[0m
[32m[2022-08-30 19:56:18,472] [    INFO][0m - loss: 12.86630096, learning_rate: 0.028174603174603175, global_step: 1150, interval_runtime: 22.2861, interval_samples_per_second: 0.359, interval_steps_per_second: 0.449, epoch: 3.0423[0m
[32m[2022-08-30 19:56:40,825] [    INFO][0m - loss: 11.70853882, learning_rate: 0.028158730158730157, global_step: 1160, interval_runtime: 22.3533, interval_samples_per_second: 0.358, interval_steps_per_second: 0.447, epoch: 3.0688[0m
[32m[2022-08-30 19:57:03,180] [    INFO][0m - loss: 10.78937912, learning_rate: 0.028142857142857143, global_step: 1170, interval_runtime: 22.3549, interval_samples_per_second: 0.358, interval_steps_per_second: 0.447, epoch: 3.0952[0m
[32m[2022-08-30 19:57:25,576] [    INFO][0m - loss: 14.17244415, learning_rate: 0.028126984126984125, global_step: 1180, interval_runtime: 22.3959, interval_samples_per_second: 0.357, interval_steps_per_second: 0.447, epoch: 3.1217[0m
[32m[2022-08-30 19:57:47,951] [    INFO][0m - loss: 14.00992584, learning_rate: 0.028111111111111108, global_step: 1190, interval_runtime: 22.3747, interval_samples_per_second: 0.358, interval_steps_per_second: 0.447, epoch: 3.1481[0m
[32m[2022-08-30 19:58:10,383] [    INFO][0m - loss: 11.28837357, learning_rate: 0.028095238095238093, global_step: 1200, interval_runtime: 22.4328, interval_samples_per_second: 0.357, interval_steps_per_second: 0.446, epoch: 3.1746[0m
[32m[2022-08-30 19:58:10,384] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 19:58:10,384] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-30 19:58:10,384] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-08-30 19:58:10,384] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-08-30 19:58:10,384] [    INFO][0m -   Total prediction steps = 172[0m
[32m[2022-08-30 20:02:27,010] [    INFO][0m - eval_loss: 18.982826232910156, eval_accuracy: 0.3372177713037145, eval_runtime: 256.6256, eval_samples_per_second: 5.35, eval_steps_per_second: 0.67, epoch: 3.1746[0m
[32m[2022-08-30 20:02:27,011] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1200[0m
[32m[2022-08-30 20:02:27,011] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 20:03:06,833] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1200/tokenizer_config.json[0m
[32m[2022-08-30 20:03:06,833] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1200/special_tokens_map.json[0m
[32m[2022-08-30 20:03:34,378] [    INFO][0m - loss: 15.18954163, learning_rate: 0.02807936507936508, global_step: 1210, interval_runtime: 323.9945, interval_samples_per_second: 0.025, interval_steps_per_second: 0.031, epoch: 3.2011[0m
[32m[2022-08-30 20:03:57,132] [    INFO][0m - loss: 17.95089569, learning_rate: 0.02806349206349206, global_step: 1220, interval_runtime: 22.7544, interval_samples_per_second: 0.352, interval_steps_per_second: 0.439, epoch: 3.2275[0m
[32m[2022-08-30 20:04:19,955] [    INFO][0m - loss: 12.39529572, learning_rate: 0.028047619047619047, global_step: 1230, interval_runtime: 22.8222, interval_samples_per_second: 0.351, interval_steps_per_second: 0.438, epoch: 3.254[0m
[32m[2022-08-30 20:04:42,784] [    INFO][0m - loss: 12.71881409, learning_rate: 0.028031746031746033, global_step: 1240, interval_runtime: 22.8296, interval_samples_per_second: 0.35, interval_steps_per_second: 0.438, epoch: 3.2804[0m
[32m[2022-08-30 20:05:05,697] [    INFO][0m - loss: 16.18332977, learning_rate: 0.028015873015873015, global_step: 1250, interval_runtime: 22.9123, interval_samples_per_second: 0.349, interval_steps_per_second: 0.436, epoch: 3.3069[0m
[32m[2022-08-30 20:05:28,535] [    INFO][0m - loss: 9.83362885, learning_rate: 0.028, global_step: 1260, interval_runtime: 22.838, interval_samples_per_second: 0.35, interval_steps_per_second: 0.438, epoch: 3.3333[0m
[32m[2022-08-30 20:05:51,474] [    INFO][0m - loss: 15.43475037, learning_rate: 0.027984126984126986, global_step: 1270, interval_runtime: 22.9392, interval_samples_per_second: 0.349, interval_steps_per_second: 0.436, epoch: 3.3598[0m
[32m[2022-08-30 20:06:14,449] [    INFO][0m - loss: 17.07977295, learning_rate: 0.027968253968253965, global_step: 1280, interval_runtime: 22.9749, interval_samples_per_second: 0.348, interval_steps_per_second: 0.435, epoch: 3.3862[0m
[32m[2022-08-30 20:06:37,419] [    INFO][0m - loss: 14.11698761, learning_rate: 0.02795238095238095, global_step: 1290, interval_runtime: 22.9699, interval_samples_per_second: 0.348, interval_steps_per_second: 0.435, epoch: 3.4127[0m
[32m[2022-08-30 20:07:00,541] [    INFO][0m - loss: 13.10874329, learning_rate: 0.027936507936507933, global_step: 1300, interval_runtime: 23.1223, interval_samples_per_second: 0.346, interval_steps_per_second: 0.432, epoch: 3.4392[0m
[32m[2022-08-30 20:07:00,541] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 20:07:00,542] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-30 20:07:00,542] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-08-30 20:07:00,542] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-08-30 20:07:00,542] [    INFO][0m -   Total prediction steps = 172[0m
[32m[2022-08-30 20:11:26,291] [    INFO][0m - eval_loss: 18.73492431640625, eval_accuracy: 0.4151493080844865, eval_runtime: 265.7489, eval_samples_per_second: 5.167, eval_steps_per_second: 0.647, epoch: 3.4392[0m
[32m[2022-08-30 20:11:26,292] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1300[0m
[32m[2022-08-30 20:11:26,292] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 20:11:44,616] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1300/tokenizer_config.json[0m
[32m[2022-08-30 20:11:44,616] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1300/special_tokens_map.json[0m
[32m[2022-08-30 20:12:09,430] [    INFO][0m - loss: 14.80080261, learning_rate: 0.02792063492063492, global_step: 1310, interval_runtime: 308.889, interval_samples_per_second: 0.026, interval_steps_per_second: 0.032, epoch: 3.4656[0m
[32m[2022-08-30 20:12:32,929] [    INFO][0m - loss: 20.16256561, learning_rate: 0.027904761904761904, global_step: 1320, interval_runtime: 23.4987, interval_samples_per_second: 0.34, interval_steps_per_second: 0.426, epoch: 3.4921[0m
[32m[2022-08-30 20:12:56,448] [    INFO][0m - loss: 15.24319305, learning_rate: 0.027888888888888887, global_step: 1330, interval_runtime: 23.5196, interval_samples_per_second: 0.34, interval_steps_per_second: 0.425, epoch: 3.5185[0m
[32m[2022-08-30 20:13:19,970] [    INFO][0m - loss: 15.13147888, learning_rate: 0.027873015873015872, global_step: 1340, interval_runtime: 23.5216, interval_samples_per_second: 0.34, interval_steps_per_second: 0.425, epoch: 3.545[0m
[32m[2022-08-30 20:13:43,561] [    INFO][0m - loss: 18.279245, learning_rate: 0.027857142857142858, global_step: 1350, interval_runtime: 23.591, interval_samples_per_second: 0.339, interval_steps_per_second: 0.424, epoch: 3.5714[0m
[32m[2022-08-30 20:14:07,124] [    INFO][0m - loss: 16.92130585, learning_rate: 0.02784126984126984, global_step: 1360, interval_runtime: 23.5631, interval_samples_per_second: 0.34, interval_steps_per_second: 0.424, epoch: 3.5979[0m
[32m[2022-08-30 20:14:30,713] [    INFO][0m - loss: 16.89552612, learning_rate: 0.027825396825396826, global_step: 1370, interval_runtime: 23.5886, interval_samples_per_second: 0.339, interval_steps_per_second: 0.424, epoch: 3.6243[0m
[32m[2022-08-30 20:14:54,373] [    INFO][0m - loss: 18.04490814, learning_rate: 0.02780952380952381, global_step: 1380, interval_runtime: 23.6601, interval_samples_per_second: 0.338, interval_steps_per_second: 0.423, epoch: 3.6508[0m
[32m[2022-08-30 20:15:17,980] [    INFO][0m - loss: 14.70375366, learning_rate: 0.02779365079365079, global_step: 1390, interval_runtime: 23.6072, interval_samples_per_second: 0.339, interval_steps_per_second: 0.424, epoch: 3.6772[0m
[32m[2022-08-30 20:15:41,691] [    INFO][0m - loss: 17.06491547, learning_rate: 0.027777777777777776, global_step: 1400, interval_runtime: 23.7106, interval_samples_per_second: 0.337, interval_steps_per_second: 0.422, epoch: 3.7037[0m
[32m[2022-08-30 20:15:41,692] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 20:15:41,692] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-30 20:15:41,692] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-08-30 20:15:41,692] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-08-30 20:15:41,692] [    INFO][0m -   Total prediction steps = 172[0m
[32m[2022-08-30 20:20:22,127] [    INFO][0m - eval_loss: 16.414352416992188, eval_accuracy: 0.4013109978150036, eval_runtime: 280.4345, eval_samples_per_second: 4.896, eval_steps_per_second: 0.613, epoch: 3.7037[0m
[32m[2022-08-30 20:20:22,127] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1400[0m
[32m[2022-08-30 20:20:22,128] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 20:20:42,159] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1400/tokenizer_config.json[0m
[32m[2022-08-30 20:20:42,160] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1400/special_tokens_map.json[0m
[32m[2022-08-30 20:21:07,621] [    INFO][0m - loss: 11.11891327, learning_rate: 0.027761904761904762, global_step: 1410, interval_runtime: 325.9305, interval_samples_per_second: 0.025, interval_steps_per_second: 0.031, epoch: 3.7302[0m
[32m[2022-08-30 20:21:31,716] [    INFO][0m - loss: 16.13727264, learning_rate: 0.027746031746031744, global_step: 1420, interval_runtime: 24.0948, interval_samples_per_second: 0.332, interval_steps_per_second: 0.415, epoch: 3.7566[0m
[32m[2022-08-30 20:21:55,859] [    INFO][0m - loss: 20.45756836, learning_rate: 0.02773015873015873, global_step: 1430, interval_runtime: 24.1432, interval_samples_per_second: 0.331, interval_steps_per_second: 0.414, epoch: 3.7831[0m
[32m[2022-08-30 20:22:19,993] [    INFO][0m - loss: 13.41858521, learning_rate: 0.027714285714285716, global_step: 1440, interval_runtime: 24.1337, interval_samples_per_second: 0.331, interval_steps_per_second: 0.414, epoch: 3.8095[0m
[32m[2022-08-30 20:22:44,210] [    INFO][0m - loss: 21.57888794, learning_rate: 0.027698412698412698, global_step: 1450, interval_runtime: 24.2167, interval_samples_per_second: 0.33, interval_steps_per_second: 0.413, epoch: 3.836[0m
[32m[2022-08-30 20:23:08,378] [    INFO][0m - loss: 16.11818085, learning_rate: 0.027682539682539684, global_step: 1460, interval_runtime: 24.1684, interval_samples_per_second: 0.331, interval_steps_per_second: 0.414, epoch: 3.8624[0m
[32m[2022-08-30 20:23:32,622] [    INFO][0m - loss: 18.10197449, learning_rate: 0.027666666666666666, global_step: 1470, interval_runtime: 24.2442, interval_samples_per_second: 0.33, interval_steps_per_second: 0.412, epoch: 3.8889[0m
[32m[2022-08-30 20:23:56,916] [    INFO][0m - loss: 22.59641418, learning_rate: 0.027650793650793648, global_step: 1480, interval_runtime: 24.2941, interval_samples_per_second: 0.329, interval_steps_per_second: 0.412, epoch: 3.9153[0m
[32m[2022-08-30 20:24:21,234] [    INFO][0m - loss: 16.08894501, learning_rate: 0.027634920634920634, global_step: 1490, interval_runtime: 24.318, interval_samples_per_second: 0.329, interval_steps_per_second: 0.411, epoch: 3.9418[0m
[32m[2022-08-30 20:24:45,768] [    INFO][0m - loss: 15.97395935, learning_rate: 0.027619047619047616, global_step: 1500, interval_runtime: 24.534, interval_samples_per_second: 0.326, interval_steps_per_second: 0.408, epoch: 3.9683[0m
[32m[2022-08-30 20:24:45,769] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 20:24:45,769] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-30 20:24:45,770] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-08-30 20:24:45,770] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-08-30 20:24:45,770] [    INFO][0m -   Total prediction steps = 172[0m
[32m[2022-08-30 20:29:36,497] [    INFO][0m - eval_loss: 18.70386505126953, eval_accuracy: 0.37144938091769847, eval_runtime: 290.7268, eval_samples_per_second: 4.723, eval_steps_per_second: 0.592, epoch: 3.9683[0m
[32m[2022-08-30 20:29:36,497] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1500[0m
[32m[2022-08-30 20:29:36,498] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 20:29:55,093] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1500/tokenizer_config.json[0m
[32m[2022-08-30 20:29:55,094] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1500/special_tokens_map.json[0m
[32m[2022-08-30 20:30:18,508] [    INFO][0m - loss: 19.22248688, learning_rate: 0.0276031746031746, global_step: 1510, interval_runtime: 332.7397, interval_samples_per_second: 0.024, interval_steps_per_second: 0.03, epoch: 3.9947[0m
[32m[2022-08-30 20:30:46,747] [    INFO][0m - loss: 14.51713562, learning_rate: 0.027587301587301587, global_step: 1520, interval_runtime: 28.239, interval_samples_per_second: 0.283, interval_steps_per_second: 0.354, epoch: 4.0212[0m
[32m[2022-08-30 20:31:11,697] [    INFO][0m - loss: 6.30888519, learning_rate: 0.02757142857142857, global_step: 1530, interval_runtime: 24.95, interval_samples_per_second: 0.321, interval_steps_per_second: 0.401, epoch: 4.0476[0m
[32m[2022-08-30 20:31:36,615] [    INFO][0m - loss: 12.56334229, learning_rate: 0.027555555555555555, global_step: 1540, interval_runtime: 24.9178, interval_samples_per_second: 0.321, interval_steps_per_second: 0.401, epoch: 4.0741[0m
[32m[2022-08-30 20:32:01,514] [    INFO][0m - loss: 11.41328278, learning_rate: 0.02753968253968254, global_step: 1550, interval_runtime: 24.8988, interval_samples_per_second: 0.321, interval_steps_per_second: 0.402, epoch: 4.1005[0m
[32m[2022-08-30 20:32:26,458] [    INFO][0m - loss: 12.41292953, learning_rate: 0.027523809523809523, global_step: 1560, interval_runtime: 24.9437, interval_samples_per_second: 0.321, interval_steps_per_second: 0.401, epoch: 4.127[0m
[32m[2022-08-30 20:32:51,424] [    INFO][0m - loss: 15.41007996, learning_rate: 0.027507936507936506, global_step: 1570, interval_runtime: 24.9666, interval_samples_per_second: 0.32, interval_steps_per_second: 0.401, epoch: 4.1534[0m
[32m[2022-08-30 20:33:16,356] [    INFO][0m - loss: 13.54613495, learning_rate: 0.02749206349206349, global_step: 1580, interval_runtime: 24.932, interval_samples_per_second: 0.321, interval_steps_per_second: 0.401, epoch: 4.1799[0m
[32m[2022-08-30 20:33:41,477] [    INFO][0m - loss: 9.63156433, learning_rate: 0.027476190476190473, global_step: 1590, interval_runtime: 25.1207, interval_samples_per_second: 0.318, interval_steps_per_second: 0.398, epoch: 4.2063[0m
[32m[2022-08-30 20:34:06,602] [    INFO][0m - loss: 14.50104675, learning_rate: 0.02746031746031746, global_step: 1600, interval_runtime: 25.1253, interval_samples_per_second: 0.318, interval_steps_per_second: 0.398, epoch: 4.2328[0m
[32m[2022-08-30 20:34:06,602] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 20:34:06,603] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-30 20:34:06,603] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-08-30 20:34:06,603] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-08-30 20:34:06,603] [    INFO][0m -   Total prediction steps = 172[0m
[32m[2022-08-30 20:39:07,102] [    INFO][0m - eval_loss: 21.064926147460938, eval_accuracy: 0.41005098324836126, eval_runtime: 300.4987, eval_samples_per_second: 4.569, eval_steps_per_second: 0.572, epoch: 4.2328[0m
[32m[2022-08-30 20:39:07,103] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1600[0m
[32m[2022-08-30 20:39:07,103] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 20:39:29,873] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1600/tokenizer_config.json[0m
[32m[2022-08-30 20:39:29,874] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1600/special_tokens_map.json[0m
[32m[2022-08-30 20:39:31,344] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-30 20:39:31,344] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-1300 (score: 0.4151493080844865).[0m
[32m[2022-08-30 20:39:43,566] [    INFO][0m - train_runtime: 7230.1936, train_samples_per_second: 20.912, train_steps_per_second: 2.614, train_loss: 17.463178400993346, epoch: 4.2328[0m
[32m[2022-08-30 20:39:43,569] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-30 20:39:43,569] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 20:40:27,431] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-30 20:40:27,432] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-30 20:40:27,433] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-30 20:40:27,433] [    INFO][0m -   epoch                    =     4.2328[0m
[32m[2022-08-30 20:40:27,434] [    INFO][0m -   train_loss               =    17.4632[0m
[32m[2022-08-30 20:40:27,434] [    INFO][0m -   train_runtime            = 2:00:30.19[0m
[32m[2022-08-30 20:40:27,434] [    INFO][0m -   train_samples_per_second =     20.912[0m
[32m[2022-08-30 20:40:27,434] [    INFO][0m -   train_steps_per_second   =      2.614[0m
[32m[2022-08-30 20:40:27,442] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-30 20:40:27,442] [    INFO][0m -   Num examples = 1749[0m
[32m[2022-08-30 20:40:27,442] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-08-30 20:40:27,442] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-08-30 20:40:27,442] [    INFO][0m -   Total prediction steps = 219[0m
[32m[2022-08-30 20:47:01,906] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-30 20:47:01,907] [    INFO][0m -   test_accuracy           =     0.4077[0m
[32m[2022-08-30 20:47:01,907] [    INFO][0m -   test_loss               =    19.1004[0m
[32m[2022-08-30 20:47:01,907] [    INFO][0m -   test_runtime            = 0:06:34.46[0m
[32m[2022-08-30 20:47:01,907] [    INFO][0m -   test_samples_per_second =      4.434[0m
[32m[2022-08-30 20:47:01,907] [    INFO][0m -   test_steps_per_second   =      0.555[0m
[32m[2022-08-30 20:47:01,908] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-30 20:47:01,908] [    INFO][0m -   Num examples = 2600[0m
[32m[2022-08-30 20:47:01,908] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-08-30 20:47:01,908] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-08-30 20:47:01,908] [    INFO][0m -   Total prediction steps = 325[0m
[32m[2022-08-30 20:57:14,948] [    INFO][0m - Predictions for iflytekf saved to ./fewclue_submit_examples.[0m
metric <function main.<locals>.compute_metrics at 0x7f091294bc10>
