 
==========
eprstmt
==========
 
[33m[2022-08-26 17:42:53,466] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-26 17:42:53,467] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-26 17:42:53,467] [    INFO][0m - ============================================================[0m
[32m[2022-08-26 17:42:53,467] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-26 17:42:53,467] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-26 17:42:53,467] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-26 17:42:53,468] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-26 17:42:53,468] [    INFO][0m - early_stop_patience           :3[0m
[32m[2022-08-26 17:42:53,468] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-26 17:42:53,468] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-08-26 17:42:53,468] [    INFO][0m - [0m
[32m[2022-08-26 17:42:53,468] [    INFO][0m - ============================================================[0m
[32m[2022-08-26 17:42:53,468] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-26 17:42:53,468] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-26 17:42:53,469] [    INFO][0m - encoder_hidden_size           :None[0m
[32m[2022-08-26 17:42:53,469] [    INFO][0m - prompt                        :{'text':'text_a'}{'soft':'ÊàëÊÑüËßâ'}{'mask'}{'soft':'ÂñúÊ¨¢„ÄÇ'}[0m
[32m[2022-08-26 17:42:53,469] [    INFO][0m - soft_encoder                  :None[0m
[32m[2022-08-26 17:42:53,469] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-26 17:42:53,469] [    INFO][0m - task_name                     :eprstmt[0m
[32m[2022-08-26 17:42:53,469] [    INFO][0m - [0m
[32m[2022-08-26 17:42:53,470] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0826 17:42:53.471438 19396 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0826 17:42:53.476713 19396 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-26 17:42:56,268] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-08-26 17:42:59,709] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-08-26 17:42:59,710] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[32m[2022-08-26 17:42:59,723] [    INFO][0m - Using template: [{'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'soft': 'Êàë'}, {'add_prefix_space': '', 'soft': 'ÊÑü'}, {'add_prefix_space': '', 'soft': 'Ëßâ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'soft': 'Âñú'}, {'add_prefix_space': '', 'soft': 'Ê¨¢'}, {'add_prefix_space': '', 'soft': '„ÄÇ'}][0m
2022-08-26 17:42:59,733 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-26 17:42:59,832] [    INFO][0m - ============================================================[0m
[32m[2022-08-26 17:42:59,832] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-26 17:42:59,832] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-26 17:42:59,832] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-26 17:42:59,833] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-26 17:42:59,833] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-26 17:42:59,833] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-26 17:42:59,833] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-26 17:42:59,833] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-26 17:42:59,833] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-26 17:42:59,833] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-26 17:42:59,833] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-26 17:42:59,833] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-26 17:42:59,833] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-26 17:42:59,833] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-26 17:42:59,833] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-26 17:42:59,834] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-26 17:42:59,834] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-26 17:42:59,834] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-26 17:42:59,834] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-26 17:42:59,834] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-26 17:42:59,834] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-26 17:42:59,834] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-26 17:42:59,834] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-26 17:42:59,834] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-26 17:42:59,834] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-26 17:42:59,834] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-26 17:42:59,834] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-26 17:42:59,834] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-26 17:42:59,835] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-26 17:42:59,835] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-26 17:42:59,835] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-26 17:42:59,835] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-26 17:42:59,835] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-26 17:42:59,835] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-26 17:42:59,835] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-26 17:42:59,835] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-26 17:42:59,835] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug26_17-42-53_instance-3bwob41y-01[0m
[32m[2022-08-26 17:42:59,835] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-26 17:42:59,835] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-26 17:42:59,835] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-26 17:42:59,835] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-26 17:42:59,835] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-26 17:42:59,836] [    INFO][0m - max_seq_length                :128[0m
[32m[2022-08-26 17:42:59,836] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-26 17:42:59,836] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-26 17:42:59,836] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-26 17:42:59,836] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-26 17:42:59,836] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-08-26 17:42:59,836] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-26 17:42:59,836] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-26 17:42:59,836] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-26 17:42:59,836] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-26 17:42:59,836] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-26 17:42:59,836] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-26 17:42:59,836] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-26 17:42:59,836] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-26 17:42:59,837] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-26 17:42:59,837] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-26 17:42:59,837] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-26 17:42:59,837] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-26 17:42:59,837] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-26 17:42:59,837] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-26 17:42:59,837] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-26 17:42:59,837] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-26 17:42:59,837] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-26 17:42:59,837] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-26 17:42:59,837] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-26 17:42:59,837] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-26 17:42:59,837] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-26 17:42:59,837] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-26 17:42:59,838] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-26 17:42:59,838] [    INFO][0m - seed                          :42[0m
[32m[2022-08-26 17:42:59,838] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-26 17:42:59,838] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-26 17:42:59,838] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-26 17:42:59,838] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-26 17:42:59,838] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-26 17:42:59,838] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-26 17:42:59,838] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-26 17:42:59,838] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-26 17:42:59,838] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-26 17:42:59,838] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-26 17:42:59,838] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-26 17:42:59,838] [    INFO][0m - [0m
[32m[2022-08-26 17:42:59,840] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-26 17:42:59,840] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 17:42:59,841] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-08-26 17:42:59,841] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-26 17:42:59,841] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-26 17:42:59,841] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-26 17:42:59,841] [    INFO][0m -   Total optimization steps = 1000.0[0m
[32m[2022-08-26 17:42:59,841] [    INFO][0m -   Total num train samples = 8000[0m
[32m[2022-08-26 17:43:01,754] [    INFO][0m - loss: 0.61692119, learning_rate: 2.97e-05, global_step: 10, interval_runtime: 1.9112, interval_samples_per_second: 4.186, interval_steps_per_second: 5.232, epoch: 0.5[0m
[32m[2022-08-26 17:43:02,448] [    INFO][0m - loss: 0.41023817, learning_rate: 2.94e-05, global_step: 20, interval_runtime: 0.6951, interval_samples_per_second: 11.509, interval_steps_per_second: 14.386, epoch: 1.0[0m
[32m[2022-08-26 17:43:03,237] [    INFO][0m - loss: 0.68711934, learning_rate: 2.91e-05, global_step: 30, interval_runtime: 0.7893, interval_samples_per_second: 10.136, interval_steps_per_second: 12.67, epoch: 1.5[0m
[32m[2022-08-26 17:43:03,947] [    INFO][0m - loss: 0.54475389, learning_rate: 2.88e-05, global_step: 40, interval_runtime: 0.7094, interval_samples_per_second: 11.277, interval_steps_per_second: 14.096, epoch: 2.0[0m
[32m[2022-08-26 17:43:04,780] [    INFO][0m - loss: 0.25480859, learning_rate: 2.8499999999999998e-05, global_step: 50, interval_runtime: 0.8332, interval_samples_per_second: 9.602, interval_steps_per_second: 12.002, epoch: 2.5[0m
[32m[2022-08-26 17:43:05,504] [    INFO][0m - loss: 0.46827397, learning_rate: 2.8199999999999998e-05, global_step: 60, interval_runtime: 0.7234, interval_samples_per_second: 11.059, interval_steps_per_second: 13.824, epoch: 3.0[0m
[32m[2022-08-26 17:43:06,391] [    INFO][0m - loss: 0.32425876, learning_rate: 2.79e-05, global_step: 70, interval_runtime: 0.8874, interval_samples_per_second: 9.015, interval_steps_per_second: 11.269, epoch: 3.5[0m
[32m[2022-08-26 17:43:07,126] [    INFO][0m - loss: 0.21347907, learning_rate: 2.7600000000000003e-05, global_step: 80, interval_runtime: 0.7348, interval_samples_per_second: 10.887, interval_steps_per_second: 13.608, epoch: 4.0[0m
[32m[2022-08-26 17:43:08,038] [    INFO][0m - loss: 0.25304, learning_rate: 2.7300000000000003e-05, global_step: 90, interval_runtime: 0.9125, interval_samples_per_second: 8.767, interval_steps_per_second: 10.958, epoch: 4.5[0m
[32m[2022-08-26 17:43:08,788] [    INFO][0m - loss: 0.28110437, learning_rate: 2.7000000000000002e-05, global_step: 100, interval_runtime: 0.7496, interval_samples_per_second: 10.673, interval_steps_per_second: 13.341, epoch: 5.0[0m
[32m[2022-08-26 17:43:08,788] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:43:08,788] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 17:43:08,788] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:43:08,789] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:43:08,789] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 17:43:09,489] [    INFO][0m - eval_loss: 0.8905057907104492, eval_accuracy: 0.875, eval_runtime: 0.7, eval_samples_per_second: 228.576, eval_steps_per_second: 7.143, epoch: 5.0[0m
[32m[2022-08-26 17:43:09,489] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-26 17:43:09,489] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:43:10,851] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-26 17:43:10,851] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-26 17:43:13,441] [    INFO][0m - loss: 0.37139277, learning_rate: 2.6700000000000002e-05, global_step: 110, interval_runtime: 4.6534, interval_samples_per_second: 1.719, interval_steps_per_second: 2.149, epoch: 5.5[0m
[32m[2022-08-26 17:43:14,207] [    INFO][0m - loss: 0.18913636, learning_rate: 2.64e-05, global_step: 120, interval_runtime: 0.7661, interval_samples_per_second: 10.443, interval_steps_per_second: 13.054, epoch: 6.0[0m
[32m[2022-08-26 17:43:15,217] [    INFO][0m - loss: 0.01033137, learning_rate: 2.61e-05, global_step: 130, interval_runtime: 1.0099, interval_samples_per_second: 7.921, interval_steps_per_second: 9.902, epoch: 6.5[0m
[32m[2022-08-26 17:43:16,008] [    INFO][0m - loss: 0.10397294, learning_rate: 2.58e-05, global_step: 140, interval_runtime: 0.7902, interval_samples_per_second: 10.124, interval_steps_per_second: 12.655, epoch: 7.0[0m
[32m[2022-08-26 17:43:17,068] [    INFO][0m - loss: 0.10050428, learning_rate: 2.55e-05, global_step: 150, interval_runtime: 1.0603, interval_samples_per_second: 7.545, interval_steps_per_second: 9.432, epoch: 7.5[0m
[32m[2022-08-26 17:43:17,862] [    INFO][0m - loss: 0.02191835, learning_rate: 2.52e-05, global_step: 160, interval_runtime: 0.7947, interval_samples_per_second: 10.066, interval_steps_per_second: 12.583, epoch: 8.0[0m
[32m[2022-08-26 17:43:18,957] [    INFO][0m - loss: 8.904e-05, learning_rate: 2.49e-05, global_step: 170, interval_runtime: 1.0947, interval_samples_per_second: 7.308, interval_steps_per_second: 9.135, epoch: 8.5[0m
[32m[2022-08-26 17:43:19,771] [    INFO][0m - loss: 0.00031737, learning_rate: 2.4599999999999998e-05, global_step: 180, interval_runtime: 0.8134, interval_samples_per_second: 9.835, interval_steps_per_second: 12.294, epoch: 9.0[0m
[32m[2022-08-26 17:43:20,931] [    INFO][0m - loss: 1.161e-05, learning_rate: 2.43e-05, global_step: 190, interval_runtime: 1.1603, interval_samples_per_second: 6.895, interval_steps_per_second: 8.619, epoch: 9.5[0m
[32m[2022-08-26 17:43:21,783] [    INFO][0m - loss: 0.00066311, learning_rate: 2.4e-05, global_step: 200, interval_runtime: 0.8524, interval_samples_per_second: 9.385, interval_steps_per_second: 11.731, epoch: 10.0[0m
[32m[2022-08-26 17:43:21,784] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:43:21,784] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 17:43:21,784] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:43:21,784] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:43:21,784] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 17:43:22,671] [    INFO][0m - eval_loss: 1.5105886459350586, eval_accuracy: 0.86875, eval_runtime: 0.8866, eval_samples_per_second: 180.466, eval_steps_per_second: 5.64, epoch: 10.0[0m
[32m[2022-08-26 17:43:22,671] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-26 17:43:22,671] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:43:24,045] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-26 17:43:24,046] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-26 17:43:26,850] [    INFO][0m - loss: 9.632e-05, learning_rate: 2.37e-05, global_step: 210, interval_runtime: 5.0666, interval_samples_per_second: 1.579, interval_steps_per_second: 1.974, epoch: 10.5[0m
[32m[2022-08-26 17:43:27,689] [    INFO][0m - loss: 1.519e-05, learning_rate: 2.3400000000000003e-05, global_step: 220, interval_runtime: 0.8388, interval_samples_per_second: 9.537, interval_steps_per_second: 11.922, epoch: 11.0[0m
[32m[2022-08-26 17:43:28,925] [    INFO][0m - loss: 5.94e-06, learning_rate: 2.3100000000000002e-05, global_step: 230, interval_runtime: 1.2365, interval_samples_per_second: 6.47, interval_steps_per_second: 8.088, epoch: 11.5[0m
[32m[2022-08-26 17:43:29,778] [    INFO][0m - loss: 0.05131478, learning_rate: 2.2800000000000002e-05, global_step: 240, interval_runtime: 0.8531, interval_samples_per_second: 9.378, interval_steps_per_second: 11.722, epoch: 12.0[0m
[32m[2022-08-26 17:43:31,051] [    INFO][0m - loss: 1.35e-05, learning_rate: 2.25e-05, global_step: 250, interval_runtime: 1.273, interval_samples_per_second: 6.284, interval_steps_per_second: 7.855, epoch: 12.5[0m
[32m[2022-08-26 17:43:31,922] [    INFO][0m - loss: 0.01169664, learning_rate: 2.22e-05, global_step: 260, interval_runtime: 0.8712, interval_samples_per_second: 9.183, interval_steps_per_second: 11.479, epoch: 13.0[0m
[32m[2022-08-26 17:43:33,242] [    INFO][0m - loss: 1.894e-05, learning_rate: 2.19e-05, global_step: 270, interval_runtime: 1.3194, interval_samples_per_second: 6.063, interval_steps_per_second: 7.579, epoch: 13.5[0m
[32m[2022-08-26 17:43:34,127] [    INFO][0m - loss: 0.16496052, learning_rate: 2.16e-05, global_step: 280, interval_runtime: 0.8854, interval_samples_per_second: 9.036, interval_steps_per_second: 11.295, epoch: 14.0[0m
[32m[2022-08-26 17:43:35,487] [    INFO][0m - loss: 0.06517055, learning_rate: 2.13e-05, global_step: 290, interval_runtime: 1.3602, interval_samples_per_second: 5.881, interval_steps_per_second: 7.352, epoch: 14.5[0m
[32m[2022-08-26 17:43:36,389] [    INFO][0m - loss: 7.73e-06, learning_rate: 2.1e-05, global_step: 300, interval_runtime: 0.9016, interval_samples_per_second: 8.873, interval_steps_per_second: 11.091, epoch: 15.0[0m
[32m[2022-08-26 17:43:36,389] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:43:36,390] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 17:43:36,390] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:43:36,390] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:43:36,390] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 17:43:37,460] [    INFO][0m - eval_loss: 1.5186268091201782, eval_accuracy: 0.875, eval_runtime: 1.07, eval_samples_per_second: 149.533, eval_steps_per_second: 4.673, epoch: 15.0[0m
[32m[2022-08-26 17:43:37,529] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-26 17:43:37,529] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:43:39,248] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-26 17:43:39,249] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-26 17:43:42,255] [    INFO][0m - loss: 0.0006545, learning_rate: 2.07e-05, global_step: 310, interval_runtime: 5.8658, interval_samples_per_second: 1.364, interval_steps_per_second: 1.705, epoch: 15.5[0m
[32m[2022-08-26 17:43:43,173] [    INFO][0m - loss: 0.00018505, learning_rate: 2.04e-05, global_step: 320, interval_runtime: 0.918, interval_samples_per_second: 8.715, interval_steps_per_second: 10.894, epoch: 16.0[0m
[32m[2022-08-26 17:43:44,640] [    INFO][0m - loss: 0.02440347, learning_rate: 2.01e-05, global_step: 330, interval_runtime: 1.4668, interval_samples_per_second: 5.454, interval_steps_per_second: 6.818, epoch: 16.5[0m
[32m[2022-08-26 17:43:45,580] [    INFO][0m - loss: 0.04611875, learning_rate: 1.98e-05, global_step: 340, interval_runtime: 0.9404, interval_samples_per_second: 8.507, interval_steps_per_second: 10.633, epoch: 17.0[0m
[32m[2022-08-26 17:43:47,148] [    INFO][0m - loss: 3.771e-05, learning_rate: 1.95e-05, global_step: 350, interval_runtime: 1.5676, interval_samples_per_second: 5.103, interval_steps_per_second: 6.379, epoch: 17.5[0m
[32m[2022-08-26 17:43:48,130] [    INFO][0m - loss: 2.28e-06, learning_rate: 1.9200000000000003e-05, global_step: 360, interval_runtime: 0.9824, interval_samples_per_second: 8.143, interval_steps_per_second: 10.179, epoch: 18.0[0m
[32m[2022-08-26 17:43:49,672] [    INFO][0m - loss: 9.16e-06, learning_rate: 1.8900000000000002e-05, global_step: 370, interval_runtime: 1.5419, interval_samples_per_second: 5.188, interval_steps_per_second: 6.485, epoch: 18.5[0m
[32m[2022-08-26 17:43:50,626] [    INFO][0m - loss: 0.0010792, learning_rate: 1.86e-05, global_step: 380, interval_runtime: 0.9543, interval_samples_per_second: 8.383, interval_steps_per_second: 10.479, epoch: 19.0[0m
[32m[2022-08-26 17:43:52,208] [    INFO][0m - loss: 0.00026746, learning_rate: 1.83e-05, global_step: 390, interval_runtime: 1.5817, interval_samples_per_second: 5.058, interval_steps_per_second: 6.322, epoch: 19.5[0m
[32m[2022-08-26 17:43:53,183] [    INFO][0m - loss: 0.12551823, learning_rate: 1.8e-05, global_step: 400, interval_runtime: 0.9753, interval_samples_per_second: 8.203, interval_steps_per_second: 10.253, epoch: 20.0[0m
[32m[2022-08-26 17:43:53,184] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:43:53,184] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 17:43:53,184] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:43:53,184] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:43:53,184] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 17:43:54,464] [    INFO][0m - eval_loss: 1.5738720893859863, eval_accuracy: 0.86875, eval_runtime: 1.2799, eval_samples_per_second: 125.013, eval_steps_per_second: 3.907, epoch: 20.0[0m
[32m[2022-08-26 17:43:54,465] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-26 17:43:54,465] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:43:55,779] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-26 17:43:55,780] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-26 17:43:57,351] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-26 17:43:57,352] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-100 (score: 0.875).[0m
[32m[2022-08-26 17:43:58,271] [    INFO][0m - train_runtime: 58.4291, train_samples_per_second: 136.918, train_steps_per_second: 17.115, train_loss: 0.13359776193247852, epoch: 20.0[0m
[32m[2022-08-26 17:43:58,272] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-26 17:43:58,273] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:43:59,593] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-26 17:43:59,593] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-26 17:43:59,594] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-26 17:43:59,595] [    INFO][0m -   epoch                    =       20.0[0m
[32m[2022-08-26 17:43:59,595] [    INFO][0m -   train_loss               =     0.1336[0m
[32m[2022-08-26 17:43:59,595] [    INFO][0m -   train_runtime            = 0:00:58.42[0m
[32m[2022-08-26 17:43:59,595] [    INFO][0m -   train_samples_per_second =    136.918[0m
[32m[2022-08-26 17:43:59,595] [    INFO][0m -   train_steps_per_second   =     17.115[0m
[32m[2022-08-26 17:43:59,597] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-26 17:43:59,598] [    INFO][0m -   Num examples = 610[0m
[32m[2022-08-26 17:43:59,598] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:43:59,598] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:43:59,598] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-08-26 17:44:04,512] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-26 17:44:04,513] [    INFO][0m -   test_accuracy           =     0.8754[0m
[32m[2022-08-26 17:44:04,513] [    INFO][0m -   test_loss               =     0.8128[0m
[32m[2022-08-26 17:44:04,513] [    INFO][0m -   test_runtime            = 0:00:04.91[0m
[32m[2022-08-26 17:44:04,513] [    INFO][0m -   test_samples_per_second =    124.119[0m
[32m[2022-08-26 17:44:04,513] [    INFO][0m -   test_steps_per_second   =      4.069[0m
[32m[2022-08-26 17:44:04,514] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-26 17:44:04,514] [    INFO][0m -   Num examples = 753[0m
[32m[2022-08-26 17:44:04,514] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:44:04,514] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:44:04,514] [    INFO][0m -   Total prediction steps = 24[0m
[32m[2022-08-26 17:44:11,531] [    INFO][0m - Predictions for eprstmt saved to ./fewclue_submit_examples.[0m
 
==========
csldcp
==========
 
[33m[2022-08-26 17:44:15,514] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-26 17:44:15,515] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-26 17:44:15,515] [    INFO][0m - ============================================================[0m
[32m[2022-08-26 17:44:15,515] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-26 17:44:15,515] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-26 17:44:15,515] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-26 17:44:15,515] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-26 17:44:15,515] [    INFO][0m - early_stop_patience           :3[0m
[32m[2022-08-26 17:44:15,515] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-26 17:44:15,515] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-08-26 17:44:15,516] [    INFO][0m - [0m
[32m[2022-08-26 17:44:15,516] [    INFO][0m - ============================================================[0m
[32m[2022-08-26 17:44:15,516] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-26 17:44:15,516] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-26 17:44:15,516] [    INFO][0m - encoder_hidden_size           :None[0m
[32m[2022-08-26 17:44:15,516] [    INFO][0m - prompt                        :{'soft':'ÈòÖËØª‰∏ãËæπ'}{'mask'}{'mask'}{'soft':'Áõ∏ÂÖ≥ÁöÑÊùêÊñô'}{'text':'text_a'}[0m
[32m[2022-08-26 17:44:15,516] [    INFO][0m - soft_encoder                  :None[0m
[32m[2022-08-26 17:44:15,516] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-26 17:44:15,516] [    INFO][0m - task_name                     :csldcp[0m
[32m[2022-08-26 17:44:15,516] [    INFO][0m - [0m
[32m[2022-08-26 17:44:15,516] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0826 17:44:15.517959 21379 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0826 17:44:15.523118 21379 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-26 17:44:18,187] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-08-26 17:44:18,322] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-08-26 17:44:18,322] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[32m[2022-08-26 17:44:18,330] [    INFO][0m - Using template: [{'add_prefix_space': '', 'soft': 'ÈòÖ'}, {'add_prefix_space': '', 'soft': 'ËØª'}, {'add_prefix_space': '', 'soft': '‰∏ã'}, {'add_prefix_space': '', 'soft': 'Ëæπ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'soft': 'Áõ∏'}, {'add_prefix_space': '', 'soft': 'ÂÖ≥'}, {'add_prefix_space': '', 'soft': 'ÁöÑ'}, {'add_prefix_space': '', 'soft': 'Êùê'}, {'add_prefix_space': '', 'soft': 'Êñô'}, {'add_prefix_space': '', 'text': 'text_a'}][0m
2022-08-26 17:44:18,345 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-26 17:44:18,507] [    INFO][0m - ============================================================[0m
[32m[2022-08-26 17:44:18,507] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-26 17:44:18,507] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-26 17:44:18,507] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-26 17:44:18,507] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-26 17:44:18,507] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-26 17:44:18,507] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-26 17:44:18,507] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-26 17:44:18,507] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-26 17:44:18,508] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-26 17:44:18,508] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-26 17:44:18,508] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-26 17:44:18,508] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-26 17:44:18,508] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-26 17:44:18,508] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-26 17:44:18,508] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-26 17:44:18,508] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-26 17:44:18,508] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-26 17:44:18,508] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-26 17:44:18,508] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-26 17:44:18,508] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-26 17:44:18,509] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-26 17:44:18,509] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-26 17:44:18,509] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-26 17:44:18,509] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-26 17:44:18,509] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-26 17:44:18,509] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-26 17:44:18,509] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-26 17:44:18,509] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-26 17:44:18,509] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-26 17:44:18,509] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-26 17:44:18,509] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-26 17:44:18,509] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-26 17:44:18,509] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-26 17:44:18,509] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-26 17:44:18,510] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-26 17:44:18,510] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-26 17:44:18,510] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug26_17-44-15_instance-3bwob41y-01[0m
[32m[2022-08-26 17:44:18,510] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-26 17:44:18,510] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-26 17:44:18,510] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-26 17:44:18,510] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-26 17:44:18,510] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-26 17:44:18,510] [    INFO][0m - max_seq_length                :256[0m
[32m[2022-08-26 17:44:18,510] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-26 17:44:18,510] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-26 17:44:18,510] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-26 17:44:18,510] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-26 17:44:18,511] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-08-26 17:44:18,511] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-26 17:44:18,511] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-26 17:44:18,511] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-26 17:44:18,511] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-26 17:44:18,511] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-26 17:44:18,511] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-26 17:44:18,511] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-26 17:44:18,511] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-26 17:44:18,511] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-26 17:44:18,511] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-26 17:44:18,511] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-26 17:44:18,511] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-26 17:44:18,511] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-26 17:44:18,512] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-26 17:44:18,512] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-26 17:44:18,512] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-26 17:44:18,512] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-26 17:44:18,512] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-26 17:44:18,512] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-26 17:44:18,512] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-26 17:44:18,512] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-26 17:44:18,512] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-26 17:44:18,512] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-26 17:44:18,512] [    INFO][0m - seed                          :42[0m
[32m[2022-08-26 17:44:18,512] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-26 17:44:18,512] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-26 17:44:18,512] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-26 17:44:18,513] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-26 17:44:18,513] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-26 17:44:18,513] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-26 17:44:18,513] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-26 17:44:18,513] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-26 17:44:18,513] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-26 17:44:18,513] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-26 17:44:18,513] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-26 17:44:18,513] [    INFO][0m - [0m
[32m[2022-08-26 17:44:18,515] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-26 17:44:18,515] [    INFO][0m -   Num examples = 2036[0m
[32m[2022-08-26 17:44:18,515] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-08-26 17:44:18,515] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-26 17:44:18,515] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-26 17:44:18,516] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-26 17:44:18,516] [    INFO][0m -   Total optimization steps = 12750.0[0m
[32m[2022-08-26 17:44:18,516] [    INFO][0m -   Total num train samples = 101800[0m
[32m[2022-08-26 17:44:20,808] [    INFO][0m - loss: 3.7598526, learning_rate: 2.9976470588235296e-05, global_step: 10, interval_runtime: 2.2911, interval_samples_per_second: 3.492, interval_steps_per_second: 4.365, epoch: 0.0392[0m
[32m[2022-08-26 17:44:22,057] [    INFO][0m - loss: 2.63003578, learning_rate: 2.9952941176470588e-05, global_step: 20, interval_runtime: 1.2491, interval_samples_per_second: 6.404, interval_steps_per_second: 8.006, epoch: 0.0784[0m
[32m[2022-08-26 17:44:23,326] [    INFO][0m - loss: 2.50593567, learning_rate: 2.9929411764705883e-05, global_step: 30, interval_runtime: 1.2695, interval_samples_per_second: 6.302, interval_steps_per_second: 7.877, epoch: 0.1176[0m
[32m[2022-08-26 17:44:24,612] [    INFO][0m - loss: 2.33939323, learning_rate: 2.9905882352941175e-05, global_step: 40, interval_runtime: 1.2848, interval_samples_per_second: 6.226, interval_steps_per_second: 7.783, epoch: 0.1569[0m
[32m[2022-08-26 17:44:25,920] [    INFO][0m - loss: 2.13103104, learning_rate: 2.988235294117647e-05, global_step: 50, interval_runtime: 1.3089, interval_samples_per_second: 6.112, interval_steps_per_second: 7.64, epoch: 0.1961[0m
[32m[2022-08-26 17:44:27,253] [    INFO][0m - loss: 2.2758625, learning_rate: 2.9858823529411763e-05, global_step: 60, interval_runtime: 1.3324, interval_samples_per_second: 6.004, interval_steps_per_second: 7.505, epoch: 0.2353[0m
[32m[2022-08-26 17:44:28,602] [    INFO][0m - loss: 2.14049034, learning_rate: 2.9835294117647058e-05, global_step: 70, interval_runtime: 1.3489, interval_samples_per_second: 5.931, interval_steps_per_second: 7.413, epoch: 0.2745[0m
[32m[2022-08-26 17:44:29,975] [    INFO][0m - loss: 1.85498848, learning_rate: 2.9811764705882357e-05, global_step: 80, interval_runtime: 1.3736, interval_samples_per_second: 5.824, interval_steps_per_second: 7.28, epoch: 0.3137[0m
[32m[2022-08-26 17:44:31,373] [    INFO][0m - loss: 2.14680347, learning_rate: 2.978823529411765e-05, global_step: 90, interval_runtime: 1.3977, interval_samples_per_second: 5.724, interval_steps_per_second: 7.155, epoch: 0.3529[0m
[32m[2022-08-26 17:44:32,792] [    INFO][0m - loss: 1.8551836, learning_rate: 2.9764705882352944e-05, global_step: 100, interval_runtime: 1.4192, interval_samples_per_second: 5.637, interval_steps_per_second: 7.046, epoch: 0.3922[0m
[32m[2022-08-26 17:44:32,792] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:44:32,793] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-26 17:44:32,793] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:44:32,793] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:44:32,793] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-26 17:44:52,068] [    INFO][0m - eval_loss: 1.6829307079315186, eval_accuracy: 0.5464216634429401, eval_runtime: 19.154, eval_samples_per_second: 107.967, eval_steps_per_second: 3.394, epoch: 0.3922[0m
[32m[2022-08-26 17:44:52,069] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-26 17:44:52,069] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:44:57,304] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-26 17:44:57,305] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-26 17:45:04,866] [    INFO][0m - loss: 2.59065514, learning_rate: 2.9741176470588236e-05, global_step: 110, interval_runtime: 32.0736, interval_samples_per_second: 0.249, interval_steps_per_second: 0.312, epoch: 0.4314[0m
[32m[2022-08-26 17:45:06,452] [    INFO][0m - loss: 1.59196053, learning_rate: 2.971764705882353e-05, global_step: 120, interval_runtime: 1.5867, interval_samples_per_second: 5.042, interval_steps_per_second: 6.302, epoch: 0.4706[0m
[32m[2022-08-26 17:45:08,049] [    INFO][0m - loss: 2.16025181, learning_rate: 2.9694117647058823e-05, global_step: 130, interval_runtime: 1.5967, interval_samples_per_second: 5.01, interval_steps_per_second: 6.263, epoch: 0.5098[0m
[32m[2022-08-26 17:45:09,688] [    INFO][0m - loss: 1.8973896, learning_rate: 2.967058823529412e-05, global_step: 140, interval_runtime: 1.6385, interval_samples_per_second: 4.882, interval_steps_per_second: 6.103, epoch: 0.549[0m
[32m[2022-08-26 17:45:11,342] [    INFO][0m - loss: 1.45927029, learning_rate: 2.9647058823529414e-05, global_step: 150, interval_runtime: 1.6548, interval_samples_per_second: 4.834, interval_steps_per_second: 6.043, epoch: 0.5882[0m
[32m[2022-08-26 17:45:13,011] [    INFO][0m - loss: 1.88609219, learning_rate: 2.9623529411764706e-05, global_step: 160, interval_runtime: 1.6683, interval_samples_per_second: 4.795, interval_steps_per_second: 5.994, epoch: 0.6275[0m
[32m[2022-08-26 17:45:14,700] [    INFO][0m - loss: 2.00674286, learning_rate: 2.96e-05, global_step: 170, interval_runtime: 1.6891, interval_samples_per_second: 4.736, interval_steps_per_second: 5.92, epoch: 0.6667[0m
[32m[2022-08-26 17:45:16,407] [    INFO][0m - loss: 1.75720978, learning_rate: 2.9576470588235293e-05, global_step: 180, interval_runtime: 1.7071, interval_samples_per_second: 4.686, interval_steps_per_second: 5.858, epoch: 0.7059[0m
[32m[2022-08-26 17:45:18,137] [    INFO][0m - loss: 1.80824242, learning_rate: 2.955294117647059e-05, global_step: 190, interval_runtime: 1.7299, interval_samples_per_second: 4.624, interval_steps_per_second: 5.781, epoch: 0.7451[0m
[32m[2022-08-26 17:45:19,891] [    INFO][0m - loss: 2.17772655, learning_rate: 2.952941176470588e-05, global_step: 200, interval_runtime: 1.754, interval_samples_per_second: 4.561, interval_steps_per_second: 5.701, epoch: 0.7843[0m
[32m[2022-08-26 17:45:19,891] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:45:19,891] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-26 17:45:19,891] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:45:19,892] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:45:19,892] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-26 17:45:45,830] [    INFO][0m - eval_loss: 1.571400761604309, eval_accuracy: 0.5488394584139265, eval_runtime: 25.938, eval_samples_per_second: 79.728, eval_steps_per_second: 2.506, epoch: 0.7843[0m
[32m[2022-08-26 17:45:45,831] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-26 17:45:45,831] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:45:50,966] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-26 17:45:50,966] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-26 17:45:58,541] [    INFO][0m - loss: 1.91226597, learning_rate: 2.9505882352941176e-05, global_step: 210, interval_runtime: 38.6503, interval_samples_per_second: 0.207, interval_steps_per_second: 0.259, epoch: 0.8235[0m
[32m[2022-08-26 17:46:01,489] [    INFO][0m - loss: 1.48473892, learning_rate: 2.948235294117647e-05, global_step: 220, interval_runtime: 2.9474, interval_samples_per_second: 2.714, interval_steps_per_second: 3.393, epoch: 0.8627[0m
[32m[2022-08-26 17:46:03,511] [    INFO][0m - loss: 1.41331968, learning_rate: 2.9458823529411763e-05, global_step: 230, interval_runtime: 2.0225, interval_samples_per_second: 3.956, interval_steps_per_second: 4.944, epoch: 0.902[0m
[32m[2022-08-26 17:46:05,541] [    INFO][0m - loss: 2.42286453, learning_rate: 2.9435294117647062e-05, global_step: 240, interval_runtime: 2.0297, interval_samples_per_second: 3.942, interval_steps_per_second: 4.927, epoch: 0.9412[0m
[32m[2022-08-26 17:46:07,497] [    INFO][0m - loss: 1.84103661, learning_rate: 2.9411764705882354e-05, global_step: 250, interval_runtime: 1.9567, interval_samples_per_second: 4.089, interval_steps_per_second: 5.111, epoch: 0.9804[0m
[32m[2022-08-26 17:46:09,628] [    INFO][0m - loss: 1.47918673, learning_rate: 2.938823529411765e-05, global_step: 260, interval_runtime: 2.1307, interval_samples_per_second: 3.755, interval_steps_per_second: 4.693, epoch: 1.0196[0m
[32m[2022-08-26 17:46:11,670] [    INFO][0m - loss: 1.205933, learning_rate: 2.9364705882352944e-05, global_step: 270, interval_runtime: 2.0412, interval_samples_per_second: 3.919, interval_steps_per_second: 4.899, epoch: 1.0588[0m
[32m[2022-08-26 17:46:13,710] [    INFO][0m - loss: 1.05727024, learning_rate: 2.9341176470588236e-05, global_step: 280, interval_runtime: 2.0408, interval_samples_per_second: 3.92, interval_steps_per_second: 4.9, epoch: 1.098[0m
[32m[2022-08-26 17:46:20,685] [    INFO][0m - loss: 1.03520479, learning_rate: 2.9317647058823532e-05, global_step: 290, interval_runtime: 2.0746, interval_samples_per_second: 3.856, interval_steps_per_second: 4.82, epoch: 1.1373[0m
[32m[2022-08-26 17:46:22,776] [    INFO][0m - loss: 1.22458477, learning_rate: 2.9294117647058824e-05, global_step: 300, interval_runtime: 6.9918, interval_samples_per_second: 1.144, interval_steps_per_second: 1.43, epoch: 1.1765[0m
[32m[2022-08-26 17:46:22,777] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:46:22,777] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-26 17:46:22,777] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:46:22,777] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:46:22,777] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-26 17:46:55,054] [    INFO][0m - eval_loss: 1.5527416467666626, eval_accuracy: 0.5701160541586073, eval_runtime: 32.2764, eval_samples_per_second: 64.072, eval_steps_per_second: 2.014, epoch: 1.1765[0m
[32m[2022-08-26 17:46:55,055] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-26 17:46:55,055] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:46:58,258] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-26 17:46:58,258] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-26 17:47:04,286] [    INFO][0m - loss: 1.21381855, learning_rate: 2.927058823529412e-05, global_step: 310, interval_runtime: 41.5096, interval_samples_per_second: 0.193, interval_steps_per_second: 0.241, epoch: 1.2157[0m
[32m[2022-08-26 17:47:06,545] [    INFO][0m - loss: 1.22544575, learning_rate: 2.924705882352941e-05, global_step: 320, interval_runtime: 2.258, interval_samples_per_second: 3.543, interval_steps_per_second: 4.429, epoch: 1.2549[0m
[32m[2022-08-26 17:47:08,828] [    INFO][0m - loss: 1.03832026, learning_rate: 2.9223529411764706e-05, global_step: 330, interval_runtime: 2.2838, interval_samples_per_second: 3.503, interval_steps_per_second: 4.379, epoch: 1.2941[0m
[32m[2022-08-26 17:47:11,139] [    INFO][0m - loss: 1.51686497, learning_rate: 2.92e-05, global_step: 340, interval_runtime: 2.3106, interval_samples_per_second: 3.462, interval_steps_per_second: 4.328, epoch: 1.3333[0m
[32m[2022-08-26 17:47:13,445] [    INFO][0m - loss: 1.62297268, learning_rate: 2.9176470588235294e-05, global_step: 350, interval_runtime: 2.3064, interval_samples_per_second: 3.469, interval_steps_per_second: 4.336, epoch: 1.3725[0m
[32m[2022-08-26 17:47:15,776] [    INFO][0m - loss: 1.22474308, learning_rate: 2.915294117647059e-05, global_step: 360, interval_runtime: 2.3307, interval_samples_per_second: 3.432, interval_steps_per_second: 4.291, epoch: 1.4118[0m
[32m[2022-08-26 17:47:18,140] [    INFO][0m - loss: 1.28181057, learning_rate: 2.912941176470588e-05, global_step: 370, interval_runtime: 2.364, interval_samples_per_second: 3.384, interval_steps_per_second: 4.23, epoch: 1.451[0m
[32m[2022-08-26 17:47:20,516] [    INFO][0m - loss: 1.04285784, learning_rate: 2.9105882352941176e-05, global_step: 380, interval_runtime: 2.3762, interval_samples_per_second: 3.367, interval_steps_per_second: 4.208, epoch: 1.4902[0m
[32m[2022-08-26 17:47:22,906] [    INFO][0m - loss: 1.63406258, learning_rate: 2.908235294117647e-05, global_step: 390, interval_runtime: 2.3901, interval_samples_per_second: 3.347, interval_steps_per_second: 4.184, epoch: 1.5294[0m
[32m[2022-08-26 17:47:25,324] [    INFO][0m - loss: 1.08600445, learning_rate: 2.9058823529411767e-05, global_step: 400, interval_runtime: 2.418, interval_samples_per_second: 3.309, interval_steps_per_second: 4.136, epoch: 1.5686[0m
[32m[2022-08-26 17:47:25,324] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:47:25,324] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-26 17:47:25,325] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:47:25,325] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:47:25,325] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-26 17:48:04,030] [    INFO][0m - eval_loss: 1.5536677837371826, eval_accuracy: 0.5735009671179884, eval_runtime: 38.7046, eval_samples_per_second: 53.43, eval_steps_per_second: 1.679, epoch: 1.5686[0m
[32m[2022-08-26 17:48:04,031] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-26 17:48:04,031] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:48:06,960] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-26 17:48:06,960] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-26 17:48:13,266] [    INFO][0m - loss: 1.08316288, learning_rate: 2.9035294117647062e-05, global_step: 410, interval_runtime: 47.9417, interval_samples_per_second: 0.167, interval_steps_per_second: 0.209, epoch: 1.6078[0m
[32m[2022-08-26 17:48:15,841] [    INFO][0m - loss: 1.46591663, learning_rate: 2.9011764705882354e-05, global_step: 420, interval_runtime: 2.5752, interval_samples_per_second: 3.107, interval_steps_per_second: 3.883, epoch: 1.6471[0m
[32m[2022-08-26 17:48:18,459] [    INFO][0m - loss: 1.01704082, learning_rate: 2.898823529411765e-05, global_step: 430, interval_runtime: 2.6184, interval_samples_per_second: 3.055, interval_steps_per_second: 3.819, epoch: 1.6863[0m
[32m[2022-08-26 17:48:21,079] [    INFO][0m - loss: 1.22419558, learning_rate: 2.896470588235294e-05, global_step: 440, interval_runtime: 2.6197, interval_samples_per_second: 3.054, interval_steps_per_second: 3.817, epoch: 1.7255[0m
[32m[2022-08-26 17:48:23,729] [    INFO][0m - loss: 0.99215584, learning_rate: 2.8941176470588237e-05, global_step: 450, interval_runtime: 2.6497, interval_samples_per_second: 3.019, interval_steps_per_second: 3.774, epoch: 1.7647[0m
[32m[2022-08-26 17:48:26,394] [    INFO][0m - loss: 1.15806046, learning_rate: 2.891764705882353e-05, global_step: 460, interval_runtime: 2.6657, interval_samples_per_second: 3.001, interval_steps_per_second: 3.751, epoch: 1.8039[0m
[32m[2022-08-26 17:48:29,073] [    INFO][0m - loss: 1.41165457, learning_rate: 2.8894117647058824e-05, global_step: 470, interval_runtime: 2.6792, interval_samples_per_second: 2.986, interval_steps_per_second: 3.732, epoch: 1.8431[0m
[32m[2022-08-26 17:48:31,785] [    INFO][0m - loss: 1.50505619, learning_rate: 2.887058823529412e-05, global_step: 480, interval_runtime: 2.7114, interval_samples_per_second: 2.95, interval_steps_per_second: 3.688, epoch: 1.8824[0m
[32m[2022-08-26 17:48:34,505] [    INFO][0m - loss: 1.21699066, learning_rate: 2.884705882352941e-05, global_step: 490, interval_runtime: 2.7204, interval_samples_per_second: 2.941, interval_steps_per_second: 3.676, epoch: 1.9216[0m
[32m[2022-08-26 17:48:37,235] [    INFO][0m - loss: 1.36913509, learning_rate: 2.8823529411764707e-05, global_step: 500, interval_runtime: 2.7297, interval_samples_per_second: 2.931, interval_steps_per_second: 3.663, epoch: 1.9608[0m
[32m[2022-08-26 17:48:37,235] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:48:37,236] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-26 17:48:37,236] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:48:37,236] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:48:37,236] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-26 17:49:22,432] [    INFO][0m - eval_loss: 1.5420044660568237, eval_accuracy: 0.5812379110251451, eval_runtime: 45.1952, eval_samples_per_second: 45.757, eval_steps_per_second: 1.438, epoch: 1.9608[0m
[32m[2022-08-26 17:49:22,432] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-08-26 17:49:22,432] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:49:23,984] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-08-26 17:49:23,984] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-08-26 17:49:27,704] [    INFO][0m - loss: 1.09882622, learning_rate: 2.88e-05, global_step: 510, interval_runtime: 50.4693, interval_samples_per_second: 0.159, interval_steps_per_second: 0.198, epoch: 2.0[0m
[32m[2022-08-26 17:49:31,543] [    INFO][0m - loss: 0.82528572, learning_rate: 2.8776470588235294e-05, global_step: 520, interval_runtime: 3.8383, interval_samples_per_second: 2.084, interval_steps_per_second: 2.605, epoch: 2.0392[0m
[32m[2022-08-26 17:49:34,532] [    INFO][0m - loss: 0.79659243, learning_rate: 2.8752941176470586e-05, global_step: 530, interval_runtime: 2.9888, interval_samples_per_second: 2.677, interval_steps_per_second: 3.346, epoch: 2.0784[0m
[32m[2022-08-26 17:49:37,474] [    INFO][0m - loss: 0.74072313, learning_rate: 2.872941176470588e-05, global_step: 540, interval_runtime: 2.9422, interval_samples_per_second: 2.719, interval_steps_per_second: 3.399, epoch: 2.1176[0m
[32m[2022-08-26 17:49:40,436] [    INFO][0m - loss: 0.80303688, learning_rate: 2.870588235294118e-05, global_step: 550, interval_runtime: 2.9623, interval_samples_per_second: 2.701, interval_steps_per_second: 3.376, epoch: 2.1569[0m
[32m[2022-08-26 17:49:43,470] [    INFO][0m - loss: 0.77301817, learning_rate: 2.8682352941176472e-05, global_step: 560, interval_runtime: 3.0337, interval_samples_per_second: 2.637, interval_steps_per_second: 3.296, epoch: 2.1961[0m
[32m[2022-08-26 17:49:46,511] [    INFO][0m - loss: 0.62698312, learning_rate: 2.8658823529411767e-05, global_step: 570, interval_runtime: 3.0407, interval_samples_per_second: 2.631, interval_steps_per_second: 3.289, epoch: 2.2353[0m
[32m[2022-08-26 17:49:49,550] [    INFO][0m - loss: 0.85661249, learning_rate: 2.863529411764706e-05, global_step: 580, interval_runtime: 3.0391, interval_samples_per_second: 2.632, interval_steps_per_second: 3.29, epoch: 2.2745[0m
[32m[2022-08-26 17:49:52,644] [    INFO][0m - loss: 1.12152586, learning_rate: 2.8611764705882355e-05, global_step: 590, interval_runtime: 3.0943, interval_samples_per_second: 2.585, interval_steps_per_second: 3.232, epoch: 2.3137[0m
[32m[2022-08-26 17:49:55,738] [    INFO][0m - loss: 0.85340242, learning_rate: 2.8588235294117647e-05, global_step: 600, interval_runtime: 3.0938, interval_samples_per_second: 2.586, interval_steps_per_second: 3.232, epoch: 2.3529[0m
[32m[2022-08-26 17:49:55,738] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:49:55,739] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-26 17:49:55,739] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:49:55,739] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:49:55,739] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-26 17:50:47,696] [    INFO][0m - eval_loss: 1.5716444253921509, eval_accuracy: 0.6088007736943907, eval_runtime: 51.9569, eval_samples_per_second: 39.802, eval_steps_per_second: 1.251, epoch: 2.3529[0m
[32m[2022-08-26 17:50:47,697] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-08-26 17:50:47,697] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:50:49,196] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-08-26 17:50:49,197] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-08-26 17:50:54,248] [    INFO][0m - loss: 0.79411583, learning_rate: 2.8564705882352942e-05, global_step: 610, interval_runtime: 58.5104, interval_samples_per_second: 0.137, interval_steps_per_second: 0.171, epoch: 2.3922[0m
[32m[2022-08-26 17:50:57,613] [    INFO][0m - loss: 0.84815378, learning_rate: 2.8541176470588237e-05, global_step: 620, interval_runtime: 3.3652, interval_samples_per_second: 2.377, interval_steps_per_second: 2.972, epoch: 2.4314[0m
[32m[2022-08-26 17:51:00,921] [    INFO][0m - loss: 0.88171549, learning_rate: 2.851764705882353e-05, global_step: 630, interval_runtime: 3.3075, interval_samples_per_second: 2.419, interval_steps_per_second: 3.023, epoch: 2.4706[0m
[32m[2022-08-26 17:51:04,241] [    INFO][0m - loss: 0.94101896, learning_rate: 2.8494117647058825e-05, global_step: 640, interval_runtime: 3.3201, interval_samples_per_second: 2.41, interval_steps_per_second: 3.012, epoch: 2.5098[0m
[32m[2022-08-26 17:51:07,553] [    INFO][0m - loss: 0.65866184, learning_rate: 2.8470588235294117e-05, global_step: 650, interval_runtime: 3.3115, interval_samples_per_second: 2.416, interval_steps_per_second: 3.02, epoch: 2.549[0m
[32m[2022-08-26 17:51:10,919] [    INFO][0m - loss: 0.70641518, learning_rate: 2.8447058823529412e-05, global_step: 660, interval_runtime: 3.3666, interval_samples_per_second: 2.376, interval_steps_per_second: 2.97, epoch: 2.5882[0m
[32m[2022-08-26 17:51:14,373] [    INFO][0m - loss: 1.10493917, learning_rate: 2.8423529411764707e-05, global_step: 670, interval_runtime: 3.4536, interval_samples_per_second: 2.316, interval_steps_per_second: 2.896, epoch: 2.6275[0m
[32m[2022-08-26 17:51:17,770] [    INFO][0m - loss: 0.78192673, learning_rate: 2.84e-05, global_step: 680, interval_runtime: 3.3965, interval_samples_per_second: 2.355, interval_steps_per_second: 2.944, epoch: 2.6667[0m
[32m[2022-08-26 17:51:21,181] [    INFO][0m - loss: 0.84752445, learning_rate: 2.8376470588235294e-05, global_step: 690, interval_runtime: 3.4112, interval_samples_per_second: 2.345, interval_steps_per_second: 2.932, epoch: 2.7059[0m
[32m[2022-08-26 17:51:24,724] [    INFO][0m - loss: 0.68453245, learning_rate: 2.835294117647059e-05, global_step: 700, interval_runtime: 3.543, interval_samples_per_second: 2.258, interval_steps_per_second: 2.822, epoch: 2.7451[0m
[32m[2022-08-26 17:51:24,724] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:51:24,724] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-26 17:51:24,725] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:51:24,725] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:51:24,725] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-26 17:52:22,490] [    INFO][0m - eval_loss: 1.6255282163619995, eval_accuracy: 0.5870406189555126, eval_runtime: 57.7643, eval_samples_per_second: 35.801, eval_steps_per_second: 1.125, epoch: 2.7451[0m
[32m[2022-08-26 17:52:22,490] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-700[0m
[32m[2022-08-26 17:52:22,490] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:52:24,021] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-700/tokenizer_config.json[0m
[32m[2022-08-26 17:52:24,021] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-700/special_tokens_map.json[0m
[32m[2022-08-26 17:52:29,338] [    INFO][0m - loss: 1.0310216, learning_rate: 2.8329411764705885e-05, global_step: 710, interval_runtime: 64.6152, interval_samples_per_second: 0.124, interval_steps_per_second: 0.155, epoch: 2.7843[0m
[32m[2022-08-26 17:52:32,891] [    INFO][0m - loss: 0.94554615, learning_rate: 2.8305882352941177e-05, global_step: 720, interval_runtime: 3.5525, interval_samples_per_second: 2.252, interval_steps_per_second: 2.815, epoch: 2.8235[0m
[32m[2022-08-26 17:52:36,482] [    INFO][0m - loss: 1.01741953, learning_rate: 2.8282352941176472e-05, global_step: 730, interval_runtime: 3.5903, interval_samples_per_second: 2.228, interval_steps_per_second: 2.785, epoch: 2.8627[0m
[32m[2022-08-26 17:52:40,058] [    INFO][0m - loss: 1.15499392, learning_rate: 2.8258823529411768e-05, global_step: 740, interval_runtime: 3.5761, interval_samples_per_second: 2.237, interval_steps_per_second: 2.796, epoch: 2.902[0m
[32m[2022-08-26 17:52:43,690] [    INFO][0m - loss: 0.85535727, learning_rate: 2.823529411764706e-05, global_step: 750, interval_runtime: 3.632, interval_samples_per_second: 2.203, interval_steps_per_second: 2.753, epoch: 2.9412[0m
[32m[2022-08-26 17:52:47,222] [    INFO][0m - loss: 0.74417868, learning_rate: 2.8211764705882355e-05, global_step: 760, interval_runtime: 3.5326, interval_samples_per_second: 2.265, interval_steps_per_second: 2.831, epoch: 2.9804[0m
[32m[2022-08-26 17:52:50,932] [    INFO][0m - loss: 0.56168113, learning_rate: 2.8188235294117647e-05, global_step: 770, interval_runtime: 3.7093, interval_samples_per_second: 2.157, interval_steps_per_second: 2.696, epoch: 3.0196[0m
[32m[2022-08-26 17:52:54,715] [    INFO][0m - loss: 0.60425296, learning_rate: 2.8164705882352942e-05, global_step: 780, interval_runtime: 3.7826, interval_samples_per_second: 2.115, interval_steps_per_second: 2.644, epoch: 3.0588[0m
[32m[2022-08-26 17:52:58,463] [    INFO][0m - loss: 0.66953168, learning_rate: 2.8141176470588234e-05, global_step: 790, interval_runtime: 3.7485, interval_samples_per_second: 2.134, interval_steps_per_second: 2.668, epoch: 3.098[0m
[32m[2022-08-26 17:53:02,194] [    INFO][0m - loss: 0.5231144, learning_rate: 2.811764705882353e-05, global_step: 800, interval_runtime: 3.731, interval_samples_per_second: 2.144, interval_steps_per_second: 2.68, epoch: 3.1373[0m
[32m[2022-08-26 17:53:02,195] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:53:02,196] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-26 17:53:02,196] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:53:02,196] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:53:02,196] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-26 17:54:06,845] [    INFO][0m - eval_loss: 1.5977332592010498, eval_accuracy: 0.6054158607350096, eval_runtime: 64.6489, eval_samples_per_second: 31.988, eval_steps_per_second: 1.005, epoch: 3.1373[0m
[32m[2022-08-26 17:54:06,846] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-800[0m
[32m[2022-08-26 17:54:06,846] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:54:08,450] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-800/tokenizer_config.json[0m
[32m[2022-08-26 17:54:08,450] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-800/special_tokens_map.json[0m
[32m[2022-08-26 17:54:14,369] [    INFO][0m - loss: 0.53822165, learning_rate: 2.8094117647058825e-05, global_step: 810, interval_runtime: 72.1754, interval_samples_per_second: 0.111, interval_steps_per_second: 0.139, epoch: 3.1765[0m
[32m[2022-08-26 17:54:18,302] [    INFO][0m - loss: 0.49093161, learning_rate: 2.8070588235294117e-05, global_step: 820, interval_runtime: 3.9331, interval_samples_per_second: 2.034, interval_steps_per_second: 2.543, epoch: 3.2157[0m
[32m[2022-08-26 17:54:22,265] [    INFO][0m - loss: 0.39464538, learning_rate: 2.8047058823529412e-05, global_step: 830, interval_runtime: 3.9622, interval_samples_per_second: 2.019, interval_steps_per_second: 2.524, epoch: 3.2549[0m
[32m[2022-08-26 17:54:26,236] [    INFO][0m - loss: 0.72245808, learning_rate: 2.8023529411764704e-05, global_step: 840, interval_runtime: 3.9713, interval_samples_per_second: 2.014, interval_steps_per_second: 2.518, epoch: 3.2941[0m
[32m[2022-08-26 17:54:30,182] [    INFO][0m - loss: 0.40826535, learning_rate: 2.8e-05, global_step: 850, interval_runtime: 3.9461, interval_samples_per_second: 2.027, interval_steps_per_second: 2.534, epoch: 3.3333[0m
[32m[2022-08-26 17:54:34,182] [    INFO][0m - loss: 0.48742757, learning_rate: 2.7976470588235295e-05, global_step: 860, interval_runtime: 4.0001, interval_samples_per_second: 2.0, interval_steps_per_second: 2.5, epoch: 3.3725[0m
[32m[2022-08-26 17:54:38,162] [    INFO][0m - loss: 0.41437712, learning_rate: 2.795294117647059e-05, global_step: 870, interval_runtime: 3.98, interval_samples_per_second: 2.01, interval_steps_per_second: 2.513, epoch: 3.4118[0m
[32m[2022-08-26 17:54:42,152] [    INFO][0m - loss: 0.94206085, learning_rate: 2.7929411764705886e-05, global_step: 880, interval_runtime: 3.9898, interval_samples_per_second: 2.005, interval_steps_per_second: 2.506, epoch: 3.451[0m
[32m[2022-08-26 17:54:46,219] [    INFO][0m - loss: 0.59392543, learning_rate: 2.7905882352941178e-05, global_step: 890, interval_runtime: 4.0677, interval_samples_per_second: 1.967, interval_steps_per_second: 2.458, epoch: 3.4902[0m
[32m[2022-08-26 17:54:50,256] [    INFO][0m - loss: 0.4069416, learning_rate: 2.7882352941176473e-05, global_step: 900, interval_runtime: 4.0366, interval_samples_per_second: 1.982, interval_steps_per_second: 2.477, epoch: 3.5294[0m
[32m[2022-08-26 17:54:50,257] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:54:50,257] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-26 17:54:50,257] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:54:50,257] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:54:50,257] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-26 17:56:00,959] [    INFO][0m - eval_loss: 1.6615389585494995, eval_accuracy: 0.6015473887814313, eval_runtime: 70.7014, eval_samples_per_second: 29.25, eval_steps_per_second: 0.919, epoch: 3.5294[0m
[32m[2022-08-26 17:56:00,960] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-900[0m
[32m[2022-08-26 17:56:00,960] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:56:02,510] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-900/tokenizer_config.json[0m
[32m[2022-08-26 17:56:02,511] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-900/special_tokens_map.json[0m
[32m[2022-08-26 17:56:04,355] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-26 17:56:04,356] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-600 (score: 0.6088007736943907).[0m
[32m[2022-08-26 17:56:05,499] [    INFO][0m - train_runtime: 706.9828, train_samples_per_second: 143.992, train_steps_per_second: 18.034, train_loss: 1.2625906088617114, epoch: 3.5294[0m
[32m[2022-08-26 17:56:05,500] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-26 17:56:05,500] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:56:08,485] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-26 17:56:08,486] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-26 17:56:08,487] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-26 17:56:08,487] [    INFO][0m -   epoch                    =     3.5294[0m
[32m[2022-08-26 17:56:08,487] [    INFO][0m -   train_loss               =     1.2626[0m
[32m[2022-08-26 17:56:08,488] [    INFO][0m -   train_runtime            = 0:11:46.98[0m
[32m[2022-08-26 17:56:08,488] [    INFO][0m -   train_samples_per_second =    143.992[0m
[32m[2022-08-26 17:56:08,488] [    INFO][0m -   train_steps_per_second   =     18.034[0m
[32m[2022-08-26 17:56:08,493] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-26 17:56:08,493] [    INFO][0m -   Num examples = 1784[0m
[32m[2022-08-26 17:56:08,493] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:56:08,493] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:56:08,493] [    INFO][0m -   Total prediction steps = 56[0m
[32m[2022-08-26 17:57:11,902] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-26 17:57:11,903] [    INFO][0m -   test_accuracy           =     0.5992[0m
[32m[2022-08-26 17:57:11,903] [    INFO][0m -   test_loss               =     1.5233[0m
[32m[2022-08-26 17:57:11,903] [    INFO][0m -   test_runtime            = 0:01:03.40[0m
[32m[2022-08-26 17:57:11,903] [    INFO][0m -   test_samples_per_second =     28.135[0m
[32m[2022-08-26 17:57:11,903] [    INFO][0m -   test_steps_per_second   =      0.883[0m
[32m[2022-08-26 17:57:11,904] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-26 17:57:11,904] [    INFO][0m -   Num examples = 2999[0m
[32m[2022-08-26 17:57:11,904] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:57:11,904] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:57:11,904] [    INFO][0m -   Total prediction steps = 94[0m
[32m[2022-08-26 17:59:08,260] [    INFO][0m - Predictions for csldcp saved to ./fewclue_submit_examples.[0m
 
==========
tnews
==========
 
[33m[2022-08-26 17:59:12,450] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-26 17:59:12,451] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-26 17:59:12,451] [    INFO][0m - ============================================================[0m
[32m[2022-08-26 17:59:12,451] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-26 17:59:12,451] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-26 17:59:12,451] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-26 17:59:12,451] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-26 17:59:12,451] [    INFO][0m - early_stop_patience           :3[0m
[32m[2022-08-26 17:59:12,452] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-26 17:59:12,452] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-08-26 17:59:12,452] [    INFO][0m - [0m
[32m[2022-08-26 17:59:12,452] [    INFO][0m - ============================================================[0m
[32m[2022-08-26 17:59:12,452] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-26 17:59:12,452] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-26 17:59:12,452] [    INFO][0m - encoder_hidden_size           :None[0m
[32m[2022-08-26 17:59:12,452] [    INFO][0m - prompt                        :{'soft':'‰∏ãËæπÊí≠Êä•‰∏ÄÂàô'}{'mask'}{'mask'}{'soft':'Êñ∞ÈóªÔºö'}{'text':'text_a'}[0m
[32m[2022-08-26 17:59:12,452] [    INFO][0m - soft_encoder                  :None[0m
[32m[2022-08-26 17:59:12,452] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-26 17:59:12,452] [    INFO][0m - task_name                     :tnews[0m
[32m[2022-08-26 17:59:12,452] [    INFO][0m - [0m
[32m[2022-08-26 17:59:12,453] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0826 17:59:12.454696 40355 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0826 17:59:12.458775 40355 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-26 17:59:15,271] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-08-26 17:59:15,296] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-08-26 17:59:15,296] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[32m[2022-08-26 17:59:15,304] [    INFO][0m - Using template: [{'add_prefix_space': '', 'soft': '‰∏ã'}, {'add_prefix_space': '', 'soft': 'Ëæπ'}, {'add_prefix_space': '', 'soft': 'Êí≠'}, {'add_prefix_space': '', 'soft': 'Êä•'}, {'add_prefix_space': '', 'soft': '‰∏Ä'}, {'add_prefix_space': '', 'soft': 'Âàô'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'soft': 'Êñ∞'}, {'add_prefix_space': '', 'soft': 'Èóª'}, {'add_prefix_space': '', 'soft': 'Ôºö'}, {'add_prefix_space': '', 'text': 'text_a'}][0m
2022-08-26 17:59:15,311 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-26 17:59:15,426] [    INFO][0m - ============================================================[0m
[32m[2022-08-26 17:59:15,427] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-26 17:59:15,427] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-26 17:59:15,427] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-26 17:59:15,427] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-26 17:59:15,427] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-26 17:59:15,427] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-26 17:59:15,427] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-26 17:59:15,427] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-26 17:59:15,427] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-26 17:59:15,427] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-26 17:59:15,427] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-26 17:59:15,428] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-26 17:59:15,428] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-26 17:59:15,428] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-26 17:59:15,428] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-26 17:59:15,428] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-26 17:59:15,428] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-26 17:59:15,428] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-26 17:59:15,428] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-26 17:59:15,428] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-26 17:59:15,428] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-26 17:59:15,428] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-26 17:59:15,428] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-26 17:59:15,428] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-26 17:59:15,429] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-26 17:59:15,429] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-26 17:59:15,429] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-26 17:59:15,429] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-26 17:59:15,429] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-26 17:59:15,429] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-26 17:59:15,429] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-26 17:59:15,429] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-26 17:59:15,429] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-26 17:59:15,429] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-26 17:59:15,429] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-26 17:59:15,429] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-26 17:59:15,429] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug26_17-59-12_instance-3bwob41y-01[0m
[32m[2022-08-26 17:59:15,429] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-26 17:59:15,430] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-26 17:59:15,430] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-26 17:59:15,430] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-26 17:59:15,430] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-26 17:59:15,430] [    INFO][0m - max_seq_length                :64[0m
[32m[2022-08-26 17:59:15,430] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-26 17:59:15,430] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-26 17:59:15,430] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-26 17:59:15,430] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-26 17:59:15,430] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-08-26 17:59:15,430] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-26 17:59:15,430] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-26 17:59:15,430] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-26 17:59:15,430] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-26 17:59:15,431] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-26 17:59:15,431] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-26 17:59:15,431] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-26 17:59:15,431] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-26 17:59:15,431] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-26 17:59:15,431] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-26 17:59:15,431] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-26 17:59:15,431] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-26 17:59:15,431] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-26 17:59:15,431] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-26 17:59:15,431] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-26 17:59:15,431] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-26 17:59:15,431] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-26 17:59:15,431] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-26 17:59:15,432] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-26 17:59:15,432] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-26 17:59:15,432] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-26 17:59:15,432] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-26 17:59:15,432] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-26 17:59:15,432] [    INFO][0m - seed                          :42[0m
[32m[2022-08-26 17:59:15,432] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-26 17:59:15,432] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-26 17:59:15,432] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-26 17:59:15,432] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-26 17:59:15,432] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-26 17:59:15,432] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-26 17:59:15,432] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-26 17:59:15,433] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-26 17:59:15,433] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-26 17:59:15,433] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-26 17:59:15,433] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-26 17:59:15,433] [    INFO][0m - [0m
[32m[2022-08-26 17:59:15,434] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-26 17:59:15,435] [    INFO][0m -   Num examples = 1185[0m
[32m[2022-08-26 17:59:15,435] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-08-26 17:59:15,435] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-26 17:59:15,435] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-26 17:59:15,435] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-26 17:59:15,435] [    INFO][0m -   Total optimization steps = 7450.0[0m
[32m[2022-08-26 17:59:15,435] [    INFO][0m -   Total num train samples = 59250[0m
[32m[2022-08-26 17:59:17,079] [    INFO][0m - loss: 2.43652706, learning_rate: 2.9959731543624162e-05, global_step: 10, interval_runtime: 1.6435, interval_samples_per_second: 4.868, interval_steps_per_second: 6.085, epoch: 0.0671[0m
[32m[2022-08-26 17:59:17,663] [    INFO][0m - loss: 1.91074753, learning_rate: 2.9919463087248323e-05, global_step: 20, interval_runtime: 0.5837, interval_samples_per_second: 13.706, interval_steps_per_second: 17.132, epoch: 0.1342[0m
[32m[2022-08-26 17:59:18,259] [    INFO][0m - loss: 1.85300007, learning_rate: 2.9879194630872484e-05, global_step: 30, interval_runtime: 0.5963, interval_samples_per_second: 13.417, interval_steps_per_second: 16.771, epoch: 0.2013[0m
[32m[2022-08-26 17:59:18,862] [    INFO][0m - loss: 1.85577965, learning_rate: 2.9838926174496645e-05, global_step: 40, interval_runtime: 0.6028, interval_samples_per_second: 13.271, interval_steps_per_second: 16.588, epoch: 0.2685[0m
[32m[2022-08-26 17:59:19,476] [    INFO][0m - loss: 1.60574188, learning_rate: 2.9798657718120806e-05, global_step: 50, interval_runtime: 0.6138, interval_samples_per_second: 13.034, interval_steps_per_second: 16.293, epoch: 0.3356[0m
[32m[2022-08-26 17:59:20,098] [    INFO][0m - loss: 1.44414911, learning_rate: 2.9758389261744967e-05, global_step: 60, interval_runtime: 0.6218, interval_samples_per_second: 12.866, interval_steps_per_second: 16.083, epoch: 0.4027[0m
[32m[2022-08-26 17:59:20,744] [    INFO][0m - loss: 1.40097132, learning_rate: 2.9718120805369125e-05, global_step: 70, interval_runtime: 0.6467, interval_samples_per_second: 12.371, interval_steps_per_second: 15.464, epoch: 0.4698[0m
[32m[2022-08-26 17:59:21,386] [    INFO][0m - loss: 1.67934227, learning_rate: 2.967785234899329e-05, global_step: 80, interval_runtime: 0.6417, interval_samples_per_second: 12.467, interval_steps_per_second: 15.584, epoch: 0.5369[0m
[32m[2022-08-26 17:59:22,049] [    INFO][0m - loss: 1.9141758, learning_rate: 2.963758389261745e-05, global_step: 90, interval_runtime: 0.6628, interval_samples_per_second: 12.07, interval_steps_per_second: 15.088, epoch: 0.604[0m
[32m[2022-08-26 17:59:22,711] [    INFO][0m - loss: 1.40757446, learning_rate: 2.9597315436241612e-05, global_step: 100, interval_runtime: 0.6625, interval_samples_per_second: 12.076, interval_steps_per_second: 15.095, epoch: 0.6711[0m
[32m[2022-08-26 17:59:22,712] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:59:22,712] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-08-26 17:59:22,712] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:59:22,712] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:59:22,712] [    INFO][0m -   Total prediction steps = 35[0m
[32m[2022-08-26 17:59:25,987] [    INFO][0m - eval_loss: 1.465886116027832, eval_accuracy: 0.5428051001821493, eval_runtime: 3.2743, eval_samples_per_second: 335.334, eval_steps_per_second: 10.689, epoch: 0.6711[0m
[32m[2022-08-26 17:59:25,987] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-26 17:59:25,988] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:59:28,954] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-26 17:59:28,954] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-26 17:59:33,324] [    INFO][0m - loss: 1.62408981, learning_rate: 2.9557046979865773e-05, global_step: 110, interval_runtime: 10.6128, interval_samples_per_second: 0.754, interval_steps_per_second: 0.942, epoch: 0.7383[0m
[32m[2022-08-26 17:59:34,073] [    INFO][0m - loss: 1.5716259, learning_rate: 2.951677852348993e-05, global_step: 120, interval_runtime: 0.7482, interval_samples_per_second: 10.692, interval_steps_per_second: 13.365, epoch: 0.8054[0m
[32m[2022-08-26 17:59:34,848] [    INFO][0m - loss: 1.59249144, learning_rate: 2.9476510067114095e-05, global_step: 130, interval_runtime: 0.7755, interval_samples_per_second: 10.316, interval_steps_per_second: 12.895, epoch: 0.8725[0m
[32m[2022-08-26 17:59:35,606] [    INFO][0m - loss: 1.90093632, learning_rate: 2.9436241610738256e-05, global_step: 140, interval_runtime: 0.758, interval_samples_per_second: 10.554, interval_steps_per_second: 13.193, epoch: 0.9396[0m
[32m[2022-08-26 17:59:36,412] [    INFO][0m - loss: 1.38726568, learning_rate: 2.9395973154362418e-05, global_step: 150, interval_runtime: 0.805, interval_samples_per_second: 9.938, interval_steps_per_second: 12.423, epoch: 1.0067[0m
[32m[2022-08-26 17:59:37,231] [    INFO][0m - loss: 1.26715946, learning_rate: 2.935570469798658e-05, global_step: 160, interval_runtime: 0.8206, interval_samples_per_second: 9.749, interval_steps_per_second: 12.186, epoch: 1.0738[0m
[32m[2022-08-26 17:59:38,051] [    INFO][0m - loss: 1.09570007, learning_rate: 2.9315436241610736e-05, global_step: 170, interval_runtime: 0.8195, interval_samples_per_second: 9.762, interval_steps_per_second: 12.203, epoch: 1.1409[0m
[32m[2022-08-26 17:59:38,853] [    INFO][0m - loss: 1.14764261, learning_rate: 2.92751677852349e-05, global_step: 180, interval_runtime: 0.8018, interval_samples_per_second: 9.978, interval_steps_per_second: 12.472, epoch: 1.2081[0m
[32m[2022-08-26 17:59:39,675] [    INFO][0m - loss: 1.47798986, learning_rate: 2.9234899328859062e-05, global_step: 190, interval_runtime: 0.8219, interval_samples_per_second: 9.733, interval_steps_per_second: 12.167, epoch: 1.2752[0m
[32m[2022-08-26 17:59:40,509] [    INFO][0m - loss: 1.05436583, learning_rate: 2.9194630872483223e-05, global_step: 200, interval_runtime: 0.8345, interval_samples_per_second: 9.587, interval_steps_per_second: 11.984, epoch: 1.3423[0m
[32m[2022-08-26 17:59:40,510] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:59:40,510] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-08-26 17:59:40,510] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:59:40,510] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:59:40,510] [    INFO][0m -   Total prediction steps = 35[0m
[32m[2022-08-26 17:59:44,901] [    INFO][0m - eval_loss: 1.5007414817810059, eval_accuracy: 0.5528233151183971, eval_runtime: 4.3899, eval_samples_per_second: 250.119, eval_steps_per_second: 7.973, epoch: 1.3423[0m
[32m[2022-08-26 17:59:44,903] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-26 17:59:44,903] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:59:47,867] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-26 17:59:47,868] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-26 17:59:52,305] [    INFO][0m - loss: 1.41838961, learning_rate: 2.915436241610738e-05, global_step: 210, interval_runtime: 11.796, interval_samples_per_second: 0.678, interval_steps_per_second: 0.848, epoch: 1.4094[0m
[32m[2022-08-26 17:59:53,183] [    INFO][0m - loss: 1.29003391, learning_rate: 2.9114093959731542e-05, global_step: 220, interval_runtime: 0.8777, interval_samples_per_second: 9.114, interval_steps_per_second: 11.393, epoch: 1.4765[0m
[32m[2022-08-26 17:59:54,075] [    INFO][0m - loss: 1.19108877, learning_rate: 2.9073825503355706e-05, global_step: 230, interval_runtime: 0.8915, interval_samples_per_second: 8.973, interval_steps_per_second: 11.217, epoch: 1.5436[0m
[32m[2022-08-26 17:59:54,984] [    INFO][0m - loss: 1.26282902, learning_rate: 2.9033557046979868e-05, global_step: 240, interval_runtime: 0.9094, interval_samples_per_second: 8.797, interval_steps_per_second: 10.996, epoch: 1.6107[0m
[32m[2022-08-26 17:59:55,901] [    INFO][0m - loss: 1.08989601, learning_rate: 2.899328859060403e-05, global_step: 250, interval_runtime: 0.9176, interval_samples_per_second: 8.719, interval_steps_per_second: 10.898, epoch: 1.6779[0m
[32m[2022-08-26 17:59:56,828] [    INFO][0m - loss: 1.28782225, learning_rate: 2.8953020134228186e-05, global_step: 260, interval_runtime: 0.9264, interval_samples_per_second: 8.635, interval_steps_per_second: 10.794, epoch: 1.745[0m
[32m[2022-08-26 17:59:57,768] [    INFO][0m - loss: 1.27416763, learning_rate: 2.891275167785235e-05, global_step: 270, interval_runtime: 0.9406, interval_samples_per_second: 8.505, interval_steps_per_second: 10.632, epoch: 1.8121[0m
[32m[2022-08-26 17:59:58,706] [    INFO][0m - loss: 1.02763987, learning_rate: 2.8872483221476512e-05, global_step: 280, interval_runtime: 0.9376, interval_samples_per_second: 8.533, interval_steps_per_second: 10.666, epoch: 1.8792[0m
[32m[2022-08-26 17:59:59,668] [    INFO][0m - loss: 1.05325222, learning_rate: 2.8832214765100673e-05, global_step: 290, interval_runtime: 0.9615, interval_samples_per_second: 8.32, interval_steps_per_second: 10.4, epoch: 1.9463[0m
[32m[2022-08-26 18:00:00,626] [    INFO][0m - loss: 1.24549541, learning_rate: 2.879194630872483e-05, global_step: 300, interval_runtime: 0.9588, interval_samples_per_second: 8.344, interval_steps_per_second: 10.43, epoch: 2.0134[0m
[32m[2022-08-26 18:00:00,627] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 18:00:00,627] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-08-26 18:00:00,627] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 18:00:00,627] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 18:00:00,627] [    INFO][0m -   Total prediction steps = 35[0m
[32m[2022-08-26 18:00:06,185] [    INFO][0m - eval_loss: 1.5649080276489258, eval_accuracy: 0.5264116575591985, eval_runtime: 5.5579, eval_samples_per_second: 197.557, eval_steps_per_second: 6.297, epoch: 2.0134[0m
[32m[2022-08-26 18:00:06,186] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-26 18:00:06,186] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 18:00:09,493] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-26 18:00:09,493] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-26 18:00:14,613] [    INFO][0m - loss: 0.6846427, learning_rate: 2.8751677852348992e-05, global_step: 310, interval_runtime: 13.9869, interval_samples_per_second: 0.572, interval_steps_per_second: 0.715, epoch: 2.0805[0m
[32m[2022-08-26 18:00:15,698] [    INFO][0m - loss: 0.8741704, learning_rate: 2.8711409395973157e-05, global_step: 320, interval_runtime: 1.0848, interval_samples_per_second: 7.374, interval_steps_per_second: 9.218, epoch: 2.1477[0m
[32m[2022-08-26 18:00:16,762] [    INFO][0m - loss: 0.68272781, learning_rate: 2.8671140939597318e-05, global_step: 330, interval_runtime: 1.0636, interval_samples_per_second: 7.522, interval_steps_per_second: 9.402, epoch: 2.2148[0m
[32m[2022-08-26 18:00:17,838] [    INFO][0m - loss: 1.10743752, learning_rate: 2.863087248322148e-05, global_step: 340, interval_runtime: 1.0755, interval_samples_per_second: 7.438, interval_steps_per_second: 9.298, epoch: 2.2819[0m
[32m[2022-08-26 18:00:18,906] [    INFO][0m - loss: 0.80877695, learning_rate: 2.8590604026845637e-05, global_step: 350, interval_runtime: 1.0686, interval_samples_per_second: 7.487, interval_steps_per_second: 9.358, epoch: 2.349[0m
[32m[2022-08-26 18:00:19,993] [    INFO][0m - loss: 0.64125791, learning_rate: 2.8550335570469798e-05, global_step: 360, interval_runtime: 1.0875, interval_samples_per_second: 7.356, interval_steps_per_second: 9.195, epoch: 2.4161[0m
[32m[2022-08-26 18:00:21,107] [    INFO][0m - loss: 0.78890972, learning_rate: 2.8510067114093962e-05, global_step: 370, interval_runtime: 1.1128, interval_samples_per_second: 7.189, interval_steps_per_second: 8.986, epoch: 2.4832[0m
[32m[2022-08-26 18:00:22,247] [    INFO][0m - loss: 0.93107338, learning_rate: 2.8469798657718123e-05, global_step: 380, interval_runtime: 1.1405, interval_samples_per_second: 7.014, interval_steps_per_second: 8.768, epoch: 2.5503[0m
[32m[2022-08-26 18:00:23,475] [    INFO][0m - loss: 0.87844448, learning_rate: 2.8429530201342284e-05, global_step: 390, interval_runtime: 1.2278, interval_samples_per_second: 6.516, interval_steps_per_second: 8.145, epoch: 2.6174[0m
[32m[2022-08-26 18:00:24,614] [    INFO][0m - loss: 0.93564425, learning_rate: 2.8389261744966442e-05, global_step: 400, interval_runtime: 1.14, interval_samples_per_second: 7.018, interval_steps_per_second: 8.772, epoch: 2.6846[0m
[32m[2022-08-26 18:00:24,615] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 18:00:24,615] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-08-26 18:00:24,615] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 18:00:24,615] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 18:00:24,615] [    INFO][0m -   Total prediction steps = 35[0m
[32m[2022-08-26 18:00:31,235] [    INFO][0m - eval_loss: 1.6286253929138184, eval_accuracy: 0.5282331511839709, eval_runtime: 6.6188, eval_samples_per_second: 165.892, eval_steps_per_second: 5.288, epoch: 2.6846[0m
[32m[2022-08-26 18:00:31,235] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-26 18:00:31,235] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 18:00:34,306] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-26 18:00:34,646] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-26 18:00:39,416] [    INFO][0m - loss: 0.77314939, learning_rate: 2.8348993288590603e-05, global_step: 410, interval_runtime: 14.8011, interval_samples_per_second: 0.54, interval_steps_per_second: 0.676, epoch: 2.7517[0m
[32m[2022-08-26 18:00:40,586] [    INFO][0m - loss: 0.90243397, learning_rate: 2.8308724832214768e-05, global_step: 420, interval_runtime: 1.1701, interval_samples_per_second: 6.837, interval_steps_per_second: 8.547, epoch: 2.8188[0m
[32m[2022-08-26 18:00:41,768] [    INFO][0m - loss: 0.96642323, learning_rate: 2.826845637583893e-05, global_step: 430, interval_runtime: 1.1825, interval_samples_per_second: 6.765, interval_steps_per_second: 8.457, epoch: 2.8859[0m
[32m[2022-08-26 18:00:42,968] [    INFO][0m - loss: 0.99087076, learning_rate: 2.8228187919463087e-05, global_step: 440, interval_runtime: 1.1997, interval_samples_per_second: 6.668, interval_steps_per_second: 8.335, epoch: 2.953[0m
[32m[2022-08-26 18:00:44,164] [    INFO][0m - loss: 1.11274834, learning_rate: 2.8187919463087248e-05, global_step: 450, interval_runtime: 1.1958, interval_samples_per_second: 6.69, interval_steps_per_second: 8.362, epoch: 3.0201[0m
[32m[2022-08-26 18:00:45,395] [    INFO][0m - loss: 0.43696566, learning_rate: 2.814765100671141e-05, global_step: 460, interval_runtime: 1.2315, interval_samples_per_second: 6.496, interval_steps_per_second: 8.12, epoch: 3.0872[0m
[32m[2022-08-26 18:00:46,659] [    INFO][0m - loss: 0.4884829, learning_rate: 2.8107382550335573e-05, global_step: 470, interval_runtime: 1.264, interval_samples_per_second: 6.329, interval_steps_per_second: 7.911, epoch: 3.1544[0m
[32m[2022-08-26 18:00:47,973] [    INFO][0m - loss: 0.40002637, learning_rate: 2.8067114093959734e-05, global_step: 480, interval_runtime: 1.3135, interval_samples_per_second: 6.091, interval_steps_per_second: 7.613, epoch: 3.2215[0m
[32m[2022-08-26 18:00:49,350] [    INFO][0m - loss: 0.64113193, learning_rate: 2.8026845637583892e-05, global_step: 490, interval_runtime: 1.3768, interval_samples_per_second: 5.811, interval_steps_per_second: 7.263, epoch: 3.2886[0m
[32m[2022-08-26 18:00:50,704] [    INFO][0m - loss: 0.40324941, learning_rate: 2.7986577181208053e-05, global_step: 500, interval_runtime: 1.354, interval_samples_per_second: 5.909, interval_steps_per_second: 7.386, epoch: 3.3557[0m
[32m[2022-08-26 18:00:50,704] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 18:00:50,704] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-08-26 18:00:50,705] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 18:00:50,705] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 18:00:50,705] [    INFO][0m -   Total prediction steps = 35[0m
[32m[2022-08-26 18:00:58,545] [    INFO][0m - eval_loss: 2.082432508468628, eval_accuracy: 0.546448087431694, eval_runtime: 7.8397, eval_samples_per_second: 140.056, eval_steps_per_second: 4.464, epoch: 3.3557[0m
[32m[2022-08-26 18:00:58,546] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-08-26 18:00:58,546] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 18:01:01,739] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-08-26 18:01:01,739] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-08-26 18:01:05,848] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-26 18:01:05,848] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-200 (score: 0.5528233151183971).[0m
[32m[2022-08-26 18:01:06,941] [    INFO][0m - train_runtime: 111.5054, train_samples_per_second: 531.364, train_steps_per_second: 66.813, train_loss: 1.1843291578292847, epoch: 3.3557[0m
[32m[2022-08-26 18:01:06,943] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-26 18:01:06,943] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 18:01:10,298] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-26 18:01:10,298] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-26 18:01:10,299] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-26 18:01:10,300] [    INFO][0m -   epoch                    =     3.3557[0m
[32m[2022-08-26 18:01:10,300] [    INFO][0m -   train_loss               =     1.1843[0m
[32m[2022-08-26 18:01:10,300] [    INFO][0m -   train_runtime            = 0:01:51.50[0m
[32m[2022-08-26 18:01:10,300] [    INFO][0m -   train_samples_per_second =    531.364[0m
[32m[2022-08-26 18:01:10,300] [    INFO][0m -   train_steps_per_second   =     66.813[0m
[32m[2022-08-26 18:01:10,303] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-26 18:01:10,303] [    INFO][0m -   Num examples = 2010[0m
[32m[2022-08-26 18:01:10,303] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 18:01:10,303] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 18:01:10,303] [    INFO][0m -   Total prediction steps = 63[0m
[32m[2022-08-26 18:01:25,117] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-26 18:01:25,117] [    INFO][0m -   test_accuracy           =     0.5731[0m
[32m[2022-08-26 18:01:25,117] [    INFO][0m -   test_loss               =     1.4325[0m
[32m[2022-08-26 18:01:25,118] [    INFO][0m -   test_runtime            = 0:00:14.81[0m
[32m[2022-08-26 18:01:25,118] [    INFO][0m -   test_samples_per_second =    135.686[0m
[32m[2022-08-26 18:01:25,118] [    INFO][0m -   test_steps_per_second   =      4.253[0m
[32m[2022-08-26 18:01:25,118] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-26 18:01:25,118] [    INFO][0m -   Num examples = 1500[0m
[32m[2022-08-26 18:01:25,118] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 18:01:25,118] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 18:01:25,118] [    INFO][0m -   Total prediction steps = 47[0m
[32m[2022-08-26 18:01:38,307] [    INFO][0m - Predictions for tnewsf saved to ./fewclue_submit_examples.[0m
 
==========
iflytek
==========
 
[33m[2022-08-26 18:01:42,000] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-26 18:01:42,000] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-26 18:01:42,001] [    INFO][0m - ============================================================[0m
[32m[2022-08-26 18:01:42,001] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-26 18:01:42,001] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-26 18:01:42,001] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-26 18:01:42,001] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-26 18:01:42,001] [    INFO][0m - early_stop_patience           :3[0m
[32m[2022-08-26 18:01:42,001] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-26 18:01:42,001] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-08-26 18:01:42,001] [    INFO][0m - [0m
[32m[2022-08-26 18:01:42,001] [    INFO][0m - ============================================================[0m
[32m[2022-08-26 18:01:42,002] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-26 18:01:42,002] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-26 18:01:42,002] [    INFO][0m - encoder_hidden_size           :None[0m
[32m[2022-08-26 18:01:42,002] [    INFO][0m - prompt                        :{'mask'}{'mask'}{'soft':'APPÊõ¥Êñ∞Êó•ÂøóÔºö'}{'text':'text_a'}[0m
[32m[2022-08-26 18:01:42,002] [    INFO][0m - soft_encoder                  :None[0m
[32m[2022-08-26 18:01:42,002] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-26 18:01:42,002] [    INFO][0m - task_name                     :iflytek[0m
[32m[2022-08-26 18:01:42,002] [    INFO][0m - [0m
[32m[2022-08-26 18:01:42,002] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0826 18:01:42.003810 43288 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0826 18:01:42.009150 43288 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-26 18:01:44,791] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-08-26 18:01:44,814] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-08-26 18:01:44,815] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[32m[2022-08-26 18:01:44,821] [    INFO][0m - Using template: [{'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'soft': 'app'}, {'add_prefix_space': '', 'soft': 'Êõ¥'}, {'add_prefix_space': '', 'soft': 'Êñ∞'}, {'add_prefix_space': '', 'soft': 'Êó•'}, {'add_prefix_space': '', 'soft': 'Âøó'}, {'add_prefix_space': '', 'soft': 'Ôºö'}, {'add_prefix_space': '', 'text': 'text_a'}][0m
2022-08-26 18:01:44,843 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-26 18:01:45,011] [    INFO][0m - ============================================================[0m
[32m[2022-08-26 18:01:45,011] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-26 18:01:45,011] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-26 18:01:45,012] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-26 18:01:45,012] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-26 18:01:45,012] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-26 18:01:45,012] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-26 18:01:45,012] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-26 18:01:45,012] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-26 18:01:45,012] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-26 18:01:45,012] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-26 18:01:45,012] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-26 18:01:45,012] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-26 18:01:45,012] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-26 18:01:45,013] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-26 18:01:45,013] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-26 18:01:45,013] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-26 18:01:45,013] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-26 18:01:45,013] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-26 18:01:45,013] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-26 18:01:45,013] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-26 18:01:45,013] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-26 18:01:45,013] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-26 18:01:45,013] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-26 18:01:45,013] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-26 18:01:45,013] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-26 18:01:45,013] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-26 18:01:45,014] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-26 18:01:45,014] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-26 18:01:45,014] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-26 18:01:45,014] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-26 18:01:45,014] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-26 18:01:45,014] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-26 18:01:45,014] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-26 18:01:45,014] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-26 18:01:45,014] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-26 18:01:45,014] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-26 18:01:45,014] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug26_18-01-42_instance-3bwob41y-01[0m
[32m[2022-08-26 18:01:45,014] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-26 18:01:45,014] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-26 18:01:45,014] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-26 18:01:45,015] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-26 18:01:45,015] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-26 18:01:45,015] [    INFO][0m - max_seq_length                :320[0m
[32m[2022-08-26 18:01:45,015] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-26 18:01:45,015] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-26 18:01:45,015] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-26 18:01:45,015] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-26 18:01:45,015] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-08-26 18:01:45,015] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-26 18:01:45,015] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-26 18:01:45,015] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-26 18:01:45,015] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-26 18:01:45,015] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-26 18:01:45,016] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-26 18:01:45,016] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-26 18:01:45,016] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-26 18:01:45,016] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-26 18:01:45,016] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-26 18:01:45,016] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-26 18:01:45,016] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-26 18:01:45,016] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-26 18:01:45,016] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-26 18:01:45,016] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-26 18:01:45,016] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-26 18:01:45,016] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-26 18:01:45,016] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-26 18:01:45,016] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-26 18:01:45,017] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-26 18:01:45,017] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-26 18:01:45,017] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-26 18:01:45,017] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-26 18:01:45,017] [    INFO][0m - seed                          :42[0m
[32m[2022-08-26 18:01:45,017] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-26 18:01:45,017] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-26 18:01:45,017] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-26 18:01:45,017] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-26 18:01:45,017] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-26 18:01:45,017] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-26 18:01:45,017] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-26 18:01:45,017] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-26 18:01:45,018] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-26 18:01:45,018] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-26 18:01:45,018] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-26 18:01:45,018] [    INFO][0m - [0m
[32m[2022-08-26 18:01:45,020] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-26 18:01:45,020] [    INFO][0m -   Num examples = 3024[0m
[32m[2022-08-26 18:01:45,020] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-08-26 18:01:45,020] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-26 18:01:45,020] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-26 18:01:45,020] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-26 18:01:45,020] [    INFO][0m -   Total optimization steps = 18900.0[0m
[32m[2022-08-26 18:01:45,020] [    INFO][0m -   Total num train samples = 151200[0m
[32m[2022-08-26 18:01:47,493] [    INFO][0m - loss: 4.02875443, learning_rate: 2.9984126984126986e-05, global_step: 10, interval_runtime: 2.4707, interval_samples_per_second: 3.238, interval_steps_per_second: 4.048, epoch: 0.0265[0m
[32m[2022-08-26 18:01:48,979] [    INFO][0m - loss: 3.55281906, learning_rate: 2.9968253968253967e-05, global_step: 20, interval_runtime: 1.488, interval_samples_per_second: 5.376, interval_steps_per_second: 6.72, epoch: 0.0529[0m
[32m[2022-08-26 18:01:50,495] [    INFO][0m - loss: 3.391399, learning_rate: 2.9952380952380952e-05, global_step: 30, interval_runtime: 1.5156, interval_samples_per_second: 5.278, interval_steps_per_second: 6.598, epoch: 0.0794[0m
[32m[2022-08-26 18:01:52,038] [    INFO][0m - loss: 2.71693077, learning_rate: 2.9936507936507937e-05, global_step: 40, interval_runtime: 1.543, interval_samples_per_second: 5.185, interval_steps_per_second: 6.481, epoch: 0.1058[0m
[32m[2022-08-26 18:01:53,617] [    INFO][0m - loss: 2.80195885, learning_rate: 2.992063492063492e-05, global_step: 50, interval_runtime: 1.5782, interval_samples_per_second: 5.069, interval_steps_per_second: 6.336, epoch: 0.1323[0m
[32m[2022-08-26 18:01:55,213] [    INFO][0m - loss: 2.91585121, learning_rate: 2.9904761904761907e-05, global_step: 60, interval_runtime: 1.5961, interval_samples_per_second: 5.012, interval_steps_per_second: 6.265, epoch: 0.1587[0m
[32m[2022-08-26 18:01:56,828] [    INFO][0m - loss: 2.94368553, learning_rate: 2.9888888888888892e-05, global_step: 70, interval_runtime: 1.6155, interval_samples_per_second: 4.952, interval_steps_per_second: 6.19, epoch: 0.1852[0m
[32m[2022-08-26 18:01:58,459] [    INFO][0m - loss: 2.58234673, learning_rate: 2.9873015873015874e-05, global_step: 80, interval_runtime: 1.6303, interval_samples_per_second: 4.907, interval_steps_per_second: 6.134, epoch: 0.2116[0m
[32m[2022-08-26 18:02:00,113] [    INFO][0m - loss: 2.34315624, learning_rate: 2.985714285714286e-05, global_step: 90, interval_runtime: 1.6543, interval_samples_per_second: 4.836, interval_steps_per_second: 6.045, epoch: 0.2381[0m
[32m[2022-08-26 18:02:01,818] [    INFO][0m - loss: 2.28943386, learning_rate: 2.984126984126984e-05, global_step: 100, interval_runtime: 1.7054, interval_samples_per_second: 4.691, interval_steps_per_second: 5.864, epoch: 0.2646[0m
[32m[2022-08-26 18:02:01,819] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 18:02:01,819] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-26 18:02:01,819] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 18:02:01,819] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 18:02:01,819] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-26 18:02:16,954] [    INFO][0m - eval_loss: 2.122549057006836, eval_accuracy: 0.4763292061179898, eval_runtime: 15.134, eval_samples_per_second: 90.723, eval_steps_per_second: 2.841, epoch: 0.2646[0m
[32m[2022-08-26 18:02:16,954] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-26 18:02:16,954] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 18:02:20,536] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-26 18:02:20,536] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-26 18:02:26,144] [    INFO][0m - loss: 2.26282272, learning_rate: 2.9825396825396825e-05, global_step: 110, interval_runtime: 24.3249, interval_samples_per_second: 0.329, interval_steps_per_second: 0.411, epoch: 0.291[0m
[32m[2022-08-26 18:02:27,975] [    INFO][0m - loss: 2.10199947, learning_rate: 2.980952380952381e-05, global_step: 120, interval_runtime: 1.8324, interval_samples_per_second: 4.366, interval_steps_per_second: 5.457, epoch: 0.3175[0m
[32m[2022-08-26 18:02:29,815] [    INFO][0m - loss: 2.48916435, learning_rate: 2.9793650793650792e-05, global_step: 130, interval_runtime: 1.8395, interval_samples_per_second: 4.349, interval_steps_per_second: 5.436, epoch: 0.3439[0m
[32m[2022-08-26 18:02:31,703] [    INFO][0m - loss: 2.1155426, learning_rate: 2.9777777777777777e-05, global_step: 140, interval_runtime: 1.8881, interval_samples_per_second: 4.237, interval_steps_per_second: 5.296, epoch: 0.3704[0m
[32m[2022-08-26 18:02:33,602] [    INFO][0m - loss: 2.35786057, learning_rate: 2.9761904761904762e-05, global_step: 150, interval_runtime: 1.8984, interval_samples_per_second: 4.214, interval_steps_per_second: 5.268, epoch: 0.3968[0m
[32m[2022-08-26 18:02:35,523] [    INFO][0m - loss: 2.52774029, learning_rate: 2.9746031746031747e-05, global_step: 160, interval_runtime: 1.9219, interval_samples_per_second: 4.162, interval_steps_per_second: 5.203, epoch: 0.4233[0m
[32m[2022-08-26 18:02:37,471] [    INFO][0m - loss: 2.12789173, learning_rate: 2.9730158730158732e-05, global_step: 170, interval_runtime: 1.9476, interval_samples_per_second: 4.108, interval_steps_per_second: 5.135, epoch: 0.4497[0m
[32m[2022-08-26 18:02:39,430] [    INFO][0m - loss: 2.55057697, learning_rate: 2.9714285714285717e-05, global_step: 180, interval_runtime: 1.9588, interval_samples_per_second: 4.084, interval_steps_per_second: 5.105, epoch: 0.4762[0m
[32m[2022-08-26 18:02:41,428] [    INFO][0m - loss: 2.15597668, learning_rate: 2.96984126984127e-05, global_step: 190, interval_runtime: 1.998, interval_samples_per_second: 4.004, interval_steps_per_second: 5.005, epoch: 0.5026[0m
[32m[2022-08-26 18:02:43,452] [    INFO][0m - loss: 2.13433628, learning_rate: 2.9682539682539683e-05, global_step: 200, interval_runtime: 2.0231, interval_samples_per_second: 3.954, interval_steps_per_second: 4.943, epoch: 0.5291[0m
[32m[2022-08-26 18:02:43,453] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 18:02:43,453] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-26 18:02:43,453] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 18:02:43,453] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 18:02:43,453] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-26 18:03:03,237] [    INFO][0m - eval_loss: 2.046265125274658, eval_accuracy: 0.4632192279679534, eval_runtime: 19.783, eval_samples_per_second: 69.403, eval_steps_per_second: 2.174, epoch: 0.5291[0m
[32m[2022-08-26 18:03:03,238] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-26 18:03:03,238] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 18:03:06,653] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-26 18:03:06,653] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-26 18:03:12,831] [    INFO][0m - loss: 2.10030441, learning_rate: 2.966666666666667e-05, global_step: 210, interval_runtime: 29.3799, interval_samples_per_second: 0.272, interval_steps_per_second: 0.34, epoch: 0.5556[0m
[32m[2022-08-26 18:03:14,996] [    INFO][0m - loss: 2.11262684, learning_rate: 2.965079365079365e-05, global_step: 220, interval_runtime: 2.165, interval_samples_per_second: 3.695, interval_steps_per_second: 4.619, epoch: 0.582[0m
[32m[2022-08-26 18:03:17,191] [    INFO][0m - loss: 2.10511608, learning_rate: 2.9634920634920635e-05, global_step: 230, interval_runtime: 2.1955, interval_samples_per_second: 3.644, interval_steps_per_second: 4.555, epoch: 0.6085[0m
[32m[2022-08-26 18:03:19,405] [    INFO][0m - loss: 2.06246395, learning_rate: 2.961904761904762e-05, global_step: 240, interval_runtime: 2.2141, interval_samples_per_second: 3.613, interval_steps_per_second: 4.516, epoch: 0.6349[0m
[32m[2022-08-26 18:03:21,647] [    INFO][0m - loss: 1.93616772, learning_rate: 2.96031746031746e-05, global_step: 250, interval_runtime: 2.2417, interval_samples_per_second: 3.569, interval_steps_per_second: 4.461, epoch: 0.6614[0m
[32m[2022-08-26 18:03:23,900] [    INFO][0m - loss: 2.09470901, learning_rate: 2.958730158730159e-05, global_step: 260, interval_runtime: 2.2527, interval_samples_per_second: 3.551, interval_steps_per_second: 4.439, epoch: 0.6878[0m
[32m[2022-08-26 18:03:26,189] [    INFO][0m - loss: 1.71078911, learning_rate: 2.9571428571428575e-05, global_step: 270, interval_runtime: 2.2887, interval_samples_per_second: 3.495, interval_steps_per_second: 4.369, epoch: 0.7143[0m
[32m[2022-08-26 18:03:28,485] [    INFO][0m - loss: 2.35802307, learning_rate: 2.9555555555555556e-05, global_step: 280, interval_runtime: 2.2965, interval_samples_per_second: 3.484, interval_steps_per_second: 4.354, epoch: 0.7407[0m
[32m[2022-08-26 18:03:30,861] [    INFO][0m - loss: 2.05029106, learning_rate: 2.953968253968254e-05, global_step: 290, interval_runtime: 2.3755, interval_samples_per_second: 3.368, interval_steps_per_second: 4.21, epoch: 0.7672[0m
[32m[2022-08-26 18:03:33,219] [    INFO][0m - loss: 1.72946644, learning_rate: 2.9523809523809523e-05, global_step: 300, interval_runtime: 2.3586, interval_samples_per_second: 3.392, interval_steps_per_second: 4.24, epoch: 0.7937[0m
[32m[2022-08-26 18:03:33,220] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 18:03:33,220] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-26 18:03:33,220] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 18:03:33,220] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 18:03:33,221] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-26 18:03:57,414] [    INFO][0m - eval_loss: 1.9770747423171997, eval_accuracy: 0.5142024763292061, eval_runtime: 24.1928, eval_samples_per_second: 56.752, eval_steps_per_second: 1.777, epoch: 0.7937[0m
[32m[2022-08-26 18:03:57,414] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-26 18:03:57,415] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 18:04:00,344] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-26 18:04:00,344] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-26 18:04:06,636] [    INFO][0m - loss: 2.40616913, learning_rate: 2.9507936507936508e-05, global_step: 310, interval_runtime: 33.4166, interval_samples_per_second: 0.239, interval_steps_per_second: 0.299, epoch: 0.8201[0m
[32m[2022-08-26 18:04:09,144] [    INFO][0m - loss: 1.65835152, learning_rate: 2.9492063492063493e-05, global_step: 320, interval_runtime: 2.5078, interval_samples_per_second: 3.19, interval_steps_per_second: 3.988, epoch: 0.8466[0m
[32m[2022-08-26 18:04:11,666] [    INFO][0m - loss: 2.22244167, learning_rate: 2.9476190476190475e-05, global_step: 330, interval_runtime: 2.5225, interval_samples_per_second: 3.171, interval_steps_per_second: 3.964, epoch: 0.873[0m
[32m[2022-08-26 18:04:14,231] [    INFO][0m - loss: 1.97425728, learning_rate: 2.946031746031746e-05, global_step: 340, interval_runtime: 2.5643, interval_samples_per_second: 3.12, interval_steps_per_second: 3.9, epoch: 0.8995[0m
[32m[2022-08-26 18:04:16,813] [    INFO][0m - loss: 2.34288521, learning_rate: 2.9444444444444445e-05, global_step: 350, interval_runtime: 2.5819, interval_samples_per_second: 3.098, interval_steps_per_second: 3.873, epoch: 0.9259[0m
[32m[2022-08-26 18:04:19,437] [    INFO][0m - loss: 2.37728424, learning_rate: 2.942857142857143e-05, global_step: 360, interval_runtime: 2.623, interval_samples_per_second: 3.05, interval_steps_per_second: 3.812, epoch: 0.9524[0m
[32m[2022-08-26 18:04:22,055] [    INFO][0m - loss: 2.09043083, learning_rate: 2.9412698412698414e-05, global_step: 370, interval_runtime: 2.6188, interval_samples_per_second: 3.055, interval_steps_per_second: 3.819, epoch: 0.9788[0m
[32m[2022-08-26 18:04:24,840] [    INFO][0m - loss: 1.63085728, learning_rate: 2.93968253968254e-05, global_step: 380, interval_runtime: 2.7854, interval_samples_per_second: 2.872, interval_steps_per_second: 3.59, epoch: 1.0053[0m
[32m[2022-08-26 18:04:27,505] [    INFO][0m - loss: 1.44423952, learning_rate: 2.938095238095238e-05, global_step: 390, interval_runtime: 2.6654, interval_samples_per_second: 3.001, interval_steps_per_second: 3.752, epoch: 1.0317[0m
[32m[2022-08-26 18:04:30,230] [    INFO][0m - loss: 1.24971733, learning_rate: 2.9365079365079366e-05, global_step: 400, interval_runtime: 2.7242, interval_samples_per_second: 2.937, interval_steps_per_second: 3.671, epoch: 1.0582[0m
[32m[2022-08-26 18:04:30,230] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 18:04:30,231] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-26 18:04:30,231] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 18:04:30,231] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 18:04:30,231] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-26 18:04:58,789] [    INFO][0m - eval_loss: 2.070162057876587, eval_accuracy: 0.5098324836125273, eval_runtime: 28.5578, eval_samples_per_second: 48.078, eval_steps_per_second: 1.506, epoch: 1.0582[0m
[32m[2022-08-26 18:04:58,790] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-26 18:04:58,790] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 18:05:02,118] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-26 18:05:02,118] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-26 18:05:08,542] [    INFO][0m - loss: 1.58558044, learning_rate: 2.934920634920635e-05, global_step: 410, interval_runtime: 38.313, interval_samples_per_second: 0.209, interval_steps_per_second: 0.261, epoch: 1.0847[0m
[32m[2022-08-26 18:05:11,381] [    INFO][0m - loss: 1.26321459, learning_rate: 2.9333333333333333e-05, global_step: 420, interval_runtime: 2.8384, interval_samples_per_second: 2.818, interval_steps_per_second: 3.523, epoch: 1.1111[0m
[32m[2022-08-26 18:05:14,277] [    INFO][0m - loss: 1.48831615, learning_rate: 2.9317460317460318e-05, global_step: 430, interval_runtime: 2.8956, interval_samples_per_second: 2.763, interval_steps_per_second: 3.454, epoch: 1.1376[0m
[32m[2022-08-26 18:05:17,163] [    INFO][0m - loss: 1.31763363, learning_rate: 2.9301587301587303e-05, global_step: 440, interval_runtime: 2.8868, interval_samples_per_second: 2.771, interval_steps_per_second: 3.464, epoch: 1.164[0m
[32m[2022-08-26 18:05:20,071] [    INFO][0m - loss: 1.45475168, learning_rate: 2.9285714285714284e-05, global_step: 450, interval_runtime: 2.9077, interval_samples_per_second: 2.751, interval_steps_per_second: 3.439, epoch: 1.1905[0m
[32m[2022-08-26 18:05:22,990] [    INFO][0m - loss: 1.68269157, learning_rate: 2.9269841269841272e-05, global_step: 460, interval_runtime: 2.9185, interval_samples_per_second: 2.741, interval_steps_per_second: 3.426, epoch: 1.2169[0m
[32m[2022-08-26 18:05:25,909] [    INFO][0m - loss: 1.64843159, learning_rate: 2.9253968253968257e-05, global_step: 470, interval_runtime: 2.9195, interval_samples_per_second: 2.74, interval_steps_per_second: 3.425, epoch: 1.2434[0m
[32m[2022-08-26 18:05:28,854] [    INFO][0m - loss: 1.44134617, learning_rate: 2.923809523809524e-05, global_step: 480, interval_runtime: 2.9454, interval_samples_per_second: 2.716, interval_steps_per_second: 3.395, epoch: 1.2698[0m
[32m[2022-08-26 18:05:31,843] [    INFO][0m - loss: 1.51093607, learning_rate: 2.9222222222222224e-05, global_step: 490, interval_runtime: 2.9886, interval_samples_per_second: 2.677, interval_steps_per_second: 3.346, epoch: 1.2963[0m
[32m[2022-08-26 18:05:34,841] [    INFO][0m - loss: 1.83103676, learning_rate: 2.9206349206349206e-05, global_step: 500, interval_runtime: 2.9977, interval_samples_per_second: 2.669, interval_steps_per_second: 3.336, epoch: 1.3228[0m
[32m[2022-08-26 18:05:34,842] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 18:05:34,842] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-26 18:05:34,842] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 18:05:34,842] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 18:05:34,842] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-26 18:06:08,006] [    INFO][0m - eval_loss: 2.009594678878784, eval_accuracy: 0.5222141296431173, eval_runtime: 33.1638, eval_samples_per_second: 41.401, eval_steps_per_second: 1.297, epoch: 1.3228[0m
[32m[2022-08-26 18:06:08,007] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-08-26 18:06:08,007] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 18:06:11,038] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-08-26 18:06:11,038] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-08-26 18:06:17,781] [    INFO][0m - loss: 1.92607307, learning_rate: 2.919047619047619e-05, global_step: 510, interval_runtime: 42.9405, interval_samples_per_second: 0.186, interval_steps_per_second: 0.233, epoch: 1.3492[0m
[32m[2022-08-26 18:06:20,928] [    INFO][0m - loss: 1.68013134, learning_rate: 2.9174603174603176e-05, global_step: 520, interval_runtime: 3.1473, interval_samples_per_second: 2.542, interval_steps_per_second: 3.177, epoch: 1.3757[0m
[32m[2022-08-26 18:06:24,121] [    INFO][0m - loss: 1.39268618, learning_rate: 2.9158730158730157e-05, global_step: 530, interval_runtime: 3.1924, interval_samples_per_second: 2.506, interval_steps_per_second: 3.132, epoch: 1.4021[0m
[32m[2022-08-26 18:06:27,338] [    INFO][0m - loss: 1.60417576, learning_rate: 2.9142857142857142e-05, global_step: 540, interval_runtime: 3.2169, interval_samples_per_second: 2.487, interval_steps_per_second: 3.109, epoch: 1.4286[0m
[32m[2022-08-26 18:06:30,560] [    INFO][0m - loss: 1.32859621, learning_rate: 2.9126984126984127e-05, global_step: 550, interval_runtime: 3.2226, interval_samples_per_second: 2.482, interval_steps_per_second: 3.103, epoch: 1.455[0m
[32m[2022-08-26 18:06:33,805] [    INFO][0m - loss: 1.42840738, learning_rate: 2.9111111111111112e-05, global_step: 560, interval_runtime: 3.2442, interval_samples_per_second: 2.466, interval_steps_per_second: 3.082, epoch: 1.4815[0m
[32m[2022-08-26 18:06:37,051] [    INFO][0m - loss: 1.61717758, learning_rate: 2.9095238095238097e-05, global_step: 570, interval_runtime: 3.2465, interval_samples_per_second: 2.464, interval_steps_per_second: 3.08, epoch: 1.5079[0m
[32m[2022-08-26 18:06:40,326] [    INFO][0m - loss: 1.6169178, learning_rate: 2.9079365079365082e-05, global_step: 580, interval_runtime: 3.2751, interval_samples_per_second: 2.443, interval_steps_per_second: 3.053, epoch: 1.5344[0m
[32m[2022-08-26 18:06:43,639] [    INFO][0m - loss: 1.66106033, learning_rate: 2.9063492063492064e-05, global_step: 590, interval_runtime: 3.3124, interval_samples_per_second: 2.415, interval_steps_per_second: 3.019, epoch: 1.5608[0m
[32m[2022-08-26 18:06:46,973] [    INFO][0m - loss: 1.56264725, learning_rate: 2.904761904761905e-05, global_step: 600, interval_runtime: 3.3344, interval_samples_per_second: 2.399, interval_steps_per_second: 2.999, epoch: 1.5873[0m
[32m[2022-08-26 18:06:46,973] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 18:06:46,974] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-26 18:06:46,974] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 18:06:46,974] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 18:06:46,974] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-26 18:07:24,615] [    INFO][0m - eval_loss: 1.9712326526641846, eval_accuracy: 0.5258557902403496, eval_runtime: 37.6402, eval_samples_per_second: 36.477, eval_steps_per_second: 1.142, epoch: 1.5873[0m
[32m[2022-08-26 18:07:24,615] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-08-26 18:07:24,615] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 18:07:27,811] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-08-26 18:07:27,811] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-08-26 18:07:34,847] [    INFO][0m - loss: 1.57451296, learning_rate: 2.9031746031746034e-05, global_step: 610, interval_runtime: 47.8737, interval_samples_per_second: 0.167, interval_steps_per_second: 0.209, epoch: 1.6138[0m
[32m[2022-08-26 18:07:38,314] [    INFO][0m - loss: 1.60875874, learning_rate: 2.9015873015873015e-05, global_step: 620, interval_runtime: 3.4672, interval_samples_per_second: 2.307, interval_steps_per_second: 2.884, epoch: 1.6402[0m
[32m[2022-08-26 18:07:41,811] [    INFO][0m - loss: 1.18518257, learning_rate: 2.9e-05, global_step: 630, interval_runtime: 3.4968, interval_samples_per_second: 2.288, interval_steps_per_second: 2.86, epoch: 1.6667[0m
[32m[2022-08-26 18:07:45,326] [    INFO][0m - loss: 1.56509161, learning_rate: 2.8984126984126985e-05, global_step: 640, interval_runtime: 3.5152, interval_samples_per_second: 2.276, interval_steps_per_second: 2.845, epoch: 1.6931[0m
[32m[2022-08-26 18:07:48,854] [    INFO][0m - loss: 1.55597019, learning_rate: 2.8968253968253967e-05, global_step: 650, interval_runtime: 3.5279, interval_samples_per_second: 2.268, interval_steps_per_second: 2.835, epoch: 1.7196[0m
[32m[2022-08-26 18:07:52,434] [    INFO][0m - loss: 1.86016388, learning_rate: 2.8952380952380955e-05, global_step: 660, interval_runtime: 3.5801, interval_samples_per_second: 2.235, interval_steps_per_second: 2.793, epoch: 1.746[0m
[32m[2022-08-26 18:07:56,002] [    INFO][0m - loss: 1.66703472, learning_rate: 2.893650793650794e-05, global_step: 670, interval_runtime: 3.5676, interval_samples_per_second: 2.242, interval_steps_per_second: 2.803, epoch: 1.7725[0m
[32m[2022-08-26 18:07:59,598] [    INFO][0m - loss: 1.68519554, learning_rate: 2.892063492063492e-05, global_step: 680, interval_runtime: 3.5965, interval_samples_per_second: 2.224, interval_steps_per_second: 2.78, epoch: 1.7989[0m
[32m[2022-08-26 18:08:03,230] [    INFO][0m - loss: 1.50643501, learning_rate: 2.8904761904761907e-05, global_step: 690, interval_runtime: 3.6318, interval_samples_per_second: 2.203, interval_steps_per_second: 2.753, epoch: 1.8254[0m
[32m[2022-08-26 18:08:06,871] [    INFO][0m - loss: 1.37343616, learning_rate: 2.8888888888888888e-05, global_step: 700, interval_runtime: 3.6407, interval_samples_per_second: 2.197, interval_steps_per_second: 2.747, epoch: 1.8519[0m
[32m[2022-08-26 18:08:06,871] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 18:08:06,871] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-26 18:08:06,871] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 18:08:06,871] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 18:08:06,871] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-26 18:08:49,158] [    INFO][0m - eval_loss: 1.9904556274414062, eval_accuracy: 0.528040786598689, eval_runtime: 42.2857, eval_samples_per_second: 32.47, eval_steps_per_second: 1.017, epoch: 1.8519[0m
[32m[2022-08-26 18:08:49,158] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-700[0m
[32m[2022-08-26 18:08:49,158] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 18:08:52,579] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-700/tokenizer_config.json[0m
[32m[2022-08-26 18:08:52,579] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-700/special_tokens_map.json[0m
[32m[2022-08-26 18:09:00,052] [    INFO][0m - loss: 1.69317379, learning_rate: 2.8873015873015873e-05, global_step: 710, interval_runtime: 53.1813, interval_samples_per_second: 0.15, interval_steps_per_second: 0.188, epoch: 1.8783[0m
[32m[2022-08-26 18:09:03,856] [    INFO][0m - loss: 1.55206738, learning_rate: 2.8857142857142858e-05, global_step: 720, interval_runtime: 3.8041, interval_samples_per_second: 2.103, interval_steps_per_second: 2.629, epoch: 1.9048[0m
[32m[2022-08-26 18:09:07,736] [    INFO][0m - loss: 1.72060394, learning_rate: 2.884126984126984e-05, global_step: 730, interval_runtime: 3.8802, interval_samples_per_second: 2.062, interval_steps_per_second: 2.577, epoch: 1.9312[0m
[32m[2022-08-26 18:09:11,575] [    INFO][0m - loss: 1.3552824, learning_rate: 2.8825396825396825e-05, global_step: 740, interval_runtime: 3.8384, interval_samples_per_second: 2.084, interval_steps_per_second: 2.605, epoch: 1.9577[0m
[32m[2022-08-26 18:09:15,448] [    INFO][0m - loss: 1.48823147, learning_rate: 2.880952380952381e-05, global_step: 750, interval_runtime: 3.8737, interval_samples_per_second: 2.065, interval_steps_per_second: 2.582, epoch: 1.9841[0m
[32m[2022-08-26 18:09:19,545] [    INFO][0m - loss: 1.13390779, learning_rate: 2.8793650793650795e-05, global_step: 760, interval_runtime: 4.096, interval_samples_per_second: 1.953, interval_steps_per_second: 2.441, epoch: 2.0106[0m
[32m[2022-08-26 18:09:23,540] [    INFO][0m - loss: 0.77779651, learning_rate: 2.877777777777778e-05, global_step: 770, interval_runtime: 3.9955, interval_samples_per_second: 2.002, interval_steps_per_second: 2.503, epoch: 2.037[0m
[32m[2022-08-26 18:09:27,524] [    INFO][0m - loss: 1.07857618, learning_rate: 2.8761904761904765e-05, global_step: 780, interval_runtime: 3.9839, interval_samples_per_second: 2.008, interval_steps_per_second: 2.51, epoch: 2.0635[0m
[32m[2022-08-26 18:09:31,535] [    INFO][0m - loss: 0.99428844, learning_rate: 2.8746031746031746e-05, global_step: 790, interval_runtime: 4.0108, interval_samples_per_second: 1.995, interval_steps_per_second: 2.493, epoch: 2.0899[0m
[32m[2022-08-26 18:09:35,529] [    INFO][0m - loss: 0.76773872, learning_rate: 2.873015873015873e-05, global_step: 800, interval_runtime: 3.9941, interval_samples_per_second: 2.003, interval_steps_per_second: 2.504, epoch: 2.1164[0m
[32m[2022-08-26 18:09:35,530] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 18:09:35,530] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-26 18:09:35,530] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 18:09:35,530] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 18:09:35,530] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-26 18:10:22,253] [    INFO][0m - eval_loss: 2.0884523391723633, eval_accuracy: 0.5091041514930809, eval_runtime: 46.7229, eval_samples_per_second: 29.386, eval_steps_per_second: 0.92, epoch: 2.1164[0m
[32m[2022-08-26 18:10:22,254] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-800[0m
[32m[2022-08-26 18:10:22,254] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 18:10:24,827] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-800/tokenizer_config.json[0m
[32m[2022-08-26 18:10:24,828] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-800/special_tokens_map.json[0m
[32m[2022-08-26 18:10:32,734] [    INFO][0m - loss: 1.04395313, learning_rate: 2.8714285714285716e-05, global_step: 810, interval_runtime: 57.2053, interval_samples_per_second: 0.14, interval_steps_per_second: 0.175, epoch: 2.1429[0m
[32m[2022-08-26 18:10:36,914] [    INFO][0m - loss: 1.14361057, learning_rate: 2.8698412698412698e-05, global_step: 820, interval_runtime: 4.1799, interval_samples_per_second: 1.914, interval_steps_per_second: 2.392, epoch: 2.1693[0m
[32m[2022-08-26 18:10:41,099] [    INFO][0m - loss: 0.96054554, learning_rate: 2.8682539682539683e-05, global_step: 830, interval_runtime: 4.1848, interval_samples_per_second: 1.912, interval_steps_per_second: 2.39, epoch: 2.1958[0m
[32m[2022-08-26 18:10:45,281] [    INFO][0m - loss: 0.96382103, learning_rate: 2.8666666666666668e-05, global_step: 840, interval_runtime: 4.1817, interval_samples_per_second: 1.913, interval_steps_per_second: 2.391, epoch: 2.2222[0m
[32m[2022-08-26 18:10:49,478] [    INFO][0m - loss: 1.07217264, learning_rate: 2.865079365079365e-05, global_step: 850, interval_runtime: 4.1976, interval_samples_per_second: 1.906, interval_steps_per_second: 2.382, epoch: 2.2487[0m
[32m[2022-08-26 18:10:53,753] [    INFO][0m - loss: 0.83644304, learning_rate: 2.8634920634920638e-05, global_step: 860, interval_runtime: 4.2753, interval_samples_per_second: 1.871, interval_steps_per_second: 2.339, epoch: 2.2751[0m
[32m[2022-08-26 18:10:58,027] [    INFO][0m - loss: 1.37072182, learning_rate: 2.8619047619047623e-05, global_step: 870, interval_runtime: 4.2734, interval_samples_per_second: 1.872, interval_steps_per_second: 2.34, epoch: 2.3016[0m
[32m[2022-08-26 18:11:02,298] [    INFO][0m - loss: 0.9946826, learning_rate: 2.8603174603174604e-05, global_step: 880, interval_runtime: 4.2707, interval_samples_per_second: 1.873, interval_steps_per_second: 2.342, epoch: 2.328[0m
[32m[2022-08-26 18:11:06,641] [    INFO][0m - loss: 1.04548941, learning_rate: 2.858730158730159e-05, global_step: 890, interval_runtime: 4.3434, interval_samples_per_second: 1.842, interval_steps_per_second: 2.302, epoch: 2.3545[0m
[32m[2022-08-26 18:11:10,980] [    INFO][0m - loss: 1.0303277, learning_rate: 2.857142857142857e-05, global_step: 900, interval_runtime: 4.3387, interval_samples_per_second: 1.844, interval_steps_per_second: 2.305, epoch: 2.381[0m
[32m[2022-08-26 18:11:10,980] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 18:11:10,981] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-26 18:11:10,981] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 18:11:10,981] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 18:11:10,981] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-26 18:12:02,221] [    INFO][0m - eval_loss: 2.131678819656372, eval_accuracy: 0.5098324836125273, eval_runtime: 51.24, eval_samples_per_second: 26.795, eval_steps_per_second: 0.839, epoch: 2.381[0m
[32m[2022-08-26 18:12:02,222] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-900[0m
[32m[2022-08-26 18:12:02,222] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 18:12:05,520] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-900/tokenizer_config.json[0m
[32m[2022-08-26 18:12:05,520] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-900/special_tokens_map.json[0m
[32m[2022-08-26 18:12:13,960] [    INFO][0m - loss: 1.1333045, learning_rate: 2.8555555555555556e-05, global_step: 910, interval_runtime: 62.9788, interval_samples_per_second: 0.127, interval_steps_per_second: 0.159, epoch: 2.4074[0m
[32m[2022-08-26 18:12:18,447] [    INFO][0m - loss: 1.17481871, learning_rate: 2.853968253968254e-05, global_step: 920, interval_runtime: 4.4886, interval_samples_per_second: 1.782, interval_steps_per_second: 2.228, epoch: 2.4339[0m
[32m[2022-08-26 18:12:22,949] [    INFO][0m - loss: 1.13352032, learning_rate: 2.8523809523809522e-05, global_step: 930, interval_runtime: 4.5021, interval_samples_per_second: 1.777, interval_steps_per_second: 2.221, epoch: 2.4603[0m
[32m[2022-08-26 18:12:27,478] [    INFO][0m - loss: 0.87532797, learning_rate: 2.8507936507936507e-05, global_step: 940, interval_runtime: 4.5287, interval_samples_per_second: 1.767, interval_steps_per_second: 2.208, epoch: 2.4868[0m
[32m[2022-08-26 18:12:32,029] [    INFO][0m - loss: 1.2340723, learning_rate: 2.8492063492063492e-05, global_step: 950, interval_runtime: 4.5516, interval_samples_per_second: 1.758, interval_steps_per_second: 2.197, epoch: 2.5132[0m
[32m[2022-08-26 18:12:36,599] [    INFO][0m - loss: 1.04456005, learning_rate: 2.8476190476190477e-05, global_step: 960, interval_runtime: 4.5692, interval_samples_per_second: 1.751, interval_steps_per_second: 2.189, epoch: 2.5397[0m
[32m[2022-08-26 18:12:41,174] [    INFO][0m - loss: 1.23570623, learning_rate: 2.8460317460317462e-05, global_step: 970, interval_runtime: 4.5753, interval_samples_per_second: 1.749, interval_steps_per_second: 2.186, epoch: 2.5661[0m
[32m[2022-08-26 18:12:45,801] [    INFO][0m - loss: 1.08531895, learning_rate: 2.8444444444444447e-05, global_step: 980, interval_runtime: 4.627, interval_samples_per_second: 1.729, interval_steps_per_second: 2.161, epoch: 2.5926[0m
[32m[2022-08-26 18:12:50,440] [    INFO][0m - loss: 1.01975079, learning_rate: 2.842857142857143e-05, global_step: 990, interval_runtime: 4.6389, interval_samples_per_second: 1.725, interval_steps_per_second: 2.156, epoch: 2.619[0m
[32m[2022-08-26 18:12:55,148] [    INFO][0m - loss: 1.18404741, learning_rate: 2.8412698412698414e-05, global_step: 1000, interval_runtime: 4.7081, interval_samples_per_second: 1.699, interval_steps_per_second: 2.124, epoch: 2.6455[0m
[32m[2022-08-26 18:12:55,148] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 18:12:55,149] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-26 18:12:55,149] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 18:12:55,149] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 18:12:55,149] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-26 18:13:50,690] [    INFO][0m - eval_loss: 2.0906074047088623, eval_accuracy: 0.5112891478514202, eval_runtime: 55.5411, eval_samples_per_second: 24.72, eval_steps_per_second: 0.774, epoch: 2.6455[0m
[32m[2022-08-26 18:13:50,691] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1000[0m
[32m[2022-08-26 18:13:50,691] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 18:13:52,066] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1000/tokenizer_config.json[0m
[32m[2022-08-26 18:13:52,066] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1000/special_tokens_map.json[0m
[32m[2022-08-26 18:13:53,721] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-26 18:13:53,721] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-700 (score: 0.528040786598689).[0m
[32m[2022-08-26 18:13:54,861] [    INFO][0m - train_runtime: 729.8401, train_samples_per_second: 207.169, train_steps_per_second: 25.896, train_loss: 1.7281429333686829, epoch: 2.6455[0m
[32m[2022-08-26 18:13:54,862] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-26 18:13:54,862] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 18:13:57,814] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-26 18:13:57,815] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-26 18:13:57,816] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-26 18:13:57,817] [    INFO][0m -   epoch                    =     2.6455[0m
[32m[2022-08-26 18:13:57,817] [    INFO][0m -   train_loss               =     1.7281[0m
[32m[2022-08-26 18:13:57,817] [    INFO][0m -   train_runtime            = 0:12:09.84[0m
[32m[2022-08-26 18:13:57,817] [    INFO][0m -   train_samples_per_second =    207.169[0m
[32m[2022-08-26 18:13:57,817] [    INFO][0m -   train_steps_per_second   =     25.896[0m
[32m[2022-08-26 18:13:57,822] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-26 18:13:57,822] [    INFO][0m -   Num examples = 1749[0m
[32m[2022-08-26 18:13:57,822] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 18:13:57,822] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 18:13:57,822] [    INFO][0m -   Total prediction steps = 55[0m
[32m[2022-08-26 18:15:10,389] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-26 18:15:10,390] [    INFO][0m -   test_accuracy           =     0.5386[0m
[32m[2022-08-26 18:15:10,390] [    INFO][0m -   test_loss               =     1.8953[0m
[32m[2022-08-26 18:15:10,390] [    INFO][0m -   test_runtime            = 0:01:12.56[0m
[32m[2022-08-26 18:15:10,390] [    INFO][0m -   test_samples_per_second =     24.102[0m
[32m[2022-08-26 18:15:10,390] [    INFO][0m -   test_steps_per_second   =      0.758[0m
[32m[2022-08-26 18:15:10,391] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-26 18:15:10,391] [    INFO][0m -   Num examples = 2600[0m
[32m[2022-08-26 18:15:10,391] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 18:15:10,391] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 18:15:10,391] [    INFO][0m -   Total prediction steps = 82[0m
[32m[2022-08-26 18:17:08,774] [    INFO][0m - Predictions for iflytekf saved to ./fewclue_submit_examples.[0m
 
==========
ocnli
==========
 
[33m[2022-08-26 18:17:12,739] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-26 18:17:12,739] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-26 18:17:12,739] [    INFO][0m - ============================================================[0m
[32m[2022-08-26 18:17:12,739] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-26 18:17:12,739] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-26 18:17:12,740] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-26 18:17:12,740] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-26 18:17:12,740] [    INFO][0m - early_stop_patience           :3[0m
[32m[2022-08-26 18:17:12,740] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-26 18:17:12,740] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-08-26 18:17:12,740] [    INFO][0m - [0m
[32m[2022-08-26 18:17:12,740] [    INFO][0m - ============================================================[0m
[32m[2022-08-26 18:17:12,740] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-26 18:17:12,740] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-26 18:17:12,740] [    INFO][0m - encoder_hidden_size           :None[0m
[32m[2022-08-26 18:17:12,740] [    INFO][0m - prompt                        :{'soft':None, 'duplicate':10}{'text':'text_a'}{'mask'}{'mask'}{'text':'text_b'}[0m
[32m[2022-08-26 18:17:12,740] [    INFO][0m - soft_encoder                  :None[0m
[32m[2022-08-26 18:17:12,741] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-26 18:17:12,741] [    INFO][0m - task_name                     :ocnli[0m
[32m[2022-08-26 18:17:12,741] [    INFO][0m - [0m
[32m[2022-08-26 18:17:12,741] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0826 18:17:12.742393 63889 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0826 18:17:12.747195 63889 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-26 18:17:15,585] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-08-26 18:17:15,610] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-08-26 18:17:15,610] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[32m[2022-08-26 18:17:15,613] [    INFO][0m - Using template: [{'add_prefix_space': '', 'soft': '[CLS]'}, {'add_prefix_space': '', 'soft': '[CLS]'}, {'add_prefix_space': '', 'soft': '[CLS]'}, {'add_prefix_space': '', 'soft': '[CLS]'}, {'add_prefix_space': '', 'soft': '[CLS]'}, {'add_prefix_space': '', 'soft': '[CLS]'}, {'add_prefix_space': '', 'soft': '[CLS]'}, {'add_prefix_space': '', 'soft': '[CLS]'}, {'add_prefix_space': '', 'soft': '[CLS]'}, {'add_prefix_space': '', 'soft': '[CLS]'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'text': 'text_b'}][0m
2022-08-26 18:17:15,619 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-26 18:17:15,744] [    INFO][0m - ============================================================[0m
[32m[2022-08-26 18:17:15,745] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-26 18:17:15,745] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-26 18:17:15,745] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-26 18:17:15,745] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-26 18:17:15,745] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-26 18:17:15,745] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-26 18:17:15,745] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-26 18:17:15,745] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-26 18:17:15,745] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-26 18:17:15,745] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-26 18:17:15,746] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-26 18:17:15,746] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-26 18:17:15,746] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-26 18:17:15,746] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-26 18:17:15,746] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-26 18:17:15,746] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-26 18:17:15,746] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-26 18:17:15,746] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-26 18:17:15,746] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-26 18:17:15,746] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-26 18:17:15,746] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-26 18:17:15,746] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-26 18:17:15,746] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-26 18:17:15,746] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-26 18:17:15,747] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-26 18:17:15,747] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-26 18:17:15,747] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-26 18:17:15,747] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-26 18:17:15,747] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-26 18:17:15,747] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-26 18:17:15,747] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-26 18:17:15,747] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-26 18:17:15,747] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-26 18:17:15,747] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-26 18:17:15,747] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-26 18:17:15,747] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-26 18:17:15,747] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug26_18-17-12_instance-3bwob41y-01[0m
[32m[2022-08-26 18:17:15,748] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-26 18:17:15,748] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-26 18:17:15,748] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-26 18:17:15,748] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-26 18:17:15,748] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-26 18:17:15,748] [    INFO][0m - max_seq_length                :64[0m
[32m[2022-08-26 18:17:15,748] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-26 18:17:15,748] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-26 18:17:15,748] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-26 18:17:15,748] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-26 18:17:15,748] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-08-26 18:17:15,748] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-26 18:17:15,748] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-26 18:17:15,749] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-26 18:17:15,749] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-26 18:17:15,749] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-26 18:17:15,749] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-26 18:17:15,749] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-26 18:17:15,749] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-26 18:17:15,749] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-26 18:17:15,749] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-26 18:17:15,749] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-26 18:17:15,749] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-26 18:17:15,749] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-26 18:17:15,749] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-26 18:17:15,749] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-26 18:17:15,749] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-26 18:17:15,750] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-26 18:17:15,750] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-26 18:17:15,750] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-26 18:17:15,750] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-26 18:17:15,750] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-26 18:17:15,750] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-26 18:17:15,750] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-26 18:17:15,750] [    INFO][0m - seed                          :42[0m
[32m[2022-08-26 18:17:15,750] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-26 18:17:15,750] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-26 18:17:15,750] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-26 18:17:15,750] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-26 18:17:15,750] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-26 18:17:15,751] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-26 18:17:15,751] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-26 18:17:15,751] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-26 18:17:15,751] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-26 18:17:15,751] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-26 18:17:15,751] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-26 18:17:15,751] [    INFO][0m - [0m
[32m[2022-08-26 18:17:15,753] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-26 18:17:15,753] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 18:17:15,753] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-08-26 18:17:15,753] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-26 18:17:15,753] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-26 18:17:15,753] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-26 18:17:15,753] [    INFO][0m -   Total optimization steps = 1000.0[0m
[32m[2022-08-26 18:17:15,754] [    INFO][0m -   Total num train samples = 8000[0m
[32m[2022-08-26 18:17:17,442] [    INFO][0m - loss: 1.29998932, learning_rate: 2.97e-05, global_step: 10, interval_runtime: 1.6874, interval_samples_per_second: 4.741, interval_steps_per_second: 5.926, epoch: 0.5[0m
[32m[2022-08-26 18:17:18,009] [    INFO][0m - loss: 1.31526461, learning_rate: 2.94e-05, global_step: 20, interval_runtime: 0.5668, interval_samples_per_second: 14.115, interval_steps_per_second: 17.644, epoch: 1.0[0m
[32m[2022-08-26 18:17:18,723] [    INFO][0m - loss: 1.12581453, learning_rate: 2.91e-05, global_step: 30, interval_runtime: 0.715, interval_samples_per_second: 11.189, interval_steps_per_second: 13.987, epoch: 1.5[0m
[32m[2022-08-26 18:17:19,293] [    INFO][0m - loss: 1.13216763, learning_rate: 2.88e-05, global_step: 40, interval_runtime: 0.5699, interval_samples_per_second: 14.038, interval_steps_per_second: 17.548, epoch: 2.0[0m
[32m[2022-08-26 18:17:20,054] [    INFO][0m - loss: 0.93335934, learning_rate: 2.8499999999999998e-05, global_step: 50, interval_runtime: 0.7602, interval_samples_per_second: 10.524, interval_steps_per_second: 13.155, epoch: 2.5[0m
[32m[2022-08-26 18:17:20,636] [    INFO][0m - loss: 0.97564001, learning_rate: 2.8199999999999998e-05, global_step: 60, interval_runtime: 0.5827, interval_samples_per_second: 13.73, interval_steps_per_second: 17.163, epoch: 3.0[0m
[32m[2022-08-26 18:17:21,386] [    INFO][0m - loss: 0.71137395, learning_rate: 2.79e-05, global_step: 70, interval_runtime: 0.7505, interval_samples_per_second: 10.659, interval_steps_per_second: 13.324, epoch: 3.5[0m
[32m[2022-08-26 18:17:21,978] [    INFO][0m - loss: 0.76020107, learning_rate: 2.7600000000000003e-05, global_step: 80, interval_runtime: 0.5913, interval_samples_per_second: 13.529, interval_steps_per_second: 16.911, epoch: 4.0[0m
[32m[2022-08-26 18:17:22,776] [    INFO][0m - loss: 0.53721213, learning_rate: 2.7300000000000003e-05, global_step: 90, interval_runtime: 0.7981, interval_samples_per_second: 10.023, interval_steps_per_second: 12.529, epoch: 4.5[0m
[32m[2022-08-26 18:17:23,404] [    INFO][0m - loss: 0.44867053, learning_rate: 2.7000000000000002e-05, global_step: 100, interval_runtime: 0.6278, interval_samples_per_second: 12.743, interval_steps_per_second: 15.928, epoch: 5.0[0m
[32m[2022-08-26 18:17:23,404] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 18:17:23,405] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 18:17:23,405] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 18:17:23,405] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 18:17:23,405] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 18:17:23,885] [    INFO][0m - eval_loss: 1.4446789026260376, eval_accuracy: 0.4875, eval_runtime: 0.4804, eval_samples_per_second: 333.029, eval_steps_per_second: 10.407, epoch: 5.0[0m
[32m[2022-08-26 18:17:23,886] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-26 18:17:23,886] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 18:17:27,021] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-26 18:17:27,022] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-26 18:17:31,906] [    INFO][0m - loss: 0.20769007, learning_rate: 2.6700000000000002e-05, global_step: 110, interval_runtime: 8.5014, interval_samples_per_second: 0.941, interval_steps_per_second: 1.176, epoch: 5.5[0m
[32m[2022-08-26 18:17:32,523] [    INFO][0m - loss: 0.22355325, learning_rate: 2.64e-05, global_step: 120, interval_runtime: 0.6178, interval_samples_per_second: 12.949, interval_steps_per_second: 16.186, epoch: 6.0[0m
[32m[2022-08-26 18:17:33,382] [    INFO][0m - loss: 0.15092212, learning_rate: 2.61e-05, global_step: 130, interval_runtime: 0.8586, interval_samples_per_second: 9.318, interval_steps_per_second: 11.647, epoch: 6.5[0m
[32m[2022-08-26 18:17:34,026] [    INFO][0m - loss: 0.12835487, learning_rate: 2.58e-05, global_step: 140, interval_runtime: 0.6439, interval_samples_per_second: 12.425, interval_steps_per_second: 15.531, epoch: 7.0[0m
[32m[2022-08-26 18:17:34,931] [    INFO][0m - loss: 0.1063904, learning_rate: 2.55e-05, global_step: 150, interval_runtime: 0.9059, interval_samples_per_second: 8.831, interval_steps_per_second: 11.039, epoch: 7.5[0m
[32m[2022-08-26 18:17:35,589] [    INFO][0m - loss: 0.1621747, learning_rate: 2.52e-05, global_step: 160, interval_runtime: 0.6576, interval_samples_per_second: 12.165, interval_steps_per_second: 15.207, epoch: 8.0[0m
[32m[2022-08-26 18:17:36,539] [    INFO][0m - loss: 0.15106688, learning_rate: 2.49e-05, global_step: 170, interval_runtime: 0.9502, interval_samples_per_second: 8.42, interval_steps_per_second: 10.525, epoch: 8.5[0m
[32m[2022-08-26 18:17:37,222] [    INFO][0m - loss: 0.01104432, learning_rate: 2.4599999999999998e-05, global_step: 180, interval_runtime: 0.6825, interval_samples_per_second: 11.721, interval_steps_per_second: 14.651, epoch: 9.0[0m
[32m[2022-08-26 18:17:38,212] [    INFO][0m - loss: 0.05758089, learning_rate: 2.43e-05, global_step: 190, interval_runtime: 0.9905, interval_samples_per_second: 8.077, interval_steps_per_second: 10.096, epoch: 9.5[0m
[32m[2022-08-26 18:17:38,872] [    INFO][0m - loss: 0.00162216, learning_rate: 2.4e-05, global_step: 200, interval_runtime: 0.6597, interval_samples_per_second: 12.126, interval_steps_per_second: 15.157, epoch: 10.0[0m
[32m[2022-08-26 18:17:38,872] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 18:17:38,873] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 18:17:38,873] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 18:17:38,873] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 18:17:38,873] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 18:17:39,471] [    INFO][0m - eval_loss: 3.6164588928222656, eval_accuracy: 0.45, eval_runtime: 0.5981, eval_samples_per_second: 267.514, eval_steps_per_second: 8.36, epoch: 10.0[0m
[32m[2022-08-26 18:17:39,472] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-26 18:17:39,472] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 18:17:42,654] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-26 18:17:42,655] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-26 18:17:47,667] [    INFO][0m - loss: 0.04199363, learning_rate: 2.37e-05, global_step: 210, interval_runtime: 8.7946, interval_samples_per_second: 0.91, interval_steps_per_second: 1.137, epoch: 10.5[0m
[32m[2022-08-26 18:17:48,342] [    INFO][0m - loss: 0.06205841, learning_rate: 2.3400000000000003e-05, global_step: 220, interval_runtime: 0.6751, interval_samples_per_second: 11.85, interval_steps_per_second: 14.813, epoch: 11.0[0m
[32m[2022-08-26 18:17:49,383] [    INFO][0m - loss: 0.00052599, learning_rate: 2.3100000000000002e-05, global_step: 230, interval_runtime: 1.0414, interval_samples_per_second: 7.682, interval_steps_per_second: 9.602, epoch: 11.5[0m
[32m[2022-08-26 18:17:50,089] [    INFO][0m - loss: 0.10757766, learning_rate: 2.2800000000000002e-05, global_step: 240, interval_runtime: 0.7048, interval_samples_per_second: 11.351, interval_steps_per_second: 14.189, epoch: 12.0[0m
[32m[2022-08-26 18:17:51,154] [    INFO][0m - loss: 0.13906498, learning_rate: 2.25e-05, global_step: 250, interval_runtime: 1.066, interval_samples_per_second: 7.505, interval_steps_per_second: 9.381, epoch: 12.5[0m
[32m[2022-08-26 18:17:51,859] [    INFO][0m - loss: 0.02456516, learning_rate: 2.22e-05, global_step: 260, interval_runtime: 0.7054, interval_samples_per_second: 11.341, interval_steps_per_second: 14.176, epoch: 13.0[0m
[32m[2022-08-26 18:17:53,032] [    INFO][0m - loss: 0.01741077, learning_rate: 2.19e-05, global_step: 270, interval_runtime: 1.1725, interval_samples_per_second: 6.823, interval_steps_per_second: 8.529, epoch: 13.5[0m
[32m[2022-08-26 18:17:53,793] [    INFO][0m - loss: 0.06335189, learning_rate: 2.16e-05, global_step: 280, interval_runtime: 0.761, interval_samples_per_second: 10.512, interval_steps_per_second: 13.14, epoch: 14.0[0m
[32m[2022-08-26 18:17:54,944] [    INFO][0m - loss: 0.00022131, learning_rate: 2.13e-05, global_step: 290, interval_runtime: 1.1513, interval_samples_per_second: 6.949, interval_steps_per_second: 8.686, epoch: 14.5[0m
[32m[2022-08-26 18:17:55,673] [    INFO][0m - loss: 0.00142951, learning_rate: 2.1e-05, global_step: 300, interval_runtime: 0.7283, interval_samples_per_second: 10.984, interval_steps_per_second: 13.731, epoch: 15.0[0m
[32m[2022-08-26 18:17:55,675] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 18:17:55,675] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 18:17:55,675] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 18:17:55,675] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 18:17:55,675] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 18:17:56,389] [    INFO][0m - eval_loss: 4.123386383056641, eval_accuracy: 0.4875, eval_runtime: 0.7136, eval_samples_per_second: 224.213, eval_steps_per_second: 7.007, epoch: 15.0[0m
[32m[2022-08-26 18:17:56,389] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-26 18:17:56,390] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 18:17:59,675] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-26 18:17:59,675] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-26 18:18:04,667] [    INFO][0m - loss: 0.01600487, learning_rate: 2.07e-05, global_step: 310, interval_runtime: 8.9943, interval_samples_per_second: 0.889, interval_steps_per_second: 1.112, epoch: 15.5[0m
[32m[2022-08-26 18:18:05,426] [    INFO][0m - loss: 0.00083342, learning_rate: 2.04e-05, global_step: 320, interval_runtime: 0.7591, interval_samples_per_second: 10.539, interval_steps_per_second: 13.174, epoch: 16.0[0m
[32m[2022-08-26 18:18:06,656] [    INFO][0m - loss: 0.02104001, learning_rate: 2.01e-05, global_step: 330, interval_runtime: 1.2299, interval_samples_per_second: 6.505, interval_steps_per_second: 8.131, epoch: 16.5[0m
[32m[2022-08-26 18:18:07,438] [    INFO][0m - loss: 0.02719266, learning_rate: 1.98e-05, global_step: 340, interval_runtime: 0.7818, interval_samples_per_second: 10.232, interval_steps_per_second: 12.79, epoch: 17.0[0m
[32m[2022-08-26 18:18:08,719] [    INFO][0m - loss: 0.00532867, learning_rate: 1.95e-05, global_step: 350, interval_runtime: 1.2816, interval_samples_per_second: 6.242, interval_steps_per_second: 7.803, epoch: 17.5[0m
[32m[2022-08-26 18:18:09,508] [    INFO][0m - loss: 0.01527335, learning_rate: 1.9200000000000003e-05, global_step: 360, interval_runtime: 0.7886, interval_samples_per_second: 10.145, interval_steps_per_second: 12.681, epoch: 18.0[0m
[32m[2022-08-26 18:18:10,817] [    INFO][0m - loss: 0.00169505, learning_rate: 1.8900000000000002e-05, global_step: 370, interval_runtime: 1.3097, interval_samples_per_second: 6.108, interval_steps_per_second: 7.636, epoch: 18.5[0m
[32m[2022-08-26 18:18:11,626] [    INFO][0m - loss: 0.06256388, learning_rate: 1.86e-05, global_step: 380, interval_runtime: 0.8078, interval_samples_per_second: 9.903, interval_steps_per_second: 12.379, epoch: 19.0[0m
[32m[2022-08-26 18:18:12,960] [    INFO][0m - loss: 0.00707919, learning_rate: 1.83e-05, global_step: 390, interval_runtime: 1.3347, interval_samples_per_second: 5.994, interval_steps_per_second: 7.492, epoch: 19.5[0m
[32m[2022-08-26 18:18:13,778] [    INFO][0m - loss: 0.02289955, learning_rate: 1.8e-05, global_step: 400, interval_runtime: 0.8182, interval_samples_per_second: 9.777, interval_steps_per_second: 12.222, epoch: 20.0[0m
[32m[2022-08-26 18:18:13,779] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 18:18:13,779] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 18:18:13,779] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 18:18:13,779] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 18:18:13,779] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 18:18:14,638] [    INFO][0m - eval_loss: 4.34714412689209, eval_accuracy: 0.4625, eval_runtime: 0.8583, eval_samples_per_second: 186.413, eval_steps_per_second: 5.825, epoch: 20.0[0m
[32m[2022-08-26 18:18:14,638] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-26 18:18:14,639] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 18:18:17,843] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-26 18:18:17,844] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-26 18:18:21,904] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-26 18:18:21,904] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-100 (score: 0.4875).[0m
[32m[2022-08-26 18:18:23,080] [    INFO][0m - train_runtime: 67.326, train_samples_per_second: 118.825, train_steps_per_second: 14.853, train_loss: 0.2769550685537979, epoch: 20.0[0m
[32m[2022-08-26 18:18:23,082] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-26 18:18:23,082] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 18:18:26,473] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-26 18:18:26,474] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-26 18:18:26,475] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-26 18:18:26,476] [    INFO][0m -   epoch                    =       20.0[0m
[32m[2022-08-26 18:18:26,476] [    INFO][0m -   train_loss               =      0.277[0m
[32m[2022-08-26 18:18:26,476] [    INFO][0m -   train_runtime            = 0:01:07.32[0m
[32m[2022-08-26 18:18:26,476] [    INFO][0m -   train_samples_per_second =    118.825[0m
[32m[2022-08-26 18:18:26,476] [    INFO][0m -   train_steps_per_second   =     14.853[0m
[32m[2022-08-26 18:18:26,479] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-26 18:18:26,479] [    INFO][0m -   Num examples = 2520[0m
[32m[2022-08-26 18:18:26,479] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 18:18:26,479] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 18:18:26,479] [    INFO][0m -   Total prediction steps = 79[0m
[32m[2022-08-26 18:18:40,732] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-26 18:18:40,733] [    INFO][0m -   test_accuracy           =     0.4556[0m
[32m[2022-08-26 18:18:40,733] [    INFO][0m -   test_loss               =      1.451[0m
[32m[2022-08-26 18:18:40,733] [    INFO][0m -   test_runtime            = 0:00:14.25[0m
[32m[2022-08-26 18:18:40,733] [    INFO][0m -   test_samples_per_second =    176.802[0m
[32m[2022-08-26 18:18:40,733] [    INFO][0m -   test_steps_per_second   =      5.543[0m
[32m[2022-08-26 18:18:40,733] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-26 18:18:40,734] [    INFO][0m -   Num examples = 3000[0m
[32m[2022-08-26 18:18:40,734] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 18:18:40,734] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 18:18:40,734] [    INFO][0m -   Total prediction steps = 94[0m
[32m[2022-08-26 18:19:02,462] [    INFO][0m - Predictions for ocnlif saved to ./fewclue_submit_examples.[0m
 
==========
bustm
==========
 
[33m[2022-08-26 18:19:06,329] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-26 18:19:06,329] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-26 18:19:06,329] [    INFO][0m - ============================================================[0m
[32m[2022-08-26 18:19:06,329] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-26 18:19:06,329] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-26 18:19:06,330] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-26 18:19:06,330] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-26 18:19:06,330] [    INFO][0m - early_stop_patience           :3[0m
[32m[2022-08-26 18:19:06,330] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-26 18:19:06,330] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-08-26 18:19:06,330] [    INFO][0m - [0m
[32m[2022-08-26 18:19:06,330] [    INFO][0m - ============================================================[0m
[32m[2022-08-26 18:19:06,330] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-26 18:19:06,330] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-26 18:19:06,330] [    INFO][0m - encoder_hidden_size           :None[0m
[32m[2022-08-26 18:19:06,330] [    INFO][0m - prompt                        :{'text':'text_a'}{'sep'}{'text':'text_b'}{'sep'}{'soft':'Ââç‰∏§Âè•ËØù'}{'mask'}{'soft':'ÂÉè'}[0m
[32m[2022-08-26 18:19:06,330] [    INFO][0m - soft_encoder                  :None[0m
[32m[2022-08-26 18:19:06,330] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-26 18:19:06,331] [    INFO][0m - task_name                     :bustm[0m
[32m[2022-08-26 18:19:06,331] [    INFO][0m - [0m
[32m[2022-08-26 18:19:06,331] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0826 18:19:06.332815 65841 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0826 18:19:06.336795 65841 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-26 18:19:09,167] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-08-26 18:19:09,193] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-08-26 18:19:09,193] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[32m[2022-08-26 18:19:09,200] [    INFO][0m - Using template: [{'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'sep': None}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'sep': None}, {'add_prefix_space': '', 'soft': 'Ââç'}, {'add_prefix_space': '', 'soft': '‰∏§'}, {'add_prefix_space': '', 'soft': 'Âè•'}, {'add_prefix_space': '', 'soft': 'ËØù'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'soft': 'ÂÉè'}][0m
2022-08-26 18:19:09,205 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-26 18:19:09,313] [    INFO][0m - ============================================================[0m
[32m[2022-08-26 18:19:09,313] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-26 18:19:09,313] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-26 18:19:09,314] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-26 18:19:09,314] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-26 18:19:09,314] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-26 18:19:09,314] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-26 18:19:09,314] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-26 18:19:09,314] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-26 18:19:09,314] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-26 18:19:09,314] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-26 18:19:09,314] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-26 18:19:09,314] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-26 18:19:09,315] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-26 18:19:09,315] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-26 18:19:09,315] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-26 18:19:09,315] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-26 18:19:09,315] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-26 18:19:09,315] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-26 18:19:09,315] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-26 18:19:09,315] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-26 18:19:09,315] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-26 18:19:09,315] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-26 18:19:09,315] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-26 18:19:09,315] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-26 18:19:09,315] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-26 18:19:09,316] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-26 18:19:09,316] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-26 18:19:09,316] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-26 18:19:09,316] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-26 18:19:09,316] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-26 18:19:09,316] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-26 18:19:09,316] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-26 18:19:09,316] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-26 18:19:09,316] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-26 18:19:09,316] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-26 18:19:09,316] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-26 18:19:09,316] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug26_18-19-06_instance-3bwob41y-01[0m
[32m[2022-08-26 18:19:09,316] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-26 18:19:09,317] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-26 18:19:09,317] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-26 18:19:09,317] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-26 18:19:09,317] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-26 18:19:09,317] [    INFO][0m - max_seq_length                :40[0m
[32m[2022-08-26 18:19:09,317] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-26 18:19:09,317] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-26 18:19:09,317] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-26 18:19:09,317] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-26 18:19:09,317] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-08-26 18:19:09,317] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-26 18:19:09,317] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-26 18:19:09,317] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-26 18:19:09,317] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-26 18:19:09,318] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-26 18:19:09,318] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-26 18:19:09,318] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-26 18:19:09,318] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-26 18:19:09,318] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-26 18:19:09,318] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-26 18:19:09,318] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-26 18:19:09,318] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-26 18:19:09,318] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-26 18:19:09,318] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-26 18:19:09,318] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-26 18:19:09,318] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-26 18:19:09,318] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-26 18:19:09,319] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-26 18:19:09,319] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-26 18:19:09,319] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-26 18:19:09,319] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-26 18:19:09,319] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-26 18:19:09,319] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-26 18:19:09,319] [    INFO][0m - seed                          :42[0m
[32m[2022-08-26 18:19:09,319] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-26 18:19:09,319] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-26 18:19:09,319] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-26 18:19:09,319] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-26 18:19:09,319] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-26 18:19:09,319] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-26 18:19:09,319] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-26 18:19:09,320] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-26 18:19:09,320] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-26 18:19:09,320] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-26 18:19:09,320] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-26 18:19:09,320] [    INFO][0m - [0m
[32m[2022-08-26 18:19:09,322] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-26 18:19:09,322] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 18:19:09,322] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-08-26 18:19:09,322] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-26 18:19:09,322] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-26 18:19:09,322] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-26 18:19:09,322] [    INFO][0m -   Total optimization steps = 1000.0[0m
[32m[2022-08-26 18:19:09,322] [    INFO][0m -   Total num train samples = 8000[0m
[32m[2022-08-26 18:19:10,950] [    INFO][0m - loss: 0.79168391, learning_rate: 2.97e-05, global_step: 10, interval_runtime: 1.6265, interval_samples_per_second: 4.918, interval_steps_per_second: 6.148, epoch: 0.5[0m
[32m[2022-08-26 18:19:11,467] [    INFO][0m - loss: 1.01042366, learning_rate: 2.94e-05, global_step: 20, interval_runtime: 0.5171, interval_samples_per_second: 15.471, interval_steps_per_second: 19.339, epoch: 1.0[0m
[32m[2022-08-26 18:19:12,133] [    INFO][0m - loss: 0.93001432, learning_rate: 2.91e-05, global_step: 30, interval_runtime: 0.6661, interval_samples_per_second: 12.01, interval_steps_per_second: 15.013, epoch: 1.5[0m
[32m[2022-08-26 18:19:12,706] [    INFO][0m - loss: 0.73753419, learning_rate: 2.88e-05, global_step: 40, interval_runtime: 0.5734, interval_samples_per_second: 13.953, interval_steps_per_second: 17.441, epoch: 2.0[0m
[32m[2022-08-26 18:19:13,420] [    INFO][0m - loss: 0.66699147, learning_rate: 2.8499999999999998e-05, global_step: 50, interval_runtime: 0.7137, interval_samples_per_second: 11.209, interval_steps_per_second: 14.011, epoch: 2.5[0m
[32m[2022-08-26 18:19:14,027] [    INFO][0m - loss: 0.67020264, learning_rate: 2.8199999999999998e-05, global_step: 60, interval_runtime: 0.6062, interval_samples_per_second: 13.197, interval_steps_per_second: 16.496, epoch: 3.0[0m
[32m[2022-08-26 18:19:14,724] [    INFO][0m - loss: 0.54022856, learning_rate: 2.79e-05, global_step: 70, interval_runtime: 0.6975, interval_samples_per_second: 11.47, interval_steps_per_second: 14.338, epoch: 3.5[0m
[32m[2022-08-26 18:19:15,296] [    INFO][0m - loss: 0.53570986, learning_rate: 2.7600000000000003e-05, global_step: 80, interval_runtime: 0.5725, interval_samples_per_second: 13.974, interval_steps_per_second: 17.467, epoch: 4.0[0m
[32m[2022-08-26 18:19:16,027] [    INFO][0m - loss: 0.43013873, learning_rate: 2.7300000000000003e-05, global_step: 90, interval_runtime: 0.7302, interval_samples_per_second: 10.956, interval_steps_per_second: 13.696, epoch: 4.5[0m
[32m[2022-08-26 18:19:16,643] [    INFO][0m - loss: 0.26067033, learning_rate: 2.7000000000000002e-05, global_step: 100, interval_runtime: 0.6165, interval_samples_per_second: 12.976, interval_steps_per_second: 16.22, epoch: 5.0[0m
[32m[2022-08-26 18:19:16,643] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 18:19:16,643] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 18:19:16,644] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 18:19:16,644] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 18:19:16,644] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 18:19:16,984] [    INFO][0m - eval_loss: 1.0319362878799438, eval_accuracy: 0.6375, eval_runtime: 0.3398, eval_samples_per_second: 470.903, eval_steps_per_second: 14.716, epoch: 5.0[0m
[32m[2022-08-26 18:19:16,984] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-26 18:19:16,984] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 18:19:20,628] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-26 18:19:20,629] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-26 18:19:25,205] [    INFO][0m - loss: 0.30641937, learning_rate: 2.6700000000000002e-05, global_step: 110, interval_runtime: 8.5617, interval_samples_per_second: 0.934, interval_steps_per_second: 1.168, epoch: 5.5[0m
[32m[2022-08-26 18:19:25,828] [    INFO][0m - loss: 0.14865046, learning_rate: 2.64e-05, global_step: 120, interval_runtime: 0.6232, interval_samples_per_second: 12.838, interval_steps_per_second: 16.047, epoch: 6.0[0m
[32m[2022-08-26 18:19:26,638] [    INFO][0m - loss: 0.05081288, learning_rate: 2.61e-05, global_step: 130, interval_runtime: 0.8107, interval_samples_per_second: 9.868, interval_steps_per_second: 12.335, epoch: 6.5[0m
[32m[2022-08-26 18:19:27,261] [    INFO][0m - loss: 0.03809426, learning_rate: 2.58e-05, global_step: 140, interval_runtime: 0.6223, interval_samples_per_second: 12.855, interval_steps_per_second: 16.068, epoch: 7.0[0m
[32m[2022-08-26 18:19:28,111] [    INFO][0m - loss: 0.03784231, learning_rate: 2.55e-05, global_step: 150, interval_runtime: 0.8503, interval_samples_per_second: 9.408, interval_steps_per_second: 11.76, epoch: 7.5[0m
[32m[2022-08-26 18:19:28,749] [    INFO][0m - loss: 0.13355994, learning_rate: 2.52e-05, global_step: 160, interval_runtime: 0.6374, interval_samples_per_second: 12.55, interval_steps_per_second: 15.688, epoch: 8.0[0m
[32m[2022-08-26 18:19:29,619] [    INFO][0m - loss: 0.06194946, learning_rate: 2.49e-05, global_step: 170, interval_runtime: 0.8705, interval_samples_per_second: 9.19, interval_steps_per_second: 11.488, epoch: 8.5[0m
[32m[2022-08-26 18:19:30,259] [    INFO][0m - loss: 0.00354429, learning_rate: 2.4599999999999998e-05, global_step: 180, interval_runtime: 0.64, interval_samples_per_second: 12.5, interval_steps_per_second: 15.625, epoch: 9.0[0m
[32m[2022-08-26 18:19:31,119] [    INFO][0m - loss: 0.00074576, learning_rate: 2.43e-05, global_step: 190, interval_runtime: 0.8601, interval_samples_per_second: 9.301, interval_steps_per_second: 11.626, epoch: 9.5[0m
[32m[2022-08-26 18:19:31,740] [    INFO][0m - loss: 0.00364271, learning_rate: 2.4e-05, global_step: 200, interval_runtime: 0.6206, interval_samples_per_second: 12.89, interval_steps_per_second: 16.113, epoch: 10.0[0m
[32m[2022-08-26 18:19:31,740] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 18:19:31,740] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 18:19:31,740] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 18:19:31,741] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 18:19:31,741] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 18:19:32,162] [    INFO][0m - eval_loss: 3.5292086601257324, eval_accuracy: 0.625, eval_runtime: 0.421, eval_samples_per_second: 380.009, eval_steps_per_second: 11.875, epoch: 10.0[0m
[32m[2022-08-26 18:19:32,162] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-26 18:19:32,162] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 18:19:35,209] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-26 18:19:35,210] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-26 18:19:40,140] [    INFO][0m - loss: 2.112e-05, learning_rate: 2.37e-05, global_step: 210, interval_runtime: 8.4, interval_samples_per_second: 0.952, interval_steps_per_second: 1.19, epoch: 10.5[0m
[32m[2022-08-26 18:19:40,771] [    INFO][0m - loss: 0.00013189, learning_rate: 2.3400000000000003e-05, global_step: 220, interval_runtime: 0.6312, interval_samples_per_second: 12.675, interval_steps_per_second: 15.844, epoch: 11.0[0m
[32m[2022-08-26 18:19:41,721] [    INFO][0m - loss: 0.00120122, learning_rate: 2.3100000000000002e-05, global_step: 230, interval_runtime: 0.9499, interval_samples_per_second: 8.422, interval_steps_per_second: 10.527, epoch: 11.5[0m
[32m[2022-08-26 18:19:42,363] [    INFO][0m - loss: 0.00042963, learning_rate: 2.2800000000000002e-05, global_step: 240, interval_runtime: 0.6423, interval_samples_per_second: 12.455, interval_steps_per_second: 15.569, epoch: 12.0[0m
[32m[2022-08-26 18:19:43,315] [    INFO][0m - loss: 0.00085386, learning_rate: 2.25e-05, global_step: 250, interval_runtime: 0.9521, interval_samples_per_second: 8.403, interval_steps_per_second: 10.504, epoch: 12.5[0m
[32m[2022-08-26 18:19:43,982] [    INFO][0m - loss: 3.899e-05, learning_rate: 2.22e-05, global_step: 260, interval_runtime: 0.6664, interval_samples_per_second: 12.004, interval_steps_per_second: 15.005, epoch: 13.0[0m
[32m[2022-08-26 18:19:44,954] [    INFO][0m - loss: 0.00732403, learning_rate: 2.19e-05, global_step: 270, interval_runtime: 0.9722, interval_samples_per_second: 8.229, interval_steps_per_second: 10.286, epoch: 13.5[0m
[32m[2022-08-26 18:19:45,607] [    INFO][0m - loss: 0.04401445, learning_rate: 2.16e-05, global_step: 280, interval_runtime: 0.6529, interval_samples_per_second: 12.254, interval_steps_per_second: 15.317, epoch: 14.0[0m
[32m[2022-08-26 18:19:46,627] [    INFO][0m - loss: 6.467e-05, learning_rate: 2.13e-05, global_step: 290, interval_runtime: 1.0201, interval_samples_per_second: 7.842, interval_steps_per_second: 9.803, epoch: 14.5[0m
[32m[2022-08-26 18:19:47,310] [    INFO][0m - loss: 6.639e-05, learning_rate: 2.1e-05, global_step: 300, interval_runtime: 0.6828, interval_samples_per_second: 11.717, interval_steps_per_second: 14.647, epoch: 15.0[0m
[32m[2022-08-26 18:19:47,310] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 18:19:47,310] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 18:19:47,310] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 18:19:47,310] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 18:19:47,310] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 18:19:47,826] [    INFO][0m - eval_loss: 4.064999580383301, eval_accuracy: 0.625, eval_runtime: 0.5158, eval_samples_per_second: 310.205, eval_steps_per_second: 9.694, epoch: 15.0[0m
[32m[2022-08-26 18:19:47,827] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-26 18:19:47,827] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 18:19:51,390] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-26 18:19:51,391] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-26 18:19:56,384] [    INFO][0m - loss: 1.44e-06, learning_rate: 2.07e-05, global_step: 310, interval_runtime: 9.0741, interval_samples_per_second: 0.882, interval_steps_per_second: 1.102, epoch: 15.5[0m
[32m[2022-08-26 18:19:57,276] [    INFO][0m - loss: 4.28e-06, learning_rate: 2.04e-05, global_step: 320, interval_runtime: 0.8919, interval_samples_per_second: 8.969, interval_steps_per_second: 11.211, epoch: 16.0[0m
[32m[2022-08-26 18:19:58,559] [    INFO][0m - loss: 0.0004068, learning_rate: 2.01e-05, global_step: 330, interval_runtime: 1.2831, interval_samples_per_second: 6.235, interval_steps_per_second: 7.793, epoch: 16.5[0m
[32m[2022-08-26 18:19:59,472] [    INFO][0m - loss: 0.00011036, learning_rate: 1.98e-05, global_step: 340, interval_runtime: 0.912, interval_samples_per_second: 8.772, interval_steps_per_second: 10.965, epoch: 17.0[0m
[32m[2022-08-26 18:20:00,785] [    INFO][0m - loss: 2.3e-07, learning_rate: 1.95e-05, global_step: 350, interval_runtime: 1.3142, interval_samples_per_second: 6.088, interval_steps_per_second: 7.609, epoch: 17.5[0m
[32m[2022-08-26 18:20:01,690] [    INFO][0m - loss: 0.00025299, learning_rate: 1.9200000000000003e-05, global_step: 360, interval_runtime: 0.9053, interval_samples_per_second: 8.837, interval_steps_per_second: 11.046, epoch: 18.0[0m
[32m[2022-08-26 18:20:03,025] [    INFO][0m - loss: 0.06354101, learning_rate: 1.8900000000000002e-05, global_step: 370, interval_runtime: 1.334, interval_samples_per_second: 5.997, interval_steps_per_second: 7.496, epoch: 18.5[0m
[32m[2022-08-26 18:20:03,769] [    INFO][0m - loss: 1.89e-06, learning_rate: 1.86e-05, global_step: 380, interval_runtime: 0.7449, interval_samples_per_second: 10.74, interval_steps_per_second: 13.425, epoch: 19.0[0m
[32m[2022-08-26 18:20:04,929] [    INFO][0m - loss: 0.0127565, learning_rate: 1.83e-05, global_step: 390, interval_runtime: 1.1599, interval_samples_per_second: 6.897, interval_steps_per_second: 8.621, epoch: 19.5[0m
[32m[2022-08-26 18:20:05,664] [    INFO][0m - loss: 5.08e-06, learning_rate: 1.8e-05, global_step: 400, interval_runtime: 0.7351, interval_samples_per_second: 10.883, interval_steps_per_second: 13.604, epoch: 20.0[0m
[32m[2022-08-26 18:20:05,665] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 18:20:05,665] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 18:20:05,665] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 18:20:05,665] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 18:20:05,665] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 18:20:06,258] [    INFO][0m - eval_loss: 4.439856052398682, eval_accuracy: 0.6, eval_runtime: 0.593, eval_samples_per_second: 269.821, eval_steps_per_second: 8.432, epoch: 20.0[0m
[32m[2022-08-26 18:20:06,258] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-26 18:20:06,259] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 18:20:09,889] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-26 18:20:09,890] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-26 18:20:13,625] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-26 18:20:13,626] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-100 (score: 0.6375).[0m
[32m[2022-08-26 18:20:14,721] [    INFO][0m - train_runtime: 65.3976, train_samples_per_second: 122.329, train_steps_per_second: 15.291, train_loss: 0.18725214858732273, epoch: 20.0[0m
[32m[2022-08-26 18:20:14,722] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-26 18:20:14,722] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 18:20:17,953] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-26 18:20:17,954] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-26 18:20:17,956] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-26 18:20:17,956] [    INFO][0m -   epoch                    =       20.0[0m
[32m[2022-08-26 18:20:17,956] [    INFO][0m -   train_loss               =     0.1873[0m
[32m[2022-08-26 18:20:17,956] [    INFO][0m -   train_runtime            = 0:01:05.39[0m
[32m[2022-08-26 18:20:17,956] [    INFO][0m -   train_samples_per_second =    122.329[0m
[32m[2022-08-26 18:20:17,957] [    INFO][0m -   train_steps_per_second   =     15.291[0m
[32m[2022-08-26 18:20:17,960] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-26 18:20:17,960] [    INFO][0m -   Num examples = 1772[0m
[32m[2022-08-26 18:20:17,960] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 18:20:17,960] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 18:20:17,960] [    INFO][0m -   Total prediction steps = 56[0m
[32m[2022-08-26 18:20:24,963] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-26 18:20:24,964] [    INFO][0m -   test_accuracy           =     0.6574[0m
[32m[2022-08-26 18:20:24,964] [    INFO][0m -   test_loss               =     0.9885[0m
[32m[2022-08-26 18:20:24,964] [    INFO][0m -   test_runtime            = 0:00:07.00[0m
[32m[2022-08-26 18:20:24,964] [    INFO][0m -   test_samples_per_second =     253.02[0m
[32m[2022-08-26 18:20:24,964] [    INFO][0m -   test_steps_per_second   =      7.996[0m
[32m[2022-08-26 18:20:24,964] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-26 18:20:24,964] [    INFO][0m -   Num examples = 2000[0m
[32m[2022-08-26 18:20:24,965] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 18:20:24,965] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 18:20:24,965] [    INFO][0m -   Total prediction steps = 63[0m
[32m[2022-08-26 18:20:34,955] [    INFO][0m - Predictions for bustm saved to ./fewclue_submit_examples.[0m
 
==========
chid
==========
 
[33m[2022-08-26 18:20:38,879] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-26 18:20:38,880] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-26 18:20:38,880] [    INFO][0m - ============================================================[0m
[32m[2022-08-26 18:20:38,880] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-26 18:20:38,880] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-26 18:20:38,880] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-26 18:20:38,880] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-26 18:20:38,880] [    INFO][0m - early_stop_patience           :3[0m
[32m[2022-08-26 18:20:38,880] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-26 18:20:38,881] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-08-26 18:20:38,881] [    INFO][0m - [0m
[32m[2022-08-26 18:20:38,881] [    INFO][0m - ============================================================[0m
[32m[2022-08-26 18:20:38,881] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-26 18:20:38,881] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-26 18:20:38,881] [    INFO][0m - encoder_hidden_size           :None[0m
[32m[2022-08-26 18:20:38,881] [    INFO][0m - prompt                        :{'text':'text_a'}{'sep'}{'soft':'ËøôÂè•ËØù'}{'mask'}{'soft':'ÈÄöÈ°∫„ÄÇ'}[0m
[32m[2022-08-26 18:20:38,881] [    INFO][0m - soft_encoder                  :None[0m
[32m[2022-08-26 18:20:38,881] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-26 18:20:38,881] [    INFO][0m - task_name                     :chid[0m
[32m[2022-08-26 18:20:38,881] [    INFO][0m - [0m
[32m[2022-08-26 18:20:38,881] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0826 18:20:38.882987 67512 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0826 18:20:38.888054 67512 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-26 18:20:41,558] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-08-26 18:20:41,582] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-08-26 18:20:41,583] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[32m[2022-08-26 18:20:41,590] [    INFO][0m - Using template: [{'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'sep': None}, {'add_prefix_space': '', 'soft': 'Ëøô'}, {'add_prefix_space': '', 'soft': 'Âè•'}, {'add_prefix_space': '', 'soft': 'ËØù'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'soft': 'ÈÄö'}, {'add_prefix_space': '', 'soft': 'È°∫'}, {'add_prefix_space': '', 'soft': '„ÄÇ'}][0m
2022-08-26 18:20:41,595 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-26 18:20:41,905] [    INFO][0m - ============================================================[0m
[32m[2022-08-26 18:20:41,905] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-26 18:20:41,905] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-26 18:20:41,905] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-26 18:20:41,905] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-26 18:20:41,905] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-26 18:20:41,906] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-26 18:20:41,906] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-26 18:20:41,906] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-26 18:20:41,906] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-26 18:20:41,906] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-26 18:20:41,906] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-26 18:20:41,906] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-26 18:20:41,906] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-26 18:20:41,906] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-26 18:20:41,906] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-26 18:20:41,906] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-26 18:20:41,906] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-26 18:20:41,906] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-26 18:20:41,907] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-26 18:20:41,907] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-26 18:20:41,907] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-26 18:20:41,907] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-26 18:20:41,907] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-26 18:20:41,907] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-26 18:20:41,907] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-26 18:20:41,907] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-26 18:20:41,907] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-26 18:20:41,907] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-26 18:20:41,907] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-26 18:20:41,907] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-26 18:20:41,907] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-26 18:20:41,907] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-26 18:20:41,908] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-26 18:20:41,908] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-26 18:20:41,908] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-26 18:20:41,908] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-26 18:20:41,908] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug26_18-20-38_instance-3bwob41y-01[0m
[32m[2022-08-26 18:20:41,908] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-26 18:20:41,908] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-26 18:20:41,908] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-26 18:20:41,908] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-26 18:20:41,908] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-26 18:20:41,908] [    INFO][0m - max_seq_length                :256[0m
[32m[2022-08-26 18:20:41,908] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-26 18:20:41,908] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-26 18:20:41,909] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-26 18:20:41,909] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-26 18:20:41,909] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-08-26 18:20:41,909] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-26 18:20:41,909] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-26 18:20:41,909] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-26 18:20:41,909] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-26 18:20:41,909] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-26 18:20:41,909] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-26 18:20:41,909] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-26 18:20:41,909] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-26 18:20:41,909] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-26 18:20:41,909] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-26 18:20:41,909] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-26 18:20:41,910] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-26 18:20:41,910] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-26 18:20:41,910] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-26 18:20:41,910] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-26 18:20:41,910] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-26 18:20:41,910] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-26 18:20:41,910] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-26 18:20:41,910] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-26 18:20:41,910] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-26 18:20:41,910] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-26 18:20:41,910] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-26 18:20:41,910] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-26 18:20:41,910] [    INFO][0m - seed                          :42[0m
[32m[2022-08-26 18:20:41,911] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-26 18:20:41,911] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-26 18:20:41,911] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-26 18:20:41,911] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-26 18:20:41,911] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-26 18:20:41,911] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-26 18:20:41,911] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-26 18:20:41,911] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-26 18:20:41,911] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-26 18:20:41,911] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-26 18:20:41,911] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-26 18:20:41,911] [    INFO][0m - [0m
[32m[2022-08-26 18:20:41,913] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-26 18:20:41,913] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-08-26 18:20:41,913] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-08-26 18:20:41,913] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-26 18:20:41,913] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-26 18:20:41,913] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-26 18:20:41,913] [    INFO][0m -   Total optimization steps = 8850.0[0m
[32m[2022-08-26 18:20:41,914] [    INFO][0m -   Total num train samples = 70700[0m
[32m[2022-08-26 18:20:44,095] [    INFO][0m - loss: 1.11857176, learning_rate: 2.9966101694915256e-05, global_step: 10, interval_runtime: 2.1809, interval_samples_per_second: 3.668, interval_steps_per_second: 4.585, epoch: 0.0565[0m
[32m[2022-08-26 18:20:45,320] [    INFO][0m - loss: 0.78306437, learning_rate: 2.9932203389830508e-05, global_step: 20, interval_runtime: 1.2253, interval_samples_per_second: 6.529, interval_steps_per_second: 8.161, epoch: 0.113[0m
[32m[2022-08-26 18:20:46,569] [    INFO][0m - loss: 0.57624502, learning_rate: 2.9898305084745767e-05, global_step: 30, interval_runtime: 1.2481, interval_samples_per_second: 6.41, interval_steps_per_second: 8.012, epoch: 0.1695[0m
[32m[2022-08-26 18:20:47,841] [    INFO][0m - loss: 0.48567057, learning_rate: 2.986440677966102e-05, global_step: 40, interval_runtime: 1.2722, interval_samples_per_second: 6.288, interval_steps_per_second: 7.86, epoch: 0.226[0m
[32m[2022-08-26 18:20:49,133] [    INFO][0m - loss: 0.37608225, learning_rate: 2.9830508474576274e-05, global_step: 50, interval_runtime: 1.2917, interval_samples_per_second: 6.193, interval_steps_per_second: 7.742, epoch: 0.2825[0m
[32m[2022-08-26 18:20:50,449] [    INFO][0m - loss: 0.26287246, learning_rate: 2.9796610169491526e-05, global_step: 60, interval_runtime: 1.3159, interval_samples_per_second: 6.079, interval_steps_per_second: 7.599, epoch: 0.339[0m
[32m[2022-08-26 18:20:51,787] [    INFO][0m - loss: 0.775914, learning_rate: 2.976271186440678e-05, global_step: 70, interval_runtime: 1.3382, interval_samples_per_second: 5.978, interval_steps_per_second: 7.473, epoch: 0.3955[0m
[32m[2022-08-26 18:20:53,146] [    INFO][0m - loss: 0.52941651, learning_rate: 2.9728813559322033e-05, global_step: 80, interval_runtime: 1.3588, interval_samples_per_second: 5.887, interval_steps_per_second: 7.359, epoch: 0.452[0m
[32m[2022-08-26 18:20:54,528] [    INFO][0m - loss: 0.5041616, learning_rate: 2.9694915254237292e-05, global_step: 90, interval_runtime: 1.3822, interval_samples_per_second: 5.788, interval_steps_per_second: 7.235, epoch: 0.5085[0m
[32m[2022-08-26 18:20:55,928] [    INFO][0m - loss: 0.46398363, learning_rate: 2.9661016949152544e-05, global_step: 100, interval_runtime: 1.4, interval_samples_per_second: 5.714, interval_steps_per_second: 7.143, epoch: 0.565[0m
[32m[2022-08-26 18:20:55,928] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 18:20:55,929] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-08-26 18:20:55,929] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 18:20:55,929] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 18:20:55,929] [    INFO][0m -   Total prediction steps = 45[0m
[32m[2022-08-26 18:21:07,685] [    INFO][0m - eval_loss: 0.4134361743927002, eval_accuracy: 0.019801980198019802, eval_runtime: 11.7558, eval_samples_per_second: 120.281, eval_steps_per_second: 3.828, epoch: 0.565[0m
[32m[2022-08-26 18:21:07,686] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-26 18:21:07,686] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 18:21:11,111] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-26 18:21:11,112] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-26 18:21:16,375] [    INFO][0m - loss: 0.45475535, learning_rate: 2.96271186440678e-05, global_step: 110, interval_runtime: 20.4474, interval_samples_per_second: 0.391, interval_steps_per_second: 0.489, epoch: 0.6215[0m
[32m[2022-08-26 18:21:17,914] [    INFO][0m - loss: 0.37522395, learning_rate: 2.959322033898305e-05, global_step: 120, interval_runtime: 1.5389, interval_samples_per_second: 5.198, interval_steps_per_second: 6.498, epoch: 0.678[0m
[32m[2022-08-26 18:21:19,469] [    INFO][0m - loss: 0.44912329, learning_rate: 2.9559322033898306e-05, global_step: 130, interval_runtime: 1.5554, interval_samples_per_second: 5.143, interval_steps_per_second: 6.429, epoch: 0.7345[0m
[32m[2022-08-26 18:21:21,063] [    INFO][0m - loss: 0.43377705, learning_rate: 2.9525423728813558e-05, global_step: 140, interval_runtime: 1.5937, interval_samples_per_second: 5.02, interval_steps_per_second: 6.275, epoch: 0.791[0m
[32m[2022-08-26 18:21:22,662] [    INFO][0m - loss: 0.51176767, learning_rate: 2.9491525423728817e-05, global_step: 150, interval_runtime: 1.5985, interval_samples_per_second: 5.005, interval_steps_per_second: 6.256, epoch: 0.8475[0m
[32m[2022-08-26 18:21:24,284] [    INFO][0m - loss: 0.4493825, learning_rate: 2.945762711864407e-05, global_step: 160, interval_runtime: 1.622, interval_samples_per_second: 4.932, interval_steps_per_second: 6.165, epoch: 0.904[0m
[32m[2022-08-26 18:21:25,924] [    INFO][0m - loss: 0.37419114, learning_rate: 2.9423728813559324e-05, global_step: 170, interval_runtime: 1.6397, interval_samples_per_second: 4.879, interval_steps_per_second: 6.099, epoch: 0.9605[0m
[32m[2022-08-26 18:21:27,600] [    INFO][0m - loss: 0.41871314, learning_rate: 2.9389830508474576e-05, global_step: 180, interval_runtime: 1.6769, interval_samples_per_second: 4.771, interval_steps_per_second: 5.963, epoch: 1.0169[0m
[32m[2022-08-26 18:21:29,280] [    INFO][0m - loss: 0.61924734, learning_rate: 2.935593220338983e-05, global_step: 190, interval_runtime: 1.6799, interval_samples_per_second: 4.762, interval_steps_per_second: 5.953, epoch: 1.0734[0m
[32m[2022-08-26 18:21:30,986] [    INFO][0m - loss: 0.44134903, learning_rate: 2.9322033898305087e-05, global_step: 200, interval_runtime: 1.7053, interval_samples_per_second: 4.691, interval_steps_per_second: 5.864, epoch: 1.1299[0m
[32m[2022-08-26 18:21:30,986] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 18:21:30,987] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-08-26 18:21:30,987] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 18:21:30,987] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 18:21:30,987] [    INFO][0m -   Total prediction steps = 45[0m
[32m[2022-08-26 18:21:46,773] [    INFO][0m - eval_loss: 0.4119147062301636, eval_accuracy: 0.1485148514851485, eval_runtime: 15.7862, eval_samples_per_second: 89.572, eval_steps_per_second: 2.851, epoch: 1.1299[0m
[32m[2022-08-26 18:21:46,774] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-26 18:21:46,774] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 18:21:49,791] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-26 18:21:49,791] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-26 18:21:55,255] [    INFO][0m - loss: 0.37050085, learning_rate: 2.9288135593220342e-05, global_step: 210, interval_runtime: 24.269, interval_samples_per_second: 0.33, interval_steps_per_second: 0.412, epoch: 1.1864[0m
[32m[2022-08-26 18:21:57,090] [    INFO][0m - loss: 0.54862599, learning_rate: 2.9254237288135594e-05, global_step: 220, interval_runtime: 1.8348, interval_samples_per_second: 4.36, interval_steps_per_second: 5.45, epoch: 1.2429[0m
[32m[2022-08-26 18:21:58,957] [    INFO][0m - loss: 0.4593327, learning_rate: 2.922033898305085e-05, global_step: 230, interval_runtime: 1.8674, interval_samples_per_second: 4.284, interval_steps_per_second: 5.355, epoch: 1.2994[0m
[32m[2022-08-26 18:22:00,835] [    INFO][0m - loss: 0.47892227, learning_rate: 2.91864406779661e-05, global_step: 240, interval_runtime: 1.8784, interval_samples_per_second: 4.259, interval_steps_per_second: 5.324, epoch: 1.3559[0m
[32m[2022-08-26 18:22:02,732] [    INFO][0m - loss: 0.31280088, learning_rate: 2.9152542372881356e-05, global_step: 250, interval_runtime: 1.8967, interval_samples_per_second: 4.218, interval_steps_per_second: 5.272, epoch: 1.4124[0m
[32m[2022-08-26 18:22:04,653] [    INFO][0m - loss: 0.48814344, learning_rate: 2.911864406779661e-05, global_step: 260, interval_runtime: 1.9206, interval_samples_per_second: 4.165, interval_steps_per_second: 5.207, epoch: 1.4689[0m
[32m[2022-08-26 18:22:06,592] [    INFO][0m - loss: 0.40154166, learning_rate: 2.9084745762711867e-05, global_step: 270, interval_runtime: 1.9392, interval_samples_per_second: 4.125, interval_steps_per_second: 5.157, epoch: 1.5254[0m
[32m[2022-08-26 18:22:08,550] [    INFO][0m - loss: 0.52075524, learning_rate: 2.905084745762712e-05, global_step: 280, interval_runtime: 1.9585, interval_samples_per_second: 4.085, interval_steps_per_second: 5.106, epoch: 1.5819[0m
[32m[2022-08-26 18:22:10,533] [    INFO][0m - loss: 0.35824821, learning_rate: 2.9016949152542374e-05, global_step: 290, interval_runtime: 1.9826, interval_samples_per_second: 4.035, interval_steps_per_second: 5.044, epoch: 1.6384[0m
[32m[2022-08-26 18:22:12,535] [    INFO][0m - loss: 0.589429, learning_rate: 2.8983050847457626e-05, global_step: 300, interval_runtime: 2.0018, interval_samples_per_second: 3.996, interval_steps_per_second: 4.996, epoch: 1.6949[0m
[32m[2022-08-26 18:22:12,535] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 18:22:12,535] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-08-26 18:22:12,536] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 18:22:12,536] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 18:22:12,536] [    INFO][0m -   Total prediction steps = 45[0m
[32m[2022-08-26 18:22:32,278] [    INFO][0m - eval_loss: 0.4123767912387848, eval_accuracy: 0.12376237623762376, eval_runtime: 19.7416, eval_samples_per_second: 71.625, eval_steps_per_second: 2.279, epoch: 1.6949[0m
[32m[2022-08-26 18:22:32,278] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-26 18:22:32,278] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 18:22:35,361] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-26 18:22:35,361] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-26 18:22:41,243] [    INFO][0m - loss: 0.38006401, learning_rate: 2.894915254237288e-05, global_step: 310, interval_runtime: 28.7087, interval_samples_per_second: 0.279, interval_steps_per_second: 0.348, epoch: 1.7514[0m
[32m[2022-08-26 18:22:43,374] [    INFO][0m - loss: 0.39649448, learning_rate: 2.8915254237288137e-05, global_step: 320, interval_runtime: 2.1305, interval_samples_per_second: 3.755, interval_steps_per_second: 4.694, epoch: 1.8079[0m
[32m[2022-08-26 18:22:45,525] [    INFO][0m - loss: 0.65850778, learning_rate: 2.8881355932203392e-05, global_step: 330, interval_runtime: 2.1508, interval_samples_per_second: 3.72, interval_steps_per_second: 4.65, epoch: 1.8644[0m
[32m[2022-08-26 18:22:47,699] [    INFO][0m - loss: 0.34744396, learning_rate: 2.8847457627118644e-05, global_step: 340, interval_runtime: 2.1742, interval_samples_per_second: 3.68, interval_steps_per_second: 4.599, epoch: 1.9209[0m
[32m[2022-08-26 18:22:49,775] [    INFO][0m - loss: 0.37938595, learning_rate: 2.88135593220339e-05, global_step: 350, interval_runtime: 2.0764, interval_samples_per_second: 3.853, interval_steps_per_second: 4.816, epoch: 1.9774[0m
[32m[2022-08-26 18:22:52,127] [    INFO][0m - loss: 0.38991711, learning_rate: 2.877966101694915e-05, global_step: 360, interval_runtime: 2.3514, interval_samples_per_second: 3.402, interval_steps_per_second: 4.253, epoch: 2.0339[0m
[32m[2022-08-26 18:22:54,438] [    INFO][0m - loss: 0.37183189, learning_rate: 2.874576271186441e-05, global_step: 370, interval_runtime: 2.3113, interval_samples_per_second: 3.461, interval_steps_per_second: 4.327, epoch: 2.0904[0m
[32m[2022-08-26 18:22:56,719] [    INFO][0m - loss: 0.55339174, learning_rate: 2.8711864406779662e-05, global_step: 380, interval_runtime: 2.2808, interval_samples_per_second: 3.508, interval_steps_per_second: 4.384, epoch: 2.1469[0m
[32m[2022-08-26 18:22:59,010] [    INFO][0m - loss: 0.47114038, learning_rate: 2.8677966101694917e-05, global_step: 390, interval_runtime: 2.2912, interval_samples_per_second: 3.492, interval_steps_per_second: 4.365, epoch: 2.2034[0m
[32m[2022-08-26 18:23:01,323] [    INFO][0m - loss: 0.45687904, learning_rate: 2.864406779661017e-05, global_step: 400, interval_runtime: 2.3123, interval_samples_per_second: 3.46, interval_steps_per_second: 4.325, epoch: 2.2599[0m
[32m[2022-08-26 18:23:01,323] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 18:23:01,323] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-08-26 18:23:01,323] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 18:23:01,324] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 18:23:01,324] [    INFO][0m -   Total prediction steps = 45[0m
[32m[2022-08-26 18:23:25,098] [    INFO][0m - eval_loss: 0.4525492787361145, eval_accuracy: 0.09900990099009901, eval_runtime: 23.7739, eval_samples_per_second: 59.477, eval_steps_per_second: 1.893, epoch: 2.2599[0m
[32m[2022-08-26 18:23:25,099] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-26 18:23:25,099] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 18:23:28,271] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-26 18:23:28,272] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-26 18:23:34,443] [    INFO][0m - loss: 0.54944372, learning_rate: 2.8610169491525424e-05, global_step: 410, interval_runtime: 33.1199, interval_samples_per_second: 0.242, interval_steps_per_second: 0.302, epoch: 2.3164[0m
[32m[2022-08-26 18:23:36,871] [    INFO][0m - loss: 0.3905194, learning_rate: 2.8576271186440676e-05, global_step: 420, interval_runtime: 2.4286, interval_samples_per_second: 3.294, interval_steps_per_second: 4.118, epoch: 2.3729[0m
[32m[2022-08-26 18:23:39,319] [    INFO][0m - loss: 0.50722733, learning_rate: 2.8542372881355935e-05, global_step: 430, interval_runtime: 2.4483, interval_samples_per_second: 3.268, interval_steps_per_second: 4.084, epoch: 2.4294[0m
[32m[2022-08-26 18:23:41,794] [    INFO][0m - loss: 0.49260449, learning_rate: 2.8508474576271187e-05, global_step: 440, interval_runtime: 2.4748, interval_samples_per_second: 3.233, interval_steps_per_second: 4.041, epoch: 2.4859[0m
[32m[2022-08-26 18:23:44,359] [    INFO][0m - loss: 0.46963744, learning_rate: 2.8474576271186442e-05, global_step: 450, interval_runtime: 2.5639, interval_samples_per_second: 3.12, interval_steps_per_second: 3.9, epoch: 2.5424[0m
[32m[2022-08-26 18:23:46,973] [    INFO][0m - loss: 0.57303462, learning_rate: 2.8440677966101694e-05, global_step: 460, interval_runtime: 2.6144, interval_samples_per_second: 3.06, interval_steps_per_second: 3.825, epoch: 2.5989[0m
[32m[2022-08-26 18:23:49,513] [    INFO][0m - loss: 0.54704599, learning_rate: 2.840677966101695e-05, global_step: 470, interval_runtime: 2.5409, interval_samples_per_second: 3.148, interval_steps_per_second: 3.936, epoch: 2.6554[0m
[32m[2022-08-26 18:23:52,070] [    INFO][0m - loss: 0.42591748, learning_rate: 2.8372881355932205e-05, global_step: 480, interval_runtime: 2.5568, interval_samples_per_second: 3.129, interval_steps_per_second: 3.911, epoch: 2.7119[0m
[32m[2022-08-26 18:23:54,666] [    INFO][0m - loss: 0.40512066, learning_rate: 2.833898305084746e-05, global_step: 490, interval_runtime: 2.5955, interval_samples_per_second: 3.082, interval_steps_per_second: 3.853, epoch: 2.7684[0m
[32m[2022-08-26 18:23:57,272] [    INFO][0m - loss: 0.36550117, learning_rate: 2.8305084745762712e-05, global_step: 500, interval_runtime: 2.6067, interval_samples_per_second: 3.069, interval_steps_per_second: 3.836, epoch: 2.8249[0m
[32m[2022-08-26 18:23:57,273] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 18:23:57,273] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-08-26 18:23:57,273] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 18:23:57,273] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 18:23:57,273] [    INFO][0m -   Total prediction steps = 45[0m
[32m[2022-08-26 18:24:25,140] [    INFO][0m - eval_loss: 0.4188205301761627, eval_accuracy: 0.09405940594059406, eval_runtime: 27.8659, eval_samples_per_second: 50.743, eval_steps_per_second: 1.615, epoch: 2.8249[0m
[32m[2022-08-26 18:24:25,140] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-08-26 18:24:25,140] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 18:24:28,301] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-08-26 18:24:28,302] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-08-26 18:24:31,965] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-26 18:24:31,966] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-200 (score: 0.1485148514851485).[0m
[32m[2022-08-26 18:24:33,070] [    INFO][0m - train_runtime: 231.1557, train_samples_per_second: 305.854, train_steps_per_second: 38.286, train_loss: 0.4812584309577942, epoch: 2.8249[0m
[32m[2022-08-26 18:24:33,072] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-26 18:24:33,072] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 18:24:36,272] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-26 18:24:36,273] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-26 18:24:36,274] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-26 18:24:36,275] [    INFO][0m -   epoch                    =     2.8249[0m
[32m[2022-08-26 18:24:36,275] [    INFO][0m -   train_loss               =     0.4813[0m
[32m[2022-08-26 18:24:36,275] [    INFO][0m -   train_runtime            = 0:03:51.15[0m
[32m[2022-08-26 18:24:36,275] [    INFO][0m -   train_samples_per_second =    305.854[0m
[32m[2022-08-26 18:24:36,275] [    INFO][0m -   train_steps_per_second   =     38.286[0m
[32m[2022-08-26 18:24:36,297] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-26 18:24:36,297] [    INFO][0m -   Num examples = 14014[0m
[32m[2022-08-26 18:24:36,297] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 18:24:36,297] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 18:24:36,297] [    INFO][0m -   Total prediction steps = 438[0m
[32m[2022-08-26 18:30:17,536] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-26 18:30:17,536] [    INFO][0m -   test_accuracy           =     0.1114[0m
[32m[2022-08-26 18:30:17,536] [    INFO][0m -   test_loss               =     0.4121[0m
[32m[2022-08-26 18:30:17,536] [    INFO][0m -   test_runtime            = 0:05:41.23[0m
[32m[2022-08-26 18:30:17,536] [    INFO][0m -   test_samples_per_second =     41.068[0m
[32m[2022-08-26 18:30:17,537] [    INFO][0m -   test_steps_per_second   =      1.284[0m
[32m[2022-08-26 18:30:17,537] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-26 18:30:17,537] [    INFO][0m -   Num examples = 14000[0m
[32m[2022-08-26 18:30:17,537] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 18:30:17,537] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 18:30:17,537] [    INFO][0m -   Total prediction steps = 438[0m
[32m[2022-08-26 18:38:17,282] [    INFO][0m - Predictions for chidf saved to ./fewclue_submit_examples.[0m
 
==========
csl
==========
 
[33m[2022-08-26 18:38:21,191] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-26 18:38:21,191] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-26 18:38:21,192] [    INFO][0m - ============================================================[0m
[32m[2022-08-26 18:38:21,192] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-26 18:38:21,192] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-26 18:38:21,192] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-26 18:38:21,192] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-26 18:38:21,192] [    INFO][0m - early_stop_patience           :3[0m
[32m[2022-08-26 18:38:21,192] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-26 18:38:21,192] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-08-26 18:38:21,192] [    INFO][0m - [0m
[32m[2022-08-26 18:38:21,192] [    INFO][0m - ============================================================[0m
[32m[2022-08-26 18:38:21,193] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-26 18:38:21,193] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-26 18:38:21,193] [    INFO][0m - encoder_hidden_size           :None[0m
[32m[2022-08-26 18:38:21,193] [    INFO][0m - prompt                        :{'text': 'text_a'}{'soft':'‰∏äÊñá‰∏≠Êâæ'}{'mask'}{'soft': 'Âá∫Ëøô‰∫õÂÖ≥ÈîÆËØçÔºö'}{'text':'text_b'}[0m
[32m[2022-08-26 18:38:21,193] [    INFO][0m - soft_encoder                  :None[0m
[32m[2022-08-26 18:38:21,193] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-26 18:38:21,193] [    INFO][0m - task_name                     :csl[0m
[32m[2022-08-26 18:38:21,193] [    INFO][0m - [0m
[32m[2022-08-26 18:38:21,193] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0826 18:38:21.194893  6666 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0826 18:38:21.200050  6666 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-26 18:38:23,977] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-08-26 18:38:24,001] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-08-26 18:38:24,001] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[32m[2022-08-26 18:38:24,010] [    INFO][0m - Using template: [{'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'soft': '‰∏ä'}, {'add_prefix_space': '', 'soft': 'Êñá'}, {'add_prefix_space': '', 'soft': '‰∏≠'}, {'add_prefix_space': '', 'soft': 'Êâæ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'soft': 'Âá∫'}, {'add_prefix_space': '', 'soft': 'Ëøô'}, {'add_prefix_space': '', 'soft': '‰∫õ'}, {'add_prefix_space': '', 'soft': 'ÂÖ≥'}, {'add_prefix_space': '', 'soft': 'ÈîÆ'}, {'add_prefix_space': '', 'soft': 'ËØç'}, {'add_prefix_space': '', 'soft': 'Ôºö'}, {'add_prefix_space': '', 'text': 'text_b'}][0m
2022-08-26 18:38:24,015 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-26 18:38:24,167] [    INFO][0m - ============================================================[0m
[32m[2022-08-26 18:38:24,167] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-26 18:38:24,167] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-26 18:38:24,167] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-26 18:38:24,167] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-26 18:38:24,167] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-26 18:38:24,167] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-26 18:38:24,167] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-26 18:38:24,167] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-26 18:38:24,168] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-26 18:38:24,168] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-26 18:38:24,168] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-26 18:38:24,168] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-26 18:38:24,168] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-26 18:38:24,168] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-26 18:38:24,168] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-26 18:38:24,168] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-26 18:38:24,168] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-26 18:38:24,168] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-26 18:38:24,168] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-26 18:38:24,168] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-26 18:38:24,168] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-26 18:38:24,169] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-26 18:38:24,169] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-26 18:38:24,169] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-26 18:38:24,169] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-26 18:38:24,169] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-26 18:38:24,169] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-26 18:38:24,169] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-26 18:38:24,169] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-26 18:38:24,169] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-26 18:38:24,169] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-26 18:38:24,169] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-26 18:38:24,169] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-26 18:38:24,169] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-26 18:38:24,170] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-26 18:38:24,170] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-26 18:38:24,170] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug26_18-38-21_instance-3bwob41y-01[0m
[32m[2022-08-26 18:38:24,170] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-26 18:38:24,170] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-26 18:38:24,170] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-26 18:38:24,170] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-26 18:38:24,170] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-26 18:38:24,170] [    INFO][0m - max_seq_length                :320[0m
[32m[2022-08-26 18:38:24,170] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-26 18:38:24,170] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-26 18:38:24,170] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-26 18:38:24,170] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-26 18:38:24,170] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-08-26 18:38:24,171] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-26 18:38:24,171] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-26 18:38:24,171] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-26 18:38:24,171] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-26 18:38:24,171] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-26 18:38:24,171] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-26 18:38:24,171] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-26 18:38:24,171] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-26 18:38:24,171] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-26 18:38:24,171] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-26 18:38:24,171] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-26 18:38:24,171] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-26 18:38:24,171] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-26 18:38:24,172] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-26 18:38:24,172] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-26 18:38:24,172] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-26 18:38:24,172] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-26 18:38:24,172] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-26 18:38:24,172] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-26 18:38:24,172] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-26 18:38:24,172] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-26 18:38:24,172] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-26 18:38:24,172] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-26 18:38:24,172] [    INFO][0m - seed                          :42[0m
[32m[2022-08-26 18:38:24,172] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-26 18:38:24,172] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-26 18:38:24,172] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-26 18:38:24,173] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-26 18:38:24,173] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-26 18:38:24,173] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-26 18:38:24,173] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-26 18:38:24,173] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-26 18:38:24,173] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-26 18:38:24,173] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-26 18:38:24,173] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-26 18:38:24,173] [    INFO][0m - [0m
[32m[2022-08-26 18:38:24,175] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-26 18:38:24,175] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 18:38:24,175] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-08-26 18:38:24,175] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-26 18:38:24,175] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-26 18:38:24,175] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-26 18:38:24,175] [    INFO][0m -   Total optimization steps = 1000.0[0m
[32m[2022-08-26 18:38:24,176] [    INFO][0m -   Total num train samples = 8000[0m
[32m[2022-08-26 18:38:26,647] [    INFO][0m - loss: 1.2237936, learning_rate: 2.97e-05, global_step: 10, interval_runtime: 2.4712, interval_samples_per_second: 3.237, interval_steps_per_second: 4.047, epoch: 0.5[0m
[32m[2022-08-26 18:38:28,114] [    INFO][0m - loss: 0.9108532, learning_rate: 2.94e-05, global_step: 20, interval_runtime: 1.4664, interval_samples_per_second: 5.455, interval_steps_per_second: 6.819, epoch: 1.0[0m
[32m[2022-08-26 18:38:29,766] [    INFO][0m - loss: 0.7885407, learning_rate: 2.91e-05, global_step: 30, interval_runtime: 1.6521, interval_samples_per_second: 4.842, interval_steps_per_second: 6.053, epoch: 1.5[0m
[32m[2022-08-26 18:38:31,258] [    INFO][0m - loss: 0.77064242, learning_rate: 2.88e-05, global_step: 40, interval_runtime: 1.4922, interval_samples_per_second: 5.361, interval_steps_per_second: 6.701, epoch: 2.0[0m
[32m[2022-08-26 18:38:32,992] [    INFO][0m - loss: 0.68331933, learning_rate: 2.8499999999999998e-05, global_step: 50, interval_runtime: 1.7334, interval_samples_per_second: 4.615, interval_steps_per_second: 5.769, epoch: 2.5[0m
[32m[2022-08-26 18:38:34,507] [    INFO][0m - loss: 0.72984972, learning_rate: 2.8199999999999998e-05, global_step: 60, interval_runtime: 1.5154, interval_samples_per_second: 5.279, interval_steps_per_second: 6.599, epoch: 3.0[0m
[32m[2022-08-26 18:38:36,316] [    INFO][0m - loss: 0.63237534, learning_rate: 2.79e-05, global_step: 70, interval_runtime: 1.8086, interval_samples_per_second: 4.423, interval_steps_per_second: 5.529, epoch: 3.5[0m
[32m[2022-08-26 18:38:37,856] [    INFO][0m - loss: 0.61121435, learning_rate: 2.7600000000000003e-05, global_step: 80, interval_runtime: 1.5407, interval_samples_per_second: 5.192, interval_steps_per_second: 6.49, epoch: 4.0[0m
[32m[2022-08-26 18:38:39,732] [    INFO][0m - loss: 0.63667607, learning_rate: 2.7300000000000003e-05, global_step: 90, interval_runtime: 1.8756, interval_samples_per_second: 4.265, interval_steps_per_second: 5.332, epoch: 4.5[0m
[32m[2022-08-26 18:38:41,297] [    INFO][0m - loss: 0.63632813, learning_rate: 2.7000000000000002e-05, global_step: 100, interval_runtime: 1.5647, interval_samples_per_second: 5.113, interval_steps_per_second: 6.391, epoch: 5.0[0m
[32m[2022-08-26 18:38:41,297] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 18:38:41,297] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 18:38:41,297] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 18:38:41,298] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 18:38:41,298] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 18:38:43,036] [    INFO][0m - eval_loss: 1.1718246936798096, eval_accuracy: 0.55625, eval_runtime: 1.7377, eval_samples_per_second: 92.073, eval_steps_per_second: 2.877, epoch: 5.0[0m
[32m[2022-08-26 18:38:43,036] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-26 18:38:43,036] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 18:38:46,076] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-26 18:38:46,076] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-26 18:38:52,052] [    INFO][0m - loss: 0.4765655, learning_rate: 2.6700000000000002e-05, global_step: 110, interval_runtime: 10.7551, interval_samples_per_second: 0.744, interval_steps_per_second: 0.93, epoch: 5.5[0m
[32m[2022-08-26 18:38:53,644] [    INFO][0m - loss: 0.50874591, learning_rate: 2.64e-05, global_step: 120, interval_runtime: 1.592, interval_samples_per_second: 5.025, interval_steps_per_second: 6.282, epoch: 6.0[0m
[32m[2022-08-26 18:38:55,677] [    INFO][0m - loss: 0.4810132, learning_rate: 2.61e-05, global_step: 130, interval_runtime: 2.0327, interval_samples_per_second: 3.936, interval_steps_per_second: 4.92, epoch: 6.5[0m
[32m[2022-08-26 18:38:57,302] [    INFO][0m - loss: 0.44078469, learning_rate: 2.58e-05, global_step: 140, interval_runtime: 1.6258, interval_samples_per_second: 4.921, interval_steps_per_second: 6.151, epoch: 7.0[0m
[32m[2022-08-26 18:38:59,415] [    INFO][0m - loss: 0.39445081, learning_rate: 2.55e-05, global_step: 150, interval_runtime: 2.1125, interval_samples_per_second: 3.787, interval_steps_per_second: 4.734, epoch: 7.5[0m
[32m[2022-08-26 18:39:01,063] [    INFO][0m - loss: 0.21400332, learning_rate: 2.52e-05, global_step: 160, interval_runtime: 1.6474, interval_samples_per_second: 4.856, interval_steps_per_second: 6.07, epoch: 8.0[0m
[32m[2022-08-26 18:39:03,254] [    INFO][0m - loss: 0.28066382, learning_rate: 2.49e-05, global_step: 170, interval_runtime: 2.1912, interval_samples_per_second: 3.651, interval_steps_per_second: 4.564, epoch: 8.5[0m
[32m[2022-08-26 18:39:04,922] [    INFO][0m - loss: 0.343558, learning_rate: 2.4599999999999998e-05, global_step: 180, interval_runtime: 1.668, interval_samples_per_second: 4.796, interval_steps_per_second: 5.995, epoch: 9.0[0m
[32m[2022-08-26 18:39:07,185] [    INFO][0m - loss: 0.216258, learning_rate: 2.43e-05, global_step: 190, interval_runtime: 2.2628, interval_samples_per_second: 3.535, interval_steps_per_second: 4.419, epoch: 9.5[0m
[32m[2022-08-26 18:39:08,886] [    INFO][0m - loss: 0.47819633, learning_rate: 2.4e-05, global_step: 200, interval_runtime: 1.7005, interval_samples_per_second: 4.705, interval_steps_per_second: 5.881, epoch: 10.0[0m
[32m[2022-08-26 18:39:08,888] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 18:39:08,888] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 18:39:08,888] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 18:39:08,888] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 18:39:08,888] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 18:39:11,025] [    INFO][0m - eval_loss: 2.0692365169525146, eval_accuracy: 0.63125, eval_runtime: 2.136, eval_samples_per_second: 74.905, eval_steps_per_second: 2.341, epoch: 10.0[0m
[32m[2022-08-26 18:39:11,026] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-26 18:39:11,026] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 18:39:14,174] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-26 18:39:14,175] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-26 18:39:20,433] [    INFO][0m - loss: 0.24370723, learning_rate: 2.37e-05, global_step: 210, interval_runtime: 11.5476, interval_samples_per_second: 0.693, interval_steps_per_second: 0.866, epoch: 10.5[0m
[32m[2022-08-26 18:39:22,173] [    INFO][0m - loss: 0.27510467, learning_rate: 2.3400000000000003e-05, global_step: 220, interval_runtime: 1.7403, interval_samples_per_second: 4.597, interval_steps_per_second: 5.746, epoch: 11.0[0m
[32m[2022-08-26 18:39:24,581] [    INFO][0m - loss: 0.12111911, learning_rate: 2.3100000000000002e-05, global_step: 230, interval_runtime: 2.4079, interval_samples_per_second: 3.322, interval_steps_per_second: 4.153, epoch: 11.5[0m
[32m[2022-08-26 18:39:26,319] [    INFO][0m - loss: 0.17563456, learning_rate: 2.2800000000000002e-05, global_step: 240, interval_runtime: 1.7377, interval_samples_per_second: 4.604, interval_steps_per_second: 5.755, epoch: 12.0[0m
[32m[2022-08-26 18:39:28,798] [    INFO][0m - loss: 0.30356526, learning_rate: 2.25e-05, global_step: 250, interval_runtime: 2.4792, interval_samples_per_second: 3.227, interval_steps_per_second: 4.034, epoch: 12.5[0m
[32m[2022-08-26 18:39:30,561] [    INFO][0m - loss: 0.14705529, learning_rate: 2.22e-05, global_step: 260, interval_runtime: 1.7628, interval_samples_per_second: 4.538, interval_steps_per_second: 5.673, epoch: 13.0[0m
[32m[2022-08-26 18:39:33,108] [    INFO][0m - loss: 0.05773981, learning_rate: 2.19e-05, global_step: 270, interval_runtime: 2.5473, interval_samples_per_second: 3.141, interval_steps_per_second: 3.926, epoch: 13.5[0m
[32m[2022-08-26 18:39:34,915] [    INFO][0m - loss: 0.15739862, learning_rate: 2.16e-05, global_step: 280, interval_runtime: 1.8074, interval_samples_per_second: 4.426, interval_steps_per_second: 5.533, epoch: 14.0[0m
[32m[2022-08-26 18:39:37,526] [    INFO][0m - loss: 0.05688972, learning_rate: 2.13e-05, global_step: 290, interval_runtime: 2.6107, interval_samples_per_second: 3.064, interval_steps_per_second: 3.83, epoch: 14.5[0m
[32m[2022-08-26 18:39:39,350] [    INFO][0m - loss: 0.13821023, learning_rate: 2.1e-05, global_step: 300, interval_runtime: 1.8244, interval_samples_per_second: 4.385, interval_steps_per_second: 5.481, epoch: 15.0[0m
[32m[2022-08-26 18:39:39,351] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 18:39:39,351] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 18:39:39,351] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 18:39:39,351] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 18:39:39,352] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 18:39:41,848] [    INFO][0m - eval_loss: 3.3725593090057373, eval_accuracy: 0.6, eval_runtime: 2.4962, eval_samples_per_second: 64.098, eval_steps_per_second: 2.003, epoch: 15.0[0m
[32m[2022-08-26 18:39:41,849] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-26 18:39:41,849] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 18:39:45,138] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-26 18:39:45,138] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-26 18:39:51,644] [    INFO][0m - loss: 0.1733683, learning_rate: 2.07e-05, global_step: 310, interval_runtime: 12.2932, interval_samples_per_second: 0.651, interval_steps_per_second: 0.813, epoch: 15.5[0m
[32m[2022-08-26 18:39:53,484] [    INFO][0m - loss: 0.13115944, learning_rate: 2.04e-05, global_step: 320, interval_runtime: 1.8404, interval_samples_per_second: 4.347, interval_steps_per_second: 5.434, epoch: 16.0[0m
[32m[2022-08-26 18:39:56,250] [    INFO][0m - loss: 0.06954907, learning_rate: 2.01e-05, global_step: 330, interval_runtime: 2.7659, interval_samples_per_second: 2.892, interval_steps_per_second: 3.615, epoch: 16.5[0m
[32m[2022-08-26 18:39:58,118] [    INFO][0m - loss: 0.13214755, learning_rate: 1.98e-05, global_step: 340, interval_runtime: 1.8679, interval_samples_per_second: 4.283, interval_steps_per_second: 5.354, epoch: 17.0[0m
[32m[2022-08-26 18:40:00,972] [    INFO][0m - loss: 0.03756823, learning_rate: 1.95e-05, global_step: 350, interval_runtime: 2.8541, interval_samples_per_second: 2.803, interval_steps_per_second: 3.504, epoch: 17.5[0m
[32m[2022-08-26 18:40:02,868] [    INFO][0m - loss: 0.09736424, learning_rate: 1.9200000000000003e-05, global_step: 360, interval_runtime: 1.8961, interval_samples_per_second: 4.219, interval_steps_per_second: 5.274, epoch: 18.0[0m
[32m[2022-08-26 18:40:05,779] [    INFO][0m - loss: 0.22441208, learning_rate: 1.8900000000000002e-05, global_step: 370, interval_runtime: 2.9107, interval_samples_per_second: 2.748, interval_steps_per_second: 3.436, epoch: 18.5[0m
[32m[2022-08-26 18:40:07,682] [    INFO][0m - loss: 0.06739458, learning_rate: 1.86e-05, global_step: 380, interval_runtime: 1.9025, interval_samples_per_second: 4.205, interval_steps_per_second: 5.256, epoch: 19.0[0m
[32m[2022-08-26 18:40:10,666] [    INFO][0m - loss: 0.08856372, learning_rate: 1.83e-05, global_step: 390, interval_runtime: 2.9849, interval_samples_per_second: 2.68, interval_steps_per_second: 3.35, epoch: 19.5[0m
[32m[2022-08-26 18:40:12,602] [    INFO][0m - loss: 0.13921837, learning_rate: 1.8e-05, global_step: 400, interval_runtime: 1.9358, interval_samples_per_second: 4.133, interval_steps_per_second: 5.166, epoch: 20.0[0m
[32m[2022-08-26 18:40:12,603] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 18:40:12,603] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 18:40:12,603] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 18:40:12,603] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 18:40:12,603] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 18:40:15,485] [    INFO][0m - eval_loss: 3.3140997886657715, eval_accuracy: 0.625, eval_runtime: 2.8819, eval_samples_per_second: 55.518, eval_steps_per_second: 1.735, epoch: 20.0[0m
[32m[2022-08-26 18:40:15,486] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-26 18:40:15,486] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 18:40:18,485] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-26 18:40:18,486] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-26 18:40:25,404] [    INFO][0m - loss: 0.12261245, learning_rate: 1.77e-05, global_step: 410, interval_runtime: 12.8021, interval_samples_per_second: 0.625, interval_steps_per_second: 0.781, epoch: 20.5[0m
[32m[2022-08-26 18:40:27,357] [    INFO][0m - loss: 0.08527581, learning_rate: 1.74e-05, global_step: 420, interval_runtime: 1.953, interval_samples_per_second: 4.096, interval_steps_per_second: 5.12, epoch: 21.0[0m
[32m[2022-08-26 18:40:30,489] [    INFO][0m - loss: 0.0621335, learning_rate: 1.71e-05, global_step: 430, interval_runtime: 3.1324, interval_samples_per_second: 2.554, interval_steps_per_second: 3.192, epoch: 21.5[0m
[32m[2022-08-26 18:40:32,462] [    INFO][0m - loss: 0.11058599, learning_rate: 1.6800000000000002e-05, global_step: 440, interval_runtime: 1.9726, interval_samples_per_second: 4.056, interval_steps_per_second: 5.069, epoch: 22.0[0m
[32m[2022-08-26 18:40:35,686] [    INFO][0m - loss: 0.12279866, learning_rate: 1.65e-05, global_step: 450, interval_runtime: 3.2235, interval_samples_per_second: 2.482, interval_steps_per_second: 3.102, epoch: 22.5[0m
[32m[2022-08-26 18:40:37,702] [    INFO][0m - loss: 0.08756237, learning_rate: 1.62e-05, global_step: 460, interval_runtime: 2.0163, interval_samples_per_second: 3.968, interval_steps_per_second: 4.96, epoch: 23.0[0m
[32m[2022-08-26 18:40:40,992] [    INFO][0m - loss: 0.14639472, learning_rate: 1.59e-05, global_step: 470, interval_runtime: 3.2903, interval_samples_per_second: 2.431, interval_steps_per_second: 3.039, epoch: 23.5[0m
[32m[2022-08-26 18:40:43,016] [    INFO][0m - loss: 0.03199312, learning_rate: 1.56e-05, global_step: 480, interval_runtime: 2.0244, interval_samples_per_second: 3.952, interval_steps_per_second: 4.94, epoch: 24.0[0m
[32m[2022-08-26 18:40:46,423] [    INFO][0m - loss: 0.10762577, learning_rate: 1.53e-05, global_step: 490, interval_runtime: 3.406, interval_samples_per_second: 2.349, interval_steps_per_second: 2.936, epoch: 24.5[0m
[32m[2022-08-26 18:40:48,484] [    INFO][0m - loss: 0.1678069, learning_rate: 1.5e-05, global_step: 500, interval_runtime: 2.0611, interval_samples_per_second: 3.881, interval_steps_per_second: 4.852, epoch: 25.0[0m
[32m[2022-08-26 18:40:48,484] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 18:40:48,484] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 18:40:48,484] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 18:40:48,484] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 18:40:48,485] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 18:40:51,791] [    INFO][0m - eval_loss: 4.054193019866943, eval_accuracy: 0.61875, eval_runtime: 3.3062, eval_samples_per_second: 48.395, eval_steps_per_second: 1.512, epoch: 25.0[0m
[32m[2022-08-26 18:40:51,792] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-08-26 18:40:51,792] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 18:40:54,759] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-08-26 18:40:54,760] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-08-26 18:40:58,278] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-26 18:40:58,278] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-200 (score: 0.63125).[0m
[32m[2022-08-26 18:40:59,439] [    INFO][0m - train_runtime: 155.2624, train_samples_per_second: 51.526, train_steps_per_second: 6.441, train_loss: 0.30679583621025086, epoch: 25.0[0m
[32m[2022-08-26 18:40:59,440] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-26 18:40:59,440] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 18:41:02,379] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-26 18:41:02,379] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-26 18:41:02,380] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-26 18:41:02,381] [    INFO][0m -   epoch                    =       25.0[0m
[32m[2022-08-26 18:41:02,381] [    INFO][0m -   train_loss               =     0.3068[0m
[32m[2022-08-26 18:41:02,381] [    INFO][0m -   train_runtime            = 0:02:35.26[0m
[32m[2022-08-26 18:41:02,381] [    INFO][0m -   train_samples_per_second =     51.526[0m
[32m[2022-08-26 18:41:02,381] [    INFO][0m -   train_steps_per_second   =      6.441[0m
[32m[2022-08-26 18:41:02,384] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-26 18:41:02,384] [    INFO][0m -   Num examples = 2838[0m
[32m[2022-08-26 18:41:02,384] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 18:41:02,384] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 18:41:02,384] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-08-26 18:42:03,732] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-26 18:42:03,732] [    INFO][0m -   test_accuracy           =     0.6092[0m
[32m[2022-08-26 18:42:03,732] [    INFO][0m -   test_loss               =     2.1619[0m
[32m[2022-08-26 18:42:03,732] [    INFO][0m -   test_runtime            = 0:01:01.34[0m
[32m[2022-08-26 18:42:03,733] [    INFO][0m -   test_samples_per_second =     46.261[0m
[32m[2022-08-26 18:42:03,733] [    INFO][0m -   test_steps_per_second   =      1.451[0m
[32m[2022-08-26 18:42:03,733] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-26 18:42:03,733] [    INFO][0m -   Num examples = 3000[0m
[32m[2022-08-26 18:42:03,733] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 18:42:03,733] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 18:42:03,733] [    INFO][0m -   Total prediction steps = 94[0m
[32m[2022-08-26 18:43:23,188] [    INFO][0m - Predictions for cslf saved to ./fewclue_submit_examples.[0m
 
==========
cluewsc
==========
 
[33m[2022-08-26 18:43:27,175] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-26 18:43:27,176] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-26 18:43:27,176] [    INFO][0m - ============================================================[0m
[32m[2022-08-26 18:43:27,176] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-26 18:43:27,176] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-26 18:43:27,176] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-26 18:43:27,176] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-26 18:43:27,176] [    INFO][0m - early_stop_patience           :3[0m
[32m[2022-08-26 18:43:27,176] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-26 18:43:27,177] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-08-26 18:43:27,177] [    INFO][0m - [0m
[32m[2022-08-26 18:43:27,177] [    INFO][0m - ============================================================[0m
[32m[2022-08-26 18:43:27,177] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-26 18:43:27,177] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-26 18:43:27,177] [    INFO][0m - encoder_hidden_size           :None[0m
[32m[2022-08-26 18:43:27,177] [    INFO][0m - prompt                        :{'text':'text_a'}{'sep'}{'soft':'ÂÖ∂‰∏≠‰ª£ËØçÁî®'}{'mask'}{'soft':'‰∫Ü„ÄÇ'}[0m
[32m[2022-08-26 18:43:27,177] [    INFO][0m - soft_encoder                  :None[0m
[32m[2022-08-26 18:43:27,177] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-26 18:43:27,177] [    INFO][0m - task_name                     :cluewsc[0m
[32m[2022-08-26 18:43:27,177] [    INFO][0m - [0m
[32m[2022-08-26 18:43:27,178] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0826 18:43:27.179033 11817 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0826 18:43:27.183243 11817 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-26 18:43:30,063] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-08-26 18:43:30,089] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-08-26 18:43:30,089] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[32m[2022-08-26 18:43:30,096] [    INFO][0m - Using template: [{'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'sep': None}, {'add_prefix_space': '', 'soft': 'ÂÖ∂'}, {'add_prefix_space': '', 'soft': '‰∏≠'}, {'add_prefix_space': '', 'soft': '‰ª£'}, {'add_prefix_space': '', 'soft': 'ËØç'}, {'add_prefix_space': '', 'soft': 'Áî®'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'soft': '‰∫Ü'}, {'add_prefix_space': '', 'soft': '„ÄÇ'}][0m
2022-08-26 18:43:30,101 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-26 18:43:30,206] [    INFO][0m - ============================================================[0m
[32m[2022-08-26 18:43:30,206] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-26 18:43:30,206] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-26 18:43:30,207] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-26 18:43:30,207] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-26 18:43:30,207] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-26 18:43:30,207] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-26 18:43:30,207] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-26 18:43:30,207] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-26 18:43:30,207] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-26 18:43:30,207] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-26 18:43:30,207] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-26 18:43:30,207] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-26 18:43:30,207] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-26 18:43:30,207] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-26 18:43:30,208] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-26 18:43:30,208] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-26 18:43:30,208] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-26 18:43:30,208] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-26 18:43:30,208] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-26 18:43:30,208] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-26 18:43:30,208] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-26 18:43:30,208] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-26 18:43:30,208] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-26 18:43:30,208] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-26 18:43:30,208] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-26 18:43:30,208] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-26 18:43:30,208] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-26 18:43:30,209] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-26 18:43:30,209] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-26 18:43:30,209] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-26 18:43:30,209] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-26 18:43:30,209] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-26 18:43:30,209] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-26 18:43:30,209] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-26 18:43:30,209] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-26 18:43:30,209] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-26 18:43:30,209] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug26_18-43-27_instance-3bwob41y-01[0m
[32m[2022-08-26 18:43:30,209] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-26 18:43:30,209] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-26 18:43:30,210] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-26 18:43:30,210] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-26 18:43:30,210] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-26 18:43:30,210] [    INFO][0m - max_seq_length                :128[0m
[32m[2022-08-26 18:43:30,210] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-26 18:43:30,210] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-26 18:43:30,210] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-26 18:43:30,210] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-26 18:43:30,210] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-08-26 18:43:30,210] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-26 18:43:30,210] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-26 18:43:30,210] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-26 18:43:30,210] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-26 18:43:30,211] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-26 18:43:30,211] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-26 18:43:30,211] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-26 18:43:30,211] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-26 18:43:30,211] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-26 18:43:30,211] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-26 18:43:30,211] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-26 18:43:30,211] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-26 18:43:30,211] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-26 18:43:30,211] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-26 18:43:30,211] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-26 18:43:30,211] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-26 18:43:30,211] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-26 18:43:30,212] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-26 18:43:30,212] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-26 18:43:30,212] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-26 18:43:30,212] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-26 18:43:30,212] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-26 18:43:30,212] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-26 18:43:30,212] [    INFO][0m - seed                          :42[0m
[32m[2022-08-26 18:43:30,212] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-26 18:43:30,212] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-26 18:43:30,212] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-26 18:43:30,212] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-26 18:43:30,212] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-26 18:43:30,212] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-26 18:43:30,213] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-26 18:43:30,213] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-26 18:43:30,213] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-26 18:43:30,213] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-26 18:43:30,213] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-26 18:43:30,213] [    INFO][0m - [0m
[32m[2022-08-26 18:43:30,215] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-26 18:43:30,215] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 18:43:30,215] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-08-26 18:43:30,215] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-26 18:43:30,215] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-26 18:43:30,215] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-26 18:43:30,215] [    INFO][0m -   Total optimization steps = 1000.0[0m
[32m[2022-08-26 18:43:30,215] [    INFO][0m -   Total num train samples = 8000[0m
[32m[2022-08-26 18:43:31,992] [    INFO][0m - loss: 0.82722197, learning_rate: 2.97e-05, global_step: 10, interval_runtime: 1.7761, interval_samples_per_second: 4.504, interval_steps_per_second: 5.63, epoch: 0.5[0m
[32m[2022-08-26 18:43:32,685] [    INFO][0m - loss: 0.77501912, learning_rate: 2.94e-05, global_step: 20, interval_runtime: 0.6934, interval_samples_per_second: 11.538, interval_steps_per_second: 14.422, epoch: 1.0[0m
[32m[2022-08-26 18:43:33,479] [    INFO][0m - loss: 0.69969168, learning_rate: 2.91e-05, global_step: 30, interval_runtime: 0.7933, interval_samples_per_second: 10.084, interval_steps_per_second: 12.606, epoch: 1.5[0m
[32m[2022-08-26 18:43:34,186] [    INFO][0m - loss: 0.70762415, learning_rate: 2.88e-05, global_step: 40, interval_runtime: 0.7077, interval_samples_per_second: 11.305, interval_steps_per_second: 14.131, epoch: 2.0[0m
[32m[2022-08-26 18:43:35,026] [    INFO][0m - loss: 0.72899842, learning_rate: 2.8499999999999998e-05, global_step: 50, interval_runtime: 0.8395, interval_samples_per_second: 9.53, interval_steps_per_second: 11.913, epoch: 2.5[0m
[32m[2022-08-26 18:43:35,746] [    INFO][0m - loss: 0.72362266, learning_rate: 2.8199999999999998e-05, global_step: 60, interval_runtime: 0.7205, interval_samples_per_second: 11.103, interval_steps_per_second: 13.879, epoch: 3.0[0m
[32m[2022-08-26 18:43:36,629] [    INFO][0m - loss: 0.6220139, learning_rate: 2.79e-05, global_step: 70, interval_runtime: 0.8826, interval_samples_per_second: 9.064, interval_steps_per_second: 11.33, epoch: 3.5[0m
[32m[2022-08-26 18:43:37,364] [    INFO][0m - loss: 0.66100636, learning_rate: 2.7600000000000003e-05, global_step: 80, interval_runtime: 0.7349, interval_samples_per_second: 10.886, interval_steps_per_second: 13.608, epoch: 4.0[0m
[32m[2022-08-26 18:43:38,284] [    INFO][0m - loss: 0.51748033, learning_rate: 2.7300000000000003e-05, global_step: 90, interval_runtime: 0.9202, interval_samples_per_second: 8.694, interval_steps_per_second: 10.867, epoch: 4.5[0m
[32m[2022-08-26 18:43:39,032] [    INFO][0m - loss: 0.47342696, learning_rate: 2.7000000000000002e-05, global_step: 100, interval_runtime: 0.7479, interval_samples_per_second: 10.696, interval_steps_per_second: 13.37, epoch: 5.0[0m
[32m[2022-08-26 18:43:39,032] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 18:43:39,033] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-26 18:43:39,033] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 18:43:39,033] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 18:43:39,033] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 18:43:39,753] [    INFO][0m - eval_loss: 0.8769397139549255, eval_accuracy: 0.5283018867924528, eval_runtime: 0.7202, eval_samples_per_second: 220.775, eval_steps_per_second: 6.943, epoch: 5.0[0m
[32m[2022-08-26 18:43:39,754] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-26 18:43:39,754] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 18:43:43,044] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-26 18:43:43,044] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-26 18:43:47,843] [    INFO][0m - loss: 0.4461772, learning_rate: 2.6700000000000002e-05, global_step: 110, interval_runtime: 8.8108, interval_samples_per_second: 0.908, interval_steps_per_second: 1.135, epoch: 5.5[0m
[32m[2022-08-26 18:43:48,608] [    INFO][0m - loss: 0.38710799, learning_rate: 2.64e-05, global_step: 120, interval_runtime: 0.7651, interval_samples_per_second: 10.456, interval_steps_per_second: 13.071, epoch: 6.0[0m
[32m[2022-08-26 18:43:49,618] [    INFO][0m - loss: 0.31821399, learning_rate: 2.61e-05, global_step: 130, interval_runtime: 1.0098, interval_samples_per_second: 7.922, interval_steps_per_second: 9.903, epoch: 6.5[0m
[32m[2022-08-26 18:43:50,395] [    INFO][0m - loss: 0.34949422, learning_rate: 2.58e-05, global_step: 140, interval_runtime: 0.7776, interval_samples_per_second: 10.288, interval_steps_per_second: 12.86, epoch: 7.0[0m
[32m[2022-08-26 18:43:51,441] [    INFO][0m - loss: 0.26981146, learning_rate: 2.55e-05, global_step: 150, interval_runtime: 1.0452, interval_samples_per_second: 7.654, interval_steps_per_second: 9.568, epoch: 7.5[0m
[32m[2022-08-26 18:43:52,233] [    INFO][0m - loss: 0.2095, learning_rate: 2.52e-05, global_step: 160, interval_runtime: 0.7922, interval_samples_per_second: 10.098, interval_steps_per_second: 12.623, epoch: 8.0[0m
[32m[2022-08-26 18:43:53,328] [    INFO][0m - loss: 0.29284656, learning_rate: 2.49e-05, global_step: 170, interval_runtime: 1.0954, interval_samples_per_second: 7.303, interval_steps_per_second: 9.129, epoch: 8.5[0m
[32m[2022-08-26 18:43:54,147] [    INFO][0m - loss: 0.32706289, learning_rate: 2.4599999999999998e-05, global_step: 180, interval_runtime: 0.8191, interval_samples_per_second: 9.767, interval_steps_per_second: 12.209, epoch: 9.0[0m
[32m[2022-08-26 18:43:55,280] [    INFO][0m - loss: 0.34344141, learning_rate: 2.43e-05, global_step: 190, interval_runtime: 1.1325, interval_samples_per_second: 7.064, interval_steps_per_second: 8.83, epoch: 9.5[0m
[32m[2022-08-26 18:43:56,103] [    INFO][0m - loss: 0.25832534, learning_rate: 2.4e-05, global_step: 200, interval_runtime: 0.8228, interval_samples_per_second: 9.722, interval_steps_per_second: 12.153, epoch: 10.0[0m
[32m[2022-08-26 18:43:56,103] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 18:43:56,103] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-26 18:43:56,103] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 18:43:56,103] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 18:43:56,104] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 18:43:57,001] [    INFO][0m - eval_loss: 1.633068561553955, eval_accuracy: 0.5094339622641509, eval_runtime: 0.8973, eval_samples_per_second: 177.197, eval_steps_per_second: 5.572, epoch: 10.0[0m
[32m[2022-08-26 18:43:57,002] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-26 18:43:57,002] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 18:44:00,295] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-26 18:44:00,295] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-26 18:44:05,546] [    INFO][0m - loss: 0.2468477, learning_rate: 2.37e-05, global_step: 210, interval_runtime: 9.4436, interval_samples_per_second: 0.847, interval_steps_per_second: 1.059, epoch: 10.5[0m
[32m[2022-08-26 18:44:06,381] [    INFO][0m - loss: 0.20959496, learning_rate: 2.3400000000000003e-05, global_step: 220, interval_runtime: 0.8348, interval_samples_per_second: 9.583, interval_steps_per_second: 11.978, epoch: 11.0[0m
[32m[2022-08-26 18:44:07,600] [    INFO][0m - loss: 0.14900366, learning_rate: 2.3100000000000002e-05, global_step: 230, interval_runtime: 1.219, interval_samples_per_second: 6.563, interval_steps_per_second: 8.204, epoch: 11.5[0m
[32m[2022-08-26 18:44:08,454] [    INFO][0m - loss: 0.32769582, learning_rate: 2.2800000000000002e-05, global_step: 240, interval_runtime: 0.8535, interval_samples_per_second: 9.373, interval_steps_per_second: 11.717, epoch: 12.0[0m
[32m[2022-08-26 18:44:09,723] [    INFO][0m - loss: 0.1866904, learning_rate: 2.25e-05, global_step: 250, interval_runtime: 1.2692, interval_samples_per_second: 6.303, interval_steps_per_second: 7.879, epoch: 12.5[0m
[32m[2022-08-26 18:44:10,584] [    INFO][0m - loss: 0.27331605, learning_rate: 2.22e-05, global_step: 260, interval_runtime: 0.8615, interval_samples_per_second: 9.287, interval_steps_per_second: 11.608, epoch: 13.0[0m
[32m[2022-08-26 18:44:11,903] [    INFO][0m - loss: 0.27121418, learning_rate: 2.19e-05, global_step: 270, interval_runtime: 1.3185, interval_samples_per_second: 6.068, interval_steps_per_second: 7.584, epoch: 13.5[0m
[32m[2022-08-26 18:44:12,787] [    INFO][0m - loss: 0.17723495, learning_rate: 2.16e-05, global_step: 280, interval_runtime: 0.8843, interval_samples_per_second: 9.046, interval_steps_per_second: 11.308, epoch: 14.0[0m
[32m[2022-08-26 18:44:14,131] [    INFO][0m - loss: 0.13484644, learning_rate: 2.13e-05, global_step: 290, interval_runtime: 1.3437, interval_samples_per_second: 5.954, interval_steps_per_second: 7.442, epoch: 14.5[0m
[32m[2022-08-26 18:44:15,021] [    INFO][0m - loss: 0.19353368, learning_rate: 2.1e-05, global_step: 300, interval_runtime: 0.8901, interval_samples_per_second: 8.987, interval_steps_per_second: 11.234, epoch: 15.0[0m
[32m[2022-08-26 18:44:15,021] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 18:44:15,022] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-26 18:44:15,022] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 18:44:15,022] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 18:44:15,022] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 18:44:16,101] [    INFO][0m - eval_loss: 2.980067253112793, eval_accuracy: 0.5345911949685535, eval_runtime: 1.0793, eval_samples_per_second: 147.316, eval_steps_per_second: 4.633, epoch: 15.0[0m
[32m[2022-08-26 18:44:16,102] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-26 18:44:16,102] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 18:44:19,383] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-26 18:44:19,384] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-26 18:44:24,454] [    INFO][0m - loss: 0.1527208, learning_rate: 2.07e-05, global_step: 310, interval_runtime: 9.4328, interval_samples_per_second: 0.848, interval_steps_per_second: 1.06, epoch: 15.5[0m
[32m[2022-08-26 18:44:25,359] [    INFO][0m - loss: 0.25519185, learning_rate: 2.04e-05, global_step: 320, interval_runtime: 0.905, interval_samples_per_second: 8.84, interval_steps_per_second: 11.05, epoch: 16.0[0m
[32m[2022-08-26 18:44:26,806] [    INFO][0m - loss: 0.15324198, learning_rate: 2.01e-05, global_step: 330, interval_runtime: 1.4472, interval_samples_per_second: 5.528, interval_steps_per_second: 6.91, epoch: 16.5[0m
[32m[2022-08-26 18:44:27,726] [    INFO][0m - loss: 0.07768716, learning_rate: 1.98e-05, global_step: 340, interval_runtime: 0.9198, interval_samples_per_second: 8.698, interval_steps_per_second: 10.872, epoch: 17.0[0m
[32m[2022-08-26 18:44:29,199] [    INFO][0m - loss: 0.18579789, learning_rate: 1.95e-05, global_step: 350, interval_runtime: 1.4732, interval_samples_per_second: 5.431, interval_steps_per_second: 6.788, epoch: 17.5[0m
[32m[2022-08-26 18:44:30,137] [    INFO][0m - loss: 0.0647764, learning_rate: 1.9200000000000003e-05, global_step: 360, interval_runtime: 0.9382, interval_samples_per_second: 8.527, interval_steps_per_second: 10.658, epoch: 18.0[0m
[32m[2022-08-26 18:44:31,662] [    INFO][0m - loss: 0.29546921, learning_rate: 1.8900000000000002e-05, global_step: 370, interval_runtime: 1.5245, interval_samples_per_second: 5.248, interval_steps_per_second: 6.56, epoch: 18.5[0m
[32m[2022-08-26 18:44:32,608] [    INFO][0m - loss: 0.0489877, learning_rate: 1.86e-05, global_step: 380, interval_runtime: 0.9465, interval_samples_per_second: 8.452, interval_steps_per_second: 10.565, epoch: 19.0[0m
[32m[2022-08-26 18:44:34,170] [    INFO][0m - loss: 0.05198084, learning_rate: 1.83e-05, global_step: 390, interval_runtime: 1.5613, interval_samples_per_second: 5.124, interval_steps_per_second: 6.405, epoch: 19.5[0m
[32m[2022-08-26 18:44:35,135] [    INFO][0m - loss: 0.05419216, learning_rate: 1.8e-05, global_step: 400, interval_runtime: 0.9648, interval_samples_per_second: 8.292, interval_steps_per_second: 10.365, epoch: 20.0[0m
[32m[2022-08-26 18:44:35,135] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 18:44:35,135] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-26 18:44:35,135] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 18:44:35,136] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 18:44:35,136] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 18:44:36,394] [    INFO][0m - eval_loss: 4.0156989097595215, eval_accuracy: 0.4779874213836478, eval_runtime: 1.2583, eval_samples_per_second: 126.359, eval_steps_per_second: 3.974, epoch: 20.0[0m
[32m[2022-08-26 18:44:36,394] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-26 18:44:36,395] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 18:44:39,370] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-26 18:44:39,370] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-26 18:44:44,578] [    INFO][0m - loss: 0.14285959, learning_rate: 1.77e-05, global_step: 410, interval_runtime: 9.4433, interval_samples_per_second: 0.847, interval_steps_per_second: 1.059, epoch: 20.5[0m
[32m[2022-08-26 18:44:45,562] [    INFO][0m - loss: 0.1121453, learning_rate: 1.74e-05, global_step: 420, interval_runtime: 0.9839, interval_samples_per_second: 8.131, interval_steps_per_second: 10.164, epoch: 21.0[0m
[32m[2022-08-26 18:44:47,227] [    INFO][0m - loss: 0.03155814, learning_rate: 1.71e-05, global_step: 430, interval_runtime: 1.6651, interval_samples_per_second: 4.804, interval_steps_per_second: 6.005, epoch: 21.5[0m
[32m[2022-08-26 18:44:48,222] [    INFO][0m - loss: 0.06317777, learning_rate: 1.6800000000000002e-05, global_step: 440, interval_runtime: 0.9954, interval_samples_per_second: 8.037, interval_steps_per_second: 10.046, epoch: 22.0[0m
[32m[2022-08-26 18:44:49,903] [    INFO][0m - loss: 0.02017523, learning_rate: 1.65e-05, global_step: 450, interval_runtime: 1.6807, interval_samples_per_second: 4.76, interval_steps_per_second: 5.95, epoch: 22.5[0m
[32m[2022-08-26 18:44:50,906] [    INFO][0m - loss: 0.00036739, learning_rate: 1.62e-05, global_step: 460, interval_runtime: 1.0029, interval_samples_per_second: 7.977, interval_steps_per_second: 9.971, epoch: 23.0[0m
[32m[2022-08-26 18:44:52,644] [    INFO][0m - loss: 4.444e-05, learning_rate: 1.59e-05, global_step: 470, interval_runtime: 1.7386, interval_samples_per_second: 4.601, interval_steps_per_second: 5.752, epoch: 23.5[0m
[32m[2022-08-26 18:44:53,674] [    INFO][0m - loss: 0.0386363, learning_rate: 1.56e-05, global_step: 480, interval_runtime: 1.0288, interval_samples_per_second: 7.776, interval_steps_per_second: 9.72, epoch: 24.0[0m
[32m[2022-08-26 18:44:55,438] [    INFO][0m - loss: 0.12486948, learning_rate: 1.53e-05, global_step: 490, interval_runtime: 1.7648, interval_samples_per_second: 4.533, interval_steps_per_second: 5.666, epoch: 24.5[0m
[32m[2022-08-26 18:44:56,484] [    INFO][0m - loss: 0.04333358, learning_rate: 1.5e-05, global_step: 500, interval_runtime: 1.0457, interval_samples_per_second: 7.65, interval_steps_per_second: 9.563, epoch: 25.0[0m
[32m[2022-08-26 18:44:56,484] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 18:44:56,484] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-26 18:44:56,484] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 18:44:56,485] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 18:44:56,485] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 18:44:57,920] [    INFO][0m - eval_loss: 6.202627658843994, eval_accuracy: 0.5031446540880503, eval_runtime: 1.4352, eval_samples_per_second: 110.784, eval_steps_per_second: 3.484, epoch: 25.0[0m
[32m[2022-08-26 18:44:57,920] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-08-26 18:44:57,921] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 18:45:01,168] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-08-26 18:45:01,169] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-08-26 18:45:06,520] [    INFO][0m - loss: 0.06211342, learning_rate: 1.47e-05, global_step: 510, interval_runtime: 10.0358, interval_samples_per_second: 0.797, interval_steps_per_second: 0.996, epoch: 25.5[0m
[32m[2022-08-26 18:45:07,576] [    INFO][0m - loss: 0.01987541, learning_rate: 1.44e-05, global_step: 520, interval_runtime: 1.0557, interval_samples_per_second: 7.578, interval_steps_per_second: 9.473, epoch: 26.0[0m
[32m[2022-08-26 18:45:09,433] [    INFO][0m - loss: 0.00766322, learning_rate: 1.4099999999999999e-05, global_step: 530, interval_runtime: 1.858, interval_samples_per_second: 4.306, interval_steps_per_second: 5.382, epoch: 26.5[0m
[32m[2022-08-26 18:45:10,502] [    INFO][0m - loss: 7.79e-06, learning_rate: 1.3800000000000002e-05, global_step: 540, interval_runtime: 1.0683, interval_samples_per_second: 7.489, interval_steps_per_second: 9.361, epoch: 27.0[0m
[32m[2022-08-26 18:45:12,451] [    INFO][0m - loss: 0.19796137, learning_rate: 1.3500000000000001e-05, global_step: 550, interval_runtime: 1.9495, interval_samples_per_second: 4.104, interval_steps_per_second: 5.129, epoch: 27.5[0m
[32m[2022-08-26 18:45:13,530] [    INFO][0m - loss: 0.09768617, learning_rate: 1.32e-05, global_step: 560, interval_runtime: 1.0791, interval_samples_per_second: 7.414, interval_steps_per_second: 9.267, epoch: 28.0[0m
[32m[2022-08-26 18:45:15,469] [    INFO][0m - loss: 0.00055655, learning_rate: 1.29e-05, global_step: 570, interval_runtime: 1.9388, interval_samples_per_second: 4.126, interval_steps_per_second: 5.158, epoch: 28.5[0m
[32m[2022-08-26 18:45:16,584] [    INFO][0m - loss: 0.00028945, learning_rate: 1.26e-05, global_step: 580, interval_runtime: 1.1143, interval_samples_per_second: 7.18, interval_steps_per_second: 8.974, epoch: 29.0[0m
[32m[2022-08-26 18:45:18,582] [    INFO][0m - loss: 6.846e-05, learning_rate: 1.2299999999999999e-05, global_step: 590, interval_runtime: 1.9983, interval_samples_per_second: 4.003, interval_steps_per_second: 5.004, epoch: 29.5[0m
[32m[2022-08-26 18:45:19,690] [    INFO][0m - loss: 0.02077564, learning_rate: 1.2e-05, global_step: 600, interval_runtime: 1.1083, interval_samples_per_second: 7.219, interval_steps_per_second: 9.023, epoch: 30.0[0m
[32m[2022-08-26 18:45:19,690] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 18:45:19,691] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-26 18:45:19,691] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 18:45:19,691] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 18:45:19,691] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 18:45:21,335] [    INFO][0m - eval_loss: 5.481443881988525, eval_accuracy: 0.5660377358490566, eval_runtime: 1.6443, eval_samples_per_second: 96.7, eval_steps_per_second: 3.041, epoch: 30.0[0m
[32m[2022-08-26 18:45:21,336] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-08-26 18:45:21,336] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 18:45:24,367] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-08-26 18:45:24,367] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-08-26 18:45:29,903] [    INFO][0m - loss: 0.00192346, learning_rate: 1.1700000000000001e-05, global_step: 610, interval_runtime: 10.213, interval_samples_per_second: 0.783, interval_steps_per_second: 0.979, epoch: 30.5[0m
[32m[2022-08-26 18:45:31,029] [    INFO][0m - loss: 0.05543452, learning_rate: 1.1400000000000001e-05, global_step: 620, interval_runtime: 1.1261, interval_samples_per_second: 7.104, interval_steps_per_second: 8.88, epoch: 31.0[0m
[32m[2022-08-26 18:45:33,119] [    INFO][0m - loss: 0.0453111, learning_rate: 1.11e-05, global_step: 630, interval_runtime: 2.0902, interval_samples_per_second: 3.827, interval_steps_per_second: 4.784, epoch: 31.5[0m
[32m[2022-08-26 18:45:34,256] [    INFO][0m - loss: 1.54e-06, learning_rate: 1.08e-05, global_step: 640, interval_runtime: 1.1369, interval_samples_per_second: 7.036, interval_steps_per_second: 8.796, epoch: 32.0[0m
[32m[2022-08-26 18:45:36,391] [    INFO][0m - loss: 3.743e-05, learning_rate: 1.05e-05, global_step: 650, interval_runtime: 2.1349, interval_samples_per_second: 3.747, interval_steps_per_second: 4.684, epoch: 32.5[0m
[32m[2022-08-26 18:45:37,553] [    INFO][0m - loss: 0.00159346, learning_rate: 1.02e-05, global_step: 660, interval_runtime: 1.1624, interval_samples_per_second: 6.882, interval_steps_per_second: 8.603, epoch: 33.0[0m
[32m[2022-08-26 18:45:39,709] [    INFO][0m - loss: 0.00289482, learning_rate: 9.9e-06, global_step: 670, interval_runtime: 2.1553, interval_samples_per_second: 3.712, interval_steps_per_second: 4.64, epoch: 33.5[0m
[32m[2022-08-26 18:45:40,873] [    INFO][0m - loss: 0.1029521, learning_rate: 9.600000000000001e-06, global_step: 680, interval_runtime: 1.1638, interval_samples_per_second: 6.874, interval_steps_per_second: 8.592, epoch: 34.0[0m
[32m[2022-08-26 18:45:43,109] [    INFO][0m - loss: 8.183e-05, learning_rate: 9.3e-06, global_step: 690, interval_runtime: 2.2364, interval_samples_per_second: 3.577, interval_steps_per_second: 4.472, epoch: 34.5[0m
[32m[2022-08-26 18:45:44,290] [    INFO][0m - loss: 0.00241271, learning_rate: 9e-06, global_step: 700, interval_runtime: 1.1813, interval_samples_per_second: 6.772, interval_steps_per_second: 8.465, epoch: 35.0[0m
[32m[2022-08-26 18:45:44,291] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 18:45:44,291] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-26 18:45:44,291] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 18:45:44,291] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 18:45:44,291] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 18:45:46,120] [    INFO][0m - eval_loss: 5.541942119598389, eval_accuracy: 0.5534591194968553, eval_runtime: 1.8277, eval_samples_per_second: 86.995, eval_steps_per_second: 2.736, epoch: 35.0[0m
[32m[2022-08-26 18:45:46,120] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-700[0m
[32m[2022-08-26 18:45:46,120] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 18:45:49,075] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-700/tokenizer_config.json[0m
[32m[2022-08-26 18:45:49,075] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-700/special_tokens_map.json[0m
[32m[2022-08-26 18:45:54,950] [    INFO][0m - loss: 0.00054845, learning_rate: 8.7e-06, global_step: 710, interval_runtime: 10.6592, interval_samples_per_second: 0.751, interval_steps_per_second: 0.938, epoch: 35.5[0m
[32m[2022-08-26 18:45:56,147] [    INFO][0m - loss: 0.00154569, learning_rate: 8.400000000000001e-06, global_step: 720, interval_runtime: 1.1976, interval_samples_per_second: 6.68, interval_steps_per_second: 8.35, epoch: 36.0[0m
[32m[2022-08-26 18:45:58,465] [    INFO][0m - loss: 4.306e-05, learning_rate: 8.1e-06, global_step: 730, interval_runtime: 2.3172, interval_samples_per_second: 3.452, interval_steps_per_second: 4.316, epoch: 36.5[0m
[32m[2022-08-26 18:45:59,675] [    INFO][0m - loss: 0.00365312, learning_rate: 7.8e-06, global_step: 740, interval_runtime: 1.2107, interval_samples_per_second: 6.608, interval_steps_per_second: 8.26, epoch: 37.0[0m
[32m[2022-08-26 18:46:02,031] [    INFO][0m - loss: 1.903e-05, learning_rate: 7.5e-06, global_step: 750, interval_runtime: 2.3556, interval_samples_per_second: 3.396, interval_steps_per_second: 4.245, epoch: 37.5[0m
[32m[2022-08-26 18:46:03,254] [    INFO][0m - loss: 6e-07, learning_rate: 7.2e-06, global_step: 760, interval_runtime: 1.2237, interval_samples_per_second: 6.538, interval_steps_per_second: 8.172, epoch: 38.0[0m
[32m[2022-08-26 18:46:05,690] [    INFO][0m - loss: 6.43e-06, learning_rate: 6.900000000000001e-06, global_step: 770, interval_runtime: 2.4352, interval_samples_per_second: 3.285, interval_steps_per_second: 4.106, epoch: 38.5[0m
[32m[2022-08-26 18:46:06,940] [    INFO][0m - loss: 7e-07, learning_rate: 6.6e-06, global_step: 780, interval_runtime: 1.2496, interval_samples_per_second: 6.402, interval_steps_per_second: 8.002, epoch: 39.0[0m
[32m[2022-08-26 18:46:09,364] [    INFO][0m - loss: 2.57e-06, learning_rate: 6.3e-06, global_step: 790, interval_runtime: 2.4248, interval_samples_per_second: 3.299, interval_steps_per_second: 4.124, epoch: 39.5[0m
[32m[2022-08-26 18:46:10,685] [    INFO][0m - loss: 6.049e-05, learning_rate: 6e-06, global_step: 800, interval_runtime: 1.3205, interval_samples_per_second: 6.058, interval_steps_per_second: 7.573, epoch: 40.0[0m
[32m[2022-08-26 18:46:10,685] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 18:46:10,685] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-26 18:46:10,686] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 18:46:10,686] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 18:46:10,686] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 18:46:12,663] [    INFO][0m - eval_loss: 5.840054988861084, eval_accuracy: 0.5471698113207547, eval_runtime: 1.9772, eval_samples_per_second: 80.418, eval_steps_per_second: 2.529, epoch: 40.0[0m
[32m[2022-08-26 18:46:12,663] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-800[0m
[32m[2022-08-26 18:46:12,664] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 18:46:15,720] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-800/tokenizer_config.json[0m
[32m[2022-08-26 18:46:15,720] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-800/special_tokens_map.json[0m
[32m[2022-08-26 18:46:21,825] [    INFO][0m - loss: 2.7e-07, learning_rate: 5.7000000000000005e-06, global_step: 810, interval_runtime: 11.1404, interval_samples_per_second: 0.718, interval_steps_per_second: 0.898, epoch: 40.5[0m
[32m[2022-08-26 18:46:23,089] [    INFO][0m - loss: 3.29e-06, learning_rate: 5.4e-06, global_step: 820, interval_runtime: 1.2641, interval_samples_per_second: 6.329, interval_steps_per_second: 7.911, epoch: 41.0[0m
[32m[2022-08-26 18:46:25,587] [    INFO][0m - loss: 1.002e-05, learning_rate: 5.1e-06, global_step: 830, interval_runtime: 2.4976, interval_samples_per_second: 3.203, interval_steps_per_second: 4.004, epoch: 41.5[0m
[32m[2022-08-26 18:46:26,873] [    INFO][0m - loss: 0.06453916, learning_rate: 4.800000000000001e-06, global_step: 840, interval_runtime: 1.2864, interval_samples_per_second: 6.219, interval_steps_per_second: 7.774, epoch: 42.0[0m
[32m[2022-08-26 18:46:29,453] [    INFO][0m - loss: 6.668e-05, learning_rate: 4.5e-06, global_step: 850, interval_runtime: 2.5796, interval_samples_per_second: 3.101, interval_steps_per_second: 3.877, epoch: 42.5[0m
[32m[2022-08-26 18:46:30,756] [    INFO][0m - loss: 4.2e-07, learning_rate: 4.2000000000000004e-06, global_step: 860, interval_runtime: 1.3035, interval_samples_per_second: 6.137, interval_steps_per_second: 7.672, epoch: 43.0[0m
[32m[2022-08-26 18:46:33,360] [    INFO][0m - loss: 4.41e-06, learning_rate: 3.9e-06, global_step: 870, interval_runtime: 2.6035, interval_samples_per_second: 3.073, interval_steps_per_second: 3.841, epoch: 43.5[0m
[32m[2022-08-26 18:46:34,712] [    INFO][0m - loss: 2.07e-06, learning_rate: 3.6e-06, global_step: 880, interval_runtime: 1.3518, interval_samples_per_second: 5.918, interval_steps_per_second: 7.397, epoch: 44.0[0m
[32m[2022-08-26 18:46:37,369] [    INFO][0m - loss: 1.98e-06, learning_rate: 3.3e-06, global_step: 890, interval_runtime: 2.6569, interval_samples_per_second: 3.011, interval_steps_per_second: 3.764, epoch: 44.5[0m
[32m[2022-08-26 18:46:38,683] [    INFO][0m - loss: 6.2e-07, learning_rate: 3e-06, global_step: 900, interval_runtime: 1.3149, interval_samples_per_second: 6.084, interval_steps_per_second: 7.605, epoch: 45.0[0m
[32m[2022-08-26 18:46:38,684] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 18:46:38,684] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-26 18:46:38,684] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 18:46:38,684] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 18:46:38,684] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 18:46:40,864] [    INFO][0m - eval_loss: 6.097701072692871, eval_accuracy: 0.5471698113207547, eval_runtime: 2.1792, eval_samples_per_second: 72.962, eval_steps_per_second: 2.294, epoch: 45.0[0m
[32m[2022-08-26 18:46:40,864] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-900[0m
[32m[2022-08-26 18:46:40,864] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 18:46:44,091] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-900/tokenizer_config.json[0m
[32m[2022-08-26 18:46:44,092] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-900/special_tokens_map.json[0m
[32m[2022-08-26 18:46:48,197] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-26 18:46:48,197] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-600 (score: 0.5660377358490566).[0m
[32m[2022-08-26 18:46:49,318] [    INFO][0m - train_runtime: 199.1017, train_samples_per_second: 40.18, train_steps_per_second: 5.023, train_loss: 0.1635047461294213, epoch: 45.0[0m
[32m[2022-08-26 18:46:49,319] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-26 18:46:49,319] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 18:46:52,609] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-26 18:46:52,610] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-26 18:46:52,612] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-26 18:46:52,612] [    INFO][0m -   epoch                    =       45.0[0m
[32m[2022-08-26 18:46:52,612] [    INFO][0m -   train_loss               =     0.1635[0m
[32m[2022-08-26 18:46:52,612] [    INFO][0m -   train_runtime            = 0:03:19.10[0m
[32m[2022-08-26 18:46:52,612] [    INFO][0m -   train_samples_per_second =      40.18[0m
[32m[2022-08-26 18:46:52,612] [    INFO][0m -   train_steps_per_second   =      5.023[0m
[32m[2022-08-26 18:46:52,617] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-26 18:46:52,617] [    INFO][0m -   Num examples = 976[0m
[32m[2022-08-26 18:46:52,617] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 18:46:52,617] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 18:46:52,617] [    INFO][0m -   Total prediction steps = 31[0m
[32m[2022-08-26 18:47:06,227] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-26 18:47:06,228] [    INFO][0m -   test_accuracy           =     0.5225[0m
[32m[2022-08-26 18:47:06,228] [    INFO][0m -   test_loss               =     6.6857[0m
[32m[2022-08-26 18:47:06,228] [    INFO][0m -   test_runtime            = 0:00:13.61[0m
[32m[2022-08-26 18:47:06,228] [    INFO][0m -   test_samples_per_second =     71.712[0m
[32m[2022-08-26 18:47:06,228] [    INFO][0m -   test_steps_per_second   =      2.278[0m
[32m[2022-08-26 18:47:06,228] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-26 18:47:06,228] [    INFO][0m -   Num examples = 290[0m
[32m[2022-08-26 18:47:06,228] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 18:47:06,229] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 18:47:06,229] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-08-26 18:47:10,703] [    INFO][0m - Predictions for cluewscf saved to ./fewclue_submit_examples.[0m
