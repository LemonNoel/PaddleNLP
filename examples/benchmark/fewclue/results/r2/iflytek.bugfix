[33m[2022-09-01 13:33:24,084] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-09-01 13:33:24,084] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-01 13:33:24,085] [    INFO][0m - ============================================================[0m
[32m[2022-09-01 13:33:24,085] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-01 13:33:24,085] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-01 13:33:24,085] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-01 13:33:24,085] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-01 13:33:24,085] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-01 13:33:24,085] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-01 13:33:24,085] [    INFO][0m - model_name_or_path            :roformer_v2_chinese_char_base[0m
[32m[2022-09-01 13:33:24,085] [    INFO][0m - [0m
[32m[2022-09-01 13:33:24,085] [    INFO][0m - ============================================================[0m
[32m[2022-09-01 13:33:24,086] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-01 13:33:24,086] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-01 13:33:24,086] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-01 13:33:24,086] [    INFO][0m - prompt                        :{'text':'text_a'}{'soft':'è¿™æ¬¾åº”ç”¨å±žäºŽ'}{'mask'}{'mask'}{'soft':'ç±»åˆ«.'}[0m
[32m[2022-09-01 13:33:24,086] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-01 13:33:24,086] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-01 13:33:24,086] [    INFO][0m - task_name                     :iflytek[0m
[32m[2022-09-01 13:33:24,086] [    INFO][0m - [0m
[32m[2022-09-01 13:33:24,087] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/roformer_v2_chinese_char_base/model_state.pdparams[0m
W0901 13:33:24.088016 13459 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0901 13:33:24.092026 13459 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-01 13:33:29,318] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/roformer_v2_chinese_char_base/vocab.txt[0m
[32m[2022-09-01 13:33:29,328] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/roformer_v2_chinese_char_base/tokenizer_config.json[0m
[32m[2022-09-01 13:33:29,329] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/roformer_v2_chinese_char_base/special_tokens_map.json[0m
[33m[2022-09-01 13:33:29,337] [ WARNING][0m - Encoder has already set as lstm, change `prompt_encoder` will reset parameters.[0m
[32m[2022-09-01 13:33:29,349] [    INFO][0m - Using template: [{'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'soft': 'è¿™'}, {'add_prefix_space': '', 'soft': 'æ¬¾'}, {'add_prefix_space': '', 'soft': 'åº”'}, {'add_prefix_space': '', 'soft': 'ç”¨'}, {'add_prefix_space': '', 'soft': 'å±ž'}, {'add_prefix_space': '', 'soft': 'äºŽ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'soft': 'ç±»'}, {'add_prefix_space': '', 'soft': 'åˆ«'}, {'add_prefix_space': '', 'soft': '.'}][0m
2022-09-01 13:33:29,350 INFO [download.py:119] unique_endpoints {''}
[32m[2022-09-01 13:33:29,532] [    INFO][0m - ============================================================[0m
[32m[2022-09-01 13:33:29,532] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-01 13:33:29,532] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-01 13:33:29,532] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-01 13:33:29,532] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-01 13:33:29,533] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-01 13:33:29,533] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-01 13:33:29,533] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-01 13:33:29,533] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-01 13:33:29,533] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-01 13:33:29,533] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-01 13:33:29,533] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-01 13:33:29,533] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-01 13:33:29,533] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-01 13:33:29,533] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-01 13:33:29,533] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-01 13:33:29,533] [    INFO][0m - do_predict                    :True[0m
[32m[2022-09-01 13:33:29,533] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-01 13:33:29,534] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-09-01 13:33:29,534] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-09-01 13:33:29,534] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-09-01 13:33:29,534] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-01 13:33:29,534] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-01 13:33:29,534] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-01 13:33:29,534] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-01 13:33:29,534] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-01 13:33:29,534] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-01 13:33:29,534] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-01 13:33:29,534] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-01 13:33:29,534] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-01 13:33:29,534] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-09-01 13:33:29,535] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-01 13:33:29,535] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-01 13:33:29,535] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-01 13:33:29,535] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-01 13:33:29,535] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-01 13:33:29,535] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-01 13:33:29,535] [    INFO][0m - logging_dir                   :./checkpoints/runs/Sep01_13-33-24_instance-3bwob41y-01[0m
[32m[2022-09-01 13:33:29,535] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-01 13:33:29,535] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-01 13:33:29,535] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-01 13:33:29,535] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-01 13:33:29,535] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-01 13:33:29,535] [    INFO][0m - max_seq_length                :320[0m
[32m[2022-09-01 13:33:29,535] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-01 13:33:29,536] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-01 13:33:29,536] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-01 13:33:29,536] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-01 13:33:29,536] [    INFO][0m - num_train_epochs              :20.0[0m
[32m[2022-09-01 13:33:29,536] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-01 13:33:29,536] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-01 13:33:29,536] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-09-01 13:33:29,536] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-01 13:33:29,536] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-01 13:33:29,536] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-09-01 13:33:29,536] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-09-01 13:33:29,536] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-01 13:33:29,537] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-01 13:33:29,537] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-01 13:33:29,537] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-09-01 13:33:29,537] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-01 13:33:29,537] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-01 13:33:29,537] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-01 13:33:29,537] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-01 13:33:29,537] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-01 13:33:29,537] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-01 13:33:29,537] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-09-01 13:33:29,537] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-01 13:33:29,537] [    INFO][0m - save_steps                    :100[0m
[32m[2022-09-01 13:33:29,537] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-09-01 13:33:29,537] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-01 13:33:29,538] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-01 13:33:29,538] [    INFO][0m - seed                          :42[0m
[32m[2022-09-01 13:33:29,538] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-01 13:33:29,538] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-01 13:33:29,538] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-01 13:33:29,538] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-09-01 13:33:29,538] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-01 13:33:29,538] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-01 13:33:29,538] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-01 13:33:29,538] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-01 13:33:29,538] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-01 13:33:29,538] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-01 13:33:29,538] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-01 13:33:29,539] [    INFO][0m - [0m
[32m[2022-09-01 13:33:29,540] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-01 13:33:29,540] [    INFO][0m -   Num examples = 3024[0m
[32m[2022-09-01 13:33:29,540] [    INFO][0m -   Num Epochs = 20[0m
[32m[2022-09-01 13:33:29,540] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-09-01 13:33:29,541] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-09-01 13:33:29,541] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-01 13:33:29,541] [    INFO][0m -   Total optimization steps = 7560.0[0m
[32m[2022-09-01 13:33:29,541] [    INFO][0m -   Total num train samples = 60480[0m
[32m[2022-09-01 13:33:32,750] [    INFO][0m - loss: 5.27695045, learning_rate: 2.996031746031746e-05, global_step: 10, interval_runtime: 3.2069, interval_samples_per_second: 2.495, interval_steps_per_second: 3.118, epoch: 0.0265[0m
[32m[2022-09-01 13:33:34,863] [    INFO][0m - loss: 5.40397034, learning_rate: 2.992063492063492e-05, global_step: 20, interval_runtime: 2.1149, interval_samples_per_second: 3.783, interval_steps_per_second: 4.728, epoch: 0.0529[0m
[32m[2022-09-01 13:33:37,006] [    INFO][0m - loss: 5.02261429, learning_rate: 2.9880952380952383e-05, global_step: 30, interval_runtime: 2.142, interval_samples_per_second: 3.735, interval_steps_per_second: 4.668, epoch: 0.0794[0m
[32m[2022-09-01 13:33:39,174] [    INFO][0m - loss: 5.18481674, learning_rate: 2.984126984126984e-05, global_step: 40, interval_runtime: 2.1682, interval_samples_per_second: 3.69, interval_steps_per_second: 4.612, epoch: 0.1058[0m
[32m[2022-09-01 13:33:41,425] [    INFO][0m - loss: 4.76885872, learning_rate: 2.98015873015873e-05, global_step: 50, interval_runtime: 2.2506, interval_samples_per_second: 3.555, interval_steps_per_second: 4.443, epoch: 0.1323[0m
[32m[2022-09-01 13:33:43,674] [    INFO][0m - loss: 4.59232864, learning_rate: 2.9761904761904762e-05, global_step: 60, interval_runtime: 2.2486, interval_samples_per_second: 3.558, interval_steps_per_second: 4.447, epoch: 0.1587[0m
[32m[2022-09-01 13:33:45,991] [    INFO][0m - loss: 4.46871338, learning_rate: 2.9722222222222223e-05, global_step: 70, interval_runtime: 2.3179, interval_samples_per_second: 3.451, interval_steps_per_second: 4.314, epoch: 0.1852[0m
[32m[2022-09-01 13:33:48,293] [    INFO][0m - loss: 3.79891701, learning_rate: 2.9682539682539683e-05, global_step: 80, interval_runtime: 2.3021, interval_samples_per_second: 3.475, interval_steps_per_second: 4.344, epoch: 0.2116[0m
[32m[2022-09-01 13:33:50,616] [    INFO][0m - loss: 4.01424103, learning_rate: 2.9642857142857144e-05, global_step: 90, interval_runtime: 2.3224, interval_samples_per_second: 3.445, interval_steps_per_second: 4.306, epoch: 0.2381[0m
[32m[2022-09-01 13:33:53,001] [    INFO][0m - loss: 3.70164757, learning_rate: 2.96031746031746e-05, global_step: 100, interval_runtime: 2.3856, interval_samples_per_second: 3.353, interval_steps_per_second: 4.192, epoch: 0.2646[0m
[32m[2022-09-01 13:33:53,002] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 13:33:53,002] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-01 13:33:53,002] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 13:33:53,002] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 13:33:53,002] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-09-01 13:34:09,796] [    INFO][0m - eval_loss: 3.1550254821777344, eval_accuracy: 0.2906045156591406, eval_runtime: 16.7933, eval_samples_per_second: 81.759, eval_steps_per_second: 2.561, epoch: 0.2646[0m
[32m[2022-09-01 13:34:09,797] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-09-01 13:34:09,797] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 13:34:11,107] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-01 13:34:11,107] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-01 13:34:15,916] [    INFO][0m - loss: 3.41221199, learning_rate: 2.9563492063492066e-05, global_step: 110, interval_runtime: 22.9151, interval_samples_per_second: 0.349, interval_steps_per_second: 0.436, epoch: 0.291[0m
[32m[2022-09-01 13:34:18,326] [    INFO][0m - loss: 3.3245163, learning_rate: 2.9523809523809523e-05, global_step: 120, interval_runtime: 2.4093, interval_samples_per_second: 3.32, interval_steps_per_second: 4.151, epoch: 0.3175[0m
[32m[2022-09-01 13:34:20,753] [    INFO][0m - loss: 3.17780094, learning_rate: 2.9484126984126984e-05, global_step: 130, interval_runtime: 2.4279, interval_samples_per_second: 3.295, interval_steps_per_second: 4.119, epoch: 0.3439[0m
[32m[2022-09-01 13:34:23,187] [    INFO][0m - loss: 3.03544674, learning_rate: 2.9444444444444445e-05, global_step: 140, interval_runtime: 2.4334, interval_samples_per_second: 3.288, interval_steps_per_second: 4.11, epoch: 0.3704[0m
[32m[2022-09-01 13:34:25,624] [    INFO][0m - loss: 2.95129852, learning_rate: 2.9404761904761905e-05, global_step: 150, interval_runtime: 2.4373, interval_samples_per_second: 3.282, interval_steps_per_second: 4.103, epoch: 0.3968[0m
[32m[2022-09-01 13:34:28,127] [    INFO][0m - loss: 3.39320297, learning_rate: 2.9365079365079366e-05, global_step: 160, interval_runtime: 2.5023, interval_samples_per_second: 3.197, interval_steps_per_second: 3.996, epoch: 0.4233[0m
[32m[2022-09-01 13:34:30,742] [    INFO][0m - loss: 2.75677376, learning_rate: 2.9325396825396827e-05, global_step: 170, interval_runtime: 2.6157, interval_samples_per_second: 3.058, interval_steps_per_second: 3.823, epoch: 0.4497[0m
[32m[2022-09-01 13:34:33,258] [    INFO][0m - loss: 3.03134766, learning_rate: 2.9285714285714284e-05, global_step: 180, interval_runtime: 2.5154, interval_samples_per_second: 3.18, interval_steps_per_second: 3.976, epoch: 0.4762[0m
[32m[2022-09-01 13:34:35,780] [    INFO][0m - loss: 2.54908752, learning_rate: 2.9246031746031748e-05, global_step: 190, interval_runtime: 2.5228, interval_samples_per_second: 3.171, interval_steps_per_second: 3.964, epoch: 0.5026[0m
[32m[2022-09-01 13:34:38,287] [    INFO][0m - loss: 2.41678467, learning_rate: 2.9206349206349206e-05, global_step: 200, interval_runtime: 2.5063, interval_samples_per_second: 3.192, interval_steps_per_second: 3.99, epoch: 0.5291[0m
[32m[2022-09-01 13:34:38,288] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 13:34:38,288] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-01 13:34:38,288] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 13:34:38,288] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 13:34:38,288] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-09-01 13:35:00,110] [    INFO][0m - eval_loss: 2.3631112575531006, eval_accuracy: 0.415877640203933, eval_runtime: 21.8215, eval_samples_per_second: 62.92, eval_steps_per_second: 1.971, epoch: 0.5291[0m
[32m[2022-09-01 13:35:00,111] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-09-01 13:35:00,111] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 13:35:01,721] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-09-01 13:35:01,722] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-09-01 13:35:06,859] [    INFO][0m - loss: 2.52508469, learning_rate: 2.9166666666666666e-05, global_step: 210, interval_runtime: 28.5729, interval_samples_per_second: 0.28, interval_steps_per_second: 0.35, epoch: 0.5556[0m
[32m[2022-09-01 13:35:09,595] [    INFO][0m - loss: 2.49837742, learning_rate: 2.9126984126984127e-05, global_step: 220, interval_runtime: 2.7348, interval_samples_per_second: 2.925, interval_steps_per_second: 3.657, epoch: 0.582[0m
[32m[2022-09-01 13:35:12,344] [    INFO][0m - loss: 2.33808537, learning_rate: 2.9087301587301588e-05, global_step: 230, interval_runtime: 2.7504, interval_samples_per_second: 2.909, interval_steps_per_second: 3.636, epoch: 0.6085[0m
[32m[2022-09-01 13:35:15,157] [    INFO][0m - loss: 2.34781284, learning_rate: 2.904761904761905e-05, global_step: 240, interval_runtime: 2.8127, interval_samples_per_second: 2.844, interval_steps_per_second: 3.555, epoch: 0.6349[0m
[32m[2022-09-01 13:35:17,943] [    INFO][0m - loss: 2.35443287, learning_rate: 2.900793650793651e-05, global_step: 250, interval_runtime: 2.786, interval_samples_per_second: 2.871, interval_steps_per_second: 3.589, epoch: 0.6614[0m
[32m[2022-09-01 13:35:20,796] [    INFO][0m - loss: 2.31504517, learning_rate: 2.8968253968253967e-05, global_step: 260, interval_runtime: 2.8525, interval_samples_per_second: 2.805, interval_steps_per_second: 3.506, epoch: 0.6878[0m
[32m[2022-09-01 13:35:23,644] [    INFO][0m - loss: 2.16567059, learning_rate: 2.892857142857143e-05, global_step: 270, interval_runtime: 2.8483, interval_samples_per_second: 2.809, interval_steps_per_second: 3.511, epoch: 0.7143[0m
[32m[2022-09-01 13:35:26,491] [    INFO][0m - loss: 2.58083649, learning_rate: 2.8888888888888888e-05, global_step: 280, interval_runtime: 2.8471, interval_samples_per_second: 2.81, interval_steps_per_second: 3.512, epoch: 0.7407[0m
[32m[2022-09-01 13:35:29,393] [    INFO][0m - loss: 2.24466209, learning_rate: 2.884920634920635e-05, global_step: 290, interval_runtime: 2.901, interval_samples_per_second: 2.758, interval_steps_per_second: 3.447, epoch: 0.7672[0m
[32m[2022-09-01 13:35:32,805] [    INFO][0m - loss: 2.08172607, learning_rate: 2.880952380952381e-05, global_step: 300, interval_runtime: 2.9207, interval_samples_per_second: 2.739, interval_steps_per_second: 3.424, epoch: 0.7937[0m
[32m[2022-09-01 13:35:32,805] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 13:35:32,806] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-01 13:35:32,806] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 13:35:32,806] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 13:35:32,806] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-09-01 13:35:59,665] [    INFO][0m - eval_loss: 2.2286927700042725, eval_accuracy: 0.45666423889293517, eval_runtime: 26.8582, eval_samples_per_second: 51.12, eval_steps_per_second: 1.601, epoch: 0.7937[0m
[32m[2022-09-01 13:35:59,665] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-09-01 13:35:59,665] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 13:36:00,803] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-09-01 13:36:00,803] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-09-01 13:36:05,922] [    INFO][0m - loss: 2.60380363, learning_rate: 2.876984126984127e-05, global_step: 310, interval_runtime: 33.6081, interval_samples_per_second: 0.238, interval_steps_per_second: 0.298, epoch: 0.8201[0m
[32m[2022-09-01 13:36:09,075] [    INFO][0m - loss: 1.99320679, learning_rate: 2.873015873015873e-05, global_step: 320, interval_runtime: 3.1536, interval_samples_per_second: 2.537, interval_steps_per_second: 3.171, epoch: 0.8466[0m
[32m[2022-09-01 13:36:12,186] [    INFO][0m - loss: 1.93896942, learning_rate: 2.8690476190476192e-05, global_step: 330, interval_runtime: 3.1113, interval_samples_per_second: 2.571, interval_steps_per_second: 3.214, epoch: 0.873[0m
[32m[2022-09-01 13:36:15,404] [    INFO][0m - loss: 2.37902832, learning_rate: 2.865079365079365e-05, global_step: 340, interval_runtime: 3.2174, interval_samples_per_second: 2.486, interval_steps_per_second: 3.108, epoch: 0.8995[0m
[32m[2022-09-01 13:36:18,591] [    INFO][0m - loss: 2.25477047, learning_rate: 2.8611111111111113e-05, global_step: 350, interval_runtime: 3.1874, interval_samples_per_second: 2.51, interval_steps_per_second: 3.137, epoch: 0.9259[0m
[32m[2022-09-01 13:36:22,017] [    INFO][0m - loss: 2.64529152, learning_rate: 2.857142857142857e-05, global_step: 360, interval_runtime: 3.2315, interval_samples_per_second: 2.476, interval_steps_per_second: 3.095, epoch: 0.9524[0m
[32m[2022-09-01 13:36:25,278] [    INFO][0m - loss: 2.2161377, learning_rate: 2.853174603174603e-05, global_step: 370, interval_runtime: 3.4556, interval_samples_per_second: 2.315, interval_steps_per_second: 2.894, epoch: 0.9788[0m
[32m[2022-09-01 13:36:28,755] [    INFO][0m - loss: 1.87632446, learning_rate: 2.8492063492063492e-05, global_step: 380, interval_runtime: 3.4769, interval_samples_per_second: 2.301, interval_steps_per_second: 2.876, epoch: 1.0053[0m
[32m[2022-09-01 13:36:31,965] [    INFO][0m - loss: 1.67587471, learning_rate: 2.8452380952380953e-05, global_step: 390, interval_runtime: 3.2094, interval_samples_per_second: 2.493, interval_steps_per_second: 3.116, epoch: 1.0317[0m
[32m[2022-09-01 13:36:35,376] [    INFO][0m - loss: 1.54146004, learning_rate: 2.8412698412698414e-05, global_step: 400, interval_runtime: 3.4113, interval_samples_per_second: 2.345, interval_steps_per_second: 2.931, epoch: 1.0582[0m
[32m[2022-09-01 13:36:35,377] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 13:36:35,377] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-01 13:36:35,377] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 13:36:35,377] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 13:36:35,377] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-09-01 13:37:06,968] [    INFO][0m - eval_loss: 2.062206268310547, eval_accuracy: 0.49745083758193737, eval_runtime: 31.5906, eval_samples_per_second: 43.462, eval_steps_per_second: 1.361, epoch: 1.0582[0m
[32m[2022-09-01 13:37:06,969] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-09-01 13:37:06,969] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 13:37:08,167] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-09-01 13:37:08,167] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-09-01 13:37:13,688] [    INFO][0m - loss: 1.66122322, learning_rate: 2.8373015873015875e-05, global_step: 410, interval_runtime: 38.3114, interval_samples_per_second: 0.209, interval_steps_per_second: 0.261, epoch: 1.0847[0m
[32m[2022-09-01 13:37:17,289] [    INFO][0m - loss: 1.67972012, learning_rate: 2.8333333333333332e-05, global_step: 420, interval_runtime: 3.6016, interval_samples_per_second: 2.221, interval_steps_per_second: 2.777, epoch: 1.1111[0m
[32m[2022-09-01 13:37:20,799] [    INFO][0m - loss: 1.49608517, learning_rate: 2.8293650793650796e-05, global_step: 430, interval_runtime: 3.5107, interval_samples_per_second: 2.279, interval_steps_per_second: 2.848, epoch: 1.1376[0m
[32m[2022-09-01 13:37:24,318] [    INFO][0m - loss: 1.71838837, learning_rate: 2.8253968253968253e-05, global_step: 440, interval_runtime: 3.5185, interval_samples_per_second: 2.274, interval_steps_per_second: 2.842, epoch: 1.164[0m
[32m[2022-09-01 13:37:27,883] [    INFO][0m - loss: 1.66214371, learning_rate: 2.8214285714285714e-05, global_step: 450, interval_runtime: 3.5644, interval_samples_per_second: 2.244, interval_steps_per_second: 2.806, epoch: 1.1905[0m
[32m[2022-09-01 13:37:31,463] [    INFO][0m - loss: 1.74500542, learning_rate: 2.8174603174603175e-05, global_step: 460, interval_runtime: 3.5802, interval_samples_per_second: 2.235, interval_steps_per_second: 2.793, epoch: 1.2169[0m
[32m[2022-09-01 13:37:35,144] [    INFO][0m - loss: 1.69198742, learning_rate: 2.8134920634920636e-05, global_step: 470, interval_runtime: 3.6813, interval_samples_per_second: 2.173, interval_steps_per_second: 2.716, epoch: 1.2434[0m
[32m[2022-09-01 13:37:38,769] [    INFO][0m - loss: 1.95043011, learning_rate: 2.8095238095238096e-05, global_step: 480, interval_runtime: 3.6247, interval_samples_per_second: 2.207, interval_steps_per_second: 2.759, epoch: 1.2698[0m
[32m[2022-09-01 13:37:42,343] [    INFO][0m - loss: 1.86698322, learning_rate: 2.8055555555555557e-05, global_step: 490, interval_runtime: 3.5742, interval_samples_per_second: 2.238, interval_steps_per_second: 2.798, epoch: 1.2963[0m
[32m[2022-09-01 13:37:46,038] [    INFO][0m - loss: 2.10264244, learning_rate: 2.8015873015873015e-05, global_step: 500, interval_runtime: 3.6951, interval_samples_per_second: 2.165, interval_steps_per_second: 2.706, epoch: 1.3228[0m
[32m[2022-09-01 13:37:46,039] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 13:37:46,040] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-01 13:37:46,040] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 13:37:46,040] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 13:37:46,040] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-09-01 13:38:22,198] [    INFO][0m - eval_loss: 2.04178786277771, eval_accuracy: 0.5003641660597232, eval_runtime: 36.1575, eval_samples_per_second: 37.973, eval_steps_per_second: 1.189, epoch: 1.3228[0m
[32m[2022-09-01 13:38:22,198] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-09-01 13:38:22,198] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 13:38:23,388] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-09-01 13:38:23,389] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-09-01 13:38:29,104] [    INFO][0m - loss: 2.02779598, learning_rate: 2.797619047619048e-05, global_step: 510, interval_runtime: 43.0654, interval_samples_per_second: 0.186, interval_steps_per_second: 0.232, epoch: 1.3492[0m
[32m[2022-09-01 13:38:32,960] [    INFO][0m - loss: 1.53408337, learning_rate: 2.7936507936507936e-05, global_step: 520, interval_runtime: 3.8566, interval_samples_per_second: 2.074, interval_steps_per_second: 2.593, epoch: 1.3757[0m
[32m[2022-09-01 13:38:36,788] [    INFO][0m - loss: 1.44315071, learning_rate: 2.7896825396825397e-05, global_step: 530, interval_runtime: 3.8282, interval_samples_per_second: 2.09, interval_steps_per_second: 2.612, epoch: 1.4021[0m
[32m[2022-09-01 13:38:40,645] [    INFO][0m - loss: 1.74369278, learning_rate: 2.7857142857142858e-05, global_step: 540, interval_runtime: 3.8573, interval_samples_per_second: 2.074, interval_steps_per_second: 2.593, epoch: 1.4286[0m
[32m[2022-09-01 13:38:44,516] [    INFO][0m - loss: 1.46689863, learning_rate: 2.781746031746032e-05, global_step: 550, interval_runtime: 3.8709, interval_samples_per_second: 2.067, interval_steps_per_second: 2.583, epoch: 1.455[0m
[32m[2022-09-01 13:38:48,546] [    INFO][0m - loss: 1.55664167, learning_rate: 2.777777777777778e-05, global_step: 560, interval_runtime: 4.029, interval_samples_per_second: 1.986, interval_steps_per_second: 2.482, epoch: 1.4815[0m
[32m[2022-09-01 13:38:52,531] [    INFO][0m - loss: 1.61420822, learning_rate: 2.773809523809524e-05, global_step: 570, interval_runtime: 3.9858, interval_samples_per_second: 2.007, interval_steps_per_second: 2.509, epoch: 1.5079[0m
[32m[2022-09-01 13:38:56,807] [    INFO][0m - loss: 1.75452557, learning_rate: 2.7698412698412697e-05, global_step: 580, interval_runtime: 3.9851, interval_samples_per_second: 2.007, interval_steps_per_second: 2.509, epoch: 1.5344[0m
[32m[2022-09-01 13:39:00,895] [    INFO][0m - loss: 2.04557571, learning_rate: 2.765873015873016e-05, global_step: 590, interval_runtime: 4.3787, interval_samples_per_second: 1.827, interval_steps_per_second: 2.284, epoch: 1.5608[0m
[32m[2022-09-01 13:39:04,972] [    INFO][0m - loss: 1.52363968, learning_rate: 2.761904761904762e-05, global_step: 600, interval_runtime: 4.077, interval_samples_per_second: 1.962, interval_steps_per_second: 2.453, epoch: 1.5873[0m
[32m[2022-09-01 13:39:04,972] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 13:39:04,973] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-01 13:39:04,973] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 13:39:04,973] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 13:39:04,973] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-09-01 13:39:46,199] [    INFO][0m - eval_loss: 1.9733470678329468, eval_accuracy: 0.5382374362709396, eval_runtime: 41.226, eval_samples_per_second: 33.304, eval_steps_per_second: 1.043, epoch: 1.5873[0m
[32m[2022-09-01 13:39:46,200] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-09-01 13:39:46,200] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 13:39:47,644] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-09-01 13:39:47,644] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-09-01 13:39:54,476] [    INFO][0m - loss: 1.60017548, learning_rate: 2.757936507936508e-05, global_step: 610, interval_runtime: 49.5033, interval_samples_per_second: 0.162, interval_steps_per_second: 0.202, epoch: 1.6138[0m
[32m[2022-09-01 13:39:58,630] [    INFO][0m - loss: 1.77236691, learning_rate: 2.753968253968254e-05, global_step: 620, interval_runtime: 4.1542, interval_samples_per_second: 1.926, interval_steps_per_second: 2.407, epoch: 1.6402[0m
[32m[2022-09-01 13:40:02,946] [    INFO][0m - loss: 1.56536636, learning_rate: 2.75e-05, global_step: 630, interval_runtime: 4.3163, interval_samples_per_second: 1.853, interval_steps_per_second: 2.317, epoch: 1.6667[0m
[32m[2022-09-01 13:40:07,223] [    INFO][0m - loss: 1.82349854, learning_rate: 2.7460317460317462e-05, global_step: 640, interval_runtime: 4.2766, interval_samples_per_second: 1.871, interval_steps_per_second: 2.338, epoch: 1.6931[0m
[32m[2022-09-01 13:40:11,538] [    INFO][0m - loss: 1.62199802, learning_rate: 2.7420634920634922e-05, global_step: 650, interval_runtime: 4.3157, interval_samples_per_second: 1.854, interval_steps_per_second: 2.317, epoch: 1.7196[0m
[32m[2022-09-01 13:40:15,865] [    INFO][0m - loss: 1.90429974, learning_rate: 2.738095238095238e-05, global_step: 660, interval_runtime: 4.3267, interval_samples_per_second: 1.849, interval_steps_per_second: 2.311, epoch: 1.746[0m
[32m[2022-09-01 13:40:20,187] [    INFO][0m - loss: 1.66166325, learning_rate: 2.7341269841269844e-05, global_step: 670, interval_runtime: 4.3221, interval_samples_per_second: 1.851, interval_steps_per_second: 2.314, epoch: 1.7725[0m
[32m[2022-09-01 13:40:24,566] [    INFO][0m - loss: 1.89825459, learning_rate: 2.73015873015873e-05, global_step: 680, interval_runtime: 4.3791, interval_samples_per_second: 1.827, interval_steps_per_second: 2.284, epoch: 1.7989[0m
[32m[2022-09-01 13:40:28,964] [    INFO][0m - loss: 1.61441631, learning_rate: 2.7261904761904762e-05, global_step: 690, interval_runtime: 4.3981, interval_samples_per_second: 1.819, interval_steps_per_second: 2.274, epoch: 1.8254[0m
[32m[2022-09-01 13:40:33,334] [    INFO][0m - loss: 1.78564072, learning_rate: 2.7222222222222223e-05, global_step: 700, interval_runtime: 4.3697, interval_samples_per_second: 1.831, interval_steps_per_second: 2.288, epoch: 1.8519[0m
[32m[2022-09-01 13:40:33,335] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 13:40:33,335] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-01 13:40:33,335] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 13:40:33,335] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 13:40:33,335] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-09-01 13:41:19,897] [    INFO][0m - eval_loss: 2.0556647777557373, eval_accuracy: 0.5091041514930809, eval_runtime: 46.5611, eval_samples_per_second: 29.488, eval_steps_per_second: 0.924, epoch: 1.8519[0m
[32m[2022-09-01 13:41:19,898] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-700[0m
[32m[2022-09-01 13:41:19,898] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 13:41:21,158] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-700/tokenizer_config.json[0m
[32m[2022-09-01 13:41:21,158] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-700/special_tokens_map.json[0m
[32m[2022-09-01 13:41:28,140] [    INFO][0m - loss: 1.86973743, learning_rate: 2.7182539682539684e-05, global_step: 710, interval_runtime: 54.8059, interval_samples_per_second: 0.146, interval_steps_per_second: 0.182, epoch: 1.8783[0m
[32m[2022-09-01 13:41:32,707] [    INFO][0m - loss: 1.68359928, learning_rate: 2.7142857142857144e-05, global_step: 720, interval_runtime: 4.5667, interval_samples_per_second: 1.752, interval_steps_per_second: 2.19, epoch: 1.9048[0m
[32m[2022-09-01 13:41:37,283] [    INFO][0m - loss: 1.83418961, learning_rate: 2.7103174603174605e-05, global_step: 730, interval_runtime: 4.5758, interval_samples_per_second: 1.748, interval_steps_per_second: 2.185, epoch: 1.9312[0m
[32m[2022-09-01 13:41:41,873] [    INFO][0m - loss: 1.62495308, learning_rate: 2.7063492063492062e-05, global_step: 740, interval_runtime: 4.5911, interval_samples_per_second: 1.742, interval_steps_per_second: 2.178, epoch: 1.9577[0m
[32m[2022-09-01 13:41:46,532] [    INFO][0m - loss: 1.70275421, learning_rate: 2.7023809523809527e-05, global_step: 750, interval_runtime: 4.6576, interval_samples_per_second: 1.718, interval_steps_per_second: 2.147, epoch: 1.9841[0m
[32m[2022-09-01 13:41:51,308] [    INFO][0m - loss: 1.31386147, learning_rate: 2.6984126984126984e-05, global_step: 760, interval_runtime: 4.7762, interval_samples_per_second: 1.675, interval_steps_per_second: 2.094, epoch: 2.0106[0m
[32m[2022-09-01 13:41:56,084] [    INFO][0m - loss: 0.92263613, learning_rate: 2.6944444444444445e-05, global_step: 770, interval_runtime: 4.7765, interval_samples_per_second: 1.675, interval_steps_per_second: 2.094, epoch: 2.037[0m
[32m[2022-09-01 13:42:00,813] [    INFO][0m - loss: 1.12101564, learning_rate: 2.6904761904761905e-05, global_step: 780, interval_runtime: 4.7285, interval_samples_per_second: 1.692, interval_steps_per_second: 2.115, epoch: 2.0635[0m
[32m[2022-09-01 13:42:05,579] [    INFO][0m - loss: 1.28849955, learning_rate: 2.6865079365079366e-05, global_step: 790, interval_runtime: 4.7662, interval_samples_per_second: 1.678, interval_steps_per_second: 2.098, epoch: 2.0899[0m
[32m[2022-09-01 13:42:10,279] [    INFO][0m - loss: 1.06935339, learning_rate: 2.6825396825396827e-05, global_step: 800, interval_runtime: 4.7007, interval_samples_per_second: 1.702, interval_steps_per_second: 2.127, epoch: 2.1164[0m
[32m[2022-09-01 13:42:10,280] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 13:42:10,280] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-01 13:42:10,280] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 13:42:10,280] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 13:42:10,280] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-09-01 13:43:01,879] [    INFO][0m - eval_loss: 2.0175814628601074, eval_accuracy: 0.5316824471959214, eval_runtime: 51.5986, eval_samples_per_second: 26.609, eval_steps_per_second: 0.833, epoch: 2.1164[0m
[32m[2022-09-01 13:43:01,880] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-800[0m
[32m[2022-09-01 13:43:01,880] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 13:43:03,011] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-800/tokenizer_config.json[0m
[32m[2022-09-01 13:43:03,012] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-800/special_tokens_map.json[0m
[32m[2022-09-01 13:43:10,312] [    INFO][0m - loss: 1.24826012, learning_rate: 2.6785714285714288e-05, global_step: 810, interval_runtime: 60.033, interval_samples_per_second: 0.133, interval_steps_per_second: 0.167, epoch: 2.1429[0m
[32m[2022-09-01 13:43:15,389] [    INFO][0m - loss: 1.20041742, learning_rate: 2.6746031746031745e-05, global_step: 820, interval_runtime: 5.0767, interval_samples_per_second: 1.576, interval_steps_per_second: 1.97, epoch: 2.1693[0m
[32m[2022-09-01 13:43:20,341] [    INFO][0m - loss: 1.07908545, learning_rate: 2.670634920634921e-05, global_step: 830, interval_runtime: 4.9516, interval_samples_per_second: 1.616, interval_steps_per_second: 2.02, epoch: 2.1958[0m
[32m[2022-09-01 13:43:25,343] [    INFO][0m - loss: 1.18945026, learning_rate: 2.6666666666666667e-05, global_step: 840, interval_runtime: 5.0027, interval_samples_per_second: 1.599, interval_steps_per_second: 1.999, epoch: 2.2222[0m
[32m[2022-09-01 13:43:30,406] [    INFO][0m - loss: 1.19809361, learning_rate: 2.6626984126984127e-05, global_step: 850, interval_runtime: 5.0631, interval_samples_per_second: 1.58, interval_steps_per_second: 1.975, epoch: 2.2487[0m
[32m[2022-09-01 13:43:35,513] [    INFO][0m - loss: 0.96757517, learning_rate: 2.6587301587301588e-05, global_step: 860, interval_runtime: 5.1066, interval_samples_per_second: 1.567, interval_steps_per_second: 1.958, epoch: 2.2751[0m
[32m[2022-09-01 13:43:40,485] [    INFO][0m - loss: 1.3595768, learning_rate: 2.654761904761905e-05, global_step: 870, interval_runtime: 4.9718, interval_samples_per_second: 1.609, interval_steps_per_second: 2.011, epoch: 2.3016[0m
[32m[2022-09-01 13:43:45,603] [    INFO][0m - loss: 1.13291693, learning_rate: 2.650793650793651e-05, global_step: 880, interval_runtime: 5.118, interval_samples_per_second: 1.563, interval_steps_per_second: 1.954, epoch: 2.328[0m
[32m[2022-09-01 13:43:51,015] [    INFO][0m - loss: 1.01690731, learning_rate: 2.646825396825397e-05, global_step: 890, interval_runtime: 5.4117, interval_samples_per_second: 1.478, interval_steps_per_second: 1.848, epoch: 2.3545[0m
[32m[2022-09-01 13:43:56,212] [    INFO][0m - loss: 0.9798111, learning_rate: 2.6428571428571428e-05, global_step: 900, interval_runtime: 5.1976, interval_samples_per_second: 1.539, interval_steps_per_second: 1.924, epoch: 2.381[0m
[32m[2022-09-01 13:43:56,213] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 13:43:56,213] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-01 13:43:56,213] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 13:43:56,213] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 13:43:56,213] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-09-01 13:44:52,606] [    INFO][0m - eval_loss: 2.0618247985839844, eval_accuracy: 0.5207574654042243, eval_runtime: 56.3923, eval_samples_per_second: 24.347, eval_steps_per_second: 0.763, epoch: 2.381[0m
[32m[2022-09-01 13:44:52,607] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-900[0m
[32m[2022-09-01 13:44:52,607] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 13:44:53,817] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-900/tokenizer_config.json[0m
[32m[2022-09-01 13:44:53,817] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-900/special_tokens_map.json[0m
[32m[2022-09-01 13:45:01,460] [    INFO][0m - loss: 1.31361866, learning_rate: 2.6388888888888892e-05, global_step: 910, interval_runtime: 65.2482, interval_samples_per_second: 0.123, interval_steps_per_second: 0.153, epoch: 2.4074[0m
[32m[2022-09-01 13:45:06,944] [    INFO][0m - loss: 0.97299271, learning_rate: 2.634920634920635e-05, global_step: 920, interval_runtime: 5.3474, interval_samples_per_second: 1.496, interval_steps_per_second: 1.87, epoch: 2.4339[0m
[32m[2022-09-01 13:45:12,272] [    INFO][0m - loss: 1.12180901, learning_rate: 2.630952380952381e-05, global_step: 930, interval_runtime: 5.4648, interval_samples_per_second: 1.464, interval_steps_per_second: 1.83, epoch: 2.4603[0m
[32m[2022-09-01 13:45:17,674] [    INFO][0m - loss: 1.0235918, learning_rate: 2.626984126984127e-05, global_step: 940, interval_runtime: 5.4014, interval_samples_per_second: 1.481, interval_steps_per_second: 1.851, epoch: 2.4868[0m
[32m[2022-09-01 13:45:23,069] [    INFO][0m - loss: 1.46989689, learning_rate: 2.623015873015873e-05, global_step: 950, interval_runtime: 5.3949, interval_samples_per_second: 1.483, interval_steps_per_second: 1.854, epoch: 2.5132[0m
[32m[2022-09-01 13:45:28,491] [    INFO][0m - loss: 1.22988987, learning_rate: 2.6190476190476192e-05, global_step: 960, interval_runtime: 5.4218, interval_samples_per_second: 1.476, interval_steps_per_second: 1.844, epoch: 2.5397[0m
[32m[2022-09-01 13:45:33,951] [    INFO][0m - loss: 1.22728024, learning_rate: 2.6150793650793653e-05, global_step: 970, interval_runtime: 5.4603, interval_samples_per_second: 1.465, interval_steps_per_second: 1.831, epoch: 2.5661[0m
[32m[2022-09-01 13:45:39,439] [    INFO][0m - loss: 1.06638012, learning_rate: 2.611111111111111e-05, global_step: 980, interval_runtime: 5.4875, interval_samples_per_second: 1.458, interval_steps_per_second: 1.822, epoch: 2.5926[0m
[32m[2022-09-01 13:45:44,944] [    INFO][0m - loss: 1.19151478, learning_rate: 2.607142857142857e-05, global_step: 990, interval_runtime: 5.5051, interval_samples_per_second: 1.453, interval_steps_per_second: 1.817, epoch: 2.619[0m
[32m[2022-09-01 13:45:50,472] [    INFO][0m - loss: 1.20874138, learning_rate: 2.6031746031746032e-05, global_step: 1000, interval_runtime: 5.5277, interval_samples_per_second: 1.447, interval_steps_per_second: 1.809, epoch: 2.6455[0m
[32m[2022-09-01 13:45:50,472] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-01 13:45:50,472] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-09-01 13:45:50,473] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 13:45:50,473] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 13:45:50,473] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-09-01 13:46:51,771] [    INFO][0m - eval_loss: 2.0866997241973877, eval_accuracy: 0.5214857975236707, eval_runtime: 61.2975, eval_samples_per_second: 22.399, eval_steps_per_second: 0.701, epoch: 2.6455[0m
[32m[2022-09-01 13:46:51,771] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1000[0m
[32m[2022-09-01 13:46:51,771] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 13:46:52,943] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1000/tokenizer_config.json[0m
[32m[2022-09-01 13:46:52,943] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1000/special_tokens_map.json[0m
[32m[2022-09-01 13:46:54,966] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-01 13:46:54,966] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-600 (score: 0.5382374362709396).[0m
[32m[2022-09-01 13:46:55,734] [    INFO][0m - train_runtime: 806.1919, train_samples_per_second: 75.019, train_steps_per_second: 9.377, train_loss: 2.1001904067993165, epoch: 2.6455[0m
[32m[2022-09-01 13:46:55,735] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-09-01 13:46:55,735] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-01 13:46:56,867] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-09-01 13:46:56,868] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-09-01 13:46:56,869] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-01 13:46:56,869] [    INFO][0m -   epoch                    =     2.6455[0m
[32m[2022-09-01 13:46:56,869] [    INFO][0m -   train_loss               =     2.1002[0m
[32m[2022-09-01 13:46:56,869] [    INFO][0m -   train_runtime            = 0:13:26.19[0m
[32m[2022-09-01 13:46:56,869] [    INFO][0m -   train_samples_per_second =     75.019[0m
[32m[2022-09-01 13:46:56,869] [    INFO][0m -   train_steps_per_second   =      9.377[0m
[32m[2022-09-01 13:46:56,874] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-01 13:46:56,874] [    INFO][0m -   Num examples = 1749[0m
[32m[2022-09-01 13:46:56,874] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 13:46:56,874] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 13:46:56,874] [    INFO][0m -   Total prediction steps = 55[0m
[32m[2022-09-01 13:48:16,354] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-01 13:48:16,355] [    INFO][0m -   test_accuracy           =      0.542[0m
[32m[2022-09-01 13:48:16,355] [    INFO][0m -   test_loss               =     1.9119[0m
[32m[2022-09-01 13:48:16,355] [    INFO][0m -   test_runtime            = 0:01:19.48[0m
[32m[2022-09-01 13:48:16,355] [    INFO][0m -   test_samples_per_second =     22.005[0m
[32m[2022-09-01 13:48:16,355] [    INFO][0m -   test_steps_per_second   =      0.692[0m
[32m[2022-09-01 13:48:16,356] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-01 13:48:16,356] [    INFO][0m -   Num examples = 2600[0m
[32m[2022-09-01 13:48:16,356] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-09-01 13:48:16,356] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-09-01 13:48:16,356] [    INFO][0m -   Total prediction steps = 82[0m
[32m[2022-09-01 13:50:26,327] [    INFO][0m - Predictions for iflytekf saved to ./fewclue_submit_examples.[0m
run.sh: line 64: --model_name_or_path: command not found
