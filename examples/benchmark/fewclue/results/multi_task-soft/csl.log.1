[33m[2022-09-05 15:56:33,613] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-09-05 15:56:33,614] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-05 15:56:33,614] [    INFO][0m - ============================================================[0m
[32m[2022-09-05 15:56:33,614] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-05 15:56:33,614] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-05 15:56:33,614] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-05 15:56:33,614] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-05 15:56:33,614] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-05 15:56:33,614] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-05 15:56:33,614] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-09-05 15:56:33,615] [    INFO][0m - [0m
[32m[2022-09-05 15:56:33,615] [    INFO][0m - ============================================================[0m
[32m[2022-09-05 15:56:33,615] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-05 15:56:33,615] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-05 15:56:33,615] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-05 15:56:33,615] [    INFO][0m - prompt                        :[0m
[32m[2022-09-05 15:56:33,615] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-05 15:56:33,615] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-05 15:56:33,615] [    INFO][0m - task_name                     :csl[0m
[32m[2022-09-05 15:56:33,615] [    INFO][0m - [0m
[32m[2022-09-05 15:56:33,616] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0905 15:56:33.617455 42880 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0905 15:56:33.622295 42880 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-05 15:56:38,564] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-09-05 15:56:38,589] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-09-05 15:56:38,589] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[32m[2022-09-05 15:56:38,590] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚ÄùËøôÂè•ËØù‰∏≠ËÆ®ËÆ∫ÁöÑÂÖ≥ÈîÆËØç'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'ÂåÖÊã¨‚Äú'}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': '‚Äù'}][0m
2022-09-05 15:56:38,593 INFO [download.py:119] unique_endpoints {''}
[32m[2022-09-05 15:56:39,126] [    INFO][0m - ============================================================[0m
[32m[2022-09-05 15:56:39,126] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-05 15:56:39,126] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-05 15:56:39,127] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-05 15:56:39,127] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-05 15:56:39,127] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-05 15:56:39,127] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-05 15:56:39,127] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-05 15:56:39,127] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-05 15:56:39,127] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-05 15:56:39,127] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-05 15:56:39,127] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-05 15:56:39,127] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-05 15:56:39,127] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-05 15:56:39,127] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-05 15:56:39,128] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-05 15:56:39,128] [    INFO][0m - do_predict                    :False[0m
[32m[2022-09-05 15:56:39,128] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-05 15:56:39,128] [    INFO][0m - eval_batch_size               :8[0m
[32m[2022-09-05 15:56:39,128] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-09-05 15:56:39,128] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-09-05 15:56:39,128] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-05 15:56:39,128] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-05 15:56:39,128] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-05 15:56:39,128] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-05 15:56:39,128] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-05 15:56:39,128] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-05 15:56:39,129] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-05 15:56:39,129] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-05 15:56:39,129] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-05 15:56:39,129] [    INFO][0m - learning_rate                 :1e-05[0m
[32m[2022-09-05 15:56:39,129] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-05 15:56:39,129] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-05 15:56:39,129] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-05 15:56:39,129] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-05 15:56:39,129] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-05 15:56:39,129] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-05 15:56:39,129] [    INFO][0m - logging_dir                   :./checkpoints_csl/runs/Sep05_15-56-33_instance-3bwob41y-01[0m
[32m[2022-09-05 15:56:39,129] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-05 15:56:39,130] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-05 15:56:39,130] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-05 15:56:39,130] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-05 15:56:39,130] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-05 15:56:39,130] [    INFO][0m - max_seq_length                :320[0m
[32m[2022-09-05 15:56:39,130] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-05 15:56:39,130] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-05 15:56:39,130] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-05 15:56:39,130] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-05 15:56:39,130] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-09-05 15:56:39,130] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-05 15:56:39,130] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-05 15:56:39,131] [    INFO][0m - output_dir                    :./checkpoints_csl/[0m
[32m[2022-09-05 15:56:39,131] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-05 15:56:39,131] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-05 15:56:39,131] [    INFO][0m - per_device_eval_batch_size    :8[0m
[32m[2022-09-05 15:56:39,131] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-09-05 15:56:39,131] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-05 15:56:39,131] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-05 15:56:39,131] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-05 15:56:39,131] [    INFO][0m - ppt_learning_rate             :0.0001[0m
[32m[2022-09-05 15:56:39,131] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-05 15:56:39,131] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-05 15:56:39,131] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-05 15:56:39,131] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-05 15:56:39,132] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-05 15:56:39,132] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-05 15:56:39,132] [    INFO][0m - run_name                      :./checkpoints_csl/[0m
[32m[2022-09-05 15:56:39,132] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-05 15:56:39,132] [    INFO][0m - save_steps                    :100[0m
[32m[2022-09-05 15:56:39,132] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-09-05 15:56:39,132] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-05 15:56:39,132] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-05 15:56:39,132] [    INFO][0m - seed                          :42[0m
[32m[2022-09-05 15:56:39,132] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-05 15:56:39,132] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-05 15:56:39,132] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-05 15:56:39,132] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-09-05 15:56:39,132] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-05 15:56:39,133] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-05 15:56:39,133] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-05 15:56:39,133] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-05 15:56:39,133] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-05 15:56:39,133] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-05 15:56:39,133] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-05 15:56:39,133] [    INFO][0m - [0m
[32m[2022-09-05 15:56:39,135] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-05 15:56:39,135] [    INFO][0m -   Num examples = 2642[0m
[32m[2022-09-05 15:56:39,135] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-09-05 15:56:39,135] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-09-05 15:56:39,135] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-09-05 15:56:39,135] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-05 15:56:39,135] [    INFO][0m -   Total optimization steps = 16550.0[0m
[32m[2022-09-05 15:56:39,135] [    INFO][0m -   Total num train samples = 132100[0m
[32m[2022-09-05 15:56:41,953] [    INFO][0m - loss: 3.30270386, learning_rate: 9.993957703927493e-06, global_step: 10, interval_runtime: 2.817, interval_samples_per_second: 2.84, interval_steps_per_second: 3.55, epoch: 0.0302[0m
[32m[2022-09-05 15:56:43,751] [    INFO][0m - loss: 1.31350355, learning_rate: 9.987915407854986e-06, global_step: 20, interval_runtime: 1.7978, interval_samples_per_second: 4.45, interval_steps_per_second: 5.562, epoch: 0.0604[0m
[32m[2022-09-05 15:56:45,547] [    INFO][0m - loss: 0.77695408, learning_rate: 9.981873111782479e-06, global_step: 30, interval_runtime: 1.7962, interval_samples_per_second: 4.454, interval_steps_per_second: 5.567, epoch: 0.0906[0m
[32m[2022-09-05 15:56:47,347] [    INFO][0m - loss: 0.85384693, learning_rate: 9.975830815709971e-06, global_step: 40, interval_runtime: 1.8002, interval_samples_per_second: 4.444, interval_steps_per_second: 5.555, epoch: 0.1208[0m
[32m[2022-09-05 15:56:49,148] [    INFO][0m - loss: 0.81423674, learning_rate: 9.969788519637464e-06, global_step: 50, interval_runtime: 1.8006, interval_samples_per_second: 4.443, interval_steps_per_second: 5.554, epoch: 0.1511[0m
[32m[2022-09-05 15:56:50,945] [    INFO][0m - loss: 0.79205408, learning_rate: 9.963746223564955e-06, global_step: 60, interval_runtime: 1.7963, interval_samples_per_second: 4.454, interval_steps_per_second: 5.567, epoch: 0.1813[0m
[32m[2022-09-05 15:56:52,750] [    INFO][0m - loss: 0.65558949, learning_rate: 9.957703927492449e-06, global_step: 70, interval_runtime: 1.8055, interval_samples_per_second: 4.431, interval_steps_per_second: 5.539, epoch: 0.2115[0m
[32m[2022-09-05 15:56:54,553] [    INFO][0m - loss: 0.70856256, learning_rate: 9.95166163141994e-06, global_step: 80, interval_runtime: 1.8034, interval_samples_per_second: 4.436, interval_steps_per_second: 5.545, epoch: 0.2417[0m
[32m[2022-09-05 15:56:56,350] [    INFO][0m - loss: 0.69168663, learning_rate: 9.945619335347432e-06, global_step: 90, interval_runtime: 1.7967, interval_samples_per_second: 4.453, interval_steps_per_second: 5.566, epoch: 0.2719[0m
[32m[2022-09-05 15:56:58,155] [    INFO][0m - loss: 0.72668571, learning_rate: 9.939577039274926e-06, global_step: 100, interval_runtime: 1.8038, interval_samples_per_second: 4.435, interval_steps_per_second: 5.544, epoch: 0.3021[0m
[32m[2022-09-05 15:56:58,156] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 15:56:58,156] [    INFO][0m -   Num examples = 2712[0m
[32m[2022-09-05 15:56:58,156] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 15:56:58,156] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 15:56:58,157] [    INFO][0m -   Total prediction steps = 339[0m
[32m[2022-09-05 15:57:41,552] [    INFO][0m - eval_loss: 0.8294533491134644, eval_accuracy: 0.4749262536873156, eval_runtime: 43.3944, eval_samples_per_second: 62.497, eval_steps_per_second: 7.812, epoch: 0.3021[0m
[32m[2022-09-05 15:57:41,601] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-100[0m
[32m[2022-09-05 15:57:41,602] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 15:57:44,690] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-05 15:57:44,691] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-05 15:57:51,533] [    INFO][0m - loss: 0.62155123, learning_rate: 9.933534743202417e-06, global_step: 110, interval_runtime: 53.3794, interval_samples_per_second: 0.15, interval_steps_per_second: 0.187, epoch: 0.3323[0m
[32m[2022-09-05 15:57:53,332] [    INFO][0m - loss: 0.819736, learning_rate: 9.92749244712991e-06, global_step: 120, interval_runtime: 1.7993, interval_samples_per_second: 4.446, interval_steps_per_second: 5.558, epoch: 0.3625[0m
[32m[2022-09-05 15:57:55,126] [    INFO][0m - loss: 0.59066463, learning_rate: 9.921450151057402e-06, global_step: 130, interval_runtime: 1.7935, interval_samples_per_second: 4.461, interval_steps_per_second: 5.576, epoch: 0.3927[0m
[32m[2022-09-05 15:57:56,926] [    INFO][0m - loss: 0.89734659, learning_rate: 9.915407854984895e-06, global_step: 140, interval_runtime: 1.7999, interval_samples_per_second: 4.445, interval_steps_per_second: 5.556, epoch: 0.423[0m
[32m[2022-09-05 15:57:58,726] [    INFO][0m - loss: 0.67300386, learning_rate: 9.909365558912388e-06, global_step: 150, interval_runtime: 1.8, interval_samples_per_second: 4.444, interval_steps_per_second: 5.555, epoch: 0.4532[0m
[32m[2022-09-05 15:58:00,521] [    INFO][0m - loss: 0.55959692, learning_rate: 9.90332326283988e-06, global_step: 160, interval_runtime: 1.795, interval_samples_per_second: 4.457, interval_steps_per_second: 5.571, epoch: 0.4834[0m
[32m[2022-09-05 15:58:02,324] [    INFO][0m - loss: 0.64075699, learning_rate: 9.897280966767373e-06, global_step: 170, interval_runtime: 1.8034, interval_samples_per_second: 4.436, interval_steps_per_second: 5.545, epoch: 0.5136[0m
[32m[2022-09-05 15:58:04,128] [    INFO][0m - loss: 0.57125921, learning_rate: 9.891238670694865e-06, global_step: 180, interval_runtime: 1.804, interval_samples_per_second: 4.434, interval_steps_per_second: 5.543, epoch: 0.5438[0m
[32m[2022-09-05 15:58:05,932] [    INFO][0m - loss: 0.55551214, learning_rate: 9.885196374622358e-06, global_step: 190, interval_runtime: 1.8038, interval_samples_per_second: 4.435, interval_steps_per_second: 5.544, epoch: 0.574[0m
[32m[2022-09-05 15:58:07,733] [    INFO][0m - loss: 0.57391138, learning_rate: 9.87915407854985e-06, global_step: 200, interval_runtime: 1.8007, interval_samples_per_second: 4.443, interval_steps_per_second: 5.553, epoch: 0.6042[0m
[32m[2022-09-05 15:58:07,733] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 15:58:07,733] [    INFO][0m -   Num examples = 2712[0m
[32m[2022-09-05 15:58:07,733] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 15:58:07,733] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 15:58:07,733] [    INFO][0m -   Total prediction steps = 339[0m
[32m[2022-09-05 15:58:50,962] [    INFO][0m - eval_loss: 1.1637420654296875, eval_accuracy: 0.4749262536873156, eval_runtime: 43.2282, eval_samples_per_second: 62.737, eval_steps_per_second: 7.842, epoch: 0.6042[0m
[32m[2022-09-05 15:58:51,000] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-200[0m
[32m[2022-09-05 15:58:51,000] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 15:58:53,972] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-200/tokenizer_config.json[0m
[32m[2022-09-05 15:58:53,973] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-200/special_tokens_map.json[0m
[32m[2022-09-05 15:59:00,841] [    INFO][0m - loss: 0.55832586, learning_rate: 9.873111782477343e-06, global_step: 210, interval_runtime: 53.1079, interval_samples_per_second: 0.151, interval_steps_per_second: 0.188, epoch: 0.6344[0m
[32m[2022-09-05 15:59:02,661] [    INFO][0m - loss: 0.4437922, learning_rate: 9.867069486404834e-06, global_step: 220, interval_runtime: 1.8201, interval_samples_per_second: 4.395, interval_steps_per_second: 5.494, epoch: 0.6647[0m
[32m[2022-09-05 15:59:04,472] [    INFO][0m - loss: 0.41706214, learning_rate: 9.861027190332328e-06, global_step: 230, interval_runtime: 1.8113, interval_samples_per_second: 4.417, interval_steps_per_second: 5.521, epoch: 0.6949[0m
[32m[2022-09-05 15:59:06,277] [    INFO][0m - loss: 0.44668651, learning_rate: 9.854984894259819e-06, global_step: 240, interval_runtime: 1.8051, interval_samples_per_second: 4.432, interval_steps_per_second: 5.54, epoch: 0.7251[0m
[32m[2022-09-05 15:59:08,083] [    INFO][0m - loss: 0.41860447, learning_rate: 9.848942598187312e-06, global_step: 250, interval_runtime: 1.8059, interval_samples_per_second: 4.43, interval_steps_per_second: 5.537, epoch: 0.7553[0m
[32m[2022-09-05 15:59:09,898] [    INFO][0m - loss: 0.29085119, learning_rate: 9.842900302114804e-06, global_step: 260, interval_runtime: 1.8145, interval_samples_per_second: 4.409, interval_steps_per_second: 5.511, epoch: 0.7855[0m
[32m[2022-09-05 15:59:11,710] [    INFO][0m - loss: 0.35807078, learning_rate: 9.836858006042297e-06, global_step: 270, interval_runtime: 1.8127, interval_samples_per_second: 4.413, interval_steps_per_second: 5.517, epoch: 0.8157[0m
[32m[2022-09-05 15:59:13,513] [    INFO][0m - loss: 0.21641219, learning_rate: 9.83081570996979e-06, global_step: 280, interval_runtime: 1.8027, interval_samples_per_second: 4.438, interval_steps_per_second: 5.547, epoch: 0.8459[0m
[32m[2022-09-05 15:59:15,325] [    INFO][0m - loss: 0.2642792, learning_rate: 9.824773413897282e-06, global_step: 290, interval_runtime: 1.8121, interval_samples_per_second: 4.415, interval_steps_per_second: 5.518, epoch: 0.8761[0m
[32m[2022-09-05 15:59:17,134] [    INFO][0m - loss: 0.51862917, learning_rate: 9.818731117824774e-06, global_step: 300, interval_runtime: 1.8091, interval_samples_per_second: 4.422, interval_steps_per_second: 5.528, epoch: 0.9063[0m
[32m[2022-09-05 15:59:17,134] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 15:59:17,135] [    INFO][0m -   Num examples = 2712[0m
[32m[2022-09-05 15:59:17,135] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 15:59:17,135] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 15:59:17,135] [    INFO][0m -   Total prediction steps = 339[0m
[32m[2022-09-05 16:00:00,319] [    INFO][0m - eval_loss: 1.7567931413650513, eval_accuracy: 0.5910766961651918, eval_runtime: 43.1836, eval_samples_per_second: 62.802, eval_steps_per_second: 7.85, epoch: 0.9063[0m
[32m[2022-09-05 16:00:00,370] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-300[0m
[32m[2022-09-05 16:00:00,370] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 16:00:03,434] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-300/tokenizer_config.json[0m
[32m[2022-09-05 16:00:03,752] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-300/special_tokens_map.json[0m
[32m[2022-09-05 16:00:11,482] [    INFO][0m - loss: 0.27535229, learning_rate: 9.812688821752267e-06, global_step: 310, interval_runtime: 54.3474, interval_samples_per_second: 0.147, interval_steps_per_second: 0.184, epoch: 0.9366[0m
[32m[2022-09-05 16:00:13,281] [    INFO][0m - loss: 0.27381067, learning_rate: 9.80664652567976e-06, global_step: 320, interval_runtime: 1.7995, interval_samples_per_second: 4.446, interval_steps_per_second: 5.557, epoch: 0.9668[0m
[32m[2022-09-05 16:00:15,074] [    INFO][0m - loss: 0.2929291, learning_rate: 9.80060422960725e-06, global_step: 330, interval_runtime: 1.7935, interval_samples_per_second: 4.461, interval_steps_per_second: 5.576, epoch: 0.997[0m
[32m[2022-09-05 16:00:16,871] [    INFO][0m - loss: 0.20830803, learning_rate: 9.794561933534745e-06, global_step: 340, interval_runtime: 1.7965, interval_samples_per_second: 4.453, interval_steps_per_second: 5.566, epoch: 1.0272[0m
[32m[2022-09-05 16:00:18,673] [    INFO][0m - loss: 0.06705871, learning_rate: 9.788519637462236e-06, global_step: 350, interval_runtime: 1.8016, interval_samples_per_second: 4.44, interval_steps_per_second: 5.551, epoch: 1.0574[0m
[32m[2022-09-05 16:00:20,476] [    INFO][0m - loss: 0.19850962, learning_rate: 9.782477341389728e-06, global_step: 360, interval_runtime: 1.8032, interval_samples_per_second: 4.436, interval_steps_per_second: 5.546, epoch: 1.0876[0m
[32m[2022-09-05 16:00:22,292] [    INFO][0m - loss: 0.16336215, learning_rate: 9.776435045317222e-06, global_step: 370, interval_runtime: 1.8163, interval_samples_per_second: 4.404, interval_steps_per_second: 5.506, epoch: 1.1178[0m
[32m[2022-09-05 16:00:24,100] [    INFO][0m - loss: 0.14859191, learning_rate: 9.770392749244713e-06, global_step: 380, interval_runtime: 1.8075, interval_samples_per_second: 4.426, interval_steps_per_second: 5.533, epoch: 1.148[0m
[32m[2022-09-05 16:00:25,913] [    INFO][0m - loss: 0.08845034, learning_rate: 9.764350453172206e-06, global_step: 390, interval_runtime: 1.8128, interval_samples_per_second: 4.413, interval_steps_per_second: 5.516, epoch: 1.1782[0m
[32m[2022-09-05 16:00:27,717] [    INFO][0m - loss: 0.02508675, learning_rate: 9.758308157099698e-06, global_step: 400, interval_runtime: 1.805, interval_samples_per_second: 4.432, interval_steps_per_second: 5.54, epoch: 1.2085[0m
[32m[2022-09-05 16:00:27,718] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 16:00:27,718] [    INFO][0m -   Num examples = 2712[0m
[32m[2022-09-05 16:00:27,718] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 16:00:27,718] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 16:00:27,718] [    INFO][0m -   Total prediction steps = 339[0m
[32m[2022-09-05 16:01:10,934] [    INFO][0m - eval_loss: 4.429725170135498, eval_accuracy: 0.47676991150442477, eval_runtime: 43.2153, eval_samples_per_second: 62.756, eval_steps_per_second: 7.844, epoch: 1.2085[0m
[32m[2022-09-05 16:01:10,979] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-400[0m
[32m[2022-09-05 16:01:10,979] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 16:01:14,031] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-400/tokenizer_config.json[0m
[32m[2022-09-05 16:01:14,032] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-400/special_tokens_map.json[0m
[32m[2022-09-05 16:01:20,881] [    INFO][0m - loss: 0.04494065, learning_rate: 9.752265861027191e-06, global_step: 410, interval_runtime: 53.1633, interval_samples_per_second: 0.15, interval_steps_per_second: 0.188, epoch: 1.2387[0m
[32m[2022-09-05 16:01:22,688] [    INFO][0m - loss: 0.0707332, learning_rate: 9.746223564954684e-06, global_step: 420, interval_runtime: 1.8066, interval_samples_per_second: 4.428, interval_steps_per_second: 5.535, epoch: 1.2689[0m
[32m[2022-09-05 16:01:24,495] [    INFO][0m - loss: 0.02370382, learning_rate: 9.740181268882176e-06, global_step: 430, interval_runtime: 1.8078, interval_samples_per_second: 4.425, interval_steps_per_second: 5.532, epoch: 1.2991[0m
[32m[2022-09-05 16:01:26,303] [    INFO][0m - loss: 0.0147947, learning_rate: 9.734138972809669e-06, global_step: 440, interval_runtime: 1.8075, interval_samples_per_second: 4.426, interval_steps_per_second: 5.532, epoch: 1.3293[0m
[32m[2022-09-05 16:01:28,104] [    INFO][0m - loss: 0.00100458, learning_rate: 9.728096676737161e-06, global_step: 450, interval_runtime: 1.8014, interval_samples_per_second: 4.441, interval_steps_per_second: 5.551, epoch: 1.3595[0m
[32m[2022-09-05 16:01:29,910] [    INFO][0m - loss: 0.00216297, learning_rate: 9.722054380664654e-06, global_step: 460, interval_runtime: 1.8058, interval_samples_per_second: 4.43, interval_steps_per_second: 5.538, epoch: 1.3897[0m
[32m[2022-09-05 16:01:31,715] [    INFO][0m - loss: 0.01391832, learning_rate: 9.716012084592146e-06, global_step: 470, interval_runtime: 1.8053, interval_samples_per_second: 4.431, interval_steps_per_second: 5.539, epoch: 1.4199[0m
[32m[2022-09-05 16:01:33,522] [    INFO][0m - loss: 0.05138293, learning_rate: 9.709969788519639e-06, global_step: 480, interval_runtime: 1.8064, interval_samples_per_second: 4.429, interval_steps_per_second: 5.536, epoch: 1.4502[0m
[32m[2022-09-05 16:01:35,328] [    INFO][0m - loss: 0.06321473, learning_rate: 9.70392749244713e-06, global_step: 490, interval_runtime: 1.8066, interval_samples_per_second: 4.428, interval_steps_per_second: 5.535, epoch: 1.4804[0m
[32m[2022-09-05 16:01:37,133] [    INFO][0m - loss: 0.03219979, learning_rate: 9.697885196374624e-06, global_step: 500, interval_runtime: 1.8045, interval_samples_per_second: 4.433, interval_steps_per_second: 5.542, epoch: 1.5106[0m
[32m[2022-09-05 16:01:37,133] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 16:01:37,133] [    INFO][0m -   Num examples = 2712[0m
[32m[2022-09-05 16:01:37,133] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 16:01:37,133] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 16:01:37,134] [    INFO][0m -   Total prediction steps = 339[0m
[32m[2022-09-05 16:02:20,554] [    INFO][0m - eval_loss: 7.905418395996094, eval_accuracy: 0.4450589970501475, eval_runtime: 43.4204, eval_samples_per_second: 62.459, eval_steps_per_second: 7.807, epoch: 1.5106[0m
[32m[2022-09-05 16:02:20,610] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-500[0m
[32m[2022-09-05 16:02:20,611] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 16:02:23,988] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-500/tokenizer_config.json[0m
[32m[2022-09-05 16:02:23,989] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-500/special_tokens_map.json[0m
[32m[2022-09-05 16:02:31,049] [    INFO][0m - loss: 0.19802091, learning_rate: 9.691842900302115e-06, global_step: 510, interval_runtime: 53.916, interval_samples_per_second: 0.148, interval_steps_per_second: 0.185, epoch: 1.5408[0m
[32m[2022-09-05 16:02:32,854] [    INFO][0m - loss: 0.12357365, learning_rate: 9.685800604229607e-06, global_step: 520, interval_runtime: 1.8046, interval_samples_per_second: 4.433, interval_steps_per_second: 5.541, epoch: 1.571[0m
[32m[2022-09-05 16:02:34,678] [    INFO][0m - loss: 0.06620614, learning_rate: 9.6797583081571e-06, global_step: 530, interval_runtime: 1.8243, interval_samples_per_second: 4.385, interval_steps_per_second: 5.482, epoch: 1.6012[0m
[32m[2022-09-05 16:02:36,489] [    INFO][0m - loss: 0.08691667, learning_rate: 9.673716012084593e-06, global_step: 540, interval_runtime: 1.8107, interval_samples_per_second: 4.418, interval_steps_per_second: 5.523, epoch: 1.6314[0m
[32m[2022-09-05 16:02:38,299] [    INFO][0m - loss: 0.00015695, learning_rate: 9.667673716012085e-06, global_step: 550, interval_runtime: 1.8107, interval_samples_per_second: 4.418, interval_steps_per_second: 5.523, epoch: 1.6616[0m
[32m[2022-09-05 16:02:40,106] [    INFO][0m - loss: 0.00512284, learning_rate: 9.661631419939578e-06, global_step: 560, interval_runtime: 1.8066, interval_samples_per_second: 4.428, interval_steps_per_second: 5.535, epoch: 1.6918[0m
[32m[2022-09-05 16:02:41,918] [    INFO][0m - loss: 0.1097901, learning_rate: 9.65558912386707e-06, global_step: 570, interval_runtime: 1.8121, interval_samples_per_second: 4.415, interval_steps_per_second: 5.518, epoch: 1.7221[0m
[32m[2022-09-05 16:02:43,728] [    INFO][0m - loss: 0.00428822, learning_rate: 9.649546827794563e-06, global_step: 580, interval_runtime: 1.8098, interval_samples_per_second: 4.42, interval_steps_per_second: 5.526, epoch: 1.7523[0m
[32m[2022-09-05 16:02:45,533] [    INFO][0m - loss: 0.00221135, learning_rate: 9.643504531722055e-06, global_step: 590, interval_runtime: 1.8049, interval_samples_per_second: 4.432, interval_steps_per_second: 5.54, epoch: 1.7825[0m
[32m[2022-09-05 16:02:47,338] [    INFO][0m - loss: 0.16151733, learning_rate: 9.637462235649548e-06, global_step: 600, interval_runtime: 1.8053, interval_samples_per_second: 4.431, interval_steps_per_second: 5.539, epoch: 1.8127[0m
[32m[2022-09-05 16:02:47,338] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 16:02:47,338] [    INFO][0m -   Num examples = 2712[0m
[32m[2022-09-05 16:02:47,339] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 16:02:47,339] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 16:02:47,339] [    INFO][0m -   Total prediction steps = 339[0m
[32m[2022-09-05 16:03:30,660] [    INFO][0m - eval_loss: 7.0029988288879395, eval_accuracy: 0.47640117994100295, eval_runtime: 43.3203, eval_samples_per_second: 62.603, eval_steps_per_second: 7.825, epoch: 1.8127[0m
[32m[2022-09-05 16:03:30,716] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-600[0m
[32m[2022-09-05 16:03:30,716] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 16:03:33,780] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-600/tokenizer_config.json[0m
[32m[2022-09-05 16:03:33,781] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-600/special_tokens_map.json[0m
[32m[2022-09-05 16:03:40,999] [    INFO][0m - loss: 0.00020798, learning_rate: 9.63141993957704e-06, global_step: 610, interval_runtime: 53.6609, interval_samples_per_second: 0.149, interval_steps_per_second: 0.186, epoch: 1.8429[0m
[32m[2022-09-05 16:03:42,803] [    INFO][0m - loss: 0.00010714, learning_rate: 9.625377643504531e-06, global_step: 620, interval_runtime: 1.8039, interval_samples_per_second: 4.435, interval_steps_per_second: 5.543, epoch: 1.8731[0m
[32m[2022-09-05 16:03:44,609] [    INFO][0m - loss: 4.44e-05, learning_rate: 9.619335347432026e-06, global_step: 630, interval_runtime: 1.8066, interval_samples_per_second: 4.428, interval_steps_per_second: 5.535, epoch: 1.9033[0m
[32m[2022-09-05 16:03:46,416] [    INFO][0m - loss: 0.02587851, learning_rate: 9.613293051359517e-06, global_step: 640, interval_runtime: 1.8067, interval_samples_per_second: 4.428, interval_steps_per_second: 5.535, epoch: 1.9335[0m
[32m[2022-09-05 16:03:48,221] [    INFO][0m - loss: 0.09101138, learning_rate: 9.60725075528701e-06, global_step: 650, interval_runtime: 1.8046, interval_samples_per_second: 4.433, interval_steps_per_second: 5.541, epoch: 1.9637[0m
[32m[2022-09-05 16:03:50,034] [    INFO][0m - loss: 0.00170547, learning_rate: 9.601208459214503e-06, global_step: 660, interval_runtime: 1.8127, interval_samples_per_second: 4.413, interval_steps_per_second: 5.517, epoch: 1.994[0m
[32m[2022-09-05 16:03:51,820] [    INFO][0m - loss: 4.859e-05, learning_rate: 9.595166163141994e-06, global_step: 670, interval_runtime: 1.7863, interval_samples_per_second: 4.478, interval_steps_per_second: 5.598, epoch: 2.0242[0m
[32m[2022-09-05 16:03:53,623] [    INFO][0m - loss: 5.765e-05, learning_rate: 9.589123867069487e-06, global_step: 680, interval_runtime: 1.8035, interval_samples_per_second: 4.436, interval_steps_per_second: 5.545, epoch: 2.0544[0m
[32m[2022-09-05 16:03:55,427] [    INFO][0m - loss: 7.153e-05, learning_rate: 9.58308157099698e-06, global_step: 690, interval_runtime: 1.8038, interval_samples_per_second: 4.435, interval_steps_per_second: 5.544, epoch: 2.0846[0m
[32m[2022-09-05 16:03:57,243] [    INFO][0m - loss: 4.531e-05, learning_rate: 9.577039274924472e-06, global_step: 700, interval_runtime: 1.8156, interval_samples_per_second: 4.406, interval_steps_per_second: 5.508, epoch: 2.1148[0m
[32m[2022-09-05 16:03:57,243] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 16:03:57,243] [    INFO][0m -   Num examples = 2712[0m
[32m[2022-09-05 16:03:57,243] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 16:03:57,243] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 16:03:57,243] [    INFO][0m -   Total prediction steps = 339[0m
[32m[2022-09-05 16:04:40,603] [    INFO][0m - eval_loss: 7.0246734619140625, eval_accuracy: 0.5221238938053098, eval_runtime: 43.3596, eval_samples_per_second: 62.547, eval_steps_per_second: 7.818, epoch: 2.1148[0m
[32m[2022-09-05 16:04:40,655] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-700[0m
[32m[2022-09-05 16:04:40,655] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 16:04:43,789] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-700/tokenizer_config.json[0m
[32m[2022-09-05 16:04:43,790] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-700/special_tokens_map.json[0m
[32m[2022-09-05 16:04:49,137] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-05 16:04:49,137] [    INFO][0m - Loading best model from ./checkpoints_csl/checkpoint-300 (score: 0.5910766961651918).[0m
[32m[2022-09-05 16:04:50,376] [    INFO][0m - train_runtime: 491.2399, train_samples_per_second: 268.911, train_steps_per_second: 33.69, train_loss: 0.342976766698079, epoch: 2.1148[0m
[32m[2022-09-05 16:04:50,377] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/[0m
[32m[2022-09-05 16:04:50,377] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 16:04:53,722] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/tokenizer_config.json[0m
[32m[2022-09-05 16:04:53,722] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/special_tokens_map.json[0m
[32m[2022-09-05 16:04:53,723] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-05 16:04:53,724] [    INFO][0m -   epoch                    =     2.1148[0m
[32m[2022-09-05 16:04:53,724] [    INFO][0m -   train_loss               =      0.343[0m
[32m[2022-09-05 16:04:53,724] [    INFO][0m -   train_runtime            = 0:08:11.23[0m
[32m[2022-09-05 16:04:53,724] [    INFO][0m -   train_samples_per_second =    268.911[0m
[32m[2022-09-05 16:04:53,724] [    INFO][0m -   train_steps_per_second   =      33.69[0m
[32m[2022-09-05 16:04:53,730] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-05 16:04:53,730] [    INFO][0m -   Num examples = 49340[0m
[32m[2022-09-05 16:04:53,730] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 16:04:53,730] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 16:04:53,730] [    INFO][0m -   Total prediction steps = 6168[0m
[32m[2022-09-05 16:27:49,518] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-09-05 16:27:49,519] [    INFO][0m -   test_accuracy           =     0.4949[0m
[32m[2022-09-05 16:27:49,519] [    INFO][0m -   test_loss               =     1.6845[0m
[32m[2022-09-05 16:27:49,519] [    INFO][0m -   test_runtime            = 0:22:55.78[0m
[32m[2022-09-05 16:27:49,519] [    INFO][0m -   test_samples_per_second =     35.863[0m
[32m[2022-09-05 16:27:49,519] [    INFO][0m -   test_steps_per_second   =      4.483[0m
[33m[2022-09-05 16:30:01,154] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-09-05 16:30:01,155] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-05 16:30:01,155] [    INFO][0m - ============================================================[0m
[32m[2022-09-05 16:30:01,155] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-05 16:30:01,155] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-05 16:30:01,155] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-05 16:30:01,155] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-05 16:30:01,155] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-05 16:30:01,155] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-05 16:30:01,155] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-09-05 16:30:01,155] [    INFO][0m - [0m
[32m[2022-09-05 16:30:01,156] [    INFO][0m - ============================================================[0m
[32m[2022-09-05 16:30:01,156] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-05 16:30:01,156] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-05 16:30:01,156] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-05 16:30:01,156] [    INFO][0m - prompt                        :[0m
[32m[2022-09-05 16:30:01,156] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-05 16:30:01,156] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-05 16:30:01,156] [    INFO][0m - task_name                     :csl[0m
[32m[2022-09-05 16:30:01,156] [    INFO][0m - [0m
[32m[2022-09-05 16:30:01,157] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
[32m[2022-09-05 16:30:02,612] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-09-05 16:30:02,635] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-09-05 16:30:02,636] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[32m[2022-09-05 16:30:02,637] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‚Äú'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‚Äù‰∏äÊñá‰∏≠Êâæ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'Âá∫Ëøô‰∫õÂÖ≥ÈîÆËØçÔºö‚Äú'}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': '‚Äù'}][0m
2022-09-05 16:30:02,639 INFO [download.py:119] unique_endpoints {''}
[32m[2022-09-05 16:30:03,037] [    INFO][0m - ============================================================[0m
[32m[2022-09-05 16:30:03,038] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-05 16:30:03,038] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-05 16:30:03,038] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-05 16:30:03,038] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-05 16:30:03,038] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-05 16:30:03,038] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-05 16:30:03,038] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-05 16:30:03,038] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-05 16:30:03,039] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-05 16:30:03,039] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-05 16:30:03,039] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-05 16:30:03,039] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-05 16:30:03,039] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-05 16:30:03,039] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-05 16:30:03,039] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-05 16:30:03,039] [    INFO][0m - do_predict                    :False[0m
[32m[2022-09-05 16:30:03,039] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-05 16:30:03,039] [    INFO][0m - eval_batch_size               :8[0m
[32m[2022-09-05 16:30:03,039] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-09-05 16:30:03,039] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-09-05 16:30:03,039] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-05 16:30:03,040] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-05 16:30:03,040] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-05 16:30:03,040] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-05 16:30:03,040] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-05 16:30:03,040] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-05 16:30:03,040] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-05 16:30:03,040] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-05 16:30:03,040] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-05 16:30:03,040] [    INFO][0m - learning_rate                 :1e-05[0m
[32m[2022-09-05 16:30:03,040] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-05 16:30:03,040] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-05 16:30:03,040] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-05 16:30:03,040] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-05 16:30:03,041] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-05 16:30:03,041] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-05 16:30:03,041] [    INFO][0m - logging_dir                   :./checkpoints_csl/runs/Sep05_16-30-01_instance-3bwob41y-01[0m
[32m[2022-09-05 16:30:03,041] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-05 16:30:03,041] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-05 16:30:03,041] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-05 16:30:03,041] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-05 16:30:03,041] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-05 16:30:03,041] [    INFO][0m - max_seq_length                :320[0m
[32m[2022-09-05 16:30:03,041] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-05 16:30:03,041] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-05 16:30:03,041] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-05 16:30:03,041] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-05 16:30:03,041] [    INFO][0m - num_train_epochs              :50.0[0m
[32m[2022-09-05 16:30:03,042] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-05 16:30:03,042] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-05 16:30:03,042] [    INFO][0m - output_dir                    :./checkpoints_csl/[0m
[32m[2022-09-05 16:30:03,042] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-05 16:30:03,042] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-05 16:30:03,042] [    INFO][0m - per_device_eval_batch_size    :8[0m
[32m[2022-09-05 16:30:03,042] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-09-05 16:30:03,042] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-05 16:30:03,042] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-05 16:30:03,042] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-05 16:30:03,042] [    INFO][0m - ppt_learning_rate             :0.0001[0m
[32m[2022-09-05 16:30:03,042] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-05 16:30:03,042] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-05 16:30:03,043] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-05 16:30:03,043] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-05 16:30:03,043] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-05 16:30:03,043] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-05 16:30:03,043] [    INFO][0m - run_name                      :./checkpoints_csl/[0m
[32m[2022-09-05 16:30:03,043] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-05 16:30:03,043] [    INFO][0m - save_steps                    :100[0m
[32m[2022-09-05 16:30:03,043] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-09-05 16:30:03,043] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-05 16:30:03,043] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-05 16:30:03,043] [    INFO][0m - seed                          :42[0m
[32m[2022-09-05 16:30:03,043] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-05 16:30:03,043] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-05 16:30:03,043] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-05 16:30:03,044] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-09-05 16:30:03,044] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-05 16:30:03,044] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-05 16:30:03,044] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-05 16:30:03,044] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-05 16:30:03,044] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-05 16:30:03,044] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-05 16:30:03,044] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-05 16:30:03,044] [    INFO][0m - [0m
[32m[2022-09-05 16:30:03,046] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-05 16:30:03,046] [    INFO][0m -   Num examples = 2642[0m
[32m[2022-09-05 16:30:03,046] [    INFO][0m -   Num Epochs = 50[0m
[32m[2022-09-05 16:30:03,046] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-09-05 16:30:03,046] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-09-05 16:30:03,046] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-05 16:30:03,046] [    INFO][0m -   Total optimization steps = 16550.0[0m
[32m[2022-09-05 16:30:03,046] [    INFO][0m -   Total num train samples = 132100[0m
[32m[2022-09-05 16:30:04,978] [    INFO][0m - loss: 3.46454735, learning_rate: 9.993957703927493e-06, global_step: 10, interval_runtime: 1.9303, interval_samples_per_second: 4.145, interval_steps_per_second: 5.181, epoch: 0.0302[0m
[32m[2022-09-05 16:30:06,783] [    INFO][0m - loss: 1.20493107, learning_rate: 9.987915407854986e-06, global_step: 20, interval_runtime: 1.8051, interval_samples_per_second: 4.432, interval_steps_per_second: 5.54, epoch: 0.0604[0m
[32m[2022-09-05 16:30:08,591] [    INFO][0m - loss: 0.85694132, learning_rate: 9.981873111782479e-06, global_step: 30, interval_runtime: 1.8078, interval_samples_per_second: 4.425, interval_steps_per_second: 5.532, epoch: 0.0906[0m
[32m[2022-09-05 16:30:10,396] [    INFO][0m - loss: 0.90332565, learning_rate: 9.975830815709971e-06, global_step: 40, interval_runtime: 1.8048, interval_samples_per_second: 4.433, interval_steps_per_second: 5.541, epoch: 0.1208[0m
[32m[2022-09-05 16:30:12,210] [    INFO][0m - loss: 0.75759778, learning_rate: 9.969788519637464e-06, global_step: 50, interval_runtime: 1.8146, interval_samples_per_second: 4.409, interval_steps_per_second: 5.511, epoch: 0.1511[0m
[32m[2022-09-05 16:30:14,018] [    INFO][0m - loss: 0.7099453, learning_rate: 9.963746223564955e-06, global_step: 60, interval_runtime: 1.8081, interval_samples_per_second: 4.425, interval_steps_per_second: 5.531, epoch: 0.1813[0m
[32m[2022-09-05 16:30:15,827] [    INFO][0m - loss: 0.64149289, learning_rate: 9.957703927492449e-06, global_step: 70, interval_runtime: 1.8083, interval_samples_per_second: 4.424, interval_steps_per_second: 5.53, epoch: 0.2115[0m
[32m[2022-09-05 16:30:17,632] [    INFO][0m - loss: 0.65081348, learning_rate: 9.95166163141994e-06, global_step: 80, interval_runtime: 1.8061, interval_samples_per_second: 4.429, interval_steps_per_second: 5.537, epoch: 0.2417[0m
[32m[2022-09-05 16:30:19,449] [    INFO][0m - loss: 0.76792817, learning_rate: 9.945619335347432e-06, global_step: 90, interval_runtime: 1.8107, interval_samples_per_second: 4.418, interval_steps_per_second: 5.523, epoch: 0.2719[0m
[32m[2022-09-05 16:30:21,267] [    INFO][0m - loss: 0.7138835, learning_rate: 9.939577039274926e-06, global_step: 100, interval_runtime: 1.8232, interval_samples_per_second: 4.388, interval_steps_per_second: 5.485, epoch: 0.3021[0m
[32m[2022-09-05 16:30:21,267] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 16:30:21,268] [    INFO][0m -   Num examples = 2712[0m
[32m[2022-09-05 16:30:21,268] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 16:30:21,268] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 16:30:21,268] [    INFO][0m -   Total prediction steps = 339[0m
[32m[2022-09-05 16:31:05,004] [    INFO][0m - eval_loss: 1.1557292938232422, eval_accuracy: 0.5320796460176991, eval_runtime: 43.735, eval_samples_per_second: 62.01, eval_steps_per_second: 7.751, epoch: 0.3021[0m
[32m[2022-09-05 16:31:05,055] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-100[0m
[32m[2022-09-05 16:31:05,056] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 16:31:08,233] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-05 16:31:08,234] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-05 16:31:15,713] [    INFO][0m - loss: 0.61015306, learning_rate: 9.933534743202417e-06, global_step: 110, interval_runtime: 54.446, interval_samples_per_second: 0.147, interval_steps_per_second: 0.184, epoch: 0.3323[0m
[32m[2022-09-05 16:31:17,526] [    INFO][0m - loss: 0.74613886, learning_rate: 9.92749244712991e-06, global_step: 120, interval_runtime: 1.8129, interval_samples_per_second: 4.413, interval_steps_per_second: 5.516, epoch: 0.3625[0m
[32m[2022-09-05 16:31:19,337] [    INFO][0m - loss: 0.61346436, learning_rate: 9.921450151057402e-06, global_step: 130, interval_runtime: 1.8119, interval_samples_per_second: 4.415, interval_steps_per_second: 5.519, epoch: 0.3927[0m
[32m[2022-09-05 16:31:21,148] [    INFO][0m - loss: 0.77581186, learning_rate: 9.915407854984895e-06, global_step: 140, interval_runtime: 1.8109, interval_samples_per_second: 4.418, interval_steps_per_second: 5.522, epoch: 0.423[0m
[32m[2022-09-05 16:31:22,965] [    INFO][0m - loss: 0.79189897, learning_rate: 9.909365558912388e-06, global_step: 150, interval_runtime: 1.8168, interval_samples_per_second: 4.403, interval_steps_per_second: 5.504, epoch: 0.4532[0m
[32m[2022-09-05 16:31:24,777] [    INFO][0m - loss: 0.56758246, learning_rate: 9.90332326283988e-06, global_step: 160, interval_runtime: 1.812, interval_samples_per_second: 4.415, interval_steps_per_second: 5.519, epoch: 0.4834[0m
[32m[2022-09-05 16:31:26,590] [    INFO][0m - loss: 0.61506925, learning_rate: 9.897280966767373e-06, global_step: 170, interval_runtime: 1.8133, interval_samples_per_second: 4.412, interval_steps_per_second: 5.515, epoch: 0.5136[0m
[32m[2022-09-05 16:31:28,416] [    INFO][0m - loss: 0.42985702, learning_rate: 9.891238670694865e-06, global_step: 180, interval_runtime: 1.826, interval_samples_per_second: 4.381, interval_steps_per_second: 5.477, epoch: 0.5438[0m
[32m[2022-09-05 16:31:30,237] [    INFO][0m - loss: 0.67585559, learning_rate: 9.885196374622358e-06, global_step: 190, interval_runtime: 1.8202, interval_samples_per_second: 4.395, interval_steps_per_second: 5.494, epoch: 0.574[0m
[32m[2022-09-05 16:31:32,062] [    INFO][0m - loss: 0.46591687, learning_rate: 9.87915407854985e-06, global_step: 200, interval_runtime: 1.8255, interval_samples_per_second: 4.382, interval_steps_per_second: 5.478, epoch: 0.6042[0m
[32m[2022-09-05 16:31:32,063] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 16:31:32,063] [    INFO][0m -   Num examples = 2712[0m
[32m[2022-09-05 16:31:32,063] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 16:31:32,063] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 16:31:32,063] [    INFO][0m -   Total prediction steps = 339[0m
[32m[2022-09-05 16:32:15,832] [    INFO][0m - eval_loss: 1.2140083312988281, eval_accuracy: 0.5224926253687315, eval_runtime: 43.7683, eval_samples_per_second: 61.963, eval_steps_per_second: 7.745, epoch: 0.6042[0m
[32m[2022-09-05 16:32:15,897] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-200[0m
[32m[2022-09-05 16:32:15,898] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 16:32:19,924] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-200/tokenizer_config.json[0m
[32m[2022-09-05 16:32:19,925] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-200/special_tokens_map.json[0m
[32m[2022-09-05 16:32:27,425] [    INFO][0m - loss: 0.63268914, learning_rate: 9.873111782477343e-06, global_step: 210, interval_runtime: 55.363, interval_samples_per_second: 0.145, interval_steps_per_second: 0.181, epoch: 0.6344[0m
[32m[2022-09-05 16:32:29,249] [    INFO][0m - loss: 0.39687891, learning_rate: 9.867069486404834e-06, global_step: 220, interval_runtime: 1.8243, interval_samples_per_second: 4.385, interval_steps_per_second: 5.481, epoch: 0.6647[0m
[32m[2022-09-05 16:32:31,057] [    INFO][0m - loss: 0.53961859, learning_rate: 9.861027190332328e-06, global_step: 230, interval_runtime: 1.8074, interval_samples_per_second: 4.426, interval_steps_per_second: 5.533, epoch: 0.6949[0m
[32m[2022-09-05 16:32:32,863] [    INFO][0m - loss: 0.40512486, learning_rate: 9.854984894259819e-06, global_step: 240, interval_runtime: 1.8061, interval_samples_per_second: 4.429, interval_steps_per_second: 5.537, epoch: 0.7251[0m
[32m[2022-09-05 16:32:34,672] [    INFO][0m - loss: 0.54804192, learning_rate: 9.848942598187312e-06, global_step: 250, interval_runtime: 1.8078, interval_samples_per_second: 4.425, interval_steps_per_second: 5.532, epoch: 0.7553[0m
[32m[2022-09-05 16:32:36,486] [    INFO][0m - loss: 0.26331303, learning_rate: 9.842900302114804e-06, global_step: 260, interval_runtime: 1.8151, interval_samples_per_second: 4.407, interval_steps_per_second: 5.509, epoch: 0.7855[0m
[32m[2022-09-05 16:32:38,304] [    INFO][0m - loss: 0.27959507, learning_rate: 9.836858006042297e-06, global_step: 270, interval_runtime: 1.8186, interval_samples_per_second: 4.399, interval_steps_per_second: 5.499, epoch: 0.8157[0m
[32m[2022-09-05 16:32:40,113] [    INFO][0m - loss: 0.24545431, learning_rate: 9.83081570996979e-06, global_step: 280, interval_runtime: 1.8079, interval_samples_per_second: 4.425, interval_steps_per_second: 5.531, epoch: 0.8459[0m
[32m[2022-09-05 16:32:41,924] [    INFO][0m - loss: 0.37251139, learning_rate: 9.824773413897282e-06, global_step: 290, interval_runtime: 1.8116, interval_samples_per_second: 4.416, interval_steps_per_second: 5.52, epoch: 0.8761[0m
[32m[2022-09-05 16:32:43,737] [    INFO][0m - loss: 0.4574924, learning_rate: 9.818731117824774e-06, global_step: 300, interval_runtime: 1.8134, interval_samples_per_second: 4.412, interval_steps_per_second: 5.514, epoch: 0.9063[0m
[32m[2022-09-05 16:32:43,737] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 16:32:43,738] [    INFO][0m -   Num examples = 2712[0m
[32m[2022-09-05 16:32:43,738] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 16:32:43,738] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 16:32:43,738] [    INFO][0m -   Total prediction steps = 339[0m
[32m[2022-09-05 16:33:27,178] [    INFO][0m - eval_loss: 1.7177540063858032, eval_accuracy: 0.4915191740412979, eval_runtime: 43.4398, eval_samples_per_second: 62.431, eval_steps_per_second: 7.804, epoch: 0.9063[0m
[32m[2022-09-05 16:33:27,226] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-300[0m
[32m[2022-09-05 16:33:27,226] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 16:33:30,295] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-300/tokenizer_config.json[0m
[32m[2022-09-05 16:33:30,296] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-300/special_tokens_map.json[0m
[32m[2022-09-05 16:33:37,543] [    INFO][0m - loss: 0.24903231, learning_rate: 9.812688821752267e-06, global_step: 310, interval_runtime: 53.8055, interval_samples_per_second: 0.149, interval_steps_per_second: 0.186, epoch: 0.9366[0m
[32m[2022-09-05 16:33:39,350] [    INFO][0m - loss: 0.26769962, learning_rate: 9.80664652567976e-06, global_step: 320, interval_runtime: 1.807, interval_samples_per_second: 4.427, interval_steps_per_second: 5.534, epoch: 0.9668[0m
[32m[2022-09-05 16:33:41,153] [    INFO][0m - loss: 0.32393596, learning_rate: 9.80060422960725e-06, global_step: 330, interval_runtime: 1.8037, interval_samples_per_second: 4.435, interval_steps_per_second: 5.544, epoch: 0.997[0m
[32m[2022-09-05 16:33:42,959] [    INFO][0m - loss: 0.19147289, learning_rate: 9.794561933534745e-06, global_step: 340, interval_runtime: 1.8061, interval_samples_per_second: 4.429, interval_steps_per_second: 5.537, epoch: 1.0272[0m
[32m[2022-09-05 16:33:44,775] [    INFO][0m - loss: 0.13400065, learning_rate: 9.788519637462236e-06, global_step: 350, interval_runtime: 1.8155, interval_samples_per_second: 4.406, interval_steps_per_second: 5.508, epoch: 1.0574[0m
[32m[2022-09-05 16:33:46,589] [    INFO][0m - loss: 0.1838308, learning_rate: 9.782477341389728e-06, global_step: 360, interval_runtime: 1.8137, interval_samples_per_second: 4.411, interval_steps_per_second: 5.514, epoch: 1.0876[0m
[32m[2022-09-05 16:33:48,400] [    INFO][0m - loss: 0.10331955, learning_rate: 9.776435045317222e-06, global_step: 370, interval_runtime: 1.8112, interval_samples_per_second: 4.417, interval_steps_per_second: 5.521, epoch: 1.1178[0m
[32m[2022-09-05 16:33:50,211] [    INFO][0m - loss: 0.23741434, learning_rate: 9.770392749244713e-06, global_step: 380, interval_runtime: 1.8116, interval_samples_per_second: 4.416, interval_steps_per_second: 5.52, epoch: 1.148[0m
[32m[2022-09-05 16:33:52,031] [    INFO][0m - loss: 0.32399454, learning_rate: 9.764350453172206e-06, global_step: 390, interval_runtime: 1.8194, interval_samples_per_second: 4.397, interval_steps_per_second: 5.496, epoch: 1.1782[0m
[32m[2022-09-05 16:33:53,840] [    INFO][0m - loss: 0.20977452, learning_rate: 9.758308157099698e-06, global_step: 400, interval_runtime: 1.8088, interval_samples_per_second: 4.423, interval_steps_per_second: 5.529, epoch: 1.2085[0m
[32m[2022-09-05 16:33:53,840] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 16:33:53,840] [    INFO][0m -   Num examples = 2712[0m
[32m[2022-09-05 16:33:53,840] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 16:33:53,840] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 16:33:53,840] [    INFO][0m -   Total prediction steps = 339[0m
[32m[2022-09-05 16:34:37,360] [    INFO][0m - eval_loss: 3.0211026668548584, eval_accuracy: 0.4638643067846608, eval_runtime: 43.5194, eval_samples_per_second: 62.317, eval_steps_per_second: 7.79, epoch: 1.2085[0m
[32m[2022-09-05 16:34:37,411] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-400[0m
[32m[2022-09-05 16:34:37,411] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 16:34:40,521] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-400/tokenizer_config.json[0m
[32m[2022-09-05 16:34:40,522] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-400/special_tokens_map.json[0m
[32m[2022-09-05 16:34:47,444] [    INFO][0m - loss: 0.0972189, learning_rate: 9.752265861027191e-06, global_step: 410, interval_runtime: 53.6048, interval_samples_per_second: 0.149, interval_steps_per_second: 0.187, epoch: 1.2387[0m
[32m[2022-09-05 16:34:49,258] [    INFO][0m - loss: 0.12651628, learning_rate: 9.746223564954684e-06, global_step: 420, interval_runtime: 1.814, interval_samples_per_second: 4.41, interval_steps_per_second: 5.513, epoch: 1.2689[0m
[32m[2022-09-05 16:34:51,074] [    INFO][0m - loss: 0.01013064, learning_rate: 9.740181268882176e-06, global_step: 430, interval_runtime: 1.8154, interval_samples_per_second: 4.407, interval_steps_per_second: 5.508, epoch: 1.2991[0m
[32m[2022-09-05 16:34:52,885] [    INFO][0m - loss: 0.01735091, learning_rate: 9.734138972809669e-06, global_step: 440, interval_runtime: 1.8117, interval_samples_per_second: 4.416, interval_steps_per_second: 5.52, epoch: 1.3293[0m
[32m[2022-09-05 16:34:54,692] [    INFO][0m - loss: 0.0375067, learning_rate: 9.728096676737161e-06, global_step: 450, interval_runtime: 1.8067, interval_samples_per_second: 4.428, interval_steps_per_second: 5.535, epoch: 1.3595[0m
[32m[2022-09-05 16:34:56,512] [    INFO][0m - loss: 0.23918257, learning_rate: 9.722054380664654e-06, global_step: 460, interval_runtime: 1.8196, interval_samples_per_second: 4.397, interval_steps_per_second: 5.496, epoch: 1.3897[0m
[32m[2022-09-05 16:34:58,329] [    INFO][0m - loss: 0.07658926, learning_rate: 9.716012084592146e-06, global_step: 470, interval_runtime: 1.8168, interval_samples_per_second: 4.403, interval_steps_per_second: 5.504, epoch: 1.4199[0m
[32m[2022-09-05 16:35:00,143] [    INFO][0m - loss: 0.03723638, learning_rate: 9.709969788519639e-06, global_step: 480, interval_runtime: 1.8145, interval_samples_per_second: 4.409, interval_steps_per_second: 5.511, epoch: 1.4502[0m
[32m[2022-09-05 16:35:01,965] [    INFO][0m - loss: 0.00263842, learning_rate: 9.70392749244713e-06, global_step: 490, interval_runtime: 1.8215, interval_samples_per_second: 4.392, interval_steps_per_second: 5.49, epoch: 1.4804[0m
[32m[2022-09-05 16:35:03,779] [    INFO][0m - loss: 0.0916437, learning_rate: 9.697885196374624e-06, global_step: 500, interval_runtime: 1.8144, interval_samples_per_second: 4.409, interval_steps_per_second: 5.512, epoch: 1.5106[0m
[32m[2022-09-05 16:35:03,779] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 16:35:03,779] [    INFO][0m -   Num examples = 2712[0m
[32m[2022-09-05 16:35:03,780] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 16:35:03,780] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 16:35:03,780] [    INFO][0m -   Total prediction steps = 339[0m
[32m[2022-09-05 16:35:46,951] [    INFO][0m - eval_loss: 4.562287330627441, eval_accuracy: 0.5047935103244838, eval_runtime: 43.171, eval_samples_per_second: 62.82, eval_steps_per_second: 7.852, epoch: 1.5106[0m
[32m[2022-09-05 16:35:47,004] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/checkpoint-500[0m
[32m[2022-09-05 16:35:47,005] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 16:35:50,173] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/checkpoint-500/tokenizer_config.json[0m
[32m[2022-09-05 16:35:50,173] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/checkpoint-500/special_tokens_map.json[0m
[32m[2022-09-05 16:35:55,263] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-05 16:35:55,263] [    INFO][0m - Loading best model from ./checkpoints_csl/checkpoint-100 (score: 0.5320796460176991).[0m
[32m[2022-09-05 16:35:56,309] [    INFO][0m - train_runtime: 353.2618, train_samples_per_second: 373.944, train_steps_per_second: 46.849, train_loss: 0.48128726629912855, epoch: 1.5106[0m
[32m[2022-09-05 16:35:56,350] [    INFO][0m - Saving model checkpoint to ./checkpoints_csl/[0m
[32m[2022-09-05 16:35:56,351] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 16:35:59,416] [    INFO][0m - tokenizer config file saved in ./checkpoints_csl/tokenizer_config.json[0m
[32m[2022-09-05 16:35:59,417] [    INFO][0m - Special tokens file saved in ./checkpoints_csl/special_tokens_map.json[0m
[32m[2022-09-05 16:35:59,418] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-05 16:35:59,418] [    INFO][0m -   epoch                    =     1.5106[0m
[32m[2022-09-05 16:35:59,418] [    INFO][0m -   train_loss               =     0.4813[0m
[32m[2022-09-05 16:35:59,418] [    INFO][0m -   train_runtime            = 0:05:53.26[0m
[32m[2022-09-05 16:35:59,418] [    INFO][0m -   train_samples_per_second =    373.944[0m
[32m[2022-09-05 16:35:59,418] [    INFO][0m -   train_steps_per_second   =     46.849[0m
[32m[2022-09-05 16:35:59,423] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-05 16:35:59,423] [    INFO][0m -   Num examples = 49340[0m
[32m[2022-09-05 16:35:59,423] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 16:35:59,423] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 16:35:59,423] [    INFO][0m -   Total prediction steps = 6168[0m
terminate called after throwing an instance of 'paddle::memory::allocation::BadAlloc'
  what():  

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   softmax_final_state_dygraph_function(paddle::experimental::Tensor const&, int)
1   paddle::experimental::softmax(paddle::experimental::Tensor const&, int)
2   void phi::SoftmaxGPUDNNKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, int, phi::DenseTensor*)
3   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, paddle::experimental::DataType, unsigned long, bool) const
4   phi::DenseTensor::AllocateFrom(phi::Allocator*, paddle::experimental::DataType, unsigned long)
5   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
6   paddle::memory::allocation::Allocator::Allocate(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
10  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
11  phi::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 7.352233GB memory on GPU 0, 26.096436GB memory has been allocated and available memory is only 5.652100GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
If the above ways do not solve the out of memory problem, you can try to use CUDA managed memory. The command is `export FLAGS_use_cuda_managed_memory=false`.
 (at /paddle/paddle/fluid/memory/allocation/cuda_allocator.cc:95)

run.sh: line 52: 42880 Aborted                 CUDA_VISIBLE_DEVICES=$device python ../train_single.py --output_dir ./checkpoints_$task_name/ --prompt "$prompt" --max_seq_length $max_length --per_device_eval_batch_size $batch_size --per_device_train_batch_size $batch_size --model_name_or_path ernie-3.0-base-zh --split_id few_all --task_name $task_name --metric_for_best_model accuracy --disable_tqdm True --do_test --eval_steps 100 --save_steps 100 --num_train_epochs 50 --logging_steps 10 --learning_rate 1e-5 --ppt_learning_rate 1e-4 --load_best_model_at_end $is_train --do_train $is_train --do_eval $is_train --do_save $is_train
run.sh: line 58: --freeze_plm: command not found
