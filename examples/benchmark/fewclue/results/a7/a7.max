 
==========
eprstmt
==========
 
[33m[2022-08-26 17:09:04,798] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-26 17:09:04,798] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-26 17:09:04,799] [    INFO][0m - ============================================================[0m
[32m[2022-08-26 17:09:04,799] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-26 17:09:04,799] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-26 17:09:04,799] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-26 17:09:04,799] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-26 17:09:04,799] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-08-26 17:09:04,799] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-26 17:09:04,799] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-08-26 17:09:04,799] [    INFO][0m - [0m
[32m[2022-08-26 17:09:04,799] [    INFO][0m - ============================================================[0m
[32m[2022-08-26 17:09:04,799] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-26 17:09:04,799] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-26 17:09:04,800] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-26 17:09:04,800] [    INFO][0m - prompt                        :{'text':'text_a'}{'hard':'ÊàëÊÑüËßâ'}{'mask'}{'hard':'ÂñúÊ¨¢„ÄÇ'}[0m
[32m[2022-08-26 17:09:04,800] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-26 17:09:04,800] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-26 17:09:04,800] [    INFO][0m - task_name                     :eprstmt[0m
[32m[2022-08-26 17:09:04,800] [    INFO][0m - [0m
[32m[2022-08-26 17:09:04,800] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0826 17:09:04.802147 52260 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0826 17:09:04.806172 52260 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-26 17:09:07,465] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-08-26 17:09:07,489] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-08-26 17:09:07,489] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[32m[2022-08-26 17:09:07,491] [    INFO][0m - Using template: [{'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': 'ÊàëÊÑüËßâ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'ÂñúÊ¨¢„ÄÇ'}][0m
2022-08-26 17:09:07,497 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-26 17:09:07,591] [    INFO][0m - ============================================================[0m
[32m[2022-08-26 17:09:07,592] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-26 17:09:07,592] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-26 17:09:07,592] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-26 17:09:07,592] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-26 17:09:07,592] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-26 17:09:07,592] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-26 17:09:07,592] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-26 17:09:07,592] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-26 17:09:07,593] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-26 17:09:07,593] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-26 17:09:07,593] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-26 17:09:07,593] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-26 17:09:07,593] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-26 17:09:07,593] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-26 17:09:07,593] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-26 17:09:07,593] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-26 17:09:07,593] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-26 17:09:07,593] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-26 17:09:07,593] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-26 17:09:07,593] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-26 17:09:07,594] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-26 17:09:07,594] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-26 17:09:07,594] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-26 17:09:07,594] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-26 17:09:07,594] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-26 17:09:07,594] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-26 17:09:07,594] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-26 17:09:07,594] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-26 17:09:07,594] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-26 17:09:07,594] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-26 17:09:07,594] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-26 17:09:07,594] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-26 17:09:07,594] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-26 17:09:07,595] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-26 17:09:07,595] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-26 17:09:07,595] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-26 17:09:07,595] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug26_17-09-04_instance-3bwob41y-01[0m
[32m[2022-08-26 17:09:07,595] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-26 17:09:07,595] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-26 17:09:07,595] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-26 17:09:07,595] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-26 17:09:07,595] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-26 17:09:07,595] [    INFO][0m - max_seq_length                :128[0m
[32m[2022-08-26 17:09:07,595] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-26 17:09:07,595] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-26 17:09:07,595] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-26 17:09:07,596] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-26 17:09:07,596] [    INFO][0m - num_train_epochs              :100.0[0m
[32m[2022-08-26 17:09:07,596] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-26 17:09:07,596] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-26 17:09:07,596] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-26 17:09:07,596] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-26 17:09:07,596] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-26 17:09:07,596] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-26 17:09:07,596] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-26 17:09:07,596] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-26 17:09:07,596] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-26 17:09:07,596] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-26 17:09:07,596] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-26 17:09:07,597] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-26 17:09:07,597] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-26 17:09:07,597] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-26 17:09:07,597] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-26 17:09:07,597] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-26 17:09:07,597] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-26 17:09:07,597] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-26 17:09:07,597] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-26 17:09:07,597] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-26 17:09:07,597] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-26 17:09:07,597] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-26 17:09:07,597] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-26 17:09:07,597] [    INFO][0m - seed                          :42[0m
[32m[2022-08-26 17:09:07,598] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-26 17:09:07,598] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-26 17:09:07,598] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-26 17:09:07,598] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-26 17:09:07,598] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-26 17:09:07,598] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-26 17:09:07,598] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-26 17:09:07,598] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-26 17:09:07,598] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-26 17:09:07,598] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-26 17:09:07,598] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-26 17:09:07,598] [    INFO][0m - [0m
[32m[2022-08-26 17:09:07,600] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-26 17:09:07,600] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 17:09:07,600] [    INFO][0m -   Num Epochs = 100[0m
[32m[2022-08-26 17:09:07,600] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-26 17:09:07,600] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-26 17:09:07,600] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-26 17:09:07,601] [    INFO][0m -   Total optimization steps = 2000.0[0m
[32m[2022-08-26 17:09:07,601] [    INFO][0m -   Total num train samples = 16000[0m
[32m[2022-08-26 17:09:09,270] [    INFO][0m - loss: 0.61598115, learning_rate: 2.985e-05, global_step: 10, interval_runtime: 1.6689, interval_samples_per_second: 4.793, interval_steps_per_second: 5.992, epoch: 0.5[0m
[32m[2022-08-26 17:09:09,967] [    INFO][0m - loss: 0.41057587, learning_rate: 2.97e-05, global_step: 20, interval_runtime: 0.6962, interval_samples_per_second: 11.49, interval_steps_per_second: 14.363, epoch: 1.0[0m
[32m[2022-08-26 17:09:10,703] [    INFO][0m - loss: 0.69258633, learning_rate: 2.955e-05, global_step: 30, interval_runtime: 0.7362, interval_samples_per_second: 10.867, interval_steps_per_second: 13.583, epoch: 1.5[0m
[32m[2022-08-26 17:09:11,399] [    INFO][0m - loss: 0.63719406, learning_rate: 2.94e-05, global_step: 40, interval_runtime: 0.6961, interval_samples_per_second: 11.493, interval_steps_per_second: 14.367, epoch: 2.0[0m
[32m[2022-08-26 17:09:12,134] [    INFO][0m - loss: 0.38005557, learning_rate: 2.925e-05, global_step: 50, interval_runtime: 0.7348, interval_samples_per_second: 10.887, interval_steps_per_second: 13.608, epoch: 2.5[0m
[32m[2022-08-26 17:09:12,829] [    INFO][0m - loss: 0.39425278, learning_rate: 2.91e-05, global_step: 60, interval_runtime: 0.6955, interval_samples_per_second: 11.503, interval_steps_per_second: 14.379, epoch: 3.0[0m
[32m[2022-08-26 17:09:13,564] [    INFO][0m - loss: 0.13043654, learning_rate: 2.895e-05, global_step: 70, interval_runtime: 0.7345, interval_samples_per_second: 10.891, interval_steps_per_second: 13.614, epoch: 3.5[0m
[32m[2022-08-26 17:09:14,260] [    INFO][0m - loss: 0.30045543, learning_rate: 2.88e-05, global_step: 80, interval_runtime: 0.6969, interval_samples_per_second: 11.48, interval_steps_per_second: 14.35, epoch: 4.0[0m
[32m[2022-08-26 17:09:14,994] [    INFO][0m - loss: 0.21836395, learning_rate: 2.865e-05, global_step: 90, interval_runtime: 0.7337, interval_samples_per_second: 10.903, interval_steps_per_second: 13.629, epoch: 4.5[0m
[32m[2022-08-26 17:09:15,692] [    INFO][0m - loss: 0.18242249, learning_rate: 2.8499999999999998e-05, global_step: 100, interval_runtime: 0.6974, interval_samples_per_second: 11.472, interval_steps_per_second: 14.339, epoch: 5.0[0m
[32m[2022-08-26 17:09:15,692] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:09:15,693] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 17:09:15,693] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:09:15,693] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:09:15,693] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 17:09:16,164] [    INFO][0m - eval_loss: 0.8639103174209595, eval_accuracy: 0.88125, eval_runtime: 0.4709, eval_samples_per_second: 339.766, eval_steps_per_second: 10.618, epoch: 5.0[0m
[32m[2022-08-26 17:09:16,164] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-26 17:09:16,164] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:09:18,409] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-26 17:09:18,410] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-26 17:09:24,053] [    INFO][0m - loss: 0.1097851, learning_rate: 2.8349999999999998e-05, global_step: 110, interval_runtime: 8.3609, interval_samples_per_second: 0.957, interval_steps_per_second: 1.196, epoch: 5.5[0m
[32m[2022-08-26 17:09:24,747] [    INFO][0m - loss: 0.00221436, learning_rate: 2.8199999999999998e-05, global_step: 120, interval_runtime: 0.6946, interval_samples_per_second: 11.517, interval_steps_per_second: 14.396, epoch: 6.0[0m
[32m[2022-08-26 17:09:25,484] [    INFO][0m - loss: 0.06908181, learning_rate: 2.805e-05, global_step: 130, interval_runtime: 0.737, interval_samples_per_second: 10.855, interval_steps_per_second: 13.569, epoch: 6.5[0m
[32m[2022-08-26 17:09:26,185] [    INFO][0m - loss: 6.152e-05, learning_rate: 2.79e-05, global_step: 140, interval_runtime: 0.7007, interval_samples_per_second: 11.417, interval_steps_per_second: 14.272, epoch: 7.0[0m
[32m[2022-08-26 17:09:26,925] [    INFO][0m - loss: 3.998e-05, learning_rate: 2.7750000000000004e-05, global_step: 150, interval_runtime: 0.7402, interval_samples_per_second: 10.807, interval_steps_per_second: 13.509, epoch: 7.5[0m
[32m[2022-08-26 17:09:27,620] [    INFO][0m - loss: 0.05718524, learning_rate: 2.7600000000000003e-05, global_step: 160, interval_runtime: 0.6946, interval_samples_per_second: 11.517, interval_steps_per_second: 14.396, epoch: 8.0[0m
[32m[2022-08-26 17:09:28,349] [    INFO][0m - loss: 0.00591526, learning_rate: 2.7450000000000003e-05, global_step: 170, interval_runtime: 0.7292, interval_samples_per_second: 10.97, interval_steps_per_second: 13.713, epoch: 8.5[0m
[32m[2022-08-26 17:09:29,046] [    INFO][0m - loss: 0.00027276, learning_rate: 2.7300000000000003e-05, global_step: 180, interval_runtime: 0.697, interval_samples_per_second: 11.478, interval_steps_per_second: 14.348, epoch: 9.0[0m
[32m[2022-08-26 17:09:29,785] [    INFO][0m - loss: 2.395e-05, learning_rate: 2.7150000000000003e-05, global_step: 190, interval_runtime: 0.7389, interval_samples_per_second: 10.827, interval_steps_per_second: 13.534, epoch: 9.5[0m
[32m[2022-08-26 17:09:30,480] [    INFO][0m - loss: 1.477e-05, learning_rate: 2.7000000000000002e-05, global_step: 200, interval_runtime: 0.6956, interval_samples_per_second: 11.5, interval_steps_per_second: 14.375, epoch: 10.0[0m
[32m[2022-08-26 17:09:30,481] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:09:30,481] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 17:09:30,481] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:09:30,481] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:09:30,481] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 17:09:30,952] [    INFO][0m - eval_loss: 1.0195223093032837, eval_accuracy: 0.89375, eval_runtime: 0.471, eval_samples_per_second: 339.712, eval_steps_per_second: 10.616, epoch: 10.0[0m
[32m[2022-08-26 17:09:30,953] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-26 17:09:30,953] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:09:33,352] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-26 17:09:33,353] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-26 17:09:39,216] [    INFO][0m - loss: 1.633e-05, learning_rate: 2.6850000000000002e-05, global_step: 210, interval_runtime: 8.7355, interval_samples_per_second: 0.916, interval_steps_per_second: 1.145, epoch: 10.5[0m
[32m[2022-08-26 17:09:39,909] [    INFO][0m - loss: 0.16723517, learning_rate: 2.6700000000000002e-05, global_step: 220, interval_runtime: 0.6932, interval_samples_per_second: 11.541, interval_steps_per_second: 14.426, epoch: 11.0[0m
[32m[2022-08-26 17:09:40,639] [    INFO][0m - loss: 7.926e-05, learning_rate: 2.655e-05, global_step: 230, interval_runtime: 0.7295, interval_samples_per_second: 10.966, interval_steps_per_second: 13.708, epoch: 11.5[0m
[32m[2022-08-26 17:09:41,334] [    INFO][0m - loss: 8.046e-05, learning_rate: 2.64e-05, global_step: 240, interval_runtime: 0.6957, interval_samples_per_second: 11.5, interval_steps_per_second: 14.375, epoch: 12.0[0m
[32m[2022-08-26 17:09:42,067] [    INFO][0m - loss: 5.404e-05, learning_rate: 2.625e-05, global_step: 250, interval_runtime: 0.7321, interval_samples_per_second: 10.928, interval_steps_per_second: 13.659, epoch: 12.5[0m
[32m[2022-08-26 17:09:42,764] [    INFO][0m - loss: 1.81e-05, learning_rate: 2.61e-05, global_step: 260, interval_runtime: 0.6973, interval_samples_per_second: 11.473, interval_steps_per_second: 14.342, epoch: 13.0[0m
[32m[2022-08-26 17:09:43,499] [    INFO][0m - loss: 0.00022792, learning_rate: 2.595e-05, global_step: 270, interval_runtime: 0.7349, interval_samples_per_second: 10.886, interval_steps_per_second: 13.608, epoch: 13.5[0m
[32m[2022-08-26 17:09:44,195] [    INFO][0m - loss: 0.00044865, learning_rate: 2.58e-05, global_step: 280, interval_runtime: 0.6965, interval_samples_per_second: 11.486, interval_steps_per_second: 14.357, epoch: 14.0[0m
[32m[2022-08-26 17:09:44,927] [    INFO][0m - loss: 0.00029626, learning_rate: 2.565e-05, global_step: 290, interval_runtime: 0.7323, interval_samples_per_second: 10.925, interval_steps_per_second: 13.656, epoch: 14.5[0m
[32m[2022-08-26 17:09:45,625] [    INFO][0m - loss: 1.271e-05, learning_rate: 2.55e-05, global_step: 300, interval_runtime: 0.6975, interval_samples_per_second: 11.469, interval_steps_per_second: 14.336, epoch: 15.0[0m
[32m[2022-08-26 17:09:45,625] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:09:45,625] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 17:09:45,626] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:09:45,626] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:09:45,626] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 17:09:46,103] [    INFO][0m - eval_loss: 1.3906357288360596, eval_accuracy: 0.88125, eval_runtime: 0.4773, eval_samples_per_second: 335.21, eval_steps_per_second: 10.475, epoch: 15.0[0m
[32m[2022-08-26 17:09:46,103] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-26 17:09:46,104] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:09:48,773] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-26 17:09:48,773] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-26 17:09:54,699] [    INFO][0m - loss: 2.174e-05, learning_rate: 2.535e-05, global_step: 310, interval_runtime: 9.0738, interval_samples_per_second: 0.882, interval_steps_per_second: 1.102, epoch: 15.5[0m
[32m[2022-08-26 17:09:55,394] [    INFO][0m - loss: 1.235e-05, learning_rate: 2.52e-05, global_step: 320, interval_runtime: 0.695, interval_samples_per_second: 11.511, interval_steps_per_second: 14.389, epoch: 16.0[0m
[32m[2022-08-26 17:09:56,127] [    INFO][0m - loss: 0.00019444, learning_rate: 2.505e-05, global_step: 330, interval_runtime: 0.7335, interval_samples_per_second: 10.907, interval_steps_per_second: 13.633, epoch: 16.5[0m
[32m[2022-08-26 17:09:56,826] [    INFO][0m - loss: 1.839e-05, learning_rate: 2.49e-05, global_step: 340, interval_runtime: 0.6986, interval_samples_per_second: 11.452, interval_steps_per_second: 14.315, epoch: 17.0[0m
[32m[2022-08-26 17:09:57,560] [    INFO][0m - loss: 1.074e-05, learning_rate: 2.475e-05, global_step: 350, interval_runtime: 0.7339, interval_samples_per_second: 10.9, interval_steps_per_second: 13.625, epoch: 17.5[0m
[32m[2022-08-26 17:09:58,259] [    INFO][0m - loss: 0.05194329, learning_rate: 2.4599999999999998e-05, global_step: 360, interval_runtime: 0.6989, interval_samples_per_second: 11.447, interval_steps_per_second: 14.309, epoch: 18.0[0m
[32m[2022-08-26 17:09:58,995] [    INFO][0m - loss: 3.4e-06, learning_rate: 2.4449999999999998e-05, global_step: 370, interval_runtime: 0.7361, interval_samples_per_second: 10.868, interval_steps_per_second: 13.585, epoch: 18.5[0m
[32m[2022-08-26 17:09:59,691] [    INFO][0m - loss: 0.00145926, learning_rate: 2.43e-05, global_step: 380, interval_runtime: 0.6961, interval_samples_per_second: 11.493, interval_steps_per_second: 14.366, epoch: 19.0[0m
[32m[2022-08-26 17:10:00,428] [    INFO][0m - loss: 0.03703554, learning_rate: 2.415e-05, global_step: 390, interval_runtime: 0.737, interval_samples_per_second: 10.855, interval_steps_per_second: 13.569, epoch: 19.5[0m
[32m[2022-08-26 17:10:01,124] [    INFO][0m - loss: 0.09529089, learning_rate: 2.4e-05, global_step: 400, interval_runtime: 0.696, interval_samples_per_second: 11.493, interval_steps_per_second: 14.367, epoch: 20.0[0m
[32m[2022-08-26 17:10:01,124] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:10:01,124] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 17:10:01,124] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:10:01,125] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:10:01,125] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 17:10:01,595] [    INFO][0m - eval_loss: 1.0686523914337158, eval_accuracy: 0.90625, eval_runtime: 0.4706, eval_samples_per_second: 340.007, eval_steps_per_second: 10.625, epoch: 20.0[0m
[32m[2022-08-26 17:10:01,596] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-26 17:10:01,596] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:10:03,952] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-26 17:10:03,953] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-26 17:10:09,711] [    INFO][0m - loss: 4.77e-06, learning_rate: 2.385e-05, global_step: 410, interval_runtime: 8.5874, interval_samples_per_second: 0.932, interval_steps_per_second: 1.165, epoch: 20.5[0m
[32m[2022-08-26 17:10:10,407] [    INFO][0m - loss: 1.66e-05, learning_rate: 2.37e-05, global_step: 420, interval_runtime: 0.6953, interval_samples_per_second: 11.505, interval_steps_per_second: 14.382, epoch: 21.0[0m
[32m[2022-08-26 17:10:11,137] [    INFO][0m - loss: 1.8e-06, learning_rate: 2.3550000000000003e-05, global_step: 430, interval_runtime: 0.7303, interval_samples_per_second: 10.954, interval_steps_per_second: 13.693, epoch: 21.5[0m
[32m[2022-08-26 17:10:11,833] [    INFO][0m - loss: 2.138e-05, learning_rate: 2.3400000000000003e-05, global_step: 440, interval_runtime: 0.6965, interval_samples_per_second: 11.486, interval_steps_per_second: 14.358, epoch: 22.0[0m
[32m[2022-08-26 17:10:12,571] [    INFO][0m - loss: 3.17e-06, learning_rate: 2.3250000000000003e-05, global_step: 450, interval_runtime: 0.7378, interval_samples_per_second: 10.842, interval_steps_per_second: 13.553, epoch: 22.5[0m
[32m[2022-08-26 17:10:13,269] [    INFO][0m - loss: 6.14e-06, learning_rate: 2.3100000000000002e-05, global_step: 460, interval_runtime: 0.6974, interval_samples_per_second: 11.471, interval_steps_per_second: 14.339, epoch: 23.0[0m
[32m[2022-08-26 17:10:14,001] [    INFO][0m - loss: 1.92e-06, learning_rate: 2.2950000000000002e-05, global_step: 470, interval_runtime: 0.732, interval_samples_per_second: 10.929, interval_steps_per_second: 13.661, epoch: 23.5[0m
[32m[2022-08-26 17:10:14,701] [    INFO][0m - loss: 4.77e-06, learning_rate: 2.2800000000000002e-05, global_step: 480, interval_runtime: 0.7004, interval_samples_per_second: 11.422, interval_steps_per_second: 14.278, epoch: 24.0[0m
[32m[2022-08-26 17:10:15,433] [    INFO][0m - loss: 3.952e-05, learning_rate: 2.265e-05, global_step: 490, interval_runtime: 0.7321, interval_samples_per_second: 10.927, interval_steps_per_second: 13.659, epoch: 24.5[0m
[32m[2022-08-26 17:10:16,132] [    INFO][0m - loss: 3.21e-06, learning_rate: 2.25e-05, global_step: 500, interval_runtime: 0.699, interval_samples_per_second: 11.445, interval_steps_per_second: 14.306, epoch: 25.0[0m
[32m[2022-08-26 17:10:16,133] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:10:16,133] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 17:10:16,133] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:10:16,133] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:10:16,133] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 17:10:16,604] [    INFO][0m - eval_loss: 1.0379968881607056, eval_accuracy: 0.90625, eval_runtime: 0.4706, eval_samples_per_second: 339.997, eval_steps_per_second: 10.625, epoch: 25.0[0m
[32m[2022-08-26 17:10:16,604] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-08-26 17:10:16,604] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:10:18,946] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-08-26 17:10:18,946] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-08-26 17:10:24,971] [    INFO][0m - loss: 2.03e-06, learning_rate: 2.235e-05, global_step: 510, interval_runtime: 8.8391, interval_samples_per_second: 0.905, interval_steps_per_second: 1.131, epoch: 25.5[0m
[32m[2022-08-26 17:10:25,669] [    INFO][0m - loss: 1.648e-05, learning_rate: 2.22e-05, global_step: 520, interval_runtime: 0.6972, interval_samples_per_second: 11.474, interval_steps_per_second: 14.342, epoch: 26.0[0m
[32m[2022-08-26 17:10:26,403] [    INFO][0m - loss: 6.5e-07, learning_rate: 2.205e-05, global_step: 530, interval_runtime: 0.7348, interval_samples_per_second: 10.887, interval_steps_per_second: 13.609, epoch: 26.5[0m
[32m[2022-08-26 17:10:27,101] [    INFO][0m - loss: 8.43e-06, learning_rate: 2.19e-05, global_step: 540, interval_runtime: 0.6971, interval_samples_per_second: 11.476, interval_steps_per_second: 14.345, epoch: 27.0[0m
[32m[2022-08-26 17:10:27,835] [    INFO][0m - loss: 1.35e-06, learning_rate: 2.175e-05, global_step: 550, interval_runtime: 0.7343, interval_samples_per_second: 10.894, interval_steps_per_second: 13.618, epoch: 27.5[0m
[32m[2022-08-26 17:10:28,533] [    INFO][0m - loss: 1.51e-06, learning_rate: 2.16e-05, global_step: 560, interval_runtime: 0.6977, interval_samples_per_second: 11.466, interval_steps_per_second: 14.333, epoch: 28.0[0m
[32m[2022-08-26 17:10:29,272] [    INFO][0m - loss: 7.4e-07, learning_rate: 2.145e-05, global_step: 570, interval_runtime: 0.7396, interval_samples_per_second: 10.817, interval_steps_per_second: 13.522, epoch: 28.5[0m
[32m[2022-08-26 17:10:29,969] [    INFO][0m - loss: 0.0117096, learning_rate: 2.13e-05, global_step: 580, interval_runtime: 0.6965, interval_samples_per_second: 11.486, interval_steps_per_second: 14.357, epoch: 29.0[0m
[32m[2022-08-26 17:10:30,701] [    INFO][0m - loss: 0.01085695, learning_rate: 2.115e-05, global_step: 590, interval_runtime: 0.7321, interval_samples_per_second: 10.928, interval_steps_per_second: 13.659, epoch: 29.5[0m
[32m[2022-08-26 17:10:31,399] [    INFO][0m - loss: 5.809e-05, learning_rate: 2.1e-05, global_step: 600, interval_runtime: 0.6984, interval_samples_per_second: 11.455, interval_steps_per_second: 14.319, epoch: 30.0[0m
[32m[2022-08-26 17:10:31,399] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:10:31,400] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 17:10:31,400] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:10:31,400] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:10:31,400] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 17:10:31,877] [    INFO][0m - eval_loss: 1.4055246114730835, eval_accuracy: 0.8875, eval_runtime: 0.4768, eval_samples_per_second: 335.585, eval_steps_per_second: 10.487, epoch: 30.0[0m
[32m[2022-08-26 17:10:31,877] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-08-26 17:10:31,877] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:10:34,217] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-08-26 17:10:34,218] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-08-26 17:10:39,935] [    INFO][0m - loss: 1.357e-05, learning_rate: 2.085e-05, global_step: 610, interval_runtime: 8.5359, interval_samples_per_second: 0.937, interval_steps_per_second: 1.172, epoch: 30.5[0m
[32m[2022-08-26 17:10:40,632] [    INFO][0m - loss: 2.741e-05, learning_rate: 2.07e-05, global_step: 620, interval_runtime: 0.6974, interval_samples_per_second: 11.471, interval_steps_per_second: 14.339, epoch: 31.0[0m
[32m[2022-08-26 17:10:41,372] [    INFO][0m - loss: 1.322e-05, learning_rate: 2.055e-05, global_step: 630, interval_runtime: 0.7395, interval_samples_per_second: 10.818, interval_steps_per_second: 13.522, epoch: 31.5[0m
[32m[2022-08-26 17:10:42,071] [    INFO][0m - loss: 0.00052689, learning_rate: 2.04e-05, global_step: 640, interval_runtime: 0.6988, interval_samples_per_second: 11.448, interval_steps_per_second: 14.311, epoch: 32.0[0m
[32m[2022-08-26 17:10:42,802] [    INFO][0m - loss: 1.745e-05, learning_rate: 2.025e-05, global_step: 650, interval_runtime: 0.7309, interval_samples_per_second: 10.945, interval_steps_per_second: 13.682, epoch: 32.5[0m
[32m[2022-08-26 17:10:43,499] [    INFO][0m - loss: 1.827e-05, learning_rate: 2.01e-05, global_step: 660, interval_runtime: 0.6979, interval_samples_per_second: 11.462, interval_steps_per_second: 14.328, epoch: 33.0[0m
[32m[2022-08-26 17:10:44,234] [    INFO][0m - loss: 2.958e-05, learning_rate: 1.995e-05, global_step: 670, interval_runtime: 0.7345, interval_samples_per_second: 10.891, interval_steps_per_second: 13.614, epoch: 33.5[0m
[32m[2022-08-26 17:10:44,934] [    INFO][0m - loss: 5.39e-06, learning_rate: 1.98e-05, global_step: 680, interval_runtime: 0.7, interval_samples_per_second: 11.429, interval_steps_per_second: 14.286, epoch: 34.0[0m
[32m[2022-08-26 17:10:45,672] [    INFO][0m - loss: 3.23e-06, learning_rate: 1.965e-05, global_step: 690, interval_runtime: 0.7384, interval_samples_per_second: 10.834, interval_steps_per_second: 13.542, epoch: 34.5[0m
[32m[2022-08-26 17:10:46,371] [    INFO][0m - loss: 3e-06, learning_rate: 1.95e-05, global_step: 700, interval_runtime: 0.699, interval_samples_per_second: 11.445, interval_steps_per_second: 14.307, epoch: 35.0[0m
[32m[2022-08-26 17:10:46,372] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:10:46,372] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 17:10:46,372] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:10:46,372] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:10:46,372] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 17:10:46,848] [    INFO][0m - eval_loss: 1.571187973022461, eval_accuracy: 0.875, eval_runtime: 0.4757, eval_samples_per_second: 336.354, eval_steps_per_second: 10.511, epoch: 35.0[0m
[32m[2022-08-26 17:10:46,848] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-700[0m
[32m[2022-08-26 17:10:46,848] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:10:49,165] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-700/tokenizer_config.json[0m
[32m[2022-08-26 17:10:49,166] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-700/special_tokens_map.json[0m
[32m[2022-08-26 17:10:54,876] [    INFO][0m - loss: 1.32e-06, learning_rate: 1.935e-05, global_step: 710, interval_runtime: 8.5045, interval_samples_per_second: 0.941, interval_steps_per_second: 1.176, epoch: 35.5[0m
[32m[2022-08-26 17:10:55,573] [    INFO][0m - loss: 1.67e-06, learning_rate: 1.9200000000000003e-05, global_step: 720, interval_runtime: 0.697, interval_samples_per_second: 11.477, interval_steps_per_second: 14.346, epoch: 36.0[0m
[32m[2022-08-26 17:10:56,311] [    INFO][0m - loss: 5.6e-07, learning_rate: 1.9050000000000002e-05, global_step: 730, interval_runtime: 0.7377, interval_samples_per_second: 10.844, interval_steps_per_second: 13.556, epoch: 36.5[0m
[32m[2022-08-26 17:10:57,009] [    INFO][0m - loss: 3.31e-06, learning_rate: 1.8900000000000002e-05, global_step: 740, interval_runtime: 0.6985, interval_samples_per_second: 11.453, interval_steps_per_second: 14.317, epoch: 37.0[0m
[32m[2022-08-26 17:10:57,742] [    INFO][0m - loss: 9e-07, learning_rate: 1.8750000000000002e-05, global_step: 750, interval_runtime: 0.733, interval_samples_per_second: 10.914, interval_steps_per_second: 13.643, epoch: 37.5[0m
[32m[2022-08-26 17:10:58,442] [    INFO][0m - loss: 2.66e-06, learning_rate: 1.86e-05, global_step: 760, interval_runtime: 0.6995, interval_samples_per_second: 11.437, interval_steps_per_second: 14.297, epoch: 38.0[0m
[32m[2022-08-26 17:10:59,179] [    INFO][0m - loss: 7.4e-07, learning_rate: 1.845e-05, global_step: 770, interval_runtime: 0.7376, interval_samples_per_second: 10.845, interval_steps_per_second: 13.557, epoch: 38.5[0m
[32m[2022-08-26 17:10:59,878] [    INFO][0m - loss: 2.12e-06, learning_rate: 1.83e-05, global_step: 780, interval_runtime: 0.6991, interval_samples_per_second: 11.443, interval_steps_per_second: 14.304, epoch: 39.0[0m
[32m[2022-08-26 17:11:00,620] [    INFO][0m - loss: 2.15e-06, learning_rate: 1.815e-05, global_step: 790, interval_runtime: 0.7415, interval_samples_per_second: 10.788, interval_steps_per_second: 13.486, epoch: 39.5[0m
[32m[2022-08-26 17:11:01,319] [    INFO][0m - loss: 0.16318663, learning_rate: 1.8e-05, global_step: 800, interval_runtime: 0.699, interval_samples_per_second: 11.445, interval_steps_per_second: 14.307, epoch: 40.0[0m
[32m[2022-08-26 17:11:01,319] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:11:01,320] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 17:11:01,320] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:11:01,320] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:11:01,320] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 17:11:01,803] [    INFO][0m - eval_loss: 1.015101671218872, eval_accuracy: 0.875, eval_runtime: 0.4828, eval_samples_per_second: 331.412, eval_steps_per_second: 10.357, epoch: 40.0[0m
[32m[2022-08-26 17:11:01,803] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-800[0m
[32m[2022-08-26 17:11:01,803] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:11:04,110] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-800/tokenizer_config.json[0m
[32m[2022-08-26 17:11:04,110] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-800/special_tokens_map.json[0m
[32m[2022-08-26 17:11:09,006] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-26 17:11:09,006] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-400 (score: 0.90625).[0m
[32m[2022-08-26 17:11:09,716] [    INFO][0m - train_runtime: 122.1146, train_samples_per_second: 131.024, train_steps_per_second: 16.378, train_loss: 0.059349962953023694, epoch: 40.0[0m
[32m[2022-08-26 17:11:09,757] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-26 17:11:09,757] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:11:12,087] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-26 17:11:12,087] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-26 17:11:12,088] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-26 17:11:12,088] [    INFO][0m -   epoch                    =       40.0[0m
[32m[2022-08-26 17:11:12,088] [    INFO][0m -   train_loss               =     0.0593[0m
[32m[2022-08-26 17:11:12,089] [    INFO][0m -   train_runtime            = 0:02:02.11[0m
[32m[2022-08-26 17:11:12,089] [    INFO][0m -   train_samples_per_second =    131.024[0m
[32m[2022-08-26 17:11:12,089] [    INFO][0m -   train_steps_per_second   =     16.378[0m
[32m[2022-08-26 17:11:12,093] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-26 17:11:12,093] [    INFO][0m -   Num examples = 610[0m
[32m[2022-08-26 17:11:12,093] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:11:12,093] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:11:12,093] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-08-26 17:11:13,917] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-26 17:11:13,917] [    INFO][0m -   test_accuracy           =     0.8738[0m
[32m[2022-08-26 17:11:13,917] [    INFO][0m -   test_loss               =     1.3703[0m
[32m[2022-08-26 17:11:13,917] [    INFO][0m -   test_runtime            = 0:00:01.82[0m
[32m[2022-08-26 17:11:13,917] [    INFO][0m -   test_samples_per_second =    334.433[0m
[32m[2022-08-26 17:11:13,917] [    INFO][0m -   test_steps_per_second   =     10.965[0m
[32m[2022-08-26 17:11:13,918] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-26 17:11:13,918] [    INFO][0m -   Num examples = 753[0m
[32m[2022-08-26 17:11:13,918] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:11:13,918] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:11:13,918] [    INFO][0m -   Total prediction steps = 24[0m
[32m[2022-08-26 17:11:16,686] [    INFO][0m - Predictions for eprstmt saved to ./fewclue_submit_examples.[0m
 
==========
ocnli
==========
 
[33m[2022-08-26 17:11:20,624] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-26 17:11:20,624] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-26 17:11:20,625] [    INFO][0m - ============================================================[0m
[32m[2022-08-26 17:11:20,625] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-26 17:11:20,625] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-26 17:11:20,625] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-26 17:11:20,625] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-26 17:11:20,625] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-08-26 17:11:20,625] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-26 17:11:20,625] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-08-26 17:11:20,625] [    INFO][0m - [0m
[32m[2022-08-26 17:11:20,625] [    INFO][0m - ============================================================[0m
[32m[2022-08-26 17:11:20,626] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-26 17:11:20,626] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-26 17:11:20,626] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-26 17:11:20,626] [    INFO][0m - prompt                        :{'hard':'ËØ∑Áî®Ê≠£Á°ÆÁöÑËøûÊé•ËØçÂ°´Á©∫Ôºö'}{'text':'text_a'}{'mask'}{'mask'}{'text':'text_b'}[0m
[32m[2022-08-26 17:11:20,626] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-26 17:11:20,626] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-26 17:11:20,626] [    INFO][0m - task_name                     :ocnli[0m
[32m[2022-08-26 17:11:20,626] [    INFO][0m - [0m
[32m[2022-08-26 17:11:20,626] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0826 17:11:20.628139 58485 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0826 17:11:20.632540 58485 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-26 17:11:24,602] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-08-26 17:11:24,628] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-08-26 17:11:24,628] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[32m[2022-08-26 17:11:24,629] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': 'ËØ∑Áî®Ê≠£Á°ÆÁöÑËøûÊé•ËØçÂ°´Á©∫Ôºö'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'text': 'text_b'}][0m
2022-08-26 17:11:24,636 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-26 17:11:24,765] [    INFO][0m - ============================================================[0m
[32m[2022-08-26 17:11:24,765] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-26 17:11:24,765] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-26 17:11:24,765] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-26 17:11:24,765] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-26 17:11:24,765] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-26 17:11:24,765] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-26 17:11:24,765] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-26 17:11:24,765] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-26 17:11:24,766] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-26 17:11:24,766] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-26 17:11:24,766] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-26 17:11:24,766] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-26 17:11:24,766] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-26 17:11:24,766] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-26 17:11:24,766] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-26 17:11:24,766] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-26 17:11:24,766] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-26 17:11:24,766] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-26 17:11:24,766] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-26 17:11:24,766] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-26 17:11:24,767] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-26 17:11:24,767] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-26 17:11:24,767] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-26 17:11:24,767] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-26 17:11:24,767] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-26 17:11:24,767] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-26 17:11:24,767] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-26 17:11:24,767] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-26 17:11:24,767] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-26 17:11:24,767] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-26 17:11:24,767] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-26 17:11:24,767] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-26 17:11:24,767] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-26 17:11:24,768] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-26 17:11:24,768] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-26 17:11:24,768] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-26 17:11:24,768] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug26_17-11-20_instance-3bwob41y-01[0m
[32m[2022-08-26 17:11:24,768] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-26 17:11:24,768] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-26 17:11:24,768] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-26 17:11:24,768] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-26 17:11:24,768] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-26 17:11:24,768] [    INFO][0m - max_seq_length                :64[0m
[32m[2022-08-26 17:11:24,768] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-26 17:11:24,768] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-26 17:11:24,768] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-26 17:11:24,769] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-26 17:11:24,769] [    INFO][0m - num_train_epochs              :100.0[0m
[32m[2022-08-26 17:11:24,769] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-26 17:11:24,769] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-26 17:11:24,769] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-26 17:11:24,769] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-26 17:11:24,769] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-26 17:11:24,769] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-26 17:11:24,769] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-26 17:11:24,769] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-26 17:11:24,769] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-26 17:11:24,769] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-26 17:11:24,769] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-26 17:11:24,769] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-26 17:11:24,770] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-26 17:11:24,770] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-26 17:11:24,770] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-26 17:11:24,770] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-26 17:11:24,770] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-26 17:11:24,770] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-26 17:11:24,770] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-26 17:11:24,770] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-26 17:11:24,770] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-26 17:11:24,770] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-26 17:11:24,770] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-26 17:11:24,770] [    INFO][0m - seed                          :42[0m
[32m[2022-08-26 17:11:24,770] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-26 17:11:24,771] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-26 17:11:24,771] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-26 17:11:24,771] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-26 17:11:24,771] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-26 17:11:24,771] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-26 17:11:24,771] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-26 17:11:24,771] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-26 17:11:24,771] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-26 17:11:24,771] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-26 17:11:24,771] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-26 17:11:24,771] [    INFO][0m - [0m
[32m[2022-08-26 17:11:24,773] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-26 17:11:24,773] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 17:11:24,773] [    INFO][0m -   Num Epochs = 100[0m
[32m[2022-08-26 17:11:24,774] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-26 17:11:24,774] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-26 17:11:24,774] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-26 17:11:24,774] [    INFO][0m -   Total optimization steps = 2000.0[0m
[32m[2022-08-26 17:11:24,774] [    INFO][0m -   Total num train samples = 16000[0m
[32m[2022-08-26 17:11:26,477] [    INFO][0m - loss: 1.31014833, learning_rate: 2.985e-05, global_step: 10, interval_runtime: 1.7027, interval_samples_per_second: 4.698, interval_steps_per_second: 5.873, epoch: 0.5[0m
[32m[2022-08-26 17:11:27,132] [    INFO][0m - loss: 1.17538681, learning_rate: 2.97e-05, global_step: 20, interval_runtime: 0.6548, interval_samples_per_second: 12.217, interval_steps_per_second: 15.271, epoch: 1.0[0m
[32m[2022-08-26 17:11:27,855] [    INFO][0m - loss: 1.10235558, learning_rate: 2.955e-05, global_step: 30, interval_runtime: 0.7225, interval_samples_per_second: 11.072, interval_steps_per_second: 13.84, epoch: 1.5[0m
[32m[2022-08-26 17:11:28,519] [    INFO][0m - loss: 1.05798092, learning_rate: 2.94e-05, global_step: 40, interval_runtime: 0.6642, interval_samples_per_second: 12.044, interval_steps_per_second: 15.055, epoch: 2.0[0m
[32m[2022-08-26 17:11:29,246] [    INFO][0m - loss: 0.91890869, learning_rate: 2.925e-05, global_step: 50, interval_runtime: 0.7267, interval_samples_per_second: 11.008, interval_steps_per_second: 13.76, epoch: 2.5[0m
[32m[2022-08-26 17:11:29,918] [    INFO][0m - loss: 0.86847324, learning_rate: 2.91e-05, global_step: 60, interval_runtime: 0.6719, interval_samples_per_second: 11.907, interval_steps_per_second: 14.884, epoch: 3.0[0m
[32m[2022-08-26 17:11:30,642] [    INFO][0m - loss: 0.62036133, learning_rate: 2.895e-05, global_step: 70, interval_runtime: 0.7241, interval_samples_per_second: 11.048, interval_steps_per_second: 13.81, epoch: 3.5[0m
[32m[2022-08-26 17:11:31,241] [    INFO][0m - loss: 0.63369336, learning_rate: 2.88e-05, global_step: 80, interval_runtime: 0.5987, interval_samples_per_second: 13.363, interval_steps_per_second: 16.703, epoch: 4.0[0m
[32m[2022-08-26 17:11:31,845] [    INFO][0m - loss: 0.42069278, learning_rate: 2.865e-05, global_step: 90, interval_runtime: 0.6046, interval_samples_per_second: 13.233, interval_steps_per_second: 16.541, epoch: 4.5[0m
[32m[2022-08-26 17:11:32,373] [    INFO][0m - loss: 0.39254513, learning_rate: 2.8499999999999998e-05, global_step: 100, interval_runtime: 0.5279, interval_samples_per_second: 15.155, interval_steps_per_second: 18.944, epoch: 5.0[0m
[32m[2022-08-26 17:11:32,374] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:11:32,374] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 17:11:32,374] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:11:32,374] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:11:32,374] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 17:11:32,680] [    INFO][0m - eval_loss: 1.622277855873108, eval_accuracy: 0.4625, eval_runtime: 0.3054, eval_samples_per_second: 523.957, eval_steps_per_second: 16.374, epoch: 5.0[0m
[32m[2022-08-26 17:11:32,680] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-26 17:11:32,680] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:11:35,423] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-26 17:11:35,424] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-26 17:11:41,254] [    INFO][0m - loss: 0.23003256, learning_rate: 2.8349999999999998e-05, global_step: 110, interval_runtime: 8.8811, interval_samples_per_second: 0.901, interval_steps_per_second: 1.126, epoch: 5.5[0m
[32m[2022-08-26 17:11:41,756] [    INFO][0m - loss: 0.22444544, learning_rate: 2.8199999999999998e-05, global_step: 120, interval_runtime: 0.5018, interval_samples_per_second: 15.944, interval_steps_per_second: 19.93, epoch: 6.0[0m
[32m[2022-08-26 17:11:42,326] [    INFO][0m - loss: 0.12971522, learning_rate: 2.805e-05, global_step: 130, interval_runtime: 0.5708, interval_samples_per_second: 14.016, interval_steps_per_second: 17.52, epoch: 6.5[0m
[32m[2022-08-26 17:11:42,832] [    INFO][0m - loss: 0.07922255, learning_rate: 2.79e-05, global_step: 140, interval_runtime: 0.5056, interval_samples_per_second: 15.824, interval_steps_per_second: 19.78, epoch: 7.0[0m
[32m[2022-08-26 17:11:43,431] [    INFO][0m - loss: 0.03648945, learning_rate: 2.7750000000000004e-05, global_step: 150, interval_runtime: 0.5988, interval_samples_per_second: 13.359, interval_steps_per_second: 16.699, epoch: 7.5[0m
[32m[2022-08-26 17:11:43,944] [    INFO][0m - loss: 0.2434711, learning_rate: 2.7600000000000003e-05, global_step: 160, interval_runtime: 0.5135, interval_samples_per_second: 15.578, interval_steps_per_second: 19.473, epoch: 8.0[0m
[32m[2022-08-26 17:11:44,532] [    INFO][0m - loss: 0.09539574, learning_rate: 2.7450000000000003e-05, global_step: 170, interval_runtime: 0.5865, interval_samples_per_second: 13.64, interval_steps_per_second: 17.05, epoch: 8.5[0m
[32m[2022-08-26 17:11:45,058] [    INFO][0m - loss: 0.004643, learning_rate: 2.7300000000000003e-05, global_step: 180, interval_runtime: 0.5269, interval_samples_per_second: 15.182, interval_steps_per_second: 18.978, epoch: 9.0[0m
[32m[2022-08-26 17:11:45,654] [    INFO][0m - loss: 0.01437195, learning_rate: 2.7150000000000003e-05, global_step: 190, interval_runtime: 0.5961, interval_samples_per_second: 13.42, interval_steps_per_second: 16.774, epoch: 9.5[0m
[32m[2022-08-26 17:11:46,177] [    INFO][0m - loss: 0.02266617, learning_rate: 2.7000000000000002e-05, global_step: 200, interval_runtime: 0.5229, interval_samples_per_second: 15.299, interval_steps_per_second: 19.124, epoch: 10.0[0m
[32m[2022-08-26 17:11:46,177] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:11:46,177] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 17:11:46,177] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:11:46,178] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:11:46,178] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 17:11:46,487] [    INFO][0m - eval_loss: 2.9954209327697754, eval_accuracy: 0.49375, eval_runtime: 0.3093, eval_samples_per_second: 517.246, eval_steps_per_second: 16.164, epoch: 10.0[0m
[32m[2022-08-26 17:11:46,488] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-26 17:11:46,488] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:11:49,093] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-26 17:11:49,094] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-26 17:11:55,088] [    INFO][0m - loss: 0.00645884, learning_rate: 2.6850000000000002e-05, global_step: 210, interval_runtime: 8.9106, interval_samples_per_second: 0.898, interval_steps_per_second: 1.122, epoch: 10.5[0m
[32m[2022-08-26 17:11:55,589] [    INFO][0m - loss: 0.00824456, learning_rate: 2.6700000000000002e-05, global_step: 220, interval_runtime: 0.5012, interval_samples_per_second: 15.963, interval_steps_per_second: 19.954, epoch: 11.0[0m
[32m[2022-08-26 17:11:56,150] [    INFO][0m - loss: 0.0557749, learning_rate: 2.655e-05, global_step: 230, interval_runtime: 0.5618, interval_samples_per_second: 14.24, interval_steps_per_second: 17.8, epoch: 11.5[0m
[32m[2022-08-26 17:11:56,657] [    INFO][0m - loss: 0.05400636, learning_rate: 2.64e-05, global_step: 240, interval_runtime: 0.5065, interval_samples_per_second: 15.796, interval_steps_per_second: 19.745, epoch: 12.0[0m
[32m[2022-08-26 17:11:57,232] [    INFO][0m - loss: 0.00071216, learning_rate: 2.625e-05, global_step: 250, interval_runtime: 0.5747, interval_samples_per_second: 13.92, interval_steps_per_second: 17.399, epoch: 12.5[0m
[32m[2022-08-26 17:11:57,736] [    INFO][0m - loss: 0.00032598, learning_rate: 2.61e-05, global_step: 260, interval_runtime: 0.5044, interval_samples_per_second: 15.861, interval_steps_per_second: 19.826, epoch: 13.0[0m
[32m[2022-08-26 17:11:58,306] [    INFO][0m - loss: 0.00450146, learning_rate: 2.595e-05, global_step: 270, interval_runtime: 0.5702, interval_samples_per_second: 14.03, interval_steps_per_second: 17.537, epoch: 13.5[0m
[32m[2022-08-26 17:11:58,809] [    INFO][0m - loss: 0.00049964, learning_rate: 2.58e-05, global_step: 280, interval_runtime: 0.5029, interval_samples_per_second: 15.907, interval_steps_per_second: 19.884, epoch: 14.0[0m
[32m[2022-08-26 17:11:59,390] [    INFO][0m - loss: 0.00512536, learning_rate: 2.565e-05, global_step: 290, interval_runtime: 0.5809, interval_samples_per_second: 13.773, interval_steps_per_second: 17.216, epoch: 14.5[0m
[32m[2022-08-26 17:11:59,904] [    INFO][0m - loss: 0.00022862, learning_rate: 2.55e-05, global_step: 300, interval_runtime: 0.5137, interval_samples_per_second: 15.572, interval_steps_per_second: 19.465, epoch: 15.0[0m
[32m[2022-08-26 17:11:59,904] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:11:59,904] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 17:11:59,904] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:11:59,905] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:11:59,905] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 17:12:00,211] [    INFO][0m - eval_loss: 4.013197898864746, eval_accuracy: 0.475, eval_runtime: 0.3057, eval_samples_per_second: 523.462, eval_steps_per_second: 16.358, epoch: 15.0[0m
[32m[2022-08-26 17:12:00,211] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-26 17:12:00,211] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:12:02,566] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-26 17:12:02,567] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-26 17:12:08,616] [    INFO][0m - loss: 0.00099718, learning_rate: 2.535e-05, global_step: 310, interval_runtime: 8.7122, interval_samples_per_second: 0.918, interval_steps_per_second: 1.148, epoch: 15.5[0m
[32m[2022-08-26 17:12:09,149] [    INFO][0m - loss: 0.00850848, learning_rate: 2.52e-05, global_step: 320, interval_runtime: 0.5331, interval_samples_per_second: 15.007, interval_steps_per_second: 18.758, epoch: 16.0[0m
[32m[2022-08-26 17:12:09,724] [    INFO][0m - loss: 0.01035284, learning_rate: 2.505e-05, global_step: 330, interval_runtime: 0.5751, interval_samples_per_second: 13.911, interval_steps_per_second: 17.389, epoch: 16.5[0m
[32m[2022-08-26 17:12:10,235] [    INFO][0m - loss: 0.00013119, learning_rate: 2.49e-05, global_step: 340, interval_runtime: 0.5107, interval_samples_per_second: 15.664, interval_steps_per_second: 19.58, epoch: 17.0[0m
[32m[2022-08-26 17:12:10,808] [    INFO][0m - loss: 0.00094341, learning_rate: 2.475e-05, global_step: 350, interval_runtime: 0.5728, interval_samples_per_second: 13.966, interval_steps_per_second: 17.458, epoch: 17.5[0m
[32m[2022-08-26 17:12:11,321] [    INFO][0m - loss: 2.926e-05, learning_rate: 2.4599999999999998e-05, global_step: 360, interval_runtime: 0.5129, interval_samples_per_second: 15.597, interval_steps_per_second: 19.496, epoch: 18.0[0m
[32m[2022-08-26 17:12:11,900] [    INFO][0m - loss: 0.06426175, learning_rate: 2.4449999999999998e-05, global_step: 370, interval_runtime: 0.5796, interval_samples_per_second: 13.803, interval_steps_per_second: 17.254, epoch: 18.5[0m
[32m[2022-08-26 17:12:12,415] [    INFO][0m - loss: 0.00448555, learning_rate: 2.43e-05, global_step: 380, interval_runtime: 0.5143, interval_samples_per_second: 15.555, interval_steps_per_second: 19.443, epoch: 19.0[0m
[32m[2022-08-26 17:12:12,991] [    INFO][0m - loss: 0.07906182, learning_rate: 2.415e-05, global_step: 390, interval_runtime: 0.5761, interval_samples_per_second: 13.885, interval_steps_per_second: 17.357, epoch: 19.5[0m
[32m[2022-08-26 17:12:13,505] [    INFO][0m - loss: 6.095e-05, learning_rate: 2.4e-05, global_step: 400, interval_runtime: 0.514, interval_samples_per_second: 15.565, interval_steps_per_second: 19.456, epoch: 20.0[0m
[32m[2022-08-26 17:12:13,505] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:12:13,505] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 17:12:13,505] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:12:13,506] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:12:13,506] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 17:12:13,815] [    INFO][0m - eval_loss: 4.124899864196777, eval_accuracy: 0.46875, eval_runtime: 0.3095, eval_samples_per_second: 516.943, eval_steps_per_second: 16.154, epoch: 20.0[0m
[32m[2022-08-26 17:12:13,816] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-26 17:12:13,816] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:12:16,224] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-26 17:12:16,225] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-26 17:12:22,167] [    INFO][0m - loss: 8.549e-05, learning_rate: 2.385e-05, global_step: 410, interval_runtime: 8.6616, interval_samples_per_second: 0.924, interval_steps_per_second: 1.155, epoch: 20.5[0m
[32m[2022-08-26 17:12:22,673] [    INFO][0m - loss: 0.00013738, learning_rate: 2.37e-05, global_step: 420, interval_runtime: 0.5068, interval_samples_per_second: 15.785, interval_steps_per_second: 19.731, epoch: 21.0[0m
[32m[2022-08-26 17:12:23,247] [    INFO][0m - loss: 0.05558456, learning_rate: 2.3550000000000003e-05, global_step: 430, interval_runtime: 0.5734, interval_samples_per_second: 13.952, interval_steps_per_second: 17.441, epoch: 21.5[0m
[32m[2022-08-26 17:12:23,752] [    INFO][0m - loss: 0.07479104, learning_rate: 2.3400000000000003e-05, global_step: 440, interval_runtime: 0.5059, interval_samples_per_second: 15.814, interval_steps_per_second: 19.768, epoch: 22.0[0m
[32m[2022-08-26 17:12:24,330] [    INFO][0m - loss: 0.14729911, learning_rate: 2.3250000000000003e-05, global_step: 450, interval_runtime: 0.5771, interval_samples_per_second: 13.863, interval_steps_per_second: 17.329, epoch: 22.5[0m
[32m[2022-08-26 17:12:24,839] [    INFO][0m - loss: 0.04370044, learning_rate: 2.3100000000000002e-05, global_step: 460, interval_runtime: 0.5095, interval_samples_per_second: 15.703, interval_steps_per_second: 19.629, epoch: 23.0[0m
[32m[2022-08-26 17:12:25,423] [    INFO][0m - loss: 0.02639089, learning_rate: 2.2950000000000002e-05, global_step: 470, interval_runtime: 0.5832, interval_samples_per_second: 13.718, interval_steps_per_second: 17.148, epoch: 23.5[0m
[32m[2022-08-26 17:12:25,929] [    INFO][0m - loss: 0.00118141, learning_rate: 2.2800000000000002e-05, global_step: 480, interval_runtime: 0.5071, interval_samples_per_second: 15.777, interval_steps_per_second: 19.721, epoch: 24.0[0m
[32m[2022-08-26 17:12:26,500] [    INFO][0m - loss: 0.00033595, learning_rate: 2.265e-05, global_step: 490, interval_runtime: 0.5713, interval_samples_per_second: 14.004, interval_steps_per_second: 17.505, epoch: 24.5[0m
[32m[2022-08-26 17:12:27,008] [    INFO][0m - loss: 0.00033453, learning_rate: 2.25e-05, global_step: 500, interval_runtime: 0.5072, interval_samples_per_second: 15.772, interval_steps_per_second: 19.715, epoch: 25.0[0m
[32m[2022-08-26 17:12:27,008] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:12:27,008] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 17:12:27,008] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:12:27,008] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:12:27,008] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 17:12:27,312] [    INFO][0m - eval_loss: 3.925367832183838, eval_accuracy: 0.49375, eval_runtime: 0.3031, eval_samples_per_second: 527.798, eval_steps_per_second: 16.494, epoch: 25.0[0m
[32m[2022-08-26 17:12:27,312] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-08-26 17:12:27,312] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:12:29,682] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-08-26 17:12:29,682] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-08-26 17:12:35,256] [    INFO][0m - loss: 6.95e-05, learning_rate: 2.235e-05, global_step: 510, interval_runtime: 8.2484, interval_samples_per_second: 0.97, interval_steps_per_second: 1.212, epoch: 25.5[0m
[32m[2022-08-26 17:12:35,765] [    INFO][0m - loss: 0.00031456, learning_rate: 2.22e-05, global_step: 520, interval_runtime: 0.5084, interval_samples_per_second: 15.734, interval_steps_per_second: 19.668, epoch: 26.0[0m
[32m[2022-08-26 17:12:36,331] [    INFO][0m - loss: 0.02430015, learning_rate: 2.205e-05, global_step: 530, interval_runtime: 0.566, interval_samples_per_second: 14.133, interval_steps_per_second: 17.667, epoch: 26.5[0m
[32m[2022-08-26 17:12:36,846] [    INFO][0m - loss: 7.62e-05, learning_rate: 2.19e-05, global_step: 540, interval_runtime: 0.5156, interval_samples_per_second: 15.516, interval_steps_per_second: 19.396, epoch: 27.0[0m
[32m[2022-08-26 17:12:37,408] [    INFO][0m - loss: 0.05641704, learning_rate: 2.175e-05, global_step: 550, interval_runtime: 0.562, interval_samples_per_second: 14.235, interval_steps_per_second: 17.794, epoch: 27.5[0m
[32m[2022-08-26 17:12:37,907] [    INFO][0m - loss: 0.01495241, learning_rate: 2.16e-05, global_step: 560, interval_runtime: 0.4989, interval_samples_per_second: 16.036, interval_steps_per_second: 20.045, epoch: 28.0[0m
[32m[2022-08-26 17:12:38,471] [    INFO][0m - loss: 4.456e-05, learning_rate: 2.145e-05, global_step: 570, interval_runtime: 0.5645, interval_samples_per_second: 14.172, interval_steps_per_second: 17.715, epoch: 28.5[0m
[32m[2022-08-26 17:12:38,975] [    INFO][0m - loss: 0.00034712, learning_rate: 2.13e-05, global_step: 580, interval_runtime: 0.5038, interval_samples_per_second: 15.881, interval_steps_per_second: 19.851, epoch: 29.0[0m
[32m[2022-08-26 17:12:39,542] [    INFO][0m - loss: 4.042e-05, learning_rate: 2.115e-05, global_step: 590, interval_runtime: 0.5662, interval_samples_per_second: 14.128, interval_steps_per_second: 17.66, epoch: 29.5[0m
[32m[2022-08-26 17:12:40,039] [    INFO][0m - loss: 0.00262406, learning_rate: 2.1e-05, global_step: 600, interval_runtime: 0.498, interval_samples_per_second: 16.066, interval_steps_per_second: 20.082, epoch: 30.0[0m
[32m[2022-08-26 17:12:40,040] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:12:40,040] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 17:12:40,040] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:12:40,040] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:12:40,040] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 17:12:40,353] [    INFO][0m - eval_loss: 3.978874683380127, eval_accuracy: 0.50625, eval_runtime: 0.3125, eval_samples_per_second: 511.942, eval_steps_per_second: 15.998, epoch: 30.0[0m
[32m[2022-08-26 17:12:40,353] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-08-26 17:12:40,353] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:12:42,703] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-08-26 17:12:42,703] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-08-26 17:12:48,552] [    INFO][0m - loss: 0.00049837, learning_rate: 2.085e-05, global_step: 610, interval_runtime: 8.5119, interval_samples_per_second: 0.94, interval_steps_per_second: 1.175, epoch: 30.5[0m
[32m[2022-08-26 17:12:49,071] [    INFO][0m - loss: 2.503e-05, learning_rate: 2.07e-05, global_step: 620, interval_runtime: 0.5194, interval_samples_per_second: 15.402, interval_steps_per_second: 19.252, epoch: 31.0[0m
[32m[2022-08-26 17:12:49,657] [    INFO][0m - loss: 0.00012613, learning_rate: 2.055e-05, global_step: 630, interval_runtime: 0.5856, interval_samples_per_second: 13.66, interval_steps_per_second: 17.075, epoch: 31.5[0m
[32m[2022-08-26 17:12:50,179] [    INFO][0m - loss: 0.00025291, learning_rate: 2.04e-05, global_step: 640, interval_runtime: 0.5221, interval_samples_per_second: 15.321, interval_steps_per_second: 19.152, epoch: 32.0[0m
[32m[2022-08-26 17:12:50,775] [    INFO][0m - loss: 0.00021937, learning_rate: 2.025e-05, global_step: 650, interval_runtime: 0.5955, interval_samples_per_second: 13.435, interval_steps_per_second: 16.794, epoch: 32.5[0m
[32m[2022-08-26 17:12:51,288] [    INFO][0m - loss: 6.936e-05, learning_rate: 2.01e-05, global_step: 660, interval_runtime: 0.5143, interval_samples_per_second: 15.556, interval_steps_per_second: 19.446, epoch: 33.0[0m
[32m[2022-08-26 17:12:51,888] [    INFO][0m - loss: 0.0001002, learning_rate: 1.995e-05, global_step: 670, interval_runtime: 0.5993, interval_samples_per_second: 13.35, interval_steps_per_second: 16.687, epoch: 33.5[0m
[32m[2022-08-26 17:12:52,412] [    INFO][0m - loss: 4.189e-05, learning_rate: 1.98e-05, global_step: 680, interval_runtime: 0.5241, interval_samples_per_second: 15.264, interval_steps_per_second: 19.08, epoch: 34.0[0m
[32m[2022-08-26 17:12:53,011] [    INFO][0m - loss: 7.2e-06, learning_rate: 1.965e-05, global_step: 690, interval_runtime: 0.5996, interval_samples_per_second: 13.342, interval_steps_per_second: 16.678, epoch: 34.5[0m
[32m[2022-08-26 17:12:53,526] [    INFO][0m - loss: 6.061e-05, learning_rate: 1.95e-05, global_step: 700, interval_runtime: 0.5145, interval_samples_per_second: 15.548, interval_steps_per_second: 19.434, epoch: 35.0[0m
[32m[2022-08-26 17:12:53,526] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:12:53,526] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 17:12:53,527] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:12:53,527] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:12:53,527] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 17:12:53,830] [    INFO][0m - eval_loss: 4.42822265625, eval_accuracy: 0.46875, eval_runtime: 0.3035, eval_samples_per_second: 527.247, eval_steps_per_second: 16.476, epoch: 35.0[0m
[32m[2022-08-26 17:12:53,831] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-700[0m
[32m[2022-08-26 17:12:53,831] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:12:56,513] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-700/tokenizer_config.json[0m
[32m[2022-08-26 17:12:56,513] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-700/special_tokens_map.json[0m
[32m[2022-08-26 17:13:02,344] [    INFO][0m - loss: 2.329e-05, learning_rate: 1.935e-05, global_step: 710, interval_runtime: 8.8179, interval_samples_per_second: 0.907, interval_steps_per_second: 1.134, epoch: 35.5[0m
[32m[2022-08-26 17:13:02,845] [    INFO][0m - loss: 4.72e-05, learning_rate: 1.9200000000000003e-05, global_step: 720, interval_runtime: 0.501, interval_samples_per_second: 15.969, interval_steps_per_second: 19.962, epoch: 36.0[0m
[32m[2022-08-26 17:13:03,416] [    INFO][0m - loss: 1.565e-05, learning_rate: 1.9050000000000002e-05, global_step: 730, interval_runtime: 0.5711, interval_samples_per_second: 14.009, interval_steps_per_second: 17.511, epoch: 36.5[0m
[32m[2022-08-26 17:13:03,918] [    INFO][0m - loss: 2.766e-05, learning_rate: 1.8900000000000002e-05, global_step: 740, interval_runtime: 0.5017, interval_samples_per_second: 15.946, interval_steps_per_second: 19.932, epoch: 37.0[0m
[32m[2022-08-26 17:13:04,485] [    INFO][0m - loss: 0.00207263, learning_rate: 1.8750000000000002e-05, global_step: 750, interval_runtime: 0.5673, interval_samples_per_second: 14.102, interval_steps_per_second: 17.628, epoch: 37.5[0m
[32m[2022-08-26 17:13:04,984] [    INFO][0m - loss: 0.00479077, learning_rate: 1.86e-05, global_step: 760, interval_runtime: 0.4987, interval_samples_per_second: 16.041, interval_steps_per_second: 20.052, epoch: 38.0[0m
[32m[2022-08-26 17:13:05,549] [    INFO][0m - loss: 1.456e-05, learning_rate: 1.845e-05, global_step: 770, interval_runtime: 0.5652, interval_samples_per_second: 14.154, interval_steps_per_second: 17.692, epoch: 38.5[0m
[32m[2022-08-26 17:13:06,050] [    INFO][0m - loss: 0.13773681, learning_rate: 1.83e-05, global_step: 780, interval_runtime: 0.5016, interval_samples_per_second: 15.949, interval_steps_per_second: 19.937, epoch: 39.0[0m
[32m[2022-08-26 17:13:06,617] [    INFO][0m - loss: 3.801e-05, learning_rate: 1.815e-05, global_step: 790, interval_runtime: 0.5669, interval_samples_per_second: 14.111, interval_steps_per_second: 17.638, epoch: 39.5[0m
[32m[2022-08-26 17:13:07,125] [    INFO][0m - loss: 5.091e-05, learning_rate: 1.8e-05, global_step: 800, interval_runtime: 0.5075, interval_samples_per_second: 15.763, interval_steps_per_second: 19.704, epoch: 40.0[0m
[32m[2022-08-26 17:13:07,125] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:13:07,125] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 17:13:07,125] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:13:07,125] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:13:07,126] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 17:13:07,430] [    INFO][0m - eval_loss: 4.240971565246582, eval_accuracy: 0.5125, eval_runtime: 0.3039, eval_samples_per_second: 526.411, eval_steps_per_second: 16.45, epoch: 40.0[0m
[32m[2022-08-26 17:13:07,430] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-800[0m
[32m[2022-08-26 17:13:07,430] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:13:09,754] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-800/tokenizer_config.json[0m
[32m[2022-08-26 17:13:09,754] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-800/special_tokens_map.json[0m
[32m[2022-08-26 17:13:15,278] [    INFO][0m - loss: 8.95e-06, learning_rate: 1.785e-05, global_step: 810, interval_runtime: 8.1535, interval_samples_per_second: 0.981, interval_steps_per_second: 1.226, epoch: 40.5[0m
[32m[2022-08-26 17:13:15,780] [    INFO][0m - loss: 7.76e-06, learning_rate: 1.77e-05, global_step: 820, interval_runtime: 0.5012, interval_samples_per_second: 15.961, interval_steps_per_second: 19.951, epoch: 41.0[0m
[32m[2022-08-26 17:13:16,350] [    INFO][0m - loss: 3.171e-05, learning_rate: 1.755e-05, global_step: 830, interval_runtime: 0.5704, interval_samples_per_second: 14.026, interval_steps_per_second: 17.532, epoch: 41.5[0m
[32m[2022-08-26 17:13:16,854] [    INFO][0m - loss: 2.062e-05, learning_rate: 1.74e-05, global_step: 840, interval_runtime: 0.504, interval_samples_per_second: 15.872, interval_steps_per_second: 19.839, epoch: 42.0[0m
[32m[2022-08-26 17:13:17,419] [    INFO][0m - loss: 9.5e-06, learning_rate: 1.725e-05, global_step: 850, interval_runtime: 0.5653, interval_samples_per_second: 14.151, interval_steps_per_second: 17.689, epoch: 42.5[0m
[32m[2022-08-26 17:13:17,920] [    INFO][0m - loss: 4.76e-06, learning_rate: 1.71e-05, global_step: 860, interval_runtime: 0.5008, interval_samples_per_second: 15.973, interval_steps_per_second: 19.967, epoch: 43.0[0m
[32m[2022-08-26 17:13:18,488] [    INFO][0m - loss: 6.42e-06, learning_rate: 1.695e-05, global_step: 870, interval_runtime: 0.5676, interval_samples_per_second: 14.095, interval_steps_per_second: 17.618, epoch: 43.5[0m
[32m[2022-08-26 17:13:18,994] [    INFO][0m - loss: 3.24e-06, learning_rate: 1.6800000000000002e-05, global_step: 880, interval_runtime: 0.5063, interval_samples_per_second: 15.801, interval_steps_per_second: 19.751, epoch: 44.0[0m
[32m[2022-08-26 17:13:19,559] [    INFO][0m - loss: 6.03e-06, learning_rate: 1.665e-05, global_step: 890, interval_runtime: 0.5647, interval_samples_per_second: 14.168, interval_steps_per_second: 17.71, epoch: 44.5[0m
[32m[2022-08-26 17:13:20,061] [    INFO][0m - loss: 1.002e-05, learning_rate: 1.65e-05, global_step: 900, interval_runtime: 0.5025, interval_samples_per_second: 15.921, interval_steps_per_second: 19.901, epoch: 45.0[0m
[32m[2022-08-26 17:13:20,062] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:13:20,062] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 17:13:20,062] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:13:20,062] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:13:20,062] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 17:13:20,364] [    INFO][0m - eval_loss: 4.224732875823975, eval_accuracy: 0.50625, eval_runtime: 0.3019, eval_samples_per_second: 530.007, eval_steps_per_second: 16.563, epoch: 45.0[0m
[32m[2022-08-26 17:13:20,364] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-900[0m
[32m[2022-08-26 17:13:20,364] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:13:22,854] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-900/tokenizer_config.json[0m
[32m[2022-08-26 17:13:22,854] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-900/special_tokens_map.json[0m
[32m[2022-08-26 17:13:28,625] [    INFO][0m - loss: 8.72e-06, learning_rate: 1.635e-05, global_step: 910, interval_runtime: 8.5637, interval_samples_per_second: 0.934, interval_steps_per_second: 1.168, epoch: 45.5[0m
[32m[2022-08-26 17:13:29,137] [    INFO][0m - loss: 1.038e-05, learning_rate: 1.62e-05, global_step: 920, interval_runtime: 0.5117, interval_samples_per_second: 15.633, interval_steps_per_second: 19.542, epoch: 46.0[0m
[32m[2022-08-26 17:13:29,736] [    INFO][0m - loss: 0.00059662, learning_rate: 1.605e-05, global_step: 930, interval_runtime: 0.5996, interval_samples_per_second: 13.341, interval_steps_per_second: 16.677, epoch: 46.5[0m
[32m[2022-08-26 17:13:30,256] [    INFO][0m - loss: 1.148e-05, learning_rate: 1.59e-05, global_step: 940, interval_runtime: 0.5201, interval_samples_per_second: 15.381, interval_steps_per_second: 19.226, epoch: 47.0[0m
[32m[2022-08-26 17:13:30,824] [    INFO][0m - loss: 5.69e-05, learning_rate: 1.575e-05, global_step: 950, interval_runtime: 0.5675, interval_samples_per_second: 14.098, interval_steps_per_second: 17.623, epoch: 47.5[0m
[32m[2022-08-26 17:13:31,326] [    INFO][0m - loss: 3.343e-05, learning_rate: 1.56e-05, global_step: 960, interval_runtime: 0.5025, interval_samples_per_second: 15.92, interval_steps_per_second: 19.9, epoch: 48.0[0m
[32m[2022-08-26 17:13:31,891] [    INFO][0m - loss: 7.62e-06, learning_rate: 1.545e-05, global_step: 970, interval_runtime: 0.5648, interval_samples_per_second: 14.163, interval_steps_per_second: 17.704, epoch: 48.5[0m
[32m[2022-08-26 17:13:32,391] [    INFO][0m - loss: 2.362e-05, learning_rate: 1.53e-05, global_step: 980, interval_runtime: 0.4993, interval_samples_per_second: 16.024, interval_steps_per_second: 20.029, epoch: 49.0[0m
[32m[2022-08-26 17:13:32,957] [    INFO][0m - loss: 1.241e-05, learning_rate: 1.515e-05, global_step: 990, interval_runtime: 0.5662, interval_samples_per_second: 14.129, interval_steps_per_second: 17.661, epoch: 49.5[0m
[32m[2022-08-26 17:13:33,455] [    INFO][0m - loss: 5.34e-06, learning_rate: 1.5e-05, global_step: 1000, interval_runtime: 0.4978, interval_samples_per_second: 16.071, interval_steps_per_second: 20.089, epoch: 50.0[0m
[32m[2022-08-26 17:13:33,455] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:13:33,455] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 17:13:33,455] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:13:33,455] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:13:33,455] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 17:13:33,757] [    INFO][0m - eval_loss: 4.407151222229004, eval_accuracy: 0.48125, eval_runtime: 0.3018, eval_samples_per_second: 530.146, eval_steps_per_second: 16.567, epoch: 50.0[0m
[32m[2022-08-26 17:13:33,757] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1000[0m
[32m[2022-08-26 17:13:33,758] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:13:36,383] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1000/tokenizer_config.json[0m
[32m[2022-08-26 17:13:36,383] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1000/special_tokens_map.json[0m
[32m[2022-08-26 17:13:41,922] [    INFO][0m - loss: 1.111e-05, learning_rate: 1.485e-05, global_step: 1010, interval_runtime: 8.4676, interval_samples_per_second: 0.945, interval_steps_per_second: 1.181, epoch: 50.5[0m
[32m[2022-08-26 17:13:42,423] [    INFO][0m - loss: 1.489e-05, learning_rate: 1.47e-05, global_step: 1020, interval_runtime: 0.5008, interval_samples_per_second: 15.975, interval_steps_per_second: 19.968, epoch: 51.0[0m
[32m[2022-08-26 17:13:42,986] [    INFO][0m - loss: 1.482e-05, learning_rate: 1.455e-05, global_step: 1030, interval_runtime: 0.5631, interval_samples_per_second: 14.206, interval_steps_per_second: 17.758, epoch: 51.5[0m
[32m[2022-08-26 17:13:43,488] [    INFO][0m - loss: 3.21e-06, learning_rate: 1.44e-05, global_step: 1040, interval_runtime: 0.5019, interval_samples_per_second: 15.94, interval_steps_per_second: 19.924, epoch: 52.0[0m
[32m[2022-08-26 17:13:44,050] [    INFO][0m - loss: 4.76e-06, learning_rate: 1.4249999999999999e-05, global_step: 1050, interval_runtime: 0.5618, interval_samples_per_second: 14.24, interval_steps_per_second: 17.8, epoch: 52.5[0m
[32m[2022-08-26 17:13:44,548] [    INFO][0m - loss: 5.68e-06, learning_rate: 1.4099999999999999e-05, global_step: 1060, interval_runtime: 0.498, interval_samples_per_second: 16.063, interval_steps_per_second: 20.079, epoch: 53.0[0m
[32m[2022-08-26 17:13:45,108] [    INFO][0m - loss: 0.00097184, learning_rate: 1.395e-05, global_step: 1070, interval_runtime: 0.56, interval_samples_per_second: 14.285, interval_steps_per_second: 17.856, epoch: 53.5[0m
[32m[2022-08-26 17:13:45,611] [    INFO][0m - loss: 5.84e-05, learning_rate: 1.3800000000000002e-05, global_step: 1080, interval_runtime: 0.5027, interval_samples_per_second: 15.913, interval_steps_per_second: 19.892, epoch: 54.0[0m
[32m[2022-08-26 17:13:46,179] [    INFO][0m - loss: 6e-06, learning_rate: 1.3650000000000001e-05, global_step: 1090, interval_runtime: 0.568, interval_samples_per_second: 14.084, interval_steps_per_second: 17.605, epoch: 54.5[0m
[32m[2022-08-26 17:13:46,678] [    INFO][0m - loss: 9.72e-06, learning_rate: 1.3500000000000001e-05, global_step: 1100, interval_runtime: 0.4997, interval_samples_per_second: 16.01, interval_steps_per_second: 20.013, epoch: 55.0[0m
[32m[2022-08-26 17:13:46,679] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:13:46,679] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 17:13:46,679] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:13:46,679] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:13:46,679] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 17:13:46,983] [    INFO][0m - eval_loss: 4.438902854919434, eval_accuracy: 0.48125, eval_runtime: 0.304, eval_samples_per_second: 526.363, eval_steps_per_second: 16.449, epoch: 55.0[0m
[32m[2022-08-26 17:13:46,984] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1100[0m
[32m[2022-08-26 17:13:46,984] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:13:49,616] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1100/tokenizer_config.json[0m
[32m[2022-08-26 17:13:49,616] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1100/special_tokens_map.json[0m
[32m[2022-08-26 17:13:55,386] [    INFO][0m - loss: 5.04e-06, learning_rate: 1.3350000000000001e-05, global_step: 1110, interval_runtime: 8.7079, interval_samples_per_second: 0.919, interval_steps_per_second: 1.148, epoch: 55.5[0m
[32m[2022-08-26 17:13:55,889] [    INFO][0m - loss: 2.737e-05, learning_rate: 1.32e-05, global_step: 1120, interval_runtime: 0.5028, interval_samples_per_second: 15.91, interval_steps_per_second: 19.888, epoch: 56.0[0m
[32m[2022-08-26 17:13:56,452] [    INFO][0m - loss: 2.85e-06, learning_rate: 1.305e-05, global_step: 1130, interval_runtime: 0.5628, interval_samples_per_second: 14.215, interval_steps_per_second: 17.769, epoch: 56.5[0m
[32m[2022-08-26 17:13:56,955] [    INFO][0m - loss: 1.471e-05, learning_rate: 1.29e-05, global_step: 1140, interval_runtime: 0.5026, interval_samples_per_second: 15.916, interval_steps_per_second: 19.895, epoch: 57.0[0m
[32m[2022-08-26 17:13:57,540] [    INFO][0m - loss: 4.54e-06, learning_rate: 1.275e-05, global_step: 1150, interval_runtime: 0.5849, interval_samples_per_second: 13.677, interval_steps_per_second: 17.096, epoch: 57.5[0m
[32m[2022-08-26 17:13:58,043] [    INFO][0m - loss: 0.00010718, learning_rate: 1.26e-05, global_step: 1160, interval_runtime: 0.5036, interval_samples_per_second: 15.887, interval_steps_per_second: 19.859, epoch: 58.0[0m
[32m[2022-08-26 17:13:58,610] [    INFO][0m - loss: 5.55e-06, learning_rate: 1.245e-05, global_step: 1170, interval_runtime: 0.5668, interval_samples_per_second: 14.115, interval_steps_per_second: 17.644, epoch: 58.5[0m
[32m[2022-08-26 17:13:59,109] [    INFO][0m - loss: 2.35e-06, learning_rate: 1.2299999999999999e-05, global_step: 1180, interval_runtime: 0.4986, interval_samples_per_second: 16.046, interval_steps_per_second: 20.057, epoch: 59.0[0m
[32m[2022-08-26 17:13:59,676] [    INFO][0m - loss: 1.204e-05, learning_rate: 1.215e-05, global_step: 1190, interval_runtime: 0.5671, interval_samples_per_second: 14.106, interval_steps_per_second: 17.632, epoch: 59.5[0m
[32m[2022-08-26 17:14:00,177] [    INFO][0m - loss: 0.0001104, learning_rate: 1.2e-05, global_step: 1200, interval_runtime: 0.5012, interval_samples_per_second: 15.962, interval_steps_per_second: 19.953, epoch: 60.0[0m
[32m[2022-08-26 17:14:00,177] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:14:00,177] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 17:14:00,177] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:14:00,177] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:14:00,178] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 17:14:00,480] [    INFO][0m - eval_loss: 4.592247009277344, eval_accuracy: 0.48125, eval_runtime: 0.3024, eval_samples_per_second: 529.03, eval_steps_per_second: 16.532, epoch: 60.0[0m
[32m[2022-08-26 17:14:00,480] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1200[0m
[32m[2022-08-26 17:14:00,480] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:14:02,752] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1200/tokenizer_config.json[0m
[32m[2022-08-26 17:14:02,753] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1200/special_tokens_map.json[0m
[32m[2022-08-26 17:14:07,636] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-26 17:14:07,637] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-800 (score: 0.5125).[0m
[32m[2022-08-26 17:14:08,346] [    INFO][0m - train_runtime: 163.572, train_samples_per_second: 97.816, train_steps_per_second: 12.227, train_loss: 0.0873601914002514, epoch: 60.0[0m
[32m[2022-08-26 17:14:08,388] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-26 17:14:08,389] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:14:10,719] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-26 17:14:10,719] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-26 17:14:10,721] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-26 17:14:10,721] [    INFO][0m -   epoch                    =       60.0[0m
[32m[2022-08-26 17:14:10,721] [    INFO][0m -   train_loss               =     0.0874[0m
[32m[2022-08-26 17:14:10,721] [    INFO][0m -   train_runtime            = 0:02:43.57[0m
[32m[2022-08-26 17:14:10,721] [    INFO][0m -   train_samples_per_second =     97.816[0m
[32m[2022-08-26 17:14:10,721] [    INFO][0m -   train_steps_per_second   =     12.227[0m
[32m[2022-08-26 17:14:10,726] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-26 17:14:10,727] [    INFO][0m -   Num examples = 2520[0m
[32m[2022-08-26 17:14:10,727] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:14:10,727] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:14:10,727] [    INFO][0m -   Total prediction steps = 79[0m
[32m[2022-08-26 17:14:15,442] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-26 17:14:15,443] [    INFO][0m -   test_accuracy           =     0.4917[0m
[32m[2022-08-26 17:14:15,443] [    INFO][0m -   test_loss               =     4.1932[0m
[32m[2022-08-26 17:14:15,443] [    INFO][0m -   test_runtime            = 0:00:04.71[0m
[32m[2022-08-26 17:14:15,443] [    INFO][0m -   test_samples_per_second =     534.36[0m
[32m[2022-08-26 17:14:15,443] [    INFO][0m -   test_steps_per_second   =     16.752[0m
[32m[2022-08-26 17:14:15,444] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-26 17:14:15,444] [    INFO][0m -   Num examples = 3000[0m
[32m[2022-08-26 17:14:15,444] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:14:15,444] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:14:15,444] [    INFO][0m -   Total prediction steps = 94[0m
[32m[2022-08-26 17:14:23,164] [    INFO][0m - Predictions for ocnlif saved to ./fewclue_submit_examples.[0m
 
==========
bustm
==========
 
[33m[2022-08-26 17:14:27,254] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-26 17:14:27,254] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-26 17:14:27,255] [    INFO][0m - ============================================================[0m
[32m[2022-08-26 17:14:27,255] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-26 17:14:27,255] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-26 17:14:27,255] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-26 17:14:27,255] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-26 17:14:27,255] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-08-26 17:14:27,255] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-26 17:14:27,255] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-08-26 17:14:27,255] [    INFO][0m - [0m
[32m[2022-08-26 17:14:27,255] [    INFO][0m - ============================================================[0m
[32m[2022-08-26 17:14:27,255] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-26 17:14:27,255] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-26 17:14:27,256] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-26 17:14:27,256] [    INFO][0m - prompt                        :{'text':'text_a'}{'sep'}{'text':'text_b'}{'sep'}{'hard':'Ââç‰∏§Âè•ËØù'}{'mask'}{'hard':'ÂÉè'}[0m
[32m[2022-08-26 17:14:27,256] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-26 17:14:27,256] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-26 17:14:27,256] [    INFO][0m - task_name                     :bustm[0m
[32m[2022-08-26 17:14:27,256] [    INFO][0m - [0m
[32m[2022-08-26 17:14:27,256] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0826 17:14:27.257654 61651 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0826 17:14:27.261618 61651 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-26 17:14:29,870] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-08-26 17:14:29,893] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-08-26 17:14:29,894] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[32m[2022-08-26 17:14:29,895] [    INFO][0m - Using template: [{'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'sep': None}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'sep': None}, {'add_prefix_space': '', 'hard': 'Ââç‰∏§Âè•ËØù'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'ÂÉè'}][0m
2022-08-26 17:14:29,901 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-26 17:14:30,003] [    INFO][0m - ============================================================[0m
[32m[2022-08-26 17:14:30,003] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-26 17:14:30,004] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-26 17:14:30,004] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-26 17:14:30,004] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-26 17:14:30,004] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-26 17:14:30,004] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-26 17:14:30,004] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-26 17:14:30,004] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-26 17:14:30,004] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-26 17:14:30,004] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-26 17:14:30,004] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-26 17:14:30,004] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-26 17:14:30,005] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-26 17:14:30,005] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-26 17:14:30,005] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-26 17:14:30,005] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-26 17:14:30,005] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-26 17:14:30,005] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-26 17:14:30,005] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-26 17:14:30,005] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-26 17:14:30,005] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-26 17:14:30,005] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-26 17:14:30,005] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-26 17:14:30,005] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-26 17:14:30,005] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-26 17:14:30,006] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-26 17:14:30,006] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-26 17:14:30,006] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-26 17:14:30,006] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-26 17:14:30,006] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-26 17:14:30,006] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-26 17:14:30,006] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-26 17:14:30,006] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-26 17:14:30,006] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-26 17:14:30,006] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-26 17:14:30,006] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-26 17:14:30,006] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug26_17-14-27_instance-3bwob41y-01[0m
[32m[2022-08-26 17:14:30,006] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-26 17:14:30,007] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-26 17:14:30,007] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-26 17:14:30,007] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-26 17:14:30,007] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-26 17:14:30,007] [    INFO][0m - max_seq_length                :40[0m
[32m[2022-08-26 17:14:30,007] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-26 17:14:30,007] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-26 17:14:30,007] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-26 17:14:30,007] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-26 17:14:30,007] [    INFO][0m - num_train_epochs              :100.0[0m
[32m[2022-08-26 17:14:30,007] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-26 17:14:30,007] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-26 17:14:30,007] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-26 17:14:30,008] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-26 17:14:30,008] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-26 17:14:30,008] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-26 17:14:30,008] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-26 17:14:30,008] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-26 17:14:30,008] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-26 17:14:30,008] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-26 17:14:30,008] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-26 17:14:30,008] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-26 17:14:30,008] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-26 17:14:30,008] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-26 17:14:30,008] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-26 17:14:30,008] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-26 17:14:30,009] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-26 17:14:30,009] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-26 17:14:30,009] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-26 17:14:30,009] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-26 17:14:30,009] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-26 17:14:30,009] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-26 17:14:30,009] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-26 17:14:30,009] [    INFO][0m - seed                          :42[0m
[32m[2022-08-26 17:14:30,009] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-26 17:14:30,009] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-26 17:14:30,009] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-26 17:14:30,009] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-26 17:14:30,009] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-26 17:14:30,010] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-26 17:14:30,010] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-26 17:14:30,010] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-26 17:14:30,010] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-26 17:14:30,010] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-26 17:14:30,010] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-26 17:14:30,010] [    INFO][0m - [0m
[32m[2022-08-26 17:14:30,012] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-26 17:14:30,012] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 17:14:30,012] [    INFO][0m -   Num Epochs = 100[0m
[32m[2022-08-26 17:14:30,012] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-26 17:14:30,012] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-26 17:14:30,012] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-26 17:14:30,012] [    INFO][0m -   Total optimization steps = 2000.0[0m
[32m[2022-08-26 17:14:30,012] [    INFO][0m -   Total num train samples = 16000[0m
[32m[2022-08-26 17:14:31,534] [    INFO][0m - loss: 0.7944953, learning_rate: 2.985e-05, global_step: 10, interval_runtime: 1.521, interval_samples_per_second: 5.26, interval_steps_per_second: 6.575, epoch: 0.5[0m
[32m[2022-08-26 17:14:32,079] [    INFO][0m - loss: 0.99964256, learning_rate: 2.97e-05, global_step: 20, interval_runtime: 0.5448, interval_samples_per_second: 14.684, interval_steps_per_second: 18.355, epoch: 1.0[0m
[32m[2022-08-26 17:14:32,684] [    INFO][0m - loss: 0.93663616, learning_rate: 2.955e-05, global_step: 30, interval_runtime: 0.6049, interval_samples_per_second: 13.226, interval_steps_per_second: 16.533, epoch: 1.5[0m
[32m[2022-08-26 17:14:33,216] [    INFO][0m - loss: 0.74423971, learning_rate: 2.94e-05, global_step: 40, interval_runtime: 0.5324, interval_samples_per_second: 15.026, interval_steps_per_second: 18.783, epoch: 2.0[0m
[32m[2022-08-26 17:14:33,783] [    INFO][0m - loss: 0.65277495, learning_rate: 2.925e-05, global_step: 50, interval_runtime: 0.5671, interval_samples_per_second: 14.107, interval_steps_per_second: 17.634, epoch: 2.5[0m
[32m[2022-08-26 17:14:34,280] [    INFO][0m - loss: 0.67155151, learning_rate: 2.91e-05, global_step: 60, interval_runtime: 0.4968, interval_samples_per_second: 16.102, interval_steps_per_second: 20.128, epoch: 3.0[0m
[32m[2022-08-26 17:14:34,836] [    INFO][0m - loss: 0.53454504, learning_rate: 2.895e-05, global_step: 70, interval_runtime: 0.5562, interval_samples_per_second: 14.384, interval_steps_per_second: 17.98, epoch: 3.5[0m
[32m[2022-08-26 17:14:35,340] [    INFO][0m - loss: 0.51156182, learning_rate: 2.88e-05, global_step: 80, interval_runtime: 0.5036, interval_samples_per_second: 15.885, interval_steps_per_second: 19.857, epoch: 4.0[0m
[32m[2022-08-26 17:14:35,902] [    INFO][0m - loss: 0.39455745, learning_rate: 2.865e-05, global_step: 90, interval_runtime: 0.5624, interval_samples_per_second: 14.225, interval_steps_per_second: 17.781, epoch: 4.5[0m
[32m[2022-08-26 17:14:36,399] [    INFO][0m - loss: 0.23532708, learning_rate: 2.8499999999999998e-05, global_step: 100, interval_runtime: 0.4972, interval_samples_per_second: 16.089, interval_steps_per_second: 20.111, epoch: 5.0[0m
[32m[2022-08-26 17:14:36,400] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:14:36,400] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 17:14:36,400] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:14:36,400] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:14:36,400] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 17:14:36,626] [    INFO][0m - eval_loss: 1.1775914430618286, eval_accuracy: 0.64375, eval_runtime: 0.2257, eval_samples_per_second: 708.958, eval_steps_per_second: 22.155, epoch: 5.0[0m
[32m[2022-08-26 17:14:36,627] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-26 17:14:36,627] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:14:38,914] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-26 17:14:38,914] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-26 17:14:44,915] [    INFO][0m - loss: 0.17435867, learning_rate: 2.8349999999999998e-05, global_step: 110, interval_runtime: 8.5146, interval_samples_per_second: 0.94, interval_steps_per_second: 1.174, epoch: 5.5[0m
[32m[2022-08-26 17:14:45,419] [    INFO][0m - loss: 0.13649923, learning_rate: 2.8199999999999998e-05, global_step: 120, interval_runtime: 0.5046, interval_samples_per_second: 15.854, interval_steps_per_second: 19.818, epoch: 6.0[0m
[32m[2022-08-26 17:14:45,974] [    INFO][0m - loss: 0.0198679, learning_rate: 2.805e-05, global_step: 130, interval_runtime: 0.5553, interval_samples_per_second: 14.407, interval_steps_per_second: 18.009, epoch: 6.5[0m
[32m[2022-08-26 17:14:46,477] [    INFO][0m - loss: 0.08060642, learning_rate: 2.79e-05, global_step: 140, interval_runtime: 0.5035, interval_samples_per_second: 15.889, interval_steps_per_second: 19.861, epoch: 7.0[0m
[32m[2022-08-26 17:14:47,049] [    INFO][0m - loss: 0.00224891, learning_rate: 2.7750000000000004e-05, global_step: 150, interval_runtime: 0.571, interval_samples_per_second: 14.011, interval_steps_per_second: 17.514, epoch: 7.5[0m
[32m[2022-08-26 17:14:47,560] [    INFO][0m - loss: 0.23316803, learning_rate: 2.7600000000000003e-05, global_step: 160, interval_runtime: 0.5117, interval_samples_per_second: 15.635, interval_steps_per_second: 19.544, epoch: 8.0[0m
[32m[2022-08-26 17:14:48,125] [    INFO][0m - loss: 0.00025707, learning_rate: 2.7450000000000003e-05, global_step: 170, interval_runtime: 0.565, interval_samples_per_second: 14.16, interval_steps_per_second: 17.701, epoch: 8.5[0m
[32m[2022-08-26 17:14:48,636] [    INFO][0m - loss: 0.00052188, learning_rate: 2.7300000000000003e-05, global_step: 180, interval_runtime: 0.5103, interval_samples_per_second: 15.677, interval_steps_per_second: 19.596, epoch: 9.0[0m
[32m[2022-08-26 17:14:49,199] [    INFO][0m - loss: 0.00016532, learning_rate: 2.7150000000000003e-05, global_step: 190, interval_runtime: 0.564, interval_samples_per_second: 14.185, interval_steps_per_second: 17.731, epoch: 9.5[0m
[32m[2022-08-26 17:14:49,698] [    INFO][0m - loss: 0.09869183, learning_rate: 2.7000000000000002e-05, global_step: 200, interval_runtime: 0.4982, interval_samples_per_second: 16.057, interval_steps_per_second: 20.071, epoch: 10.0[0m
[32m[2022-08-26 17:14:49,698] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:14:49,698] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 17:14:49,698] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:14:49,698] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:14:49,698] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 17:14:49,926] [    INFO][0m - eval_loss: 3.413071870803833, eval_accuracy: 0.64375, eval_runtime: 0.2274, eval_samples_per_second: 703.663, eval_steps_per_second: 21.989, epoch: 10.0[0m
[32m[2022-08-26 17:14:49,926] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-26 17:14:49,926] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:14:52,248] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-26 17:14:52,248] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-26 17:14:58,179] [    INFO][0m - loss: 0.00117165, learning_rate: 2.6850000000000002e-05, global_step: 210, interval_runtime: 8.4811, interval_samples_per_second: 0.943, interval_steps_per_second: 1.179, epoch: 10.5[0m
[32m[2022-08-26 17:14:58,674] [    INFO][0m - loss: 0.00022975, learning_rate: 2.6700000000000002e-05, global_step: 220, interval_runtime: 0.4953, interval_samples_per_second: 16.152, interval_steps_per_second: 20.19, epoch: 11.0[0m
[32m[2022-08-26 17:14:59,232] [    INFO][0m - loss: 0.00034308, learning_rate: 2.655e-05, global_step: 230, interval_runtime: 0.5582, interval_samples_per_second: 14.331, interval_steps_per_second: 17.914, epoch: 11.5[0m
[32m[2022-08-26 17:14:59,730] [    INFO][0m - loss: 2.751e-05, learning_rate: 2.64e-05, global_step: 240, interval_runtime: 0.4976, interval_samples_per_second: 16.076, interval_steps_per_second: 20.094, epoch: 12.0[0m
[32m[2022-08-26 17:15:00,299] [    INFO][0m - loss: 2.055e-05, learning_rate: 2.625e-05, global_step: 250, interval_runtime: 0.5683, interval_samples_per_second: 14.078, interval_steps_per_second: 17.597, epoch: 12.5[0m
[32m[2022-08-26 17:15:00,800] [    INFO][0m - loss: 1.465e-05, learning_rate: 2.61e-05, global_step: 260, interval_runtime: 0.5019, interval_samples_per_second: 15.941, interval_steps_per_second: 19.926, epoch: 13.0[0m
[32m[2022-08-26 17:15:01,358] [    INFO][0m - loss: 0.00061072, learning_rate: 2.595e-05, global_step: 270, interval_runtime: 0.5583, interval_samples_per_second: 14.329, interval_steps_per_second: 17.912, epoch: 13.5[0m
[32m[2022-08-26 17:15:01,862] [    INFO][0m - loss: 0.0015026, learning_rate: 2.58e-05, global_step: 280, interval_runtime: 0.5032, interval_samples_per_second: 15.897, interval_steps_per_second: 19.871, epoch: 14.0[0m
[32m[2022-08-26 17:15:02,431] [    INFO][0m - loss: 0.04516121, learning_rate: 2.565e-05, global_step: 290, interval_runtime: 0.5689, interval_samples_per_second: 14.062, interval_steps_per_second: 17.577, epoch: 14.5[0m
[32m[2022-08-26 17:15:02,929] [    INFO][0m - loss: 6.12e-06, learning_rate: 2.55e-05, global_step: 300, interval_runtime: 0.4984, interval_samples_per_second: 16.053, interval_steps_per_second: 20.066, epoch: 15.0[0m
[32m[2022-08-26 17:15:02,929] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:15:02,929] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 17:15:02,929] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:15:02,929] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:15:02,930] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 17:15:03,153] [    INFO][0m - eval_loss: 4.100323677062988, eval_accuracy: 0.6125, eval_runtime: 0.2237, eval_samples_per_second: 715.28, eval_steps_per_second: 22.353, epoch: 15.0[0m
[32m[2022-08-26 17:15:03,154] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-26 17:15:03,154] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:15:05,488] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-26 17:15:05,488] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-26 17:15:11,651] [    INFO][0m - loss: 0.00056154, learning_rate: 2.535e-05, global_step: 310, interval_runtime: 8.7218, interval_samples_per_second: 0.917, interval_steps_per_second: 1.147, epoch: 15.5[0m
[32m[2022-08-26 17:15:12,167] [    INFO][0m - loss: 0.00010726, learning_rate: 2.52e-05, global_step: 320, interval_runtime: 0.5159, interval_samples_per_second: 15.508, interval_steps_per_second: 19.385, epoch: 16.0[0m
[32m[2022-08-26 17:15:12,733] [    INFO][0m - loss: 1.539e-05, learning_rate: 2.505e-05, global_step: 330, interval_runtime: 0.5668, interval_samples_per_second: 14.114, interval_steps_per_second: 17.642, epoch: 16.5[0m
[32m[2022-08-26 17:15:13,240] [    INFO][0m - loss: 2.134e-05, learning_rate: 2.49e-05, global_step: 340, interval_runtime: 0.5071, interval_samples_per_second: 15.776, interval_steps_per_second: 19.72, epoch: 17.0[0m
[32m[2022-08-26 17:15:13,791] [    INFO][0m - loss: 7.1e-07, learning_rate: 2.475e-05, global_step: 350, interval_runtime: 0.5503, interval_samples_per_second: 14.538, interval_steps_per_second: 18.173, epoch: 17.5[0m
[32m[2022-08-26 17:15:14,288] [    INFO][0m - loss: 5.87e-06, learning_rate: 2.4599999999999998e-05, global_step: 360, interval_runtime: 0.4973, interval_samples_per_second: 16.088, interval_steps_per_second: 20.11, epoch: 18.0[0m
[32m[2022-08-26 17:15:14,848] [    INFO][0m - loss: 1.068e-05, learning_rate: 2.4449999999999998e-05, global_step: 370, interval_runtime: 0.56, interval_samples_per_second: 14.286, interval_steps_per_second: 17.858, epoch: 18.5[0m
[32m[2022-08-26 17:15:15,346] [    INFO][0m - loss: 2.648e-05, learning_rate: 2.43e-05, global_step: 380, interval_runtime: 0.4984, interval_samples_per_second: 16.053, interval_steps_per_second: 20.066, epoch: 19.0[0m
[32m[2022-08-26 17:15:15,906] [    INFO][0m - loss: 1.062e-05, learning_rate: 2.415e-05, global_step: 390, interval_runtime: 0.5596, interval_samples_per_second: 14.297, interval_steps_per_second: 17.871, epoch: 19.5[0m
[32m[2022-08-26 17:15:16,409] [    INFO][0m - loss: 4.24e-06, learning_rate: 2.4e-05, global_step: 400, interval_runtime: 0.5033, interval_samples_per_second: 15.896, interval_steps_per_second: 19.87, epoch: 20.0[0m
[32m[2022-08-26 17:15:16,410] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:15:16,410] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 17:15:16,410] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:15:16,410] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:15:16,410] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 17:15:16,635] [    INFO][0m - eval_loss: 4.118353843688965, eval_accuracy: 0.65, eval_runtime: 0.225, eval_samples_per_second: 711.264, eval_steps_per_second: 22.227, epoch: 20.0[0m
[32m[2022-08-26 17:15:16,635] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-26 17:15:16,635] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:15:19,308] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-26 17:15:19,309] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-26 17:15:25,326] [    INFO][0m - loss: 1.542e-05, learning_rate: 2.385e-05, global_step: 410, interval_runtime: 8.9173, interval_samples_per_second: 0.897, interval_steps_per_second: 1.121, epoch: 20.5[0m
[32m[2022-08-26 17:15:25,822] [    INFO][0m - loss: 1.13e-06, learning_rate: 2.37e-05, global_step: 420, interval_runtime: 0.4955, interval_samples_per_second: 16.145, interval_steps_per_second: 20.181, epoch: 21.0[0m
[32m[2022-08-26 17:15:26,380] [    INFO][0m - loss: 0.10482204, learning_rate: 2.3550000000000003e-05, global_step: 430, interval_runtime: 0.5581, interval_samples_per_second: 14.336, interval_steps_per_second: 17.919, epoch: 21.5[0m
[32m[2022-08-26 17:15:26,877] [    INFO][0m - loss: 2.16e-06, learning_rate: 2.3400000000000003e-05, global_step: 440, interval_runtime: 0.4965, interval_samples_per_second: 16.114, interval_steps_per_second: 20.142, epoch: 22.0[0m
[32m[2022-08-26 17:15:27,431] [    INFO][0m - loss: 0.09133437, learning_rate: 2.3250000000000003e-05, global_step: 450, interval_runtime: 0.5546, interval_samples_per_second: 14.426, interval_steps_per_second: 18.033, epoch: 22.5[0m
[32m[2022-08-26 17:15:27,930] [    INFO][0m - loss: 5.95e-06, learning_rate: 2.3100000000000002e-05, global_step: 460, interval_runtime: 0.4993, interval_samples_per_second: 16.023, interval_steps_per_second: 20.029, epoch: 23.0[0m
[32m[2022-08-26 17:15:28,484] [    INFO][0m - loss: 2.88e-06, learning_rate: 2.2950000000000002e-05, global_step: 470, interval_runtime: 0.5536, interval_samples_per_second: 14.45, interval_steps_per_second: 18.063, epoch: 23.5[0m
[32m[2022-08-26 17:15:28,983] [    INFO][0m - loss: 3.04e-06, learning_rate: 2.2800000000000002e-05, global_step: 480, interval_runtime: 0.4992, interval_samples_per_second: 16.026, interval_steps_per_second: 20.032, epoch: 24.0[0m
[32m[2022-08-26 17:15:29,541] [    INFO][0m - loss: 1.074e-05, learning_rate: 2.265e-05, global_step: 490, interval_runtime: 0.5582, interval_samples_per_second: 14.331, interval_steps_per_second: 17.913, epoch: 24.5[0m
[32m[2022-08-26 17:15:30,039] [    INFO][0m - loss: 2.393e-05, learning_rate: 2.25e-05, global_step: 500, interval_runtime: 0.4979, interval_samples_per_second: 16.067, interval_steps_per_second: 20.084, epoch: 25.0[0m
[32m[2022-08-26 17:15:30,040] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:15:30,040] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 17:15:30,040] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:15:30,040] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:15:30,040] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 17:15:30,264] [    INFO][0m - eval_loss: 3.975978374481201, eval_accuracy: 0.6375, eval_runtime: 0.2233, eval_samples_per_second: 716.398, eval_steps_per_second: 22.387, epoch: 25.0[0m
[32m[2022-08-26 17:15:30,264] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-08-26 17:15:30,264] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:15:32,602] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-08-26 17:15:32,602] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-08-26 17:15:38,232] [    INFO][0m - loss: 2.82e-06, learning_rate: 2.235e-05, global_step: 510, interval_runtime: 8.1924, interval_samples_per_second: 0.977, interval_steps_per_second: 1.221, epoch: 25.5[0m
[32m[2022-08-26 17:15:38,738] [    INFO][0m - loss: 0.00082801, learning_rate: 2.22e-05, global_step: 520, interval_runtime: 0.5058, interval_samples_per_second: 15.815, interval_steps_per_second: 19.769, epoch: 26.0[0m
[32m[2022-08-26 17:15:39,292] [    INFO][0m - loss: 7.57e-06, learning_rate: 2.205e-05, global_step: 530, interval_runtime: 0.5544, interval_samples_per_second: 14.43, interval_steps_per_second: 18.037, epoch: 26.5[0m
[32m[2022-08-26 17:15:39,795] [    INFO][0m - loss: 2.25e-06, learning_rate: 2.19e-05, global_step: 540, interval_runtime: 0.5022, interval_samples_per_second: 15.928, interval_steps_per_second: 19.911, epoch: 27.0[0m
[32m[2022-08-26 17:15:40,362] [    INFO][0m - loss: 4.833e-05, learning_rate: 2.175e-05, global_step: 550, interval_runtime: 0.5673, interval_samples_per_second: 14.101, interval_steps_per_second: 17.626, epoch: 27.5[0m
[32m[2022-08-26 17:15:40,857] [    INFO][0m - loss: 3.444e-05, learning_rate: 2.16e-05, global_step: 560, interval_runtime: 0.495, interval_samples_per_second: 16.161, interval_steps_per_second: 20.202, epoch: 28.0[0m
[32m[2022-08-26 17:15:41,417] [    INFO][0m - loss: 2.53e-06, learning_rate: 2.145e-05, global_step: 570, interval_runtime: 0.5606, interval_samples_per_second: 14.269, interval_steps_per_second: 17.837, epoch: 28.5[0m
[32m[2022-08-26 17:15:41,924] [    INFO][0m - loss: 3.306e-05, learning_rate: 2.13e-05, global_step: 580, interval_runtime: 0.5071, interval_samples_per_second: 15.777, interval_steps_per_second: 19.721, epoch: 29.0[0m
[32m[2022-08-26 17:15:42,483] [    INFO][0m - loss: 8.8e-07, learning_rate: 2.115e-05, global_step: 590, interval_runtime: 0.5582, interval_samples_per_second: 14.333, interval_steps_per_second: 17.916, epoch: 29.5[0m
[32m[2022-08-26 17:15:42,979] [    INFO][0m - loss: 9.86e-06, learning_rate: 2.1e-05, global_step: 600, interval_runtime: 0.4969, interval_samples_per_second: 16.1, interval_steps_per_second: 20.125, epoch: 30.0[0m
[32m[2022-08-26 17:15:42,980] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:15:42,980] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 17:15:42,980] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:15:42,980] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:15:42,980] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 17:15:43,206] [    INFO][0m - eval_loss: 4.46428108215332, eval_accuracy: 0.63125, eval_runtime: 0.226, eval_samples_per_second: 707.899, eval_steps_per_second: 22.122, epoch: 30.0[0m
[32m[2022-08-26 17:15:43,207] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-08-26 17:15:43,207] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:15:45,576] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-08-26 17:15:45,576] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-08-26 17:15:51,523] [    INFO][0m - loss: 1.61e-06, learning_rate: 2.085e-05, global_step: 610, interval_runtime: 8.5432, interval_samples_per_second: 0.936, interval_steps_per_second: 1.171, epoch: 30.5[0m
[32m[2022-08-26 17:15:52,016] [    INFO][0m - loss: 3.33e-06, learning_rate: 2.07e-05, global_step: 620, interval_runtime: 0.4936, interval_samples_per_second: 16.207, interval_steps_per_second: 20.259, epoch: 31.0[0m
[32m[2022-08-26 17:15:52,566] [    INFO][0m - loss: 5.13e-06, learning_rate: 2.055e-05, global_step: 630, interval_runtime: 0.55, interval_samples_per_second: 14.545, interval_steps_per_second: 18.181, epoch: 31.5[0m
[32m[2022-08-26 17:15:53,059] [    INFO][0m - loss: 4.111e-05, learning_rate: 2.04e-05, global_step: 640, interval_runtime: 0.4932, interval_samples_per_second: 16.222, interval_steps_per_second: 20.277, epoch: 32.0[0m
[32m[2022-08-26 17:15:53,614] [    INFO][0m - loss: 0.01148641, learning_rate: 2.025e-05, global_step: 650, interval_runtime: 0.5547, interval_samples_per_second: 14.422, interval_steps_per_second: 18.027, epoch: 32.5[0m
[32m[2022-08-26 17:15:54,108] [    INFO][0m - loss: 1.61e-06, learning_rate: 2.01e-05, global_step: 660, interval_runtime: 0.494, interval_samples_per_second: 16.196, interval_steps_per_second: 20.245, epoch: 33.0[0m
[32m[2022-08-26 17:15:54,659] [    INFO][0m - loss: 0.01419441, learning_rate: 1.995e-05, global_step: 670, interval_runtime: 0.5512, interval_samples_per_second: 14.513, interval_steps_per_second: 18.141, epoch: 33.5[0m
[32m[2022-08-26 17:15:55,151] [    INFO][0m - loss: 0.00014621, learning_rate: 1.98e-05, global_step: 680, interval_runtime: 0.4913, interval_samples_per_second: 16.282, interval_steps_per_second: 20.353, epoch: 34.0[0m
[32m[2022-08-26 17:15:55,706] [    INFO][0m - loss: 0.00069052, learning_rate: 1.965e-05, global_step: 690, interval_runtime: 0.555, interval_samples_per_second: 14.415, interval_steps_per_second: 18.019, epoch: 34.5[0m
[32m[2022-08-26 17:15:56,198] [    INFO][0m - loss: 9.302e-05, learning_rate: 1.95e-05, global_step: 700, interval_runtime: 0.4923, interval_samples_per_second: 16.249, interval_steps_per_second: 20.311, epoch: 35.0[0m
[32m[2022-08-26 17:15:56,198] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:15:56,198] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 17:15:56,199] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:15:56,199] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:15:56,199] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 17:15:56,420] [    INFO][0m - eval_loss: 5.3743696212768555, eval_accuracy: 0.61875, eval_runtime: 0.2212, eval_samples_per_second: 723.229, eval_steps_per_second: 22.601, epoch: 35.0[0m
[32m[2022-08-26 17:15:56,420] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-700[0m
[32m[2022-08-26 17:15:56,420] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:15:59,166] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-700/tokenizer_config.json[0m
[32m[2022-08-26 17:15:59,166] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-700/special_tokens_map.json[0m
[32m[2022-08-26 17:16:04,996] [    INFO][0m - loss: 7.58e-06, learning_rate: 1.935e-05, global_step: 710, interval_runtime: 8.7982, interval_samples_per_second: 0.909, interval_steps_per_second: 1.137, epoch: 35.5[0m
[32m[2022-08-26 17:16:05,504] [    INFO][0m - loss: 0.0625691, learning_rate: 1.9200000000000003e-05, global_step: 720, interval_runtime: 0.5077, interval_samples_per_second: 15.759, interval_steps_per_second: 19.698, epoch: 36.0[0m
[32m[2022-08-26 17:16:06,075] [    INFO][0m - loss: 8.68e-06, learning_rate: 1.9050000000000002e-05, global_step: 730, interval_runtime: 0.5712, interval_samples_per_second: 14.005, interval_steps_per_second: 17.506, epoch: 36.5[0m
[32m[2022-08-26 17:16:06,571] [    INFO][0m - loss: 3.65e-06, learning_rate: 1.8900000000000002e-05, global_step: 740, interval_runtime: 0.4963, interval_samples_per_second: 16.119, interval_steps_per_second: 20.149, epoch: 37.0[0m
[32m[2022-08-26 17:16:07,122] [    INFO][0m - loss: 0.02761357, learning_rate: 1.8750000000000002e-05, global_step: 750, interval_runtime: 0.5509, interval_samples_per_second: 14.521, interval_steps_per_second: 18.151, epoch: 37.5[0m
[32m[2022-08-26 17:16:07,630] [    INFO][0m - loss: 1.09e-06, learning_rate: 1.86e-05, global_step: 760, interval_runtime: 0.5072, interval_samples_per_second: 15.772, interval_steps_per_second: 19.715, epoch: 38.0[0m
[32m[2022-08-26 17:16:08,228] [    INFO][0m - loss: 1.3e-07, learning_rate: 1.845e-05, global_step: 770, interval_runtime: 0.5984, interval_samples_per_second: 13.369, interval_steps_per_second: 16.712, epoch: 38.5[0m
[32m[2022-08-26 17:16:08,762] [    INFO][0m - loss: 0.00048326, learning_rate: 1.83e-05, global_step: 780, interval_runtime: 0.5342, interval_samples_per_second: 14.975, interval_steps_per_second: 18.719, epoch: 39.0[0m
[32m[2022-08-26 17:16:09,337] [    INFO][0m - loss: 6.59e-06, learning_rate: 1.815e-05, global_step: 790, interval_runtime: 0.5746, interval_samples_per_second: 13.923, interval_steps_per_second: 17.404, epoch: 39.5[0m
[32m[2022-08-26 17:16:09,843] [    INFO][0m - loss: 1.17e-06, learning_rate: 1.8e-05, global_step: 800, interval_runtime: 0.5057, interval_samples_per_second: 15.82, interval_steps_per_second: 19.775, epoch: 40.0[0m
[32m[2022-08-26 17:16:09,843] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:16:09,843] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 17:16:09,843] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:16:09,843] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:16:09,843] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 17:16:10,070] [    INFO][0m - eval_loss: 5.305644989013672, eval_accuracy: 0.64375, eval_runtime: 0.2262, eval_samples_per_second: 707.216, eval_steps_per_second: 22.101, epoch: 40.0[0m
[32m[2022-08-26 17:16:10,070] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-800[0m
[32m[2022-08-26 17:16:10,070] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:16:12,472] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-800/tokenizer_config.json[0m
[32m[2022-08-26 17:16:12,472] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-800/special_tokens_map.json[0m
[32m[2022-08-26 17:16:17,451] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-26 17:16:17,451] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-400 (score: 0.65).[0m
[32m[2022-08-26 17:16:18,153] [    INFO][0m - train_runtime: 108.1403, train_samples_per_second: 147.956, train_steps_per_second: 18.494, train_loss: 0.09482647960096642, epoch: 40.0[0m
[32m[2022-08-26 17:16:18,191] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-26 17:16:18,192] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:16:20,518] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-26 17:16:20,518] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-26 17:16:20,519] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-26 17:16:20,519] [    INFO][0m -   epoch                    =       40.0[0m
[32m[2022-08-26 17:16:20,519] [    INFO][0m -   train_loss               =     0.0948[0m
[32m[2022-08-26 17:16:20,519] [    INFO][0m -   train_runtime            = 0:01:48.14[0m
[32m[2022-08-26 17:16:20,520] [    INFO][0m -   train_samples_per_second =    147.956[0m
[32m[2022-08-26 17:16:20,520] [    INFO][0m -   train_steps_per_second   =     18.494[0m
[32m[2022-08-26 17:16:20,523] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-26 17:16:20,524] [    INFO][0m -   Num examples = 1772[0m
[32m[2022-08-26 17:16:20,524] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:16:20,524] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:16:20,524] [    INFO][0m -   Total prediction steps = 56[0m
[32m[2022-08-26 17:16:22,980] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-26 17:16:22,980] [    INFO][0m -   test_accuracy           =     0.6574[0m
[32m[2022-08-26 17:16:22,980] [    INFO][0m -   test_loss               =     3.8035[0m
[32m[2022-08-26 17:16:22,980] [    INFO][0m -   test_runtime            = 0:00:02.45[0m
[32m[2022-08-26 17:16:22,980] [    INFO][0m -   test_samples_per_second =    721.442[0m
[32m[2022-08-26 17:16:22,980] [    INFO][0m -   test_steps_per_second   =       22.8[0m
[32m[2022-08-26 17:16:22,981] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-26 17:16:22,981] [    INFO][0m -   Num examples = 2000[0m
[32m[2022-08-26 17:16:22,981] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:16:22,981] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:16:22,981] [    INFO][0m -   Total prediction steps = 63[0m
[32m[2022-08-26 17:16:26,960] [    INFO][0m - Predictions for bustm saved to ./fewclue_submit_examples.[0m
 
==========
csl
==========
 
[33m[2022-08-26 17:16:30,719] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-26 17:16:30,719] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-26 17:16:30,719] [    INFO][0m - ============================================================[0m
[32m[2022-08-26 17:16:30,719] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-26 17:16:30,719] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-26 17:16:30,719] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-26 17:16:30,720] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-26 17:16:30,720] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-08-26 17:16:30,720] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-26 17:16:30,720] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-08-26 17:16:30,720] [    INFO][0m - [0m
[32m[2022-08-26 17:16:30,720] [    INFO][0m - ============================================================[0m
[32m[2022-08-26 17:16:30,720] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-26 17:16:30,720] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-26 17:16:30,720] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-26 17:16:30,720] [    INFO][0m - prompt                        :{'text': 'text_a'}{'hard':'‰∏äÊñá‰∏≠Êâæ'}{'mask'}{'hard': 'Âá∫Ëøô‰∫õÂÖ≥ÈîÆËØçÔºö'}{'text':'text_b'}[0m
[32m[2022-08-26 17:16:30,720] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-26 17:16:30,720] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-26 17:16:30,721] [    INFO][0m - task_name                     :csl[0m
[32m[2022-08-26 17:16:30,721] [    INFO][0m - [0m
[32m[2022-08-26 17:16:30,721] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0826 17:16:30.722285 64311 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0826 17:16:30.726243 64311 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-26 17:16:33,564] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-08-26 17:16:33,588] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-08-26 17:16:33,588] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[32m[2022-08-26 17:16:33,589] [    INFO][0m - Using template: [{'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '‰∏äÊñá‰∏≠Êâæ'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'Âá∫Ëøô‰∫õÂÖ≥ÈîÆËØçÔºö'}, {'add_prefix_space': '', 'text': 'text_b'}][0m
2022-08-26 17:16:33,596 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-26 17:16:33,747] [    INFO][0m - ============================================================[0m
[32m[2022-08-26 17:16:33,748] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-26 17:16:33,748] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-26 17:16:33,748] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-26 17:16:33,748] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-26 17:16:33,748] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-26 17:16:33,748] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-26 17:16:33,748] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-26 17:16:33,748] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-26 17:16:33,748] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-26 17:16:33,748] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-26 17:16:33,749] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-26 17:16:33,749] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-26 17:16:33,749] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-26 17:16:33,749] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-26 17:16:33,749] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-26 17:16:33,749] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-26 17:16:33,749] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-26 17:16:33,749] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-26 17:16:33,749] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-26 17:16:33,749] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-26 17:16:33,749] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-26 17:16:33,749] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-26 17:16:33,749] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-26 17:16:33,750] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-26 17:16:33,750] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-26 17:16:33,750] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-26 17:16:33,750] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-26 17:16:33,750] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-26 17:16:33,750] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-26 17:16:33,750] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-26 17:16:33,750] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-26 17:16:33,750] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-26 17:16:33,750] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-26 17:16:33,750] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-26 17:16:33,750] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-26 17:16:33,750] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-26 17:16:33,750] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug26_17-16-30_instance-3bwob41y-01[0m
[32m[2022-08-26 17:16:33,751] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-26 17:16:33,751] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-26 17:16:33,751] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-26 17:16:33,751] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-26 17:16:33,751] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-26 17:16:33,751] [    INFO][0m - max_seq_length                :320[0m
[32m[2022-08-26 17:16:33,751] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-26 17:16:33,751] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-26 17:16:33,751] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-26 17:16:33,751] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-26 17:16:33,751] [    INFO][0m - num_train_epochs              :100.0[0m
[32m[2022-08-26 17:16:33,751] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-26 17:16:33,751] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-26 17:16:33,752] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-26 17:16:33,752] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-26 17:16:33,752] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-26 17:16:33,752] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-26 17:16:33,752] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-26 17:16:33,752] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-26 17:16:33,752] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-26 17:16:33,752] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-26 17:16:33,752] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-26 17:16:33,752] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-26 17:16:33,752] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-26 17:16:33,752] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-26 17:16:33,752] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-26 17:16:33,753] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-26 17:16:33,753] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-26 17:16:33,753] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-26 17:16:33,753] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-26 17:16:33,753] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-26 17:16:33,753] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-26 17:16:33,753] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-26 17:16:33,753] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-26 17:16:33,753] [    INFO][0m - seed                          :42[0m
[32m[2022-08-26 17:16:33,753] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-26 17:16:33,753] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-26 17:16:33,753] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-26 17:16:33,753] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-26 17:16:33,753] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-26 17:16:33,754] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-26 17:16:33,754] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-26 17:16:33,754] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-26 17:16:33,754] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-26 17:16:33,754] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-26 17:16:33,754] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-26 17:16:33,754] [    INFO][0m - [0m
[32m[2022-08-26 17:16:33,756] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-26 17:16:33,756] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 17:16:33,760] [    INFO][0m -   Num Epochs = 100[0m
[32m[2022-08-26 17:16:33,760] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-26 17:16:33,760] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-26 17:16:33,760] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-26 17:16:33,760] [    INFO][0m -   Total optimization steps = 2000.0[0m
[32m[2022-08-26 17:16:33,761] [    INFO][0m -   Total num train samples = 16000[0m
[32m[2022-08-26 17:16:36,236] [    INFO][0m - loss: 1.18406687, learning_rate: 2.985e-05, global_step: 10, interval_runtime: 2.4743, interval_samples_per_second: 3.233, interval_steps_per_second: 4.042, epoch: 0.5[0m
[32m[2022-08-26 17:16:37,721] [    INFO][0m - loss: 0.73852949, learning_rate: 2.97e-05, global_step: 20, interval_runtime: 1.4853, interval_samples_per_second: 5.386, interval_steps_per_second: 6.733, epoch: 1.0[0m
[32m[2022-08-26 17:16:39,305] [    INFO][0m - loss: 0.66407676, learning_rate: 2.955e-05, global_step: 30, interval_runtime: 1.5837, interval_samples_per_second: 5.051, interval_steps_per_second: 6.314, epoch: 1.5[0m
[32m[2022-08-26 17:16:40,795] [    INFO][0m - loss: 0.81445522, learning_rate: 2.94e-05, global_step: 40, interval_runtime: 1.4896, interval_samples_per_second: 5.371, interval_steps_per_second: 6.713, epoch: 2.0[0m
[32m[2022-08-26 17:16:42,391] [    INFO][0m - loss: 0.81025991, learning_rate: 2.925e-05, global_step: 50, interval_runtime: 1.5964, interval_samples_per_second: 5.011, interval_steps_per_second: 6.264, epoch: 2.5[0m
[32m[2022-08-26 17:16:43,882] [    INFO][0m - loss: 0.82656317, learning_rate: 2.91e-05, global_step: 60, interval_runtime: 1.4917, interval_samples_per_second: 5.363, interval_steps_per_second: 6.704, epoch: 3.0[0m
[32m[2022-08-26 17:16:45,487] [    INFO][0m - loss: 0.63890276, learning_rate: 2.895e-05, global_step: 70, interval_runtime: 1.6047, interval_samples_per_second: 4.985, interval_steps_per_second: 6.232, epoch: 3.5[0m
[32m[2022-08-26 17:16:46,978] [    INFO][0m - loss: 0.65908699, learning_rate: 2.88e-05, global_step: 80, interval_runtime: 1.4904, interval_samples_per_second: 5.368, interval_steps_per_second: 6.71, epoch: 4.0[0m
[32m[2022-08-26 17:16:48,576] [    INFO][0m - loss: 0.43814549, learning_rate: 2.865e-05, global_step: 90, interval_runtime: 1.5984, interval_samples_per_second: 5.005, interval_steps_per_second: 6.256, epoch: 4.5[0m
[32m[2022-08-26 17:16:50,072] [    INFO][0m - loss: 0.52165656, learning_rate: 2.8499999999999998e-05, global_step: 100, interval_runtime: 1.4963, interval_samples_per_second: 5.347, interval_steps_per_second: 6.683, epoch: 5.0[0m
[32m[2022-08-26 17:16:50,073] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:16:50,073] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 17:16:50,073] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:16:50,073] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:16:50,073] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 17:16:51,375] [    INFO][0m - eval_loss: 1.977468729019165, eval_accuracy: 0.50625, eval_runtime: 1.302, eval_samples_per_second: 122.889, eval_steps_per_second: 3.84, epoch: 5.0[0m
[32m[2022-08-26 17:16:51,376] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-26 17:16:51,376] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:16:53,950] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-26 17:16:53,950] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-26 17:17:01,344] [    INFO][0m - loss: 0.61367431, learning_rate: 2.8349999999999998e-05, global_step: 110, interval_runtime: 11.2718, interval_samples_per_second: 0.71, interval_steps_per_second: 0.887, epoch: 5.5[0m
[32m[2022-08-26 17:17:02,837] [    INFO][0m - loss: 0.75682168, learning_rate: 2.8199999999999998e-05, global_step: 120, interval_runtime: 1.4921, interval_samples_per_second: 5.362, interval_steps_per_second: 6.702, epoch: 6.0[0m
[32m[2022-08-26 17:17:04,425] [    INFO][0m - loss: 0.60739903, learning_rate: 2.805e-05, global_step: 130, interval_runtime: 1.5889, interval_samples_per_second: 5.035, interval_steps_per_second: 6.294, epoch: 6.5[0m
[32m[2022-08-26 17:17:05,923] [    INFO][0m - loss: 0.47592611, learning_rate: 2.79e-05, global_step: 140, interval_runtime: 1.4976, interval_samples_per_second: 5.342, interval_steps_per_second: 6.677, epoch: 7.0[0m
[32m[2022-08-26 17:17:07,523] [    INFO][0m - loss: 0.3914269, learning_rate: 2.7750000000000004e-05, global_step: 150, interval_runtime: 1.6002, interval_samples_per_second: 4.999, interval_steps_per_second: 6.249, epoch: 7.5[0m
[32m[2022-08-26 17:17:09,020] [    INFO][0m - loss: 0.33692861, learning_rate: 2.7600000000000003e-05, global_step: 160, interval_runtime: 1.4971, interval_samples_per_second: 5.344, interval_steps_per_second: 6.68, epoch: 8.0[0m
[32m[2022-08-26 17:17:10,630] [    INFO][0m - loss: 0.32260754, learning_rate: 2.7450000000000003e-05, global_step: 170, interval_runtime: 1.6105, interval_samples_per_second: 4.968, interval_steps_per_second: 6.209, epoch: 8.5[0m
[32m[2022-08-26 17:17:12,118] [    INFO][0m - loss: 0.39856727, learning_rate: 2.7300000000000003e-05, global_step: 180, interval_runtime: 1.4877, interval_samples_per_second: 5.377, interval_steps_per_second: 6.722, epoch: 9.0[0m
[32m[2022-08-26 17:17:13,717] [    INFO][0m - loss: 0.14238567, learning_rate: 2.7150000000000003e-05, global_step: 190, interval_runtime: 1.5993, interval_samples_per_second: 5.002, interval_steps_per_second: 6.253, epoch: 9.5[0m
[32m[2022-08-26 17:17:15,212] [    INFO][0m - loss: 0.20176783, learning_rate: 2.7000000000000002e-05, global_step: 200, interval_runtime: 1.4949, interval_samples_per_second: 5.352, interval_steps_per_second: 6.689, epoch: 10.0[0m
[32m[2022-08-26 17:17:15,213] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:17:15,213] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 17:17:15,213] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:17:15,213] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:17:15,213] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 17:17:16,515] [    INFO][0m - eval_loss: 3.1064014434814453, eval_accuracy: 0.55, eval_runtime: 1.3012, eval_samples_per_second: 122.959, eval_steps_per_second: 3.842, epoch: 10.0[0m
[32m[2022-08-26 17:17:16,515] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-26 17:17:16,516] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:17:19,230] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-26 17:17:19,230] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-26 17:17:26,151] [    INFO][0m - loss: 0.16967487, learning_rate: 2.6850000000000002e-05, global_step: 210, interval_runtime: 10.9388, interval_samples_per_second: 0.731, interval_steps_per_second: 0.914, epoch: 10.5[0m
[32m[2022-08-26 17:17:27,642] [    INFO][0m - loss: 0.06834698, learning_rate: 2.6700000000000002e-05, global_step: 220, interval_runtime: 1.4904, interval_samples_per_second: 5.368, interval_steps_per_second: 6.71, epoch: 11.0[0m
[32m[2022-08-26 17:17:29,239] [    INFO][0m - loss: 0.31218326, learning_rate: 2.655e-05, global_step: 230, interval_runtime: 1.5975, interval_samples_per_second: 5.008, interval_steps_per_second: 6.26, epoch: 11.5[0m
[32m[2022-08-26 17:17:30,729] [    INFO][0m - loss: 0.29684443, learning_rate: 2.64e-05, global_step: 240, interval_runtime: 1.4896, interval_samples_per_second: 5.37, interval_steps_per_second: 6.713, epoch: 12.0[0m
[32m[2022-08-26 17:17:32,328] [    INFO][0m - loss: 0.08527505, learning_rate: 2.625e-05, global_step: 250, interval_runtime: 1.5998, interval_samples_per_second: 5.001, interval_steps_per_second: 6.251, epoch: 12.5[0m
[32m[2022-08-26 17:17:33,822] [    INFO][0m - loss: 0.01372935, learning_rate: 2.61e-05, global_step: 260, interval_runtime: 1.4939, interval_samples_per_second: 5.355, interval_steps_per_second: 6.694, epoch: 13.0[0m
[32m[2022-08-26 17:17:35,422] [    INFO][0m - loss: 0.00324856, learning_rate: 2.595e-05, global_step: 270, interval_runtime: 1.5998, interval_samples_per_second: 5.001, interval_steps_per_second: 6.251, epoch: 13.5[0m
[32m[2022-08-26 17:17:36,917] [    INFO][0m - loss: 0.00659719, learning_rate: 2.58e-05, global_step: 280, interval_runtime: 1.4949, interval_samples_per_second: 5.352, interval_steps_per_second: 6.69, epoch: 14.0[0m
[32m[2022-08-26 17:17:38,513] [    INFO][0m - loss: 0.0502714, learning_rate: 2.565e-05, global_step: 290, interval_runtime: 1.5961, interval_samples_per_second: 5.012, interval_steps_per_second: 6.265, epoch: 14.5[0m
[32m[2022-08-26 17:17:40,011] [    INFO][0m - loss: 0.05039949, learning_rate: 2.55e-05, global_step: 300, interval_runtime: 1.4977, interval_samples_per_second: 5.342, interval_steps_per_second: 6.677, epoch: 15.0[0m
[32m[2022-08-26 17:17:40,011] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:17:40,011] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 17:17:40,011] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:17:40,011] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:17:40,012] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 17:17:41,310] [    INFO][0m - eval_loss: 5.556523323059082, eval_accuracy: 0.5375, eval_runtime: 1.2983, eval_samples_per_second: 123.237, eval_steps_per_second: 3.851, epoch: 15.0[0m
[32m[2022-08-26 17:17:41,310] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-26 17:17:41,310] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:17:44,079] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-26 17:17:44,079] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-26 17:17:51,057] [    INFO][0m - loss: 0.00175884, learning_rate: 2.535e-05, global_step: 310, interval_runtime: 11.0463, interval_samples_per_second: 0.724, interval_steps_per_second: 0.905, epoch: 15.5[0m
[32m[2022-08-26 17:17:52,540] [    INFO][0m - loss: 0.00123355, learning_rate: 2.52e-05, global_step: 320, interval_runtime: 1.4827, interval_samples_per_second: 5.396, interval_steps_per_second: 6.744, epoch: 16.0[0m
[32m[2022-08-26 17:17:54,125] [    INFO][0m - loss: 0.0001714, learning_rate: 2.505e-05, global_step: 330, interval_runtime: 1.5858, interval_samples_per_second: 5.045, interval_steps_per_second: 6.306, epoch: 16.5[0m
[32m[2022-08-26 17:17:55,618] [    INFO][0m - loss: 0.26287823, learning_rate: 2.49e-05, global_step: 340, interval_runtime: 1.4924, interval_samples_per_second: 5.36, interval_steps_per_second: 6.7, epoch: 17.0[0m
[32m[2022-08-26 17:17:57,245] [    INFO][0m - loss: 0.06735183, learning_rate: 2.475e-05, global_step: 350, interval_runtime: 1.6267, interval_samples_per_second: 4.918, interval_steps_per_second: 6.147, epoch: 17.5[0m
[32m[2022-08-26 17:17:58,739] [    INFO][0m - loss: 0.02660311, learning_rate: 2.4599999999999998e-05, global_step: 360, interval_runtime: 1.4941, interval_samples_per_second: 5.354, interval_steps_per_second: 6.693, epoch: 18.0[0m
[32m[2022-08-26 17:18:00,332] [    INFO][0m - loss: 0.03694485, learning_rate: 2.4449999999999998e-05, global_step: 370, interval_runtime: 1.5927, interval_samples_per_second: 5.023, interval_steps_per_second: 6.279, epoch: 18.5[0m
[32m[2022-08-26 17:18:01,828] [    INFO][0m - loss: 0.09442518, learning_rate: 2.43e-05, global_step: 380, interval_runtime: 1.496, interval_samples_per_second: 5.347, interval_steps_per_second: 6.684, epoch: 19.0[0m
[32m[2022-08-26 17:18:03,428] [    INFO][0m - loss: 0.05705218, learning_rate: 2.415e-05, global_step: 390, interval_runtime: 1.6007, interval_samples_per_second: 4.998, interval_steps_per_second: 6.247, epoch: 19.5[0m
[32m[2022-08-26 17:18:04,925] [    INFO][0m - loss: 7.945e-05, learning_rate: 2.4e-05, global_step: 400, interval_runtime: 1.4965, interval_samples_per_second: 5.346, interval_steps_per_second: 6.682, epoch: 20.0[0m
[32m[2022-08-26 17:18:04,925] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:18:04,925] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 17:18:04,926] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:18:04,926] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:18:04,926] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 17:18:06,231] [    INFO][0m - eval_loss: 4.572381019592285, eval_accuracy: 0.58125, eval_runtime: 1.3053, eval_samples_per_second: 122.581, eval_steps_per_second: 3.831, epoch: 20.0[0m
[32m[2022-08-26 17:18:06,231] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-26 17:18:06,232] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:18:09,227] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-26 17:18:09,227] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-26 17:18:16,809] [    INFO][0m - loss: 4.193e-05, learning_rate: 2.385e-05, global_step: 410, interval_runtime: 11.8843, interval_samples_per_second: 0.673, interval_steps_per_second: 0.841, epoch: 20.5[0m
[32m[2022-08-26 17:18:18,295] [    INFO][0m - loss: 0.00729068, learning_rate: 2.37e-05, global_step: 420, interval_runtime: 1.4854, interval_samples_per_second: 5.386, interval_steps_per_second: 6.732, epoch: 21.0[0m
[32m[2022-08-26 17:18:19,889] [    INFO][0m - loss: 0.01396491, learning_rate: 2.3550000000000003e-05, global_step: 430, interval_runtime: 1.5943, interval_samples_per_second: 5.018, interval_steps_per_second: 6.272, epoch: 21.5[0m
[32m[2022-08-26 17:18:21,379] [    INFO][0m - loss: 0.00010223, learning_rate: 2.3400000000000003e-05, global_step: 440, interval_runtime: 1.4905, interval_samples_per_second: 5.367, interval_steps_per_second: 6.709, epoch: 22.0[0m
[32m[2022-08-26 17:18:22,977] [    INFO][0m - loss: 7.508e-05, learning_rate: 2.3250000000000003e-05, global_step: 450, interval_runtime: 1.5978, interval_samples_per_second: 5.007, interval_steps_per_second: 6.259, epoch: 22.5[0m
[32m[2022-08-26 17:18:24,474] [    INFO][0m - loss: 1.462e-05, learning_rate: 2.3100000000000002e-05, global_step: 460, interval_runtime: 1.4955, interval_samples_per_second: 5.349, interval_steps_per_second: 6.687, epoch: 23.0[0m
[32m[2022-08-26 17:18:26,073] [    INFO][0m - loss: 0.00025306, learning_rate: 2.2950000000000002e-05, global_step: 470, interval_runtime: 1.5998, interval_samples_per_second: 5.001, interval_steps_per_second: 6.251, epoch: 23.5[0m
[32m[2022-08-26 17:18:27,566] [    INFO][0m - loss: 2.152e-05, learning_rate: 2.2800000000000002e-05, global_step: 480, interval_runtime: 1.4935, interval_samples_per_second: 5.356, interval_steps_per_second: 6.695, epoch: 24.0[0m
[32m[2022-08-26 17:18:29,167] [    INFO][0m - loss: 0.04104089, learning_rate: 2.265e-05, global_step: 490, interval_runtime: 1.6005, interval_samples_per_second: 4.998, interval_steps_per_second: 6.248, epoch: 24.5[0m
[32m[2022-08-26 17:18:30,666] [    INFO][0m - loss: 0.06110258, learning_rate: 2.25e-05, global_step: 500, interval_runtime: 1.499, interval_samples_per_second: 5.337, interval_steps_per_second: 6.671, epoch: 25.0[0m
[32m[2022-08-26 17:18:30,666] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:18:30,666] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 17:18:30,666] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:18:30,666] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:18:30,666] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 17:18:31,976] [    INFO][0m - eval_loss: 5.638273239135742, eval_accuracy: 0.53125, eval_runtime: 1.3096, eval_samples_per_second: 122.179, eval_steps_per_second: 3.818, epoch: 25.0[0m
[32m[2022-08-26 17:18:31,977] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-08-26 17:18:31,977] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:18:34,333] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-08-26 17:18:34,333] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-08-26 17:18:41,016] [    INFO][0m - loss: 0.07288352, learning_rate: 2.235e-05, global_step: 510, interval_runtime: 10.3503, interval_samples_per_second: 0.773, interval_steps_per_second: 0.966, epoch: 25.5[0m
[32m[2022-08-26 17:18:42,502] [    INFO][0m - loss: 1.305e-05, learning_rate: 2.22e-05, global_step: 520, interval_runtime: 1.4865, interval_samples_per_second: 5.382, interval_steps_per_second: 6.727, epoch: 26.0[0m
[32m[2022-08-26 17:18:44,088] [    INFO][0m - loss: 0.1123212, learning_rate: 2.205e-05, global_step: 530, interval_runtime: 1.5857, interval_samples_per_second: 5.045, interval_steps_per_second: 6.307, epoch: 26.5[0m
[32m[2022-08-26 17:18:45,575] [    INFO][0m - loss: 5.081e-05, learning_rate: 2.19e-05, global_step: 540, interval_runtime: 1.4868, interval_samples_per_second: 5.381, interval_steps_per_second: 6.726, epoch: 27.0[0m
[32m[2022-08-26 17:18:47,463] [    INFO][0m - loss: 3.662e-05, learning_rate: 2.175e-05, global_step: 550, interval_runtime: 1.8875, interval_samples_per_second: 4.238, interval_steps_per_second: 5.298, epoch: 27.5[0m
[32m[2022-08-26 17:18:48,956] [    INFO][0m - loss: 0.00011496, learning_rate: 2.16e-05, global_step: 560, interval_runtime: 1.4934, interval_samples_per_second: 5.357, interval_steps_per_second: 6.696, epoch: 28.0[0m
[32m[2022-08-26 17:18:50,550] [    INFO][0m - loss: 0.08427097, learning_rate: 2.145e-05, global_step: 570, interval_runtime: 1.5943, interval_samples_per_second: 5.018, interval_steps_per_second: 6.272, epoch: 28.5[0m
[32m[2022-08-26 17:18:52,046] [    INFO][0m - loss: 0.01028643, learning_rate: 2.13e-05, global_step: 580, interval_runtime: 1.4957, interval_samples_per_second: 5.348, interval_steps_per_second: 6.686, epoch: 29.0[0m
[32m[2022-08-26 17:18:53,641] [    INFO][0m - loss: 1.637e-05, learning_rate: 2.115e-05, global_step: 590, interval_runtime: 1.5955, interval_samples_per_second: 5.014, interval_steps_per_second: 6.268, epoch: 29.5[0m
[32m[2022-08-26 17:18:55,135] [    INFO][0m - loss: 9.941e-05, learning_rate: 2.1e-05, global_step: 600, interval_runtime: 1.4939, interval_samples_per_second: 5.355, interval_steps_per_second: 6.694, epoch: 30.0[0m
[32m[2022-08-26 17:18:55,136] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:18:55,136] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 17:18:55,136] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:18:55,136] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:18:55,136] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 17:18:56,445] [    INFO][0m - eval_loss: 4.092185974121094, eval_accuracy: 0.5875, eval_runtime: 1.3085, eval_samples_per_second: 122.278, eval_steps_per_second: 3.821, epoch: 30.0[0m
[32m[2022-08-26 17:18:56,445] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-08-26 17:18:56,445] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:18:58,846] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-08-26 17:18:58,846] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-08-26 17:19:05,449] [    INFO][0m - loss: 0.03584276, learning_rate: 2.085e-05, global_step: 610, interval_runtime: 10.3135, interval_samples_per_second: 0.776, interval_steps_per_second: 0.97, epoch: 30.5[0m
[32m[2022-08-26 17:19:06,941] [    INFO][0m - loss: 1.606e-05, learning_rate: 2.07e-05, global_step: 620, interval_runtime: 1.4919, interval_samples_per_second: 5.362, interval_steps_per_second: 6.703, epoch: 31.0[0m
[32m[2022-08-26 17:19:08,538] [    INFO][0m - loss: 3.177e-05, learning_rate: 2.055e-05, global_step: 630, interval_runtime: 1.5973, interval_samples_per_second: 5.009, interval_steps_per_second: 6.261, epoch: 31.5[0m
[32m[2022-08-26 17:19:10,028] [    INFO][0m - loss: 0.00089865, learning_rate: 2.04e-05, global_step: 640, interval_runtime: 1.4902, interval_samples_per_second: 5.368, interval_steps_per_second: 6.71, epoch: 32.0[0m
[32m[2022-08-26 17:19:11,624] [    INFO][0m - loss: 0.00056058, learning_rate: 2.025e-05, global_step: 650, interval_runtime: 1.5959, interval_samples_per_second: 5.013, interval_steps_per_second: 6.266, epoch: 32.5[0m
[32m[2022-08-26 17:19:13,125] [    INFO][0m - loss: 4.96e-06, learning_rate: 2.01e-05, global_step: 660, interval_runtime: 1.5007, interval_samples_per_second: 5.331, interval_steps_per_second: 6.664, epoch: 33.0[0m
[32m[2022-08-26 17:19:14,723] [    INFO][0m - loss: 9.27e-06, learning_rate: 1.995e-05, global_step: 670, interval_runtime: 1.598, interval_samples_per_second: 5.006, interval_steps_per_second: 6.258, epoch: 33.5[0m
[32m[2022-08-26 17:19:16,222] [    INFO][0m - loss: 6.43e-06, learning_rate: 1.98e-05, global_step: 680, interval_runtime: 1.499, interval_samples_per_second: 5.337, interval_steps_per_second: 6.671, epoch: 34.0[0m
[32m[2022-08-26 17:19:17,821] [    INFO][0m - loss: 9.499e-05, learning_rate: 1.965e-05, global_step: 690, interval_runtime: 1.5994, interval_samples_per_second: 5.002, interval_steps_per_second: 6.252, epoch: 34.5[0m
[32m[2022-08-26 17:19:19,316] [    INFO][0m - loss: 6.23e-06, learning_rate: 1.95e-05, global_step: 700, interval_runtime: 1.4944, interval_samples_per_second: 5.353, interval_steps_per_second: 6.692, epoch: 35.0[0m
[32m[2022-08-26 17:19:19,316] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:19:19,316] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 17:19:19,316] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:19:19,316] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:19:19,316] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 17:19:20,620] [    INFO][0m - eval_loss: 4.067996501922607, eval_accuracy: 0.59375, eval_runtime: 1.3033, eval_samples_per_second: 122.761, eval_steps_per_second: 3.836, epoch: 35.0[0m
[32m[2022-08-26 17:19:20,620] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-700[0m
[32m[2022-08-26 17:19:20,620] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:19:23,226] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-700/tokenizer_config.json[0m
[32m[2022-08-26 17:19:23,227] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-700/special_tokens_map.json[0m
[32m[2022-08-26 17:19:29,876] [    INFO][0m - loss: 5.169e-05, learning_rate: 1.935e-05, global_step: 710, interval_runtime: 10.5606, interval_samples_per_second: 0.758, interval_steps_per_second: 0.947, epoch: 35.5[0m
[32m[2022-08-26 17:19:31,368] [    INFO][0m - loss: 7.49e-06, learning_rate: 1.9200000000000003e-05, global_step: 720, interval_runtime: 1.4918, interval_samples_per_second: 5.363, interval_steps_per_second: 6.703, epoch: 36.0[0m
[32m[2022-08-26 17:19:32,957] [    INFO][0m - loss: 0.00373508, learning_rate: 1.9050000000000002e-05, global_step: 730, interval_runtime: 1.5891, interval_samples_per_second: 5.034, interval_steps_per_second: 6.293, epoch: 36.5[0m
[32m[2022-08-26 17:19:34,451] [    INFO][0m - loss: 0.07692357, learning_rate: 1.8900000000000002e-05, global_step: 740, interval_runtime: 1.4943, interval_samples_per_second: 5.354, interval_steps_per_second: 6.692, epoch: 37.0[0m
[32m[2022-08-26 17:19:36,040] [    INFO][0m - loss: 0.00185725, learning_rate: 1.8750000000000002e-05, global_step: 750, interval_runtime: 1.5881, interval_samples_per_second: 5.038, interval_steps_per_second: 6.297, epoch: 37.5[0m
[32m[2022-08-26 17:19:37,535] [    INFO][0m - loss: 6.09e-05, learning_rate: 1.86e-05, global_step: 760, interval_runtime: 1.4955, interval_samples_per_second: 5.35, interval_steps_per_second: 6.687, epoch: 38.0[0m
[32m[2022-08-26 17:19:39,134] [    INFO][0m - loss: 0.00155317, learning_rate: 1.845e-05, global_step: 770, interval_runtime: 1.5992, interval_samples_per_second: 5.003, interval_steps_per_second: 6.253, epoch: 38.5[0m
[32m[2022-08-26 17:19:40,633] [    INFO][0m - loss: 6.08e-06, learning_rate: 1.83e-05, global_step: 780, interval_runtime: 1.4987, interval_samples_per_second: 5.338, interval_steps_per_second: 6.673, epoch: 39.0[0m
[32m[2022-08-26 17:19:42,246] [    INFO][0m - loss: 0.00135714, learning_rate: 1.815e-05, global_step: 790, interval_runtime: 1.6136, interval_samples_per_second: 4.958, interval_steps_per_second: 6.197, epoch: 39.5[0m
[32m[2022-08-26 17:19:43,744] [    INFO][0m - loss: 0.0001082, learning_rate: 1.8e-05, global_step: 800, interval_runtime: 1.4973, interval_samples_per_second: 5.343, interval_steps_per_second: 6.678, epoch: 40.0[0m
[32m[2022-08-26 17:19:43,744] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:19:43,744] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 17:19:43,744] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:19:43,744] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:19:43,744] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 17:19:45,049] [    INFO][0m - eval_loss: 5.6160783767700195, eval_accuracy: 0.51875, eval_runtime: 1.3047, eval_samples_per_second: 122.635, eval_steps_per_second: 3.832, epoch: 40.0[0m
[32m[2022-08-26 17:19:45,050] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-800[0m
[32m[2022-08-26 17:19:45,050] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:19:47,402] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-800/tokenizer_config.json[0m
[32m[2022-08-26 17:19:47,402] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-800/special_tokens_map.json[0m
[32m[2022-08-26 17:19:54,289] [    INFO][0m - loss: 0.1367745, learning_rate: 1.785e-05, global_step: 810, interval_runtime: 10.5445, interval_samples_per_second: 0.759, interval_steps_per_second: 0.948, epoch: 40.5[0m
[32m[2022-08-26 17:19:55,776] [    INFO][0m - loss: 5.78e-06, learning_rate: 1.77e-05, global_step: 820, interval_runtime: 1.4879, interval_samples_per_second: 5.377, interval_steps_per_second: 6.721, epoch: 41.0[0m
[32m[2022-08-26 17:19:57,365] [    INFO][0m - loss: 8.083e-05, learning_rate: 1.755e-05, global_step: 830, interval_runtime: 1.5892, interval_samples_per_second: 5.034, interval_steps_per_second: 6.293, epoch: 41.5[0m
[32m[2022-08-26 17:19:58,859] [    INFO][0m - loss: 0.00020275, learning_rate: 1.74e-05, global_step: 840, interval_runtime: 1.4938, interval_samples_per_second: 5.355, interval_steps_per_second: 6.694, epoch: 42.0[0m
[32m[2022-08-26 17:20:00,455] [    INFO][0m - loss: 8.06e-06, learning_rate: 1.725e-05, global_step: 850, interval_runtime: 1.5962, interval_samples_per_second: 5.012, interval_steps_per_second: 6.265, epoch: 42.5[0m
[32m[2022-08-26 17:20:01,949] [    INFO][0m - loss: 0.00244649, learning_rate: 1.71e-05, global_step: 860, interval_runtime: 1.4939, interval_samples_per_second: 5.355, interval_steps_per_second: 6.694, epoch: 43.0[0m
[32m[2022-08-26 17:20:03,545] [    INFO][0m - loss: 0.11925542, learning_rate: 1.695e-05, global_step: 870, interval_runtime: 1.5954, interval_samples_per_second: 5.015, interval_steps_per_second: 6.268, epoch: 43.5[0m
[32m[2022-08-26 17:20:05,039] [    INFO][0m - loss: 1.866e-05, learning_rate: 1.6800000000000002e-05, global_step: 880, interval_runtime: 1.4943, interval_samples_per_second: 5.354, interval_steps_per_second: 6.692, epoch: 44.0[0m
[32m[2022-08-26 17:20:06,657] [    INFO][0m - loss: 6.48e-06, learning_rate: 1.665e-05, global_step: 890, interval_runtime: 1.6177, interval_samples_per_second: 4.945, interval_steps_per_second: 6.182, epoch: 44.5[0m
[32m[2022-08-26 17:20:08,152] [    INFO][0m - loss: 0.0016196, learning_rate: 1.65e-05, global_step: 900, interval_runtime: 1.4958, interval_samples_per_second: 5.348, interval_steps_per_second: 6.685, epoch: 45.0[0m
[32m[2022-08-26 17:20:08,153] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:20:08,153] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 17:20:08,153] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:20:08,153] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:20:08,153] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 17:20:09,454] [    INFO][0m - eval_loss: 4.3170695304870605, eval_accuracy: 0.56875, eval_runtime: 1.3005, eval_samples_per_second: 123.03, eval_steps_per_second: 3.845, epoch: 45.0[0m
[32m[2022-08-26 17:20:09,454] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-900[0m
[32m[2022-08-26 17:20:09,454] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:20:11,894] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-900/tokenizer_config.json[0m
[32m[2022-08-26 17:20:11,894] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-900/special_tokens_map.json[0m
[32m[2022-08-26 17:20:18,451] [    INFO][0m - loss: 9.544e-05, learning_rate: 1.635e-05, global_step: 910, interval_runtime: 10.2981, interval_samples_per_second: 0.777, interval_steps_per_second: 0.971, epoch: 45.5[0m
[32m[2022-08-26 17:20:19,939] [    INFO][0m - loss: 1.495e-05, learning_rate: 1.62e-05, global_step: 920, interval_runtime: 1.488, interval_samples_per_second: 5.376, interval_steps_per_second: 6.721, epoch: 46.0[0m
[32m[2022-08-26 17:20:21,541] [    INFO][0m - loss: 0.09701769, learning_rate: 1.605e-05, global_step: 930, interval_runtime: 1.6024, interval_samples_per_second: 4.992, interval_steps_per_second: 6.241, epoch: 46.5[0m
[32m[2022-08-26 17:20:23,031] [    INFO][0m - loss: 0.00031778, learning_rate: 1.59e-05, global_step: 940, interval_runtime: 1.4903, interval_samples_per_second: 5.368, interval_steps_per_second: 6.71, epoch: 47.0[0m
[32m[2022-08-26 17:20:24,621] [    INFO][0m - loss: 6.94e-06, learning_rate: 1.575e-05, global_step: 950, interval_runtime: 1.5895, interval_samples_per_second: 5.033, interval_steps_per_second: 6.291, epoch: 47.5[0m
[32m[2022-08-26 17:20:26,114] [    INFO][0m - loss: 0.12599748, learning_rate: 1.56e-05, global_step: 960, interval_runtime: 1.4929, interval_samples_per_second: 5.359, interval_steps_per_second: 6.698, epoch: 48.0[0m
[32m[2022-08-26 17:20:27,718] [    INFO][0m - loss: 0.00039778, learning_rate: 1.545e-05, global_step: 970, interval_runtime: 1.6041, interval_samples_per_second: 4.987, interval_steps_per_second: 6.234, epoch: 48.5[0m
[32m[2022-08-26 17:20:29,217] [    INFO][0m - loss: 0.00061236, learning_rate: 1.53e-05, global_step: 980, interval_runtime: 1.4986, interval_samples_per_second: 5.338, interval_steps_per_second: 6.673, epoch: 49.0[0m
[32m[2022-08-26 17:20:30,827] [    INFO][0m - loss: 1.387e-05, learning_rate: 1.515e-05, global_step: 990, interval_runtime: 1.6106, interval_samples_per_second: 4.967, interval_steps_per_second: 6.209, epoch: 49.5[0m
[32m[2022-08-26 17:20:32,322] [    INFO][0m - loss: 1.973e-05, learning_rate: 1.5e-05, global_step: 1000, interval_runtime: 1.4945, interval_samples_per_second: 5.353, interval_steps_per_second: 6.691, epoch: 50.0[0m
[32m[2022-08-26 17:20:32,322] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:20:32,322] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 17:20:32,322] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:20:32,322] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:20:32,322] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 17:20:33,622] [    INFO][0m - eval_loss: 3.7009406089782715, eval_accuracy: 0.63125, eval_runtime: 1.2991, eval_samples_per_second: 123.163, eval_steps_per_second: 3.849, epoch: 50.0[0m
[32m[2022-08-26 17:20:33,622] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1000[0m
[32m[2022-08-26 17:20:33,622] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:20:36,288] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1000/tokenizer_config.json[0m
[32m[2022-08-26 17:20:36,288] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1000/special_tokens_map.json[0m
[32m[2022-08-26 17:20:43,239] [    INFO][0m - loss: 3.625e-05, learning_rate: 1.485e-05, global_step: 1010, interval_runtime: 10.9171, interval_samples_per_second: 0.733, interval_steps_per_second: 0.916, epoch: 50.5[0m
[32m[2022-08-26 17:20:44,728] [    INFO][0m - loss: 1.177e-05, learning_rate: 1.47e-05, global_step: 1020, interval_runtime: 1.4894, interval_samples_per_second: 5.371, interval_steps_per_second: 6.714, epoch: 51.0[0m
[32m[2022-08-26 17:20:46,334] [    INFO][0m - loss: 7.48e-06, learning_rate: 1.455e-05, global_step: 1030, interval_runtime: 1.6053, interval_samples_per_second: 4.984, interval_steps_per_second: 6.229, epoch: 51.5[0m
[32m[2022-08-26 17:20:47,824] [    INFO][0m - loss: 6.87e-06, learning_rate: 1.44e-05, global_step: 1040, interval_runtime: 1.4907, interval_samples_per_second: 5.367, interval_steps_per_second: 6.708, epoch: 52.0[0m
[32m[2022-08-26 17:20:49,425] [    INFO][0m - loss: 4.78e-06, learning_rate: 1.4249999999999999e-05, global_step: 1050, interval_runtime: 1.6007, interval_samples_per_second: 4.998, interval_steps_per_second: 6.247, epoch: 52.5[0m
[32m[2022-08-26 17:20:50,919] [    INFO][0m - loss: 1.795e-05, learning_rate: 1.4099999999999999e-05, global_step: 1060, interval_runtime: 1.4941, interval_samples_per_second: 5.354, interval_steps_per_second: 6.693, epoch: 53.0[0m
[32m[2022-08-26 17:20:52,524] [    INFO][0m - loss: 1.579e-05, learning_rate: 1.395e-05, global_step: 1070, interval_runtime: 1.6045, interval_samples_per_second: 4.986, interval_steps_per_second: 6.232, epoch: 53.5[0m
[32m[2022-08-26 17:20:54,019] [    INFO][0m - loss: 5.19e-06, learning_rate: 1.3800000000000002e-05, global_step: 1080, interval_runtime: 1.4957, interval_samples_per_second: 5.348, interval_steps_per_second: 6.686, epoch: 54.0[0m
[32m[2022-08-26 17:20:55,619] [    INFO][0m - loss: 4.64e-06, learning_rate: 1.3650000000000001e-05, global_step: 1090, interval_runtime: 1.5989, interval_samples_per_second: 5.003, interval_steps_per_second: 6.254, epoch: 54.5[0m
[32m[2022-08-26 17:20:57,122] [    INFO][0m - loss: 4.91e-06, learning_rate: 1.3500000000000001e-05, global_step: 1100, interval_runtime: 1.5035, interval_samples_per_second: 5.321, interval_steps_per_second: 6.651, epoch: 55.0[0m
[32m[2022-08-26 17:20:57,122] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:20:57,122] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 17:20:57,122] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:20:57,123] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:20:57,123] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 17:20:58,450] [    INFO][0m - eval_loss: 4.341835021972656, eval_accuracy: 0.59375, eval_runtime: 1.3278, eval_samples_per_second: 120.503, eval_steps_per_second: 3.766, epoch: 55.0[0m
[32m[2022-08-26 17:20:58,451] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1100[0m
[32m[2022-08-26 17:20:58,451] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:21:00,771] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1100/tokenizer_config.json[0m
[32m[2022-08-26 17:21:00,772] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1100/special_tokens_map.json[0m
[32m[2022-08-26 17:21:07,661] [    INFO][0m - loss: 8.21e-06, learning_rate: 1.3350000000000001e-05, global_step: 1110, interval_runtime: 10.5388, interval_samples_per_second: 0.759, interval_steps_per_second: 0.949, epoch: 55.5[0m
[32m[2022-08-26 17:21:09,149] [    INFO][0m - loss: 5.53e-06, learning_rate: 1.32e-05, global_step: 1120, interval_runtime: 1.4884, interval_samples_per_second: 5.375, interval_steps_per_second: 6.719, epoch: 56.0[0m
[32m[2022-08-26 17:21:10,754] [    INFO][0m - loss: 4.46e-06, learning_rate: 1.305e-05, global_step: 1130, interval_runtime: 1.6046, interval_samples_per_second: 4.986, interval_steps_per_second: 6.232, epoch: 56.5[0m
[32m[2022-08-26 17:21:12,248] [    INFO][0m - loss: 3.71e-06, learning_rate: 1.29e-05, global_step: 1140, interval_runtime: 1.4946, interval_samples_per_second: 5.353, interval_steps_per_second: 6.691, epoch: 57.0[0m
[32m[2022-08-26 17:21:13,846] [    INFO][0m - loss: 4.23e-06, learning_rate: 1.275e-05, global_step: 1150, interval_runtime: 1.5984, interval_samples_per_second: 5.005, interval_steps_per_second: 6.256, epoch: 57.5[0m
[32m[2022-08-26 17:21:15,344] [    INFO][0m - loss: 3.5e-06, learning_rate: 1.26e-05, global_step: 1160, interval_runtime: 1.4975, interval_samples_per_second: 5.342, interval_steps_per_second: 6.678, epoch: 58.0[0m
[32m[2022-08-26 17:21:16,939] [    INFO][0m - loss: 5.59e-06, learning_rate: 1.245e-05, global_step: 1170, interval_runtime: 1.5955, interval_samples_per_second: 5.014, interval_steps_per_second: 6.268, epoch: 58.5[0m
[32m[2022-08-26 17:21:18,435] [    INFO][0m - loss: 3.55e-06, learning_rate: 1.2299999999999999e-05, global_step: 1180, interval_runtime: 1.4955, interval_samples_per_second: 5.349, interval_steps_per_second: 6.687, epoch: 59.0[0m
[32m[2022-08-26 17:21:20,037] [    INFO][0m - loss: 3.68e-06, learning_rate: 1.215e-05, global_step: 1190, interval_runtime: 1.6023, interval_samples_per_second: 4.993, interval_steps_per_second: 6.241, epoch: 59.5[0m
[32m[2022-08-26 17:21:21,534] [    INFO][0m - loss: 1.185e-05, learning_rate: 1.2e-05, global_step: 1200, interval_runtime: 1.4963, interval_samples_per_second: 5.346, interval_steps_per_second: 6.683, epoch: 60.0[0m
[32m[2022-08-26 17:21:21,534] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:21:21,534] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 17:21:21,534] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:21:21,534] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:21:21,534] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 17:21:22,836] [    INFO][0m - eval_loss: 4.47092342376709, eval_accuracy: 0.59375, eval_runtime: 1.3015, eval_samples_per_second: 122.939, eval_steps_per_second: 3.842, epoch: 60.0[0m
[32m[2022-08-26 17:21:22,836] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1200[0m
[32m[2022-08-26 17:21:22,836] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:21:25,479] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1200/tokenizer_config.json[0m
[32m[2022-08-26 17:21:25,480] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1200/special_tokens_map.json[0m
[32m[2022-08-26 17:21:31,960] [    INFO][0m - loss: 4.77e-06, learning_rate: 1.185e-05, global_step: 1210, interval_runtime: 10.4262, interval_samples_per_second: 0.767, interval_steps_per_second: 0.959, epoch: 60.5[0m
[32m[2022-08-26 17:21:33,451] [    INFO][0m - loss: 0.00309598, learning_rate: 1.1700000000000001e-05, global_step: 1220, interval_runtime: 1.4915, interval_samples_per_second: 5.364, interval_steps_per_second: 6.705, epoch: 61.0[0m
[32m[2022-08-26 17:21:35,056] [    INFO][0m - loss: 3.25e-06, learning_rate: 1.1550000000000001e-05, global_step: 1230, interval_runtime: 1.6051, interval_samples_per_second: 4.984, interval_steps_per_second: 6.23, epoch: 61.5[0m
[32m[2022-08-26 17:21:36,548] [    INFO][0m - loss: 0.11585734, learning_rate: 1.1400000000000001e-05, global_step: 1240, interval_runtime: 1.4917, interval_samples_per_second: 5.363, interval_steps_per_second: 6.704, epoch: 62.0[0m
[32m[2022-08-26 17:21:38,146] [    INFO][0m - loss: 0.01593623, learning_rate: 1.125e-05, global_step: 1250, interval_runtime: 1.5977, interval_samples_per_second: 5.007, interval_steps_per_second: 6.259, epoch: 62.5[0m
[32m[2022-08-26 17:21:39,645] [    INFO][0m - loss: 9.71e-06, learning_rate: 1.11e-05, global_step: 1260, interval_runtime: 1.4987, interval_samples_per_second: 5.338, interval_steps_per_second: 6.672, epoch: 63.0[0m
[32m[2022-08-26 17:21:41,238] [    INFO][0m - loss: 7.77e-06, learning_rate: 1.095e-05, global_step: 1270, interval_runtime: 1.5934, interval_samples_per_second: 5.021, interval_steps_per_second: 6.276, epoch: 63.5[0m
[32m[2022-08-26 17:21:42,744] [    INFO][0m - loss: 4.98e-06, learning_rate: 1.08e-05, global_step: 1280, interval_runtime: 1.5063, interval_samples_per_second: 5.311, interval_steps_per_second: 6.639, epoch: 64.0[0m
[32m[2022-08-26 17:21:44,346] [    INFO][0m - loss: 4.579e-05, learning_rate: 1.065e-05, global_step: 1290, interval_runtime: 1.6014, interval_samples_per_second: 4.996, interval_steps_per_second: 6.245, epoch: 64.5[0m
[32m[2022-08-26 17:21:45,842] [    INFO][0m - loss: 3.15e-06, learning_rate: 1.05e-05, global_step: 1300, interval_runtime: 1.4968, interval_samples_per_second: 5.345, interval_steps_per_second: 6.681, epoch: 65.0[0m
[32m[2022-08-26 17:21:45,843] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:21:45,843] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 17:21:45,843] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:21:45,843] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:21:45,843] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 17:21:47,143] [    INFO][0m - eval_loss: 4.548888206481934, eval_accuracy: 0.58125, eval_runtime: 1.2995, eval_samples_per_second: 123.123, eval_steps_per_second: 3.848, epoch: 65.0[0m
[32m[2022-08-26 17:21:47,143] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1300[0m
[32m[2022-08-26 17:21:47,143] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:21:49,448] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1300/tokenizer_config.json[0m
[32m[2022-08-26 17:21:49,448] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1300/special_tokens_map.json[0m
[32m[2022-08-26 17:21:56,256] [    INFO][0m - loss: 3.81e-06, learning_rate: 1.035e-05, global_step: 1310, interval_runtime: 10.4133, interval_samples_per_second: 0.768, interval_steps_per_second: 0.96, epoch: 65.5[0m
[32m[2022-08-26 17:21:57,744] [    INFO][0m - loss: 4e-06, learning_rate: 1.02e-05, global_step: 1320, interval_runtime: 1.4882, interval_samples_per_second: 5.376, interval_steps_per_second: 6.72, epoch: 66.0[0m
[32m[2022-08-26 17:21:59,340] [    INFO][0m - loss: 3.01e-06, learning_rate: 1.005e-05, global_step: 1330, interval_runtime: 1.5962, interval_samples_per_second: 5.012, interval_steps_per_second: 6.265, epoch: 66.5[0m
[32m[2022-08-26 17:22:00,833] [    INFO][0m - loss: 5.03e-06, learning_rate: 9.9e-06, global_step: 1340, interval_runtime: 1.4933, interval_samples_per_second: 5.357, interval_steps_per_second: 6.696, epoch: 67.0[0m
[32m[2022-08-26 17:22:02,438] [    INFO][0m - loss: 4.36e-06, learning_rate: 9.75e-06, global_step: 1350, interval_runtime: 1.6049, interval_samples_per_second: 4.985, interval_steps_per_second: 6.231, epoch: 67.5[0m
[32m[2022-08-26 17:22:03,940] [    INFO][0m - loss: 3.02e-06, learning_rate: 9.600000000000001e-06, global_step: 1360, interval_runtime: 1.502, interval_samples_per_second: 5.326, interval_steps_per_second: 6.658, epoch: 68.0[0m
[32m[2022-08-26 17:22:05,537] [    INFO][0m - loss: 2.59e-06, learning_rate: 9.450000000000001e-06, global_step: 1370, interval_runtime: 1.5967, interval_samples_per_second: 5.01, interval_steps_per_second: 6.263, epoch: 68.5[0m
[32m[2022-08-26 17:22:07,037] [    INFO][0m - loss: 5.79e-06, learning_rate: 9.3e-06, global_step: 1380, interval_runtime: 1.4997, interval_samples_per_second: 5.334, interval_steps_per_second: 6.668, epoch: 69.0[0m
[32m[2022-08-26 17:22:08,639] [    INFO][0m - loss: 3.22e-06, learning_rate: 9.15e-06, global_step: 1390, interval_runtime: 1.6024, interval_samples_per_second: 4.992, interval_steps_per_second: 6.24, epoch: 69.5[0m
[32m[2022-08-26 17:22:10,145] [    INFO][0m - loss: 2.87e-06, learning_rate: 9e-06, global_step: 1400, interval_runtime: 1.5052, interval_samples_per_second: 5.315, interval_steps_per_second: 6.644, epoch: 70.0[0m
[32m[2022-08-26 17:22:10,145] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:22:10,145] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 17:22:10,145] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:22:10,145] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:22:10,146] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 17:22:11,454] [    INFO][0m - eval_loss: 4.713723182678223, eval_accuracy: 0.58125, eval_runtime: 1.3081, eval_samples_per_second: 122.311, eval_steps_per_second: 3.822, epoch: 70.0[0m
[32m[2022-08-26 17:22:11,454] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1400[0m
[32m[2022-08-26 17:22:11,454] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:22:14,069] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1400/tokenizer_config.json[0m
[32m[2022-08-26 17:22:14,069] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1400/special_tokens_map.json[0m
[32m[2022-08-26 17:22:19,376] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-26 17:22:19,377] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-1000 (score: 0.63125).[0m
[32m[2022-08-26 17:22:20,083] [    INFO][0m - train_runtime: 346.3216, train_samples_per_second: 46.2, train_steps_per_second: 5.775, train_loss: 0.10211099767831391, epoch: 70.0[0m
[32m[2022-08-26 17:22:20,125] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-26 17:22:20,125] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:22:22,500] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-26 17:22:22,500] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-26 17:22:22,501] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-26 17:22:22,501] [    INFO][0m -   epoch                    =       70.0[0m
[32m[2022-08-26 17:22:22,501] [    INFO][0m -   train_loss               =     0.1021[0m
[32m[2022-08-26 17:22:22,501] [    INFO][0m -   train_runtime            = 0:05:46.32[0m
[32m[2022-08-26 17:22:22,501] [    INFO][0m -   train_samples_per_second =       46.2[0m
[32m[2022-08-26 17:22:22,502] [    INFO][0m -   train_steps_per_second   =      5.775[0m
[32m[2022-08-26 17:22:22,508] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-26 17:22:22,508] [    INFO][0m -   Num examples = 2838[0m
[32m[2022-08-26 17:22:22,508] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:22:22,508] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:22:22,508] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-08-26 17:22:45,921] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-26 17:22:45,922] [    INFO][0m -   test_accuracy           =     0.5592[0m
[32m[2022-08-26 17:22:45,922] [    INFO][0m -   test_loss               =     4.4628[0m
[32m[2022-08-26 17:22:45,922] [    INFO][0m -   test_runtime            = 0:00:23.41[0m
[32m[2022-08-26 17:22:45,922] [    INFO][0m -   test_samples_per_second =    121.213[0m
[32m[2022-08-26 17:22:45,922] [    INFO][0m -   test_steps_per_second   =      3.801[0m
[32m[2022-08-26 17:22:45,923] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-26 17:22:45,923] [    INFO][0m -   Num examples = 3000[0m
[32m[2022-08-26 17:22:45,923] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:22:45,923] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:22:45,923] [    INFO][0m -   Total prediction steps = 94[0m
[32m[2022-08-26 17:23:17,604] [    INFO][0m - Predictions for cslf saved to ./fewclue_submit_examples.[0m
