 
==========
eprstmt
==========
 
[33m[2022-08-30 11:59:58,713] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-30 11:59:58,713] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-30 11:59:58,713] [    INFO][0m - ============================================================[0m
[32m[2022-08-30 11:59:58,713] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-30 11:59:58,714] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-30 11:59:58,714] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-30 11:59:58,714] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-30 11:59:58,714] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-08-30 11:59:58,714] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-30 11:59:58,714] [    INFO][0m - model_name_or_path            :hfl/roberta-wwm-ext-large[0m
[32m[2022-08-30 11:59:58,714] [    INFO][0m - [0m
[32m[2022-08-30 11:59:58,714] [    INFO][0m - ============================================================[0m
[32m[2022-08-30 11:59:58,714] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-30 11:59:58,714] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-30 11:59:58,714] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-30 11:59:58,714] [    INFO][0m - prompt                        :{'text':'text_a'}{'hard':'我感觉'}{'mask'}{'hard':'喜欢。'}[0m
[32m[2022-08-30 11:59:58,715] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-30 11:59:58,715] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-30 11:59:58,715] [    INFO][0m - task_name                     :eprstmt[0m
[32m[2022-08-30 11:59:58,715] [    INFO][0m - [0m
[32m[2022-08-30 11:59:58,715] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/hfl/roberta-wwm-ext-large/roberta_chn_large.pdparams[0m
W0830 11:59:58.716497 46315 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0830 11:59:58.720367 46315 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-30 12:00:03,482] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/hfl/roberta-wwm-ext-large/vocab.txt[0m
[32m[2022-08-30 12:00:03,497] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/hfl/roberta-wwm-ext-large/tokenizer_config.json[0m
[32m[2022-08-30 12:00:03,498] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/hfl/roberta-wwm-ext-large/special_tokens_map.json[0m
[32m[2022-08-30 12:00:03,498] [    INFO][0m - Using template: [{'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '我感觉'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': '喜欢。'}][0m
2022-08-30 12:00:03,504 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-30 12:00:03,603] [    INFO][0m - ============================================================[0m
[32m[2022-08-30 12:00:03,604] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-30 12:00:03,604] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-30 12:00:03,604] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-30 12:00:03,604] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-30 12:00:03,604] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-30 12:00:03,604] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-30 12:00:03,604] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-30 12:00:03,604] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-30 12:00:03,605] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-30 12:00:03,605] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-30 12:00:03,605] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-30 12:00:03,605] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-30 12:00:03,605] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-30 12:00:03,605] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-30 12:00:03,605] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-30 12:00:03,605] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-30 12:00:03,605] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-30 12:00:03,605] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-30 12:00:03,605] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-30 12:00:03,605] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-30 12:00:03,606] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-30 12:00:03,606] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-30 12:00:03,606] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-30 12:00:03,606] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-30 12:00:03,606] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-30 12:00:03,606] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-30 12:00:03,606] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-30 12:00:03,606] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-30 12:00:03,606] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-30 12:00:03,606] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-30 12:00:03,606] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-30 12:00:03,606] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-30 12:00:03,606] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-30 12:00:03,607] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-30 12:00:03,607] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-30 12:00:03,607] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-30 12:00:03,607] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug30_11-59-58_instance-3bwob41y-01[0m
[32m[2022-08-30 12:00:03,607] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-30 12:00:03,607] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-30 12:00:03,607] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-30 12:00:03,607] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-30 12:00:03,607] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-30 12:00:03,607] [    INFO][0m - max_seq_length                :128[0m
[32m[2022-08-30 12:00:03,607] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-30 12:00:03,607] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-30 12:00:03,607] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-30 12:00:03,607] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-30 12:00:03,608] [    INFO][0m - num_train_epochs              :100.0[0m
[32m[2022-08-30 12:00:03,608] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-30 12:00:03,608] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-30 12:00:03,608] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-30 12:00:03,608] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-30 12:00:03,608] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-30 12:00:03,608] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-30 12:00:03,608] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-30 12:00:03,608] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-30 12:00:03,608] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-30 12:00:03,608] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-30 12:00:03,608] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-30 12:00:03,608] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-30 12:00:03,609] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-30 12:00:03,609] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-30 12:00:03,609] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-30 12:00:03,609] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-30 12:00:03,609] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-30 12:00:03,609] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-30 12:00:03,609] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-30 12:00:03,609] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-30 12:00:03,609] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-30 12:00:03,609] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-30 12:00:03,609] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-30 12:00:03,609] [    INFO][0m - seed                          :42[0m
[32m[2022-08-30 12:00:03,609] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-30 12:00:03,610] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-30 12:00:03,610] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-30 12:00:03,610] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-30 12:00:03,610] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-30 12:00:03,610] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-30 12:00:03,610] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-30 12:00:03,610] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-30 12:00:03,610] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-30 12:00:03,610] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-30 12:00:03,610] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-30 12:00:03,610] [    INFO][0m - [0m
[32m[2022-08-30 12:00:03,613] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-30 12:00:03,614] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 12:00:03,614] [    INFO][0m -   Num Epochs = 100[0m
[32m[2022-08-30 12:00:03,614] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-30 12:00:03,614] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-30 12:00:03,614] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-30 12:00:03,614] [    INFO][0m -   Total optimization steps = 2000.0[0m
[32m[2022-08-30 12:00:03,614] [    INFO][0m -   Total num train samples = 16000[0m
[32m[2022-08-30 12:00:07,097] [    INFO][0m - loss: 0.82196989, learning_rate: 2.985e-05, global_step: 10, interval_runtime: 3.4811, interval_samples_per_second: 2.298, interval_steps_per_second: 2.873, epoch: 0.5[0m
[32m[2022-08-30 12:00:09,180] [    INFO][0m - loss: 0.59029427, learning_rate: 2.97e-05, global_step: 20, interval_runtime: 2.0838, interval_samples_per_second: 3.839, interval_steps_per_second: 4.799, epoch: 1.0[0m
[32m[2022-08-30 12:00:11,304] [    INFO][0m - loss: 0.28546429, learning_rate: 2.955e-05, global_step: 30, interval_runtime: 2.1235, interval_samples_per_second: 3.767, interval_steps_per_second: 4.709, epoch: 1.5[0m
[32m[2022-08-30 12:00:13,389] [    INFO][0m - loss: 0.09974904, learning_rate: 2.94e-05, global_step: 40, interval_runtime: 2.0858, interval_samples_per_second: 3.836, interval_steps_per_second: 4.794, epoch: 2.0[0m
[32m[2022-08-30 12:00:15,521] [    INFO][0m - loss: 0.00112784, learning_rate: 2.925e-05, global_step: 50, interval_runtime: 2.1322, interval_samples_per_second: 3.752, interval_steps_per_second: 4.69, epoch: 2.5[0m
[32m[2022-08-30 12:00:17,606] [    INFO][0m - loss: 0.19859473, learning_rate: 2.91e-05, global_step: 60, interval_runtime: 2.0851, interval_samples_per_second: 3.837, interval_steps_per_second: 4.796, epoch: 3.0[0m
[32m[2022-08-30 12:00:19,739] [    INFO][0m - loss: 0.02405265, learning_rate: 2.895e-05, global_step: 70, interval_runtime: 2.1324, interval_samples_per_second: 3.752, interval_steps_per_second: 4.69, epoch: 3.5[0m
[32m[2022-08-30 12:00:21,832] [    INFO][0m - loss: 0.12980517, learning_rate: 2.88e-05, global_step: 80, interval_runtime: 2.0931, interval_samples_per_second: 3.822, interval_steps_per_second: 4.778, epoch: 4.0[0m
[32m[2022-08-30 12:00:23,967] [    INFO][0m - loss: 0.04781945, learning_rate: 2.865e-05, global_step: 90, interval_runtime: 2.1353, interval_samples_per_second: 3.747, interval_steps_per_second: 4.683, epoch: 4.5[0m
[32m[2022-08-30 12:00:26,061] [    INFO][0m - loss: 0.00022036, learning_rate: 2.8499999999999998e-05, global_step: 100, interval_runtime: 2.094, interval_samples_per_second: 3.82, interval_steps_per_second: 4.775, epoch: 5.0[0m
[32m[2022-08-30 12:00:26,062] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 12:00:26,062] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 12:00:26,062] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 12:00:26,063] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 12:00:26,063] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 12:00:27,346] [    INFO][0m - eval_loss: 1.3639466762542725, eval_accuracy: 0.83125, eval_runtime: 1.2831, eval_samples_per_second: 124.7, eval_steps_per_second: 3.897, epoch: 5.0[0m
[32m[2022-08-30 12:00:27,346] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-30 12:00:27,347] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 12:00:31,379] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-30 12:00:31,379] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-30 12:00:41,915] [    INFO][0m - loss: 0.00024716, learning_rate: 2.8349999999999998e-05, global_step: 110, interval_runtime: 15.8533, interval_samples_per_second: 0.505, interval_steps_per_second: 0.631, epoch: 5.5[0m
[32m[2022-08-30 12:00:43,999] [    INFO][0m - loss: 0.00011158, learning_rate: 2.8199999999999998e-05, global_step: 120, interval_runtime: 2.0844, interval_samples_per_second: 3.838, interval_steps_per_second: 4.797, epoch: 6.0[0m
[32m[2022-08-30 12:00:46,128] [    INFO][0m - loss: 2.434e-05, learning_rate: 2.805e-05, global_step: 130, interval_runtime: 2.1288, interval_samples_per_second: 3.758, interval_steps_per_second: 4.697, epoch: 6.5[0m
[32m[2022-08-30 12:00:48,224] [    INFO][0m - loss: 1.882e-05, learning_rate: 2.79e-05, global_step: 140, interval_runtime: 2.0963, interval_samples_per_second: 3.816, interval_steps_per_second: 4.77, epoch: 7.0[0m
[32m[2022-08-30 12:00:50,390] [    INFO][0m - loss: 1.643e-05, learning_rate: 2.7750000000000004e-05, global_step: 150, interval_runtime: 2.1658, interval_samples_per_second: 3.694, interval_steps_per_second: 4.617, epoch: 7.5[0m
[32m[2022-08-30 12:00:52,485] [    INFO][0m - loss: 1.434e-05, learning_rate: 2.7600000000000003e-05, global_step: 160, interval_runtime: 2.0947, interval_samples_per_second: 3.819, interval_steps_per_second: 4.774, epoch: 8.0[0m
[32m[2022-08-30 12:00:54,624] [    INFO][0m - loss: 1.406e-05, learning_rate: 2.7450000000000003e-05, global_step: 170, interval_runtime: 2.1389, interval_samples_per_second: 3.74, interval_steps_per_second: 4.675, epoch: 8.5[0m
[32m[2022-08-30 12:00:56,724] [    INFO][0m - loss: 1.119e-05, learning_rate: 2.7300000000000003e-05, global_step: 180, interval_runtime: 2.1006, interval_samples_per_second: 3.808, interval_steps_per_second: 4.761, epoch: 9.0[0m
[32m[2022-08-30 12:00:58,870] [    INFO][0m - loss: 1.223e-05, learning_rate: 2.7150000000000003e-05, global_step: 190, interval_runtime: 2.1455, interval_samples_per_second: 3.729, interval_steps_per_second: 4.661, epoch: 9.5[0m
[32m[2022-08-30 12:01:00,965] [    INFO][0m - loss: 1.216e-05, learning_rate: 2.7000000000000002e-05, global_step: 200, interval_runtime: 2.0958, interval_samples_per_second: 3.817, interval_steps_per_second: 4.771, epoch: 10.0[0m
[32m[2022-08-30 12:01:00,966] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 12:01:00,966] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 12:01:00,966] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 12:01:00,966] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 12:01:00,966] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 12:01:02,250] [    INFO][0m - eval_loss: 1.3554456233978271, eval_accuracy: 0.8375, eval_runtime: 1.2834, eval_samples_per_second: 124.669, eval_steps_per_second: 3.896, epoch: 10.0[0m
[32m[2022-08-30 12:01:02,250] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-30 12:01:02,250] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 12:01:05,298] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-30 12:01:05,298] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-30 12:01:13,414] [    INFO][0m - loss: 1.022e-05, learning_rate: 2.6850000000000002e-05, global_step: 210, interval_runtime: 12.4484, interval_samples_per_second: 0.643, interval_steps_per_second: 0.803, epoch: 10.5[0m
[32m[2022-08-30 12:01:15,509] [    INFO][0m - loss: 1.052e-05, learning_rate: 2.6700000000000002e-05, global_step: 220, interval_runtime: 2.0952, interval_samples_per_second: 3.818, interval_steps_per_second: 4.773, epoch: 11.0[0m
[32m[2022-08-30 12:01:17,648] [    INFO][0m - loss: 9.6e-06, learning_rate: 2.655e-05, global_step: 230, interval_runtime: 2.1388, interval_samples_per_second: 3.74, interval_steps_per_second: 4.675, epoch: 11.5[0m
[32m[2022-08-30 12:01:19,747] [    INFO][0m - loss: 8.61e-06, learning_rate: 2.64e-05, global_step: 240, interval_runtime: 2.0988, interval_samples_per_second: 3.812, interval_steps_per_second: 4.765, epoch: 12.0[0m
[32m[2022-08-30 12:01:21,885] [    INFO][0m - loss: 8.34e-06, learning_rate: 2.625e-05, global_step: 250, interval_runtime: 2.1386, interval_samples_per_second: 3.741, interval_steps_per_second: 4.676, epoch: 12.5[0m
[32m[2022-08-30 12:01:23,990] [    INFO][0m - loss: 8.51e-06, learning_rate: 2.61e-05, global_step: 260, interval_runtime: 2.1045, interval_samples_per_second: 3.801, interval_steps_per_second: 4.752, epoch: 13.0[0m
[32m[2022-08-30 12:01:26,132] [    INFO][0m - loss: 8.63e-06, learning_rate: 2.595e-05, global_step: 270, interval_runtime: 2.1424, interval_samples_per_second: 3.734, interval_steps_per_second: 4.668, epoch: 13.5[0m
[32m[2022-08-30 12:01:28,240] [    INFO][0m - loss: 6.57e-06, learning_rate: 2.58e-05, global_step: 280, interval_runtime: 2.1078, interval_samples_per_second: 3.795, interval_steps_per_second: 4.744, epoch: 14.0[0m
[32m[2022-08-30 12:01:30,384] [    INFO][0m - loss: 6.39e-06, learning_rate: 2.565e-05, global_step: 290, interval_runtime: 2.1436, interval_samples_per_second: 3.732, interval_steps_per_second: 4.665, epoch: 14.5[0m
[32m[2022-08-30 12:01:32,490] [    INFO][0m - loss: 7.02e-06, learning_rate: 2.55e-05, global_step: 300, interval_runtime: 2.1067, interval_samples_per_second: 3.797, interval_steps_per_second: 4.747, epoch: 15.0[0m
[32m[2022-08-30 12:01:32,491] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 12:01:32,491] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 12:01:32,491] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 12:01:32,491] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 12:01:32,491] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 12:01:33,775] [    INFO][0m - eval_loss: 1.382681131362915, eval_accuracy: 0.8375, eval_runtime: 1.2836, eval_samples_per_second: 124.653, eval_steps_per_second: 3.895, epoch: 15.0[0m
[32m[2022-08-30 12:01:33,775] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-30 12:01:33,775] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 12:01:37,009] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-30 12:01:37,009] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-30 12:01:45,577] [    INFO][0m - loss: 7.02e-06, learning_rate: 2.535e-05, global_step: 310, interval_runtime: 13.0867, interval_samples_per_second: 0.611, interval_steps_per_second: 0.764, epoch: 15.5[0m
[32m[2022-08-30 12:01:47,678] [    INFO][0m - loss: 6.45e-06, learning_rate: 2.52e-05, global_step: 320, interval_runtime: 2.1014, interval_samples_per_second: 3.807, interval_steps_per_second: 4.759, epoch: 16.0[0m
[32m[2022-08-30 12:01:49,820] [    INFO][0m - loss: 6.49e-06, learning_rate: 2.505e-05, global_step: 330, interval_runtime: 2.1413, interval_samples_per_second: 3.736, interval_steps_per_second: 4.67, epoch: 16.5[0m
[32m[2022-08-30 12:01:51,926] [    INFO][0m - loss: 5.94e-06, learning_rate: 2.49e-05, global_step: 340, interval_runtime: 2.1067, interval_samples_per_second: 3.797, interval_steps_per_second: 4.747, epoch: 17.0[0m
[32m[2022-08-30 12:01:54,079] [    INFO][0m - loss: 6.47e-06, learning_rate: 2.475e-05, global_step: 350, interval_runtime: 2.1526, interval_samples_per_second: 3.716, interval_steps_per_second: 4.646, epoch: 17.5[0m
[32m[2022-08-30 12:01:56,189] [    INFO][0m - loss: 4.68e-06, learning_rate: 2.4599999999999998e-05, global_step: 360, interval_runtime: 2.1102, interval_samples_per_second: 3.791, interval_steps_per_second: 4.739, epoch: 18.0[0m
[32m[2022-08-30 12:01:58,347] [    INFO][0m - loss: 5.38e-06, learning_rate: 2.4449999999999998e-05, global_step: 370, interval_runtime: 2.1582, interval_samples_per_second: 3.707, interval_steps_per_second: 4.634, epoch: 18.5[0m
[32m[2022-08-30 12:02:00,458] [    INFO][0m - loss: 5.29e-06, learning_rate: 2.43e-05, global_step: 380, interval_runtime: 2.1103, interval_samples_per_second: 3.791, interval_steps_per_second: 4.739, epoch: 19.0[0m
[32m[2022-08-30 12:02:02,611] [    INFO][0m - loss: 4.69e-06, learning_rate: 2.415e-05, global_step: 390, interval_runtime: 2.1535, interval_samples_per_second: 3.715, interval_steps_per_second: 4.644, epoch: 19.5[0m
[32m[2022-08-30 12:02:04,722] [    INFO][0m - loss: 5.24e-06, learning_rate: 2.4e-05, global_step: 400, interval_runtime: 2.1108, interval_samples_per_second: 3.79, interval_steps_per_second: 4.737, epoch: 20.0[0m
[32m[2022-08-30 12:02:04,722] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 12:02:04,722] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 12:02:04,723] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 12:02:04,723] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 12:02:04,723] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 12:02:06,013] [    INFO][0m - eval_loss: 1.4048324823379517, eval_accuracy: 0.8375, eval_runtime: 1.2899, eval_samples_per_second: 124.039, eval_steps_per_second: 3.876, epoch: 20.0[0m
[32m[2022-08-30 12:02:06,013] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-30 12:02:06,013] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 12:02:08,946] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-30 12:02:08,946] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-30 12:02:17,207] [    INFO][0m - loss: 4.79e-06, learning_rate: 2.385e-05, global_step: 410, interval_runtime: 12.4851, interval_samples_per_second: 0.641, interval_steps_per_second: 0.801, epoch: 20.5[0m
[32m[2022-08-30 12:02:19,309] [    INFO][0m - loss: 4.52e-06, learning_rate: 2.37e-05, global_step: 420, interval_runtime: 2.1016, interval_samples_per_second: 3.807, interval_steps_per_second: 4.758, epoch: 21.0[0m
[32m[2022-08-30 12:02:21,455] [    INFO][0m - loss: 4.39e-06, learning_rate: 2.3550000000000003e-05, global_step: 430, interval_runtime: 2.1461, interval_samples_per_second: 3.728, interval_steps_per_second: 4.66, epoch: 21.5[0m
[32m[2022-08-30 12:02:23,563] [    INFO][0m - loss: 4.11e-06, learning_rate: 2.3400000000000003e-05, global_step: 440, interval_runtime: 2.1077, interval_samples_per_second: 3.796, interval_steps_per_second: 4.744, epoch: 22.0[0m
[32m[2022-08-30 12:02:25,713] [    INFO][0m - loss: 3.93e-06, learning_rate: 2.3250000000000003e-05, global_step: 450, interval_runtime: 2.1509, interval_samples_per_second: 3.719, interval_steps_per_second: 4.649, epoch: 22.5[0m
[32m[2022-08-30 12:02:27,823] [    INFO][0m - loss: 4.47e-06, learning_rate: 2.3100000000000002e-05, global_step: 460, interval_runtime: 2.1094, interval_samples_per_second: 3.792, interval_steps_per_second: 4.741, epoch: 23.0[0m
[32m[2022-08-30 12:02:29,972] [    INFO][0m - loss: 4.21e-06, learning_rate: 2.2950000000000002e-05, global_step: 470, interval_runtime: 2.1496, interval_samples_per_second: 3.722, interval_steps_per_second: 4.652, epoch: 23.5[0m
[32m[2022-08-30 12:02:32,086] [    INFO][0m - loss: 3.68e-06, learning_rate: 2.2800000000000002e-05, global_step: 480, interval_runtime: 2.113, interval_samples_per_second: 3.786, interval_steps_per_second: 4.733, epoch: 24.0[0m
[32m[2022-08-30 12:02:34,238] [    INFO][0m - loss: 3.6e-06, learning_rate: 2.265e-05, global_step: 490, interval_runtime: 2.1527, interval_samples_per_second: 3.716, interval_steps_per_second: 4.645, epoch: 24.5[0m
[32m[2022-08-30 12:02:36,351] [    INFO][0m - loss: 4.01e-06, learning_rate: 2.25e-05, global_step: 500, interval_runtime: 2.113, interval_samples_per_second: 3.786, interval_steps_per_second: 4.733, epoch: 25.0[0m
[32m[2022-08-30 12:02:36,352] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 12:02:36,352] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 12:02:36,352] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 12:02:36,352] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 12:02:36,352] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 12:02:37,640] [    INFO][0m - eval_loss: 1.4218956232070923, eval_accuracy: 0.8375, eval_runtime: 1.2883, eval_samples_per_second: 124.191, eval_steps_per_second: 3.881, epoch: 25.0[0m
[32m[2022-08-30 12:02:37,641] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-08-30 12:02:37,641] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 12:02:40,546] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-08-30 12:02:40,547] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-08-30 12:02:48,633] [    INFO][0m - loss: 3.83e-06, learning_rate: 2.235e-05, global_step: 510, interval_runtime: 12.282, interval_samples_per_second: 0.651, interval_steps_per_second: 0.814, epoch: 25.5[0m
[32m[2022-08-30 12:02:50,740] [    INFO][0m - loss: 3.54e-06, learning_rate: 2.22e-05, global_step: 520, interval_runtime: 2.1068, interval_samples_per_second: 3.797, interval_steps_per_second: 4.747, epoch: 26.0[0m
[32m[2022-08-30 12:02:52,891] [    INFO][0m - loss: 3.27e-06, learning_rate: 2.205e-05, global_step: 530, interval_runtime: 2.1515, interval_samples_per_second: 3.718, interval_steps_per_second: 4.648, epoch: 26.5[0m
[32m[2022-08-30 12:02:55,003] [    INFO][0m - loss: 3.28e-06, learning_rate: 2.19e-05, global_step: 540, interval_runtime: 2.1113, interval_samples_per_second: 3.789, interval_steps_per_second: 4.736, epoch: 27.0[0m
[32m[2022-08-30 12:02:57,151] [    INFO][0m - loss: 3.35e-06, learning_rate: 2.175e-05, global_step: 550, interval_runtime: 2.1479, interval_samples_per_second: 3.725, interval_steps_per_second: 4.656, epoch: 27.5[0m
[32m[2022-08-30 12:02:59,270] [    INFO][0m - loss: 2.8e-06, learning_rate: 2.16e-05, global_step: 560, interval_runtime: 2.119, interval_samples_per_second: 3.775, interval_steps_per_second: 4.719, epoch: 28.0[0m
[32m[2022-08-30 12:03:01,431] [    INFO][0m - loss: 3.35e-06, learning_rate: 2.145e-05, global_step: 570, interval_runtime: 2.1615, interval_samples_per_second: 3.701, interval_steps_per_second: 4.626, epoch: 28.5[0m
[32m[2022-08-30 12:03:03,547] [    INFO][0m - loss: 2.8e-06, learning_rate: 2.13e-05, global_step: 580, interval_runtime: 2.1155, interval_samples_per_second: 3.782, interval_steps_per_second: 4.727, epoch: 29.0[0m
[32m[2022-08-30 12:03:05,701] [    INFO][0m - loss: 3.07e-06, learning_rate: 2.115e-05, global_step: 590, interval_runtime: 2.1547, interval_samples_per_second: 3.713, interval_steps_per_second: 4.641, epoch: 29.5[0m
[32m[2022-08-30 12:03:07,820] [    INFO][0m - loss: 3.03e-06, learning_rate: 2.1e-05, global_step: 600, interval_runtime: 2.1186, interval_samples_per_second: 3.776, interval_steps_per_second: 4.72, epoch: 30.0[0m
[32m[2022-08-30 12:03:07,820] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 12:03:07,820] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 12:03:07,820] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 12:03:07,821] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 12:03:07,821] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 12:03:09,113] [    INFO][0m - eval_loss: 1.4368865489959717, eval_accuracy: 0.8375, eval_runtime: 1.2922, eval_samples_per_second: 123.822, eval_steps_per_second: 3.869, epoch: 30.0[0m
[32m[2022-08-30 12:03:09,113] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-08-30 12:03:09,114] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 12:03:12,037] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-08-30 12:03:12,038] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-08-30 12:03:17,877] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-30 12:03:17,877] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-200 (score: 0.8375).[0m
[32m[2022-08-30 12:03:19,473] [    INFO][0m - train_runtime: 195.8581, train_samples_per_second: 81.692, train_steps_per_second: 10.211, train_loss: 0.03666326802163894, epoch: 30.0[0m
[32m[2022-08-30 12:03:19,475] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-30 12:03:19,475] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 12:03:22,181] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-30 12:03:22,182] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-30 12:03:22,183] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-30 12:03:22,183] [    INFO][0m -   epoch                    =       30.0[0m
[32m[2022-08-30 12:03:22,183] [    INFO][0m -   train_loss               =     0.0367[0m
[32m[2022-08-30 12:03:22,183] [    INFO][0m -   train_runtime            = 0:03:15.85[0m
[32m[2022-08-30 12:03:22,183] [    INFO][0m -   train_samples_per_second =     81.692[0m
[32m[2022-08-30 12:03:22,184] [    INFO][0m -   train_steps_per_second   =     10.211[0m
[32m[2022-08-30 12:03:22,187] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-30 12:03:22,187] [    INFO][0m -   Num examples = 610[0m
[32m[2022-08-30 12:03:22,187] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 12:03:22,187] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 12:03:22,187] [    INFO][0m -   Total prediction steps = 20[0m
[32m[2022-08-30 12:03:27,089] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-30 12:03:27,090] [    INFO][0m -   test_accuracy           =     0.8574[0m
[32m[2022-08-30 12:03:27,090] [    INFO][0m -   test_loss               =     1.2477[0m
[32m[2022-08-30 12:03:27,090] [    INFO][0m -   test_runtime            = 0:00:04.90[0m
[32m[2022-08-30 12:03:27,090] [    INFO][0m -   test_samples_per_second =    124.418[0m
[32m[2022-08-30 12:03:27,090] [    INFO][0m -   test_steps_per_second   =      4.079[0m
[32m[2022-08-30 12:03:27,090] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-30 12:03:27,091] [    INFO][0m -   Num examples = 753[0m
[32m[2022-08-30 12:03:27,091] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 12:03:27,091] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 12:03:27,091] [    INFO][0m -   Total prediction steps = 24[0m
[32m[2022-08-30 12:03:33,716] [    INFO][0m - Predictions for eprstmt saved to ./fewclue_submit_examples.[0m
metric <function main.<locals>.compute_metrics at 0x7f9fc4d4d4c0>
 
==========
csldcp
==========
 
[33m[2022-08-30 12:03:37,835] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-30 12:03:37,835] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-30 12:03:37,836] [    INFO][0m - ============================================================[0m
[32m[2022-08-30 12:03:37,836] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-30 12:03:37,836] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-30 12:03:37,836] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-30 12:03:37,836] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-30 12:03:37,836] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-08-30 12:03:37,836] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-30 12:03:37,836] [    INFO][0m - model_name_or_path            :hfl/roberta-wwm-ext-large[0m
[32m[2022-08-30 12:03:37,836] [    INFO][0m - [0m
[32m[2022-08-30 12:03:37,836] [    INFO][0m - ============================================================[0m
[32m[2022-08-30 12:03:37,837] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-30 12:03:37,837] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-30 12:03:37,837] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-30 12:03:37,837] [    INFO][0m - prompt                        :{'hard':'阅读下边'}{'mask'}{'mask'}{'hard':'相关的材料'}{'text':'text_a'}[0m
[32m[2022-08-30 12:03:37,837] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-30 12:03:37,837] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-30 12:03:37,837] [    INFO][0m - task_name                     :csldcp[0m
[32m[2022-08-30 12:03:37,837] [    INFO][0m - [0m
[32m[2022-08-30 12:03:37,837] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/hfl/roberta-wwm-ext-large/roberta_chn_large.pdparams[0m
W0830 12:03:37.838797 54387 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0830 12:03:37.842984 54387 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-30 12:03:43,179] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/hfl/roberta-wwm-ext-large/vocab.txt[0m
[32m[2022-08-30 12:03:43,193] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/hfl/roberta-wwm-ext-large/tokenizer_config.json[0m
[32m[2022-08-30 12:03:43,193] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/hfl/roberta-wwm-ext-large/special_tokens_map.json[0m
[32m[2022-08-30 12:03:43,194] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '阅读下边'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': '相关的材料'}, {'add_prefix_space': '', 'text': 'text_a'}][0m
2022-08-30 12:03:43,211 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-30 12:03:43,370] [    INFO][0m - ============================================================[0m
[32m[2022-08-30 12:03:43,370] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-30 12:03:43,371] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-30 12:03:43,371] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-30 12:03:43,371] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-30 12:03:43,371] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-30 12:03:43,371] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-30 12:03:43,371] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-30 12:03:43,371] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-30 12:03:43,371] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-30 12:03:43,371] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-30 12:03:43,371] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-30 12:03:43,371] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-30 12:03:43,372] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-30 12:03:43,372] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-30 12:03:43,372] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-30 12:03:43,372] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-30 12:03:43,372] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-30 12:03:43,372] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-30 12:03:43,372] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-30 12:03:43,372] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-30 12:03:43,372] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-30 12:03:43,372] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-30 12:03:43,372] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-30 12:03:43,372] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-30 12:03:43,373] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-30 12:03:43,373] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-30 12:03:43,373] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-30 12:03:43,373] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-30 12:03:43,373] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-30 12:03:43,373] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-30 12:03:43,373] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-30 12:03:43,373] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-30 12:03:43,373] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-30 12:03:43,373] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-30 12:03:43,373] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-30 12:03:43,373] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-30 12:03:43,373] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug30_12-03-37_instance-3bwob41y-01[0m
[32m[2022-08-30 12:03:43,374] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-30 12:03:43,374] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-30 12:03:43,374] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-30 12:03:43,374] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-30 12:03:43,374] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-30 12:03:43,374] [    INFO][0m - max_seq_length                :256[0m
[32m[2022-08-30 12:03:43,374] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-30 12:03:43,374] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-30 12:03:43,374] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-30 12:03:43,374] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-30 12:03:43,374] [    INFO][0m - num_train_epochs              :100.0[0m
[32m[2022-08-30 12:03:43,374] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-30 12:03:43,374] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-30 12:03:43,375] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-30 12:03:43,375] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-30 12:03:43,375] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-30 12:03:43,375] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-30 12:03:43,375] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-30 12:03:43,375] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-30 12:03:43,375] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-30 12:03:43,375] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-30 12:03:43,375] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-30 12:03:43,375] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-30 12:03:43,375] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-30 12:03:43,375] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-30 12:03:43,375] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-30 12:03:43,376] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-30 12:03:43,376] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-30 12:03:43,376] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-30 12:03:43,376] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-30 12:03:43,376] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-30 12:03:43,376] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-30 12:03:43,376] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-30 12:03:43,376] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-30 12:03:43,376] [    INFO][0m - seed                          :42[0m
[32m[2022-08-30 12:03:43,376] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-30 12:03:43,376] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-30 12:03:43,376] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-30 12:03:43,376] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-30 12:03:43,377] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-30 12:03:43,377] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-30 12:03:43,377] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-30 12:03:43,377] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-30 12:03:43,377] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-30 12:03:43,377] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-30 12:03:43,377] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-30 12:03:43,377] [    INFO][0m - [0m
[32m[2022-08-30 12:03:43,380] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-30 12:03:43,380] [    INFO][0m -   Num examples = 2036[0m
[32m[2022-08-30 12:03:43,380] [    INFO][0m -   Num Epochs = 100[0m
[32m[2022-08-30 12:03:43,380] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-30 12:03:43,380] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-30 12:03:43,381] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-30 12:03:43,381] [    INFO][0m -   Total optimization steps = 25500.0[0m
[32m[2022-08-30 12:03:43,381] [    INFO][0m -   Total num train samples = 203600[0m
[33m[2022-08-30 12:03:43,417] [ WARNING][0m - Token indices sequence length is longer than the specified maximum sequence length for this model (518 > 512). Running this sequence through the model will result in indexing errors[0m
[32m[2022-08-30 12:03:48,420] [    INFO][0m - loss: 4.28498573, learning_rate: 2.998823529411765e-05, global_step: 10, interval_runtime: 5.0382, interval_samples_per_second: 1.588, interval_steps_per_second: 1.985, epoch: 0.0392[0m
[32m[2022-08-30 12:03:52,415] [    INFO][0m - loss: 4.40488815, learning_rate: 2.9976470588235296e-05, global_step: 20, interval_runtime: 3.9952, interval_samples_per_second: 2.002, interval_steps_per_second: 2.503, epoch: 0.0784[0m
[32m[2022-08-30 12:03:56,546] [    INFO][0m - loss: 4.43412704, learning_rate: 2.9964705882352942e-05, global_step: 30, interval_runtime: 3.9993, interval_samples_per_second: 2.0, interval_steps_per_second: 2.5, epoch: 0.1176[0m
[32m[2022-08-30 12:04:00,529] [    INFO][0m - loss: 4.48620148, learning_rate: 2.9952941176470588e-05, global_step: 40, interval_runtime: 4.1144, interval_samples_per_second: 1.944, interval_steps_per_second: 2.431, epoch: 0.1569[0m
[32m[2022-08-30 12:04:04,515] [    INFO][0m - loss: 4.42020874, learning_rate: 2.9941176470588237e-05, global_step: 50, interval_runtime: 3.9861, interval_samples_per_second: 2.007, interval_steps_per_second: 2.509, epoch: 0.1961[0m
[32m[2022-08-30 12:04:08,523] [    INFO][0m - loss: 4.45018692, learning_rate: 2.9929411764705883e-05, global_step: 60, interval_runtime: 4.0083, interval_samples_per_second: 1.996, interval_steps_per_second: 2.495, epoch: 0.2353[0m
[32m[2022-08-30 12:04:12,528] [    INFO][0m - loss: 4.30359726, learning_rate: 2.9917647058823533e-05, global_step: 70, interval_runtime: 4.0048, interval_samples_per_second: 1.998, interval_steps_per_second: 2.497, epoch: 0.2745[0m
[32m[2022-08-30 12:04:16,535] [    INFO][0m - loss: 4.2987648, learning_rate: 2.9905882352941175e-05, global_step: 80, interval_runtime: 4.007, interval_samples_per_second: 1.996, interval_steps_per_second: 2.496, epoch: 0.3137[0m
[32m[2022-08-30 12:04:20,554] [    INFO][0m - loss: 4.02886658, learning_rate: 2.9894117647058825e-05, global_step: 90, interval_runtime: 4.019, interval_samples_per_second: 1.991, interval_steps_per_second: 2.488, epoch: 0.3529[0m
[32m[2022-08-30 12:04:24,556] [    INFO][0m - loss: 3.92614822, learning_rate: 2.988235294117647e-05, global_step: 100, interval_runtime: 4.0022, interval_samples_per_second: 1.999, interval_steps_per_second: 2.499, epoch: 0.3922[0m
[32m[2022-08-30 12:04:24,557] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 12:04:24,557] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-30 12:04:24,557] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 12:04:24,557] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 12:04:24,557] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-30 12:04:59,935] [    INFO][0m - eval_loss: 3.54070782661438, eval_accuracy: 0.1581237911025145, eval_runtime: 35.377, eval_samples_per_second: 58.456, eval_steps_per_second: 1.837, epoch: 0.3922[0m
[32m[2022-08-30 12:04:59,935] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-30 12:04:59,935] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 12:05:07,342] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-30 12:05:07,342] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-30 12:05:25,784] [    INFO][0m - loss: 3.90293846, learning_rate: 2.987058823529412e-05, global_step: 110, interval_runtime: 61.2276, interval_samples_per_second: 0.131, interval_steps_per_second: 0.163, epoch: 0.4314[0m
[32m[2022-08-30 12:05:29,765] [    INFO][0m - loss: 3.09728928, learning_rate: 2.9858823529411763e-05, global_step: 120, interval_runtime: 3.9815, interval_samples_per_second: 2.009, interval_steps_per_second: 2.512, epoch: 0.4706[0m
[32m[2022-08-30 12:05:33,758] [    INFO][0m - loss: 2.88105812, learning_rate: 2.9847058823529412e-05, global_step: 130, interval_runtime: 3.9931, interval_samples_per_second: 2.003, interval_steps_per_second: 2.504, epoch: 0.5098[0m
[32m[2022-08-30 12:05:37,768] [    INFO][0m - loss: 3.00031128, learning_rate: 2.9835294117647058e-05, global_step: 140, interval_runtime: 4.0097, interval_samples_per_second: 1.995, interval_steps_per_second: 2.494, epoch: 0.549[0m
[32m[2022-08-30 12:05:41,792] [    INFO][0m - loss: 2.59806843, learning_rate: 2.9823529411764707e-05, global_step: 150, interval_runtime: 4.0239, interval_samples_per_second: 1.988, interval_steps_per_second: 2.485, epoch: 0.5882[0m
[32m[2022-08-30 12:05:45,789] [    INFO][0m - loss: 2.48246479, learning_rate: 2.9811764705882357e-05, global_step: 160, interval_runtime: 3.997, interval_samples_per_second: 2.001, interval_steps_per_second: 2.502, epoch: 0.6275[0m
[32m[2022-08-30 12:05:49,793] [    INFO][0m - loss: 2.60111237, learning_rate: 2.98e-05, global_step: 170, interval_runtime: 4.0038, interval_samples_per_second: 1.998, interval_steps_per_second: 2.498, epoch: 0.6667[0m
[32m[2022-08-30 12:05:53,782] [    INFO][0m - loss: 2.21950912, learning_rate: 2.978823529411765e-05, global_step: 180, interval_runtime: 3.9899, interval_samples_per_second: 2.005, interval_steps_per_second: 2.506, epoch: 0.7059[0m
[32m[2022-08-30 12:05:57,789] [    INFO][0m - loss: 2.39449043, learning_rate: 2.9776470588235295e-05, global_step: 190, interval_runtime: 4.0066, interval_samples_per_second: 1.997, interval_steps_per_second: 2.496, epoch: 0.7451[0m
[32m[2022-08-30 12:06:01,791] [    INFO][0m - loss: 2.45082989, learning_rate: 2.9764705882352944e-05, global_step: 200, interval_runtime: 4.0019, interval_samples_per_second: 1.999, interval_steps_per_second: 2.499, epoch: 0.7843[0m
[32m[2022-08-30 12:06:01,791] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 12:06:01,792] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-30 12:06:01,792] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 12:06:01,792] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 12:06:01,792] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-30 12:06:37,249] [    INFO][0m - eval_loss: 2.0636818408966064, eval_accuracy: 0.4472920696324952, eval_runtime: 35.4568, eval_samples_per_second: 58.325, eval_steps_per_second: 1.833, epoch: 0.7843[0m
[32m[2022-08-30 12:06:37,250] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-30 12:06:37,250] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 12:06:44,819] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-30 12:06:44,820] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-30 12:07:03,188] [    INFO][0m - loss: 2.22759056, learning_rate: 2.975294117647059e-05, global_step: 210, interval_runtime: 61.3974, interval_samples_per_second: 0.13, interval_steps_per_second: 0.163, epoch: 0.8235[0m
[32m[2022-08-30 12:07:07,189] [    INFO][0m - loss: 1.93703823, learning_rate: 2.9741176470588236e-05, global_step: 220, interval_runtime: 4.0004, interval_samples_per_second: 2.0, interval_steps_per_second: 2.5, epoch: 0.8627[0m
[32m[2022-08-30 12:07:11,179] [    INFO][0m - loss: 1.87283859, learning_rate: 2.9729411764705882e-05, global_step: 230, interval_runtime: 3.9904, interval_samples_per_second: 2.005, interval_steps_per_second: 2.506, epoch: 0.902[0m
[32m[2022-08-30 12:07:15,193] [    INFO][0m - loss: 2.35588913, learning_rate: 2.971764705882353e-05, global_step: 240, interval_runtime: 4.014, interval_samples_per_second: 1.993, interval_steps_per_second: 2.491, epoch: 0.9412[0m
[32m[2022-08-30 12:07:19,209] [    INFO][0m - loss: 1.95240803, learning_rate: 2.9705882352941177e-05, global_step: 250, interval_runtime: 4.0153, interval_samples_per_second: 1.992, interval_steps_per_second: 2.49, epoch: 0.9804[0m
[32m[2022-08-30 12:07:23,104] [    INFO][0m - loss: 1.80469151, learning_rate: 2.9694117647058823e-05, global_step: 260, interval_runtime: 3.8952, interval_samples_per_second: 2.054, interval_steps_per_second: 2.567, epoch: 1.0196[0m
[32m[2022-08-30 12:07:27,124] [    INFO][0m - loss: 1.47429762, learning_rate: 2.968235294117647e-05, global_step: 270, interval_runtime: 4.0204, interval_samples_per_second: 1.99, interval_steps_per_second: 2.487, epoch: 1.0588[0m
[32m[2022-08-30 12:07:31,126] [    INFO][0m - loss: 1.35089884, learning_rate: 2.967058823529412e-05, global_step: 280, interval_runtime: 4.0014, interval_samples_per_second: 1.999, interval_steps_per_second: 2.499, epoch: 1.098[0m
[32m[2022-08-30 12:07:35,139] [    INFO][0m - loss: 1.57637615, learning_rate: 2.9658823529411765e-05, global_step: 290, interval_runtime: 4.0135, interval_samples_per_second: 1.993, interval_steps_per_second: 2.492, epoch: 1.1373[0m
[32m[2022-08-30 12:07:39,164] [    INFO][0m - loss: 1.58837786, learning_rate: 2.9647058823529414e-05, global_step: 300, interval_runtime: 4.0246, interval_samples_per_second: 1.988, interval_steps_per_second: 2.485, epoch: 1.1765[0m
[32m[2022-08-30 12:07:39,164] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 12:07:39,164] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-30 12:07:39,164] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 12:07:39,165] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 12:07:39,165] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-30 12:08:14,588] [    INFO][0m - eval_loss: 1.8462735414505005, eval_accuracy: 0.47678916827852996, eval_runtime: 35.4234, eval_samples_per_second: 58.38, eval_steps_per_second: 1.835, epoch: 1.1765[0m
[32m[2022-08-30 12:08:14,589] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-30 12:08:14,589] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 12:08:21,575] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-30 12:08:21,575] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-30 12:08:40,349] [    INFO][0m - loss: 1.42190475, learning_rate: 2.963529411764706e-05, global_step: 310, interval_runtime: 61.1851, interval_samples_per_second: 0.131, interval_steps_per_second: 0.163, epoch: 1.2157[0m
[32m[2022-08-30 12:08:44,355] [    INFO][0m - loss: 1.27839012, learning_rate: 2.9623529411764706e-05, global_step: 320, interval_runtime: 4.0059, interval_samples_per_second: 1.997, interval_steps_per_second: 2.496, epoch: 1.2549[0m
[32m[2022-08-30 12:08:48,363] [    INFO][0m - loss: 1.28520975, learning_rate: 2.9611764705882355e-05, global_step: 330, interval_runtime: 4.0075, interval_samples_per_second: 1.996, interval_steps_per_second: 2.495, epoch: 1.2941[0m
[32m[2022-08-30 12:08:52,384] [    INFO][0m - loss: 1.76590195, learning_rate: 2.96e-05, global_step: 340, interval_runtime: 4.0214, interval_samples_per_second: 1.989, interval_steps_per_second: 2.487, epoch: 1.3333[0m
[32m[2022-08-30 12:08:56,387] [    INFO][0m - loss: 1.9234993, learning_rate: 2.958823529411765e-05, global_step: 350, interval_runtime: 4.003, interval_samples_per_second: 1.999, interval_steps_per_second: 2.498, epoch: 1.3725[0m
[32m[2022-08-30 12:09:00,411] [    INFO][0m - loss: 1.40382061, learning_rate: 2.9576470588235293e-05, global_step: 360, interval_runtime: 4.0243, interval_samples_per_second: 1.988, interval_steps_per_second: 2.485, epoch: 1.4118[0m
[32m[2022-08-30 12:09:04,428] [    INFO][0m - loss: 1.48448505, learning_rate: 2.9564705882352943e-05, global_step: 370, interval_runtime: 4.0171, interval_samples_per_second: 1.991, interval_steps_per_second: 2.489, epoch: 1.451[0m
[32m[2022-08-30 12:09:08,449] [    INFO][0m - loss: 1.52647524, learning_rate: 2.955294117647059e-05, global_step: 380, interval_runtime: 4.0205, interval_samples_per_second: 1.99, interval_steps_per_second: 2.487, epoch: 1.4902[0m
[32m[2022-08-30 12:09:12,477] [    INFO][0m - loss: 1.40439701, learning_rate: 2.9541176470588238e-05, global_step: 390, interval_runtime: 4.0283, interval_samples_per_second: 1.986, interval_steps_per_second: 2.482, epoch: 1.5294[0m
[32m[2022-08-30 12:09:16,502] [    INFO][0m - loss: 1.44925375, learning_rate: 2.952941176470588e-05, global_step: 400, interval_runtime: 4.0252, interval_samples_per_second: 1.988, interval_steps_per_second: 2.484, epoch: 1.5686[0m
[32m[2022-08-30 12:09:16,503] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 12:09:16,503] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-30 12:09:16,503] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 12:09:16,503] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 12:09:16,503] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-30 12:09:51,893] [    INFO][0m - eval_loss: 1.699582576751709, eval_accuracy: 0.5294970986460348, eval_runtime: 35.3891, eval_samples_per_second: 58.436, eval_steps_per_second: 1.837, epoch: 1.5686[0m
[32m[2022-08-30 12:09:51,894] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-30 12:09:51,894] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 12:09:59,216] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-30 12:09:59,217] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-30 12:10:17,704] [    INFO][0m - loss: 1.14364929, learning_rate: 2.951764705882353e-05, global_step: 410, interval_runtime: 61.2017, interval_samples_per_second: 0.131, interval_steps_per_second: 0.163, epoch: 1.6078[0m
[32m[2022-08-30 12:10:21,681] [    INFO][0m - loss: 1.46622076, learning_rate: 2.9505882352941176e-05, global_step: 420, interval_runtime: 3.9775, interval_samples_per_second: 2.011, interval_steps_per_second: 2.514, epoch: 1.6471[0m
[32m[2022-08-30 12:10:25,685] [    INFO][0m - loss: 1.36169052, learning_rate: 2.9494117647058825e-05, global_step: 430, interval_runtime: 4.0038, interval_samples_per_second: 1.998, interval_steps_per_second: 2.498, epoch: 1.6863[0m
[32m[2022-08-30 12:10:29,699] [    INFO][0m - loss: 1.4747323, learning_rate: 2.948235294117647e-05, global_step: 440, interval_runtime: 4.0138, interval_samples_per_second: 1.993, interval_steps_per_second: 2.491, epoch: 1.7255[0m
[32m[2022-08-30 12:10:33,691] [    INFO][0m - loss: 1.51936359, learning_rate: 2.9470588235294117e-05, global_step: 450, interval_runtime: 3.9918, interval_samples_per_second: 2.004, interval_steps_per_second: 2.505, epoch: 1.7647[0m
[32m[2022-08-30 12:10:37,681] [    INFO][0m - loss: 1.32762337, learning_rate: 2.9458823529411763e-05, global_step: 460, interval_runtime: 3.9904, interval_samples_per_second: 2.005, interval_steps_per_second: 2.506, epoch: 1.8039[0m
[32m[2022-08-30 12:10:41,707] [    INFO][0m - loss: 1.15885668, learning_rate: 2.9447058823529412e-05, global_step: 470, interval_runtime: 4.0255, interval_samples_per_second: 1.987, interval_steps_per_second: 2.484, epoch: 1.8431[0m
[32m[2022-08-30 12:10:45,734] [    INFO][0m - loss: 1.36145515, learning_rate: 2.9435294117647062e-05, global_step: 480, interval_runtime: 4.0274, interval_samples_per_second: 1.986, interval_steps_per_second: 2.483, epoch: 1.8824[0m
[32m[2022-08-30 12:10:49,774] [    INFO][0m - loss: 1.27011271, learning_rate: 2.9423529411764708e-05, global_step: 490, interval_runtime: 4.0402, interval_samples_per_second: 1.98, interval_steps_per_second: 2.475, epoch: 1.9216[0m
[32m[2022-08-30 12:10:53,809] [    INFO][0m - loss: 1.59035635, learning_rate: 2.9411764705882354e-05, global_step: 500, interval_runtime: 4.0349, interval_samples_per_second: 1.983, interval_steps_per_second: 2.478, epoch: 1.9608[0m
[32m[2022-08-30 12:10:53,810] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 12:10:53,810] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-30 12:10:53,810] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 12:10:53,810] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 12:10:53,810] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-30 12:11:29,180] [    INFO][0m - eval_loss: 1.7087079286575317, eval_accuracy: 0.5357833655705996, eval_runtime: 35.3698, eval_samples_per_second: 58.468, eval_steps_per_second: 1.838, epoch: 1.9608[0m
[32m[2022-08-30 12:11:29,181] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-08-30 12:11:29,181] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 12:11:36,494] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-08-30 12:11:36,494] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-08-30 12:11:54,622] [    INFO][0m - loss: 1.39473858, learning_rate: 2.94e-05, global_step: 510, interval_runtime: 60.8124, interval_samples_per_second: 0.132, interval_steps_per_second: 0.164, epoch: 2.0[0m
[32m[2022-08-30 12:11:58,769] [    INFO][0m - loss: 0.71835127, learning_rate: 2.938823529411765e-05, global_step: 520, interval_runtime: 4.1471, interval_samples_per_second: 1.929, interval_steps_per_second: 2.411, epoch: 2.0392[0m
[32m[2022-08-30 12:12:02,775] [    INFO][0m - loss: 0.7898634, learning_rate: 2.9376470588235295e-05, global_step: 530, interval_runtime: 4.0065, interval_samples_per_second: 1.997, interval_steps_per_second: 2.496, epoch: 2.0784[0m
[32m[2022-08-30 12:12:06,782] [    INFO][0m - loss: 0.66890297, learning_rate: 2.9364705882352944e-05, global_step: 540, interval_runtime: 4.0071, interval_samples_per_second: 1.996, interval_steps_per_second: 2.496, epoch: 2.1176[0m
[32m[2022-08-30 12:12:10,789] [    INFO][0m - loss: 0.79348826, learning_rate: 2.9352941176470587e-05, global_step: 550, interval_runtime: 4.0069, interval_samples_per_second: 1.997, interval_steps_per_second: 2.496, epoch: 2.1569[0m
[32m[2022-08-30 12:12:14,804] [    INFO][0m - loss: 0.72770905, learning_rate: 2.9341176470588236e-05, global_step: 560, interval_runtime: 4.0147, interval_samples_per_second: 1.993, interval_steps_per_second: 2.491, epoch: 2.1961[0m
[32m[2022-08-30 12:12:18,838] [    INFO][0m - loss: 0.53515306, learning_rate: 2.9329411764705882e-05, global_step: 570, interval_runtime: 4.0337, interval_samples_per_second: 1.983, interval_steps_per_second: 2.479, epoch: 2.2353[0m
[32m[2022-08-30 12:12:22,878] [    INFO][0m - loss: 0.7662899, learning_rate: 2.9317647058823532e-05, global_step: 580, interval_runtime: 4.0401, interval_samples_per_second: 1.98, interval_steps_per_second: 2.475, epoch: 2.2745[0m
[32m[2022-08-30 12:12:26,902] [    INFO][0m - loss: 0.94322863, learning_rate: 2.9305882352941174e-05, global_step: 590, interval_runtime: 4.0242, interval_samples_per_second: 1.988, interval_steps_per_second: 2.485, epoch: 2.3137[0m
[32m[2022-08-30 12:12:30,928] [    INFO][0m - loss: 0.72752323, learning_rate: 2.9294117647058824e-05, global_step: 600, interval_runtime: 4.0266, interval_samples_per_second: 1.987, interval_steps_per_second: 2.483, epoch: 2.3529[0m
[32m[2022-08-30 12:12:30,929] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 12:12:30,929] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-30 12:12:30,929] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 12:12:30,929] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 12:12:30,929] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-30 12:13:06,272] [    INFO][0m - eval_loss: 1.7151414155960083, eval_accuracy: 0.5739845261121856, eval_runtime: 35.3426, eval_samples_per_second: 58.513, eval_steps_per_second: 1.839, epoch: 2.3529[0m
[32m[2022-08-30 12:13:06,273] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-08-30 12:13:06,273] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 12:13:13,169] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-08-30 12:13:13,169] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-08-30 12:13:31,437] [    INFO][0m - loss: 0.65421557, learning_rate: 2.928235294117647e-05, global_step: 610, interval_runtime: 60.5084, interval_samples_per_second: 0.132, interval_steps_per_second: 0.165, epoch: 2.3922[0m
[32m[2022-08-30 12:13:35,432] [    INFO][0m - loss: 0.77129097, learning_rate: 2.927058823529412e-05, global_step: 620, interval_runtime: 3.9952, interval_samples_per_second: 2.002, interval_steps_per_second: 2.503, epoch: 2.4314[0m
[32m[2022-08-30 12:13:39,437] [    INFO][0m - loss: 0.86441832, learning_rate: 2.925882352941177e-05, global_step: 630, interval_runtime: 4.0042, interval_samples_per_second: 1.998, interval_steps_per_second: 2.497, epoch: 2.4706[0m
[32m[2022-08-30 12:13:43,439] [    INFO][0m - loss: 0.74600897, learning_rate: 2.924705882352941e-05, global_step: 640, interval_runtime: 4.0028, interval_samples_per_second: 1.999, interval_steps_per_second: 2.498, epoch: 2.5098[0m
[32m[2022-08-30 12:13:47,433] [    INFO][0m - loss: 0.53800302, learning_rate: 2.923529411764706e-05, global_step: 650, interval_runtime: 3.9939, interval_samples_per_second: 2.003, interval_steps_per_second: 2.504, epoch: 2.549[0m
[32m[2022-08-30 12:13:51,439] [    INFO][0m - loss: 0.60862298, learning_rate: 2.9223529411764706e-05, global_step: 660, interval_runtime: 4.0056, interval_samples_per_second: 1.997, interval_steps_per_second: 2.497, epoch: 2.5882[0m
[32m[2022-08-30 12:13:55,441] [    INFO][0m - loss: 0.78478007, learning_rate: 2.9211764705882356e-05, global_step: 670, interval_runtime: 4.0022, interval_samples_per_second: 1.999, interval_steps_per_second: 2.499, epoch: 2.6275[0m
[32m[2022-08-30 12:13:59,460] [    INFO][0m - loss: 0.81358328, learning_rate: 2.92e-05, global_step: 680, interval_runtime: 4.0188, interval_samples_per_second: 1.991, interval_steps_per_second: 2.488, epoch: 2.6667[0m
[32m[2022-08-30 12:14:03,486] [    INFO][0m - loss: 0.67372527, learning_rate: 2.9188235294117648e-05, global_step: 690, interval_runtime: 4.0263, interval_samples_per_second: 1.987, interval_steps_per_second: 2.484, epoch: 2.7059[0m
[32m[2022-08-30 12:14:07,512] [    INFO][0m - loss: 0.8083602, learning_rate: 2.9176470588235294e-05, global_step: 700, interval_runtime: 4.026, interval_samples_per_second: 1.987, interval_steps_per_second: 2.484, epoch: 2.7451[0m
[32m[2022-08-30 12:14:07,512] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 12:14:07,513] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-30 12:14:07,513] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 12:14:07,513] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 12:14:07,513] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-30 12:14:42,925] [    INFO][0m - eval_loss: 1.7779978513717651, eval_accuracy: 0.5517408123791102, eval_runtime: 35.4119, eval_samples_per_second: 58.398, eval_steps_per_second: 1.836, epoch: 2.7451[0m
[32m[2022-08-30 12:14:42,926] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-700[0m
[32m[2022-08-30 12:14:42,926] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 12:14:45,884] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-700/tokenizer_config.json[0m
[32m[2022-08-30 12:14:45,884] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-700/special_tokens_map.json[0m
[32m[2022-08-30 12:14:55,675] [    INFO][0m - loss: 0.94652719, learning_rate: 2.9164705882352943e-05, global_step: 710, interval_runtime: 48.1627, interval_samples_per_second: 0.166, interval_steps_per_second: 0.208, epoch: 2.7843[0m
[32m[2022-08-30 12:14:59,683] [    INFO][0m - loss: 0.84500866, learning_rate: 2.915294117647059e-05, global_step: 720, interval_runtime: 4.0085, interval_samples_per_second: 1.996, interval_steps_per_second: 2.495, epoch: 2.8235[0m
[32m[2022-08-30 12:15:03,695] [    INFO][0m - loss: 0.80853825, learning_rate: 2.9141176470588235e-05, global_step: 730, interval_runtime: 4.0122, interval_samples_per_second: 1.994, interval_steps_per_second: 2.492, epoch: 2.8627[0m
[32m[2022-08-30 12:15:07,719] [    INFO][0m - loss: 0.91960459, learning_rate: 2.912941176470588e-05, global_step: 740, interval_runtime: 4.0234, interval_samples_per_second: 1.988, interval_steps_per_second: 2.485, epoch: 2.902[0m
[32m[2022-08-30 12:15:11,741] [    INFO][0m - loss: 0.90704918, learning_rate: 2.911764705882353e-05, global_step: 750, interval_runtime: 4.0221, interval_samples_per_second: 1.989, interval_steps_per_second: 2.486, epoch: 2.9412[0m
[32m[2022-08-30 12:15:15,773] [    INFO][0m - loss: 0.65421581, learning_rate: 2.9105882352941176e-05, global_step: 760, interval_runtime: 4.0319, interval_samples_per_second: 1.984, interval_steps_per_second: 2.48, epoch: 2.9804[0m
[32m[2022-08-30 12:15:19,694] [    INFO][0m - loss: 0.63866453, learning_rate: 2.9094117647058826e-05, global_step: 770, interval_runtime: 3.9214, interval_samples_per_second: 2.04, interval_steps_per_second: 2.55, epoch: 3.0196[0m
[32m[2022-08-30 12:15:27,587] [    INFO][0m - loss: 0.28467906, learning_rate: 2.908235294117647e-05, global_step: 780, interval_runtime: 7.8929, interval_samples_per_second: 1.014, interval_steps_per_second: 1.267, epoch: 3.0588[0m
[32m[2022-08-30 12:15:31,608] [    INFO][0m - loss: 0.42776251, learning_rate: 2.9070588235294118e-05, global_step: 790, interval_runtime: 4.0212, interval_samples_per_second: 1.989, interval_steps_per_second: 2.487, epoch: 3.098[0m
[32m[2022-08-30 12:15:35,631] [    INFO][0m - loss: 0.40148687, learning_rate: 2.9058823529411767e-05, global_step: 800, interval_runtime: 4.0225, interval_samples_per_second: 1.989, interval_steps_per_second: 2.486, epoch: 3.1373[0m
[32m[2022-08-30 12:15:35,631] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 12:15:35,632] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-30 12:15:35,632] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 12:15:35,632] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 12:15:35,632] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-30 12:16:11,200] [    INFO][0m - eval_loss: 1.6052119731903076, eval_accuracy: 0.6141199226305609, eval_runtime: 35.5673, eval_samples_per_second: 58.143, eval_steps_per_second: 1.828, epoch: 3.1373[0m
[32m[2022-08-30 12:16:11,200] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-800[0m
[32m[2022-08-30 12:16:11,200] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 12:16:14,130] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-800/tokenizer_config.json[0m
[32m[2022-08-30 12:16:14,130] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-800/special_tokens_map.json[0m
[32m[2022-08-30 12:16:23,962] [    INFO][0m - loss: 0.34144418, learning_rate: 2.9047058823529413e-05, global_step: 810, interval_runtime: 48.3304, interval_samples_per_second: 0.166, interval_steps_per_second: 0.207, epoch: 3.1765[0m
[32m[2022-08-30 12:16:27,991] [    INFO][0m - loss: 0.35468063, learning_rate: 2.9035294117647062e-05, global_step: 820, interval_runtime: 4.0296, interval_samples_per_second: 1.985, interval_steps_per_second: 2.482, epoch: 3.2157[0m
[32m[2022-08-30 12:16:32,011] [    INFO][0m - loss: 0.17229736, learning_rate: 2.9023529411764705e-05, global_step: 830, interval_runtime: 4.0199, interval_samples_per_second: 1.99, interval_steps_per_second: 2.488, epoch: 3.2549[0m
[32m[2022-08-30 12:16:36,053] [    INFO][0m - loss: 0.40706587, learning_rate: 2.9011764705882354e-05, global_step: 840, interval_runtime: 4.0422, interval_samples_per_second: 1.979, interval_steps_per_second: 2.474, epoch: 3.2941[0m
[32m[2022-08-30 12:16:40,084] [    INFO][0m - loss: 0.36981902, learning_rate: 2.9e-05, global_step: 850, interval_runtime: 4.03, interval_samples_per_second: 1.985, interval_steps_per_second: 2.481, epoch: 3.3333[0m
[32m[2022-08-30 12:16:44,121] [    INFO][0m - loss: 0.50382915, learning_rate: 2.898823529411765e-05, global_step: 860, interval_runtime: 4.0381, interval_samples_per_second: 1.981, interval_steps_per_second: 2.476, epoch: 3.3725[0m
[32m[2022-08-30 12:16:48,143] [    INFO][0m - loss: 0.41022806, learning_rate: 2.8976470588235296e-05, global_step: 870, interval_runtime: 4.0216, interval_samples_per_second: 1.989, interval_steps_per_second: 2.487, epoch: 3.4118[0m
[32m[2022-08-30 12:16:54,022] [    INFO][0m - loss: 0.4342226, learning_rate: 2.896470588235294e-05, global_step: 880, interval_runtime: 4.0449, interval_samples_per_second: 1.978, interval_steps_per_second: 2.472, epoch: 3.451[0m
[32m[2022-08-30 12:16:58,051] [    INFO][0m - loss: 0.38888283, learning_rate: 2.8952941176470587e-05, global_step: 890, interval_runtime: 5.8624, interval_samples_per_second: 1.365, interval_steps_per_second: 1.706, epoch: 3.4902[0m
[32m[2022-08-30 12:17:02,085] [    INFO][0m - loss: 0.34155915, learning_rate: 2.8941176470588237e-05, global_step: 900, interval_runtime: 4.0348, interval_samples_per_second: 1.983, interval_steps_per_second: 2.478, epoch: 3.5294[0m
[32m[2022-08-30 12:17:02,086] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 12:17:02,086] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-30 12:17:02,086] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 12:17:02,086] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 12:17:02,086] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-30 12:17:37,699] [    INFO][0m - eval_loss: 1.8557449579238892, eval_accuracy: 0.59284332688588, eval_runtime: 35.6125, eval_samples_per_second: 58.07, eval_steps_per_second: 1.825, epoch: 3.5294[0m
[32m[2022-08-30 12:17:37,700] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-900[0m
[32m[2022-08-30 12:17:37,700] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 12:17:40,585] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-900/tokenizer_config.json[0m
[32m[2022-08-30 12:17:40,586] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-900/special_tokens_map.json[0m
[32m[2022-08-30 12:17:50,390] [    INFO][0m - loss: 0.28242321, learning_rate: 2.8929411764705883e-05, global_step: 910, interval_runtime: 48.3052, interval_samples_per_second: 0.166, interval_steps_per_second: 0.207, epoch: 3.5686[0m
[32m[2022-08-30 12:17:54,416] [    INFO][0m - loss: 0.35209179, learning_rate: 2.891764705882353e-05, global_step: 920, interval_runtime: 4.0259, interval_samples_per_second: 1.987, interval_steps_per_second: 2.484, epoch: 3.6078[0m
[32m[2022-08-30 12:17:58,423] [    INFO][0m - loss: 0.51469688, learning_rate: 2.8905882352941175e-05, global_step: 930, interval_runtime: 4.0064, interval_samples_per_second: 1.997, interval_steps_per_second: 2.496, epoch: 3.6471[0m
[32m[2022-08-30 12:18:02,450] [    INFO][0m - loss: 0.26173306, learning_rate: 2.8894117647058824e-05, global_step: 940, interval_runtime: 4.0272, interval_samples_per_second: 1.986, interval_steps_per_second: 2.483, epoch: 3.6863[0m
[32m[2022-08-30 12:18:06,486] [    INFO][0m - loss: 0.8164855, learning_rate: 2.8882352941176473e-05, global_step: 950, interval_runtime: 4.0361, interval_samples_per_second: 1.982, interval_steps_per_second: 2.478, epoch: 3.7255[0m
[32m[2022-08-30 12:18:10,519] [    INFO][0m - loss: 0.48385062, learning_rate: 2.887058823529412e-05, global_step: 960, interval_runtime: 4.0332, interval_samples_per_second: 1.984, interval_steps_per_second: 2.479, epoch: 3.7647[0m
[32m[2022-08-30 12:18:14,550] [    INFO][0m - loss: 0.41760535, learning_rate: 2.8858823529411765e-05, global_step: 970, interval_runtime: 4.031, interval_samples_per_second: 1.985, interval_steps_per_second: 2.481, epoch: 3.8039[0m
[32m[2022-08-30 12:18:20,671] [    INFO][0m - loss: 0.5042119, learning_rate: 2.884705882352941e-05, global_step: 980, interval_runtime: 4.0478, interval_samples_per_second: 1.976, interval_steps_per_second: 2.47, epoch: 3.8431[0m
[32m[2022-08-30 12:18:24,706] [    INFO][0m - loss: 0.46564946, learning_rate: 2.883529411764706e-05, global_step: 990, interval_runtime: 6.1078, interval_samples_per_second: 1.31, interval_steps_per_second: 1.637, epoch: 3.8824[0m
[32m[2022-08-30 12:18:28,746] [    INFO][0m - loss: 0.3260653, learning_rate: 2.8823529411764707e-05, global_step: 1000, interval_runtime: 4.0399, interval_samples_per_second: 1.98, interval_steps_per_second: 2.475, epoch: 3.9216[0m
[32m[2022-08-30 12:18:28,746] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 12:18:28,746] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-30 12:18:28,746] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 12:18:28,747] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 12:18:28,747] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-30 12:19:04,386] [    INFO][0m - eval_loss: 1.8975143432617188, eval_accuracy: 0.5880077369439072, eval_runtime: 35.6393, eval_samples_per_second: 58.026, eval_steps_per_second: 1.824, epoch: 3.9216[0m
[32m[2022-08-30 12:19:04,387] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1000[0m
[32m[2022-08-30 12:19:04,387] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 12:19:07,212] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1000/tokenizer_config.json[0m
[32m[2022-08-30 12:19:07,212] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1000/special_tokens_map.json[0m
[32m[2022-08-30 12:19:16,920] [    INFO][0m - loss: 0.38412571, learning_rate: 2.8811764705882356e-05, global_step: 1010, interval_runtime: 48.1743, interval_samples_per_second: 0.166, interval_steps_per_second: 0.208, epoch: 3.9608[0m
[32m[2022-08-30 12:19:20,817] [    INFO][0m - loss: 0.59894438, learning_rate: 2.88e-05, global_step: 1020, interval_runtime: 3.8086, interval_samples_per_second: 2.1, interval_steps_per_second: 2.626, epoch: 4.0[0m
[32m[2022-08-30 12:19:24,965] [    INFO][0m - loss: 0.12528375, learning_rate: 2.8788235294117648e-05, global_step: 1030, interval_runtime: 4.2363, interval_samples_per_second: 1.888, interval_steps_per_second: 2.361, epoch: 4.0392[0m
[32m[2022-08-30 12:19:29,024] [    INFO][0m - loss: 0.13711131, learning_rate: 2.8776470588235294e-05, global_step: 1040, interval_runtime: 4.059, interval_samples_per_second: 1.971, interval_steps_per_second: 2.464, epoch: 4.0784[0m
[32m[2022-08-30 12:19:33,082] [    INFO][0m - loss: 0.18069816, learning_rate: 2.8764705882352943e-05, global_step: 1050, interval_runtime: 4.058, interval_samples_per_second: 1.971, interval_steps_per_second: 2.464, epoch: 4.1176[0m
[32m[2022-08-30 12:19:37,148] [    INFO][0m - loss: 0.19693114, learning_rate: 2.8752941176470586e-05, global_step: 1060, interval_runtime: 4.066, interval_samples_per_second: 1.968, interval_steps_per_second: 2.459, epoch: 4.1569[0m
[32m[2022-08-30 12:19:41,206] [    INFO][0m - loss: 0.2044982, learning_rate: 2.8741176470588235e-05, global_step: 1070, interval_runtime: 4.0575, interval_samples_per_second: 1.972, interval_steps_per_second: 2.465, epoch: 4.1961[0m
[32m[2022-08-30 12:19:45,248] [    INFO][0m - loss: 0.15052116, learning_rate: 2.872941176470588e-05, global_step: 1080, interval_runtime: 4.0427, interval_samples_per_second: 1.979, interval_steps_per_second: 2.474, epoch: 4.2353[0m
[32m[2022-08-30 12:19:49,304] [    INFO][0m - loss: 0.26841357, learning_rate: 2.871764705882353e-05, global_step: 1090, interval_runtime: 4.0555, interval_samples_per_second: 1.973, interval_steps_per_second: 2.466, epoch: 4.2745[0m
[32m[2022-08-30 12:19:53,354] [    INFO][0m - loss: 0.19902762, learning_rate: 2.870588235294118e-05, global_step: 1100, interval_runtime: 4.0503, interval_samples_per_second: 1.975, interval_steps_per_second: 2.469, epoch: 4.3137[0m
[32m[2022-08-30 12:19:53,354] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 12:19:53,355] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-30 12:19:53,355] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 12:19:53,355] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 12:19:53,355] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-30 12:20:28,906] [    INFO][0m - eval_loss: 2.041905164718628, eval_accuracy: 0.5933268858800773, eval_runtime: 35.5509, eval_samples_per_second: 58.17, eval_steps_per_second: 1.828, epoch: 4.3137[0m
[32m[2022-08-30 12:20:28,907] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1100[0m
[32m[2022-08-30 12:20:28,907] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 12:20:31,735] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1100/tokenizer_config.json[0m
[32m[2022-08-30 12:20:31,735] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1100/special_tokens_map.json[0m
[32m[2022-08-30 12:20:41,336] [    INFO][0m - loss: 0.11257594, learning_rate: 2.8694117647058823e-05, global_step: 1110, interval_runtime: 47.9817, interval_samples_per_second: 0.167, interval_steps_per_second: 0.208, epoch: 4.3529[0m
[32m[2022-08-30 12:20:45,345] [    INFO][0m - loss: 0.32594545, learning_rate: 2.8682352941176472e-05, global_step: 1120, interval_runtime: 4.0091, interval_samples_per_second: 1.995, interval_steps_per_second: 2.494, epoch: 4.3922[0m
[32m[2022-08-30 12:20:49,384] [    INFO][0m - loss: 0.27181129, learning_rate: 2.8670588235294118e-05, global_step: 1130, interval_runtime: 4.0393, interval_samples_per_second: 1.981, interval_steps_per_second: 2.476, epoch: 4.4314[0m
[32m[2022-08-30 12:20:53,419] [    INFO][0m - loss: 0.14670991, learning_rate: 2.8658823529411767e-05, global_step: 1140, interval_runtime: 4.035, interval_samples_per_second: 1.983, interval_steps_per_second: 2.478, epoch: 4.4706[0m
[32m[2022-08-30 12:20:57,479] [    INFO][0m - loss: 0.29789681, learning_rate: 2.8647058823529413e-05, global_step: 1150, interval_runtime: 4.0603, interval_samples_per_second: 1.97, interval_steps_per_second: 2.463, epoch: 4.5098[0m
[32m[2022-08-30 12:21:01,545] [    INFO][0m - loss: 0.17408156, learning_rate: 2.863529411764706e-05, global_step: 1160, interval_runtime: 4.0651, interval_samples_per_second: 1.968, interval_steps_per_second: 2.46, epoch: 4.549[0m
[32m[2022-08-30 12:21:05,582] [    INFO][0m - loss: 0.2538445, learning_rate: 2.8623529411764705e-05, global_step: 1170, interval_runtime: 4.038, interval_samples_per_second: 1.981, interval_steps_per_second: 2.476, epoch: 4.5882[0m
[32m[2022-08-30 12:21:09,614] [    INFO][0m - loss: 0.41075125, learning_rate: 2.8611764705882355e-05, global_step: 1180, interval_runtime: 4.0316, interval_samples_per_second: 1.984, interval_steps_per_second: 2.48, epoch: 4.6275[0m
[32m[2022-08-30 12:21:13,662] [    INFO][0m - loss: 0.13296262, learning_rate: 2.86e-05, global_step: 1190, interval_runtime: 4.0482, interval_samples_per_second: 1.976, interval_steps_per_second: 2.47, epoch: 4.6667[0m
[32m[2022-08-30 12:21:17,712] [    INFO][0m - loss: 0.21297193, learning_rate: 2.8588235294117647e-05, global_step: 1200, interval_runtime: 4.0496, interval_samples_per_second: 1.976, interval_steps_per_second: 2.469, epoch: 4.7059[0m
[32m[2022-08-30 12:21:17,712] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 12:21:17,712] [    INFO][0m -   Num examples = 2068[0m
[32m[2022-08-30 12:21:17,713] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 12:21:17,713] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 12:21:17,713] [    INFO][0m -   Total prediction steps = 65[0m
[32m[2022-08-30 12:21:53,294] [    INFO][0m - eval_loss: 2.06146240234375, eval_accuracy: 0.5957446808510638, eval_runtime: 35.5809, eval_samples_per_second: 58.121, eval_steps_per_second: 1.827, epoch: 4.7059[0m
[32m[2022-08-30 12:21:53,295] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1200[0m
[32m[2022-08-30 12:21:53,295] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 12:21:56,075] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1200/tokenizer_config.json[0m
[32m[2022-08-30 12:21:56,076] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1200/special_tokens_map.json[0m
[32m[2022-08-30 12:22:01,739] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-30 12:22:01,740] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-800 (score: 0.6141199226305609).[0m
[32m[2022-08-30 12:22:03,548] [    INFO][0m - train_runtime: 1100.1664, train_samples_per_second: 185.063, train_steps_per_second: 23.178, train_loss: 1.2709966139992077, epoch: 4.7059[0m
[32m[2022-08-30 12:22:03,549] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-30 12:22:03,550] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 12:22:10,523] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-30 12:22:10,524] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-30 12:22:10,525] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-30 12:22:10,525] [    INFO][0m -   epoch                    =     4.7059[0m
[32m[2022-08-30 12:22:10,525] [    INFO][0m -   train_loss               =      1.271[0m
[32m[2022-08-30 12:22:10,526] [    INFO][0m -   train_runtime            = 0:18:20.16[0m
[32m[2022-08-30 12:22:10,526] [    INFO][0m -   train_samples_per_second =    185.063[0m
[32m[2022-08-30 12:22:10,526] [    INFO][0m -   train_steps_per_second   =     23.178[0m
[32m[2022-08-30 12:22:10,531] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-30 12:22:10,532] [    INFO][0m -   Num examples = 1784[0m
[32m[2022-08-30 12:22:10,532] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 12:22:10,532] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 12:22:10,532] [    INFO][0m -   Total prediction steps = 56[0m
[32m[2022-08-30 12:22:41,083] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-30 12:22:41,084] [    INFO][0m -   test_accuracy           =      0.611[0m
[32m[2022-08-30 12:22:41,084] [    INFO][0m -   test_loss               =     1.6075[0m
[32m[2022-08-30 12:22:41,084] [    INFO][0m -   test_runtime            = 0:00:30.55[0m
[32m[2022-08-30 12:22:41,084] [    INFO][0m -   test_samples_per_second =     58.393[0m
[32m[2022-08-30 12:22:41,084] [    INFO][0m -   test_steps_per_second   =      1.833[0m
[32m[2022-08-30 12:22:41,084] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-30 12:22:41,085] [    INFO][0m -   Num examples = 2999[0m
[32m[2022-08-30 12:22:41,085] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 12:22:41,085] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 12:22:41,085] [    INFO][0m -   Total prediction steps = 94[0m
[32m[2022-08-30 12:23:38,453] [    INFO][0m - Predictions for csldcp saved to ./fewclue_submit_examples.[0m
metric <function main.<locals>.compute_metrics at 0x7fa74e4c9790>
 
==========
tnews
==========
 
[33m[2022-08-30 12:23:42,386] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-30 12:23:42,386] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-30 12:23:42,386] [    INFO][0m - ============================================================[0m
[32m[2022-08-30 12:23:42,386] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-30 12:23:42,387] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-30 12:23:42,387] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-30 12:23:42,387] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-30 12:23:42,387] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-08-30 12:23:42,387] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-30 12:23:42,387] [    INFO][0m - model_name_or_path            :hfl/roberta-wwm-ext-large[0m
[32m[2022-08-30 12:23:42,387] [    INFO][0m - [0m
[32m[2022-08-30 12:23:42,387] [    INFO][0m - ============================================================[0m
[32m[2022-08-30 12:23:42,387] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-30 12:23:42,387] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-30 12:23:42,387] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-30 12:23:42,387] [    INFO][0m - prompt                        :{'hard':'下边播报一则'}{'mask'}{'mask'}{'hard':'新闻：'}{'text':'text_a'}[0m
[32m[2022-08-30 12:23:42,387] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-30 12:23:42,388] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-30 12:23:42,388] [    INFO][0m - task_name                     :tnews[0m
[32m[2022-08-30 12:23:42,388] [    INFO][0m - [0m
[32m[2022-08-30 12:23:42,388] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/hfl/roberta-wwm-ext-large/roberta_chn_large.pdparams[0m
W0830 12:23:42.389369 77214 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0830 12:23:42.394263 77214 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-30 12:23:47,023] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/hfl/roberta-wwm-ext-large/vocab.txt[0m
[32m[2022-08-30 12:23:47,039] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/hfl/roberta-wwm-ext-large/tokenizer_config.json[0m
[32m[2022-08-30 12:23:47,040] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/hfl/roberta-wwm-ext-large/special_tokens_map.json[0m
[32m[2022-08-30 12:23:47,040] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '下边播报一则'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': '新闻：'}, {'add_prefix_space': '', 'text': 'text_a'}][0m
2022-08-30 12:23:47,049 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-30 12:23:47,165] [    INFO][0m - ============================================================[0m
[32m[2022-08-30 12:23:47,165] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-30 12:23:47,165] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-30 12:23:47,165] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-30 12:23:47,165] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-30 12:23:47,165] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-30 12:23:47,165] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-30 12:23:47,165] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-30 12:23:47,165] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-30 12:23:47,166] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-30 12:23:47,166] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-30 12:23:47,166] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-30 12:23:47,166] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-30 12:23:47,166] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-30 12:23:47,166] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-30 12:23:47,166] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-30 12:23:47,166] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-30 12:23:47,166] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-30 12:23:47,166] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-30 12:23:47,166] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-30 12:23:47,166] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-30 12:23:47,166] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-30 12:23:47,167] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-30 12:23:47,167] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-30 12:23:47,167] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-30 12:23:47,167] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-30 12:23:47,167] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-30 12:23:47,167] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-30 12:23:47,167] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-30 12:23:47,167] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-30 12:23:47,167] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-30 12:23:47,167] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-30 12:23:47,167] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-30 12:23:47,167] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-30 12:23:47,167] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-30 12:23:47,167] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-30 12:23:47,168] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-30 12:23:47,168] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug30_12-23-42_instance-3bwob41y-01[0m
[32m[2022-08-30 12:23:47,168] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-30 12:23:47,168] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-30 12:23:47,168] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-30 12:23:47,168] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-30 12:23:47,168] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-30 12:23:47,168] [    INFO][0m - max_seq_length                :64[0m
[32m[2022-08-30 12:23:47,168] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-30 12:23:47,168] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-30 12:23:47,168] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-30 12:23:47,168] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-30 12:23:47,168] [    INFO][0m - num_train_epochs              :100.0[0m
[32m[2022-08-30 12:23:47,168] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-30 12:23:47,169] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-30 12:23:47,169] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-30 12:23:47,169] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-30 12:23:47,169] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-30 12:23:47,169] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-30 12:23:47,169] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-30 12:23:47,169] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-30 12:23:47,169] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-30 12:23:47,169] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-30 12:23:47,169] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-30 12:23:47,169] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-30 12:23:47,169] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-30 12:23:47,169] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-30 12:23:47,169] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-30 12:23:47,170] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-30 12:23:47,170] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-30 12:23:47,170] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-30 12:23:47,170] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-30 12:23:47,170] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-30 12:23:47,170] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-30 12:23:47,170] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-30 12:23:47,170] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-30 12:23:47,170] [    INFO][0m - seed                          :42[0m
[32m[2022-08-30 12:23:47,170] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-30 12:23:47,170] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-30 12:23:47,170] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-30 12:23:47,170] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-30 12:23:47,171] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-30 12:23:47,171] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-30 12:23:47,171] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-30 12:23:47,171] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-30 12:23:47,171] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-30 12:23:47,171] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-30 12:23:47,171] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-30 12:23:47,171] [    INFO][0m - [0m
[32m[2022-08-30 12:23:47,174] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-30 12:23:47,174] [    INFO][0m -   Num examples = 1185[0m
[32m[2022-08-30 12:23:47,174] [    INFO][0m -   Num Epochs = 100[0m
[32m[2022-08-30 12:23:47,174] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-30 12:23:47,174] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-30 12:23:47,174] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-30 12:23:47,175] [    INFO][0m -   Total optimization steps = 14900.0[0m
[32m[2022-08-30 12:23:47,175] [    INFO][0m -   Total num train samples = 118500[0m
[32m[2022-08-30 12:23:49,694] [    INFO][0m - loss: 2.98420162, learning_rate: 2.9979865771812083e-05, global_step: 10, interval_runtime: 2.5175, interval_samples_per_second: 3.178, interval_steps_per_second: 3.972, epoch: 0.0671[0m
[32m[2022-08-30 12:23:50,977] [    INFO][0m - loss: 2.52758217, learning_rate: 2.9959731543624162e-05, global_step: 20, interval_runtime: 1.2837, interval_samples_per_second: 6.232, interval_steps_per_second: 7.79, epoch: 0.1342[0m
[32m[2022-08-30 12:23:52,254] [    INFO][0m - loss: 1.92743397, learning_rate: 2.9939597315436244e-05, global_step: 30, interval_runtime: 1.2771, interval_samples_per_second: 6.264, interval_steps_per_second: 7.83, epoch: 0.2013[0m
[32m[2022-08-30 12:23:53,535] [    INFO][0m - loss: 1.83701706, learning_rate: 2.9919463087248323e-05, global_step: 40, interval_runtime: 1.2804, interval_samples_per_second: 6.248, interval_steps_per_second: 7.81, epoch: 0.2685[0m
[32m[2022-08-30 12:23:54,821] [    INFO][0m - loss: 1.57614441, learning_rate: 2.9899328859060402e-05, global_step: 50, interval_runtime: 1.2864, interval_samples_per_second: 6.219, interval_steps_per_second: 7.774, epoch: 0.3356[0m
[32m[2022-08-30 12:23:56,107] [    INFO][0m - loss: 1.43341551, learning_rate: 2.9879194630872484e-05, global_step: 60, interval_runtime: 1.2865, interval_samples_per_second: 6.218, interval_steps_per_second: 7.773, epoch: 0.4027[0m
[32m[2022-08-30 12:23:57,403] [    INFO][0m - loss: 1.45595179, learning_rate: 2.9859060402684563e-05, global_step: 70, interval_runtime: 1.2954, interval_samples_per_second: 6.176, interval_steps_per_second: 7.72, epoch: 0.4698[0m
[32m[2022-08-30 12:23:58,693] [    INFO][0m - loss: 1.68933697, learning_rate: 2.9838926174496645e-05, global_step: 80, interval_runtime: 1.2904, interval_samples_per_second: 6.2, interval_steps_per_second: 7.749, epoch: 0.5369[0m
[32m[2022-08-30 12:23:59,981] [    INFO][0m - loss: 1.91555347, learning_rate: 2.9818791946308724e-05, global_step: 90, interval_runtime: 1.2876, interval_samples_per_second: 6.213, interval_steps_per_second: 7.766, epoch: 0.604[0m
[32m[2022-08-30 12:24:01,271] [    INFO][0m - loss: 1.41064959, learning_rate: 2.9798657718120806e-05, global_step: 100, interval_runtime: 1.2906, interval_samples_per_second: 6.199, interval_steps_per_second: 7.748, epoch: 0.6711[0m
[32m[2022-08-30 12:24:01,272] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 12:24:01,272] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-08-30 12:24:01,272] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 12:24:01,272] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 12:24:01,272] [    INFO][0m -   Total prediction steps = 35[0m
[32m[2022-08-30 12:24:06,264] [    INFO][0m - eval_loss: 1.6407816410064697, eval_accuracy: 0.5245901639344263, eval_runtime: 4.9914, eval_samples_per_second: 219.979, eval_steps_per_second: 7.012, epoch: 0.6711[0m
[32m[2022-08-30 12:24:06,264] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-30 12:24:06,265] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 12:24:13,577] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-30 12:24:13,577] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-30 12:24:30,592] [    INFO][0m - loss: 1.97943058, learning_rate: 2.977852348993289e-05, global_step: 110, interval_runtime: 29.3209, interval_samples_per_second: 0.273, interval_steps_per_second: 0.341, epoch: 0.7383[0m
[32m[2022-08-30 12:24:31,865] [    INFO][0m - loss: 1.55032072, learning_rate: 2.9758389261744967e-05, global_step: 120, interval_runtime: 1.2729, interval_samples_per_second: 6.285, interval_steps_per_second: 7.856, epoch: 0.8054[0m
[32m[2022-08-30 12:24:33,141] [    INFO][0m - loss: 1.76999092, learning_rate: 2.973825503355705e-05, global_step: 130, interval_runtime: 1.2761, interval_samples_per_second: 6.269, interval_steps_per_second: 7.836, epoch: 0.8725[0m
[32m[2022-08-30 12:24:34,415] [    INFO][0m - loss: 1.76222038, learning_rate: 2.9718120805369125e-05, global_step: 140, interval_runtime: 1.2743, interval_samples_per_second: 6.278, interval_steps_per_second: 7.847, epoch: 0.9396[0m
[32m[2022-08-30 12:24:35,688] [    INFO][0m - loss: 1.69882584, learning_rate: 2.9697986577181207e-05, global_step: 150, interval_runtime: 1.2726, interval_samples_per_second: 6.286, interval_steps_per_second: 7.858, epoch: 1.0067[0m
[32m[2022-08-30 12:24:36,969] [    INFO][0m - loss: 1.00037556, learning_rate: 2.967785234899329e-05, global_step: 160, interval_runtime: 1.2811, interval_samples_per_second: 6.244, interval_steps_per_second: 7.806, epoch: 1.0738[0m
[32m[2022-08-30 12:24:38,247] [    INFO][0m - loss: 0.78803859, learning_rate: 2.965771812080537e-05, global_step: 170, interval_runtime: 1.2777, interval_samples_per_second: 6.261, interval_steps_per_second: 7.826, epoch: 1.1409[0m
[32m[2022-08-30 12:24:39,524] [    INFO][0m - loss: 0.90281267, learning_rate: 2.963758389261745e-05, global_step: 180, interval_runtime: 1.2777, interval_samples_per_second: 6.261, interval_steps_per_second: 7.827, epoch: 1.2081[0m
[32m[2022-08-30 12:24:40,800] [    INFO][0m - loss: 1.09636707, learning_rate: 2.961744966442953e-05, global_step: 190, interval_runtime: 1.276, interval_samples_per_second: 6.27, interval_steps_per_second: 7.837, epoch: 1.2752[0m
[32m[2022-08-30 12:24:42,081] [    INFO][0m - loss: 0.9390336, learning_rate: 2.9597315436241612e-05, global_step: 200, interval_runtime: 1.2809, interval_samples_per_second: 6.246, interval_steps_per_second: 7.807, epoch: 1.3423[0m
[32m[2022-08-30 12:24:42,082] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 12:24:42,082] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-08-30 12:24:42,082] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 12:24:42,082] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 12:24:42,082] [    INFO][0m -   Total prediction steps = 35[0m
[32m[2022-08-30 12:24:47,052] [    INFO][0m - eval_loss: 1.5930116176605225, eval_accuracy: 0.5145719489981785, eval_runtime: 4.9688, eval_samples_per_second: 220.981, eval_steps_per_second: 7.044, epoch: 1.3423[0m
[32m[2022-08-30 12:24:47,052] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-30 12:24:47,052] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 12:24:54,084] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-30 12:24:54,084] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-30 12:25:09,721] [    INFO][0m - loss: 0.99308634, learning_rate: 2.9577181208053694e-05, global_step: 210, interval_runtime: 27.6399, interval_samples_per_second: 0.289, interval_steps_per_second: 0.362, epoch: 1.4094[0m
[32m[2022-08-30 12:25:10,992] [    INFO][0m - loss: 0.91336184, learning_rate: 2.9557046979865773e-05, global_step: 220, interval_runtime: 1.2707, interval_samples_per_second: 6.296, interval_steps_per_second: 7.87, epoch: 1.4765[0m
[32m[2022-08-30 12:25:12,264] [    INFO][0m - loss: 1.00190277, learning_rate: 2.9536912751677852e-05, global_step: 230, interval_runtime: 1.2722, interval_samples_per_second: 6.289, interval_steps_per_second: 7.861, epoch: 1.5436[0m
[32m[2022-08-30 12:25:13,536] [    INFO][0m - loss: 1.0824173, learning_rate: 2.951677852348993e-05, global_step: 240, interval_runtime: 1.2715, interval_samples_per_second: 6.292, interval_steps_per_second: 7.865, epoch: 1.6107[0m
[32m[2022-08-30 12:25:14,809] [    INFO][0m - loss: 0.76667938, learning_rate: 2.9496644295302013e-05, global_step: 250, interval_runtime: 1.2738, interval_samples_per_second: 6.28, interval_steps_per_second: 7.85, epoch: 1.6779[0m
[32m[2022-08-30 12:25:16,084] [    INFO][0m - loss: 1.03083544, learning_rate: 2.9476510067114095e-05, global_step: 260, interval_runtime: 1.275, interval_samples_per_second: 6.275, interval_steps_per_second: 7.843, epoch: 1.745[0m
[32m[2022-08-30 12:25:17,360] [    INFO][0m - loss: 1.29720936, learning_rate: 2.9456375838926174e-05, global_step: 270, interval_runtime: 1.2755, interval_samples_per_second: 6.272, interval_steps_per_second: 7.84, epoch: 1.8121[0m
[32m[2022-08-30 12:25:18,634] [    INFO][0m - loss: 0.96545858, learning_rate: 2.9436241610738256e-05, global_step: 280, interval_runtime: 1.2737, interval_samples_per_second: 6.281, interval_steps_per_second: 7.851, epoch: 1.8792[0m
[32m[2022-08-30 12:25:19,909] [    INFO][0m - loss: 0.93107405, learning_rate: 2.9416107382550335e-05, global_step: 290, interval_runtime: 1.2749, interval_samples_per_second: 6.275, interval_steps_per_second: 7.844, epoch: 1.9463[0m
[32m[2022-08-30 12:25:21,183] [    INFO][0m - loss: 0.90262852, learning_rate: 2.9395973154362418e-05, global_step: 300, interval_runtime: 1.2743, interval_samples_per_second: 6.278, interval_steps_per_second: 7.847, epoch: 2.0134[0m
[32m[2022-08-30 12:25:21,183] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 12:25:21,183] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-08-30 12:25:21,184] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 12:25:21,184] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 12:25:21,184] [    INFO][0m -   Total prediction steps = 35[0m
[32m[2022-08-30 12:25:26,150] [    INFO][0m - eval_loss: 1.6904189586639404, eval_accuracy: 0.5264116575591985, eval_runtime: 4.9663, eval_samples_per_second: 221.091, eval_steps_per_second: 7.048, epoch: 2.0134[0m
[32m[2022-08-30 12:25:26,150] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-30 12:25:26,151] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 12:25:33,437] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-30 12:25:33,438] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-30 12:25:49,039] [    INFO][0m - loss: 0.49691191, learning_rate: 2.93758389261745e-05, global_step: 310, interval_runtime: 27.8555, interval_samples_per_second: 0.287, interval_steps_per_second: 0.359, epoch: 2.0805[0m
[32m[2022-08-30 12:25:50,308] [    INFO][0m - loss: 0.33189905, learning_rate: 2.935570469798658e-05, global_step: 320, interval_runtime: 1.2697, interval_samples_per_second: 6.301, interval_steps_per_second: 7.876, epoch: 2.1477[0m
[32m[2022-08-30 12:25:51,578] [    INFO][0m - loss: 0.49401526, learning_rate: 2.9335570469798658e-05, global_step: 330, interval_runtime: 1.2699, interval_samples_per_second: 6.3, interval_steps_per_second: 7.874, epoch: 2.2148[0m
[32m[2022-08-30 12:25:52,847] [    INFO][0m - loss: 0.44373083, learning_rate: 2.9315436241610736e-05, global_step: 340, interval_runtime: 1.2693, interval_samples_per_second: 6.303, interval_steps_per_second: 7.879, epoch: 2.2819[0m
[32m[2022-08-30 12:25:54,122] [    INFO][0m - loss: 0.32020493, learning_rate: 2.929530201342282e-05, global_step: 350, interval_runtime: 1.2749, interval_samples_per_second: 6.275, interval_steps_per_second: 7.843, epoch: 2.349[0m
[32m[2022-08-30 12:25:55,395] [    INFO][0m - loss: 0.51700153, learning_rate: 2.92751677852349e-05, global_step: 360, interval_runtime: 1.2725, interval_samples_per_second: 6.287, interval_steps_per_second: 7.858, epoch: 2.4161[0m
[32m[2022-08-30 12:25:56,666] [    INFO][0m - loss: 0.34406333, learning_rate: 2.925503355704698e-05, global_step: 370, interval_runtime: 1.2716, interval_samples_per_second: 6.291, interval_steps_per_second: 7.864, epoch: 2.4832[0m
[32m[2022-08-30 12:25:57,945] [    INFO][0m - loss: 0.30843554, learning_rate: 2.9234899328859062e-05, global_step: 380, interval_runtime: 1.2786, interval_samples_per_second: 6.257, interval_steps_per_second: 7.821, epoch: 2.5503[0m
[32m[2022-08-30 12:25:59,221] [    INFO][0m - loss: 0.44280162, learning_rate: 2.921476510067114e-05, global_step: 390, interval_runtime: 1.2761, interval_samples_per_second: 6.269, interval_steps_per_second: 7.837, epoch: 2.6174[0m
[32m[2022-08-30 12:26:00,499] [    INFO][0m - loss: 0.34196913, learning_rate: 2.9194630872483223e-05, global_step: 400, interval_runtime: 1.2782, interval_samples_per_second: 6.259, interval_steps_per_second: 7.824, epoch: 2.6846[0m
[32m[2022-08-30 12:26:00,500] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 12:26:00,500] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-08-30 12:26:00,500] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 12:26:00,500] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 12:26:00,500] [    INFO][0m -   Total prediction steps = 35[0m
[32m[2022-08-30 12:26:05,483] [    INFO][0m - eval_loss: 2.1327643394470215, eval_accuracy: 0.51183970856102, eval_runtime: 4.9827, eval_samples_per_second: 220.361, eval_steps_per_second: 7.024, epoch: 2.6846[0m
[32m[2022-08-30 12:26:05,483] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-30 12:26:05,483] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 12:26:12,464] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-30 12:26:12,464] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-30 12:26:28,049] [    INFO][0m - loss: 0.39242001, learning_rate: 2.9174496644295305e-05, global_step: 410, interval_runtime: 27.5496, interval_samples_per_second: 0.29, interval_steps_per_second: 0.363, epoch: 2.7517[0m
[32m[2022-08-30 12:26:29,317] [    INFO][0m - loss: 0.58243589, learning_rate: 2.915436241610738e-05, global_step: 420, interval_runtime: 1.2683, interval_samples_per_second: 6.307, interval_steps_per_second: 7.884, epoch: 2.8188[0m
[32m[2022-08-30 12:26:30,590] [    INFO][0m - loss: 0.71425347, learning_rate: 2.9134228187919463e-05, global_step: 430, interval_runtime: 1.273, interval_samples_per_second: 6.285, interval_steps_per_second: 7.856, epoch: 2.8859[0m
[32m[2022-08-30 12:26:31,861] [    INFO][0m - loss: 0.40578594, learning_rate: 2.9114093959731542e-05, global_step: 440, interval_runtime: 1.2706, interval_samples_per_second: 6.296, interval_steps_per_second: 7.87, epoch: 2.953[0m
[32m[2022-08-30 12:26:33,133] [    INFO][0m - loss: 0.3991116, learning_rate: 2.9093959731543624e-05, global_step: 450, interval_runtime: 1.2713, interval_samples_per_second: 6.293, interval_steps_per_second: 7.866, epoch: 3.0201[0m
[32m[2022-08-30 12:26:34,408] [    INFO][0m - loss: 0.09175537, learning_rate: 2.9073825503355706e-05, global_step: 460, interval_runtime: 1.2761, interval_samples_per_second: 6.269, interval_steps_per_second: 7.836, epoch: 3.0872[0m
[32m[2022-08-30 12:26:35,685] [    INFO][0m - loss: 0.1732358, learning_rate: 2.9053691275167785e-05, global_step: 470, interval_runtime: 1.2767, interval_samples_per_second: 6.266, interval_steps_per_second: 7.833, epoch: 3.1544[0m
[32m[2022-08-30 12:26:36,962] [    INFO][0m - loss: 0.17718092, learning_rate: 2.9033557046979868e-05, global_step: 480, interval_runtime: 1.2768, interval_samples_per_second: 6.266, interval_steps_per_second: 7.832, epoch: 3.2215[0m
[32m[2022-08-30 12:26:38,238] [    INFO][0m - loss: 0.24843314, learning_rate: 2.9013422818791946e-05, global_step: 490, interval_runtime: 1.2768, interval_samples_per_second: 6.266, interval_steps_per_second: 7.832, epoch: 3.2886[0m
[32m[2022-08-30 12:26:39,514] [    INFO][0m - loss: 0.2714993, learning_rate: 2.899328859060403e-05, global_step: 500, interval_runtime: 1.2754, interval_samples_per_second: 6.272, interval_steps_per_second: 7.84, epoch: 3.3557[0m
[32m[2022-08-30 12:26:39,514] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 12:26:39,514] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-08-30 12:26:39,515] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 12:26:39,515] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 12:26:39,515] [    INFO][0m -   Total prediction steps = 35[0m
[32m[2022-08-30 12:26:44,474] [    INFO][0m - eval_loss: 2.3183789253234863, eval_accuracy: 0.5391621129326047, eval_runtime: 4.9595, eval_samples_per_second: 221.393, eval_steps_per_second: 7.057, epoch: 3.3557[0m
[32m[2022-08-30 12:26:44,475] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-08-30 12:26:44,475] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 12:26:51,574] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-08-30 12:26:51,575] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-08-30 12:27:07,303] [    INFO][0m - loss: 0.26220672, learning_rate: 2.8973154362416108e-05, global_step: 510, interval_runtime: 27.7892, interval_samples_per_second: 0.288, interval_steps_per_second: 0.36, epoch: 3.4228[0m
[32m[2022-08-30 12:27:08,601] [    INFO][0m - loss: 0.17625967, learning_rate: 2.8953020134228186e-05, global_step: 520, interval_runtime: 1.2978, interval_samples_per_second: 6.165, interval_steps_per_second: 7.706, epoch: 3.4899[0m
[32m[2022-08-30 12:27:09,903] [    INFO][0m - loss: 0.1717634, learning_rate: 2.893288590604027e-05, global_step: 530, interval_runtime: 1.3021, interval_samples_per_second: 6.144, interval_steps_per_second: 7.68, epoch: 3.557[0m
[32m[2022-08-30 12:27:11,194] [    INFO][0m - loss: 0.19955542, learning_rate: 2.891275167785235e-05, global_step: 540, interval_runtime: 1.291, interval_samples_per_second: 6.197, interval_steps_per_second: 7.746, epoch: 3.6242[0m
[32m[2022-08-30 12:27:12,468] [    INFO][0m - loss: 0.12114452, learning_rate: 2.889261744966443e-05, global_step: 550, interval_runtime: 1.2737, interval_samples_per_second: 6.281, interval_steps_per_second: 7.851, epoch: 3.6913[0m
[32m[2022-08-30 12:27:13,742] [    INFO][0m - loss: 0.23377347, learning_rate: 2.8872483221476512e-05, global_step: 560, interval_runtime: 1.274, interval_samples_per_second: 6.28, interval_steps_per_second: 7.849, epoch: 3.7584[0m
[32m[2022-08-30 12:27:15,017] [    INFO][0m - loss: 0.06054811, learning_rate: 2.885234899328859e-05, global_step: 570, interval_runtime: 1.2755, interval_samples_per_second: 6.272, interval_steps_per_second: 7.84, epoch: 3.8255[0m
[32m[2022-08-30 12:27:16,291] [    INFO][0m - loss: 0.1996737, learning_rate: 2.8832214765100673e-05, global_step: 580, interval_runtime: 1.274, interval_samples_per_second: 6.28, interval_steps_per_second: 7.85, epoch: 3.8926[0m
[32m[2022-08-30 12:27:17,563] [    INFO][0m - loss: 0.22140851, learning_rate: 2.8812080536912755e-05, global_step: 590, interval_runtime: 1.2719, interval_samples_per_second: 6.29, interval_steps_per_second: 7.863, epoch: 3.9597[0m
[32m[2022-08-30 12:27:18,843] [    INFO][0m - loss: 0.25690882, learning_rate: 2.879194630872483e-05, global_step: 600, interval_runtime: 1.28, interval_samples_per_second: 6.25, interval_steps_per_second: 7.813, epoch: 4.0268[0m
[32m[2022-08-30 12:27:18,843] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 12:27:18,844] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-08-30 12:27:18,844] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 12:27:18,844] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 12:27:18,844] [    INFO][0m -   Total prediction steps = 35[0m
[32m[2022-08-30 12:27:23,816] [    INFO][0m - eval_loss: 2.632861852645874, eval_accuracy: 0.517304189435337, eval_runtime: 4.9716, eval_samples_per_second: 220.857, eval_steps_per_second: 7.04, epoch: 4.0268[0m
[32m[2022-08-30 12:27:23,816] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-08-30 12:27:23,817] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 12:27:30,794] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-08-30 12:27:30,795] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-08-30 12:27:46,496] [    INFO][0m - loss: 0.12800354, learning_rate: 2.8771812080536913e-05, global_step: 610, interval_runtime: 27.6532, interval_samples_per_second: 0.289, interval_steps_per_second: 0.362, epoch: 4.094[0m
[32m[2022-08-30 12:27:47,767] [    INFO][0m - loss: 0.03336639, learning_rate: 2.8751677852348992e-05, global_step: 620, interval_runtime: 1.2704, interval_samples_per_second: 6.297, interval_steps_per_second: 7.872, epoch: 4.1611[0m
[32m[2022-08-30 12:27:49,039] [    INFO][0m - loss: 0.05529352, learning_rate: 2.8731543624161074e-05, global_step: 630, interval_runtime: 1.2719, interval_samples_per_second: 6.29, interval_steps_per_second: 7.862, epoch: 4.2282[0m
[32m[2022-08-30 12:27:50,311] [    INFO][0m - loss: 0.02834536, learning_rate: 2.8711409395973157e-05, global_step: 640, interval_runtime: 1.2725, interval_samples_per_second: 6.287, interval_steps_per_second: 7.858, epoch: 4.2953[0m
[32m[2022-08-30 12:27:51,583] [    INFO][0m - loss: 0.08363931, learning_rate: 2.8691275167785235e-05, global_step: 650, interval_runtime: 1.2724, interval_samples_per_second: 6.287, interval_steps_per_second: 7.859, epoch: 4.3624[0m
[32m[2022-08-30 12:27:52,856] [    INFO][0m - loss: 0.0372333, learning_rate: 2.8671140939597318e-05, global_step: 660, interval_runtime: 1.2724, interval_samples_per_second: 6.287, interval_steps_per_second: 7.859, epoch: 4.4295[0m
[32m[2022-08-30 12:27:54,127] [    INFO][0m - loss: 0.09245595, learning_rate: 2.8651006711409397e-05, global_step: 670, interval_runtime: 1.2715, interval_samples_per_second: 6.292, interval_steps_per_second: 7.865, epoch: 4.4966[0m
[32m[2022-08-30 12:27:55,398] [    INFO][0m - loss: 0.11273321, learning_rate: 2.863087248322148e-05, global_step: 680, interval_runtime: 1.2703, interval_samples_per_second: 6.298, interval_steps_per_second: 7.872, epoch: 4.5638[0m
[32m[2022-08-30 12:27:56,670] [    INFO][0m - loss: 0.24089947, learning_rate: 2.8610738255033558e-05, global_step: 690, interval_runtime: 1.2708, interval_samples_per_second: 6.295, interval_steps_per_second: 7.869, epoch: 4.6309[0m
[32m[2022-08-30 12:27:57,942] [    INFO][0m - loss: 0.00870084, learning_rate: 2.8590604026845637e-05, global_step: 700, interval_runtime: 1.2731, interval_samples_per_second: 6.284, interval_steps_per_second: 7.855, epoch: 4.698[0m
[32m[2022-08-30 12:27:57,942] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 12:27:57,942] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-08-30 12:27:57,942] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 12:27:57,942] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 12:27:57,942] [    INFO][0m -   Total prediction steps = 35[0m
[32m[2022-08-30 12:28:02,906] [    INFO][0m - eval_loss: 2.9813718795776367, eval_accuracy: 0.5200364298724954, eval_runtime: 4.9629, eval_samples_per_second: 221.243, eval_steps_per_second: 7.052, epoch: 4.698[0m
[32m[2022-08-30 12:28:02,906] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-700[0m
[32m[2022-08-30 12:28:02,906] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 12:28:10,054] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-700/tokenizer_config.json[0m
[32m[2022-08-30 12:28:10,054] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-700/special_tokens_map.json[0m
[32m[2022-08-30 12:28:25,575] [    INFO][0m - loss: 0.02906314, learning_rate: 2.857046979865772e-05, global_step: 710, interval_runtime: 27.6333, interval_samples_per_second: 0.29, interval_steps_per_second: 0.362, epoch: 4.7651[0m
[32m[2022-08-30 12:28:26,840] [    INFO][0m - loss: 0.15163188, learning_rate: 2.8550335570469798e-05, global_step: 720, interval_runtime: 1.2652, interval_samples_per_second: 6.323, interval_steps_per_second: 7.904, epoch: 4.8322[0m
[32m[2022-08-30 12:28:28,110] [    INFO][0m - loss: 0.10449286, learning_rate: 2.853020134228188e-05, global_step: 730, interval_runtime: 1.2695, interval_samples_per_second: 6.302, interval_steps_per_second: 7.877, epoch: 4.8993[0m
[32m[2022-08-30 12:28:29,380] [    INFO][0m - loss: 0.08885643, learning_rate: 2.8510067114093962e-05, global_step: 740, interval_runtime: 1.2703, interval_samples_per_second: 6.298, interval_steps_per_second: 7.872, epoch: 4.9664[0m
[32m[2022-08-30 12:28:30,648] [    INFO][0m - loss: 0.10005085, learning_rate: 2.848993288590604e-05, global_step: 750, interval_runtime: 1.2682, interval_samples_per_second: 6.308, interval_steps_per_second: 7.885, epoch: 5.0336[0m
[32m[2022-08-30 12:28:31,924] [    INFO][0m - loss: 0.00430895, learning_rate: 2.8469798657718123e-05, global_step: 760, interval_runtime: 1.2755, interval_samples_per_second: 6.272, interval_steps_per_second: 7.84, epoch: 5.1007[0m
[32m[2022-08-30 12:28:33,200] [    INFO][0m - loss: 0.14102327, learning_rate: 2.8449664429530202e-05, global_step: 770, interval_runtime: 1.2761, interval_samples_per_second: 6.269, interval_steps_per_second: 7.837, epoch: 5.1678[0m
[32m[2022-08-30 12:28:34,473] [    INFO][0m - loss: 0.00706, learning_rate: 2.8429530201342284e-05, global_step: 780, interval_runtime: 1.2726, interval_samples_per_second: 6.286, interval_steps_per_second: 7.858, epoch: 5.2349[0m
[32m[2022-08-30 12:28:35,744] [    INFO][0m - loss: 0.01439904, learning_rate: 2.8409395973154363e-05, global_step: 790, interval_runtime: 1.2718, interval_samples_per_second: 6.29, interval_steps_per_second: 7.863, epoch: 5.302[0m
[32m[2022-08-30 12:28:37,019] [    INFO][0m - loss: 0.00584139, learning_rate: 2.8389261744966442e-05, global_step: 800, interval_runtime: 1.2751, interval_samples_per_second: 6.274, interval_steps_per_second: 7.843, epoch: 5.3691[0m
[32m[2022-08-30 12:28:37,020] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 12:28:37,020] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-08-30 12:28:37,020] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 12:28:37,020] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 12:28:37,020] [    INFO][0m -   Total prediction steps = 35[0m
[32m[2022-08-30 12:28:41,997] [    INFO][0m - eval_loss: 3.0728554725646973, eval_accuracy: 0.5136612021857924, eval_runtime: 4.9768, eval_samples_per_second: 220.623, eval_steps_per_second: 7.033, epoch: 5.3691[0m
[32m[2022-08-30 12:28:41,997] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-800[0m
[32m[2022-08-30 12:28:41,997] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 12:28:48,845] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-800/tokenizer_config.json[0m
[32m[2022-08-30 12:28:48,845] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-800/special_tokens_map.json[0m
[32m[2022-08-30 12:29:04,364] [    INFO][0m - loss: 0.00604283, learning_rate: 2.8369127516778524e-05, global_step: 810, interval_runtime: 27.3443, interval_samples_per_second: 0.293, interval_steps_per_second: 0.366, epoch: 5.4362[0m
[32m[2022-08-30 12:29:05,632] [    INFO][0m - loss: 0.0055782, learning_rate: 2.8348993288590603e-05, global_step: 820, interval_runtime: 1.2684, interval_samples_per_second: 6.307, interval_steps_per_second: 7.884, epoch: 5.5034[0m
[32m[2022-08-30 12:29:06,903] [    INFO][0m - loss: 0.00698095, learning_rate: 2.8328859060402685e-05, global_step: 830, interval_runtime: 1.2705, interval_samples_per_second: 6.296, interval_steps_per_second: 7.871, epoch: 5.5705[0m
[32m[2022-08-30 12:29:08,170] [    INFO][0m - loss: 0.00795922, learning_rate: 2.8308724832214768e-05, global_step: 840, interval_runtime: 1.2673, interval_samples_per_second: 6.313, interval_steps_per_second: 7.891, epoch: 5.6376[0m
[32m[2022-08-30 12:29:09,445] [    INFO][0m - loss: 0.10088712, learning_rate: 2.8288590604026847e-05, global_step: 850, interval_runtime: 1.2751, interval_samples_per_second: 6.274, interval_steps_per_second: 7.843, epoch: 5.7047[0m
[32m[2022-08-30 12:29:10,721] [    INFO][0m - loss: 0.01366443, learning_rate: 2.826845637583893e-05, global_step: 860, interval_runtime: 1.2759, interval_samples_per_second: 6.27, interval_steps_per_second: 7.838, epoch: 5.7718[0m
[32m[2022-08-30 12:29:11,994] [    INFO][0m - loss: 0.00288684, learning_rate: 2.8248322147651008e-05, global_step: 870, interval_runtime: 1.2731, interval_samples_per_second: 6.284, interval_steps_per_second: 7.855, epoch: 5.8389[0m
[32m[2022-08-30 12:29:13,268] [    INFO][0m - loss: 0.00799489, learning_rate: 2.8228187919463087e-05, global_step: 880, interval_runtime: 1.2738, interval_samples_per_second: 6.28, interval_steps_per_second: 7.851, epoch: 5.906[0m
[32m[2022-08-30 12:29:14,544] [    INFO][0m - loss: 0.00195275, learning_rate: 2.820805369127517e-05, global_step: 890, interval_runtime: 1.2762, interval_samples_per_second: 6.268, interval_steps_per_second: 7.836, epoch: 5.9732[0m
[32m[2022-08-30 12:29:15,812] [    INFO][0m - loss: 0.00066224, learning_rate: 2.8187919463087248e-05, global_step: 900, interval_runtime: 1.2677, interval_samples_per_second: 6.311, interval_steps_per_second: 7.888, epoch: 6.0403[0m
[32m[2022-08-30 12:29:15,812] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 12:29:15,812] [    INFO][0m -   Num examples = 1098[0m
[32m[2022-08-30 12:29:15,812] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 12:29:15,812] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 12:29:15,812] [    INFO][0m -   Total prediction steps = 35[0m
[32m[2022-08-30 12:29:20,760] [    INFO][0m - eval_loss: 3.309741497039795, eval_accuracy: 0.5255009107468124, eval_runtime: 4.9478, eval_samples_per_second: 221.916, eval_steps_per_second: 7.074, epoch: 6.0403[0m
[32m[2022-08-30 12:29:20,761] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-900[0m
[32m[2022-08-30 12:29:20,761] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 12:29:27,593] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-900/tokenizer_config.json[0m
[32m[2022-08-30 12:29:27,593] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-900/special_tokens_map.json[0m
[32m[2022-08-30 12:29:41,746] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-30 12:29:41,746] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-500 (score: 0.5391621129326047).[0m
[32m[2022-08-30 12:29:43,695] [    INFO][0m - train_runtime: 356.519, train_samples_per_second: 332.381, train_steps_per_second: 41.793, train_loss: 0.5905750126696916, epoch: 6.0403[0m
[32m[2022-08-30 12:29:43,698] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-30 12:29:43,698] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 12:29:50,907] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-30 12:29:50,907] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-30 12:29:50,909] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-30 12:29:50,909] [    INFO][0m -   epoch                    =     6.0403[0m
[32m[2022-08-30 12:29:50,910] [    INFO][0m -   train_loss               =     0.5906[0m
[32m[2022-08-30 12:29:50,910] [    INFO][0m -   train_runtime            = 0:05:56.51[0m
[32m[2022-08-30 12:29:50,910] [    INFO][0m -   train_samples_per_second =    332.381[0m
[32m[2022-08-30 12:29:50,910] [    INFO][0m -   train_steps_per_second   =     41.793[0m
[32m[2022-08-30 12:29:50,915] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-30 12:29:50,915] [    INFO][0m -   Num examples = 2010[0m
[32m[2022-08-30 12:29:50,915] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 12:29:50,915] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 12:29:50,915] [    INFO][0m -   Total prediction steps = 63[0m
[32m[2022-08-30 12:30:00,023] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-30 12:30:00,023] [    INFO][0m -   test_accuracy           =     0.5433[0m
[32m[2022-08-30 12:30:00,023] [    INFO][0m -   test_loss               =     2.2575[0m
[32m[2022-08-30 12:30:00,023] [    INFO][0m -   test_runtime            = 0:00:09.10[0m
[32m[2022-08-30 12:30:00,024] [    INFO][0m -   test_samples_per_second =    220.686[0m
[32m[2022-08-30 12:30:00,024] [    INFO][0m -   test_steps_per_second   =      6.917[0m
[32m[2022-08-30 12:30:00,024] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-30 12:30:00,024] [    INFO][0m -   Num examples = 1500[0m
[32m[2022-08-30 12:30:00,024] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 12:30:00,024] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 12:30:00,024] [    INFO][0m -   Total prediction steps = 47[0m
[32m[2022-08-30 12:30:07,739] [    INFO][0m - Predictions for tnewsf saved to ./fewclue_submit_examples.[0m
metric <function main.<locals>.compute_metrics at 0x7fbefe50a3a0>
 
==========
iflytek
==========
 
[33m[2022-08-30 12:30:12,043] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-30 12:30:12,043] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-30 12:30:12,043] [    INFO][0m - ============================================================[0m
[32m[2022-08-30 12:30:12,043] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-30 12:30:12,043] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-30 12:30:12,043] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-30 12:30:12,043] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-30 12:30:12,044] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-08-30 12:30:12,044] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-30 12:30:12,044] [    INFO][0m - model_name_or_path            :hfl/roberta-wwm-ext-large[0m
[32m[2022-08-30 12:30:12,044] [    INFO][0m - [0m
[32m[2022-08-30 12:30:12,044] [    INFO][0m - ============================================================[0m
[32m[2022-08-30 12:30:12,044] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-30 12:30:12,044] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-30 12:30:12,044] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-30 12:30:12,044] [    INFO][0m - prompt                        :{'mask'}{'mask'}{'hard':'APP更新日志：'}{'text':'text_a'}[0m
[32m[2022-08-30 12:30:12,044] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-30 12:30:12,044] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-30 12:30:12,044] [    INFO][0m - task_name                     :iflytek[0m
[32m[2022-08-30 12:30:12,044] [    INFO][0m - [0m
[32m[2022-08-30 12:30:12,045] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/hfl/roberta-wwm-ext-large/roberta_chn_large.pdparams[0m
W0830 12:30:12.046085  1452 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0830 12:30:12.051365  1452 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-30 12:30:17,364] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/hfl/roberta-wwm-ext-large/vocab.txt[0m
[32m[2022-08-30 12:30:17,377] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/hfl/roberta-wwm-ext-large/tokenizer_config.json[0m
[32m[2022-08-30 12:30:17,377] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/hfl/roberta-wwm-ext-large/special_tokens_map.json[0m
[32m[2022-08-30 12:30:17,378] [    INFO][0m - Using template: [{'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'APP更新日志：'}, {'add_prefix_space': '', 'text': 'text_a'}][0m
2022-08-30 12:30:17,404 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-30 12:30:17,573] [    INFO][0m - ============================================================[0m
[32m[2022-08-30 12:30:17,573] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-30 12:30:17,573] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-30 12:30:17,574] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-30 12:30:17,574] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-30 12:30:17,574] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-30 12:30:17,574] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-30 12:30:17,574] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-30 12:30:17,574] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-30 12:30:17,574] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-30 12:30:17,574] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-30 12:30:17,574] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-30 12:30:17,574] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-30 12:30:17,574] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-30 12:30:17,575] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-30 12:30:17,575] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-30 12:30:17,575] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-30 12:30:17,575] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-30 12:30:17,575] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-30 12:30:17,575] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-30 12:30:17,575] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-30 12:30:17,575] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-30 12:30:17,575] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-30 12:30:17,575] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-30 12:30:17,575] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-30 12:30:17,575] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-30 12:30:17,575] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-30 12:30:17,576] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-30 12:30:17,576] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-30 12:30:17,576] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-30 12:30:17,576] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-30 12:30:17,576] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-30 12:30:17,576] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-30 12:30:17,576] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-30 12:30:17,576] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-30 12:30:17,576] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-30 12:30:17,576] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-30 12:30:17,576] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug30_12-30-12_instance-3bwob41y-01[0m
[32m[2022-08-30 12:30:17,576] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-30 12:30:17,576] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-30 12:30:17,576] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-30 12:30:17,577] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-30 12:30:17,577] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-30 12:30:17,577] [    INFO][0m - max_seq_length                :320[0m
[32m[2022-08-30 12:30:17,577] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-30 12:30:17,577] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-30 12:30:17,577] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-30 12:30:17,577] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-30 12:30:17,577] [    INFO][0m - num_train_epochs              :100.0[0m
[32m[2022-08-30 12:30:17,577] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-30 12:30:17,577] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-30 12:30:17,577] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-30 12:30:17,577] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-30 12:30:17,577] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-30 12:30:17,578] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-30 12:30:17,578] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-30 12:30:17,578] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-30 12:30:17,578] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-30 12:30:17,578] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-30 12:30:17,578] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-30 12:30:17,578] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-30 12:30:17,578] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-30 12:30:17,578] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-30 12:30:17,578] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-30 12:30:17,578] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-30 12:30:17,578] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-30 12:30:17,578] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-30 12:30:17,578] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-30 12:30:17,579] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-30 12:30:17,579] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-30 12:30:17,579] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-30 12:30:17,579] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-30 12:30:17,579] [    INFO][0m - seed                          :42[0m
[32m[2022-08-30 12:30:17,579] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-30 12:30:17,579] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-30 12:30:17,579] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-30 12:30:17,579] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-30 12:30:17,579] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-30 12:30:17,579] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-30 12:30:17,579] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-30 12:30:17,579] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-30 12:30:17,579] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-30 12:30:17,580] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-30 12:30:17,580] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-30 12:30:17,580] [    INFO][0m - [0m
[32m[2022-08-30 12:30:17,583] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-30 12:30:17,583] [    INFO][0m -   Num examples = 3024[0m
[32m[2022-08-30 12:30:17,583] [    INFO][0m -   Num Epochs = 100[0m
[32m[2022-08-30 12:30:17,583] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-30 12:30:17,583] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-30 12:30:17,583] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-30 12:30:17,583] [    INFO][0m -   Total optimization steps = 37800.0[0m
[32m[2022-08-30 12:30:17,583] [    INFO][0m -   Total num train samples = 302400[0m
[33m[2022-08-30 12:30:17,593] [ WARNING][0m - Token indices sequence length is longer than the specified maximum sequence length for this model (637 > 512). Running this sequence through the model will result in indexing errors[0m
[32m[2022-08-30 12:30:23,321] [    INFO][0m - loss: 5.03960152, learning_rate: 2.999206349206349e-05, global_step: 10, interval_runtime: 5.736, interval_samples_per_second: 1.395, interval_steps_per_second: 1.743, epoch: 0.0265[0m
[32m[2022-08-30 12:30:27,991] [    INFO][0m - loss: 5.09981766, learning_rate: 2.9984126984126986e-05, global_step: 20, interval_runtime: 4.6704, interval_samples_per_second: 1.713, interval_steps_per_second: 2.141, epoch: 0.0529[0m
[32m[2022-08-30 12:30:32,655] [    INFO][0m - loss: 4.49874954, learning_rate: 2.9976190476190477e-05, global_step: 30, interval_runtime: 4.664, interval_samples_per_second: 1.715, interval_steps_per_second: 2.144, epoch: 0.0794[0m
[32m[2022-08-30 12:30:37,307] [    INFO][0m - loss: 4.13460159, learning_rate: 2.9968253968253967e-05, global_step: 40, interval_runtime: 4.6526, interval_samples_per_second: 1.719, interval_steps_per_second: 2.149, epoch: 0.1058[0m
[32m[2022-08-30 12:30:42,015] [    INFO][0m - loss: 3.81334267, learning_rate: 2.996031746031746e-05, global_step: 50, interval_runtime: 4.708, interval_samples_per_second: 1.699, interval_steps_per_second: 2.124, epoch: 0.1323[0m
[32m[2022-08-30 12:30:46,689] [    INFO][0m - loss: 3.83369522, learning_rate: 2.9952380952380952e-05, global_step: 60, interval_runtime: 4.6737, interval_samples_per_second: 1.712, interval_steps_per_second: 2.14, epoch: 0.1587[0m
[32m[2022-08-30 12:30:51,334] [    INFO][0m - loss: 3.57679138, learning_rate: 2.9944444444444443e-05, global_step: 70, interval_runtime: 4.6448, interval_samples_per_second: 1.722, interval_steps_per_second: 2.153, epoch: 0.1852[0m
[32m[2022-08-30 12:30:55,994] [    INFO][0m - loss: 2.85429478, learning_rate: 2.9936507936507937e-05, global_step: 80, interval_runtime: 4.6602, interval_samples_per_second: 1.717, interval_steps_per_second: 2.146, epoch: 0.2116[0m
[32m[2022-08-30 12:31:00,652] [    INFO][0m - loss: 2.91746254, learning_rate: 2.9928571428571428e-05, global_step: 90, interval_runtime: 4.6585, interval_samples_per_second: 1.717, interval_steps_per_second: 2.147, epoch: 0.2381[0m
[32m[2022-08-30 12:31:05,331] [    INFO][0m - loss: 2.58230743, learning_rate: 2.992063492063492e-05, global_step: 100, interval_runtime: 4.6781, interval_samples_per_second: 1.71, interval_steps_per_second: 2.138, epoch: 0.2646[0m
[32m[2022-08-30 12:31:05,332] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 12:31:05,332] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-30 12:31:05,332] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 12:31:05,332] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 12:31:05,332] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-30 12:31:34,758] [    INFO][0m - eval_loss: 2.581101894378662, eval_accuracy: 0.41369264384559357, eval_runtime: 29.4258, eval_samples_per_second: 46.66, eval_steps_per_second: 1.461, epoch: 0.2646[0m
[32m[2022-08-30 12:31:34,759] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-30 12:31:34,759] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 12:31:41,697] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-30 12:31:41,697] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-30 12:32:00,575] [    INFO][0m - loss: 2.63411884, learning_rate: 2.9912698412698416e-05, global_step: 110, interval_runtime: 55.2444, interval_samples_per_second: 0.145, interval_steps_per_second: 0.181, epoch: 0.291[0m
[32m[2022-08-30 12:32:05,241] [    INFO][0m - loss: 2.51130314, learning_rate: 2.9904761904761907e-05, global_step: 120, interval_runtime: 4.6657, interval_samples_per_second: 1.715, interval_steps_per_second: 2.143, epoch: 0.3175[0m
[32m[2022-08-30 12:32:09,899] [    INFO][0m - loss: 2.56297169, learning_rate: 2.9896825396825398e-05, global_step: 130, interval_runtime: 4.6589, interval_samples_per_second: 1.717, interval_steps_per_second: 2.146, epoch: 0.3439[0m
[32m[2022-08-30 12:32:14,590] [    INFO][0m - loss: 2.46824493, learning_rate: 2.9888888888888892e-05, global_step: 140, interval_runtime: 4.6907, interval_samples_per_second: 1.705, interval_steps_per_second: 2.132, epoch: 0.3704[0m
[32m[2022-08-30 12:32:19,272] [    INFO][0m - loss: 2.60955582, learning_rate: 2.9880952380952383e-05, global_step: 150, interval_runtime: 4.6822, interval_samples_per_second: 1.709, interval_steps_per_second: 2.136, epoch: 0.3968[0m
[32m[2022-08-30 12:32:23,937] [    INFO][0m - loss: 2.87306805, learning_rate: 2.9873015873015874e-05, global_step: 160, interval_runtime: 4.6644, interval_samples_per_second: 1.715, interval_steps_per_second: 2.144, epoch: 0.4233[0m
[32m[2022-08-30 12:32:28,635] [    INFO][0m - loss: 2.29784088, learning_rate: 2.9865079365079368e-05, global_step: 170, interval_runtime: 4.698, interval_samples_per_second: 1.703, interval_steps_per_second: 2.129, epoch: 0.4497[0m
[32m[2022-08-30 12:32:33,317] [    INFO][0m - loss: 2.6703434, learning_rate: 2.985714285714286e-05, global_step: 180, interval_runtime: 4.6825, interval_samples_per_second: 1.709, interval_steps_per_second: 2.136, epoch: 0.4762[0m
[32m[2022-08-30 12:32:38,015] [    INFO][0m - loss: 2.35959415, learning_rate: 2.984920634920635e-05, global_step: 190, interval_runtime: 4.6975, interval_samples_per_second: 1.703, interval_steps_per_second: 2.129, epoch: 0.5026[0m
[32m[2022-08-30 12:32:42,682] [    INFO][0m - loss: 2.13110371, learning_rate: 2.984126984126984e-05, global_step: 200, interval_runtime: 4.6674, interval_samples_per_second: 1.714, interval_steps_per_second: 2.143, epoch: 0.5291[0m
[32m[2022-08-30 12:32:42,683] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 12:32:42,683] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-30 12:32:42,683] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 12:32:42,683] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 12:32:42,683] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-30 12:33:12,117] [    INFO][0m - eval_loss: 2.364745616912842, eval_accuracy: 0.4144209759650401, eval_runtime: 29.4338, eval_samples_per_second: 46.647, eval_steps_per_second: 1.461, epoch: 0.5291[0m
[32m[2022-08-30 12:33:12,118] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-30 12:33:12,118] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 12:33:19,320] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-30 12:33:19,320] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-30 12:33:38,732] [    INFO][0m - loss: 2.4059288, learning_rate: 2.9833333333333335e-05, global_step: 210, interval_runtime: 56.0496, interval_samples_per_second: 0.143, interval_steps_per_second: 0.178, epoch: 0.5556[0m
[32m[2022-08-30 12:33:43,391] [    INFO][0m - loss: 2.31424255, learning_rate: 2.9825396825396825e-05, global_step: 220, interval_runtime: 4.659, interval_samples_per_second: 1.717, interval_steps_per_second: 2.146, epoch: 0.582[0m
[32m[2022-08-30 12:33:48,076] [    INFO][0m - loss: 2.14993782, learning_rate: 2.9817460317460316e-05, global_step: 230, interval_runtime: 4.6849, interval_samples_per_second: 1.708, interval_steps_per_second: 2.135, epoch: 0.6085[0m
[32m[2022-08-30 12:33:52,756] [    INFO][0m - loss: 2.10771713, learning_rate: 2.980952380952381e-05, global_step: 240, interval_runtime: 4.6799, interval_samples_per_second: 1.709, interval_steps_per_second: 2.137, epoch: 0.6349[0m
[32m[2022-08-30 12:33:57,436] [    INFO][0m - loss: 2.2030262, learning_rate: 2.98015873015873e-05, global_step: 250, interval_runtime: 4.6808, interval_samples_per_second: 1.709, interval_steps_per_second: 2.136, epoch: 0.6614[0m
[32m[2022-08-30 12:34:02,124] [    INFO][0m - loss: 2.01003132, learning_rate: 2.9793650793650792e-05, global_step: 260, interval_runtime: 4.6873, interval_samples_per_second: 1.707, interval_steps_per_second: 2.133, epoch: 0.6878[0m
[32m[2022-08-30 12:34:06,781] [    INFO][0m - loss: 1.97281342, learning_rate: 2.9785714285714286e-05, global_step: 270, interval_runtime: 4.6573, interval_samples_per_second: 1.718, interval_steps_per_second: 2.147, epoch: 0.7143[0m
[32m[2022-08-30 12:34:11,465] [    INFO][0m - loss: 2.2248476, learning_rate: 2.9777777777777777e-05, global_step: 280, interval_runtime: 4.6838, interval_samples_per_second: 1.708, interval_steps_per_second: 2.135, epoch: 0.7407[0m
[32m[2022-08-30 12:34:16,138] [    INFO][0m - loss: 2.11026516, learning_rate: 2.9769841269841268e-05, global_step: 290, interval_runtime: 4.6735, interval_samples_per_second: 1.712, interval_steps_per_second: 2.14, epoch: 0.7672[0m
[32m[2022-08-30 12:34:20,817] [    INFO][0m - loss: 1.82513657, learning_rate: 2.9761904761904762e-05, global_step: 300, interval_runtime: 4.6783, interval_samples_per_second: 1.71, interval_steps_per_second: 2.138, epoch: 0.7937[0m
[32m[2022-08-30 12:34:20,817] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 12:34:20,817] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-30 12:34:20,817] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 12:34:20,817] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 12:34:20,817] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-30 12:34:50,269] [    INFO][0m - eval_loss: 2.1217093467712402, eval_accuracy: 0.4668608885651857, eval_runtime: 29.451, eval_samples_per_second: 46.62, eval_steps_per_second: 1.46, epoch: 0.7937[0m
[32m[2022-08-30 12:34:50,269] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-30 12:34:50,269] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 12:34:57,225] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-30 12:34:57,226] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-30 12:35:15,882] [    INFO][0m - loss: 2.35086994, learning_rate: 2.9753968253968256e-05, global_step: 310, interval_runtime: 55.0655, interval_samples_per_second: 0.145, interval_steps_per_second: 0.182, epoch: 0.8201[0m
[32m[2022-08-30 12:35:20,552] [    INFO][0m - loss: 1.81427517, learning_rate: 2.9746031746031747e-05, global_step: 320, interval_runtime: 4.6697, interval_samples_per_second: 1.713, interval_steps_per_second: 2.141, epoch: 0.8466[0m
[32m[2022-08-30 12:35:25,216] [    INFO][0m - loss: 2.24639416, learning_rate: 2.973809523809524e-05, global_step: 330, interval_runtime: 4.6639, interval_samples_per_second: 1.715, interval_steps_per_second: 2.144, epoch: 0.873[0m
[32m[2022-08-30 12:35:29,898] [    INFO][0m - loss: 2.24877014, learning_rate: 2.9730158730158732e-05, global_step: 340, interval_runtime: 4.6819, interval_samples_per_second: 1.709, interval_steps_per_second: 2.136, epoch: 0.8995[0m
[32m[2022-08-30 12:35:34,574] [    INFO][0m - loss: 2.25418491, learning_rate: 2.9722222222222223e-05, global_step: 350, interval_runtime: 4.6765, interval_samples_per_second: 1.711, interval_steps_per_second: 2.138, epoch: 0.9259[0m
[32m[2022-08-30 12:35:39,250] [    INFO][0m - loss: 2.54186497, learning_rate: 2.9714285714285717e-05, global_step: 360, interval_runtime: 4.6756, interval_samples_per_second: 1.711, interval_steps_per_second: 2.139, epoch: 0.9524[0m
[32m[2022-08-30 12:35:43,938] [    INFO][0m - loss: 2.17436428, learning_rate: 2.9706349206349208e-05, global_step: 370, interval_runtime: 4.6881, interval_samples_per_second: 1.706, interval_steps_per_second: 2.133, epoch: 0.9788[0m
[32m[2022-08-30 12:35:48,707] [    INFO][0m - loss: 1.88073311, learning_rate: 2.96984126984127e-05, global_step: 380, interval_runtime: 4.7688, interval_samples_per_second: 1.678, interval_steps_per_second: 2.097, epoch: 1.0053[0m
[32m[2022-08-30 12:35:53,390] [    INFO][0m - loss: 1.33644409, learning_rate: 2.9690476190476193e-05, global_step: 390, interval_runtime: 4.6828, interval_samples_per_second: 1.708, interval_steps_per_second: 2.135, epoch: 1.0317[0m
[32m[2022-08-30 12:35:58,072] [    INFO][0m - loss: 1.29815197, learning_rate: 2.9682539682539683e-05, global_step: 400, interval_runtime: 4.6829, interval_samples_per_second: 1.708, interval_steps_per_second: 2.135, epoch: 1.0582[0m
[32m[2022-08-30 12:35:58,073] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 12:35:58,073] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-30 12:35:58,073] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 12:35:58,073] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 12:35:58,073] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-30 12:36:27,527] [    INFO][0m - eval_loss: 2.0455217361450195, eval_accuracy: 0.4967225054624909, eval_runtime: 29.4536, eval_samples_per_second: 46.616, eval_steps_per_second: 1.46, epoch: 1.0582[0m
[32m[2022-08-30 12:36:27,527] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-30 12:36:27,528] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 12:36:34,827] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-30 12:36:34,827] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-30 12:36:54,687] [    INFO][0m - loss: 1.39179478, learning_rate: 2.9674603174603174e-05, global_step: 410, interval_runtime: 56.6146, interval_samples_per_second: 0.141, interval_steps_per_second: 0.177, epoch: 1.0847[0m
[32m[2022-08-30 12:36:59,383] [    INFO][0m - loss: 1.27212276, learning_rate: 2.966666666666667e-05, global_step: 420, interval_runtime: 4.6956, interval_samples_per_second: 1.704, interval_steps_per_second: 2.13, epoch: 1.1111[0m
[32m[2022-08-30 12:37:04,068] [    INFO][0m - loss: 1.2270031, learning_rate: 2.965873015873016e-05, global_step: 430, interval_runtime: 4.6855, interval_samples_per_second: 1.707, interval_steps_per_second: 2.134, epoch: 1.1376[0m
[32m[2022-08-30 12:37:08,742] [    INFO][0m - loss: 1.29164295, learning_rate: 2.965079365079365e-05, global_step: 440, interval_runtime: 4.6731, interval_samples_per_second: 1.712, interval_steps_per_second: 2.14, epoch: 1.164[0m
[32m[2022-08-30 12:37:13,438] [    INFO][0m - loss: 1.26934357, learning_rate: 2.9642857142857144e-05, global_step: 450, interval_runtime: 4.6966, interval_samples_per_second: 1.703, interval_steps_per_second: 2.129, epoch: 1.1905[0m
[32m[2022-08-30 12:37:18,133] [    INFO][0m - loss: 1.44727964, learning_rate: 2.9634920634920635e-05, global_step: 460, interval_runtime: 4.6946, interval_samples_per_second: 1.704, interval_steps_per_second: 2.13, epoch: 1.2169[0m
[32m[2022-08-30 12:37:22,816] [    INFO][0m - loss: 1.4508009, learning_rate: 2.9626984126984126e-05, global_step: 470, interval_runtime: 4.6832, interval_samples_per_second: 1.708, interval_steps_per_second: 2.135, epoch: 1.2434[0m
[32m[2022-08-30 12:37:27,503] [    INFO][0m - loss: 1.58012819, learning_rate: 2.961904761904762e-05, global_step: 480, interval_runtime: 4.6874, interval_samples_per_second: 1.707, interval_steps_per_second: 2.133, epoch: 1.2698[0m
[32m[2022-08-30 12:37:32,184] [    INFO][0m - loss: 1.41897354, learning_rate: 2.961111111111111e-05, global_step: 490, interval_runtime: 4.6803, interval_samples_per_second: 1.709, interval_steps_per_second: 2.137, epoch: 1.2963[0m
[32m[2022-08-30 12:37:36,859] [    INFO][0m - loss: 1.54229832, learning_rate: 2.96031746031746e-05, global_step: 500, interval_runtime: 4.676, interval_samples_per_second: 1.711, interval_steps_per_second: 2.139, epoch: 1.3228[0m
[32m[2022-08-30 12:37:36,860] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 12:37:36,860] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-30 12:37:36,860] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 12:37:36,860] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 12:37:36,860] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-30 12:38:06,329] [    INFO][0m - eval_loss: 2.0658822059631348, eval_accuracy: 0.49745083758193737, eval_runtime: 29.4682, eval_samples_per_second: 46.593, eval_steps_per_second: 1.459, epoch: 1.3228[0m
[32m[2022-08-30 12:38:06,329] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-08-30 12:38:06,330] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 12:38:13,635] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-08-30 12:38:13,636] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-08-30 12:38:32,689] [    INFO][0m - loss: 1.82508736, learning_rate: 2.95952380952381e-05, global_step: 510, interval_runtime: 55.8296, interval_samples_per_second: 0.143, interval_steps_per_second: 0.179, epoch: 1.3492[0m
[32m[2022-08-30 12:38:37,355] [    INFO][0m - loss: 1.35064611, learning_rate: 2.958730158730159e-05, global_step: 520, interval_runtime: 4.6657, interval_samples_per_second: 1.715, interval_steps_per_second: 2.143, epoch: 1.3757[0m
[32m[2022-08-30 12:38:42,040] [    INFO][0m - loss: 1.36500454, learning_rate: 2.957936507936508e-05, global_step: 530, interval_runtime: 4.6849, interval_samples_per_second: 1.708, interval_steps_per_second: 2.135, epoch: 1.4021[0m
[32m[2022-08-30 12:38:46,750] [    INFO][0m - loss: 1.41773319, learning_rate: 2.9571428571428575e-05, global_step: 540, interval_runtime: 4.7098, interval_samples_per_second: 1.699, interval_steps_per_second: 2.123, epoch: 1.4286[0m
[32m[2022-08-30 12:38:51,448] [    INFO][0m - loss: 1.42805719, learning_rate: 2.9563492063492066e-05, global_step: 550, interval_runtime: 4.6985, interval_samples_per_second: 1.703, interval_steps_per_second: 2.128, epoch: 1.455[0m
[32m[2022-08-30 12:38:56,124] [    INFO][0m - loss: 1.30429678, learning_rate: 2.9555555555555556e-05, global_step: 560, interval_runtime: 4.6758, interval_samples_per_second: 1.711, interval_steps_per_second: 2.139, epoch: 1.4815[0m
[32m[2022-08-30 12:39:00,804] [    INFO][0m - loss: 1.60746765, learning_rate: 2.954761904761905e-05, global_step: 570, interval_runtime: 4.6799, interval_samples_per_second: 1.709, interval_steps_per_second: 2.137, epoch: 1.5079[0m
[32m[2022-08-30 12:39:05,493] [    INFO][0m - loss: 1.67181358, learning_rate: 2.953968253968254e-05, global_step: 580, interval_runtime: 4.6897, interval_samples_per_second: 1.706, interval_steps_per_second: 2.132, epoch: 1.5344[0m
[32m[2022-08-30 12:39:10,177] [    INFO][0m - loss: 1.49697723, learning_rate: 2.9531746031746032e-05, global_step: 590, interval_runtime: 4.6837, interval_samples_per_second: 1.708, interval_steps_per_second: 2.135, epoch: 1.5608[0m
[32m[2022-08-30 12:39:14,873] [    INFO][0m - loss: 1.50356388, learning_rate: 2.9523809523809523e-05, global_step: 600, interval_runtime: 4.6956, interval_samples_per_second: 1.704, interval_steps_per_second: 2.13, epoch: 1.5873[0m
[32m[2022-08-30 12:39:14,873] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 12:39:14,873] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-30 12:39:14,873] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 12:39:14,873] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 12:39:14,873] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-30 12:39:44,322] [    INFO][0m - eval_loss: 2.0971453189849854, eval_accuracy: 0.5200291332847778, eval_runtime: 29.4482, eval_samples_per_second: 46.624, eval_steps_per_second: 1.46, epoch: 1.5873[0m
[32m[2022-08-30 12:39:44,323] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-08-30 12:39:44,323] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 12:39:51,338] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-08-30 12:39:51,339] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-08-30 12:40:10,200] [    INFO][0m - loss: 1.46392145, learning_rate: 2.9515873015873017e-05, global_step: 610, interval_runtime: 55.3273, interval_samples_per_second: 0.145, interval_steps_per_second: 0.181, epoch: 1.6138[0m
[32m[2022-08-30 12:40:14,858] [    INFO][0m - loss: 1.67183495, learning_rate: 2.9507936507936508e-05, global_step: 620, interval_runtime: 4.6584, interval_samples_per_second: 1.717, interval_steps_per_second: 2.147, epoch: 1.6402[0m
[32m[2022-08-30 12:40:19,521] [    INFO][0m - loss: 1.1869669, learning_rate: 2.95e-05, global_step: 630, interval_runtime: 4.6625, interval_samples_per_second: 1.716, interval_steps_per_second: 2.145, epoch: 1.6667[0m
[32m[2022-08-30 12:40:24,225] [    INFO][0m - loss: 1.74318218, learning_rate: 2.9492063492063493e-05, global_step: 640, interval_runtime: 4.7039, interval_samples_per_second: 1.701, interval_steps_per_second: 2.126, epoch: 1.6931[0m
[32m[2022-08-30 12:40:28,925] [    INFO][0m - loss: 1.46052208, learning_rate: 2.9484126984126984e-05, global_step: 650, interval_runtime: 4.7001, interval_samples_per_second: 1.702, interval_steps_per_second: 2.128, epoch: 1.7196[0m
[32m[2022-08-30 12:40:33,626] [    INFO][0m - loss: 1.53076286, learning_rate: 2.9476190476190475e-05, global_step: 660, interval_runtime: 4.7014, interval_samples_per_second: 1.702, interval_steps_per_second: 2.127, epoch: 1.746[0m
[32m[2022-08-30 12:40:38,291] [    INFO][0m - loss: 1.74583454, learning_rate: 2.946825396825397e-05, global_step: 670, interval_runtime: 4.6649, interval_samples_per_second: 1.715, interval_steps_per_second: 2.144, epoch: 1.7725[0m
[32m[2022-08-30 12:40:42,966] [    INFO][0m - loss: 1.62646217, learning_rate: 2.946031746031746e-05, global_step: 680, interval_runtime: 4.6747, interval_samples_per_second: 1.711, interval_steps_per_second: 2.139, epoch: 1.7989[0m
[32m[2022-08-30 12:40:47,651] [    INFO][0m - loss: 1.49436293, learning_rate: 2.945238095238095e-05, global_step: 690, interval_runtime: 4.6848, interval_samples_per_second: 1.708, interval_steps_per_second: 2.135, epoch: 1.8254[0m
[32m[2022-08-30 12:40:52,334] [    INFO][0m - loss: 1.34315281, learning_rate: 2.9444444444444445e-05, global_step: 700, interval_runtime: 4.6834, interval_samples_per_second: 1.708, interval_steps_per_second: 2.135, epoch: 1.8519[0m
[32m[2022-08-30 12:40:52,335] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 12:40:52,335] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-30 12:40:52,335] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 12:40:52,335] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 12:40:52,335] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-30 12:41:21,859] [    INFO][0m - eval_loss: 2.1053595542907715, eval_accuracy: 0.4967225054624909, eval_runtime: 29.5233, eval_samples_per_second: 46.506, eval_steps_per_second: 1.456, epoch: 1.8519[0m
[32m[2022-08-30 12:41:21,859] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-700[0m
[32m[2022-08-30 12:41:21,859] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 12:41:29,124] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-700/tokenizer_config.json[0m
[32m[2022-08-30 12:41:29,125] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-700/special_tokens_map.json[0m
[32m[2022-08-30 12:41:48,030] [    INFO][0m - loss: 1.4622839, learning_rate: 2.943650793650794e-05, global_step: 710, interval_runtime: 55.696, interval_samples_per_second: 0.144, interval_steps_per_second: 0.18, epoch: 1.8783[0m
[32m[2022-08-30 12:41:52,687] [    INFO][0m - loss: 1.4092392, learning_rate: 2.942857142857143e-05, global_step: 720, interval_runtime: 4.6571, interval_samples_per_second: 1.718, interval_steps_per_second: 2.147, epoch: 1.9048[0m
[32m[2022-08-30 12:41:57,377] [    INFO][0m - loss: 1.65408344, learning_rate: 2.9420634920634924e-05, global_step: 730, interval_runtime: 4.6899, interval_samples_per_second: 1.706, interval_steps_per_second: 2.132, epoch: 1.9312[0m
[32m[2022-08-30 12:42:02,062] [    INFO][0m - loss: 1.41478977, learning_rate: 2.9412698412698414e-05, global_step: 740, interval_runtime: 4.6844, interval_samples_per_second: 1.708, interval_steps_per_second: 2.135, epoch: 1.9577[0m
[32m[2022-08-30 12:42:06,784] [    INFO][0m - loss: 1.26123257, learning_rate: 2.9404761904761905e-05, global_step: 750, interval_runtime: 4.7225, interval_samples_per_second: 1.694, interval_steps_per_second: 2.118, epoch: 1.9841[0m
[32m[2022-08-30 12:42:11,530] [    INFO][0m - loss: 1.20551443, learning_rate: 2.93968253968254e-05, global_step: 760, interval_runtime: 4.7463, interval_samples_per_second: 1.686, interval_steps_per_second: 2.107, epoch: 2.0106[0m
[32m[2022-08-30 12:42:16,236] [    INFO][0m - loss: 0.65954289, learning_rate: 2.938888888888889e-05, global_step: 770, interval_runtime: 4.7051, interval_samples_per_second: 1.7, interval_steps_per_second: 2.125, epoch: 2.037[0m
[32m[2022-08-30 12:42:20,946] [    INFO][0m - loss: 0.78735795, learning_rate: 2.938095238095238e-05, global_step: 780, interval_runtime: 4.7108, interval_samples_per_second: 1.698, interval_steps_per_second: 2.123, epoch: 2.0635[0m
[32m[2022-08-30 12:42:25,844] [    INFO][0m - loss: 0.90730104, learning_rate: 2.9373015873015875e-05, global_step: 790, interval_runtime: 4.7055, interval_samples_per_second: 1.7, interval_steps_per_second: 2.125, epoch: 2.0899[0m
[32m[2022-08-30 12:42:30,515] [    INFO][0m - loss: 0.57920055, learning_rate: 2.9365079365079366e-05, global_step: 800, interval_runtime: 4.8637, interval_samples_per_second: 1.645, interval_steps_per_second: 2.056, epoch: 2.1164[0m
[32m[2022-08-30 12:42:30,516] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 12:42:30,516] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-30 12:42:30,516] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 12:42:30,516] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 12:42:30,516] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-30 12:43:00,014] [    INFO][0m - eval_loss: 2.127288818359375, eval_accuracy: 0.4901675163874727, eval_runtime: 29.4977, eval_samples_per_second: 46.546, eval_steps_per_second: 1.458, epoch: 2.1164[0m
[32m[2022-08-30 12:43:00,015] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-800[0m
[32m[2022-08-30 12:43:00,015] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 12:43:07,136] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-800/tokenizer_config.json[0m
[32m[2022-08-30 12:43:07,136] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-800/special_tokens_map.json[0m
[32m[2022-08-30 12:43:26,160] [    INFO][0m - loss: 0.6136044, learning_rate: 2.9357142857142857e-05, global_step: 810, interval_runtime: 55.6439, interval_samples_per_second: 0.144, interval_steps_per_second: 0.18, epoch: 2.1429[0m
[32m[2022-08-30 12:43:30,820] [    INFO][0m - loss: 0.80786848, learning_rate: 2.934920634920635e-05, global_step: 820, interval_runtime: 4.6608, interval_samples_per_second: 1.716, interval_steps_per_second: 2.146, epoch: 2.1693[0m
[32m[2022-08-30 12:43:35,515] [    INFO][0m - loss: 0.6772697, learning_rate: 2.9341269841269842e-05, global_step: 830, interval_runtime: 4.6945, interval_samples_per_second: 1.704, interval_steps_per_second: 2.13, epoch: 2.1958[0m
[32m[2022-08-30 12:43:40,198] [    INFO][0m - loss: 0.78892617, learning_rate: 2.9333333333333333e-05, global_step: 840, interval_runtime: 4.6835, interval_samples_per_second: 1.708, interval_steps_per_second: 2.135, epoch: 2.2222[0m
[32m[2022-08-30 12:43:44,883] [    INFO][0m - loss: 0.91306696, learning_rate: 2.9325396825396827e-05, global_step: 850, interval_runtime: 4.6843, interval_samples_per_second: 1.708, interval_steps_per_second: 2.135, epoch: 2.2487[0m
[32m[2022-08-30 12:43:49,585] [    INFO][0m - loss: 0.70113869, learning_rate: 2.9317460317460318e-05, global_step: 860, interval_runtime: 4.7022, interval_samples_per_second: 1.701, interval_steps_per_second: 2.127, epoch: 2.2751[0m
[32m[2022-08-30 12:43:54,257] [    INFO][0m - loss: 0.897892, learning_rate: 2.930952380952381e-05, global_step: 870, interval_runtime: 4.6724, interval_samples_per_second: 1.712, interval_steps_per_second: 2.14, epoch: 2.3016[0m
[32m[2022-08-30 12:43:58,938] [    INFO][0m - loss: 0.69668951, learning_rate: 2.9301587301587303e-05, global_step: 880, interval_runtime: 4.6807, interval_samples_per_second: 1.709, interval_steps_per_second: 2.136, epoch: 2.328[0m
[32m[2022-08-30 12:44:03,654] [    INFO][0m - loss: 0.76516027, learning_rate: 2.9293650793650793e-05, global_step: 890, interval_runtime: 4.7162, interval_samples_per_second: 1.696, interval_steps_per_second: 2.12, epoch: 2.3545[0m
[32m[2022-08-30 12:44:08,342] [    INFO][0m - loss: 0.6205493, learning_rate: 2.9285714285714284e-05, global_step: 900, interval_runtime: 4.6882, interval_samples_per_second: 1.706, interval_steps_per_second: 2.133, epoch: 2.381[0m
[32m[2022-08-30 12:44:08,343] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 12:44:08,343] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-30 12:44:08,343] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 12:44:08,343] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 12:44:08,343] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-30 12:44:37,910] [    INFO][0m - eval_loss: 2.168074131011963, eval_accuracy: 0.5243991260014567, eval_runtime: 29.566, eval_samples_per_second: 46.438, eval_steps_per_second: 1.454, epoch: 2.381[0m
[32m[2022-08-30 12:44:37,910] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-900[0m
[32m[2022-08-30 12:44:37,910] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 12:44:45,243] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-900/tokenizer_config.json[0m
[32m[2022-08-30 12:44:45,244] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-900/special_tokens_map.json[0m
[32m[2022-08-30 12:45:04,803] [    INFO][0m - loss: 0.97568283, learning_rate: 2.9277777777777778e-05, global_step: 910, interval_runtime: 56.4602, interval_samples_per_second: 0.142, interval_steps_per_second: 0.177, epoch: 2.4074[0m
[32m[2022-08-30 12:45:09,494] [    INFO][0m - loss: 0.93442345, learning_rate: 2.9269841269841272e-05, global_step: 920, interval_runtime: 4.6918, interval_samples_per_second: 1.705, interval_steps_per_second: 2.131, epoch: 2.4339[0m
[32m[2022-08-30 12:45:14,195] [    INFO][0m - loss: 0.83138981, learning_rate: 2.9261904761904763e-05, global_step: 930, interval_runtime: 4.7011, interval_samples_per_second: 1.702, interval_steps_per_second: 2.127, epoch: 2.4603[0m
[32m[2022-08-30 12:45:18,869] [    INFO][0m - loss: 0.76965637, learning_rate: 2.9253968253968257e-05, global_step: 940, interval_runtime: 4.6736, interval_samples_per_second: 1.712, interval_steps_per_second: 2.14, epoch: 2.4868[0m
[32m[2022-08-30 12:45:23,583] [    INFO][0m - loss: 0.77616773, learning_rate: 2.9246031746031748e-05, global_step: 950, interval_runtime: 4.7141, interval_samples_per_second: 1.697, interval_steps_per_second: 2.121, epoch: 2.5132[0m
[32m[2022-08-30 12:45:28,279] [    INFO][0m - loss: 0.94710369, learning_rate: 2.923809523809524e-05, global_step: 960, interval_runtime: 4.6962, interval_samples_per_second: 1.704, interval_steps_per_second: 2.129, epoch: 2.5397[0m
[32m[2022-08-30 12:45:32,958] [    INFO][0m - loss: 0.9725193, learning_rate: 2.9230158730158733e-05, global_step: 970, interval_runtime: 4.6789, interval_samples_per_second: 1.71, interval_steps_per_second: 2.137, epoch: 2.5661[0m
[32m[2022-08-30 12:45:37,667] [    INFO][0m - loss: 0.79506607, learning_rate: 2.9222222222222224e-05, global_step: 980, interval_runtime: 4.7091, interval_samples_per_second: 1.699, interval_steps_per_second: 2.124, epoch: 2.5926[0m
[32m[2022-08-30 12:45:42,342] [    INFO][0m - loss: 0.85807228, learning_rate: 2.9214285714285715e-05, global_step: 990, interval_runtime: 4.6748, interval_samples_per_second: 1.711, interval_steps_per_second: 2.139, epoch: 2.619[0m
[32m[2022-08-30 12:45:47,021] [    INFO][0m - loss: 0.82692404, learning_rate: 2.9206349206349206e-05, global_step: 1000, interval_runtime: 4.6787, interval_samples_per_second: 1.71, interval_steps_per_second: 2.137, epoch: 2.6455[0m
[32m[2022-08-30 12:45:47,021] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 12:45:47,021] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-30 12:45:47,021] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 12:45:47,022] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 12:45:47,022] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-30 12:46:16,448] [    INFO][0m - eval_loss: 2.3384828567504883, eval_accuracy: 0.49745083758193737, eval_runtime: 29.4256, eval_samples_per_second: 46.66, eval_steps_per_second: 1.461, epoch: 2.6455[0m
[32m[2022-08-30 12:46:16,448] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1000[0m
[32m[2022-08-30 12:46:16,448] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 12:46:23,694] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1000/tokenizer_config.json[0m
[32m[2022-08-30 12:46:23,694] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1000/special_tokens_map.json[0m
[32m[2022-08-30 12:46:43,212] [    INFO][0m - loss: 0.91495781, learning_rate: 2.91984126984127e-05, global_step: 1010, interval_runtime: 56.1907, interval_samples_per_second: 0.142, interval_steps_per_second: 0.178, epoch: 2.672[0m
[32m[2022-08-30 12:46:47,849] [    INFO][0m - loss: 0.76705885, learning_rate: 2.919047619047619e-05, global_step: 1020, interval_runtime: 4.6369, interval_samples_per_second: 1.725, interval_steps_per_second: 2.157, epoch: 2.6984[0m
[32m[2022-08-30 12:46:52,488] [    INFO][0m - loss: 0.80705738, learning_rate: 2.918253968253968e-05, global_step: 1030, interval_runtime: 4.6393, interval_samples_per_second: 1.724, interval_steps_per_second: 2.155, epoch: 2.7249[0m
[32m[2022-08-30 12:46:57,167] [    INFO][0m - loss: 0.81275043, learning_rate: 2.9174603174603176e-05, global_step: 1040, interval_runtime: 4.6786, interval_samples_per_second: 1.71, interval_steps_per_second: 2.137, epoch: 2.7513[0m
[32m[2022-08-30 12:47:01,837] [    INFO][0m - loss: 1.15005836, learning_rate: 2.9166666666666666e-05, global_step: 1050, interval_runtime: 4.6704, interval_samples_per_second: 1.713, interval_steps_per_second: 2.141, epoch: 2.7778[0m
[32m[2022-08-30 12:47:06,535] [    INFO][0m - loss: 1.03901882, learning_rate: 2.9158730158730157e-05, global_step: 1060, interval_runtime: 4.6985, interval_samples_per_second: 1.703, interval_steps_per_second: 2.128, epoch: 2.8042[0m
[32m[2022-08-30 12:47:11,208] [    INFO][0m - loss: 1.09267521, learning_rate: 2.915079365079365e-05, global_step: 1070, interval_runtime: 4.6727, interval_samples_per_second: 1.712, interval_steps_per_second: 2.14, epoch: 2.8307[0m
[32m[2022-08-30 12:47:15,895] [    INFO][0m - loss: 0.80186434, learning_rate: 2.9142857142857142e-05, global_step: 1080, interval_runtime: 4.6868, interval_samples_per_second: 1.707, interval_steps_per_second: 2.134, epoch: 2.8571[0m
[32m[2022-08-30 12:47:20,560] [    INFO][0m - loss: 0.80311871, learning_rate: 2.9134920634920633e-05, global_step: 1090, interval_runtime: 4.6648, interval_samples_per_second: 1.715, interval_steps_per_second: 2.144, epoch: 2.8836[0m
[32m[2022-08-30 12:47:25,258] [    INFO][0m - loss: 0.8965271, learning_rate: 2.9126984126984127e-05, global_step: 1100, interval_runtime: 4.6981, interval_samples_per_second: 1.703, interval_steps_per_second: 2.129, epoch: 2.9101[0m
[32m[2022-08-30 12:47:25,258] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 12:47:25,258] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-30 12:47:25,258] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 12:47:25,258] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 12:47:25,259] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-30 12:47:54,749] [    INFO][0m - eval_loss: 2.250976324081421, eval_accuracy: 0.5134741442097597, eval_runtime: 29.49, eval_samples_per_second: 46.558, eval_steps_per_second: 1.458, epoch: 2.9101[0m
[32m[2022-08-30 12:47:54,750] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1100[0m
[32m[2022-08-30 12:47:54,750] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 12:48:01,962] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1100/tokenizer_config.json[0m
[32m[2022-08-30 12:48:01,962] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1100/special_tokens_map.json[0m
[32m[2022-08-30 12:48:22,922] [    INFO][0m - loss: 0.85746469, learning_rate: 2.9119047619047618e-05, global_step: 1110, interval_runtime: 57.6645, interval_samples_per_second: 0.139, interval_steps_per_second: 0.173, epoch: 2.9365[0m
[32m[2022-08-30 12:48:27,598] [    INFO][0m - loss: 0.66278501, learning_rate: 2.9111111111111112e-05, global_step: 1120, interval_runtime: 4.6753, interval_samples_per_second: 1.711, interval_steps_per_second: 2.139, epoch: 2.963[0m
[32m[2022-08-30 12:48:32,269] [    INFO][0m - loss: 0.952911, learning_rate: 2.9103174603174606e-05, global_step: 1130, interval_runtime: 4.6711, interval_samples_per_second: 1.713, interval_steps_per_second: 2.141, epoch: 2.9894[0m
[32m[2022-08-30 12:48:37,011] [    INFO][0m - loss: 0.64189167, learning_rate: 2.9095238095238097e-05, global_step: 1140, interval_runtime: 4.7423, interval_samples_per_second: 1.687, interval_steps_per_second: 2.109, epoch: 3.0159[0m
[32m[2022-08-30 12:48:41,697] [    INFO][0m - loss: 0.4704875, learning_rate: 2.9087301587301588e-05, global_step: 1150, interval_runtime: 4.6858, interval_samples_per_second: 1.707, interval_steps_per_second: 2.134, epoch: 3.0423[0m
[32m[2022-08-30 12:48:46,369] [    INFO][0m - loss: 0.68885584, learning_rate: 2.9079365079365082e-05, global_step: 1160, interval_runtime: 4.6724, interval_samples_per_second: 1.712, interval_steps_per_second: 2.14, epoch: 3.0688[0m
[32m[2022-08-30 12:48:51,033] [    INFO][0m - loss: 0.386571, learning_rate: 2.9071428571428573e-05, global_step: 1170, interval_runtime: 4.6643, interval_samples_per_second: 1.715, interval_steps_per_second: 2.144, epoch: 3.0952[0m
[32m[2022-08-30 12:48:55,736] [    INFO][0m - loss: 0.45824866, learning_rate: 2.9063492063492064e-05, global_step: 1180, interval_runtime: 4.7028, interval_samples_per_second: 1.701, interval_steps_per_second: 2.126, epoch: 3.1217[0m
[32m[2022-08-30 12:49:00,412] [    INFO][0m - loss: 0.52323103, learning_rate: 2.9055555555555558e-05, global_step: 1190, interval_runtime: 4.6761, interval_samples_per_second: 1.711, interval_steps_per_second: 2.139, epoch: 3.1481[0m
[32m[2022-08-30 12:49:05,113] [    INFO][0m - loss: 0.49127898, learning_rate: 2.904761904761905e-05, global_step: 1200, interval_runtime: 4.7009, interval_samples_per_second: 1.702, interval_steps_per_second: 2.127, epoch: 3.1746[0m
[32m[2022-08-30 12:49:05,114] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 12:49:05,114] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-30 12:49:05,114] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 12:49:05,114] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 12:49:05,114] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-30 12:49:34,631] [    INFO][0m - eval_loss: 2.3476943969726562, eval_accuracy: 0.4981791697013838, eval_runtime: 29.5167, eval_samples_per_second: 46.516, eval_steps_per_second: 1.457, epoch: 3.1746[0m
[32m[2022-08-30 12:49:34,632] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1200[0m
[32m[2022-08-30 12:49:34,632] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 12:49:41,995] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1200/tokenizer_config.json[0m
[32m[2022-08-30 12:49:41,995] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1200/special_tokens_map.json[0m
[32m[2022-08-30 12:50:00,774] [    INFO][0m - loss: 0.66054163, learning_rate: 2.903968253968254e-05, global_step: 1210, interval_runtime: 55.6604, interval_samples_per_second: 0.144, interval_steps_per_second: 0.18, epoch: 3.2011[0m
[32m[2022-08-30 12:50:05,417] [    INFO][0m - loss: 0.51328592, learning_rate: 2.9031746031746034e-05, global_step: 1220, interval_runtime: 4.6429, interval_samples_per_second: 1.723, interval_steps_per_second: 2.154, epoch: 3.2275[0m
[32m[2022-08-30 12:50:10,095] [    INFO][0m - loss: 0.36001089, learning_rate: 2.9023809523809524e-05, global_step: 1230, interval_runtime: 4.6782, interval_samples_per_second: 1.71, interval_steps_per_second: 2.138, epoch: 3.254[0m
[32m[2022-08-30 12:50:14,757] [    INFO][0m - loss: 0.41845036, learning_rate: 2.9015873015873015e-05, global_step: 1240, interval_runtime: 4.6622, interval_samples_per_second: 1.716, interval_steps_per_second: 2.145, epoch: 3.2804[0m
[32m[2022-08-30 12:50:19,423] [    INFO][0m - loss: 0.32934718, learning_rate: 2.900793650793651e-05, global_step: 1250, interval_runtime: 4.6657, interval_samples_per_second: 1.715, interval_steps_per_second: 2.143, epoch: 3.3069[0m
[32m[2022-08-30 12:50:24,099] [    INFO][0m - loss: 0.4719336, learning_rate: 2.9e-05, global_step: 1260, interval_runtime: 4.676, interval_samples_per_second: 1.711, interval_steps_per_second: 2.139, epoch: 3.3333[0m
[32m[2022-08-30 12:50:28,777] [    INFO][0m - loss: 0.4188314, learning_rate: 2.899206349206349e-05, global_step: 1270, interval_runtime: 4.6783, interval_samples_per_second: 1.71, interval_steps_per_second: 2.138, epoch: 3.3598[0m
[32m[2022-08-30 12:50:33,467] [    INFO][0m - loss: 0.67029676, learning_rate: 2.8984126984126985e-05, global_step: 1280, interval_runtime: 4.6901, interval_samples_per_second: 1.706, interval_steps_per_second: 2.132, epoch: 3.3862[0m
[32m[2022-08-30 12:50:38,174] [    INFO][0m - loss: 0.39670904, learning_rate: 2.8976190476190476e-05, global_step: 1290, interval_runtime: 4.7066, interval_samples_per_second: 1.7, interval_steps_per_second: 2.125, epoch: 3.4127[0m
[32m[2022-08-30 12:50:42,829] [    INFO][0m - loss: 0.27575071, learning_rate: 2.8968253968253967e-05, global_step: 1300, interval_runtime: 4.655, interval_samples_per_second: 1.719, interval_steps_per_second: 2.148, epoch: 3.4392[0m
[32m[2022-08-30 12:50:42,829] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 12:50:42,829] [    INFO][0m -   Num examples = 1373[0m
[32m[2022-08-30 12:50:42,829] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 12:50:42,829] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 12:50:42,829] [    INFO][0m -   Total prediction steps = 43[0m
[32m[2022-08-30 12:51:12,299] [    INFO][0m - eval_loss: 2.3398020267486572, eval_accuracy: 0.5222141296431173, eval_runtime: 29.4689, eval_samples_per_second: 46.592, eval_steps_per_second: 1.459, epoch: 3.4392[0m
[32m[2022-08-30 12:51:12,299] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1300[0m
[32m[2022-08-30 12:51:12,299] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 12:51:15,139] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1300/tokenizer_config.json[0m
[32m[2022-08-30 12:51:15,140] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1300/special_tokens_map.json[0m
[32m[2022-08-30 12:51:20,972] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-30 12:51:20,972] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-900 (score: 0.5243991260014567).[0m
[32m[2022-08-30 12:51:22,885] [    INFO][0m - train_runtime: 1265.3006, train_samples_per_second: 238.995, train_steps_per_second: 29.874, train_loss: 1.5100887159200815, epoch: 3.4392[0m
[32m[2022-08-30 12:51:22,887] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-30 12:51:22,887] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 12:51:33,922] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-30 12:51:33,923] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-30 12:51:33,924] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-30 12:51:33,925] [    INFO][0m -   epoch                    =     3.4392[0m
[32m[2022-08-30 12:51:33,925] [    INFO][0m -   train_loss               =     1.5101[0m
[32m[2022-08-30 12:51:33,925] [    INFO][0m -   train_runtime            = 0:21:05.30[0m
[32m[2022-08-30 12:51:33,925] [    INFO][0m -   train_samples_per_second =    238.995[0m
[32m[2022-08-30 12:51:33,925] [    INFO][0m -   train_steps_per_second   =     29.874[0m
[32m[2022-08-30 12:51:33,931] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-30 12:51:33,931] [    INFO][0m -   Num examples = 1749[0m
[32m[2022-08-30 12:51:33,932] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 12:51:33,932] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 12:51:33,932] [    INFO][0m -   Total prediction steps = 55[0m
[32m[2022-08-30 12:52:11,399] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-30 12:52:11,400] [    INFO][0m -   test_accuracy           =     0.5334[0m
[32m[2022-08-30 12:52:11,400] [    INFO][0m -   test_loss               =     2.1098[0m
[32m[2022-08-30 12:52:11,400] [    INFO][0m -   test_runtime            = 0:00:37.46[0m
[32m[2022-08-30 12:52:11,400] [    INFO][0m -   test_samples_per_second =      46.68[0m
[32m[2022-08-30 12:52:11,400] [    INFO][0m -   test_steps_per_second   =      1.468[0m
[32m[2022-08-30 12:52:11,401] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-30 12:52:11,401] [    INFO][0m -   Num examples = 2600[0m
[32m[2022-08-30 12:52:11,401] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 12:52:11,401] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 12:52:11,401] [    INFO][0m -   Total prediction steps = 82[0m
[32m[2022-08-30 12:53:13,374] [    INFO][0m - Predictions for iflytekf saved to ./fewclue_submit_examples.[0m
metric <function main.<locals>.compute_metrics at 0x7ff88de2f3a0>
 
==========
ocnli
==========
 
[33m[2022-08-30 12:53:17,549] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-30 12:53:17,549] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-30 12:53:17,550] [    INFO][0m - ============================================================[0m
[32m[2022-08-30 12:53:17,550] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-30 12:53:17,550] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-30 12:53:17,550] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-30 12:53:17,550] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-30 12:53:17,550] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-08-30 12:53:17,550] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-30 12:53:17,550] [    INFO][0m - model_name_or_path            :hfl/roberta-wwm-ext-large[0m
[32m[2022-08-30 12:53:17,550] [    INFO][0m - [0m
[32m[2022-08-30 12:53:17,550] [    INFO][0m - ============================================================[0m
[32m[2022-08-30 12:53:17,550] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-30 12:53:17,550] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-30 12:53:17,551] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-30 12:53:17,551] [    INFO][0m - prompt                        :{'hard':'请用正确的连接词填空：'}{'text':'text_a'}{'mask'}{'mask'}{'text':'text_b'}[0m
[32m[2022-08-30 12:53:17,551] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-30 12:53:17,551] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-30 12:53:17,551] [    INFO][0m - task_name                     :ocnli[0m
[32m[2022-08-30 12:53:17,551] [    INFO][0m - [0m
[32m[2022-08-30 12:53:17,551] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/hfl/roberta-wwm-ext-large/roberta_chn_large.pdparams[0m
W0830 12:53:17.552598 31028 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0830 12:53:17.557827 31028 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-30 12:53:22,845] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/hfl/roberta-wwm-ext-large/vocab.txt[0m
[32m[2022-08-30 12:53:22,862] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/hfl/roberta-wwm-ext-large/tokenizer_config.json[0m
[32m[2022-08-30 12:53:22,862] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/hfl/roberta-wwm-ext-large/special_tokens_map.json[0m
[32m[2022-08-30 12:53:22,863] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '请用正确的连接词填空：'}, {'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'text': 'text_b'}][0m
2022-08-30 12:53:22,871 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-30 12:53:23,033] [    INFO][0m - ============================================================[0m
[32m[2022-08-30 12:53:23,033] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-30 12:53:23,033] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-30 12:53:23,033] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-30 12:53:23,033] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-30 12:53:23,033] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-30 12:53:23,033] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-30 12:53:23,034] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-30 12:53:23,034] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-30 12:53:23,034] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-30 12:53:23,034] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-30 12:53:23,034] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-30 12:53:23,034] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-30 12:53:23,034] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-30 12:53:23,034] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-30 12:53:23,034] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-30 12:53:23,034] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-30 12:53:23,034] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-30 12:53:23,034] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-30 12:53:23,035] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-30 12:53:23,035] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-30 12:53:23,035] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-30 12:53:23,035] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-30 12:53:23,035] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-30 12:53:23,035] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-30 12:53:23,035] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-30 12:53:23,035] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-30 12:53:23,035] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-30 12:53:23,035] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-30 12:53:23,035] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-30 12:53:23,035] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-30 12:53:23,036] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-30 12:53:23,036] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-30 12:53:23,036] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-30 12:53:23,036] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-30 12:53:23,036] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-30 12:53:23,036] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-30 12:53:23,036] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug30_12-53-17_instance-3bwob41y-01[0m
[32m[2022-08-30 12:53:23,036] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-30 12:53:23,036] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-30 12:53:23,036] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-30 12:53:23,036] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-30 12:53:23,036] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-30 12:53:23,037] [    INFO][0m - max_seq_length                :64[0m
[32m[2022-08-30 12:53:23,037] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-30 12:53:23,037] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-30 12:53:23,037] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-30 12:53:23,037] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-30 12:53:23,037] [    INFO][0m - num_train_epochs              :100.0[0m
[32m[2022-08-30 12:53:23,037] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-30 12:53:23,037] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-30 12:53:23,037] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-30 12:53:23,037] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-30 12:53:23,037] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-30 12:53:23,037] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-30 12:53:23,037] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-30 12:53:23,038] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-30 12:53:23,038] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-30 12:53:23,038] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-30 12:53:23,038] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-30 12:53:23,038] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-30 12:53:23,038] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-30 12:53:23,038] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-30 12:53:23,038] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-30 12:53:23,038] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-30 12:53:23,038] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-30 12:53:23,038] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-30 12:53:23,038] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-30 12:53:23,038] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-30 12:53:23,039] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-30 12:53:23,039] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-30 12:53:23,039] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-30 12:53:23,039] [    INFO][0m - seed                          :42[0m
[32m[2022-08-30 12:53:23,039] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-30 12:53:23,039] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-30 12:53:23,039] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-30 12:53:23,039] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-30 12:53:23,039] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-30 12:53:23,039] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-30 12:53:23,039] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-30 12:53:23,039] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-30 12:53:23,039] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-30 12:53:23,040] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-30 12:53:23,040] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-30 12:53:23,040] [    INFO][0m - [0m
[32m[2022-08-30 12:53:23,043] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-30 12:53:23,043] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 12:53:23,043] [    INFO][0m -   Num Epochs = 100[0m
[32m[2022-08-30 12:53:23,043] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-30 12:53:23,043] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-30 12:53:23,044] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-30 12:53:23,044] [    INFO][0m -   Total optimization steps = 2000.0[0m
[32m[2022-08-30 12:53:23,044] [    INFO][0m -   Total num train samples = 16000[0m
[32m[2022-08-30 12:53:25,748] [    INFO][0m - loss: 1.21245193, learning_rate: 2.985e-05, global_step: 10, interval_runtime: 2.7027, interval_samples_per_second: 2.96, interval_steps_per_second: 3.7, epoch: 0.5[0m
[32m[2022-08-30 12:53:27,031] [    INFO][0m - loss: 1.2045248, learning_rate: 2.97e-05, global_step: 20, interval_runtime: 1.2842, interval_samples_per_second: 6.23, interval_steps_per_second: 7.787, epoch: 1.0[0m
[32m[2022-08-30 12:53:28,360] [    INFO][0m - loss: 0.65456767, learning_rate: 2.955e-05, global_step: 30, interval_runtime: 1.3287, interval_samples_per_second: 6.021, interval_steps_per_second: 7.526, epoch: 1.5[0m
[32m[2022-08-30 12:53:29,649] [    INFO][0m - loss: 0.47388206, learning_rate: 2.94e-05, global_step: 40, interval_runtime: 1.2892, interval_samples_per_second: 6.205, interval_steps_per_second: 7.757, epoch: 2.0[0m
[32m[2022-08-30 12:53:30,982] [    INFO][0m - loss: 0.06729674, learning_rate: 2.925e-05, global_step: 50, interval_runtime: 1.3323, interval_samples_per_second: 6.005, interval_steps_per_second: 7.506, epoch: 2.5[0m
[32m[2022-08-30 12:53:32,268] [    INFO][0m - loss: 0.06421367, learning_rate: 2.91e-05, global_step: 60, interval_runtime: 1.2867, interval_samples_per_second: 6.218, interval_steps_per_second: 7.772, epoch: 3.0[0m
[32m[2022-08-30 12:53:33,600] [    INFO][0m - loss: 0.00149346, learning_rate: 2.895e-05, global_step: 70, interval_runtime: 1.3319, interval_samples_per_second: 6.006, interval_steps_per_second: 7.508, epoch: 3.5[0m
[32m[2022-08-30 12:53:34,882] [    INFO][0m - loss: 0.0007882, learning_rate: 2.88e-05, global_step: 80, interval_runtime: 1.2825, interval_samples_per_second: 6.238, interval_steps_per_second: 7.797, epoch: 4.0[0m
[32m[2022-08-30 12:53:36,212] [    INFO][0m - loss: 0.00093454, learning_rate: 2.865e-05, global_step: 90, interval_runtime: 1.3293, interval_samples_per_second: 6.018, interval_steps_per_second: 7.523, epoch: 4.5[0m
[32m[2022-08-30 12:53:37,496] [    INFO][0m - loss: 0.03833073, learning_rate: 2.8499999999999998e-05, global_step: 100, interval_runtime: 1.2838, interval_samples_per_second: 6.231, interval_steps_per_second: 7.789, epoch: 5.0[0m
[32m[2022-08-30 12:53:37,496] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 12:53:37,496] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 12:53:37,496] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 12:53:37,496] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 12:53:37,497] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 12:53:38,252] [    INFO][0m - eval_loss: 2.7674694061279297, eval_accuracy: 0.49375, eval_runtime: 0.7554, eval_samples_per_second: 211.798, eval_steps_per_second: 6.619, epoch: 5.0[0m
[32m[2022-08-30 12:53:38,253] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-30 12:53:38,253] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 12:53:45,494] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-30 12:53:45,494] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-30 12:54:00,958] [    INFO][0m - loss: 0.00022046, learning_rate: 2.8349999999999998e-05, global_step: 110, interval_runtime: 23.4621, interval_samples_per_second: 0.341, interval_steps_per_second: 0.426, epoch: 5.5[0m
[32m[2022-08-30 12:54:02,233] [    INFO][0m - loss: 0.00016361, learning_rate: 2.8199999999999998e-05, global_step: 120, interval_runtime: 1.2752, interval_samples_per_second: 6.273, interval_steps_per_second: 7.842, epoch: 6.0[0m
[32m[2022-08-30 12:54:03,555] [    INFO][0m - loss: 0.00047182, learning_rate: 2.805e-05, global_step: 130, interval_runtime: 1.3219, interval_samples_per_second: 6.052, interval_steps_per_second: 7.565, epoch: 6.5[0m
[32m[2022-08-30 12:54:04,844] [    INFO][0m - loss: 0.00011299, learning_rate: 2.79e-05, global_step: 140, interval_runtime: 1.2891, interval_samples_per_second: 6.206, interval_steps_per_second: 7.757, epoch: 7.0[0m
[32m[2022-08-30 12:54:06,172] [    INFO][0m - loss: 9.058e-05, learning_rate: 2.7750000000000004e-05, global_step: 150, interval_runtime: 1.3282, interval_samples_per_second: 6.023, interval_steps_per_second: 7.529, epoch: 7.5[0m
[32m[2022-08-30 12:54:07,457] [    INFO][0m - loss: 8.687e-05, learning_rate: 2.7600000000000003e-05, global_step: 160, interval_runtime: 1.2847, interval_samples_per_second: 6.227, interval_steps_per_second: 7.784, epoch: 8.0[0m
[32m[2022-08-30 12:54:08,786] [    INFO][0m - loss: 8.89e-05, learning_rate: 2.7450000000000003e-05, global_step: 170, interval_runtime: 1.3288, interval_samples_per_second: 6.021, interval_steps_per_second: 7.526, epoch: 8.5[0m
[32m[2022-08-30 12:54:10,077] [    INFO][0m - loss: 6.438e-05, learning_rate: 2.7300000000000003e-05, global_step: 180, interval_runtime: 1.2916, interval_samples_per_second: 6.194, interval_steps_per_second: 7.742, epoch: 9.0[0m
[32m[2022-08-30 12:54:11,417] [    INFO][0m - loss: 6.622e-05, learning_rate: 2.7150000000000003e-05, global_step: 190, interval_runtime: 1.3396, interval_samples_per_second: 5.972, interval_steps_per_second: 7.465, epoch: 9.5[0m
[32m[2022-08-30 12:54:12,706] [    INFO][0m - loss: 6.106e-05, learning_rate: 2.7000000000000002e-05, global_step: 200, interval_runtime: 1.2887, interval_samples_per_second: 6.208, interval_steps_per_second: 7.76, epoch: 10.0[0m
[32m[2022-08-30 12:54:12,706] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 12:54:12,706] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 12:54:12,706] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 12:54:12,706] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 12:54:12,706] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 12:54:13,457] [    INFO][0m - eval_loss: 2.4472899436950684, eval_accuracy: 0.5125, eval_runtime: 0.7506, eval_samples_per_second: 213.16, eval_steps_per_second: 6.661, epoch: 10.0[0m
[32m[2022-08-30 12:54:13,458] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-30 12:54:13,458] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 12:54:21,984] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-30 12:54:21,984] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-30 12:54:37,749] [    INFO][0m - loss: 5.974e-05, learning_rate: 2.6850000000000002e-05, global_step: 210, interval_runtime: 25.0429, interval_samples_per_second: 0.319, interval_steps_per_second: 0.399, epoch: 10.5[0m
[32m[2022-08-30 12:54:39,032] [    INFO][0m - loss: 5.625e-05, learning_rate: 2.6700000000000002e-05, global_step: 220, interval_runtime: 1.2832, interval_samples_per_second: 6.234, interval_steps_per_second: 7.793, epoch: 11.0[0m
[32m[2022-08-30 12:54:40,359] [    INFO][0m - loss: 4.993e-05, learning_rate: 2.655e-05, global_step: 230, interval_runtime: 1.3271, interval_samples_per_second: 6.028, interval_steps_per_second: 7.535, epoch: 11.5[0m
[32m[2022-08-30 12:54:41,640] [    INFO][0m - loss: 5.96e-05, learning_rate: 2.64e-05, global_step: 240, interval_runtime: 1.2811, interval_samples_per_second: 6.245, interval_steps_per_second: 7.806, epoch: 12.0[0m
[32m[2022-08-30 12:54:42,960] [    INFO][0m - loss: 5.928e-05, learning_rate: 2.625e-05, global_step: 250, interval_runtime: 1.3205, interval_samples_per_second: 6.058, interval_steps_per_second: 7.573, epoch: 12.5[0m
[32m[2022-08-30 12:54:44,240] [    INFO][0m - loss: 4.493e-05, learning_rate: 2.61e-05, global_step: 260, interval_runtime: 1.2795, interval_samples_per_second: 6.252, interval_steps_per_second: 7.815, epoch: 13.0[0m
[32m[2022-08-30 12:54:45,567] [    INFO][0m - loss: 4.509e-05, learning_rate: 2.595e-05, global_step: 270, interval_runtime: 1.3272, interval_samples_per_second: 6.028, interval_steps_per_second: 7.535, epoch: 13.5[0m
[32m[2022-08-30 12:54:46,857] [    INFO][0m - loss: 5.08e-05, learning_rate: 2.58e-05, global_step: 280, interval_runtime: 1.2899, interval_samples_per_second: 6.202, interval_steps_per_second: 7.752, epoch: 14.0[0m
[32m[2022-08-30 12:54:48,183] [    INFO][0m - loss: 4.699e-05, learning_rate: 2.565e-05, global_step: 290, interval_runtime: 1.3262, interval_samples_per_second: 6.032, interval_steps_per_second: 7.54, epoch: 14.5[0m
[32m[2022-08-30 12:54:49,467] [    INFO][0m - loss: 3.833e-05, learning_rate: 2.55e-05, global_step: 300, interval_runtime: 1.2835, interval_samples_per_second: 6.233, interval_steps_per_second: 7.791, epoch: 15.0[0m
[32m[2022-08-30 12:54:49,467] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 12:54:49,467] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 12:54:49,467] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 12:54:49,467] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 12:54:49,468] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 12:54:50,221] [    INFO][0m - eval_loss: 2.566254138946533, eval_accuracy: 0.525, eval_runtime: 0.7533, eval_samples_per_second: 212.387, eval_steps_per_second: 6.637, epoch: 15.0[0m
[32m[2022-08-30 12:54:50,221] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-30 12:54:50,222] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 12:54:57,708] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-30 12:54:57,709] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-30 12:55:15,019] [    INFO][0m - loss: 4.021e-05, learning_rate: 2.535e-05, global_step: 310, interval_runtime: 25.552, interval_samples_per_second: 0.313, interval_steps_per_second: 0.391, epoch: 15.5[0m
[32m[2022-08-30 12:55:16,294] [    INFO][0m - loss: 3.564e-05, learning_rate: 2.52e-05, global_step: 320, interval_runtime: 1.2756, interval_samples_per_second: 6.271, interval_steps_per_second: 7.839, epoch: 16.0[0m
[32m[2022-08-30 12:55:17,615] [    INFO][0m - loss: 3.416e-05, learning_rate: 2.505e-05, global_step: 330, interval_runtime: 1.3202, interval_samples_per_second: 6.06, interval_steps_per_second: 7.574, epoch: 16.5[0m
[32m[2022-08-30 12:55:18,894] [    INFO][0m - loss: 3.63e-05, learning_rate: 2.49e-05, global_step: 340, interval_runtime: 1.2792, interval_samples_per_second: 6.254, interval_steps_per_second: 7.818, epoch: 17.0[0m
[32m[2022-08-30 12:55:20,218] [    INFO][0m - loss: 3.359e-05, learning_rate: 2.475e-05, global_step: 350, interval_runtime: 1.3239, interval_samples_per_second: 6.043, interval_steps_per_second: 7.554, epoch: 17.5[0m
[32m[2022-08-30 12:55:21,497] [    INFO][0m - loss: 3.137e-05, learning_rate: 2.4599999999999998e-05, global_step: 360, interval_runtime: 1.2794, interval_samples_per_second: 6.253, interval_steps_per_second: 7.816, epoch: 18.0[0m
[32m[2022-08-30 12:55:22,821] [    INFO][0m - loss: 3.078e-05, learning_rate: 2.4449999999999998e-05, global_step: 370, interval_runtime: 1.3243, interval_samples_per_second: 6.041, interval_steps_per_second: 7.551, epoch: 18.5[0m
[32m[2022-08-30 12:55:24,103] [    INFO][0m - loss: 3.161e-05, learning_rate: 2.43e-05, global_step: 380, interval_runtime: 1.2817, interval_samples_per_second: 6.242, interval_steps_per_second: 7.802, epoch: 19.0[0m
[32m[2022-08-30 12:55:25,427] [    INFO][0m - loss: 3.039e-05, learning_rate: 2.415e-05, global_step: 390, interval_runtime: 1.3237, interval_samples_per_second: 6.044, interval_steps_per_second: 7.554, epoch: 19.5[0m
[32m[2022-08-30 12:55:26,710] [    INFO][0m - loss: 3.079e-05, learning_rate: 2.4e-05, global_step: 400, interval_runtime: 1.2834, interval_samples_per_second: 6.233, interval_steps_per_second: 7.792, epoch: 20.0[0m
[32m[2022-08-30 12:55:26,711] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 12:55:26,711] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 12:55:26,711] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 12:55:26,711] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 12:55:26,711] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 12:55:27,457] [    INFO][0m - eval_loss: 2.6638264656066895, eval_accuracy: 0.5375, eval_runtime: 0.7457, eval_samples_per_second: 214.578, eval_steps_per_second: 6.706, epoch: 20.0[0m
[32m[2022-08-30 12:55:27,457] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-30 12:55:27,457] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 12:55:34,806] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-30 12:55:34,806] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-30 12:55:51,627] [    INFO][0m - loss: 2.857e-05, learning_rate: 2.385e-05, global_step: 410, interval_runtime: 24.9165, interval_samples_per_second: 0.321, interval_steps_per_second: 0.401, epoch: 20.5[0m
[32m[2022-08-30 12:55:52,901] [    INFO][0m - loss: 2.735e-05, learning_rate: 2.37e-05, global_step: 420, interval_runtime: 1.2744, interval_samples_per_second: 6.277, interval_steps_per_second: 7.847, epoch: 21.0[0m
[32m[2022-08-30 12:55:54,223] [    INFO][0m - loss: 2.32e-05, learning_rate: 2.3550000000000003e-05, global_step: 430, interval_runtime: 1.322, interval_samples_per_second: 6.052, interval_steps_per_second: 7.564, epoch: 21.5[0m
[32m[2022-08-30 12:55:55,502] [    INFO][0m - loss: 3.237e-05, learning_rate: 2.3400000000000003e-05, global_step: 440, interval_runtime: 1.2789, interval_samples_per_second: 6.255, interval_steps_per_second: 7.819, epoch: 22.0[0m
[32m[2022-08-30 12:55:56,826] [    INFO][0m - loss: 2.563e-05, learning_rate: 2.3250000000000003e-05, global_step: 450, interval_runtime: 1.3241, interval_samples_per_second: 6.042, interval_steps_per_second: 7.552, epoch: 22.5[0m
[32m[2022-08-30 12:55:58,107] [    INFO][0m - loss: 2.794e-05, learning_rate: 2.3100000000000002e-05, global_step: 460, interval_runtime: 1.2808, interval_samples_per_second: 6.246, interval_steps_per_second: 7.807, epoch: 23.0[0m
[32m[2022-08-30 12:55:59,430] [    INFO][0m - loss: 2.496e-05, learning_rate: 2.2950000000000002e-05, global_step: 470, interval_runtime: 1.3231, interval_samples_per_second: 6.046, interval_steps_per_second: 7.558, epoch: 23.5[0m
[32m[2022-08-30 12:56:00,713] [    INFO][0m - loss: 2.677e-05, learning_rate: 2.2800000000000002e-05, global_step: 480, interval_runtime: 1.283, interval_samples_per_second: 6.236, interval_steps_per_second: 7.794, epoch: 24.0[0m
[32m[2022-08-30 12:56:02,033] [    INFO][0m - loss: 2.379e-05, learning_rate: 2.265e-05, global_step: 490, interval_runtime: 1.3201, interval_samples_per_second: 6.06, interval_steps_per_second: 7.575, epoch: 24.5[0m
[32m[2022-08-30 12:56:03,314] [    INFO][0m - loss: 2.38e-05, learning_rate: 2.25e-05, global_step: 500, interval_runtime: 1.2803, interval_samples_per_second: 6.248, interval_steps_per_second: 7.811, epoch: 25.0[0m
[32m[2022-08-30 12:56:03,314] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 12:56:03,314] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 12:56:03,314] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 12:56:03,315] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 12:56:03,315] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 12:56:04,064] [    INFO][0m - eval_loss: 2.7449309825897217, eval_accuracy: 0.5375, eval_runtime: 0.7495, eval_samples_per_second: 213.466, eval_steps_per_second: 6.671, epoch: 25.0[0m
[32m[2022-08-30 12:56:04,065] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-08-30 12:56:04,065] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 12:56:11,105] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-08-30 12:56:11,373] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-08-30 12:56:27,560] [    INFO][0m - loss: 2.272e-05, learning_rate: 2.235e-05, global_step: 510, interval_runtime: 24.2467, interval_samples_per_second: 0.33, interval_steps_per_second: 0.412, epoch: 25.5[0m
[32m[2022-08-30 12:56:28,834] [    INFO][0m - loss: 2.07e-05, learning_rate: 2.22e-05, global_step: 520, interval_runtime: 1.2738, interval_samples_per_second: 6.28, interval_steps_per_second: 7.85, epoch: 26.0[0m
[32m[2022-08-30 12:56:30,148] [    INFO][0m - loss: 2.016e-05, learning_rate: 2.205e-05, global_step: 530, interval_runtime: 1.3138, interval_samples_per_second: 6.089, interval_steps_per_second: 7.612, epoch: 26.5[0m
[32m[2022-08-30 12:56:31,424] [    INFO][0m - loss: 2.155e-05, learning_rate: 2.19e-05, global_step: 540, interval_runtime: 1.275, interval_samples_per_second: 6.275, interval_steps_per_second: 7.843, epoch: 27.0[0m
[32m[2022-08-30 12:56:32,740] [    INFO][0m - loss: 2.177e-05, learning_rate: 2.175e-05, global_step: 550, interval_runtime: 1.3176, interval_samples_per_second: 6.072, interval_steps_per_second: 7.59, epoch: 27.5[0m
[32m[2022-08-30 12:56:34,017] [    INFO][0m - loss: 1.784e-05, learning_rate: 2.16e-05, global_step: 560, interval_runtime: 1.2771, interval_samples_per_second: 6.264, interval_steps_per_second: 7.831, epoch: 28.0[0m
[32m[2022-08-30 12:56:35,340] [    INFO][0m - loss: 1.713e-05, learning_rate: 2.145e-05, global_step: 570, interval_runtime: 1.3224, interval_samples_per_second: 6.05, interval_steps_per_second: 7.562, epoch: 28.5[0m
[32m[2022-08-30 12:56:36,617] [    INFO][0m - loss: 2.169e-05, learning_rate: 2.13e-05, global_step: 580, interval_runtime: 1.2767, interval_samples_per_second: 6.266, interval_steps_per_second: 7.833, epoch: 29.0[0m
[32m[2022-08-30 12:56:37,935] [    INFO][0m - loss: 1.866e-05, learning_rate: 2.115e-05, global_step: 590, interval_runtime: 1.3184, interval_samples_per_second: 6.068, interval_steps_per_second: 7.585, epoch: 29.5[0m
[32m[2022-08-30 12:56:39,216] [    INFO][0m - loss: 1.941e-05, learning_rate: 2.1e-05, global_step: 600, interval_runtime: 1.2811, interval_samples_per_second: 6.245, interval_steps_per_second: 7.806, epoch: 30.0[0m
[32m[2022-08-30 12:56:39,217] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 12:56:39,217] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 12:56:39,217] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 12:56:39,217] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 12:56:39,217] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 12:56:39,967] [    INFO][0m - eval_loss: 2.814058780670166, eval_accuracy: 0.5375, eval_runtime: 0.7497, eval_samples_per_second: 213.429, eval_steps_per_second: 6.67, epoch: 30.0[0m
[32m[2022-08-30 12:56:39,967] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-08-30 12:56:39,967] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 12:56:47,377] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-08-30 12:56:47,377] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-08-30 12:57:02,984] [    INFO][0m - loss: 1.88e-05, learning_rate: 2.085e-05, global_step: 610, interval_runtime: 23.7672, interval_samples_per_second: 0.337, interval_steps_per_second: 0.421, epoch: 30.5[0m
[32m[2022-08-30 12:57:04,259] [    INFO][0m - loss: 1.899e-05, learning_rate: 2.07e-05, global_step: 620, interval_runtime: 1.2755, interval_samples_per_second: 6.272, interval_steps_per_second: 7.84, epoch: 31.0[0m
[32m[2022-08-30 12:57:05,572] [    INFO][0m - loss: 1.847e-05, learning_rate: 2.055e-05, global_step: 630, interval_runtime: 1.3134, interval_samples_per_second: 6.091, interval_steps_per_second: 7.614, epoch: 31.5[0m
[32m[2022-08-30 12:57:06,850] [    INFO][0m - loss: 1.754e-05, learning_rate: 2.04e-05, global_step: 640, interval_runtime: 1.2776, interval_samples_per_second: 6.262, interval_steps_per_second: 7.827, epoch: 32.0[0m
[32m[2022-08-30 12:57:08,167] [    INFO][0m - loss: 1.817e-05, learning_rate: 2.025e-05, global_step: 650, interval_runtime: 1.3173, interval_samples_per_second: 6.073, interval_steps_per_second: 7.591, epoch: 32.5[0m
[32m[2022-08-30 12:57:09,449] [    INFO][0m - loss: 1.633e-05, learning_rate: 2.01e-05, global_step: 660, interval_runtime: 1.2818, interval_samples_per_second: 6.241, interval_steps_per_second: 7.802, epoch: 33.0[0m
[32m[2022-08-30 12:57:10,769] [    INFO][0m - loss: 1.599e-05, learning_rate: 1.995e-05, global_step: 670, interval_runtime: 1.3199, interval_samples_per_second: 6.061, interval_steps_per_second: 7.576, epoch: 33.5[0m
[32m[2022-08-30 12:57:12,046] [    INFO][0m - loss: 1.719e-05, learning_rate: 1.98e-05, global_step: 680, interval_runtime: 1.2774, interval_samples_per_second: 6.263, interval_steps_per_second: 7.829, epoch: 34.0[0m
[32m[2022-08-30 12:57:13,366] [    INFO][0m - loss: 1.499e-05, learning_rate: 1.965e-05, global_step: 690, interval_runtime: 1.3197, interval_samples_per_second: 6.062, interval_steps_per_second: 7.578, epoch: 34.5[0m
[32m[2022-08-30 12:57:14,647] [    INFO][0m - loss: 1.72e-05, learning_rate: 1.95e-05, global_step: 700, interval_runtime: 1.2806, interval_samples_per_second: 6.247, interval_steps_per_second: 7.809, epoch: 35.0[0m
[32m[2022-08-30 12:57:14,647] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 12:57:14,647] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 12:57:14,647] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 12:57:14,647] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 12:57:14,647] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 12:57:15,395] [    INFO][0m - eval_loss: 2.8729350566864014, eval_accuracy: 0.5375, eval_runtime: 0.7474, eval_samples_per_second: 214.068, eval_steps_per_second: 6.69, epoch: 35.0[0m
[32m[2022-08-30 12:57:15,395] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-700[0m
[32m[2022-08-30 12:57:15,395] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 12:57:22,402] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-700/tokenizer_config.json[0m
[32m[2022-08-30 12:57:22,403] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-700/special_tokens_map.json[0m
[32m[2022-08-30 12:57:38,151] [    INFO][0m - loss: 1.693e-05, learning_rate: 1.935e-05, global_step: 710, interval_runtime: 23.5045, interval_samples_per_second: 0.34, interval_steps_per_second: 0.425, epoch: 35.5[0m
[32m[2022-08-30 12:57:39,424] [    INFO][0m - loss: 1.448e-05, learning_rate: 1.9200000000000003e-05, global_step: 720, interval_runtime: 1.2726, interval_samples_per_second: 6.286, interval_steps_per_second: 7.858, epoch: 36.0[0m
[32m[2022-08-30 12:57:40,740] [    INFO][0m - loss: 1.659e-05, learning_rate: 1.9050000000000002e-05, global_step: 730, interval_runtime: 1.3158, interval_samples_per_second: 6.08, interval_steps_per_second: 7.6, epoch: 36.5[0m
[32m[2022-08-30 12:57:42,014] [    INFO][0m - loss: 1.353e-05, learning_rate: 1.8900000000000002e-05, global_step: 740, interval_runtime: 1.2744, interval_samples_per_second: 6.278, interval_steps_per_second: 7.847, epoch: 37.0[0m
[32m[2022-08-30 12:57:43,333] [    INFO][0m - loss: 1.492e-05, learning_rate: 1.8750000000000002e-05, global_step: 750, interval_runtime: 1.3183, interval_samples_per_second: 6.068, interval_steps_per_second: 7.585, epoch: 37.5[0m
[32m[2022-08-30 12:57:44,610] [    INFO][0m - loss: 1.584e-05, learning_rate: 1.86e-05, global_step: 760, interval_runtime: 1.2777, interval_samples_per_second: 6.261, interval_steps_per_second: 7.826, epoch: 38.0[0m
[32m[2022-08-30 12:57:45,928] [    INFO][0m - loss: 1.494e-05, learning_rate: 1.845e-05, global_step: 770, interval_runtime: 1.3182, interval_samples_per_second: 6.069, interval_steps_per_second: 7.586, epoch: 38.5[0m
[32m[2022-08-30 12:57:47,205] [    INFO][0m - loss: 1.275e-05, learning_rate: 1.83e-05, global_step: 780, interval_runtime: 1.2767, interval_samples_per_second: 6.266, interval_steps_per_second: 7.833, epoch: 39.0[0m
[32m[2022-08-30 12:57:48,527] [    INFO][0m - loss: 1.499e-05, learning_rate: 1.815e-05, global_step: 790, interval_runtime: 1.3212, interval_samples_per_second: 6.055, interval_steps_per_second: 7.569, epoch: 39.5[0m
[32m[2022-08-30 12:57:49,803] [    INFO][0m - loss: 1.317e-05, learning_rate: 1.8e-05, global_step: 800, interval_runtime: 1.2771, interval_samples_per_second: 6.264, interval_steps_per_second: 7.83, epoch: 40.0[0m
[32m[2022-08-30 12:57:49,804] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 12:57:49,804] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 12:57:49,804] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 12:57:49,804] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 12:57:49,804] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 12:57:50,555] [    INFO][0m - eval_loss: 2.9296255111694336, eval_accuracy: 0.54375, eval_runtime: 0.7509, eval_samples_per_second: 213.082, eval_steps_per_second: 6.659, epoch: 40.0[0m
[32m[2022-08-30 12:57:50,555] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-800[0m
[32m[2022-08-30 12:57:50,556] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 12:57:57,654] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-800/tokenizer_config.json[0m
[32m[2022-08-30 12:57:57,654] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-800/special_tokens_map.json[0m
[32m[2022-08-30 12:58:13,129] [    INFO][0m - loss: 1.371e-05, learning_rate: 1.785e-05, global_step: 810, interval_runtime: 23.3261, interval_samples_per_second: 0.343, interval_steps_per_second: 0.429, epoch: 40.5[0m
[32m[2022-08-30 12:58:14,414] [    INFO][0m - loss: 1.327e-05, learning_rate: 1.77e-05, global_step: 820, interval_runtime: 1.285, interval_samples_per_second: 6.225, interval_steps_per_second: 7.782, epoch: 41.0[0m
[32m[2022-08-30 12:58:15,728] [    INFO][0m - loss: 1.355e-05, learning_rate: 1.755e-05, global_step: 830, interval_runtime: 1.3134, interval_samples_per_second: 6.091, interval_steps_per_second: 7.614, epoch: 41.5[0m
[32m[2022-08-30 12:58:17,004] [    INFO][0m - loss: 1.243e-05, learning_rate: 1.74e-05, global_step: 840, interval_runtime: 1.276, interval_samples_per_second: 6.27, interval_steps_per_second: 7.837, epoch: 42.0[0m
[32m[2022-08-30 12:58:18,319] [    INFO][0m - loss: 1.323e-05, learning_rate: 1.725e-05, global_step: 850, interval_runtime: 1.3148, interval_samples_per_second: 6.085, interval_steps_per_second: 7.606, epoch: 42.5[0m
[32m[2022-08-30 12:58:19,596] [    INFO][0m - loss: 1.224e-05, learning_rate: 1.71e-05, global_step: 860, interval_runtime: 1.2776, interval_samples_per_second: 6.262, interval_steps_per_second: 7.827, epoch: 43.0[0m
[32m[2022-08-30 12:58:20,916] [    INFO][0m - loss: 1.24e-05, learning_rate: 1.695e-05, global_step: 870, interval_runtime: 1.3198, interval_samples_per_second: 6.061, interval_steps_per_second: 7.577, epoch: 43.5[0m
[32m[2022-08-30 12:58:22,193] [    INFO][0m - loss: 1.193e-05, learning_rate: 1.6800000000000002e-05, global_step: 880, interval_runtime: 1.2766, interval_samples_per_second: 6.267, interval_steps_per_second: 7.833, epoch: 44.0[0m
[32m[2022-08-30 12:58:23,512] [    INFO][0m - loss: 1.258e-05, learning_rate: 1.665e-05, global_step: 890, interval_runtime: 1.319, interval_samples_per_second: 6.065, interval_steps_per_second: 7.582, epoch: 44.5[0m
[32m[2022-08-30 12:58:24,794] [    INFO][0m - loss: 1.18e-05, learning_rate: 1.65e-05, global_step: 900, interval_runtime: 1.2825, interval_samples_per_second: 6.238, interval_steps_per_second: 7.797, epoch: 45.0[0m
[32m[2022-08-30 12:58:24,795] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 12:58:24,795] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 12:58:24,795] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 12:58:24,795] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 12:58:24,795] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 12:58:25,542] [    INFO][0m - eval_loss: 2.9785525798797607, eval_accuracy: 0.54375, eval_runtime: 0.7468, eval_samples_per_second: 214.251, eval_steps_per_second: 6.695, epoch: 45.0[0m
[32m[2022-08-30 12:58:25,542] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-900[0m
[32m[2022-08-30 12:58:25,542] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 12:58:32,646] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-900/tokenizer_config.json[0m
[32m[2022-08-30 12:58:32,647] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-900/special_tokens_map.json[0m
[32m[2022-08-30 12:58:47,967] [    INFO][0m - loss: 1.22e-05, learning_rate: 1.635e-05, global_step: 910, interval_runtime: 23.173, interval_samples_per_second: 0.345, interval_steps_per_second: 0.432, epoch: 45.5[0m
[32m[2022-08-30 12:58:49,238] [    INFO][0m - loss: 1.145e-05, learning_rate: 1.62e-05, global_step: 920, interval_runtime: 1.2712, interval_samples_per_second: 6.293, interval_steps_per_second: 7.867, epoch: 46.0[0m
[32m[2022-08-30 12:58:50,556] [    INFO][0m - loss: 1.239e-05, learning_rate: 1.605e-05, global_step: 930, interval_runtime: 1.3173, interval_samples_per_second: 6.073, interval_steps_per_second: 7.591, epoch: 46.5[0m
[32m[2022-08-30 12:58:51,832] [    INFO][0m - loss: 1.086e-05, learning_rate: 1.59e-05, global_step: 940, interval_runtime: 1.2768, interval_samples_per_second: 6.266, interval_steps_per_second: 7.832, epoch: 47.0[0m
[32m[2022-08-30 12:58:53,149] [    INFO][0m - loss: 1.202e-05, learning_rate: 1.575e-05, global_step: 950, interval_runtime: 1.3163, interval_samples_per_second: 6.077, interval_steps_per_second: 7.597, epoch: 47.5[0m
[32m[2022-08-30 12:58:54,424] [    INFO][0m - loss: 1.113e-05, learning_rate: 1.56e-05, global_step: 960, interval_runtime: 1.2754, interval_samples_per_second: 6.272, interval_steps_per_second: 7.841, epoch: 48.0[0m
[32m[2022-08-30 12:58:55,740] [    INFO][0m - loss: 1.099e-05, learning_rate: 1.545e-05, global_step: 970, interval_runtime: 1.3158, interval_samples_per_second: 6.08, interval_steps_per_second: 7.6, epoch: 48.5[0m
[32m[2022-08-30 12:58:57,020] [    INFO][0m - loss: 1.034e-05, learning_rate: 1.53e-05, global_step: 980, interval_runtime: 1.2795, interval_samples_per_second: 6.252, interval_steps_per_second: 7.815, epoch: 49.0[0m
[32m[2022-08-30 12:58:58,337] [    INFO][0m - loss: 1.168e-05, learning_rate: 1.515e-05, global_step: 990, interval_runtime: 1.3174, interval_samples_per_second: 6.073, interval_steps_per_second: 7.591, epoch: 49.5[0m
[32m[2022-08-30 12:58:59,617] [    INFO][0m - loss: 9.51e-06, learning_rate: 1.5e-05, global_step: 1000, interval_runtime: 1.2797, interval_samples_per_second: 6.251, interval_steps_per_second: 7.814, epoch: 50.0[0m
[32m[2022-08-30 12:58:59,617] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 12:58:59,617] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 12:58:59,617] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 12:58:59,617] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 12:58:59,617] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 12:59:00,365] [    INFO][0m - eval_loss: 3.0189414024353027, eval_accuracy: 0.54375, eval_runtime: 0.7474, eval_samples_per_second: 214.089, eval_steps_per_second: 6.69, epoch: 50.0[0m
[32m[2022-08-30 12:59:00,365] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1000[0m
[32m[2022-08-30 12:59:00,365] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 12:59:07,488] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1000/tokenizer_config.json[0m
[32m[2022-08-30 12:59:07,489] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1000/special_tokens_map.json[0m
[32m[2022-08-30 12:59:22,811] [    INFO][0m - loss: 1.09e-05, learning_rate: 1.485e-05, global_step: 1010, interval_runtime: 23.1942, interval_samples_per_second: 0.345, interval_steps_per_second: 0.431, epoch: 50.5[0m
[32m[2022-08-30 12:59:24,084] [    INFO][0m - loss: 1.065e-05, learning_rate: 1.47e-05, global_step: 1020, interval_runtime: 1.2733, interval_samples_per_second: 6.283, interval_steps_per_second: 7.853, epoch: 51.0[0m
[32m[2022-08-30 12:59:25,402] [    INFO][0m - loss: 1.079e-05, learning_rate: 1.455e-05, global_step: 1030, interval_runtime: 1.3181, interval_samples_per_second: 6.069, interval_steps_per_second: 7.587, epoch: 51.5[0m
[32m[2022-08-30 12:59:26,678] [    INFO][0m - loss: 1.028e-05, learning_rate: 1.44e-05, global_step: 1040, interval_runtime: 1.2753, interval_samples_per_second: 6.273, interval_steps_per_second: 7.842, epoch: 52.0[0m
[32m[2022-08-30 12:59:27,996] [    INFO][0m - loss: 1.038e-05, learning_rate: 1.4249999999999999e-05, global_step: 1050, interval_runtime: 1.3182, interval_samples_per_second: 6.069, interval_steps_per_second: 7.586, epoch: 52.5[0m
[32m[2022-08-30 12:59:29,273] [    INFO][0m - loss: 9.87e-06, learning_rate: 1.4099999999999999e-05, global_step: 1060, interval_runtime: 1.2773, interval_samples_per_second: 6.263, interval_steps_per_second: 7.829, epoch: 53.0[0m
[32m[2022-08-30 12:59:30,592] [    INFO][0m - loss: 9.43e-06, learning_rate: 1.395e-05, global_step: 1070, interval_runtime: 1.3187, interval_samples_per_second: 6.067, interval_steps_per_second: 7.583, epoch: 53.5[0m
[32m[2022-08-30 12:59:31,871] [    INFO][0m - loss: 1.073e-05, learning_rate: 1.3800000000000002e-05, global_step: 1080, interval_runtime: 1.2791, interval_samples_per_second: 6.255, interval_steps_per_second: 7.818, epoch: 54.0[0m
[32m[2022-08-30 12:59:33,191] [    INFO][0m - loss: 9.56e-06, learning_rate: 1.3650000000000001e-05, global_step: 1090, interval_runtime: 1.3201, interval_samples_per_second: 6.06, interval_steps_per_second: 7.575, epoch: 54.5[0m
[32m[2022-08-30 12:59:34,472] [    INFO][0m - loss: 9.95e-06, learning_rate: 1.3500000000000001e-05, global_step: 1100, interval_runtime: 1.2808, interval_samples_per_second: 6.246, interval_steps_per_second: 7.807, epoch: 55.0[0m
[32m[2022-08-30 12:59:34,472] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 12:59:34,472] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 12:59:34,472] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 12:59:34,473] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 12:59:34,473] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 12:59:35,218] [    INFO][0m - eval_loss: 3.057192802429199, eval_accuracy: 0.54375, eval_runtime: 0.7458, eval_samples_per_second: 214.54, eval_steps_per_second: 6.704, epoch: 55.0[0m
[32m[2022-08-30 12:59:35,219] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1100[0m
[32m[2022-08-30 12:59:35,219] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 12:59:42,021] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1100/tokenizer_config.json[0m
[32m[2022-08-30 12:59:42,022] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1100/special_tokens_map.json[0m
[32m[2022-08-30 12:59:57,612] [    INFO][0m - loss: 9.95e-06, learning_rate: 1.3350000000000001e-05, global_step: 1110, interval_runtime: 23.1399, interval_samples_per_second: 0.346, interval_steps_per_second: 0.432, epoch: 55.5[0m
[32m[2022-08-30 12:59:58,880] [    INFO][0m - loss: 8.99e-06, learning_rate: 1.32e-05, global_step: 1120, interval_runtime: 1.2686, interval_samples_per_second: 6.306, interval_steps_per_second: 7.883, epoch: 56.0[0m
[32m[2022-08-30 13:00:00,194] [    INFO][0m - loss: 9.44e-06, learning_rate: 1.305e-05, global_step: 1130, interval_runtime: 1.3141, interval_samples_per_second: 6.088, interval_steps_per_second: 7.61, epoch: 56.5[0m
[32m[2022-08-30 13:00:01,469] [    INFO][0m - loss: 9.9e-06, learning_rate: 1.29e-05, global_step: 1140, interval_runtime: 1.2749, interval_samples_per_second: 6.275, interval_steps_per_second: 7.844, epoch: 57.0[0m
[32m[2022-08-30 13:00:02,783] [    INFO][0m - loss: 9.88e-06, learning_rate: 1.275e-05, global_step: 1150, interval_runtime: 1.3138, interval_samples_per_second: 6.089, interval_steps_per_second: 7.611, epoch: 57.5[0m
[32m[2022-08-30 13:00:04,060] [    INFO][0m - loss: 8.87e-06, learning_rate: 1.26e-05, global_step: 1160, interval_runtime: 1.2765, interval_samples_per_second: 6.267, interval_steps_per_second: 7.834, epoch: 58.0[0m
[32m[2022-08-30 13:00:05,376] [    INFO][0m - loss: 9.87e-06, learning_rate: 1.245e-05, global_step: 1170, interval_runtime: 1.3162, interval_samples_per_second: 6.078, interval_steps_per_second: 7.597, epoch: 58.5[0m
[32m[2022-08-30 13:00:06,654] [    INFO][0m - loss: 8.71e-06, learning_rate: 1.2299999999999999e-05, global_step: 1180, interval_runtime: 1.2785, interval_samples_per_second: 6.257, interval_steps_per_second: 7.821, epoch: 59.0[0m
[32m[2022-08-30 13:00:07,976] [    INFO][0m - loss: 9.54e-06, learning_rate: 1.215e-05, global_step: 1190, interval_runtime: 1.3211, interval_samples_per_second: 6.056, interval_steps_per_second: 7.57, epoch: 59.5[0m
[32m[2022-08-30 13:00:09,255] [    INFO][0m - loss: 8.23e-06, learning_rate: 1.2e-05, global_step: 1200, interval_runtime: 1.2793, interval_samples_per_second: 6.254, interval_steps_per_second: 7.817, epoch: 60.0[0m
[32m[2022-08-30 13:00:09,255] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 13:00:09,255] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 13:00:09,255] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 13:00:09,255] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 13:00:09,256] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 13:00:10,000] [    INFO][0m - eval_loss: 3.090832471847534, eval_accuracy: 0.54375, eval_runtime: 0.7443, eval_samples_per_second: 214.97, eval_steps_per_second: 6.718, epoch: 60.0[0m
[32m[2022-08-30 13:00:10,000] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1200[0m
[32m[2022-08-30 13:00:10,000] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 13:00:16,786] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1200/tokenizer_config.json[0m
[32m[2022-08-30 13:00:16,786] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1200/special_tokens_map.json[0m
[32m[2022-08-30 13:00:30,909] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-30 13:00:30,909] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-800 (score: 0.54375).[0m
[32m[2022-08-30 13:00:32,796] [    INFO][0m - train_runtime: 429.7516, train_samples_per_second: 37.231, train_steps_per_second: 4.654, train_loss: 0.03101649912680538, epoch: 60.0[0m
[32m[2022-08-30 13:00:32,798] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-30 13:00:32,798] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 13:00:39,728] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-30 13:00:39,728] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-30 13:00:39,729] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-30 13:00:39,729] [    INFO][0m -   epoch                    =       60.0[0m
[32m[2022-08-30 13:00:39,730] [    INFO][0m -   train_loss               =      0.031[0m
[32m[2022-08-30 13:00:39,730] [    INFO][0m -   train_runtime            = 0:07:09.75[0m
[32m[2022-08-30 13:00:39,730] [    INFO][0m -   train_samples_per_second =     37.231[0m
[32m[2022-08-30 13:00:39,730] [    INFO][0m -   train_steps_per_second   =      4.654[0m
[32m[2022-08-30 13:00:39,735] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-30 13:00:39,735] [    INFO][0m -   Num examples = 2520[0m
[32m[2022-08-30 13:00:39,736] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 13:00:39,736] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 13:00:39,736] [    INFO][0m -   Total prediction steps = 79[0m
[32m[2022-08-30 13:00:51,515] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-30 13:00:51,516] [    INFO][0m -   test_accuracy           =     0.5103[0m
[32m[2022-08-30 13:00:51,516] [    INFO][0m -   test_loss               =     3.0287[0m
[32m[2022-08-30 13:00:51,516] [    INFO][0m -   test_runtime            = 0:00:11.77[0m
[32m[2022-08-30 13:00:51,516] [    INFO][0m -   test_samples_per_second =    213.927[0m
[32m[2022-08-30 13:00:51,516] [    INFO][0m -   test_steps_per_second   =      6.706[0m
[32m[2022-08-30 13:00:51,517] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-30 13:00:51,517] [    INFO][0m -   Num examples = 3000[0m
[32m[2022-08-30 13:00:51,517] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 13:00:51,517] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 13:00:51,517] [    INFO][0m -   Total prediction steps = 94[0m
[32m[2022-08-30 13:01:07,723] [    INFO][0m - Predictions for ocnlif saved to ./fewclue_submit_examples.[0m
metric <function main.<locals>.compute_metrics at 0x7f28262e63a0>
 
==========
bustm
==========
 
[33m[2022-08-30 13:01:11,698] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-30 13:01:11,698] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-30 13:01:11,698] [    INFO][0m - ============================================================[0m
[32m[2022-08-30 13:01:11,698] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-30 13:01:11,698] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-30 13:01:11,699] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-30 13:01:11,699] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-30 13:01:11,699] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-08-30 13:01:11,699] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-30 13:01:11,699] [    INFO][0m - model_name_or_path            :hfl/roberta-wwm-ext-large[0m
[32m[2022-08-30 13:01:11,699] [    INFO][0m - [0m
[32m[2022-08-30 13:01:11,699] [    INFO][0m - ============================================================[0m
[32m[2022-08-30 13:01:11,699] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-30 13:01:11,699] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-30 13:01:11,699] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-30 13:01:11,699] [    INFO][0m - prompt                        :{'text':'text_a'}{'sep'}{'text':'text_b'}{'sep'}{'hard':'前两句话'}{'mask'}{'hard':'像'}[0m
[32m[2022-08-30 13:01:11,699] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-30 13:01:11,699] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-30 13:01:11,700] [    INFO][0m - task_name                     :bustm[0m
[32m[2022-08-30 13:01:11,700] [    INFO][0m - [0m
[32m[2022-08-30 13:01:11,700] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/hfl/roberta-wwm-ext-large/roberta_chn_large.pdparams[0m
W0830 13:01:11.701336 38698 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0830 13:01:11.706712 38698 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-30 13:01:16,883] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/hfl/roberta-wwm-ext-large/vocab.txt[0m
[32m[2022-08-30 13:01:16,896] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/hfl/roberta-wwm-ext-large/tokenizer_config.json[0m
[32m[2022-08-30 13:01:16,896] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/hfl/roberta-wwm-ext-large/special_tokens_map.json[0m
[32m[2022-08-30 13:01:16,897] [    INFO][0m - Using template: [{'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'sep': None}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'sep': None}, {'add_prefix_space': '', 'hard': '前两句话'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': '像'}][0m
2022-08-30 13:01:16,904 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-30 13:01:17,010] [    INFO][0m - ============================================================[0m
[32m[2022-08-30 13:01:17,010] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-30 13:01:17,010] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-30 13:01:17,010] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-30 13:01:17,010] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-30 13:01:17,011] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-30 13:01:17,011] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-30 13:01:17,011] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-30 13:01:17,011] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-30 13:01:17,011] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-30 13:01:17,011] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-30 13:01:17,011] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-30 13:01:17,011] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-30 13:01:17,011] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-30 13:01:17,011] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-30 13:01:17,011] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-30 13:01:17,011] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-30 13:01:17,012] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-30 13:01:17,012] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-30 13:01:17,012] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-30 13:01:17,012] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-30 13:01:17,012] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-30 13:01:17,012] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-30 13:01:17,012] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-30 13:01:17,012] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-30 13:01:17,012] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-30 13:01:17,012] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-30 13:01:17,012] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-30 13:01:17,012] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-30 13:01:17,012] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-30 13:01:17,012] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-30 13:01:17,013] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-30 13:01:17,013] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-30 13:01:17,013] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-30 13:01:17,013] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-30 13:01:17,013] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-30 13:01:17,013] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-30 13:01:17,013] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug30_13-01-11_instance-3bwob41y-01[0m
[32m[2022-08-30 13:01:17,013] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-30 13:01:17,013] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-30 13:01:17,013] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-30 13:01:17,013] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-30 13:01:17,013] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-30 13:01:17,013] [    INFO][0m - max_seq_length                :40[0m
[32m[2022-08-30 13:01:17,014] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-30 13:01:17,014] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-30 13:01:17,014] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-30 13:01:17,014] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-30 13:01:17,014] [    INFO][0m - num_train_epochs              :100.0[0m
[32m[2022-08-30 13:01:17,014] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-30 13:01:17,014] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-30 13:01:17,014] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-30 13:01:17,014] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-30 13:01:17,014] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-30 13:01:17,014] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-30 13:01:17,014] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-30 13:01:17,014] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-30 13:01:17,014] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-30 13:01:17,015] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-30 13:01:17,015] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-30 13:01:17,015] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-30 13:01:17,015] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-30 13:01:17,015] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-30 13:01:17,015] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-30 13:01:17,015] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-30 13:01:17,015] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-30 13:01:17,015] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-30 13:01:17,015] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-30 13:01:17,015] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-30 13:01:17,015] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-30 13:01:17,015] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-30 13:01:17,015] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-30 13:01:17,016] [    INFO][0m - seed                          :42[0m
[32m[2022-08-30 13:01:17,016] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-30 13:01:17,016] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-30 13:01:17,016] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-30 13:01:17,016] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-30 13:01:17,016] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-30 13:01:17,016] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-30 13:01:17,016] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-30 13:01:17,016] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-30 13:01:17,016] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-30 13:01:17,016] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-30 13:01:17,016] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-30 13:01:17,016] [    INFO][0m - [0m
[32m[2022-08-30 13:01:17,020] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-30 13:01:17,020] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 13:01:17,020] [    INFO][0m -   Num Epochs = 100[0m
[32m[2022-08-30 13:01:17,020] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-30 13:01:17,020] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-30 13:01:17,020] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-30 13:01:17,020] [    INFO][0m -   Total optimization steps = 2000.0[0m
[32m[2022-08-30 13:01:17,020] [    INFO][0m -   Total num train samples = 16000[0m
[32m[2022-08-30 13:01:19,357] [    INFO][0m - loss: 0.78151779, learning_rate: 2.985e-05, global_step: 10, interval_runtime: 2.3356, interval_samples_per_second: 3.425, interval_steps_per_second: 4.282, epoch: 0.5[0m
[32m[2022-08-30 13:01:20,341] [    INFO][0m - loss: 0.77297034, learning_rate: 2.97e-05, global_step: 20, interval_runtime: 0.9841, interval_samples_per_second: 8.129, interval_steps_per_second: 10.161, epoch: 1.0[0m
[32m[2022-08-30 13:01:21,392] [    INFO][0m - loss: 0.49653244, learning_rate: 2.955e-05, global_step: 30, interval_runtime: 1.0514, interval_samples_per_second: 7.609, interval_steps_per_second: 9.512, epoch: 1.5[0m
[32m[2022-08-30 13:01:22,404] [    INFO][0m - loss: 0.73590331, learning_rate: 2.94e-05, global_step: 40, interval_runtime: 1.012, interval_samples_per_second: 7.905, interval_steps_per_second: 9.882, epoch: 2.0[0m
[32m[2022-08-30 13:01:23,466] [    INFO][0m - loss: 0.29914031, learning_rate: 2.925e-05, global_step: 50, interval_runtime: 1.0613, interval_samples_per_second: 7.538, interval_steps_per_second: 9.422, epoch: 2.5[0m
[32m[2022-08-30 13:01:24,451] [    INFO][0m - loss: 0.22891638, learning_rate: 2.91e-05, global_step: 60, interval_runtime: 0.9857, interval_samples_per_second: 8.116, interval_steps_per_second: 10.145, epoch: 3.0[0m
[32m[2022-08-30 13:01:25,474] [    INFO][0m - loss: 0.07504805, learning_rate: 2.895e-05, global_step: 70, interval_runtime: 1.0227, interval_samples_per_second: 7.823, interval_steps_per_second: 9.778, epoch: 3.5[0m
[32m[2022-08-30 13:01:26,483] [    INFO][0m - loss: 0.17284586, learning_rate: 2.88e-05, global_step: 80, interval_runtime: 1.0088, interval_samples_per_second: 7.93, interval_steps_per_second: 9.913, epoch: 4.0[0m
[32m[2022-08-30 13:01:27,505] [    INFO][0m - loss: 0.00395266, learning_rate: 2.865e-05, global_step: 90, interval_runtime: 1.0224, interval_samples_per_second: 7.824, interval_steps_per_second: 9.781, epoch: 4.5[0m
[32m[2022-08-30 13:01:28,494] [    INFO][0m - loss: 0.0079064, learning_rate: 2.8499999999999998e-05, global_step: 100, interval_runtime: 0.9889, interval_samples_per_second: 8.089, interval_steps_per_second: 10.112, epoch: 5.0[0m
[32m[2022-08-30 13:01:28,495] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 13:01:28,495] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 13:01:28,495] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 13:01:28,495] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 13:01:28,495] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 13:01:28,981] [    INFO][0m - eval_loss: 3.2838432788848877, eval_accuracy: 0.59375, eval_runtime: 0.4852, eval_samples_per_second: 329.733, eval_steps_per_second: 10.304, epoch: 5.0[0m
[32m[2022-08-30 13:01:28,981] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-30 13:01:28,981] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 13:01:36,581] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-30 13:01:36,582] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-30 13:01:53,586] [    INFO][0m - loss: 0.00490615, learning_rate: 2.8349999999999998e-05, global_step: 110, interval_runtime: 25.0922, interval_samples_per_second: 0.319, interval_steps_per_second: 0.399, epoch: 5.5[0m
[32m[2022-08-30 13:01:54,827] [    INFO][0m - loss: 2.169e-05, learning_rate: 2.8199999999999998e-05, global_step: 120, interval_runtime: 1.2405, interval_samples_per_second: 6.449, interval_steps_per_second: 8.061, epoch: 6.0[0m
[32m[2022-08-30 13:01:56,143] [    INFO][0m - loss: 0.00038769, learning_rate: 2.805e-05, global_step: 130, interval_runtime: 1.3163, interval_samples_per_second: 6.077, interval_steps_per_second: 7.597, epoch: 6.5[0m
[32m[2022-08-30 13:01:57,395] [    INFO][0m - loss: 3.502e-05, learning_rate: 2.79e-05, global_step: 140, interval_runtime: 1.2514, interval_samples_per_second: 6.393, interval_steps_per_second: 7.991, epoch: 7.0[0m
[32m[2022-08-30 13:01:58,708] [    INFO][0m - loss: 0.00629289, learning_rate: 2.7750000000000004e-05, global_step: 150, interval_runtime: 1.3139, interval_samples_per_second: 6.089, interval_steps_per_second: 7.611, epoch: 7.5[0m
[32m[2022-08-30 13:01:59,951] [    INFO][0m - loss: 0.0001136, learning_rate: 2.7600000000000003e-05, global_step: 160, interval_runtime: 1.2425, interval_samples_per_second: 6.439, interval_steps_per_second: 8.049, epoch: 8.0[0m
[32m[2022-08-30 13:02:00,985] [    INFO][0m - loss: 1.037e-05, learning_rate: 2.7450000000000003e-05, global_step: 170, interval_runtime: 1.0344, interval_samples_per_second: 7.734, interval_steps_per_second: 9.667, epoch: 8.5[0m
[32m[2022-08-30 13:02:01,978] [    INFO][0m - loss: 8.61e-06, learning_rate: 2.7300000000000003e-05, global_step: 180, interval_runtime: 0.993, interval_samples_per_second: 8.056, interval_steps_per_second: 10.07, epoch: 9.0[0m
[32m[2022-08-30 13:02:03,014] [    INFO][0m - loss: 7.96e-06, learning_rate: 2.7150000000000003e-05, global_step: 190, interval_runtime: 1.0361, interval_samples_per_second: 7.722, interval_steps_per_second: 9.652, epoch: 9.5[0m
[32m[2022-08-30 13:02:04,012] [    INFO][0m - loss: 1.339e-05, learning_rate: 2.7000000000000002e-05, global_step: 200, interval_runtime: 0.9976, interval_samples_per_second: 8.019, interval_steps_per_second: 10.024, epoch: 10.0[0m
[32m[2022-08-30 13:02:04,012] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 13:02:04,013] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 13:02:04,013] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 13:02:04,013] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 13:02:04,013] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 13:02:04,504] [    INFO][0m - eval_loss: 3.1701300144195557, eval_accuracy: 0.60625, eval_runtime: 0.4907, eval_samples_per_second: 326.044, eval_steps_per_second: 10.189, epoch: 10.0[0m
[32m[2022-08-30 13:02:04,504] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-30 13:02:04,504] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 13:02:11,991] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-30 13:02:11,992] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-30 13:02:29,421] [    INFO][0m - loss: 6.42e-06, learning_rate: 2.6850000000000002e-05, global_step: 210, interval_runtime: 25.4085, interval_samples_per_second: 0.315, interval_steps_per_second: 0.394, epoch: 10.5[0m
[32m[2022-08-30 13:02:30,425] [    INFO][0m - loss: 1.115e-05, learning_rate: 2.6700000000000002e-05, global_step: 220, interval_runtime: 1.0042, interval_samples_per_second: 7.966, interval_steps_per_second: 9.958, epoch: 11.0[0m
[32m[2022-08-30 13:02:31,453] [    INFO][0m - loss: 8.09e-06, learning_rate: 2.655e-05, global_step: 230, interval_runtime: 1.0285, interval_samples_per_second: 7.778, interval_steps_per_second: 9.723, epoch: 11.5[0m
[32m[2022-08-30 13:02:32,456] [    INFO][0m - loss: 8.89e-06, learning_rate: 2.64e-05, global_step: 240, interval_runtime: 1.0025, interval_samples_per_second: 7.98, interval_steps_per_second: 9.975, epoch: 12.0[0m
[32m[2022-08-30 13:02:33,504] [    INFO][0m - loss: 7.83e-06, learning_rate: 2.625e-05, global_step: 250, interval_runtime: 1.0483, interval_samples_per_second: 7.631, interval_steps_per_second: 9.539, epoch: 12.5[0m
[32m[2022-08-30 13:02:34,489] [    INFO][0m - loss: 7.83e-06, learning_rate: 2.61e-05, global_step: 260, interval_runtime: 0.9837, interval_samples_per_second: 8.132, interval_steps_per_second: 10.165, epoch: 13.0[0m
[32m[2022-08-30 13:02:35,523] [    INFO][0m - loss: 5.89e-06, learning_rate: 2.595e-05, global_step: 270, interval_runtime: 1.0356, interval_samples_per_second: 7.725, interval_steps_per_second: 9.656, epoch: 13.5[0m
[32m[2022-08-30 13:02:36,522] [    INFO][0m - loss: 9.2e-06, learning_rate: 2.58e-05, global_step: 280, interval_runtime: 0.9982, interval_samples_per_second: 8.015, interval_steps_per_second: 10.018, epoch: 14.0[0m
[32m[2022-08-30 13:02:37,553] [    INFO][0m - loss: 6.42e-06, learning_rate: 2.565e-05, global_step: 290, interval_runtime: 1.0312, interval_samples_per_second: 7.758, interval_steps_per_second: 9.698, epoch: 14.5[0m
[32m[2022-08-30 13:02:38,539] [    INFO][0m - loss: 6.92e-06, learning_rate: 2.55e-05, global_step: 300, interval_runtime: 0.9862, interval_samples_per_second: 8.112, interval_steps_per_second: 10.14, epoch: 15.0[0m
[32m[2022-08-30 13:02:38,540] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 13:02:38,540] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 13:02:38,540] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 13:02:38,540] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 13:02:38,540] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 13:02:39,024] [    INFO][0m - eval_loss: 3.2240824699401855, eval_accuracy: 0.6, eval_runtime: 0.4836, eval_samples_per_second: 330.837, eval_steps_per_second: 10.339, epoch: 15.0[0m
[32m[2022-08-30 13:02:39,024] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-30 13:02:39,024] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 13:02:46,589] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-30 13:02:46,589] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-30 13:03:03,442] [    INFO][0m - loss: 6.94e-06, learning_rate: 2.535e-05, global_step: 310, interval_runtime: 24.9031, interval_samples_per_second: 0.321, interval_steps_per_second: 0.402, epoch: 15.5[0m
[32m[2022-08-30 13:03:04,639] [    INFO][0m - loss: 5.51e-06, learning_rate: 2.52e-05, global_step: 320, interval_runtime: 1.1962, interval_samples_per_second: 6.688, interval_steps_per_second: 8.36, epoch: 16.0[0m
[32m[2022-08-30 13:03:05,887] [    INFO][0m - loss: 7.82e-06, learning_rate: 2.505e-05, global_step: 330, interval_runtime: 1.2491, interval_samples_per_second: 6.405, interval_steps_per_second: 8.006, epoch: 16.5[0m
[32m[2022-08-30 13:03:07,080] [    INFO][0m - loss: 4.94e-06, learning_rate: 2.49e-05, global_step: 340, interval_runtime: 1.1926, interval_samples_per_second: 6.708, interval_steps_per_second: 8.385, epoch: 17.0[0m
[32m[2022-08-30 13:03:08,105] [    INFO][0m - loss: 6.34e-06, learning_rate: 2.475e-05, global_step: 350, interval_runtime: 1.0245, interval_samples_per_second: 7.809, interval_steps_per_second: 9.761, epoch: 17.5[0m
[32m[2022-08-30 13:03:09,081] [    INFO][0m - loss: 5.36e-06, learning_rate: 2.4599999999999998e-05, global_step: 360, interval_runtime: 0.9766, interval_samples_per_second: 8.192, interval_steps_per_second: 10.24, epoch: 18.0[0m
[32m[2022-08-30 13:03:10,098] [    INFO][0m - loss: 6.55e-06, learning_rate: 2.4449999999999998e-05, global_step: 370, interval_runtime: 1.0172, interval_samples_per_second: 7.865, interval_steps_per_second: 9.831, epoch: 18.5[0m
[32m[2022-08-30 13:03:11,075] [    INFO][0m - loss: 4.4e-06, learning_rate: 2.43e-05, global_step: 380, interval_runtime: 0.9766, interval_samples_per_second: 8.192, interval_steps_per_second: 10.24, epoch: 19.0[0m
[32m[2022-08-30 13:03:12,104] [    INFO][0m - loss: 6.57e-06, learning_rate: 2.415e-05, global_step: 390, interval_runtime: 1.0295, interval_samples_per_second: 7.771, interval_steps_per_second: 9.713, epoch: 19.5[0m
[32m[2022-08-30 13:03:13,087] [    INFO][0m - loss: 5.71e-06, learning_rate: 2.4e-05, global_step: 400, interval_runtime: 0.9828, interval_samples_per_second: 8.14, interval_steps_per_second: 10.175, epoch: 20.0[0m
[32m[2022-08-30 13:03:13,088] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 13:03:13,088] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 13:03:13,088] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 13:03:13,088] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 13:03:13,088] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 13:03:13,583] [    INFO][0m - eval_loss: 3.2724366188049316, eval_accuracy: 0.6125, eval_runtime: 0.4951, eval_samples_per_second: 323.194, eval_steps_per_second: 10.1, epoch: 20.0[0m
[32m[2022-08-30 13:03:13,584] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-30 13:03:13,584] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 13:03:21,299] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-30 13:03:21,299] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-30 13:03:37,221] [    INFO][0m - loss: 4.56e-06, learning_rate: 2.385e-05, global_step: 410, interval_runtime: 24.1338, interval_samples_per_second: 0.331, interval_steps_per_second: 0.414, epoch: 20.5[0m
[32m[2022-08-30 13:03:38,206] [    INFO][0m - loss: 6.76e-06, learning_rate: 2.37e-05, global_step: 420, interval_runtime: 0.9849, interval_samples_per_second: 8.123, interval_steps_per_second: 10.154, epoch: 21.0[0m
[32m[2022-08-30 13:03:39,243] [    INFO][0m - loss: 4.11e-06, learning_rate: 2.3550000000000003e-05, global_step: 430, interval_runtime: 1.0375, interval_samples_per_second: 7.711, interval_steps_per_second: 9.638, epoch: 21.5[0m
[32m[2022-08-30 13:03:40,303] [    INFO][0m - loss: 4.74e-06, learning_rate: 2.3400000000000003e-05, global_step: 440, interval_runtime: 1.0591, interval_samples_per_second: 7.554, interval_steps_per_second: 9.442, epoch: 22.0[0m
[32m[2022-08-30 13:03:41,564] [    INFO][0m - loss: 4.28e-06, learning_rate: 2.3250000000000003e-05, global_step: 450, interval_runtime: 1.2608, interval_samples_per_second: 6.345, interval_steps_per_second: 7.932, epoch: 22.5[0m
[32m[2022-08-30 13:03:42,658] [    INFO][0m - loss: 4.82e-06, learning_rate: 2.3100000000000002e-05, global_step: 460, interval_runtime: 1.0946, interval_samples_per_second: 7.309, interval_steps_per_second: 9.136, epoch: 23.0[0m
[32m[2022-08-30 13:03:43,696] [    INFO][0m - loss: 4.52e-06, learning_rate: 2.2950000000000002e-05, global_step: 470, interval_runtime: 1.038, interval_samples_per_second: 7.707, interval_steps_per_second: 9.634, epoch: 23.5[0m
[32m[2022-08-30 13:03:44,678] [    INFO][0m - loss: 5.03e-06, learning_rate: 2.2800000000000002e-05, global_step: 480, interval_runtime: 0.9825, interval_samples_per_second: 8.142, interval_steps_per_second: 10.178, epoch: 24.0[0m
[32m[2022-08-30 13:03:45,699] [    INFO][0m - loss: 4.43e-06, learning_rate: 2.265e-05, global_step: 490, interval_runtime: 1.0204, interval_samples_per_second: 7.84, interval_steps_per_second: 9.8, epoch: 24.5[0m
[32m[2022-08-30 13:03:46,683] [    INFO][0m - loss: 4.09e-06, learning_rate: 2.25e-05, global_step: 500, interval_runtime: 0.9838, interval_samples_per_second: 8.132, interval_steps_per_second: 10.164, epoch: 25.0[0m
[32m[2022-08-30 13:03:46,684] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 13:03:46,684] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 13:03:46,684] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 13:03:46,684] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 13:03:46,684] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 13:03:47,171] [    INFO][0m - eval_loss: 3.3155345916748047, eval_accuracy: 0.6125, eval_runtime: 0.4871, eval_samples_per_second: 328.486, eval_steps_per_second: 10.265, epoch: 25.0[0m
[32m[2022-08-30 13:03:47,172] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-08-30 13:03:47,172] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 13:03:54,840] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-08-30 13:03:54,840] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-08-30 13:04:11,236] [    INFO][0m - loss: 4.18e-06, learning_rate: 2.235e-05, global_step: 510, interval_runtime: 24.5531, interval_samples_per_second: 0.326, interval_steps_per_second: 0.407, epoch: 25.5[0m
[32m[2022-08-30 13:04:12,239] [    INFO][0m - loss: 4.19e-06, learning_rate: 2.22e-05, global_step: 520, interval_runtime: 1.0031, interval_samples_per_second: 7.975, interval_steps_per_second: 9.969, epoch: 26.0[0m
[32m[2022-08-30 13:04:13,283] [    INFO][0m - loss: 4.87e-06, learning_rate: 2.205e-05, global_step: 530, interval_runtime: 1.0438, interval_samples_per_second: 7.664, interval_steps_per_second: 9.58, epoch: 26.5[0m
[32m[2022-08-30 13:04:14,275] [    INFO][0m - loss: 3.98e-06, learning_rate: 2.19e-05, global_step: 540, interval_runtime: 0.9917, interval_samples_per_second: 8.067, interval_steps_per_second: 10.084, epoch: 27.0[0m
[32m[2022-08-30 13:04:15,301] [    INFO][0m - loss: 4.88e-06, learning_rate: 2.175e-05, global_step: 550, interval_runtime: 1.0265, interval_samples_per_second: 7.793, interval_steps_per_second: 9.742, epoch: 27.5[0m
[32m[2022-08-30 13:04:16,296] [    INFO][0m - loss: 3.97e-06, learning_rate: 2.16e-05, global_step: 560, interval_runtime: 0.9937, interval_samples_per_second: 8.051, interval_steps_per_second: 10.064, epoch: 28.0[0m
[32m[2022-08-30 13:04:17,342] [    INFO][0m - loss: 3.47e-06, learning_rate: 2.145e-05, global_step: 570, interval_runtime: 1.0476, interval_samples_per_second: 7.636, interval_steps_per_second: 9.545, epoch: 28.5[0m
[32m[2022-08-30 13:04:18,356] [    INFO][0m - loss: 4.36e-06, learning_rate: 2.13e-05, global_step: 580, interval_runtime: 1.0133, interval_samples_per_second: 7.895, interval_steps_per_second: 9.869, epoch: 29.0[0m
[32m[2022-08-30 13:04:19,385] [    INFO][0m - loss: 4.47e-06, learning_rate: 2.115e-05, global_step: 590, interval_runtime: 1.0292, interval_samples_per_second: 7.773, interval_steps_per_second: 9.716, epoch: 29.5[0m
[32m[2022-08-30 13:04:20,429] [    INFO][0m - loss: 3.39e-06, learning_rate: 2.1e-05, global_step: 600, interval_runtime: 1.0439, interval_samples_per_second: 7.664, interval_steps_per_second: 9.58, epoch: 30.0[0m
[32m[2022-08-30 13:04:20,430] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 13:04:20,430] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 13:04:20,430] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 13:04:20,430] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 13:04:20,431] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 13:04:20,919] [    INFO][0m - eval_loss: 3.354531764984131, eval_accuracy: 0.6125, eval_runtime: 0.4885, eval_samples_per_second: 327.561, eval_steps_per_second: 10.236, epoch: 30.0[0m
[32m[2022-08-30 13:04:20,919] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-08-30 13:04:20,919] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 13:04:29,069] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-08-30 13:04:29,070] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-08-30 13:04:45,615] [    INFO][0m - loss: 4.24e-06, learning_rate: 2.085e-05, global_step: 610, interval_runtime: 25.186, interval_samples_per_second: 0.318, interval_steps_per_second: 0.397, epoch: 30.5[0m
[32m[2022-08-30 13:04:46,607] [    INFO][0m - loss: 3.42e-06, learning_rate: 2.07e-05, global_step: 620, interval_runtime: 0.9919, interval_samples_per_second: 8.065, interval_steps_per_second: 10.081, epoch: 31.0[0m
[32m[2022-08-30 13:04:47,649] [    INFO][0m - loss: 4.44e-06, learning_rate: 2.055e-05, global_step: 630, interval_runtime: 1.0426, interval_samples_per_second: 7.673, interval_steps_per_second: 9.592, epoch: 31.5[0m
[32m[2022-08-30 13:04:48,644] [    INFO][0m - loss: 2.95e-06, learning_rate: 2.04e-05, global_step: 640, interval_runtime: 0.9945, interval_samples_per_second: 8.044, interval_steps_per_second: 10.055, epoch: 32.0[0m
[32m[2022-08-30 13:04:49,678] [    INFO][0m - loss: 3.7e-06, learning_rate: 2.025e-05, global_step: 650, interval_runtime: 1.0345, interval_samples_per_second: 7.734, interval_steps_per_second: 9.667, epoch: 32.5[0m
[32m[2022-08-30 13:04:50,671] [    INFO][0m - loss: 2.88e-06, learning_rate: 2.01e-05, global_step: 660, interval_runtime: 0.9926, interval_samples_per_second: 8.059, interval_steps_per_second: 10.074, epoch: 33.0[0m
[32m[2022-08-30 13:04:51,702] [    INFO][0m - loss: 3.04e-06, learning_rate: 1.995e-05, global_step: 670, interval_runtime: 1.0314, interval_samples_per_second: 7.756, interval_steps_per_second: 9.696, epoch: 33.5[0m
[32m[2022-08-30 13:04:52,690] [    INFO][0m - loss: 2.87e-06, learning_rate: 1.98e-05, global_step: 680, interval_runtime: 0.9882, interval_samples_per_second: 8.096, interval_steps_per_second: 10.12, epoch: 34.0[0m
[32m[2022-08-30 13:04:53,721] [    INFO][0m - loss: 3.31e-06, learning_rate: 1.965e-05, global_step: 690, interval_runtime: 1.031, interval_samples_per_second: 7.759, interval_steps_per_second: 9.699, epoch: 34.5[0m
[32m[2022-08-30 13:04:54,714] [    INFO][0m - loss: 2.97e-06, learning_rate: 1.95e-05, global_step: 700, interval_runtime: 0.9924, interval_samples_per_second: 8.061, interval_steps_per_second: 10.076, epoch: 35.0[0m
[32m[2022-08-30 13:04:54,715] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 13:04:54,715] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 13:04:54,715] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 13:04:54,715] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 13:04:54,715] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 13:04:55,200] [    INFO][0m - eval_loss: 3.389251708984375, eval_accuracy: 0.6125, eval_runtime: 0.4846, eval_samples_per_second: 330.203, eval_steps_per_second: 10.319, epoch: 35.0[0m
[32m[2022-08-30 13:04:55,200] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-700[0m
[32m[2022-08-30 13:04:55,200] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 13:05:02,243] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-700/tokenizer_config.json[0m
[32m[2022-08-30 13:05:02,244] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-700/special_tokens_map.json[0m
[32m[2022-08-30 13:05:18,103] [    INFO][0m - loss: 3.01e-06, learning_rate: 1.935e-05, global_step: 710, interval_runtime: 23.3887, interval_samples_per_second: 0.342, interval_steps_per_second: 0.428, epoch: 35.5[0m
[32m[2022-08-30 13:05:19,090] [    INFO][0m - loss: 3.39e-06, learning_rate: 1.9200000000000003e-05, global_step: 720, interval_runtime: 0.9876, interval_samples_per_second: 8.1, interval_steps_per_second: 10.125, epoch: 36.0[0m
[32m[2022-08-30 13:05:20,116] [    INFO][0m - loss: 2.93e-06, learning_rate: 1.9050000000000002e-05, global_step: 730, interval_runtime: 1.0254, interval_samples_per_second: 7.802, interval_steps_per_second: 9.753, epoch: 36.5[0m
[32m[2022-08-30 13:05:21,107] [    INFO][0m - loss: 3.08e-06, learning_rate: 1.8900000000000002e-05, global_step: 740, interval_runtime: 0.9911, interval_samples_per_second: 8.072, interval_steps_per_second: 10.09, epoch: 37.0[0m
[32m[2022-08-30 13:05:22,129] [    INFO][0m - loss: 3e-06, learning_rate: 1.8750000000000002e-05, global_step: 750, interval_runtime: 1.0226, interval_samples_per_second: 7.823, interval_steps_per_second: 9.779, epoch: 37.5[0m
[32m[2022-08-30 13:05:23,114] [    INFO][0m - loss: 3.58e-06, learning_rate: 1.86e-05, global_step: 760, interval_runtime: 0.9841, interval_samples_per_second: 8.129, interval_steps_per_second: 10.161, epoch: 38.0[0m
[32m[2022-08-30 13:05:24,154] [    INFO][0m - loss: 2.65e-06, learning_rate: 1.845e-05, global_step: 770, interval_runtime: 1.0401, interval_samples_per_second: 7.692, interval_steps_per_second: 9.615, epoch: 38.5[0m
[32m[2022-08-30 13:05:25,140] [    INFO][0m - loss: 3.02e-06, learning_rate: 1.83e-05, global_step: 780, interval_runtime: 0.9862, interval_samples_per_second: 8.112, interval_steps_per_second: 10.14, epoch: 39.0[0m
[32m[2022-08-30 13:05:26,163] [    INFO][0m - loss: 2.74e-06, learning_rate: 1.815e-05, global_step: 790, interval_runtime: 1.0237, interval_samples_per_second: 7.815, interval_steps_per_second: 9.769, epoch: 39.5[0m
[32m[2022-08-30 13:05:27,149] [    INFO][0m - loss: 2.77e-06, learning_rate: 1.8e-05, global_step: 800, interval_runtime: 0.9853, interval_samples_per_second: 8.119, interval_steps_per_second: 10.149, epoch: 40.0[0m
[32m[2022-08-30 13:05:27,149] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 13:05:27,150] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 13:05:27,150] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 13:05:27,150] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 13:05:27,150] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 13:05:27,632] [    INFO][0m - eval_loss: 3.4207510948181152, eval_accuracy: 0.6125, eval_runtime: 0.4824, eval_samples_per_second: 331.707, eval_steps_per_second: 10.366, epoch: 40.0[0m
[32m[2022-08-30 13:05:27,633] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-800[0m
[32m[2022-08-30 13:05:27,633] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 13:05:34,839] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-800/tokenizer_config.json[0m
[32m[2022-08-30 13:05:34,839] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-800/special_tokens_map.json[0m
[32m[2022-08-30 13:05:49,027] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-30 13:05:49,027] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-400 (score: 0.6125).[0m
[32m[2022-08-30 13:05:50,896] [    INFO][0m - train_runtime: 273.8754, train_samples_per_second: 58.421, train_steps_per_second: 7.303, train_loss: 0.04483528515369926, epoch: 40.0[0m
[32m[2022-08-30 13:05:50,898] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-30 13:05:50,898] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 13:05:57,970] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-30 13:05:57,971] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-30 13:05:57,972] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-30 13:05:57,972] [    INFO][0m -   epoch                    =       40.0[0m
[32m[2022-08-30 13:05:57,973] [    INFO][0m -   train_loss               =     0.0448[0m
[32m[2022-08-30 13:05:57,973] [    INFO][0m -   train_runtime            = 0:04:33.87[0m
[32m[2022-08-30 13:05:57,973] [    INFO][0m -   train_samples_per_second =     58.421[0m
[32m[2022-08-30 13:05:57,973] [    INFO][0m -   train_steps_per_second   =      7.303[0m
[32m[2022-08-30 13:05:57,977] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-30 13:05:57,977] [    INFO][0m -   Num examples = 1772[0m
[32m[2022-08-30 13:05:57,977] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 13:05:57,977] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 13:05:57,977] [    INFO][0m -   Total prediction steps = 56[0m
[32m[2022-08-30 13:06:03,266] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-30 13:06:03,266] [    INFO][0m -   test_accuracy           =     0.6292[0m
[32m[2022-08-30 13:06:03,266] [    INFO][0m -   test_loss               =      2.906[0m
[32m[2022-08-30 13:06:03,266] [    INFO][0m -   test_runtime            = 0:00:05.28[0m
[32m[2022-08-30 13:06:03,266] [    INFO][0m -   test_samples_per_second =    335.052[0m
[32m[2022-08-30 13:06:03,266] [    INFO][0m -   test_steps_per_second   =     10.589[0m
[32m[2022-08-30 13:06:03,267] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-30 13:06:03,267] [    INFO][0m -   Num examples = 2000[0m
[32m[2022-08-30 13:06:03,267] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 13:06:03,267] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 13:06:03,267] [    INFO][0m -   Total prediction steps = 63[0m
[32m[2022-08-30 13:06:10,606] [    INFO][0m - Predictions for bustm saved to ./fewclue_submit_examples.[0m
metric <function main.<locals>.compute_metrics at 0x7f152de96a60>
 
==========
chid
==========
 
[33m[2022-08-30 13:06:14,921] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-30 13:06:14,922] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-30 13:06:14,922] [    INFO][0m - ============================================================[0m
[32m[2022-08-30 13:06:14,922] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-30 13:06:14,922] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-30 13:06:14,922] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-30 13:06:14,922] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-30 13:06:14,922] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-08-30 13:06:14,923] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-30 13:06:14,923] [    INFO][0m - model_name_or_path            :hfl/roberta-wwm-ext-large[0m
[32m[2022-08-30 13:06:14,923] [    INFO][0m - [0m
[32m[2022-08-30 13:06:14,923] [    INFO][0m - ============================================================[0m
[32m[2022-08-30 13:06:14,923] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-30 13:06:14,923] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-30 13:06:14,923] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-30 13:06:14,923] [    INFO][0m - prompt                        :{'text':'text_a'}{'sep'}{'hard':'这句话'}{'mask'}{'hard':'通顺。'}[0m
[32m[2022-08-30 13:06:14,923] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-30 13:06:14,923] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-30 13:06:14,923] [    INFO][0m - task_name                     :chid[0m
[32m[2022-08-30 13:06:14,923] [    INFO][0m - [0m
[32m[2022-08-30 13:06:14,924] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/hfl/roberta-wwm-ext-large/roberta_chn_large.pdparams[0m
W0830 13:06:14.925077 44633 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0830 13:06:14.930560 44633 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-30 13:06:19,384] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/hfl/roberta-wwm-ext-large/vocab.txt[0m
[32m[2022-08-30 13:06:19,397] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/hfl/roberta-wwm-ext-large/tokenizer_config.json[0m
[32m[2022-08-30 13:06:19,398] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/hfl/roberta-wwm-ext-large/special_tokens_map.json[0m
[32m[2022-08-30 13:06:19,398] [    INFO][0m - Using template: [{'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'sep': None}, {'add_prefix_space': '', 'hard': '这句话'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': '通顺。'}][0m
2022-08-30 13:06:19,405 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-30 13:06:19,691] [    INFO][0m - ============================================================[0m
[32m[2022-08-30 13:06:19,691] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-30 13:06:19,691] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-30 13:06:19,692] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-30 13:06:19,692] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-30 13:06:19,692] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-30 13:06:19,692] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-30 13:06:19,692] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-30 13:06:19,692] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-30 13:06:19,692] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-30 13:06:19,692] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-30 13:06:19,692] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-30 13:06:19,692] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-30 13:06:19,692] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-30 13:06:19,693] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-30 13:06:19,693] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-30 13:06:19,693] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-30 13:06:19,693] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-30 13:06:19,693] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-30 13:06:19,693] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-30 13:06:19,693] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-30 13:06:19,693] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-30 13:06:19,693] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-30 13:06:19,693] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-30 13:06:19,693] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-30 13:06:19,693] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-30 13:06:19,693] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-30 13:06:19,694] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-30 13:06:19,694] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-30 13:06:19,694] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-30 13:06:19,694] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-30 13:06:19,694] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-30 13:06:19,694] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-30 13:06:19,694] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-30 13:06:19,694] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-30 13:06:19,694] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-30 13:06:19,694] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-30 13:06:19,694] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug30_13-06-14_instance-3bwob41y-01[0m
[32m[2022-08-30 13:06:19,694] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-30 13:06:19,694] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-30 13:06:19,695] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-30 13:06:19,695] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-30 13:06:19,695] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-30 13:06:19,695] [    INFO][0m - max_seq_length                :256[0m
[32m[2022-08-30 13:06:19,695] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-30 13:06:19,695] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-30 13:06:19,695] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-30 13:06:19,695] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-30 13:06:19,695] [    INFO][0m - num_train_epochs              :100.0[0m
[32m[2022-08-30 13:06:19,695] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-30 13:06:19,695] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-30 13:06:19,695] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-30 13:06:19,695] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-30 13:06:19,696] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-30 13:06:19,696] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-30 13:06:19,696] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-30 13:06:19,696] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-30 13:06:19,696] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-30 13:06:19,696] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-30 13:06:19,696] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-30 13:06:19,696] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-30 13:06:19,696] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-30 13:06:19,696] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-30 13:06:19,696] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-30 13:06:19,696] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-30 13:06:19,696] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-30 13:06:19,696] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-30 13:06:19,697] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-30 13:06:19,697] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-30 13:06:19,697] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-30 13:06:19,697] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-30 13:06:19,697] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-30 13:06:19,697] [    INFO][0m - seed                          :42[0m
[32m[2022-08-30 13:06:19,697] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-30 13:06:19,697] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-30 13:06:19,697] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-30 13:06:19,697] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-30 13:06:19,697] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-30 13:06:19,697] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-30 13:06:19,697] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-30 13:06:19,698] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-30 13:06:19,698] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-30 13:06:19,698] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-30 13:06:19,698] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-30 13:06:19,698] [    INFO][0m - [0m
[32m[2022-08-30 13:06:19,701] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-30 13:06:19,701] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-08-30 13:06:19,701] [    INFO][0m -   Num Epochs = 100[0m
[32m[2022-08-30 13:06:19,701] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-30 13:06:19,702] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-30 13:06:19,702] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-30 13:06:19,702] [    INFO][0m -   Total optimization steps = 17700.0[0m
[32m[2022-08-30 13:06:19,702] [    INFO][0m -   Total num train samples = 141400[0m
[32m[2022-08-30 13:06:24,767] [    INFO][0m - loss: 0.96598616, learning_rate: 2.9983050847457627e-05, global_step: 10, interval_runtime: 5.0639, interval_samples_per_second: 1.58, interval_steps_per_second: 1.975, epoch: 0.0565[0m
[32m[2022-08-30 13:06:28,668] [    INFO][0m - loss: 0.38757837, learning_rate: 2.9966101694915256e-05, global_step: 20, interval_runtime: 3.901, interval_samples_per_second: 2.051, interval_steps_per_second: 2.563, epoch: 0.113[0m
[32m[2022-08-30 13:06:32,583] [    INFO][0m - loss: 0.46861525, learning_rate: 2.9949152542372882e-05, global_step: 30, interval_runtime: 3.9154, interval_samples_per_second: 2.043, interval_steps_per_second: 2.554, epoch: 0.1695[0m
[32m[2022-08-30 13:06:36,508] [    INFO][0m - loss: 0.48911242, learning_rate: 2.9932203389830508e-05, global_step: 40, interval_runtime: 3.9247, interval_samples_per_second: 2.038, interval_steps_per_second: 2.548, epoch: 0.226[0m
[32m[2022-08-30 13:06:40,420] [    INFO][0m - loss: 0.35915976, learning_rate: 2.9915254237288134e-05, global_step: 50, interval_runtime: 3.9127, interval_samples_per_second: 2.045, interval_steps_per_second: 2.556, epoch: 0.2825[0m
[32m[2022-08-30 13:06:44,332] [    INFO][0m - loss: 0.27777619, learning_rate: 2.9898305084745767e-05, global_step: 60, interval_runtime: 3.9116, interval_samples_per_second: 2.045, interval_steps_per_second: 2.556, epoch: 0.339[0m
[32m[2022-08-30 13:06:48,264] [    INFO][0m - loss: 0.68482876, learning_rate: 2.9881355932203393e-05, global_step: 70, interval_runtime: 3.9325, interval_samples_per_second: 2.034, interval_steps_per_second: 2.543, epoch: 0.3955[0m
[32m[2022-08-30 13:06:52,189] [    INFO][0m - loss: 0.50251703, learning_rate: 2.986440677966102e-05, global_step: 80, interval_runtime: 3.9242, interval_samples_per_second: 2.039, interval_steps_per_second: 2.548, epoch: 0.452[0m
[32m[2022-08-30 13:06:56,114] [    INFO][0m - loss: 0.46277289, learning_rate: 2.9847457627118645e-05, global_step: 90, interval_runtime: 3.9259, interval_samples_per_second: 2.038, interval_steps_per_second: 2.547, epoch: 0.5085[0m
[32m[2022-08-30 13:07:00,034] [    INFO][0m - loss: 0.41249547, learning_rate: 2.9830508474576274e-05, global_step: 100, interval_runtime: 3.9198, interval_samples_per_second: 2.041, interval_steps_per_second: 2.551, epoch: 0.565[0m
[32m[2022-08-30 13:07:00,035] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 13:07:00,035] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-08-30 13:07:00,035] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 13:07:00,035] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 13:07:00,035] [    INFO][0m -   Total prediction steps = 45[0m
[32m[2022-08-30 13:07:23,260] [    INFO][0m - eval_loss: 0.4111037254333496, eval_accuracy: 0.2623762376237624, eval_runtime: 23.2244, eval_samples_per_second: 60.884, eval_steps_per_second: 1.938, epoch: 0.565[0m
[32m[2022-08-30 13:07:23,261] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-30 13:07:23,261] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 13:07:32,065] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-30 13:07:32,065] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-30 13:07:50,230] [    INFO][0m - loss: 0.43636808, learning_rate: 2.98135593220339e-05, global_step: 110, interval_runtime: 50.1955, interval_samples_per_second: 0.159, interval_steps_per_second: 0.199, epoch: 0.6215[0m
[32m[2022-08-30 13:07:54,144] [    INFO][0m - loss: 0.36832361, learning_rate: 2.9796610169491526e-05, global_step: 120, interval_runtime: 3.9143, interval_samples_per_second: 2.044, interval_steps_per_second: 2.555, epoch: 0.678[0m
[32m[2022-08-30 13:07:58,076] [    INFO][0m - loss: 0.45987759, learning_rate: 2.9779661016949152e-05, global_step: 130, interval_runtime: 3.932, interval_samples_per_second: 2.035, interval_steps_per_second: 2.543, epoch: 0.7345[0m
[32m[2022-08-30 13:08:02,008] [    INFO][0m - loss: 0.50803561, learning_rate: 2.976271186440678e-05, global_step: 140, interval_runtime: 3.9315, interval_samples_per_second: 2.035, interval_steps_per_second: 2.544, epoch: 0.791[0m
[32m[2022-08-30 13:08:05,936] [    INFO][0m - loss: 0.52840796, learning_rate: 2.9745762711864407e-05, global_step: 150, interval_runtime: 3.9283, interval_samples_per_second: 2.037, interval_steps_per_second: 2.546, epoch: 0.8475[0m
[32m[2022-08-30 13:08:09,857] [    INFO][0m - loss: 0.50271525, learning_rate: 2.9728813559322033e-05, global_step: 160, interval_runtime: 3.9211, interval_samples_per_second: 2.04, interval_steps_per_second: 2.55, epoch: 0.904[0m
[32m[2022-08-30 13:08:13,784] [    INFO][0m - loss: 0.38032629, learning_rate: 2.9711864406779662e-05, global_step: 170, interval_runtime: 3.9273, interval_samples_per_second: 2.037, interval_steps_per_second: 2.546, epoch: 0.9605[0m
[32m[2022-08-30 13:08:17,699] [    INFO][0m - loss: 0.41340237, learning_rate: 2.9694915254237292e-05, global_step: 180, interval_runtime: 3.9151, interval_samples_per_second: 2.043, interval_steps_per_second: 2.554, epoch: 1.0169[0m
[32m[2022-08-30 13:08:21,639] [    INFO][0m - loss: 0.48727212, learning_rate: 2.9677966101694918e-05, global_step: 190, interval_runtime: 3.94, interval_samples_per_second: 2.03, interval_steps_per_second: 2.538, epoch: 1.0734[0m
[32m[2022-08-30 13:08:25,558] [    INFO][0m - loss: 0.43115635, learning_rate: 2.9661016949152544e-05, global_step: 200, interval_runtime: 3.9191, interval_samples_per_second: 2.041, interval_steps_per_second: 2.552, epoch: 1.1299[0m
[32m[2022-08-30 13:08:25,559] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 13:08:25,559] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-08-30 13:08:25,559] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 13:08:25,559] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 13:08:25,559] [    INFO][0m -   Total prediction steps = 45[0m
[32m[2022-08-30 13:08:48,741] [    INFO][0m - eval_loss: 0.4104170501232147, eval_accuracy: 0.2079207920792079, eval_runtime: 23.1817, eval_samples_per_second: 60.996, eval_steps_per_second: 1.941, epoch: 1.1299[0m
[32m[2022-08-30 13:08:48,742] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-30 13:08:48,742] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 13:08:55,930] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-30 13:08:55,931] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-30 13:09:14,057] [    INFO][0m - loss: 0.38092456, learning_rate: 2.964406779661017e-05, global_step: 210, interval_runtime: 48.4986, interval_samples_per_second: 0.165, interval_steps_per_second: 0.206, epoch: 1.1864[0m
[32m[2022-08-30 13:09:17,977] [    INFO][0m - loss: 0.50194559, learning_rate: 2.96271186440678e-05, global_step: 220, interval_runtime: 3.9196, interval_samples_per_second: 2.041, interval_steps_per_second: 2.551, epoch: 1.2429[0m
[32m[2022-08-30 13:09:21,899] [    INFO][0m - loss: 0.46266294, learning_rate: 2.9610169491525425e-05, global_step: 230, interval_runtime: 3.9223, interval_samples_per_second: 2.04, interval_steps_per_second: 2.55, epoch: 1.2994[0m
[32m[2022-08-30 13:09:25,823] [    INFO][0m - loss: 0.53293943, learning_rate: 2.959322033898305e-05, global_step: 240, interval_runtime: 3.9241, interval_samples_per_second: 2.039, interval_steps_per_second: 2.548, epoch: 1.3559[0m
[32m[2022-08-30 13:09:29,755] [    INFO][0m - loss: 0.28991296, learning_rate: 2.9576271186440677e-05, global_step: 250, interval_runtime: 3.9314, interval_samples_per_second: 2.035, interval_steps_per_second: 2.544, epoch: 1.4124[0m
[32m[2022-08-30 13:09:33,682] [    INFO][0m - loss: 0.51711588, learning_rate: 2.9559322033898306e-05, global_step: 260, interval_runtime: 3.9273, interval_samples_per_second: 2.037, interval_steps_per_second: 2.546, epoch: 1.4689[0m
[32m[2022-08-30 13:09:37,613] [    INFO][0m - loss: 0.39206729, learning_rate: 2.9542372881355932e-05, global_step: 270, interval_runtime: 3.9309, interval_samples_per_second: 2.035, interval_steps_per_second: 2.544, epoch: 1.5254[0m
[32m[2022-08-30 13:09:41,545] [    INFO][0m - loss: 0.49938254, learning_rate: 2.9525423728813558e-05, global_step: 280, interval_runtime: 3.9323, interval_samples_per_second: 2.034, interval_steps_per_second: 2.543, epoch: 1.5819[0m
[32m[2022-08-30 13:09:45,481] [    INFO][0m - loss: 0.31340923, learning_rate: 2.9508474576271187e-05, global_step: 290, interval_runtime: 3.9366, interval_samples_per_second: 2.032, interval_steps_per_second: 2.54, epoch: 1.6384[0m
[32m[2022-08-30 13:09:49,413] [    INFO][0m - loss: 0.49338441, learning_rate: 2.9491525423728817e-05, global_step: 300, interval_runtime: 3.9317, interval_samples_per_second: 2.035, interval_steps_per_second: 2.543, epoch: 1.6949[0m
[32m[2022-08-30 13:09:49,414] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 13:09:49,414] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-08-30 13:09:49,414] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 13:09:49,414] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 13:09:49,414] [    INFO][0m -   Total prediction steps = 45[0m
[32m[2022-08-30 13:10:12,625] [    INFO][0m - eval_loss: 0.42496612668037415, eval_accuracy: 0.32673267326732675, eval_runtime: 23.2109, eval_samples_per_second: 60.92, eval_steps_per_second: 1.939, epoch: 1.6949[0m
[32m[2022-08-30 13:10:12,626] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-30 13:10:12,626] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 13:10:19,635] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-30 13:10:19,636] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-30 13:10:37,897] [    INFO][0m - loss: 0.39013977, learning_rate: 2.9474576271186443e-05, global_step: 310, interval_runtime: 48.4842, interval_samples_per_second: 0.165, interval_steps_per_second: 0.206, epoch: 1.7514[0m
[32m[2022-08-30 13:10:41,819] [    INFO][0m - loss: 0.38274431, learning_rate: 2.945762711864407e-05, global_step: 320, interval_runtime: 3.9215, interval_samples_per_second: 2.04, interval_steps_per_second: 2.55, epoch: 1.8079[0m
[32m[2022-08-30 13:10:45,741] [    INFO][0m - loss: 0.55654259, learning_rate: 2.9440677966101695e-05, global_step: 330, interval_runtime: 3.9225, interval_samples_per_second: 2.04, interval_steps_per_second: 2.549, epoch: 1.8644[0m
[32m[2022-08-30 13:10:49,672] [    INFO][0m - loss: 0.35619788, learning_rate: 2.9423728813559324e-05, global_step: 340, interval_runtime: 3.9305, interval_samples_per_second: 2.035, interval_steps_per_second: 2.544, epoch: 1.9209[0m
[32m[2022-08-30 13:10:53,601] [    INFO][0m - loss: 0.34087474, learning_rate: 2.940677966101695e-05, global_step: 350, interval_runtime: 3.9294, interval_samples_per_second: 2.036, interval_steps_per_second: 2.545, epoch: 1.9774[0m
[32m[2022-08-30 13:10:57,492] [    INFO][0m - loss: 0.4047287, learning_rate: 2.9389830508474576e-05, global_step: 360, interval_runtime: 3.891, interval_samples_per_second: 2.056, interval_steps_per_second: 2.57, epoch: 2.0339[0m
[32m[2022-08-30 13:11:01,432] [    INFO][0m - loss: 0.33574815, learning_rate: 2.9372881355932202e-05, global_step: 370, interval_runtime: 3.9402, interval_samples_per_second: 2.03, interval_steps_per_second: 2.538, epoch: 2.0904[0m
[32m[2022-08-30 13:11:05,373] [    INFO][0m - loss: 0.47719164, learning_rate: 2.935593220338983e-05, global_step: 380, interval_runtime: 3.941, interval_samples_per_second: 2.03, interval_steps_per_second: 2.537, epoch: 2.1469[0m
[32m[2022-08-30 13:11:09,302] [    INFO][0m - loss: 0.61724319, learning_rate: 2.9338983050847457e-05, global_step: 390, interval_runtime: 3.9289, interval_samples_per_second: 2.036, interval_steps_per_second: 2.545, epoch: 2.2034[0m
[32m[2022-08-30 13:11:13,244] [    INFO][0m - loss: 0.42247014, learning_rate: 2.9322033898305087e-05, global_step: 400, interval_runtime: 3.9417, interval_samples_per_second: 2.03, interval_steps_per_second: 2.537, epoch: 2.2599[0m
[32m[2022-08-30 13:11:13,245] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 13:11:13,245] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-08-30 13:11:13,245] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 13:11:13,245] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 13:11:13,245] [    INFO][0m -   Total prediction steps = 45[0m
[32m[2022-08-30 13:11:36,331] [    INFO][0m - eval_loss: 0.48055902123451233, eval_accuracy: 0.08415841584158416, eval_runtime: 23.0861, eval_samples_per_second: 61.249, eval_steps_per_second: 1.949, epoch: 2.2599[0m
[32m[2022-08-30 13:11:36,332] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-30 13:11:36,332] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 13:11:43,518] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-30 13:11:43,518] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-30 13:12:01,619] [    INFO][0m - loss: 0.94213028, learning_rate: 2.9305084745762713e-05, global_step: 410, interval_runtime: 48.3749, interval_samples_per_second: 0.165, interval_steps_per_second: 0.207, epoch: 2.3164[0m
[32m[2022-08-30 13:12:05,536] [    INFO][0m - loss: 0.34853661, learning_rate: 2.9288135593220342e-05, global_step: 420, interval_runtime: 3.917, interval_samples_per_second: 2.042, interval_steps_per_second: 2.553, epoch: 2.3729[0m
[32m[2022-08-30 13:12:09,458] [    INFO][0m - loss: 0.49389191, learning_rate: 2.9271186440677968e-05, global_step: 430, interval_runtime: 3.9217, interval_samples_per_second: 2.04, interval_steps_per_second: 2.55, epoch: 2.4294[0m
[32m[2022-08-30 13:12:13,378] [    INFO][0m - loss: 0.58014655, learning_rate: 2.9254237288135594e-05, global_step: 440, interval_runtime: 3.9206, interval_samples_per_second: 2.04, interval_steps_per_second: 2.551, epoch: 2.4859[0m
[32m[2022-08-30 13:12:17,312] [    INFO][0m - loss: 0.49895048, learning_rate: 2.923728813559322e-05, global_step: 450, interval_runtime: 3.9335, interval_samples_per_second: 2.034, interval_steps_per_second: 2.542, epoch: 2.5424[0m
[32m[2022-08-30 13:12:21,245] [    INFO][0m - loss: 0.58522124, learning_rate: 2.922033898305085e-05, global_step: 460, interval_runtime: 3.9336, interval_samples_per_second: 2.034, interval_steps_per_second: 2.542, epoch: 2.5989[0m
[32m[2022-08-30 13:12:25,185] [    INFO][0m - loss: 0.54531031, learning_rate: 2.9203389830508475e-05, global_step: 470, interval_runtime: 3.9392, interval_samples_per_second: 2.031, interval_steps_per_second: 2.539, epoch: 2.6554[0m
[32m[2022-08-30 13:12:29,124] [    INFO][0m - loss: 0.43321886, learning_rate: 2.91864406779661e-05, global_step: 480, interval_runtime: 3.9397, interval_samples_per_second: 2.031, interval_steps_per_second: 2.538, epoch: 2.7119[0m
[32m[2022-08-30 13:12:33,058] [    INFO][0m - loss: 0.41080694, learning_rate: 2.9169491525423727e-05, global_step: 490, interval_runtime: 3.934, interval_samples_per_second: 2.034, interval_steps_per_second: 2.542, epoch: 2.7684[0m
[32m[2022-08-30 13:12:36,992] [    INFO][0m - loss: 0.38089337, learning_rate: 2.9152542372881356e-05, global_step: 500, interval_runtime: 3.9334, interval_samples_per_second: 2.034, interval_steps_per_second: 2.542, epoch: 2.8249[0m
[32m[2022-08-30 13:12:36,992] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 13:12:36,992] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-08-30 13:12:36,992] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 13:12:36,992] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 13:12:36,993] [    INFO][0m -   Total prediction steps = 45[0m
[32m[2022-08-30 13:13:00,093] [    INFO][0m - eval_loss: 0.4589107036590576, eval_accuracy: 0.15346534653465346, eval_runtime: 23.1008, eval_samples_per_second: 61.21, eval_steps_per_second: 1.948, epoch: 2.8249[0m
[32m[2022-08-30 13:13:00,094] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-08-30 13:13:00,094] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 13:13:06,857] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-08-30 13:13:06,857] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-08-30 13:13:24,625] [    INFO][0m - loss: 0.39222276, learning_rate: 2.9135593220338986e-05, global_step: 510, interval_runtime: 47.6334, interval_samples_per_second: 0.168, interval_steps_per_second: 0.21, epoch: 2.8814[0m
[32m[2022-08-30 13:13:28,540] [    INFO][0m - loss: 0.36428089, learning_rate: 2.911864406779661e-05, global_step: 520, interval_runtime: 3.9153, interval_samples_per_second: 2.043, interval_steps_per_second: 2.554, epoch: 2.9379[0m
[32m[2022-08-30 13:13:32,460] [    INFO][0m - loss: 0.30635631, learning_rate: 2.9101694915254238e-05, global_step: 530, interval_runtime: 3.92, interval_samples_per_second: 2.041, interval_steps_per_second: 2.551, epoch: 2.9944[0m
[32m[2022-08-30 13:13:36,349] [    INFO][0m - loss: 0.36753683, learning_rate: 2.9084745762711867e-05, global_step: 540, interval_runtime: 3.888, interval_samples_per_second: 2.058, interval_steps_per_second: 2.572, epoch: 3.0508[0m
[32m[2022-08-30 13:13:40,294] [    INFO][0m - loss: 0.40791683, learning_rate: 2.9067796610169493e-05, global_step: 550, interval_runtime: 3.9455, interval_samples_per_second: 2.028, interval_steps_per_second: 2.535, epoch: 3.1073[0m
[32m[2022-08-30 13:13:44,219] [    INFO][0m - loss: 0.45389643, learning_rate: 2.905084745762712e-05, global_step: 560, interval_runtime: 3.9248, interval_samples_per_second: 2.038, interval_steps_per_second: 2.548, epoch: 3.1638[0m
[32m[2022-08-30 13:13:48,150] [    INFO][0m - loss: 0.44717445, learning_rate: 2.9033898305084745e-05, global_step: 570, interval_runtime: 3.9317, interval_samples_per_second: 2.035, interval_steps_per_second: 2.543, epoch: 3.2203[0m
[32m[2022-08-30 13:13:52,077] [    INFO][0m - loss: 0.41910019, learning_rate: 2.9016949152542374e-05, global_step: 580, interval_runtime: 3.927, interval_samples_per_second: 2.037, interval_steps_per_second: 2.546, epoch: 3.2768[0m
[32m[2022-08-30 13:13:56,005] [    INFO][0m - loss: 0.45030971, learning_rate: 2.9e-05, global_step: 590, interval_runtime: 3.9269, interval_samples_per_second: 2.037, interval_steps_per_second: 2.547, epoch: 3.3333[0m
[32m[2022-08-30 13:13:59,935] [    INFO][0m - loss: 0.47178121, learning_rate: 2.8983050847457626e-05, global_step: 600, interval_runtime: 3.9309, interval_samples_per_second: 2.035, interval_steps_per_second: 2.544, epoch: 3.3898[0m
[32m[2022-08-30 13:13:59,936] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 13:13:59,936] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-08-30 13:13:59,936] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 13:13:59,936] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 13:13:59,936] [    INFO][0m -   Total prediction steps = 45[0m
[32m[2022-08-30 13:14:23,002] [    INFO][0m - eval_loss: 0.4103202223777771, eval_accuracy: 0.15841584158415842, eval_runtime: 23.0653, eval_samples_per_second: 61.304, eval_steps_per_second: 1.951, epoch: 3.3898[0m
[32m[2022-08-30 13:14:23,002] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-08-30 13:14:23,003] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 13:14:30,176] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-08-30 13:14:30,176] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-08-30 13:14:48,942] [    INFO][0m - loss: 0.37680643, learning_rate: 2.8966101694915252e-05, global_step: 610, interval_runtime: 49.0062, interval_samples_per_second: 0.163, interval_steps_per_second: 0.204, epoch: 3.4463[0m
[32m[2022-08-30 13:14:52,857] [    INFO][0m - loss: 0.43700886, learning_rate: 2.894915254237288e-05, global_step: 620, interval_runtime: 3.9146, interval_samples_per_second: 2.044, interval_steps_per_second: 2.555, epoch: 3.5028[0m
[32m[2022-08-30 13:14:56,773] [    INFO][0m - loss: 0.41553335, learning_rate: 2.893220338983051e-05, global_step: 630, interval_runtime: 3.9166, interval_samples_per_second: 2.043, interval_steps_per_second: 2.553, epoch: 3.5593[0m
[32m[2022-08-30 13:15:00,693] [    INFO][0m - loss: 0.45501766, learning_rate: 2.8915254237288137e-05, global_step: 640, interval_runtime: 3.9198, interval_samples_per_second: 2.041, interval_steps_per_second: 2.551, epoch: 3.6158[0m
[32m[2022-08-30 13:15:04,625] [    INFO][0m - loss: 0.37466464, learning_rate: 2.8898305084745763e-05, global_step: 650, interval_runtime: 3.9319, interval_samples_per_second: 2.035, interval_steps_per_second: 2.543, epoch: 3.6723[0m
[32m[2022-08-30 13:15:08,557] [    INFO][0m - loss: 0.40672021, learning_rate: 2.8881355932203392e-05, global_step: 660, interval_runtime: 3.9323, interval_samples_per_second: 2.034, interval_steps_per_second: 2.543, epoch: 3.7288[0m
[32m[2022-08-30 13:15:12,484] [    INFO][0m - loss: 0.33091514, learning_rate: 2.8864406779661018e-05, global_step: 670, interval_runtime: 3.9267, interval_samples_per_second: 2.037, interval_steps_per_second: 2.547, epoch: 3.7853[0m
[32m[2022-08-30 13:15:16,415] [    INFO][0m - loss: 0.40926881, learning_rate: 2.8847457627118644e-05, global_step: 680, interval_runtime: 3.9312, interval_samples_per_second: 2.035, interval_steps_per_second: 2.544, epoch: 3.8418[0m
[32m[2022-08-30 13:15:20,343] [    INFO][0m - loss: 0.41910791, learning_rate: 2.883050847457627e-05, global_step: 690, interval_runtime: 3.9279, interval_samples_per_second: 2.037, interval_steps_per_second: 2.546, epoch: 3.8983[0m
[32m[2022-08-30 13:15:24,273] [    INFO][0m - loss: 0.46104994, learning_rate: 2.88135593220339e-05, global_step: 700, interval_runtime: 3.9304, interval_samples_per_second: 2.035, interval_steps_per_second: 2.544, epoch: 3.9548[0m
[32m[2022-08-30 13:15:24,273] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 13:15:24,274] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-08-30 13:15:24,274] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 13:15:24,274] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 13:15:24,274] [    INFO][0m -   Total prediction steps = 45[0m
[32m[2022-08-30 13:15:47,323] [    INFO][0m - eval_loss: 0.4103703498840332, eval_accuracy: 0.12376237623762376, eval_runtime: 23.0485, eval_samples_per_second: 61.349, eval_steps_per_second: 1.952, epoch: 3.9548[0m
[32m[2022-08-30 13:15:47,323] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-700[0m
[32m[2022-08-30 13:15:47,323] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 13:15:54,669] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-700/tokenizer_config.json[0m
[32m[2022-08-30 13:15:54,670] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-700/special_tokens_map.json[0m
[32m[2022-08-30 13:16:08,786] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-30 13:16:08,786] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-300 (score: 0.32673267326732675).[0m
[32m[2022-08-30 13:16:10,424] [    INFO][0m - train_runtime: 590.7211, train_samples_per_second: 239.368, train_steps_per_second: 29.963, train_loss: 0.4511446942601885, epoch: 3.9548[0m
[32m[2022-08-30 13:16:10,426] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-30 13:16:10,426] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 13:16:17,466] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-30 13:16:17,466] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-30 13:16:17,468] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-30 13:16:17,468] [    INFO][0m -   epoch                    =     3.9548[0m
[32m[2022-08-30 13:16:17,468] [    INFO][0m -   train_loss               =     0.4511[0m
[32m[2022-08-30 13:16:17,468] [    INFO][0m -   train_runtime            = 0:09:50.72[0m
[32m[2022-08-30 13:16:17,468] [    INFO][0m -   train_samples_per_second =    239.368[0m
[32m[2022-08-30 13:16:17,468] [    INFO][0m -   train_steps_per_second   =     29.963[0m
[32m[2022-08-30 13:16:17,472] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-30 13:16:17,472] [    INFO][0m -   Num examples = 14014[0m
[32m[2022-08-30 13:16:17,472] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 13:16:17,473] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 13:16:17,473] [    INFO][0m -   Total prediction steps = 438[0m
[32m[2022-08-30 13:20:08,283] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-30 13:20:08,284] [    INFO][0m -   test_accuracy           =     0.3007[0m
[32m[2022-08-30 13:20:08,284] [    INFO][0m -   test_loss               =      0.425[0m
[32m[2022-08-30 13:20:08,284] [    INFO][0m -   test_runtime            = 0:03:50.81[0m
[32m[2022-08-30 13:20:08,284] [    INFO][0m -   test_samples_per_second =     60.716[0m
[32m[2022-08-30 13:20:08,284] [    INFO][0m -   test_steps_per_second   =      1.898[0m
[32m[2022-08-30 13:20:08,285] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-30 13:20:08,285] [    INFO][0m -   Num examples = 14000[0m
[32m[2022-08-30 13:20:08,285] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 13:20:08,285] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 13:20:08,285] [    INFO][0m -   Total prediction steps = 438[0m
[32m[2022-08-30 13:24:17,066] [    INFO][0m - Predictions for chidf saved to ./fewclue_submit_examples.[0m
metric <function main.<locals>.chid_compute_metrics at 0x7f47fc24a3a0>
 
==========
csl
==========
 
[33m[2022-08-30 13:24:21,347] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-30 13:24:21,347] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-30 13:24:21,348] [    INFO][0m - ============================================================[0m
[32m[2022-08-30 13:24:21,348] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-30 13:24:21,348] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-30 13:24:21,348] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-30 13:24:21,348] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-30 13:24:21,349] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-08-30 13:24:21,349] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-30 13:24:21,349] [    INFO][0m - model_name_or_path            :hfl/roberta-wwm-ext-large[0m
[32m[2022-08-30 13:24:21,349] [    INFO][0m - [0m
[32m[2022-08-30 13:24:21,349] [    INFO][0m - ============================================================[0m
[32m[2022-08-30 13:24:21,349] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-30 13:24:21,349] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-30 13:24:21,349] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-30 13:24:21,349] [    INFO][0m - prompt                        :{'text': 'text_a'}{'hard':'上文中找'}{'mask'}{'hard': '出这些关键词：'}{'text':'text_b'}[0m
[32m[2022-08-30 13:24:21,350] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-30 13:24:21,350] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-30 13:24:21,350] [    INFO][0m - task_name                     :csl[0m
[32m[2022-08-30 13:24:21,350] [    INFO][0m - [0m
[32m[2022-08-30 13:24:21,350] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/hfl/roberta-wwm-ext-large/roberta_chn_large.pdparams[0m
W0830 13:24:21.351648 71298 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0830 13:24:21.357013 71298 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-30 13:24:26,115] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/hfl/roberta-wwm-ext-large/vocab.txt[0m
[32m[2022-08-30 13:24:26,128] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/hfl/roberta-wwm-ext-large/tokenizer_config.json[0m
[32m[2022-08-30 13:24:26,128] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/hfl/roberta-wwm-ext-large/special_tokens_map.json[0m
[32m[2022-08-30 13:24:26,129] [    INFO][0m - Using template: [{'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': '上文中找'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': '出这些关键词：'}, {'add_prefix_space': '', 'text': 'text_b'}][0m
2022-08-30 13:24:26,136 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-30 13:24:26,291] [    INFO][0m - ============================================================[0m
[32m[2022-08-30 13:24:26,291] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-30 13:24:26,291] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-30 13:24:26,291] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-30 13:24:26,291] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-30 13:24:26,291] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-30 13:24:26,292] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-30 13:24:26,292] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-30 13:24:26,292] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-30 13:24:26,292] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-30 13:24:26,292] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-30 13:24:26,292] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-30 13:24:26,292] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-30 13:24:26,292] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-30 13:24:26,292] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-30 13:24:26,292] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-30 13:24:26,292] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-30 13:24:26,292] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-30 13:24:26,292] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-30 13:24:26,293] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-30 13:24:26,293] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-30 13:24:26,293] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-30 13:24:26,293] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-30 13:24:26,293] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-30 13:24:26,293] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-30 13:24:26,293] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-30 13:24:26,293] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-30 13:24:26,293] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-30 13:24:26,293] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-30 13:24:26,293] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-30 13:24:26,293] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-30 13:24:26,293] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-30 13:24:26,294] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-30 13:24:26,294] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-30 13:24:26,294] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-30 13:24:26,294] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-30 13:24:26,294] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-30 13:24:26,294] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug30_13-24-21_instance-3bwob41y-01[0m
[32m[2022-08-30 13:24:26,294] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-30 13:24:26,294] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-30 13:24:26,294] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-30 13:24:26,294] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-30 13:24:26,294] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-30 13:24:26,294] [    INFO][0m - max_seq_length                :320[0m
[32m[2022-08-30 13:24:26,294] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-30 13:24:26,294] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-30 13:24:26,295] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-30 13:24:26,295] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-30 13:24:26,295] [    INFO][0m - num_train_epochs              :100.0[0m
[32m[2022-08-30 13:24:26,295] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-30 13:24:26,295] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-30 13:24:26,295] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-30 13:24:26,295] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-30 13:24:26,295] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-30 13:24:26,295] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-30 13:24:26,295] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-30 13:24:26,295] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-30 13:24:26,295] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-30 13:24:26,295] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-30 13:24:26,296] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-30 13:24:26,296] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-30 13:24:26,296] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-30 13:24:26,296] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-30 13:24:26,296] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-30 13:24:26,296] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-30 13:24:26,296] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-30 13:24:26,296] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-30 13:24:26,296] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-30 13:24:26,296] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-30 13:24:26,296] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-30 13:24:26,296] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-30 13:24:26,296] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-30 13:24:26,296] [    INFO][0m - seed                          :42[0m
[32m[2022-08-30 13:24:26,297] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-30 13:24:26,297] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-30 13:24:26,297] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-30 13:24:26,297] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-30 13:24:26,297] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-30 13:24:26,297] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-30 13:24:26,297] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-30 13:24:26,297] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-30 13:24:26,297] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-30 13:24:26,297] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-30 13:24:26,297] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-30 13:24:26,297] [    INFO][0m - [0m
[32m[2022-08-30 13:24:26,301] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-30 13:24:26,301] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 13:24:26,302] [    INFO][0m -   Num Epochs = 100[0m
[32m[2022-08-30 13:24:26,302] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-30 13:24:26,302] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-30 13:24:26,302] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-30 13:24:26,302] [    INFO][0m -   Total optimization steps = 2000.0[0m
[32m[2022-08-30 13:24:26,302] [    INFO][0m -   Total num train samples = 16000[0m
[33m[2022-08-30 13:24:28,584] [ WARNING][0m - Token indices sequence length is longer than the specified maximum sequence length for this model (608 > 512). Running this sequence through the model will result in indexing errors[0m
[32m[2022-08-30 13:24:32,334] [    INFO][0m - loss: 0.87309399, learning_rate: 2.985e-05, global_step: 10, interval_runtime: 6.0312, interval_samples_per_second: 1.326, interval_steps_per_second: 1.658, epoch: 0.5[0m
[32m[2022-08-30 13:24:36,979] [    INFO][0m - loss: 1.03004704, learning_rate: 2.97e-05, global_step: 20, interval_runtime: 4.645, interval_samples_per_second: 1.722, interval_steps_per_second: 2.153, epoch: 1.0[0m
[32m[2022-08-30 13:24:41,940] [    INFO][0m - loss: 0.68556061, learning_rate: 2.955e-05, global_step: 30, interval_runtime: 4.9609, interval_samples_per_second: 1.613, interval_steps_per_second: 2.016, epoch: 1.5[0m
[32m[2022-08-30 13:24:46,613] [    INFO][0m - loss: 0.75392647, learning_rate: 2.94e-05, global_step: 40, interval_runtime: 4.6733, interval_samples_per_second: 1.712, interval_steps_per_second: 2.14, epoch: 2.0[0m
[32m[2022-08-30 13:24:51,440] [    INFO][0m - loss: 0.59878788, learning_rate: 2.925e-05, global_step: 50, interval_runtime: 4.827, interval_samples_per_second: 1.657, interval_steps_per_second: 2.072, epoch: 2.5[0m
[32m[2022-08-30 13:24:56,105] [    INFO][0m - loss: 0.73602476, learning_rate: 2.91e-05, global_step: 60, interval_runtime: 4.6652, interval_samples_per_second: 1.715, interval_steps_per_second: 2.144, epoch: 3.0[0m
[32m[2022-08-30 13:25:00,924] [    INFO][0m - loss: 0.47704668, learning_rate: 2.895e-05, global_step: 70, interval_runtime: 4.8189, interval_samples_per_second: 1.66, interval_steps_per_second: 2.075, epoch: 3.5[0m
[32m[2022-08-30 13:25:05,590] [    INFO][0m - loss: 0.45928817, learning_rate: 2.88e-05, global_step: 80, interval_runtime: 4.666, interval_samples_per_second: 1.715, interval_steps_per_second: 2.143, epoch: 4.0[0m
[32m[2022-08-30 13:25:10,407] [    INFO][0m - loss: 0.14090142, learning_rate: 2.865e-05, global_step: 90, interval_runtime: 4.8168, interval_samples_per_second: 1.661, interval_steps_per_second: 2.076, epoch: 4.5[0m
[32m[2022-08-30 13:25:15,071] [    INFO][0m - loss: 0.32683344, learning_rate: 2.8499999999999998e-05, global_step: 100, interval_runtime: 4.6636, interval_samples_per_second: 1.715, interval_steps_per_second: 2.144, epoch: 5.0[0m
[32m[2022-08-30 13:25:15,071] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 13:25:15,072] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 13:25:15,072] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 13:25:15,072] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 13:25:15,072] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 13:25:18,511] [    INFO][0m - eval_loss: 1.4202048778533936, eval_accuracy: 0.56875, eval_runtime: 3.4394, eval_samples_per_second: 46.52, eval_steps_per_second: 1.454, epoch: 5.0[0m
[32m[2022-08-30 13:25:18,512] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-30 13:25:18,512] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 13:25:26,328] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-30 13:25:26,328] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-30 13:25:46,073] [    INFO][0m - loss: 0.0192936, learning_rate: 2.8349999999999998e-05, global_step: 110, interval_runtime: 31.0018, interval_samples_per_second: 0.258, interval_steps_per_second: 0.323, epoch: 5.5[0m
[32m[2022-08-30 13:25:50,710] [    INFO][0m - loss: 0.00781478, learning_rate: 2.8199999999999998e-05, global_step: 120, interval_runtime: 4.6372, interval_samples_per_second: 1.725, interval_steps_per_second: 2.156, epoch: 6.0[0m
[32m[2022-08-30 13:25:55,501] [    INFO][0m - loss: 7.956e-05, learning_rate: 2.805e-05, global_step: 130, interval_runtime: 4.7914, interval_samples_per_second: 1.67, interval_steps_per_second: 2.087, epoch: 6.5[0m
[32m[2022-08-30 13:26:00,151] [    INFO][0m - loss: 0.02171839, learning_rate: 2.79e-05, global_step: 140, interval_runtime: 4.6497, interval_samples_per_second: 1.721, interval_steps_per_second: 2.151, epoch: 7.0[0m
[32m[2022-08-30 13:26:04,966] [    INFO][0m - loss: 0.06562874, learning_rate: 2.7750000000000004e-05, global_step: 150, interval_runtime: 4.8153, interval_samples_per_second: 1.661, interval_steps_per_second: 2.077, epoch: 7.5[0m
[32m[2022-08-30 13:26:09,612] [    INFO][0m - loss: 0.04104929, learning_rate: 2.7600000000000003e-05, global_step: 160, interval_runtime: 4.6463, interval_samples_per_second: 1.722, interval_steps_per_second: 2.152, epoch: 8.0[0m
[32m[2022-08-30 13:26:14,447] [    INFO][0m - loss: 3.555e-05, learning_rate: 2.7450000000000003e-05, global_step: 170, interval_runtime: 4.8344, interval_samples_per_second: 1.655, interval_steps_per_second: 2.069, epoch: 8.5[0m
[32m[2022-08-30 13:26:19,093] [    INFO][0m - loss: 4.96e-05, learning_rate: 2.7300000000000003e-05, global_step: 180, interval_runtime: 4.6464, interval_samples_per_second: 1.722, interval_steps_per_second: 2.152, epoch: 9.0[0m
[32m[2022-08-30 13:26:23,905] [    INFO][0m - loss: 1.471e-05, learning_rate: 2.7150000000000003e-05, global_step: 190, interval_runtime: 4.8116, interval_samples_per_second: 1.663, interval_steps_per_second: 2.078, epoch: 9.5[0m
[32m[2022-08-30 13:26:28,565] [    INFO][0m - loss: 2.976e-05, learning_rate: 2.7000000000000002e-05, global_step: 200, interval_runtime: 4.6597, interval_samples_per_second: 1.717, interval_steps_per_second: 2.146, epoch: 10.0[0m
[32m[2022-08-30 13:26:28,565] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 13:26:28,565] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 13:26:28,565] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 13:26:28,566] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 13:26:28,566] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 13:26:32,010] [    INFO][0m - eval_loss: 3.2050323486328125, eval_accuracy: 0.5625, eval_runtime: 3.4438, eval_samples_per_second: 46.46, eval_steps_per_second: 1.452, epoch: 10.0[0m
[32m[2022-08-30 13:26:32,010] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-30 13:26:32,010] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 13:26:39,256] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-30 13:26:39,256] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-30 13:26:58,539] [    INFO][0m - loss: 1.95e-05, learning_rate: 2.6850000000000002e-05, global_step: 210, interval_runtime: 29.9743, interval_samples_per_second: 0.267, interval_steps_per_second: 0.334, epoch: 10.5[0m
[32m[2022-08-30 13:27:03,187] [    INFO][0m - loss: 2.27e-05, learning_rate: 2.6700000000000002e-05, global_step: 220, interval_runtime: 4.6482, interval_samples_per_second: 1.721, interval_steps_per_second: 2.151, epoch: 11.0[0m
[32m[2022-08-30 13:27:08,001] [    INFO][0m - loss: 2.252e-05, learning_rate: 2.655e-05, global_step: 230, interval_runtime: 4.8136, interval_samples_per_second: 1.662, interval_steps_per_second: 2.077, epoch: 11.5[0m
[32m[2022-08-30 13:27:12,643] [    INFO][0m - loss: 1.09e-05, learning_rate: 2.64e-05, global_step: 240, interval_runtime: 4.6419, interval_samples_per_second: 1.723, interval_steps_per_second: 2.154, epoch: 12.0[0m
[32m[2022-08-30 13:27:17,455] [    INFO][0m - loss: 1.212e-05, learning_rate: 2.625e-05, global_step: 250, interval_runtime: 4.8125, interval_samples_per_second: 1.662, interval_steps_per_second: 2.078, epoch: 12.5[0m
[32m[2022-08-30 13:27:22,100] [    INFO][0m - loss: 1.381e-05, learning_rate: 2.61e-05, global_step: 260, interval_runtime: 4.6444, interval_samples_per_second: 1.722, interval_steps_per_second: 2.153, epoch: 13.0[0m
[32m[2022-08-30 13:27:26,934] [    INFO][0m - loss: 1.342e-05, learning_rate: 2.595e-05, global_step: 270, interval_runtime: 4.8343, interval_samples_per_second: 1.655, interval_steps_per_second: 2.069, epoch: 13.5[0m
[32m[2022-08-30 13:27:31,583] [    INFO][0m - loss: 1.23e-05, learning_rate: 2.58e-05, global_step: 280, interval_runtime: 4.6496, interval_samples_per_second: 1.721, interval_steps_per_second: 2.151, epoch: 14.0[0m
[32m[2022-08-30 13:27:36,390] [    INFO][0m - loss: 1.281e-05, learning_rate: 2.565e-05, global_step: 290, interval_runtime: 4.8063, interval_samples_per_second: 1.664, interval_steps_per_second: 2.081, epoch: 14.5[0m
[32m[2022-08-30 13:27:41,059] [    INFO][0m - loss: 8.61e-06, learning_rate: 2.55e-05, global_step: 300, interval_runtime: 4.6695, interval_samples_per_second: 1.713, interval_steps_per_second: 2.142, epoch: 15.0[0m
[32m[2022-08-30 13:27:41,060] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 13:27:41,060] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 13:27:41,060] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 13:27:41,060] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 13:27:41,060] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 13:27:44,508] [    INFO][0m - eval_loss: 3.3657584190368652, eval_accuracy: 0.5875, eval_runtime: 3.4478, eval_samples_per_second: 46.406, eval_steps_per_second: 1.45, epoch: 15.0[0m
[32m[2022-08-30 13:27:44,509] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-30 13:27:44,509] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 13:27:51,878] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-30 13:27:51,879] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-30 13:28:12,029] [    INFO][0m - loss: 1.093e-05, learning_rate: 2.535e-05, global_step: 310, interval_runtime: 30.9693, interval_samples_per_second: 0.258, interval_steps_per_second: 0.323, epoch: 15.5[0m
[32m[2022-08-30 13:28:16,657] [    INFO][0m - loss: 9.52e-06, learning_rate: 2.52e-05, global_step: 320, interval_runtime: 4.6284, interval_samples_per_second: 1.728, interval_steps_per_second: 2.161, epoch: 16.0[0m
[32m[2022-08-30 13:28:21,467] [    INFO][0m - loss: 7.82e-06, learning_rate: 2.505e-05, global_step: 330, interval_runtime: 4.8095, interval_samples_per_second: 1.663, interval_steps_per_second: 2.079, epoch: 16.5[0m
[32m[2022-08-30 13:28:26,116] [    INFO][0m - loss: 9.46e-06, learning_rate: 2.49e-05, global_step: 340, interval_runtime: 4.6489, interval_samples_per_second: 1.721, interval_steps_per_second: 2.151, epoch: 17.0[0m
[32m[2022-08-30 13:28:30,936] [    INFO][0m - loss: 7.22e-06, learning_rate: 2.475e-05, global_step: 350, interval_runtime: 4.8199, interval_samples_per_second: 1.66, interval_steps_per_second: 2.075, epoch: 17.5[0m
[32m[2022-08-30 13:28:35,600] [    INFO][0m - loss: 9.68e-06, learning_rate: 2.4599999999999998e-05, global_step: 360, interval_runtime: 4.6644, interval_samples_per_second: 1.715, interval_steps_per_second: 2.144, epoch: 18.0[0m
[32m[2022-08-30 13:28:40,437] [    INFO][0m - loss: 7.29e-06, learning_rate: 2.4449999999999998e-05, global_step: 370, interval_runtime: 4.8366, interval_samples_per_second: 1.654, interval_steps_per_second: 2.068, epoch: 18.5[0m
[32m[2022-08-30 13:28:45,092] [    INFO][0m - loss: 8.48e-06, learning_rate: 2.43e-05, global_step: 380, interval_runtime: 4.6552, interval_samples_per_second: 1.719, interval_steps_per_second: 2.148, epoch: 19.0[0m
[32m[2022-08-30 13:28:49,923] [    INFO][0m - loss: 7.35e-06, learning_rate: 2.415e-05, global_step: 390, interval_runtime: 4.831, interval_samples_per_second: 1.656, interval_steps_per_second: 2.07, epoch: 19.5[0m
[32m[2022-08-30 13:28:54,584] [    INFO][0m - loss: 7.98e-06, learning_rate: 2.4e-05, global_step: 400, interval_runtime: 4.6611, interval_samples_per_second: 1.716, interval_steps_per_second: 2.145, epoch: 20.0[0m
[32m[2022-08-30 13:28:54,585] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 13:28:54,585] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 13:28:54,585] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 13:28:54,585] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 13:28:54,585] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 13:28:58,029] [    INFO][0m - eval_loss: 3.4714088439941406, eval_accuracy: 0.59375, eval_runtime: 3.4435, eval_samples_per_second: 46.465, eval_steps_per_second: 1.452, epoch: 20.0[0m
[32m[2022-08-30 13:28:58,029] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-30 13:28:58,030] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 13:29:05,552] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-30 13:29:05,552] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-30 13:29:24,993] [    INFO][0m - loss: 5.49e-06, learning_rate: 2.385e-05, global_step: 410, interval_runtime: 30.4087, interval_samples_per_second: 0.263, interval_steps_per_second: 0.329, epoch: 20.5[0m
[32m[2022-08-30 13:29:29,631] [    INFO][0m - loss: 7.68e-06, learning_rate: 2.37e-05, global_step: 420, interval_runtime: 4.6384, interval_samples_per_second: 1.725, interval_steps_per_second: 2.156, epoch: 21.0[0m
[32m[2022-08-30 13:29:34,430] [    INFO][0m - loss: 5.31e-06, learning_rate: 2.3550000000000003e-05, global_step: 430, interval_runtime: 4.7994, interval_samples_per_second: 1.667, interval_steps_per_second: 2.084, epoch: 21.5[0m
[32m[2022-08-30 13:29:39,075] [    INFO][0m - loss: 6.94e-06, learning_rate: 2.3400000000000003e-05, global_step: 440, interval_runtime: 4.6446, interval_samples_per_second: 1.722, interval_steps_per_second: 2.153, epoch: 22.0[0m
[32m[2022-08-30 13:29:43,874] [    INFO][0m - loss: 6.43e-06, learning_rate: 2.3250000000000003e-05, global_step: 450, interval_runtime: 4.7985, interval_samples_per_second: 1.667, interval_steps_per_second: 2.084, epoch: 22.5[0m
[32m[2022-08-30 13:29:48,525] [    INFO][0m - loss: 6.21e-06, learning_rate: 2.3100000000000002e-05, global_step: 460, interval_runtime: 4.6517, interval_samples_per_second: 1.72, interval_steps_per_second: 2.15, epoch: 23.0[0m
[32m[2022-08-30 13:29:53,341] [    INFO][0m - loss: 5.74e-06, learning_rate: 2.2950000000000002e-05, global_step: 470, interval_runtime: 4.8162, interval_samples_per_second: 1.661, interval_steps_per_second: 2.076, epoch: 23.5[0m
[32m[2022-08-30 13:29:57,991] [    INFO][0m - loss: 6.76e-06, learning_rate: 2.2800000000000002e-05, global_step: 480, interval_runtime: 4.6496, interval_samples_per_second: 1.721, interval_steps_per_second: 2.151, epoch: 24.0[0m
[32m[2022-08-30 13:30:02,797] [    INFO][0m - loss: 4.54e-06, learning_rate: 2.265e-05, global_step: 490, interval_runtime: 4.8058, interval_samples_per_second: 1.665, interval_steps_per_second: 2.081, epoch: 24.5[0m
[32m[2022-08-30 13:30:07,444] [    INFO][0m - loss: 6.49e-06, learning_rate: 2.25e-05, global_step: 500, interval_runtime: 4.6476, interval_samples_per_second: 1.721, interval_steps_per_second: 2.152, epoch: 25.0[0m
[32m[2022-08-30 13:30:07,445] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 13:30:07,445] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 13:30:07,445] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 13:30:07,445] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 13:30:07,445] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 13:30:10,873] [    INFO][0m - eval_loss: 3.553412675857544, eval_accuracy: 0.6, eval_runtime: 3.4275, eval_samples_per_second: 46.681, eval_steps_per_second: 1.459, epoch: 25.0[0m
[32m[2022-08-30 13:30:10,873] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-08-30 13:30:10,873] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 13:30:17,672] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-08-30 13:30:17,672] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-08-30 13:30:36,492] [    INFO][0m - loss: 5.99e-06, learning_rate: 2.235e-05, global_step: 510, interval_runtime: 29.0479, interval_samples_per_second: 0.275, interval_steps_per_second: 0.344, epoch: 25.5[0m
[32m[2022-08-30 13:30:41,112] [    INFO][0m - loss: 4.66e-06, learning_rate: 2.22e-05, global_step: 520, interval_runtime: 4.6201, interval_samples_per_second: 1.732, interval_steps_per_second: 2.164, epoch: 26.0[0m
[32m[2022-08-30 13:30:46,409] [    INFO][0m - loss: 4.57e-06, learning_rate: 2.205e-05, global_step: 530, interval_runtime: 4.7982, interval_samples_per_second: 1.667, interval_steps_per_second: 2.084, epoch: 26.5[0m
[32m[2022-08-30 13:30:51,064] [    INFO][0m - loss: 5.92e-06, learning_rate: 2.19e-05, global_step: 540, interval_runtime: 5.1528, interval_samples_per_second: 1.553, interval_steps_per_second: 1.941, epoch: 27.0[0m
[32m[2022-08-30 13:30:55,873] [    INFO][0m - loss: 4.61e-06, learning_rate: 2.175e-05, global_step: 550, interval_runtime: 4.8092, interval_samples_per_second: 1.663, interval_steps_per_second: 2.079, epoch: 27.5[0m
[32m[2022-08-30 13:31:00,524] [    INFO][0m - loss: 5.01e-06, learning_rate: 2.16e-05, global_step: 560, interval_runtime: 4.6513, interval_samples_per_second: 1.72, interval_steps_per_second: 2.15, epoch: 28.0[0m
[32m[2022-08-30 13:31:05,340] [    INFO][0m - loss: 3.9e-06, learning_rate: 2.145e-05, global_step: 570, interval_runtime: 4.8159, interval_samples_per_second: 1.661, interval_steps_per_second: 2.076, epoch: 28.5[0m
[32m[2022-08-30 13:31:10,000] [    INFO][0m - loss: 5.39e-06, learning_rate: 2.13e-05, global_step: 580, interval_runtime: 4.66, interval_samples_per_second: 1.717, interval_steps_per_second: 2.146, epoch: 29.0[0m
[32m[2022-08-30 13:31:14,853] [    INFO][0m - loss: 4.62e-06, learning_rate: 2.115e-05, global_step: 590, interval_runtime: 4.8529, interval_samples_per_second: 1.649, interval_steps_per_second: 2.061, epoch: 29.5[0m
[32m[2022-08-30 13:31:19,512] [    INFO][0m - loss: 4.18e-06, learning_rate: 2.1e-05, global_step: 600, interval_runtime: 4.6594, interval_samples_per_second: 1.717, interval_steps_per_second: 2.146, epoch: 30.0[0m
[32m[2022-08-30 13:31:19,513] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 13:31:19,513] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 13:31:19,513] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 13:31:19,513] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 13:31:19,513] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 13:31:22,958] [    INFO][0m - eval_loss: 3.624316453933716, eval_accuracy: 0.6, eval_runtime: 3.445, eval_samples_per_second: 46.444, eval_steps_per_second: 1.451, epoch: 30.0[0m
[32m[2022-08-30 13:31:22,959] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-08-30 13:31:22,959] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 13:31:39,390] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-08-30 13:31:39,390] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-08-30 13:32:00,388] [    INFO][0m - loss: 4.44e-06, learning_rate: 2.085e-05, global_step: 610, interval_runtime: 40.8754, interval_samples_per_second: 0.196, interval_steps_per_second: 0.245, epoch: 30.5[0m
[32m[2022-08-30 13:32:05,015] [    INFO][0m - loss: 4.24e-06, learning_rate: 2.07e-05, global_step: 620, interval_runtime: 4.6272, interval_samples_per_second: 1.729, interval_steps_per_second: 2.161, epoch: 31.0[0m
[32m[2022-08-30 13:32:09,805] [    INFO][0m - loss: 4.77e-06, learning_rate: 2.055e-05, global_step: 630, interval_runtime: 4.7906, interval_samples_per_second: 1.67, interval_steps_per_second: 2.087, epoch: 31.5[0m
[32m[2022-08-30 13:32:14,431] [    INFO][0m - loss: 3.76e-06, learning_rate: 2.04e-05, global_step: 640, interval_runtime: 4.6257, interval_samples_per_second: 1.729, interval_steps_per_second: 2.162, epoch: 32.0[0m
[32m[2022-08-30 13:32:19,239] [    INFO][0m - loss: 3.68e-06, learning_rate: 2.025e-05, global_step: 650, interval_runtime: 4.8084, interval_samples_per_second: 1.664, interval_steps_per_second: 2.08, epoch: 32.5[0m
[32m[2022-08-30 13:32:23,884] [    INFO][0m - loss: 3.64e-06, learning_rate: 2.01e-05, global_step: 660, interval_runtime: 4.6442, interval_samples_per_second: 1.723, interval_steps_per_second: 2.153, epoch: 33.0[0m
[32m[2022-08-30 13:32:28,683] [    INFO][0m - loss: 3.93e-06, learning_rate: 1.995e-05, global_step: 670, interval_runtime: 4.7992, interval_samples_per_second: 1.667, interval_steps_per_second: 2.084, epoch: 33.5[0m
[32m[2022-08-30 13:32:33,340] [    INFO][0m - loss: 3.32e-06, learning_rate: 1.98e-05, global_step: 680, interval_runtime: 4.6572, interval_samples_per_second: 1.718, interval_steps_per_second: 2.147, epoch: 34.0[0m
[32m[2022-08-30 13:32:38,154] [    INFO][0m - loss: 3.83e-06, learning_rate: 1.965e-05, global_step: 690, interval_runtime: 4.8135, interval_samples_per_second: 1.662, interval_steps_per_second: 2.077, epoch: 34.5[0m
[32m[2022-08-30 13:32:42,799] [    INFO][0m - loss: 3.39e-06, learning_rate: 1.95e-05, global_step: 700, interval_runtime: 4.6459, interval_samples_per_second: 1.722, interval_steps_per_second: 2.152, epoch: 35.0[0m
[32m[2022-08-30 13:32:42,800] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 13:32:42,800] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 13:32:42,800] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 13:32:42,800] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 13:32:42,800] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 13:32:46,228] [    INFO][0m - eval_loss: 3.6797690391540527, eval_accuracy: 0.60625, eval_runtime: 3.4273, eval_samples_per_second: 46.684, eval_steps_per_second: 1.459, epoch: 35.0[0m
[32m[2022-08-30 13:32:46,228] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-700[0m
[32m[2022-08-30 13:32:46,228] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 13:32:53,434] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-700/tokenizer_config.json[0m
[32m[2022-08-30 13:32:53,434] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-700/special_tokens_map.json[0m
[32m[2022-08-30 13:33:12,296] [    INFO][0m - loss: 4.14e-06, learning_rate: 1.935e-05, global_step: 710, interval_runtime: 29.4968, interval_samples_per_second: 0.271, interval_steps_per_second: 0.339, epoch: 35.5[0m
[32m[2022-08-30 13:33:16,939] [    INFO][0m - loss: 3.86e-06, learning_rate: 1.9200000000000003e-05, global_step: 720, interval_runtime: 4.6422, interval_samples_per_second: 1.723, interval_steps_per_second: 2.154, epoch: 36.0[0m
[32m[2022-08-30 13:33:21,734] [    INFO][0m - loss: 2.93e-06, learning_rate: 1.9050000000000002e-05, global_step: 730, interval_runtime: 4.7957, interval_samples_per_second: 1.668, interval_steps_per_second: 2.085, epoch: 36.5[0m
[32m[2022-08-30 13:33:26,361] [    INFO][0m - loss: 4.13e-06, learning_rate: 1.8900000000000002e-05, global_step: 740, interval_runtime: 4.6271, interval_samples_per_second: 1.729, interval_steps_per_second: 2.161, epoch: 37.0[0m
[32m[2022-08-30 13:33:31,148] [    INFO][0m - loss: 2.7e-06, learning_rate: 1.8750000000000002e-05, global_step: 750, interval_runtime: 4.7866, interval_samples_per_second: 1.671, interval_steps_per_second: 2.089, epoch: 37.5[0m
[32m[2022-08-30 13:33:35,786] [    INFO][0m - loss: 3.86e-06, learning_rate: 1.86e-05, global_step: 760, interval_runtime: 4.6381, interval_samples_per_second: 1.725, interval_steps_per_second: 2.156, epoch: 38.0[0m
[32m[2022-08-30 13:33:40,586] [    INFO][0m - loss: 3.43e-06, learning_rate: 1.845e-05, global_step: 770, interval_runtime: 4.7997, interval_samples_per_second: 1.667, interval_steps_per_second: 2.083, epoch: 38.5[0m
[32m[2022-08-30 13:33:45,234] [    INFO][0m - loss: 2.72e-06, learning_rate: 1.83e-05, global_step: 780, interval_runtime: 4.6485, interval_samples_per_second: 1.721, interval_steps_per_second: 2.151, epoch: 39.0[0m
[32m[2022-08-30 13:33:50,069] [    INFO][0m - loss: 3.38e-06, learning_rate: 1.815e-05, global_step: 790, interval_runtime: 4.8349, interval_samples_per_second: 1.655, interval_steps_per_second: 2.068, epoch: 39.5[0m
[32m[2022-08-30 13:33:54,721] [    INFO][0m - loss: 2.82e-06, learning_rate: 1.8e-05, global_step: 800, interval_runtime: 4.652, interval_samples_per_second: 1.72, interval_steps_per_second: 2.15, epoch: 40.0[0m
[32m[2022-08-30 13:33:54,722] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 13:33:54,722] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 13:33:54,722] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 13:33:54,722] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 13:33:54,722] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 13:33:58,159] [    INFO][0m - eval_loss: 3.7277214527130127, eval_accuracy: 0.60625, eval_runtime: 3.4367, eval_samples_per_second: 46.557, eval_steps_per_second: 1.455, epoch: 40.0[0m
[32m[2022-08-30 13:33:58,159] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-800[0m
[32m[2022-08-30 13:33:58,159] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 13:34:05,444] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-800/tokenizer_config.json[0m
[32m[2022-08-30 13:34:05,445] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-800/special_tokens_map.json[0m
[32m[2022-08-30 13:34:26,272] [    INFO][0m - loss: 2.66e-06, learning_rate: 1.785e-05, global_step: 810, interval_runtime: 31.55, interval_samples_per_second: 0.254, interval_steps_per_second: 0.317, epoch: 40.5[0m
[32m[2022-08-30 13:34:30,904] [    INFO][0m - loss: 3.3e-06, learning_rate: 1.77e-05, global_step: 820, interval_runtime: 4.6325, interval_samples_per_second: 1.727, interval_steps_per_second: 2.159, epoch: 41.0[0m
[32m[2022-08-30 13:34:35,697] [    INFO][0m - loss: 2.93e-06, learning_rate: 1.755e-05, global_step: 830, interval_runtime: 4.7925, interval_samples_per_second: 1.669, interval_steps_per_second: 2.087, epoch: 41.5[0m
[32m[2022-08-30 13:34:40,339] [    INFO][0m - loss: 2.88e-06, learning_rate: 1.74e-05, global_step: 840, interval_runtime: 4.6428, interval_samples_per_second: 1.723, interval_steps_per_second: 2.154, epoch: 42.0[0m
[32m[2022-08-30 13:34:45,152] [    INFO][0m - loss: 3.59e-06, learning_rate: 1.725e-05, global_step: 850, interval_runtime: 4.8133, interval_samples_per_second: 1.662, interval_steps_per_second: 2.078, epoch: 42.5[0m
[32m[2022-08-30 13:34:49,800] [    INFO][0m - loss: 2.4e-06, learning_rate: 1.71e-05, global_step: 860, interval_runtime: 4.6472, interval_samples_per_second: 1.721, interval_steps_per_second: 2.152, epoch: 43.0[0m
[32m[2022-08-30 13:34:54,816] [    INFO][0m - loss: 3.04e-06, learning_rate: 1.695e-05, global_step: 870, interval_runtime: 4.8648, interval_samples_per_second: 1.644, interval_steps_per_second: 2.056, epoch: 43.5[0m
[32m[2022-08-30 13:34:59,539] [    INFO][0m - loss: 2.46e-06, learning_rate: 1.6800000000000002e-05, global_step: 880, interval_runtime: 4.8742, interval_samples_per_second: 1.641, interval_steps_per_second: 2.052, epoch: 44.0[0m
[32m[2022-08-30 13:35:04,355] [    INFO][0m - loss: 3.77e-06, learning_rate: 1.665e-05, global_step: 890, interval_runtime: 4.8162, interval_samples_per_second: 1.661, interval_steps_per_second: 2.076, epoch: 44.5[0m
[32m[2022-08-30 13:35:08,981] [    INFO][0m - loss: 2e-06, learning_rate: 1.65e-05, global_step: 900, interval_runtime: 4.6264, interval_samples_per_second: 1.729, interval_steps_per_second: 2.162, epoch: 45.0[0m
[32m[2022-08-30 13:35:08,982] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 13:35:08,982] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 13:35:08,982] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 13:35:08,982] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 13:35:08,982] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 13:35:12,421] [    INFO][0m - eval_loss: 3.7711052894592285, eval_accuracy: 0.60625, eval_runtime: 3.4385, eval_samples_per_second: 46.532, eval_steps_per_second: 1.454, epoch: 45.0[0m
[32m[2022-08-30 13:35:12,421] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-900[0m
[32m[2022-08-30 13:35:12,421] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 13:35:18,906] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-900/tokenizer_config.json[0m
[32m[2022-08-30 13:35:18,906] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-900/special_tokens_map.json[0m
[32m[2022-08-30 13:35:37,807] [    INFO][0m - loss: 2.66e-06, learning_rate: 1.635e-05, global_step: 910, interval_runtime: 28.8258, interval_samples_per_second: 0.278, interval_steps_per_second: 0.347, epoch: 45.5[0m
[32m[2022-08-30 13:35:42,417] [    INFO][0m - loss: 2.5e-06, learning_rate: 1.62e-05, global_step: 920, interval_runtime: 4.6094, interval_samples_per_second: 1.736, interval_steps_per_second: 2.169, epoch: 46.0[0m
[32m[2022-08-30 13:35:47,211] [    INFO][0m - loss: 3.2e-06, learning_rate: 1.605e-05, global_step: 930, interval_runtime: 4.7948, interval_samples_per_second: 1.668, interval_steps_per_second: 2.086, epoch: 46.5[0m
[32m[2022-08-30 13:35:51,843] [    INFO][0m - loss: 1.91e-06, learning_rate: 1.59e-05, global_step: 940, interval_runtime: 4.6316, interval_samples_per_second: 1.727, interval_steps_per_second: 2.159, epoch: 47.0[0m
[32m[2022-08-30 13:35:56,647] [    INFO][0m - loss: 2.61e-06, learning_rate: 1.575e-05, global_step: 950, interval_runtime: 4.804, interval_samples_per_second: 1.665, interval_steps_per_second: 2.082, epoch: 47.5[0m
[32m[2022-08-30 13:36:01,282] [    INFO][0m - loss: 2.64e-06, learning_rate: 1.56e-05, global_step: 960, interval_runtime: 4.6354, interval_samples_per_second: 1.726, interval_steps_per_second: 2.157, epoch: 48.0[0m
[32m[2022-08-30 13:36:06,105] [    INFO][0m - loss: 2.72e-06, learning_rate: 1.545e-05, global_step: 970, interval_runtime: 4.8221, interval_samples_per_second: 1.659, interval_steps_per_second: 2.074, epoch: 48.5[0m
[32m[2022-08-30 13:36:10,750] [    INFO][0m - loss: 2.64e-06, learning_rate: 1.53e-05, global_step: 980, interval_runtime: 4.6454, interval_samples_per_second: 1.722, interval_steps_per_second: 2.153, epoch: 49.0[0m
[32m[2022-08-30 13:36:15,559] [    INFO][0m - loss: 2.67e-06, learning_rate: 1.515e-05, global_step: 990, interval_runtime: 4.809, interval_samples_per_second: 1.664, interval_steps_per_second: 2.079, epoch: 49.5[0m
[32m[2022-08-30 13:36:20,200] [    INFO][0m - loss: 2.2e-06, learning_rate: 1.5e-05, global_step: 1000, interval_runtime: 4.6414, interval_samples_per_second: 1.724, interval_steps_per_second: 2.155, epoch: 50.0[0m
[32m[2022-08-30 13:36:20,201] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 13:36:20,201] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 13:36:20,201] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 13:36:20,201] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 13:36:20,201] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 13:36:23,638] [    INFO][0m - eval_loss: 3.8071084022521973, eval_accuracy: 0.60625, eval_runtime: 3.4364, eval_samples_per_second: 46.56, eval_steps_per_second: 1.455, epoch: 50.0[0m
[32m[2022-08-30 13:36:23,638] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1000[0m
[32m[2022-08-30 13:36:23,638] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 13:36:30,053] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1000/tokenizer_config.json[0m
[32m[2022-08-30 13:36:30,374] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1000/special_tokens_map.json[0m
[32m[2022-08-30 13:36:49,064] [    INFO][0m - loss: 2.66e-06, learning_rate: 1.485e-05, global_step: 1010, interval_runtime: 28.8633, interval_samples_per_second: 0.277, interval_steps_per_second: 0.346, epoch: 50.5[0m
[32m[2022-08-30 13:36:53,691] [    INFO][0m - loss: 2.15e-06, learning_rate: 1.47e-05, global_step: 1020, interval_runtime: 4.6272, interval_samples_per_second: 1.729, interval_steps_per_second: 2.161, epoch: 51.0[0m
[32m[2022-08-30 13:36:58,485] [    INFO][0m - loss: 1.77e-06, learning_rate: 1.455e-05, global_step: 1030, interval_runtime: 4.7943, interval_samples_per_second: 1.669, interval_steps_per_second: 2.086, epoch: 51.5[0m
[32m[2022-08-30 13:37:03,130] [    INFO][0m - loss: 2.99e-06, learning_rate: 1.44e-05, global_step: 1040, interval_runtime: 4.6449, interval_samples_per_second: 1.722, interval_steps_per_second: 2.153, epoch: 52.0[0m
[32m[2022-08-30 13:37:07,945] [    INFO][0m - loss: 2.11e-06, learning_rate: 1.4249999999999999e-05, global_step: 1050, interval_runtime: 4.8151, interval_samples_per_second: 1.661, interval_steps_per_second: 2.077, epoch: 52.5[0m
[32m[2022-08-30 13:37:12,584] [    INFO][0m - loss: 2.25e-06, learning_rate: 1.4099999999999999e-05, global_step: 1060, interval_runtime: 4.6389, interval_samples_per_second: 1.725, interval_steps_per_second: 2.156, epoch: 53.0[0m
[32m[2022-08-30 13:37:17,401] [    INFO][0m - loss: 1.79e-06, learning_rate: 1.395e-05, global_step: 1070, interval_runtime: 4.8163, interval_samples_per_second: 1.661, interval_steps_per_second: 2.076, epoch: 53.5[0m
[32m[2022-08-30 13:37:22,040] [    INFO][0m - loss: 2.49e-06, learning_rate: 1.3800000000000002e-05, global_step: 1080, interval_runtime: 4.6399, interval_samples_per_second: 1.724, interval_steps_per_second: 2.155, epoch: 54.0[0m
[32m[2022-08-30 13:37:26,848] [    INFO][0m - loss: 2.18e-06, learning_rate: 1.3650000000000001e-05, global_step: 1090, interval_runtime: 4.8072, interval_samples_per_second: 1.664, interval_steps_per_second: 2.08, epoch: 54.5[0m
[32m[2022-08-30 13:37:31,497] [    INFO][0m - loss: 2.16e-06, learning_rate: 1.3500000000000001e-05, global_step: 1100, interval_runtime: 4.6495, interval_samples_per_second: 1.721, interval_steps_per_second: 2.151, epoch: 55.0[0m
[32m[2022-08-30 13:37:31,498] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 13:37:31,498] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 13:37:31,498] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 13:37:31,498] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 13:37:31,498] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 13:37:34,935] [    INFO][0m - eval_loss: 3.8385539054870605, eval_accuracy: 0.60625, eval_runtime: 3.4361, eval_samples_per_second: 46.564, eval_steps_per_second: 1.455, epoch: 55.0[0m
[32m[2022-08-30 13:37:34,935] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1100[0m
[32m[2022-08-30 13:37:34,935] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 13:37:41,850] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-1100/tokenizer_config.json[0m
[32m[2022-08-30 13:37:41,850] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-1100/special_tokens_map.json[0m
[32m[2022-08-30 13:37:56,216] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-30 13:37:56,216] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-700 (score: 0.60625).[0m
[32m[2022-08-30 13:37:58,027] [    INFO][0m - train_runtime: 811.7241, train_samples_per_second: 19.711, train_steps_per_second: 2.464, train_loss: 0.05670654116430424, epoch: 55.0[0m
[32m[2022-08-30 13:37:58,028] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-30 13:37:58,029] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 13:38:05,112] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-30 13:38:05,113] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-30 13:38:05,114] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-30 13:38:05,114] [    INFO][0m -   epoch                    =       55.0[0m
[32m[2022-08-30 13:38:05,115] [    INFO][0m -   train_loss               =     0.0567[0m
[32m[2022-08-30 13:38:05,115] [    INFO][0m -   train_runtime            = 0:13:31.72[0m
[32m[2022-08-30 13:38:05,115] [    INFO][0m -   train_samples_per_second =     19.711[0m
[32m[2022-08-30 13:38:05,115] [    INFO][0m -   train_steps_per_second   =      2.464[0m
[32m[2022-08-30 13:38:05,120] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-30 13:38:05,120] [    INFO][0m -   Num examples = 2838[0m
[32m[2022-08-30 13:38:05,120] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 13:38:05,120] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 13:38:05,120] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-08-30 13:39:06,199] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-30 13:39:06,199] [    INFO][0m -   test_accuracy           =     0.5384[0m
[32m[2022-08-30 13:39:06,199] [    INFO][0m -   test_loss               =     4.4141[0m
[32m[2022-08-30 13:39:06,199] [    INFO][0m -   test_runtime            = 0:01:01.07[0m
[32m[2022-08-30 13:39:06,199] [    INFO][0m -   test_samples_per_second =     46.465[0m
[32m[2022-08-30 13:39:06,199] [    INFO][0m -   test_steps_per_second   =      1.457[0m
[32m[2022-08-30 13:39:06,200] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-30 13:39:06,200] [    INFO][0m -   Num examples = 3000[0m
[32m[2022-08-30 13:39:06,200] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 13:39:06,200] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 13:39:06,200] [    INFO][0m -   Total prediction steps = 94[0m
[32m[2022-08-30 13:40:18,755] [    INFO][0m - Predictions for cslf saved to ./fewclue_submit_examples.[0m
metric <function main.<locals>.compute_metrics at 0x7f4a424b8a60>
 
==========
cluewsc
==========
 
[33m[2022-08-30 13:40:22,854] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-30 13:40:22,854] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-30 13:40:22,854] [    INFO][0m - ============================================================[0m
[32m[2022-08-30 13:40:22,854] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-30 13:40:22,854] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-30 13:40:22,854] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-30 13:40:22,854] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-30 13:40:22,855] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-08-30 13:40:22,855] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-30 13:40:22,855] [    INFO][0m - model_name_or_path            :hfl/roberta-wwm-ext-large[0m
[32m[2022-08-30 13:40:22,855] [    INFO][0m - [0m
[32m[2022-08-30 13:40:22,855] [    INFO][0m - ============================================================[0m
[32m[2022-08-30 13:40:22,855] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-30 13:40:22,855] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-30 13:40:22,855] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-30 13:40:22,855] [    INFO][0m - prompt                        :{'text':'text_a'}{'sep'}{'hard':'其中代词用'}{'mask'}{'hard':'了。'}[0m
[32m[2022-08-30 13:40:22,855] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-30 13:40:22,855] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-30 13:40:22,855] [    INFO][0m - task_name                     :cluewsc[0m
[32m[2022-08-30 13:40:22,855] [    INFO][0m - [0m
[32m[2022-08-30 13:40:22,856] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/hfl/roberta-wwm-ext-large/roberta_chn_large.pdparams[0m
W0830 13:40:22.856936 12347 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0830 13:40:22.860824 12347 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-30 13:40:29,008] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/hfl/roberta-wwm-ext-large/vocab.txt[0m
[32m[2022-08-30 13:40:29,022] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/hfl/roberta-wwm-ext-large/tokenizer_config.json[0m
[32m[2022-08-30 13:40:29,022] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/hfl/roberta-wwm-ext-large/special_tokens_map.json[0m
[32m[2022-08-30 13:40:29,023] [    INFO][0m - Using template: [{'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'sep': None}, {'add_prefix_space': '', 'hard': '其中代词用'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': '了。'}][0m
2022-08-30 13:40:29,028 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-30 13:40:29,133] [    INFO][0m - ============================================================[0m
[32m[2022-08-30 13:40:29,133] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-30 13:40:29,133] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-30 13:40:29,133] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-30 13:40:29,133] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-30 13:40:29,134] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-30 13:40:29,134] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-30 13:40:29,134] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-30 13:40:29,134] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-30 13:40:29,134] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-30 13:40:29,134] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-30 13:40:29,134] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-30 13:40:29,134] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-30 13:40:29,134] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-30 13:40:29,134] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-30 13:40:29,134] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-30 13:40:29,134] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-30 13:40:29,135] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-30 13:40:29,135] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-30 13:40:29,135] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-30 13:40:29,135] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-30 13:40:29,135] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-30 13:40:29,135] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-30 13:40:29,135] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-30 13:40:29,135] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-30 13:40:29,135] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-30 13:40:29,135] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-30 13:40:29,135] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-30 13:40:29,135] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-30 13:40:29,135] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-30 13:40:29,135] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-30 13:40:29,136] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-30 13:40:29,136] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-30 13:40:29,136] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-30 13:40:29,136] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-30 13:40:29,136] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-30 13:40:29,136] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-30 13:40:29,136] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug30_13-40-22_instance-3bwob41y-01[0m
[32m[2022-08-30 13:40:29,136] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-30 13:40:29,136] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-30 13:40:29,136] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-30 13:40:29,136] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-30 13:40:29,136] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-30 13:40:29,136] [    INFO][0m - max_seq_length                :128[0m
[32m[2022-08-30 13:40:29,136] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-30 13:40:29,137] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-30 13:40:29,137] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-30 13:40:29,137] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-30 13:40:29,137] [    INFO][0m - num_train_epochs              :100.0[0m
[32m[2022-08-30 13:40:29,137] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-30 13:40:29,137] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-30 13:40:29,137] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-30 13:40:29,137] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-30 13:40:29,137] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-30 13:40:29,137] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-30 13:40:29,137] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-30 13:40:29,137] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-30 13:40:29,137] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-30 13:40:29,138] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-30 13:40:29,138] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-30 13:40:29,138] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-30 13:40:29,138] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-30 13:40:29,138] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-30 13:40:29,138] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-30 13:40:29,138] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-30 13:40:29,138] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-30 13:40:29,138] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-30 13:40:29,138] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-30 13:40:29,138] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-30 13:40:29,138] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-30 13:40:29,138] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-30 13:40:29,138] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-30 13:40:29,139] [    INFO][0m - seed                          :42[0m
[32m[2022-08-30 13:40:29,139] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-30 13:40:29,139] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-30 13:40:29,139] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-30 13:40:29,139] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-30 13:40:29,139] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-30 13:40:29,139] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-30 13:40:29,139] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-30 13:40:29,139] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-30 13:40:29,139] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-30 13:40:29,139] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-30 13:40:29,139] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-30 13:40:29,139] [    INFO][0m - [0m
[32m[2022-08-30 13:40:29,142] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-30 13:40:29,142] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-30 13:40:29,142] [    INFO][0m -   Num Epochs = 100[0m
[32m[2022-08-30 13:40:29,143] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-30 13:40:29,143] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-30 13:40:29,143] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-30 13:40:29,143] [    INFO][0m -   Total optimization steps = 2000.0[0m
[32m[2022-08-30 13:40:29,143] [    INFO][0m -   Total num train samples = 16000[0m
[32m[2022-08-30 13:40:32,514] [    INFO][0m - loss: 1.02335529, learning_rate: 2.985e-05, global_step: 10, interval_runtime: 3.3706, interval_samples_per_second: 2.373, interval_steps_per_second: 2.967, epoch: 0.5[0m
[32m[2022-08-30 13:40:34,664] [    INFO][0m - loss: 0.74879341, learning_rate: 2.97e-05, global_step: 20, interval_runtime: 2.1498, interval_samples_per_second: 3.721, interval_steps_per_second: 4.651, epoch: 1.0[0m
[32m[2022-08-30 13:40:36,867] [    INFO][0m - loss: 0.53881521, learning_rate: 2.955e-05, global_step: 30, interval_runtime: 2.2029, interval_samples_per_second: 3.632, interval_steps_per_second: 4.54, epoch: 1.5[0m
[32m[2022-08-30 13:40:39,015] [    INFO][0m - loss: 0.75041161, learning_rate: 2.94e-05, global_step: 40, interval_runtime: 2.1481, interval_samples_per_second: 3.724, interval_steps_per_second: 4.655, epoch: 2.0[0m
[32m[2022-08-30 13:40:41,220] [    INFO][0m - loss: 0.50739574, learning_rate: 2.925e-05, global_step: 50, interval_runtime: 2.2042, interval_samples_per_second: 3.629, interval_steps_per_second: 4.537, epoch: 2.5[0m
[32m[2022-08-30 13:40:43,373] [    INFO][0m - loss: 0.50117502, learning_rate: 2.91e-05, global_step: 60, interval_runtime: 2.1539, interval_samples_per_second: 3.714, interval_steps_per_second: 4.643, epoch: 3.0[0m
[32m[2022-08-30 13:40:45,582] [    INFO][0m - loss: 0.35767462, learning_rate: 2.895e-05, global_step: 70, interval_runtime: 2.2087, interval_samples_per_second: 3.622, interval_steps_per_second: 4.527, epoch: 3.5[0m
[32m[2022-08-30 13:40:47,738] [    INFO][0m - loss: 0.37166064, learning_rate: 2.88e-05, global_step: 80, interval_runtime: 2.1567, interval_samples_per_second: 3.709, interval_steps_per_second: 4.637, epoch: 4.0[0m
[32m[2022-08-30 13:40:49,953] [    INFO][0m - loss: 0.30213048, learning_rate: 2.865e-05, global_step: 90, interval_runtime: 2.2143, interval_samples_per_second: 3.613, interval_steps_per_second: 4.516, epoch: 4.5[0m
[32m[2022-08-30 13:40:52,105] [    INFO][0m - loss: 0.3530889, learning_rate: 2.8499999999999998e-05, global_step: 100, interval_runtime: 2.1526, interval_samples_per_second: 3.717, interval_steps_per_second: 4.646, epoch: 5.0[0m
[32m[2022-08-30 13:40:52,106] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 13:40:52,106] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-30 13:40:52,106] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 13:40:52,106] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 13:40:52,106] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 13:40:53,484] [    INFO][0m - eval_loss: 1.2256637811660767, eval_accuracy: 0.5283018867924528, eval_runtime: 1.378, eval_samples_per_second: 115.387, eval_steps_per_second: 3.629, epoch: 5.0[0m
[32m[2022-08-30 13:40:53,485] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-30 13:40:53,485] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 13:41:00,848] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-30 13:41:00,848] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-30 13:41:17,949] [    INFO][0m - loss: 0.3299371, learning_rate: 2.8349999999999998e-05, global_step: 110, interval_runtime: 25.8432, interval_samples_per_second: 0.31, interval_steps_per_second: 0.387, epoch: 5.5[0m
[32m[2022-08-30 13:41:20,092] [    INFO][0m - loss: 0.21507859, learning_rate: 2.8199999999999998e-05, global_step: 120, interval_runtime: 2.1433, interval_samples_per_second: 3.733, interval_steps_per_second: 4.666, epoch: 6.0[0m
[32m[2022-08-30 13:41:22,287] [    INFO][0m - loss: 0.2042444, learning_rate: 2.805e-05, global_step: 130, interval_runtime: 2.1944, interval_samples_per_second: 3.646, interval_steps_per_second: 4.557, epoch: 6.5[0m
[32m[2022-08-30 13:41:24,435] [    INFO][0m - loss: 0.35341809, learning_rate: 2.79e-05, global_step: 140, interval_runtime: 2.1484, interval_samples_per_second: 3.724, interval_steps_per_second: 4.655, epoch: 7.0[0m
[32m[2022-08-30 13:41:26,635] [    INFO][0m - loss: 0.24266226, learning_rate: 2.7750000000000004e-05, global_step: 150, interval_runtime: 2.2003, interval_samples_per_second: 3.636, interval_steps_per_second: 4.545, epoch: 7.5[0m
[32m[2022-08-30 13:41:28,789] [    INFO][0m - loss: 0.17832474, learning_rate: 2.7600000000000003e-05, global_step: 160, interval_runtime: 2.1536, interval_samples_per_second: 3.715, interval_steps_per_second: 4.643, epoch: 8.0[0m
[32m[2022-08-30 13:41:30,993] [    INFO][0m - loss: 0.19444472, learning_rate: 2.7450000000000003e-05, global_step: 170, interval_runtime: 2.2042, interval_samples_per_second: 3.629, interval_steps_per_second: 4.537, epoch: 8.5[0m
[32m[2022-08-30 13:41:33,147] [    INFO][0m - loss: 0.24523113, learning_rate: 2.7300000000000003e-05, global_step: 180, interval_runtime: 2.1537, interval_samples_per_second: 3.715, interval_steps_per_second: 4.643, epoch: 9.0[0m
[32m[2022-08-30 13:41:35,361] [    INFO][0m - loss: 0.2057085, learning_rate: 2.7150000000000003e-05, global_step: 190, interval_runtime: 2.2143, interval_samples_per_second: 3.613, interval_steps_per_second: 4.516, epoch: 9.5[0m
[32m[2022-08-30 13:41:37,518] [    INFO][0m - loss: 0.11772742, learning_rate: 2.7000000000000002e-05, global_step: 200, interval_runtime: 2.1573, interval_samples_per_second: 3.708, interval_steps_per_second: 4.635, epoch: 10.0[0m
[32m[2022-08-30 13:41:37,519] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 13:41:37,519] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-30 13:41:37,519] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 13:41:37,519] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 13:41:37,519] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 13:41:38,861] [    INFO][0m - eval_loss: 2.0698156356811523, eval_accuracy: 0.4276729559748428, eval_runtime: 1.3411, eval_samples_per_second: 118.564, eval_steps_per_second: 3.728, epoch: 10.0[0m
[32m[2022-08-30 13:41:38,861] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-30 13:41:38,861] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 13:41:46,063] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-30 13:41:46,063] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-30 13:42:02,607] [    INFO][0m - loss: 0.10352782, learning_rate: 2.6850000000000002e-05, global_step: 210, interval_runtime: 25.0882, interval_samples_per_second: 0.319, interval_steps_per_second: 0.399, epoch: 10.5[0m
[32m[2022-08-30 13:42:04,742] [    INFO][0m - loss: 0.22861316, learning_rate: 2.6700000000000002e-05, global_step: 220, interval_runtime: 2.1352, interval_samples_per_second: 3.747, interval_steps_per_second: 4.683, epoch: 11.0[0m
[32m[2022-08-30 13:42:06,936] [    INFO][0m - loss: 0.00866094, learning_rate: 2.655e-05, global_step: 230, interval_runtime: 2.1933, interval_samples_per_second: 3.647, interval_steps_per_second: 4.559, epoch: 11.5[0m
[32m[2022-08-30 13:42:09,080] [    INFO][0m - loss: 0.45469594, learning_rate: 2.64e-05, global_step: 240, interval_runtime: 2.1454, interval_samples_per_second: 3.729, interval_steps_per_second: 4.661, epoch: 12.0[0m
[32m[2022-08-30 13:42:11,284] [    INFO][0m - loss: 0.11415246, learning_rate: 2.625e-05, global_step: 250, interval_runtime: 2.2034, interval_samples_per_second: 3.631, interval_steps_per_second: 4.538, epoch: 12.5[0m
[32m[2022-08-30 13:42:13,435] [    INFO][0m - loss: 0.08128306, learning_rate: 2.61e-05, global_step: 260, interval_runtime: 2.1508, interval_samples_per_second: 3.72, interval_steps_per_second: 4.649, epoch: 13.0[0m
[32m[2022-08-30 13:42:15,642] [    INFO][0m - loss: 0.24343436, learning_rate: 2.595e-05, global_step: 270, interval_runtime: 2.2075, interval_samples_per_second: 3.624, interval_steps_per_second: 4.53, epoch: 13.5[0m
[32m[2022-08-30 13:42:17,796] [    INFO][0m - loss: 0.08722522, learning_rate: 2.58e-05, global_step: 280, interval_runtime: 2.154, interval_samples_per_second: 3.714, interval_steps_per_second: 4.643, epoch: 14.0[0m
[32m[2022-08-30 13:42:20,004] [    INFO][0m - loss: 0.34688768, learning_rate: 2.565e-05, global_step: 290, interval_runtime: 2.2078, interval_samples_per_second: 3.624, interval_steps_per_second: 4.529, epoch: 14.5[0m
[32m[2022-08-30 13:42:22,162] [    INFO][0m - loss: 0.16010324, learning_rate: 2.55e-05, global_step: 300, interval_runtime: 2.158, interval_samples_per_second: 3.707, interval_steps_per_second: 4.634, epoch: 15.0[0m
[32m[2022-08-30 13:42:22,162] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 13:42:22,162] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-30 13:42:22,163] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 13:42:22,163] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 13:42:22,163] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 13:42:23,507] [    INFO][0m - eval_loss: 1.9784215688705444, eval_accuracy: 0.4779874213836478, eval_runtime: 1.3437, eval_samples_per_second: 118.331, eval_steps_per_second: 3.721, epoch: 15.0[0m
[32m[2022-08-30 13:42:23,507] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-30 13:42:23,507] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 13:42:31,379] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-30 13:42:31,379] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-30 13:42:48,244] [    INFO][0m - loss: 0.07858734, learning_rate: 2.535e-05, global_step: 310, interval_runtime: 26.0821, interval_samples_per_second: 0.307, interval_steps_per_second: 0.383, epoch: 15.5[0m
[32m[2022-08-30 13:42:50,382] [    INFO][0m - loss: 0.14517432, learning_rate: 2.52e-05, global_step: 320, interval_runtime: 2.138, interval_samples_per_second: 3.742, interval_steps_per_second: 4.677, epoch: 16.0[0m
[32m[2022-08-30 13:42:52,580] [    INFO][0m - loss: 0.0798072, learning_rate: 2.505e-05, global_step: 330, interval_runtime: 2.1978, interval_samples_per_second: 3.64, interval_steps_per_second: 4.55, epoch: 16.5[0m
[32m[2022-08-30 13:42:54,724] [    INFO][0m - loss: 0.07448816, learning_rate: 2.49e-05, global_step: 340, interval_runtime: 2.1442, interval_samples_per_second: 3.731, interval_steps_per_second: 4.664, epoch: 17.0[0m
[32m[2022-08-30 13:42:56,916] [    INFO][0m - loss: 0.00953235, learning_rate: 2.475e-05, global_step: 350, interval_runtime: 2.1918, interval_samples_per_second: 3.65, interval_steps_per_second: 4.563, epoch: 17.5[0m
[32m[2022-08-30 13:42:59,063] [    INFO][0m - loss: 0.02161602, learning_rate: 2.4599999999999998e-05, global_step: 360, interval_runtime: 2.1475, interval_samples_per_second: 3.725, interval_steps_per_second: 4.657, epoch: 18.0[0m
[32m[2022-08-30 13:43:01,266] [    INFO][0m - loss: 0.00033853, learning_rate: 2.4449999999999998e-05, global_step: 370, interval_runtime: 2.2028, interval_samples_per_second: 3.632, interval_steps_per_second: 4.54, epoch: 18.5[0m
[32m[2022-08-30 13:43:03,415] [    INFO][0m - loss: 0.17698299, learning_rate: 2.43e-05, global_step: 380, interval_runtime: 2.149, interval_samples_per_second: 3.723, interval_steps_per_second: 4.653, epoch: 19.0[0m
[32m[2022-08-30 13:43:05,625] [    INFO][0m - loss: 0.00068497, learning_rate: 2.415e-05, global_step: 390, interval_runtime: 2.2103, interval_samples_per_second: 3.619, interval_steps_per_second: 4.524, epoch: 19.5[0m
[32m[2022-08-30 13:43:07,775] [    INFO][0m - loss: 0.03512506, learning_rate: 2.4e-05, global_step: 400, interval_runtime: 2.1491, interval_samples_per_second: 3.722, interval_steps_per_second: 4.653, epoch: 20.0[0m
[32m[2022-08-30 13:43:07,775] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 13:43:07,775] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-30 13:43:07,775] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 13:43:07,775] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 13:43:07,776] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 13:43:09,124] [    INFO][0m - eval_loss: 2.866312265396118, eval_accuracy: 0.5471698113207547, eval_runtime: 1.348, eval_samples_per_second: 117.95, eval_steps_per_second: 3.709, epoch: 20.0[0m
[32m[2022-08-30 13:43:09,124] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-30 13:43:09,124] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 13:43:15,949] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-30 13:43:15,949] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-30 13:43:34,056] [    INFO][0m - loss: 0.00236308, learning_rate: 2.385e-05, global_step: 410, interval_runtime: 26.2814, interval_samples_per_second: 0.304, interval_steps_per_second: 0.38, epoch: 20.5[0m
[32m[2022-08-30 13:43:36,191] [    INFO][0m - loss: 0.06529495, learning_rate: 2.37e-05, global_step: 420, interval_runtime: 2.1353, interval_samples_per_second: 3.747, interval_steps_per_second: 4.683, epoch: 21.0[0m
[32m[2022-08-30 13:43:38,387] [    INFO][0m - loss: 0.00032846, learning_rate: 2.3550000000000003e-05, global_step: 430, interval_runtime: 2.1958, interval_samples_per_second: 3.643, interval_steps_per_second: 4.554, epoch: 21.5[0m
[32m[2022-08-30 13:43:40,534] [    INFO][0m - loss: 7.156e-05, learning_rate: 2.3400000000000003e-05, global_step: 440, interval_runtime: 2.1474, interval_samples_per_second: 3.725, interval_steps_per_second: 4.657, epoch: 22.0[0m
[32m[2022-08-30 13:43:42,735] [    INFO][0m - loss: 0.000378, learning_rate: 2.3250000000000003e-05, global_step: 450, interval_runtime: 2.2004, interval_samples_per_second: 3.636, interval_steps_per_second: 4.545, epoch: 22.5[0m
[32m[2022-08-30 13:43:44,884] [    INFO][0m - loss: 6.476e-05, learning_rate: 2.3100000000000002e-05, global_step: 460, interval_runtime: 2.1492, interval_samples_per_second: 3.722, interval_steps_per_second: 4.653, epoch: 23.0[0m
[32m[2022-08-30 13:43:47,091] [    INFO][0m - loss: 8.678e-05, learning_rate: 2.2950000000000002e-05, global_step: 470, interval_runtime: 2.2059, interval_samples_per_second: 3.627, interval_steps_per_second: 4.533, epoch: 23.5[0m
[32m[2022-08-30 13:43:49,241] [    INFO][0m - loss: 3.779e-05, learning_rate: 2.2800000000000002e-05, global_step: 480, interval_runtime: 2.1512, interval_samples_per_second: 3.719, interval_steps_per_second: 4.649, epoch: 24.0[0m
[32m[2022-08-30 13:43:51,442] [    INFO][0m - loss: 4.358e-05, learning_rate: 2.265e-05, global_step: 490, interval_runtime: 2.2009, interval_samples_per_second: 3.635, interval_steps_per_second: 4.544, epoch: 24.5[0m
[32m[2022-08-30 13:43:53,594] [    INFO][0m - loss: 3.797e-05, learning_rate: 2.25e-05, global_step: 500, interval_runtime: 2.1524, interval_samples_per_second: 3.717, interval_steps_per_second: 4.646, epoch: 25.0[0m
[32m[2022-08-30 13:43:53,595] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 13:43:53,595] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-30 13:43:53,595] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 13:43:53,595] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 13:43:53,595] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 13:43:54,940] [    INFO][0m - eval_loss: 3.3875808715820312, eval_accuracy: 0.48427672955974843, eval_runtime: 1.3443, eval_samples_per_second: 118.275, eval_steps_per_second: 3.719, epoch: 25.0[0m
[32m[2022-08-30 13:43:54,940] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-08-30 13:43:54,940] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 13:44:01,693] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-08-30 13:44:01,694] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-08-30 13:44:20,024] [    INFO][0m - loss: 4.019e-05, learning_rate: 2.235e-05, global_step: 510, interval_runtime: 26.4291, interval_samples_per_second: 0.303, interval_steps_per_second: 0.378, epoch: 25.5[0m
[32m[2022-08-30 13:44:22,164] [    INFO][0m - loss: 3.303e-05, learning_rate: 2.22e-05, global_step: 520, interval_runtime: 2.1406, interval_samples_per_second: 3.737, interval_steps_per_second: 4.672, epoch: 26.0[0m
[32m[2022-08-30 13:44:24,360] [    INFO][0m - loss: 3.121e-05, learning_rate: 2.205e-05, global_step: 530, interval_runtime: 2.1961, interval_samples_per_second: 3.643, interval_steps_per_second: 4.554, epoch: 26.5[0m
[32m[2022-08-30 13:44:26,503] [    INFO][0m - loss: 3.45e-05, learning_rate: 2.19e-05, global_step: 540, interval_runtime: 2.1427, interval_samples_per_second: 3.734, interval_steps_per_second: 4.667, epoch: 27.0[0m
[32m[2022-08-30 13:44:28,702] [    INFO][0m - loss: 2.977e-05, learning_rate: 2.175e-05, global_step: 550, interval_runtime: 2.1988, interval_samples_per_second: 3.638, interval_steps_per_second: 4.548, epoch: 27.5[0m
[32m[2022-08-30 13:44:30,846] [    INFO][0m - loss: 3.344e-05, learning_rate: 2.16e-05, global_step: 560, interval_runtime: 2.1445, interval_samples_per_second: 3.731, interval_steps_per_second: 4.663, epoch: 28.0[0m
[32m[2022-08-30 13:44:33,052] [    INFO][0m - loss: 2.75e-05, learning_rate: 2.145e-05, global_step: 570, interval_runtime: 2.2062, interval_samples_per_second: 3.626, interval_steps_per_second: 4.533, epoch: 28.5[0m
[32m[2022-08-30 13:44:35,204] [    INFO][0m - loss: 2.707e-05, learning_rate: 2.13e-05, global_step: 580, interval_runtime: 2.1514, interval_samples_per_second: 3.718, interval_steps_per_second: 4.648, epoch: 29.0[0m
[32m[2022-08-30 13:44:37,406] [    INFO][0m - loss: 3.404e-05, learning_rate: 2.115e-05, global_step: 590, interval_runtime: 2.2021, interval_samples_per_second: 3.633, interval_steps_per_second: 4.541, epoch: 29.5[0m
[32m[2022-08-30 13:44:39,563] [    INFO][0m - loss: 2.073e-05, learning_rate: 2.1e-05, global_step: 600, interval_runtime: 2.1573, interval_samples_per_second: 3.708, interval_steps_per_second: 4.635, epoch: 30.0[0m
[32m[2022-08-30 13:44:39,564] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 13:44:39,564] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-30 13:44:39,564] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 13:44:39,564] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 13:44:39,564] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 13:44:40,909] [    INFO][0m - eval_loss: 3.5644142627716064, eval_accuracy: 0.4716981132075472, eval_runtime: 1.345, eval_samples_per_second: 118.212, eval_steps_per_second: 3.717, epoch: 30.0[0m
[32m[2022-08-30 13:44:40,910] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-08-30 13:44:40,910] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 13:44:47,769] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-08-30 13:44:47,770] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-08-30 13:45:04,466] [    INFO][0m - loss: 2.557e-05, learning_rate: 2.085e-05, global_step: 610, interval_runtime: 24.9022, interval_samples_per_second: 0.321, interval_steps_per_second: 0.402, epoch: 30.5[0m
[32m[2022-08-30 13:45:06,600] [    INFO][0m - loss: 2.125e-05, learning_rate: 2.07e-05, global_step: 620, interval_runtime: 2.1342, interval_samples_per_second: 3.748, interval_steps_per_second: 4.686, epoch: 31.0[0m
[32m[2022-08-30 13:45:08,926] [    INFO][0m - loss: 2.155e-05, learning_rate: 2.055e-05, global_step: 630, interval_runtime: 2.1976, interval_samples_per_second: 3.64, interval_steps_per_second: 4.55, epoch: 31.5[0m
[32m[2022-08-30 13:45:11,066] [    INFO][0m - loss: 2.091e-05, learning_rate: 2.04e-05, global_step: 640, interval_runtime: 2.2693, interval_samples_per_second: 3.525, interval_steps_per_second: 4.407, epoch: 32.0[0m
[32m[2022-08-30 13:45:13,264] [    INFO][0m - loss: 2.725e-05, learning_rate: 2.025e-05, global_step: 650, interval_runtime: 2.1973, interval_samples_per_second: 3.641, interval_steps_per_second: 4.551, epoch: 32.5[0m
[32m[2022-08-30 13:45:15,412] [    INFO][0m - loss: 2.002e-05, learning_rate: 2.01e-05, global_step: 660, interval_runtime: 2.1482, interval_samples_per_second: 3.724, interval_steps_per_second: 4.655, epoch: 33.0[0m
[32m[2022-08-30 13:45:17,610] [    INFO][0m - loss: 2.375e-05, learning_rate: 1.995e-05, global_step: 670, interval_runtime: 2.1984, interval_samples_per_second: 3.639, interval_steps_per_second: 4.549, epoch: 33.5[0m
[32m[2022-08-30 13:45:19,762] [    INFO][0m - loss: 1.969e-05, learning_rate: 1.98e-05, global_step: 680, interval_runtime: 2.1514, interval_samples_per_second: 3.718, interval_steps_per_second: 4.648, epoch: 34.0[0m
[32m[2022-08-30 13:45:21,966] [    INFO][0m - loss: 1.612e-05, learning_rate: 1.965e-05, global_step: 690, interval_runtime: 2.2042, interval_samples_per_second: 3.629, interval_steps_per_second: 4.537, epoch: 34.5[0m
[32m[2022-08-30 13:45:24,125] [    INFO][0m - loss: 2.203e-05, learning_rate: 1.95e-05, global_step: 700, interval_runtime: 2.1585, interval_samples_per_second: 3.706, interval_steps_per_second: 4.633, epoch: 35.0[0m
[32m[2022-08-30 13:45:24,125] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 13:45:24,125] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-30 13:45:24,125] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 13:45:24,126] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 13:45:24,126] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 13:45:25,472] [    INFO][0m - eval_loss: 3.6921517848968506, eval_accuracy: 0.4716981132075472, eval_runtime: 1.3459, eval_samples_per_second: 118.136, eval_steps_per_second: 3.715, epoch: 35.0[0m
[32m[2022-08-30 13:45:25,472] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-700[0m
[32m[2022-08-30 13:45:25,472] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 13:45:32,598] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-700/tokenizer_config.json[0m
[32m[2022-08-30 13:45:32,598] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-700/special_tokens_map.json[0m
[32m[2022-08-30 13:45:48,770] [    INFO][0m - loss: 1.736e-05, learning_rate: 1.935e-05, global_step: 710, interval_runtime: 24.6451, interval_samples_per_second: 0.325, interval_steps_per_second: 0.406, epoch: 35.5[0m
[32m[2022-08-30 13:45:50,909] [    INFO][0m - loss: 1.717e-05, learning_rate: 1.9200000000000003e-05, global_step: 720, interval_runtime: 2.1388, interval_samples_per_second: 3.74, interval_steps_per_second: 4.676, epoch: 36.0[0m
[32m[2022-08-30 13:45:53,101] [    INFO][0m - loss: 1.55e-05, learning_rate: 1.9050000000000002e-05, global_step: 730, interval_runtime: 2.1923, interval_samples_per_second: 3.649, interval_steps_per_second: 4.561, epoch: 36.5[0m
[32m[2022-08-30 13:45:55,243] [    INFO][0m - loss: 1.788e-05, learning_rate: 1.8900000000000002e-05, global_step: 740, interval_runtime: 2.142, interval_samples_per_second: 3.735, interval_steps_per_second: 4.668, epoch: 37.0[0m
[32m[2022-08-30 13:45:57,449] [    INFO][0m - loss: 1.612e-05, learning_rate: 1.8750000000000002e-05, global_step: 750, interval_runtime: 2.2062, interval_samples_per_second: 3.626, interval_steps_per_second: 4.533, epoch: 37.5[0m
[32m[2022-08-30 13:45:59,597] [    INFO][0m - loss: 1.381e-05, learning_rate: 1.86e-05, global_step: 760, interval_runtime: 2.1477, interval_samples_per_second: 3.725, interval_steps_per_second: 4.656, epoch: 38.0[0m
[32m[2022-08-30 13:46:01,807] [    INFO][0m - loss: 1.639e-05, learning_rate: 1.845e-05, global_step: 770, interval_runtime: 2.2103, interval_samples_per_second: 3.619, interval_steps_per_second: 4.524, epoch: 38.5[0m
[32m[2022-08-30 13:46:03,958] [    INFO][0m - loss: 1.219e-05, learning_rate: 1.83e-05, global_step: 780, interval_runtime: 2.1506, interval_samples_per_second: 3.72, interval_steps_per_second: 4.65, epoch: 39.0[0m
[32m[2022-08-30 13:46:06,162] [    INFO][0m - loss: 1.68e-05, learning_rate: 1.815e-05, global_step: 790, interval_runtime: 2.2042, interval_samples_per_second: 3.629, interval_steps_per_second: 4.537, epoch: 39.5[0m
[32m[2022-08-30 13:46:08,316] [    INFO][0m - loss: 1.62e-05, learning_rate: 1.8e-05, global_step: 800, interval_runtime: 2.1538, interval_samples_per_second: 3.714, interval_steps_per_second: 4.643, epoch: 40.0[0m
[32m[2022-08-30 13:46:08,316] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-30 13:46:08,316] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-30 13:46:08,316] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 13:46:08,316] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 13:46:08,317] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-30 13:46:09,675] [    INFO][0m - eval_loss: 3.7908458709716797, eval_accuracy: 0.4716981132075472, eval_runtime: 1.3586, eval_samples_per_second: 117.036, eval_steps_per_second: 3.68, epoch: 40.0[0m
[32m[2022-08-30 13:46:09,676] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-800[0m
[32m[2022-08-30 13:46:09,676] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 13:46:16,598] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-800/tokenizer_config.json[0m
[32m[2022-08-30 13:46:16,599] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-800/special_tokens_map.json[0m
[32m[2022-08-30 13:46:30,703] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-30 13:46:30,703] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-400 (score: 0.5471698113207547).[0m
[32m[2022-08-30 13:46:32,572] [    INFO][0m - train_runtime: 363.4281, train_samples_per_second: 44.025, train_steps_per_second: 5.503, train_loss: 0.12826993320479232, epoch: 40.0[0m
[32m[2022-08-30 13:46:32,573] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-30 13:46:32,573] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-30 13:46:39,712] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-30 13:46:39,713] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-30 13:46:39,715] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-30 13:46:39,715] [    INFO][0m -   epoch                    =       40.0[0m
[32m[2022-08-30 13:46:39,715] [    INFO][0m -   train_loss               =     0.1283[0m
[32m[2022-08-30 13:46:39,715] [    INFO][0m -   train_runtime            = 0:06:03.42[0m
[32m[2022-08-30 13:46:39,716] [    INFO][0m -   train_samples_per_second =     44.025[0m
[32m[2022-08-30 13:46:39,716] [    INFO][0m -   train_steps_per_second   =      5.503[0m
[32m[2022-08-30 13:46:39,724] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-30 13:46:39,724] [    INFO][0m -   Num examples = 976[0m
[32m[2022-08-30 13:46:39,725] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 13:46:39,725] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 13:46:39,725] [    INFO][0m -   Total prediction steps = 31[0m
[32m[2022-08-30 13:46:47,809] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-30 13:46:47,809] [    INFO][0m -   test_accuracy           =     0.4969[0m
[32m[2022-08-30 13:46:47,810] [    INFO][0m -   test_loss               =     3.5104[0m
[32m[2022-08-30 13:46:47,810] [    INFO][0m -   test_runtime            = 0:00:08.08[0m
[32m[2022-08-30 13:46:47,810] [    INFO][0m -   test_samples_per_second =    120.719[0m
[32m[2022-08-30 13:46:47,810] [    INFO][0m -   test_steps_per_second   =      3.834[0m
[32m[2022-08-30 13:46:47,810] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-30 13:46:47,810] [    INFO][0m -   Num examples = 290[0m
[32m[2022-08-30 13:46:47,810] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-30 13:46:47,810] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-30 13:46:47,810] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-08-30 13:46:50,522] [    INFO][0m - Predictions for cluewscf saved to ./fewclue_submit_examples.[0m
metric <function main.<locals>.compute_metrics at 0x7f50dc50a4c0>
