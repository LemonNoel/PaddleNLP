[33m[2022-08-26 16:31:54,875] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-26 16:31:54,876] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-26 16:31:54,876] [    INFO][0m - ============================================================[0m
[32m[2022-08-26 16:31:54,876] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-26 16:31:54,876] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-26 16:31:54,876] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-26 16:31:54,876] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-26 16:31:54,876] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-08-26 16:31:54,876] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-26 16:31:54,876] [    INFO][0m - model_name_or_path            :/ssd2/wanghuijuan03/tmp/ernie-3.0-large[0m
[32m[2022-08-26 16:31:54,877] [    INFO][0m - [0m
[32m[2022-08-26 16:31:54,877] [    INFO][0m - ============================================================[0m
[32m[2022-08-26 16:31:54,877] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-26 16:31:54,877] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-26 16:31:54,877] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-26 16:31:54,877] [    INFO][0m - prompt                        :{'mask'}{'mask'}ç”¨{'text':'text_b'}æ¦‚æ‹¬{'text':'text_a'}[0m
[32m[2022-08-26 16:31:54,877] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-26 16:31:54,877] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-26 16:31:54,877] [    INFO][0m - task_name                     :csl[0m
[32m[2022-08-26 16:31:54,877] [    INFO][0m - [0m
W0826 16:31:54.879482   892 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0826 16:31:54.883520   892 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-26 16:32:03,488] [    INFO][0m - Using template: [{'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': 'ç”¨'}, {'add_prefix_space': '', 'text': 'text_b'}, {'add_prefix_space': '', 'hard': 'æ¦‚æ‹¬'}, {'add_prefix_space': '', 'text': 'text_a'}][0m
2022-08-26 16:32:03,495 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-26 16:32:03,643] [    INFO][0m - ============================================================[0m
[32m[2022-08-26 16:32:03,643] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-26 16:32:03,643] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-26 16:32:03,643] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-26 16:32:03,643] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-26 16:32:03,644] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-26 16:32:03,644] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-26 16:32:03,644] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-26 16:32:03,644] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-26 16:32:03,644] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-26 16:32:03,644] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-26 16:32:03,644] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-26 16:32:03,644] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-26 16:32:03,644] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-26 16:32:03,644] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-26 16:32:03,644] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-26 16:32:03,644] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-26 16:32:03,645] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-26 16:32:03,645] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-26 16:32:03,645] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-26 16:32:03,645] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-26 16:32:03,645] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-26 16:32:03,645] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-26 16:32:03,645] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-26 16:32:03,645] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-26 16:32:03,645] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-26 16:32:03,645] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-26 16:32:03,645] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-26 16:32:03,645] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-26 16:32:03,645] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-26 16:32:03,645] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-26 16:32:03,646] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-26 16:32:03,646] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-26 16:32:03,646] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-26 16:32:03,646] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-26 16:32:03,646] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-26 16:32:03,646] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-26 16:32:03,646] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug26_16-31-54_instance-3bwob41y-01[0m
[32m[2022-08-26 16:32:03,646] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-26 16:32:03,646] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-26 16:32:03,646] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-26 16:32:03,646] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-26 16:32:03,646] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-26 16:32:03,646] [    INFO][0m - max_seq_length                :320[0m
[32m[2022-08-26 16:32:03,647] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-26 16:32:03,647] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-26 16:32:03,647] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-26 16:32:03,647] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-26 16:32:03,647] [    INFO][0m - num_train_epochs              :20.0[0m
[32m[2022-08-26 16:32:03,647] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-26 16:32:03,647] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-26 16:32:03,647] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-26 16:32:03,647] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-26 16:32:03,647] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-26 16:32:03,647] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-26 16:32:03,647] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-26 16:32:03,647] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-26 16:32:03,647] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-26 16:32:03,648] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-26 16:32:03,648] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-26 16:32:03,648] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-26 16:32:03,648] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-26 16:32:03,648] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-26 16:32:03,648] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-26 16:32:03,648] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-26 16:32:03,648] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-26 16:32:03,648] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-26 16:32:03,648] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-26 16:32:03,648] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-26 16:32:03,648] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-26 16:32:03,648] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-26 16:32:03,649] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-26 16:32:03,649] [    INFO][0m - seed                          :42[0m
[32m[2022-08-26 16:32:03,649] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-26 16:32:03,649] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-26 16:32:03,649] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-26 16:32:03,649] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-26 16:32:03,649] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-26 16:32:03,649] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-26 16:32:03,649] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-26 16:32:03,649] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-26 16:32:03,649] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-26 16:32:03,649] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-26 16:32:03,649] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-26 16:32:03,649] [    INFO][0m - [0m
[32m[2022-08-26 16:32:03,652] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-26 16:32:03,652] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 16:32:03,652] [    INFO][0m -   Num Epochs = 20[0m
[32m[2022-08-26 16:32:03,652] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-26 16:32:03,652] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-26 16:32:03,653] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-26 16:32:03,653] [    INFO][0m -   Total optimization steps = 400.0[0m
[32m[2022-08-26 16:32:03,653] [    INFO][0m -   Total num train samples = 3200[0m
[32m[2022-08-26 16:32:09,163] [    INFO][0m - loss: 1.22235489, learning_rate: 2.925e-05, global_step: 10, interval_runtime: 5.5095, interval_samples_per_second: 1.452, interval_steps_per_second: 1.815, epoch: 0.5[0m
[32m[2022-08-26 16:32:13,735] [    INFO][0m - loss: 0.78738194, learning_rate: 2.8499999999999998e-05, global_step: 20, interval_runtime: 4.5718, interval_samples_per_second: 1.75, interval_steps_per_second: 2.187, epoch: 1.0[0m
[32m[2022-08-26 16:32:18,438] [    INFO][0m - loss: 0.74827876, learning_rate: 2.7750000000000004e-05, global_step: 30, interval_runtime: 4.7024, interval_samples_per_second: 1.701, interval_steps_per_second: 2.127, epoch: 1.5[0m
[32m[2022-08-26 16:32:23,036] [    INFO][0m - loss: 0.90916138, learning_rate: 2.7000000000000002e-05, global_step: 40, interval_runtime: 4.5985, interval_samples_per_second: 1.74, interval_steps_per_second: 2.175, epoch: 2.0[0m
[32m[2022-08-26 16:32:27,753] [    INFO][0m - loss: 0.74613256, learning_rate: 2.625e-05, global_step: 50, interval_runtime: 4.717, interval_samples_per_second: 1.696, interval_steps_per_second: 2.12, epoch: 2.5[0m
[32m[2022-08-26 16:32:32,344] [    INFO][0m - loss: 0.76913891, learning_rate: 2.55e-05, global_step: 60, interval_runtime: 4.5911, interval_samples_per_second: 1.743, interval_steps_per_second: 2.178, epoch: 3.0[0m
[32m[2022-08-26 16:32:37,065] [    INFO][0m - loss: 0.81035709, learning_rate: 2.475e-05, global_step: 70, interval_runtime: 4.7205, interval_samples_per_second: 1.695, interval_steps_per_second: 2.118, epoch: 3.5[0m
[32m[2022-08-26 16:32:41,673] [    INFO][0m - loss: 0.74079986, learning_rate: 2.4e-05, global_step: 80, interval_runtime: 4.6087, interval_samples_per_second: 1.736, interval_steps_per_second: 2.17, epoch: 4.0[0m
[32m[2022-08-26 16:32:46,430] [    INFO][0m - loss: 0.71220255, learning_rate: 2.3250000000000003e-05, global_step: 90, interval_runtime: 4.757, interval_samples_per_second: 1.682, interval_steps_per_second: 2.102, epoch: 4.5[0m
[32m[2022-08-26 16:32:51,034] [    INFO][0m - loss: 0.69001942, learning_rate: 2.25e-05, global_step: 100, interval_runtime: 4.6038, interval_samples_per_second: 1.738, interval_steps_per_second: 2.172, epoch: 5.0[0m
[32m[2022-08-26 16:32:51,035] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 16:32:51,035] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 16:32:51,035] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 16:32:51,035] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 16:32:51,035] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 16:32:54,596] [    INFO][0m - eval_loss: 1.124694585800171, eval_accuracy: 0.49375, eval_runtime: 3.5605, eval_samples_per_second: 44.937, eval_steps_per_second: 1.404, epoch: 5.0[0m
[32m[2022-08-26 16:32:54,596] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-26 16:32:54,596] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 16:33:02,999] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-26 16:33:03,000] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-26 16:33:25,133] [    INFO][0m - loss: 0.60453057, learning_rate: 2.175e-05, global_step: 110, interval_runtime: 34.0986, interval_samples_per_second: 0.235, interval_steps_per_second: 0.293, epoch: 5.5[0m
[32m[2022-08-26 16:33:29,727] [    INFO][0m - loss: 0.69822888, learning_rate: 2.1e-05, global_step: 120, interval_runtime: 4.5948, interval_samples_per_second: 1.741, interval_steps_per_second: 2.176, epoch: 6.0[0m
[32m[2022-08-26 16:33:34,477] [    INFO][0m - loss: 0.64580884, learning_rate: 2.025e-05, global_step: 130, interval_runtime: 4.7498, interval_samples_per_second: 1.684, interval_steps_per_second: 2.105, epoch: 6.5[0m
[32m[2022-08-26 16:33:39,105] [    INFO][0m - loss: 0.66424026, learning_rate: 1.95e-05, global_step: 140, interval_runtime: 4.6281, interval_samples_per_second: 1.729, interval_steps_per_second: 2.161, epoch: 7.0[0m
[32m[2022-08-26 16:33:43,881] [    INFO][0m - loss: 0.76537952, learning_rate: 1.8750000000000002e-05, global_step: 150, interval_runtime: 4.7761, interval_samples_per_second: 1.675, interval_steps_per_second: 2.094, epoch: 7.5[0m
[32m[2022-08-26 16:33:48,500] [    INFO][0m - loss: 0.4783412, learning_rate: 1.8e-05, global_step: 160, interval_runtime: 4.6186, interval_samples_per_second: 1.732, interval_steps_per_second: 2.165, epoch: 8.0[0m
[32m[2022-08-26 16:33:53,288] [    INFO][0m - loss: 0.36905839, learning_rate: 1.725e-05, global_step: 170, interval_runtime: 4.7885, interval_samples_per_second: 1.671, interval_steps_per_second: 2.088, epoch: 8.5[0m
[32m[2022-08-26 16:33:57,903] [    INFO][0m - loss: 0.25706635, learning_rate: 1.65e-05, global_step: 180, interval_runtime: 4.6149, interval_samples_per_second: 1.734, interval_steps_per_second: 2.167, epoch: 9.0[0m
[32m[2022-08-26 16:34:02,682] [    INFO][0m - loss: 0.20548389, learning_rate: 1.575e-05, global_step: 190, interval_runtime: 4.7785, interval_samples_per_second: 1.674, interval_steps_per_second: 2.093, epoch: 9.5[0m
[32m[2022-08-26 16:34:07,327] [    INFO][0m - loss: 0.52706366, learning_rate: 1.5e-05, global_step: 200, interval_runtime: 4.6451, interval_samples_per_second: 1.722, interval_steps_per_second: 2.153, epoch: 10.0[0m
[32m[2022-08-26 16:34:07,327] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 16:34:07,327] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 16:34:07,328] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 16:34:07,328] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 16:34:07,328] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 16:34:10,714] [    INFO][0m - eval_loss: 6.005822658538818, eval_accuracy: 0.5125, eval_runtime: 3.3857, eval_samples_per_second: 47.258, eval_steps_per_second: 1.477, epoch: 10.0[0m
[32m[2022-08-26 16:34:10,714] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-26 16:34:10,714] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 16:34:17,684] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-26 16:34:17,685] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-26 16:34:41,926] [    INFO][0m - loss: 0.01344265, learning_rate: 1.4249999999999999e-05, global_step: 210, interval_runtime: 34.5986, interval_samples_per_second: 0.231, interval_steps_per_second: 0.289, epoch: 10.5[0m
[32m[2022-08-26 16:34:46,548] [    INFO][0m - loss: 0.67408624, learning_rate: 1.3500000000000001e-05, global_step: 220, interval_runtime: 4.6223, interval_samples_per_second: 1.731, interval_steps_per_second: 2.163, epoch: 11.0[0m
[32m[2022-08-26 16:34:51,327] [    INFO][0m - loss: 0.06812229, learning_rate: 1.275e-05, global_step: 230, interval_runtime: 4.7796, interval_samples_per_second: 1.674, interval_steps_per_second: 2.092, epoch: 11.5[0m
[32m[2022-08-26 16:34:55,968] [    INFO][0m - loss: 0.02852527, learning_rate: 1.2e-05, global_step: 240, interval_runtime: 4.6407, interval_samples_per_second: 1.724, interval_steps_per_second: 2.155, epoch: 12.0[0m
[32m[2022-08-26 16:35:00,740] [    INFO][0m - loss: 0.16025928, learning_rate: 1.125e-05, global_step: 250, interval_runtime: 4.7723, interval_samples_per_second: 1.676, interval_steps_per_second: 2.095, epoch: 12.5[0m
[32m[2022-08-26 16:35:05,381] [    INFO][0m - loss: 0.0973552, learning_rate: 1.05e-05, global_step: 260, interval_runtime: 4.6403, interval_samples_per_second: 1.724, interval_steps_per_second: 2.155, epoch: 13.0[0m
[32m[2022-08-26 16:35:10,166] [    INFO][0m - loss: 0.07125853, learning_rate: 9.75e-06, global_step: 270, interval_runtime: 4.7858, interval_samples_per_second: 1.672, interval_steps_per_second: 2.09, epoch: 13.5[0m
[32m[2022-08-26 16:35:14,808] [    INFO][0m - loss: 3.377e-05, learning_rate: 9e-06, global_step: 280, interval_runtime: 4.6418, interval_samples_per_second: 1.723, interval_steps_per_second: 2.154, epoch: 14.0[0m
[32m[2022-08-26 16:35:19,583] [    INFO][0m - loss: 8.04e-05, learning_rate: 8.25e-06, global_step: 290, interval_runtime: 4.7741, interval_samples_per_second: 1.676, interval_steps_per_second: 2.095, epoch: 14.5[0m
[32m[2022-08-26 16:35:24,247] [    INFO][0m - loss: 1.838e-05, learning_rate: 7.5e-06, global_step: 300, interval_runtime: 4.6645, interval_samples_per_second: 1.715, interval_steps_per_second: 2.144, epoch: 15.0[0m
[32m[2022-08-26 16:35:24,247] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 16:35:24,248] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 16:35:24,248] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 16:35:24,248] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 16:35:24,248] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 16:35:27,655] [    INFO][0m - eval_loss: 6.487696647644043, eval_accuracy: 0.54375, eval_runtime: 3.4074, eval_samples_per_second: 46.956, eval_steps_per_second: 1.467, epoch: 15.0[0m
[32m[2022-08-26 16:35:27,656] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-26 16:35:27,656] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 16:35:35,314] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-26 16:35:35,314] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-26 16:35:57,260] [    INFO][0m - loss: 0.08408659, learning_rate: 6.750000000000001e-06, global_step: 310, interval_runtime: 33.0135, interval_samples_per_second: 0.242, interval_steps_per_second: 0.303, epoch: 15.5[0m
[32m[2022-08-26 16:36:01,876] [    INFO][0m - loss: 4.5e-05, learning_rate: 6e-06, global_step: 320, interval_runtime: 4.6154, interval_samples_per_second: 1.733, interval_steps_per_second: 2.167, epoch: 16.0[0m
[32m[2022-08-26 16:36:06,649] [    INFO][0m - loss: 0.04550832, learning_rate: 5.25e-06, global_step: 330, interval_runtime: 4.7734, interval_samples_per_second: 1.676, interval_steps_per_second: 2.095, epoch: 16.5[0m
[32m[2022-08-26 16:36:11,295] [    INFO][0m - loss: 2.1e-05, learning_rate: 4.5e-06, global_step: 340, interval_runtime: 4.6455, interval_samples_per_second: 1.722, interval_steps_per_second: 2.153, epoch: 17.0[0m
[32m[2022-08-26 16:36:16,080] [    INFO][0m - loss: 2.487e-05, learning_rate: 3.75e-06, global_step: 350, interval_runtime: 4.7847, interval_samples_per_second: 1.672, interval_steps_per_second: 2.09, epoch: 17.5[0m
[32m[2022-08-26 16:36:20,738] [    INFO][0m - loss: 0.00334831, learning_rate: 3e-06, global_step: 360, interval_runtime: 4.6585, interval_samples_per_second: 1.717, interval_steps_per_second: 2.147, epoch: 18.0[0m
[32m[2022-08-26 16:36:25,539] [    INFO][0m - loss: 4.415e-05, learning_rate: 2.25e-06, global_step: 370, interval_runtime: 4.8014, interval_samples_per_second: 1.666, interval_steps_per_second: 2.083, epoch: 18.5[0m
[32m[2022-08-26 16:36:30,179] [    INFO][0m - loss: 0.07164401, learning_rate: 1.5e-06, global_step: 380, interval_runtime: 4.6394, interval_samples_per_second: 1.724, interval_steps_per_second: 2.155, epoch: 19.0[0m
[32m[2022-08-26 16:36:34,972] [    INFO][0m - loss: 3.74e-05, learning_rate: 7.5e-07, global_step: 390, interval_runtime: 4.793, interval_samples_per_second: 1.669, interval_steps_per_second: 2.086, epoch: 19.5[0m
[32m[2022-08-26 16:36:39,624] [    INFO][0m - loss: 0.16890446, learning_rate: 0.0, global_step: 400, interval_runtime: 4.6525, interval_samples_per_second: 1.72, interval_steps_per_second: 2.149, epoch: 20.0[0m
[32m[2022-08-26 16:36:39,625] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 16:36:39,625] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 16:36:39,625] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 16:36:39,625] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 16:36:39,625] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 16:36:43,027] [    INFO][0m - eval_loss: 7.302728176116943, eval_accuracy: 0.54375, eval_runtime: 3.4016, eval_samples_per_second: 47.036, eval_steps_per_second: 1.47, epoch: 20.0[0m
[32m[2022-08-26 16:36:43,027] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-26 16:36:43,027] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 16:36:50,407] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-26 16:36:50,407] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-26 16:37:06,533] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-26 16:37:06,534] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-300 (score: 0.54375).[0m
[32m[2022-08-26 16:37:08,364] [    INFO][0m - train_runtime: 304.7112, train_samples_per_second: 10.502, train_steps_per_second: 1.313, train_loss: 0.37094687582270125, epoch: 20.0[0m
[32m[2022-08-26 16:37:08,416] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-26 16:37:08,417] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 16:37:15,959] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-26 16:37:15,960] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-26 16:37:15,961] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-26 16:37:15,962] [    INFO][0m -   epoch                    =       20.0[0m
[32m[2022-08-26 16:37:15,962] [    INFO][0m -   train_loss               =     0.3709[0m
[32m[2022-08-26 16:37:15,962] [    INFO][0m -   train_runtime            = 0:05:04.71[0m
[32m[2022-08-26 16:37:15,962] [    INFO][0m -   train_samples_per_second =     10.502[0m
[32m[2022-08-26 16:37:15,962] [    INFO][0m -   train_steps_per_second   =      1.313[0m
[32m[2022-08-26 16:37:15,964] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-26 16:37:15,964] [    INFO][0m -   Num examples = 2838[0m
[32m[2022-08-26 16:37:15,965] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 16:37:15,965] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 16:37:15,965] [    INFO][0m -   Total prediction steps = 89[0m
[32m[2022-08-26 16:38:16,546] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-26 16:38:16,546] [    INFO][0m -   test_accuracy           =     0.5599[0m
[32m[2022-08-26 16:38:16,547] [    INFO][0m -   test_loss               =      6.354[0m
[32m[2022-08-26 16:38:16,547] [    INFO][0m -   test_runtime            = 0:01:00.58[0m
[32m[2022-08-26 16:38:16,547] [    INFO][0m -   test_samples_per_second =     46.846[0m
[32m[2022-08-26 16:38:16,547] [    INFO][0m -   test_steps_per_second   =      1.469[0m
[32m[2022-08-26 16:38:16,547] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-26 16:38:16,547] [    INFO][0m -   Num examples = 3000[0m
[32m[2022-08-26 16:38:16,547] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 16:38:16,547] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 16:38:16,548] [    INFO][0m -   Total prediction steps = 94[0m
[32m[2022-08-26 16:39:27,922] [    INFO][0m - Predictions for cslf saved to ./fewclue_submit_examples.[0m
