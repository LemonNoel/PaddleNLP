[33m[2022-08-26 16:58:18,965] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-26 16:58:18,965] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-26 16:58:18,965] [    INFO][0m - ============================================================[0m
[32m[2022-08-26 16:58:18,966] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-26 16:58:18,966] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-26 16:58:18,966] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-26 16:58:18,966] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-26 16:58:18,966] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-08-26 16:58:18,966] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-26 16:58:18,966] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-08-26 16:58:18,966] [    INFO][0m - [0m
[32m[2022-08-26 16:58:18,966] [    INFO][0m - ============================================================[0m
[32m[2022-08-26 16:58:18,966] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-26 16:58:18,966] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-26 16:58:18,966] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-26 16:58:18,966] [    INFO][0m - prompt                        :‰∏ãÈù¢Âè•Â≠êÁöÑÊåá‰ª£ÂÖ≥Á≥ªÊ≠£Á°ÆÂêóÔºü{'mask'}{'mask'}{'text':'text_a'}[0m
[32m[2022-08-26 16:58:18,967] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-26 16:58:18,967] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-26 16:58:18,967] [    INFO][0m - task_name                     :cluewsc[0m
[32m[2022-08-26 16:58:18,967] [    INFO][0m - [0m
[32m[2022-08-26 16:58:18,967] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0826 16:58:18.969043 38867 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0826 16:58:18.973124 38867 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-26 16:58:21,777] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-08-26 16:58:21,802] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-08-26 16:58:21,802] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[32m[2022-08-26 16:58:21,803] [    INFO][0m - Using template: [{'add_prefix_space': '', 'hard': '‰∏ãÈù¢Âè•Â≠êÁöÑÊåá‰ª£ÂÖ≥Á≥ªÊ≠£Á°ÆÂêóÔºü'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'text': 'text_a'}][0m
2022-08-26 16:58:21,811 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-26 16:58:21,910] [    INFO][0m - ============================================================[0m
[32m[2022-08-26 16:58:21,910] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-26 16:58:21,910] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-26 16:58:21,911] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-26 16:58:21,911] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-26 16:58:21,911] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-26 16:58:21,911] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-26 16:58:21,911] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-26 16:58:21,911] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-26 16:58:21,911] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-26 16:58:21,911] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-26 16:58:21,911] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-26 16:58:21,911] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-26 16:58:21,911] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-26 16:58:21,912] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-26 16:58:21,912] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-26 16:58:21,912] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-26 16:58:21,912] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-26 16:58:21,912] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-26 16:58:21,912] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-26 16:58:21,912] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-26 16:58:21,912] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-26 16:58:21,912] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-26 16:58:21,912] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-26 16:58:21,912] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-26 16:58:21,912] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-26 16:58:21,912] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-26 16:58:21,913] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-26 16:58:21,913] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-26 16:58:21,913] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-26 16:58:21,913] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-26 16:58:21,913] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-26 16:58:21,913] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-26 16:58:21,913] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-26 16:58:21,913] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-26 16:58:21,913] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-26 16:58:21,913] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-26 16:58:21,913] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug26_16-58-18_instance-3bwob41y-01[0m
[32m[2022-08-26 16:58:21,913] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-26 16:58:21,913] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-26 16:58:21,913] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-26 16:58:21,914] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-26 16:58:21,914] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-26 16:58:21,914] [    INFO][0m - max_seq_length                :128[0m
[32m[2022-08-26 16:58:21,914] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-26 16:58:21,914] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-26 16:58:21,914] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-26 16:58:21,914] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-26 16:58:21,914] [    INFO][0m - num_train_epochs              :100.0[0m
[32m[2022-08-26 16:58:21,914] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-26 16:58:21,914] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-26 16:58:21,914] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-26 16:58:21,914] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-26 16:58:21,914] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-26 16:58:21,915] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-26 16:58:21,915] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-26 16:58:21,915] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-26 16:58:21,915] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-26 16:58:21,915] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-26 16:58:21,915] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-26 16:58:21,915] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-26 16:58:21,915] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-26 16:58:21,915] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-26 16:58:21,915] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-26 16:58:21,915] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-26 16:58:21,915] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-26 16:58:21,915] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-26 16:58:21,915] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-26 16:58:21,916] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-26 16:58:21,916] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-26 16:58:21,916] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-26 16:58:21,916] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-26 16:58:21,916] [    INFO][0m - seed                          :42[0m
[32m[2022-08-26 16:58:21,916] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-26 16:58:21,916] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-26 16:58:21,916] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-26 16:58:21,916] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-26 16:58:21,916] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-26 16:58:21,916] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-26 16:58:21,916] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-26 16:58:21,916] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-26 16:58:21,916] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-26 16:58:21,917] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-26 16:58:21,917] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-26 16:58:21,917] [    INFO][0m - [0m
[32m[2022-08-26 16:58:21,918] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-26 16:58:21,919] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 16:58:21,919] [    INFO][0m -   Num Epochs = 100[0m
[32m[2022-08-26 16:58:21,919] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-26 16:58:21,919] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-26 16:58:21,919] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-26 16:58:21,919] [    INFO][0m -   Total optimization steps = 2000.0[0m
[32m[2022-08-26 16:58:21,919] [    INFO][0m -   Total num train samples = 16000[0m
[32m[2022-08-26 16:58:23,695] [    INFO][0m - loss: 0.84046612, learning_rate: 2.985e-05, global_step: 10, interval_runtime: 1.7752, interval_samples_per_second: 4.507, interval_steps_per_second: 5.633, epoch: 0.5[0m
[32m[2022-08-26 16:58:24,403] [    INFO][0m - loss: 0.82646189, learning_rate: 2.97e-05, global_step: 20, interval_runtime: 0.7075, interval_samples_per_second: 11.308, interval_steps_per_second: 14.134, epoch: 1.0[0m
[32m[2022-08-26 16:58:25,165] [    INFO][0m - loss: 0.69628196, learning_rate: 2.955e-05, global_step: 30, interval_runtime: 0.7624, interval_samples_per_second: 10.493, interval_steps_per_second: 13.116, epoch: 1.5[0m
[32m[2022-08-26 16:58:25,870] [    INFO][0m - loss: 0.75807009, learning_rate: 2.94e-05, global_step: 40, interval_runtime: 0.7051, interval_samples_per_second: 11.346, interval_steps_per_second: 14.182, epoch: 2.0[0m
[32m[2022-08-26 16:58:26,626] [    INFO][0m - loss: 0.7083806, learning_rate: 2.925e-05, global_step: 50, interval_runtime: 0.7565, interval_samples_per_second: 10.576, interval_steps_per_second: 13.22, epoch: 2.5[0m
[32m[2022-08-26 16:58:27,334] [    INFO][0m - loss: 0.76986527, learning_rate: 2.91e-05, global_step: 60, interval_runtime: 0.7073, interval_samples_per_second: 11.311, interval_steps_per_second: 14.139, epoch: 3.0[0m
[32m[2022-08-26 16:58:28,090] [    INFO][0m - loss: 0.63398056, learning_rate: 2.895e-05, global_step: 70, interval_runtime: 0.7568, interval_samples_per_second: 10.57, interval_steps_per_second: 13.213, epoch: 3.5[0m
[32m[2022-08-26 16:58:28,796] [    INFO][0m - loss: 0.66497908, learning_rate: 2.88e-05, global_step: 80, interval_runtime: 0.7056, interval_samples_per_second: 11.338, interval_steps_per_second: 14.173, epoch: 4.0[0m
[32m[2022-08-26 16:58:29,556] [    INFO][0m - loss: 0.45132852, learning_rate: 2.865e-05, global_step: 90, interval_runtime: 0.7599, interval_samples_per_second: 10.527, interval_steps_per_second: 13.159, epoch: 4.5[0m
[32m[2022-08-26 16:58:30,262] [    INFO][0m - loss: 0.46665874, learning_rate: 2.8499999999999998e-05, global_step: 100, interval_runtime: 0.7055, interval_samples_per_second: 11.34, interval_steps_per_second: 14.175, epoch: 5.0[0m
[32m[2022-08-26 16:58:30,262] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 16:58:30,262] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-26 16:58:30,262] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 16:58:30,262] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 16:58:30,263] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 16:58:30,776] [    INFO][0m - eval_loss: 1.1770209074020386, eval_accuracy: 0.5723270440251572, eval_runtime: 0.5125, eval_samples_per_second: 310.246, eval_steps_per_second: 9.756, epoch: 5.0[0m
[32m[2022-08-26 16:58:30,776] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-26 16:58:30,776] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 16:58:33,314] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-26 16:58:33,315] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-26 16:58:39,805] [    INFO][0m - loss: 0.48494682, learning_rate: 2.8349999999999998e-05, global_step: 110, interval_runtime: 9.5436, interval_samples_per_second: 0.838, interval_steps_per_second: 1.048, epoch: 5.5[0m
[32m[2022-08-26 16:58:40,509] [    INFO][0m - loss: 0.39303725, learning_rate: 2.8199999999999998e-05, global_step: 120, interval_runtime: 0.7041, interval_samples_per_second: 11.362, interval_steps_per_second: 14.202, epoch: 6.0[0m
[32m[2022-08-26 16:58:41,263] [    INFO][0m - loss: 0.28505023, learning_rate: 2.805e-05, global_step: 130, interval_runtime: 0.7538, interval_samples_per_second: 10.614, interval_steps_per_second: 13.267, epoch: 6.5[0m
[32m[2022-08-26 16:58:41,971] [    INFO][0m - loss: 0.26889775, learning_rate: 2.79e-05, global_step: 140, interval_runtime: 0.7072, interval_samples_per_second: 11.312, interval_steps_per_second: 14.139, epoch: 7.0[0m
[32m[2022-08-26 16:58:42,727] [    INFO][0m - loss: 0.22719636, learning_rate: 2.7750000000000004e-05, global_step: 150, interval_runtime: 0.7563, interval_samples_per_second: 10.578, interval_steps_per_second: 13.222, epoch: 7.5[0m
[32m[2022-08-26 16:58:43,433] [    INFO][0m - loss: 0.1519311, learning_rate: 2.7600000000000003e-05, global_step: 160, interval_runtime: 0.7062, interval_samples_per_second: 11.328, interval_steps_per_second: 14.16, epoch: 8.0[0m
[32m[2022-08-26 16:58:44,191] [    INFO][0m - loss: 0.25308084, learning_rate: 2.7450000000000003e-05, global_step: 170, interval_runtime: 0.7578, interval_samples_per_second: 10.556, interval_steps_per_second: 13.195, epoch: 8.5[0m
[32m[2022-08-26 16:58:44,907] [    INFO][0m - loss: 0.19494116, learning_rate: 2.7300000000000003e-05, global_step: 180, interval_runtime: 0.7163, interval_samples_per_second: 11.169, interval_steps_per_second: 13.962, epoch: 9.0[0m
[32m[2022-08-26 16:58:45,664] [    INFO][0m - loss: 0.06137077, learning_rate: 2.7150000000000003e-05, global_step: 190, interval_runtime: 0.7567, interval_samples_per_second: 10.573, interval_steps_per_second: 13.216, epoch: 9.5[0m
[32m[2022-08-26 16:58:46,372] [    INFO][0m - loss: 0.3760035, learning_rate: 2.7000000000000002e-05, global_step: 200, interval_runtime: 0.708, interval_samples_per_second: 11.299, interval_steps_per_second: 14.124, epoch: 10.0[0m
[32m[2022-08-26 16:58:46,372] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 16:58:46,372] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-26 16:58:46,372] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 16:58:46,373] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 16:58:46,373] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 16:58:46,882] [    INFO][0m - eval_loss: 4.085379123687744, eval_accuracy: 0.5283018867924528, eval_runtime: 0.5096, eval_samples_per_second: 311.994, eval_steps_per_second: 9.811, epoch: 10.0[0m
[32m[2022-08-26 16:58:46,883] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-26 16:58:46,883] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 16:58:49,732] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-26 16:58:49,732] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-26 16:58:56,201] [    INFO][0m - loss: 0.24776049, learning_rate: 2.6850000000000002e-05, global_step: 210, interval_runtime: 9.8287, interval_samples_per_second: 0.814, interval_steps_per_second: 1.017, epoch: 10.5[0m
[32m[2022-08-26 16:58:56,908] [    INFO][0m - loss: 0.05537092, learning_rate: 2.6700000000000002e-05, global_step: 220, interval_runtime: 0.708, interval_samples_per_second: 11.3, interval_steps_per_second: 14.124, epoch: 11.0[0m
[32m[2022-08-26 16:58:57,670] [    INFO][0m - loss: 0.17128664, learning_rate: 2.655e-05, global_step: 230, interval_runtime: 0.7614, interval_samples_per_second: 10.507, interval_steps_per_second: 13.134, epoch: 11.5[0m
[32m[2022-08-26 16:58:58,377] [    INFO][0m - loss: 0.05852931, learning_rate: 2.64e-05, global_step: 240, interval_runtime: 0.707, interval_samples_per_second: 11.316, interval_steps_per_second: 14.145, epoch: 12.0[0m
[32m[2022-08-26 16:58:59,135] [    INFO][0m - loss: 0.11092778, learning_rate: 2.625e-05, global_step: 250, interval_runtime: 0.7575, interval_samples_per_second: 10.561, interval_steps_per_second: 13.201, epoch: 12.5[0m
[32m[2022-08-26 16:58:59,845] [    INFO][0m - loss: 0.25953767, learning_rate: 2.61e-05, global_step: 260, interval_runtime: 0.7108, interval_samples_per_second: 11.256, interval_steps_per_second: 14.069, epoch: 13.0[0m
[32m[2022-08-26 16:59:00,608] [    INFO][0m - loss: 0.13660207, learning_rate: 2.595e-05, global_step: 270, interval_runtime: 0.763, interval_samples_per_second: 10.485, interval_steps_per_second: 13.106, epoch: 13.5[0m
[32m[2022-08-26 16:59:01,330] [    INFO][0m - loss: 0.2874624, learning_rate: 2.58e-05, global_step: 280, interval_runtime: 0.7219, interval_samples_per_second: 11.083, interval_steps_per_second: 13.853, epoch: 14.0[0m
[32m[2022-08-26 16:59:02,093] [    INFO][0m - loss: 0.08456824, learning_rate: 2.565e-05, global_step: 290, interval_runtime: 0.7628, interval_samples_per_second: 10.487, interval_steps_per_second: 13.109, epoch: 14.5[0m
[32m[2022-08-26 16:59:02,801] [    INFO][0m - loss: 0.14818962, learning_rate: 2.55e-05, global_step: 300, interval_runtime: 0.7078, interval_samples_per_second: 11.303, interval_steps_per_second: 14.129, epoch: 15.0[0m
[32m[2022-08-26 16:59:02,801] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 16:59:02,801] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-26 16:59:02,801] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 16:59:02,801] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 16:59:02,802] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 16:59:03,314] [    INFO][0m - eval_loss: 3.181626319885254, eval_accuracy: 0.5660377358490566, eval_runtime: 0.5118, eval_samples_per_second: 310.665, eval_steps_per_second: 9.769, epoch: 15.0[0m
[32m[2022-08-26 16:59:03,315] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-26 16:59:03,315] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 16:59:05,894] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-26 16:59:05,895] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-26 16:59:12,319] [    INFO][0m - loss: 0.06144022, learning_rate: 2.535e-05, global_step: 310, interval_runtime: 9.5185, interval_samples_per_second: 0.84, interval_steps_per_second: 1.051, epoch: 15.5[0m
[32m[2022-08-26 16:59:13,025] [    INFO][0m - loss: 0.0572899, learning_rate: 2.52e-05, global_step: 320, interval_runtime: 0.7059, interval_samples_per_second: 11.333, interval_steps_per_second: 14.166, epoch: 16.0[0m
[32m[2022-08-26 16:59:13,782] [    INFO][0m - loss: 0.09048299, learning_rate: 2.505e-05, global_step: 330, interval_runtime: 0.7569, interval_samples_per_second: 10.57, interval_steps_per_second: 13.213, epoch: 16.5[0m
[32m[2022-08-26 16:59:14,493] [    INFO][0m - loss: 0.01269472, learning_rate: 2.49e-05, global_step: 340, interval_runtime: 0.7104, interval_samples_per_second: 11.261, interval_steps_per_second: 14.077, epoch: 17.0[0m
[32m[2022-08-26 16:59:15,249] [    INFO][0m - loss: 0.26633921, learning_rate: 2.475e-05, global_step: 350, interval_runtime: 0.7571, interval_samples_per_second: 10.566, interval_steps_per_second: 13.208, epoch: 17.5[0m
[32m[2022-08-26 16:59:15,958] [    INFO][0m - loss: 0.13801413, learning_rate: 2.4599999999999998e-05, global_step: 360, interval_runtime: 0.7086, interval_samples_per_second: 11.291, interval_steps_per_second: 14.113, epoch: 18.0[0m
[32m[2022-08-26 16:59:16,718] [    INFO][0m - loss: 0.04960516, learning_rate: 2.4449999999999998e-05, global_step: 370, interval_runtime: 0.7598, interval_samples_per_second: 10.529, interval_steps_per_second: 13.162, epoch: 18.5[0m
[32m[2022-08-26 16:59:17,425] [    INFO][0m - loss: 0.16404192, learning_rate: 2.43e-05, global_step: 380, interval_runtime: 0.7072, interval_samples_per_second: 11.313, interval_steps_per_second: 14.141, epoch: 19.0[0m
[32m[2022-08-26 16:59:18,185] [    INFO][0m - loss: 0.00067904, learning_rate: 2.415e-05, global_step: 390, interval_runtime: 0.7601, interval_samples_per_second: 10.526, interval_steps_per_second: 13.157, epoch: 19.5[0m
[32m[2022-08-26 16:59:18,894] [    INFO][0m - loss: 0.02969188, learning_rate: 2.4e-05, global_step: 400, interval_runtime: 0.7092, interval_samples_per_second: 11.28, interval_steps_per_second: 14.1, epoch: 20.0[0m
[32m[2022-08-26 16:59:18,895] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 16:59:18,895] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-26 16:59:18,895] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 16:59:18,895] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 16:59:18,895] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 16:59:19,403] [    INFO][0m - eval_loss: 4.130456447601318, eval_accuracy: 0.559748427672956, eval_runtime: 0.5083, eval_samples_per_second: 312.833, eval_steps_per_second: 9.838, epoch: 20.0[0m
[32m[2022-08-26 16:59:19,404] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-26 16:59:19,404] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 16:59:22,196] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-26 16:59:22,196] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-26 16:59:28,481] [    INFO][0m - loss: 0.00663971, learning_rate: 2.385e-05, global_step: 410, interval_runtime: 9.5866, interval_samples_per_second: 0.834, interval_steps_per_second: 1.043, epoch: 20.5[0m
[32m[2022-08-26 16:59:29,191] [    INFO][0m - loss: 0.00746306, learning_rate: 2.37e-05, global_step: 420, interval_runtime: 0.7102, interval_samples_per_second: 11.265, interval_steps_per_second: 14.081, epoch: 21.0[0m
[32m[2022-08-26 16:59:29,945] [    INFO][0m - loss: 0.14366012, learning_rate: 2.3550000000000003e-05, global_step: 430, interval_runtime: 0.7539, interval_samples_per_second: 10.612, interval_steps_per_second: 13.265, epoch: 21.5[0m
[32m[2022-08-26 16:59:30,666] [    INFO][0m - loss: 0.05185609, learning_rate: 2.3400000000000003e-05, global_step: 440, interval_runtime: 0.7211, interval_samples_per_second: 11.095, interval_steps_per_second: 13.868, epoch: 22.0[0m
[32m[2022-08-26 16:59:31,428] [    INFO][0m - loss: 0.0006962, learning_rate: 2.3250000000000003e-05, global_step: 450, interval_runtime: 0.7616, interval_samples_per_second: 10.504, interval_steps_per_second: 13.13, epoch: 22.5[0m
[32m[2022-08-26 16:59:32,137] [    INFO][0m - loss: 0.01462074, learning_rate: 2.3100000000000002e-05, global_step: 460, interval_runtime: 0.7095, interval_samples_per_second: 11.276, interval_steps_per_second: 14.095, epoch: 23.0[0m
[32m[2022-08-26 16:59:32,898] [    INFO][0m - loss: 0.00509849, learning_rate: 2.2950000000000002e-05, global_step: 470, interval_runtime: 0.7605, interval_samples_per_second: 10.519, interval_steps_per_second: 13.148, epoch: 23.5[0m
[32m[2022-08-26 16:59:33,607] [    INFO][0m - loss: 0.07112489, learning_rate: 2.2800000000000002e-05, global_step: 480, interval_runtime: 0.7097, interval_samples_per_second: 11.272, interval_steps_per_second: 14.09, epoch: 24.0[0m
[32m[2022-08-26 16:59:34,398] [    INFO][0m - loss: 0.0009208, learning_rate: 2.265e-05, global_step: 490, interval_runtime: 0.791, interval_samples_per_second: 10.114, interval_steps_per_second: 12.642, epoch: 24.5[0m
[32m[2022-08-26 16:59:35,114] [    INFO][0m - loss: 0.00032786, learning_rate: 2.25e-05, global_step: 500, interval_runtime: 0.7153, interval_samples_per_second: 11.184, interval_steps_per_second: 13.98, epoch: 25.0[0m
[32m[2022-08-26 16:59:35,114] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 16:59:35,114] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-26 16:59:35,114] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 16:59:35,115] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 16:59:35,115] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 16:59:35,632] [    INFO][0m - eval_loss: 5.295051574707031, eval_accuracy: 0.559748427672956, eval_runtime: 0.5167, eval_samples_per_second: 307.706, eval_steps_per_second: 9.676, epoch: 25.0[0m
[32m[2022-08-26 16:59:35,632] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-08-26 16:59:35,632] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 16:59:38,788] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-08-26 16:59:38,789] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-08-26 16:59:45,455] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-26 16:59:45,456] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-100 (score: 0.5723270440251572).[0m
[32m[2022-08-26 16:59:46,173] [    INFO][0m - train_runtime: 84.2526, train_samples_per_second: 189.905, train_steps_per_second: 23.738, train_loss: 0.24491701723169534, epoch: 25.0[0m
[32m[2022-08-26 16:59:46,224] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-26 16:59:46,225] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 16:59:48,793] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-26 16:59:48,793] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-26 16:59:48,795] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-26 16:59:48,795] [    INFO][0m -   epoch                    =       25.0[0m
[32m[2022-08-26 16:59:48,795] [    INFO][0m -   train_loss               =     0.2449[0m
[32m[2022-08-26 16:59:48,795] [    INFO][0m -   train_runtime            = 0:01:24.25[0m
[32m[2022-08-26 16:59:48,795] [    INFO][0m -   train_samples_per_second =    189.905[0m
[32m[2022-08-26 16:59:48,795] [    INFO][0m -   train_steps_per_second   =     23.738[0m
[32m[2022-08-26 16:59:48,798] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-26 16:59:48,798] [    INFO][0m -   Num examples = 976[0m
[32m[2022-08-26 16:59:48,799] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 16:59:48,799] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 16:59:48,799] [    INFO][0m -   Total prediction steps = 31[0m
[32m[2022-08-26 16:59:51,940] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-26 16:59:51,940] [    INFO][0m -   test_accuracy           =     0.4949[0m
[32m[2022-08-26 16:59:51,941] [    INFO][0m -   test_loss               =     1.2188[0m
[32m[2022-08-26 16:59:51,941] [    INFO][0m -   test_runtime            = 0:00:03.14[0m
[32m[2022-08-26 16:59:51,941] [    INFO][0m -   test_samples_per_second =    310.693[0m
[32m[2022-08-26 16:59:51,941] [    INFO][0m -   test_steps_per_second   =      9.868[0m
[32m[2022-08-26 16:59:51,941] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-26 16:59:51,941] [    INFO][0m -   Num examples = 290[0m
[32m[2022-08-26 16:59:51,941] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 16:59:51,942] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 16:59:51,942] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-08-26 16:59:53,142] [    INFO][0m - Predictions for cluewscf saved to ./fewclue_submit_examples.[0m
