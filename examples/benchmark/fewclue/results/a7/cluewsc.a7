[33m[2022-08-26 17:01:36,536] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-08-26 17:01:36,537] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-26 17:01:36,537] [    INFO][0m - ============================================================[0m
[32m[2022-08-26 17:01:36,537] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-26 17:01:36,537] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-26 17:01:36,537] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-26 17:01:36,537] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-26 17:01:36,537] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-08-26 17:01:36,537] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-26 17:01:36,537] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-08-26 17:01:36,537] [    INFO][0m - [0m
[32m[2022-08-26 17:01:36,538] [    INFO][0m - ============================================================[0m
[32m[2022-08-26 17:01:36,538] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-26 17:01:36,538] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-26 17:01:36,538] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-26 17:01:36,538] [    INFO][0m - prompt                        :{'text':'text_a'}{'sep'}{'hard':'ÂÖ∂‰∏≠‰ª£ËØçÁî®'}{'mask'}{'hard':'‰∫Ü„ÄÇ'}[0m
[32m[2022-08-26 17:01:36,538] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-26 17:01:36,538] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-26 17:01:36,538] [    INFO][0m - task_name                     :cluewsc[0m
[32m[2022-08-26 17:01:36,538] [    INFO][0m - [0m
[32m[2022-08-26 17:01:36,538] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0826 17:01:36.539819 43530 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0826 17:01:36.543530 43530 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-26 17:01:39,258] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-08-26 17:01:39,282] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-08-26 17:01:39,282] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[32m[2022-08-26 17:01:39,283] [    INFO][0m - Using template: [{'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'sep': None}, {'add_prefix_space': '', 'hard': 'ÂÖ∂‰∏≠‰ª£ËØçÁî®'}, {'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'hard': '‰∫Ü„ÄÇ'}][0m
2022-08-26 17:01:39,291 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-26 17:01:39,393] [    INFO][0m - ============================================================[0m
[32m[2022-08-26 17:01:39,393] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-26 17:01:39,393] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-26 17:01:39,393] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-26 17:01:39,393] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-26 17:01:39,393] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-26 17:01:39,394] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-26 17:01:39,394] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-26 17:01:39,394] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-26 17:01:39,394] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-26 17:01:39,394] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-26 17:01:39,394] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-26 17:01:39,394] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-26 17:01:39,394] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-26 17:01:39,394] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-26 17:01:39,394] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-26 17:01:39,394] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-26 17:01:39,394] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-26 17:01:39,395] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-26 17:01:39,395] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-08-26 17:01:39,395] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-08-26 17:01:39,395] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-26 17:01:39,395] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-26 17:01:39,395] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-26 17:01:39,395] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-26 17:01:39,395] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-26 17:01:39,395] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-26 17:01:39,395] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-26 17:01:39,395] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-26 17:01:39,395] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-26 17:01:39,395] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-08-26 17:01:39,396] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-26 17:01:39,396] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-26 17:01:39,396] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-26 17:01:39,396] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-26 17:01:39,396] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-26 17:01:39,396] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-26 17:01:39,396] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug26_17-01-36_instance-3bwob41y-01[0m
[32m[2022-08-26 17:01:39,396] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-26 17:01:39,396] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-26 17:01:39,396] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-26 17:01:39,396] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-26 17:01:39,396] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-26 17:01:39,396] [    INFO][0m - max_seq_length                :128[0m
[32m[2022-08-26 17:01:39,397] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-26 17:01:39,397] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-26 17:01:39,397] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-26 17:01:39,397] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-26 17:01:39,397] [    INFO][0m - num_train_epochs              :100.0[0m
[32m[2022-08-26 17:01:39,397] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-26 17:01:39,397] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-26 17:01:39,397] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-26 17:01:39,397] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-26 17:01:39,397] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-26 17:01:39,397] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-26 17:01:39,397] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-08-26 17:01:39,397] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-26 17:01:39,398] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-26 17:01:39,398] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-26 17:01:39,398] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-08-26 17:01:39,398] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-26 17:01:39,398] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-26 17:01:39,398] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-26 17:01:39,398] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-26 17:01:39,398] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-26 17:01:39,398] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-26 17:01:39,398] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-26 17:01:39,398] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-26 17:01:39,398] [    INFO][0m - save_steps                    :100[0m
[32m[2022-08-26 17:01:39,398] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-08-26 17:01:39,399] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-26 17:01:39,399] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-26 17:01:39,399] [    INFO][0m - seed                          :42[0m
[32m[2022-08-26 17:01:39,399] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-26 17:01:39,399] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-26 17:01:39,399] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-26 17:01:39,399] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-08-26 17:01:39,399] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-26 17:01:39,399] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-26 17:01:39,399] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-26 17:01:39,399] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-26 17:01:39,399] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-26 17:01:39,399] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-26 17:01:39,400] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-26 17:01:39,400] [    INFO][0m - [0m
[32m[2022-08-26 17:01:39,401] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-26 17:01:39,402] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-26 17:01:39,402] [    INFO][0m -   Num Epochs = 100[0m
[32m[2022-08-26 17:01:39,402] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-08-26 17:01:39,402] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-08-26 17:01:39,402] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-26 17:01:39,402] [    INFO][0m -   Total optimization steps = 2000.0[0m
[32m[2022-08-26 17:01:39,402] [    INFO][0m -   Total num train samples = 16000[0m
[32m[2022-08-26 17:01:41,164] [    INFO][0m - loss: 0.88325195, learning_rate: 2.985e-05, global_step: 10, interval_runtime: 1.7608, interval_samples_per_second: 4.543, interval_steps_per_second: 5.679, epoch: 0.5[0m
[32m[2022-08-26 17:01:41,871] [    INFO][0m - loss: 0.76250415, learning_rate: 2.97e-05, global_step: 20, interval_runtime: 0.7069, interval_samples_per_second: 11.317, interval_steps_per_second: 14.146, epoch: 1.0[0m
[32m[2022-08-26 17:01:42,634] [    INFO][0m - loss: 0.7382206, learning_rate: 2.955e-05, global_step: 30, interval_runtime: 0.7635, interval_samples_per_second: 10.479, interval_steps_per_second: 13.098, epoch: 1.5[0m
[32m[2022-08-26 17:01:43,342] [    INFO][0m - loss: 0.7279356, learning_rate: 2.94e-05, global_step: 40, interval_runtime: 0.7078, interval_samples_per_second: 11.302, interval_steps_per_second: 14.128, epoch: 2.0[0m
[32m[2022-08-26 17:01:44,100] [    INFO][0m - loss: 0.8014101, learning_rate: 2.925e-05, global_step: 50, interval_runtime: 0.7583, interval_samples_per_second: 10.55, interval_steps_per_second: 13.188, epoch: 2.5[0m
[32m[2022-08-26 17:01:44,807] [    INFO][0m - loss: 0.75029435, learning_rate: 2.91e-05, global_step: 60, interval_runtime: 0.7067, interval_samples_per_second: 11.32, interval_steps_per_second: 14.15, epoch: 3.0[0m
[32m[2022-08-26 17:01:45,563] [    INFO][0m - loss: 0.6781724, learning_rate: 2.895e-05, global_step: 70, interval_runtime: 0.7562, interval_samples_per_second: 10.58, interval_steps_per_second: 13.225, epoch: 3.5[0m
[32m[2022-08-26 17:01:46,271] [    INFO][0m - loss: 0.64608216, learning_rate: 2.88e-05, global_step: 80, interval_runtime: 0.708, interval_samples_per_second: 11.299, interval_steps_per_second: 14.124, epoch: 4.0[0m
[32m[2022-08-26 17:01:47,032] [    INFO][0m - loss: 0.55744424, learning_rate: 2.865e-05, global_step: 90, interval_runtime: 0.7612, interval_samples_per_second: 10.51, interval_steps_per_second: 13.137, epoch: 4.5[0m
[32m[2022-08-26 17:01:47,743] [    INFO][0m - loss: 0.49508171, learning_rate: 2.8499999999999998e-05, global_step: 100, interval_runtime: 0.711, interval_samples_per_second: 11.251, interval_steps_per_second: 14.064, epoch: 5.0[0m
[32m[2022-08-26 17:01:47,744] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:01:47,744] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-26 17:01:47,744] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:01:47,744] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:01:47,744] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 17:01:48,265] [    INFO][0m - eval_loss: 0.9028733968734741, eval_accuracy: 0.49056603773584906, eval_runtime: 0.5211, eval_samples_per_second: 305.146, eval_steps_per_second: 9.596, epoch: 5.0[0m
[32m[2022-08-26 17:01:48,266] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-100[0m
[32m[2022-08-26 17:01:48,266] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:01:50,696] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-100/tokenizer_config.json[0m
[32m[2022-08-26 17:01:50,697] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-100/special_tokens_map.json[0m
[32m[2022-08-26 17:01:56,680] [    INFO][0m - loss: 0.53147736, learning_rate: 2.8349999999999998e-05, global_step: 110, interval_runtime: 8.9369, interval_samples_per_second: 0.895, interval_steps_per_second: 1.119, epoch: 5.5[0m
[32m[2022-08-26 17:01:57,386] [    INFO][0m - loss: 0.57907619, learning_rate: 2.8199999999999998e-05, global_step: 120, interval_runtime: 0.7058, interval_samples_per_second: 11.335, interval_steps_per_second: 14.168, epoch: 6.0[0m
[32m[2022-08-26 17:01:58,145] [    INFO][0m - loss: 0.40378299, learning_rate: 2.805e-05, global_step: 130, interval_runtime: 0.758, interval_samples_per_second: 10.554, interval_steps_per_second: 13.192, epoch: 6.5[0m
[32m[2022-08-26 17:01:58,854] [    INFO][0m - loss: 0.28558414, learning_rate: 2.79e-05, global_step: 140, interval_runtime: 0.7101, interval_samples_per_second: 11.266, interval_steps_per_second: 14.082, epoch: 7.0[0m
[32m[2022-08-26 17:01:59,611] [    INFO][0m - loss: 0.30609906, learning_rate: 2.7750000000000004e-05, global_step: 150, interval_runtime: 0.7574, interval_samples_per_second: 10.563, interval_steps_per_second: 13.204, epoch: 7.5[0m
[32m[2022-08-26 17:02:00,322] [    INFO][0m - loss: 0.26233296, learning_rate: 2.7600000000000003e-05, global_step: 160, interval_runtime: 0.7106, interval_samples_per_second: 11.259, interval_steps_per_second: 14.074, epoch: 8.0[0m
[32m[2022-08-26 17:02:01,082] [    INFO][0m - loss: 0.31271844, learning_rate: 2.7450000000000003e-05, global_step: 170, interval_runtime: 0.76, interval_samples_per_second: 10.526, interval_steps_per_second: 13.157, epoch: 8.5[0m
[32m[2022-08-26 17:02:01,791] [    INFO][0m - loss: 0.14901378, learning_rate: 2.7300000000000003e-05, global_step: 180, interval_runtime: 0.7086, interval_samples_per_second: 11.29, interval_steps_per_second: 14.113, epoch: 9.0[0m
[32m[2022-08-26 17:02:02,551] [    INFO][0m - loss: 0.40111547, learning_rate: 2.7150000000000003e-05, global_step: 190, interval_runtime: 0.7604, interval_samples_per_second: 10.521, interval_steps_per_second: 13.152, epoch: 9.5[0m
[32m[2022-08-26 17:02:03,261] [    INFO][0m - loss: 0.31911497, learning_rate: 2.7000000000000002e-05, global_step: 200, interval_runtime: 0.7101, interval_samples_per_second: 11.266, interval_steps_per_second: 14.082, epoch: 10.0[0m
[32m[2022-08-26 17:02:03,262] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:02:03,262] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-26 17:02:03,262] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:02:03,262] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:02:03,262] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 17:02:03,780] [    INFO][0m - eval_loss: 2.1402509212493896, eval_accuracy: 0.4716981132075472, eval_runtime: 0.5176, eval_samples_per_second: 307.195, eval_steps_per_second: 9.66, epoch: 10.0[0m
[32m[2022-08-26 17:02:03,780] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-26 17:02:03,781] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:02:06,594] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-26 17:02:06,594] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-26 17:02:12,737] [    INFO][0m - loss: 0.20515316, learning_rate: 2.6850000000000002e-05, global_step: 210, interval_runtime: 9.4758, interval_samples_per_second: 0.844, interval_steps_per_second: 1.055, epoch: 10.5[0m
[32m[2022-08-26 17:02:13,441] [    INFO][0m - loss: 0.23134513, learning_rate: 2.6700000000000002e-05, global_step: 220, interval_runtime: 0.704, interval_samples_per_second: 11.364, interval_steps_per_second: 14.205, epoch: 11.0[0m
[32m[2022-08-26 17:02:14,203] [    INFO][0m - loss: 0.19741071, learning_rate: 2.655e-05, global_step: 230, interval_runtime: 0.7617, interval_samples_per_second: 10.503, interval_steps_per_second: 13.129, epoch: 11.5[0m
[32m[2022-08-26 17:02:14,913] [    INFO][0m - loss: 0.35763478, learning_rate: 2.64e-05, global_step: 240, interval_runtime: 0.7098, interval_samples_per_second: 11.27, interval_steps_per_second: 14.088, epoch: 12.0[0m
[32m[2022-08-26 17:02:15,670] [    INFO][0m - loss: 0.33671618, learning_rate: 2.625e-05, global_step: 250, interval_runtime: 0.7572, interval_samples_per_second: 10.565, interval_steps_per_second: 13.207, epoch: 12.5[0m
[32m[2022-08-26 17:02:16,378] [    INFO][0m - loss: 0.21727848, learning_rate: 2.61e-05, global_step: 260, interval_runtime: 0.7085, interval_samples_per_second: 11.291, interval_steps_per_second: 14.114, epoch: 13.0[0m
[32m[2022-08-26 17:02:17,140] [    INFO][0m - loss: 0.18801881, learning_rate: 2.595e-05, global_step: 270, interval_runtime: 0.7614, interval_samples_per_second: 10.507, interval_steps_per_second: 13.133, epoch: 13.5[0m
[32m[2022-08-26 17:02:17,851] [    INFO][0m - loss: 0.38821468, learning_rate: 2.58e-05, global_step: 280, interval_runtime: 0.7118, interval_samples_per_second: 11.24, interval_steps_per_second: 14.05, epoch: 14.0[0m
[32m[2022-08-26 17:02:18,609] [    INFO][0m - loss: 0.16664629, learning_rate: 2.565e-05, global_step: 290, interval_runtime: 0.7576, interval_samples_per_second: 10.559, interval_steps_per_second: 13.199, epoch: 14.5[0m
[32m[2022-08-26 17:02:19,319] [    INFO][0m - loss: 0.16519213, learning_rate: 2.55e-05, global_step: 300, interval_runtime: 0.7102, interval_samples_per_second: 11.265, interval_steps_per_second: 14.081, epoch: 15.0[0m
[32m[2022-08-26 17:02:19,320] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:02:19,320] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-26 17:02:19,320] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:02:19,320] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:02:19,320] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 17:02:19,885] [    INFO][0m - eval_loss: 2.57096266746521, eval_accuracy: 0.5345911949685535, eval_runtime: 0.5648, eval_samples_per_second: 281.503, eval_steps_per_second: 8.852, epoch: 15.0[0m
[32m[2022-08-26 17:02:19,886] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-300[0m
[32m[2022-08-26 17:02:19,886] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:02:22,584] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-300/tokenizer_config.json[0m
[32m[2022-08-26 17:02:22,584] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-300/special_tokens_map.json[0m
[32m[2022-08-26 17:02:28,720] [    INFO][0m - loss: 0.35391257, learning_rate: 2.535e-05, global_step: 310, interval_runtime: 9.4006, interval_samples_per_second: 0.851, interval_steps_per_second: 1.064, epoch: 15.5[0m
[32m[2022-08-26 17:02:29,430] [    INFO][0m - loss: 0.14521896, learning_rate: 2.52e-05, global_step: 320, interval_runtime: 0.7103, interval_samples_per_second: 11.263, interval_steps_per_second: 14.078, epoch: 16.0[0m
[32m[2022-08-26 17:02:30,190] [    INFO][0m - loss: 0.14571811, learning_rate: 2.505e-05, global_step: 330, interval_runtime: 0.7596, interval_samples_per_second: 10.532, interval_steps_per_second: 13.165, epoch: 16.5[0m
[32m[2022-08-26 17:02:30,902] [    INFO][0m - loss: 0.15120075, learning_rate: 2.49e-05, global_step: 340, interval_runtime: 0.7124, interval_samples_per_second: 11.23, interval_steps_per_second: 14.038, epoch: 17.0[0m
[32m[2022-08-26 17:02:31,664] [    INFO][0m - loss: 0.21895909, learning_rate: 2.475e-05, global_step: 350, interval_runtime: 0.7623, interval_samples_per_second: 10.495, interval_steps_per_second: 13.119, epoch: 17.5[0m
[32m[2022-08-26 17:02:32,373] [    INFO][0m - loss: 0.20422633, learning_rate: 2.4599999999999998e-05, global_step: 360, interval_runtime: 0.7088, interval_samples_per_second: 11.287, interval_steps_per_second: 14.108, epoch: 18.0[0m
[32m[2022-08-26 17:02:33,138] [    INFO][0m - loss: 0.29665861, learning_rate: 2.4449999999999998e-05, global_step: 370, interval_runtime: 0.7643, interval_samples_per_second: 10.467, interval_steps_per_second: 13.084, epoch: 18.5[0m
[32m[2022-08-26 17:02:33,847] [    INFO][0m - loss: 0.08879644, learning_rate: 2.43e-05, global_step: 380, interval_runtime: 0.7098, interval_samples_per_second: 11.271, interval_steps_per_second: 14.089, epoch: 19.0[0m
[32m[2022-08-26 17:02:34,611] [    INFO][0m - loss: 0.07849365, learning_rate: 2.415e-05, global_step: 390, interval_runtime: 0.7636, interval_samples_per_second: 10.476, interval_steps_per_second: 13.095, epoch: 19.5[0m
[32m[2022-08-26 17:02:35,320] [    INFO][0m - loss: 0.12445078, learning_rate: 2.4e-05, global_step: 400, interval_runtime: 0.7089, interval_samples_per_second: 11.286, interval_steps_per_second: 14.107, epoch: 20.0[0m
[32m[2022-08-26 17:02:35,320] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:02:35,320] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-26 17:02:35,320] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:02:35,321] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:02:35,321] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 17:02:35,841] [    INFO][0m - eval_loss: 3.5138092041015625, eval_accuracy: 0.5283018867924528, eval_runtime: 0.5198, eval_samples_per_second: 305.889, eval_steps_per_second: 9.619, epoch: 20.0[0m
[32m[2022-08-26 17:02:35,841] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-26 17:02:35,841] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:02:38,231] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-26 17:02:38,232] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-26 17:02:44,348] [    INFO][0m - loss: 0.09404784, learning_rate: 2.385e-05, global_step: 410, interval_runtime: 9.0282, interval_samples_per_second: 0.886, interval_steps_per_second: 1.108, epoch: 20.5[0m
[32m[2022-08-26 17:02:45,057] [    INFO][0m - loss: 0.02902265, learning_rate: 2.37e-05, global_step: 420, interval_runtime: 0.7086, interval_samples_per_second: 11.289, interval_steps_per_second: 14.112, epoch: 21.0[0m
[32m[2022-08-26 17:02:45,812] [    INFO][0m - loss: 0.03632205, learning_rate: 2.3550000000000003e-05, global_step: 430, interval_runtime: 0.7551, interval_samples_per_second: 10.595, interval_steps_per_second: 13.243, epoch: 21.5[0m
[32m[2022-08-26 17:02:46,519] [    INFO][0m - loss: 0.03668596, learning_rate: 2.3400000000000003e-05, global_step: 440, interval_runtime: 0.7074, interval_samples_per_second: 11.309, interval_steps_per_second: 14.136, epoch: 22.0[0m
[32m[2022-08-26 17:02:47,278] [    INFO][0m - loss: 0.17561824, learning_rate: 2.3250000000000003e-05, global_step: 450, interval_runtime: 0.7593, interval_samples_per_second: 10.536, interval_steps_per_second: 13.17, epoch: 22.5[0m
[32m[2022-08-26 17:02:47,988] [    INFO][0m - loss: 0.11848942, learning_rate: 2.3100000000000002e-05, global_step: 460, interval_runtime: 0.71, interval_samples_per_second: 11.268, interval_steps_per_second: 14.085, epoch: 23.0[0m
[32m[2022-08-26 17:02:48,751] [    INFO][0m - loss: 0.11050743, learning_rate: 2.2950000000000002e-05, global_step: 470, interval_runtime: 0.7627, interval_samples_per_second: 10.489, interval_steps_per_second: 13.111, epoch: 23.5[0m
[32m[2022-08-26 17:02:49,460] [    INFO][0m - loss: 0.02101983, learning_rate: 2.2800000000000002e-05, global_step: 480, interval_runtime: 0.7087, interval_samples_per_second: 11.288, interval_steps_per_second: 14.11, epoch: 24.0[0m
[32m[2022-08-26 17:02:50,221] [    INFO][0m - loss: 0.04216163, learning_rate: 2.265e-05, global_step: 490, interval_runtime: 0.7606, interval_samples_per_second: 10.518, interval_steps_per_second: 13.148, epoch: 24.5[0m
[32m[2022-08-26 17:02:50,930] [    INFO][0m - loss: 0.01201062, learning_rate: 2.25e-05, global_step: 500, interval_runtime: 0.7098, interval_samples_per_second: 11.27, interval_steps_per_second: 14.088, epoch: 25.0[0m
[32m[2022-08-26 17:02:50,931] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:02:50,931] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-26 17:02:50,931] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:02:50,931] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:02:50,931] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 17:02:51,451] [    INFO][0m - eval_loss: 5.982546329498291, eval_accuracy: 0.5534591194968553, eval_runtime: 0.5201, eval_samples_per_second: 305.704, eval_steps_per_second: 9.613, epoch: 25.0[0m
[32m[2022-08-26 17:02:51,452] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-500[0m
[32m[2022-08-26 17:02:51,452] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:02:53,798] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json[0m
[32m[2022-08-26 17:02:53,799] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json[0m
[32m[2022-08-26 17:02:59,940] [    INFO][0m - loss: 0.01661367, learning_rate: 2.235e-05, global_step: 510, interval_runtime: 9.0092, interval_samples_per_second: 0.888, interval_steps_per_second: 1.11, epoch: 25.5[0m
[32m[2022-08-26 17:03:00,648] [    INFO][0m - loss: 0.10896237, learning_rate: 2.22e-05, global_step: 520, interval_runtime: 0.7087, interval_samples_per_second: 11.288, interval_steps_per_second: 14.11, epoch: 26.0[0m
[32m[2022-08-26 17:03:01,408] [    INFO][0m - loss: 3.428e-05, learning_rate: 2.205e-05, global_step: 530, interval_runtime: 0.7599, interval_samples_per_second: 10.527, interval_steps_per_second: 13.159, epoch: 26.5[0m
[32m[2022-08-26 17:03:02,119] [    INFO][0m - loss: 0.06592489, learning_rate: 2.19e-05, global_step: 540, interval_runtime: 0.7104, interval_samples_per_second: 11.261, interval_steps_per_second: 14.076, epoch: 27.0[0m
[32m[2022-08-26 17:03:02,878] [    INFO][0m - loss: 0.20038795, learning_rate: 2.175e-05, global_step: 550, interval_runtime: 0.7594, interval_samples_per_second: 10.534, interval_steps_per_second: 13.168, epoch: 27.5[0m
[32m[2022-08-26 17:03:03,589] [    INFO][0m - loss: 0.00156014, learning_rate: 2.16e-05, global_step: 560, interval_runtime: 0.7109, interval_samples_per_second: 11.253, interval_steps_per_second: 14.066, epoch: 28.0[0m
[32m[2022-08-26 17:03:04,348] [    INFO][0m - loss: 0.13565438, learning_rate: 2.145e-05, global_step: 570, interval_runtime: 0.7595, interval_samples_per_second: 10.533, interval_steps_per_second: 13.166, epoch: 28.5[0m
[32m[2022-08-26 17:03:05,063] [    INFO][0m - loss: 4.521e-05, learning_rate: 2.13e-05, global_step: 580, interval_runtime: 0.7144, interval_samples_per_second: 11.198, interval_steps_per_second: 13.998, epoch: 29.0[0m
[32m[2022-08-26 17:03:05,822] [    INFO][0m - loss: 0.01738247, learning_rate: 2.115e-05, global_step: 590, interval_runtime: 0.7589, interval_samples_per_second: 10.541, interval_steps_per_second: 13.176, epoch: 29.5[0m
[32m[2022-08-26 17:03:06,531] [    INFO][0m - loss: 5.446e-05, learning_rate: 2.1e-05, global_step: 600, interval_runtime: 0.7093, interval_samples_per_second: 11.279, interval_steps_per_second: 14.098, epoch: 30.0[0m
[32m[2022-08-26 17:03:06,532] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:03:06,532] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-26 17:03:06,532] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:03:06,532] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:03:06,532] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 17:03:07,051] [    INFO][0m - eval_loss: 4.432095050811768, eval_accuracy: 0.5534591194968553, eval_runtime: 0.5193, eval_samples_per_second: 306.191, eval_steps_per_second: 9.629, epoch: 30.0[0m
[32m[2022-08-26 17:03:07,052] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-08-26 17:03:07,052] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:03:09,379] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-08-26 17:03:09,380] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-08-26 17:03:15,125] [    INFO][0m - loss: 0.02939349, learning_rate: 2.085e-05, global_step: 610, interval_runtime: 8.5941, interval_samples_per_second: 0.931, interval_steps_per_second: 1.164, epoch: 30.5[0m
[32m[2022-08-26 17:03:15,835] [    INFO][0m - loss: 1.402e-05, learning_rate: 2.07e-05, global_step: 620, interval_runtime: 0.7099, interval_samples_per_second: 11.269, interval_steps_per_second: 14.087, epoch: 31.0[0m
[32m[2022-08-26 17:03:16,599] [    INFO][0m - loss: 0.02771229, learning_rate: 2.055e-05, global_step: 630, interval_runtime: 0.7639, interval_samples_per_second: 10.473, interval_steps_per_second: 13.091, epoch: 31.5[0m
[32m[2022-08-26 17:03:17,308] [    INFO][0m - loss: 0.00509391, learning_rate: 2.04e-05, global_step: 640, interval_runtime: 0.7093, interval_samples_per_second: 11.278, interval_steps_per_second: 14.098, epoch: 32.0[0m
[32m[2022-08-26 17:03:18,071] [    INFO][0m - loss: 5.592e-05, learning_rate: 2.025e-05, global_step: 650, interval_runtime: 0.7624, interval_samples_per_second: 10.493, interval_steps_per_second: 13.116, epoch: 32.5[0m
[32m[2022-08-26 17:03:18,781] [    INFO][0m - loss: 0.0649398, learning_rate: 2.01e-05, global_step: 660, interval_runtime: 0.7102, interval_samples_per_second: 11.264, interval_steps_per_second: 14.081, epoch: 33.0[0m
[32m[2022-08-26 17:03:19,540] [    INFO][0m - loss: 1.707e-05, learning_rate: 1.995e-05, global_step: 670, interval_runtime: 0.7593, interval_samples_per_second: 10.537, interval_steps_per_second: 13.171, epoch: 33.5[0m
[32m[2022-08-26 17:03:20,252] [    INFO][0m - loss: 0.00066226, learning_rate: 1.98e-05, global_step: 680, interval_runtime: 0.7113, interval_samples_per_second: 11.247, interval_steps_per_second: 14.059, epoch: 34.0[0m
[32m[2022-08-26 17:03:21,021] [    INFO][0m - loss: 4.58e-06, learning_rate: 1.965e-05, global_step: 690, interval_runtime: 0.769, interval_samples_per_second: 10.403, interval_steps_per_second: 13.003, epoch: 34.5[0m
[32m[2022-08-26 17:03:21,733] [    INFO][0m - loss: 0.00029608, learning_rate: 1.95e-05, global_step: 700, interval_runtime: 0.7123, interval_samples_per_second: 11.23, interval_steps_per_second: 14.038, epoch: 35.0[0m
[32m[2022-08-26 17:03:21,734] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:03:21,734] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-26 17:03:21,734] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:03:21,734] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:03:21,734] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 17:03:22,251] [    INFO][0m - eval_loss: 5.37076997756958, eval_accuracy: 0.5094339622641509, eval_runtime: 0.5167, eval_samples_per_second: 307.708, eval_steps_per_second: 9.676, epoch: 35.0[0m
[32m[2022-08-26 17:03:22,251] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-700[0m
[32m[2022-08-26 17:03:22,252] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:03:24,572] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-700/tokenizer_config.json[0m
[32m[2022-08-26 17:03:24,573] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-700/special_tokens_map.json[0m
[32m[2022-08-26 17:03:30,263] [    INFO][0m - loss: 0.06208596, learning_rate: 1.935e-05, global_step: 710, interval_runtime: 8.5295, interval_samples_per_second: 0.938, interval_steps_per_second: 1.172, epoch: 35.5[0m
[32m[2022-08-26 17:03:30,973] [    INFO][0m - loss: 8.69e-06, learning_rate: 1.9200000000000003e-05, global_step: 720, interval_runtime: 0.7106, interval_samples_per_second: 11.258, interval_steps_per_second: 14.072, epoch: 36.0[0m
[32m[2022-08-26 17:03:31,733] [    INFO][0m - loss: 0.00065527, learning_rate: 1.9050000000000002e-05, global_step: 730, interval_runtime: 0.7601, interval_samples_per_second: 10.525, interval_steps_per_second: 13.156, epoch: 36.5[0m
[32m[2022-08-26 17:03:32,441] [    INFO][0m - loss: 0.01069213, learning_rate: 1.8900000000000002e-05, global_step: 740, interval_runtime: 0.708, interval_samples_per_second: 11.299, interval_steps_per_second: 14.123, epoch: 37.0[0m
[32m[2022-08-26 17:03:33,201] [    INFO][0m - loss: 1.38e-06, learning_rate: 1.8750000000000002e-05, global_step: 750, interval_runtime: 0.76, interval_samples_per_second: 10.526, interval_steps_per_second: 13.158, epoch: 37.5[0m
[32m[2022-08-26 17:03:33,912] [    INFO][0m - loss: 0.00028587, learning_rate: 1.86e-05, global_step: 760, interval_runtime: 0.7103, interval_samples_per_second: 11.263, interval_steps_per_second: 14.078, epoch: 38.0[0m
[32m[2022-08-26 17:03:34,673] [    INFO][0m - loss: 1.534e-05, learning_rate: 1.845e-05, global_step: 770, interval_runtime: 0.7612, interval_samples_per_second: 10.509, interval_steps_per_second: 13.136, epoch: 38.5[0m
[32m[2022-08-26 17:03:35,381] [    INFO][0m - loss: 9.1e-07, learning_rate: 1.83e-05, global_step: 780, interval_runtime: 0.7081, interval_samples_per_second: 11.298, interval_steps_per_second: 14.122, epoch: 39.0[0m
[32m[2022-08-26 17:03:36,139] [    INFO][0m - loss: 8.41e-06, learning_rate: 1.815e-05, global_step: 790, interval_runtime: 0.7585, interval_samples_per_second: 10.547, interval_steps_per_second: 13.184, epoch: 39.5[0m
[32m[2022-08-26 17:03:36,850] [    INFO][0m - loss: 3.26e-06, learning_rate: 1.8e-05, global_step: 800, interval_runtime: 0.7107, interval_samples_per_second: 11.257, interval_steps_per_second: 14.071, epoch: 40.0[0m
[32m[2022-08-26 17:03:36,851] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:03:36,851] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-26 17:03:36,851] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:03:36,851] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:03:36,851] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 17:03:37,370] [    INFO][0m - eval_loss: 5.715388298034668, eval_accuracy: 0.49056603773584906, eval_runtime: 0.5194, eval_samples_per_second: 306.148, eval_steps_per_second: 9.627, epoch: 40.0[0m
[32m[2022-08-26 17:03:37,371] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-800[0m
[32m[2022-08-26 17:03:37,371] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:03:39,670] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-800/tokenizer_config.json[0m
[32m[2022-08-26 17:03:39,670] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-800/special_tokens_map.json[0m
[32m[2022-08-26 17:03:45,373] [    INFO][0m - loss: 0.00651733, learning_rate: 1.785e-05, global_step: 810, interval_runtime: 8.5231, interval_samples_per_second: 0.939, interval_steps_per_second: 1.173, epoch: 40.5[0m
[32m[2022-08-26 17:03:46,081] [    INFO][0m - loss: 7.1e-06, learning_rate: 1.77e-05, global_step: 820, interval_runtime: 0.7078, interval_samples_per_second: 11.303, interval_steps_per_second: 14.128, epoch: 41.0[0m
[32m[2022-08-26 17:03:46,842] [    INFO][0m - loss: 4.22e-06, learning_rate: 1.755e-05, global_step: 830, interval_runtime: 0.7606, interval_samples_per_second: 10.518, interval_steps_per_second: 13.148, epoch: 41.5[0m
[32m[2022-08-26 17:03:47,552] [    INFO][0m - loss: 0.10671253, learning_rate: 1.74e-05, global_step: 840, interval_runtime: 0.7105, interval_samples_per_second: 11.26, interval_steps_per_second: 14.075, epoch: 42.0[0m
[32m[2022-08-26 17:03:48,310] [    INFO][0m - loss: 3.77e-06, learning_rate: 1.725e-05, global_step: 850, interval_runtime: 0.7583, interval_samples_per_second: 10.55, interval_steps_per_second: 13.188, epoch: 42.5[0m
[32m[2022-08-26 17:03:49,023] [    INFO][0m - loss: 0.00281128, learning_rate: 1.71e-05, global_step: 860, interval_runtime: 0.7125, interval_samples_per_second: 11.228, interval_steps_per_second: 14.035, epoch: 43.0[0m
[32m[2022-08-26 17:03:49,785] [    INFO][0m - loss: 0.10144879, learning_rate: 1.695e-05, global_step: 870, interval_runtime: 0.7617, interval_samples_per_second: 10.502, interval_steps_per_second: 13.128, epoch: 43.5[0m
[32m[2022-08-26 17:03:50,494] [    INFO][0m - loss: 3.2e-06, learning_rate: 1.6800000000000002e-05, global_step: 880, interval_runtime: 0.7094, interval_samples_per_second: 11.278, interval_steps_per_second: 14.097, epoch: 44.0[0m
[32m[2022-08-26 17:03:51,254] [    INFO][0m - loss: 3.542e-05, learning_rate: 1.665e-05, global_step: 890, interval_runtime: 0.7596, interval_samples_per_second: 10.532, interval_steps_per_second: 13.165, epoch: 44.5[0m
[32m[2022-08-26 17:03:51,964] [    INFO][0m - loss: 9e-07, learning_rate: 1.65e-05, global_step: 900, interval_runtime: 0.7103, interval_samples_per_second: 11.263, interval_steps_per_second: 14.078, epoch: 45.0[0m
[32m[2022-08-26 17:03:51,964] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-26 17:03:51,964] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-26 17:03:51,965] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:03:51,965] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:03:51,965] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-26 17:03:52,485] [    INFO][0m - eval_loss: 6.18953275680542, eval_accuracy: 0.5031446540880503, eval_runtime: 0.52, eval_samples_per_second: 305.759, eval_steps_per_second: 9.615, epoch: 45.0[0m
[32m[2022-08-26 17:03:52,485] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-900[0m
[32m[2022-08-26 17:03:52,485] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:03:54,803] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-900/tokenizer_config.json[0m
[32m[2022-08-26 17:03:54,804] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-900/special_tokens_map.json[0m
[32m[2022-08-26 17:03:59,747] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-26 17:03:59,748] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-500 (score: 0.5534591194968553).[0m
[32m[2022-08-26 17:04:00,538] [    INFO][0m - train_runtime: 141.1357, train_samples_per_second: 113.366, train_steps_per_second: 14.171, train_loss: 0.1832661657981003, epoch: 45.0[0m
[32m[2022-08-26 17:04:00,580] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-26 17:04:00,580] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-26 17:04:02,914] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-26 17:04:02,915] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-26 17:04:02,916] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-26 17:04:02,916] [    INFO][0m -   epoch                    =       45.0[0m
[32m[2022-08-26 17:04:02,916] [    INFO][0m -   train_loss               =     0.1833[0m
[32m[2022-08-26 17:04:02,916] [    INFO][0m -   train_runtime            = 0:02:21.13[0m
[32m[2022-08-26 17:04:02,916] [    INFO][0m -   train_samples_per_second =    113.366[0m
[32m[2022-08-26 17:04:02,916] [    INFO][0m -   train_steps_per_second   =     14.171[0m
[32m[2022-08-26 17:04:02,921] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-26 17:04:02,921] [    INFO][0m -   Num examples = 976[0m
[32m[2022-08-26 17:04:02,921] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:04:02,921] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:04:02,921] [    INFO][0m -   Total prediction steps = 31[0m
[32m[2022-08-26 17:04:06,067] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-26 17:04:06,067] [    INFO][0m -   test_accuracy           =     0.4969[0m
[32m[2022-08-26 17:04:06,068] [    INFO][0m -   test_loss               =     7.2024[0m
[32m[2022-08-26 17:04:06,068] [    INFO][0m -   test_runtime            = 0:00:03.14[0m
[32m[2022-08-26 17:04:06,068] [    INFO][0m -   test_samples_per_second =    310.207[0m
[32m[2022-08-26 17:04:06,068] [    INFO][0m -   test_steps_per_second   =      9.853[0m
[32m[2022-08-26 17:04:06,068] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-26 17:04:06,068] [    INFO][0m -   Num examples = 290[0m
[32m[2022-08-26 17:04:06,068] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-26 17:04:06,068] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-26 17:04:06,068] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-08-26 17:04:07,289] [    INFO][0m - Predictions for cluewscf saved to ./fewclue_submit_examples.[0m
