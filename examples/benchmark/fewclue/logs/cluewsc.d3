[32m[2022-08-25 19:30:06,966] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-25 19:30:06,966] [    INFO][0m - ============================================================[0m
[32m[2022-08-25 19:30:06,967] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-25 19:30:06,967] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-25 19:30:06,967] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-25 19:30:06,967] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-25 19:30:06,967] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-25 19:30:06,967] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-08-25 19:30:06,967] [    INFO][0m - [0m
[32m[2022-08-25 19:30:06,967] [    INFO][0m - ============================================================[0m
[32m[2022-08-25 19:30:06,967] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-25 19:30:06,967] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-25 19:30:06,967] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-25 19:30:06,967] [    INFO][0m - prompt                        :{'mask'}{'soft':'ÂêàÁêÜ„ÄÇ'}{'text':'text_a'}[0m
[32m[2022-08-25 19:30:06,968] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-25 19:30:06,968] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-25 19:30:06,968] [    INFO][0m - task_name                     :cluewsc[0m
[32m[2022-08-25 19:30:06,968] [    INFO][0m - [0m
[32m[2022-08-25 19:30:06,968] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0825 19:30:06.969377 48064 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0825 19:30:06.973266 48064 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-25 19:30:09,717] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-08-25 19:30:09,740] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-08-25 19:30:09,741] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[33m[2022-08-25 19:30:09,747] [ WARNING][0m - Encoder has already set as lstm, change `prompt_encoder` will reset parameters.[0m
[32m[2022-08-25 19:30:09,755] [    INFO][0m - Using template: [{'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'soft': 'Âêà'}, {'add_prefix_space': '', 'soft': 'ÁêÜ'}, {'add_prefix_space': '', 'soft': '„ÄÇ'}, {'add_prefix_space': '', 'text': 'text_a'}][0m
2022-08-25 19:30:09,760 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-25 19:30:09,858] [    INFO][0m - ============================================================[0m
[32m[2022-08-25 19:30:09,858] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-25 19:30:09,858] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-25 19:30:09,858] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-25 19:30:09,858] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-25 19:30:09,858] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-25 19:30:09,858] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-25 19:30:09,859] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-25 19:30:09,859] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-25 19:30:09,859] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-25 19:30:09,859] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-25 19:30:09,859] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-25 19:30:09,859] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-25 19:30:09,859] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-25 19:30:09,859] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-25 19:30:09,859] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-25 19:30:09,859] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-25 19:30:09,859] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-25 19:30:09,859] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-25 19:30:09,860] [    INFO][0m - eval_steps                    :10[0m
[32m[2022-08-25 19:30:09,860] [    INFO][0m - evaluation_strategy           :IntervalStrategy.EPOCH[0m
[32m[2022-08-25 19:30:09,860] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-25 19:30:09,860] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-25 19:30:09,860] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-25 19:30:09,860] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-25 19:30:09,860] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-25 19:30:09,860] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-25 19:30:09,860] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-25 19:30:09,860] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-25 19:30:09,860] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-25 19:30:09,860] [    INFO][0m - learning_rate                 :1e-05[0m
[32m[2022-08-25 19:30:09,860] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-25 19:30:09,861] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-25 19:30:09,861] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-25 19:30:09,861] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-25 19:30:09,861] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-25 19:30:09,861] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-25 19:30:09,861] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug25_19-30-06_instance-3bwob41y-01[0m
[32m[2022-08-25 19:30:09,861] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-25 19:30:09,861] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-25 19:30:09,861] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-25 19:30:09,861] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-25 19:30:09,861] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-25 19:30:09,861] [    INFO][0m - max_seq_length                :128[0m
[32m[2022-08-25 19:30:09,861] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-25 19:30:09,861] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-25 19:30:09,862] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-25 19:30:09,862] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-25 19:30:09,862] [    INFO][0m - num_train_epochs              :20.0[0m
[32m[2022-08-25 19:30:09,862] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-25 19:30:09,862] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-25 19:30:09,862] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-25 19:30:09,862] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-25 19:30:09,862] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-25 19:30:09,862] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-25 19:30:09,862] [    INFO][0m - per_device_train_batch_size   :4[0m
[32m[2022-08-25 19:30:09,862] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-25 19:30:09,862] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-25 19:30:09,862] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-25 19:30:09,863] [    INFO][0m - ppt_learning_rate             :0.0001[0m
[32m[2022-08-25 19:30:09,863] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-25 19:30:09,863] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-25 19:30:09,863] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-25 19:30:09,863] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-25 19:30:09,863] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-25 19:30:09,863] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-25 19:30:09,863] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-25 19:30:09,863] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-25 19:30:09,863] [    INFO][0m - save_steps                    :500[0m
[32m[2022-08-25 19:30:09,863] [    INFO][0m - save_strategy                 :IntervalStrategy.EPOCH[0m
[32m[2022-08-25 19:30:09,863] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-25 19:30:09,863] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-25 19:30:09,864] [    INFO][0m - seed                          :42[0m
[32m[2022-08-25 19:30:09,864] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-25 19:30:09,864] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-25 19:30:09,864] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-25 19:30:09,864] [    INFO][0m - train_batch_size              :4[0m
[32m[2022-08-25 19:30:09,864] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-25 19:30:09,864] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-25 19:30:09,864] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-25 19:30:09,864] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-25 19:30:09,864] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-25 19:30:09,864] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-25 19:30:09,864] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-25 19:30:09,864] [    INFO][0m - [0m
[32m[2022-08-25 19:30:09,866] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-25 19:30:09,866] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-25 19:30:09,866] [    INFO][0m -   Num Epochs = 20[0m
[32m[2022-08-25 19:30:09,867] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2022-08-25 19:30:09,867] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 4[0m
[32m[2022-08-25 19:30:09,867] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-25 19:30:09,867] [    INFO][0m -   Total optimization steps = 800.0[0m
[32m[2022-08-25 19:30:09,867] [    INFO][0m -   Total num train samples = 3200[0m
[32m[2022-08-25 19:30:12,112] [    INFO][0m - loss: 0.76363916, learning_rate: 9.875000000000001e-06, global_step: 10, interval_runtime: 2.2445, interval_samples_per_second: 1.782, interval_steps_per_second: 4.455, epoch: 0.25[0m
[32m[2022-08-25 19:30:12,883] [    INFO][0m - loss: 0.73448186, learning_rate: 9.75e-06, global_step: 20, interval_runtime: 0.7712, interval_samples_per_second: 5.186, interval_steps_per_second: 12.966, epoch: 0.5[0m
[32m[2022-08-25 19:30:13,668] [    INFO][0m - loss: 0.82118893, learning_rate: 9.625e-06, global_step: 30, interval_runtime: 0.7845, interval_samples_per_second: 5.099, interval_steps_per_second: 12.747, epoch: 0.75[0m
[32m[2022-08-25 19:30:14,406] [    INFO][0m - loss: 0.73550406, learning_rate: 9.5e-06, global_step: 40, interval_runtime: 0.7386, interval_samples_per_second: 5.416, interval_steps_per_second: 13.54, epoch: 1.0[0m
[32m[2022-08-25 19:30:14,407] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 19:30:14,407] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-25 19:30:14,407] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 19:30:14,407] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 19:30:14,408] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-25 19:30:15,050] [    INFO][0m - eval_loss: 0.6861154437065125, eval_accuracy: 0.559748427672956, eval_runtime: 0.6423, eval_samples_per_second: 247.551, eval_steps_per_second: 7.785, epoch: 1.0[0m
[32m[2022-08-25 19:30:15,050] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-40[0m
[32m[2022-08-25 19:30:15,050] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 19:30:18,320] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-40/tokenizer_config.json[0m
[32m[2022-08-25 19:30:18,321] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-40/special_tokens_map.json[0m
[32m[2022-08-25 19:30:22,925] [    INFO][0m - loss: 0.67673244, learning_rate: 9.375000000000001e-06, global_step: 50, interval_runtime: 8.5189, interval_samples_per_second: 0.47, interval_steps_per_second: 1.174, epoch: 1.25[0m
[32m[2022-08-25 19:30:23,763] [    INFO][0m - loss: 0.5908782, learning_rate: 9.250000000000001e-06, global_step: 60, interval_runtime: 0.8382, interval_samples_per_second: 4.772, interval_steps_per_second: 11.93, epoch: 1.5[0m
[32m[2022-08-25 19:30:24,554] [    INFO][0m - loss: 0.70573006, learning_rate: 9.125e-06, global_step: 70, interval_runtime: 0.7901, interval_samples_per_second: 5.063, interval_steps_per_second: 12.657, epoch: 1.75[0m
[32m[2022-08-25 19:30:25,400] [    INFO][0m - loss: 0.78160925, learning_rate: 9e-06, global_step: 80, interval_runtime: 0.8464, interval_samples_per_second: 4.726, interval_steps_per_second: 11.814, epoch: 2.0[0m
[32m[2022-08-25 19:30:25,401] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 19:30:25,401] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-25 19:30:25,401] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 19:30:25,401] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 19:30:25,401] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-25 19:30:26,112] [    INFO][0m - eval_loss: 0.8339371681213379, eval_accuracy: 0.5157232704402516, eval_runtime: 0.7102, eval_samples_per_second: 223.865, eval_steps_per_second: 7.04, epoch: 2.0[0m
[32m[2022-08-25 19:30:26,112] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-80[0m
[32m[2022-08-25 19:30:26,112] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 19:30:29,251] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-80/tokenizer_config.json[0m
[32m[2022-08-25 19:30:29,252] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-80/special_tokens_map.json[0m
[32m[2022-08-25 19:30:33,986] [    INFO][0m - loss: 0.70657773, learning_rate: 8.875e-06, global_step: 90, interval_runtime: 8.5856, interval_samples_per_second: 0.466, interval_steps_per_second: 1.165, epoch: 2.25[0m
[32m[2022-08-25 19:30:34,858] [    INFO][0m - loss: 0.63819008, learning_rate: 8.750000000000001e-06, global_step: 100, interval_runtime: 0.8728, interval_samples_per_second: 4.583, interval_steps_per_second: 11.458, epoch: 2.5[0m
[32m[2022-08-25 19:30:35,702] [    INFO][0m - loss: 0.62075863, learning_rate: 8.625000000000001e-06, global_step: 110, interval_runtime: 0.8438, interval_samples_per_second: 4.741, interval_steps_per_second: 11.851, epoch: 2.75[0m
[32m[2022-08-25 19:30:36,511] [    INFO][0m - loss: 0.76375823, learning_rate: 8.5e-06, global_step: 120, interval_runtime: 0.8084, interval_samples_per_second: 4.948, interval_steps_per_second: 12.37, epoch: 3.0[0m
[32m[2022-08-25 19:30:36,512] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 19:30:36,512] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-25 19:30:36,512] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 19:30:36,512] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 19:30:36,512] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-25 19:30:37,299] [    INFO][0m - eval_loss: 0.7403739094734192, eval_accuracy: 0.4779874213836478, eval_runtime: 0.7874, eval_samples_per_second: 201.936, eval_steps_per_second: 6.35, epoch: 3.0[0m
[32m[2022-08-25 19:30:37,300] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-120[0m
[32m[2022-08-25 19:30:37,300] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 19:30:41,160] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-120/tokenizer_config.json[0m
[32m[2022-08-25 19:30:41,161] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-120/special_tokens_map.json[0m
[32m[2022-08-25 19:30:46,290] [    INFO][0m - loss: 0.57481294, learning_rate: 8.375e-06, global_step: 130, interval_runtime: 9.7796, interval_samples_per_second: 0.409, interval_steps_per_second: 1.023, epoch: 3.25[0m
[32m[2022-08-25 19:30:47,189] [    INFO][0m - loss: 0.64582462, learning_rate: 8.25e-06, global_step: 140, interval_runtime: 0.8994, interval_samples_per_second: 4.447, interval_steps_per_second: 11.119, epoch: 3.5[0m
[32m[2022-08-25 19:30:48,098] [    INFO][0m - loss: 0.77948599, learning_rate: 8.125000000000001e-06, global_step: 150, interval_runtime: 0.9086, interval_samples_per_second: 4.402, interval_steps_per_second: 11.006, epoch: 3.75[0m
[32m[2022-08-25 19:30:48,895] [    INFO][0m - loss: 0.64252853, learning_rate: 8.000000000000001e-06, global_step: 160, interval_runtime: 0.7966, interval_samples_per_second: 5.021, interval_steps_per_second: 12.553, epoch: 4.0[0m
[32m[2022-08-25 19:30:48,895] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 19:30:48,895] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-25 19:30:48,896] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 19:30:48,896] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 19:30:48,896] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-25 19:30:49,793] [    INFO][0m - eval_loss: 0.7535749673843384, eval_accuracy: 0.559748427672956, eval_runtime: 0.8973, eval_samples_per_second: 177.197, eval_steps_per_second: 5.572, epoch: 4.0[0m
[32m[2022-08-25 19:30:49,794] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-160[0m
[32m[2022-08-25 19:30:49,794] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 19:30:53,026] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-160/tokenizer_config.json[0m
[32m[2022-08-25 19:30:53,027] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-160/special_tokens_map.json[0m
[32m[2022-08-25 19:30:57,840] [    INFO][0m - loss: 0.55264854, learning_rate: 7.875e-06, global_step: 170, interval_runtime: 8.9451, interval_samples_per_second: 0.447, interval_steps_per_second: 1.118, epoch: 4.25[0m
[32m[2022-08-25 19:30:58,855] [    INFO][0m - loss: 0.71296911, learning_rate: 7.75e-06, global_step: 180, interval_runtime: 1.0154, interval_samples_per_second: 3.939, interval_steps_per_second: 9.848, epoch: 4.5[0m
[32m[2022-08-25 19:30:59,838] [    INFO][0m - loss: 0.5529531, learning_rate: 7.625e-06, global_step: 190, interval_runtime: 0.9831, interval_samples_per_second: 4.069, interval_steps_per_second: 10.172, epoch: 4.75[0m
[32m[2022-08-25 19:31:00,725] [    INFO][0m - loss: 0.52690578, learning_rate: 7.500000000000001e-06, global_step: 200, interval_runtime: 0.8862, interval_samples_per_second: 4.514, interval_steps_per_second: 11.284, epoch: 5.0[0m
[32m[2022-08-25 19:31:00,726] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 19:31:00,726] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-25 19:31:00,726] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 19:31:00,726] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 19:31:00,726] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-25 19:31:01,709] [    INFO][0m - eval_loss: 0.8188482522964478, eval_accuracy: 0.4716981132075472, eval_runtime: 0.9826, eval_samples_per_second: 161.809, eval_steps_per_second: 5.088, epoch: 5.0[0m
[32m[2022-08-25 19:31:01,709] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-25 19:31:01,709] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 19:31:04,878] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-25 19:31:04,878] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-25 19:31:08,629] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-25 19:31:08,629] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-40 (score: 0.559748427672956).[0m
[32m[2022-08-25 19:31:09,822] [    INFO][0m - train_runtime: 59.9543, train_samples_per_second: 53.374, train_steps_per_second: 13.343, train_loss: 0.6763588619232178, epoch: 5.0[0m
[32m[2022-08-25 19:31:09,823] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-25 19:31:09,823] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 19:31:13,041] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-25 19:31:13,041] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-25 19:31:13,042] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-25 19:31:13,042] [    INFO][0m -   epoch                    =        5.0[0m
[32m[2022-08-25 19:31:13,042] [    INFO][0m -   train_loss               =     0.6764[0m
[32m[2022-08-25 19:31:13,042] [    INFO][0m -   train_runtime            = 0:00:59.95[0m
[32m[2022-08-25 19:31:13,042] [    INFO][0m -   train_samples_per_second =     53.374[0m
[32m[2022-08-25 19:31:13,043] [    INFO][0m -   train_steps_per_second   =     13.343[0m
[32m[2022-08-25 19:31:13,044] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-25 19:31:13,045] [    INFO][0m -   Num examples = 976[0m
[32m[2022-08-25 19:31:13,045] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 19:31:13,045] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 19:31:13,045] [    INFO][0m -   Total prediction steps = 31[0m
[32m[2022-08-25 19:31:18,924] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-25 19:31:18,925] [    INFO][0m -   test_accuracy           =     0.5266[0m
[32m[2022-08-25 19:31:18,925] [    INFO][0m -   test_loss               =     0.7183[0m
[32m[2022-08-25 19:31:18,925] [    INFO][0m -   test_runtime            = 0:00:05.87[0m
[32m[2022-08-25 19:31:18,925] [    INFO][0m -   test_samples_per_second =    165.994[0m
[32m[2022-08-25 19:31:18,925] [    INFO][0m -   test_steps_per_second   =      5.272[0m
[32m[2022-08-25 19:31:18,926] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-25 19:31:18,926] [    INFO][0m -   Num examples = 290[0m
[32m[2022-08-25 19:31:18,926] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 19:31:18,926] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 19:31:18,926] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-08-25 19:31:21,041] [    INFO][0m - Predictions for cluewscf saved to ./fewclue_submit_examples.[0m
[32m[2022-08-25 19:40:07,610] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-25 19:40:07,611] [    INFO][0m - ============================================================[0m
[32m[2022-08-25 19:40:07,611] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-25 19:40:07,611] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-25 19:40:07,611] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-25 19:40:07,611] [    INFO][0m - do_test                       :False[0m
[32m[2022-08-25 19:40:07,611] [    INFO][0m - early_stop_patience           :6[0m
[32m[2022-08-25 19:40:07,611] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-25 19:40:07,611] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-08-25 19:40:07,611] [    INFO][0m - [0m
[32m[2022-08-25 19:40:07,611] [    INFO][0m - ============================================================[0m
[32m[2022-08-25 19:40:07,612] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-25 19:40:07,612] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-25 19:40:07,612] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-25 19:40:07,612] [    INFO][0m - prompt                        :{'mask'}{'soft':'ÂêàÁêÜ„ÄÇ'}{'text':'text_a'}[0m
[32m[2022-08-25 19:40:07,612] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-25 19:40:07,612] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-25 19:40:07,612] [    INFO][0m - task_name                     :cluewsc[0m
[32m[2022-08-25 19:40:07,612] [    INFO][0m - [0m
[32m[2022-08-25 19:40:07,612] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0825 19:40:07.613723 30972 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0825 19:40:07.617857 30972 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-25 19:40:10,391] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-08-25 19:40:10,415] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-08-25 19:40:10,415] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[33m[2022-08-25 19:40:10,422] [ WARNING][0m - Encoder has already set as lstm, change `prompt_encoder` will reset parameters.[0m
[32m[2022-08-25 19:40:10,430] [    INFO][0m - Using template: [{'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'soft': 'Âêà'}, {'add_prefix_space': '', 'soft': 'ÁêÜ'}, {'add_prefix_space': '', 'soft': '„ÄÇ'}, {'add_prefix_space': '', 'text': 'text_a'}][0m
2022-08-25 19:40:10,435 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-25 19:40:10,534] [    INFO][0m - ============================================================[0m
[32m[2022-08-25 19:40:10,534] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-25 19:40:10,534] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-25 19:40:10,534] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-25 19:40:10,534] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-25 19:40:10,534] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-25 19:40:10,534] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-25 19:40:10,535] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-25 19:40:10,535] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-25 19:40:10,535] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-25 19:40:10,535] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-25 19:40:10,535] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-25 19:40:10,535] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-25 19:40:10,535] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-25 19:40:10,535] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-25 19:40:10,535] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-25 19:40:10,535] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-25 19:40:10,535] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-25 19:40:10,535] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-25 19:40:10,536] [    INFO][0m - eval_steps                    :10[0m
[32m[2022-08-25 19:40:10,536] [    INFO][0m - evaluation_strategy           :IntervalStrategy.EPOCH[0m
[32m[2022-08-25 19:40:10,536] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-25 19:40:10,536] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-25 19:40:10,536] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-25 19:40:10,536] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-25 19:40:10,536] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-25 19:40:10,536] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-25 19:40:10,536] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-25 19:40:10,536] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-25 19:40:10,536] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-25 19:40:10,536] [    INFO][0m - learning_rate                 :1e-05[0m
[32m[2022-08-25 19:40:10,536] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-25 19:40:10,537] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-25 19:40:10,537] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-25 19:40:10,537] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-25 19:40:10,537] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-25 19:40:10,537] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-25 19:40:10,537] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug25_19-40-07_instance-3bwob41y-01[0m
[32m[2022-08-25 19:40:10,537] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-25 19:40:10,537] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-25 19:40:10,537] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-25 19:40:10,537] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-25 19:40:10,537] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-25 19:40:10,537] [    INFO][0m - max_seq_length                :128[0m
[32m[2022-08-25 19:40:10,537] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-25 19:40:10,537] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-25 19:40:10,538] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-25 19:40:10,538] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-25 19:40:10,538] [    INFO][0m - num_train_epochs              :20.0[0m
[32m[2022-08-25 19:40:10,538] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-25 19:40:10,538] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-25 19:40:10,538] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-25 19:40:10,538] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-25 19:40:10,538] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-25 19:40:10,538] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-25 19:40:10,538] [    INFO][0m - per_device_train_batch_size   :4[0m
[32m[2022-08-25 19:40:10,538] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-25 19:40:10,538] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-25 19:40:10,538] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-25 19:40:10,539] [    INFO][0m - ppt_learning_rate             :0.0001[0m
[32m[2022-08-25 19:40:10,539] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-25 19:40:10,539] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-25 19:40:10,539] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-25 19:40:10,539] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-25 19:40:10,539] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-25 19:40:10,539] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-25 19:40:10,539] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-25 19:40:10,539] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-25 19:40:10,539] [    INFO][0m - save_steps                    :500[0m
[32m[2022-08-25 19:40:10,539] [    INFO][0m - save_strategy                 :IntervalStrategy.EPOCH[0m
[32m[2022-08-25 19:40:10,539] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-25 19:40:10,539] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-25 19:40:10,539] [    INFO][0m - seed                          :42[0m
[32m[2022-08-25 19:40:10,540] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-25 19:40:10,540] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-25 19:40:10,540] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-25 19:40:10,540] [    INFO][0m - train_batch_size              :4[0m
[32m[2022-08-25 19:40:10,540] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-25 19:40:10,540] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-25 19:40:10,540] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-25 19:40:10,540] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-25 19:40:10,540] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-25 19:40:10,540] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-25 19:40:10,540] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-25 19:40:10,540] [    INFO][0m - [0m
[32m[2022-08-25 19:40:10,542] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-25 19:40:10,542] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-25 19:40:10,542] [    INFO][0m -   Num Epochs = 20[0m
[32m[2022-08-25 19:40:10,542] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2022-08-25 19:40:10,543] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 4[0m
[32m[2022-08-25 19:40:10,543] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-25 19:40:10,543] [    INFO][0m -   Total optimization steps = 800.0[0m
[32m[2022-08-25 19:40:10,543] [    INFO][0m -   Total num train samples = 3200[0m
[32m[2022-08-25 19:40:12,793] [    INFO][0m - loss: 0.80145359, learning_rate: 9.875000000000001e-06, global_step: 10, interval_runtime: 2.249, interval_samples_per_second: 1.779, interval_steps_per_second: 4.446, epoch: 0.25[0m
[32m[2022-08-25 19:40:13,677] [    INFO][0m - loss: 0.73660288, learning_rate: 9.75e-06, global_step: 20, interval_runtime: 0.8845, interval_samples_per_second: 4.522, interval_steps_per_second: 11.306, epoch: 0.5[0m
[32m[2022-08-25 19:40:14,444] [    INFO][0m - loss: 0.76675386, learning_rate: 9.625e-06, global_step: 30, interval_runtime: 0.7673, interval_samples_per_second: 5.213, interval_steps_per_second: 13.033, epoch: 0.75[0m
[32m[2022-08-25 19:40:15,263] [    INFO][0m - loss: 0.78052006, learning_rate: 9.5e-06, global_step: 40, interval_runtime: 0.8185, interval_samples_per_second: 4.887, interval_steps_per_second: 12.218, epoch: 1.0[0m
[32m[2022-08-25 19:40:15,263] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 19:40:15,264] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-25 19:40:15,264] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 19:40:15,264] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 19:40:15,264] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-25 19:40:15,903] [    INFO][0m - eval_loss: 0.7210463881492615, eval_accuracy: 0.49056603773584906, eval_runtime: 0.6394, eval_samples_per_second: 248.657, eval_steps_per_second: 7.819, epoch: 1.0[0m
[32m[2022-08-25 19:40:15,904] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-40[0m
[32m[2022-08-25 19:40:15,904] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 19:40:18,986] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-40/tokenizer_config.json[0m
[32m[2022-08-25 19:40:18,986] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-40/special_tokens_map.json[0m
[32m[2022-08-25 19:40:23,885] [    INFO][0m - loss: 0.64472089, learning_rate: 9.375000000000001e-06, global_step: 50, interval_runtime: 8.6221, interval_samples_per_second: 0.464, interval_steps_per_second: 1.16, epoch: 1.25[0m
[32m[2022-08-25 19:40:24,704] [    INFO][0m - loss: 0.66936193, learning_rate: 9.250000000000001e-06, global_step: 60, interval_runtime: 0.8188, interval_samples_per_second: 4.885, interval_steps_per_second: 12.213, epoch: 1.5[0m
[32m[2022-08-25 19:40:25,503] [    INFO][0m - loss: 0.66978245, learning_rate: 9.125e-06, global_step: 70, interval_runtime: 0.7996, interval_samples_per_second: 5.002, interval_steps_per_second: 12.506, epoch: 1.75[0m
[32m[2022-08-25 19:40:26,238] [    INFO][0m - loss: 0.79257693, learning_rate: 9e-06, global_step: 80, interval_runtime: 0.735, interval_samples_per_second: 5.442, interval_steps_per_second: 13.606, epoch: 2.0[0m
[32m[2022-08-25 19:40:26,239] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 19:40:26,239] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-25 19:40:26,239] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 19:40:26,239] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 19:40:26,239] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-25 19:40:26,974] [    INFO][0m - eval_loss: 0.7537552118301392, eval_accuracy: 0.4779874213836478, eval_runtime: 0.7345, eval_samples_per_second: 216.487, eval_steps_per_second: 6.808, epoch: 2.0[0m
[32m[2022-08-25 19:40:26,974] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-80[0m
[32m[2022-08-25 19:40:26,974] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 19:40:30,059] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-80/tokenizer_config.json[0m
[32m[2022-08-25 19:40:30,059] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-80/special_tokens_map.json[0m
[32m[2022-08-25 19:40:34,682] [    INFO][0m - loss: 0.71517076, learning_rate: 8.875e-06, global_step: 90, interval_runtime: 8.4441, interval_samples_per_second: 0.474, interval_steps_per_second: 1.184, epoch: 2.25[0m
[32m[2022-08-25 19:40:35,518] [    INFO][0m - loss: 0.66836882, learning_rate: 8.750000000000001e-06, global_step: 100, interval_runtime: 0.836, interval_samples_per_second: 4.785, interval_steps_per_second: 11.961, epoch: 2.5[0m
[32m[2022-08-25 19:40:36,410] [    INFO][0m - loss: 0.57929945, learning_rate: 8.625000000000001e-06, global_step: 110, interval_runtime: 0.8917, interval_samples_per_second: 4.486, interval_steps_per_second: 11.214, epoch: 2.75[0m
[32m[2022-08-25 19:40:37,161] [    INFO][0m - loss: 0.80599508, learning_rate: 8.5e-06, global_step: 120, interval_runtime: 0.7512, interval_samples_per_second: 5.325, interval_steps_per_second: 13.313, epoch: 3.0[0m
[32m[2022-08-25 19:40:37,162] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 19:40:37,162] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-25 19:40:37,162] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 19:40:37,162] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 19:40:37,162] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-25 19:40:37,950] [    INFO][0m - eval_loss: 0.7545753121376038, eval_accuracy: 0.4716981132075472, eval_runtime: 0.7876, eval_samples_per_second: 201.873, eval_steps_per_second: 6.348, epoch: 3.0[0m
[32m[2022-08-25 19:40:37,950] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-120[0m
[32m[2022-08-25 19:40:37,950] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 19:40:41,365] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-120/tokenizer_config.json[0m
[32m[2022-08-25 19:40:41,365] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-120/special_tokens_map.json[0m
[32m[2022-08-25 19:40:46,099] [    INFO][0m - loss: 0.57433505, learning_rate: 8.375e-06, global_step: 130, interval_runtime: 8.9379, interval_samples_per_second: 0.448, interval_steps_per_second: 1.119, epoch: 3.25[0m
[32m[2022-08-25 19:40:46,991] [    INFO][0m - loss: 0.5724813, learning_rate: 8.25e-06, global_step: 140, interval_runtime: 0.8923, interval_samples_per_second: 4.483, interval_steps_per_second: 11.207, epoch: 3.5[0m
[32m[2022-08-25 19:40:47,889] [    INFO][0m - loss: 0.66598058, learning_rate: 8.125000000000001e-06, global_step: 150, interval_runtime: 0.8972, interval_samples_per_second: 4.458, interval_steps_per_second: 11.146, epoch: 3.75[0m
[32m[2022-08-25 19:40:48,690] [    INFO][0m - loss: 0.6214467, learning_rate: 8.000000000000001e-06, global_step: 160, interval_runtime: 0.8011, interval_samples_per_second: 4.993, interval_steps_per_second: 12.483, epoch: 4.0[0m
[32m[2022-08-25 19:40:48,690] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 19:40:48,691] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-25 19:40:48,691] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 19:40:48,691] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 19:40:48,691] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-25 19:40:49,562] [    INFO][0m - eval_loss: 0.7713260650634766, eval_accuracy: 0.5220125786163522, eval_runtime: 0.8712, eval_samples_per_second: 182.511, eval_steps_per_second: 5.739, epoch: 4.0[0m
[32m[2022-08-25 19:40:49,563] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-160[0m
[32m[2022-08-25 19:40:49,563] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 19:40:53,010] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-160/tokenizer_config.json[0m
[32m[2022-08-25 19:40:53,011] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-160/special_tokens_map.json[0m
[32m[2022-08-25 19:40:58,070] [    INFO][0m - loss: 0.59363952, learning_rate: 7.875e-06, global_step: 170, interval_runtime: 9.3802, interval_samples_per_second: 0.426, interval_steps_per_second: 1.066, epoch: 4.25[0m
[32m[2022-08-25 19:40:59,002] [    INFO][0m - loss: 0.59748049, learning_rate: 7.75e-06, global_step: 180, interval_runtime: 0.9317, interval_samples_per_second: 4.293, interval_steps_per_second: 10.733, epoch: 4.5[0m
[32m[2022-08-25 19:40:59,966] [    INFO][0m - loss: 0.52864223, learning_rate: 7.625e-06, global_step: 190, interval_runtime: 0.9647, interval_samples_per_second: 4.146, interval_steps_per_second: 10.366, epoch: 4.75[0m
[32m[2022-08-25 19:41:00,785] [    INFO][0m - loss: 0.59848523, learning_rate: 7.500000000000001e-06, global_step: 200, interval_runtime: 0.8182, interval_samples_per_second: 4.889, interval_steps_per_second: 12.222, epoch: 5.0[0m
[32m[2022-08-25 19:41:00,785] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 19:41:00,785] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-25 19:41:00,785] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 19:41:00,785] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 19:41:00,785] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-25 19:41:01,737] [    INFO][0m - eval_loss: 0.7612478733062744, eval_accuracy: 0.5157232704402516, eval_runtime: 0.9501, eval_samples_per_second: 167.342, eval_steps_per_second: 5.262, epoch: 5.0[0m
[32m[2022-08-25 19:41:01,737] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-25 19:41:01,737] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 19:41:04,824] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-25 19:41:04,825] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-25 19:41:10,071] [    INFO][0m - loss: 0.49641418, learning_rate: 7.375000000000001e-06, global_step: 210, interval_runtime: 9.2863, interval_samples_per_second: 0.431, interval_steps_per_second: 1.077, epoch: 5.25[0m
[32m[2022-08-25 19:41:11,095] [    INFO][0m - loss: 0.62456541, learning_rate: 7.25e-06, global_step: 220, interval_runtime: 1.0242, interval_samples_per_second: 3.906, interval_steps_per_second: 9.764, epoch: 5.5[0m
[32m[2022-08-25 19:41:12,088] [    INFO][0m - loss: 0.46299152, learning_rate: 7.125e-06, global_step: 230, interval_runtime: 0.9928, interval_samples_per_second: 4.029, interval_steps_per_second: 10.073, epoch: 5.75[0m
[32m[2022-08-25 19:41:12,954] [    INFO][0m - loss: 0.47206845, learning_rate: 7e-06, global_step: 240, interval_runtime: 0.8663, interval_samples_per_second: 4.617, interval_steps_per_second: 11.543, epoch: 6.0[0m
[32m[2022-08-25 19:41:12,955] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 19:41:12,955] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-25 19:41:12,955] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 19:41:12,955] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 19:41:12,955] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-25 19:41:13,971] [    INFO][0m - eval_loss: 0.8429760932922363, eval_accuracy: 0.5157232704402516, eval_runtime: 1.0159, eval_samples_per_second: 156.506, eval_steps_per_second: 4.922, epoch: 6.0[0m
[32m[2022-08-25 19:41:13,971] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-240[0m
[32m[2022-08-25 19:41:13,972] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 19:41:17,453] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-240/tokenizer_config.json[0m
[32m[2022-08-25 19:41:17,454] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-240/special_tokens_map.json[0m
[32m[2022-08-25 19:41:22,824] [    INFO][0m - loss: 0.44618998, learning_rate: 6.875e-06, global_step: 250, interval_runtime: 9.8695, interval_samples_per_second: 0.405, interval_steps_per_second: 1.013, epoch: 6.25[0m
[32m[2022-08-25 19:41:23,915] [    INFO][0m - loss: 0.45292501, learning_rate: 6.750000000000001e-06, global_step: 260, interval_runtime: 1.0909, interval_samples_per_second: 3.667, interval_steps_per_second: 9.167, epoch: 6.5[0m
[32m[2022-08-25 19:41:24,966] [    INFO][0m - loss: 0.4604877, learning_rate: 6.625e-06, global_step: 270, interval_runtime: 1.0514, interval_samples_per_second: 3.804, interval_steps_per_second: 9.511, epoch: 6.75[0m
[32m[2022-08-25 19:41:25,896] [    INFO][0m - loss: 0.4794229, learning_rate: 6.5000000000000004e-06, global_step: 280, interval_runtime: 0.9296, interval_samples_per_second: 4.303, interval_steps_per_second: 10.757, epoch: 7.0[0m
[32m[2022-08-25 19:41:25,897] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 19:41:25,897] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-25 19:41:25,897] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 19:41:25,897] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 19:41:25,897] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-25 19:41:26,993] [    INFO][0m - eval_loss: 0.8552320003509521, eval_accuracy: 0.5345911949685535, eval_runtime: 1.0959, eval_samples_per_second: 145.083, eval_steps_per_second: 4.562, epoch: 7.0[0m
[32m[2022-08-25 19:41:26,994] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-280[0m
[32m[2022-08-25 19:41:26,994] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 19:41:28,482] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-280/tokenizer_config.json[0m
[32m[2022-08-25 19:41:28,482] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-280/special_tokens_map.json[0m
[32m[2022-08-25 19:41:31,650] [    INFO][0m - loss: 0.39276743, learning_rate: 6.375e-06, global_step: 290, interval_runtime: 5.7544, interval_samples_per_second: 0.695, interval_steps_per_second: 1.738, epoch: 7.25[0m
[32m[2022-08-25 19:41:32,729] [    INFO][0m - loss: 0.37272105, learning_rate: 6.25e-06, global_step: 300, interval_runtime: 1.0777, interval_samples_per_second: 3.712, interval_steps_per_second: 9.279, epoch: 7.5[0m
[32m[2022-08-25 19:41:33,847] [    INFO][0m - loss: 0.47286792, learning_rate: 6.125000000000001e-06, global_step: 310, interval_runtime: 1.1189, interval_samples_per_second: 3.575, interval_steps_per_second: 8.937, epoch: 7.75[0m
[32m[2022-08-25 19:41:34,757] [    INFO][0m - loss: 0.38850131, learning_rate: 6e-06, global_step: 320, interval_runtime: 0.9101, interval_samples_per_second: 4.395, interval_steps_per_second: 10.988, epoch: 8.0[0m
[32m[2022-08-25 19:41:34,757] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 19:41:34,758] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-25 19:41:34,758] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 19:41:34,758] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 19:41:34,758] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-25 19:41:35,979] [    INFO][0m - eval_loss: 0.9248054623603821, eval_accuracy: 0.5345911949685535, eval_runtime: 1.2211, eval_samples_per_second: 130.206, eval_steps_per_second: 4.095, epoch: 8.0[0m
[32m[2022-08-25 19:41:35,980] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-320[0m
[32m[2022-08-25 19:41:35,980] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 19:41:37,431] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-320/tokenizer_config.json[0m
[32m[2022-08-25 19:41:37,431] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-320/special_tokens_map.json[0m
[32m[2022-08-25 19:41:40,415] [    INFO][0m - loss: 0.47319078, learning_rate: 5.8750000000000005e-06, global_step: 330, interval_runtime: 5.658, interval_samples_per_second: 0.707, interval_steps_per_second: 1.767, epoch: 8.25[0m
[32m[2022-08-25 19:41:41,569] [    INFO][0m - loss: 0.20731263, learning_rate: 5.75e-06, global_step: 340, interval_runtime: 1.154, interval_samples_per_second: 3.466, interval_steps_per_second: 8.666, epoch: 8.5[0m
[32m[2022-08-25 19:41:42,736] [    INFO][0m - loss: 0.18687149, learning_rate: 5.625e-06, global_step: 350, interval_runtime: 1.1666, interval_samples_per_second: 3.429, interval_steps_per_second: 8.572, epoch: 8.75[0m
[32m[2022-08-25 19:41:43,644] [    INFO][0m - loss: 0.39617476, learning_rate: 5.500000000000001e-06, global_step: 360, interval_runtime: 0.9086, interval_samples_per_second: 4.403, interval_steps_per_second: 11.007, epoch: 9.0[0m
[32m[2022-08-25 19:41:43,645] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 19:41:43,645] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-25 19:41:43,645] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 19:41:43,645] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 19:41:43,645] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-25 19:41:44,908] [    INFO][0m - eval_loss: 1.0701905488967896, eval_accuracy: 0.5157232704402516, eval_runtime: 1.2628, eval_samples_per_second: 125.911, eval_steps_per_second: 3.959, epoch: 9.0[0m
[32m[2022-08-25 19:41:44,908] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-360[0m
[32m[2022-08-25 19:41:44,908] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 19:41:46,342] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-360/tokenizer_config.json[0m
[32m[2022-08-25 19:41:46,343] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-360/special_tokens_map.json[0m
[32m[2022-08-25 19:41:49,419] [    INFO][0m - loss: 0.17521141, learning_rate: 5.375e-06, global_step: 370, interval_runtime: 5.7751, interval_samples_per_second: 0.693, interval_steps_per_second: 1.732, epoch: 9.25[0m
[32m[2022-08-25 19:41:50,618] [    INFO][0m - loss: 0.5248517, learning_rate: 5.2500000000000006e-06, global_step: 380, interval_runtime: 1.1985, interval_samples_per_second: 3.338, interval_steps_per_second: 8.344, epoch: 9.5[0m
[32m[2022-08-25 19:41:51,803] [    INFO][0m - loss: 0.22906656, learning_rate: 5.125e-06, global_step: 390, interval_runtime: 1.1855, interval_samples_per_second: 3.374, interval_steps_per_second: 8.435, epoch: 9.75[0m
[32m[2022-08-25 19:41:52,802] [    INFO][0m - loss: 0.27756686, learning_rate: 5e-06, global_step: 400, interval_runtime: 0.9984, interval_samples_per_second: 4.006, interval_steps_per_second: 10.016, epoch: 10.0[0m
[32m[2022-08-25 19:41:52,802] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 19:41:52,802] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-25 19:41:52,802] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 19:41:52,802] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 19:41:52,803] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-25 19:41:54,129] [    INFO][0m - eval_loss: 1.2068365812301636, eval_accuracy: 0.5220125786163522, eval_runtime: 1.3267, eval_samples_per_second: 119.849, eval_steps_per_second: 3.769, epoch: 10.0[0m
[32m[2022-08-25 19:41:54,130] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-400[0m
[32m[2022-08-25 19:41:54,130] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 19:41:55,555] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-400/tokenizer_config.json[0m
[32m[2022-08-25 19:41:55,555] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-400/special_tokens_map.json[0m
[32m[2022-08-25 19:41:59,750] [    INFO][0m - loss: 0.29930282, learning_rate: 4.875e-06, global_step: 410, interval_runtime: 5.9479, interval_samples_per_second: 0.673, interval_steps_per_second: 1.681, epoch: 10.25[0m
[32m[2022-08-25 19:42:00,964] [    INFO][0m - loss: 0.34004261, learning_rate: 4.75e-06, global_step: 420, interval_runtime: 2.2143, interval_samples_per_second: 1.806, interval_steps_per_second: 4.516, epoch: 10.5[0m
[32m[2022-08-25 19:42:02,131] [    INFO][0m - loss: 0.12471268, learning_rate: 4.625000000000001e-06, global_step: 430, interval_runtime: 1.1671, interval_samples_per_second: 3.427, interval_steps_per_second: 8.568, epoch: 10.75[0m
[32m[2022-08-25 19:42:03,109] [    INFO][0m - loss: 0.61880188, learning_rate: 4.5e-06, global_step: 440, interval_runtime: 0.9782, interval_samples_per_second: 4.089, interval_steps_per_second: 10.223, epoch: 11.0[0m
[32m[2022-08-25 19:42:03,110] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 19:42:03,110] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-25 19:42:03,110] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 19:42:03,110] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 19:42:03,110] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-25 19:42:04,507] [    INFO][0m - eval_loss: 1.4481505155563354, eval_accuracy: 0.5471698113207547, eval_runtime: 1.3968, eval_samples_per_second: 113.832, eval_steps_per_second: 3.58, epoch: 11.0[0m
[32m[2022-08-25 19:42:04,508] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-440[0m
[32m[2022-08-25 19:42:04,509] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 19:42:05,955] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-440/tokenizer_config.json[0m
[32m[2022-08-25 19:42:05,955] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-440/special_tokens_map.json[0m
[32m[2022-08-25 19:42:09,402] [    INFO][0m - loss: 0.37874088, learning_rate: 4.3750000000000005e-06, global_step: 450, interval_runtime: 6.2924, interval_samples_per_second: 0.636, interval_steps_per_second: 1.589, epoch: 11.25[0m
[32m[2022-08-25 19:42:10,644] [    INFO][0m - loss: 0.13564543, learning_rate: 4.25e-06, global_step: 460, interval_runtime: 1.2425, interval_samples_per_second: 3.219, interval_steps_per_second: 8.048, epoch: 11.5[0m
[32m[2022-08-25 19:42:11,858] [    INFO][0m - loss: 0.41966553, learning_rate: 4.125e-06, global_step: 470, interval_runtime: 1.2143, interval_samples_per_second: 3.294, interval_steps_per_second: 8.235, epoch: 11.75[0m
[32m[2022-08-25 19:42:12,874] [    INFO][0m - loss: 0.26389658, learning_rate: 4.000000000000001e-06, global_step: 480, interval_runtime: 1.016, interval_samples_per_second: 3.937, interval_steps_per_second: 9.843, epoch: 12.0[0m
[32m[2022-08-25 19:42:12,875] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 19:42:12,875] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-25 19:42:12,875] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 19:42:12,876] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 19:42:12,876] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-25 19:42:14,378] [    INFO][0m - eval_loss: 1.4833389520645142, eval_accuracy: 0.4779874213836478, eval_runtime: 1.5025, eval_samples_per_second: 105.821, eval_steps_per_second: 3.328, epoch: 12.0[0m
[32m[2022-08-25 19:42:14,379] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-480[0m
[32m[2022-08-25 19:42:14,379] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 19:42:16,144] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-480/tokenizer_config.json[0m
[32m[2022-08-25 19:42:16,144] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-480/special_tokens_map.json[0m
[32m[2022-08-25 19:42:20,033] [    INFO][0m - loss: 0.09831322, learning_rate: 3.875e-06, global_step: 490, interval_runtime: 7.1589, interval_samples_per_second: 0.559, interval_steps_per_second: 1.397, epoch: 12.25[0m
[32m[2022-08-25 19:42:21,320] [    INFO][0m - loss: 0.17445548, learning_rate: 3.7500000000000005e-06, global_step: 500, interval_runtime: 1.2863, interval_samples_per_second: 3.11, interval_steps_per_second: 7.774, epoch: 12.5[0m
[32m[2022-08-25 19:42:22,674] [    INFO][0m - loss: 0.54092808, learning_rate: 3.625e-06, global_step: 510, interval_runtime: 1.3545, interval_samples_per_second: 2.953, interval_steps_per_second: 7.383, epoch: 12.75[0m
[32m[2022-08-25 19:42:23,744] [    INFO][0m - loss: 0.305406, learning_rate: 3.5e-06, global_step: 520, interval_runtime: 1.0704, interval_samples_per_second: 3.737, interval_steps_per_second: 9.343, epoch: 13.0[0m
[32m[2022-08-25 19:42:23,745] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 19:42:23,745] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-25 19:42:23,745] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 19:42:23,745] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 19:42:23,745] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-25 19:42:25,345] [    INFO][0m - eval_loss: 1.6598025560379028, eval_accuracy: 0.48427672955974843, eval_runtime: 1.5995, eval_samples_per_second: 99.408, eval_steps_per_second: 3.126, epoch: 13.0[0m
[32m[2022-08-25 19:42:25,345] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-520[0m
[32m[2022-08-25 19:42:25,346] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 19:42:27,309] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-520/tokenizer_config.json[0m
[32m[2022-08-25 19:42:27,309] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-520/special_tokens_map.json[0m
[32m[2022-08-25 19:42:31,264] [    INFO][0m - loss: 0.0521432, learning_rate: 3.3750000000000003e-06, global_step: 530, interval_runtime: 7.5194, interval_samples_per_second: 0.532, interval_steps_per_second: 1.33, epoch: 13.25[0m
[32m[2022-08-25 19:42:32,548] [    INFO][0m - loss: 0.66893654, learning_rate: 3.2500000000000002e-06, global_step: 540, interval_runtime: 1.2842, interval_samples_per_second: 3.115, interval_steps_per_second: 7.787, epoch: 13.5[0m
[32m[2022-08-25 19:42:33,885] [    INFO][0m - loss: 0.2119272, learning_rate: 3.125e-06, global_step: 550, interval_runtime: 1.3365, interval_samples_per_second: 2.993, interval_steps_per_second: 7.482, epoch: 13.75[0m
[32m[2022-08-25 19:42:34,965] [    INFO][0m - loss: 0.67252889, learning_rate: 3e-06, global_step: 560, interval_runtime: 1.0803, interval_samples_per_second: 3.703, interval_steps_per_second: 9.257, epoch: 14.0[0m
[32m[2022-08-25 19:42:34,966] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 19:42:34,966] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-25 19:42:34,966] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 19:42:34,966] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 19:42:34,966] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-25 19:42:36,809] [    INFO][0m - eval_loss: 1.7599072456359863, eval_accuracy: 0.4968553459119497, eval_runtime: 1.8431, eval_samples_per_second: 86.27, eval_steps_per_second: 2.713, epoch: 14.0[0m
[32m[2022-08-25 19:42:36,810] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-560[0m
[32m[2022-08-25 19:42:36,810] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 19:42:38,757] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-560/tokenizer_config.json[0m
[32m[2022-08-25 19:42:38,758] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-560/special_tokens_map.json[0m
[32m[2022-08-25 19:42:42,845] [    INFO][0m - loss: 0.10302368, learning_rate: 2.875e-06, global_step: 570, interval_runtime: 7.8804, interval_samples_per_second: 0.508, interval_steps_per_second: 1.269, epoch: 14.25[0m
[32m[2022-08-25 19:42:44,216] [    INFO][0m - loss: 0.20859892, learning_rate: 2.7500000000000004e-06, global_step: 580, interval_runtime: 1.3709, interval_samples_per_second: 2.918, interval_steps_per_second: 7.295, epoch: 14.5[0m
[32m[2022-08-25 19:42:45,574] [    INFO][0m - loss: 0.25984442, learning_rate: 2.6250000000000003e-06, global_step: 590, interval_runtime: 1.3576, interval_samples_per_second: 2.946, interval_steps_per_second: 7.366, epoch: 14.75[0m
[32m[2022-08-25 19:42:46,641] [    INFO][0m - loss: 0.17511014, learning_rate: 2.5e-06, global_step: 600, interval_runtime: 1.067, interval_samples_per_second: 3.749, interval_steps_per_second: 9.372, epoch: 15.0[0m
[32m[2022-08-25 19:42:46,641] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 19:42:46,641] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-25 19:42:46,642] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 19:42:46,642] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 19:42:46,642] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-25 19:42:48,371] [    INFO][0m - eval_loss: 1.886879801750183, eval_accuracy: 0.5094339622641509, eval_runtime: 1.7288, eval_samples_per_second: 91.972, eval_steps_per_second: 2.892, epoch: 15.0[0m
[32m[2022-08-25 19:42:48,371] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-600[0m
[32m[2022-08-25 19:42:48,371] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 19:42:50,356] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-600/tokenizer_config.json[0m
[32m[2022-08-25 19:42:50,356] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-600/special_tokens_map.json[0m
[32m[2022-08-25 19:42:54,469] [    INFO][0m - loss: 0.40607338, learning_rate: 2.375e-06, global_step: 610, interval_runtime: 7.8282, interval_samples_per_second: 0.511, interval_steps_per_second: 1.277, epoch: 15.25[0m
[32m[2022-08-25 19:42:55,922] [    INFO][0m - loss: 0.13969698, learning_rate: 2.25e-06, global_step: 620, interval_runtime: 1.453, interval_samples_per_second: 2.753, interval_steps_per_second: 6.882, epoch: 15.5[0m
[32m[2022-08-25 19:42:57,353] [    INFO][0m - loss: 0.1668013, learning_rate: 2.125e-06, global_step: 630, interval_runtime: 1.4309, interval_samples_per_second: 2.796, interval_steps_per_second: 6.989, epoch: 15.75[0m
[32m[2022-08-25 19:42:58,421] [    INFO][0m - loss: 0.30311589, learning_rate: 2.0000000000000003e-06, global_step: 640, interval_runtime: 1.0679, interval_samples_per_second: 3.746, interval_steps_per_second: 9.364, epoch: 16.0[0m
[32m[2022-08-25 19:42:58,422] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 19:42:58,422] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-25 19:42:58,422] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 19:42:58,422] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 19:42:58,422] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-25 19:43:00,273] [    INFO][0m - eval_loss: 1.9537088871002197, eval_accuracy: 0.5094339622641509, eval_runtime: 1.8502, eval_samples_per_second: 85.936, eval_steps_per_second: 2.702, epoch: 16.0[0m
[32m[2022-08-25 19:43:00,273] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-640[0m
[32m[2022-08-25 19:43:00,273] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 19:43:02,171] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-640/tokenizer_config.json[0m
[32m[2022-08-25 19:43:02,171] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-640/special_tokens_map.json[0m
[32m[2022-08-25 19:43:06,300] [    INFO][0m - loss: 0.28383026, learning_rate: 1.8750000000000003e-06, global_step: 650, interval_runtime: 7.8789, interval_samples_per_second: 0.508, interval_steps_per_second: 1.269, epoch: 16.25[0m
[32m[2022-08-25 19:43:07,789] [    INFO][0m - loss: 0.01708049, learning_rate: 1.75e-06, global_step: 660, interval_runtime: 1.4888, interval_samples_per_second: 2.687, interval_steps_per_second: 6.717, epoch: 16.5[0m
[32m[2022-08-25 19:43:09,352] [    INFO][0m - loss: 0.39632788, learning_rate: 1.6250000000000001e-06, global_step: 670, interval_runtime: 1.5637, interval_samples_per_second: 2.558, interval_steps_per_second: 6.395, epoch: 16.75[0m
[32m[2022-08-25 19:43:10,474] [    INFO][0m - loss: 0.0938212, learning_rate: 1.5e-06, global_step: 680, interval_runtime: 1.1217, interval_samples_per_second: 3.566, interval_steps_per_second: 8.915, epoch: 17.0[0m
[32m[2022-08-25 19:43:10,475] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 19:43:10,475] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-25 19:43:10,475] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 19:43:10,475] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 19:43:10,475] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-25 19:43:12,399] [    INFO][0m - eval_loss: 2.046992540359497, eval_accuracy: 0.5157232704402516, eval_runtime: 1.9241, eval_samples_per_second: 82.635, eval_steps_per_second: 2.599, epoch: 17.0[0m
[32m[2022-08-25 19:43:12,400] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-680[0m
[32m[2022-08-25 19:43:12,400] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 19:43:14,328] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-680/tokenizer_config.json[0m
[32m[2022-08-25 19:43:14,328] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-680/special_tokens_map.json[0m
[32m[2022-08-25 19:43:16,601] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-25 19:43:16,602] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-440 (score: 0.5471698113207547).[0m
[32m[2022-08-25 19:43:17,686] [    INFO][0m - train_runtime: 187.1424, train_samples_per_second: 17.099, train_steps_per_second: 4.275, train_loss: 0.4299123298815068, epoch: 17.0[0m
[32m[2022-08-25 19:43:17,687] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-25 19:43:17,687] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 19:43:21,488] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-25 19:43:21,488] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-25 19:43:21,490] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-25 19:43:21,490] [    INFO][0m -   epoch                    =       17.0[0m
[32m[2022-08-25 19:43:21,490] [    INFO][0m -   train_loss               =     0.4299[0m
[32m[2022-08-25 19:43:21,490] [    INFO][0m -   train_runtime            = 0:03:07.14[0m
[32m[2022-08-25 19:43:21,490] [    INFO][0m -   train_samples_per_second =     17.099[0m
[32m[2022-08-25 19:43:21,490] [    INFO][0m -   train_steps_per_second   =      4.275[0m
[32m[2022-08-25 19:43:21,494] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-25 19:43:21,494] [    INFO][0m -   Num examples = 290[0m
[32m[2022-08-25 19:43:21,494] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 19:43:21,494] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 19:43:21,494] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-08-25 19:43:25,198] [    INFO][0m - Predictions for cluewscf saved to ./fewclue_submit_examples.[0m
[32m[2022-08-25 19:45:21,923] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-08-25 19:45:21,923] [    INFO][0m - ============================================================[0m
[32m[2022-08-25 19:45:21,924] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-08-25 19:45:21,924] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-25 19:45:21,924] [    INFO][0m - do_save                       :True[0m
[32m[2022-08-25 19:45:21,924] [    INFO][0m - do_test                       :True[0m
[32m[2022-08-25 19:45:21,924] [    INFO][0m - early_stop_patience           :6[0m
[32m[2022-08-25 19:45:21,924] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-08-25 19:45:21,924] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-08-25 19:45:21,924] [    INFO][0m - [0m
[32m[2022-08-25 19:45:21,924] [    INFO][0m - ============================================================[0m
[32m[2022-08-25 19:45:21,924] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-08-25 19:45:21,924] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-25 19:45:21,924] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-08-25 19:45:21,925] [    INFO][0m - prompt                        :{'mask'}{'soft':'ÂêàÁêÜ„ÄÇ'}{'text':'text_a'}[0m
[32m[2022-08-25 19:45:21,925] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-08-25 19:45:21,925] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-08-25 19:45:21,925] [    INFO][0m - task_name                     :cluewsc[0m
[32m[2022-08-25 19:45:21,925] [    INFO][0m - [0m
[32m[2022-08-25 19:45:21,925] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0825 19:45:21.927093 68642 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0825 19:45:21.933665 68642 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-08-25 19:45:25,957] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-08-25 19:45:25,981] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-08-25 19:45:25,981] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[33m[2022-08-25 19:45:25,988] [ WARNING][0m - Encoder has already set as lstm, change `prompt_encoder` will reset parameters.[0m
[32m[2022-08-25 19:45:25,997] [    INFO][0m - Using template: [{'add_prefix_space': '', 'mask': None}, {'add_prefix_space': '', 'soft': 'Âêà'}, {'add_prefix_space': '', 'soft': 'ÁêÜ'}, {'add_prefix_space': '', 'soft': '„ÄÇ'}, {'add_prefix_space': '', 'text': 'text_a'}][0m
2022-08-25 19:45:26,002 INFO [download.py:119] unique_endpoints {''}
[32m[2022-08-25 19:45:26,100] [    INFO][0m - ============================================================[0m
[32m[2022-08-25 19:45:26,101] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-08-25 19:45:26,101] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-08-25 19:45:26,101] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-08-25 19:45:26,101] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-08-25 19:45:26,101] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-08-25 19:45:26,101] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-08-25 19:45:26,101] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-08-25 19:45:26,101] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-08-25 19:45:26,101] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-08-25 19:45:26,101] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-08-25 19:45:26,102] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-08-25 19:45:26,102] [    INFO][0m - device                        :gpu[0m
[32m[2022-08-25 19:45:26,102] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-08-25 19:45:26,102] [    INFO][0m - do_eval                       :True[0m
[32m[2022-08-25 19:45:26,102] [    INFO][0m - do_export                     :False[0m
[32m[2022-08-25 19:45:26,102] [    INFO][0m - do_predict                    :True[0m
[32m[2022-08-25 19:45:26,102] [    INFO][0m - do_train                      :True[0m
[32m[2022-08-25 19:45:26,102] [    INFO][0m - eval_batch_size               :32[0m
[32m[2022-08-25 19:45:26,102] [    INFO][0m - eval_steps                    :10[0m
[32m[2022-08-25 19:45:26,102] [    INFO][0m - evaluation_strategy           :IntervalStrategy.EPOCH[0m
[32m[2022-08-25 19:45:26,102] [    INFO][0m - first_max_length              :None[0m
[32m[2022-08-25 19:45:26,102] [    INFO][0m - fp16                          :False[0m
[32m[2022-08-25 19:45:26,102] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-08-25 19:45:26,103] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-08-25 19:45:26,103] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-08-25 19:45:26,103] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-08-25 19:45:26,103] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-08-25 19:45:26,103] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-08-25 19:45:26,103] [    INFO][0m - label_names                   :None[0m
[32m[2022-08-25 19:45:26,103] [    INFO][0m - learning_rate                 :1e-05[0m
[32m[2022-08-25 19:45:26,103] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-08-25 19:45:26,103] [    INFO][0m - local_process_index           :0[0m
[32m[2022-08-25 19:45:26,103] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-08-25 19:45:26,103] [    INFO][0m - log_level                     :-1[0m
[32m[2022-08-25 19:45:26,103] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-08-25 19:45:26,103] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-08-25 19:45:26,103] [    INFO][0m - logging_dir                   :./checkpoints/runs/Aug25_19-45-21_instance-3bwob41y-01[0m
[32m[2022-08-25 19:45:26,104] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-08-25 19:45:26,104] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-08-25 19:45:26,104] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-08-25 19:45:26,104] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-08-25 19:45:26,104] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-08-25 19:45:26,104] [    INFO][0m - max_seq_length                :128[0m
[32m[2022-08-25 19:45:26,104] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-08-25 19:45:26,104] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-08-25 19:45:26,104] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-08-25 19:45:26,104] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-08-25 19:45:26,104] [    INFO][0m - num_train_epochs              :20.0[0m
[32m[2022-08-25 19:45:26,104] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-08-25 19:45:26,104] [    INFO][0m - other_max_length              :None[0m
[32m[2022-08-25 19:45:26,104] [    INFO][0m - output_dir                    :./checkpoints/[0m
[32m[2022-08-25 19:45:26,105] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-08-25 19:45:26,105] [    INFO][0m - past_index                    :-1[0m
[32m[2022-08-25 19:45:26,105] [    INFO][0m - per_device_eval_batch_size    :32[0m
[32m[2022-08-25 19:45:26,105] [    INFO][0m - per_device_train_batch_size   :4[0m
[32m[2022-08-25 19:45:26,105] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-08-25 19:45:26,105] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-08-25 19:45:26,105] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-08-25 19:45:26,105] [    INFO][0m - ppt_learning_rate             :0.0001[0m
[32m[2022-08-25 19:45:26,105] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-08-25 19:45:26,105] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-08-25 19:45:26,105] [    INFO][0m - process_index                 :0[0m
[32m[2022-08-25 19:45:26,105] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-08-25 19:45:26,105] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-08-25 19:45:26,105] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-08-25 19:45:26,106] [    INFO][0m - run_name                      :./checkpoints/[0m
[32m[2022-08-25 19:45:26,106] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-08-25 19:45:26,106] [    INFO][0m - save_steps                    :500[0m
[32m[2022-08-25 19:45:26,106] [    INFO][0m - save_strategy                 :IntervalStrategy.EPOCH[0m
[32m[2022-08-25 19:45:26,106] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-08-25 19:45:26,106] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-08-25 19:45:26,106] [    INFO][0m - seed                          :42[0m
[32m[2022-08-25 19:45:26,106] [    INFO][0m - should_log                    :True[0m
[32m[2022-08-25 19:45:26,106] [    INFO][0m - should_save                   :True[0m
[32m[2022-08-25 19:45:26,106] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-08-25 19:45:26,106] [    INFO][0m - train_batch_size              :4[0m
[32m[2022-08-25 19:45:26,106] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-08-25 19:45:26,106] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-08-25 19:45:26,107] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-08-25 19:45:26,107] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-08-25 19:45:26,107] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-08-25 19:45:26,107] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-08-25 19:45:26,107] [    INFO][0m - world_size                    :1[0m
[32m[2022-08-25 19:45:26,107] [    INFO][0m - [0m
[32m[2022-08-25 19:45:26,109] [    INFO][0m - ***** Running training *****[0m
[32m[2022-08-25 19:45:26,109] [    INFO][0m -   Num examples = 160[0m
[32m[2022-08-25 19:45:26,109] [    INFO][0m -   Num Epochs = 20[0m
[32m[2022-08-25 19:45:26,109] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2022-08-25 19:45:26,109] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 4[0m
[32m[2022-08-25 19:45:26,109] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-08-25 19:45:26,109] [    INFO][0m -   Total optimization steps = 800.0[0m
[32m[2022-08-25 19:45:26,109] [    INFO][0m -   Total num train samples = 3200[0m
[32m[2022-08-25 19:45:28,321] [    INFO][0m - loss: 0.80145082, learning_rate: 9.875000000000001e-06, global_step: 10, interval_runtime: 2.2115, interval_samples_per_second: 1.809, interval_steps_per_second: 4.522, epoch: 0.25[0m
[32m[2022-08-25 19:45:29,045] [    INFO][0m - loss: 0.75986662, learning_rate: 9.75e-06, global_step: 20, interval_runtime: 0.724, interval_samples_per_second: 5.525, interval_steps_per_second: 13.811, epoch: 0.5[0m
[32m[2022-08-25 19:45:29,843] [    INFO][0m - loss: 0.75108891, learning_rate: 9.625e-06, global_step: 30, interval_runtime: 0.7973, interval_samples_per_second: 5.017, interval_steps_per_second: 12.542, epoch: 0.75[0m
[32m[2022-08-25 19:45:30,552] [    INFO][0m - loss: 0.76337862, learning_rate: 9.5e-06, global_step: 40, interval_runtime: 0.7091, interval_samples_per_second: 5.641, interval_steps_per_second: 14.103, epoch: 1.0[0m
[32m[2022-08-25 19:45:30,552] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 19:45:30,553] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-25 19:45:30,553] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 19:45:30,553] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 19:45:30,553] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-25 19:45:31,187] [    INFO][0m - eval_loss: 0.7249712944030762, eval_accuracy: 0.5408805031446541, eval_runtime: 0.6341, eval_samples_per_second: 250.766, eval_steps_per_second: 7.886, epoch: 1.0[0m
[32m[2022-08-25 19:45:31,188] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-40[0m
[32m[2022-08-25 19:45:31,188] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 19:45:34,465] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-40/tokenizer_config.json[0m
[32m[2022-08-25 19:45:34,465] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-40/special_tokens_map.json[0m
[32m[2022-08-25 19:45:39,549] [    INFO][0m - loss: 0.65267243, learning_rate: 9.375000000000001e-06, global_step: 50, interval_runtime: 8.9971, interval_samples_per_second: 0.445, interval_steps_per_second: 1.111, epoch: 1.25[0m
[32m[2022-08-25 19:45:40,364] [    INFO][0m - loss: 0.64230609, learning_rate: 9.250000000000001e-06, global_step: 60, interval_runtime: 0.8149, interval_samples_per_second: 4.909, interval_steps_per_second: 12.272, epoch: 1.5[0m
[32m[2022-08-25 19:45:41,292] [    INFO][0m - loss: 0.68602586, learning_rate: 9.125e-06, global_step: 70, interval_runtime: 0.928, interval_samples_per_second: 4.311, interval_steps_per_second: 10.776, epoch: 1.75[0m
[32m[2022-08-25 19:45:42,110] [    INFO][0m - loss: 0.79587517, learning_rate: 9e-06, global_step: 80, interval_runtime: 0.8173, interval_samples_per_second: 4.894, interval_steps_per_second: 12.235, epoch: 2.0[0m
[32m[2022-08-25 19:45:42,111] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 19:45:42,111] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-25 19:45:42,111] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 19:45:42,111] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 19:45:42,111] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-25 19:45:42,873] [    INFO][0m - eval_loss: 0.7996474504470825, eval_accuracy: 0.4968553459119497, eval_runtime: 0.7621, eval_samples_per_second: 208.627, eval_steps_per_second: 6.561, epoch: 2.0[0m
[32m[2022-08-25 19:45:42,874] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-80[0m
[32m[2022-08-25 19:45:42,874] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 19:45:46,073] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-80/tokenizer_config.json[0m
[32m[2022-08-25 19:45:46,073] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-80/special_tokens_map.json[0m
[32m[2022-08-25 19:45:51,109] [    INFO][0m - loss: 0.69962502, learning_rate: 8.875e-06, global_step: 90, interval_runtime: 9.0001, interval_samples_per_second: 0.444, interval_steps_per_second: 1.111, epoch: 2.25[0m
[32m[2022-08-25 19:45:51,996] [    INFO][0m - loss: 0.6495502, learning_rate: 8.750000000000001e-06, global_step: 100, interval_runtime: 0.887, interval_samples_per_second: 4.51, interval_steps_per_second: 11.274, epoch: 2.5[0m
[32m[2022-08-25 19:45:52,827] [    INFO][0m - loss: 0.61643896, learning_rate: 8.625000000000001e-06, global_step: 110, interval_runtime: 0.8304, interval_samples_per_second: 4.817, interval_steps_per_second: 12.042, epoch: 2.75[0m
[32m[2022-08-25 19:45:53,633] [    INFO][0m - loss: 0.87402344, learning_rate: 8.5e-06, global_step: 120, interval_runtime: 0.8055, interval_samples_per_second: 4.966, interval_steps_per_second: 12.415, epoch: 3.0[0m
[32m[2022-08-25 19:45:53,634] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 19:45:53,634] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-25 19:45:53,634] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 19:45:53,634] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 19:45:53,634] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-25 19:45:54,466] [    INFO][0m - eval_loss: 0.7094400525093079, eval_accuracy: 0.4968553459119497, eval_runtime: 0.8311, eval_samples_per_second: 191.305, eval_steps_per_second: 6.016, epoch: 3.0[0m
[32m[2022-08-25 19:45:54,466] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-120[0m
[32m[2022-08-25 19:45:54,466] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 19:45:57,636] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-120/tokenizer_config.json[0m
[32m[2022-08-25 19:45:57,636] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-120/special_tokens_map.json[0m
[32m[2022-08-25 19:46:02,583] [    INFO][0m - loss: 0.62985854, learning_rate: 8.375e-06, global_step: 130, interval_runtime: 8.95, interval_samples_per_second: 0.447, interval_steps_per_second: 1.117, epoch: 3.25[0m
[32m[2022-08-25 19:46:03,533] [    INFO][0m - loss: 0.62187867, learning_rate: 8.25e-06, global_step: 140, interval_runtime: 0.9505, interval_samples_per_second: 4.208, interval_steps_per_second: 10.521, epoch: 3.5[0m
[32m[2022-08-25 19:46:04,421] [    INFO][0m - loss: 0.75757027, learning_rate: 8.125000000000001e-06, global_step: 150, interval_runtime: 0.8879, interval_samples_per_second: 4.505, interval_steps_per_second: 11.263, epoch: 3.75[0m
[32m[2022-08-25 19:46:05,321] [    INFO][0m - loss: 0.6223546, learning_rate: 8.000000000000001e-06, global_step: 160, interval_runtime: 0.9003, interval_samples_per_second: 4.443, interval_steps_per_second: 11.108, epoch: 4.0[0m
[32m[2022-08-25 19:46:05,322] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 19:46:05,322] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-25 19:46:05,322] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 19:46:05,322] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 19:46:05,322] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-25 19:46:06,183] [    INFO][0m - eval_loss: 0.7571119070053101, eval_accuracy: 0.4968553459119497, eval_runtime: 0.8605, eval_samples_per_second: 184.772, eval_steps_per_second: 5.81, epoch: 4.0[0m
[32m[2022-08-25 19:46:06,183] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-160[0m
[32m[2022-08-25 19:46:06,184] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 19:46:09,648] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-160/tokenizer_config.json[0m
[32m[2022-08-25 19:46:09,649] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-160/special_tokens_map.json[0m
[32m[2022-08-25 19:46:14,470] [    INFO][0m - loss: 0.62334652, learning_rate: 7.875e-06, global_step: 170, interval_runtime: 9.1494, interval_samples_per_second: 0.437, interval_steps_per_second: 1.093, epoch: 4.25[0m
[32m[2022-08-25 19:46:15,388] [    INFO][0m - loss: 0.69935765, learning_rate: 7.75e-06, global_step: 180, interval_runtime: 0.9177, interval_samples_per_second: 4.358, interval_steps_per_second: 10.896, epoch: 4.5[0m
[32m[2022-08-25 19:46:16,421] [    INFO][0m - loss: 0.53840952, learning_rate: 7.625e-06, global_step: 190, interval_runtime: 1.0328, interval_samples_per_second: 3.873, interval_steps_per_second: 9.683, epoch: 4.75[0m
[32m[2022-08-25 19:46:17,248] [    INFO][0m - loss: 0.60408087, learning_rate: 7.500000000000001e-06, global_step: 200, interval_runtime: 0.8276, interval_samples_per_second: 4.833, interval_steps_per_second: 12.084, epoch: 5.0[0m
[32m[2022-08-25 19:46:17,249] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 19:46:17,249] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-25 19:46:17,249] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 19:46:17,249] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 19:46:17,249] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-25 19:46:18,221] [    INFO][0m - eval_loss: 0.745248019695282, eval_accuracy: 0.5031446540880503, eval_runtime: 0.9711, eval_samples_per_second: 163.733, eval_steps_per_second: 5.149, epoch: 5.0[0m
[32m[2022-08-25 19:46:18,221] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-200[0m
[32m[2022-08-25 19:46:18,221] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 19:46:21,344] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-200/tokenizer_config.json[0m
[32m[2022-08-25 19:46:21,344] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-200/special_tokens_map.json[0m
[32m[2022-08-25 19:46:26,689] [    INFO][0m - loss: 0.49245443, learning_rate: 7.375000000000001e-06, global_step: 210, interval_runtime: 9.4408, interval_samples_per_second: 0.424, interval_steps_per_second: 1.059, epoch: 5.25[0m
[32m[2022-08-25 19:46:27,734] [    INFO][0m - loss: 0.73785176, learning_rate: 7.25e-06, global_step: 220, interval_runtime: 1.0451, interval_samples_per_second: 3.827, interval_steps_per_second: 9.569, epoch: 5.5[0m
[32m[2022-08-25 19:46:28,766] [    INFO][0m - loss: 0.54542866, learning_rate: 7.125e-06, global_step: 230, interval_runtime: 1.0314, interval_samples_per_second: 3.878, interval_steps_per_second: 9.695, epoch: 5.75[0m
[32m[2022-08-25 19:46:29,695] [    INFO][0m - loss: 0.60484056, learning_rate: 7e-06, global_step: 240, interval_runtime: 0.929, interval_samples_per_second: 4.306, interval_steps_per_second: 10.764, epoch: 6.0[0m
[32m[2022-08-25 19:46:29,695] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 19:46:29,695] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-25 19:46:29,696] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 19:46:29,696] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 19:46:29,696] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-25 19:46:30,714] [    INFO][0m - eval_loss: 0.7938120365142822, eval_accuracy: 0.5157232704402516, eval_runtime: 1.0182, eval_samples_per_second: 156.158, eval_steps_per_second: 4.911, epoch: 6.0[0m
[32m[2022-08-25 19:46:30,714] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-240[0m
[32m[2022-08-25 19:46:30,715] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 19:46:34,144] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-240/tokenizer_config.json[0m
[32m[2022-08-25 19:46:34,144] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-240/special_tokens_map.json[0m
[32m[2022-08-25 19:46:39,416] [    INFO][0m - loss: 0.49611044, learning_rate: 6.875e-06, global_step: 250, interval_runtime: 9.721, interval_samples_per_second: 0.411, interval_steps_per_second: 1.029, epoch: 6.25[0m
[32m[2022-08-25 19:46:40,491] [    INFO][0m - loss: 0.45461736, learning_rate: 6.750000000000001e-06, global_step: 260, interval_runtime: 1.0749, interval_samples_per_second: 3.721, interval_steps_per_second: 9.303, epoch: 6.5[0m
[32m[2022-08-25 19:46:41,542] [    INFO][0m - loss: 0.53197975, learning_rate: 6.625e-06, global_step: 270, interval_runtime: 1.0513, interval_samples_per_second: 3.805, interval_steps_per_second: 9.512, epoch: 6.75[0m
[32m[2022-08-25 19:46:42,471] [    INFO][0m - loss: 0.51503572, learning_rate: 6.5000000000000004e-06, global_step: 280, interval_runtime: 0.9294, interval_samples_per_second: 4.304, interval_steps_per_second: 10.759, epoch: 7.0[0m
[32m[2022-08-25 19:46:42,472] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-08-25 19:46:42,472] [    INFO][0m -   Num examples = 159[0m
[32m[2022-08-25 19:46:42,472] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 19:46:42,472] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 19:46:42,472] [    INFO][0m -   Total prediction steps = 5[0m
[32m[2022-08-25 19:46:43,569] [    INFO][0m - eval_loss: 0.7799866795539856, eval_accuracy: 0.5031446540880503, eval_runtime: 1.0962, eval_samples_per_second: 145.047, eval_steps_per_second: 4.561, epoch: 7.0[0m
[32m[2022-08-25 19:46:43,569] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-280[0m
[32m[2022-08-25 19:46:43,569] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 19:46:46,668] [    INFO][0m - tokenizer config file saved in ./checkpoints/checkpoint-280/tokenizer_config.json[0m
[32m[2022-08-25 19:46:46,669] [    INFO][0m - Special tokens file saved in ./checkpoints/checkpoint-280/special_tokens_map.json[0m
[32m[2022-08-25 19:46:50,601] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-08-25 19:46:50,601] [    INFO][0m - Loading best model from ./checkpoints/checkpoint-40 (score: 0.5408805031446541).[0m
[32m[2022-08-25 19:46:51,871] [    INFO][0m - train_runtime: 85.7607, train_samples_per_second: 37.313, train_steps_per_second: 9.328, train_loss: 0.6488384808812823, epoch: 7.0[0m
[32m[2022-08-25 19:46:51,872] [    INFO][0m - Saving model checkpoint to ./checkpoints/[0m
[32m[2022-08-25 19:46:51,873] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-08-25 19:46:55,137] [    INFO][0m - tokenizer config file saved in ./checkpoints/tokenizer_config.json[0m
[32m[2022-08-25 19:46:55,137] [    INFO][0m - Special tokens file saved in ./checkpoints/special_tokens_map.json[0m
[32m[2022-08-25 19:46:55,139] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-08-25 19:46:55,139] [    INFO][0m -   epoch                    =        7.0[0m
[32m[2022-08-25 19:46:55,139] [    INFO][0m -   train_loss               =     0.6488[0m
[32m[2022-08-25 19:46:55,139] [    INFO][0m -   train_runtime            = 0:01:25.76[0m
[32m[2022-08-25 19:46:55,139] [    INFO][0m -   train_samples_per_second =     37.313[0m
[32m[2022-08-25 19:46:55,139] [    INFO][0m -   train_steps_per_second   =      9.328[0m
[32m[2022-08-25 19:46:55,142] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-25 19:46:55,142] [    INFO][0m -   Num examples = 976[0m
[32m[2022-08-25 19:46:55,142] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 19:46:55,142] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 19:46:55,142] [    INFO][0m -   Total prediction steps = 31[0m
[32m[2022-08-25 19:47:01,970] [    INFO][0m - ***** test metrics *****[0m
[32m[2022-08-25 19:47:01,971] [    INFO][0m -   test_accuracy           =     0.5051[0m
[32m[2022-08-25 19:47:01,971] [    INFO][0m -   test_loss               =     0.7281[0m
[32m[2022-08-25 19:47:01,971] [    INFO][0m -   test_runtime            = 0:00:06.82[0m
[32m[2022-08-25 19:47:01,971] [    INFO][0m -   test_samples_per_second =    142.937[0m
[32m[2022-08-25 19:47:01,971] [    INFO][0m -   test_steps_per_second   =       4.54[0m
[32m[2022-08-25 19:47:01,971] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-08-25 19:47:01,971] [    INFO][0m -   Num examples = 290[0m
[32m[2022-08-25 19:47:01,972] [    INFO][0m -   Pre device batch size = 32[0m
[32m[2022-08-25 19:47:01,972] [    INFO][0m -   Total Batch size = 32[0m
[32m[2022-08-25 19:47:01,972] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2022-08-25 19:47:04,341] [    INFO][0m - Predictions for cluewscf saved to ./fewclue_submit_examples.[0m
