[33m[2022-09-05 11:54:45,771] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2022-09-05 11:54:45,771] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2022-09-05 11:54:45,772] [    INFO][0m - ============================================================[0m
[32m[2022-09-05 11:54:45,772] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2022-09-05 11:54:45,772] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-05 11:54:45,772] [    INFO][0m - do_save                       :True[0m
[32m[2022-09-05 11:54:45,772] [    INFO][0m - do_test                       :True[0m
[32m[2022-09-05 11:54:45,772] [    INFO][0m - early_stop_patience           :4[0m
[32m[2022-09-05 11:54:45,772] [    INFO][0m - export_type                   :paddle[0m
[32m[2022-09-05 11:54:45,773] [    INFO][0m - model_name_or_path            :ernie-3.0-base-zh[0m
[32m[2022-09-05 11:54:45,773] [    INFO][0m - [0m
[32m[2022-09-05 11:54:45,773] [    INFO][0m - ============================================================[0m
[32m[2022-09-05 11:54:45,773] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2022-09-05 11:54:45,773] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-05 11:54:45,773] [    INFO][0m - encoder_hidden_size           :200[0m
[32m[2022-09-05 11:54:45,773] [    INFO][0m - prompt                        :[0m
[32m[2022-09-05 11:54:45,773] [    INFO][0m - soft_encoder                  :lstm[0m
[32m[2022-09-05 11:54:45,774] [    INFO][0m - split_id                      :few_all[0m
[32m[2022-09-05 11:54:45,774] [    INFO][0m - task_name                     :chid[0m
[32m[2022-09-05 11:54:45,774] [    INFO][0m - [0m
[32m[2022-09-05 11:54:45,774] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams[0m
W0905 11:54:45.776042 73438 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0905 11:54:45.781373 73438 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[32m[2022-09-05 11:54:51,841] [    INFO][0m - Already cached /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2022-09-05 11:54:51,865] [    INFO][0m - tokenizer config file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2022-09-05 11:54:51,866] [    INFO][0m - Special tokens file saved in /ssd2/wanghuijuan03/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[32m[2022-09-05 11:54:51,867] [    INFO][0m - Using template: [{'add_prefix_space': '', 'text': 'text_a'}, {'add_prefix_space': '', 'hard': 'Êñá‰∏≠ÊàêËØ≠ÊòØÂê¶Ê≠£Á°ÆÔºü'}, {'add_prefix_space': '', 'mask': None}][0m
2022-09-05 11:54:51,870 INFO [download.py:119] unique_endpoints {''}
[32m[2022-09-05 11:54:52,192] [    INFO][0m - ============================================================[0m
[32m[2022-09-05 11:54:52,192] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2022-09-05 11:54:52,192] [    INFO][0m - paddle commit id              :65f388690048d0965ec7d3b43fb6ee9d8c6dee7c[0m
[32m[2022-09-05 11:54:52,192] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2022-09-05 11:54:52,192] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2022-09-05 11:54:52,192] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2022-09-05 11:54:52,192] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2022-09-05 11:54:52,193] [    INFO][0m - alpha_rdrop                   :5.0[0m
[32m[2022-09-05 11:54:52,193] [    INFO][0m - alpha_rgl                     :0.5[0m
[32m[2022-09-05 11:54:52,193] [    INFO][0m - current_device                :gpu:0[0m
[32m[2022-09-05 11:54:52,193] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2022-09-05 11:54:52,193] [    INFO][0m - dataloader_num_workers        :0[0m
[32m[2022-09-05 11:54:52,193] [    INFO][0m - device                        :gpu[0m
[32m[2022-09-05 11:54:52,193] [    INFO][0m - disable_tqdm                  :True[0m
[32m[2022-09-05 11:54:52,193] [    INFO][0m - do_eval                       :True[0m
[32m[2022-09-05 11:54:52,193] [    INFO][0m - do_export                     :False[0m
[32m[2022-09-05 11:54:52,193] [    INFO][0m - do_predict                    :False[0m
[32m[2022-09-05 11:54:52,193] [    INFO][0m - do_train                      :True[0m
[32m[2022-09-05 11:54:52,193] [    INFO][0m - eval_batch_size               :8[0m
[32m[2022-09-05 11:54:52,194] [    INFO][0m - eval_steps                    :100[0m
[32m[2022-09-05 11:54:52,194] [    INFO][0m - evaluation_strategy           :IntervalStrategy.STEPS[0m
[32m[2022-09-05 11:54:52,194] [    INFO][0m - first_max_length              :None[0m
[32m[2022-09-05 11:54:52,194] [    INFO][0m - fp16                          :False[0m
[32m[2022-09-05 11:54:52,194] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2022-09-05 11:54:52,194] [    INFO][0m - freeze_dropout                :False[0m
[32m[2022-09-05 11:54:52,194] [    INFO][0m - freeze_plm                    :False[0m
[32m[2022-09-05 11:54:52,194] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2022-09-05 11:54:52,194] [    INFO][0m - greater_is_better             :True[0m
[32m[2022-09-05 11:54:52,194] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2022-09-05 11:54:52,194] [    INFO][0m - label_names                   :None[0m
[32m[2022-09-05 11:54:52,194] [    INFO][0m - learning_rate                 :3e-05[0m
[32m[2022-09-05 11:54:52,194] [    INFO][0m - load_best_model_at_end        :True[0m
[32m[2022-09-05 11:54:52,194] [    INFO][0m - local_process_index           :0[0m
[32m[2022-09-05 11:54:52,195] [    INFO][0m - local_rank                    :-1[0m
[32m[2022-09-05 11:54:52,195] [    INFO][0m - log_level                     :-1[0m
[32m[2022-09-05 11:54:52,195] [    INFO][0m - log_level_replica             :-1[0m
[32m[2022-09-05 11:54:52,195] [    INFO][0m - log_on_each_node              :True[0m
[32m[2022-09-05 11:54:52,195] [    INFO][0m - logging_dir                   :./checkpoints_chid/runs/Sep05_11-54-45_instance-3bwob41y-01[0m
[32m[2022-09-05 11:54:52,195] [    INFO][0m - logging_first_step            :False[0m
[32m[2022-09-05 11:54:52,195] [    INFO][0m - logging_steps                 :10[0m
[32m[2022-09-05 11:54:52,195] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2022-09-05 11:54:52,195] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2022-09-05 11:54:52,195] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2022-09-05 11:54:52,195] [    INFO][0m - max_seq_length                :256[0m
[32m[2022-09-05 11:54:52,195] [    INFO][0m - max_steps                     :-1[0m
[32m[2022-09-05 11:54:52,195] [    INFO][0m - metric_for_best_model         :accuracy[0m
[32m[2022-09-05 11:54:52,195] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2022-09-05 11:54:52,196] [    INFO][0m - no_cuda                       :False[0m
[32m[2022-09-05 11:54:52,196] [    INFO][0m - num_train_epochs              :20.0[0m
[32m[2022-09-05 11:54:52,196] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2022-09-05 11:54:52,196] [    INFO][0m - other_max_length              :None[0m
[32m[2022-09-05 11:54:52,196] [    INFO][0m - output_dir                    :./checkpoints_chid/[0m
[32m[2022-09-05 11:54:52,196] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2022-09-05 11:54:52,196] [    INFO][0m - past_index                    :-1[0m
[32m[2022-09-05 11:54:52,196] [    INFO][0m - per_device_eval_batch_size    :8[0m
[32m[2022-09-05 11:54:52,196] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2022-09-05 11:54:52,196] [    INFO][0m - ppt_adam_beta1                :0.9[0m
[32m[2022-09-05 11:54:52,196] [    INFO][0m - ppt_adam_beta2                :0.999[0m
[32m[2022-09-05 11:54:52,196] [    INFO][0m - ppt_adam_epsilon              :1e-08[0m
[32m[2022-09-05 11:54:52,196] [    INFO][0m - ppt_learning_rate             :0.0003[0m
[32m[2022-09-05 11:54:52,197] [    INFO][0m - ppt_weight_decay              :0.0[0m
[32m[2022-09-05 11:54:52,197] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2022-09-05 11:54:52,197] [    INFO][0m - process_index                 :0[0m
[32m[2022-09-05 11:54:52,197] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2022-09-05 11:54:52,197] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2022-09-05 11:54:52,197] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2022-09-05 11:54:52,197] [    INFO][0m - run_name                      :./checkpoints_chid/[0m
[32m[2022-09-05 11:54:52,197] [    INFO][0m - save_on_each_node             :False[0m
[32m[2022-09-05 11:54:52,197] [    INFO][0m - save_steps                    :100[0m
[32m[2022-09-05 11:54:52,197] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2022-09-05 11:54:52,197] [    INFO][0m - save_total_limit              :None[0m
[32m[2022-09-05 11:54:52,197] [    INFO][0m - scale_loss                    :32768[0m
[32m[2022-09-05 11:54:52,197] [    INFO][0m - seed                          :42[0m
[32m[2022-09-05 11:54:52,197] [    INFO][0m - should_log                    :True[0m
[32m[2022-09-05 11:54:52,198] [    INFO][0m - should_save                   :True[0m
[32m[2022-09-05 11:54:52,198] [    INFO][0m - task_type                     :multi-class[0m
[32m[2022-09-05 11:54:52,198] [    INFO][0m - train_batch_size              :8[0m
[32m[2022-09-05 11:54:52,198] [    INFO][0m - truncate_mode                 :tail[0m
[32m[2022-09-05 11:54:52,198] [    INFO][0m - use_rdrop                     :False[0m
[32m[2022-09-05 11:54:52,198] [    INFO][0m - use_rgl                       :False[0m
[32m[2022-09-05 11:54:52,198] [    INFO][0m - warmup_ratio                  :0.0[0m
[32m[2022-09-05 11:54:52,198] [    INFO][0m - warmup_steps                  :0[0m
[32m[2022-09-05 11:54:52,198] [    INFO][0m - weight_decay                  :0.0[0m
[32m[2022-09-05 11:54:52,198] [    INFO][0m - world_size                    :1[0m
[32m[2022-09-05 11:54:52,198] [    INFO][0m - [0m
[32m[2022-09-05 11:54:52,200] [    INFO][0m - ***** Running training *****[0m
[32m[2022-09-05 11:54:52,200] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-05 11:54:52,200] [    INFO][0m -   Num Epochs = 20[0m
[32m[2022-09-05 11:54:52,200] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2022-09-05 11:54:52,200] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2022-09-05 11:54:52,200] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2022-09-05 11:54:52,200] [    INFO][0m -   Total optimization steps = 3540.0[0m
[32m[2022-09-05 11:54:52,200] [    INFO][0m -   Total num train samples = 28280[0m
[32m[2022-09-05 11:54:54,727] [    INFO][0m - loss: 3.60566826, learning_rate: 2.9915254237288134e-05, global_step: 10, interval_runtime: 2.5254, interval_samples_per_second: 3.168, interval_steps_per_second: 3.96, epoch: 0.0565[0m
[32m[2022-09-05 11:54:56,215] [    INFO][0m - loss: 0.56586781, learning_rate: 2.9830508474576274e-05, global_step: 20, interval_runtime: 1.4883, interval_samples_per_second: 5.375, interval_steps_per_second: 6.719, epoch: 0.113[0m
[32m[2022-09-05 11:54:57,699] [    INFO][0m - loss: 0.64883313, learning_rate: 2.9745762711864407e-05, global_step: 30, interval_runtime: 1.4838, interval_samples_per_second: 5.392, interval_steps_per_second: 6.74, epoch: 0.1695[0m
[32m[2022-09-05 11:54:59,186] [    INFO][0m - loss: 0.62902775, learning_rate: 2.9661016949152544e-05, global_step: 40, interval_runtime: 1.4866, interval_samples_per_second: 5.381, interval_steps_per_second: 6.727, epoch: 0.226[0m
[32m[2022-09-05 11:55:00,671] [    INFO][0m - loss: 0.42835212, learning_rate: 2.9576271186440677e-05, global_step: 50, interval_runtime: 1.4855, interval_samples_per_second: 5.385, interval_steps_per_second: 6.732, epoch: 0.2825[0m
[32m[2022-09-05 11:55:02,160] [    INFO][0m - loss: 0.29386015, learning_rate: 2.9491525423728817e-05, global_step: 60, interval_runtime: 1.4892, interval_samples_per_second: 5.372, interval_steps_per_second: 6.715, epoch: 0.339[0m
[32m[2022-09-05 11:55:03,643] [    INFO][0m - loss: 0.87277441, learning_rate: 2.940677966101695e-05, global_step: 70, interval_runtime: 1.4825, interval_samples_per_second: 5.396, interval_steps_per_second: 6.746, epoch: 0.3955[0m
[32m[2022-09-05 11:55:05,127] [    INFO][0m - loss: 0.58620863, learning_rate: 2.9322033898305087e-05, global_step: 80, interval_runtime: 1.483, interval_samples_per_second: 5.394, interval_steps_per_second: 6.743, epoch: 0.452[0m
[32m[2022-09-05 11:55:06,617] [    INFO][0m - loss: 0.51718664, learning_rate: 2.923728813559322e-05, global_step: 90, interval_runtime: 1.4909, interval_samples_per_second: 5.366, interval_steps_per_second: 6.707, epoch: 0.5085[0m
[32m[2022-09-05 11:55:08,102] [    INFO][0m - loss: 0.48686686, learning_rate: 2.9152542372881356e-05, global_step: 100, interval_runtime: 1.482, interval_samples_per_second: 5.398, interval_steps_per_second: 6.747, epoch: 0.565[0m
[32m[2022-09-05 11:55:08,103] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 11:55:08,103] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-05 11:55:08,103] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 11:55:08,104] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 11:55:08,104] [    INFO][0m -   Total prediction steps = 177[0m
[32m[2022-09-05 11:55:19,242] [    INFO][0m - eval_loss: 0.41538676619529724, eval_accuracy: 0.12871287128712872, eval_runtime: 11.137, eval_samples_per_second: 126.964, eval_steps_per_second: 15.893, epoch: 0.565[0m
[32m[2022-09-05 11:55:19,269] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-100[0m
[32m[2022-09-05 11:55:19,269] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 11:55:22,669] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-100/tokenizer_config.json[0m
[32m[2022-09-05 11:55:22,669] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-100/special_tokens_map.json[0m
[32m[2022-09-05 11:55:29,528] [    INFO][0m - loss: 0.47409873, learning_rate: 2.9067796610169493e-05, global_step: 110, interval_runtime: 21.4294, interval_samples_per_second: 0.373, interval_steps_per_second: 0.467, epoch: 0.6215[0m
[32m[2022-09-05 11:55:31,015] [    INFO][0m - loss: 0.4202745, learning_rate: 2.8983050847457626e-05, global_step: 120, interval_runtime: 1.4868, interval_samples_per_second: 5.381, interval_steps_per_second: 6.726, epoch: 0.678[0m
[32m[2022-09-05 11:55:32,505] [    INFO][0m - loss: 0.49582367, learning_rate: 2.8898305084745763e-05, global_step: 130, interval_runtime: 1.4903, interval_samples_per_second: 5.368, interval_steps_per_second: 6.71, epoch: 0.7345[0m
[32m[2022-09-05 11:55:33,994] [    INFO][0m - loss: 0.51787786, learning_rate: 2.88135593220339e-05, global_step: 140, interval_runtime: 1.4891, interval_samples_per_second: 5.372, interval_steps_per_second: 6.716, epoch: 0.791[0m
[32m[2022-09-05 11:55:35,478] [    INFO][0m - loss: 0.51845293, learning_rate: 2.8728813559322036e-05, global_step: 150, interval_runtime: 1.484, interval_samples_per_second: 5.391, interval_steps_per_second: 6.739, epoch: 0.8475[0m
[32m[2022-09-05 11:55:36,965] [    INFO][0m - loss: 0.53921642, learning_rate: 2.864406779661017e-05, global_step: 160, interval_runtime: 1.487, interval_samples_per_second: 5.38, interval_steps_per_second: 6.725, epoch: 0.904[0m
[32m[2022-09-05 11:55:38,449] [    INFO][0m - loss: 0.41169252, learning_rate: 2.855932203389831e-05, global_step: 170, interval_runtime: 1.4844, interval_samples_per_second: 5.389, interval_steps_per_second: 6.737, epoch: 0.9605[0m
[32m[2022-09-05 11:55:39,946] [    INFO][0m - loss: 0.40887976, learning_rate: 2.8474576271186442e-05, global_step: 180, interval_runtime: 1.4963, interval_samples_per_second: 5.347, interval_steps_per_second: 6.683, epoch: 1.0169[0m
[32m[2022-09-05 11:55:41,431] [    INFO][0m - loss: 0.62043257, learning_rate: 2.8389830508474575e-05, global_step: 190, interval_runtime: 1.4851, interval_samples_per_second: 5.387, interval_steps_per_second: 6.733, epoch: 1.0734[0m
[32m[2022-09-05 11:55:42,921] [    INFO][0m - loss: 0.46491714, learning_rate: 2.8305084745762712e-05, global_step: 200, interval_runtime: 1.4907, interval_samples_per_second: 5.367, interval_steps_per_second: 6.708, epoch: 1.1299[0m
[32m[2022-09-05 11:55:42,922] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 11:55:42,922] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-05 11:55:42,922] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 11:55:42,922] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 11:55:42,922] [    INFO][0m -   Total prediction steps = 177[0m
[32m[2022-09-05 11:55:53,951] [    INFO][0m - eval_loss: 0.436301052570343, eval_accuracy: 0.2871287128712871, eval_runtime: 11.0284, eval_samples_per_second: 128.214, eval_steps_per_second: 16.049, epoch: 1.1299[0m
[32m[2022-09-05 11:55:53,978] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-200[0m
[32m[2022-09-05 11:55:53,979] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 11:55:57,218] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-200/tokenizer_config.json[0m
[32m[2022-09-05 11:55:57,219] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-200/special_tokens_map.json[0m
[32m[2022-09-05 11:56:04,256] [    INFO][0m - loss: 0.42683167, learning_rate: 2.822033898305085e-05, global_step: 210, interval_runtime: 21.3343, interval_samples_per_second: 0.375, interval_steps_per_second: 0.469, epoch: 1.1864[0m
[32m[2022-09-05 11:56:05,740] [    INFO][0m - loss: 0.68661494, learning_rate: 2.8135593220338985e-05, global_step: 220, interval_runtime: 1.4843, interval_samples_per_second: 5.39, interval_steps_per_second: 6.737, epoch: 1.2429[0m
[32m[2022-09-05 11:56:07,227] [    INFO][0m - loss: 0.50860624, learning_rate: 2.805084745762712e-05, global_step: 230, interval_runtime: 1.4865, interval_samples_per_second: 5.382, interval_steps_per_second: 6.727, epoch: 1.2994[0m
[32m[2022-09-05 11:56:08,709] [    INFO][0m - loss: 0.61066027, learning_rate: 2.7966101694915255e-05, global_step: 240, interval_runtime: 1.4823, interval_samples_per_second: 5.397, interval_steps_per_second: 6.746, epoch: 1.3559[0m
[32m[2022-09-05 11:56:10,195] [    INFO][0m - loss: 0.34387603, learning_rate: 2.788135593220339e-05, global_step: 250, interval_runtime: 1.4865, interval_samples_per_second: 5.382, interval_steps_per_second: 6.727, epoch: 1.4124[0m
[32m[2022-09-05 11:56:11,682] [    INFO][0m - loss: 0.51428666, learning_rate: 2.7796610169491528e-05, global_step: 260, interval_runtime: 1.4861, interval_samples_per_second: 5.383, interval_steps_per_second: 6.729, epoch: 1.4689[0m
[32m[2022-09-05 11:56:13,166] [    INFO][0m - loss: 0.45358911, learning_rate: 2.771186440677966e-05, global_step: 270, interval_runtime: 1.4849, interval_samples_per_second: 5.387, interval_steps_per_second: 6.734, epoch: 1.5254[0m
[32m[2022-09-05 11:56:14,650] [    INFO][0m - loss: 0.54423518, learning_rate: 2.7627118644067794e-05, global_step: 280, interval_runtime: 1.4834, interval_samples_per_second: 5.393, interval_steps_per_second: 6.741, epoch: 1.5819[0m
[32m[2022-09-05 11:56:16,131] [    INFO][0m - loss: 0.33501754, learning_rate: 2.7542372881355934e-05, global_step: 290, interval_runtime: 1.4812, interval_samples_per_second: 5.401, interval_steps_per_second: 6.751, epoch: 1.6384[0m
[32m[2022-09-05 11:56:17,615] [    INFO][0m - loss: 0.52890615, learning_rate: 2.7457627118644068e-05, global_step: 300, interval_runtime: 1.4836, interval_samples_per_second: 5.392, interval_steps_per_second: 6.74, epoch: 1.6949[0m
[32m[2022-09-05 11:56:17,615] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 11:56:17,616] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-05 11:56:17,616] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 11:56:17,616] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 11:56:17,616] [    INFO][0m -   Total prediction steps = 177[0m
[32m[2022-09-05 11:56:28,615] [    INFO][0m - eval_loss: 0.49136123061180115, eval_accuracy: 0.21287128712871287, eval_runtime: 10.9992, eval_samples_per_second: 128.555, eval_steps_per_second: 16.092, epoch: 1.6949[0m
[32m[2022-09-05 11:56:28,642] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-300[0m
[32m[2022-09-05 11:56:28,642] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 11:56:31,832] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-300/tokenizer_config.json[0m
[32m[2022-09-05 11:56:31,832] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-300/special_tokens_map.json[0m
[32m[2022-09-05 11:56:38,873] [    INFO][0m - loss: 0.4284781, learning_rate: 2.7372881355932204e-05, global_step: 310, interval_runtime: 21.258, interval_samples_per_second: 0.376, interval_steps_per_second: 0.47, epoch: 1.7514[0m
[32m[2022-09-05 11:56:40,362] [    INFO][0m - loss: 0.40348873, learning_rate: 2.7288135593220337e-05, global_step: 320, interval_runtime: 1.4881, interval_samples_per_second: 5.376, interval_steps_per_second: 6.72, epoch: 1.8079[0m
[32m[2022-09-05 11:56:41,851] [    INFO][0m - loss: 0.60747266, learning_rate: 2.7203389830508477e-05, global_step: 330, interval_runtime: 1.4901, interval_samples_per_second: 5.369, interval_steps_per_second: 6.711, epoch: 1.8644[0m
[32m[2022-09-05 11:56:43,340] [    INFO][0m - loss: 0.40645547, learning_rate: 2.711864406779661e-05, global_step: 340, interval_runtime: 1.4885, interval_samples_per_second: 5.374, interval_steps_per_second: 6.718, epoch: 1.9209[0m
[32m[2022-09-05 11:56:44,823] [    INFO][0m - loss: 0.37103195, learning_rate: 2.7033898305084747e-05, global_step: 350, interval_runtime: 1.4832, interval_samples_per_second: 5.394, interval_steps_per_second: 6.742, epoch: 1.9774[0m
[32m[2022-09-05 11:56:46,318] [    INFO][0m - loss: 0.41577706, learning_rate: 2.6949152542372884e-05, global_step: 360, interval_runtime: 1.4956, interval_samples_per_second: 5.349, interval_steps_per_second: 6.686, epoch: 2.0339[0m
[32m[2022-09-05 11:56:47,807] [    INFO][0m - loss: 0.3668056, learning_rate: 2.6864406779661017e-05, global_step: 370, interval_runtime: 1.4886, interval_samples_per_second: 5.374, interval_steps_per_second: 6.718, epoch: 2.0904[0m
[32m[2022-09-05 11:56:49,291] [    INFO][0m - loss: 0.54646659, learning_rate: 2.6779661016949153e-05, global_step: 380, interval_runtime: 1.4841, interval_samples_per_second: 5.39, interval_steps_per_second: 6.738, epoch: 2.1469[0m
[32m[2022-09-05 11:56:50,779] [    INFO][0m - loss: 0.48607502, learning_rate: 2.6694915254237287e-05, global_step: 390, interval_runtime: 1.4883, interval_samples_per_second: 5.375, interval_steps_per_second: 6.719, epoch: 2.2034[0m
[32m[2022-09-05 11:56:52,272] [    INFO][0m - loss: 0.43336382, learning_rate: 2.6610169491525427e-05, global_step: 400, interval_runtime: 1.4931, interval_samples_per_second: 5.358, interval_steps_per_second: 6.698, epoch: 2.2599[0m
[32m[2022-09-05 11:56:52,273] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 11:56:52,273] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-05 11:56:52,273] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 11:56:52,273] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 11:56:52,273] [    INFO][0m -   Total prediction steps = 177[0m
[32m[2022-09-05 11:57:03,339] [    INFO][0m - eval_loss: 0.42708754539489746, eval_accuracy: 0.09405940594059406, eval_runtime: 11.0649, eval_samples_per_second: 127.792, eval_steps_per_second: 15.997, epoch: 2.2599[0m
[32m[2022-09-05 11:57:03,366] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-400[0m
[32m[2022-09-05 11:57:03,366] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 11:57:06,520] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-400/tokenizer_config.json[0m
[32m[2022-09-05 11:57:06,521] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-400/special_tokens_map.json[0m
[32m[2022-09-05 11:57:13,553] [    INFO][0m - loss: 0.5145093, learning_rate: 2.652542372881356e-05, global_step: 410, interval_runtime: 21.2806, interval_samples_per_second: 0.376, interval_steps_per_second: 0.47, epoch: 2.3164[0m
[32m[2022-09-05 11:57:15,034] [    INFO][0m - loss: 0.40842419, learning_rate: 2.6440677966101696e-05, global_step: 420, interval_runtime: 1.4808, interval_samples_per_second: 5.403, interval_steps_per_second: 6.753, epoch: 2.3729[0m
[32m[2022-09-05 11:57:16,521] [    INFO][0m - loss: 0.47473187, learning_rate: 2.635593220338983e-05, global_step: 430, interval_runtime: 1.4871, interval_samples_per_second: 5.38, interval_steps_per_second: 6.724, epoch: 2.4294[0m
[32m[2022-09-05 11:57:18,005] [    INFO][0m - loss: 0.5764739, learning_rate: 2.627118644067797e-05, global_step: 440, interval_runtime: 1.4843, interval_samples_per_second: 5.39, interval_steps_per_second: 6.737, epoch: 2.4859[0m
[32m[2022-09-05 11:57:19,489] [    INFO][0m - loss: 0.44583459, learning_rate: 2.6186440677966103e-05, global_step: 450, interval_runtime: 1.4835, interval_samples_per_second: 5.393, interval_steps_per_second: 6.741, epoch: 2.5424[0m
[32m[2022-09-05 11:57:20,974] [    INFO][0m - loss: 0.57536488, learning_rate: 2.6101694915254236e-05, global_step: 460, interval_runtime: 1.4855, interval_samples_per_second: 5.385, interval_steps_per_second: 6.732, epoch: 2.5989[0m
[32m[2022-09-05 11:57:22,461] [    INFO][0m - loss: 0.5794363, learning_rate: 2.6016949152542372e-05, global_step: 470, interval_runtime: 1.4866, interval_samples_per_second: 5.382, interval_steps_per_second: 6.727, epoch: 2.6554[0m
[32m[2022-09-05 11:57:23,950] [    INFO][0m - loss: 0.43202877, learning_rate: 2.593220338983051e-05, global_step: 480, interval_runtime: 1.4893, interval_samples_per_second: 5.372, interval_steps_per_second: 6.714, epoch: 2.7119[0m
[32m[2022-09-05 11:57:25,434] [    INFO][0m - loss: 0.37606223, learning_rate: 2.5847457627118646e-05, global_step: 490, interval_runtime: 1.4843, interval_samples_per_second: 5.39, interval_steps_per_second: 6.737, epoch: 2.7684[0m
[32m[2022-09-05 11:57:26,920] [    INFO][0m - loss: 0.37482421, learning_rate: 2.576271186440678e-05, global_step: 500, interval_runtime: 1.4859, interval_samples_per_second: 5.384, interval_steps_per_second: 6.73, epoch: 2.8249[0m
[32m[2022-09-05 11:57:26,921] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 11:57:26,921] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-05 11:57:26,921] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 11:57:26,921] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 11:57:26,921] [    INFO][0m -   Total prediction steps = 177[0m
[32m[2022-09-05 11:57:37,893] [    INFO][0m - eval_loss: 0.4410605728626251, eval_accuracy: 0.21782178217821782, eval_runtime: 10.9715, eval_samples_per_second: 128.879, eval_steps_per_second: 16.133, epoch: 2.8249[0m
[32m[2022-09-05 11:57:37,920] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-500[0m
[32m[2022-09-05 11:57:37,920] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 11:57:41,402] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-500/tokenizer_config.json[0m
[32m[2022-09-05 11:57:41,402] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-500/special_tokens_map.json[0m
[32m[2022-09-05 11:57:48,447] [    INFO][0m - loss: 0.36632986, learning_rate: 2.567796610169492e-05, global_step: 510, interval_runtime: 21.5269, interval_samples_per_second: 0.372, interval_steps_per_second: 0.465, epoch: 2.8814[0m
[32m[2022-09-05 11:57:49,936] [    INFO][0m - loss: 0.35532248, learning_rate: 2.5593220338983052e-05, global_step: 520, interval_runtime: 1.489, interval_samples_per_second: 5.373, interval_steps_per_second: 6.716, epoch: 2.9379[0m
[32m[2022-09-05 11:57:51,423] [    INFO][0m - loss: 0.32380049, learning_rate: 2.550847457627119e-05, global_step: 530, interval_runtime: 1.4862, interval_samples_per_second: 5.383, interval_steps_per_second: 6.728, epoch: 2.9944[0m
[32m[2022-09-05 11:57:52,929] [    INFO][0m - loss: 0.4026195, learning_rate: 2.5423728813559322e-05, global_step: 540, interval_runtime: 1.5064, interval_samples_per_second: 5.311, interval_steps_per_second: 6.638, epoch: 3.0508[0m
[32m[2022-09-05 11:57:54,427] [    INFO][0m - loss: 0.46686306, learning_rate: 2.5338983050847458e-05, global_step: 550, interval_runtime: 1.4981, interval_samples_per_second: 5.34, interval_steps_per_second: 6.675, epoch: 3.1073[0m
[32m[2022-09-05 11:57:55,922] [    INFO][0m - loss: 0.46841826, learning_rate: 2.5254237288135595e-05, global_step: 560, interval_runtime: 1.4948, interval_samples_per_second: 5.352, interval_steps_per_second: 6.69, epoch: 3.1638[0m
[32m[2022-09-05 11:57:57,414] [    INFO][0m - loss: 0.48549824, learning_rate: 2.5169491525423728e-05, global_step: 570, interval_runtime: 1.4919, interval_samples_per_second: 5.362, interval_steps_per_second: 6.703, epoch: 3.2203[0m
[32m[2022-09-05 11:57:58,904] [    INFO][0m - loss: 0.43424649, learning_rate: 2.5084745762711865e-05, global_step: 580, interval_runtime: 1.4907, interval_samples_per_second: 5.366, interval_steps_per_second: 6.708, epoch: 3.2768[0m
[32m[2022-09-05 11:58:00,400] [    INFO][0m - loss: 0.44393711, learning_rate: 2.5e-05, global_step: 590, interval_runtime: 1.4955, interval_samples_per_second: 5.349, interval_steps_per_second: 6.687, epoch: 3.3333[0m
[32m[2022-09-05 11:58:01,894] [    INFO][0m - loss: 0.48641191, learning_rate: 2.4915254237288138e-05, global_step: 600, interval_runtime: 1.4943, interval_samples_per_second: 5.354, interval_steps_per_second: 6.692, epoch: 3.3898[0m
[32m[2022-09-05 11:58:01,895] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 11:58:01,895] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-05 11:58:01,895] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 11:58:01,895] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 11:58:01,895] [    INFO][0m -   Total prediction steps = 177[0m
[32m[2022-09-05 11:58:12,948] [    INFO][0m - eval_loss: 0.41155633330345154, eval_accuracy: 0.3217821782178218, eval_runtime: 11.0526, eval_samples_per_second: 127.934, eval_steps_per_second: 16.014, epoch: 3.3898[0m
[32m[2022-09-05 11:58:12,975] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-600[0m
[32m[2022-09-05 11:58:12,975] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 11:58:16,142] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-600/tokenizer_config.json[0m
[32m[2022-09-05 11:58:16,143] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-600/special_tokens_map.json[0m
[32m[2022-09-05 11:58:23,118] [    INFO][0m - loss: 0.38281915, learning_rate: 2.483050847457627e-05, global_step: 610, interval_runtime: 21.2241, interval_samples_per_second: 0.377, interval_steps_per_second: 0.471, epoch: 3.4463[0m
[32m[2022-09-05 11:58:24,608] [    INFO][0m - loss: 0.4627243, learning_rate: 2.4745762711864408e-05, global_step: 620, interval_runtime: 1.4889, interval_samples_per_second: 5.373, interval_steps_per_second: 6.716, epoch: 3.5028[0m
[32m[2022-09-05 11:58:26,091] [    INFO][0m - loss: 0.43669629, learning_rate: 2.4661016949152544e-05, global_step: 630, interval_runtime: 1.4837, interval_samples_per_second: 5.392, interval_steps_per_second: 6.74, epoch: 3.5593[0m
[32m[2022-09-05 11:58:27,574] [    INFO][0m - loss: 0.42505598, learning_rate: 2.4576271186440677e-05, global_step: 640, interval_runtime: 1.483, interval_samples_per_second: 5.394, interval_steps_per_second: 6.743, epoch: 3.6158[0m
[32m[2022-09-05 11:58:29,057] [    INFO][0m - loss: 0.39487, learning_rate: 2.4491525423728814e-05, global_step: 650, interval_runtime: 1.4835, interval_samples_per_second: 5.393, interval_steps_per_second: 6.741, epoch: 3.6723[0m
[32m[2022-09-05 11:58:30,539] [    INFO][0m - loss: 0.40979381, learning_rate: 2.440677966101695e-05, global_step: 660, interval_runtime: 1.4816, interval_samples_per_second: 5.399, interval_steps_per_second: 6.749, epoch: 3.7288[0m
[32m[2022-09-05 11:58:32,026] [    INFO][0m - loss: 0.33752956, learning_rate: 2.4322033898305087e-05, global_step: 670, interval_runtime: 1.4872, interval_samples_per_second: 5.379, interval_steps_per_second: 6.724, epoch: 3.7853[0m
[32m[2022-09-05 11:58:33,516] [    INFO][0m - loss: 0.39280076, learning_rate: 2.423728813559322e-05, global_step: 680, interval_runtime: 1.4902, interval_samples_per_second: 5.368, interval_steps_per_second: 6.711, epoch: 3.8418[0m
[32m[2022-09-05 11:58:35,001] [    INFO][0m - loss: 0.45744977, learning_rate: 2.4152542372881357e-05, global_step: 690, interval_runtime: 1.4839, interval_samples_per_second: 5.391, interval_steps_per_second: 6.739, epoch: 3.8983[0m
[32m[2022-09-05 11:58:36,482] [    INFO][0m - loss: 0.4807982, learning_rate: 2.4067796610169493e-05, global_step: 700, interval_runtime: 1.482, interval_samples_per_second: 5.398, interval_steps_per_second: 6.748, epoch: 3.9548[0m
[32m[2022-09-05 11:58:36,483] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 11:58:36,483] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-05 11:58:36,483] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 11:58:36,483] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 11:58:36,483] [    INFO][0m -   Total prediction steps = 177[0m
[32m[2022-09-05 11:58:47,448] [    INFO][0m - eval_loss: 0.41338542103767395, eval_accuracy: 0.26732673267326734, eval_runtime: 10.9644, eval_samples_per_second: 128.963, eval_steps_per_second: 16.143, epoch: 3.9548[0m
[32m[2022-09-05 11:58:47,478] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-700[0m
[32m[2022-09-05 11:58:47,478] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 11:58:50,645] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-700/tokenizer_config.json[0m
[32m[2022-09-05 11:58:50,646] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-700/special_tokens_map.json[0m
[32m[2022-09-05 11:58:57,573] [    INFO][0m - loss: 0.53869219, learning_rate: 2.3983050847457627e-05, global_step: 710, interval_runtime: 21.0904, interval_samples_per_second: 0.379, interval_steps_per_second: 0.474, epoch: 4.0113[0m
[32m[2022-09-05 11:58:59,056] [    INFO][0m - loss: 0.38820484, learning_rate: 2.3898305084745763e-05, global_step: 720, interval_runtime: 1.4829, interval_samples_per_second: 5.395, interval_steps_per_second: 6.744, epoch: 4.0678[0m
[32m[2022-09-05 11:59:00,545] [    INFO][0m - loss: 0.47860165, learning_rate: 2.3813559322033896e-05, global_step: 730, interval_runtime: 1.4886, interval_samples_per_second: 5.374, interval_steps_per_second: 6.718, epoch: 4.1243[0m
[32m[2022-09-05 11:59:02,035] [    INFO][0m - loss: 0.40819225, learning_rate: 2.3728813559322036e-05, global_step: 740, interval_runtime: 1.4904, interval_samples_per_second: 5.368, interval_steps_per_second: 6.709, epoch: 4.1808[0m
[32m[2022-09-05 11:59:03,527] [    INFO][0m - loss: 0.37766347, learning_rate: 2.364406779661017e-05, global_step: 750, interval_runtime: 1.4917, interval_samples_per_second: 5.363, interval_steps_per_second: 6.704, epoch: 4.2373[0m
[32m[2022-09-05 11:59:05,013] [    INFO][0m - loss: 0.52415671, learning_rate: 2.3559322033898306e-05, global_step: 760, interval_runtime: 1.4869, interval_samples_per_second: 5.38, interval_steps_per_second: 6.725, epoch: 4.2938[0m
[32m[2022-09-05 11:59:06,498] [    INFO][0m - loss: 0.41205544, learning_rate: 2.347457627118644e-05, global_step: 770, interval_runtime: 1.4841, interval_samples_per_second: 5.391, interval_steps_per_second: 6.738, epoch: 4.3503[0m
[32m[2022-09-05 11:59:07,984] [    INFO][0m - loss: 0.53358254, learning_rate: 2.338983050847458e-05, global_step: 780, interval_runtime: 1.4867, interval_samples_per_second: 5.381, interval_steps_per_second: 6.726, epoch: 4.4068[0m
[32m[2022-09-05 11:59:09,471] [    INFO][0m - loss: 0.47713895, learning_rate: 2.3305084745762712e-05, global_step: 790, interval_runtime: 1.4864, interval_samples_per_second: 5.382, interval_steps_per_second: 6.728, epoch: 4.4633[0m
[32m[2022-09-05 11:59:10,960] [    INFO][0m - loss: 0.41653962, learning_rate: 2.3220338983050846e-05, global_step: 800, interval_runtime: 1.489, interval_samples_per_second: 5.373, interval_steps_per_second: 6.716, epoch: 4.5198[0m
[32m[2022-09-05 11:59:10,960] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 11:59:10,960] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-05 11:59:10,960] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 11:59:10,960] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 11:59:10,961] [    INFO][0m -   Total prediction steps = 177[0m
[32m[2022-09-05 11:59:21,973] [    INFO][0m - eval_loss: 0.4110664129257202, eval_accuracy: 0.06435643564356436, eval_runtime: 11.0116, eval_samples_per_second: 128.41, eval_steps_per_second: 16.074, epoch: 4.5198[0m
[32m[2022-09-05 11:59:22,003] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-800[0m
[32m[2022-09-05 11:59:22,003] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 11:59:25,195] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-800/tokenizer_config.json[0m
[32m[2022-09-05 11:59:25,195] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-800/special_tokens_map.json[0m
[32m[2022-09-05 11:59:31,946] [    INFO][0m - loss: 0.42976484, learning_rate: 2.3135593220338986e-05, global_step: 810, interval_runtime: 20.9859, interval_samples_per_second: 0.381, interval_steps_per_second: 0.477, epoch: 4.5763[0m
[32m[2022-09-05 11:59:33,435] [    INFO][0m - loss: 0.41645718, learning_rate: 2.305084745762712e-05, global_step: 820, interval_runtime: 1.4889, interval_samples_per_second: 5.373, interval_steps_per_second: 6.716, epoch: 4.6328[0m
[32m[2022-09-05 11:59:34,921] [    INFO][0m - loss: 0.45348916, learning_rate: 2.2966101694915255e-05, global_step: 830, interval_runtime: 1.4862, interval_samples_per_second: 5.383, interval_steps_per_second: 6.728, epoch: 4.6893[0m
[32m[2022-09-05 11:59:36,408] [    INFO][0m - loss: 0.50835161, learning_rate: 2.288135593220339e-05, global_step: 840, interval_runtime: 1.4873, interval_samples_per_second: 5.379, interval_steps_per_second: 6.724, epoch: 4.7458[0m
[32m[2022-09-05 11:59:37,899] [    INFO][0m - loss: 0.50551896, learning_rate: 2.279661016949153e-05, global_step: 850, interval_runtime: 1.4905, interval_samples_per_second: 5.367, interval_steps_per_second: 6.709, epoch: 4.8023[0m
[32m[2022-09-05 11:59:39,388] [    INFO][0m - loss: 0.47917624, learning_rate: 2.271186440677966e-05, global_step: 860, interval_runtime: 1.4893, interval_samples_per_second: 5.372, interval_steps_per_second: 6.715, epoch: 4.8588[0m
[32m[2022-09-05 11:59:40,878] [    INFO][0m - loss: 0.33890135, learning_rate: 2.2627118644067798e-05, global_step: 870, interval_runtime: 1.4904, interval_samples_per_second: 5.368, interval_steps_per_second: 6.71, epoch: 4.9153[0m
[32m[2022-09-05 11:59:42,373] [    INFO][0m - loss: 0.47029867, learning_rate: 2.254237288135593e-05, global_step: 880, interval_runtime: 1.4951, interval_samples_per_second: 5.351, interval_steps_per_second: 6.688, epoch: 4.9718[0m
[32m[2022-09-05 11:59:43,868] [    INFO][0m - loss: 0.39959545, learning_rate: 2.2457627118644068e-05, global_step: 890, interval_runtime: 1.4942, interval_samples_per_second: 5.354, interval_steps_per_second: 6.693, epoch: 5.0282[0m
[32m[2022-09-05 11:59:45,357] [    INFO][0m - loss: 0.56410675, learning_rate: 2.2372881355932205e-05, global_step: 900, interval_runtime: 1.4893, interval_samples_per_second: 5.372, interval_steps_per_second: 6.714, epoch: 5.0847[0m
[32m[2022-09-05 11:59:45,357] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 11:59:45,358] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-05 11:59:45,358] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 11:59:45,358] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 11:59:45,358] [    INFO][0m -   Total prediction steps = 177[0m
[32m[2022-09-05 11:59:56,495] [    INFO][0m - eval_loss: 0.5551040172576904, eval_accuracy: 0.22277227722772278, eval_runtime: 11.1364, eval_samples_per_second: 126.972, eval_steps_per_second: 15.894, epoch: 5.0847[0m
[32m[2022-09-05 11:59:56,526] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-900[0m
[32m[2022-09-05 11:59:56,526] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 11:59:59,540] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-900/tokenizer_config.json[0m
[32m[2022-09-05 11:59:59,540] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-900/special_tokens_map.json[0m
[32m[2022-09-05 12:00:06,576] [    INFO][0m - loss: 0.50713325, learning_rate: 2.2288135593220338e-05, global_step: 910, interval_runtime: 21.2192, interval_samples_per_second: 0.377, interval_steps_per_second: 0.471, epoch: 5.1412[0m
[32m[2022-09-05 12:00:08,061] [    INFO][0m - loss: 0.60927248, learning_rate: 2.2203389830508474e-05, global_step: 920, interval_runtime: 1.485, interval_samples_per_second: 5.387, interval_steps_per_second: 6.734, epoch: 5.1977[0m
[32m[2022-09-05 12:00:09,550] [    INFO][0m - loss: 0.46059375, learning_rate: 2.211864406779661e-05, global_step: 930, interval_runtime: 1.4885, interval_samples_per_second: 5.374, interval_steps_per_second: 6.718, epoch: 5.2542[0m
[32m[2022-09-05 12:00:11,033] [    INFO][0m - loss: 0.28520627, learning_rate: 2.2033898305084748e-05, global_step: 940, interval_runtime: 1.4831, interval_samples_per_second: 5.394, interval_steps_per_second: 6.743, epoch: 5.3107[0m
[32m[2022-09-05 12:00:12,523] [    INFO][0m - loss: 0.749121, learning_rate: 2.194915254237288e-05, global_step: 950, interval_runtime: 1.4894, interval_samples_per_second: 5.371, interval_steps_per_second: 6.714, epoch: 5.3672[0m
[32m[2022-09-05 12:00:14,011] [    INFO][0m - loss: 0.45004778, learning_rate: 2.1864406779661017e-05, global_step: 960, interval_runtime: 1.4885, interval_samples_per_second: 5.375, interval_steps_per_second: 6.718, epoch: 5.4237[0m
[32m[2022-09-05 12:00:15,494] [    INFO][0m - loss: 0.43131948, learning_rate: 2.1779661016949154e-05, global_step: 970, interval_runtime: 1.4837, interval_samples_per_second: 5.392, interval_steps_per_second: 6.74, epoch: 5.4802[0m
[32m[2022-09-05 12:00:16,986] [    INFO][0m - loss: 0.4220005, learning_rate: 2.1694915254237287e-05, global_step: 980, interval_runtime: 1.491, interval_samples_per_second: 5.366, interval_steps_per_second: 6.707, epoch: 5.5367[0m
[32m[2022-09-05 12:00:18,471] [    INFO][0m - loss: 0.48177443, learning_rate: 2.1610169491525424e-05, global_step: 990, interval_runtime: 1.4856, interval_samples_per_second: 5.385, interval_steps_per_second: 6.731, epoch: 5.5932[0m
[32m[2022-09-05 12:00:19,958] [    INFO][0m - loss: 0.41065483, learning_rate: 2.152542372881356e-05, global_step: 1000, interval_runtime: 1.4867, interval_samples_per_second: 5.381, interval_steps_per_second: 6.726, epoch: 5.6497[0m
[32m[2022-09-05 12:00:19,958] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2022-09-05 12:00:19,958] [    INFO][0m -   Num examples = 1414[0m
[32m[2022-09-05 12:00:19,958] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 12:00:19,959] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 12:00:19,959] [    INFO][0m -   Total prediction steps = 177[0m
====================
[1 2 4 0 2 0 3 2 3 6 3 0 0 5 6 6 6 5 3 6 0 1 0 2 0 4 6 2 5 5 5 0 4 4 3 1 5
 3 5 2 5 3 0 5 0 5 6 4 2 5 5 3 5 6 4 0 2 4 2 3 5 1 3 1 2 3 0 1 1 5 3 5 0 4
 1 3 1 2 1 5 0 2 1 1 4 1 1 4 6 0 4 3 5 2 4 0 3 4 5 2 0 0 2 1 3 0 2 2 0 5 2
 2 4 2 2 6 4 5 3 3 5 1 3 1 6 1 2 3 5 3 0 2 1 4 6 4 1 5 4 5 1 5 3 3 5 4 4 4
 5 1 5 6 2 5 3 1 4 3 6 2 4 2 2 1 6 0 2 5 5 2 5 3 5 1 5 2 1 2 2 5 6 3 6 5 2
 5 5 6 4 4 2 3 5 2 5 5 4 1 4 0 2 6]
[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3
 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6
 6 6 6 6 6 6 6 6 6 6 6 6 6 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4
 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
[2 2 4 3 5 3 4 6 2 4 0 6 1 4 5 1 6 1 3 4 0 6 2 5 2 2 0 0 4 3 5 3 1 0 5 5 1
 3 0 4 3 4 5 2 3 5 2 4 3 2 1 3 1 1 3 4 1 0 1 0 5 6 2 2 6 6 1 2 0 6 3 6 6 3
 5 6 0 1 3 6 2 6 0 0 6 0 6 3 2 5 2 5 5 5 4 2 4 6 1 5 5 4 1 5 6 2 2 2 0 0 4
 4 4 2 4 5 3 5 5 2 2 6 0 5 1 0 4 3 4 3 2 0 6 6 5 1 1 5 2 6 6 3 5 3 5 0 0 0
 3 1 3 3 5 5 4 3 0 4 4 0 2 2 5 0 2 1 3 3 0 0 4 2 6 1 1 1 5 1 0 2 1 5 0 2 0
 3 0 1 0 0 3 2 4 1 2 1 1 1 0 1 1 2]
[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3
 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6
 6 6 6 6 6 6 6 6 6 6 6 6 6 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4
 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
[5 3 0 2 5 4 3 2 2 3 6 2 0 3 5 0 5 2 6 2 5 6 5 2 6 6 2 2 6 6 5 4 6 4 3 2 5
 5 4 3 1 3 0 3 2 5 2 4 1 0 5 5 2 3 3 2 1 2 2 0 4 3 5 1 5 6 0 6 3 0 3 5 0 1
 5 6 1 1 3 2 0 4 0 5 5 0 2 4 2 0 2 2 1 3 6 4 0 5 3 6 2 3 4 5 6 0 6 2 0 0 1
 1 4 5 6 4 4 3 3 4 2 5 3 1 1 5 4 4 5 3 2 4 2 1 3 2 6 5 5 1 5 5 5 3 0 4 0 0
 5 5 1 4 1 1 2 2 0 3 0 6 5 4 1 5 0 3 6 5 0 5 5 5 1 6 2 6 3 2 5 4 0 4 1 5 0
 5 5 0 1 3 1 2 2 0 2 6 6 5 0 6 1 2]
[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3
 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6
 6 6 6 6 6 6 6 6 6 6 6 6 6 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4
 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
[0 6 0 6 1 0 3 0 2 3 3 2 0 0 5 1 6 5 3 1 0 0 1 3 1 3 0 3 6 5 5 1 0 2 0 1 4
 6 5 2 1 1 3 5 5 5 5 2 3 0 4 2 3 2 5 2 2 0 1 1 5 3 4 1 3 5 1 6 4 0 3 2 4 3
 6 0 5 1 4 6 1 0 0 0 4 0 6 4 1 4 2 1 3 3 2 6 0 3 2 1 6 3 2 1 0 3 6 2 6 2 1
 1 0 3 4 1 3 3 3 3 0 0 4 1 2 5 2 4 6 5 0 6 2 2 0 4 2 6 1 5 0 5 2 3 1 0 5 6
 4 3 1 3 5 1 6 6 5 1 6 3 6 4 1 1 5 1 6 1 0 2 4 5 5 5 5 4 5 2 1 4 3 2 1 5 6
 5 5 2 4 6 2 5 4 4 5 0 4 4 0 3 5 6]
[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3
 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6
 6 6 6 6 6 6 6 6 6 6 6 6 6 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4
 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
[5 2 0 2 5 2 6 2 2 6 3 1 2 0 5 1 3 1 1 6 0 0 1 6 0 3 5 0 5 6 5 0 6 5 5 5 6
 5 5 3 5 1 1 5 0 5 0 3 1 1 3 5 0 3 1 2 2 4 1 1 5 4 2 1 4 5 4 6 3 6 3 4 4 3
 4 5 5 5 1 6 6 0 0 5 0 3 6 3 0 4 1 5 5 3 4 1 2 0 1 4 6 4 4 1 0 2 4 2 6 4 1
 1 4 3 1 4 4 4 0 3 5 5 2 5 2 5 4 5 5 5 0 2 1 1 6 0 0 5 1 6 6 5 2 3 4 2 5 0
 1 4 3 1 5 1 3 6 0 2 1 1 4 2 5 0 4 3 1 2 5 1 5 6 5 6 5 1 5 2 1 1 0 6 1 5 6
 4 3 2 1 5 1 0 4 6 4 3 4 1 0 6 0 2]
[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3
 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6
 6 6 6 6 6 6 6 6 6 6 6 6 6 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4
 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
[5 5 5 2 1 2 4 2 5 2 5 1 2 0 2 5 1 5 2 4 0 0 2 2 6 3 2 2 2 5 5 2 3 4 3 5 6
 2 4 5 5 6 0 4 0 2 2 3 4 1 5 4 5 0 3 2 0 6 6 5 5 4 6 1 6 6 0 5 3 6 6 4 1 3
 5 5 6 6 1 6 1 5 3 5 1 6 6 6 1 0 4 5 5 4 4 4 0 0 1 4 1 2 1 1 3 3 6 0 0 0 4
 6 4 5 4 4 0 6 0 6 2 5 3 0 2 5 5 5 3 0 6 4 2 5 3 0 2 5 0 3 1 0 5 0 1 2 1 0
 1 2 5 2 6 4 6 4 6 1 1 5 3 0 0 0 0 3 5 0 0 1 6 2 0 2 1 0 1 1 3 3 1 1 1 1 2
 5 5 3 3 1 1 1 6 0 2 1 6 1 1 1 0 5]
[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3
 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6
 6 6 6 6 6 6 6 6 6 6 6 6 6 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4
 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
[2 2 0 2 1 0 3 2 5 2 5 5 0 2 5 6 6 5 6 4 0 0 6 4 2 4 6 3 0 4 5 0 3 3 3 1 4
 3 4 3 3 3 1 6 4 5 6 3 4 6 5 4 5 2 3 0 6 0 6 5 5 1 6 1 3 3 0 0 0 6 5 4 0 0
 5 0 5 4 4 6 6 1 0 4 6 4 1 6 4 4 1 2 5 2 4 4 0 2 1 4 0 6 1 0 0 0 2 4 0 0 4
 2 4 3 4 4 4 4 4 3 4 5 3 5 5 1 5 2 2 0 3 0 1 4 6 6 2 1 1 3 0 2 5 3 2 4 4 2
 0 6 5 2 5 0 3 6 3 1 2 5 3 0 0 0 0 1 4 5 4 2 6 2 6 2 1 3 5 1 0 3 3 1 1 5 6
 5 5 2 6 1 1 1 3 0 2 3 4 1 0 1 2 1]
[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3
 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6
 6 6 6 6 6 6 6 6 6 6 6 6 6 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4
 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
[5 1 1 2 3 0 3 0 0 4 5 2 5 4 5 6 6 0 3 6 0 3 1 4 2 3 6 3 5 4 5 0 2 2 0 1 6
 1 4 5 1 1 1 6 5 5 5 3 0 4 4 2 5 2 3 0 2 3 1 2 4 1 1 1 3 3 0 4 0 3 5 5 2 0
 5 0 5 1 3 4 2 0 0 0 4 3 5 4 5 2 1 2 1 1 0 1 0 2 5 3 6 1 2 1 0 2 1 5 3 5 2
 2 5 3 1 0 4 2 3 3 0 5 2 3 1 6 1 3 4 0 0 6 2 4 6 6 2 4 4 6 0 5 5 3 1 0 4 1
 5 6 5 2 0 5 3 6 2 2 6 5 3 2 0 1 3 5 4 1 4 2 3 6 2 6 2 4 5 2 3 1 3 0 4 5 6
 5 3 2 5 6 4 2 4 3 5 5 4 2 2 2 2 0]
[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3
 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6
 6 6 6 6 6 6 6 6 6 6 6 6 6 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4
 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
[2 1 4 2 5 0 3 1 1 0 4 1 2 0 0 3 0 5 3 2 0 3 3 4 0 5 6 2 4 4 5 4 0 5 4 1 6
 3 4 3 3 3 1 1 3 2 0 3 0 6 3 6 5 2 0 6 2 3 2 0 5 3 3 6 6 3 4 3 3 4 3 5 0 3
 5 5 5 4 4 6 6 1 1 3 4 4 5 6 0 4 5 2 4 4 5 4 2 2 5 4 2 0 4 1 5 0 6 4 0 4 2
 3 3 3 4 4 4 6 3 6 4 1 2 3 5 2 1 1 5 0 0 3 1 3 5 1 4 5 4 6 0 5 2 3 6 4 0 1
 5 4 5 2 5 1 3 6 3 5 6 5 0 2 0 0 0 5 4 5 0 0 0 2 5 5 1 4 5 1 3 3 0 0 6 5 0
 5 5 0 3 5 1 6 0 6 5 2 4 1 0 1 2 5]
[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3
 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6
 6 6 6 6 6 6 6 6 6 6 6 6 6 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4
 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
[2 5 0 2 3 6 5 2 1 4 0 1 1 2 5 3 6 2 1 6 2 3 1 4 0 0 5 2 5 4 5 0 0 2 4 1 6
 3 4 3 3 1 5 1 3 0 0 4 5 6 2 2 5 2 3 0 1 0 5 0 2 1 2 1 2 5 0 2 5 6 0 4 0 0
 5 0 5 4 1 5 6 1 1 4 3 5 6 0 0 4 3 5 4 1 4 4 5 0 1 3 2 1 4 1 4 2 4 4 0 0 2
 4 4 3 4 4 4 5 3 3 2 5 3 6 6 5 5 5 2 0 0 6 2 4 5 1 2 6 1 5 2 4 5 3 5 0 4 1
 1 2 5 2 0 0 3 6 6 2 3 5 3 2 5 0 0 2 1 0 0 4 0 6 5 6 0 4 5 1 3 1 3 0 1 5 6
 1 5 2 5 3 1 2 0 6 5 2 4 1 2 2 2 2]
[32m[2022-09-05 12:00:31,014] [    INFO][0m - eval_loss: 0.41095730662345886, eval_accuracy: 0.24752475247524752, eval_runtime: 11.0546, eval_samples_per_second: 127.911, eval_steps_per_second: 16.011, epoch: 5.6497[0m
[32m[2022-09-05 12:00:31,041] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/checkpoint-1000[0m
[32m[2022-09-05 12:00:31,042] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 12:00:34,252] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/checkpoint-1000/tokenizer_config.json[0m
[32m[2022-09-05 12:00:34,252] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/checkpoint-1000/special_tokens_map.json[0m
[32m[2022-09-05 12:00:39,441] [    INFO][0m - 
Training completed. 
[0m
[32m[2022-09-05 12:00:39,441] [    INFO][0m - Loading best model from ./checkpoints_chid/checkpoint-600 (score: 0.3217821782178218).[0m
[32m[2022-09-05 12:00:40,400] [    INFO][0m - train_runtime: 348.1989, train_samples_per_second: 81.218, train_steps_per_second: 10.167, train_loss: 0.5002363774776458, epoch: 5.6497[0m
[32m[2022-09-05 12:00:40,442] [    INFO][0m - Saving model checkpoint to ./checkpoints_chid/[0m
[32m[2022-09-05 12:00:40,442] [    INFO][0m - Trainer.model is not a `PretrainedModel`, only saving its state dict.[0m
[32m[2022-09-05 12:00:43,614] [    INFO][0m - tokenizer config file saved in ./checkpoints_chid/tokenizer_config.json[0m
[32m[2022-09-05 12:00:43,614] [    INFO][0m - Special tokens file saved in ./checkpoints_chid/special_tokens_map.json[0m
[32m[2022-09-05 12:00:43,615] [    INFO][0m - ***** train metrics *****[0m
[32m[2022-09-05 12:00:43,616] [    INFO][0m -   epoch                    =     5.6497[0m
[32m[2022-09-05 12:00:43,616] [    INFO][0m -   train_loss               =     0.5002[0m
[32m[2022-09-05 12:00:43,616] [    INFO][0m -   train_runtime            = 0:05:48.19[0m
[32m[2022-09-05 12:00:43,616] [    INFO][0m -   train_samples_per_second =     81.218[0m
[32m[2022-09-05 12:00:43,616] [    INFO][0m -   train_steps_per_second   =     10.167[0m
[32m[2022-09-05 12:00:43,623] [    INFO][0m - ***** Running Prediction *****[0m
[32m[2022-09-05 12:00:43,623] [    INFO][0m -   Num examples = 14014[0m
[32m[2022-09-05 12:00:43,623] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2022-09-05 12:00:43,623] [    INFO][0m -   Total Batch size = 8[0m
[32m[2022-09-05 12:00:43,623] [    INFO][0m -   Total prediction steps = 1752[0m
run.sh: line 52: 73438 Killed                  CUDA_VISIBLE_DEVICES=$device python train_single.py --output_dir ./checkpoints_$task_name/ --prompt "$prompt" --max_seq_length $max_length --per_device_eval_batch_size $batch_size --per_device_train_batch_size $batch_size --model_name_or_path ernie-3.0-base-zh --split_id few_all --task_name $task_name --metric_for_best_model accuracy --disable_tqdm True --do_test --eval_steps 100 --save_steps 100 --num_train_epochs 20 --logging_steps 10 --learning_rate 3e-5 --ppt_learning_rate 3e-4 --load_best_model_at_end $is_train --do_train $is_train --do_eval $is_train --do_save $is_train
run.sh: line 58: --freeze_plm: command not found
